- en: Preprocessing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理
- en: From here, all our chapters will mostly contain code. I want to remind all my
    readers to run and develop the code at their end. Let's start the coding ninja
    journey.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 从此处开始，我们的所有章节大多将包含代码。我想提醒所有读者在自己端运行并开发代码。让我们开始编程忍者之旅。
- en: 'In this chapter, we will be learning how to do preprocessing according to the
    different NLP applications. We will learn the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何根据不同的NLP应用进行预处理。我们将学习以下主题：
- en: Handling corpus-raw text
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理语料库-原始文本
- en: Handling corpus-raw sentences
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理语料库-原始句子
- en: Basic preprocessing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础预处理
- en: Practical and customized preprocessing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实践和定制化预处理
- en: Handling corpus-raw text
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理语料库-原始文本
- en: In this section, we will see how to get the raw text and, in the following section,
    we will preprocess text and identify the sentences.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解如何获取原始文本，在接下来的章节中，我们将对文本进行预处理并识别句子。
- en: 'The process for this section is given in *Figure 4.1*:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的过程见*图 4.1*：
- en: '![](img/85eda8bb-a6e5-4c06-9569-9b370ab22d78.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85eda8bb-a6e5-4c06-9569-9b370ab22d78.png)'
- en: 'Figure 4.1: Process of handling corpus-raw text'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：处理语料库-原始文本的过程
- en: Getting raw text
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取原始文本
- en: In this section, we will use three sources where we can get the raw text data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用三个来源来获取原始文本数据。
- en: 'The following are the data sources:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是数据来源：
- en: Raw text file
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始文本文件
- en: Define raw data text inside a script in the form of a local variable
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在脚本中以本地变量的形式定义原始数据文本
- en: Use any of the available corpus from `nltk`
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`nltk`提供的任何语料库
- en: 'Let''s begin:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始：
- en: 'Raw text file access: I have a `.txt` file saved on my local computer which
    contains text data in the form of a paragraph. I want to read the content of that
    file and then load the content as the next step. I will run a sentence tokenizer
    to get the sentences out of it.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始文本文件访问：我在本地计算机上保存了一个`.txt`文件，文件中包含以段落形式呈现的文本数据。我希望读取该文件的内容，并将其作为下一步加载。我将运行一个句子分词器以提取其中的句子。
- en: 'Define raw data text inside a script in the form of a local variable: If we
    have a small amount of data, then we can assign the data to a local string variable.
    For example: **Text = This is the sentence, this is another example**.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在脚本中以本地变量的形式定义原始数据文本：如果我们有少量数据，可以将数据分配给本地字符串变量。例如：**Text = 这是一个句子，这是另一个示例**。
- en: 'Use an available corpus from `nltk`: We can import an available corpus such
    as the `brown` corpus, `gutenberg` corpus, and so on from `nltk` and load the
    content.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`nltk`提供的语料库：我们可以从`nltk`导入可用的语料库，如`brown`语料库、`gutenberg`语料库等，并加载其内容。
- en: 'I have defined three functions:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我已定义了三个函数：
- en: '`fileread()`: This reads the content of a file'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fileread()`：此函数读取文件内容'
- en: '`localtextvalue()`: This loads locally defined text'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`localtextvalue()`：此函数加载本地定义的文本'
- en: '`readcorpus()`: This reads the `gutenberg` corpus content'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readcorpus()`：此函数读取`gutenberg`语料库内容'
- en: 'Refer to the code snippet given in *Figure 4.2*, which describes all the three
    cases previously defined:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅*图 4.2*中给出的代码片段，其中描述了之前定义的三种情况：
- en: '![](img/fa9692ee-16ab-4ad2-be27-3a636801b560.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa9692ee-16ab-4ad2-be27-3a636801b560.png)'
- en: 'Figure 4.2: Various ways to get the raw data'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：获取原始数据的各种方式
- en: 'You can find the code by clicking on the GitHub link: [https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_1_processrawtext.py](https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_1_processrawtext.py)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过点击GitHub链接找到代码：[https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_1_processrawtext.py](https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_1_processrawtext.py)
- en: Lowercase conversion
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小写转换
- en: Converting all your data to lowercase helps in the process of preprocessing
    and in later stages in the NLP application, when you are doing parsing.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有数据转换为小写有助于预处理过程，并且在NLP应用的后续阶段（例如解析时）也会有帮助。
- en: 'So, converting the text to its lowercase format is quite easy. You can find
    the code on this GitHub link: [https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_4_wordtokenization.py](https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_4_wordtokenization.py)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将文本转换为小写格式非常简单。你可以通过此GitHub链接找到代码：[https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_4_wordtokenization.py](https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_4_wordtokenization.py)
- en: 'You can find the code snippet in *Figure 4.3*:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 4.3*中找到代码片段：
- en: '![](img/0ab65a96-ba98-434d-925a-2309c62f409f.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ab65a96-ba98-434d-925a-2309c62f409f.png)'
- en: 'Figure 4.3: Converting data to lowercase'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：将数据转换为小写
- en: 'The output of the preceding code snippet is as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码片段的输出如下：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Sentence tokenization
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 句子分词
- en: In raw text data, data is in paragraph form. Now, if you want the sentences
    from the paragraph, then you need to tokenize at sentence level.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始文本数据中，数据是段落形式的。如果你想从段落中提取句子，那么就需要进行句子级别的标记化。
- en: Sentence tokenization is the process of identifying the boundary of the sentences.
    It is also called **sentence boundary detection** or **sentence segmentation**
    or **sentence boundary disambiguation**. This process identifies the sentences
    starting and ending points.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 句子标记化是识别句子边界的过程。它也被称为**句子边界检测**、**句子分割**或**句子边界消歧义**。这个过程识别句子的起始和结束点。
- en: Some of the specialized cases need a customized rule for the sentence tokenizer
    as well.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一些专门的情况需要为句子标记化器定制规则。
- en: 'The following open source tools are available for performing sentence tokenization:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是可用于执行句子标记化的开源工具：
- en: OpenNLP
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenNLP
- en: Stanford CoreNLP
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stanford CoreNLP
- en: GATE
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GATE
- en: nltk
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: nltk
- en: Here we are using the `nltk` sentence tokenizer.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们使用的是`nltk`句子标记化器。
- en: 'We are using `sent_tokenize` from `nltk` and will import it as `st`:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用来自`nltk`的`sent_tokenize`并将其导入为`st`：
- en: '`sent_tokenize(rawtext)`: This takes a raw data string as an argument'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sent_tokenize(rawtext)`：它接受一个原始数据字符串作为参数。'
- en: '`st(filecontentdetails)`: This is our customized raw data, which is provided
    as an input argument'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`st(filecontentdetails)`：这是我们自定义的原始数据，它作为输入参数提供。'
- en: 'You can find the code on this GitHub Link: [https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_1_processrawtext.py](https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_1_processrawtext.py).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个 GitHub 链接上找到代码：[https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_1_processrawtext.py](https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_1_processrawtext.py)。
- en: 'You can see the code in the following code snippet in *Figure 4.4*:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 4.4*中的以下代码片段中查看代码：
- en: '![](img/9b94d00c-8a6e-480b-bebf-722e48dc8940.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b94d00c-8a6e-480b-bebf-722e48dc8940.png)'
- en: 'Figure 4.4: Code snippet for nltk sentence tokenizer'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：nltk 句子标记化器的代码片段
- en: Challenges of sentence tokenization
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 句子标记化的挑战
- en: At first glance, you would ask, what's the big deal about finding out the sentence
    boundary from the given raw text?
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，你可能会问，从给定的原始文本中找出句子边界有什么大不了的？
- en: Sentence tokenization varies from language to language.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 句子标记化因语言而异。
- en: 'Things get complicated when you have the following scenarios to handle. We
    are using examples to explain the cases:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要处理以下情况时，事情变得复杂起来。我们将使用示例来解释这些情况：
- en: 'If there is small letter after a dot, then the sentence should not split after
    the dot. The following is an example:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果句号后面跟着小写字母，则句子不应在句号后分割。以下是一个例子：
- en: 'Sentence: He has completed his Ph.D. degree. He is happy.'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子：他完成了他的 Ph.D. 学位。他很高兴。
- en: In the preceding example, the sentence tokenizer should split the sentence after
    **degree**, not after **Ph.D.**
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在上面的例子中，句子标记化器应该在**degree**后分割句子，而不是在**Ph.D.**后分割。
- en: 'If there is a small letter after the dot, then the sentence should be split
    after the dot. This is a common mistake. Let''s take an example:'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果句号后面跟着小写字母，则句子应在句号后进行分割。这是一个常见的错误。让我们来看一个例子：
- en: 'Sentence: This is an apple.an apple is good for health.'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子：这是一个苹果。一个苹果有益于健康。
- en: In the preceding example, the sentence tokenizer should split the sentence after
    **apple**.
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在上面的例子中，句子标记化器应该在**apple**后分割句子。
- en: 'If there is an initial name in the sentence, then the sentence should not split
    after the initials:'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果句子中有首字母缩写名，则句子不应在缩写后分割：
- en: 'Sentence: Harry Potter was written by J.K. Rowling. It is an entertaining one.'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子：哈利·波特是由 J.K. Rowling 写的。它是一部有趣的作品。
- en: In the preceding example, the sentence should not split after **J.** It should
    ideally split after **Rowling**.
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在上面的例子中，句子不应在**J.**后分割。理想情况下，它应在**Rowling**后分割。
- en: 'Grammarly Inc., the grammar correction software, customized a rule for the
    identification of sentences and achieves high accuracy for sentence boundary detection.
    See the blog link:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grammarly Inc.，语法纠正软件，为句子识别定制了一条规则，并实现了高准确度的句子边界检测。请参见博客链接：
- en: '[https://tech.grammarly.com/blog/posts/How-to-Split-Sentences.html](https://tech.grammarly.com/blog/posts/How-to-Split-Sentences.html).'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://tech.grammarly.com/blog/posts/How-to-Split-Sentences.html](https://tech.grammarly.com/blog/posts/How-to-Split-Sentences.html)。'
- en: 'To overcome the previous challenges, you can take the following approaches,
    but the accuracy of each approach depends on the implementation. The approaches
    are as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服之前的挑战，你可以采取以下方法，但每种方法的准确性取决于实现。这些方法如下：
- en: 'You can develop a rule-based system to increase the performance of the sentence
    tokenizer:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以开发基于规则的系统来提高句子分词器的性能：
- en: For the previous approach, you can use **name entity recognition** (**NER**)
    tools, POS taggers, or parsers, and then analyze the output of the described tools,
    as well as the sentence tokenizer output and rectify where the sentence tokenizer
    went wrong. With the help of NER tools, POS taggers, and parsers, can you fix
    the wrong output of the sentence tokenizer. In this case, write a rule, then code
    it, and check whether the output is as expected.
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于前面的方法，你可以使用**命名实体识别**（**NER**）工具、词性标注器或解析器，然后分析所描述工具的输出，以及句子分词器的输出，并修正句子分词器出错的地方。在NER工具、词性标注器和解析器的帮助下，你能否修正句子分词器的错误输出？在这种情况下，编写规则，编写代码，检查输出是否符合预期。
- en: 'Test your code! You need to check for exceptional cases. Does your code perform
    well? If yes, great! And, if not, change it a bit:'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试你的代码！你需要检查异常情况。你的代码表现如何？如果是，太棒了！如果不是，稍作修改：
- en: 'You can improve the sentence tokenizer by using **machine learning** (**ML**)
    or deep learning techniques:'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过使用**机器学习**（**ML**）或深度学习技术来改进句子分词器：
- en: If you have enough data that is annotated by a human, then you can train the
    model using an annotated dataset. Based on that trained model, we can generate
    a new prediction from where the sentence boundary should end.
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你有足够的人工标注数据，那么你可以使用标注数据集训练模型。基于该训练模型，我们可以生成一个新的预测，来确定句子边界应该在哪里结束。
- en: In this method, you need to check how the model will perform.
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这种方法中，你需要检查模型的表现。
- en: Stemming for raw text
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原始文本的词干提取
- en: As we saw in [Chapter 3](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml), *Understanding
    Structure of Sentences*, stemming is the process of converting each word of the
    sentence to its root form by deleting or replacing suffixes.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第3章](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml)《理解句子结构》中看到的，*词干提取*是通过删除或替换后缀将句子中的每个单词转换为其词根形式的过程。
- en: In this section, we will apply the `Stemmer` concept on the raw text.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对原始文本应用`Stemmer`概念。
- en: 'Here, we have code where we are using the `PorterStemmer` available in `nltk`.
    Refer to *Figure 4.5*:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是代码，我们使用的是`nltk`中可用的`PorterStemmer`。请参见*图4.5*：
- en: '![](img/7c3b44ed-01b0-4319-a4d1-9548c1221033.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c3b44ed-01b0-4319-a4d1-9548c1221033.png)'
- en: 'Figure 4.5: PorterStemmer code for raw text'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：原始文本的PorterStemmer代码
- en: 'The output of the preceding code is:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出是：
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When you compare the preceding output with the original text, then we can see
    the following changes:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将前面的输出与原始文本进行比较时，我们可以看到以下变化：
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you want to see the difference, then you can refer to the highlighted words
    to see the difference.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想看到差异，你可以参考高亮显示的单词来查看区别。
- en: Challenges of stemming for raw text
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原始文本的词干提取挑战
- en: Initially, stemming tools were made for the English language. The accuracy of
    stemming tools for the English language is high, but for languages such as Urdu
    and Hebrew, stemming tools do not perform well. So, to develop stemming tools
    for other languages is quite challenging. It is still an open research area.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，词干提取工具是为英语语言开发的。英语语言的词干提取工具准确度较高，但对于像乌尔都语和希伯来语这样的语言，词干提取工具的表现不佳。因此，开发适用于其他语言的词干提取工具是相当具有挑战性的。这仍然是一个开放的研究领域。
- en: Lemmatization of raw text
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原始文本的词形还原
- en: Lemmatization is the process that identifies the correct intended **part-of-speech**
    (**POS**) and the meaning of words that are present in sentences.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原是一个过程，它识别句子中单词的正确词性（**part-of-speech**，**POS**）和意义。
- en: In lemmatization, we remove the inflection endings and convert the word into
    its base form, present in a dictionary or in the vocabulary. If we use vocabulary
    and morphological analysis of all the words present in the raw text properly,
    then we can get high accuracy for lemmatization.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在词形还原中，我们去除屈折词尾，将单词转换为其基本形式，该基本形式在字典或词汇表中存在。如果我们正确使用词汇表和原始文本中所有单词的形态学分析，那么我们可以获得较高的词形还原准确性。
- en: Lemmatization transforms words present in the raw text to its lemma by using
    a tagged dictionary such as WordNet.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原通过使用标注字典（如WordNet）将原始文本中的单词转换为其词元。
- en: Lemmatization is closely related to stemming.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原与词干提取密切相关。
- en: In lemmatization, we consider POS tags, and in stemming we do not consider POS
    tags and the context of words.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在词形还原中，我们考虑词性标签，而在词干提取中，我们不考虑词性标签和单词的上下文。
- en: 'Let''s take some examples to make the concepts clear. The following are the
    sentences:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举几个例子来澄清这些概念。以下是这些句子：
- en: 'Sentence 1: It is better for you.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子 1：对你来说更好。
- en: There is a word **better** present in sentence 1\. So, the lemma of word **better**
    is as **good** as a lemma. But stemming is missing as it requires a dictionary
    lookup.
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子 1 中有一个单词 **better**。因此，单词 **better** 的词形还原是 **good**，作为一个词元。但词干提取是缺失的，因为它需要查阅字典。
- en: 'Sentence 2: Man is walking.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子 2：人正在走路。
- en: The word **walking** is derived from the base word walk and here, stemming and
    lemmatization are both the same.
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词 **walking** 来源于基本单词 walk，在这里，词干提取和词形还原是相同的。
- en: 'Sentence 3: We are meeting tomorrow.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子 3：我们明天见面。
- en: Here, to meet is the base form. The word **meeting** is derived from the base
    form. The base form meet can be a noun or it can be a verb. So it depends on the
    context it will use. So, lemmatization attempts to select the right lemma based
    on their POS tags.
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里，to meet 是基本形式。单词 **meeting** 来源于该基本形式。基本形式 meet 可以是名词，也可以是动词。所以这取决于上下文。词形还原尝试根据词性标签选择正确的词元。
- en: 'Refer to the code snippet in *Figure 4.6* for the lemmatization of raw text:'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请参阅 *图 4.6* 中的代码片段进行原始文本的词形还原：
- en: '![](img/e48d0681-646c-4cf2-b4de-42c921b1f663.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e48d0681-646c-4cf2-b4de-42c921b1f663.png)'
- en: 'Figure 4.6: Stemming and lemmatization of raw text'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6：原始文本的词干提取和词形还原
- en: 'The output of the preceding code is given as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出如下所示：
- en: 'The given input is:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 给定的输入是：
- en: '[PRE3]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output is given as:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE4]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In lemmatization, we use different POS tags. The abbreviation description is
    as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在词形还原中，我们使用不同的词性标签（POS 标签）。其缩写说明如下：
- en: '`v` stands for verbs'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`v` 代表动词'
- en: '`n` stands for nouns'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n` 代表名词'
- en: '`a` stands for adjectives'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`a` 代表形容词'
- en: '`s` stands for satellite adjectives'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s` 代表卫星形容词'
- en: '`r` stands for adverbs'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`r` 代表副词'
- en: You can see that, inside the `lemmatizer()`function, I have used all the described
    POS tags.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，在 `lemmatizer()` 函数内，我已经使用了所有描述过的词性标签。
- en: 'You can download the code from the GitHub link at: [https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_2_rawtext_Stemmers.py](https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_2_rawtext_Stemmers.py).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 GitHub 链接下载代码：[https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_2_rawtext_Stemmers.py](https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_2_rawtext_Stemmers.py)。
- en: Challenges of lemmatization of raw text
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词形还原原始文本的挑战
- en: Lemmatization uses a tagged dictionary such as WordNet. Mostly, it's a human-tagged
    dictionary. So human efforts and the time it takes to make WordNet for different
    languages is challenging.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原使用像 WordNet 这样的标注字典。通常，这是一个人工标注的字典。因此，制作不同语言的 WordNet 所需的人工努力和时间是具有挑战性的。
- en: Stop word removal
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 停用词移除
- en: Stop word removal is an important preprocessing step for some NLP applications,
    such as sentiment analysis, text summarization, and so on.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词移除是一些 NLP 应用程序（如情感分析、文本摘要等）的重要预处理步骤。
- en: 'Removing stop words, as well as removing commonly occurring words, is a basic
    but important step. The following is a list of stop words which are going to be
    removed. This list has been generated from `nltk`. Refer to the following code
    snippet in Figure 4.7:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 移除停用词以及常见词的移除是一个基础但重要的步骤。以下是将要移除的停用词列表。此列表已从 `nltk` 生成。请参阅图 4.7 中的代码片段：
- en: '![](img/1982b360-e918-4f5b-b9cd-f476c91600cd.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1982b360-e918-4f5b-b9cd-f476c91600cd.png)'
- en: 'Figure 4.7: Code to see the list of stop words for the English language'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7：查看英语停用词列表的代码
- en: 'The output of the preceding code is a list of stop words available in `nltk`,
    refer to *Figure 4.8*:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出是 `nltk` 中可用的停用词列表，参见 *图 4.8*：
- en: '![](img/6fba5fc2-d654-4d6a-b4de-e384e8b48194.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6fba5fc2-d654-4d6a-b4de-e384e8b48194.png)'
- en: 'Figure 4.8: Output of nltk stop words list for the English language'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8：英语停用词列表的 nltk 输出
- en: The `nltk` has a readily available list of stop words for the English language.
    You can also customize which words you want to remove according to the NLP application
    that you are developing.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`nltk` 提供了现成的英语停用词列表。你也可以根据正在开发的 NLP 应用程序自定义要移除的单词。'
- en: 'You can see the code snippet for removing customized stop words in *Figure
    4.9*:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 *图 4.9* 中看到移除自定义停用词的代码片段：
- en: '![](img/deeb5441-8493-4495-a956-3c8034eba7c3.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/deeb5441-8493-4495-a956-3c8034eba7c3.png)'
- en: 'Figure 4.9: Removing customized stop words'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9：移除自定义停用词
- en: 'The output of the code given in *Figure 4.9* is as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 中代码的输出如下：
- en: '[PRE5]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The code snippet in *Figure 4.10* performs actual stop word removal from raw
    text and this raw text is in the English language:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 中的代码片段执行从原始文本中实际移除停用词的操作，该原始文本是英语文本：
- en: '![](img/e99189e3-380d-4a09-9297-c824f24be3ce.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e99189e3-380d-4a09-9297-c824f24be3ce.png)'
- en: 'Figure 4.10: Stop words removal from raw text'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10：从原始文本中去除停用词
- en: 'The output of the preceding code snippet is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段的输出如下：
- en: '[PRE6]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Exercise
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Take a file which is placed in the data folder with the name `rawtextcorpus.txt`,
    open the file in read mode, load the content, and then remove the stop words by
    using the nltk stop word list. Please analyze the output to get a better idea
    of how things are working out.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个位于数据文件夹中名为`rawtextcorpus.txt`的文件为例，打开文件并以读取模式加载内容，然后使用nltk停用词列表去除停用词。请分析输出结果，以更好地理解其工作原理。
- en: Up until this section, we have analyzed raw text. In the next section, we will
    do preprocessing on sentence levels and word levels.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 直到这一节，我们一直在分析原始文本。下一节，我们将进行句子级别和单词级别的预处理。
- en: Handling corpus-raw sentences
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理语料库原始句子
- en: In the previous section, we were processing on raw text and looked at concepts
    at the sentence level. In this section, we are going to look at the concepts of
    tokenization, lemmatization, and so on at the word level.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们处理了原始文本，并探讨了句子级别的概念。在这一节中，我们将讨论在单词级别上进行标记化、词形还原等相关概念。
- en: Word tokenization
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词标记化
- en: Word tokenization is defined as the process of chopping a stream of text up
    into words, phrases, and meaningful strings. This process is called **word tokenization**.
    The output of the process are words that we will get as an output after tokenization.
    This is called a **token**.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 单词标记化定义为将一段文本切割成单词、短语和有意义的字符串的过程。这个过程叫做**单词标记化**。该过程的输出是经过标记化后得到的单词，这些被称为**标记（token）**。
- en: 'Let''s see the code snippet given in *Figure 4.11* of tokenized words:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看*图4.11*中的单词标记化代码片段：
- en: '![](img/d6b59dcf-3062-40f4-8d65-d0203b591ff3.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6b59dcf-3062-40f4-8d65-d0203b591ff3.png)'
- en: 'Figure 4.11: Word tokenized code snippet'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11：单词标记化代码片段
- en: 'The output of the code given in *Figure 4.11* is as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.11*中的代码输出如下：'
- en: 'The input for word tokenization is:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 单词标记化的输入为：
- en: '[PRE7]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output for word tokenization is:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 单词标记化的输出为：
- en: '[PRE8]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Challenges for word tokenization
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词标记化的挑战
- en: If you analyze the preceding output, then you can observe that the word `don't`
    is tokenized as `do, n't know`. Tokenizing these kinds of words is pretty painful
    using the `word_tokenize` of `nltk`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你分析前面的输出，你会发现单词`don't`被标记化为`do, n't know`。使用`nltk`的`word_tokenize`处理这类单词是相当麻烦的。
- en: To solve the preceding problem, you can write exception codes and improvise
    the accuracy. You need to write pattern matching rules, which solve the defined
    challenge, but are so customized and vary from application to application.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述问题，你可以编写例外代码并提高准确性。你需要编写模式匹配规则，这些规则解决了定义的挑战，但它们是高度定制化的，并且在不同的应用中有所不同。
- en: Another challenge involves some languages such as Urdu, Hebrew, Arabic, and
    so on. They are quite difficult in terms of deciding on the word boundary and
    find out meaningful tokens from the sentences.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个挑战涉及一些语言，如乌尔都语、希伯来语、阿拉伯语等。它们在确定单词边界和从句子中找出有意义的标记方面相当困难。
- en: Word lemmatization
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词词形还原
- en: Word lemmatization is the same concept that we defined in the first section.
    We will just do a quick revision of it and then we will implement lemmatization
    on the word level.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 单词词形还原是我们在第一部分定义的相同概念。我们将快速回顾一下它，然后在单词级别实现词形还原。
- en: Word lemmatization is converting a word from its inflected form to its base
    form. In word lemmatization, we consider the POS tags and, according to the POS
    tags, we can derive the base form which is available to the lexical WordNet.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 单词词形还原是将一个单词从其屈折形式转换为其基本形式。在单词词形还原中，我们会考虑词性标签（POS），根据词性标签，我们可以得出在词汇库WordNet中可用的基本形式。
- en: 'You can find the code snippet in *Figure 4.12*:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图4.12*中找到代码片段：
- en: '![](img/6822ae40-ec0e-4d7b-bb99-5a1ceedd8318.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6822ae40-ec0e-4d7b-bb99-5a1ceedd8318.png)'
- en: 'Figure 4.12: Word lemmatization code snippet'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12：单词词形还原代码片段
- en: 'The output of the word lemmatization is as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 单词词形还原的输出如下：
- en: '[PRE9]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Challenges for word lemmatization
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词词形还原的挑战
- en: It is time consuming to build a lexical dictionary. If you want to build a lemmatization
    tool that can consider a larger context, taking into account the context of preceding
    sentences, it is still an open area in research.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 构建词汇字典是一个耗时的过程。如果你想要构建一个可以考虑更大上下文的词形还原工具，考虑前文的上下文，那么这个问题仍然是研究中的一个开放领域。
- en: Basic preprocessing
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本预处理
- en: In basic preprocessing, we include things that are simple and easy to code but
    seek our attention when we are doing preprocessing for NLP applications.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在基本预处理过程中，我们包括了一些简单且易于编码的内容，但当我们为NLP应用进行预处理时，它们会引起我们的注意。
- en: Regular expressions
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则表达式
- en: Now we will begin some of the interesting concepts of preprocessing, which are
    the most useful. We will look at some of the advanced levels of regular expression.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将开始一些有趣的预处理概念，这些概念最为有用。我们将看看一些更高级的正则表达式应用。
- en: For those who are new to regular expression, I want to explain the basic concept
    of **regular expression** (**regex**).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对于初学正则表达式的人，我想解释一下**正则表达式**（**regex**）的基本概念。
- en: Regular expression is helpful to find or find-replace specific patterns from
    a sequence of characters. There is particular syntax which you need to follow
    when you are making regex.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式有助于从字符序列中查找或查找替换特定模式。当你编写正则表达式时，需要遵循特定的语法。
- en: 'There are many online tools available which can give you the facility to develop
    and test your regex. One of my favorite online regex development tool links is
    given here: [https://regex101.com/](https://regex101.com/)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多在线工具可以为你提供开发和测试正则表达式的功能。我最喜欢的在线正则表达式开发工具链接如下：[https://regex101.com/](https://regex101.com/)
- en: 'You can also refer to the Python regex library documentation at: [https://docs.python.org/2/library/re.html](https://docs.python.org/2/library/re.html)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以参考Python正则表达式库的文档：[https://docs.python.org/2/library/re.html](https://docs.python.org/2/library/re.html)
- en: Basic level regular expression
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础级别正则表达式
- en: Regex is a powerful tool when you want to do customized preprocessing or when
    you have noisy data with you.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式是一个强大的工具，当你需要进行自定义预处理或当你有嘈杂数据时非常有用。
- en: 'Here, I''m presenting some of the basic syntax and then we will see the actual
    implementation on Python. In Python, the `re` library is available and by using
    this library we can implement regex. You can find the code on this GitHub link:
    [https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_5_regualrexpression.py](https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_5_regualrexpression.py)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我展示了一些基本的语法，然后我们将看到在Python中的实际实现。在Python中，`re`库是可用的，使用这个库我们可以实现正则表达式。你可以在这个GitHub链接找到代码：[https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_5_regualrexpression.py](https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_5_regualrexpression.py)
- en: Basic flags
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本标志
- en: 'The basic flags are `I`, `L`, `M`, `S`, `U`, `X`:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 基本标志有`I`、`L`、`M`、`S`、`U`、`X`：
- en: '`re.I`: This flag is used for ignoring casing'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`re.I`：这个标志用于忽略大小写'
- en: '`re.M`: This flag is useful if you want to find patterns throughout multiple
    lines'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`re.M`：如果你想在多行中查找模式，这个标志很有用'
- en: '`re.L`: This flag is used to find a local dependent'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`re.L`：这个标志用于查找本地依赖'
- en: '`re.S`: This flag is used to find dot matches'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`re.S`：这个标志用于查找点符号匹配'
- en: '`re.U`: This flag is used to work for unicode data'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`re.U`：这个标志用于处理unicode数据'
- en: '`re.X`: This flag is used for writing regex in a more readable format'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`re.X`：这个标志用于以更易读的格式编写正则表达式'
- en: We have mainly used `re.I`, `re.M`, `re.L`, and `re.U` flags.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要使用了`re.I`、`re.M`、`re.L`和`re.U`标志。
- en: We are using the `re.match()` and `re.search()` functions. Both are used to
    find the patterns and then you can process them according to the requirements
    of your application.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用`re.match()`和`re.search()`函数。它们都用于查找模式，然后你可以根据应用的需求进行处理。
- en: 'Let''s look at the differences between `re.match()` and `re.search()`:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看`re.match()`和`re.search()`之间的区别：
- en: '`re.match()`: This checks for a match of the string only at the beginning of
    the string. So, if it finds the pattern at the beginning of the input string then
    it returns the matched pattern, otherwise; it returns a noun.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`re.match()`：它仅检查字符串开头是否存在匹配项。如果在输入字符串的开头找到模式，它会返回匹配的模式，否则返回`None`。'
- en: '`re.search()`: This checks for a match of the string anywhere in the string.
    It finds all the occurrences of the pattern in the given input string or data.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`re.search()`：它检查字符串中的任何位置是否存在匹配项。它会查找给定输入字符串或数据中模式的所有出现。'
- en: 'Refer to the code snippet given in *Figure 4.13*:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考*图4.13*中给出的代码片段：
- en: '![](img/9f086497-45e5-4f5c-8820-635c14dfa4e1.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f086497-45e5-4f5c-8820-635c14dfa4e1.png)'
- en: 'Figure 4.13: Code snippet to see the difference between re.match() versus re.search()'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13：代码片段，用于查看`re.match()`与`re.search()`的区别
- en: 'The output of the code snippet of *Figure 4.13* is given in *Figure 4.14*:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.13*中的代码片段输出在*图4.14*中给出：'
- en: '![](img/40421689-c806-48dc-89a5-8d7fc8d6814c.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40421689-c806-48dc-89a5-8d7fc8d6814c.png)'
- en: 'Figure 4.14: Output of the re.match() versus re.search()'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14：re.match() 与 re.search() 的输出
- en: 'The syntax is as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 语法如下：
- en: 'Find the single occurrence of character `a` and `b`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 查找字符`a`和`b`的单次出现：
- en: '[PRE10]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Find characters except `a` and `b`:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 查找除`a`和`b`之外的字符：
- en: '[PRE11]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Find the character range of `a` to `z`:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 查找`a`到`z`的字符范围：
- en: '[PRE12]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Find range except to `z`:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 查找除`z`之外的范围：
- en: '[PRE13]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Find all the characters `a` to `z` as well as `A` to `Z`:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 查找所有字符`a`到`z`以及`A`到`Z`：
- en: '[PRE14]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Any single character:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 任意单个字符：
- en: '[PRE15]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Any whitespace character:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 任意空白字符：
- en: '[PRE16]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Any non-whitespace character:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 任意非空白字符：
- en: '[PRE17]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Any digit:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 任意数字字符：
- en: '[PRE18]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Any non-digit:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 任意非数字字符：
- en: '[PRE19]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Any non-words:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 任意非单词字符：
- en: '[PRE20]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Any words:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 任意单词：
- en: '[PRE21]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Either match `a` or `b`:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配`a`或`b`：
- en: '[PRE22]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Occurrence of `a` is either zero or one:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '`a`的出现次数为零次或一次：'
- en: '[PRE23]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Occurrence of `a` is zero time or more than that:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '`a`的出现次数为零次或更多：'
- en: '[PRE24]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Occurrence of `a` is one time or more than that:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`a`的出现次数为一次或更多：'
- en: '[PRE25]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Exactly match three occurrences of `a`:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 精确匹配三次出现的`a`：
- en: '[PRE26]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Match simultaneous occurrences of `a` with `3` or more than `3`:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 同时匹配`a`的出现次数为`3`次或更多：
- en: '[PRE27]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Match simultaneous occurrences of `a` between `3` to `6`:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 同时匹配`a`的出现次数在`3`到`6`之间：
- en: '[PRE28]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Starting of the string:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串的开始：
- en: '[PRE29]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Ending of the string:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串的结束：
- en: '[PRE30]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Match word boundary:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配单词边界：
- en: '[PRE31]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Non-word boundary:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 非单词边界：
- en: '[PRE32]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The basic code snippet is given in *Figure 4.15*:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 基本代码片段见*图 4.15*：
- en: '![](img/e4e39a15-d25a-497a-b904-8a9ccc4da46c.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4e39a15-d25a-497a-b904-8a9ccc4da46c.png)'
- en: 'Figure 4.15: Basic regex functions code snippet'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15：基本正则表达式函数代码片段
- en: 'The output of the code snippet of *Figure 4.15* is given in *Figure 4.16*:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4.15*的代码片段输出见*图 4.16*：'
- en: '![](img/6a83361e-378b-4931-b811-311f6be3cce7.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a83361e-378b-4931-b811-311f6be3cce7.png)'
- en: 'Figure 4.16: Output of the basic regex function code snippet'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.16：基本正则表达式函数代码片段的输出
- en: Advanced level regular expression
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级正则表达式
- en: There are advanced concepts of regex which will be very useful.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些正则表达式的高级概念将会非常有用。
- en: The **lookahead** and **lookbehind** are used to find out substring patterns
    from your data. Let's begin. We will understand the concepts in the basic language.
    Then we will look at the implementation of them.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '**前瞻**和**后瞻**用于从数据中查找子串模式。让我们开始。我们将用基础语言理解这些概念。然后我们会看它们的实现。'
- en: Positive lookahead
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正向前瞻
- en: 'Positive lookahead matches the substring from a string if the defined pattern
    is followed by the substring. If you don''t understand, then let''s look at the
    following example:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 正向前瞻会在字符串中匹配子串，前提是定义的模式被该子串所跟随。如果您不理解，那么让我们看一下以下示例：
- en: 'Consider a sentence: I play on the playground.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑句子：I play on the playground.
- en: Now, you want to extract *play* as a pattern but only if it follows *ground*.
    In this situation, you can use positive lookahead.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，您希望提取*play*作为模式，但仅当它跟随*ground*时。在这种情况下，您可以使用正向前瞻。
- en: The syntax of positive lookahead is `(?=pattern)`
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 正向前瞻的语法是`(?=pattern)`
- en: The regex `rplay(?=ground)` matches *play*, but only if it is followed by *ground*.
    Thus, the first *play* in the text string won't be matched.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式`rplay(?=ground)`匹配*play*，但仅当它后面跟着*ground*时才匹配。因此，文本中的第一个*play*不会被匹配。
- en: Positive lookbehind
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正向后瞻
- en: 'Positive lookbehind matches the substring from a string if the defined pattern
    is preceded by the substring. Refer to the following example:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 正向后瞻会在字符串中匹配子串，前提是定义的模式被该子串所前置。参考以下示例：
- en: 'Consider the sentence: I play on the playground. It is the best ground.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑句子：I play on the playground. It is the best ground.
- en: Now you want to extract *ground*, if it is preceded by the string *play*. In
    this case, you can use positive lookbehind.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在您希望提取*ground*，如果它前面有字符串*play*。在这种情况下，您可以使用正向后瞻。
- en: The syntax of positive lookbehind is `(?<=pattern)`
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 正向后瞻的语法是`(?<=pattern)`
- en: The regex `r(?<=play)ground` matches *ground*, but only if it is preceded by
    *play*.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式`r(?<=play)ground`匹配*ground*，但仅当它前面有*play*时才匹配。
- en: Negative lookahead
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负向前瞻
- en: Negative lookahead matches the string which is definitely not followed by the
    pattern which we have defined in the regex pattern part.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 负向前瞻匹配的是那些后面绝对不会跟着我们在正则表达式中定义的模式的字符串。
- en: 'Let''s give an example to understand the negative lookahead:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子来理解负向前瞻：
- en: 'Consider the sentence: I play on the playground. It is the best ground.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑句子：I play on the playground. It is the best ground.
- en: Now you want to extract *play* only if it is not followed by the string *ground*.
    In this case, you can use negative lookahead.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在您希望提取*play*，仅当它后面没有字符串*ground*时。在这种情况下，您可以使用负向前瞻。
- en: The syntax of negative lookahead is `(?!pattern)`
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 负向前瞻的语法是`(?!pattern)`
- en: The regex `r play(?!ground)` matches *play*, but only if it is not followed
    by *ground*. Thus the *play* just before *on* is matched.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式`r play(?!ground)`匹配*play*，但仅当其后不是*ground*时才匹配。因此，*play*紧跟*on*时会被匹配。
- en: Negative lookbehind
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 否定前瞻
- en: Negative lookbehind matches the string which is definitely not preceded by the
    pattern which we have defined in the regex pattern part.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 否定前瞻匹配的字符串是明确没有被我们在正则表达式中定义的模式所预先出现的字符串。
- en: 'Let''s see an example to understand the negative lookbehind:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子来理解否定前瞻：
- en: 'Consider the sentence: I play on the playground. It is the best ground.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑这个句子：I play on the playground. It is the best ground.
- en: Now you want to extract *ground* only if it is not preceded by the string *play*.
    In this case, you can use negative lookbehind.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在你只想提取*ground*，但仅当其前面没有*play*字符串时。在这种情况下，你可以使用否定前瞻。
- en: The syntax of negative lookbehind is `(?<!pattern)`
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 否定前瞻的语法是`(?<!pattern)`
- en: The regex `r(?<!play)ground` matches *ground*, but only if it is not preceded
    by *play*.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式`r(?<!play)ground`匹配*ground*，但仅当其前面没有*play*时。
- en: 'You can see the code snippet which is an implementation of `advanceregex()`
    in *Figure 4.17*:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到代码片段，这是`advanceregex()`在*图4.17*中的实现：
- en: '![](img/74f4c83a-883b-4b81-89ca-d529aa3f8f23.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/74f4c83a-883b-4b81-89ca-d529aa3f8f23.png)'
- en: 'Figure 4.17: Advanced regex code snippet'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17：高级正则表达式代码片段
- en: 'The output of the code snippet of *Figure 4.17* is given in *Figure 4.18*:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.17*的代码片段输出在*图4.18*中给出：'
- en: '![](img/6efff615-cc21-4535-9207-b0fd8f01e31d.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6efff615-cc21-4535-9207-b0fd8f01e31d.png)'
- en: 'Figure 4.18: Output of code snippet of advanced regex'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.18：高级正则表达式代码片段的输出
- en: Practical and customized preprocessing
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践中的定制化预处理
- en: When we start preprocessing for NLP applications, sometimes you need to do some
    customization according to your NLP application. At that time, it might be possible
    that you need to think about some of the points which I have described as follows.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始为NLP应用程序进行预处理时，有时你需要根据自己的NLP应用程序做一些定制。那时，可能需要考虑我在下面描述的一些要点。
- en: Decide by yourself
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自己决定
- en: This section is a discussion of how to approach preprocessing when you don't
    know what kind of preprocessing is required for developing an NLP application.
    In this kind of situation, what you can do is simply ask the following questions
    to yourself and make a decision.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了当你不知道开发NLP应用程序需要什么样的预处理时，如何进行预处理。在这种情况下，你可以做的是向自己提出以下问题并做出决策。
- en: What is your NLP application and what kind of data do you need to build the
    NLP application?
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 你的NLP应用程序是什么？你需要什么样的数据来构建这个NLP应用程序？
- en: Once you have understood the problem statement, as well as having clarity on
    what your output should be, then you are in a good situation.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦你理解了问题陈述，并且清楚了输出应该是什么，那么你就处于一个很好的位置。
- en: Once you know about the problem statement and the expected output, now think
    what all the data points are that you need from your raw data set.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦你了解了问题陈述和预期输出，接下来就思考一下，你需要从原始数据集中提取哪些数据点。
- en: To understand the previous two points, let's take an example. If you want to
    make a text summarization application, suppose you are using a news articles that
    are on the web, which you want to use for building news text summarization application.
    Now, you have built a scraper that scrapes news articles from the web. This raw
    news article dataset may contain HTML tags, long texts, and so on.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了理解前面提到的两点，让我们举个例子。如果你想做一个文本摘要应用程序，假设你正在使用网络上的新闻文章，你想用这些文章来构建新闻文本摘要应用程序。现在，你已经建立了一个抓取器，从网络上抓取新闻文章。这些原始新闻文章数据集可能包含HTML标签、长文本等。
- en: For news text summarization, how will we do preprocessing? In order to answer
    that, we need to ask ourselves a few questions. So, let's jump to a few questions
    about preprocessing.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 对于新闻文本摘要，我们如何进行预处理？为了回答这个问题，我们需要问自己几个问题。所以，让我们来看看一些关于预处理的问题。
- en: Is preprocessing required?
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 是否需要进行预处理？
- en: Now you have raw-data for text summarization and your dataset contains HTML
    tags, repeated text, and so on.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在你有了用于文本摘要的原始数据，并且你的数据集包含HTML标签、重复的文本等。
- en: If your raw-data has all the content that I described in the first point, then
    preprocessing is required and, in this case, we need to remove HTML tags and repeated
    sentences; otherwise, preprocessing is not required.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的原始数据包含我在第一点中描述的所有内容，那么就需要进行预处理，在这种情况下，我们需要删除HTML标签和重复的句子；否则，不需要预处理。
- en: You also need to apply lowercase convention.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你还需要应用小写字母规则。
- en: After that, you need to apply sentence tokenizer on your text summarization
    dataset.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之后，你需要在你的文本摘要数据集上应用句子标记器。
- en: Finally, you need to apply word tokenizer on your text summarization dataset.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，你需要在你的文本摘要数据集上应用词汇标记器。
- en: Whether your dataset needs preprocessing depends on your problem statement and
    what data your raw dataset contains.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否需要预处理你的数据集，取决于你的问题陈述和原始数据集所包含的数据。
- en: You can see the flowchart in *Figure 4.19:*
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 4.19*中看到流程图：
- en: '![](img/36192456-01df-4e30-91bc-3261e57a43e2.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36192456-01df-4e30-91bc-3261e57a43e2.png)'
- en: 'Figure 4.19: Basic flowchart for performing preprocessing of text-summarization'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.19：文本摘要预处理的基本流程图
- en: What kind of preprocessing is required?
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 需要什么样的预处理？
- en: 'In our example of text summarization, if a raw dataset contains HTML tags,
    long text, repeated text, then during the process of developing your application,
    as well as in your output, you don''t need the following data:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们关于文本摘要的示例中，如果原始数据集包含 HTML 标签、长文本、重复文本，那么在你开发应用程序的过程中，以及在输出中，你不需要以下数据：
- en: You don't need HTML tags, so you can remove them
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不需要 HTML 标签，因此你可以将其删除。
- en: You don't need repeated sentences, so you can remove them as well
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不需要重复的句子，所以你也可以将它们删除。
- en: If there is long text content then if you can find stop words and high frequency
    small words, you should remove them
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果文本内容较长，并且你能找到停用词和高频小词，那么你应该将它们删除。
- en: Understanding case studies of preprocessing
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解预处理的案例研究
- en: Whatever I have explained here regarding customized preprocessing will make
    more sense to you when you have some real life case studies explained.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里解释的关于定制预处理的内容，当你有一些真实案例研究时，会对你更有意义。
- en: Grammar correction system
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语法纠错系统
- en: You are making a grammar correction system. Now, think of the sub-task of it.
    You want to build a system which predicts the placement of articles a, an, and
    the in a particular sentence.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你正在建立一个语法纠错系统。现在，考虑它的子任务。你想建立一个系统，预测在特定句子中冠词 a、an 和 the 的位置。
- en: For this kind of system, if you are thinking I need to remove stop words every
    time, then, OOPs, you are wrong because this time we really can't remove all the
    stop words blindly. In the end, we need to predict the articles a, an, and the.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于这种类型的系统，如果你认为每次都需要去除停用词，那么，哎呀，你错了，因为这次我们真的不能盲目地删除所有停用词。最终，我们需要预测冠词 a、an 和
    the。
- en: You can remove words which are not meaningful at all, such as when your dataset
    contains math symbols, then you can remove them. But this time, you need to do
    a detailed analysis as to whether you can remove the small length words, such
    as abbreviations, because your system also needs to predict which abbreviations
    don't take an article and which do.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以删除完全没有意义的词，例如当数据集中包含数学符号时，你可以将其删除。但这时，你需要进行详细分析，判断是否能删除短小的词汇，比如缩写，因为你的系统还需要预测哪些缩写不带冠词，哪些带冠词。
- en: Now, let's look at a system where you can apply all the preprocessing techniques
    that we have described here. Let's follow the points inside sentiment analysis.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一个可以应用我们在这里描述的所有预处理技术的系统。让我们按照情感分析中的要点进行操作。
- en: Sentiment analysis
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析
- en: 'Sentiment analysis is all about evaluating the reviews of your customers and
    categorizing them into positive, negative, and neutral categories:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析的核心是评估客户的评价，并将其分类为正面、负面和中性类别：
- en: For this kind of system, your dataset contains user reviews so user writing
    generally contains casual language.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于这种类型的系统，你的数据集包含用户评价，因此用户的写作通常包含日常语言。
- en: The data contains informal language so we need to remove stop words such as
    Hi, Hey, Hello, and so on. We do not use Hi, Hey, How are u? to conclude whether
    the user review is positive, negative, or neutral.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据包含非正式语言，因此我们需要去除停用词，例如 Hi、Hey、Hello 等。我们不会通过 Hi、Hey、How are u? 来判断用户评价是正面、负面还是中性。
- en: Apart from that, you can remove the repeated reviews.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除此之外，你还可以去除重复的评论。
- en: You can also preprocess data by using word tokenization and lemmatization.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你还可以通过使用词汇标记化和词形还原来预处理数据。
- en: Machine translation
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器翻译
- en: 'Machine translation is also one of the widely used NLP applications. In machine
    translation, our goal is to translate one language to another language in a logical
    manner. So, if we want to translate the English language to the German language,
    then you may the apply the following preprocessing steps:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译也是一种广泛使用的NLP应用。在机器翻译中，我们的目标是以逻辑的方式将一种语言翻译成另一种语言。所以，如果我们想将英语翻译成德语，那么你可以应用以下预处理步骤：
- en: We can apply convert to the whole dataset to be converted into lowercase.
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以将整个数据集转换为小写字母。
- en: Apply sentence splitter on the dataset so you can get the boundary for each
    of the sentences.
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据集应用句子分割器，这样你可以获取每个句子的边界。
- en: Now, suppose you have corpus where all English sentences are in `English_Sentence_File`
    and all German sentence are in `German_Sentence_File`. Now, you know for each
    English sentence there is a corresponding German sentence present in `German_Sentence_File`.
    This kind of corpus is called **parallel** corpus. So in this case, you also need
    to check that all sentences in both files are aligned appropriately.
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，假设你有一个语料库，其中所有的英文句子都在`English_Sentence_File`中，所有的德文句子都在`German_Sentence_File`中。现在，你知道每个英文句子都有一个对应的德文句子存在于`German_Sentence_File`中。这种类型的语料库被称为**平行**语料库。所以，在这种情况下，你还需要检查这两个文件中的所有句子是否对齐。
- en: You can also apply stemming for each of the words of the sentences.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还可以对每个句子的单词应用词干提取。
- en: Spelling correction
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拼写纠正
- en: Spelling correction can be a very useful tool for preprocessing as well, as
    it helps to improve your NLP application.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 拼写纠正也是一种非常有用的预处理工具，它有助于改善你的NLP应用。
- en: Approach
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: The concept of spelling correction came from the concept of how much similarity
    is contained by two strings. This concept is used to compare two strings. The
    same concept has been used everywhere nowadays. We will consider some examples
    to better understand how this concept of checking the similarity of two strings
    can be helpful to us.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 拼写纠正的概念来源于两个字符串之间的相似性。这一概念用于比较两个字符串。现在这个概念已经广泛应用。我们将考虑一些示例，以更好地理解如何利用这个检查字符串相似性的概念来帮助我们。
- en: 'When you search on Google, if you make a typing mistake in your search query,
    then you get a suggestion on the browser, Did you mean: with your corrected query
    with the right spelling. This mechanism rectifies your spelling mistake and Google
    has its own way of providing almost perfect results every time. Google does not
    just do a spelling correction, but is also indexes on your submitted query and
    displays the best result for you. So, the concept behind the spelling correction
    is the similarity between two strings.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在谷歌搜索时，如果你的搜索查询中有拼写错误，浏览器会给你一个建议，显示“Did you mean:”，并显示你纠正后的查询和正确的拼写。这个机制可以纠正你的拼写错误，谷歌也有自己的一套方法，每次几乎都能提供完美的结果。谷歌不仅仅做拼写纠正，它还会对你提交的查询进行索引，并展示最适合的结果。所以，拼写纠正背后的概念是两个字符串之间的相似性。
- en: 'Take another example: If you are developing a machine translation system, then
    when you see the string translated by the machine, your next step is probably
    to validate your output. So now you will compare the output of the machine with
    a human translator and situation, which may not be perfectly similar to the output
    of the machine.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 再举一个例子：如果你正在开发一个机器翻译系统，当你看到机器翻译的字符串时，你的下一步可能是验证你的输出。所以现在你将把机器的输出与人工翻译进行比较，而这种情况可能与机器的输出并不完全相同。
- en: 'If the machine translated string is: **She said could she help me?**, the human
    string translated would say: **She asked could she help me?** When you are checking
    the similarity between a system string and a human string, you may find that *said*
    is replace by asked.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如果机器翻译的字符串是：**She said could she help me?**，那么人工翻译的字符串可能是：**She asked could
    she help me?** 当你检查系统翻译字符串与人工翻译字符串的相似性时，你可能会发现*said*被替换成了asked。
- en: So, this concept of the similarity of two strings can be used in many applications,
    including speech recognition, NLP applications, and so on.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，两个字符串相似性的概念可以应用于许多应用，包括语音识别、自然语言处理（NLP）应用等等。
- en: There are three major operations when we are talking about measuring the similarity
    of two strings. The operations are insertion, deletion, and substitution. These
    operations are used for the implementation of the spelling correction operation.
    Right now, to avoid complexity, we are not considering transpose and long string
    editing operations.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论衡量两个字符串的相似度时，有三种主要操作：插入、删除和替换。这些操作用于拼写修正操作的实现。现在，为了避免复杂性，我们不考虑转置和长字符串编辑操作。
- en: Let's start with the operations and then we will look at the algorithm specifically
    for the spelling correction.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从操作开始，然后将具体讨论拼写修正的算法。
- en: '**Insertion operation**'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '**插入操作**'
- en: If you have an incorrect string, now after inserting one or more characters,
    you will get the correct string or expected string.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个不正确的字符串，现在通过插入一个或多个字符，你将得到正确的字符串或预期的字符串。
- en: Let's see an example.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子。
- en: If I have entered a string `aple`,then after inserting `p` we will get `apple`,
    which is right. If you have entered a string `staemnt` then after inserting `t`
    and `e` you will get `statement`, which is right.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我输入了字符串`aple`，那么在插入`p`之后我们会得到`apple`，这是正确的。如果你输入了字符串`staemnt`，那么在插入`t`和`e`之后你会得到`statement`，这是正确的。
- en: '**Deletion operation**'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '**删除操作**'
- en: You may have an incorrect string which can be converted into a correct string
    after deleting one or more characters of the string.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能有一个不正确的字符串，可以通过删除一个或多个字符来将其转换为正确的字符串。
- en: 'An example is as follows:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个例子：
- en: If I have entered `caroot`, then to get the correct string we need to delete
    one `o`. After that, we will get the correct string `carrot`.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我输入了`caroot`，那么为了得到正确的字符串，我们需要删除一个`o`。之后，我们会得到正确的字符串`carrot`。
- en: '**Substitution operation**'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '**替换操作**'
- en: If you get the correct string by substituting one or more characters, then it
    is called a **substitution operation**.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你通过替换一个或多个字符得到正确的字符串，那就叫做**替换操作**。
- en: Suppose you have a string `implemantation`. To make it correct, you need to
    substitute the first `a` to `e` and you will get the correct string `implementation`.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个字符串`implemantation`。要使其正确，你需要将第一个`a`替换为`e`，这样你就会得到正确的字符串`implementation`。
- en: '**Algorithms for spelling corrections**'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '**拼写修正算法**'
- en: We are using the minimum edit distance algorithm to understand spelling corrections.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用最小编辑距离算法来理解拼写修正。
- en: '**Minimum edit distance**'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '**最小编辑距离**'
- en: This algorithm is for converting one string `X` into another string `Y` and
    we need to find out what the minimum edit cost is to convert string `X` to string
    `Y`. So, here you can either do insertion, deletion, or substitution operations
    to convert string `X` to `Y` with the minimum possible sequences of the character
    edits.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法是将一个字符串`X`转换为另一个字符串`Y`，我们需要找出将字符串`X`转换为字符串`Y`的最小编辑成本。因此，你可以进行插入、删除或替换操作，将字符串`X`转化为字符串`Y`，并用最少的字符编辑序列实现。
- en: Suppose you have a string `X` with a length of `n`,and string `Y` with a length
    of `m`.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个长度为`n`的字符串`X`，和一个长度为`m`的字符串`Y`。
- en: 'Follow the steps of the algorithm:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 按照算法的步骤进行：
- en: '[PRE33]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let''s look at the following steps:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下以下步骤：
- en: Set `n` to a length of P.
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置`n`为P的长度。
- en: Set `m` to a length of Q.
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置`m`为Q的长度。
- en: If `n = 0`, return `m` and exit.
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`n = 0`，返回`m`并退出。
- en: If `m = 0`, return `n` and exit.
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果`m = 0`，返回`n`并退出。
- en: Create a matrix containing 0..*m* rows and 0..*n* columns.
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含0到*m*行和0到*n*列的矩阵。
- en: Initialize the first row to 0..*n*.
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化第一行为0到*n*。
- en: Initialize first column to 0..*m*.
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 初始化第一列为0到*m*。
- en: Iterate each character of P (`i` from 1 to *n*).
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历P的每个字符（`i` 从 1 到 *n*）。
- en: Iterate each character of Q (`j` from 1 to *m*).
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 遍历Q的每个字符（`j` 从 1 到 *m*）。
- en: If P[i] equals Q[j], the cost is 0.
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果P[i]等于Q[j]，则成本为0。
- en: If Q[i] doesn't equal Q[j], the cost is 1.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果Q[i]不等于Q[j]，则成本为1。
- en: 'Set the value at cell `v[i,j]` of the matrix equal to the minimum of all three
    of the following  points:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 设置矩阵中单元格`v[i,j]`的值为以下三点的最小值：
- en: 'The cell immediately previous plus 1: `v[i-1,j] + 1`'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 紧挨上面的单元格加1：`v[i-1,j] + 1`
- en: 'The cell immediately to the left plus 1: `v[i,j-1] + 1`'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 紧挨左侧的单元格加1：`v[i,j-1] + 1`
- en: 'The cell diagonally previous and to the left plus the cost: `v[i-1,j-1] +1`
    for minimum edit distance. If you are using the Levenshtein distance then `v[i-1,j-1]
    +` cost should be considered'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于最小编辑距离，单元格对角线前一个加上成本：`v[i-1,j-1] +1`。如果你使用的是Levenshtein距离，则应考虑`v[i-1,j-1]
    +` 成本。
- en: After the iteration in *step 7* to *step 9* has been completed, the distance
    is found in cell `v[n,m]`.
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *步骤 7* 到 *步骤 9* 的迭代完成后，距离值存储在单元格 `v[n,m]` 中。
- en: The previous steps are the basic algorithm to develop the logic of spelling
    corrections but we can use probability distribution of words and take a consideration
    of that as well. This kind of algorithmic approach is based on dynamic programing.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的步骤是开发拼写校正逻辑的基本算法，但我们还可以使用单词的概率分布并考虑这一点。这种算法方法基于动态规划。
- en: 'Let''s convert the string `tutour` to `tutor` by understanding that we need
    to delete `u`. The edit distance is therefore 1\. The table which is developed
    by using the defined algorithm is shown in *Figure 4.20* for computing the minimum
    edit distance:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过理解需要删除 `u`，将字符串 `tutour` 转换为 `tutor`。因此，编辑距离为 1。由定义的算法生成的表格用于计算最小编辑距离，如
    *图 4.20* 所示：
- en: '![](img/3044f3f6-aa26-4af7-857e-846d55589d53.png)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3044f3f6-aa26-4af7-857e-846d55589d53.png)'
- en: 'Figure 4.20: Computing minimum edit distance'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.20：计算最小编辑距离
- en: '**Implementation**'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现**'
- en: 'Now, for the spelling correction, we need to add a dictionary or extract the
    words from the large documents. So, in the implementation, we have used a big
    document from where we have extracted words. Apart from that, we have used the
    probability of occurring words in the document to get an idea about the distribution.
    You can see more details regarding the implementation part by clicking on this
    link: [http://norvig.com/spell-correct.html](http://norvig.com/spell-correct.html)'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于拼写校正，我们需要添加一个字典或从大文档中提取单词。因此，在实现中，我们使用了一个大文档，并从中提取了单词。除此之外，我们还使用了文档中单词出现的概率来了解分布情况。您可以通过点击此链接查看有关实现部分的更多详细信息：[http://norvig.com/spell-correct.html](http://norvig.com/spell-correct.html)
- en: We have implemented the spelling correction for the minimum edit distance 2.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经实现了最小编辑距离 2 的拼写校正。
- en: 'See the implementation of the spelling correction in *Figure 4.21*:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见 *图 4.21* 中的拼写校正实现：
- en: '![](img/aa9bb806-49ed-4658-8f9e-8acb601484be.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa9bb806-49ed-4658-8f9e-8acb601484be.png)'
- en: 'Figure 4.21: Implementation of spelling correction'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.21：拼写校正的实现
- en: See the output of the spelling correction in *Figure 4.22*.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见 *图 4.22* 中拼写校正的输出。
- en: 'We are providing the string `aple`, which is converted to `apple` successfully:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了字符串 `aple`，它成功地转换为 `apple`：
- en: '![](img/9d23d7f2-847a-4f5d-a2f6-def409aaface.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d23d7f2-847a-4f5d-a2f6-def409aaface.png)'
- en: 'Figure 4.22: Output of spelling correction'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.22：拼写校正的输出
- en: Summary
  id: totrans-389
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we have looked at all kinds of preprocessing techniques which
    will be useful to you when you are developing an NLP system or an NLP application.
    We have also touched upon a spelling correction system which you can consider
    as part of the preprocessing technique because it will be useful for many of the
    NLP applications that you develop in the future. By the way, you can access the
    code on GitHub by clicking the following link: [https://github.com/jalajthanaki/NLPython/tree/master/ch4](https://github.com/jalajthanaki/NLPython/tree/master/ch4)'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了各种预处理技术，这些技术在您开发 NLP 系统或 NLP 应用程序时会非常有用。我们还提到了一个拼写校正系统，您可以将其视为预处理技术的一部分，因为它对未来开发的许多
    NLP 应用程序都很有帮助。顺便说一下，您可以通过点击以下链接访问 GitHub 上的代码：[https://github.com/jalajthanaki/NLPython/tree/master/ch4](https://github.com/jalajthanaki/NLPython/tree/master/ch4)
- en: 'In the next chapter, we will look at the most important part for any NLP system:
    feature engineering. The performance of an NLP system mainly depends on what kind
    of data we provide to the NLP system. Feature engineering is an art and skill
    which you are going to adopt from the next chapter onwards and, trust me, it is
    the most important ingredient in developing the NLP systems, so read it and definitely
    implement it to enrich your skills.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论任何 NLP 系统中最重要的部分：特征工程。NLP 系统的性能主要取决于我们提供给系统的数据类型。特征工程是一项艺术和技能，您将在下一章开始时掌握它。相信我，它是开发
    NLP 系统中最重要的组成部分，因此请好好阅读，并将其付诸实践以丰富您的技能。
