- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Training Pipeline
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练管道
- en: In this chapter, we’ll explore the key components of an LLM training pipeline,
    from data ingestion and preprocessing to model architecture and optimization strategies.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨 LLM 训练管道的关键组成部分，从数据摄取和预处理到模型架构和优化策略。
- en: You’ll gain insights into implementing effective monitoring and logging systems,
    ensuring you can track your model’s progress and make data-driven decisions throughout
    the training process.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你将深入了解实施有效的监控和记录系统，确保你可以在整个训练过程中跟踪你的模型进度并做出数据驱动的决策。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Components of a training pipeline
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练管道的组成部分
- en: Data input and preprocessing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据输入和预处理
- en: LLM architecture design considerations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 架构设计考虑因素
- en: Loss functions and optimization strategies
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数和优化策略
- en: Logging
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录
- en: Pipeline modularity and reusability
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道模块化和可重用性
- en: Scaling your training pipeline for larger models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展你的训练管道以适应更大的模型
- en: Components of a training pipeline
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练管道的组成部分
- en: 'An LLM training pipeline consists of several interconnected steps, each playing
    a role in the model’s development. We’ll present a basic pipeline here and explore
    many of these components in further depth as we progress through the chapter:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 训练管道由几个相互关联的步骤组成，每个步骤在模型的发展中扮演着角色。我们将在这里展示一个基本管道，并在本章的后续部分深入探讨许多这些组件：
- en: '**Dataset creation**: Builds preprocessed data into a format suitable for training,
    often involving shuffling and batching.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集创建**：将预处理数据构建成适合训练的格式，通常涉及洗牌和分批处理。'
- en: '**Model architecture**: Defines the structure of the LLM, including the number
    of layers, attention mechanisms, and other architectural choices.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型架构**：定义了 LLM 的结构，包括层数、注意力机制和其他架构选择。'
- en: '**Training loop**: The core of the pipeline where the model learns from the
    data through forward and backward passes.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练循环**：管道的核心，模型通过正向和反向传递从数据中学习。'
- en: '**Optimization**: Handles parameter updates based on calculated gradients and
    chosen optimization strategies.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化**：根据计算出的梯度和选择的优化策略处理参数更新。'
- en: '**Evaluation**: Regularly assesses model performance on validation data to
    track progress and prevent overfitting. We will cover this topic in more detail
    in [*Chapter 14*](B31249_14.xhtml#_idTextAnchor230).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估**：定期评估模型在验证数据上的性能，以跟踪进度并防止过拟合。我们将在 [*第 14 章*](B31249_14.xhtml#_idTextAnchor230)
    中更详细地讨论这个主题。'
- en: '**Checkpointing**: Periodically saves model states to resume training or use
    for inference. We will cover this topic in detail in [*Chapter 10*](B31249_10.xhtml#_idTextAnchor162).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检查点**：定期保存模型状态以恢复训练或用于推理。我们将在 [*第 10 章*](B31249_10.xhtml#_idTextAnchor162)
    中详细讨论这个主题。'
- en: '**Logging and monitoring**: Continuously tracks training metrics and resource
    utilization.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记录和监控**：持续跟踪训练指标和资源利用率。'
- en: 'We’ll implement a basic LLM training pipeline using PyTorch and the Transformers
    library:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 PyTorch 和 Transformers 库实现一个基本的 LLM 训练管道：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: PyTorch is a popular deep learning framework that enables building neural networks
    through a dynamic computational graph, while the Transformers library implements
    the popular transformer architecture we discussed in [*Chapter 1*](B31249_01.xhtml#_idTextAnchor014).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 是一个流行的深度学习框架，它通过动态计算图允许构建神经网络，而 Transformers 库实现了我们在 [*第 1 章*](B31249_01.xhtml#_idTextAnchor014)
    中讨论的流行变压器架构。
- en: 'The following code block demonstrates the loading of a Wikipedia dataset and
    the tokenization of its text content using a pre-trained GPT-2 tokenizer:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块展示了使用预训练的 GPT-2 分词器加载维基百科数据集并对其文本内容进行分词的过程：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding code block, we’re setting up the data ingestion and preprocessing
    components of our pipeline. We use the Hugging Face Datasets library to load a
    Wikipedia dataset, which provides a large corpus of text suitable for training
    an LLM. We then initialize a tokenizer based on the **GPT-2 model**, which will
    be used to preprocess our text data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们正在设置管道的数据摄取和预处理组件。我们使用 Hugging Face Datasets 库加载维基百科数据集，它提供了一个适合训练
    LLM 的大型文本语料库。然后，我们初始化一个基于 **GPT-2 模型**的分词器，它将被用于预处理我们的文本数据。
- en: The `preprocess_function` defined above takes raw text examples and tokenizes
    them, truncating to a maximum length of 512 tokens and padding shorter sequences
    to this length. This ensures all our input sequences have the same length, which
    is necessary for efficient batch processing. We choose a `max_length` value of
    `512` as a balance between context length and memory efficiency. Longer sequences
    provide more context but require more memory and computation. Some recent LLM
    models, such as **Gemini 1.5 Pro**, can get as many as 2 million tokens in content
    length ([https://cloud.google.com/vertex-ai/generative-ai/docs/long-context](https://cloud.google.com/vertex-ai/generative-ai/docs/long-context)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 上文定义的`preprocess_function`函数将原始文本示例进行分词，截断到最大512个token的长度，并将较短的序列填充到这个长度。这确保了所有输入序列的长度相同，这对于高效的批量处理是必要的。我们选择`max_length`值为`512`，这是在上下文长度和内存效率之间的平衡。较长的序列提供更多的上下文，但需要更多的内存和计算。一些最近的LLM模型，如**Gemini
    1.5 Pro**，其内容长度可以达到多达200万个token（[https://cloud.google.com/vertex-ai/generative-ai/docs/long-context](https://cloud.google.com/vertex-ai/generative-ai/docs/long-context)）。
- en: 'Next, we create our training DataLoader, which will handle batching and shuffling
    of our dataset during training:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建我们的训练数据加载器，它将在训练过程中处理数据集的批处理和打乱：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We set the batch size to `8`, which is chosen as a balance between memory usage
    and training efficiency. Larger batch sizes can lead to faster training but require
    more GPU memory. For LLMs, which often have a large number of parameters, smaller
    batch sizes are often necessary to fit the model and data in GPU memory.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将批大小设置为`8`，这是在内存使用和训练效率之间做出的平衡选择。更大的批大小可以加快训练速度，但需要更多的GPU内存。对于具有大量参数的LLM，通常需要较小的批大小才能将模型和数据放入GPU内存中。
- en: 'We then initialize our model architecture using the pre-trained GPT-2 model.
    This gives us a strong starting point for our LLM, leveraging the knowledge already
    captured in the pre-trained weights. Using a pre-trained model as a starting point
    is a common practice in transfer learning, allowing us to benefit from the general
    language understanding learned by the model on a large corpus of text. See the
    following code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用预训练的GPT-2模型初始化我们的模型架构。这为我们LLM提供了一个强大的起点，利用了预训练权重中已经捕获的知识。使用预训练模型作为起点是迁移学习中的一种常见做法，使我们能够从模型在大量文本语料库上学习到的通用语言理解中受益。以下代码展示了这一过程：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As shown in the preceding code, for optimization, we use the `lr`) to `5e-5`,
    which is a common choice for fine-tuning pre-trained models. The learning rate
    is a hyperparameter that determines the size of the adjustments made to the model’s
    weights during training, influencing how quickly and effectively the model learns.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，为了优化，我们将学习率`lr`设置为`5e-5`，这是微调预训练模型时的一个常见选择。学习率是一个超参数，它决定了在训练过程中对模型权重进行调整的大小，影响着模型学习的速度和效率。
- en: This learning rate offers a good balance between learning speed and stability.
    It’s small enough to allow for fine-grained updates to the pre-trained weights,
    but large enough to allow meaningful learning to occur.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个学习率在学习和稳定性之间提供了良好的平衡。它足够小，可以允许对预训练权重进行精细的更新，但又足够大，以允许有意义的学习发生。
- en: The subsequent code blocks outline the essential stages of training a language
    model, including setting up the training process, initializing a logging tool,
    executing the main training loop with forward and backward passes, performing
    evaluation to assess model performance, and saving checkpoints of the model’s
    parameters during training.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块概述了训练语言模型的基本阶段，包括设置训练过程、初始化日志工具、执行带有正向和反向传递的主训练循环、执行评估以评估模型性能，以及在训练过程中保存模型参数的检查点。
- en: 'We start by setting up the training loop:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先设置训练循环：
- en: '[PRE4]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we initialize the Weights & Biases (`wandb`) library for experiment tracking
    and logging of training metrics:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们初始化Weights & Biases (`wandb`)库以进行实验跟踪和训练指标的日志记录：
- en: '[PRE5]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We next implement an evaluation phase to assess the model’s performance on
    the training data:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们实现一个评估阶段来评估模型在训练数据上的性能：
- en: '[PRE6]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, we save a checkpoint of the model’s state dictionary at the end of
    each epoch:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在每个epoch结束时，我们保存模型状态字典的检查点：
- en: '[PRE7]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'These snippets implement the training loop, evaluation, checkpointing, and
    logging components of our pipeline:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代码片段实现了我们流水线中的训练循环、评估、检查点和日志记录组件：
- en: We set the number of training epochs to `3`, which means the model will iterate
    through the entire dataset three times during training. This hyperparameter can
    be adjusted based on your specific needs – increasing it may lead to better model
    performance if the model is underfitting, and decreasing it can help prevent overfitting
    and reduce training time. Monitor validation loss during training to determine
    the optimal number of epochs for your particular dataset and model architecture.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练时代数设置为`3`，这意味着模型将在训练期间遍历整个数据集三次。这个超参数可以根据你的具体需求进行调整——如果模型欠拟合，增加它可能会导致更好的模型性能，而减少它可以帮助防止过拟合并减少训练时间。在训练期间监控验证损失，以确定特定数据集和模型架构的最佳时代数。
- en: The learning rate scheduler implements a linear decay with warmup, which helps
    stabilize training in the early stages and then gradually reduces the learning
    rate to fine-tune the model more precisely. The learning rate controls how much
    a model adjusts its internal parameters during training – a higher rate means
    bigger adjustments but potential overshooting, while a lower rate means more precise
    but slower learning.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率调度器实现了一个带有预热阶段的线性衰减，这有助于在训练的早期阶段稳定训练，然后逐渐降低学习率以更精确地微调模型。学习率控制模型在训练期间调整其内部参数的程度——较高的速率意味着更大的调整但可能存在过度调整的风险，而较低的速率意味着更精确但学习速度较慢。
- en: We use `wandb`) for logging, which allows us to track our training progress
    in real time and compare different runs ([https://wandb.ai/site](https://wandb.ai/site)).
    This is crucial for monitoring the training process and making informed decisions
    about hyperparameter tuning and model architecture changes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`wandb`进行记录，这允许我们实时跟踪我们的训练进度并比较不同的运行([https://wandb.ai/site](https://wandb.ai/site))。这对于监控训练过程和在超参数调整和模型架构更改方面做出明智的决定至关重要。
- en: 'The training loop iterates over our data for the specified number of epochs.
    In each iteration, we do the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环遍历指定数量的时代数据。在每次迭代中，我们执行以下操作：
- en: Move the batch to the appropriate device (GPU if available)
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将批次移动到适当的设备（如果有GPU则使用GPU）
- en: Perform a forward pass through the model
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过模型执行正向传递
- en: Calculate the loss
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失
- en: Perform backpropagation
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行反向传播
- en: Update the model parameters
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新模型参数
- en: Update the learning rate scheduler
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新学习率调度器
- en: Log the training loss
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录训练损失
- en: After each epoch, we perform a simple evaluation of the training data (in a
    real scenario, you’d use a separate validation set), log the evaluation loss,
    and save a checkpoint of the model. Checkpointing is needed for long-running training
    processes, allowing us to resume training from a saved state if needed.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时代之后，我们执行对训练数据的简单评估（在实际场景中，你会使用一个单独的验证集），记录评估损失，并保存模型的检查点。检查点对于长时间运行的训练过程是必需的，允许我们在需要时从保存的状态恢复训练。
- en: As we’ve seen, the training pipeline involves several essential steps. Before
    the model architecture and training loop can function effectively, however, we
    must address data input and preprocessing, which we will discuss next.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，训练管道涉及几个基本步骤。然而，在模型架构和训练循环可以有效地运行之前，我们必须解决数据输入和预处理问题，我们将在下一节中讨论。
- en: Data input and preprocessing
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据输入和预处理
- en: 'Efficient data handling is crucial for LLM training, as we discussed in *Part
    1* of this book. Here, let’s explore advanced techniques for data input and preprocessing:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的数据处理对于LLM训练至关重要，正如我们在本书的*第1部分*中讨论的那样。在这里，让我们探讨数据输入和预处理的高级技术：
- en: 'Import the required Python packages:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的Python包：
- en: '[PRE8]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Load and combine multiple datasets:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载和组合多个数据集：
- en: '[PRE9]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Initialize the tokenizer and perform `preprocess`:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化分词器并执行`preprocess`：
- en: '[PRE10]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Create the DataLoader:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建数据加载器：
- en: '[PRE11]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In this enhanced preprocessing pipeline, we’re loading multiple datasets to
    increase the diversity of our training data. This is needed for LLMs, as a diverse
    dataset helps the model learn a broader range of language patterns and knowledge.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个增强的预处理管道中，我们正在加载多个数据集以增加我们训练数据的多样性。这对于LLM是必要的，因为多样化的数据集有助于模型学习更广泛的语言模式和知识。
- en: We use a longer `max_length` value of `1024` tokens to provide more context
    to the model. This increased context length allows the model to capture longer-range
    dependencies in the text, which can be beneficial for many language-understanding
    tasks. However, it also increases memory usage and computational requirements,
    so there’s a trade-off to consider.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用较长的`max_length`值为`1024`个标记，为模型提供更多上下文。这种增加的上下文长度允许模型捕获文本中的长距离依赖关系，这对许多语言理解任务可能有益。然而，这也增加了内存使用和计算需求，因此需要权衡考虑。
- en: The `preprocess_function` now creates labels for causal language modeling by
    shifting the input sequences. This is a common approach for training language
    models, where the model’s task is to predict the next token given the previous
    tokens. During preprocessing, handling edge cases such as emojis, URLs, and non-standard
    characters can enhance model performance. Emojis can convey nuanced emotions and
    context, requiring appropriate encoding or tokenization to preserve their meaning
    without introducing noise. URLs often contain valuable information but can vary
    widely in structure, so they might be replaced with placeholder tokens to maintain
    consistency while preventing the model from overfitting to specific links. Non-standard
    characters, including symbols from different languages or special punctuation,
    need careful normalization or removal to reduce complexity and avoid confusion
    during training. By addressing these edge cases through strategies such as normalization,
    token replacement, and selective filtering, preprocessing pipelines can better
    prepare diverse and complex data, enhancing the robustness and accuracy of the
    resulting language models.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`preprocess_function`现在通过移位输入序列为因果语言建模创建标签。这是训练语言模型的一种常见方法，其中模型的任务是预测给定前一个标记的下一个标记。在预处理过程中，处理边缘情况，如表情符号、URL和非标准字符，可以提高模型性能。表情符号可以传达细微的情感和上下文，需要适当的编码或标记化以保留其含义而不引入噪声。URL通常包含有价值的信息，但结构可能差异很大，因此可能用占位符标记替换以保持一致性，同时防止模型过度拟合到特定链接。非标准字符，包括来自不同语言的符号或特殊标点符号，需要仔细归一化或删除以减少复杂性和避免训练时的混淆。通过采用归一化、标记替换和选择性过滤等策略解决这些边缘情况，预处理管道可以更好地准备多样化和复杂的数据，从而提高结果语言模型的鲁棒性和准确性。'
- en: We use multiprocessing (`num_proc=4`) to speed up the preprocessing. The number
    of processes should be adjusted based on your CPU cores and available memory.
    Multiprocessing can significantly reduce preprocessing time, especially for large
    datasets.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用多进程（`num_proc=4`）来加速预处理。进程数应根据您的CPU核心数和可用内存进行调整。多进程可以显著减少预处理时间，特别是对于大型数据集。
- en: The batch size is increased to `16`, which is more suitable for larger GPU memory.
    The custom `collate_fn` in the DataLoader ensures proper batching of our preprocessed
    data. This function stacks the arrays for each key in the batch, creating tensor-like
    structures that can be efficiently processed by PyTorch.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理大小增加到`16`，这更适合较大的GPU内存。DataLoader中的自定义`collate_fn`确保了我们的预处理数据的正确批处理。此函数将批处理中每个键的数组堆叠，创建出可以被PyTorch高效处理的张量结构。
- en: With the data appropriately prepared, we now turn our attention to the LLM architecture
    design considerations, which dictate the model’s capacity to effectively learn
    from and understand data input.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据适当准备后，我们现在将注意力转向LLM架构设计考虑因素，这些因素决定了模型有效学习并理解数据输入的能力。
- en: LLM architecture design considerations
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM架构设计考虑因素
- en: When designing the architecture for an LLM, several factors come into play.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计LLM的架构时，有几个因素需要考虑。
- en: 'Here are the key factors influencing LLM architecture:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 影响LLM架构的关键因素如下：
- en: '**Vocabulary size**: Determines the size of the input and output embedding
    layers'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词汇量大小**：决定了输入和输出嵌入层的大小'
- en: '**Maximum sequence length (context size)**: Defines the amount of preceding
    text the model can consider'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大序列长度（上下文大小）**：定义了模型可以考虑的先前文本的数量'
- en: '**Embedding dimension**: Specifies the size of each token’s vector representation,
    influencing the model’s ability to capture information'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入维度**：指定每个标记的向量表示的大小，影响模型捕获信息的能力'
- en: '**Number of transformer layers**: Represents the depth of the network, impacting
    the complexity of patterns the model can learn'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**transformer层数量**：代表网络的深度，影响模型可以学习的模式复杂性'
- en: '**Number of attention heads**: Allows the model to attend to different parts
    of the input simultaneously'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注意力头数量**：允许模型同时关注输入的不同部分'
- en: '**Model size (number of parameters)**: Overall capacity of the model, influenced
    by embedding dimension, number of layers, and attention heads'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型大小（参数数量）**：模型的整体容量，受嵌入维度、层数和注意力头数的影响'
- en: '**Dataset size**: The amount and diversity of training data'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集大小**：训练数据的数量和多样性'
- en: '**Number of training steps**: The duration of the optimization process'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练步数数量**：优化过程的持续时间'
- en: '**Computational resources**: Hardware constraints that affect model size, training
    speed, and overall feasibility.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算资源**：影响模型大小、训练速度和整体可行性的硬件限制。'
- en: '**Risk of overfitting**: Higher with larger models and smaller datasets'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合风险**：随着模型规模增大和数据集减小而增加'
- en: '**Data quality**: The cleanliness and relevance of the training data'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据质量**：训练数据的清洁度和相关性'
- en: '**Efficiency of model architecture**: Design choices that can improve performance
    without drastically increasing model size'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型架构效率**：可以在不大幅增加模型大小的情况下提高性能的设计选择'
- en: '**Training algorithms**: Optimization techniques and strategies'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练算法**：优化技术和策略'
- en: '**Data curation practices**: Methods for selecting and preparing training data'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据整理实践**：选择和准备训练数据的方法'
- en: '**Test time compute**: Computational resources available during inference'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理时间计算资源**：推理过程中可用的计算资源'
- en: In the following code block, we provide examples of configuring some of these
    factors using a GPT-2 style language model, specifying key architectural parameters.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码块中，我们提供了使用 GPT-2 风格的语言模型配置一些这些因素的示例，指定关键架构参数。
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This configuration creates a GPT-2 style model with `12` layers and `12` attention
    heads. Let’s break down the key parameters:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置创建了一个具有 `12` 层和 `12` 个注意力头的 GPT-2 风格模型。让我们分解关键参数：
- en: '`vocab_size`: Set to `50257`, which is the vocabulary size of the original
    GPT-2 model. This determines the size of the embedding layer and the output layer.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`：设置为 `50257`，这是原始 GPT-2 模型的词汇量。这决定了嵌入层和输出层的大小。'
- en: '`n_positions` and `n_ctx`: Both are set to `1024`, matching our preprocessing
    step. This defines the maximum sequence length the model can handle.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_positions` 和 `n_ctx`：两者都设置为 `1024`，与我们的预处理步骤相匹配。这定义了模型可以处理的最大序列长度。'
- en: '`n_embd`: The embedding dimension, set to `768`. This determines the size of
    the hidden states throughout the model.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_embd`：嵌入维度，设置为 `768`。这决定了模型中隐藏状态的大小。'
- en: '`n_layer`: The number of transformer layers, set to `12`. More layers can capture
    more complex patterns but increase computational requirements.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_layer`：transformer 层数的数量，设置为 `12`。更多的层可以捕捉更复杂的模式，但会增加计算需求。'
- en: '`n_head`: The number of attention heads, set to `12`. Multiple attention heads
    allow the model to focus on different aspects of the input simultaneously.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_head`：注意力头的数量，设置为 `12`。多个注意力头允许模型同时关注输入的不同方面。'
- en: The embedding dimension of `768` and the `12` layers provide a balanced trade-off
    between model capacity and computational efficiency. This configuration results
    in a model with about 124 million parameters, which is substantial but still trainable
    on common GPU hardware.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`768` 的嵌入维度和 `12` 层提供了在模型容量和计算效率之间的平衡折衷。这种配置产生了一个大约有 1.24 亿个参数的模型，这相当大，但仍然可以在常见的
    GPU 硬件上训练。'
- en: For larger models, you might increase `n_layer`, `n_embd`, and `n_head`. However,
    this would also increase the computational requirements and the risk of overfitting,
    especially on smaller datasets. When scaling up, consider techniques such as gradient
    accumulation, mixed precision training, and distributed training to manage the
    increased computational load.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更大的模型，你可能需要增加 `n_layer`、`n_embd` 和 `n_head`。然而，这也会增加计算需求以及过拟合的风险，尤其是在较小的数据集上。在扩展规模时，考虑使用梯度累积、混合精度训练和分布式训练等技术来管理增加的计算负载。
- en: 'In a broader scope, **scaling laws** can be considered. The scaling laws for
    LLMs describe how performance improves predictably as three key factors increase:
    model size (number of parameters), dataset size (amount of training data), and
    the number of training steps (optimization iterations). Specifically, larger models
    tend to capture more complex patterns and exhibit better generalization, larger
    datasets provide more diverse information for learning, and more training steps
    allow the model to refine its understanding and reduce errors. For optimal performance,
    these factors should scale proportionally – for instance, increasing the model
    size should be matched by a corresponding increase in dataset size and training
    steps. This balanced scaling ensures that each component supports the others,
    preventing bottlenecks such as overfitting smaller models on vast datasets or
    undertraining large models with insufficient data.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在更广泛的范围内，可以考虑**扩展定律**。LLM的扩展定律描述了随着三个关键因素的提高，性能如何可预测地提升：模型大小（参数数量）、数据集大小（训练数据量）和训练步数（优化迭代次数）。具体来说，更大的模型倾向于捕捉更复杂的模式并表现出更好的泛化能力，更大的数据集为学习提供了更多样化的信息，更多的训练步数允许模型细化其理解并减少错误。为了获得最佳性能，这些因素应该成比例扩展——例如，增加模型大小应该与数据集大小和训练步数的相应增加相匹配。这种平衡扩展确保每个组件都支持其他组件，防止了诸如在庞大的数据集上过度拟合较小模型或用不足的数据训练大型模型等瓶颈问题。
- en: However, recent advancements and practical challenges have shown that simply
    scaling these factors is not always sufficient for continual performance improvements.
    Issues such as diminishing returns, where each additional parameter or data point
    contributes less to overall performance, have become more apparent. Additionally,
    the immense computational and energy resources required for training increasingly
    large models raise sustainability and accessibility concerns. Data quality also
    becomes a critical factor, as larger datasets may introduce more noise and biases,
    potentially degrading model performance. For more details about this, please see
    the article at [https://www.pcgamer.com/software/ai/open-ai-co-founder-reckons-ai-training-has-hit-a-wall-forcing-ai-labs-to-train-their-models-smarter-not-just-bigger/](https://www.pcgamer.com/software/ai/open-ai-co-founder-reckons-ai-training-has-hit-a-wall-forcing-ai-labs-to-train-their-models-smarter-not-just-bigger/).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最近的发展和实际挑战表明，仅仅扩展这些因素并不总是足以实现持续的性能提升。例如，边际效益递减的问题，即每个额外的参数或数据点对整体性能的贡献越来越少，变得更加明显。此外，训练越来越大的模型所需的巨大计算和能源资源引发了可持续性和可访问性的担忧。数据质量也成为了一个关键因素，因为更大的数据集可能会引入更多的噪声和偏差，从而降低模型性能。关于这方面的更多细节，请参阅[https://www.pcgamer.com/software/ai/open-ai-co-founder-reckons-ai-training-has-hit-a-wall-forcing-ai-labs-to-train-their-models-smarter-not-just-bigger/](https://www.pcgamer.com/software/ai/open-ai-co-founder-reckons-ai-training-has-hit-a-wall-forcing-ai-labs-to-train-their-models-smarter-not-just-bigger/)上的文章。
- en: 'To address these challenges, researchers are exploring more efficient model
    architectures, improved training algorithms, better data curation practices, and
    test time compute. See my Medium article for more details on test time compute:
    [https://kenhuangus.medium.com/test-time-compute-3633a4c55716](https://kenhuangus.medium.com/test-time-compute-3633a4c55716)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，研究人员正在探索更有效的模型架构、改进的训练算法、更好的数据整理实践以及测试时间计算。有关测试时间计算的更多细节，请参阅我的Medium文章：[https://kenhuangus.medium.com/test-time-compute-3633a4c55716](https://kenhuangus.medium.com/test-time-compute-3633a4c55716)。
- en: At the beginning of 2025, DeepSeek (an AI startup in China) announced some model
    training innovations by introducing a suite of techniques aimed at significantly
    increasing efficiency and reducing costs, while simultaneously enhancing the model’s
    reasoning capabilities ([https://arxiv.org/abs/2501.12948](https://arxiv.org/abs/2501.12948)).
    Unlike traditional approaches that rely heavily on vast computational resources
    and human-supervised fine-tuning, DeepSeek leverages large-scale reinforcement
    learning focused on reasoning tasks, using automated reward systems rather than
    human feedback. Key innovations include multi-token prediction, which allows the
    model to learn from multiple future tokens at once, increasing sample efficiency,
    and speeding up training. DeepSeek also employs a mixture-of-experts architecture
    to activate only relevant sub-networks for each task, thus reducing computational
    load. By optimizing both algorithms and hardware, DeepSeek has managed to train
    highly capable models at a fraction of the cost and time required by competitors,
    setting new standards for open, efficient, and powerful AI development.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 2025 年初，DeepSeek（一家中国的人工智能初创公司）通过引入一系列旨在显著提高效率和降低成本的同时增强模型推理能力的技术的套件，宣布了一些模型训练创新（[https://arxiv.org/abs/2501.12948](https://arxiv.org/abs/2501.12948)）。与严重依赖大量计算资源和人工监督微调的传统方法不同，DeepSeek
    利用针对推理任务的大规模强化学习，使用自动奖励系统而不是人类反馈。关键创新包括多标记预测，这使得模型能够一次学习多个未来的标记，从而提高样本效率并加快训练速度。DeepSeek
    还采用专家混合架构，只为每个任务激活相关的子网络，从而减少计算负载。通过优化算法和硬件，DeepSeek 已经能够以竞争对手所需成本和时间的一小部分训练出高度能干的模型，为开放、高效和强大的
    AI 开发设定了新的标准。
- en: Having explored the architectural design considerations and model training innovations
    for LLMs — along with a code example demonstrating how to configure model training
    parameters — we are now ready to examine how these architectural choices are actually
    learned during training. In this next section, we will discuss the loss function
    and optimization strategies, which serve as the engine that drives the model to
    adjust its internal parameters based on both the training data and the architecture
    we have defined.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨了 LLMs 的架构设计考虑因素和模型训练创新，以及一个演示如何配置模型训练参数的代码示例之后，我们现在准备检查这些架构选择在训练过程中是如何实际学习的。在接下来的这一节中，我们将讨论损失函数和优化策略，它们是推动模型根据训练数据和我们所定义的架构调整其内部参数的引擎。
- en: Loss functions and optimization strategies
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数和优化策略
- en: LLMs typically use **cross-entropy loss** for training. This approach measures
    the difference between the model’s predicted probability distribution of words
    and the actual distribution in the training data. By minimizing this loss, LLMs
    learn to generate more accurate and contextually appropriate text. Cross-entropy
    loss is particularly well-suited for language tasks due to its ability to handle
    the high dimensionality and discrete nature of textual data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 通常使用 **交叉熵损失** 进行训练。这种方法衡量模型预测的单词概率分布与训练数据中实际分布之间的差异。通过最小化这种损失，LLMs 学习生成更准确和上下文相关的文本。由于交叉熵损失能够处理文本数据的高维性和离散性，因此它特别适合语言任务。
- en: 'Let’s implement this along with some advanced optimization techniques:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们结合一些高级优化技术来实现这个功能：
- en: 'First, we import the required PyTorch libraries and specific modules from the
    Transformers library for optimization:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的 PyTorch 库以及来自 Transformers 库的特定模块以进行优化：
- en: '[PRE13]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we configure the AdamW optimizer with a specified learning rate and weight
    decay:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们配置 AdamW 优化器，指定学习率和权重衰减：
- en: '[PRE14]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, we define a linear learning rate scheduler with a warm-up period:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义一个具有预热期的线性学习率调度器：
- en: '[PRE15]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Subsequently, we set up the training device and initiate the main training
    loop:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们设置训练设备和启动主训练循环：
- en: '[PRE16]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, we implement gradient clipping to prevent exploding gradients during
    training:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们实现梯度裁剪以防止训练过程中的梯度爆炸：
- en: '[PRE17]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In this optimization setup, we use the `AdamW` optimizer with a learning rate
    of `5e-5` and weight decay of `0.01`. The algorithm adapts the learning rate for
    each parameter based on the first and second moments of the gradients, allowing
    it to handle sparse gradients effectively. This makes `AdamW` particularly useful
    for training large neural networks.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个优化设置中，我们使用学习率为`5e-5`和权重衰减为`0.01`的`AdamW`优化器。该算法根据梯度的第一和第二矩来调整每个参数的学习率，使其能够有效地处理稀疏梯度。这使得`AdamW`对于训练大型神经网络特别有用。
- en: The weight decay of `0.01` adds a small regularization term to the loss function,
    which can help prevent overfitting by penalizing large weight values.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 权重衰减`0.01`向损失函数中添加了一个小的正则化项，这可以通过惩罚大的权重值来帮助防止过拟合。
- en: We implement a `warmup`. The `warmup` phase helps stabilize training in the
    early stages by gradually increasing the learning rate from a very small value.
    After the `warmup` phase, the learning rate decreases linearly. This schedule
    can help the model converge to a better optimum.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了一个`预热`阶段。预热阶段通过逐渐增加学习率从一个非常小的值来帮助在早期阶段稳定训练。在预热阶段之后，学习率线性下降。这种时间表可以帮助模型收敛到一个更好的最优解。
- en: In the training loop, we implement `max_norm` value of `1.0`. Gradient clipping
    prevents exploding gradients by scaling down gradient values that exceed a certain
    threshold. This is particularly important for LLMs, which can be prone to unstable
    gradients due to their depth and the long-range dependencies they capture.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练循环中，我们实现了`max_norm`值为`1.0`。梯度裁剪通过缩小超过某个阈值的梯度值来防止梯度爆炸。这对于LLM尤其重要，因为LLM由于其深度和捕获的长距离依赖性，可能会出现不稳定的梯度。
- en: In this section, we learned about AdamW optimization, learning rate scheduling
    with warmup, and gradient clipping for stable LLM training. Next, we talk about
    logging the training process, which is crucial for monitoring progress and using
    tools like **TensorBoard** to gain insights for improvement.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了关于AdamW优化、带有预热的学习率调度以及梯度裁剪以稳定LLM训练的内容。接下来，我们将讨论记录训练过程，这对于监控进度和使用如**TensorBoard**等工具以获得改进的见解至关重要。
- en: Logging
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志记录
- en: Effective logging can be useful for tracking the progress of LLM training.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的日志记录对于跟踪LLM训练的进度非常有用。
- en: The following code blocks demonstrate how to integrate TensorBoard for effective
    logging during the training of an LLM using PyTorch. Let’s break down each part.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块演示了如何使用PyTorch在训练LLM时集成TensorBoard以进行有效的日志记录。让我们分解每个部分。
- en: 'We first initialize the TensorBoard `SummaryWriter` for logging training progress:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先初始化TensorBoard的`SummaryWriter`以记录训练进度：
- en: '[PRE18]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we set the model to training mode, initialize variables for tracking
    loss, define the logging interval, and record the start time to monitor training
    performance:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将模型设置为训练模式，初始化跟踪损失的变量，定义日志间隔，并记录开始时间以监控训练性能：
- en: '[PRE19]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, we move on to the training loop. We process each batch by moving data
    to the appropriate device, performing forward and backward passes, applying gradient
    clipping, and updating the model’s parameters using the optimizer and scheduler:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们进入训练循环。我们通过将数据移动到适当的设备，执行正向和反向传递，应用梯度裁剪，并使用优化器和调度器更新模型的参数来处理每个批次：
- en: '[PRE20]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We log the training metrics to TensorBoard at specified intervals, calculate
    the average loss, measure the elapsed time, print the progress to the console,
    and reset the tracking variables for the next interval:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在指定的间隔将训练指标记录到TensorBoard中，计算平均损失，测量经过的时间，将进度打印到控制台，并为下一个间隔重置跟踪变量：
- en: '[PRE21]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This enhanced training loop uses TensorBoard for logging training loss and
    learning rate. TensorBoard is a powerful tool for visualizing training progress
    and comparing different runs. We log the following metrics:'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个增强的训练循环使用TensorBoard来记录训练损失和学习率。TensorBoard是一个强大的工具，用于可视化训练进度和比较不同的运行。我们记录以下指标：
- en: '`log_interval` batches. A decreasing trend in this metric indicates that the
    model is learning.'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_interval`批次。此指标呈下降趋势表明模型正在学习。'
- en: '**Learning rate**: We log the current learning rate to visualize how it changes
    over time due to our learning rate scheduler.'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**：我们记录当前的学习率，以可视化由于我们的学习率调度器而随时间变化的情况。'
- en: We set `log_interval` to `100`, meaning we log and print out progress information
    every 100 batches. This interval strikes a balance between getting frequent updates
    and not slowing down training too much with logging operations. You may need to
    adjust this based on your dataset size and training speed.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`log_interval`设置为`100`，这意味着我们每100个批次记录和打印进度信息。这个间隔在获取频繁更新和不过度减慢训练速度之间取得了平衡。您可能需要根据您的数据集大小和训练速度进行调整。
- en: 'The output or log information includes the following:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 输出或日志信息包括以下内容：
- en: Current epoch and batch number
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前周期和批次号
- en: Current learning rate
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前学习率
- en: Time per batch (in milliseconds)
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每批次的耗时（以毫秒为单位）
- en: Current loss
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前损失
- en: This detailed logging allows you to monitor the training process closely, helping
    you identify issues such as unstable loss, learning rate problems, or unexpectedly
    slow training.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这种详细的日志记录允许您密切监控训练过程，帮助您识别不稳定损失、学习率问题或意外缓慢的训练等问题。
- en: Pipeline modularity and reusability
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管道模块化和可重用性
- en: '**Modularity** and **reusability** are fundamental principles for building
    efficient pipelines because they make code more maintainable, adaptable, and reliable.
    By breaking down a pipeline into independent, reusable modules (such as data preprocessing,
    model training, and evaluation components), developers can easily modify individual
    parts without affecting others, test each component separately, and reuse proven
    code across different projects.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**模块化**和**可重用性**是构建高效管道的基本原则，因为它们使代码更易于维护、适应和可靠。通过将管道分解为独立的、可重用的模块（如数据预处理、模型训练和评估组件），开发者可以轻松修改单个部分而不影响其他部分，单独测试每个组件，并在不同项目之间重用经过验证的代码。'
- en: This approach not only saves development time but also ensures consistency in
    operations, reduces the chance of errors, and makes it easier for teams to collaborate
    by working on separate modules while maintaining clear interfaces between components.
    In the case of training pipelines, encapsulating processes in reusable classes
    allows for flexible configuration, seamless integration with different datasets,
    and straightforward sharing of standardized implementations across multiple projects.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法不仅节省了开发时间，还确保了操作的连续性，减少了出错的机会，并使团队在维护组件之间清晰接口的同时，通过在单独的模块上工作而更容易协作。在训练管道的情况下，将过程封装在可重用类中允许灵活配置，与不同数据集的无缝集成，以及在不同项目之间轻松共享标准化实现。
- en: 'To make our pipeline more modular and reusable, let’s encapsulate our training
    process in a class:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的管道更加模块化和可重用，让我们将我们的训练过程封装在一个类中：
- en: 'We start with class definition:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从类定义开始：
- en: '[PRE22]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, we define the train epoch function. The function sets the model to training
    mode and iterates over the training data, processing each batch by computing the
    loss, performing backpropagation with gradient clipping, and updating the model
    parameters using the optimizer and scheduler:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义训练周期函数。该函数将模型设置为训练模式，并遍历训练数据，通过计算损失、执行梯度裁剪的逆向传播以及使用优化器和调度器更新模型参数来处理每个批次：
- en: '[PRE23]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we periodically log the training progress to both TensorBoard and the
    console by checking if the current batch index is a multiple of the `log_interval`;
    if it is, we calculate the average loss and elapsed time since the last log, record
    the training loss and learning rate to TensorBoard using the `SummaryWriter`,
    print a formatted progress update including batch number, learning rate, milliseconds
    per batch, and current loss to the console, and then reset the accumulated `total_loss`
    and `start_time` for the next logging interval:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通过检查当前批次索引是否是`log_interval`的倍数来定期将训练进度记录到TensorBoard和控制台；如果是，我们计算平均损失和自上次日志以来的经过时间，使用`SummaryWriter`将训练损失和学习率记录到TensorBoard，打印包括批次号、学习率、每批次的毫秒数和当前损失等信息的格式化进度更新到控制台，然后重置下一次日志间隔的累积`total_loss`和`start_time`：
- en: '[PRE24]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, the `train` function orchestrates the training process by looping through
    the specified number of epochs, printing a message at the start of each epoch,
    invoking the `train_epoch` method to perform training for that epoch, and, finally,
    closing the writer once all epochs are completed. It serves as the main entry
    point for training, providing a structure where additional features such as validation
    and checkpointing can be integrated as needed:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，`train` 函数通过遍历指定的周期数来协调训练过程，在每个周期的开始打印一条消息，调用 `train_epoch` 方法执行该周期的训练，并在所有周期完成后关闭写入器。它是训练的主要入口点，提供了一个结构，其中可以按需集成如验证和检查点等附加功能：
- en: '[PRE25]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Lastly, we instantiate the `LLMTrainer` class with the specified model, training
    data loader, optimizer, scheduler, and device. Then, the training process is started
    by calling the `train` method to execute three full training epochs, thereby initiating
    and managing the model’s learning cycle:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用指定的模型、训练数据加载器、优化器、调度器和设备实例化 `LLMTrainer` 类。然后，通过调用 `train` 方法来执行三个完整的训练周期，从而启动并管理模型的学习周期：
- en: '[PRE26]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This modular design offers several advantages:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模块化设计提供了几个优点：
- en: '`LLMTrainer` class, making it easier to manage and understand.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LLMTrainer` 类，使其更容易管理和理解。'
- en: '**Reusability**: You can easily use this trainer for different models or datasets
    by creating a new instance with different parameters.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可重用性**：你可以通过创建具有不同参数的新实例，轻松地将此训练器用于不同的模型或数据集。'
- en: '**Extensibility**: The class structure makes it easy to add new functionality.
    For example, you could add methods for validation, checkpointing, or early stopping.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：类结构使得添加新功能变得容易。例如，你可以添加用于验证、检查点或早期停止的方法。'
- en: '**Separation of concerns**: The training logic is separated from the model
    definition and data preparation, following good software engineering principles.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关注点分离**：训练逻辑与模型定义和数据准备分离，遵循良好的软件工程原则。'
- en: 'The following log demonstrates the training process over 3 epochs, with periodic
    logging every 100 batches. Each log entry includes the current batch number, total
    batches, learning rate, milliseconds per batch, and the average loss:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下日志演示了在 3 个周期内进行的训练过程，每隔 100 批次进行一次定期记录。每个日志条目包括当前批次号、总批次数、学习率、每批次的毫秒数和平均损失：
- en: '[PRE27]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here is an explanation of the above simulated log:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是对上述模拟日志的解释：
- en: '`Starting epoch 1`, indicating the commencement of a new training cycle'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`开始第 1 个周期`，表示新的训练周期的开始'
- en: '`100/1000 batches`'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`100/1000 批次`'
- en: '`lr 0.01`'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr 0.01`'
- en: '`ms/batch 45.67`'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ms/批次 45.67`'
- en: '`loss 2.35`'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`损失 2.35`'
- en: '**Learning rate schedule**: Notice how the learning rate decreases over epochs,
    reflecting the scheduler’s adjustments to facilitate better convergence*   `Training
    completed. Writer closed.`) indicates the end of the training process and the
    closure of the logging writer'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率调度**：注意学习率在周期内是如何降低的，这反映了调度器为促进更好的收敛所做的调整*   `训练完成。写入器已关闭。`) 表示训练过程的结束和日志写入器的关闭'
- en: The log provides a clear overview of the training dynamics, allowing developers
    and researchers to monitor the model’s learning progress, adjust hyperparameters
    if necessary, and ensure that the training is proceeding as expected.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 日志提供了对训练动态的清晰概述，允许开发人员和研究人员监控模型的学习进度，如有必要，调整超参数，并确保训练按预期进行。
- en: Scaling your training pipeline for larger models
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展您的训练流程以适应更大的模型
- en: To train larger models, we need to employ techniques such as gradient accumulation
    and mixed precision training.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练更大的模型，我们需要采用梯度累积和混合精度训练等技术。
- en: 'To train very large language models that might not fit on a single GPU, the
    following code introduces a special `LargeScaleLLMTrainer`. It uses two main tricks
    to handle this:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练可能不适合单个 GPU 的大规模语言模型，以下代码引入了一个特殊的 `LargeScaleLLMTrainer`。它使用两个主要技巧来处理这个问题：
- en: First, gradient accumulation allows us to simulate having access to a larger
    GPU. Instead of updating the model's parameters after every small batch of data,
    we process several small batches, accumulating their gradients along the way.
    Only after a predefined number of batches do we perform an actual update to the
    model's parameters. This technique enables the model to learn as if it had seen
    a much larger batch of data, without requiring the memory capacity of an extremely
    large GPU.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，梯度累积允许我们模拟访问更大的GPU。我们不是在每处理一小批数据后更新模型的参数，而是在处理几个小批量的过程中累积它们的梯度。只有在预定义的批次数之后，我们才对模型的参数进行实际更新。这种技术使模型能够像看到了一个更大的数据批次一样学习，而不需要极端大GPU的内存容量。
- en: Second, it employs mixed precision training, a technique where the computer
    performs many calculations using smaller, lower-precision numbers (which require
    less memory and are faster to compute), while reserving higher-precision numbers
    for situations where accuracy is critical. This approach accelerates training
    and reduces overall memory usage. To mitigate potential issues that can arise
    from using lower-precision values, GradScaler is used to maintain numerical stability
    during backpropagation.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，它采用混合精度训练，这是一种计算机使用较小的、低精度数字（需要更少的内存且计算速度更快）进行许多计算的技术，同时为精度至关重要的场合保留高精度数字。这种方法加速了训练并减少了整体内存使用。为了减轻使用低精度值可能出现的潜在问题，GradScaler在反向传播过程中保持数值稳定性。
- en: 'The following code defines how this special trainer works, including how it
    processes data, calculates the loss, and updates the model’s learning using these
    tricks. It also still includes important steps like making sure the gradients
    (how the model should change) don’t get too big and logging progress so we can
    see how the training is going. Finally, it shows a simple example of how to use
    this special trainer. Now, let us break it into several parts:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码定义了这种特殊训练器的工作方式，包括如何处理数据、计算损失以及使用这些技巧更新模型的参数、学习率和梯度缩放器。它还包括确保梯度（模型应该如何改变）不会太大以及记录进度以便我们可以看到训练进展的重要步骤。最后，它展示了如何使用这个特殊训练器的简单示例。现在，让我们将其分解成几个部分：
- en: 'Let us start by importing the relevant Python package and defining the class:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先导入相关的Python包并定义类：
- en: '[PRE28]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can then define the training epoch:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以定义训练的迭代次数：
- en: '[PRE29]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, we implement the following code block, which updates the model’s parameters,
    learning rate, and gradient scaler only after processing a defined number of batches
    (`accumulation_steps)`, effectively simulating a larger batch size while managing
    memory constraints:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们实现以下代码块，它仅在处理了定义数量的批次（`accumulation_steps`）之后更新模型的参数、学习率和梯度缩放器，有效地模拟更大的批次数同时管理内存限制：
- en: '[PRE30]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We then periodically calculate and log the average training loss and learning
    rate to TensorBoard, while also printing a summary of the current training progress
    to the console at intervals defined by `log_interval`:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定期计算并记录平均训练损失和学习率到TensorBoard，同时按照`log_interval`定义的时间间隔在控制台上打印当前训练进度的摘要：
- en: '[PRE31]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We demonstrate the initialization and execution of a large-scale language model
    training process:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们展示了大规模语言模型训练过程的初始化和执行：
- en: '[PRE32]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This enhanced trainer uses two key techniques for scaling to larger models:'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此增强型训练器使用两种关键技术来扩展到更大的模型：
- en: '`accumulation_steps`). This allows us to effectively increase the batch size
    without increasing memory usage, which is effective for training large models
    on limited GPU memory. We divide the loss by `accumulation_steps` to maintain
    the same effective learning rate.'
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: (`accumulation_steps`)。这允许我们有效地增加批次数而不增加内存使用，这对于在有限的GPU内存上训练大型模型是有效的。我们将损失除以`accumulation_steps`以保持相同的有效学习率。
- en: '`float16`, where possible, while maintaining `float32` master weights. This
    can significantly speed up training and reduce memory usage, especially on modern
    GPUs with tensor cores.'
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在可能的情况下使用`float16`，同时保留`float32`的主权重。这可以显著加快训练速度并减少内存使用，尤其是在具有张量核心的现代GPU上。
- en: '`GradScaler` is used to prevent underflow in `float16` calculations. It scales
    the loss to prevent small gradient values, then unscales before the optimizer
    step.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`GradScaler`用于防止`float16`计算中的下溢。它将损失缩放以防止小的梯度值，然后在优化器步骤之前进行缩放。'
- en: We still apply gradient clipping, but now it’s done after unscaling the gradients
    to ensure we’re clipping the true gradient values.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然应用梯度裁剪，但现在是在将梯度缩放回原始尺度之后进行的，以确保我们裁剪的是真实的梯度值。
- en: For even larger models, you might consider techniques such as **model parallelism**
    (splitting the model across multiple GPUs), **pipeline parallelism** (splitting
    the model into stages), or using specialized libraries such as **DeepSpeed** or
    **Megatron-LM**. These advanced techniques allow the training of models with billions
    of parameters across multiple GPUs or even multiple machines. Memory offloading
    can be a good alternative when GPU memory is insufficient to handle vast amounts
    of data and model parameters. Memory offloading involves transferring parts of
    the model’s data or computations to alternative memory storage, such as **Non-Volatile
    Memory Express** (**NVMe**) SSDs. By leveraging NVMe memory, which offers high-speed
    data access compared to traditional storage, systems can effectively manage and
    store intermediate activations, gradients, and model states that exceed GPU memory
    capacity. This approach allows for training larger models or using higher batch
    sizes without requiring immediate GPU memory expansion. However, it introduces
    additional latency due to data transfer between the GPU and NVMe storage, which
    can impact training speed. Optimizing data access patterns and utilizing efficient
    offloading strategies can minimize performance overhead and maintain effective
    training workflows when employing memory offloading techniques.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更大的模型，你可能需要考虑诸如**模型并行**（将模型分割到多个GPU上）、**流水线并行**（将模型分割成阶段）或使用如**DeepSpeed**或**Megatron-LM**等专用库等技术。这些高级技术允许在多个GPU甚至多台机器上训练具有数十亿参数的模型。当GPU内存不足以处理大量数据和模型参数时，内存卸载可以是一个好的替代方案。内存卸载涉及将模型数据或计算的部分转移到替代内存存储，例如**非易失性内存表达式**（**NVMe**）SSD。通过利用NVMe内存，它提供与传统存储相比的高速数据访问，系统可以有效地管理和存储超出GPU内存容量的中间激活、梯度和模型状态。这种方法允许在不要求立即扩展GPU内存的情况下训练更大的模型或使用更大的批量大小。然而，由于GPU和NVMe存储之间的数据传输，它引入了额外的延迟，这可能会影响训练速度。优化数据访问模式并利用有效的卸载策略可以在采用内存卸载技术时最小化性能开销并保持有效的训练工作流程。
- en: Summary
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned about a practical pattern of pipeline design for
    training LLMs. You learned how to create efficient data preprocessing workflows,
    implement model architectures, and apply advanced optimization strategies. You
    now understand how to set up effective logging systems to track your model’s progress.
    You also explored techniques for building modular and reusable pipelines and discovered
    methods for scaling your training process to accommodate larger models. With these
    skills, you’re well equipped to train state-of-the-art language models efficiently
    and effectively.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解到了训练LLM的实际管道设计模式。你学习了如何创建高效的数据预处理工作流程，实现模型架构，并应用高级优化策略。你现在理解了如何设置有效的日志系统来跟踪你的模型进度。你还探索了构建模块化和可重用管道的技术，并发现了扩展你的训练过程以适应更大模型的方法。有了这些技能，你将能够高效且有效地训练最先进的语言模型。
- en: In the next chapter, we’ll explore the hyperparameter tuning pattern.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨超参数调整模式。
