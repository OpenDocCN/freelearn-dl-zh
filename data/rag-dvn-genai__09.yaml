- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: 'Empowering AI Models: Fine-Tuning RAG Data and Human Feedback'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 赋能 AI 模型：微调 RAG 数据和人类反馈
- en: An organization that continually increases the volume of its RAG data will reach
    the threshold of non-parametric data (not pretrained on an LLM). At that point,
    the mass of RAG data accumulated might become extremely challenging to manage,
    posing issues related to storage costs, retrieval resources, and the capacity
    of the generative AI models themselves. Moreover, a pretrained generative AI model
    is trained up to a cutoff date. The model ignores new knowledge starting the very
    next day. This means that it will be impossible for a user to interact with a
    chat model on the content of a newspaper edition published after the cutoff date.
    That is when retrieval has a key role to play in providing RAG-driven content.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 一个不断增加其 RAG 数据量的组织将达到非参数数据的阈值（未在 LLM 上预训练）。到那时，积累的 RAG 数据量可能会变得极其难以管理，引发与存储成本、检索资源和生成
    AI 模型自身容量相关的问题。此外，预训练的生成 AI 模型是训练到某个截止日期的。从第二天开始，该模型将忽略新的知识。这意味着用户将无法与截止日期之后出版的报纸版面的内容进行交互。那时，检索在提供
    RAG 驱动的内容中扮演关键角色。
- en: Companies like Google, Microsoft, Amazon, and other web giants may require exponential
    data and resources. Certain domains, such as the legal rulings in the United States,
    may indeed require vast amounts of data. However, this doesn’t apply to a wide
    range of domains. Many corporations do not need to maintain such large datasets,
    and in some cases, large portions of static data—like those in hard sciences—can
    remain stable for a long time. Such static data can be fine-tuned to reduce the
    volume of RAG data required.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 Google、Microsoft、Amazon 和其他网络巨头可能需要指数级的数据和资源。某些领域，如美国的法律判决，可能确实需要大量数据。然而，这并不适用于广泛的领域。许多公司不需要维护如此大的数据集，在某些情况下，大量静态数据——如硬科学中的数据——可以长时间保持稳定。此类静态数据可以微调以减少所需的
    RAG 数据量。
- en: In this chapter, therefore, we will first examine the architecture of RAG data
    reduction through fine-tuning. We will focus on a dataset that contains ready-to-use
    documents but also stresses the human-feedback factor. We will demonstrate how
    to transform non-parametric data into parametric, fine-tuned data in an OpenAI
    model. Then, we will download and prepare the dataset from the previous chapter,
    converting the data into well-formatted prompt and completion pairs for fine-tuning
    in JSONL. We will fine-tune a cost-effective OpenAI model, `GPT-4o-mini`, which
    will prove sufficient for the completion task we will implement. Once the model
    is fine-tuned, we will test it on our dataset to verify that it has successfully
    taken our data into account. Finally, we will explore OpenAI’s metrics interface,
    which enables us to monitor our technical metrics, such as accuracy and usage
    metrics, to assess the cost-effectiveness of our approach.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们将首先检查通过微调实现的 RAG 数据减少的架构。我们将关注包含现成文档的数据集，同时也强调人类反馈因素。我们将展示如何在 OpenAI
    模型中将非参数数据转换为参数化、微调数据。然后，我们将从上一章下载并准备数据集，将数据转换为格式良好的提示和完成对，以便在 JSONL 中进行微调。我们将微调一个成本效益高的
    OpenAI 模型，`GPT-4o-mini`，这将为我们将要实施的完成任务证明是足够的。一旦模型微调完成，我们将在我们的数据集上对其进行测试，以验证它是否成功考虑了我们的数据。最后，我们将探索
    OpenAI 的指标界面，它使我们能够监控我们的技术指标，如准确性和使用指标，以评估我们方法的成本效益。
- en: 'To sum up, this chapter covers the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本章涵盖了以下主题：
- en: The limits of managing RAG data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理RAG数据的限制
- en: The challenge of determining what data to fine-tune
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定要微调哪些数据的挑战
- en: Preparing a JSON dataset for fine-tuning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备 JSON 数据集以进行微调
- en: Running OpenAI’s processing tool to produce a JSONL dataset
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行 OpenAI 的处理工具以生成 JSONL 数据集
- en: Fine-tuning an OpenAI model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调 OpenAI 模型
- en: Managing the fine-tuning processing time
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理微调处理时间
- en: Running the fine-tuned model
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行微调后的模型
- en: Let’s begin by defining the architecture of the fine-tuning process.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先定义微调过程的架构。
- en: The architecture of fine-tuning static RAG data
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调静态 RAG 数据的架构
- en: 'In this section, we question the usage of non-parametric RAG data when it exceeds
    a manageable threshold, as described in the *RAG versus fine-tuning* section in
    *Chapter 1*, *Why Retrieval Augmented Generation?*, which stated the principle
    of a threshold. *Figure 9.1* adapts the principle to this section:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将质疑当非参数RAG数据超过可管理阈值时的使用，正如在*第一章*的*RAG与微调*部分中描述的，该部分提出了阈值原则。*图9.1*将这一原则应用于本节：
- en: '![Diagram of a diagram of a heater  Description automatically generated](img/B31169_09_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![一个加热器图表的图表 描述自动生成](img/B31169_09_01.png)'
- en: 'Figure 9.1: Fine-tuning threshold reached for RAG data'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：RAG数据的微调阈值已达到
- en: 'Notice that the processing (**D2**) and storage (**D3**) thresholds have been
    reached for static data versus the dynamic data in the RAG data environment. The
    threshold depends on each project and parameters such as:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于RAG数据环境中的静态数据与动态数据，处理（**D2**）和存储（**D3**）阈值已经达到。阈值取决于每个项目和诸如以下参数之类的参数：
- en: '**The volume of RAG data to process**: Embedding data requires human and machine
    resources. Even if we don’t embed the data, piling up static data (data that is
    stable over a long period of time) makes no sense.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理RAG数据的量**：嵌入数据需要人类和机器资源。即使我们不嵌入数据，堆积静态数据（在长时间内稳定的数据）也没有意义。'
- en: '**The volume of RAG data to store and retrieve**: At some point, if we keep
    stacking data up, much of it may overlap.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储和检索RAG数据的量**：在某个时刻，如果我们继续堆积数据，其中很大一部分可能会重叠。'
- en: '**The retrievals require resources**: Even if the system is open source, there
    is still an increasing number of resources to manage.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索需要资源**：即使系统是开源的，仍然需要管理越来越多的资源。'
- en: Other factors, too, may come into play for each project. Whatever the reason,
    fine-tuning can be a good solution when we reach the RAG data threshold.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其他因素也可能在每个项目中发挥作用。无论原因如何，当我们达到RAG数据阈值时，微调都可以是一个好的解决方案。
- en: The RAG ecosystem
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG生态系统
- en: 'In this section, we will return to the RAG ecosystem described in *Chapter
    1*. We will focus on the specific components we need for this chapter. The following
    figure presents the fine-tuning components in color and the ones we will not need
    in gray:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回到*第一章*中描述的RAG生态系统。我们将专注于本章所需的特定组件。以下图表以颜色展示了微调组件，以及我们不需要的组件以灰色表示：
- en: '![A diagram of a diagram  Description automatically generated](img/B31169_09_02.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![一个图表的图表 描述自动生成](img/B31169_09_02.png)'
- en: 'Figure 9.2: Fine-tuning components of the RAG ecosystem'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：RAG生态系统的微调组件
- en: 'The key features of the fine-tuning ecosystems we will build can be summarized
    in the following points:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要构建的微调生态系统的关键特性可以总结如下几点：
- en: '**Collecting (D1) and preparing (D2) the dataset**: We will download and process
    the human-crafted crowdsourced SciQ hard science dataset we implemented in the
    previous chapter: [https://huggingface.co/datasets/sciq](https://huggingface.co/datasets/sciq).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收集（D1）和准备（D2）数据集**：我们将下载并处理我们在上一章中实现的人类定制众包SciQ硬科学数据集：[https://huggingface.co/datasets/sciq](https://huggingface.co/datasets/sciq)。'
- en: '**Human feedback (E2)**: We can assume that human feedback played an important
    role in the SciQ hard science dataset. The dataset was controlled by humans and
    updated so we can think of it as a simulation of how reliable human feedback can
    be fine-tuned to alleviate the volume of RAG datasets. We can go further and say
    it is possible that, in real-life projects, the explanations present in the SciQ
    dataset can sometimes come from human evaluations of models, as we explored in
    *Chapter 5*, *Boosting RAG Performance with Expert Human Feedback*.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人类反馈（E2）**：我们可以假设人类反馈在SciQ硬科学数据集中发挥了重要作用。数据集由人类控制并更新，因此我们可以将其视为一种模拟，即可靠的人类反馈如何被微调以减轻RAG数据集的量。我们可以更进一步地说，在现实生活中的项目中，SciQ数据集中存在的解释有时可能来自对模型的评估，正如我们在*第五章*，*通过专家人类反馈提升RAG性能*中探讨的那样。'
- en: '**Fine-tuning (T2)**: We will fine-tune a cost-effective OpenAI model, `GPT-4o-mini`.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调（T2）**：我们将微调一个经济高效的OpenAI模型，`GPT-4o-mini`。'
- en: '**Prompt engineering (G3) and generation and output (G4)**: We will engineer
    the prompts as recommended by OpenAI and display the output.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示工程（G3）和生成及输出（G4）**：我们将根据OpenAI的建议进行提示工程，并展示输出。'
- en: '**Metrics (E1)**: We will look at the main features of OpenAI’s Metrics interface.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指标（E1）**：我们将查看OpenAI指标界面的主要特性。'
- en: Let’s now go to our keyboards to collect and process the SciQ dataset.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在去键盘上收集和处理SciQ数据集。
- en: Installing the environment
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装环境
- en: Installing an environment has become complex with the rapid evolution of AI
    and cross-platform dependency conflicts, as we saw in *Chapter 2*, *RAG Embedding
    Vector Stores with Deep Lake and OpenAI*, in the *Setting up the environment*
    section. We will thus freeze the package versions when possible.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 随着AI的快速发展和跨平台依赖冲突，安装环境变得复杂，正如我们在第2章的*使用Deep Lake和OpenAI的RAG嵌入向量存储*部分中看到的。因此，我们将尽可能冻结包版本。
- en: 'For this program, open the `Fine_tuning_OpenAI_GPT_4o_mini.ipynb` notebook
    in the `Chapter09` directory on GitHub. The program first retrieves the OpenAI
    API key:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个程序，请在GitHub上的`Chapter09`目录中打开`Fine_tuning_OpenAI_GPT_4o_mini.ipynb`笔记本。程序首先检索OpenAI
    API密钥：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We then install `openai` and set the API key:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然后安装`openai`并设置API密钥：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we install `jsonlines` to generate JSONL data:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们安装`jsonlines`以生成JSONL数据：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We now install `datasets`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在安装`datasets`：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Read the *Installing the environment* section of *Chapter 8*, *Dynamic RAG with
    Chroma and Hugging Face Llama*, for explanations of the dependency conflicts involved
    when installing `datasets`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读第8章的*安装环境*部分，*使用Chroma和Hugging Face Llama的动态RAG*，以了解安装`datasets`时涉及的依赖冲突解释。
- en: Some issues with the installation may occur but the dataset will be downloaded
    anyway. We must expect and accept such issues as the leading platforms continually
    update their packages and create conflicts with pre-installed environments such
    as Google Colab. You can create a special environment for this program. Bear in
    mind that your other programs might encounter issues due to other package constraints.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 安装过程中可能会出现一些问题，但数据集仍然会被下载。我们必须预料并接受这些问题，因为主要平台持续更新它们的包，并可能与预安装的环境（如Google Colab）产生冲突。您可以为这个程序创建一个特殊环境。请记住，您的其他程序可能会因为其他包约束而遇到问题。
- en: We are now ready to prepare the dataset.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已准备好准备数据集。
- en: 1\. Preparing the dataset for fine-tuning
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 准备数据集以进行微调
- en: 'Fine-tuning an OpenAI model requires careful preparation; otherwise, the fine-tuning
    job will fail. In this section, we will carry out the following steps:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 微调OpenAI模型需要仔细的准备；否则，微调任务将失败。在本节中，我们将执行以下步骤：
- en: Download the dataset from Hugging Face and prepare it by processing its columns.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Hugging Face下载数据集，并通过处理其列来准备它。
- en: Stream the dataset to a JSON file in JSONL format.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集流式传输到JSON文件，格式为JSONL。
- en: The program begins by downloading the dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 程序首先下载数据集。
- en: 1.1\. Downloading and visualizing the dataset
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1\. 下载并可视化数据集
- en: We will download the SciQ dataset we embedded in *Chapter 8*. As we saw, embedding
    thousands of documents takes time and resources. In this section, we will download
    the dataset, but this time, *we will not embed it*. We will let the OpenAI model
    handle that for us while fine-tuning the data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将下载第8章中嵌入的SciQ数据集。正如我们所见，嵌入数千个文档需要时间和资源。在本节中，我们将下载数据集，但这次，*我们不会嵌入它*。在微调数据时，我们将让OpenAI模型为我们处理。
- en: 'The program downloads the same Hugging Face dataset as in *Chapter 8* and filters
    the training portion of the dataset to include only non-empty records with the
    correct answer and support text to explain the answer to the questions:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 程序下载与第8章相同的Hugging Face数据集，并过滤数据集的训练部分，只包含包含正确答案和支持文本（解释问题的答案）的非空记录：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding code then prints the number of filtered questions with support
    text. The output shows that we have a subset of 10,481 records:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码随后打印出带有支持文本的过滤问题的数量。输出显示我们有一个包含10,481条记录的子集：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we will load the dataset to a DataFrame and drop the distractor columns
    (those with wrong answers to the questions):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将数据集加载到DataFrame中，并删除干扰列（那些包含错误答案的列）：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output displays the three columns we need:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了我们需要的三列：
- en: '![A screenshot of a computer  Description automatically generated](img/B31169_09_03.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成的描述](img/B31169_09_03.png)'
- en: 'Figure 9.3: Output displaying three columns'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：显示三列的输出
- en: We need the question that will become the prompt. The `correct_answer` and `support`
    columns will be used for the completion. Now that we have examined the dataset,
    we can stream the dataset directly to a JSON file.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将成为提示的问题。将使用`correct_answer`和`support`列来完成。现在我们已经检查了数据集，我们可以直接将数据集流式传输到JSON文件。
- en: 1.2\. Preparing the dataset for fine-tuning
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2\. 准备数据集以进行微调
- en: To train the completion model we will use, we need to write a JSON file in the
    very precise JSONL format as required.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练我们将使用的完成模型，我们需要编写一个符合要求的非常精确的JSONL格式的JSON文件。
- en: We download and process the dataset in the same way as we did to visualize it
    in the *1.1\. Downloading and visualizing the dataset* section, which is recommended
    to check the dataset before fine-tuning it.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以与在*1.1. 下载和可视化数据集*部分中可视化的相同方式下载并处理数据集，建议在微调之前检查数据集。
- en: 'We now write the messages for GPT-4o-mini in JSONL:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将GPT-4o-mini的消息写入JSONL格式：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We first define the detailed answer (`detailed_answer`) with the correct answer
    (`'correct_answer'`) and a supporting (`support`) explanation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义详细的答案（`detailed_answer`），包括正确的答案（`'correct_answer'`）和辅助的（`support`）解释。
- en: 'Then we define the messages (`messages`) for the GPT-4o-mini model:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义GPT-4o-mini模型的`messages`：
- en: '`{"role": "system", "content": ...}`: This sets the initial instruction for
    the language model, telling it to provide detailed answers to science questions.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`{"role": "system", "content": ...}`：这设置了语言模型的初始指令，告诉它为科学问题提供详细的答案。'
- en: '`{"role": "user", "content": row[''question'']}`: This represents the user
    asking a question, taken from the `question` column of the DataFrame.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`{"role": "user", "content": row[''question'']}`：这代表用户提出问题，取自DataFrame的`question`列。'
- en: '`{"role": "assistant", "content": detailed_answer}`: This represents the assistant’s
    response, providing the detailed answer constructed earlier.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`{"role": "assistant", "content": detailed_answer}`：这代表助手响应，提供之前构建的详细答案。'
- en: 'We can now write our JSONL dataset to a file:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将我们的JSONL数据集写入文件：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We have given the OpenAI model a structure it expects and has been trained
    to understand. We can load the JSON file we just created in a pandas DataFrame
    to verify its content:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为OpenAI模型提供了一个它期望的结构，并且已经训练过以理解它。我们可以将我们刚刚创建的JSON文件加载到pandas DataFrame中以验证其内容：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following excerpt of the file shows that we have successfully prepared
    the JSON file:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 文件的部分摘录显示我们已经成功准备了JSON文件：
- en: '![](img/B31169_09_04.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31169_09_04.png)'
- en: 'Figure 9.4: File excerpt'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：文件摘录
- en: That’s it! We are now ready to run a fine-tuning job.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是全部！我们现在可以运行微调作业了。
- en: 2\. Fine-tuning the model
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 微调模型
- en: 'To train the model, we retrieve our training file and create a fine-tuning
    job. We begin by creating an OpenAI client:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练模型，我们检索我们的训练文件并创建一个微调作业。我们首先创建一个OpenAI客户端：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then we use the file we generated to create another training file that is uploaded
    to OpenAI:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用我们生成的文件创建另一个训练文件，并将其上传到OpenAI：
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We print the file information for the dataset we are going to use for fine-tuning:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将打印出用于微调的数据集的文件信息：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We now create and display the fine-tuning job:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在创建并显示微调作业：
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output first provides the name of the file, its purpose, its status, and
    the OpenAI name of the file ID:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 输出首先提供文件的名称、目的、状态和OpenAI文件ID的名称：
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The code displays the details of the fine-tuning job:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 代码显示了微调作业的详细信息：
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output provides the details we need to monitor the job. Here is a brief
    description of some of the key-value pairs in the output:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 输出提供了我们需要监控作业的详细信息。以下是输出中一些关键值对的简要描述：
- en: '`Job ID`: `ftjob-O1OEE7eEyFNJsO2Eu5otzWA8`.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`作业ID`: `ftjob-O1OEE7eEyFNJsO2Eu5otzWA8`。'
- en: '`Status`: `validating_files`. This means OpenAI is currently checking the training
    file to make sure it’s suitable for fine-tuning.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`状态`: `validating_files`。这意味着OpenAI目前正在检查训练文件，以确保它适合微调。'
- en: '`Model`: `gpt-4o-mini-2024-07-18`. We’re using a smaller, more cost-effective
    version of GPT-4 for fine-tuning.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`模型`: `gpt-4o-mini-2024-07-18`。我们正在使用一个更小、成本效益更高的GPT-4版本进行微调。'
- en: '`Training File`: `file-EUPGmm1yAd3axrQ0pyoeAKuE`. This is the file we’ve provided
    that contains the examples to teach the model.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`训练文件`: `file-EUPGmm1yAd3axrQ0pyoeAKuE`。这是我们提供的包含示例以教授模型的文件。'
- en: 'Some key hyperparameters are:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一些关键的超参数包括：
- en: '`n_epochs`: `''auto''`: OpenAI will automatically determine the best number
    of training cycles.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_epochs`: `''auto''`：OpenAI将自动确定最佳的训练周期数。'
- en: '`batch_size`: `''auto''`: OpenAI will automatically choose the optimal batch
    size for training.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`: `''auto''`：OpenAI将自动选择训练的最佳批量大小。'
- en: '`learning_rate_multiplier`: `''auto''`: OpenAI will automatically adjust the
    learning rate during training.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate_multiplier`: `''auto''`：OpenAI将在训练过程中自动调整学习率。'
- en: '`Created` `at`: `2024-06-30 08:20:50`.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`创建于`: `2024-06-30 08:20:50`。'
- en: This information will prove useful if you wish to perform an in-depth study
    of fine-tuning OpenAI models. We can also use it to monitor and manage our fine-tuning
    process.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想深入研究微调 OpenAI 模型，这些信息将非常有用。我们还可以用它来监控和管理我们的微调过程。
- en: 2.1\. Monitoring the fine-tunes
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1. 监控微调
- en: 'In this section, we will extract the minimum information we need to monitor
    the jobs for all our fine-tunes. We will first query OpenAI to obtain the three
    latest fine-tuning jobs:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将提取监控所有微调作业所需的最基本信息。我们首先查询 OpenAI 以获取最新的三个微调作业：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We then initialize the lists of information we want to visualize:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们初始化我们想要可视化的信息列表：
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Following that, we iterate through `response` to retrieve the information we
    need:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，我们遍历 `response` 来检索所需的信息：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We now create a DataFrame with the information we extracted:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在创建一个包含我们提取信息的 DataFrame：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, we convert the timestamps to readable format and display the list
    of fine-tunes and their status:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将时间戳转换为可读格式并显示微调列表及其状态：
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output provides a monitoring dashboard of the list of our jobs, as shown
    in *Figure 9.5*:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 输出提供了一个监控仪表板，显示了我们的作业列表，如图 *9.5* 所示：
- en: '![A screenshot of a computer  Description automatically generated](img/B31169_09_05.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图，描述自动生成](img/B31169_09_05.png)'
- en: 'Figure 9.5: Job list in the pandas DataFrame'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：pandas DataFrame 中的作业列表
- en: You can see that for job `0`, the status of the task is `running`. The status
    informs you of the different steps of the process such as validating the files,
    running, failed, or succeeded. In this case, the fine-tuning process is running.
    If you refresh this cell regularly, you will see the status.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，对于作业 `0`，任务状态为 `running`。状态会通知您过程的各个步骤，例如验证文件、运行、失败或成功。在这种情况下，微调过程正在运行。如果您定期刷新此单元格，您将看到状态。
- en: 'We will now retrieve the most recent model trained for the `Fine-Tuned Model`
    column. If the training fails, this column will be empty. If not, we can retrieve
    it:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将检索为 `Fine-Tuned Model` 列训练的最新模型。如果训练失败，此列将为空。如果不为空，我们可以检索它：
- en: '[PRE21]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output will display the name of the latest fine-tuned model if there is
    one or inform us that no fine-tuned model is found. In this case, GPT-4o-mini
    was successfully trained:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将显示如果有最新微调模型，则显示其名称，或者通知我们未找到微调模型。在这种情况下，GPT-4o-mini 已成功训练：
- en: '[PRE22]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: If a fine-tuned model is found, `generation=True`, it will trigger the OpenAI
    completion calls in the following cells. If no model is found, `generation=False`,
    it will not run the OpenAI API in the rest of the notebook to avoid using models
    that you are not training. You can set generation to `True` in a new cell and
    then select any fine-tuned model you wish.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果找到微调模型，`generation=True`，它将在以下单元格中触发 OpenAI 完成调用。如果没有找到模型，`generation=False`，则不会在笔记本的其余部分运行
    OpenAI API，以避免使用您未训练的模型。您可以在新单元格中将生成设置为 `True`，然后选择您希望使用的任何微调模型。
- en: We know that the training job can take a while. You can refresh the pandas DataFrame
    from time to time. You can write code that checks the status of another job and
    waits for a name to appear for your training job or an error message. You can
    also wait for OpenAI to send you an email informing you that the training job
    is finished. If the training job fails, we must verify our training data for any
    inconsistencies, missing values, or incorrect labels. Additionally, ensure that
    the JSON file format adheres to OpenAI’s specified schema, including correct field
    names, data types, and structure.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道训练作业可能需要一段时间。您可以不时刷新 pandas DataFrame。您可以编写代码检查另一个作业的状态，并等待训练作业出现名称或错误消息。您还可以等待
    OpenAI 发送电子邮件通知您训练作业已完成。如果训练作业失败，我们必须验证我们的训练数据是否存在任何不一致、缺失值或错误的标签。此外，确保 JSON 文件格式符合
    OpenAI 指定的模式，包括正确的字段名、数据类型和结构。
- en: Once the training job is finished, we can run completion tasks.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练作业完成，我们就可以运行完成任务。
- en: 3\. Using the fine-tuned OpenAI model
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 使用微调后的 OpenAI 模型
- en: 'We are now ready to use our fine-tuned OpenAI `GPT-4o-mini` model. We will
    begin by defining a prompt based on a question taken from our initial dataset:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好使用我们的微调 OpenAI `GPT-4o-mini` 模型。我们将从定义一个基于初始数据集中的问题的提示开始：
- en: '[PRE23]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The goal is to verify whether the dataset has been properly trained and will
    produce results similar to the completions we defined. We can now run the fine-tuned
    model:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是验证数据集是否已正确训练，并将产生与我们所定义的完成结果相似的结果。我们现在可以运行微调模型：
- en: '[PRE24]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The parameters of the request must fit our scenario:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 请求的参数必须适合我们的场景：
- en: '`model=first_non_empty_model` is our pretrained model.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model=first_non_empty_model` 是我们的预训练模型。'
- en: '`prompt=prompt` is our predefined prompt.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt=prompt` 是我们的预定义提示。'
- en: '`temperature=0.0` is set to a low value because we do not want any “creativity”
    for this hard science completion task.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temperature=0.0` 设置为低值，因为我们不希望这个硬科学完成任务有任何“创造性”。'
- en: Once we run the request, we can format and display the response. The following
    code contains two cells to display and extract the response.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行了请求，我们就可以格式化和显示响应。以下代码包含两个单元格，用于显示和提取响应。
- en: 'First, we can print the raw response:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以打印原始响应：
- en: '[PRE25]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output contains the response and information on the process:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 输出包含响应和过程信息：
- en: '[PRE26]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We then extract the text of the response:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后提取响应的文本：
- en: '[PRE27]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is a string:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是一个字符串：
- en: '[PRE28]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, we can format the response string into a nice paragraph with the Python
    wrapper:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用 Python 包装器将响应字符串格式化为一个漂亮的段落：
- en: '[PRE29]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output shows that our data has been taken into account:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示我们的数据已被考虑在内：
- en: '[PRE30]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let’s look at the initial completion for our prompt:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们提示的初始完成：
- en: '![A close up of text  Description automatically generated](img/B31169_09_06.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![文本特写  自动生成的描述](img/B31169_09_06.png)'
- en: 'Figure 9.6: Initial completion'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：初始完成
- en: The response is thus satisfactory. This might not always be the case and might
    require more work on the datasets (better data, large volumes of data, etc.) incrementally
    until you have reached a satisfactory goal.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，响应是令人满意的。这不一定总是如此，可能需要逐步对数据集（更好的数据、大量数据等）进行更多工作，直到达到令人满意的目标。
- en: 'You can save the name of your model in a text file or anywhere you wish. You
    can now run your model in another program using the name of your trained model,
    or you can reload this notebook at any time:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将您模型的名称保存在文本文件或您希望的地方。现在，您可以使用训练好的模型名称在另一个程序中运行您的模型，或者您可以在任何时候重新加载这个笔记本：
- en: Run the `Installing the environment` section of this notebook.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行这个笔记本的“安装环境”部分。
- en: Define a prompt of your choice related to the dataset we trained.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个与我们所训练的数据集相关的提示。
- en: Enter the name of your model in the OpenAI completion request.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 OpenAI 完成请求中输入您模型的名称。
- en: Run the request and analyze the response.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行请求并分析响应。
- en: 'You can consult OpenAI’s fine-tuning documentation for further information
    if necessary: [https://platform.openai.com/docs/guides/fine-tuning/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning/fine-tuning).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要更多信息，您可以查阅 OpenAI 的微调文档：[https://platform.openai.com/docs/guides/fine-tuning/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning/fine-tuning)。
- en: Metrics
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指标
- en: OpenAI provides a user interface to analyze the metrics of the training process
    and model. You can access the metrics related to your fine-tuned models at [https://platform.openai.com/finetune/](https://platform.openai.com/finetune/).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 提供用户界面来分析训练过程和模型的指标。您可以在 [https://platform.openai.com/finetune/](https://platform.openai.com/finetune/)
    访问与您微调模型相关的指标。
- en: 'The interface displays the list of your fine-tuned jobs:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 界面显示您微调作业的列表：
- en: '![A screenshot of a phone  Description automatically generated](img/B31169_09_07.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![手机截图  自动生成的描述](img/B31169_09_07.png)'
- en: 'Figure 9.7: List of fine-tuned jobs'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：微调作业列表
- en: 'You can choose to view all the fine-tuning jobs, the ones that were successful,
    or the ones that failed. If we choose a job that was successful, for example,
    we can view the job details as shown in the following excerpt:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择查看所有微调作业，包括成功的或失败的。如果我们选择一个成功的作业，例如，我们可以查看以下摘录中的作业详情：
- en: '![A screenshot of a computer  Description automatically generated](img/B31169_09_08.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图  自动生成的描述](img/B31169_09_08.png)'
- en: 'Figure 9.8: Example view'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8：示例视图
- en: 'Let’s go through the information provided in this figure:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这个图提供的信息：
- en: '**Status**: Indicates the status of the fine-tuning process. In this case,
    we can see that the process was completed successfully.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态**：指示微调过程的状态。在这种情况下，我们可以看到过程已成功完成。'
- en: '**Job ID**: A unique identifier for the fine-tuning job. This can be used to
    reference the job in queries or for support purposes.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作业 ID**：微调作业的唯一标识符。这可以用于在查询或支持目的中引用作业。'
- en: '**Base model**: Specifies the pretrained model used as the starting point for
    fine-tuning. In this case, `gpt-4o-mini` is a version of OpenAI’s models.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础模型**：指定用于微调的起始点所使用的预训练模型。在这种情况下，`gpt-4o-mini` 是 OpenAI 模型的一个版本。'
- en: '**Output model**: This is the identifier for the model resulting from the fine-tuning.
    It incorporates changes and optimizations based on the specific training data
    provided.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出模型**：这是微调后模型的标识符。它结合了基于特定训练数据的更改和优化。'
- en: '**Created at**: The date and time when the fine-tuning job was initiated.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建时间**：微调作业启动的日期和时间。'
- en: '**Trained tokens**: The total number of tokens (pieces of text, such as words
    or punctuation) that were processed during training. This metric helps gauge the
    extent of training.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练过的标记**：在训练过程中处理的标记总数（如文本片段，例如单词或标点符号）。这个指标有助于衡量训练的广度。'
- en: '**Epochs**: The number of complete passes the training data went through during
    fine-tuning. More epochs can lead to better learning but too many may lead to
    overfitting.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**周期**：在微调过程中训练数据完整遍历的次数。更多的周期可能导致更好的学习，但过多的周期可能会导致过拟合。'
- en: '**Batch size**: The number of training examples utilized in one iteration of
    model training. Smaller batch sizes can offer more updates and refined learning
    but may take longer to train.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量大小**：在一次模型训练迭代中使用的训练示例数量。较小的批量大小可以提供更多更新和更精细的学习，但可能需要更长的时间来训练。'
- en: '**LR multiplier**: This refers to the learning rate multiplier, affecting how
    much the learning rate for the base model is adjusted during the fine-tuning process.
    A smaller multiplier can lead to smaller, more conservative updates to model weights.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率乘数**：这指的是学习率乘数，它影响在微调过程中如何调整基础模型的学习率。较小的乘数可能导致模型权重更新更小、更保守。'
- en: '**Seed**: A seed for the random number generator used in the training process.
    Providing a seed ensures that the training process is reproducible, meaning you
    can get the same results with the same input conditions.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**种子**：训练过程中使用的随机数生成器的种子。提供种子确保训练过程是可重复的，这意味着你可以用相同的输入条件得到相同的结果。'
- en: 'This information will help tailor the fine-tuning jobs to meet the specific
    needs of a project and explore alternative approaches to optimization and customization.
    In addition, the interface contains more information that we can explore to get
    an in-depth vision of the fine-tuning process. If we scroll down on the **Information**
    tab of our model, we can see metrics as shown here:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息将帮助定制微调作业以满足项目的特定需求，并探索优化和定制的替代方法。此外，界面中还包含更多我们可以探索的信息，以获得微调过程的深入视角。如果我们滚动到模型**信息**标签页，我们可以看到如下所示的指标：
- en: '![A green line graph with numbers  Description automatically generated](img/B31169_09_09-01.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![一条带有数字的绿色线形图描述自动生成](img/B31169_09_09-01.png)'
- en: 'Figure 9.9: Metrics for a fine-tuned model'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9：微调模型的指标
- en: Training loss and the other available information can guide our training strategies
    (data, files, and parameters).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 训练损失和其他可用信息可以指导我们的训练策略（数据、文件和参数）。
- en: '**Training loss** is a reliable metric used to evaluate the performance of
    a machine learning model during training. In this case, `Training loss (1.1570)`
    represents the model’s average error on the training dataset. Lower training loss
    values indicate that the model is better fitting the training data. A training
    loss of `1.1570` suggests that the model has learned to predict or classify its
    training data well during the fine-tuning process.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练损失**是评估机器学习模型在训练期间性能的一个可靠指标。在这种情况下，`训练损失（1.1570）`表示模型在训练数据集上的平均误差。较低的训练损失值表明模型更好地拟合了训练数据。训练损失为`1.1570`表明，在微调过程中，模型已经学会了很好地预测或分类其训练数据。'
- en: 'We can also examine these values with the `Time` and `Step` information:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`时间`和`步骤`信息来检查这些值：
- en: '![A screenshot of a phone  Description automatically generated](img/B31169_09_10.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![手机屏幕截图描述自动生成](img/B31169_09_10.png)'
- en: 'Figure 9.10: Training loss during the training job'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10：训练作业中的训练损失
- en: We must also measure the usage to monitor the cost per period and model. OpenAI
    provides a detailed interface at [https://platform.openai.com/usage](https://platform.openai.com/usage).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须衡量使用情况以监控每期和模型的成本。OpenAI在[https://platform.openai.com/usage](https://platform.openai.com/usage)提供了一个详细的界面。
- en: Fine-tuning can indeed be an effective way to optimize RAG data if we make sure
    to train a model with high-quality data and the right parameters. Now, it’s time
    for us to summarize our journey and move to our next RAG-driven generative AI
    implementation.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 精调确实可以是一种有效的优化RAG数据的方法，如果我们确保使用高质量的数据和正确的参数来训练模型。现在，是我们总结我们的旅程并转向下一个RAG驱动的生成式AI实现的时候了。
- en: Summary
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter’s goal was to show that as we accumulate RAG data, some data is
    dynamic and requires constant updates, and as such, cannot be fine-tuned easily.
    However, some data is static, meaning that it will remain stable for long periods
    of time. This data can become parametric (stored in the weights of a trained LLM).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是表明，随着我们积累RAG数据，一些数据是动态的，需要不断更新，因此无法轻松微调。然而，一些数据是静态的，这意味着它将在很长时间内保持稳定。这种数据可以成为参数化的（存储在训练好的LLM的权重中）。
- en: We first downloaded and processed the SciQ dataset, which contains hard science
    questions. This stable data perfectly suits fine-tuning. It contains a question,
    answer, and support (explanation) structure, which makes the data effective for
    fine-tuning. Also, we can assume human feedback was required. We can even go as
    far as imagining this feedback could be provided by analyzing generative AI model
    outputs.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先下载并处理了包含硬科学问题的SciQ数据集。这些稳定的数据非常适合微调。它包含问题、答案和支持（解释）结构，这使得数据对微调有效。此外，我们可以假设需要人类反馈。我们甚至可以进一步想象这种反馈可以通过分析生成式AI模型的输出提供。
- en: We converted the data we prepared into prompts and completions in a JSONL file
    following the recommendations of OpenAI’s preparation tool. The structure of JSONL
    was meant to be compatible with a completion model (prompt and completion) such
    as `GPT-4o-mini`. The program then fine-tuned the cost-effective `GPT-4o-mini`
    OpenAI model, following which we ran the model and found that the output was satisfactory.
    Finally, we explored the metrics of the fine-tuned model in the OpenAI metrics
    user interface.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据OpenAI准备工具的建议，将准备好的数据转换为JSONL文件中的提示和完成内容。JSONL的结构旨在与`GPT-4o-mini`等完成模型（提示和完成）兼容。然后程序微调了成本效益高的`GPT-4o-mini`
    OpenAI模型，随后我们运行了模型，发现输出令人满意。最后，我们在OpenAI指标用户界面中探索了微调模型的指标。
- en: We can conclude that fine-tuning can optimize RAG data in certain cases when
    necessary. However, we will take this process further in the next chapter, *Chapter
    10*, *RAG for Video Stock Production with Pinecone and OpenAI*, when we run the
    full-blown RAG-driven generative AI ecosystem.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以得出结论，在某些情况下，当需要时，微调可以优化RAG数据。然而，我们将在下一章，*第10章*，*使用Pinecone和OpenAI的视频股票生产RAG*中进一步探讨这个过程，届时我们将运行完整的RAG驱动的生成式AI生态系统。
- en: Questions
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Answer the following questions with yes or no:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 用是或否回答以下问题：
- en: Do all organizations need to manage large volumes of RAG data?
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有组织都需要管理大量RAG数据吗？
- en: Is the GPT-4o-mini model described as insufficient for fine-tuning tasks?
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPT-4o-mini模型描述为不足以进行微调任务吗？
- en: Can pretrained models update their knowledge base after the cutoff date without
    retrieval systems?
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预训练模型在截止日期后没有检索系统的情况下，能否更新其知识库？
- en: Is it the case that static data never changes and thus never requires updates?
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 静态数据从不改变，因此从不需要更新，是这样吗？
- en: Is downloading data from Hugging Face the only source for preparing datasets?
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Hugging Face下载数据是准备数据集的唯一来源吗？
- en: Is all RAG data eventually embedded into the trained model’s parameters according
    to the document?
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据文档，所有RAG数据最终都会嵌入到训练模型的参数中吗？
- en: Does the chapter recommend using only new data for fine-tuning AI models?
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这章建议仅使用新数据对AI模型进行微调吗？
- en: Is the OpenAI Metrics interface used to adjust the learning rate during model
    training?
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenAI指标界面是否用于在模型训练期间调整学习率？
- en: Can the fine-tuning process be effectively monitored using the OpenAI dashboard?
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是否可以使用OpenAI仪表板有效地监控微调过程？
- en: Is human feedback deemed unnecessary in the preparation of hard science datasets
    such as SciQ?
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在准备像SciQ这样的硬科学数据集时，认为人类反馈是不必要的吗？
- en: References
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'OpenAI fine-tuning documentation: [https://platform.openai.com/docs/guides/fine-tuning/](https://platform.openai.com/docs/guides/fine-tuning/)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI微调文档：[https://platform.openai.com/docs/guides/fine-tuning/](https://platform.openai.com/docs/guides/fine-tuning/)
- en: 'OpenAI pricing: [https://openai.com/api/pricing/](https://openai.com/api/pricing/)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI定价：[https://openai.com/api/pricing/](https://openai.com/api/pricing/)
- en: Further reading
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Test of Fine-Tuning GPT by Astrophysical Data* by Yu Wang et al. is an interesting
    article on fine-tuning hard science data, which requires careful data preparation:
    [https://arxiv.org/pdf/2404.10019](https://arxiv.org/pdf/2404.10019)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由王宇等人撰写的*通过天体物理数据微调GPT的测试*是一篇关于微调硬科学数据的有趣文章，它需要仔细的数据准备：[https://arxiv.org/pdf/2404.10019](https://arxiv.org/pdf/2404.10019)
- en: Join our community on Discord
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://www.packt.link/rag](https://www.packt.link/rag)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.packt.link/rag](https://www.packt.link/rag)'
- en: '![](img/QR_Code50409000288080484.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/QR_Code50409000288080484.png)'
