- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: First Steps with LangChain
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LangChain 的第一步
- en: In the previous chapter, we explored LLMs and introduced LangChain as a powerful
    framework for building LLM-powered applications. We discussed how LLMs have revolutionized
    natural language processing with their ability to understand context, generate
    human-like text, and perform complex reasoning. While these capabilities are impressive,
    we also examined their limitations—hallucinations, context constraints, and lack
    of up-to-date knowledge.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了大型语言模型（LLMs）并介绍了 LangChain 作为构建 LLM 驱动的应用程序的强大框架。我们讨论了 LLMs 如何通过理解上下文、生成类似人类的文本和执行复杂推理的能力而彻底改变了自然语言处理。虽然这些功能令人印象深刻，但我们还考察了它们的局限性——幻觉、上下文限制和缺乏最新知识。
- en: 'In this chapter, we’ll move from theory to practice by building our first LangChain
    application. We’ll start with the fundamentals: setting up a proper development
    environment, understanding LangChain’s core components, and creating simple chains.
    From there, we’ll explore more advanced capabilities, including running local
    models for privacy and cost efficiency and building multimodal applications that
    combine text with visual understanding. By the end of this chapter, you’ll have
    a solid foundation in LangChain’s building blocks and be ready to create increasingly
    sophisticated AI applications in subsequent chapters.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过构建我们的第一个 LangChain 应用程序，从理论转向实践。我们将从基础开始：设置合适的发展环境，理解 LangChain 的核心组件，并创建简单的链。从那里，我们将探索更高级的功能，包括为了隐私和成本效益运行本地模型，以及构建结合文本和视觉理解的跨模态应用程序。到本章结束时，你将拥有
    LangChain 构建块的良好基础，并准备好在后续章节中创建越来越复杂的 AI 应用程序。
- en: 'To sum up, this chapter will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本章将涵盖以下主题：
- en: Setting up dependencies
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置依赖项
- en: Exploring LangChain’s building blocks (model interfaces, prompts and templates,
    and LCEL)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索 LangChain 的构建块（模型接口、提示和模板以及 LCEL）
- en: Running local models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行本地模型
- en: Multimodal AI applications
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨模态 AI 应用程序
- en: 'Given the rapid evolution of both LangChain and the broader AI field, we maintain
    up-to-date code examples and resources in our GitHub repository: [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 LangChain 和更广泛的 AI 领域都在快速发展，我们在 GitHub 仓库中维护了最新的代码示例和资源：[https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)。
- en: 'For questions or troubleshooting help, please create an issue on GitHub or
    join our Discord community: [https://packt.link/lang](https://packt.link/lang).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如有疑问或需要故障排除帮助，请在 GitHub 上创建一个问题或加入我们的 Discord 社区：[https://packt.link/lang](https://packt.link/lang)。
- en: Setting up dependencies for this book
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为本书设置依赖项
- en: This book provides multiple options for running the code examples, from zero-setup
    cloud notebooks to local development environments. Choose the approach that best
    fits your experience level and preferences. Even if you are familiar with dependency
    management, please read these instructions since all code in this book will depend
    on the correct installation of the environment as outlined here.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本书提供了多种运行代码示例的选项，从零配置的云笔记本到本地开发环境。选择最适合你经验和偏好的方法。即使你熟悉依赖项管理，也请阅读这些说明，因为本书中的所有代码都将依赖于此处概述的正确环境安装。
- en: 'For the quickest start with no local setup required, we provide ready-to-use
    online notebooks for every chapter:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不需要本地设置，我们为每一章提供现成的在线笔记本：
- en: '**Google Colab**: Run examples with free GPU access'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Colab**：使用免费 GPU 访问运行示例'
- en: '**Kaggle Notebooks**: Experiment with integrated datasets'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kaggle 笔记本**：在集成数据集上进行实验'
- en: '**Gradient Notebooks**: Access higher-performance compute options'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度笔记本**：访问高性能计算选项'
- en: All code examples you find in this book are available as online notebooks on
    GitHub at [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这本书中找到的所有代码示例都可以在 GitHub 上以在线笔记本的形式找到，网址为 [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)。
- en: 'These notebooks don’t have all dependencies pre-configured but, usually, a
    few install commands get you going. These tools allow you to start experimenting
    immediately without worrying about setup. If you prefer working locally, we recommend
    using conda for environment management:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些笔记本没有预先配置所有依赖项，但通常只需要几个安装命令就可以开始。这些工具允许你立即开始实验，无需担心设置。如果你更喜欢在本地工作，我们建议使用 conda
    进行环境管理：
- en: Install Miniconda if you don’t have it already.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你还没有安装 Miniconda，请先安装。
- en: Download it from [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html).
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html)下载它。
- en: 'Create a new environment with Python 3.11:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Python 3.11创建一个新的环境：
- en: '[PRE0]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Activate the environment:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活环境：
- en: '[PRE1]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Install Jupyter and core dependencies:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装Jupyter和核心依赖项：
- en: '[PRE2]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Launch Jupyter Notebook:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Jupyter Notebook：
- en: '[PRE3]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This approach provides a clean, isolated environment for working with LangChain.
    For experienced developers with established workflows, we also support:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法为使用LangChain提供了一个干净、隔离的工作环境。对于有固定工作流程的资深开发者，我们还支持：
- en: '**pip with venv**: Instructions in the GitHub repository'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pip with venv**：GitHub仓库中的说明'
- en: '**Docker containers**: Dockerfiles provided in the GitHub repository'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Docker容器**：GitHub仓库中提供的Dockerfile'
- en: '**Poetry**: Configuration files available in the GitHub repository'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Poetry**：GitHub仓库中可用的配置文件'
- en: Choose the method you’re most comfortable with but remember that all examples
    assume a Python 3.10+ environment with the dependencies listed in requirements.txt.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 选择你最舒适的方法，但请记住，所有示例都假设有一个Python 3.10+环境，并具有requirements.txt中列出的依赖项。
- en: For developers, Docker, which provides isolation via containers, is a good option.
    The downside is that it uses a lot of disk space and is more complex than the
    other options. For data scientists, I’d recommend Conda or Poetry.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于开发者来说，Docker，通过容器提供隔离，是一个不错的选择。缺点是它占用大量磁盘空间，并且比其他选项更复杂。对于数据科学家，我推荐使用Conda或Poetry。
- en: Conda handles intricate dependencies efficiently, although it can be excruciatingly
    slow in large environments. Poetry resolves dependencies well and manages environments;
    however, it doesn’t capture system dependencies.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Conda在处理复杂依赖项方面效率很高，尽管在大环境中可能会非常慢。Poetry很好地解决依赖项并管理环境；然而，它不捕获系统依赖项。
- en: All tools allow sharing and replicating dependencies from configuration files.
    You can find a set of instructions and the corresponding configuration files in
    the book’s repository at [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所有工具都允许从配置文件中共享和复制依赖项。你可以在书的GitHub仓库[https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)中找到一组说明和相应的配置文件。
- en: Once you are finished, please make sure you have LangChain version 0.3.17 installed.
    You can check this with the command `pip show langchain`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，请确保你已经安装了LangChain版本0.3.17。你可以使用命令`pip show langchain`来检查。
- en: 'With the rapid pace of innovation in the LLM field, library updates are frequent.
    The code in this book is tested with LangChain 0.3.17, but newer versions may
    introduce changes. If you encounter any issues running the examples:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM领域的创新步伐加快，库的更新很频繁。本书中的代码是用LangChain 0.3.17测试的，但新版本可能会引入变化。如果你在运行示例时遇到任何问题：
- en: Create an issue on our GitHub repository
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的GitHub仓库创建一个问题
- en: Join the discussion on Discord at [https://packt.link/lang](https://packt.link/lang
    )
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[https://packt.link/lang](https://packt.link/lang)上的Discord加入讨论
- en: Check the errata on the book’s Packt page
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在书的Packt页面上检查勘误表
- en: This community support ensures you’ll be able to successfully implement all
    projects regardless of library updates.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这种社区支持确保你能够成功实施所有项目，无论库的更新如何。
- en: API key setup
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: API密钥设置
- en: LangChain’s provider-agnostic approach supports a wide range of LLM providers,
    each with unique strengths and characteristics. Unless you use a local LLM, to
    use these services, you’ll need to obtain the appropriate authentication credentials.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain的无提供商方法支持广泛的LLM提供商，每个都有其独特的优势和特点。除非你使用本地LLM，否则要使用这些服务，你需要获得适当的认证凭据。
- en: '| **Provider** | **Environment Variable** | **Setup URL** | **Free Tier?**
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| **提供商** | **环境变量** | **设置URL** | **免费层** |'
- en: '| OpenAI | `OPENAI_API_KEY` | [platform.openai.com](https://platform.openai.com/)
    | No |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI | `OPENAI_API_KEY` | [platform.openai.com](https://platform.openai.com/)
    | 否 |'
- en: '| HuggingFace | `HUGGINGFACEHUB_API_TOKEN` | [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
    | Yes |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| HuggingFace | `HUGGINGFACEHUB_API_TOKEN` | [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
    | 是 |'
- en: '| Anthropic | `ANTHROPIC_API_KEY` | [console.anthropic.com](https://console.anthropic.com)
    | No |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Anthropic | `ANTHROPIC_API_KEY` | [console.anthropic.com](https://console.anthropic.com)
    | 否 |'
- en: '| Google AI | `GOOGLE_API_KEY` | [ai.google.dev/gemini-api](https://ai.google.dev/gemini-api)
    | Yes |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Google AI | `GOOGLE_API_KEY` | [ai.google.dev/gemini-api](https://ai.google.dev/gemini-api)
    | 是 |'
- en: '| Google VertexAI | `Application Default Credentials` | [cloud.google.com/vertex-ai](https://cloud.google.com/vertex-ai)
    | Yes (with limits) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Google VertexAI | `应用程序默认凭证` | [cloud.google.com/vertex-ai](https://cloud.google.com/vertex-ai)
    | 是（有限制）|'
- en: '| Replicate | `REPLICATE_API_TOKEN` | [replicate.com](https://replicate.com)
    | No |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Replicate | `REPLICATE_API_TOKEN` | [replicate.com](https://replicate.com)
    | 否 |'
- en: 'Table 2.1: API keys reference table (overview)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.1：API 密钥参考表（概述）
- en: Most providers require an API key, while cloud providers like AWS and Google
    Cloud also support alternative authentication methods like **Application Default
    Credentials** (**ADC**). Many providers offer free tiers without requiring credit
    card details, making it easy to get started.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数提供商需要 API 密钥，而像 AWS 和 Google Cloud 这样的云提供商也支持其他身份验证方法，如 **应用程序默认凭证**（**ADC**）。许多提供商提供免费层，无需信用卡详细信息，这使得入门变得容易。
- en: 'To set an API key in an environment, in Python, we can execute the following
    lines:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要在环境中设置 API 密钥，在 Python 中，我们可以执行以下行：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, `OPENAI_API_KEY` is the environment key that is appropriate for OpenAI.
    Setting the keys in your environment has the advantage of not needing to include
    them as parameters in your code every time you use a model or service integration.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`OPENAI_API_KEY` 是适用于 OpenAI 的环境密钥。在您的环境中设置密钥的优点是，每次使用模型或服务集成时，无需将它们作为参数包含在您的代码中。
- en: 'You can also expose these variables in your system environment from your terminal.
    In Linux and macOS, you can set a system environment variable from the terminal
    using the `export` command:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以从终端在您的系统环境中暴露这些变量。在 Linux 和 macOS 中，您可以使用 `export` 命令从终端设置系统环境变量：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To permanently set the environment variable in Linux or macOS, you would need
    to add the preceding line to the `~/.bashrc` or `~/.bash_profile` files, and then
    reload the shell using the command `source ~/.bashrc` or `source ~/.bash_profile`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Linux 或 macOS 中永久设置环境变量，您需要将前面的行添加到 `~/.bashrc` 或 `~/.bash_profile` 文件中，然后使用命令
    `source ~/.bashrc` 或 `source ~/.bash_profile` 重新加载 shell。
- en: For Windows users, you can set the environment variable by searching for “Environment
    Variables” in the system settings, editing either “User variables” or “System
    variables,” and adding `export` `OPENAI_API_KEY=your_key_here`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Windows 用户，您可以通过在系统设置中搜索“环境变量”来设置环境变量，编辑“用户变量”或“系统变量”，并添加 `export` `OPENAI_API_KEY=your_key_here`。
- en: 'Our choice is to create a `config.py` file where all API keys are stored. We
    then import a function from this module that loads these keys into the environment
    variables. This approach centralizes credential management and makes it easier
    to update keys when needed:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的选择是创建一个 `config.py` 文件，其中存储所有 API 密钥。然后我们从该模块导入一个函数，将这些密钥加载到环境变量中。这种方法集中管理凭证，并在需要时更容易更新密钥：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If you search for this file in the GitHub repository, you’ll notice it’s missing.
    This is intentional – I’ve excluded it from Git tracking using the `.gitignore`
    file. The `.gitignore` file tells Git which files to ignore when committing changes,
    which is essential for:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在 GitHub 仓库中搜索此文件，您会注意到它缺失。这是故意的 - 我已经使用 `.gitignore` 文件将其排除在 Git 跟踪之外。`.gitignore`
    文件告诉 Git 在提交更改时要忽略哪些文件，这对于：
- en: Preventing sensitive credentials from being publicly exposed
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 防止敏感凭证被公开暴露
- en: Avoiding accidental commits of personal API keys
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 避免意外提交个人 API 密钥
- en: Protecting yourself from unauthorized usage charges
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保护自己免受未经授权的使用费用
- en: 'To implement this yourself, simply add `config.py` to your `.gitignore` file:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要自行实现此功能，只需将 `config.py` 添加到您的 `.gitignore` 文件中：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can set all your keys in the `config.py` file. This function, `set_environment()`,
    loads all the keys into the environment as mentioned. Anytime you want to run
    an application, you import the function and run it like so:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 `config.py` 文件中设置所有您的密钥。此函数 `set_environment()` 将所有密钥加载到环境变量中，如前所述。任何您想要运行应用程序的时候，您都可以导入此函数并像这样运行它：
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For production environments, consider using dedicated secrets management services
    or environment variables injected at runtime. These approaches provide additional
    security while maintaining the separation between code and credentials.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生产环境，考虑使用专用的密钥管理服务或运行时注入的环境变量。这些方法提供了额外的安全性，同时保持了代码和凭证之间的分离。
- en: While OpenAI’s models remain influential, the LLM ecosystem has rapidly diversified,
    offering developers multiple options for their applications. To maintain clarity,
    we’ll separate LLMs from the model gateways that provide access to them.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然OpenAI的模型仍然具有影响力，但LLM生态系统已经迅速多元化，为开发者提供了多种应用选项。为了保持清晰，我们将LLM与其提供访问权限的模型网关分开。
- en: '**Key LLM families**'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关键LLM家族**'
- en: '**Anthropic Claude**: Excels in reasoning, long-form content processing, and
    vision analysis with up to 200K token context windows'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Anthropic Claude**：在推理、长文本内容处理和视觉分析方面表现出色，具有高达200K个token的上下文窗口'
- en: '**Mistral models**: Powerful open-source models with strong multilingual capabilities
    and exceptional reasoning abilities'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mistral模型**：功能强大的开源模型，具有强大的多语言能力和卓越的推理能力'
- en: '**Google Gemini**: Advanced multimodal models with industry-leading 1M token
    context window and real-time information access'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Gemini**：具有行业领先的1M个token上下文窗口和实时信息访问的高级多模态模型'
- en: '**OpenAI GPT-o**: Leading omnimodal capabilities accepting text, audio, image,
    and video with enhanced reasoning'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI GPT-o**：具有领先的跨模态能力，接受文本、音频、图像和视频，并具有增强的推理能力'
- en: '**DeepSeek models:** Specialized in coding and technical reasoning with state-of-the-art
    performance on programming tasks'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DeepSeek模型**：专注于编码和技术推理，在编程任务上具有最先进的性能'
- en: '**AI21 Labs Jurassic:** Strong in academic applications and long-form content
    generation'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI21 Labs Jurassic**：在学术应用和长文本内容生成方面表现强劲'
- en: '**Inflection Pi**: Optimized for conversational AI with exceptional emotional
    intelligence'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Inflection Pi**：针对对话AI优化，具有卓越的情感智能'
- en: '**Perplexity models**: Focused on accurate, cited answers for research applications'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Perplexity模型**：专注于为研究应用提供准确、有引用的答案'
- en: '**Cohere models**: Specialized for enterprise applications with strong multilingual
    capabilities'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cohere模型**：针对企业应用，具有强大的多语言能力'
- en: '**Cloud provider gateways**'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云提供商网关**'
- en: '**Amazon Bedrock**: Unified API access to models from Anthropic, AI21, Cohere,
    Mistral, and others with AWS integration'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Bedrock**：通过AWS集成提供Anthropic、AI21、Cohere、Mistral和其他模型的一站式API访问'
- en: '**Azure OpenAI Service**: Enterprise-grade access to OpenAI and other models
    with robust security and Microsoft ecosystem integration'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure OpenAI服务**：提供企业级访问OpenAI和其他模型，具有强大的安全性和微软生态系统集成'
- en: '**Google Vertex AI**: Access to Gemini and other models with seamless Google
    Cloud integration'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Vertex AI**：通过无缝的Google Cloud集成访问Gemini和其他模型'
- en: '**Independent platforms**'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立平台**'
- en: '**Together AI**: Hosts 200+ open-source models with both serverless and dedicated
    GPU options'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Together AI**：托管200多个开源模型，提供无服务器和专用GPU选项'
- en: '**Replicate**: Specializes in deploying multimodal open-source models with
    pay-as-you-go pricing'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Replicate**：专注于部署按使用付费的多模态开源模型'
- en: '**HuggingFace Inference Endpoints**: Production deployment of thousands of
    open-source models with fine-tuning capabilities'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HuggingFace推理端点**：具有微调能力的数千个开源模型的量产部署'
- en: Throughout this book, we’ll work with various models accessed through different
    providers, giving you the flexibility to choose the best option for your specific
    needs and infrastructure requirements.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将与通过不同提供商访问的各种模型一起工作，为您提供选择最适合您特定需求和基础设施要求的最佳选项的灵活性。
- en: We will use OpenAI for many applications but will also try LLMs from other organizations.
    Refer to the *Appendix* at the end of the book to learn how to get API keys for
    OpenAI, Hugging Face, Google, and other providers.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用OpenAI进行许多应用，但也会尝试来自其他组织的LLM。请参考本书末尾的*附录*了解如何获取OpenAI、Hugging Face、Google和其他提供商的API密钥。
- en: 'There are two main integration packages:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个主要的集成包：
- en: '`langchain-google-vertexai`'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`langchain-google-vertexai`'
- en: '`langchain-google-genai`'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`langchain-google-genai`'
- en: We’ll be using `langchain-google-genai`, the package recommended by LangChain
    for individual developers. The setup is a lot simpler, only requiring a Google
    account and API key. It is recommended to move to `langchain-google-vertexai`
    for larger projects. This integration offers enterprise features such as customer
    encryption keys, virtual private cloud integration, and more, requiring a Google
    Cloud account with billing.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用LangChain推荐的`langchain-google-genai`包，对于个人开发者来说，设置要简单得多，只需一个Google账户和API密钥。对于更大的项目，建议迁移到`langchain-google-vertexai`。此集成提供了企业功能，如客户加密密钥、虚拟私有云集成等，需要具有计费功能的Google
    Cloud账户。
- en: If you’ve followed the instructions on GitHub, as indicated in the previous
    section, you should already have the `langchain-google-genai` package installed.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经按照上一节中指示的GitHub上的说明操作，那么你应该已经安装了`langchain-google-genai`包。
- en: Exploring LangChain’s building blocks
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索LangChain的构建块
- en: To build practical applications, we need to know how to work with different
    model providers. Let’s explore the various options available, from cloud services
    to local deployments. We’ll start with fundamental concepts like LLMs and chat
    models, then dive into prompts, chains, and memory systems.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建实际的应用程序，我们需要了解如何与不同的模型提供者合作。让我们探索从云服务到本地部署的各种选项。我们将从LLM和聊天模型等基本概念开始，然后深入到提示、链和记忆系统。
- en: Model interfaces
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型接口
- en: LangChain provides a unified interface for working with various LLM providers.
    This abstraction makes it easy to switch between different models while maintaining
    a consistent code structure. The following examples demonstrate how to implement
    LangChain’s core components in practical scenarios.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain提供了一个统一的接口来处理各种LLM提供者。这种抽象使得在保持一致代码结构的同时轻松地在不同模型之间切换变得容易。以下示例演示了如何在实际场景中实现LangChain的核心组件。
- en: Please note that users should almost exclusively be using the newer chat models
    as most model providers have adopted a chat-like interface for interacting with
    language models. We still provide the LLM interface, because it’s very easy to
    use as string-in, string-out.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，用户几乎应该只使用较新的聊天模型，因为大多数模型提供者已经采用了类似聊天的界面来与语言模型交互。我们仍然提供LLM接口，因为它作为字符串输入、字符串输出非常容易使用。
- en: LLM interaction patterns
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM交互模式
- en: The LLM interface represents traditional text completion models that take a
    string input and return a string output. More and more use cases in LangChain
    use only the ChatModel interface, mainly because it’s better suited for building
    complex workflows and developing agents. The LangChain documentation is now deprecating
    the LLM interface and recommending the use of chat-based interfaces. While this
    chapter demonstrates both interfaces, we recommend using chat models as they represent
    the current standard to be up to date with LangChain.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: LLM接口代表传统的文本完成模型，它接受字符串输入并返回字符串输出。在LangChain中越来越多的用例仅使用ChatModel接口，主要是因为它更适合构建复杂的工作流程和开发代理。LangChain文档现在正在弃用LLM接口，并推荐使用基于聊天的接口。虽然本章演示了这两个接口，但我们建议使用聊天模型，因为它们代表了LangChain的当前标准，以便保持最新。
- en: 'Let’s see the LLM interface in action:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看LLM接口的实际应用：
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Please note that you must set your environment variables to the provider keys
    when you run this. For example, when running this I’d start the file by calling
    `set_environment() from config`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当你运行此程序时，你必须设置你的环境变量为提供者的密钥。例如，当运行此程序时，我会首先通过调用`set_environment()`从`config`文件开始：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We get this output:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For the Gemini model, we can run:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Gemini模型，我们可以运行：
- en: '[PRE13]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'For me, Gemini comes up with this joke:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我来说，Gemini提出了这个笑话：
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Notice how we use the same `invoke()` method regardless of the provider. This
    consistency makes it easy to experiment with different models or switch providers
    in production.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们如何无论提供者如何都使用相同的`invoke()`方法。这种一致性使得在实验不同模型或在生产中切换提供者变得容易。
- en: Development testing
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开发测试
- en: 'During development, you might want to test your application without making
    actual API calls. LangChain provides `FakeListLLM` for this purpose:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发过程中，你可能想在不实际进行API调用的情况下测试你的应用程序。LangChain提供了`FakeListLLM`用于此目的：
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Working with chat models
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与聊天模型合作
- en: 'Chat models are LLMs that are fine-tuned for multi-turn interaction between
    a model and a human. These days most LLMs are fine-tuned for multi-turned conversations.
    Instead of providing input to the model, such as:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天模型是针对模型与人类之间多轮交互进行微调的LLM。如今，大多数LLM都是针对多轮对话进行微调的。而不是向模型提供输入，例如：
- en: '[PRE16]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: where we expect it to generate an output by continuing the conversation, these
    days model providers typically expose an API that expects each turn as a separate
    well-formatted part of the payload. Model providers typically don’t store the
    chat history server-side, they get the full history sent each time from the client
    and only format the final prompt server-side.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们期望它通过继续对话生成输出时，这些天模型提供者通常提供一个API，该API期望每个回合作为有效载荷中格式良好的独立部分。模型提供者通常不会在服务器端存储聊天历史，他们每次都从客户端接收完整的历史记录，并在服务器端仅格式化最终提示。
- en: 'LangChain follows the same pattern with ChatModels, processing conversations
    through structured messages with roles and content. Each message contains:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 与 ChatModels 采用了相同的模式，通过具有角色和内容的结构化消息处理对话。每条消息包含：
- en: Role (who’s speaking), which is defined by the message class (all messages inherit
    from BaseMessage)
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 角色（谁在说话），由消息类（所有消息都继承自 BaseMessage）定义
- en: Content (what’s being said)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容（所说的内容）
- en: 'Message types include:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 消息类型包括：
- en: '`SystemMessage`: Sets behavior and context for the model. Example:'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SystemMessage`：设置模型的行为和上下文。例如：'
- en: '[PRE17]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`HumanMessage`: Represents user input like questions, commands, and data. Example:'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HumanMessage`：表示用户输入，如问题、命令和数据。例如：'
- en: '[PRE18]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`AIMessage`: Contains model responses'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AIMessage`：包含模型响应'
- en: 'Let’s see this in action:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个动作：
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Claude comes up with a function, an explanation, and examples for calling the
    function.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 克劳德提出了一个函数、解释和调用函数的示例。
- en: 'Here’s a Python function that calculates the factorial of a given number:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个计算给定数字阶乘的 Python 函数：
- en: '[PRE21]python'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE21]python'
- en: 'def factorial(n):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 'def factorial(n):'
- en: 'if n < 0:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 'if n < 0:'
- en: raise ValueError("Factorial is not defined for negative numbers.")
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: raise ValueError("负数没有定义阶乘。")
- en: 'elif n == 0:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 'elif n == 0:'
- en: return 1
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: return 1
- en: 'else:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: result = 1
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: result = 1
- en: 'for i in range(1, n + 1):'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(1, n + 1):'
- en: result *= i
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: result *= i
- en: return result
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: return result
- en: '[PRE22]python'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE22]python'
- en: 'print(factorial(0))  # Output: 1'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 'print(factorial(0))  # 输出：1'
- en: 'print(factorial(5))  # Output: 120'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 'print(factorial(5))  # 输出：120'
- en: 'print(factorial(10))  # Output: 3628800'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 'print(factorial(10))  # 输出：3628800'
- en: 'print(factorial(-5))  # Raises ValueError: Factorial is not defined for negative
    numbers.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 'print(factorial(-5))  # 抛出 ValueError：负数没有定义阶乘。'
- en: '[PRE23]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Similarly, we could have asked an OpenAI model such as GPT-4 or GPT-4o:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们也可以询问 OpenAI 的模型，如 GPT-4 或 GPT-4o：
- en: '[PRE25]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Reasoning models
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理模型
- en: Anthropic’s Claude 3.7 Sonnet introduces a powerful capability called *extended
    thinking* that allows the model to show its reasoning process before delivering
    a final answer. This feature represents a significant advancement in how developers
    can leverage LLMs for complex reasoning tasks.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic 的 Claude 3.7 Sonnet 引入了一种名为 *扩展思考* 的强大功能，允许模型在提供最终答案之前展示其推理过程。这一功能代表了开发者如何利用
    LLMs 进行复杂推理任务的重大进步。
- en: 'Here’s how to configure extended thinking through the ChatAnthropic class:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何通过 ChatAnthropic 类配置扩展思考的：
- en: '[PRE26]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The response will include Claude’s step-by-step reasoning about algorithm selection,
    complexity analysis, and optimization considerations before presenting its final
    solution. In the preceding example:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 响应将包括克劳德关于算法选择、复杂度分析和优化考虑的逐步推理，在呈现最终解决方案之前。在先前的例子中：
- en: Out of the 64,000-token maximum response length, up to 15,000 tokens can be
    used for Claude’s thinking process.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 64,000 个令牌的最大响应长度中，最多可以使用 15,000 个令牌用于克劳德的思考过程。
- en: The remaining ~49,000 tokens are available for the final response.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剩余的 ~49,000 个令牌可用于最终响应。
- en: Claude doesn’t always use the entire thinking budget—it uses what it needs for
    the specific task. If Claude runs out of thinking tokens, it will transition to
    its final answer.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克劳德并不总是使用全部的思考预算——它只使用特定任务所需的预算。如果克劳德用完了思考令牌，它将过渡到最终答案。
- en: 'While Claude offers explicit thinking configuration, you can achieve similar
    (though not identical) results with other providers through different techniques:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 克劳德 提供了显式的思考配置，但你也可以通过不同的技术通过其他提供商获得类似（但不完全相同）的结果：
- en: '[PRE28]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `reasoning_effort` parameter streamlines your workflow by eliminating the
    need for complex reasoning prompts, allows you to adjust performance by reducing
    effort when speed matters more than detailed analysis, and helps manage token
    consumption by controlling how much processing power goes toward reasoning processes.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`reasoning_effort` 参数通过消除对复杂推理提示的需求，允许你在速度比详细分析更重要时通过减少努力来调整性能，并有助于通过控制推理过程所需的处理能力来管理令牌消耗。'
- en: DeepSeek models also offer explicit thinking configuration through the LangChain
    integration.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSeek 模型还通过 LangChain 集成提供显式的思考配置。
- en: Controlling model behavior
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制模型行为
- en: Understanding how to control an LLM’s behavior is crucial for tailoring its
    output to specific needs. Without careful parameter adjustments, the model might
    produce overly creative, inconsistent, or verbose responses that are unsuitable
    for practical applications. For instance, in customer service, you’d want consistent,
    factual answers, while in content generation, you might aim for more creative
    and promotional outputs.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 理解如何控制大型语言模型（LLM）的行为对于调整其输出以满足特定需求至关重要。如果没有仔细调整参数，模型可能会产生过于创意、不一致或冗长的响应，这些响应不适合实际应用。例如，在客户服务中，你希望得到一致、事实性的答案，而在内容生成中，你可能希望得到更多创意和促销的输出。
- en: 'LLMs offer several parameters that allow fine-grained control over generation
    behavior, though exact implementation may vary between providers. Let’s explore
    the most important ones:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs提供了一些参数，允许对生成行为进行精细控制，尽管具体的实现可能因提供商而异。让我们探讨其中最重要的几个：
- en: '| **Parameter** | **Description** | **Typical Range** | **Best For** |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| **参数** | **描述** | **典型范围** | **最佳用途** |'
- en: '| **Temperature** | Controls randomness in text generation | 0.0-1.0 (OpenAI,
    Anthropic)0.0-2.0 (Gemini) | Lower (0.0-0.3): Factual tasks, Q&AHigher (0.7+):
    Creative writing, brainstorming |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| **温度** | 控制文本生成的随机性 | 0.0-1.0（OpenAI，Anthropic）0.0-2.0（Gemini） | 较低（0.0-0.3）：事实性任务，问答
    | 较高（0.7+）：创意写作，头脑风暴 |'
- en: '| **Top-k** | Limits token selection to k most probable tokens | 1-100 | Lower
    values (1-10): More focused outputsHigher values: More diverse completions |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| **Top-k** | 限制标记选择为k个最可能的标记 | 1-100 | 较低值（1-10）：更聚焦的输出 | 较高值：更多样化的完成 |'
- en: '| **Top-p (Nucleus Sampling)** | Considers tokens until cumulative probability
    reaches threshold | 0.0-1.0 | Lower values (0.5): More focused outputsHigher values
    (0.9): More exploratory responses |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| **Top-p（核采样）** | 考虑标记直到累积概率达到阈值 | 0.0-1.0 | 较低值（0.5）：更聚焦的输出 | 较高值（0.9）：更具探索性的响应
    |'
- en: '|'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '**Max tokens**'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**最大标记数**'
- en: '| Limits maximum response length | Model-specific | Controlling costs and preventing
    verbose outputs |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 限制最大响应长度 | 模型特定 | 控制成本和防止冗长输出 |'
- en: '| **Presence/frequency penalties** | Discourages repetition by penalizing tokens
    that have appeared | -2.0 to 2.0 | Longer content generation where repetition
    is undesirable |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| **存在/频率惩罚** | 通过惩罚已出现的标记来阻止重复 | -2.0到2.0 | 长内容生成，其中重复是不希望的 |'
- en: '| **Stop sequences** | Tells model when to stop generating | Custom strings
    | Controlling exact ending points of generation |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| **停止序列** | 告诉模型何时停止生成 | 自定义字符串 | 控制生成的确切结束点 |'
- en: 'Table 2.2: Parameters offered by LLMs'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.2：LLM提供的参数
- en: 'These parameters work together to shape model output:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数共同塑造模型输出：
- en: '**Temperature + Top-k/Top-p**: First, Top-k/Top-p filter the token distribution,
    and then temperature affects randomness within that filtered set'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**温度 + Top-k/Top-p**：首先，Top-k/Top-p过滤标记分布，然后温度影响过滤集内的随机性。'
- en: '**Penalties + Temperature**: Higher temperatures with low penalties can produce
    creative but potentially repetitive text'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**惩罚 + 温度**：较高的温度和较低的惩罚可以产生创意但可能重复的文本。'
- en: 'LangChain provides a consistent interface for setting these parameters across
    different LLM providers:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain为在不同LLM提供商之间设置这些参数提供了一个一致的接口：
- en: '[PRE30]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'A few provider-specific considerations to keep in mind are:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一些建议的特定提供商考虑因素：
- en: '**OpenAI**: Known for consistent behavior with temperature in the 0.0-1.0 range'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI**：以在0.0-1.0范围内温度的一致行为而闻名'
- en: '**Anthropic**: May need lower temperature settings to achieve similar creativity
    levels to other providers'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Anthropic**：可能需要较低的温度设置才能达到与其他提供商相似的创意水平。'
- en: '**Gemini**: Supports temperature up to 2.0, allowing for more extreme creativity
    at higher settings'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gemini**：支持高达2.0的温度，允许在较高设置下实现更极端的创意'
- en: '**Open-source models**: Often require different parameter combinations than
    commercial APIs'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开源模型**：通常需要与商业API不同的参数组合。'
- en: Choosing parameters for applications
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为应用选择参数
- en: For enterprise applications requiring consistency and accuracy, lower temperatures
    (0.0-0.3) combined with moderate top-p values (0.5-0.7) are typically preferred.
    For creative assistants or brainstorming tools, higher temperatures produce more
    diverse outputs, especially when paired with higher top-p values.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要一致性和准确性的企业应用，通常更倾向于使用较低的温度（0.0-0.3）和适中的top-p值（0.5-0.7）。对于创意助手或头脑风暴工具，较高的温度会产生更多样化的输出，尤其是在搭配较高的top-p值时。
- en: Remember that parameter tuning is often empirical – start with provider recommendations,
    then adjust based on your specific application needs and observed outputs.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 记住参数调整通常是经验性的——从提供商的建议开始，然后根据您的具体应用程序需求和观察到的输出进行调整。
- en: Prompts and templates
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示和模板
- en: 'Prompt engineering is a crucial skill for LLM application development, particularly
    in production environments. LangChain provides a robust system for managing prompts
    with features that address common development challenges:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程是 LLM 应用程序开发的关键技能，尤其是在生产环境中。LangChain 提供了一个强大的系统来管理提示，其功能解决了常见的开发挑战：
- en: '**Template systems** for dynamic prompt generation'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模板系统**以动态生成提示'
- en: '**Prompt management and versioning** for tracking changes'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示管理和版本控制**以跟踪更改'
- en: '**Few-shot example management** for improved model performance'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**少量示例管理**以提高模型性能'
- en: '**Output parsing and validation** for reliable results'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出解析和验证**以获得可靠的结果'
- en: 'LangChain’s prompt templates transform static text into dynamic prompts with
    variable substitution – compare these two approaches to see the key differences:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 的提示模板将静态文本转换为具有变量替换的动态提示——比较这两种方法以查看关键差异：
- en: 'Static use – problematic at scale:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 静态使用——在规模上存在问题：
- en: '[PRE31]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'PromptTemplate – production-ready:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PromptTemplate – 生产就绪：
- en: '[PRE32]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Templates matter – here’s why:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 模板很重要——原因如下：
- en: '**Consistency**: They standardize prompts across your application.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性**：它们在您的应用程序中标准化提示。'
- en: '**Maintainability**: They allow you to change the prompt structure in one place
    instead of throughout your codebase.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可维护性**：它们允许您在一个地方更改提示结构，而不是在整个代码库中。'
- en: '**Readability**: They clearly separate template logic from business logic.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可读性**：它们清楚地分离了模板逻辑和业务逻辑。'
- en: '**Testability**: It is easier to unit test prompt generation separately from
    LLM calls.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可测试性**：单独对提示生成进行单元测试比从 LLM 调用中分离出来更容易。'
- en: In production applications, you’ll often need to manage dozens or hundreds of
    prompts. Templates provide a scalable way to organize this complexity.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产应用程序中，您通常会需要管理数十或数百个提示。模板提供了一种可扩展的方式来组织这种复杂性。
- en: Chat prompt templates
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聊天提示模板
- en: 'For chat models, we can create more structured prompts that incorporate different
    roles:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对于聊天模型，我们可以创建更多结构化的提示，这些提示融合了不同的角色：
- en: '[PRE33]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Let’s start by looking at **LangChain Expression Language** (**LCEL**), which
    provides a clean, intuitive way to build LLM applications.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从**LangChain 表达式语言**（**LCEL**）开始，它提供了一种干净、直观的方式来构建 LLM 应用程序。
- en: LangChain Expression Language (LCEL)
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LangChain 表达式语言 (LCEL)
- en: LCEL represents a significant evolution in how we build LLM-powered applications
    with LangChain. Introduced in August 2023, LCEL is a declarative approach to constructing
    complex LLM workflows. Rather than focusing on *how* to execute each step, LCEL
    lets you define *what* you want to accomplish, allowing LangChain to handle the
    execution details behind the scenes.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: LCEL 代表了我们构建使用 LangChain 的 LLM 应用程序方式的重大进步。于 2023 年 8 月推出，LCEL 是构建复杂 LLM 工作流的一种声明式方法。LCEL
    不关注*如何*执行每个步骤，而是让您定义*想要完成什么*，从而允许 LangChain 在幕后处理执行细节。
- en: 'At its core, LCEL serves as a minimalist code layer that makes it remarkably
    easy to connect different LangChain components. If you’re familiar with Unix pipes
    or data processing libraries like pandas, you’ll recognize the intuitive syntax:
    components are connected using the pipe operator (|) to create processing pipelines.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，LCEL 作为一个极简代码层，使得连接不同的 LangChain 组件变得非常容易。如果您熟悉 Unix 管道或 pandas 等数据处理库，您会认识到直观的语法：组件通过管道运算符（|）连接以创建处理管道。
- en: As we briefly introduced in [*Chapter 1*](E_Chapter_1.xhtml#_idTextAnchor001),
    LangChain has always used the concept of a “chain” as its fundamental pattern
    for connecting components. Chains represent sequences of operations that transform
    inputs into outputs.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[*第一章*](E_Chapter_1.xhtml#_idTextAnchor001)中简要介绍的，LangChain 一直使用“链”的概念作为其连接组件的基本模式。链代表将输入转换为输出的操作序列。
- en: Originally, LangChain implemented this pattern through specific `Chain` classes
    like `LLMChain` and `ConversationChain`. While these legacy classes still exist,
    they’ve been deprecated in favor of the more flexible and powerful LCEL approach,
    which is built upon the Runnable interface.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，LangChain 通过特定的 `Chain` 类如 `LLMChain` 和 `ConversationChain` 实现了此模式。虽然这些遗留类仍然存在，但它们已被弃用，转而采用更灵活、更强大的
    LCEL 方法，该方法建立在可运行接口之上。
- en: 'The Runnable interface is the cornerstone of modern LangChain. A Runnable is
    any component that can process inputs and produce outputs in a standardized way.
    Every component built with LCEL adheres to this interface, which provides consistent
    methods including:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Runnable接口是现代LangChain的基石。Runnable是指任何可以以标准化的方式处理输入并产生输出的组件。每个使用LCEL构建的组件都遵循此接口，它提供了一致的方法，包括：
- en: '`invoke()`: Processes a single input synchronously and returns an output'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`invoke()`: 同步处理单个输入并返回输出'
- en: '`stream()`: Streams output as it’s being generated'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stream()`: 以生成时的形式输出流'
- en: '`batch()`: Efficiently processes multiple inputs in parallel'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch()`: 高效并行处理多个输入'
- en: '`ainvoke()`, `abatch()`, `astream()`: Asynchronous versions of the above methods'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ainvoke()`、`abatch()`、`astream()`：上述方法的异步版本'
- en: This standardization means any Runnable component—whether it’s an LLM, a prompt
    template, a document retriever, or a custom function—can be connected to any other
    Runnable, creating a powerful composability system.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这种标准化意味着任何Runnable组件——无论是LLM、提示模板、文档检索器还是自定义函数——都可以连接到任何其他Runnable，从而创建一个强大的可组合性系统。
- en: 'Every Runnable implements a consistent set of methods including:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Runnable实现了一组一致的方法，包括：
- en: '`invoke()`: Processes a single input synchronously and returns an output'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`invoke()`: 同步处理单个输入并返回输出'
- en: '`stream()`: Streams output as it’s being generated'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stream()`: 以生成时的形式输出流'
- en: This standardization is powerful because it means any Runnable component—whether
    it’s an LLM, a prompt template, a document retriever, or a custom function—can
    be connected to any other Runnable. The consistency of this interface enables
    complex applications to be built from simpler building blocks.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这种标准化非常强大，因为它意味着任何Runnable组件——无论是LLM、提示模板、文档检索器还是自定义函数——都可以连接到任何其他Runnable。此接口的一致性使得可以从更简单的构建块构建复杂的应用程序。
- en: 'LCEL offers several advantages that make it the preferred approach for building
    LangChain applications:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: LCEL提供了几个优势，使其成为构建LangChain应用程序的首选方法：
- en: '**Rapid development**: The declarative syntax enables faster prototyping and
    iteration of complex chains.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**快速开发**：声明性语法使得快速原型设计和复杂链的迭代变得更快。'
- en: '**Production-ready features**: LCEL provides built-in support for streaming,
    asynchronous execution, and parallel processing.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生产就绪功能**：LCEL提供了对流、异步执行和并行处理的内置支持。'
- en: '**Improved readability**: The pipe syntax makes it easy to visualize data flow
    through your application.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可读性提高**：管道语法使得可视化数据流通过你的应用程序变得容易。'
- en: '**Seamless ecosystem integration**: Applications built with LCEL automatically
    work with LangSmith for observability and LangServe for deployment.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无缝生态系统集成**：使用LCEL构建的应用程序可以自动与LangSmith进行监控和LangServe进行部署。'
- en: '**Customizability**: Easily incorporate custom Python functions into your chains
    with RunnableLambda.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可定制性**：使用RunnableLambda轻松将自定义Python函数集成到你的链中。'
- en: '**Runtime optimization**: LangChain can automatically optimize the execution
    of LCEL-defined chains.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行时优化**：LangChain可以自动优化LCEL定义的链的执行。'
- en: LCEL truly shines when you need to build complex applications that combine multiple
    components in sophisticated workflows. In the next sections, we’ll explore how
    to use LCEL to build real-world applications, starting with the basic building
    blocks and gradually incorporating more advanced patterns.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要构建复杂的应用程序，这些应用程序结合了多个组件在复杂的流程中时，LCEL真正大放异彩。在接下来的章节中，我们将探讨如何使用LCEL构建实际的应用程序，从基本的构建块开始，并逐步引入更高级的模式。
- en: 'The pipe operator (|) serves as the cornerstone of LCEL, allowing you to chain
    components sequentially:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 管道操作符（|）是LCEL的基石，允许你按顺序连接组件：
- en: '[PRE34]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Here, `StrOutputParser()` is a simple output parser that extracts the string
    response from an LLM. It takes the structured output from an LLM and converts
    it to a plain string, making it easier to work with. This parser is especially
    useful when you need just the text content without metadata.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`StrOutputParser()`是一个简单的输出解析器，它从LLM中提取字符串响应。它将LLM的结构化输出转换为普通字符串，使其更容易处理。这个解析器在只需要文本内容而不需要元数据时特别有用。
- en: Under the hood, LCEL uses Python’s operator overloading to transform this expression
    into a RunnableSequence where each component’s output flows into the next component’s
    input. The pipe (|) is syntactic sugar that overrides the `__or__` hidden method,
    in other words, `A | B` is equivalent to `B.__or__(A)`.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，LCEL使用Python的操作符重载将这个表达式转换成一个RunnableSequence，其中每个组件的输出流向下一个组件的输入。管道（|）是语法糖，它覆盖了`__or__`隐藏方法，换句话说，`A
    | B`等价于`B.__or__(A)`。
- en: 'The pipe syntax is equivalent to creating a `RunnableSequence` programmatically:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 管道语法等价于程序性地创建一个`RunnableSequence`：
- en: '[PRE35]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'For more complex workflows, you can incorporate branching logic:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的工作流程，你可以结合分支逻辑：
- en: '[PRE36]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Non-Runnable elements like functions and dictionaries are automatically converted
    to appropriate Runnable types:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 非Runnable元素，如函数和字典，会自动转换为适当的Runnable类型：
- en: '[PRE37]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The flexible, composable nature of LCEL will allow us to tackle real-world LLM
    application challenges with elegant, maintainable code.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: LCEL的灵活和可组合特性将使我们能够用优雅、可维护的代码解决实际的LLM应用挑战。
- en: Simple workflows with LCEL
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用LCEL的简单工作流程
- en: 'As we’ve seen, LCEL provides a declarative syntax for composing LLM application
    components using the pipe operator. This approach dramatically simplifies workflow
    construction compared to traditional imperative code. Let’s build a simple joke
    generator to see LCEL in action:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，LCEL提供了一个声明性语法，用于使用管道操作符组合LLM应用程序组件。与传统的命令式代码相比，这种方法大大简化了工作流程构建。让我们构建一个简单的笑话生成器来查看LCEL的实际应用：
- en: '[PRE38]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This produces a programming joke:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了一个编程笑话：
- en: '[PRE40]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Without LCEL, the same workflow is equivalent to separate function calls with
    manual data passing:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 没有LCEL，相同的流程等同于单独的函数调用，并手动传递数据：
- en: '[PRE41]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As you can see, we have detached chain construction from its execution.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们已经将链式构建与其执行分离。
- en: In production applications, this pattern becomes even more valuable when handling
    complex workflows with branching logic, error handling, or parallel processing
    – topics we’ll explore in [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产应用中，当处理具有分支逻辑、错误处理或并行处理的复杂工作流程时，这种模式变得更加有价值——这些内容我们将在[*第3章*](E_Chapter_3.xhtml#_idTextAnchor107)中探讨。
- en: Complex chain example
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 复杂链示例
- en: While the simple joke generator demonstrated basic LCEL usage, real-world applications
    typically require more sophisticated data handling. Let’s explore advanced patterns
    using a story generation and analysis example.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然简单的笑话生成器展示了基本的LCEL使用，但现实世界的应用通常需要更复杂的数据处理。让我们通过一个故事生成和分析示例来探索高级模式。
- en: 'In this example, we’ll build a multi-stage workflow that demonstrates how to:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将构建一个多阶段工作流程，展示如何：
- en: Generate content with one LLM call
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一次LLM调用生成内容
- en: Feed that content into a second LLM call
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将该内容输入到第二次LLM调用
- en: Preserve and transform data throughout the chain
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在整个链中保留和转换数据
- en: '[PRE42]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We can compose these two chains together. Our first simple approach pipes the
    story directly into the analysis chain:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这两个链组合在一起。我们的第一个简单方法直接将故事管道输入到分析链中：
- en: '[PRE44]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'I get a long analysis. Here’s how it starts:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我得到了一个长的分析。这是它的开始：
- en: '[PRE45]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'While this works, we’ve lost the original story in our result – we only get
    the analysis! In production applications, we typically want to preserve context
    throughout the chain:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可行，但我们已经失去了结果中的原始故事——我们只得到了分析！在生产应用中，我们通常希望在整个链中保留上下文：
- en: '[PRE46]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'For more control over the output structure, we could also construct dictionaries
    manually:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对输出结构有更多控制，我们也可以手动构建字典：
- en: '[PRE48]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We can simplify this with dictionary conversion using a LCEL shorthand:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用LCEL缩写进行字典转换来简化这个过程：
- en: '[PRE49]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: What makes these examples more complex than our simple joke generator?
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子比我们的简单笑话生成器更复杂的是什么？
- en: '**M****ultiple LLM calls**: Rather than a single prompt ![](img/Icon.png) LLM
    ![](img/Icon.png) parser flow, we’re chaining multiple LLM interactions'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多次LLM调用**：而不是单一的提示！[](img/Icon.png)LLM！[](img/Icon.png)解析流程，我们正在链式多个LLM交互'
- en: '**Data transformation**: Using tools like `RunnablePassthrough` and `itemgetter`
    to manage and transform data'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据转换**：使用`RunnablePassthrough`和`itemgetter`等工具来管理和转换数据'
- en: '**Dictionary preservation**: Maintaining context throughout the chain rather
    than just passing single values'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字典保留**：在整个链中维护上下文，而不仅仅是传递单个值'
- en: '**Structured outputs**: Creating structured output dictionaries rather than
    simple strings'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构化输出**：创建结构化输出字典而不是简单的字符串'
- en: 'These patterns are essential for production applications where you need to:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模式对于需要在生产应用中进行以下操作的情况至关重要：
- en: Track the provenance of generated content
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪生成内容的来源
- en: Combine results from multiple operations
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合多个操作的结果
- en: Structure data for downstream processing or display
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化数据以进行下游处理或显示
- en: Implement more sophisticated error handling
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现更复杂的错误处理
- en: While LCEL handles many complex workflows elegantly, for state management and
    advanced branching logic, you’ll want to explore LangGraph, which we’ll cover
    in [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 LCEL 以优雅的方式处理许多复杂的工作流程，但对于状态管理和高级分支逻辑，您可能希望探索 LangGraph，我们将在 [*第3章*](E_Chapter_3.xhtml#_idTextAnchor107)
    中介绍。
- en: While our previous examples used cloud-based models like OpenAI and Google’s
    Gemini, LangChain’s LCEL and other functionality work seamlessly with local models
    as well. This flexibility allows you to choose the right deployment approach for
    your specific needs.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们之前的示例使用了基于云的模型，如 OpenAI 和 Google 的 Gemini，但 LangChain 的 LCEL 和其他功能也与本地模型无缝协作。这种灵活性允许您根据特定需求选择正确的部署方法。
- en: Running local models
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行本地模型
- en: When building LLM applications with LangChain, you need to decide where your
    models will run.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 LangChain 构建LLM应用时，您需要决定模型将运行在哪里。
- en: 'Advantages of local models:'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地模型的优点：
- en: Complete data control and privacy
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全的数据控制和隐私
- en: No API costs or usage limits
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无 API 成本或使用限制
- en: No internet dependency
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无网络依赖
- en: Control over model parameters and fine-tuning
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制模型参数和微调
- en: 'Advantages of cloud models:'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云模型的优点：
- en: No hardware requirements or setup complexity
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无硬件要求或设置复杂性
- en: Access to the most powerful, state-of-the-art models
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问最强大、最前沿的模型
- en: Elastic scaling without infrastructure management
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无需基础设施管理即可弹性扩展
- en: Continuous model improvements without manual updates
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无需手动更新即可持续改进模型
- en: 'When to choose local models:'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择本地模型的时候：
- en: Applications with strict data privacy requirements
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据隐私要求严格的应用
- en: Development and testing environments
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发和测试环境
- en: Edge or offline deployment scenarios
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边缘或离线部署场景
- en: Cost-sensitive applications with predictable, high-volume usage
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对成本敏感的应用，具有可预测的高容量使用
- en: Let’s start with one of the most developer-friendly options for running local
    models.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最符合开发者友好的本地模型运行选项之一开始。
- en: Getting started with Ollama
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用 Ollama
- en: 'Ollama provides a developer-friendly way to run powerful open-source models
    locally. It provides a simple interface for downloading and running various open-source
    models. The `langchain-ollama` dependency should already be installed if you’ve
    followed the instructions in this chapter; however, let’s go through them briefly
    anyway:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: Ollama 提供了一种开发者友好的方式来本地运行强大的开源模型。它提供了一个简单的界面来下载和运行各种开源模型。如果您已遵循本章中的说明，`langchain-ollama`
    依赖项应该已经安装；然而，我们仍然简要地介绍一下：
- en: 'Install the LangChain Ollama integration:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 LangChain Ollama 集成：
- en: '[PRE50]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Then pull a model. From the command line, a terminal such as bash or the WindowsPowerShell,
    run:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后拉取一个模型。从命令行，例如 bash 或 WindowsPowerShell 终端，运行：
- en: '[PRE51]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Start the Ollama server:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 Ollama 服务器：
- en: '[PRE52]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Here’s how to integrate Ollama with the LCEL patterns we’ve explored:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何将 Ollama 与我们探索的 LCEL 模式集成的：
- en: '[PRE53]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: This LCEL chain functions identically to our cloud-based examples, demonstrating
    LangChain’s model-agnostic design.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 LCEL 链与我们的云模型示例功能相同，展示了 LangChain 的模型无关设计。
- en: Please note that since you are running a local model, you don’t need to set
    up any keys. The answer is very long – although quite reasonable. You can run
    this yourself and see what answers you get.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于您正在运行本地模型，您不需要设置任何密钥。答案非常长——尽管相当合理。您可以自己运行并查看您会得到什么答案。
- en: Now that we’ve seen basic text generation, let’s look at another integration.
    Hugging Face offers an approachable way to run models locally, with access to
    a vast ecosystem of pre-trained models.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了基本的文本生成，让我们看看另一个集成。Hugging Face 提供了一种易于使用的方法来本地运行模型，并可以访问庞大的预训练模型生态系统。
- en: Working with Hugging Face models locally
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在本地使用 Hugging Face 模型
- en: 'With Hugging Face, you can either run a model locally (HuggingFacePipeline)
    or on the Hugging Face Hub (HuggingFaceEndpoint). Here, we are talking about local
    runs, so we’ll focus on `HuggingFacePipeline`. Here we go:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Hugging Face，您可以选择在本地（HuggingFacePipeline）或 Hugging Face Hub（HuggingFaceEndpoint）上运行模型。在这里，我们讨论的是本地运行，因此我们将重点关注
    `HuggingFacePipeline`。让我们开始吧：
- en: '[PRE54]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: This can take quite a while, especially the first time, since the model has
    to be downloaded first. We’ve omitted the model response for the sake of brevity.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要相当长的时间，尤其是第一次，因为模型需要先下载。为了简洁，我们省略了模型响应。
- en: 'LangChain supports running models locally through other integrations as well,
    for example:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain还支持通过其他集成在本地运行模型，例如：
- en: '**llama.cpp:** This high-performance C++ implementation allows running LLaMA-based
    models efficiently on consumer hardware. While we won’t cover the setup process
    in detail, LangChain provides straightforward integration with llama.cpp for both
    inference and fine-tuning.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**llama.cpp**：这个高性能的C++实现允许在消费级硬件上高效运行基于LLaMA的模型。虽然我们不会详细介绍设置过程，但LangChain提供了与llama.cpp的简单集成，用于推理和微调。'
- en: '**GPT4All**: GPT4All offers lightweight models that can run on consumer hardware.
    LangChain’s integration makes it easy to use these models as drop-in replacements
    for cloud-based LLMs in many applications.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT4All**：GPT4All提供轻量级模型，可以在消费级硬件上运行。LangChain的集成使得在许多应用程序中将这些模型作为云LLM的即插即用替代变得容易。'
- en: As you begin working with local models, you’ll want to optimize their performance
    and handle common challenges. Here are some essential tips and patterns that will
    help you get the most out of your local deployments with LangChain.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始使用本地模型时，你会想要优化它们的性能并处理常见的挑战。以下是一些基本的技巧和模式，这些将帮助你从LangChain的本地部署中获得最大收益。
- en: Tips for local models
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地模型的技巧
- en: 'When working with local models, keep these points in mind:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用本地模型时，请记住以下要点：
- en: '**Resource management**: Local models require careful configuration to balance
    performance and resource usage. The following example demonstrates how to configure
    an Ollama model for efficient operation:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**资源管理**：本地模型需要仔细配置以平衡性能和资源使用。以下示例演示了如何配置Ollama模型以实现高效操作：'
- en: '[PRE55]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Let’s look at what each parameter does:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看每个参数的作用：
- en: '**model=”mistral:q4_K_M”**: Specifies a 4-bit quantized version of the Mistral
    model. Quantization reduces the model size by representing weights with fewer
    bits, trading minimal precision for significant memory savings. For example:'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**model="mistral:q4_K_M"**：指定Mistral模型的4位量化版本。量化通过使用更少的位来表示权重，以最小的精度换取显著的内存节省。例如：'
- en: 'Full precision model: ~8GB RAM required'
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整精度模型：需要约8GB RAM
- en: '4-bit quantized model: ~2GB RAM required'
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4位量化模型：需要约2GB RAM
- en: '**num_gpu=1**: Allocates GPU resources. Options include:'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**num_gpu=1**：分配GPU资源。选项包括：'
- en: '0: CPU-only mode (slower but works without a GPU)'
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0：仅CPU模式（较慢但无需GPU即可工作）
- en: '1: Uses a single GPU (appropriate for most desktop setups)'
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1: 使用单个GPU（适用于大多数桌面配置）'
- en: 'Higher values: For multi-GPU systems only'
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较高值：仅适用于多GPU系统
- en: '**num_thread=4**: Controls CPU parallelization:'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**num_thread=4**：控制CPU并行化：'
- en: 'Lower values (2-4): Good for running alongside other applications'
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较低值（2-4）：适合与其他应用程序一起运行
- en: 'Higher values (8-16): Maximizes performance on dedicated servers'
  id: totrans-347
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较高值（8-16）：在专用服务器上最大化性能
- en: 'Optimal setting: Usually matches your CPU’s physical core count'
  id: totrans-348
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳设置：通常与CPU的物理核心数相匹配
- en: '**Error handling**: Local models can encounter various errors, from out-of-memory
    conditions to unexpected terminations. A robust error-handling strategy is essential:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**错误处理**：本地模型可能会遇到各种错误，从内存不足到意外的终止。一个健壮的错误处理策略是必不可少的：'
- en: '[PRE56]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Common local model errors you might run into are as follows:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会遇到以下常见的本地模型错误：
- en: '**Out of memory**: Occurs when the model requires more VRAM than available'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存不足**：当模型需要的VRAM超过可用量时发生'
- en: '**Model loading failure**: When model files are corrupt or incompatible'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型加载失败**：当模型文件损坏或不兼容时'
- en: '**Timeout issues**: When inference takes too long on resource-constrained systems'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超时问题**：在资源受限的系统上推理时间过长'
- en: '**Context length errors**: When input exceeds the model’s maximum token limit'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文长度错误**：当输入超过模型的最大令牌限制时'
- en: By implementing these optimizations and error-handling strategies, you can create
    robust LangChain applications that leverage local models effectively while maintaining
    a good user experience even when issues arise.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实施这些优化和错误处理策略，你可以创建健壮的LangChain应用程序，有效地利用本地模型，即使在出现问题时也能保持良好的用户体验。
- en: '![Figure 2.1: Decision chart for choosing between local and cloud-based models](img/B32363_02_01.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1：选择本地和基于云模型的决策图](img/B32363_02_01.png)'
- en: 'Figure 2.1: Decision chart for choosing between local and cloud-based models'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：选择本地和基于云模型的决策图
- en: Having explored how to build text-based applications with LangChain, we’ll now
    extend our understanding to multimodal capabilities. As AI systems increasingly
    work with multiple forms of data, LangChain provides interfaces for both generating
    images from text and understanding visual content – capabilities that complement
    the text processing we’ve already covered and open new possibilities for more
    immersive applications.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨了如何使用LangChain构建基于文本的应用程序之后，我们现在将扩展我们对多模态功能的理解。随着人工智能系统越来越多地与多种形式的数据一起工作，LangChain提供了生成图像和理解视觉内容的接口——这些功能补充了我们已经涵盖的文本处理，并为更沉浸式的应用程序开辟了新的可能性。
- en: Multimodal AI applications
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态人工智能应用
- en: AI systems have evolved beyond text-only processing to work with diverse data
    types. In the current landscape, we can distinguish between two key capabilities
    that are often confused but represent different technological approaches.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能系统已经超越了仅处理文本的阶段，开始处理多种数据类型。在当前环境中，我们可以区分两种关键能力，这两种能力经常被混淆，但代表了不同的技术方法。
- en: Multimodal understanding represents the ability of models to process multiple
    types of inputs simultaneously to perform reasoning and generate responses. These
    advanced systems can understand the relationships between different modalities,
    accepting inputs like text, images, PDFs, audio, video, and structured data. Their
    processing capabilities include cross-modal reasoning, context awareness, and
    sophisticated information extraction. Models like Gemini 2.5, GPT-4V, Sonnet 3.7,
    and Llama 4 exemplify this capability. For instance, a multimodal model can analyze
    a chart image along with a text question to provide insights about the data trend,
    combining visual and textual understanding in a single processing flow.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态理解代表了模型能够同时处理多种类型的输入以进行推理和生成响应的能力。这些先进系统可以理解不同模态之间的关系，接受输入如文本、图像、PDF、音频、视频和结构化数据。它们的处理能力包括跨模态推理、情境感知和复杂的信息提取。Gemini
    2.5、GPT-4V、Sonnet 3.7和Llama 4等模型体现了这种能力。例如，一个多模态模型可以分析图表图像和文本问题，以提供关于数据趋势的见解，在单个处理流程中将视觉和文本理解结合起来。
- en: Content generation capabilities, by contrast, focus on creating specific types
    of media, often with extraordinary quality but more specialized functionality.
    Text-to-image models create visual content from descriptions, text-to-video systems
    generate video clips from prompts, text-to-audio tools produce music or speech,
    and image-to-image models transform existing visuals. Examples include Midjourney,
    DALL-E, and Stable Diffusion for images; Sora and Pika for video; and Suno and
    ElevenLabs for audio. Unlike true multimodal models, many generation systems are
    specialized for their specific output modality, even if they can accept multiple
    input types. They excel at creation rather than understanding.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，内容生成能力专注于创建特定类型的媒体，通常具有非凡的质量但更专业的功能。文本到图像模型从描述中创建视觉内容，文本到视频系统从提示中生成视频片段，文本到音频工具生成音乐或语音，图像到图像模型转换现有的视觉内容。例如，Midjourney、DALL-E和Stable
    Diffusion用于图像；Sora和Pika用于视频；Suno和ElevenLabs用于音频。与真正的多模态模型不同，许多生成系统针对其特定的输出模态进行了专门化，即使它们可以接受多种输入类型。它们在创作方面表现出色，而不是在理解方面。
- en: As LLMs evolve beyond text, LangChain is expanding to support both multimodal
    understanding and content generation workflows. The framework provides developers
    with tools to incorporate these advanced capabilities into their applications
    without needing to implement complex integrations from scratch. Let’s start with
    generating images from text descriptions. LangChain provides several approaches
    to incorporate image generation through external integrations and wrappers. We’ll
    explore multiple implementation patterns, starting with the simplest and progressing
    to more sophisticated techniques that can be incorporated into your applications.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）的发展超越文本，LangChain正在扩展以支持多模态理解和内容生成工作流程。该框架为开发者提供了工具，使他们能够将高级功能集成到应用程序中，而无需从头开始实现复杂的集成。让我们从根据文本描述生成图像开始。LangChain提供了多种通过外部集成和包装器实现图像生成的方法。我们将探索多种实现模式，从最简单的开始，逐步过渡到更复杂的技巧，这些技巧可以集成到您的应用程序中。
- en: Text-to-image
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到图像
- en: 'LangChain integrates with various image generation models and services, allowing
    you to:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain与各种图像生成模型和服务集成，允许您：
- en: Generate images from text descriptions
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本描述生成图像
- en: Edit existing images based on text prompts
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据文本提示编辑现有图像
- en: Control image generation parameters
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制图像生成参数
- en: Handle image variations and styles
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理图像变化和风格
- en: LangChain includes wrappers and models for popular image generation services.
    First, let’s see how to generate images with OpenAI’s DALL-E model series.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 包括对流行的图像生成服务的包装和模型。首先，让我们看看如何使用 OpenAI 的 DALL-E 模型系列生成图像。
- en: Using DALL-E through OpenAI
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过 OpenAI 使用 DALL-E
- en: LangChain’s wrapper for DALL-E simplifies the process of generating images from
    text prompts. The implementation uses OpenAI’s API under the hood but provides
    a standardized interface consistent with other LangChain components.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 为 DALL-E 提供的包装简化了从文本提示生成图像的过程。该实现底层使用 OpenAI 的 API，但提供了一个与其他 LangChain
    组件一致的标准化接口。
- en: '[PRE58]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Here’s the image we got:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的图像：
- en: '![Figure 2.2: An image generated by OpenAI’s DALL-E Image Generator](img/B32363_02_02.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2：由 OpenAI 的 DALL-E 图像生成器生成的图像](img/B32363_02_02.png)'
- en: 'Figure 2.2: An image generated by OpenAI’s DALL-E Image Generator'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：由 OpenAI 的 DALL-E 图像生成器生成的图像
- en: You might notice that text generation within these images is not one of the
    strong suites of these models. You can find a lot of models for image generation
    on Replicate, including the latest Stable Diffusion models, so this is what we’ll
    use now.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，在这些图像中的文本生成不是这些模型的优势之一。你可以在 Replicate 上找到许多图像生成模型，包括最新的 Stable Diffusion
    模型，因此我们现在将使用这些模型。
- en: Using Stable Diffusion
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Stable Diffusion
- en: Stable Diffusion 3.5 Large is Stability AI’s latest text-to-image model, released
    in March 2024\. It’s a **Multimodal Diffusion Transformer** (**MMDiT**) that generates
    high-resolution images with remarkable detail and quality.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion 3.5 Large 是 Stability AI 在 2024 年 3 月发布的最新文本到图像模型。它是一个 **多模态扩散变换器**（**MMDiT**），能够生成具有显著细节和质量的超高分辨率图像。
- en: This model uses three fixed, pre-trained text encoders and implements Query-Key
    Normalization for improved training stability. It’s capable of producing diverse
    outputs from the same prompt and supports various artistic styles.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型使用三个固定的、预训练的文本编码器，并实现了查询-键归一化以改善训练稳定性。它能够从相同的提示生成多样化的输出，并支持各种艺术风格。
- en: '[PRE60]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The recommended parameters for the new model include:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 新模型推荐参数包括：
- en: '**prompt_strength**: Controls how closely the image follows the prompt (0.85)'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prompt_strength**：控制图像与提示的匹配程度（0.85）'
- en: '**cfg**: Controls how strictly the model follows the prompt (4.5)'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cfg**：控制模型遵循提示的严格程度（4.5）'
- en: '**steps**: More steps result in higher-quality images (40)'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**steps**：更多步骤会产生更高质量的图像（40）'
- en: '**aspect_ratio**: Set to 1:1 for square images'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**aspect_ratio**：设置为 1:1 以获得方形图像'
- en: '**output_format**: Using WebP for a better quality-to-size ratio'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**output_format**：使用 WebP 以获得更好的质量与尺寸比'
- en: '**output_quality**: Set to 90 for high-quality output'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**output_quality**：设置为 90 以获得高质量输出'
- en: 'Here’s the image we got:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的图像：
- en: '![Figure 2.3: An image generated by Stable Diffusion](img/B32363_02_03.png)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3：由 Stable Diffusion 生成的图像](img/B32363_02_03.png)'
- en: 'Figure 2.3: An image generated by Stable Diffusion'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：由 Stable Diffusion 生成的图像
- en: Now let’s explore how to analyze and understand images using multimodal models.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探索如何使用多模态模型分析和理解图像。
- en: Image understanding
  id: totrans-396
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像理解
- en: Image understanding refers to an AI system’s ability to interpret and analyze
    visual information in ways similar to human visual perception. Unlike traditional
    computer vision (which focuses on specific tasks like object detection or facial
    recognition), modern multimodal models can perform general reasoning about images,
    understanding context, relationships, and even implicit meaning within visual
    content.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 图像理解指的是人工智能系统以类似于人类视觉感知的方式解释和分析视觉信息的能力。与传统的计算机视觉（专注于特定任务，如目标检测或人脸识别）不同，现代多模态模型可以对图像进行一般推理，理解上下文、关系，甚至视觉内容中的隐含意义。
- en: Gemini 2.5 Pro and GPT-4 Vision, among other models, can analyze images and
    provide detailed descriptions or answer questions about them.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: Gemini 2.5 Pro 和 GPT-4 Vision 等模型可以分析图像并提供详细的描述或回答有关它们的问题。
- en: Using Gemini 1.5 Pro
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Gemini 1.5 Pro
- en: LangChain handles multimodal input through the same `ChatModel` interface. It
    accepts `Messages` as an input, and a `Message` object has a `content` field.
    IA `content` can consist of multiple parts, and each part can represent a different
    modality (that allows you to mix different modalities in your prompt).
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 通过相同的 `ChatModel` 接口处理多模态输入。它接受 `Messages` 作为输入，一个 `Message` 对象有一个
    `content` 字段。IA `content` 可以由多个部分组成，每个部分可以代表不同的模态（这允许你在提示中混合不同的模态）。
- en: 'You can send multimodal input by value or by reference. To send it by value,
    you should encode bytes as a string and construct an `image_url` variable formatted
    as in the example below using the image we generated using Stable Diffusion:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过值或引用发送多模态输入。要按值发送，你应该将字节编码为字符串，并构建一个格式如下所示的`image_url`变量，使用我们使用Stable Diffusion生成的图像：
- en: '[PRE61]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: As multimodal inputs typically have a large size, sending raw bytes as part
    of your request might not be the best idea. You can send it by reference by pointing
    to the blob storage, but the specific type of storage depends on the model’s provider.
    For example, Gemini accepts multimedia input as a reference to Google Cloud Storage
    – a blob storage service provided by Google Cloud.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 由于多模态输入通常具有很大的体积，将原始字节作为请求的一部分发送可能不是最佳选择。你可以通过指向blob存储来按引用发送它，但具体的存储类型取决于模型的提供者。例如，Gemini接受多媒体输入作为对Google
    Cloud Storage的引用——这是由Google Cloud提供的一个blob存储服务。
- en: '[PRE63]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Exact details on how to construct a multimodal input might depend on the provider
    of the LLM (and a corresponding LangChain integration handles a dictionary corresponding
    to a part of a `content` field accordingly). For example, Gemini accepts an additional
    `"video_metadata"` key that can point to the start and/or end offset of a video
    piece to be analyzed:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 如何构建多模态输入的详细说明可能取决于LLM的提供者（以及相应的LangChain集成相应地处理`content`字段的一部分的字典）。例如，Gemini接受一个额外的`"video_metadata"`键，可以指向要分析的视频片段的开始和/或结束偏移量：
- en: '[PRE64]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'And, of course, such multimodal parts can also be templated. Let’s demonstrate
    it with a simple template that expects an `image_bytes_str` argument that contains
    encoded bytes:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这样的多模态部分也可以进行模板化。让我们用一个简单的模板来演示，该模板期望一个包含编码字节的`image_bytes_str`参数：
- en: '[PRE65]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Using GPT-4 Vision
  id: totrans-410
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用GPT-4 Vision
- en: After having explored image generation, let’s examine how LangChain handles
    image understanding using multimodal models. GPT-4 Vision capabilities (available
    in models like GPT-4o and GPT-4o-mini) allow us to analyze images alongside text,
    enabling applications that can “see” and reason about visual content.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索了图像生成之后，让我们看看LangChain如何使用多模态模型处理图像理解。GPT-4 Vision功能（在GPT-4o和GPT-4o-mini等模型中可用）使我们能够在文本旁边分析图像，使能够“看到”并对视觉内容进行推理的应用成为可能。
- en: 'LangChain simplifies working with these models by providing a consistent interface
    for multimodal inputs. Let’s implement a flexible image analyzer:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain通过提供多模态输入的一致接口简化了与这些模型的工作。让我们实现一个灵活的图像分析器：
- en: '[PRE66]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The model provides a rich, detailed analysis of our generated cityscape:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型为我们生成的城市景观提供了丰富、详细的分析：
- en: '[PRE68]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: This capability opens numerous possibilities for LangChain applications. By
    combining image analysis with the text processing patterns we explored earlier
    in this chapter, you can build sophisticated applications that reason across modalities.
    In the next chapter, we’ll build on these concepts to create more sophisticated
    multimodal applications.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 这种能力为LangChain应用开辟了众多可能性。通过将图像分析与我们在本章早期探索的文本处理模式相结合，你可以构建跨模态推理的复杂应用。在下一章中，我们将在此基础上创建更复杂的多模态应用。
- en: Summary
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: After setting up our development environment and configuring necessary API keys,
    we’ve explored the foundations of LangChain development, from basic chains to
    multimodal capabilities. We’ve seen how LCEL simplifies complex workflows and
    how LangChain integrates with both text and image processing. These building blocks
    prepare us for more advanced applications in the coming chapters.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置我们的开发环境并配置必要的API密钥后，我们已经探索了LangChain开发的基础，从基本链到多模态功能。我们看到了LCEL如何简化复杂的工作流程，以及LangChain如何与文本和图像处理集成。这些构建块为我们下一章中更高级的应用做好了准备。
- en: In the next chapter, we’ll expand on these concepts to create more sophisticated
    multimodal applications with enhanced control flow, structured outputs, and advanced
    prompt techniques. You’ll learn how to combine multiple modalities in complex
    chains, incorporate more sophisticated error handling, and build applications
    that leverage the full potential of modern LLMs.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将扩展这些概念，以创建具有增强控制流、结构化输出和高级提示技术的更复杂的多模态应用。你将学习如何将多种模态结合到复杂的链中，整合更复杂的错误处理，并构建充分利用现代LLM全部潜力的应用。
- en: Review questions
  id: totrans-421
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复习问题
- en: What are the three main limitations of raw LLMs that LangChain addresses?
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LangChain解决了原始LLM的哪三个主要限制？
- en: Memory limitations
  id: totrans-423
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存限制
- en: Tool integration
  id: totrans-424
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工具集成
- en: Context constraints
  id: totrans-425
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文约束
- en: Processing speed
  id: totrans-426
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理速度
- en: Cost optimization
  id: totrans-427
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本优化
- en: Which of the following best describes the purpose of LCEL (LangChain Expression
    Language)?
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪项最能描述 LCEL (LangChain 表达语言) 的目的？
- en: A programming language for LLMs
  id: totrans-429
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 的编程语言
- en: A unified interface for composing LangChain components
  id: totrans-430
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组成 LangChain 组件的统一接口
- en: A template system for prompts
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示模板系统
- en: A testing framework for LLMs
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs 的测试框架
- en: Name three types of memory systems available in LangChain
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出 LangChain 中可用的三种内存系统类型
- en: Compare and contrast LLMs and chat models in LangChain. How do their interfaces
    and use cases differ?
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较 LangChain 中的 LLMs 和聊天模型，它们的接口和使用案例有何不同？
- en: What role do Runnables play in LangChain? How do they contribute to building
    modular LLM applications?
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Runnables 在 LangChain 中扮演什么角色？它们如何有助于构建模块化的 LLM 应用程序？
- en: When running models locally, which factors affect model performance? (Select
    all that apply)
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当在本地运行模型时，哪些因素会影响模型性能？（选择所有适用的）
- en: Available RAM
  id: totrans-437
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用 RAM
- en: CPU/GPU capabilities
  id: totrans-438
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU/GPU 功能
- en: Internet connection speed
  id: totrans-439
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 互联网连接速度
- en: Model quantization level
  id: totrans-440
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型量化级别
- en: Operating system type
  id: totrans-441
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统类型
- en: 'Compare the following model deployment options and identify scenarios where
    each would be most appropriate:'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较以下模型部署选项，并确定每个选项最合适的场景：
- en: Cloud-based models (e.g., OpenAI)
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于云的模型（例如，OpenAI）
- en: Local models with llama.cpp
  id: totrans-444
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 llama.cpp 的本地模型
- en: GPT4All integration
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT4All 集成
- en: 'Design a basic chain using LCEL that would:'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 LCEL 设计一个基本的链，该链将：
- en: Take a user question about a product
  id: totrans-447
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对一个产品的用户问题进行提问
- en: Query a database for product information
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询数据库以获取产品信息
- en: Generate a response using an LLM
  id: totrans-449
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 LLM 生成响应
- en: Provide a sketch outlining the components and how they connect.
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供一个概述组件及其连接方式的草图。
- en: 'Compare the following approaches for image analysis and mention the trade-offs
    between them:'
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较以下图像分析方法，并提及它们之间的权衡：
- en: Approach A
  id: totrans-452
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方法 A
- en: '[PRE69]'
  id: totrans-453
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Approach B
  id: totrans-454
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方法 B
- en: '[PRE70]'
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Subscribe to our weekly newsletter
  id: totrans-456
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 订阅我们的每周通讯简报
- en: Subscribe to AI_Distilled, the go-to newsletter for AI professionals, researchers,
    and innovators, at [https://packt.link/Q5UyU](https://packt.link/Q5UyU).
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 订阅 AI_Distilled，这是 AI 专业人士、研究人员和创新者的首选通讯简报，请访问 [https://packt.link/Q5UyU](https://packt.link/Q5UyU)。
- en: '![](img/Newsletter_QRcode1.jpg)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Newsletter_QRcode1.jpg)'
