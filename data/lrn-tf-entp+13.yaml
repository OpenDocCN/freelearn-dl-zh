- en: 'Chapter 9:'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: '第9章:'
- en: Serving a TensorFlow Model
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提供TensorFlow模型
- en: 'By now, after learning all the previous chapters, you have seen many facets
    of a model building process in **TensorFlow Enterprise (TFE)**. Now it is time
    to wrap up what we have done and look at how we can serve the model we have built.
    In this chapter, we are going to look at the fundamentals of serving a TensorFlow
    model, which is through a RESTful API in localhost. The easiest way to get started
    is by using **TensorFlow Serving (TFS)**. Out of the box, TFS is a system for
    serving machine learning models built with TensorFlow. Although it is not yet
    officially supported by TFE, you will see that it works with models built by TFE
    2\. It can run as either a server or as a Docker container. For our ease, we are
    going to use a Docker container, as it is really the easiest way to start using
    TFS, regardless of your local environment, as long as you have a Docker engine
    available. In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在学习了前面所有章节之后，您已经看到了**TensorFlow Enterprise (TFE)**模型构建过程的许多方面。现在是时候总结我们所做的工作，并看看如何服务我们构建的模型了。在本章中，我们将研究通过本地主机中的RESTful
    API提供TensorFlow模型的基础知识。开始的最简单方式是使用**TensorFlow Serving (TFS)**。TFS是一个用于提供使用TensorFlow构建的机器学习模型的系统，尽管它目前尚未得到TFE的官方支持，但您会发现它适用于由TFE
    2构建的模型。它可以作为服务器运行，也可以作为Docker容器运行。为了我们的方便起见，我们将使用Docker容器，因为这确实是开始使用TFS的最简单方式，无论您的本地环境如何，只要您有可用的Docker引擎。在本章中，我们将涵盖以下主题：
- en: Running Local Serving
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行本地服务
- en: Understanding TFS with Docker
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解使用Docker的TFS
- en: Downloading TFS Docker images
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载TFS Docker镜像
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To follow along with this chapter, and for trying the example code here: https://github.com/PacktPublishing/learn-tensorflow-enterprise,
    you will need to clone the GitHub repository for this book, and navigate to the
    folder in `chapter_09`. You may clone the repository with the following command:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 要按照本章进行操作，并尝试此处的示例代码：https://github.com/PacktPublishing/learn-tensorflow-enterprise，您需要克隆此书的GitHub存储库，并导航到`chapter_09`文件夹。您可以使用以下命令克隆存储库：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will work from the folder named `chapter_09`. Inside this folder, there is
    a Jupyter notebook containing source code. You will also find the `flowerclassifier/001`
    directory, which contains a `saved_model.pb` file ready for your use. In the `raw_images`
    directory, you will find a few raw JPG images for testing.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从名为`chapter_09`的文件夹开始工作。在此文件夹中，有一个包含源代码的Jupyter笔记本。您还会在`flowerclassifier/001`目录中找到一个`saved_model.pb`文件，准备好供您使用。在`raw_images`目录中，您会找到一些用于测试的原始JPG图像。
- en: Running Local Serving
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行本地服务
- en: A prerequisite to serving the model is serialization of the model structure
    and its assets, such as weights and biases matrices. A trained TensorFlow model
    is typically saved in a `SavedModel` format. A `SavedModel` format consists of
    the complete TensorFlow program with weights, biases, and computation ops. This
    is done through the low-level `tf.saved_model` API.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 服务模型的先决条件是对模型结构及其资产（如权重和偏差矩阵）进行序列化。训练过的TensorFlow模型通常以`SavedModel`格式保存。`SavedModel`格式包含具有权重、偏差和计算操作的完整TensorFlow程序。这是通过低级`tf.saved_model`
    API完成的。
- en: 'Typically, when you execute a model training process using Fit, you end up
    with something like this:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在使用Fit进行模型训练过程时，您最终会得到类似这样的东西：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After you''ve executed the preceding code, you have a model object, `mdl`,
    that can be saved via the following syntax:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行了上述代码之后，您将拥有一个名为`mdl`的模型对象，可以通过以下语法保存：
- en: '[PRE6]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you take a look at the current directory, you will find a `saved_model.pb`
    file there.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看当前目录，您会在那里找到一个`saved_model.pb`文件。
- en: 'For your convenience, a `saved_model` file is provided for this exercise. In
    the `flowerclassifier/001` directory, you will find the following output:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，本练习提供了一个`saved_model`文件。在`flowerclassifier/001`目录中，您会找到以下输出：
- en: '[PRE8]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Notice that `save_model_path` is defined as `null`. This indicates that the
    model is to be saved in the current directory. If you have another directory that
    you want to use, you need to specify the full or relative path for that directory.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`save_model_path`被定义为`null`。这表示模型将保存在当前目录中。如果您有另一个要使用的目录，您需要为该目录指定完整或相对路径。
- en: '`saved_model.pb` is the Protobuf format of the model structure. The `assets`
    folder contains objects such as a vocabulary list or any lookup table, which are
    necessary for model execution. It may be empty if no such objects are created
    or required. The `variables` folder contains the weights and bias values as the
    result of training. These items constitute `SavedModel`. We are going to take
    a look at how to invoke `SavedModel` for scoring test data. Now let''s turn our
    attention to the Jupyter notebook in this chapter''s GitHub repository:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`saved_model.pb`是模型结构的 Protobuf 格式。`assets`文件夹包含词汇表或任何查找表等对象，这些是模型执行所必需的。如果没有创建或需要这些对象，它可能为空。`variables`文件夹包含训练结果的权重和偏差值。这些项目构成了`SavedModel`。我们将看看如何调用`SavedModel`对测试数据进行评分。现在，让我们关注本章
    GitHub 仓库中的 Jupyter notebook：'
- en: 'If you simply want to use a Python script to invoke `SavedModel`, it is very
    simple. All you need to do is load the model as follows:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你只是想使用 Python 脚本调用`SavedModel`，这非常简单。你需要做的就是按照以下方式加载模型：
- en: '[PRE11]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Each `SavedModel` has a default model signature that describes model inputs
    and outputs structures. This signature also has a name associated with it. We
    need to find out what this name is:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个`SavedModel`都有一个默认的模型签名，用于描述模型的输入和输出结构。该签名还关联了一个名称。我们需要找出这个名称是什么：
- en: '[PRE12]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Since the signature name is not specified during the save process, the output
    of the signature name is as follows:'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于在保存过程中没有指定签名名称，签名名称的输出如下：
- en: '[PRE13]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we need to create an inference object, `infer`, and then find the name
    for the model output as well as its shape, which are required when using this
    model to score test data:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建一个推理对象`infer`，然后找到模型输出的名称及其形状，这些在使用模型对测试数据进行评分时是必需的：
- en: '[PRE14]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will output the following:'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE15]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is named `custom_class`, and it is a tensor with a floating-point
    NumPy array of `shape=(None, 5)`. This indicates that the output is an array of
    probabilities for each of the five flower types. And the position index of the
    array with the highest probability is what we need to map to the flower type.
    We have seen this map in [*Chapter 7*](B16070_07_Final_JM_ePub.xhtml#_idTextAnchor200),
    *Model Optimization*, where we learned how to process TFRecord to build and train
    this model. This is the map:'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的名称为`custom_class`，它是一个形状为`shape=(None, 5)`的浮点型 NumPy 数组张量。这表示输出是一个五种花卉类型的概率数组。数组中概率最高的索引位置是我们需要映射到花卉类型的地方。我们在[*第七章*](B16070_07_Final_JM_ePub.xhtml#_idTextAnchor200)《模型优化》中看到了这个映射，在那里我们学习了如何处理
    TFRecord 来构建和训练这个模型。这个映射如下：
- en: '[PRE16]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If the highest probability is in the first position in the `custom_class` output's
    array, then the prediction is mapped to `roses`. If it is in the fifth position,
    then the prediction is mapped to `tulips`.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果`custom_class`输出数组中最高的概率在第一个位置，则预测结果映射为`roses`。如果它在第五个位置，则预测结果映射为`tulips`。
- en: 'Another thing we need to confirm is the shape of the input expected by the
    model. We may use `save_model_cli` to give us this information. We may execute
    this inline command in the Jupyter notebook cell:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个我们需要确认的是模型期望的输入形状。我们可以使用`save_model_cli`来获取这些信息。我们可以在 Jupyter notebook 单元格中执行以下内联命令：
- en: '[PRE17]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You will observe that the output of this command includes the following:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将看到该命令的输出包括以下内容：
- en: '[PRE18]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Notice the `shape` requirement. We know that (`224`, `224`, `3`) refers to the
    image dimensions. `-1` in the first dimension indicates this input is set up to
    handle multiple (batches) of (`224`, `224`, `3`) image arrays. Therefore, if we
    want to score one image, we need to expand that image by a dimension.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意`shape`的要求。我们知道（`224`，`224`，`3`）指的是图像的尺寸。第一维中的`-1`表示此输入设置为处理多个（批次）(`224`，`224`，`3`)图像数组。因此，如果我们想对一张图像进行评分，我们需要通过添加一个维度来扩展该图像。
- en: 'Let''s use a test image in the `raw_image` directory and read the image with
    the `nvision` library''s `imread`:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`raw_image`目录中的一张测试图像，并通过`nvision`库的`imread`读取图像：
- en: '[PRE19]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Notice that we only need to provide the height and width for resizing images
    to the correct pixel count in each dimension.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，我们只需要提供图像的高度和宽度，以便将图像调整到每个维度的正确像素数。
- en: 'Use the `infer` object to score this image:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`infer`对象对该图像进行评分：
- en: '[PRE20]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This produces the prediction for each of the five flower types, given `img1_np`:'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会为每个五种花卉类型生成预测结果，给定`img1_np`：
- en: '[PRE21]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This generates the following output:'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE22]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The highest probability occurs in the fifth position with a value of `9.9995959e-01`.
    Therefore, based on the aforementioned map in *step 3*, this image is mapped to
    `tulips`.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在第五个位置，最高的概率值为`9.9995959e-01`。因此，基于*步骤3*中提到的映射，这张图片被映射为`tulips`。
- en: We have seen how to use `SavedModel` for inference. This requires us to work
    in a Python runtime to load the model, read the image, and pass it to the model
    for scoring. In a production or application environment, however, the call to
    model is usually through TFS. In the next section, we are going to see how to
    make this model work with this approach.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过如何使用`SavedModel`进行推理。这要求我们在Python运行时中加载模型、读取图片，并将其传递给模型进行评分。然而，在生产或应用环境中，通常是通过TFS来调用模型。在下一节中，我们将了解如何让这个模型通过这种方式工作。
- en: Understanding TensorFlow Serving with Docker
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Docker了解TensorFlow Serving
- en: At the core of TFS is actually a TensorFlow model server that runs a model Protobuf
    file. Installing the model server is not straightforward, as there are many dependencies.
    As a convenience, the TensorFlow team also provides this model server in a Docker
    container, which is a platform that uses virtualization at the operating system
    level, and it is self-contained with all the necessary dependencies (that is,
    libraries or modules) to run in an isolated environment.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: TFS的核心实际上是一个运行模型Protobuf文件的TensorFlow模型服务器。安装模型服务器并不简单，因为它有很多依赖项。为了方便起见，TensorFlow团队还提供了一个包含所有必要依赖项（即库或模块）并在隔离环境中运行的Docker容器平台，这个平台在操作系统级别使用虚拟化。
- en: 'Therefore, the easiest way to deploy a TensorFlow `SavedModel` is by means
    of TFS with a Docker container. To install Docker, you can refer to the Docker
    site ([https://docs.docker.com/install/](https://docs.docker.com/install/)), along
    with the instructions for Mac, Windows, or Linux installations. For our chapter,
    a community version will suffice. We will be using Docker Desktop 2.4 running
    in macOS Catalina 10.15.6 with specs as indicated in *Figure 9.1*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，部署TensorFlow `SavedModel`最简单的方式是通过TFS和Docker容器。要安装Docker，你可以参考Docker网站的安装说明（[https://docs.docker.com/install/](https://docs.docker.com/install/)），包括Mac、Windows或Linux安装的具体指导。对于本章内容，社区版已经足够。我们将使用在macOS
    Catalina 10.15.6上运行的Docker Desktop 2.4，具体配置如*图9.1*所示：
- en: '![Figure 9.1 – The Docker version used for this chapter'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.1 – 本章使用的Docker版本'
- en: '](img/image0014.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image0014.jpg)'
- en: Figure 9.1 – The Docker version used for this chapter
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 本章使用的Docker版本
- en: It is assumed that you have installed Docker Desktop properly and that it is
    running. At a high level, we are going to download a TFS Docker image, add our
    model to it, and build a new Docker image on top of the base image, which is TFS.
    The final image is exposed through a TCP/IP port, which handles a RESTful API
    call from a client.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经正确安装并运行了Docker Desktop。在高层次上，我们将下载一个TFS Docker镜像，添加我们的模型，并在基础镜像TFS之上构建一个新的Docker镜像。最终的镜像通过TCP/IP端口暴露，用于处理来自客户端的RESTful
    API调用。
- en: Downloading TensorFlow Serving Docker images
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载TensorFlow Serving Docker镜像
- en: 'Once the Docker engine is up and running, you are ready to perform the following
    steps:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Docker引擎启动并运行，你就可以执行以下步骤：
- en: 'You may pull the latest TFS Docker image with this Docker command:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用以下Docker命令拉取最新的TFS Docker镜像：
- en: '[PRE23]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This is now our base image. In order to add our model on top of this image,
    we need to run this base image first:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这就是我们当前的基础镜像。为了在这个镜像上添加我们的模型，我们需要先运行这个基础镜像：
- en: '[PRE24]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the preceding command, we invoked the `tensorflow/serving` image and now
    it is running as a Docker container. We also name this container `serv_base_img`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的命令中，我们调用了`tensorflow/serving`镜像，并且现在它作为一个Docker容器在运行。我们还将这个容器命名为`serv_base_img`。
- en: Creating a new image with the model and serving it
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个包含模型的新镜像并提供服务
- en: 'Let''s now take a look at the file directory here. For this example, the directory
    structure is as shown in the following figure:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下这里的文件目录。对于这个示例，目录结构如下面的图所示：
- en: '![Figure 9.2 – Directory structure for creating a custom Docker container'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.2 – 创建自定义Docker容器的目录结构'
- en: '](img/image0021.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image0021.jpg)'
- en: Figure 9.2 – Directory structure for creating a custom Docker container
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 创建自定义Docker容器的目录结构
- en: We will execute the following commands from the same directory as `Tensorflow_Serving.ipynb`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在与`Tensorflow_Serving.ipynb`相同的目录下执行以下命令。
- en: 'After we have the TFS base Docker image up and running as a container, we are
    ready to put our own `SavedModel` into this container:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在将TFS基础Docker镜像启动并运行为容器后，我们准备将我们自己的`SavedModel`放入该容器：
- en: 'Basically, we have to copy our model into the TFS container''s `model` folder:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基本上，我们需要将模型复制到TFS容器的`model`文件夹中：
- en: '[PRE25]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now we commit our change to the base image and give the container a name that
    matches our model directory:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将更改提交到基础镜像，并为容器命名，使其与我们的模型目录匹配：
- en: '[PRE26]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We no longer require the base image to be running. Now we can just kill it:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不再需要运行基础镜像。现在，我们可以直接停止它：
- en: '[PRE27]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'To serve the image and score on our test image, we will run the following command:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了服务图像并对我们的测试图像进行评分，我们将运行以下命令：
- en: '[PRE28]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'At your command terminal, you will observe output such as that shown in the
    following figure:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的命令终端，您将看到如下图所示的输出：
- en: '![Figure 9.3 – Docker container running with a custom-built model'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.3 – Docker容器运行自定义构建的模型](img/image0033.jpg)'
- en: '](img/image0033.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.3 – Docker容器运行自定义构建的模型](img/image0033.jpg)'
- en: Figure 9.3 – Docker container running with a custom-built model
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – Docker容器运行自定义构建的模型
- en: 'Notice the following in the preceding screenshot: **Successfully loaded servable
    version {name: flowerclassifier version: 1}**.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意前面的截图中的以下内容：**成功加载可服务版本 {name: flowerclassifier version: 1}**。'
- en: This line indicates that the container successfully found our model, `flowerclassifier`,
    and the next directory in the naming hierarchy, `001`. This is a general pattern
    that TFS needs in order to make TFS work with a custom model that you have built.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这一行表示容器成功找到了我们的模型`flowerclassifier`，以及命名层级中的下一个目录`001`。这是TFS所需的一种通用模式，以便与您构建的自定义模型一起使用TFS。
- en: Now the model is served. In the next section, we will see how to build our client
    that calls this model using the Python JSON library.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已被服务。在接下来的部分，我们将看到如何构建调用该模型的客户端，使用Python的JSON库。
- en: Scoring through the RESTful API
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过RESTful API进行评分
- en: 'Now let''s return to our Jupyter environment. We will see how to pick up from
    what we did in Local Serving and continue from there. Recall that we used the
    `nvision` library to normalize and standardize our test image. We also need to
    expand the image dimension because the model expects to have a batch dimension
    in our input. After we have performed these steps as in Local Serving, we will
    have `img1_np` as the properly shaped NumPy array. Let''s build this array into
    a JSON payload, and pass the payload to our model through the RESTful API with
    the help of the following steps:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们返回到Jupyter环境。我们将看到如何从我们在本地服务中所做的工作继续进行。回想一下，我们使用`nvision`库来规范化和标准化我们的测试图像。我们还需要扩展图像的维度，因为模型期望在输入中有一个批次维度。完成这些步骤后，正如在本地服务中一样，我们将得到一个正确形状的NumPy数组`img1_np`。现在让我们将这个数组构建成一个JSON负载，并通过RESTful
    API将负载传递给我们的模型，按照以下步骤进行：
- en: 'We will build the JSON payload with the following command:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用以下命令构建JSON负载：
- en: '[PRE29]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the preceding code, we converted the test image into a JSON payload format
    and we defined a header for our RESTful API call to indicate that the payload
    is in JSON format for the application to consume.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将测试图像转换为JSON负载格式，并为我们的RESTful API调用定义了一个头部，指示负载是JSON格式，供应用程序使用。
- en: As per TFS, the payload must encode the scoring data with a key-value pair by
    the key name of the instances and the NumPy array to be converted to a list as
    the input. We also need a header to be defined for the JSON payload as well.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据TFS的要求，负载必须通过实例的键名对和要转换为列表的NumPy数组来编码评分数据作为输入。我们还需要为JSON负载定义一个头部。
- en: 'We will score our test image, `img1_np`, for this data with the following command:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将用以下命令为这个数据评分，使用我们的测试图像`img1_np`：
- en: '[PRE30]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The preceding command will produce a `response` payload back from our TFS container.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上面的命令会从TFS容器返回一个`response`负载。
- en: 'We will examine the `response` payload by using the following command:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用以下命令检查`response`负载：
- en: '[PRE31]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following is the output of the preceding command:'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是前面命令的输出：
- en: '[PRE32]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This is a dictionary containing the key prediction and the probability values
    for each of the five flower types. These values are identical to what we saw in
    Local Serving. Therefore, we know that the model is correctly served via TFS using
    a Docker container.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个字典，包含键`prediction`以及每种花卉类型的概率值。这些值与我们在本地服务中看到的相同。因此，我们知道该模型已经通过TFS在Docker容器中正确服务。
- en: Summary
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned how to deploy a TensorFlow `SavedModel`. This is
    by no means the most common method to use in enterprise deployment. In an enterprise
    deployment scenario, many factors determine how the deployment pipeline should
    be built, and depending on the use cases, it can quickly diverge in terms of deployment
    patterns and choices from there. For example, some organizations use AirFlow as
    their orchestration tool, and some may prefer KubeFlow, while many others still
    use Jenkins.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你学习了如何部署TensorFlow `SavedModel`。这绝不是企业部署中最常用的方法。在企业部署场景中，许多因素决定了部署管道应该如何构建，并且根据用例的不同，从那里开始，部署模式和选择可能会迅速发生变化。例如，一些组织使用AirFlow作为其编排工具，而有些可能更喜欢KubeFlow，另外还有很多组织仍然使用Jenkins。
- en: The goal of this book is to show you how to leverage the latest and most reliable
    implementation of TensorFlow Enterprise from a data scientist/machine learning
    model builder's perspective.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的目标是从数据科学家/机器学习模型构建者的角度，向你展示如何利用最新且最可靠的TensorFlow Enterprise实现。
- en: From here, depending on your interests or priorities, you may take up what you
    learned in this book and pursue many other topics, such as MLOps, model orchestration,
    drift monitoring, and redeployment. These are some of the important topics in
    any enterprise machine learning discussions from a use case perspective. Use cases,
    IT infrastructure, and business considerations typically determine how a model
    is actually served. Further considerations include what kind of service-level
    agreement the serving pipeline has to meet, and the security and compliance issues
    related to authentication and model safety.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，依据你的兴趣或优先事项，你可以根据本书中的学习内容，继续探索其他主题，如MLOps、模型编排、漂移监控和重新部署。这些是任何企业机器学习讨论中，从用例角度来看，一些重要的议题。用例、IT基础设施和业务考量通常决定了一个模型如何实际提供服务。进一步的考量包括服务管道必须满足什么样的服务水平协议，以及与身份验证和模型安全相关的安全性和合规性问题。
