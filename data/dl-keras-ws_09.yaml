- en: 9\. Sequential Modeling with Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9\. 使用循环神经网络进行序列建模
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter will introduce you to sequential modeling—creating models to predict
    the next value or series of values in a sequence. By the end of this chapter,
    you will be able to build sequential models, explain `RNNs` with `LSTM` architectures
    to predict the value of the future stock price value of Alphabet and Amazon.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍序列建模——创建模型来预测序列中的下一个值或一系列值。在本章结束时，你将能够构建序列模型，解释`RNN`与`LSTM`架构，并预测Alphabet和Amazon未来股价的值。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapter, we learned about pre-trained networks and how to utilize
    them for our own applications via transfer learning. We experimented with `VGG16`
    and `ResNet50`, two pre-trained networks that are used for image classification,
    and used them to classify new images and fine-tune them for our own applications.
    By utilizing pre-trained networks, we were able to train more accurate models
    quicker than the convolutional neural networks we trained in previous chapters.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，我们学习了预训练网络以及如何通过迁移学习将它们应用于我们自己的应用。我们实验了`VGG16`和`ResNet50`，这两种用于图像分类的预训练网络，并利用它们对新图像进行分类，并为我们的应用进行微调。通过利用预训练网络，我们能够比在之前的章节中训练的卷积神经网络更快地训练出更准确的模型。
- en: In traditional neural networks (and every neural network architecture covered
    in prior chapters), data passes sequentially through the network from the input
    layer, and through the hidden layers (if any), to the output layer. Information
    passes through the network once and the outputs are considered independent of
    each other, and only dependent on the inputs to the model. However, there are
    instances where a particular output is dependent on the previous output of the
    system.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统神经网络（以及前几章中介绍的所有神经网络架构）中，数据从输入层开始，顺序通过网络，经过隐藏层（如果有的话），直到输出层。信息只通过一次网络，输出被认为是相互独立的，只依赖于模型的输入。然而，在某些情况下，特定的输出依赖于系统之前的输出。
- en: 'Consider the stock price of a company as an example: the output at the end
    of any given day is related to the output of the previous day. Similarly, in **Natural
    Language Processing** (**NLP**), the final words in a sentence are highly dependent
    on the previous words in the sentence if the sentence is to make grammatical sense.
    NLP is a specific application of sequential modeling in which the dataset being
    processed and analyzed is natural language data. A special type of neural network,
    called a **Recurrent Neural Network** (**RNN**), is used to solve these types
    of problems where the network needs to remember previous outputs.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以公司的股价为例：某一天结束时的输出与前一天的输出相关。类似地，在**自然语言处理**（**NLP**）中，为了使句子语法正确，句子中的最后几个词依赖于前面出现的词。如果要使句子合乎语法，最终的词语往往与前面的词语高度相关。NLP是序列建模的一个特殊应用，在该应用中，处理和分析的数据集是自然语言数据。为了解决这些类型的问题，使用了一种特殊类型的神经网络——**循环神经网络**（**RNN**）。RNN能够记住之前的输出，并用于处理这类问题。
- en: This chapter introduces and explores the concepts and applications of RNNs.
    It also explains how RNNs are different from standard feedforward neural networks.
    You will also gain an understanding of the vanishing gradient problem and **Long-Short-Term-Memory**
    (**LSTM**) networks. This chapter also introduces you to sequential data and how
    it's processed. We will be working with share market data for stock price forecasting
    to learn all about these concepts.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍并探讨了RNN的概念和应用。还解释了RNN如何与标准的前馈神经网络不同。你还将理解梯度消失问题以及**长短期记忆**（**LSTM**）网络。本章还将介绍序列数据及其处理方式。我们将通过使用股市数据进行股价预测来学习这些概念。
- en: Sequential Memory and Sequential Modeling
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列记忆与序列建模
- en: 'If we analyze the stock price of **Alphabet** for the past 6 months, as shown
    in the following screenshot, we can see that there is a trend. To predict or forecast
    future stock prices, we need to gain an understanding of this trend and then do
    our mathematical computations while keeping this trend in mind:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们分析**Alphabet**过去6个月的股价，如下图所示，我们可以看到一个趋势。为了预测或预报未来的股价，我们需要理解这个趋势，然后在进行数学计算时牢记这一趋势：
- en: '![Figure 9.1: Alphabet''s stock price over the last 6 months'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.1：Alphabet过去6个月的股价'
- en: '](img/B15777_09_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_09_01.jpg)'
- en: 'Figure 9.1: Alphabet''s stock price over the last 6 months'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：Alphabet过去6个月的股价
- en: This trend is deeply related to sequential memory and sequential modeling. If
    you have a model that can remember the previous outputs and then predict the next
    output based on the previous outputs, we say that the model has sequential memory.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这一趋势与顺序记忆和顺序建模密切相关。如果你有一个可以记住之前输出并基于这些输出预测下一个输出的模型，我们说这个模型具有顺序记忆。
- en: The modeling that is done to process this sequential memory is known as **sequential
    modeling**. This is not only true for stock market data, but it is also true in
    NLP applications; we will look at one such example in the next section when we
    study RNNs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这种顺序记忆的建模过程被称为 **顺序建模**。这不仅适用于股市数据，也适用于自然语言处理（NLP）应用；我们将在下一节研究 `RNNs` 时看到一个这样的例子。
- en: Recurrent Neural Networks (RNNs)
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络（RNNs）
- en: '`RNNs` are a class of neural networks that are built on the concept of sequential
    memory. Unlike traditional neural networks, an `RNN` predicts the results in sequential
    data. Currently, an `RNN` is the most robust technique that''s available for processing
    sequential data.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`RNNs` 是一种基于顺序记忆概念构建的神经网络。与传统神经网络不同，`RNN` 预测顺序数据中的结果。目前，`RNN` 是处理顺序数据的最强大技术。'
- en: 'If you have access to a smartphone that has Google Assistant, try opening it
    and asking the question: "When was the United Nations formed?" The answer is displayed
    in the following screenshot:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一部能使用 Google Assistant 的智能手机，试着打开它并问：“联合国是什么时候成立的？” 答案将在以下截图中显示：
- en: '![Figure 9.2: Google Assistant''s output'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.2：Google Assistant 的输出'
- en: '](img/B15777_09_02.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_09_02.jpg)'
- en: 'Figure 9.2: Google Assistant''s output'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：Google Assistant 的输出
- en: 'Now, ask a second question, "Why was it formed?", as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，问第二个问题：“为什么它被成立？”，如下：
- en: '![Figure 9.3: Google Assistant''s contextual output'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.3：Google Assistant 的上下文输出'
- en: '](img/B15777_09_03.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_09_03.jpg)'
- en: 'Figure 9.3: Google Assistant''s contextual output'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3：Google Assistant 的上下文输出
- en: 'Now, ask the third question, "Where are its headquarters?", and you should
    get the following answer:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，问第三个问题：“它的总部在哪里？”，你应该得到如下答案：
- en: '![Figure 9.4: Google Assistant''s output'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.4：Google Assistant 的输出'
- en: '](img/B15777_09_04.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_09_04.jpg)'
- en: 'Figure 9.4: Google Assistant''s output'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4：Google Assistant 的输出
- en: One interesting thing to note here is that we only mentioned the "United Nations"
    in the first question. In the second and third questions, we simply asked the
    assistant `why it was formed` and `where the headquarters were`, respectively.
    Google Assistant understood that since the previous question was about the United
    Nations, the next questions were also in the context of the United Nations. This
    is not a simple thing for a machine.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个有趣的事情值得注意，我们在第一个问题中只提到了“联合国”。在第二和第三个问题中，我们分别问了助手 `为什么它被成立` 和 `总部在哪里`。Google
    Assistant 理解到，由于前一个问题是关于联合国的，接下来的问题也与联合国相关。这对于机器来说并不是一件简单的事情。
- en: The machine was able to show the expected result because it had processed data
    in the form of a sequence. The machine understands that the current question is
    related to the previous question, and so, essentially, it remembers the previous question.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 机器能够显示预期结果，因为它已经以序列的形式处理了数据。机器明白当前的问题与前一个问题有关，因此本质上，它记住了之前的问题。
- en: 'Let''s consider another simple example. Say that we want to predict the next
    number in the following sequence: `7`, `8`, `9`, and `?`. We want the next output
    to be `9` + `1`. Alternatively, if we provide the sequence, `3`, `6`, `9`, and
    `?` we would like to get `9` + `3` as the output. While in both cases the last
    number is `9`, the prediction outcome should be different (that is, when we take
    into account the contextual information of the previous values and not only the
    last value). The key here is to remember the contextual information that was obtained
    from the previous values.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑另一个简单的例子。假设我们想要预测以下序列中的下一个数字：`7`、`8`、`9` 和 `?`。我们希望下一个输出是 `9` + `1`。另外，如果我们提供序列
    `3`、`6`、`9` 和 `?`，我们希望得到 `9` + `3` 作为输出。虽然在这两种情况下，最后一个数字都是 `9`，但是预测结果应该是不同的（也就是说，当我们考虑到前一个值的上下文信息，而不仅仅是最后一个值时）。这里的关键是记住从前一个值中获取的上下文信息。
- en: 'At a high level, such networks that can remember previous states are referred
    to as recurrent networks. To completely understand `RNNs`, let''s revisit the
    traditional neural networks, also known as `feedforward neural networks`. This
    is a neural network in which the connections of the neural network do not form
    cycles; that is, the data only flows in one direction, as shown in the following
    diagram:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，这些能够记住先前状态的网络被称为递归网络。为了完全理解`RNN`，我们先回顾一下传统的神经网络，也就是`前馈神经网络`。这是一种神经网络，其中神经网络的连接不形成循环；也就是说，数据仅沿一个方向流动，如下图所示：
- en: '![Figure 9.5: A feedforward neural network'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.5：一个前馈神经网络'
- en: '](img/B15777_09_05.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_09_05.jpg)'
- en: 'Figure 9.5: A feedforward neural network'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：一个前馈神经网络
- en: In a `feedforward neural network`, such as the one shown in the preceding diagram,
    the input layer (the green circles on the left) gets the data and passes it to
    a hidden layer (with weights, illustrated by the blue circles in the middle).
    Later, the data from the hidden layer is passed to the output layer (illustrated
    by the red circle on the right). Based on the thresholds, the data is backpropagated,
    but there is no cyclical flow of data in the hidden layers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个`前馈神经网络`中，如上图所示，输入层（左侧的绿色圆圈）接收数据并将其传递给隐藏层（带权重的蓝色圆圈）。然后，隐藏层的数据传递给输出层（右侧的红色圆圈）。根据阈值，数据会进行反向传播，但在隐藏层中没有数据的循环流动。
- en: 'In an `RNN`, the hidden layer of the network allows the cycle of data and information.
    As shown in the following diagram, the architecture is similar to a feedforward
    neural network; however, here, the data and information also flow in cycles:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在`RNN`中，网络的隐藏层允许数据和信息的循环。如下图所示，结构类似于前馈神经网络；然而，在这里，数据和信息也在循环流动：
- en: '![Figure 9.6: An RNN'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.6：一个RNN'
- en: '](img/B15777_09_06.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_09_06.jpg)'
- en: 'Figure 9.6: An RNN'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：一个RNN
- en: 'Here, the defining property of the `RNN` is that the hidden layer not only
    gives the output, but it also feeds back the information of the output into itself.
    Before taking a deep dive into RNNs, let''s discuss why we need `RNNs` and why
    `Convolutional Neural Networks` (`CNNs`) or normal `Artificial Neural Networks`
    (`ANNs`) fall short when it comes to processing sequential data. Suppose that
    we are using a `CNN` to identify images; first, we input an image of a dog, and
    the `CNN` will label the image as "dog". Then, we input an image of a mango, and
    the CNN will label the image as "mango". Let''s input the image of the dog at
    time `t`, as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`RNN`的定义特性是隐藏层不仅给出输出，而且还将输出的信息反馈到自身。在深入了解RNN之前，我们先讨论一下为什么我们需要`RNN`，以及为什么`卷积神经网络`（`CNN`）或普通的`人工神经网络`（`ANN`）在处理序列数据时存在不足。假设我们正在使用`CNN`来识别图像；首先，我们输入一张狗的图片，`CNN`会将该图像标记为“狗”。然后，我们输入一张芒果的图片，`CNN`会将该图像标记为“芒果”。我们假设在时间`t`输入狗的图片，如下所示：
- en: '![Figure 9.7: An image of a dog with a CNN'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.7：一个使用CNN的狗的图像'
- en: '](img/B15777_09_07.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_09_07.jpg)'
- en: 'Figure 9.7: An image of a dog with a CNN'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：一个使用CNN的狗的图像
- en: 'Now, let''s input the image of the mango at time `t + 1`, as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们输入时间`t + 1`的芒果图像，如下所示：
- en: '![Figure 9.8: An image of a mango with a CNN'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.8：一个使用CNN的芒果图像'
- en: '](img/B15777_09_08.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_09_08.jpg)'
- en: 'Figure 9.8: An image of a mango with a CNN'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8：一个使用CNN的芒果图像
- en: Here, you can clearly see that the output at time `t` for the dog image and
    the output at time `t + 1` for the mango image are totally independent of each
    other. Therefore, we don't need our algorithms to remember previous instances
    of the output. However, as we mentioned in the Google Assistant example where
    we asked `when the United Nations was formed` and `why it was formed`, the output
    of the previous instance has to be remembered by the algorithm for it to process
    the sequential data. `CNNs` or `ANNs` are not able to do this, so we need to use
    `RNNs` instead.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以清楚地看到，时间`t`的狗图像输出和时间`t + 1`的芒果图像输出是完全独立的。因此，我们不需要我们的算法记住先前的输出实例。然而，正如我们在Google助手的例子中提到的，当我们问“联合国何时成立”和“为什么成立”时，算法必须记住先前实例的输出，才能处理序列数据。`CNN`或`ANN`无法做到这一点，因此我们需要使用`RNN`。
- en: 'In an `RNN`, we can have multiple outputs over multiple instances of time.
    The following diagram is a pictorial representation of an `RNN`. It represents
    the state of the network from time `t – 1` to time `t + n`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在`RNN`中，我们可以在多个时间点上得到多个输出。下面的图示表示一个`RNN`的示意图。它展示了网络从时间`t – 1`到时间`t + n`的状态：
- en: '![Figure 9.9: An unfolded RNN at various timestamps'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.9：在不同时间戳下展开的RNN'
- en: '](img/B15777_09_09.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_09_09.jpg)'
- en: 'Figure 9.9: An unfolded RNN at various timestamps'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9：在不同时间戳下展开的RNN
- en: There are some issues that you may face when training `RNNs` that are related
    to the unique architecture of `RNNs`. They concern the value of the gradient because,
    as the depth of the `RNN` increases, the gradient can either vanish or explode,
    as we will learn in the next section.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练`RNN`时，你可能会遇到一些与`RNN`独特架构相关的问题。这些问题涉及梯度的值，因为随着`RNN`深度的增加，梯度可能会消失或爆炸，正如我们将在下一节中学习的那样。
- en: The Vanishing Gradient Problem
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消失梯度问题
- en: If someone asks you "What did you have for dinner last night?", it is pretty
    easy to remember and correctly answer them. Now, if someone asks you "What did
    you have for dinner over the past 30 days?", then you might be able to remember
    the menu of the past 3 or 4 days, but then the menu for the days before that will
    be a bit difficult to remember. This ability to recall information from the past
    is the basis of the vanishing gradient problem, which we will be studying in this
    section. Put simply, the vanishing gradient problem refers to information that
    is lost or has decayed over a period of time.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有人问你：“你昨晚吃了什么？”你可能很容易记住并正确回答他们。现在，如果有人问你：“过去30天你吃了什么？”你可能能记住过去3到4天的菜单，但之前的菜单就很难记得清楚了。能够回忆过去的信息是消失梯度问题的基础，我们将在本节中研究这个问题。简而言之，消失梯度问题指的是在一段时间内丢失或衰减的信息。
- en: 'The following diagram represents the state of the `RNN` at different instances
    of time `t`. The top dots (in red) represent the output layer, the middle dots
    (in blue) represent the hidden layer, and the bottom dots (in green) represent
    the input layer:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示表示`RNN`在不同时间点`t`的状态。顶部的点（红色）表示输出层，中间的点（蓝色）表示隐藏层，底部的点（绿色）表示输入层：
- en: '![Figure: 9.10: Information decaying over time'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.10：信息随时间衰减'
- en: '](img/B15777_09_10.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_09_10.jpg)'
- en: 'Figure: 9.10: Information decaying over time'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10：信息随时间衰减
- en: 'If you are at `t + 10`, it will be difficult for you to remember what dinner
    menu you had at time `t` (which is 10 days prior to the current day). Additionally,
    if you are at `t + 100`, it is likely to be impossible for you to remember your
    dinner menu prior to 100 days, assuming that there is no pattern to the dinner
    you choose to make. In the context of machine learning, the vanishing gradient
    problem is a difficulty that is found when training ANNs using gradient-based
    learning methods and backpropagation. Let''s recall how a neural network works,
    as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你处在`t + 10`时刻，你很难记得10天前（即时间`t`）的晚餐菜单。此外，如果你处在`t + 100`时刻，你很可能根本记不起100天前的晚餐菜单，假设你做的晚餐没有规律可循。在机器学习的上下文中，消失梯度问题是在使用基于梯度的学习方法和反向传播训练ANN时遇到的一个困难。让我们回顾一下神经网络是如何工作的，如下所示：
- en: First, we initialize the network with random weights and bias values.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们用随机的权重和偏置值初始化网络。
- en: We get a predicted output; this output is compared with the actual output and
    the difference is known as the cost.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们得到一个预测输出；这个输出与实际输出进行比较，二者的差异被称为代价。
- en: The training process utilizes a gradient, which measures the rate at which the
    cost changes with respect to the weights or biases.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练过程利用梯度，它衡量的是代价相对于权重或偏置变化的速率。
- en: Then, we try to lower the cost by adjusting the weights and biases repeatedly
    throughout the training process, until the lowest possible value is obtained.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们通过在训练过程中反复调整权重和偏置，尽可能降低代价，直到获得最低的可能值。
- en: For example, if you place a ball on a steep floor, then the ball will roll quickly;
    however, if you place the ball on a flat surface, it will roll slowly, or not
    at all. Similarly, in a deep neural network, the model learns quickly when the
    gradient is large. However, if the gradient is small, then the model's learning
    rate becomes very low. Remember that, at any point, the gradient is the product
    of all the gradients up to that point (that is, it follows the calculus chain
    rule).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你把一个球放在陡坡上，球会迅速滚动；然而，如果你把球放在平坦的表面上，它会滚得很慢，甚至不滚动。类似地，在深度神经网络中，当梯度较大时，模型学习得很快。然而，如果梯度较小，模型的学习速度就会变得非常慢。记住，在任何时刻，梯度是到该点为止所有梯度的乘积（即遵循微积分链式法则）。
- en: 'Additionally, the gradient is usually a small number between `0` and `1`, and
    the product of two numbers between `0` and `1` gives you an even smaller number.
    The deeper your network is, the smaller the gradient is in the initial layers
    of the network. In some cases, it reaches a point that is so small that no training
    happens in that network; this is the vanishing gradient problem. The following
    diagram shows the gradients following the calculus chain rule:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，梯度通常是一个介于 `0` 和 `1` 之间的小数，而两个介于 `0` 和 `1` 之间的数字相乘会得到一个更小的数字。你的网络越深，网络初始层的梯度就越小。在某些情况下，它会变得非常小，以至于网络根本无法进行训练；这就是梯度消失问题。下图展示了遵循微积分链式法则的梯度：
- en: '![Figure 9.11: The vanishing gradient with cost, C, and the calculus chain
    rule'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.11：带有成本C和微积分链式法则的梯度消失'
- en: '](img/B15777_09_11.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_09_11.jpg)'
- en: 'Figure 9.11: The vanishing gradient with cost, C, and the calculus chain rule'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11：带有成本C和微积分链式法则的梯度消失
- en: Referring to *Figure 9.10*, suppose that we are at the `t + 10` instance and
    we get an output that will be backpropagated to `t`, which is 10 steps away. Now,
    when the weight is updated, there will be 10 gradients (which are themselves very
    small), and when they multiply by each other, the number becomes so small that
    it is almost negligible. This is known as the vanishing gradient.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 参见*图9.10*，假设我们处于 `t + 10` 时刻，我们得到一个输出，这个输出将反向传播到 `t`，即10步之遥。现在，当权重被更新时，会有10个梯度（它们本身就非常小），当它们相乘时，结果变得非常小，以至于几乎可以忽略不计。这就是梯度消失问题。
- en: A Brief Explanation of the Exploding Gradient Problem
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度爆炸问题的简要解释
- en: If instead of the weights being small, the weights are greater than `1`, then
    the subsequent multiplication will increase the gradient exponentially; this is
    known as the exploding gradient. The exploding gradient is simply the opposite
    of the vanishing gradient as in the case of the vanishing gradient, the values
    become too small, while in the case of the exploding gradient, the values become
    very large. As a result, the network suffers heavily and is unable to predict
    anything. We don't get the exploding gradient problem as frequently as vanishing
    gradients, but it is good to have a brief understanding of what exploding gradients
    are.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果权重不是很小，而是大于 `1`，那么随后的乘法将使梯度指数级增长；这就是梯度爆炸问题。梯度爆炸实际上是梯度消失的对立面，梯度消失时值变得太小，而梯度爆炸时值变得非常大。结果，网络会受到严重影响，无法做出任何预测。虽然梯度爆炸问题比梯度消失问题少见，但了解梯度爆炸是什么还是有必要的。
- en: There are some approaches we take to overcome the challenges that are faced
    with the vanishing or exploding gradient problem. The one approach that we will
    learn about is `Long Short-Term Memory`, which overcomes issues with the gradients
    by having memory about information for long periods of time.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些方法可以帮助我们克服在面临梯度消失或梯度爆炸问题时遇到的挑战。我们将在这里学习的一种方法是`长短时记忆（LSTM）`，它通过记住长期的信息来克服梯度问题。
- en: Long Short-Term Memory (LSTM)
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短时记忆（LSTM）
- en: '`LSTMs` are `RNNs` whose main objective is to overcome the shortcomings of
    the vanishing gradient and exploding gradient problems. The architecture is built
    so that they remember data and information for a long period of time.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`LSTM` 是 `RNN` 的一种，主要目标是克服梯度消失和梯度爆炸问题的缺点。它的架构设计使得它能够长时间记住数据和信息。'
- en: '`LSTMs` were designed to overcome the limitation of the vanishing and exploding
    gradient problems. `LSTM` networks are a special kind of `RNN` that are capable
    of learning long-term dependencies. They are designed to avoid the long-term dependency
    problem; being able to remember information for long intervals of time is how
    they are wired. The following diagram displays a standard recurrent network where
    the repeating module has a `tanh activation` function. This is a simple `RNN`.
    In this architecture, we often have to face the vanishing gradient problem:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`LSTM` 旨在克服消失梯度和爆炸梯度问题的限制。`LSTM` 网络是一种特殊的 `RNN`，能够学习长期依赖关系。它们被设计来避免长期依赖问题；能够记住长时间间隔的信息就是它们的设计方式。下图展示了一个标准的递归网络，其中重复模块具有
    `tanh 激活` 函数。这是一个简单的 `RNN`。在这种架构中，我们通常需要面对消失梯度问题：'
- en: '![Figure 9.12: A simple RNN model'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.12：一个简单的 RNN 模型'
- en: '](img/B15777_09_12.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_09_12.jpg)'
- en: 'Figure 9.12: A simple RNN model'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12：一个简单的 RNN 模型
- en: 'The `LSTM` architecture is similar to simple `RNNs`, but their repeating module
    has different components, as shown in the following diagram:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`LSTM` 架构与简单的 `RNN` 相似，但它们的重复模块包含不同的组件，如下图所示：'
- en: '![Figure 9.13: The LSTM model architecture'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.13：LSTM 模型架构'
- en: '](img/B15777_09_13.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_09_13.jpg)'
- en: 'Figure 9.13: The LSTM model architecture'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13：LSTM 模型架构
- en: 'In addition to a simple `RNN`, an `LSTM` consists of the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 除了简单的 `RNN`，`LSTM` 还包括以下内容：
- en: '`Sigmoid activation` functions (`σ`)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Sigmoid 激活` 函数（`σ`）'
- en: Mathematical computational functions (the black circles with + and x)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数学计算功能（带有 + 和 x 的黑色圆圈）
- en: 'Gated cells (or gates):'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有门控单元（或称为门）：
- en: '![Figure 9.14: An LSTM in detail'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.14：LSTM 详细解析'
- en: '](img/B15777_09_14.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_09_14.jpg)'
- en: 'Figure 9.14: An LSTM in detail'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14：LSTM 详细解析
- en: The main difference between a simple `RNN` and an `LSTM` is the presence of
    gated cells. You can think of gates as computer memory, where information can
    be written, read, or stored. The preceding diagram shows a detailed image of an
    `LSTM`. The cells in the gates (represented by the black circles) make decisions
    on what to store and when to allow values to be read or written. The gates accept
    any information from `0` to `1`; that is, if it is 0, then the information is
    blocked; if it is `1`, then all the information flows through. If the input is
    between `0` and `1`, then only partial information flows.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的 `RNN` 和 `LSTM` 之间的主要区别是是否存在门控单元。你可以将门视为计算机内存，在内存中可以进行信息的写入、读取或存储。前面的图展示了
    `LSTM` 的详细图像。门中的单元（由黑色圆圈表示）决定了存储什么信息以及何时允许读取或写入值。这些门接受从 `0` 到 `1` 的任何信息；也就是说，如果是
    0，那么信息会被阻止；如果是 `1`，则所有信息都会流通。如果输入值介于 `0` 和 `1` 之间，则仅部分信息会流通。
- en: 'Besides these input gates, the gradient of a network is dependent on two factors:
    weight and the activation function. The gates decide which piece of information
    needs to persist within the `LSTM` cell and which needs to be forgotten or deleted.
    In this way, the gates are like water valves; that is, the network can select
    which valve will allow the water to flow and which valve won''t allow the water
    to flow.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些输入门，网络的梯度还依赖于两个因素：权重和激活函数。门决定了哪些信息需要在 `LSTM` 单元中保持，哪些需要被遗忘或删除。这样，门就像水阀一样；也就是说，网络可以选择哪个阀门允许水流通，哪个阀门不允许水流通。
- en: The valves are adjusted in such a way that the values of the output will never
    yield a gradient (vanishing or exploding) problem. For example, if the value becomes
    too large, then there is a forget gate that will forget the value and no longer
    consider it for computations. Essentially, what a forget gate does is multiply
    the information by `0` or `1`. If the information needs to be processed further,
    then the forget gate multiplies the information by `1`, and if it needs to be
    forgotten, then it multiplies the information by `0`. Each gate is assisted by
    a sigmoid function that squashes the information between `0` and `1`. For us to
    gain a better understanding of this, let's take a look at some activities and
    exercises.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这些阀门的调整方式使得输出值永远不会导致梯度（消失或爆炸）问题。例如，如果值变得过大，那么会有一个遗忘门将忘记该值，并不再将其考虑用于计算。遗忘门的作用本质上是将信息乘以
    `0` 或 `1`。如果信息需要进一步处理，遗忘门将信息乘以 `1`，如果需要忘记，则将信息乘以 `0`。每个门都由一个 sigmoid 函数辅助，该函数将信息压缩到
    `0` 和 `1` 之间。为了更好地理解这一点，我们来看一些活动和练习。
- en: Note
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: All the activities and exercises in this chapter will be developed in Jupyter
    notebooks. You can download this book's GitHub repository, along with all the
    prepared templates, at [https://packt.live/2vtdA8o](https://packt.live/2vtdA8o).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有活动和练习将在 Jupyter notebook 中进行。您可以在[https://packt.live/2vtdA8o](https://packt.live/2vtdA8o)下载本书的
    GitHub 仓库，并获取所有准备好的模板。
- en: 'Exercise 9.01: Predicting the Trend of Alphabet''s Stock Price Using an LSTM
    with 50 Units (Neurons)'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 9.01：使用 50 个单元（神经元）的 LSTM 预测 Alphabet 股票价格的趋势
- en: 'In this exercise, we will examine the stock price of Alphabet over a period
    of 5 years—that is, from January 1, 2014, to December 31, 2018\. In doing so,
    we will try to predict and forecast the company''s future trend for January 2019
    using `RNNs`. We have the actual values for January 2019, so we will be able to
    compare our predictions with the actual values later. Follow these steps to complete
    this exercise:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将研究 Alphabet 股票在 5 年期间的价格——即从 2014 年 1 月 1 日到 2018 年 12 月 31 日。在此过程中，我们将尝试使用`RNNs`预测并预测公司
    2019 年 1 月的未来趋势。我们拥有 2019 年 1 月的实际值，因此稍后我们将能够将我们的预测与实际值进行比较。按照以下步骤完成本练习：
- en: 'Import the required libraries:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE0]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the dataset using the pandas `read_csv` function and look at the first
    five rows of the dataset using the `head` method:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 pandas 的 `read_csv` 函数导入数据集，并使用 `head` 方法查看数据集的前五行：
- en: '[PRE1]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following figure shows the output of the preceding code:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下图展示了前述代码的输出：
- en: '![Figure 9.15: The first five rows of the GOOG_Training dataset'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.15：GOOG_Training 数据集的前五行'
- en: '](img/B15777_09_15.jpg)'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15777_09_15.jpg)'
- en: 'Figure 9.15: The first five rows of the GOOG_Training dataset'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.15：GOOG_Training 数据集的前五行
- en: 'We are going to make our prediction using the `Open` stock price; therefore,
    select the `Open` stock price column from the dataset and print the values:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用 `Open` 股票价格进行预测；因此，从数据集中选择 `Open` 股票价格列并打印其值：
- en: '[PRE2]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding code produces the following output:'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前述代码生成了以下输出：
- en: '[PRE3]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, perform feature scaling by normalizing the data using `MinMaxScaler`
    and setting the range of the features so that they have a minimum value of `0`
    and a maximum value of one. Use the `fit_transform` method of the scaler on the
    training data:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，通过使用 `MinMaxScaler` 进行数据标准化来执行特征缩放，并设置特征范围，使它们的最小值为 `0`，最大值为 `1`。在训练数据上使用缩放器的
    `fit_transform` 方法：
- en: '[PRE4]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding code produces the following output:'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前述代码生成了以下输出：
- en: '[PRE5]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Create the data to get 60 timestamps from the current instance. We chose `60`
    here as this will give us a sufficient number of previous instances so that we
    can understand the trend; technically, this can be any number, but `60` is the
    optimal value. Additionally, the upper bound value here is `1258`, which is the
    index or count of rows (or records) in the training set:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建数据以获取当前实例的 60 个时间戳。我们在这里选择了`60`，因为这样可以提供足够数量的前实例，帮助我们理解趋势；从技术上讲，这个值可以是任何数字，但`60`是最优值。此外，这里的上限值是`1258`，即训练集中的行数（或记录数）的索引或计数：
- en: '[PRE6]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, reshape the data to add an extra dimension to the end of `X_train` using
    NumPy''s `reshape` function:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用 NumPy 的 `reshape` 函数调整数据形状，为 `X_train` 的末尾添加一个额外的维度：
- en: '[PRE7]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following figure shows the output of the preceding code:'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下图展示了前述代码的输出：
- en: '![Figure 9.16: The data of a few timestamps from the current instance'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.16：当前实例中几个时间戳的数据'
- en: '](img/B15777_09_16.jpg)'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15777_09_16.jpg)'
- en: 'Figure 9.16: The data of a few timestamps from the current instance'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.16：当前实例中几个时间戳的数据
- en: 'Import the following Keras libraries to build the `RNN`:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入以下 Keras 库以构建 `RNN`：
- en: '[PRE8]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Set the seed and initiate the sequential model, as follows:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置随机种子并初始化顺序模型，如下所示：
- en: '[PRE9]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Add an LSTM layer to the network with 50 units, set the `return_sequences`
    argument to `True`, and set the `input_shape` argument to `(X_train.shape[1],
    1)`. Add three additional LSTM layers, each with 50 units, and set the `return_sequences`
    argument to `True` for the first two, as follows:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向网络中添加一个包含 50 个单元的 LSTM 层，设置 `return_sequences` 参数为 `True`，并将 `input_shape`
    参数设置为 `(X_train.shape[1], 1)`。再添加三个包含 50 个单元的 LSTM 层，并为前两个设置 `return_sequences`
    参数为 `True`，如下所示：
- en: '[PRE10]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Compile the network with an `adam` optimizer and use `Mean Squared Error` for
    the loss. Fit the model to the training data for `100` epochs with a batch size
    of `32`:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `adam` 优化器编译网络，并使用 `均方误差`（Mean Squared Error）作为损失函数。将模型拟合到训练数据上，训练 `100`
    个周期，每批次大小为 `32`：
- en: '[PRE11]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Load and process the `test` data (which is treated as actual data here) and
    select the column representing the value of `Open` stock data:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载并处理`test`数据（这里将其视为实际数据），并选择表示`Open`股票数据的列：
- en: '[PRE12]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following figure shows the output of the preceding code:'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下图展示了前面代码的输出：
- en: '![Figure 9.17: The actual processed data'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图9.17：实际处理过的数据'
- en: '](img/B15777_09_17.jpg)'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15777_09_17.jpg)'
- en: 'Figure 9.17: The actual processed data'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.17：实际处理过的数据
- en: 'Concatenate the data; we will need `60` previous instances in order to get
    the stock price for each day. Therefore, we will need both training and test data:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拼接数据；我们需要`60`个前期实例来获得每一天的股票价格。因此，我们将需要训练和测试数据：
- en: '[PRE13]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Reshape and scale the input to prepare the test data. Note that we are predicting
    the January monthly trend, which has `21` financial days, so in order to prepare
    the test set, we take the lower bound value as 60 and the upper bound value as
    81\. This ensures that the difference of `21` is maintained:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新调整和缩放输入，以准备测试数据。请注意，我们预测的是1月的月度趋势，它有`21`个交易日，因此为了准备测试集，我们将下限值设为60，上限值设为81。这确保了`21`天的差异得以保持：
- en: '[PRE14]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Visualize the results by plotting the actual stock price and then plotting
    the predicted stock price:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过绘制实际股票价格和预测股票价格来可视化结果：
- en: '[PRE15]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Please note that your results may differ slightly to the actual stock price
    of Alphabet.
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，您的结果可能与实际的Alphabet股票价格略有不同。
- en: '**Expected output**:'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**期望的输出**：'
- en: '![Figure 9.18: The real versus predicted stock price'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图9.18：实际股票价格与预测股票价格的对比'
- en: '](img/B15777_09_18.jpg)'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15777_09_18.jpg)'
- en: 'Figure 9.18: The real versus predicted stock price'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.18：实际股票价格与预测股票价格的对比
- en: This concludes *Exercise 9.01*, *Predicting the Trend of Alphabet's Stock Price
    Using an LSTM with 50 Units (Neurons)*, where we have predicted Alphabet's stock
    trends with the help of an `LSTM`. As shown in the preceding plot, the trend has
    been captured fairly.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是*练习9.01*，*使用具有50个单元（神经元）的LSTM预测Alphabet股票价格的趋势*的总结，在此我们通过`LSTM`预测了Alphabet的股票趋势。如前述图所示，趋势已被较好地捕捉。
- en: Note
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZwdAzW](https://packt.live/2ZwdAzW).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问该特定部分的源代码，请参考[https://packt.live/2ZwdAzW](https://packt.live/2ZwdAzW)。
- en: You can also run this example online at [https://packt.live/2YV3PvX](https://packt.live/2YV3PvX).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在[https://packt.live/2YV3PvX](https://packt.live/2YV3PvX)在线运行此示例。
- en: In the next activity, we will test our knowledge and practice building `RNNs`
    with `LSTM` layers by predicting the trend of Amazon's stock price over the last
    5 years.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个活动中，我们将测试我们的知识并练习通过预测亚马逊过去5年股票价格的趋势来构建`RNNs`和`LSTM`层。
- en: 'Activity 9.01: Predicting the Trend of Amazon''s Stock Price Using an LSTM
    with 50 Units (Neurons)'
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动9.01：使用具有50个单元（神经元）的LSTM预测亚马逊股票价格的趋势
- en: 'In this activity, we will examine the stock price of Amazon for the last 5
    years—that is, from January 1, 2014, to December 31, 2018\. In doing so, we will
    try to predict and forecast the company''s future trend for January 2019 using
    an RNN and LSTM. We have the actual values for January 2019, so we can compare
    our predictions to the actual values later. Follow these steps to complete this
    activity:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将考察亚马逊过去5年的股票价格——即从2014年1月1日到2018年12月31日。通过这种方式，我们将尝试使用RNN和LSTM预测并预测该公司2019年1月的未来趋势。我们已经有了2019年1月的实际值，因此稍后可以将我们的预测与实际值进行比较。按照以下步骤完成此活动：
- en: Import the required libraries.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库。
- en: From the full dataset, extract the `Open` column as the predictions will be
    made on the open stock value. Download the dataset from this book's GitHub repository.
    You can find the dataset at [https://packt.live/2vtdA8o](https://packt.live/2vtdA8o).
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从完整的数据集中提取`Open`列，因为预测将基于开盘股票值。可以从本书的GitHub库下载数据集。数据集可以在[https://packt.live/2vtdA8o](https://packt.live/2vtdA8o)找到。
- en: Normalize the data between 0 and 1.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据规范化到0和1之间。
- en: Then, create timestamps. The values of each day in January 2019 will be predicted
    by the previous 60 days; so, if January 1 is predicted by using the value from
    the *n*th day up to December 31, then January 2 will be predicted by using the
    *n* + *1*st day and January 1, and so on.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，创建时间戳。2019年1月每一天的股票价格将由前60天的值来预测；因此，如果1月1日的预测是使用第*n*天到12月31日的值，那么1月2日将使用第*n*
    + 1天和1月1日的值来预测，以此类推。
- en: Reshape the data into three dimensions since the network needs data in three dimensions.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据重新调整为三维，因为网络需要三维数据。
- en: Build an `RNN` model in `Keras` using `50` units (here, units refer to neurons)
    with four `LSTM` layers. The first step should provide the input shape. Note that
    the final `LSTM` layer always adds `return_sequences=True`, so it doesn't have
    to be explicitly defined.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Keras`中构建一个包含`50`个单元的`RNN`模型（这里，单元指的是神经元），并使用四个`LSTM`层。第一步应该提供输入形状。请注意，最后一个`LSTM`层总是会添加`return_sequences=True`，因此不必显式定义。
- en: Process and prepare the test data that is the actual data for January 2019.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理并准备测试数据，即2019年1月的实际数据。
- en: Combine and process the training and test data.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合并并处理训练数据和测试数据。
- en: Visualize the results.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化结果。
- en: 'After implementing these steps, you should see the following expected output:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现这些步骤后，您应该看到以下预期输出：
- en: '![Figure 9.19: Real versus predicted stock prices'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.19：实际股票价格与预测股票价格的对比'
- en: '](img/B15777_09_19.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_09_19.jpg)'
- en: 'Figure 9.19: Real versus predicted stock prices'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.19：实际股票价格与预测股票价格的对比
- en: Note
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 452.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第452页找到。
- en: 'Now, let''s try and improve performance by tweaking our `LSTM`. There is no
    gold standard on how to build an `LSTM`; however, the following permutation combinations
    can be tried in order to improve performance:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试通过调整`LSTM`来提升性能。关于如何构建`LSTM`没有绝对标准；然而，以下的排列组合可以尝试，以改进性能：
- en: Build an `LSTM` with moderate units, such as `50`
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个具有适中单元数的`LSTM`，例如`50`
- en: Build an `LSTM` with over `100` units
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个包含超过`100`个单元的`LSTM`模型
- en: Use more data; that is, instead of `5` years, take data from `10` years
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更多数据；也就是说，除了`5`年的数据外，获取`10`年的数据
- en: Apply regularization using `100` units
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`100`个单元应用正则化
- en: Apply regularization using `50` units
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`50`个单元应用正则化
- en: Apply regularization using more data and `50` units
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更多数据和`50`个单元应用正则化
- en: This list can have a number of combinations; whichever combination offers the
    best results can be considered a good algorithm for that particular dataset. In
    the next exercise, we will explore one of these options by adding more units to
    our `LSTM` layer and observing the performance.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表可以有多种组合；任何能够提供最佳结果的组合都可以视为该数据集的好算法。在下一个练习中，我们将通过增加`LSTM`层的单元数来探索其中的一个选项，并观察性能表现。
- en: 'Exercise 9.02: Predicting the Trend of Alphabet''s Stock Price Using an LSTM
    with 100 units'
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 9.02：使用100个单元的LSTM预测字母表公司股票价格的趋势
- en: 'In this exercise, we will examine the stock price of Alphabet over the last
    5 years, from January 1, 2014, to December 31, 2018\. In doing so, we will try
    to predict and forecast the company''s future trend for January 2019 using RNNs.
    We have the actual values for January 2019, so we will compare our predictions
    with the actual values later. This is the same task as the first exercise, but
    now we''re using 100 units instead. Make sure that you compare the output with
    *Exercise 9.01*, *Predicting the Trend of Alphabet''s Stock Price Using an LSTM
    with 50 Units (Neurons)*. Follow these steps to complete this exercise:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将研究字母表公司在过去5年的股票价格，从2014年1月1日到2018年12月31日。在此过程中，我们将尝试使用RNN预测和预测该公司在2019年1月的未来趋势。我们有2019年1月的实际数据，因此稍后我们将把预测值与实际值进行比较。这与第一个练习相同，但这次我们使用了100个单元。确保与*练习
    9.01*、*使用50个单元（神经元）的LSTM预测字母表公司股票价格的趋势*进行比较。按照以下步骤完成此练习：
- en: 'Import the required libraries:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE16]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Import the dataset using the pandas `read_csv` function and look at the first
    five rows of the dataset using the `head` method:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用pandas的`read_csv`函数导入数据集，并使用`head`方法查看数据集的前五行：
- en: '[PRE17]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We are going to make our prediction using the `Open` stock price; therefore,
    select the `Open` stock price column from the dataset and print the values:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`Open`股票价格进行预测；因此，从数据集中选择`Open`股票价格列并打印值：
- en: '[PRE18]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, perform feature scaling by normalizing the data using `MinMaxScaler`
    and setting the range of the features so that they have a minimum value of zero
    and a maximum value of one. Use the `fit_transform` method of the scaler on the
    training data:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，通过使用`MinMaxScaler`对数据进行标准化，执行特征缩放，并设置特征的范围，使其最小值为零，最大值为一。对训练数据使用标准化器的`fit_transform`方法：
- en: '[PRE19]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Create the data to get `60` timestamps from the current instance. We chose
    `60` here as it will give us a sufficient number of previous instances in order
    to understand the trend; technically, this can be any number, but `60` is the
    optimal value. Additionally, the upper bound value here is `1258`, which is the
    index or count of rows (or records) in the `training` set:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建数据以从当前实例中获取`60`个时间戳。我们在这里选择`60`，因为它将提供足够多的前期实例以便理解趋势；从技术角度讲，这可以是任何数字，但`60`是最佳值。此外，这里的上限值为`1258`，它表示`training`集中的行数或记录数：
- en: '[PRE20]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Reshape the data to add an extra dimension to the end of `X_train` using NumPy''s
    `reshape` function:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用NumPy的`reshape`函数调整数据的形状，在`X_train`的末尾添加一个额外的维度：
- en: '[PRE21]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Import the following `Keras` libraries to build the `RNN`:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入以下`Keras`库以构建`RNN`：
- en: '[PRE22]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Set the seed and initiate the sequential model, as follows:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置种子并初始化序列模型，如下所示：
- en: '[PRE23]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Add an `LSTM` layer to the network with `50` units, set the `return_sequences`
    argument to `True`, and set the `input_shape` argument to `(X_train.shape[1],
    1)`. Add three additional `LSTM` layers, each with `50` units, and set the `return_sequences`
    argument to `True` for the first two. Add a final output layer of size `1`:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向网络中添加一个具有`50`个单元的`LSTM`层，将`return_sequences`参数设置为`True`，并将`input_shape`参数设置为`(X_train.shape[1],
    1)`。再添加三个具有`50`个单元的`LSTM`层，并将前两个`LSTM`层的`return_sequences`参数设置为`True`。最后添加一个大小为`1`的输出层：
- en: '[PRE24]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Compile the network with an `adam` optimizer and use `Mean Squared Error` for
    the loss. Fit the model to the training data for `100` epochs with a batch size
    of `32`:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`adam`优化器编译网络，并使用`均方误差`作为损失函数。用`32`的批量大小在`100`个epochs上拟合训练数据：
- en: '[PRE25]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Load and process the test data (which is treated as actual data here) and select
    the column representing the value of `Open` stock data:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载并处理测试数据（在此视为实际数据），并选择代表`Open`股价数据的列：
- en: '[PRE26]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Concatenate the data since we will need `60` previous instances to get the
    stock price for each day. Therefore, we will need both the training and test data:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据连接起来，因为我们需要`60`个前期实例来预测每一天的股价。因此，我们将需要训练数据和测试数据：
- en: '[PRE27]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Reshape and scale the input to prepare the test data. Note that we are predicting
    the January monthly trend, which has `21` financial days, so in order to prepare
    the test set, we take the lower bound value as `60` and the upper bound value
    as `81`. This ensures that the difference of `21` is maintained:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整和缩放输入以准备测试数据。请注意，我们正在预测1月的月度趋势，1月有`21`个交易日，因此为了准备测试集，我们将下限值设为`60`，上限值设为`81`。这样可以确保`21`的差值保持不变：
- en: '[PRE28]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Visualize the results by plotting the actual stock price and plotting the predicted
    stock price:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过绘制实际股价和预测股价来可视化结果：
- en: '[PRE29]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '**Expected output**:'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出**：'
- en: '![Figure 9.20: Real versus predicted stock price'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图9.20：实际与预测的股价'
- en: '](img/B15777_09_20.jpg)'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15777_09_20.jpg)'
- en: 'Figure 9.20: Real versus predicted stock price'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.20：实际与预测的股价
- en: Note
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZDggf4](https://packt.live/2ZDggf4).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参见 [https://packt.live/2ZDggf4](https://packt.live/2ZDggf4)。
- en: You can also run this example online at [https://packt.live/2O4ZoJ7](https://packt.live/2O4ZoJ7).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个例子，访问 [https://packt.live/2O4ZoJ7](https://packt.live/2O4ZoJ7)。
- en: 'Now, if we compare the `LSTM` of *Exercise 9.01*, *Predicting the Trend of
    Alphabet''s Stock Price Using an LSTM with 50 Units (Neurons)*, which had `50`
    neurons (units), with this `LSTM`, which uses `100` units, we can see that, unlike
    in the case of the Amazon stock price, the Alphabet stock trend is captured better
    using an `LSTM` with `100` units:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们将*练习9.01*中的`LSTM`，即*使用50个单元（神经元）的LSTM预测字母股价的趋势*，它有`50`个神经元（单元），与这个使用`100`个单元的`LSTM`进行比较，我们可以看到，与亚马逊股价的情况不同，使用`100`个单元的`LSTM`能够更好地捕捉到字母股价的趋势：
- en: '![Figure 9.21: Comparing the output with the LSTM of Exercise 9.01'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.21：与练习9.01的LSTM输出比较'
- en: '](img/B15777_09_21.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_09_21.jpg)'
- en: 'Figure 9.21: Comparing the output with the LSTM of Exercise 9.01'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.21：与练习9.01的LSTM输出比较
- en: Thus, we can clearly see that an `LSTM` with `100` units predicts a more accurate
    trend than an `LSTM` with `50` units. Do keep in mind that an `LSTM` with `100`
    units will need more computational time but provides better results in this scenario.
    As well as modifying our model by adding more units, we can also add regularization.
    The following activity will test whether adding regularization can make our Amazon
    model more accurate.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以清楚地看到，`100`个单元的`LSTM`预测的趋势比`50`个单元的`LSTM`更准确。请记住，`100`个单元的`LSTM`虽然需要更多的计算时间，但在这种情况下提供了更好的结果。除了通过增加单元数来修改我们的模型，我们还可以添加正则化。接下来的活动将测试添加正则化是否能使我们的亚马逊模型更加准确。
- en: 'Activity 9.02: Predicting Amazon''s Stock Price with Added Regularization'
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动9.02：使用添加正则化的LSTM预测亚马逊股票价格
- en: 'In this activity, we will examine the stock price of Amazon over the last 5
    years, from January 1, 2014, to December 31, 2018\. In doing so, we will try to
    predict and forecast the company''s future trend for January 2019 using RNNs and
    an LSTM. We have the actual values for January 2019, so we will be able to compare
    our predictions with the actual values later. Initially, we predicted the trend
    of Amazon''s stock price using an LSTM with 50 units (or neurons). Here, we will
    also add dropout regularization and compare the results with *Activity 9.01*,
    *Predicting the Trend of Amazon''s Stock Price Using an LSTM with 50 Units (Neurons)*.
    Follow these steps to complete this activity:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将研究亚马逊股票在过去5年的价格，从2014年1月1日到2018年12月31日。在此过程中，我们将尝试使用RNN和LSTM预测并预报公司2019年1月的未来趋势。我们已经有2019年1月的实际值，因此稍后可以将我们的预测与实际值进行比较。最初，我们使用50个单元（或神经元）的LSTM预测了亚马逊股票的趋势。在这里，我们还将添加dropout正则化，并将结果与*活动9.01*，*使用50个单元（神经元）的LSTM预测亚马逊股票价格的趋势*进行比较。请按照以下步骤完成此活动：
- en: Import the required libraries.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库。
- en: From the full dataset, extract the `Open` column since the predictions will
    be made on the open stock value. You can download the dataset from this book's
    GitHub repository at [https://packt.live/2vtdA8o](https://packt.live/2vtdA8o).
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从完整的数据集中提取`Open`列，因为预测将基于开盘股票值进行。你可以从本书的GitHub仓库下载数据集，链接为[https://packt.live/2vtdA8o](https://packt.live/2vtdA8o)。
- en: Normalize the data between 0 and 1.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据规范化到0和1之间。
- en: Then, create timestamps. The values of each day in January 2019 will be predicted
    by the previous `60` days. So, if January 1 is predicted by using the value from
    the *n*th day up to December 31, then January 2 will be predicted by using the
    *n + 1*st day and January 1, and so on.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，创建时间戳。2019年1月的每一天的值将通过前`60`天的数据进行预测。所以，如果1月1日的预测是通过第*n*天到12月31日的数据进行的，那么1月2日将通过第*n+1*天和1月1日的数据进行预测，依此类推。
- en: Reshape the data into three dimensions since the network needs the data to be
    in three dimensions.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据重新塑形为三维，因为网络需要三维数据。
- en: Build an RNN with four LSTM layers in Keras, each with `50` units (here, units
    refer to neurons), and a 20% dropout after each LSTM layer. The first step should provide
    the input shape. Note that the final LSTM layer always adds `return_sequences=True`.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Keras构建一个包含四个LSTM层的RNN，每个LSTM层有`50`个单元（此处，单元指神经元），每个LSTM层后有20%的dropout。第一步应提供输入形状。请注意，最后一个LSTM层总是设置`return_sequences=True`。
- en: Process and prepare the test data, which is the actual data for January 2019.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理并准备测试数据，即2019年1月的实际数据。
- en: Combine and process the train and test data.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合并并处理训练数据和测试数据。
- en: Finally, visualize the results.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，可视化结果。
- en: 'After implementing these steps, you should get the following expected output:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现这些步骤后，你应该得到以下预期的输出：
- en: '![Figure 9.22: Real versus predicted stock prices'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.22：实际股票价格与预测股票价格'
- en: '](img/B15777_09_22.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_09_22.jpg)'
- en: 'Figure 9.22: Real versus predicted stock prices'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.22：实际股票价格与预测股票价格
- en: Note
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 457.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第457页找到。
- en: In the next activity, we will experiment with building an `RNN` with `100` units
    in each `LSTM` layer and compare this with how the `RNN` performed with only `50`
    units.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个活动中，我们将尝试构建每个`LSTM`层有`100`个单元的`RNN`，并将其与只有`50`个单元的`RNN`表现进行比较。
- en: 'Activity 9.03: Predicting the Trend of Amazon''s Stock Price Using an LSTM
    with an Increasing Number of LSTM Neurons (100 Units)'
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动9.03：使用逐渐增加LSTM神经元数量（100个单元）预测亚马逊股票价格的趋势
- en: 'In this activity, we will examine the stock price of Amazon over the last 5
    years, from January 1, 2014, to December 31, 2018\. In doing so, we will try to
    predict and forecast the company''s future trend for January 2019 using RNNs.
    We have the actual values for January 2019, so we will be able to compare our
    predictions with the actual values later. You can also compare the output difference
    with *Activity 9.01*, *Predicting the Trend of Amazon''s Stock Price Using an
    LSTM with 50 Units (Neurons)*. Follow these steps to complete this activity:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将研究亚马逊过去5年的股票价格，从2014年1月1日到2018年12月31日。在此过程中，我们将尝试使用RNN预测和预测该公司2019年1月的未来趋势。我们拥有2019年1月的实际值，因此稍后可以将我们的预测与实际值进行比较。你还可以将输出的差异与*活动9.01*，“使用50个单元（神经元）的LSTM预测亚马逊股价趋势”进行比较。按照以下步骤完成此活动：
- en: Import the required libraries.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库。
- en: From the full dataset, extract the `Open` column since the predictions will
    be made on the `Open` stock value.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从完整的数据集中提取`Open`列，因为预测将基于`Open`股价进行。
- en: Normalize the data between 0 and 1.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据归一化到0到1之间。
- en: Then, create timestamps. The values of each day in January 2019 will be predicted
    by the previous `60` days; so, if January 1 is predicted by using the value from
    the nth day up to December 31, then January 2 will be predicted by using the *n
    + 1*st day and January 1, and so on.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，创建时间戳。2019年1月每一天的值将由前`60`天的数据预测；因此，如果1月1日是根据第n天到12月31日的数据预测的，那么1月2日将根据第*n
    + 1*天和1月1日的数据来预测，依此类推。
- en: Reshape the data into three dimensions since the network needs data to be in
    three dimensions.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据重塑为三维，因为网络需要三维数据。
- en: Build an LSTM in Keras with 100 units (here, units refer to neurons). The first
    step should provide the input shape. Note that the final `LSTM` layer always adds
    `return_sequences=True`. Compile and fit the model to the training data.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Keras中构建一个包含100个单元的LSTM（此处的单元指的是神经元）。第一步应该提供输入形状。请注意，最终的`LSTM`层始终需要设置`return_sequences=True`。编译并将模型拟合到训练数据中。
- en: Process and prepare the test data, which is the actual data for January 2019.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理并准备测试数据，即2019年1月的实际数据。
- en: Combine and process the training and test data.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合并和处理训练数据与测试数据。
- en: Visualize the results.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化结果。
- en: 'After implementing these steps, you should get the following expected output:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，你应该得到以下预期输出：
- en: '![Figure 9.23: Real versus predicted stock prices'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.23：实际股价与预测股价'
- en: '](img/B15777_09_23.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_09_23.jpg)'
- en: 'Figure 9.23: Real versus predicted stock prices'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.23：实际股价与预测股价
- en: Note
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 462.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第462页找到。
- en: In this activity, we created an `RNN` with four `LSTM` layers, each with `100`
    units. We compared this to the results of *Activity 9.02*, *Predicting Amazon's
    Stock Price with Added Regularization*, in which there were `50` units per layer.
    The difference between the two models was minimal, so a model with fewer units
    is preferable due to the decrease in computational time and there being a smaller
    possibility of overfitting the training data.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们创建了一个包含四个`LSTM`层的`RNN`，每层有`100`个单元。我们将其与*活动9.02*，“使用添加正则化的LSTM预测亚马逊股价”中的结果进行了比较，在该活动中每层有`50`个单元。两个模型之间的差异很小，因此，具有较少单元的模型更为优选，因为它减少了计算时间，并且更不容易出现过拟合训练数据的情况。
- en: Summary
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we learned about sequential modeling and sequential memory
    by examining some real-life cases with Google Assistant. Then, we learned how
    sequential modeling is related to `RNNs`, as well as how `RNNs` are different
    from traditional feedforward networks. We learned about the vanishing gradient
    problem in detail and how using an `LSTM` is better than a simple `RNN` to overcome
    the vanishing gradient problem. We applied what we learned to time series problems
    by predicting stock trends.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过研究一些Google Assistant的实际案例，学习了序列建模和序列记忆。然后，我们学习了序列建模如何与`RNN`相关，并且`RNN`与传统前馈网络的不同之处。我们详细了解了梯度消失问题，以及为什么使用`LSTM`比简单的`RNN`更好地克服梯度消失问题。我们将所学应用于时间序列问题，通过预测股价趋势来实践。
- en: In this workshop, we learned the basics of machine learning and Python, while
    also gaining an in-depth understanding of applying Keras to develop efficient
    deep learning solutions. We explored the difference between machine and deep learning.
    We began the workshop by building a logistic regression model, first with scikit-learn,
    and then with Keras.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个工作坊中，我们学习了机器学习和 Python 的基础知识，同时深入了解了如何应用 Keras 开发高效的深度学习解决方案。我们探索了机器学习和深度学习之间的差异。我们通过首先使用
    scikit-learn，然后使用 Keras 来构建逻辑回归模型，开始了本次工作坊。
- en: Then, we explored Keras and its different models further by creating prediction
    models for various real-world scenarios, such as classifying online shoppers into
    those with purchase intention and those without. We learned how to evaluate, optimize,
    and improve models to achieve maximum information to create robust models that
    perform well on new, unseen data.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过创建各种实际场景的预测模型进一步探索了 Keras 及其不同的模型，例如将在线购物者分类为有购买意图和没有购买意图的群体。我们学习了如何评估、优化和改进模型，以实现最大的信息量，创建在新数据上表现良好的强大模型。
- en: We also incorporated cross-validation by building Keras models with wrappers
    for scikit-learn that help those familiar with scikit-learn workstreams utilize
    Keras models easily. Then, we learned how to apply `L1`, `L2`, and `dropout regularization`
    techniques to improve the accuracy of models and to help prevent our models from
    overfitting the training data.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还通过构建带有 scikit-learn 包装器的 Keras 模型来实现交叉验证，帮助那些熟悉 scikit-learn 工作流程的人轻松使用 Keras
    模型。然后，我们学习了如何应用 `L1`、`L2` 和 `dropout regularization` 技术，以提高模型的准确性并防止模型过拟合训练数据。
- en: Next, we explored model evaluation further by applying techniques such as null
    accuracy for baseline comparison and evaluation metrics such as precision, the
    `AUC-ROC` score, and more to understand how our model scores classification tasks.
    Ultimately, these advanced evaluation techniques helped us understand under what
    conditions our model is performing well and where there is room for improvement.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过应用诸如空准确率作为基准比较技术，以及评估指标如精确度、`AUC-ROC` 得分等，进一步探索了模型评估，以理解我们的模型如何对分类任务进行评分。最终，这些高级评估技术帮助我们了解了模型在什么情况下表现良好，哪里有改进的空间。
- en: We ended the workshop by creating some advanced models with Keras. We explored
    computer vision by building `CNN` models with various parameters to classify images.
    Then, we used pre-trained models to classify new images and fine-tuned those pre-trained
    models so that we could utilize them for our own applications. Finally, we covered
    sequential modeling, which is used for modeling sequences such as stock prices
    and natural language processing. We tested this knowledge by creating `RNN` networks
    with `LSTM` layers to predict the stock price of real stock data and experimented
    with various numbers of units in each layer and the effect of dropout regularization
    on the model's performance.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用 Keras 创建一些高级模型来结束了工作坊。我们通过构建带有各种参数的 `CNN` 模型来探索计算机视觉，以分类图像。然后，我们使用预训练模型对新图像进行分类，并对这些预训练模型进行了微调，以便我们可以将它们应用于自己的应用程序。最后，我们介绍了序列建模，这用于建模诸如股票价格和自然语言处理等序列。我们通过创建带有
    `LSTM` 层的 `RNN` 网络，测试了使用真实股票数据预测股价的知识，并实验了每层中不同单元数的影响以及 dropout 正则化对模型性能的影响。
- en: Overall, we have gained a comprehensive understanding of how to use Keras to
    solve a variety of problems using real-world datasets. We covered the classification
    tasks of online shoppers, hepatitis C data, and failure data for Scania trucks,
    as well as regression tasks such as predicting the aquatic toxicity of various
    chemicals when given various chemical attributes. We also performed image classification
    tasks and built `CNN` models to predict whether images are of flowers or cars,
    and also built regression tasks to predict future stock prices with `RNNs`. By
    using this workshop to build models with real-word datasets, you are ready to
    apply your learning and understanding to your own problem-solving and create your
    own applications.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，我们全面了解了如何使用 Keras 解决各种实际问题。我们涵盖了在线购物者的分类任务、丙型肝炎数据和 Scania 卡车的故障数据，还包括回归任务，如在给定各种化学属性时预测各种化学物质的水生毒性。我们还进行了图像分类任务，构建了
    `CNN` 模型来预测图像是花朵还是汽车，并构建了回归任务来预测未来的股价，使用的是 `RNNs`。通过在这个工作坊中使用真实世界的数据集构建模型，你已准备好将所学知识应用到自己的问题解决中，并创建自己的应用程序。
