- en: 6\. Training Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 训练技巧
- en: This chapter describes important ideas in neural network training, including
    the optimization techniques that are used to search for optimal weight parameters,
    the initial values of weight parameters, and the method for setting hyperparameters—all
    of which are important topics when it comes to neural network training. We will
    look at regularization methods such as weight decay and dropout to prevent overfitting
    and implement them. Lastly, we will look at batch normalization, which has been
    used in a lot of research in recent years. By using the methods described in this
    chapter, you will be able to promote neural network training efficiently to improve
    recognition accuracy.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述了神经网络训练中的重要思想，包括用于寻找最优权重参数的优化技术、权重参数的初始值以及设置超参数的方法——这些都是神经网络训练中的重要话题。我们将讨论正则化方法，如权重衰减和丢弃法，以防止过拟合并加以实现。最后，我们将探讨批量归一化，这在近年来的研究中被广泛使用。通过使用本章介绍的方法，你将能够有效地推进神经网络训练，从而提高识别精度。
- en: Updating Parameters
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新参数
- en: The purpose of neural network training is to find the parameters that minimize
    the value of the loss function. The problem is finding the optimal parameters—a
    process called **optimization**. Unfortunately, the optimization is difficult
    because the parameter space is very complicated, and the optimal solution is difficult
    to find. You cannot do this by solving an equation to obtain the minimum value
    immediately. In a deep network, it is more difficult because the number of parameters
    is huge.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络训练的目的是找到使损失函数值最小化的参数。问题在于找到最优参数——这一过程被称为**优化**。不幸的是，优化非常困难，因为参数空间非常复杂，且最优解很难找到。你不能通过解方程来立即得到最小值。在深度网络中，情况更加困难，因为参数的数量非常庞大。
- en: So far, we have depended on the gradients (derivatives) of the parameters to
    find the optimal parameters. By repeatedly using the gradients of the parameters
    to update the parameters in the gradient direction, we approach the optimal parameters
    gradually. This is a simple method called **stochastic gradient descent** (**SGD**),
    but it is a "smarter" method than searching the parameter space randomly. However,
    SGD is a simple method, and (for some problems) there are some methods that work
    better. So, let's first consider the disadvantage of SGD and introduce other optimization
    techniques.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们依赖参数的梯度（导数）来寻找最优参数。通过反复使用参数的梯度在梯度方向上更新参数，我们逐步接近最优参数。这是一种简单的方法，称为**随机梯度下降法**（**SGD**），但它比随机搜索参数空间是一种“更聪明”的方法。然而，SGD是一种简单的方法，（对于某些问题）也有一些方法能够更好地工作。那么，我们首先来考虑SGD的缺点，并介绍其他优化技术。
- en: Story of an Adventurer
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 冒险者的故事
- en: Before moving on to the main topic, we can consider an allegory to describe
    the situation we are in regarding optimization.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入主题之前，我们可以通过一个寓言来描述我们在优化方面所面临的情况。
- en: Note
  id: totrans-7
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: There is a strange adventurer. He travels through a vast dry region to find
    a deep valley floor every day. His goal is to reach the deepest valley bottom,
    which he calls the "deep place." It is the reason why he travels. In addition,
    he has put two strict "restrictions" on himself. One of them is to not use a map,
    while the other is to cover his eyes. Therefore, he does not know where the deepest
    valley bottom exists in the vast land, and he cannot see anything. Under these
    strict conditions, how can this adventurer look for the "deep place"? How can
    he move to find the "deep place" efficiently?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个奇怪的冒险者。他每天穿越广阔的干旱地区，寻找一片深谷的底部。他的目标是到达最深的谷底，他称之为“深处”。这也是他旅行的原因。此外，他对自己设定了两个严格的“限制”。其中一个是不能使用地图，另一个是要蒙住双眼。因此，他不知道最深的谷底在哪个地方，也看不见任何东西。在这种严格的条件下，这个冒险者如何寻找“深处”？他如何高效地移动以寻找“深处”？
- en: The situation we are in when searching for the optimal parameters is a world
    of darkness just like that of this adventurer. We must look for the "deep place"
    blindfolded and without a map in a vast and complicated landscape
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在寻找最优参数时所处的情境，就像这个冒险者的世界一样是一片黑暗。我们必须在广阔且复杂的地形中，蒙着眼睛、没有地图地寻找“深处”。
- en: What is important in this difficult situation is the "inclination" of the ground.
    The adventurer cannot see around him, but he knows the inclination of the ground
    due to where he stands (his feet can feel it). So, moving in the direction where
    the inclination is the steepest is the strategy of SGD. "By repeating this, I
    may be able to reach the "deep place" someday," the brave adventurer thinks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种困难的情况下，重要的是“地面的倾斜度”。冒险者看不到周围的环境，但他知道地面的倾斜度，因为他能从站立的位置（他的双脚可以感觉到）感知到它。因此，SGD的策略是沿着地面倾斜度最陡的方向前进。勇敢的冒险者想：“通过不断重复，我可能某天能够到达‘深处’。”
- en: SGD
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SGD
- en: 'Now that we understand the difficulty of this optimization problem, let''s
    start by reviewing SGD. Equation 6.1 represents SGD as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经理解了这个优化问题的困难，接下来让我们回顾一下SGD。方程6.1表示SGD如下：
- en: '| ![65](img/Figure_6.1a.png) | (6.1) |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| ![65](img/Figure_6.1a.png) | (6.1) |'
- en: 'Here, the weight parameters to update are W and the gradients of the loss function
    for W are ![66](img/Figure_6.1b.png). η is the learning rate. We need to predefine
    it as a value, such as 0.01 or 0.001\. `<-` in the equation indicates that the
    value on the right-hand side is used to update the value on the left-hand side.
    As equation 6.1 shows, SGD is a simple method that moves a certain distance in
    the gradient direction. Now, we will implement `SGD` as a class in Python:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，要更新的权重参数是W，W的损失函数梯度是![66](img/Figure_6.1b.png)。η是学习率，我们需要预定义它的值，如0.01或0.001。方程中的`<-`表示右侧的值将用于更新左侧的值。如方程6.1所示，SGD是一个简单的方法，它在梯度方向上移动一定的距离。现在，我们将以Python类的形式实现`SGD`：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, the argument at initialization, `lr`, is the learning rate. The learning
    rate is retained as an instance variable. We will also define the `update(params,
    grads)` method, which is called repeatedly in SGD. The arguments, `params` and
    `grads`, are dictionary variables (as in the implementation of neural networks
    so far). Like `params[''W1'']` and `grads[''W1'']`, each element stores a weight
    parameter or a gradient. By using the `SGD` class, you can update the parameters
    in a neural network as follows (the following code is pseudocode that doesn''t
    run):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，初始化时的参数`lr`是学习率。学习率被保留为一个实例变量。我们还将定义`update(params, grads)`方法，这个方法会在SGD中被反复调用。`params`和`grads`是字典类型的变量（如同我们至今在神经网络实现中所做的那样）。例如，`params['W1']`和`grads['W1']`，每个元素存储着权重参数或梯度。通过使用`SGD`类，你可以像下面这样更新神经网络中的参数（以下代码是伪代码，不可直接运行）：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The name of the variable that appears here, `optimizer`, means a "person who
    optimizes." Here, SGD plays this role. The `optimizer` variable takes responsibility
    for updating the parameters. All we need to do here is pass information regarding
    the parameters and gradients to the optimizer.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里出现的变量名`optimizer`表示“优化者”。在此，SGD扮演着这一角色。`optimizer`变量负责更新参数。我们所需要做的就是将关于参数和梯度的信息传递给优化器。
- en: Thus, separately implementing the class that optimizes facilitates the modularization
    of the features. For example, we will soon implement another optimization technique
    called `update(params, grads)`. Then, we can switch from SGD to Momentum by changing
    the `optimizer = SGD()` statement to `optimizer = Momentum()`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，单独实现一个优化类有助于特性的模块化。例如，我们很快就会实现另一个优化技术，叫做`update(params, grads)`。然后，我们只需将`optimizer
    = SGD()`语句更改为`optimizer = Momentum()`，就能从SGD切换到Momentum优化技术。
- en: Note
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: In many deep learning frameworks, various optimization techniques are implemented,
    and a mechanism is provided so that we can switch between them easily. For example,
    in a deep learning framework called Lasagne, optimization techniques are implemented
    as functions in the `updates.py` file ([http://github.com/Lasagne/Lasagne/blob/master/lasagne/updates.py](http://github.com/Lasagne/Lasagne/blob/master/lasagne/updates.py)).
    The user can select the desired optimization technique from them.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多深度学习框架中，已经实现了各种优化技术，并提供了机制，让我们可以轻松地在它们之间切换。例如，在一个叫Lasagne的深度学习框架中，优化技术作为函数实现，并存放在`updates.py`文件中（[http://github.com/Lasagne/Lasagne/blob/master/lasagne/updates.py](http://github.com/Lasagne/Lasagne/blob/master/lasagne/updates.py)）。用户可以从中选择所需的优化技术。
- en: Disadvantage of SGD
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SGD的缺点
- en: 'Although SGD is simple and easy to implement, it may be inefficient for some
    problems. To discuss the disadvantage of SGD, let''s consider a problem that calculates
    the minimum value of the following function:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然SGD（随机梯度下降）简单且易于实现，但在某些问题上可能效率较低。为了讨论SGD的缺点，我们考虑一个计算以下函数最小值的问题：
- en: '| ![67](img/Figure_6.1c.png) | (6.2) |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| ![67](img/Figure_6.1c.png) | (6.2) |'
- en: The shape of the function represented by equation 6.2 looks like a "bowl" stretched
    in the x-axis direction, as shown in the following plots. Actually, the contour
    lines of equation 6.2 look like ellipses extended in the x-axis direction.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由方程6.2表示的函数形状像一个沿x轴方向拉伸的“碗”，如下面的图所示。实际上，方程6.2的等高线像是沿x轴方向延伸的椭圆。
- en: Now, let's look at the gradients of the function that are represented by equation
    6.2\. *Figure 6.2* shows the gradients. These gradients are large in the y-axis
    direction and small in the x-axis direction. In other words, the inclination in
    the y-axis direction is steep, while in the x-axis direction, it's gradual. Note
    that the position of the minimum value of equation 6.2 is `(x, y) = (0, 0)` but
    that the gradients in *Figure 6.2* do not point to the (0, 0) direction in many
    places.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下由方程6.2表示的函数的梯度。*图6.2*展示了这些梯度。这些梯度在y轴方向上很大，而在x轴方向上很小。换句话说，y轴方向的倾斜度很陡，而x轴方向则比较平缓。注意，方程6.2的最小值位置是`(x,
    y) = (0, 0)`，但在许多地方，*图6.2*中的梯度并未指向(0, 0)方向。
- en: 'Let''s apply SGD to the function that has the shape shown in the following
    plots. It starts searching at (x, y) = (−7.0, 2.0) (initial values). *Figure 6.3*
    shows the result:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将SGD应用于如下图所示的函数。它从`(x, y) = (-7.0, 2.0)`（初始值）开始搜索。*图6.3*显示了结果：
- en: '![Figure 6.1: Graph of  (left) and its contour lines (right)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.1：图像（左）及其等高线（右）'
- en: '](img/fig06_1.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_1.jpg)'
- en: 'Figure 6.1: Graph of ![69](img/Figure_6.1d.png) (left) and its contour lines
    (right)'
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.1：![
- en: '![Figure 6.2: Gradients of'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.2：梯度'
- en: '](img/fig06_2.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_2.jpg)'
- en: 'Figure 6.2: Gradients of ![68](img/Figure_6.1e.png)'
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.2：梯度！[68](img/Figure_6.1e.png)
- en: 'SGD moves in a zigzag, as shown in the following plot. The disadvantage of
    SGD is that its search path becomes inefficient if the shape of a function is
    not isotropic—that is, if it is elongated. So, we need a method that is smarter
    than SGD that moves only in the gradient direction. The root cause of SGD''s search
    path being inefficient is that the gradients do not point to the correct minimum
    values:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: SGD以锯齿状方式移动，如下图所示。SGD的缺点是，如果一个函数的形状不是各向同性的，也就是说它是拉长的，它的搜索路径会变得低效。因此，我们需要比SGD更智能的方法，能够仅沿梯度方向移动。SGD搜索路径低效的根本原因是梯度并没有指向正确的最小值：
- en: '![Figure 6.3: Update path of optimization by SGD – inefficient because it moves'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.3：SGD优化更新路径 — 由于它以锯齿状方式逼近最小值(0, 0)，效率低'
- en: in a zigzag to the minimum value (0, 0)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以锯齿状方式逼近最小值(0, 0)
- en: '](img/fig06_3.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_3.jpg)'
- en: 'Figure 6.3: Update path of optimization by SGD – inefficient because it moves
    in a zigzag to the minimum value (0, 0)'
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.3：SGD优化更新路径 — 由于它以锯齿状方式逼近最小值(0, 0)，效率低
- en: 'To improve the disadvantage of SGD, we will introduce three alternative methods:
    Momentum, AdaGrad, and Adam. We will describe each of them briefly and show their
    equations and implementations in Python.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改进SGD的缺点，我们将介绍三种替代方法：动量法、AdaGrad和Adam。我们将简要描述它们，并展示它们的方程和Python实现。
- en: Momentum
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动量
- en: 'Momentum is related to physics; it means the "quantity of motion." The Momentum
    technique is represented by the following equations:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 动量与物理学有关；它意味着“运动量”。动量法通过以下方程表示：
- en: '| ![70](img/Figure_6.3a.png) | (6.3) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| ![70](img/Figure_6.3a.png) | (6.3) |'
- en: '| ![71](img/Figure_6.3b.png) | (6.4) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| ![71](img/Figure_6.3b.png) | (6.4) |'
- en: 'Just like SGD, W is the weight parameter to update, ![72](img/Figure_6.3c.png)
    is the gradients of the loss function for W, and η is the learning rate. A new
    variable that appears here, v, is the "velocity" in physics. Equation 6.3 represents
    a physical law stating that an object receives a force in the gradient direction
    and is accelerated by this force. In Momentum, update functions are used as if
    a ball had been rolled on the ground, as shown in the following diagram:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 就像SGD一样，W是要更新的权重参数，![72](img/Figure_6.3c.png)是W的损失函数梯度，η是学习率。这里出现的新变量v是物理学中的“速度”。方程6.3表示了一个物理法则，说明物体在梯度方向上受力并由此加速。在动量法中，更新函数就像球在地面上滚动一样，如下图所示：
- en: '![Figure 6.4: Image of Momentum – a ball rolls on the slope of the ground'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.4：动量图像 — 一个球在地面坡道上滚动'
- en: '](img/fig06_4.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_4.jpg)'
- en: 'Figure 6.4: Image of Momentum – a ball rolls on the slope of the ground'
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.4：动量图像 — 一个球在地面坡道上滚动
- en: 'The term αv in equation 6.3 slows the object down gradually when it receives
    no force (a value such as 0.9 is set for α). This is the friction created by the
    ground or air resistance. The following code shows the implementation of Momentum
    (the source code is located at `common/optimizer.py`):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 方程6.3中的项αv会在物体不受力时逐渐减速（为α设置一个值，如0.9）。这就是地面或空气阻力造成的摩擦力。以下代码展示了动量的实现（源代码位于`common/optimizer.py`）：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The instance variable, `v`, retains the velocity of the object. At initialization,
    `v` retains nothing. When `update()` is called, it retains the data of the same
    structure as a dictionary variable. The remaining implementation is simple: it
    just implements equations 6.3 and 6.4.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 实例变量`v`保留了物体的速度。在初始化时，`v`不保留任何值。当调用`update()`时，它保留了与字典变量结构相同的数据。剩下的实现很简单：它只需实现方程
    6.3 和 6.4。
- en: Now, let's use Momentum to solve the optimization problem of equation 6.2\.
    The following image shows the result.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用动量来解决方程6.2中的优化问题。下图展示了结果。
- en: 'As shown in the following plot, the update path moves like a ball being rolled
    around in a bowl. You can see that "the degree of zigzag" is reduced compared
    to SGD. The force in the x-axis direction is very small, but the object always
    receives the force in the same direction and is accelerated constantly in the
    same direction. On the other hand, the force in the y-axis direction is large,
    but the object receives the forces in the positive and negative directions alternately.
    They cancel each other out, so the velocity in the y-axis direction is unstable.
    This can accelerate the motion in the x-axis direction and reduce the zigzag motion
    compared to SGD:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，更新路径像一个球在碗中滚动。可以看到，与SGD相比，“之字形程度”有所减少。x轴方向上的力非常小，但物体始终在相同的方向上受到力，并且在相同的方向上不断加速。另一方面，y轴方向上的力很大，但物体在正负方向上交替受力，彼此抵消，因此y轴方向上的速度不稳定。这可以加速x轴方向上的运动，并减少与SGD相比的之字形运动：
- en: '![Figure 6.5: Update path for optimization by Momentum'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.5：通过动量优化的更新路径'
- en: '](img/fig06_5.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_5.jpg)'
- en: 'Figure 6.5: Update path for optimization by Momentum'
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.5：通过动量优化的更新路径
- en: AdaGrad
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AdaGrad
- en: In neural network training, the value of the learning `rate--η` in the `equation--`
    is important. If it is too small, training takes too long. If it is too large,
    divergence occurs, and correct training cannot be achieved.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络训练中，学习`rate--η`在`equation--`中的值非常重要。如果它太小，训练会持续太长时间。如果它太大，训练会发散，无法达到正确的训练效果。
- en: There is an effective technique for the learning rate called **learning rate
    decay**. It uses a lower learning rate as training advances. This method is often
    used in neural network training. A neural network learns "much" first and learns
    "less" gradually.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种有效的学习率技术叫做**学习率衰减**。它随着训练的进行使用更低的学习率。这种方法在神经网络训练中经常使用。神经网络首先学习“很多”，然后逐渐学习“更少”。
- en: 'Reducing the learning rate gradually is the same as reducing the values of
    the learning rates for all the parameters collectively. AdaGrad ( *John Duchi,
    Elad Hazan, and Yoram Singer (2011): Adaptive Subgradient Methods for Online Learning
    and Stochastic Optimization. Journal of Machine Learning Research 12, Jul (2011),
    2121 – 2159.*) is an advanced version of this method. AdaGrad creates a custom-made
    value for each parameter.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '逐渐降低学习率等同于逐渐减小所有参数的学习率。AdaGrad（*John Duchi, Elad Hazan, and Yoram Singer (2011):
    Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.
    Journal of Machine Learning Research 12, Jul (2011), 2121 – 2159.*）是这种方法的一个高级版本。AdaGrad为每个参数创建了一个定制的值。'
- en: 'AdaGrad adjusts the learning rate for each element of the parameter adaptively
    for training (the "Ada" in AdaGrad comes from "Adaptive"). Now, we will show AdaGrad''s
    update method with equations:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad会自适应地调整每个参数元素的学习率以进行训练（AdaGrad中的"Ada"来自于"Adaptive"）。现在，我们将通过方程展示AdaGrad的更新方法：
- en: '| ![73](img/Figure_6.5a.png) | (6.5) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| ![73](img/Figure_6.5a.png) | (6.5) |'
- en: '| ![74](img/Figure_6.5b.png) | (6.6) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| ![74](img/Figure_6.5b.png) | (6.6) |'
- en: Just like SGD, W is the weight parameters to update, ![75](img/Figure_6.5c.png)
    is the gradients of the loss function for W, and η is the learning rate. Here,
    a new variable, h, appears. The h variable stores the sum of the squared gradient
    values thus far, as shown in equation 6.5 (⊙ in equation 6.5 indicates multiplication
    between array elements). When updating parameters, AdaGrad adjusts the scale of
    learning by multiplying ![76](img/Figure_6.5d.png). For the parameter element
    that moved significantly (i.e., was updated heavily), the learning rate becomes
    smaller. Thus, you can attenuate the learning rate for each parameter element
    by gradually reducing the learning rate of the parameter that moved significantly.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 就像SGD一样，W是需要更新的权重参数，![75](img/Figure_6.5c.png)是W的损失函数梯度，η是学习率。这里引入了一个新变量h。h变量存储了迄今为止梯度值的平方和，如方程6.5所示（方程6.5中的⊙表示数组元素之间的乘法）。在更新参数时，AdaGrad通过乘以![76](img/Figure_6.5d.png)来调整学习的规模。对于已经显著移动的参数元素（即已经进行了大量更新的参数），学习率会变小。因此，您可以通过逐渐减少那些显著移动的参数的学习率来衰减每个参数元素的学习率。
- en: Note
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'AdaGrad records all the past gradients as the sum of squares. Therefore, as
    learning advances, the degree of update becomes small. When learning is conducted
    infinitely, the degree of update becomes 0, resulting in no update. The RMSProp
    (*Tieleman, T., & Hinton, G. (2012): Lecture 6.5—RMSProp: Divide the gradient
    by a running average of its recent magnitude. COURSERA: Neural Networks for Machine
    Learning*) method solves this problem. It does not add all the past gradients
    equally. It forgets the past gradients gradually and conducts addition so that
    the information about new gradients is clearly reflected. This reduces the scale
    of the past gradients exponentially, which is called the "exponential moving average."'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 'AdaGrad将所有过去的梯度记录为平方和。因此，随着学习的进行，更新的程度变小。当学习无限进行时，更新的程度变为0，导致没有更新。RMSProp（*Tieleman,
    T., & Hinton, G. (2012): Lecture 6.5—RMSProp: Divide the gradient by a running
    average of its recent magnitude. COURSERA: Neural Networks for Machine Learning*）方法解决了这个问题。它不会对所有过去的梯度进行等权重处理，而是逐渐遗忘过去的梯度，并进行加权处理，从而使新梯度的信息得以清晰反映。这样，过去梯度的规模会指数衰减，这就是所谓的“指数加权平均”。'
- en: 'Now, let''s implement AdaGrad. You can implement AdaGrad as follows (the source
    code is located at `common/optimizer.py`):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来实现AdaGrad。您可以按如下方式实现AdaGrad（源代码位于`common/optimizer.py`）：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that a small value of `1e-7` was added in the last line. This prevents
    division by `0` when `self.h[key]` contains `0`. In many deep learning frameworks,
    you can configure this small value as a parameter, but here, a fixed value, `1e-7`,
    is used.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，最后一行添加了一个小的值`1e-7`。这可以防止当`self.h[key]`包含`0`时发生除以`0`的情况。在许多深度学习框架中，您可以将这个小值配置为一个参数，但在这里，使用了一个固定值`1e-7`。
- en: 'Now, let''s use AdaGrad to solve the optimization problem of equation 6.2\.
    The following image shows the result:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用AdaGrad来解决方程6.2中的优化问题。下图展示了结果：
- en: '![Figure 6.6: Update path for optimization by AdaGrad'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.6：AdaGrad优化的更新路径'
- en: '](img/fig06_6.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_6.jpg)'
- en: 'Figure 6.6: Update path for optimization by AdaGrad'
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.6：AdaGrad优化的更新路径
- en: The result shown in the preceding image shows that the parameters are moving
    efficiently to the minimum value. The parameters move a lot at first because the
    gradient in the y-axis direction is large. Adjustment is conducted in proportion
    to the large motion so that the update step becomes small. Thus, the degree of
    update in the y-axis direction is weakened, reducing the zigzag motion.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图像显示了参数高效地向最小值移动。最初，参数移动较多，因为y轴方向的梯度很大。调整与大幅运动成比例进行，因此更新步长变小。这样，y轴方向的更新程度被减弱，从而减少了锯齿形的波动。
- en: Adam
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Adam
- en: In Momentum, the parameters move based on physical law, such as a ball rolled
    in a bowl. AdaGrad adjusts the update step adaptively for each parameter element.
    So, what happens when the two techniques, Momentum and AdaGrad, are combined?
    This is the basic idea of the technique called Adam (this explanation of Adam
    is intuitive and lacking some of the finer technical details. For a more granular
    definition, please see the original article).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在Momentum中，参数的更新基于物理法则，就像一个球在碗中滚动一样。AdaGrad为每个参数元素自适应地调整更新步长。那么，当Momentum和AdaGrad这两种技术结合时，会发生什么呢？这就是Adam技术的基本思想（这里对Adam的解释是直观的，缺少一些更细致的技术细节。如需更精确的定义，请参阅原文）。
- en: 'Adam is a new technique that was proposed in 2015\. The theory is slightly
    complicated. Intuitively, it is like a combination of Momentum and AdaGrad. By
    combining the advantages of these two techniques, we can expect to search the
    parameter space efficiently. The "bias correction" of hyperparameters is also
    a characteristic of Adam. For more details, please see the original paper (*Diederik
    Kingma and Jimmy Ba. (2014): Adam: A Method for Stochastic Optimization. arXiv:1412.6980[cs]
    (December 2014)*). It is implemented in Python as the `Adam` class in `common/optimizer.py`.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 'Adam 是一种新的技术，于2015年提出。其理论略显复杂。直观上来说，它类似于动量和AdaGrad的结合体。通过结合这两种技术的优势，我们可以期待有效地搜索参数空间。超参数的“偏差校正”也是Adam的一个特点。更多细节，请参阅原始论文（*Diederik
    Kingma 和 Jimmy Ba. (2014): Adam: A Method for Stochastic Optimization. arXiv:1412.6980[cs]
    (2014年12月)*）。在Python中实现为`common/optimizer.py`中的`Adam`类。'
- en: Now, let's use Adam to solve the optimization problem of equation 6.2\. The
    following figure shows the result.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用Adam来解决方程6.2的优化问题。下图显示了结果。
- en: '![Figure 6.7: Update path for optimization by Adam'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.7: Adam优化的更新路径'
- en: '](img/fig06_7.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_7.jpg)'
- en: 'Figure 6.7: Update path for optimization by Adam'
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 6.7: Adam优化的更新路径'
- en: As shown in *Figure 6.7*, the update path by Adam moves as if a ball has been
    rolled in a bowl. The motion is similar to that in Momentum, but the left and
    right motions of the ball are smaller. This advantage is caused by the adaptive
    adjustment of the learning rate.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 6.7*所示，Adam的更新路径就像是把球滚进碗里一样移动。这种运动类似于动量，但球的左右运动较小。这种优势是由学习率的自适应调整引起的。
- en: Note
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Adam has three hyperparameters. The first is the learning rate (appearing as
    α in the paper). The others are the coefficient for the primary moment, β1, and
    the coefficient for the secondary moment, β2\. The article states that the standard
    values are 0.9 for β1 and 0.999 for β2, which are effective in many cases.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Adam有三个超参数。第一个是学习率（在论文中表示为α）。其他两个是主动量的系数β1和次动量的系数β2。文章指出，标准值分别为β1为0.9，β2为0.999，在许多情况下非常有效。
- en: Which Update Technique Should We Use?
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们应该使用哪种更新技术？
- en: We have considered four-parameter updating techniques so far. Here, we will
    compare their results (the source code is located at `ch06/optimizer_compare_naive.py`).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经考虑了四种参数更新技术。在这里，我们将比较它们的结果（源代码位于`ch06/optimizer_compare_naive.py`）。
- en: 'As shown in *Figure 6.8*, different techniques use different paths to update
    the parameters. This image seems to show that AdaGrad is the best, but note that
    the results vary depending on the problems being solved. Naturally, the results
    also vary depending on the values of the hyperparameters (such as the learning
    rate):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 6.8*所示，不同的技术使用不同的路径来更新参数。这幅图似乎显示AdaGrad是最好的，但请注意，结果因解决的问题而异。当然，结果也会因超参数的值（如学习率）而异：
- en: '![Figure 6.8: Comparison of optimization techniques – SGD, Momentum, AdaGrad,
    and Adam'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.8: 优化技术的比较 – SGD、动量、AdaGrad 和 Adam'
- en: '](img/fig06_8.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_8.jpg)'
- en: 'Figure 6.8: Comparison of optimization techniques – SGD, Momentum, AdaGrad,
    and Adam'
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 6.8: 优化技术的比较 – SGD、动量、AdaGrad 和 Adam'
- en: 'So far, we have looked at four techniques: SGD, Momentum, AdaGrad, and Adam.
    But which should we use? Unfortunately, there is no one technique currently known
    that is good at solving all problems. Each has its own distinct characteristics
    and advantages, which make it better suited to certain problems over others. Therefore,
    it''s important to know which technique works best given a specific set of circumstances.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看过四种技术：SGD、动量、AdaGrad 和 Adam。但我们应该使用哪一种？遗憾的是，目前没有一种被广泛认可的技术能够解决所有问题。每种技术都有其独特的特点和优势，使其更适合某些问题而不适合其他问题。因此，了解在特定情况下哪种技术最有效非常重要。
- en: SGD is still used in a lot of research. Momentum and AdaGrad are also worth
    trying. Recently, many researchers and engineers seem to prefer Adam. This book
    mainly uses SGD and Adam. You can try the other techniques as you like.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: SGD仍然被广泛应用于许多研究中。动量和AdaGrad也值得尝试。最近，许多研究人员和工程师似乎更喜欢Adam。本书主要使用SGD和Adam。您可以根据需要尝试其他技术。
- en: Using the MNIST Dataset to Compare the Update Techniques
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用MNIST数据集比较更新技术
- en: 'For handwritten digit recognition, we will compare the four techniques we''ve
    described so far: SGD, Momentum, AdaGrad, and Adam. Let''s explore how each technique
    works in the progress of training. *Figure 6.9* shows the results (the source
    code is located at `h06/optimizer_compare_mnist.py`):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于手写数字识别，我们将比较迄今为止描述的四种技术：SGD、动量法、AdaGrad 和 Adam。让我们探讨每种技术在训练过程中的工作原理。*图 6.9*
    显示了结果（源代码位于 `h06/optimizer_compare_mnist.py`）：
- en: '![Figure 6.9: Using the MNIST dataset to compare the four update techniques
    – the horizontal axis indicates the iterations of learning, while the vertical
    axis indicates the values of the loss function'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.9：使用 MNIST 数据集比较四种更新技术——横轴表示学习的迭代次数，纵轴表示损失函数的值'
- en: '](img/fig06_9.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_9.jpg)'
- en: 'Figure 6.9: Using the MNIST dataset to compare the four update techniques –
    the horizontal axis indicates the iterations of learning, while the vertical axis
    indicates the values of the loss function'
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.9：使用 MNIST 数据集比较四种更新技术——横轴表示学习的迭代次数，纵轴表示损失函数的值
- en: This experiment used a five-layer neural network, and each layer had 100 neurons.
    ReLU was used as the activation function.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本实验使用了一个五层神经网络，每层有 100 个神经元。ReLU 被用作激活函数。
- en: The result of *Figure 6.9* shows that other techniques learned faster than SGD.
    It seems that the remaining three techniques learned similarly quickly. When we
    look closer, it seems that AdaGrad learned a little faster. In this experiment,
    note that the results are different depending on the hyperparameter of the learning
    rate and the structure of the neural network (the number of layers). However,
    generally, the other three techniques can learn faster than SGD and sometimes
    achieve better final recognition performance.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.9* 的结果显示，其他技术的学习速度比 SGD 更快。似乎剩下的三种技术学习得差不多快。当我们仔细观察时，AdaGrad 学得略快一些。在本实验中，值得注意的是，结果会因为学习率的超参数和神经网络的结构（层数）而有所不同。然而，一般来说，其他三种技术的学习速度通常会比
    SGD 快，有时还能获得更好的最终识别性能。'
- en: Initial Weight Values
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始权重值
- en: The initial weight values are especially important in neural network training.
    What values are set as the initial weight values often determines the success
    or failure of neural network training. In this section, we will explain the recommended
    initial weight values, then conduct an experiment to check that they accelerate
    neural network learning.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 初始权重值在神经网络训练中尤为重要。设置什么样的初始权重值通常决定了神经网络训练的成败。本节将解释推荐的初始权重值，并通过实验验证它们是否能加速神经网络的学习。
- en: How About Setting the Initial Weight Values to 0?
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何将初始权重值设置为 0？
- en: Later, we will look at a technique called weight decay, which reduces overfitting
    and improves generalization performance. In short, weight decay is a technique
    that reduces the values of the weight parameters to prevent overfitting.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后，我们将介绍一种叫做权重衰减的技术，它可以减少过拟合并提高泛化性能。简而言之，权重衰减是一种减少权重参数值以防止过拟合的技术。
- en: If we want the weights to be small, starting with the smallest possible initial
    values is probably a good approach. Here, we use an initial weight value such
    as `0.01 * np.random.randn(10, 100)`. This small value is the value generated
    from the Gaussian distribution multiplied by 0.01—a Gaussian distribution with
    a standard deviation of 0.01.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望权重较小，从最小的初始值开始可能是一个不错的方法。在这里，我们使用 `0.01 * np.random.randn(10, 100)` 作为初始权重值。这个小值是从高斯分布中生成的，乘以
    0.01——高斯分布的标准差为 0.01。
- en: If we want the weight values to be small, how about setting all the initial
    weight values to 0? This is a bad idea as it prevents us from training correctly.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望权重值较小，怎样将所有初始权重值设置为 0 呢？这是一个糟糕的主意，因为它会阻止我们正确地训练。
- en: Why should the initial weight values not be 0? Or in other words, why should
    the weights not be uniform values? Well, because all weight values are updated
    uniformly (in the same way) in backpropagation. So, say that layers 1 and 2 have
    0 as their weights in a two-layer neural network. Then, in forward propagation,
    the same value is propagated to all the neurons in layer 2 because the weight
    of the input layer is 0\. When the same values are entered for all the neurons
    in layer 2, all the weights in layer 2 are updated similarly in backward propagation
    (please remember "backward propagation in a multiplication node"). Therefore,
    the weights are updated with the same value and become symmetrical values (duplicate
    values). Due to this, there is no meaning in having many weights. To prevent the
    weights from being uniform or breaking their symmetrical structure, random initial
    values are required.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么初始权重值不应该是0？换句话说，为什么权重不应该是统一值？因为在反向传播中，所有权重值会以相同的方式均匀更新。所以，假设在一个两层神经网络中，第一层和第二层的权重都是0。那么，在前向传播时，输入层的所有神经元的值都会被传递到第二层，因为输入层的权重是0。当相同的值输入到第二层的所有神经元时，在反向传播时，第二层的所有权重都会以相同的方式更新（请记住“在乘法节点中的反向传播”）。因此，权重以相同的值更新，并且变成对称值（重复值）。因此，拥有许多权重没有意义。为了防止权重统一或破坏其对称结构，需要使用随机初始值。
- en: Distribution of Activations in the Hidden Layers
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐藏层激活值的分布
- en: 'Observing the distribution of activations (referring here to the output data
    after the activation function, though some literature calls the data that flows
    between layers an "activation") in the hidden layers provides a lot of information.
    Here, we will conduct a simple experiment to see how the initial weight values
    change the activations in the hidden layers. We will enter some randomly generated
    data into a five-layer neural network (using a sigmoid function as the activation
    function) and show the data distribution of the activations in each layer in a
    histogram. This experiment is based on the CS231n (*CS231n: Convolutional Neural
    Networks for Visual Recognition* ([http://cs231n.github.io/](http://cs231n.github.io/)))
    course at Stanford University.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '观察隐藏层中激活值的分布（这里指的是激活函数后的输出数据，尽管一些文献称流经各层的数据为“激活”）提供了很多信息。在这里，我们将进行一个简单的实验，看看初始权重值如何改变隐藏层中的激活值。我们将一些随机生成的数据输入到一个五层神经网络中（使用sigmoid函数作为激活函数），并通过直方图展示每一层激活值的分布。这个实验基于斯坦福大学的CS231n课程（*CS231n:
    卷积神经网络与视觉识别* ([http://cs231n.github.io/](http://cs231n.github.io/)))。'
- en: 'The source code for the experiment is located at `ch06/weight_init_activation_histogram.py`.
    The following is part of this code:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 实验的源代码位于`ch06/weight_init_activation_histogram.py`。以下是部分代码：
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here, there are five layers and that each layer has 100 neurons. As input data,
    1,000 pieces of data are generated at random with Gaussian distribution and are
    provided to the five-layer neural network. A sigmoid function is used as the activation
    function, and the activation results of each layer are stored in the `activations`
    variable. Please note the weight scale. Here, a Gaussian distribution with a standard
    deviation of 1 is being used. The purpose of this experiment is to observe how
    the distribution of `activations` changes by changing this scale (standard deviation).
    Now, let''s show the data of each layer that is stored in `activations` in a histogram:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有五层，每一层有100个神经元。作为输入数据，随机生成1,000个数据点，服从高斯分布，并提供给这五层神经网络。使用sigmoid函数作为激活函数，每一层的激活结果存储在`activations`变量中。请注意权重的规模。这里使用标准差为1的高斯分布。这个实验的目的是通过改变这个尺度（标准差）来观察`activations`的分布如何变化。现在，让我们展示存储在`activations`中的每一层数据，并以直方图的形式呈现：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Executing this code creates the histograms shown in the following image.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码将生成下图所示的直方图。
- en: This image shows that the activations of each layer are mainly 0 and 1\. The
    sigmoid function that's being used here is an S-curve function. As the output
    of the sigmoid function approaches 0 (or 1), the value of the differential approaches
    0\. Therefore, when the data is mainly 0s and 1s, the values of the gradients
    in backward propagation get smaller until they vanish. This is a problem called
    **gradient vanishing**. In deep learning, where there's a large number of layers,
    gradient vanishing can be a more serious problem.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 该图显示，每一层的激活值主要为 0 和 1。这里使用的 sigmoid 函数是一种 S 曲线函数。当 sigmoid 函数的输出接近 0（或 1）时，微分的值会接近
    0。因此，当数据主要是 0 和 1 时，反向传播中的梯度值会变得越来越小，直到消失。这就是所谓的**梯度消失**问题。在深度学习中，层数较多时，梯度消失问题可能会变得更加严重。
- en: 'Next, let''s conduct the same experiment, but this time with the standard deviation
    of the weights as 0.01\. To set the initial weight values, you will need to modify
    the previous code, as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们进行相同的实验，但这次权重的标准差为 0.01。为了设置初始权重值，您需要修改之前的代码，如下所示：
- en: '[PRE6]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Figure 6.10: Distribution of the activations of each layer when a Gaussian
    distribution with a standard deviation of 1 is used for the initial weight values'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.10：当使用标准差为 1 的高斯分布作为初始权重值时，各层激活值的分布'
- en: '](img/fig06_10.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_10.jpg)'
- en: 'Figure 6.10: Distribution of the activations of each layer when a Gaussian
    distribution with a standard deviation of 1 is used for the initial weight values'
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.10：当使用标准差为 1 的高斯分布作为初始权重值时，各层激活值的分布
- en: 'Observe the results. The following image shows the distribution of the activations
    of each layer when a Gaussian distribution with a standard deviation of 0.01 is
    used:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 观察结果。以下图像显示了当使用标准差为 0.01 的高斯分布作为初始权重值时，各层激活值的分布：
- en: '![Figure 6.11: Distribution of the activations of each layer when a Gaussian
    distribution with a standard deviation of 0.01 is used for the initial weight
    values'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.11：当使用标准差为 0.01 的高斯分布作为初始权重值时，各层激活值的分布'
- en: '](img/fig06_11.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_11.jpg)'
- en: 'Figure 6.11: Distribution of the activations of each layer when a Gaussian
    distribution with a standard deviation of 0.01 is used for the initial weight
    values'
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.11：当使用标准差为 0.01 的高斯分布作为初始权重值时，各层激活值的分布
- en: Now, the activations concentrate around 0.5\. Unlike the previous example, they
    are not biased toward 0 and 1\. The problem of gradient vanishing does not occur.
    However, when activations are biased, it causes a large problem in terms of its
    representation. If multiple neurons output almost the same values, there is no
    meaning in the existence of multiple neurons. For example, when 100 neurons output
    almost the same values, one neuron can represent almost the same thing. Therefore,
    the biased activations cause a problem because representation is limited.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，激活值集中在 0.5 附近。与之前的例子不同，它们并不偏向于 0 和 1。梯度消失问题没有发生。然而，当激活值存在偏差时，会在表示能力上造成很大问题。如果多个神经元输出几乎相同的值，那么这些神经元就没有存在的意义。例如，当
    100 个神经元输出几乎相同的值时，一个神经元就可以代表几乎相同的内容。因此，偏置的激活值会导致问题，因为表示能力受到限制。
- en: Note
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The distribution of the activations in each layer needs to be spread properly.
    This is because, when moderately diverse data flows in each layer, a neural network
    learns efficiently. On the other hand, when biased data flows, training may not
    go well because of the gradient vanishing and "limited representation."
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 各层激活值的分布需要适当地分散。这是因为，当每一层的输入数据适度多样时，神经网络能够高效地学习。另一方面，当数据有偏时，训练可能会因为梯度消失和“有限的表示能力”而变得不顺利。
- en: 'Next, we will use the initial weight values that were recommended in a paper
    by Xavier Glorot et al. (*Xavier Glorot and Yoshua Bengio (2010): Understanding
    the difficulty of training deep feedforward neural networks. In Proceedings of
    the International Conference on Artificial Intelligence and Statistics (AISTATS2010).
    Society for Artificial Intelligence and Statistics*). This is called "Xavier initialization."
    Currently, the Xavier initializer is usually used in ordinary deep learning frameworks.
    For example, in the Caffe framework, you can specify the `xavier` argument for
    the initial weight setting to use the Xavier initializer.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 Xavier Glorot 等人在论文中推荐的初始权重值（*Xavier Glorot 和 Yoshua Bengio（2010）：理解训练深度前馈神经网络的难度。发表于《国际人工智能与统计会议论文集》（AISTATS2010）。人工智能与统计学会*）。这就是所谓的“Xavier
    初始化”。目前，Xavier 初始化器通常在普通的深度学习框架中使用。例如，在 Caffe 框架中，你可以为初始权重设置指定 `xavier` 参数来使用
    Xavier 初始化器。
- en: 'Xavier''s paper obtained the appropriate scale of weights so that the activation
    of each layer was spread similarly. It concluded that distribution with a standard
    deviation of ![6a](img/Figure_6.11a.png) should be used when the number of nodes
    in the previous layer is n (Xavier''s paper suggested setting values that consider
    both the number of input nodes in the previous layer and the number of output
    nodes in the next layer. However, in framework implementations such as Caffe,
    the values are only calculated based on the input nodes in the previous layer
    for simplification, as described here). This can be seen in the following diagram:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Xavier 的论文得出了适当的权重尺度，以便每一层的激活值能够相似地分布。论文中指出，当前一层有 n 个节点时，应使用标准差为 ![6a](img/Figure_6.11a.png)
    的分布作为初始值（Xavier 的论文建议设置同时考虑前一层输入节点和下一层输出节点的数量。然而，在像 Caffe 这样的框架实现中，值仅基于前一层的输入节点数量计算，以简化实现，如这里所述）。这一点可以从下图中看到：
- en: '![Figure 6.12: Xavier initializer – when n nodes in the previous layer are
    connected, a distribution with the standard deviation of  is used for initial
    values'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.12：Xavier 初始化器 – 当前一层有 n 个节点连接时，初始值使用标准差为的分布'
- en: '](img/fig06_12.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_12.jpg)'
- en: 'Figure 6.12: Xavier initializer – when n nodes in the previous layer are connected,
    a distribution with the standard deviation of ![6b](img/Figure_6.12a.png) is used
    for initial values'
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.12：Xavier 初始化器 – 当前一层有 n 个节点连接时，初始值使用标准差为 ![6b](img/Figure_6.12a.png) 的分布
- en: 'When the Xavier initializer is used, since the number of nodes in the previous
    layer is larger, the weight scale that is set for the initial values for the target
    nodes is smaller. Now, let''s use the Xavier initializer to complete some experiments.
    You only have to modify the initial weight value, as follows (the implementation
    is simplified here because the number of nodes is 100 in all the layers):'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Xavier 初始化器时，由于前一层的节点数量较大，因此为目标节点设置的初始值权重的尺度较小。现在，让我们使用 Xavier 初始化器来完成一些实验。你只需修改初始权重值，如下所示（这里的实现进行了简化，因为所有层的节点数量均为
    100）：
- en: '[PRE7]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Figure 6.13: Distribution of the activations of each layer when the Xavier
    initializer is used as the initial weight value'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.13：当使用 Xavier 初始化器作为初始权重值时，各层激活值的分布'
- en: '](img/fig06_13.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_13.jpg)'
- en: 'Figure 6.13: Distribution of the activations of each layer when the Xavier
    initializer is used as the initial weight value'
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.13：当使用 Xavier 初始化器作为初始权重值时，各层激活值的分布
- en: The preceding image shows the results when the Xavier initializer is used. It
    shows that distributions are spread more widely, although a higher layer has a
    more distorted shape. We can expect that training is conducted efficiently because
    the data that flows in each layer is spread properly, and the representation of
    the sigmoid function is not limited.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了使用 Xavier 初始化器时的结果。它表明分布更广泛，尽管较高的层呈现出更扭曲的形状。我们可以预期，由于每层中流动的数据得到了正确分布，训练将会高效进行，并且
    sigmoid 函数的表示不会受到限制。
- en: Note
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Also, the distributions of the upper layers are slightly distorted in terms
    of their shape. The distorted shape is improved when a `tanh` function (hyperbolic
    function) is used instead of a `sigmoid` function. Actually, when a `tanh` function
    is used, distributions will have a bell shape. The `tanh` function is an S-curve
    function, like a `sigmoid` function. The `tanh` function is symmetrical about
    the origin (0, 0), while the `sigmoid` function is symmetrical about `(x, y) =
    (0, 0.5)`. It is best to use the `tanh` function so that the activation function
    is symmetrical about the origin.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，上层的分布在形状上略有扭曲。使用`tanh`函数（双曲函数）代替`sigmoid`函数时，扭曲的形状有所改善。实际上，当使用`tanh`函数时，分布将呈现钟形曲线。`tanh`函数是一种S型曲线函数，类似于`sigmoid`函数。`tanh`函数关于原点(0,
    0)对称，而`sigmoid`函数则关于`(x, y) = (0, 0.5)`对称。最好使用`tanh`函数，这样激活函数就能关于原点对称。
- en: Initial Weight Values for ReLU
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ReLU的初始权重值
- en: 'The Xavier initializer is based on the assumption that the activation function
    is linear. The Xavier initializer is suitable because the `sigmoid` and `tanh`
    functions are symmetrical and can be regarded as linear functions around their
    centers. Meanwhile, for ReLU, using the initial value is recommended. This is
    known as the He initializer and was recommended by Kaiming He and et. al. *(Kaiming
    He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun (2015): Delving Deep into Rectifiers:
    Surpassing Human-Level Performance on ImageNet Classification. In 1026 – 1034*).
    The He initializer uses a Gaussian distribution with a standard deviation of ![6d](img/Figure_6.13a.png)
    when the number of nodes in the previous layer is n. When we consider that the
    Xavier initializer is ![6e](img/Figure_6.13b.png), we can assume (intuitively)
    that the coefficient must be doubled to provide more spread because a negative
    area is 0 for ReLU.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Xavier初始化器基于激活函数是线性的假设。Xavier初始化器是合适的，因为`sigmoid`和`tanh`函数是对称的，可以在其中心附近被视为线性函数。与此同时，对于ReLU，建议使用初始值。这被称为He初始化器，由Kaiming
    He等人提出并推荐（*Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun (2015)：深入研究整流器：超越ImageNet分类中的人类水平表现。在1026
    – 1034*）。He初始化器使用标准差为![6d](img/Figure_6.13a.png)的高斯分布，当前一层的节点数为n时。如果我们考虑到Xavier初始化器是![6e](img/Figure_6.13b.png)，我们可以直观地假设系数必须加倍，以提供更广的分布，因为对于ReLU，负区域为0。
- en: Let's look at the distribution of activations when ReLU is used as the activation
    function. We will consider the results of three experiments after using a Gaussian
    distribution with a standard deviation of 0.01 (that is, `std=0.01`), the Xavier
    initializer, and the He initializer, which is specifically used for ReLU (*Figure
    6.14*).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下当ReLU作为激活函数时激活分布的情况。我们将考虑在使用标准差为0.01（即`std=0.01`）的高斯分布、Xavier初始化器和专为ReLU设计的He初始化器后进行的三个实验的结果（*图6.14*）。
- en: 'The results indicate that the activations of each layer are very small (the
    averages of the distributions are as follows: layer 1: 0.0396, layer 2: 0.00290,
    layer 3: 0.000197, layer 4: 1.32e-5, and layer 5: 9.46e-7) for `std=0.01`. When
    small data flows through a neural network, the gradients of the weights in backward
    propagation are also small. This is a serious problem as training will barely
    advance.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，当`std=0.01`时，每一层的激活值都非常小（各层分布的平均值如下：第1层：0.0396，第2层：0.00290，第3层：0.000197，第4层：1.32e-5，第5层：9.46e-7）。当小数据流过神经网络时，反向传播中权重的梯度也很小。这是一个严重的问题，因为训练几乎无法推进。
- en: Next, let's look at the results from using the Xavier initializer. This shows
    that the bias becomes larger little by little as the layers become deeper—as do
    the activations. Gradient vanishing will be a problem when it comes to training.
    On the other hand, for the He initializer, the spread of Gaussian distribution
    in each layer is similar. The spread of data is similar even when the layers are
    deeper. So, we can expect that appropriate values also flow for backward propagation.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看一下使用Xavier初始化器的结果。这表明，随着层数加深，偏置逐渐增大，激活值也一样。训练时会出现梯度消失的问题。另一方面，对于He初始化器，每一层中高斯分布的扩展是相似的。即使层数更深，数据的扩展也相似。因此，我们可以预期在反向传播时也能流动适当的值。
- en: In summary, when you use ReLU as the activation function, use the He initializer,
    and for S-curve functions such as `sigmoid` and `tanh`, use the Xavier initializer.
    As of the time of writing, this is the best practice.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，当使用ReLU作为激活函数时，应使用He初始化器；对于像`sigmoid`和`tanh`这样的S型曲线函数，应使用Xavier初始化器。至于目前写作时，这是最佳实践。
- en: Using the MNIST Dataset to Compare the Weight Initializers
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用MNIST数据集比较权重初始化器
- en: 'Let''s use actual data to see how neural network learning is affected by different
    weight initializers. We will use `std=0.01`, the Xavier initializer, and the He
    initializer in our experiments (the source code is located at `ch06/weight_init_compare.py`).
    The following image shows the results:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用实际数据来看不同权重初始化器如何影响神经网络的学习。我们将在实验中使用`std=0.01`、Xavier初始化器和He初始化器（源代码位于`ch06/weight_init_compare.py`）。下图展示了结果：
- en: '![Figure 6.14: Change of activation distribution by weight initializers when
    ReLU is used'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.14：当使用ReLU作为激活函数时，权重初始化器对激活值分布的变化'
- en: as the activation function
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 作为激活函数
- en: '](img/fig06_14.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_14.jpg)'
- en: 'Figure 6.14: Change of activation distribution by weight initializers when
    ReLU is used as the activation function'
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.14：当使用ReLU作为激活函数时，权重初始化器对激活值分布的变化
- en: 'This experiment uses a five-layer neural network (100 neurons in each layer)
    and ReLU as the activation function. The results shown in the following image
    reveal that no learning is conducted for `std=0.01`. This is because small values
    (data near 0) flow in forward propagation, as we observed in the distribution
    of activations earlier. Thus, the gradients to obtain are also small in backward
    propagation, resulting in few updates occurring for the weights. On the other
    hand, training is performed smoothly for the Xavier and He initializers. The following
    image also shows that training advances fast for the He initializer:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 本实验使用一个五层的神经网络（每层100个神经元）和ReLU作为激活函数。下图所示的结果揭示了对于`std=0.01`，没有进行学习。这是因为在前向传播时，较小的值（接近0的数据）流动，就像我们在激活值分布中看到的那样。因此，在反向传播时计算得到的梯度也很小，导致权重更新的次数很少。另一方面，Xavier和He初始化器的训练过程顺利进行。下图也显示了对于He初始化器，训练进展较快：
- en: '![Figure 6.15: Using the MNIST dataset to compare the weight initializers –
    the horizontal axis indicates the iterations of training, while the vertical axis
    indicates the values of the loss function'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.15：使用MNIST数据集比较权重初始化器——横轴表示训练的迭代次数，纵轴表示损失函数的值'
- en: '](img/fig06_15.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_15.jpg)'
- en: 'Figure 6.15: Using the MNIST dataset to compare the weight initializers – the
    horizontal axis indicates the iterations of training, while the vertical axis
    indicates the values of the loss function'
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.15：使用MNIST数据集比较权重初始化器——横轴表示训练的迭代次数，纵轴表示损失函数的值
- en: As we have seen, the initial weight values are very important in neural network
    training. They often determine their success or failure. Although the importance
    of the initial weight values is sometimes overlooked, the starting (initial) value
    is important for everything.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，初始权重值在神经网络训练中非常重要。它们往往决定了训练的成功与否。尽管初始权重值的重要性有时被忽视，但起始（初始）值对一切都是重要的。
- en: Batch Normalization
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量归一化
- en: In the previous section, we observed the distribution of activations in each
    layer. We learned that the appropriate initial weight values provide a proper
    spread for the distribution of activations of each layer, thus enabling smooth
    training. So, how about adjusting the distribution of activations "forcefully"
    so that there's a proper spread in each layer?
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们观察了每一层激活值的分布。我们了解到，适当的初始权重值为每层激活值的分布提供了一个适当的扩展，从而使训练过程顺利进行。那么，强制调整激活值的分布，使其在每一层中都有适当的扩展，效果如何呢？
- en: 'This technique is based on the idea of batch normalization (*Sergey Ioffe and
    Christian Szegedy (2015): Batch Normalization: Accelerating Deep Network Training
    by Reducing Internal Covariate Shift. arXiv:1502.03167[cs] (February 2015)*).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术基于批量归一化的思想（*Sergey Ioffe和Christian Szegedy（2015）：批量归一化：通过减少内部协变量偏移加速深度网络训练。arXiv:1502.03167[cs]（2015年2月）*）。
- en: Batch Normalization Algorithm
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量归一化算法
- en: Batch normalization (also known as batch norm) was first proposed in 2015\.
    Although batch norm is a new technique, it is widely used by many researchers
    and engineers. In fact, in competitions surrounding machine learning, batch norm
    often achieves excellent results.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化（也称为batch norm）最早在2015年提出。尽管批量归一化是一项新技术，但它已被许多研究人员和工程师广泛应用。事实上，在围绕机器学习的竞赛中，批量归一化常常能够取得优异的成绩。
- en: 'Batch norm attracts a lot of attention due to the following advantages:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化因其以下优点而受到广泛关注：
- en: It can accelerate learning (it can increase the learning rate).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以加速学习（可以增加学习率）。
- en: It is not as dependent on the initial weight values (you do not need to be cautious
    about the initial values).
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不那么依赖于初始权重值（你不需要对初始值过于小心）。
- en: It reduces overfitting (it reduces the necessity of dropout).
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它减少了过拟合（减少了对dropout的需求）。
- en: The first advantage is particularly attractive because deep learning takes a
    lot of time. With batch norm there's no need to be anxious about the initial weight
    values, and due to it reducing overfitting, it removes this cause of anxiety from
    deep learning.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个优点特别吸引人，因为深度学习需要大量时间。通过批量归一化，初始权重值无需过于担心，而且由于它减少了过拟合，它消除了深度学习中的这种焦虑来源。
- en: 'As we described earlier, the purpose of batch norm is to adjust the distribution
    of the activations in each layer so that it has a proper spread. To do that, the
    layer that normalizes data distribution is inserted into a neural network as the
    batch normalization layer (also known as the batch norm layer), as shown in the
    following diagram:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所述，批量归一化的目的是调整每一层激活值的分布，使其具有适当的分布范围。为此，规范化数据分布的层被插入到神经网络中，成为批量归一化层（也称为批量归一化层），如下面的图所示：
- en: '![Figure 6.16: Neural network example that uses batch normalization (the batch
    norm layers'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.16：使用批量归一化的神经网络示例（批量归一化层以灰色显示）'
- en: are shown in gray)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以灰色显示）
- en: '](img/fig06_16.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_16.jpg)'
- en: 'Figure 6.16: Neural network example that uses batch normalization (the batch
    norm layers are shown in gray)'
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.16：使用批量归一化的神经网络示例（批量归一化层以灰色显示）
- en: 'As its name indicates, batch norm normalizes each mini-batch that is used for
    training. Specifically, it normalizes data so that the average is 0 and the variance
    is 1\. The following equation shows this:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名称所示，批量归一化对用于训练的每个小批次进行规范化。具体来说，它将数据规范化，使得平均值为0，方差为1。以下方程展示了这一点：
- en: '| ![77](img/Figure_6.16a.png) | (6.7) |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| ![77](img/Figure_6.16a.png) | (6.7) |'
- en: Here, a set of m input data, b ![78](img/Figure_6.16d.png), is treated as a
    mini-batch and its average, ![79](img/Figure_6.16e.png), and variance, ![80](img/Figure_6.16f.png),
    are calculated. The input data is normalized so that its average is 0 and its
    variance is 1 for the appropriate distribution. In equation 6.7, ε is a small
    value (such as 10e-7). This prevents division by 0.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，一组m个输入数据，b ![78](img/Figure_6.16d.png)，被视为一个小批次，并计算其平均值，![79](img/Figure_6.16e.png)，和方差，![80](img/Figure_6.16f.png)。输入数据被规范化，使其平均值为0，方差为1，以实现适当的分布。在方程6.7中，ε是一个小值（如10e-7）。这可以防止除以0的情况。
- en: 'Equation 6.7 simply converts the input data for a mini-batch, ![81](img/Figure_6.16g.png),
    into data with an average of 0 and a variance of 1, ![82](img/Figure_6.16h.png).
    By inserting this process before (or after) the activation function (see (*Sergey
    Ioffe and Christian Szegedy (2015): Batch Normalization: Accelerating Deep Network
    Training by Reducing Internal Covariate Shift. arXiv:1502.03167[cs] (February
    2015)*) and (*Dmytro Mishkin and Jiri Matas (2015): All you need is a good init.
    arXiv:1511.06422[cs] (November 2015)*) for a discussion (and experiments) on whether
    batch normalization should be inserted before or after the activation function),
    you can reduce the distribution bias of the data.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 方程6.7简单地将一个小批次的输入数据，![81](img/Figure_6.16g.png)，转换为平均值为0、方差为1的数据，![82](img/Figure_6.16h.png)。通过将此过程插入激活函数之前（或之后）（请参见*Sergey
    Ioffe 和 Christian Szegedy (2015)：批量归一化：通过减少内部协变量偏移来加速深度网络训练. arXiv:1502.03167[cs]
    (2015年2月)*和*Dmytro Mishkin 和 Jiri Matas (2015)：你需要的只是一个好的初始化. arXiv:1511.06422[cs]
    (2015年11月)*讨论（和实验）是否应将批量归一化插入激活函数之前或之后），你可以减少数据的分布偏差。
- en: 'In addition, the batch norm layer converts the normalized data with a peculiar
    scale and shift. The following equation shows this conversion:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，批量归一化层将数据规范化为具有特殊缩放和平移的形式。以下方程展示了这一转换：
- en: '| ![83](img/Figure_6.16i.png) | (6.8) |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| ![83](img/Figure_6.16i.png) | (6.8) |'
- en: Here, γ and β are parameters. They start with γ = 1 and β = 0 and will be adjusted
    to the appropriate values through training.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，γ 和 β 是参数。它们从 γ = 1 和 β = 0 开始，并将通过训练调整为适当的值。
- en: This is the algorithm of batch norm. This algorithm provides the forward propagation
    in a neural network. By using a computational graph, as described in *Chapter
    5*, *Backpropagation*, we can represent batch norm as follows.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这是批量归一化的算法。该算法提供了神经网络中的前向传播。通过使用计算图，如*第5章*《反向传播》所述，我们可以将批量归一化表示如下。
- en: 'We won''t go into detail about how to derive backward propagation in batch
    norm here because it is a little complicated. When you use a computational graph,
    such as the one shown in the following image, you can derive the backward propagation
    of batch norm relatively easily. Frederik Kratzert''s blog, *Understanding the
    Backward Pass through the Batch Normalization Layer* ([https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)),
    provides a detailed description of this. Please refer to it if you are interested:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不会详细讨论如何推导批量归一化的反向传播，因为它有些复杂。当你使用计算图时，像下图所示的图，你可以相对容易地推导出批量归一化的反向传播。Frederik
    Kratzert 的博客 *理解批量归一化层中的反向传播* ([https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html))
    提供了详细的描述。如果你感兴趣，请参考：
- en: '![Figure 6.17: Computational graph of batch normalization'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.17：批量归一化的计算图'
- en: '](img/fig06_17.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_17.jpg)'
- en: 'Figure 6.17: Computational graph of batch normalization'
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.17：批量归一化的计算图
- en: Note
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '*Figure 6.17* is cited from reference, *Frederik Kratzert''s blog "Understanding
    the backward pass through Batch Normalization Layer"* ([https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.17* 引用了参考文献，*Frederik Kratzert 的博客 “理解批量归一化层中的反向传播”* ([https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html))。'
- en: Evaluating Batch Normalization
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估批量归一化
- en: Now, let's use the batch norm layer to conduct some experiments. First, we will
    use the MNIST dataset to see how the progress of learning changes with and without
    the batch norm layer (the source code can be found at `ch06/batch_norm_test.py`).
    *Figure 6.18* shows the result.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用批量归一化层进行一些实验。首先，我们将使用 MNIST 数据集，看看在有无批量归一化层的情况下，学习进度如何变化（源代码可以在 `ch06/batch_norm_test.py`
    找到）。*图 6.18* 展示了结果。
- en: '*Figure 6.18* shows that batch norm accelerates training. Next, let''s see
    how the progress of training changes when various scales for the initial values
    are used. *Figure 6.19* contains graphs that show the progress of training when
    the standard deviations of the initial weight values are changed.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.18* 显示了批量归一化加速训练的效果。接下来，我们看看在使用不同初始值规模时，训练进度如何变化。*图 6.19* 包含了当初始权重值的标准差发生变化时，训练进度的图表。'
- en: This indicates that batch norm accelerates training in almost all cases. In
    fact, when batch norm is not used, training does not advance at all without a
    good scale of initial values.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明在几乎所有情况下，批量归一化都能加速训练。事实上，当不使用批量归一化时，如果没有合适的初始值规模，训练根本无法进行。
- en: As we have seen, using batch norm can accelerate training and provides robustness
    to the initial weight values ("robustness to the initial values" means having
    a little dependence on them). Batch norm will play an active part in many situations
    because it has such wonderful characteristics.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，使用批量归一化可以加速训练，并且为初始权重值提供了鲁棒性（“鲁棒性”意味着对初始值的依赖较小）。由于其如此优秀的特性，批量归一化将在许多情况下发挥积极作用。
- en: Regularization
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化
- en: '**Overfitting** often creates difficulties in machine learning problems. In
    overfitting, the model fits the training data too well and cannot properly handle
    other data that is not contained in the training data. Machine learning aims at
    generalizing performance. It is desirable for the model to properly recognize
    unknown data that is not contained in the training data. While you can create
    a complicated and representative model this way, reducing overfitting is also
    important:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**过拟合** 通常会给机器学习问题带来困难。在过拟合的情况下，模型过度拟合训练数据，无法正确处理不包含在训练数据中的其他数据。机器学习的目标是泛化性能。理想情况下，模型应该能够正确识别不包含在训练数据中的未知数据。虽然你可以通过这种方式创建一个复杂且具有代表性的模型，但减少过拟合同样很重要：'
- en: '![Figure 6.18: Effect of batch norm – batch norm accelerates learning'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.18：批量归一化的效果——批量归一化加速学习'
- en: '](img/fig06_18.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_18.jpg)'
- en: 'Figure 6.18: Effect of batch norm – batch norm accelerates learning'
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.18：批量归一化的效果——批量归一化加速学习
- en: Overfitting
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过拟合
- en: 'The main two causes of overfitting are as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合的主要原因有两个：
- en: The model has many parameters and is representative.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型参数较多，且具有代表性。
- en: The training data is insufficient.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据不足。
- en: 'Here, we will generate overfitting by providing these two causes. Out of 60,000
    pieces of training data in the MNIST dataset, only 300 are provided, and a seven-layer
    network is used to increase the network''s complexity. It has 100 neurons in each
    layer. ReLU is used as the activation function:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过提供这两个原因来产生过拟合。在MNIST数据集中，60,000条训练数据中仅提供了300条，并且使用了一个七层的网络来增加网络的复杂性。每层有100个神经元。ReLU作为激活函数：
- en: '![Figure 6.19: The solid lines show the results of using batch norm, while
    the dotted lines show the results without it – the title of each graph indicates
    the standard deviation of the initial weight values'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.19：实线表示使用批归一化的结果，虚线表示未使用批归一化的结果——每个图的标题表示初始权重值的标准差'
- en: '](img/fig06_19.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_19.jpg)'
- en: 'Figure 6.19: The solid lines show the results of using batch norm, while the
    dotted lines show the results without it – the title of each graph indicates the
    standard deviation of the initial weight values'
  id: totrans-202
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.19：实线表示使用批归一化的结果，虚线表示未使用批归一化的结果——每个图的标题表示初始权重值的标准差
- en: 'The following is part of the code for this experiment (the source file is at
    `ch06/overfit_weight_decay.py`). First, the code loads the data:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本实验的一部分代码（源文件位于`ch06/overfit_weight_decay.py`）。首先，代码加载数据：
- en: '[PRE8]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following code conducts training. Here, the recognition accuracy is calculated
    for each epoch for all the training data and all the test data:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码进行训练。在这里，识别准确率对于每个训练周期的所有训练数据和测试数据都进行计算：
- en: '[PRE9]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `train_acc_list` and `test_acc_list` lists store the recognition accuracies
    for each epoch. An epoch indicates that all the training data has been used. Let's
    draw graphs based on these lists (`train_acc_list` and `test_acc_list`). The following
    plot shows the results.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_acc_list` 和 `test_acc_list` 列表存储了每个周期的识别准确率。一个周期表示所有训练数据已被使用。我们基于这些列表（`train_acc_list`
    和 `test_acc_list`）绘制图表。以下图表显示了结果。'
- en: 'The recognition accuracies that were measured using the training data reached
    almost 100% after 100 epochs, but the recognition accuracies on the test data
    are far below 100%. These large differences are caused by overfitting the training
    data. This graph shows that the model cannot handle general data (test data) that
    was not used in training properly:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练数据测量的识别准确率在100个周期后几乎达到了100%，但在测试数据上的识别准确率远低于100%。这些大的差异是由于训练数据的过拟合造成的。该图显示了模型无法正确处理在训练中未使用的通用数据（测试数据）：
- en: '![Figure 6.20: Transition of recognition accuracies for the training data (train)
    and test data (test)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.20：训练数据（train）和测试数据（test）识别准确率的变化'
- en: '](img/fig06_20.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_20.jpg)'
- en: 'Figure 6.20: Transition of recognition accuracies for the training data (train)
    and test data (test)'
  id: totrans-211
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.20：训练数据（train）和测试数据（test）识别准确率的变化
- en: Weight Decay
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权重衰减
- en: The **weight decay** technique has often been used to reduce overfitting. It
    avoids overfitting by imposing a penalty on large weights during training. Overfitting
    often occurs when a weight parameter takes a large value.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**权重衰减**技术常用于减少过拟合。通过在训练过程中对大权重施加惩罚，它避免了过拟合。当一个权重参数取较大值时，往往会发生过拟合。'
- en: As described earlier, the purpose of neural network training is to reduce the
    value of the loss function. For example, you can add the squared norm (L2 norm)
    of the weight to the loss function. Then, you could prevent the weight from being
    large. When the weights are W, the L2 norm of the weight decay is ![84](img/Figure_6.20a.png).
    This ![85](img/Figure_6.20b.png) is added to the loss function. Here, λ is the
    hyperparameter that controls the strength of regularization. If you set a larger
    value to λ, you can impose a stronger penalty on a large weight. ![88](img/Figure_6.20c.png)
    at the beginning of ![86](img/Figure_6.20b.png) is a constant for adjustment so
    that the differential of ![87](img/Figure_6.20b.png) is λW.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，神经网络训练的目的是减少损失函数的值。例如，您可以将权重的平方范数（L2范数）添加到损失函数中。这样，您就可以防止权重过大。当权重为W时，权重衰减的L2范数为![84](img/Figure_6.20a.png)。此项![85](img/Figure_6.20b.png)被添加到损失函数中。在这里，λ是控制正则化强度的超参数。如果您将较大的值设置给λ，您可以对较大的权重施加更强的惩罚。![88](img/Figure_6.20c.png)出现在![86](img/Figure_6.20b.png)的开始处，是一个常数，用于调整，以便![87](img/Figure_6.20b.png)的微分为λW。
- en: Weight decay adds ![89](img/Figure_6.20b.png) to the loss function for all weights.
    Therefore, the differential of the regularization term, λW, is added to the result
    of backpropagation when calculating the gradient of a weight.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 权重衰减将![89](img/Figure_6.20b.png)添加到所有权重的损失函数中。因此，在计算权重的梯度时，正则化项λW的微分会被添加到反向传播的结果中。
- en: The L2 norm is the sum of squares of each element. In addition to the L2 norm,
    L1 and L ∞ norms also exist. The L1 norm is the sum of absolute values, that is,
    |w1| + |w2| + ... + |wn|. The L ∞ norm is also called the max norm. It is the
    largest among the absolute values of all the elements. You can use any of these
    norms as a regularization term. Each has its own characteristics, but we will
    only implement the L2 norm here since it's the most commonly used.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: L2范数是每个元素的平方和。除了L2范数，还有L1范数和L ∞范数。L1范数是绝对值之和，即|w1| + |w2| + ... + |wn|。L ∞范数也称为最大范数，是所有元素绝对值中的最大值。你可以选择这些范数中的任何一个作为正则化项。每个范数都有其特点，但这里我们只实现L2范数，因为它是最常用的。
- en: 'Now, let''s conduct an experiment. We will apply the weight decay of λ= 0.1
    to the preceding experiment. The following plot shows the results (the network
    that supports weight decay is located at `common/multi_layer_net.py` and the code
    for the experiment is located at `ch06/overfit_weight_decay.py`):'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进行一个实验。我们将在前述实验中应用λ= 0.1的权重衰减。下图显示了实验结果（支持权重衰减的网络位于`common/multi_layer_net.py`，实验代码位于`ch06/overfit_weight_decay.py`）：
- en: '![Figure 6.21: Transition of recognition accuracies for the training data (train)
    and test data (test) when weight decay is used'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.21：使用权重衰减时，训练数据（train）和测试数据（test）识别准确率的变化'
- en: '](img/fig06_21.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_21.jpg)'
- en: 'Figure 6.21: Transition of recognition accuracies for the training data (train)
    and test data (test) when weight decay is used'
  id: totrans-220
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.21：使用权重衰减时，训练数据（train）和测试数据（test）识别准确率的变化
- en: The preceding image shows that the recognition accuracies of the training data
    and test data are different, but that the difference is smaller than in the one
    shown in *Figure 6.20* where weight decay was not used. This indicates that overfitting
    was reduced. Note that the recognition accuracies of the training data have not
    reached 100% (1.0).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了训练数据和测试数据的识别准确率有所不同，但与*图6.20*（未使用权重衰减）所示的情况相比，差异较小。这表明过拟合已被减少。注意，训练数据的识别准确率尚未达到100%（1.0）。
- en: Dropout
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dropout
- en: 'The previous section described the weight decay technique. It adds the L2 norm
    of the weights to the loss function to reduce overfitting. Weight decay is easy
    to implement and can reduce overfitting to some extent. However, as a neural network
    model becomes more complicated, weight decay is often insufficient. This is when
    the dropout technique (*N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,
    and R. Salakhutdinov (2014): Dropout: A simple way to prevent neural networks
    from overfitting. The Journal of Machine Learning Research, pages 1929 – 1958,
    2014*) is often used.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节描述了权重衰减技术。它将权重的L2范数添加到损失函数中以减少过拟合。权重衰减实现简单，并且可以在一定程度上减少过拟合。然而，随着神经网络模型的复杂化，权重衰减往往不足够。这时，常常使用Dropout技术（*N.
    Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, 和 R. Salakhutdinov（2014）：Dropout：一种防止神经网络过拟合的简单方法。《机器学习研究杂志》，1929–1958页，2014年*）。
- en: 'Dropout erases neurons at random during training. During training, it selects
    neurons in a hidden layer at random to erase them. As shown in the following image,
    the erased neurons do not transmit signals. During training, the neurons to be
    erased are selected at random each time data flows. During testing, the signals
    of all the neurons are propagated. The output of each neuron is multiplied by
    the rate of the erased neurons during training:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout在训练过程中随机删除神经元。训练时，它会随机选择隐藏层中的神经元并将其删除。如下面的图所示，被删除的神经元不会传递信号。训练时，每次数据流动时，都会随机选择要删除的神经元。测试时，所有神经元的信号都会传播。每个神经元的输出会乘以训练时被删除神经元的比率：
- en: '![Figure 6.22: Concept of dropout'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.22：Dropout概念'
- en: '](img/fig06_22.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_22.jpg)'
- en: 'Figure 6.22: Concept of dropout'
  id: totrans-227
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.22：Dropout概念
- en: Note
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '*Figure 6.22* is cited from reference, *N. Srivastava, G. Hinton, A. Krizhevsky,
    I. Sutskever, and R. Salakhutdinov (2014): Dropout: A simple way to prevent neural
    networks from overfitting. The Journal of Machine Learning Research pages 1929–1958,
    2014*.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.22* 引用自文献，*N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, 和 R.
    Salakhutdinov（2014）：Dropout：一种防止神经网络过拟合的简单方法。《机器学习研究杂志》，1929–1958页，2014年*。'
- en: The left-hand image shows an ordinary neural network, while the right-hand image
    shows a network that dropout has been applied to. Dropout selects neurons at random
    and erases them to stop the transmission of subsequent signals.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧图片展示了普通神经网络，而右侧图片展示了应用了dropout的网络。Dropout随机选择神经元并将其删除，以阻止后续信号的传递。
- en: 'Now, let''s implement dropout. Simplicity is emphasized in the implementation
    here. If appropriate calculation is conducted during training, we only have to
    flow data through forward propagation (without multiplying the rate of the erased
    neurons). Such an implementation is conducted in deep learning frameworks. For
    efficient implementation, the dropout implemented in the Chainer framework, for
    example, may be useful:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现dropout。这里实现的重点是简洁性。如果在训练过程中进行适当的计算，我们只需通过前向传播流动数据（无需乘以被删除神经元的比率）。这种实现方式在深度学习框架中得到了广泛应用。例如，Chainer框架中实现的dropout可能会很有用：
- en: '[PRE10]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Please note that, in each forward propagation, the neurons to erase are stored
    as `False` in `self.mask`. `self.mask` generates an array of the same shape as
    `x` at random and sets the elements to `True` when their values are larger than
    `dropout_ratio`. The behavior in backward propagation is the same as that in ReLU.
    If a neuron is passed a signal in forward propagation, it passes the received
    signal without changing it in backward propagation. If a neuron doesn't pass a
    signal in forward propagation, it stops the received signal in backward propagation.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在每次前向传播中，要删除的神经元会在`self.mask`中被标记为`False`。`self.mask`会随机生成一个与`x`形状相同的数组，并在其值大于`dropout_ratio`时将对应元素设置为`True`。反向传播中的行为与ReLU相同。如果一个神经元在前向传播中传递了信号，它会在反向传播中不加改变地传递该信号。如果一个神经元在前向传播中没有传递信号，它将在反向传播中停止该信号的传递。
- en: We will use the MNIST dataset to validate the effect of dropout. The source
    code can be found in `ch06/overfit_dropout.py`. It uses the `Trainer` class to
    simplify implementation.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用MNIST数据集来验证dropout的效果。源代码可以在`ch06/overfit_dropout.py`中找到。它使用`Trainer`类来简化实现。
- en: The `Trainer` class is implemented in `common/trainer.py`. It conducts network
    training that has been conducted so far in this chapter. For details, please see
    `common/trainer.py` and `ch06/overfit_dropout.py`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`Trainer`类在`common/trainer.py`中实现。它执行了本章迄今为止的网络训练。有关详细信息，请参见`common/trainer.py`和`ch06/overfit_dropout.py`。'
- en: To experiment with dropout, we'll use a seven-layer network (where 100 neurons
    exist in each layer and ReLU is used as the activation function), as in the previous
    experiment. One of the experiments will use dropout, while the other won't. The
    following image shows the results.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实验dropout，我们将使用一个七层的网络（每层包含100个神经元，并且ReLU作为激活函数），与之前的实验相同。一个实验将使用dropout，而另一个则不使用。下图展示了结果。
- en: 'As we can see, using dropout reduces the difference between the recognition
    accuracies of training data and test data. It also indicates that the recognition
    accuracy of the training data has not reached 100%. Due to this, you can use dropout
    to reduce overfitting, even in a representative network:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，使用dropout可以减少训练数据和测试数据之间的识别准确率差异。它还表明，训练数据的识别准确率并没有达到100%。因此，即使在一个典型的网络中，你也可以使用dropout来减少过拟合：
- en: '![Figure 6.23: The left-hand image shows the experiment without dropout, while
    the right-hand image shows the experiment with dropout (dropout_rate=0.15)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.23：左侧图片展示了没有使用dropout的实验，右侧图片展示了使用了dropout（dropout_rate=0.15）的实验）'
- en: '](img/fig06_23.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_23.jpg)'
- en: 'Figure 6.23: The left-hand image shows the experiment without dropout, while
    the right-hand image shows the experiment with dropout (dropout_rate=0.15)'
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.23：左侧图片展示了没有使用dropout的实验，右侧图片展示了使用了dropout（dropout_rate=0.15）的实验
- en: Note
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: In machine learning, ensemble learning is often used in which multiple models
    learn separately, and their multiple outputs are averaged through prediction.
    For example, when we use it in a neural network, we prepare five networks with
    the same (or similar) structure and train each of them. Then, we average the five
    outputs during testing to obtain the result. Experiments have shown that ensemble
    learning improves a neural network's recognition accuracy by several percent.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，常常使用集成学习，其中多个模型分别学习，然后通过预测对它们的多个输出进行平均。例如，当我们在神经网络中使用集成学习时，我们准备五个具有相同（或相似）结构的网络并训练每一个。然后，在测试时，我们将五个输出的平均值作为结果。实验表明，集成学习能将神经网络的识别准确率提高几个百分点。
- en: Ensemble learning is close to dropout. Erasing neurons at random while training
    in dropout can be interpreted as providing a different model to learn data each
    time. While predicting, the output from the neurons is multiplied by the rate
    of the erasures (0.5, for example) to average the models. Thus, we can say that
    dropout simulates ensemble learning in one network.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习与 dropout 很相似。在 dropout 中随机删除神经元的过程可以解释为每次提供一个不同的模型来学习数据。在预测时，神经元的输出会乘以删除率（例如
    0.5）来对模型进行平均。因此，我们可以说 dropout 在一个网络中模拟了集成学习。
- en: Validating Hyperparameters
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证超参数
- en: A neural network uses many hyperparameters, as well as parameters such as weights
    and biases. The hyperparameters here include the number of neurons in each layer,
    batch size, the learning rate for updating parameters, and weight decay. Setting
    the hyperparameters to inappropriate values deteriorates the performance of the
    model. The values of these hyperparameters are very important, but determining
    them usually requires a lot of trial and error. This section describes how to
    search for hyperparameter values as efficiently as possible.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络使用许多超参数，以及如权重和偏置等参数。这里的超参数包括每层神经元的数量、批次大小、更新参数的学习率以及权重衰减。将超参数设置为不合适的值会导致模型性能下降。这些超参数的值非常重要，但确定它们通常需要大量的反复试验。本节将介绍如何尽可能高效地搜索超参数值。
- en: Validation Data
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证数据
- en: In the dataset we've used so far, the training data and test data are separate.
    The training data is used to train a network, while the test data is used to evaluate
    generalization performance. Thus, you can determine whether or not the network
    conforms too well only to the training data (that is, whether overfitting occurs)
    and how large the generalization performance is.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们到目前为止使用的数据集中，训练数据和测试数据是分开的。训练数据用于训练网络，而测试数据用于评估泛化性能。因此，你可以判断网络是否过于拟合训练数据（即是否发生了过拟合），以及其泛化性能有多大。
- en: We will use various hyperparameter settings for validation. Please note that
    you must not use test data to evaluate the performance of hyperparameters. This
    is very important but is often overlooked.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用不同的超参数设置进行验证。请注意，绝不能使用测试数据来评估超参数的性能。这一点非常重要，但往往被忽视。
- en: So, why can't we use test data to evaluate the performance of hyperparameters?
    Well, if we use test data to adjust hyperparameters, the hyperparameter values
    will overfit the test data. In other words, it uses test data to check that the
    hyperparameter values are "good," so the hyperparameter values are adjusted so
    that they only fit the test data. Here, the model may provide low generalization
    performance and cannot fit other data.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么不能使用测试数据来评估超参数的性能呢？如果使用测试数据来调整超参数，超参数的值会过拟合测试数据。换句话说，它使用测试数据来检查超参数值是否“合适”，因此调整超参数值使其仅适应测试数据。在这种情况下，模型可能会提供较低的泛化性能，并且无法适应其他数据。
- en: Therefore, we need to use verification data (called **validation data**) to
    adjust them. This validation data is used to evaluate the quality of our hyperparameters.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要使用验证数据（称为**验证数据**）来调整它们。这些验证数据用于评估我们超参数的质量。
- en: Training data is used for learning parameters (weights and biases). Validation
    data is used to evaluate the performance of hyperparameters. Test data is used
    (once, ideally) at the end of training to check generalization performance.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据用于学习参数（权重和偏置）。验证数据用于评估超参数的性能。测试数据则用于（理想情况下只用一次）训练结束后检查泛化性能。
- en: 'Some datasets provide training data, validation data, and test data separately.
    Some provide only training data and test data, while some provide only one type
    of data. In that case, you must separate the data manually. For the MNIST dataset,
    the simplest way to obtain the validation data is to separate 20% of the training
    data beforehand and use that as validation data. The following code shows this:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据集提供单独的训练数据、验证数据和测试数据。有些则只提供训练数据和测试数据，而有些只提供一种类型的数据。在这种情况下，你必须手动分离数据。对于 MNIST
    数据集，获取验证数据的最简单方法是预先分离出 20% 的训练数据并将其作为验证数据。以下代码展示了这一点：
- en: '[PRE11]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, the input data and labeled data are shuffled before separating the training
    data. This is because some datasets may have biased data (for example, numbers
    "0" to "10" are arranged in this order). The `shuffle_dataset` function uses `np.random.shuffle`
    and is contained in `common/util.py`.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，输入数据和标注数据在分离训练数据之前会进行洗牌。这是因为某些数据集可能存在偏差（例如，数字“0”到“10”按顺序排列）。`shuffle_dataset`函数使用`np.random.shuffle`，并包含在`common/util.py`中。
- en: Next, let's use validation data to look at the technique that's used for optimizing
    hyperparameters.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用验证数据来看一下优化超参数的技术。
- en: Optimizing Hyperparameters
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化超参数
- en: What is important when optimizing hyperparameters is to gradually narrow down
    the range where "good" hyperparameter values exist. To do this, we will set a
    broad range initially, select hyperparameters at random from the range (sampling),
    and use the sampled values to evaluate the recognition accuracy. Next, we will
    repeat these steps several times and observe the result of the recognition accuracy.
    Based on the result, we will narrow down the range of "good" hyperparameter values.
    By repeating this procedure, we can gradually limit the range of appropriate hyperparameters.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化超参数时，重要的是逐步缩小包含“好”超参数值的范围。为此，我们将首先设置一个较大的范围，从该范围内随机选择超参数（采样），并使用采样得到的值来评估识别准确率。接下来，我们将重复这些步骤几次，并观察识别准确率的结果。根据结果，我们将缩小“好”超参数值的范围。通过重复这个过程，我们可以逐渐限制合适超参数的范围。
- en: 'It has been reported that random sampling before a search provides better results
    than a systematic search, such as a grid search, to optimize hyperparameters in
    a neural network (*James Bergstra and Yoshua Bengio (2012): Random Search for
    Hyper-Parameter Optimization. Journal of Machine Learning Research 13, Feb (2012),
    281 – 305*). This is because the degree by which the final recognition accuracy
    will be affected is different among different hyperparameters.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 有报告称，在进行搜索之前进行随机采样比系统性搜索（如网格搜索）在优化神经网络的超参数时提供更好的结果 (*James Bergstra 和 Yoshua
    Bengio (2012)：随机搜索超参数优化. 机器学习研究期刊 13, 2012年2月，281–305*)。这是因为不同的超参数对最终识别准确率的影响程度不同。
- en: Specifying a "broad" range of hyperparameters is effective. We will specify
    the range in "powers of 10," such as from 0.001 (10−3) to 1,000 (103) (this is
    also called "specifying on a log scale").
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 指定“广泛”范围的超参数是有效的。我们将以“10的幂”为单位指定范围，例如从0.001（10^−3）到1000（10^3）（这也叫做“在对数尺度上指定”）。
- en: 'Please note that when optimizing hyperparameters, deep learning takes a lot
    of time (even a few days or weeks). Therefore, any hyperparameters that seem inappropriate
    must be abandoned while searching for them. When optimizing hyperparameters, it
    is effective to reduce the size of epoch for training to shorten the time that
    one evaluation takes. We discussed the optimization of hyperparameters previously.
    The following summarizes this discussion:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在优化超参数时，深度学习需要大量时间（甚至可能需要几天或几周）。因此，在寻找超参数时，任何看似不合适的超参数必须被舍弃。在优化超参数时，减少训练的epoch大小以缩短每次评估所需的时间是有效的。我们之前已经讨论过超参数的优化，以下是对此讨论的总结：
- en: '**Step 0**'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 0**'
- en: Specify the range of the hyperparameters.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 指定超参数的范围。
- en: '**Step 1**'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1**'
- en: Sample the hyperparameters from the range at random.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 从范围内随机采样超参数。
- en: '**Step 2**'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2**'
- en: Use the hyperparameter values sampled in *Step 1* for training and use the validation
    data to evaluate the recognition accuracy (set small epochs).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 使用在*步骤 1*中采样得到的超参数值进行训练，并使用验证数据评估识别准确率（设置较小的epoch）。
- en: '**Step 3**'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3**'
- en: Repeat *steps 1* and *2* a certain number of times (such as 100 times) and narrow
    down the range of hyperparameters based on the result of the recognition accuracy.
    When the range is narrowed down to some extent, select one hyperparameter value
    from it. This is one practical approach to optimizing hyperparameters.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 重复*步骤 1*和*步骤 2*若干次（例如100次），并根据识别准确率的结果逐步缩小超参数的范围。当范围缩小到一定程度时，从中选择一个超参数值。这是优化超参数的一个实际方法。
- en: Note
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'However, you may feel that this approach is the "wisdom" of engineers rather
    than science. If you need a more refined technique for optimizing hyperparameters,
    you can use **Bayesian optimization**. It makes good use of mathematical theories
    such as Bayes'' theorem to provide stricter and more efficient optimization. For
    details, please see the paper *Practical Bayesian Optimization of Machine Learning
    Algorithms* (*Jasper Snoek, Hugo Larochelle, and Ryan P. Adams (2012): Practical
    Bayesian Optimization of Machine Learning Algorithms. In F. Pereira, C. J. C.
    Burges, L. Bottou, & K. Q. Weinberger, eds. Advances in Neural Information Processing
    Systems 25\. Curran Associates, Inc., 2951 – 2959*).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你可能会觉得这种方法更多是工程师的“智慧”而非科学。如果你需要一种更精细的优化超参数的技术，可以使用**贝叶斯优化**。它很好地利用了贝叶斯定理等数学理论，提供了更严格且更高效的优化。详细内容请参见论文
    *Practical Bayesian Optimization of Machine Learning Algorithms*（*Jasper Snoek,
    Hugo Larochelle, and Ryan P. Adams (2012)：Practical Bayesian Optimization of Machine
    Learning Algorithms. In F. Pereira, C. J. C. Burges, L. Bottou, & K. Q. Weinberger,
    eds. Advances in Neural Information Processing Systems 25. Curran Associates,
    Inc., 2951 – 2959*）。
- en: Implementing Hyperparameter Optimization
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现超参数优化
- en: 'Now, let''s use the MNIST dataset to optimize some hyperparameters. We will
    look for two hyperparameters: the learning rate and the weight decay rate. The
    weight decay rate controls the strength of weight decay. This problem and solution
    are based on the *CS231n* (*CS231n: Convolutional Neural Networks for Visual Recognition*
    ([http://cs231n.github.io/](http://cs231n.github.io/))) course at Stanford University.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，让我们使用 MNIST 数据集来优化一些超参数。我们将寻找两个超参数：学习率和权重衰减率。权重衰减率控制权重衰减的强度。这个问题和解决方案基于斯坦福大学的*CS231n*（*CS231n:
    Convolutional Neural Networks for Visual Recognition*（[http://cs231n.github.io/](http://cs231n.github.io/)））课程。'
- en: 'As described earlier, hyperparameters are validated by sampling them at random
    from the range on a log scale, such as from 0.001 (10−3) to 1,000 (103). We can
    write this as `10 ** np.random.uniform(-3, 3)` in Python. This experiment will
    start with a range from 10−8 to 10−4 for the weight decay rate and from 10−6 to
    10−2 for the learning rate. In this case, we can write the random sampling of
    the hyperparameters as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，超参数是通过在对数尺度范围内随机采样来验证的，例如从 0.001（10−3）到 1,000（103）。我们可以在 Python 中写成 `10
    ** np.random.uniform(-3, 3)`。这个实验将从权重衰减率的范围 10−8 到 10−4 以及学习率的范围 10−6 到 10−2 开始。在这种情况下，我们可以将超参数的随机采样写成如下形式：
- en: '[PRE12]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, the hyperparameters were sampled at random, and the sampled values were
    used for training. Then, training is repeated several times by using various hyperparameter
    values to find where the appropriate hyperparameters exist. Here, the details
    of implementation have been omitted, and only the result has been shown. The source
    code for optimizing hyperparameters is located at `ch06/hyperparameter_optimization.py`.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，超参数是随机采样的，采样值用于训练。然后，使用不同的超参数值重复训练多次，以寻找适合的超参数值。这里省略了实现的细节，只显示了结果。优化超参数的源代码位于`ch06/hyperparameter_optimization.py`。
- en: 'When we have a range of 10−8 to 10−4 for the weight decay rate and a range
    of 10−6 to 10−2 for the learning rate, we get the following results. Here, we
    can see the transitions in learning the validation data in descending order of
    high-recognition accuracies:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有权重衰减率的范围 10−8 到 10−4 和学习率的范围 10−6 到 10−2 时，我们得到以下结果。在这里，我们可以看到在学习验证数据时，按高识别准确率的降序排列的过渡：
- en: '![Figure 6.24: The solid lines show the recognition accuracies of the validation
    data, while the dotted lines show the recognition accuracies of the training data'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.24：实线表示验证数据的识别准确率，虚线表示训练数据的识别准确率'
- en: '](img/fig06_24.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_24.jpg)'
- en: 'Figure 6.24: The solid lines show the recognition accuracies of the validation
    data, while the dotted lines show the recognition accuracies of the training data'
  id: totrans-279
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.24：实线表示验证数据的识别准确率，虚线表示训练数据的识别准确率
- en: 'This indicates that the training advanced smoothly from `Best-1` to `Best-5`.
    Let''s check the hyperparameter values (that is, the learning rate and weight
    decay rate) of `Best-1` to `Best-5`. These are the results:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明训练从 `Best-1` 到 `Best-5` 顺利进行。让我们检查一下 `Best-1` 到 `Best-5` 的超参数值（即学习率和权重衰减率）。这些是结果：
- en: '[PRE13]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, we can see that when the learning rate was 0.001 to 0.01 and the weight
    decay rate was 10−8 to 10−6, learning advanced well. Due to this, the range of
    the hyperparameters where training is likely to succeed is observed to narrow
    the range of values. You can repeat the same procedure in the narrowed range.
    Thus, you can narrow the range where appropriate hyperparameters exist and select
    each of the final hyperparameters at a certain stage.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，当学习率为0.001到0.01，权重衰减率为10−8到10−6时，学习进展良好。因此，观察到训练成功的超参数范围有所缩小。你可以在缩小的范围内重复相同的过程，从而逐步缩小适合超参数的范围，并在某个阶段选择最终的超参数。
- en: Summary
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'This chapter described some important techniques that are used for neural network
    training. How to update parameters, how to specify initial weight values, batch
    normalization, and dropout are all essential techniques that are used in modern
    neural networks. The techniques described here are often used in state-of-the-art
    deep learning. In this chapter, we learned about the following:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一些用于神经网络训练的重要技术。如何更新参数、如何指定初始权重值、批量归一化和Dropout是现代神经网络中使用的基本技术。这里描述的技术通常用于最先进的深度学习。在本章中，我们学习了以下内容：
- en: 'Four famous methods for updating parameters: Momentum, AdaGrad, Adam, and SGD.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四种著名的参数更新方法：Momentum、AdaGrad、Adam和SGD。
- en: How to specify initial weight values, which is very important if we wish to
    train correctly.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何指定初始权重值，这是正确训练时非常重要的。
- en: The Xavier initializer and He initializer, which are effective as initial weight
    values.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xavier初始化器和He初始化器，这些是有效的初始权重值。
- en: Batch normalization accelerates training and provides robustness to the initial
    weight values.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量归一化加速训练并提高初始权重值的鲁棒性。
- en: Weight decay and dropout are regularization techniques that are used to reduce overfitting.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重衰减和Dropout是减少过拟合的正则化技术。
- en: To search for good hyperparameters, gradually narrowing down the range where
    appropriate values exist is an efficient method.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找合适超参数的有效方法是逐渐缩小合适值所在的范围。
