- en: Deep Learning for Games
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 游戏中的深度学习
- en: Welcome to *Hands-on Deep Learning for Games*. This book is for anyone wanting
    an extremely practical approach to the complexity of **deep learning** (**DL**)
    for games. Importantly, the concepts discussed in this book aren't solely limited
    to games. Much of what we'll learn here will easily carry over to other applications/simulations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到*《游戏中的深度学习实践》*。本书适用于任何希望以极具实践性的方式学习游戏中的**深度学习**（**DL**）的人。值得注意的是，本书讨论的概念不仅仅局限于游戏。我们在这里学到的许多内容将轻松地转移到其他应用或模拟中。
- en: '**Reinforcement learning** (**RL**), which will be a core element we talk about
    in later chapters, is quickly becoming the dominant **machine learning** (**ML**)
    technology. It has been applied to everything from server optimization to predicting
    customer activity for retail markets. Our journey in this book will primarily
    be focused on game development, and our goal will be to build a working adventure
    game. Keep in the back of your mind how the same principles you discover in this
    book could be applied to other problems, such as simulations, robotics, and lots
    more.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**（**RL**），这是我们在后续章节中将讨论的核心内容之一，正迅速成为主流的**机器学习**（**ML**）技术。它已经应用于从服务器优化到预测零售市场客户活动等方方面面。本书的旅程将主要集中在游戏开发上，我们的目标是构建一个可运行的冒险游戏。请始终记住，您在本书中发现的相同原理也可以应用于其他问题，比如模拟、机器人技术等。'
- en: 'In this chapter, we are going to start from the very basics of neural networks
    and deep learning. We will discuss the background of neural networks, working
    our way toward building a neural network that can play a simple text game. Specifically,
    this chapter will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从神经网络和深度学习的基本知识开始。我们将讨论神经网络的背景，并逐步构建一个能够玩简单文本游戏的神经网络。具体来说，本章将涉及以下主题：
- en: The past, present, and future of DL
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的过去、现在与未来
- en: Neural networks – the foundation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络 – 基础
- en: Multilayer perceptron in **TensorFlow** (**TF**)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**TensorFlow**（**TF**）中实现多层感知机
- en: Understanding TensorFlow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解TensorFlow
- en: Training neural networks with backpropagation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用反向传播训练神经网络
- en: Building an Autoencoder in Keras
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Keras中构建自动编码器
- en: This book assumes that you have a working knowledge of Python. You should be
    able to set up and activate a virtual environment. Later chapters will use Unity
    3D, which is limited to Windows and macOS (apologies to those hardcore Linux users).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本书假设您具备Python的基础知识。您应该能够设置并激活虚拟环境。后续章节将使用Unity 3D，该软件仅限于Windows和macOS（对硬核Linux用户表示歉意）。
- en: You might be inclined to skip this chapter if you've already grasped deep learning.
    Regardless, this chapter is well worth reading and will establish the terminology we
    use throughout the book. At the very least, do the hands-on exercise—you will
    thank yourself later!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经掌握了深度学习，您可能会倾向于跳过这一章。然而，无论如何，这一章非常值得一读，并将为我们在全书中使用的术语奠定基础。至少做一下动手练习——您稍后会感谢自己的！
- en: The past, present, and future of DL
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的过去、现在与未来
- en: While the term *de**ep* *learning* was first associated with neural networks
    in 2000 by Igor Aizenberg and colleagues, it has only become popular in the last
    5 years. Prior to this, we called this type of algorithm an **artificial neural
    network **(**ANN**). However, deep learning refers to something broader than ANNs
    and includes many other areas of connected machines. Therefore, to clarify, we
    will be discussing the ANN form of DL for much of the remainder of this book.
    However, we will also discuss some other forms of DL that can be used in games,
    in [Chapter 5](6ca7a117-1a8c-49f9-89c0-ee2f2a1e8baf.xhtml), *Introducing DRL*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然*深*度*学习*这一术语最早是由Igor Aizenberg及其同事在2000年与神经网络相关联的，但它在过去五年中才真正流行开来。在此之前，我们称这种类型的算法为**人工神经网络**（**ANN**）。然而，深度学习所指的是比人工神经网络更广泛的内容，涵盖了许多其他领域的互联机器。因此，为了澄清，我们将在本书的后续部分主要讨论ANN形式的深度学习。不过，我们也会在[第五章](6ca7a117-1a8c-49f9-89c0-ee2f2a1e8baf.xhtml)中讨论一些可以在游戏中使用的其他形式的深度学习，*介绍DRL*。
- en: The past
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过去
- en: The first form of a **multilayer perceptron** (**MLP**) network, or what we
    now  call an ANN, was introduced by Alexey Ivakhnenko in 1965\. Ivakhnenko waited
    several years before writing about the multilayer perceptron in 1971\. The concept
    took a while to percolate and it wasn't until the 1980s that more research began.
    This time, image classification and speech recognition were attempted, and they
    failed, but progress was being made. It took another 10 years, and in the late
    90s, it became popular again. So much so that ANNs made their way into some games,
    again, until better methods came along. Things quietened down and another decade
    or so passed.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**多层感知器**（**MLP**）网络的第一个形式，或者我们现在称之为人工神经网络（ANN），是由Alexey Ivakhnenko于1965年提出的。Ivakhnenko等了好几年才在1971年写下关于多层感知器的文章。这个概念花了一些时间才被理解，直到1980年代才开始有更多的研究。这一次，尝试了图像分类和语音识别，虽然失败了，但进展已经开始。又过了10年，到了90年代末，人工神经网络再次流行起来。流行的程度甚至让ANN进入了某些游戏，直到更好的方法出现。之后局势平静下来，又过了大约十年。'
- en: Then, in 2012, Andrew Ng and Jeff Dean used an ANN to recognize cats in videos,
    and the interest in deep learning exploded. Their stride was one of several trivial
    (yet entertaining) advancements which made people sit up and take notice of deep
    learning. Then, in 2015, Google's **DeepMind** team built AlphaGo, and this time
    the whole world sat up. AlphaGo was shown to solidly beat the best players in
    the world at the game of Go, and that changed everything. Other techniques soon
    followed, **Deep Reinforcement Learning** (**DRL**) being one, showing that human
    performance could be consistently beaten in areas where that was previously not
    thought of as possible.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在2012年，Andrew Ng 和 Jeff Dean 使用人工神经网络（ANN）来识别视频中的猫咪，深度学习的兴趣爆发了。他们的进展是若干微不足道（但有趣的）突破之一，使得人们开始关注深度学习。接着，在2015年，谷歌的**DeepMind**团队开发了AlphaGo，这一次全世界都注意到了。AlphaGo被证明能够轻松战胜世界上最顶尖的围棋选手，这改变了一切。很快，其他技术也跟进，**深度强化学习**（**DRL**）就是其中之一，证明了在以前被认为不可能的领域，人类的表现可以被持续超越。
- en: 'While teaching their students about neural networks, there is a humorous and
    pertinent tale professors enjoy sharing: *The US Army did early research in the
    ''80s using an ANN to recognize enemy tanks. The algorithm worked 100% of the
    time, and the army organized a big demonstration to showcase its success. Unfortunately,
    nothing** worked at the demonstration, and every test failed miserably. After
    going back and analyzing things, the army realized the ANN wasn''t recognizing
    enemy tanks at all. Instead, it had been trained on images taken on a cloudy day,
    and all it was doing was recognizing the clouds.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在教授学生们神经网络时，教授们喜欢分享一个幽默且贴切的故事：*美国陆军在80年代做过早期研究，使用人工神经网络识别敌方坦克。这个算法100%有效，陆军还组织了一个大型演示来展示其成功。不幸的是，在演示中什么都没能正常工作，每个测试都惨败。回去分析之后，陆军才意识到这个人工神经网络根本没有识别敌方坦克。相反，它是经过在多云天拍摄的图像进行训练的，它做的只是识别云层。*
- en: The present
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在的情况
- en: At present, at least at the time of writing, we are still in the midst of a
    deep learning explosion with debris and chaos, and it is our job as developers
    to make sense of all this. Neural networks are currently the basis for many DL
    technologies, several of which we will cover in this book. Except, it seems that every
    day, new and more powerful techniques emerge, and researchers scramble to make
    sense of them. Now, this rush of ideas can actually stall a technology, as researchers
    spend more and more time trying to replicate results. It is most certainly a cause
    for much of the earlier stalling that ANNs (deep learning) previously suffered
    from. In fact, many skeptics in the industry are predicting yet another hiccup.
    So, should you be worried, is reading this book worth it? The short answer is
    *yes*. The long answer is *probably not,* this time things are very different
    and many deep learning concepts are now generating revenue, which is a good thing.
    The fact that DL technology is now a proven money-earner puts investors at ease
    and only encourages new investment and growth. Exactly how much growth is yet
    to be seen, but the machine and DL space is now ripe with opportunity and growth
    from all sectors.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，至少在写作时，我们仍处于深度学习爆炸的中期，充满了碎片和混乱，作为开发者，我们的任务就是理清这一切。神经网络目前是许多深度学习技术的基础，其中几项我们将在本书中讲解。只是，似乎每天都有新的、更强大的技术出现，研究人员争先恐后地去理解它们。实际上，这种思想的激增可能会使一项技术陷入停滞，因为研究人员花费越来越多的时间试图复制结果。这无疑是先前人工神经网络（深度学习）停滞不前的主要原因之一。事实上，行业中许多怀疑者预测，这种情况可能会再次发生。那么，你应该担心吗？读这本书值得吗？简短的回答是*值得*。长答案是*可能不值得*，这一次的情况非常不同，许多深度学习概念现在已经在创造收入，这是一个好兆头。深度学习技术现在是经过验证的赚钱工具，这让投资者感到放心，并且鼓励新的投资和增长。究竟增长会有多大还未可知，但机器和深度学习领域现在充满了各行业的机会和增长。
- en: So, is it still possible that the game industry will again turn its back on
    games? That is also unlikely, generally because many of the more recent major advances,
    like reinforcement learning, were built to play classic Atari games, and use games
    as the problem. This only encourages more research into deep learning using games.
    Unity 3D, the game platform, has made a major investment into reinforcement learning
    for playing games. In fact, Unity is developing some of the most cutting-edge
    technology in reinforcement learning and we will be working with this platform
    later. Unity does use C# for scripting but uses Python to build and train deep
    learning models.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，游戏行业是否还可能再次抛弃游戏？这也不太可能，通常是因为许多最近的重要进展，如强化学习，都是为了玩经典的Atari游戏而构建的，并以游戏为问题。这只会促使更多的研究通过游戏来进行深度学习。游戏平台Unity
    3D已经对强化学习在游戏中的应用做出了重大投资。实际上，Unity正在开发一些最前沿的强化学习技术，我们稍后会与这个平台合作。Unity确实使用C#进行脚本编写，但使用Python来构建和训练深度学习模型。
- en: The future
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来
- en: Predicting the future of anything is extremely difficult, but if you watch carefully
    enough, you may gain some insight into what, where, or how things will develop.
    Of course, having a crystal ball or a well-trained neural network would certainly
    help, but a lot of what becomes popular often hinges on the next great achievement.
    Without any ability to predict that, what can we observe about the current trend
    in deep learning research and commercial development? Well, the current trend
    is to use ML to generate DL; that is, a machine essentially assembles itself a
    neural network that is addressed to solve a problem. Google is currently investing
    considerable resources into building a technology called **AutoML**, which generates
    a neural network inference model that can recognize objects/activities in images,
    speech recognition, or handwriting recognition, and more. Geoffery Hinton, who
    is often cited as the godfather of the ANN, has recently shown that complex deep
    network systems can be decomposed into reusable layers. Essentially, you can construct
    a network using layers extracted from various pre-trained models. This will certainly
    evolve into more interesting tech and plays well into the DL search but also makes
    way for the next phase in computing.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 预测任何事物的未来都是极其困难的，但如果你足够仔细地观察，可能会对事物的发展方向、发展地点或发展方式有所洞察。当然，拥有一个水晶球或训练有素的神经网络肯定会有所帮助，但许多流行的事物往往依赖于下一个伟大的成就。没有任何预测的能力，我们可以观察到深度学习研究和商业开发中当前的趋势是什么吗？嗯，目前的趋势是使用机器学习（ML）来生成深度学习（DL）；也就是说，一台机器基本上会自己组装一个神经网络，解决一个问题。谷歌目前正在大量投资建设一项名为**AutoML**的技术，它可以生成一个神经网络推理模型，能够识别图像中的物体/活动、语音识别或手写识别等。Geoffery
    Hinton，通常被誉为人工神经网络的教父，最近展示了复杂的深度网络系统可以被分解成可重用的层。基本上，你可以使用从各种预训练模型中提取的层来构建一个网络。这无疑会发展成更有趣的技术，并且在深度学习的探索中发挥重要作用，同时也为计算的下一个阶段铺平道路。
- en: Now, programming code is going to become too tedious, difficult, and expensive
    at some point. We can already see this with the explosion of offshore development,
    with companies seeking the cheapest developers. It is now estimated that code
    costs an average of $10-$20 per line, yes, per *line*. So, at what point will
    the developer start building their code in the form of an ANN or **TensorFlow **(**TF**)
    inference graph? Well, for most of this book, the DL code we develop will be generated
    down to a TF inference graph; a brain, if you will. We will then use these brains
    in the last chapter of the book to build intelligence in our adventure game. The
    technique of building graph models is quickly becoming mainstream. Many online
    ML apps now allow users to build models that can recognize things in images, speech,
    and videos, all by just uploading training content and pressing a button. Does
    this mean that apps could be developed this way in the future without any programming?
    The answer is yes, and it is already happening.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，编程代码在某些时候将变得过于繁琐、困难和昂贵。我们已经能看到这种情况的爆发，许多公司正在寻找最便宜的开发人员。现在估计代码的平均成本是每行$10-$20，是的，**每行**。那么，开发人员在什么时候会开始以人工神经网络（ANN）或**TensorFlow**（**TF**）推理图的形式构建他们的代码呢？嗯，在本书的大部分内容中，我们开发的深度学习（DL）代码将生成到TF推理图，也可以说是一个大脑。我们将在书的最后一章使用这些“大脑”来构建我们冒险游戏中的智能。构建图模型的技术正迅速成为主流。许多在线机器学习应用程序现在允许用户通过上传训练内容并按下按钮来构建可以识别图像、语音和视频中的物体的模型。这是否意味着将来应用程序可以这样开发而不需要编程？答案是肯定的，而且这种情况已经在发生。
- en: Now that we have explored the past, present, and future of deep learning, we
    can start to dig into more of the nomenclature and how neural networks actually
    work, in the next section.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了深度学习的过去、现在和未来，接下来可以开始深入研究更多的术语以及神经网络是如何工作的，下一部分将会展开讨论。
- en: Neural networks – the foundation
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络 – 基础
- en: 'The inspiration for neural networks or multilayer perceptrons is the human
    brain and nervous system. At the heart of our nervous system is the neuron pictured
    above the computer analog, which is a perceptron:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络或多层感知器的灵感来源于人类的大脑和神经系统。我们神经系统的核心是上图所示的类比计算机的神经元，它就是一个感知器：
- en: '![](img/8455ea99-f7e7-4664-a948-86f3e3fb1811.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8455ea99-f7e7-4664-a948-86f3e3fb1811.png)'
- en: Example of human neuron beside a perceptron
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 人类神经元与感知器的示意图
- en: 'The neurons in our brain collect input, do something, and then spit out a response
    much like the computer analog, the **perceptron**. A perceptron takes a set of
    inputs, sums them all up, and passes them through an activation function. That
    activation function determines whether to send output, and at what level to send
    it when activated. Let''s take a closer look at the perceptron, as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们大脑中的神经元会收集输入，进行处理，然后像计算机的 **感知器** 一样输出响应。感知器接受一组输入，将它们加总，并通过激活函数处理。激活函数决定是否输出，以及在激活时以什么水平输出。让我们仔细看看感知器，具体如下：
- en: '![](img/9734ac86-d48c-4952-9f17-e790fe5fe746.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9734ac86-d48c-4952-9f17-e790fe5fe746.png)'
- en: Perceptron
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器
- en: 'On the left-hand side of the preceding diagram, you can see the set of inputs
    getting pushed in, plus a constant bias. We will get more into the bias later.
    Then the inputs are multiplied by a set of individual weights and passed through
    an activation function. In Python code, it is as simple as the one in `Chapter_1_1.py`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面图表的左侧，你可以看到一组输入被推送进来，并加上一个常数偏置。稍后我们会详细讨论这个偏置。然后，输入会被一组单独的权重相乘，并通过激活函数处理。在
    Python 代码中，它就像 `Chapter_1_1.py` 中的那样简单：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note how the `weights` list has one more element than the `inputs` list; that
    is to account for the bias (`weights[0]`). Other than that, you can see we just
    simply loop through the `inputs`, multiplying them by the designated weight and
    adding the bias. Then the `activation` is compared to `0.0`, and if it is greater
    than 0, we output. In this very simple example, we are just comparing the value
    to 0, which is essentially a simple step function. We will spend some time later
    revisiting various activation functions over and over again; consider this simple
    model an essential part of carrying out those functions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`weights` 列表比 `inputs` 列表多一个元素；这是为了考虑偏置（`weights[0]`）。除此之外，你可以看到我们只是简单地遍历
    `inputs`，将它们与指定的权重相乘并加上偏置。然后，将 `activation` 与 `0.0` 进行比较，如果大于 0，则输出。在这个非常简单的示例中，我们只是将值与
    0 进行比较，本质上是一个简单的阶跃函数。稍后我们会花时间多次回顾各种激活函数，可以认为这个简单模型是执行这些函数的基本组成部分。
- en: What is the output from the preceding block of sample code? See whether you
    can figure it out, or take the less challenging route and copy and paste it into
    your favorite Python editor and run it. The code will run as is and requires no
    special libraries.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例代码的输出是什么？看看你能否自己找出答案，或者采取更简单的方式，复制粘贴到你最喜欢的 Python 编辑器中并运行。代码将直接运行，无需任何特殊库。
- en: In the previous code example, we are looking at one point of input data, `[1,2]`,
    which is hardly useful when it comes to DL. DL models typically require hundreds,
    thousands, or even millions of data points or sets of input data to train and
    learn effectively. Fortunately, with one perceptron, the amount of data we need
    is less than 10.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们看到的是一个输入数据点 `[1,2]`，但在深度学习中，这样的单一数据点几乎没什么用处。深度学习模型通常需要数百、数千甚至数百万个数据点或数据集来进行有效的训练和学习。幸运的是，通过一个感知器，我们所需的数据量不到
    10 个。
- en: 'Let''s expand on the preceding example and run a training set of 10 points
    through the `perceptron_predict` function by opening up your preferred Python
    editor and following these steps:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们扩展前面的例子，并通过打开你喜欢的 Python 编辑器，按照以下步骤将一个包含 10 个点的训练集输入到 `perceptron_predict`
    函数中：
- en: We will use Visual Studio code for most of the major coding sections later in
    this book. By all means, use your preferred editor, but if you are relatively
    new to Python, give the code a try. Code is available for Windows, macOS, and
    Linux.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的后续章节中使用 Visual Studio Code 来处理大部分主要的编码部分。当然，你可以使用你喜欢的编辑器，但如果你是 Python
    新手，不妨试试这段代码。代码适用于 Windows、macOS 和 Linux。
- en: 'Enter the following block of code in your preferred Python editor or open `Chapter_1_2.py`
    from the downloaded source code:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你喜欢的 Python 编辑器中输入以下代码块，或者打开从下载的源代码中提取的 `Chapter_1_2.py`：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This code just extends the earlier example we looked at. In this case, we are
    testing multiple points of data defined in the `train` list. Then we just iterate
    through each item in the list and print out the predicted value.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码只是扩展了我们之前看到的例子。在这个例子中，我们正在测试定义在 `train` 列表中的多个数据点。然后，我们只需遍历列表中的每个项目，并打印出预测值。
- en: Run the code and observe the output. If you are unsure of how to run Python
    code, be sure to take that course first before going any further.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行代码并观察输出。如果你不确定如何运行 Python 代码，确保先学习相关课程再继续深入。
- en: You should see an output of repeating 1.0s, which essentially means all input
    values are recognized as the same. This is not something that is very useful.
    The reason for this is that we have not trained or adjusted the input weights
    to match a known output. What we need to do is train the weights to recognize
    the data, and we will look at how to do that in the next section.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到一个输出，重复显示1.0，这意味着所有输入值都被识别为相同的。这并不是很有用。原因在于我们没有训练或调整输入权重以匹配已知的输出。我们需要做的是训练这些权重以识别数据，接下来我们将看看如何做到这一点。
- en: Training a perceptron in Python
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Python中训练感知机
- en: 'Perfect! We created a simple perceptron that takes input and spits out output
    but doesn''t really do anything. Our perceptron needs to have its weights trained
    in order to actually do something. Fortunately, there is a well-defined method,
    known as **gradient descent**, that we can use to adjust each of those weights.
    Open up your Python editor again and update or enter the following code or open
    `Chapter_1_3.py` from the code download:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 完美！我们创建了一个简单的感知机，它接受输入并输出结果，但实际上并没有做任何事情。我们的感知机需要训练它的权重，才能真正发挥作用。幸运的是，有一种已定义的方法，叫做**梯度下降**，我们可以用它来调整这些权重。重新打开你的Python编辑器，更新或输入以下代码，或者从代码下载中打开`Chapter_1_3.py`：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `train_weights` function is new and will be used to train the perceptron
    using iterative error minimization and will be a basis for when we use gradient
    descent in more complex networks. There is a lot going on here, so we will break
    it down piece by piece. First, we initialize the `weights` list to a value of
    `0.0` with this line:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_weights`函数是新的，将用于通过迭代误差最小化来训练感知机，并且为我们在更复杂的网络中使用梯度下降打下基础。这里有很多内容，我们将逐步解析。首先，我们用这一行将`weights`列表初始化为`0.0`：'
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then we start training each epoch in a `for` loop. An **epoch** is essentially
    one pass through our training data. The reason we make multiple passes is to allow
    our weights to converge at a global minimum and not a local one. During each epoch,
    the weights are trained using the following equation:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们开始在`for`循环中训练每个周期。**周期（epoch）**本质上是通过我们的训练数据的一次传递。之所以需要多次传递，是为了让我们的权重在全局最小值而非局部最小值处收敛。在每个周期中，权重是通过以下方程训练的：
- en: '![](img/cc2e7fbe-9265-435e-bf04-279555c2ca09.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc2e7fbe-9265-435e-bf04-279555c2ca09.png)'
- en: 'Consider the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下内容：
- en: '![](img/6b8441cd-52a3-48a8-857d-e9280eb3ad9a.png) = weight'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/6b8441cd-52a3-48a8-857d-e9280eb3ad9a.png) = 权重'
- en: '![](img/e022d5c3-8424-4f64-8a6c-eec5b51c0975.png) = the rate at which the perceptron
    learns'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/e022d5c3-8424-4f64-8a6c-eec5b51c0975.png) = 感知机学习的速率'
- en: '![](img/f83850ef-a8ce-4917-8535-66c37dad9d62.png) = the labeled training value'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/f83850ef-a8ce-4917-8535-66c37dad9d62.png) = 标注的训练值'
- en: '![](img/c42999b3-45b8-4275-a516-20131ec52ec0.png) = the value returned from
    the perceptron'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/c42999b3-45b8-4275-a516-20131ec52ec0.png) = 感知机返回的值'
- en: '![](img/d9d34cbd-58a9-4f2a-b685-5c0997c5c4fc.png) = ![](img/5937c8c0-55a8-459d-aa32-fe722b51b262.png) -
    ![](img/3ec50e08-1a20-4317-b539-4bb85fc26986.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9d34cbd-58a9-4f2a-b685-5c0997c5c4fc.png) = ![](img/5937c8c0-55a8-459d-aa32-fe722b51b262.png)
    - ![](img/3ec50e08-1a20-4317-b539-4bb85fc26986.png)'
- en: 'The bias is trained in a similar manner, but just recall it is `weight`. Note
    also how we are labeling our data points now in the `train` list, with an end
    value of `0.0` or `1.0`. A value of `0.0` means *no match*, while a value of `1.0`
    means *perfect match*, as shown in the following code excerpt:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差以类似的方式进行训练，但只需记住它是`weight`。还需要注意的是，我们现在如何在`train`列表中标注数据点，结束值为`0.0`或`1.0`。`0.0`表示*不匹配*，而`1.0`表示*完全匹配*，如下代码片段所示：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This labeling of data is common in training neural networks and is called **supervised
    training**. We will explore other unsupervised and semi-supervised training methods
    in later chapters. If you run the preceding code, you will see the following output:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据标注在训练神经网络中非常常见，被称为**监督训练**。我们将在后续章节中探索其他无监督和半监督的训练方法。如果你运行之前的代码，你将看到以下输出：
- en: '![](img/2ce90bda-11c8-4219-b997-831957fcfed7.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ce90bda-11c8-4219-b997-831957fcfed7.png)'
- en: Example output from sample training run
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 示例输出来自样本训练运行
- en: Now, if you have some previous ML experience, you will immediately recognize
    the training wobbling going on around some local minima, making our training unable
    to converge. You will likely come across this type of wobble several more times
    in your DL career, so it is helpful to understand how to fix it.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你有一些机器学习经验，你会立即识别出训练过程中围绕某些局部最小值的波动，这导致我们的训练无法收敛。你可能在深度学习的过程中多次遇到这种波动，所以了解如何解决它是非常有帮助的。
- en: 'In this case, our issue is likely the choice of the `activation` function,
    which, as you may recall, was just a simple step function. We can fix this by
    entering a new function, called a **Rectified Linear Unit** (**ReLU**). An example
    of the `step` and `ReLU` functions, side by side, are shown in the following diagram:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的问题很可能出在`激活`函数的选择上，正如你可能记得的那样，它只是一个简单的步进函数。我们可以通过输入一个新的函数来解决这个问题，称为**修正线性单元**（**ReLU**）。下图展示了`step`和`ReLU`函数并排的示例：
- en: '![](img/d11b04f2-d5cc-4206-8b6c-05ef1427b83c.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d11b04f2-d5cc-4206-8b6c-05ef1427b83c.png)'
- en: Comparison of step and ReLU activation functions
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 步进函数与ReLU激活函数的对比
- en: 'In order to change the activation function, open up the previous code listing
    and follow along:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改激活函数，请打开之前的代码清单并跟随操作：
- en: 'Locate the following line of code:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定位以下代码行：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Modify it, like so:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像这样修改它：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: That subtle difference in multiplying the activation function by itself if its
    value is greater than 0 is the implementation of the `ReLU` function. Yes, it
    is that deceptively easy.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果激活函数的值大于0，则通过自身相乘的微妙差异即为`ReLU`函数的实现。是的，它看起来就这么简单。
- en: Run the code and observe the change in output.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行代码并观察输出的变化。
- en: When you run the code, the values quickly converge and remain stable. This is
    a tremendous improvement in our training and a cause of changing the activation
    function to `ReLU`. The reason for this is that now our perceptron weights can
    more slowly converge to a global maximum, whereas before they just wobbled around
    a local minimum by using the `step` function. There are plenty of other activation
    functions we will test through the course of this book. In the next section, we
    look at how things get much more complicated when we start to combine our perceptrons
    into multiple layers.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行代码时，数值迅速收敛并保持稳定。这是我们训练的巨大进步，也是将激活函数更改为`ReLU`的原因。原因在于，现在我们的感知器权重可以更慢地收敛到全局最大值，而之前使用`step`函数时，它们仅在局部最小值附近波动。接下来，我们将在本书的过程中测试许多其他激活函数。在下一节中，我们将讨论当我们开始将感知器组合成多个层时，情况变得更加复杂。
- en: Multilayer perceptron in TF
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow中的多层感知器
- en: 'Thus far, we have been looking at a simple example of a single perceptron and
    how to train it. This worked well for our small dataset, but as the number of
    inputs increases, the complexity of our networks increases, and this cascades
    into the math as well. The following diagram shows a multilayer perceptron, or
    what we commonly refer to as an ANN:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在看一个简单的单一感知器示例以及如何训练它。这对我们的小数据集来说很有效，但随着输入数量的增加，网络的复杂性也在增加，这也反映在数学计算上。下图展示了一个多层感知器，或者我们通常所称的ANN：
- en: '![](img/b188eaf0-7904-4c2b-9580-efb1446135d7.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b188eaf0-7904-4c2b-9580-efb1446135d7.png)'
- en: Multilayer perceptron or ANN
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器或ANN
- en: In the diagram, we see a network with one input, one hidden, and one output
    layer. The inputs are now shared across an input layer of neurons. The first layer
    of neurons processes the inputs, and outputs the results to be processed by the
    hidden layer and so on, until they finally reach the output layer.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在图示中，我们看到一个具有一个输入层、一个隐藏层和一个输出层的网络。输入现在通过一个神经元输入层共享。第一层神经元处理输入，并将结果输出到隐藏层进行处理，依此类推，直到最终到达输出层。
- en: Multilayer networks can get quite complex, and the code for these models is
    often abstracted away by high-level interfaces such as Keras, PyTorch, and so
    on. These tools work well for quickly exploring network architecture and understanding
    DL concepts. However, when it comes to performance, which is key in games, it
    really requires the models to be built in TensorFlow or an API that supports low-level
    math operations. In this book, we will swap from Keras, a higher-level SDK, to
    TensorFlow and back for the introductory DL chapters. This will allow you to see
    the differences and similarities between working with either interface.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 多层网络可能变得相当复杂，这些模型的代码通常通过高级接口如Keras、PyTorch等进行抽象化。这些工具非常适合快速探索网络架构和理解深度学习概念。然而，当涉及到性能时，尤其是在游戏中，确实需要在TensorFlow或支持低级数学运算的API中构建模型。在本书中，我们将在引导性的深度学习章节中从Keras（一个高级SDK）切换到TensorFlow，并来回切换。这将帮助你看到使用这两种接口时的差异与相似之处。
- en: Unity ML-Agents was first prototyped with Keras but has since progressed to
    TensorFlow.  Most certainly, the team at Unity, as well as others, has done this
    for reasons of performance and, to some extent, control. Working with TensorFlow
    is akin to writing your own shaders. While it is quite difficult to write shaders
    and TF code, the ability to customize your own rendering and now learning will
    make your game be unique, and it will stand out.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Unity ML-Agents 最初是使用 Keras 原型开发的，但后来已迁移到 TensorFlow。毫无疑问，Unity 团队以及其他团队这样做是出于性能和某种程度上的控制考虑。与
    TensorFlow 一起工作类似于编写你自己的着色器。虽然编写着色器和 TensorFlow 代码都非常困难，但定制你自己的渲染和现在的学习能力将使你的游戏独一无二，脱颖而出。
- en: 'There is a great TensorFlow example of a multilayer perceptron next for your
    reference, listing `Chapter_1_4.py`. In order to run this code using TensorFlow,
    follow the next steps:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来有一个非常好的 TensorFlow 多层感知机示例，供你参考，列出在 `Chapter_1_4.py` 中。为了使用 TensorFlow 运行此代码，请按照以下步骤操作：
- en: We won't cover the basics of TensorFlow until the next section. This is so you
    can see TF in action first before we bore you with the details.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节之前，我们不会涉及 TensorFlow 的基础知识。这是为了让你先看到 TensorFlow 的实际应用，再避免因细节让你感到乏味。
- en: 'First, install TensorFlow using the following command from a Python 3.5/3.6
    window on Windows or macOS. You can also use an Anaconda Prompt, with administrator
    rights:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，通过以下命令在 Windows 或 macOS 上的 Python 3.5/3.6 环境中安装 TensorFlow。你也可以使用带有管理员权限的
    Anaconda Prompt：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Make sure you install TensorFlow suited to the default Python environment. We
    will worry about creating more structured virtual environments later. If you are
    not sure what a Python virtual environment is, step away from the book and take
    a course in Python right away.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保安装适用于默认 Python 环境的 TensorFlow。我们稍后会关注创建更结构化的虚拟环境。如果你不确定什么是 Python 虚拟环境，建议先离开书本，立即参加
    Python 课程。
- en: In this exercise, we are loading the **MNIST** handwritten digits database.
    If you have read anything at all about ML and DL, you have most likely seen or
    heard about this dataset already. If you haven't, just quickly Google *MNIST* to
    get a sense of what these digits look like.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们正在加载 **MNIST** 手写数字数据库。如果你读过任何关于机器学习（ML）和深度学习（DL）的资料，你很可能已经见过或听说过这个数据集。如果没有，只需快速
    Google *MNIST*，就可以了解这些数字的样子。
- en: 'The following Python code is from the `Chapter_1_4.py` listing, with each section
    explained in the following steps:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下的 Python 代码来自 `Chapter_1_4.py` 列表，每个部分将在接下来的步骤中解释：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We start by loading the `mnist` training set. The `mnist` dataset is a collection
    of 28 x 28 pixel images showing hand-drawn representations of the digits 0-9,
    or what we will refer to as 10 classes:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载 `mnist` 训练集。`mnist` 数据集包含了 28 x 28 像素的图像，显示了手绘的 0-9 数字，或者我们称之为 10 个类别：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then we import the `tensorflow` library as `tf`. Next, we set a number of parameters
    we will use later. Note how we are defining the inputs and hidden parameters as
    well:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们导入 `tensorflow` 库并指定为 `tf`。接着，我们设置一些稍后将使用的参数。注意我们如何定义输入和隐藏层参数：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we set up a couple of TensorFlow placeholders with `tf.placeholder`,
    to hold the number of inputs and classes as type `''float''`. Then we create and
    initialize variables using `tf.Variable`, first doing the weights and then the
    biases. Inside the variable declaration, we initialize normally distributed data
    into a 2D matrix or tensor with dimensions equal to `n_input` and `n_hidden_1`
    using `tf.random_normal`, which fills a tensor with randomly distributed data:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `tf.placeholder` 设置一些 TensorFlow 占位符，用来存储输入数量和类别，数据类型为 `'float'`。然后，我们使用
    `tf.Variable` 创建并初始化变量，首先是权重，再是偏置。在变量声明中，我们使用 `tf.random_normal` 初始化正态分布的数据，将其填充到一个二维矩阵或张量中，矩阵的维度等于
    `n_input` 和 `n_hidden_1`，从而填充一个随机分布的数据张量：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Then we create the model by multiplying the weights and biases for each layer
    operation. What we are doing here is essentially converting our activation equation
    into a matrix/tensor of equations. Now instead of doing a single pass, we perform
    multiple passes in one operation using matrix/tensor multiplication. This allows
    us to run multiple training images or sets of data at a time, which is a technique
    we use to better generalize learning.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们通过对每一层操作的权重和偏置进行乘法运算来创建模型。我们在这里做的基本上是将我们的激活方程转化为一个矩阵/张量方程。现在，我们不再进行单次传递，而是通过矩阵/张量乘法在一次操作中进行多次传递。这使得我们可以一次性处理多个训练图像或数据集，这是我们用来更好地推广学习的技术。
- en: 'For each layer in our neural network, we use `tf.add` and `tf.matmul` to add
    matrix multiplication operations to what we commonly call a **TensorFlow inference
    graph**. You can see by the code we are creating that there are two hidden layers
    and one output layer for our model:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们神经网络中的每一层，我们使用`tf.add`和`tf.matmul`将矩阵乘法操作加入到我们通常所说的**TensorFlow推理图**中。从我们创建的代码中可以看到，我们的模型有两个隐藏层和一个输出层：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we define a `loss` function and optimizer. `loss_op` is used to calculate
    the total loss of the network. Then `AdamOptimizer` is what does the optimizing
    according to the `loss` or `cost` function. We will explain these terms in detail
    later, so don''t worry if things are still fuzzy:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个`loss`函数和优化器。`loss_op`用于计算网络的总损失。然后，`AdamOptimizer`根据`loss`或`cost`函数进行优化。我们稍后会详细解释这些术语，因此如果现在不太明白也不用担心：
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then we initialize a new TensorFlow session by creating a new session and running
    it. We use that epoch iterative training method again to loop over each batch
    of images. Remember, an entire batch of images goes through the network at the
    same time, not just one image. Then, we loop through each batch of images in each
    epoch and optimize (backpropagate and train) the cost, or minimize the cost if
    you will:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们通过创建一个新会话并运行它来初始化一个新的TensorFlow会话。我们再次使用周期性迭代训练方法，对每一批图像进行循环训练。记住，一整个图像批次会同时通过网络，而不仅仅是一张图像。然后，我们在每个周期中循环遍历每一批图像，并优化（反向传播和训练）成本，或者说，最小化成本：
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then we output the results of each epoch run, showing how the network is minimizing
    the error:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们输出每个周期运行的结果，展示网络如何最小化误差：
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we actually run the prediction with the preceding code and determine
    the percentage of correct values using the optimizer we selected before on the
    `logits` model:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们实际运行前面的代码进行预测，并使用我们之前选择的优化器来确定`logits`模型中正确值的百分比：
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Finally, we calculate and output the `accuracy` of our model. If you run the
    exercise, don't just go into how accurate the model is but think of ways the accuracy
    could be improved.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们计算并输出模型的`accuracy`。如果你运行这个练习，不要只是关注模型的准确度，还要思考如何提高准确度的方法。
- en: There is plenty going on in the preceding reference example, and we will break
    it down further in the next sections. Hopefully, you can see at this point how
    complex things can get. This is why for most of the fundamental chapters in this
    book, we will teach the concepts with Keras first. Keras is a powerful and simple
    framework that will help us build complex networks in no time and makes it much
    simpler for us to teach and for you to learn. We will also provide duplicate examples
    developed in TensorFlow and show some of the key differences as we progress through
    the book.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的参考示例中有很多内容，我们将在接下来的章节中进一步讲解。希望你此时已经能够看到事情变得复杂的程度。这也是为什么在本书的大部分基础章节中，我们会先通过Keras讲解概念。Keras是一个强大且简单的框架，能够帮助我们迅速构建复杂的网络，同时也使我们教学和学习变得更加简单。我们还将提供在TensorFlow中开发的重复示例，并在书中进展过程中展示一些关键的差异。
- en: In the next section, we explain the basic concepts of TensorFlow, what it is,
    and how we use it.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将解释TensorFlow的基本概念，它是什么，以及我们如何使用它。
- en: TensorFlow Basics
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow基础
- en: '**TensorFlow** (**TF**) is quickly becoming the technology that powers many
    DL applications. There are other APIs, such as Theano, but it is the one that
    has gathered the greatest interest and mostly applies to us. Overarching frameworks,
    such as Keras, offer the ability to deploy TF or Theano models, for instance.
    This is great for prototyping and building a quick proof of concept, but, as a
    game developer, you know that when it comes to games, the dominant requirements
    are always performance and control. TF provides better performance and more control
    than any higher-level framework such as Keras. In other words, to be a serious
    DL developer, you likely need and want to learn TF.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow**（**TF**）正在迅速成为许多深度学习应用的核心技术。虽然还有像Theano这样的其他API，但它是最受关注的，且对我们来说最适用。像Keras这样的高层框架提供了部署TF或Theano模型的能力。例如，这对于原型设计和快速验证概念非常有帮助，但作为游戏开发者，你知道，对于游戏而言，最重要的要求总是性能和控制。TF比任何高层框架（如Keras）提供更好的性能和更多的控制。换句话说，要成为一名认真的深度学习开发者，你很可能需要并且希望学习TF。'
- en: TF, as its name suggests, is all about tensors. A tensor is a mathematical concept
    that describes a set of data organized in *n* dimensions, where *n* could be 1,
    2 x 2, 4 x 4 x 4, and so on. A one-dimensional tensor would describe a single
    number, say ![](img/6fa7dde2-1b00-44af-8f4c-041e252a6be9.png), a 2 x 2 tensor
    would be![](img/7077b58d-f2f0-4f9e-843e-fe60c0e44cfe.png)or what you may refer
    to as a matrix. A 3 x 3 x 3 tensor would describe a cube shape. Essentially, any
    operation that you would apply on a matrix can be applied to a tensor and everything
    in TF is a tensor. It is often helpful when you first start working with tensors,
    as someone with a game development background, to think of them as a matrix or
    vector.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，TF（TensorFlow）就是围绕张量展开的。张量是一个数学概念，描述的是一个在*n*维度中组织的数据集合，其中*n*可以是1、2x2、4x4x4，等等。一个一维张量将描述一个单一的数字，例如![](img/6fa7dde2-1b00-44af-8f4c-041e252a6be9.png)，一个2x2张量将是![](img/7077b58d-f2f0-4f9e-843e-fe60c0e44cfe.png)，或者你可能称之为矩阵。一个3x3x3的张量将描述一个立方体形状。本质上，你可以将任何应用于矩阵的操作应用于张量，而且在TF中，一切都是张量。当你刚开始使用张量时，作为一个有游戏开发背景的人，将它们视为矩阵或向量是很有帮助的。
- en: 'Tensors are nothing more than multidimensional arrays, vectors, or matrices,
    and many examples are shown in the following diagram:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 张量不过是多维数组、向量或矩阵，许多示例展示在下面的图表中：
- en: '![](img/971d6c2e-1fa2-4e56-8d3b-7410e1ab3118.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/971d6c2e-1fa2-4e56-8d3b-7410e1ab3118.png)'
- en: Tensor in many forms (placeholder)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 多种形式的张量（占位符）
- en: 'Let''s go back and open up `Chapter_1_4.py` and follow the next steps in order
    to better understand how the TF example runs:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到`Chapter_1_4.py`并按照接下来的步骤操作，以更好地理解TF示例是如何运行的：
- en: 'First, examine the top section again and pay special attention to where the
    placeholder and variable is declared; this is shown again in the following snippet:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，再次检查顶部部分，特别注意占位符和变量的声明；这在下面的代码片段中再次展示：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `placeholder` is used to define the input and output tensors. `Variable`
    sets up a variable tensor that can be manipulated while the TF session or program
    executes. In the case of the example, a helper method called `random_normal` populates
    the hidden weights with a normally distributed dataset. There are other helper
    methods such as this that can be used; check the docs for more info.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`placeholder`用于定义输入和输出张量。`Variable`设置一个变量张量，可以在TF会话或程序执行时进行操作。在这个例子中，一个名为`random_normal`的辅助方法将隐藏权重填充为正态分布的数据集。还有其他类似的辅助方法可以使用；有关更多信息，请查看文档。'
- en: 'Next, we construct the `logits` model as a function called `multilayer_perceptron`, as
    follows:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们构建`logits`模型，它是一个名为`multilayer_perceptron`的函数，如下所示：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Inside the function, we see the definition of three network layers, two input
    and one output. Each layer is constructed by using the add or `+` function to
    add the results of the `matmul (x, weights['h1'])` and the `biases['b1']`. `Matmul`
    does a simple matrix multiplication of each weight times the input *x*. Think
    back to our first example perceptron; this is the same as multiplying all our
    weights by the input and then adding the bias. Note how the resultant tensors
    `(layer_1, layer_2)` are used as inputs into the following layer.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在函数内部，我们看到定义了三个网络层，其中两个是输入层，一个是输出层。每个层使用`add`或`+`函数来将`matmul(x, weights['h1'])`和`biases['b1']`的结果相加。`Matmul`做的是每个权重与输入*x*的简单矩阵乘法。回想一下我们第一个例子中的感知机；这就像将所有权重乘以输入，然后加上偏差。注意，结果张量`(layer_1,
    layer_2)`作为输入传递到下一个层。
- en: 'Skip down to around line 50 and note how we grab references to the `loss`,
    `optimizer`, and `initialization` functions:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跳到第50行左右，注意我们是如何获取`loss`、`optimizer`和`initialization`函数的引用：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: It is important to understand that we are storing references to the functions
    and not executing them just yet. The loss and optimizer functions have been covered
    in some depth already, but also pay special attention to the `global_variables_initalizer()`
    function. This function is where all the variables are initialized, and we are
    required to run this function first.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重要的是要理解，我们这里只是存储了对函数的引用，并没有立即执行它们。损失和优化器函数已经详细讨论过了，但也要特别注意`global_variables_initializer()`函数。这个函数是所有变量初始化的地方，我们必须先运行这个函数。
- en: 'Next, scroll down to the start of the session initialization and start, as
    follows:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，滚动到会话初始化和启动的开始，如下所示：
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We construct `Session` in TF as a container of execution or what is called a
    graph. This is a mathematical graph that describes nodes and connections, not
    that unlike the networks we are simulating. Everything in TF needs to happen within
    a session. Then we run the first function, `(init)`, with `run`.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在 TF 中构建 `Session`，作为执行的容器或所谓的图。这个数学图描述了节点和连接，与我们模拟的网络有相似之处。TF 中的所有操作都需要在一个
    session 内进行。然后我们运行第一个函数 `init`，通过 `run` 来执行。
- en: 'As we have already covered the training in some detail, the next element we
    will look at is the next function, `run`, executed by the following code:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们已经详细介绍了训练过程，接下来我们将查看的下一个元素是通过以下代码执行的下一个函数 `run`：
- en: '[PRE21]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: A lot is going on in the `run` function. We input as a set the training and
    loss functions `train_op` and `loss_op` using the current `feed_dict` dictionary
    as input. The resultant output value, `c`, is equal to the total cost. Note that
    the input function set is defined as `train_op` then `loss_op`. In this case,
    the order is defined as `train`/`loss`, but it could be also reversed if you choose.
    You would also need to reverse the output values as well, since the output order
    matches the input order.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`run` 函数中发生了很多事情。我们通过当前的 `feed_dict` 字典作为输入，将训练和损失函数 `train_op` 和 `loss_op`
    作为一组输入。结果输出值 `c` 等于总成本。请注意，输入函数集被定义为 `train_op` 然后是 `loss_op`。在这种情况下，顺序定义为 `train`/`loss`，但如果你选择，也可以将其反转。你还需要反转输出值，因为输出顺序与输入顺序相匹配。'
- en: The rest of the code has already been defined in some detail, but it is important
    to understand some of the key differences when building your models with TF. As
    you can see, it is relatively easy for us to now build complex neural networks
    quickly. Yet, we are still missing some critical knowledge that will be useful
    in constructing more complex networks later. What we have been missing is the
    underlying math used to train a neural network, which we will explore in the next
    section.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的其余部分已经在某种程度上定义了，但理解在使用 TF 构建模型时的一些关键区别还是很重要的。如你所见，现在我们已经能够相对快速地构建复杂的神经网络。然而，我们仍然缺少一些在构建更复杂的网络时将会非常有用的关键知识。我们缺少的正是训练神经网络时使用的基础数学，这部分内容将在下一节进行探讨。
- en: Training neural networks with backpropagation
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用反向传播训练神经网络
- en: Calculating the activation of a neuron, the forward part, or what we call **feed-forward
    propagation**, is quite straightforward to process. The complexity we encounter
    now is training the errors back through the network. When we train the network
    now, we start at the last output layer and determine the total error, just as
    we did with a single perceptron, but now we need to sum up all errors across the
    output layer. Then we need to use this value to backpropagate the error back through
    the network, updating each of the weights based on their contribution to the total
    error. Understanding the contribution of a single weight in a network with thousands
    or millions of weights could be quite complicated, except thankfully for the help
    of differentiation and the chain rule. Before we get to the complicated math,
    we first need to discuss the `Cost` function and how we calculate errors in the
    next section.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 计算神经元的激活、前向传播，或者我们称之为 **前馈传播**，是相当简单的过程。我们现在遇到的复杂性是将误差反向传播通过网络。当我们现在训练网络时，我们从最后的输出层开始，确定总误差，就像我们在单个感知机中做的那样，但现在我们需要总结所有输出层的误差。然后我们需要使用这个值将误差反向传播回网络，通过每个权重更新它们，基于它们对总误差的贡献。理解一个在成千上万甚至数百万个权重的网络中，单个权重的贡献可能会非常复杂，幸运的是，通过微分和链式法则的帮助，这一过程变得可行。在我们进入复杂的数学之前，我们需要先讨论
    `Cost` 函数以及如何计算误差，下一节会详细讲解。
- en: While the math of backpropagation is complicated and may be intimidating, at
    some point, you will want or need to understand it well. However, for the purposes
    of this book, you can omit or just revisit this section as needed. All the networks
    we develop in later chapters will automatically handle backpropagation for us.
    Of course, you can't run away from the math either; it is everywhere in deep learning.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然反向传播的数学原理很复杂，可能让人感到有些困难，但总有一天，你会希望或者需要理解它。尽管如此，对于本书的目的，你可以忽略这一部分，或者根据需要再回顾这部分内容。我们在后面的章节中开发的所有网络都会自动为我们处理反向传播。当然，你也不能完全避免数学的部分；它是深度学习中无处不在的。
- en: The Cost function
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本函数
- en: 'A `Cost` function describes the average sum of errors for a batch in our entire
    network and is often defined by this equation:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`Cost` 函数描述了我们整个网络中一个批次的误差的平均和，通常由以下方程定义：'
- en: '![](img/17490b69-202e-4ae1-bdb0-45b96d921c7f.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17490b69-202e-4ae1-bdb0-45b96d921c7f.png)'
- en: The input is defined as each weight and the output is the total average cost
    we encountered over the processed batch. Think of this cost as the average sum
    of errors. Now, our goal here is to minimize this function or the cost of errors
    to the lowest value possible. In the previous couple of examples, we have seen
    a technique called **gradient descent** being used to minimize this cost function.
    Gradient descent works by differentiating the `Cost` function and determining
    the gradient with respect to each weight. Then, for each weight, or dimension
    if you will, the algorithm alters the weight based on the calculated gradient
    that minimizes the `Cost` function.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 输入定义为每个权重，输出是我们在处理的批次中遇到的总平均成本。可以将这个成本看作是误差的平均和。现在，我们的目标是将这个函数或误差成本最小化到尽可能低的值。在前面几个示例中，我们已经看到一种名为**梯度下降**的技术被用来最小化这个成本函数。梯度下降的原理是通过对`Cost`函数进行微分，并确定相对于每个权重的梯度。然后，对于每个权重，或者说每个维度，算法会根据计算出的梯度调整权重，以最小化`Cost`函数。
- en: 'Before we get into the heavy math that explains the differentiation, let''s
    see how gradient descent works in two dimensions, with the following diagram:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入讨论解释微分的复杂数学之前，先来看一下梯度下降在二维空间中的工作原理，参考下图：
- en: '![](img/c32e3474-a1e4-4d14-9c54-0c9cb5b6d698.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c32e3474-a1e4-4d14-9c54-0c9cb5b6d698.png)'
- en: Example of gradient descent finding a global minimum
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降找到全局最小值的示例
- en: In simpler terms, all that the algorithm is doing is just trying to find the
    minimum in slow gradual steps. We use small steps in order to avoid overshooting
    the minimum, which as you have seen earlier can happen (remember the wobble).
    That is where the term **learning rate** also comes in, which determines how fast
    we want to train. The slower the training, the more confident you will be in your
    results, but usually at a cost of time. The alternative is to train quicker, using
    a higher learning rate, but, as you can see now, it may be easy to overshoot any
    global minimum.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，算法所做的就是通过缓慢渐进的步骤寻找最小值。我们使用小步伐来避免超过最小值，正如你之前看到的那样，这种情况可能会发生（记住“摆动”现象）。这时**学习率**的概念就出现了，它决定了我们训练的速度。训练越慢，你对结果的信心就越高，但通常会以时间为代价。另一种选择是通过提高学习率来加快训练速度，但正如你现在看到的，可能很容易超过任何全局最小值。
- en: Gradient descent is the simplest form we will talk about, but keep in mind that
    there are also several advanced variations of other optimization algorithms we
    will explore. In the TF example, for instance, we used `AdamOptimizer` to minimize
    the `Cost` function, but there are several other variations. For now, though,
    we will focus on how to calculate the gradient of the `Cost` function and understand
    the basics of backpropagation with gradient descent in the next section.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是我们将要讨论的最简单形式，但请记住，还有许多其他优化算法的高级变体，我们将进一步探讨。例如，在TF示例中，我们使用了`AdamOptimizer`来最小化`Cost`函数，但还有许多其他变体。不过，暂时我们将专注于如何计算`Cost`函数的梯度，并在下一节中了解梯度下降的反向传播基础。
- en: Partial differentiation and the chain rule
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏导数和链式法则
- en: 'Before we get into the details of calculating each weight, let''s review a
    little bit of calculus and differentiation. If you recall your favorite math class,
    calculus, you can determine the slope of change for any point on a function by
    differentiating. A calculus refresher is shown in the following diagram:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入计算每个权重的细节之前，让我们稍微回顾一下微积分和微分。如果你还记得你最喜欢的数学课——微积分，你可以通过微分来确定函数中任何一点的变化斜率。下面的图示是一个微积分复习：
- en: '![](img/9c72438b-cc34-4723-9e40-cee35840839a.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c72438b-cc34-4723-9e40-cee35840839a.png)'
- en: A review of basic calculus equations
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 基本微积分方程的回顾
- en: In the diagram, we have a nonlinear function, **f**, that describes the equation
    of the blue line. We can determine the slope (rate of change) on any point by
    differentiating to f' and solving. Recall that we can also determine the functions
    of local and global minimum or maximum using this new function and as shown in
    the diagram. Simple differentiation allows us to solve for one variable, but we
    need to solve for multiple weights, so we will use partial derivatives or differentiating
    with respect to one variable.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，我们有一个非线性函数**f**，它描述了蓝色线条的方程。通过对f'进行微分并求解，我们可以确定任何一点的斜率（变化率）。回想一下，我们也可以利用这个新函数来确定局部和全局的最小值或最大值，正如图中所示。简单的微分使我们能够求解一个变量，但我们需要求解多个权重，因此我们将使用偏导数或对一个变量进行微分。
- en: 'As you may recall, partial differentiation allows us to derive for a single
    variable with respect to the other variables, which we then treat as constants.
    Let''s go back to our `Cost` function and see how to differentiate that with respect
    to a single weight:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所记得的，偏微分使我们能够对一个变量进行求导，同时将其他变量视为常数。让我们回到我们的 `Cost` 函数，看看如何对一个单一的权重进行求导：
- en: '*![](img/540eed90-acc5-4e90-9e5f-5a4a2da7a90f.png)* is our cost function described
    by the following:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*![](img/540eed90-acc5-4e90-9e5f-5a4a2da7a90f.png)* 是我们描述的成本函数，表达如下：'
- en: '![](img/de02ffdb-0412-45e9-bdc9-d1b72f38f618.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de02ffdb-0412-45e9-bdc9-d1b72f38f618.png)'
- en: 'We can differentiate this function with respect to a single variable weight
    as follows:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过如下方式，对这个函数进行关于单一变量权重的求导：
- en: '![](img/80566a95-aafa-43b8-ba8a-9bc0d258169e.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/80566a95-aafa-43b8-ba8a-9bc0d258169e.png)'
- en: '![](img/cc8b62c3-a3dd-4c94-9ff8-fc11d9a2a622.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc8b62c3-a3dd-4c94-9ff8-fc11d9a2a622.png)'
- en: 'If we collect all of these partial derivatives together, we get the vector
    gradient for our `Cost` function, ![](img/0fb9abc8-223f-4fa3-96a3-b025e13774ac.png), denoted
    by the following:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们将所有这些偏导数合并在一起，我们就得到了我们的 `Cost` 函数的向量梯度， ![](img/0fb9abc8-223f-4fa3-96a3-b025e13774ac.png)，表示为如下：
- en: '![](img/50074553-fd8d-432d-9d74-20562d73b5d4.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50074553-fd8d-432d-9d74-20562d73b5d4.png)'
- en: 'This gradient defines a vector direction that we want to negate and use to
    minimize the `Cost` function. In the case of our previous example, there are over
    13,000 components to this vector. These correspond to over 13,000 weights in the
    network that we need to optimize. That is a lot of partial derivatives we need
    to combine in order to calculate the gradient. Fortunately, the chain rule in
    calculus can come to our rescue and greatly simplify the math. Recall that the
    chain rule is defined by the following:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个梯度定义了我们希望取反并用来最小化 `Cost` 函数的向量方向。在我们之前的例子中，这个向量包含了超过 13,000 个分量。这些分量对应着网络中需要优化的超过
    13,000 个权重。为了计算梯度，我们需要结合很多偏导数。幸运的是，微积分中的链式法则能够帮助我们简化这个数学过程。回想一下，链式法则的定义如下：
- en: '![](img/405bae41-bd8e-43fa-b46c-1be1b4d0657d.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/405bae41-bd8e-43fa-b46c-1be1b4d0657d.png)'
- en: 'This now allows us to define the gradient for a single weight using the chain
    rule as such:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这使得我们能够使用链式法则为单一权重定义梯度，如下所示：
- en: '![](img/95d4ef61-83e7-4680-b338-2637ad32a9df.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95d4ef61-83e7-4680-b338-2637ad32a9df.png)'
- en: '![](img/237b0114-cc6f-4cb2-b9ca-f527d882243a.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/237b0114-cc6f-4cb2-b9ca-f527d882243a.png)'
- en: 'Here, ![](img/53d1990c-21bb-4d13-97a4-d69c40d87558.png) represents the input
    number and ![](img/d7bf0016-5adf-4f73-88c6-a93ebacf1caa.png) the neuron position.
    Note how we now need to take the partial derivative of the activation function, *a*, for
    the given neuron, and that is again summarized by the following:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里，![](img/53d1990c-21bb-4d13-97a4-d69c40d87558.png) 表示输入值，而 ![](img/d7bf0016-5adf-4f73-88c6-a93ebacf1caa.png)
    表示神经元位置。注意，我们现在需要对给定神经元的激活函数 *a* 进行偏导数，这一过程可以通过以下方式总结：
- en: '![](img/4ed6a86d-bbe9-435e-8490-8e05e0d56102.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ed6a86d-bbe9-435e-8490-8e05e0d56102.png)'
- en: The superscript notation ![](img/674a1fc1-17c6-4627-a679-e4a020bc511c.png) denotes
    the current layer and ![](img/7768eb39-9ec9-40af-b61a-bd226e3544ca.png) denotes
    the previous layer. ![](img/b73cb21f-571d-4923-a76b-f6227090187d.png) denotes
    either the input or the output from the previous layer. ![](img/5b84bf4a-5db8-43b0-bcb0-d8a9531484ee.png) denotes
    the activation function, recall that we previously used the `Step` and `ReLU`
    functions for this role.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 上标符号 ![](img/674a1fc1-17c6-4627-a679-e4a020bc511c.png) 表示当前层，而 ![](img/7768eb39-9ec9-40af-b61a-bd226e3544ca.png)
    表示上一层。![](img/b73cb21f-571d-4923-a76b-f6227090187d.png) 表示来自上一层的输入或输出。![](img/5b84bf4a-5db8-43b0-bcb0-d8a9531484ee.png)
    表示激活函数，回想一下我们之前使用过 `Step` 和 `ReLU` 函数来扮演这个角色。
- en: 'Then, we take the partial derivative of this function, like so:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们对这个函数进行偏导数，如下所示：
- en: '![](img/2b7fef57-c426-4338-b802-fe934fdf278a.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b7fef57-c426-4338-b802-fe934fdf278a.png)'
- en: 'For convenience, we define the following:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们定义如下：
- en: '![](img/e26b112d-5a95-468a-9e28-291bcd88df10.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e26b112d-5a95-468a-9e28-291bcd88df10.png)'
- en: 'At this point, things may look a lot more complicated than they are. Try to
    understand all the subtleties of the notation and remember all we are looking
    at is essentially the partial derivative of the activation with respect to the
    `Cost` function. All that the extra notation does is allow us to index the individual
    weight, neuron, and layer. We can then express this as follows:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一点上，事情可能看起来比实际情况复杂得多。试着理解符号的所有细微之处，记住我们现在所看的本质上是激活函数对 `Cost` 函数的偏导数。额外的符号仅仅是帮助我们索引各个权重、神经元和层。然后我们可以将其表达为如下：
- en: '![](img/700a7c49-24b2-4c81-81ac-1660a520c017.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/700a7c49-24b2-4c81-81ac-1660a520c017.png)'
- en: 'Again, all we are doing is defining the gradient (![](img/c7be7304-2015-4fb0-bce6-d80275794496.png))
    for the weight at the ![](img/625ab8a1-2e70-4937-95b8-1ac620c209c5.png)^(th) input,
    ![](img/aa2eb4de-35af-441a-b21a-eedb4d796372.png)^(th) neuron, and layer ![](img/3fb99529-b043-4057-8400-e4d5924454df.png).
    Along with gradient descent, we need to backpropagate the adjustment to the weights
    using the preceding base formula. For the output layer (last layer), this now
    can be summarized as follows:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次强调，我们所做的只是为 ![](img/625ab8a1-2e70-4937-95b8-1ac620c209c5.png)^(th) 输入、![](img/aa2eb4de-35af-441a-b21a-eedb4d796372.png)^(th)
    神经元和层 ![](img/3fb99529-b043-4057-8400-e4d5924454df.png)的权重定义梯度 (![](img/c7be7304-2015-4fb0-bce6-d80275794496.png))。结合梯度下降，我们需要使用前面的基础公式将调整反向传播到权重中。对于输出层（最后一层），这现在可以总结如下：
- en: '![](img/a7244943-32ae-4e8c-8a70-5bdef000f42c.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7244943-32ae-4e8c-8a70-5bdef000f42c.png)'
- en: 'For an internal or a hidden layer, the equation comes out to this:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于内部或隐藏层，方程式的结果如下：
- en: '![](img/018674a5-8d61-42fd-a036-427bf65bab66.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/018674a5-8d61-42fd-a036-427bf65bab66.png)'
- en: 'And with a few more substitutions and manipulations of the general equation,
    we end up with this:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经过一些替换和对一般方程的操作，我们最终得到了这个结果：
- en: '![](img/6318bfc0-51f2-486e-ab6e-c87bc93d5e67.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6318bfc0-51f2-486e-ab6e-c87bc93d5e67.png)'
- en: Here, *f'* denotes the derivative of the activation function.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里， *f'* 表示激活函数的导数。
- en: 'The preceding equation allows us to run the network and backpropagate the errors
    back through, using the following procedure:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的方程式使我们能够运行网络并通过以下过程进行误差反向传播：
- en: You first calculate the activations ![](img/12a18170-2428-4904-bbc5-71fcfc00cffd.png) and ![](img/c475dd78-297d-41ea-99f8-582ae07a5009.png) for
    each layer starting with the input layer and propagate forward.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你需要计算每一层的激活值 ![](img/12a18170-2428-4904-bbc5-71fcfc00cffd.png) 和 ![](img/c475dd78-297d-41ea-99f8-582ae07a5009.png)，从输入层开始并向前传播。
- en: We then evaluate the term ![](img/4cad7f38-ff20-4529-a65a-046d1ca4dd6d.png)at
    the output layer using ![](img/abb13af1-a57e-435b-95b2-e8bd980a33ab.png).
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们使用![](img/abb13af1-a57e-435b-95b2-e8bd980a33ab.png)在输出层评估该项 ![](img/4cad7f38-ff20-4529-a65a-046d1ca4dd6d.png)。
- en: We do this by using the remainder to evaluate each layer using ![](img/3a68e2e9-7e67-4bdc-9919-75611c86fe29.png), starting
    with the output layer and propagating backward.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过使用余项来评估每一层，使用 ![](img/3a68e2e9-7e67-4bdc-9919-75611c86fe29.png)，从输出层开始并向后传播。
- en: Again, we are using the partial derivative ![](img/3fcd1213-41e8-4d5d-b7b9-2a737e0b210c.png) to
    obtain the required derivatives in each layer.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次强调，我们使用偏导数 ![](img/3fcd1213-41e8-4d5d-b7b9-2a737e0b210c.png) 来获得每一层所需的导数。
- en: It may take you a few reads through this section in order to grasp all the concepts.
    What can also be useful is to run the previous examples and watch the training,
    trying to imagine how each of the weights is getting updated. We are by no means
    completely done here, and there are a couple more steps—using automatic differentiation
    being one of them. Unless you are developing your own low-level networks, just
    having a basic understanding of that math should give you a better understanding
    of the needs in training a neural network. In the next section, we get back to
    some more hands-on basics and put our new knowledge to use by building a neural
    network agent.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要多读几遍这一部分才能理解所有概念。另一个有用的做法是运行前面的示例并观察训练过程，尝试想象每个权重是如何被更新的。我们还没有完全完成这里的内容，接下来还有几个步骤——使用自动微分就是其中之一。除非你在开发自己的低级网络，否则仅仅对这些数学知识有基本的理解就能帮助你更好地理解训练神经网络的需求。在下一部分，我们将回到一些更基本的操作，并通过构建神经网络代理将我们的新知识付诸实践。
- en: Learning does not and likely should not all come from the same source. Be sure
    to diversify your learning to other books, videos, and courses. You will not only
    be more successful in learning but likely also understand more in the process.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 学习不应也很可能不应完全依赖于同一个来源。务必将学习扩展到其他书籍、视频和课程。这样不仅能让你在学习上更为成功，还能在过程中获得更多理解。
- en: Building an autoencoder with Keras
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 构建自动编码器
- en: 'While we have covered a lot of important ground we will need for understanding
    DL, what we haven''t done yet is build something that can really do anything.
    One of the first problems we tackle when starting with DL is to build autoencoders
    to encode and reform data. Working through this exercise allows us to confirm
    that what goes into a network can also come back out of a network and essentially
    reassures us that an ANN is not a complete black box. Building and working with
    autoencoders also allows us to tweak and test various parameters in order to understand
    their function. Let''s get started by opening up the `Chapter_1_5.py` listing
    and following these steps:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经涵盖了理解深度学习所需的许多重要内容，但我们还没有构建出能够真正执行任何操作的东西。当我们开始学习深度学习时，第一个要解决的问题之一就是构建自编码器来编码和重构数据。通过完成这个练习，我们可以确认网络中的输入也能从网络中输出，基本上能让我们确信人工神经网络并不是一个完全的黑箱。构建和使用自编码器还允许我们调整和测试各种参数，以便理解它们的作用。让我们开始吧，首先打开
    `Chapter_1_5.py` 文件，并按照以下步骤操作：
- en: 'We will go through the listing section by section. First, we input the base
    layers `Input` and `Dense`, then `Model`, all from the `tensorflow.keras` module,
    with the following imports:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将逐节讲解这个代码。首先，我们输入基础层 `Input` 和 `Dense`，然后是 `Model`，这些都来自 `tensorflow.keras`
    模块，具体的导入如下：
- en: '[PRE22]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Instead of single neurons, we define our DL model in Keras using layers or neurons.
    The `Input` and `Dense` layers are the most common ones we use, but we will see
    others as well. As their name suggests, `Input` layers deal with input, while
    `Dense` layers are more or less your typical fully connected neuron layer, which
    we have already looked at.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在 Keras 中定义深度学习模型时，使用的是层或神经元，而非单独的神经元。`Input` 和 `Dense` 层是我们最常用的层，但我们也会看到其他层。如其名称所示，`Input`
    层处理输入，而 `Dense` 层则是典型的完全连接神经元层，我们之前已经看过这一点。
- en: We are using the embedded version of Keras here. The original sample was taken
    from the Keras blog and converted to TensorFlow.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用的是嵌入式版本的 Keras。原始示例来自 Keras 博客，并已转换为 TensorFlow。
- en: 'Next, we set the number of `encoding` dimensions with the following line:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通过以下代码设置 `encoding` 维度的数量：
- en: '[PRE23]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This is the number of dimensions we want to reduce our sample down to. In this
    case, it is just 32, which is just around 24 times the compression for an image
    with 784 input dimensions. Remember, we get `784` input dimensions because our
    input images are 28 x 28, and we flatten them to a vector of length 784, with
    each pixel representing a single value or dimension. Next, we set up the `Input` layer
    with the 784 input dimensions with the following:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是我们希望将样本降维到的维度数量。在这种情况下，它只是 32，即对一个有 784 输入维度的图像进行大约 24 倍的压缩。记住，我们得到 `784`
    输入维度是因为我们的输入图像是 28 x 28，并且我们将其展平为一个长度为 784 的向量，每个像素代表一个单独的值或维度。接下来，我们使用以下代码设置具有
    784 输入维度的 `Input` 层：
- en: '[PRE24]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'That line creates an `Input` layer with a shape of 784 inputs. Then we are
    going to encode those 784 dimensions into our next `Dense` layer using the following
    line:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 那一行代码创建了一个形状为 784 输入的 `Input` 层。然后，我们将使用以下代码将这 784 个维度编码到我们的下一个 `Dense` 层：
- en: '[PRE25]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The preceding code simply creates our fully connected hidden (`Dense`) layer
    of 32 (`encoding_dim`) neurons and builds the encoder. You can see that the `input_img`,
    the `Input` layer, is used as input and our activation function is `ReLU`. The
    next line constructs a `Model` using the `Input` layer (`input_img`) and the `Dense`
    (`encoded`) layer. With two layers, we encode the image from 784 dimensions to
    32.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述代码简单地创建了我们完全连接的隐藏层（`Dense`），包含 32 个（`encoding_dim`）神经元，并构建了编码器。可以看到，`input_img`，即
    `Input` 层，被用作输入，我们的激活函数是 `ReLU`。下一行使用 `Input` 层（`input_img`）和 `Dense`（`encoded`）层构建了一个
    `Model`。通过这两层，我们将图像从 784 维编码到 32 维。
- en: 'Next, we need to decode the image using more layers with the following code:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要使用更多的层来解码图像，代码如下：
- en: '[PRE26]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The next set of layers and model we build will be used to decode the images
    back to 784 dimensions. The last line of code at the bottom is where we compile
    the `autoencoder` model with an `adadelta` optimizer call, using a `loss` function
    of `binary_crossentropy`. We will spend more time on the types of loss and optimization
    parameters later, but for now just note that when we compile a model, we are in
    essence just setting it up to do backpropagation and use an optimization algorithm.
    Remember, all of this is automatically done for us, and we don't have to deal
    with any of that nasty math.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一组层和模型将用于将图像解码回 784 维度。底部的最后一行代码是我们使用 `adadelta` 优化器调用来编译 `autoencoder` 模型，并使用
    `binary_crossentropy` 作为 `loss` 函数。我们稍后会更多地讨论损失函数和优化参数的类型，但现在只需要注意，当我们编译一个模型时，实际上就是在为反向传播和优化算法设置模型。记住，所有这些操作都是自动完成的，我们无需处理这些复杂的数学问题。
- en: That sets up the main parts of our models, the encoder, decoder, and full autoencoder
    model, which we further compiled for later training. In the next section, we deal
    with training the model and making predictions.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这设置了我们模型的主要部分，包括编码器、解码器和完整的自动编码器模型，我们随后会编译这些模型以便进行训练。在接下来的部分，我们将处理模型的训练和预测。
- en: Training the model
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Next, we need to train our model with a sample set of data. We will again be
    using the MNIST set of handwritten digits; this is easy, free, and convenient.
    Get back into the code listing and continue the exercise as follows:'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们需要用一组数据样本来训练我们的模型。我们将再次使用 MNIST 手写数字数据集；这个数据集既简单、免费又方便。回到代码列表，按照以下步骤继续练习：
- en: 'Pick up where we left off and locate the following section of code:'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们上次停下的地方继续，找到以下代码段：
- en: '[PRE27]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We start by importing the `mnist` library and `numpy` then loads the data into
    `x_train` and `x_test` sets of data. As a general rule in data science and machine
    learning, you typically want a training set for learning and then an evaluation
    set for testing. These datasets are often generated by randomly splitting the
    data into `80` percent for training and `20` percent for testing.
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入 `mnist` 库和 `numpy`，然后将数据加载到 `x_train` 和 `x_test` 数据集。作为数据科学和机器学习中的一般规则，通常我们会使用一个训练集来进行学习，然后使用评估集来进行测试。这些数据集通常通过随机拆分数据来生成，通常是将
    `80%` 用于训练，`20%` 用于测试。
- en: 'Then we further define our training and testing inputs with the following code:'
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们使用以下代码进一步定义我们的训练和测试输入：
- en: '[PRE28]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The first two lines are normalizing our input gray scale pixel color values
    and a number from `0` to `255`, by dividing by `255`. This gives us a number from
    `0` to `1`. We generally want to try to normalize our inputs. Next, we reshape
    the training and testing sets into an input `Tensor`.
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前两行代码将我们的输入灰度像素颜色值从 `0` 到 `255` 进行归一化，方法是除以 `255`。这会将值转换为从 `0` 到 `1` 的范围。通常，我们希望尝试对输入数据进行归一化处理。接下来，我们将训练和测试集重塑为输入的
    `Tensor`。
- en: 'With the models all built and compiled, it is time to start training. The next
    few lines are where the network will learn how to encode and decode the images:'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型都已经构建并编译好，现在是时候开始训练了。接下来的几行代码将是网络学习如何编码和解码图像的部分：
- en: '[PRE29]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You can see in our code that we are setting up to fit the data using `x_train`
    as input and output. We are using `50` `epochs` with a `batch size` of `256` images.
    Feel free to play with these parameters on your own later to see what effect they
    have on training. After that, the `encoder` and then the `decoder` models are
    used to predict test images.
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以看到我们的代码中，我们正在设置使用 `x_train` 作为输入和输出来拟合数据。我们使用 `50` 个 `epochs` 和 `256` 张图像的
    `batch size`。稍后可以自行调整这些参数，看看它们对训练的影响。之后，`encoder` 和 `decoder` 模型被用来预测测试图像。
- en: That completes the model and training setup we need for this model, or models
    if you will. Remember, we are taking a 28 x 28 image, decompressing it to essentially
    32 numbers, and then rebuilding the image using a neural network. With our model
    complete and trained this time, we want to review the output and we will do that
    in the next section.
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这完成了我们为这个模型（或者如果你愿意的话，多个模型）所需的模型和训练设置。记住，我们正在将一个 28 x 28 的图像解压缩为本质上的 32 个数字，然后使用神经网络重建图像。现在我们的模型已经完成并训练好了，接下来我们将回顾输出结果，下一节我们将进行这个操作。
- en: Examining the output
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查输出
- en: 'Our final step this time around will be to see what is actually happening with
    the images. We will finish this exercise by outputting a small sample of images
    in order to get our success rate. Follow along in the next exercise in order to
    finish the code and run the autoencoder:'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们这次的最后一步是查看图像实际发生了什么。我们将通过输出一小部分图像来获得我们的成功率，完成这个练习。继续下一个练习，完成代码并运行自编码器：
- en: 'Continuing from the last exercise, locate the following last section of code:'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续上一个练习，找到以下代码的最后一部分：
- en: '[PRE30]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In this section of code, we are just outputting the input and resultant auto-encoded
    images after all the training is done. This section of code starts with importing
    `mathplotlib` for plotting, and then we loop through a number of images to display
    the results. The rest of the code just outputs the images.
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这段代码中，我们只是输出所有训练完成后输入和结果自编码图像。代码从导入 `mathplotlib` 开始，用于绘图，然后我们循环遍历多张图像来显示结果。其余的代码只是输出图像。
- en: Run the Python code as you normally would, and this time expect the training
    to take several minutes. After everything is done, you should see an image similar
    to the following:![](img/57c7b3de-945e-42fb-af0a-cf5258f7f7f5.png)
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像往常一样运行 Python 代码，这次预计训练会花费几分钟。完成后，你应该会看到一张类似于下面的图片：![](img/57c7b3de-945e-42fb-af0a-cf5258f7f7f5.png)
- en: Example of raw input images compared to encoded and decoded output images
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 原始输入图像与编码和解码后的输出图像的示例
- en: That completes our look into building a simple Keras model that can encode and
    then decode images. This allows us to see how each small piece of a multilayer
    neural network is written in Keras functions. In the final section, we invite
    you, the reader, to undertake some additional exercises for further learning.
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这就是我们构建一个简单的 Keras 模型的过程，该模型可以对图像进行编码然后解码。通过这个过程，我们可以看到多层神经网络的每个小部分是如何用 Keras
    函数编写的。在最后一节中，我们邀请你，读者，进行一些额外的练习以便进一步学习。
- en: Exercises
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Use these additional exercises to assist in your learning and test your knowledge
    further.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用这些额外的练习来帮助你学习并进一步测试你的知识。
- en: 'Answer the following questions:'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回答以下问题：
- en: Name three different activation functions. Remember, Google is your friend.
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列举三种不同的激活函数。记住，Google 是你的好朋友。
- en: What is the purpose of a bias?
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 偏置的作用是什么？
- en: What would you expect to happen if you reduced the number of epochs in one of
    the chapter samples? Did you try it?
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你减少了某个章节示例中的 epoch 数量，你预期会发生什么？你试过了吗？
- en: What is the purpose of backpropagation?
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播的作用是什么？
- en: Explain the purpose of the Cost function.
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释代价函数的作用。
- en: What happens when you increase or decrease the number of encoding dimensions
    in the Keras autoencoder example?
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Keras 自编码器示例中，增加或减少编码维度的数量会发生什么情况？
- en: What is the name of the layer type that we feed input into?
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们输入的层类型叫什么名字？
- en: What happens when you increase or decrease the batch size?
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加或减少批量大小会发生什么？
- en: 'What is the shape of the input `Tensor` for the Keras example? Hint: we already
    have a print statement displaying this.'
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Keras 示例中输入的 `Tensor` 形状是什么？提示：我们已经有一个打印语句显示了这一点。
- en: In the last exercise, how many MNIST samples do we train and test with?
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上一个练习中，我们使用了多少个 MNIST 样本进行训练和测试？
- en: As we progress in the book, the additional exercises will certainly become more
    difficult. For now, though, take some time to answer the questions and test your
    knowledge.
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随着我们在书中的进展，额外的练习肯定会变得更加困难。不过现在，花些时间回答这些问题并测试你的知识。
- en: Summary
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored the foundations of DL from the basics of the simple
    single perceptron to more complex multilayer perceptron models. We started with
    the past, present, and future of DL and, from there, we built a basic reference
    implementation of a single perceptron so that we could understand the raw simplicity
    of DL. Then we built on our knowledge by adding more perceptrons into a multiple
    layer implementation using TF. Using TF allowed us to see how a raw internal model
    is represented and trained with a much more complex dataset, MNIST. Then we took
    a long journey through the math, and although a lot of the complex math was abstracted
    away from us with Keras, we took an in-depth look at how gradient descent and
    backpropagation work. Finally, we finished off the chapter with another reference
    implementation from Keras that featured an autoencoder. Auto encoding allows us
    to train a network with multiple purposes and extends our understanding of how
    network architecture doesn't have to be linear.
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了深度学习的基础，从简单的单层感知器到更复杂的多层感知器模型。我们从深度学习的过去、现在和未来开始，从那里构建了一个基本的单层感知器参考实现，以帮助我们理解深度学习的基本简单性。接着，我们通过将多个感知器添加到一个多层实现中，进一步扩展了我们的知识，使用了TensorFlow（TF）。使用TF使我们能够看到如何表示和训练一个原始内部模型，并用一个更复杂的数据集MNIST进行训练。然后，我们经历了数学的长篇旅程，尽管很多复杂的数学被Keras抽象化了，我们还是深入探讨了梯度下降和反向传播的工作原理。最后，我们通过Keras的另一个参考实现结束了这一章，其中包括了一个自动编码器。自动编码让我们能够训练一个具有多重用途的网络，并扩展了我们对网络架构不必线性的理解。
- en: For the next chapter, we will build on our current level of knowledge and discover
    **convolutional** and **recurrent** neural networks. These extensions provide
    additional capabilities to the base form of a neural network and have played a
    significant part in our most recent DL advances.
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在下一章中，我们将在现有知识的基础上，探索**卷积神经网络**和**循环神经网络**。这些扩展为神经网络的基础形式提供了额外的功能，并在我们最近的深度学习进展中发挥了重要作用。
- en: For the next chapter, we will begin our journey into building components for
    games when we look at another element considered foundational to DL—the GAN. GANs
    are like a Swiss Army knife in DL and, as we will see in the next chapter, they
    offer us plenty of uses.
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始构建游戏组件的旅程，探索被认为是深度学习（DL）基础的另一个元素——生成对抗网络（GAN）。GAN在深度学习中就像是一把瑞士军刀，正如我们将在下一章中看到的那样，它为我们提供了很多用途。
