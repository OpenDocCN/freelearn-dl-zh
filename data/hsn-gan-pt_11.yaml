- en: Image Generation from Description Text
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从描述文本生成图像
- en: In the previous chapters, we have been mainly dealing with image synthesis and
    image-to-image translation tasks. Now, it's time for us to move from the CV field
    to the NLP field and discover the potential of GANs in other applications. Perhaps
    you have seen some CNN models being used for image/video captioning. Wouldn't
    it be great if we could reverse this process and generate images from description
    text?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们主要处理了图像合成和图像到图像的翻译任务。现在，是时候让我们从计算机视觉（CV）领域转向自然语言处理（NLP）领域，探索生成对抗网络（GANs）在其他应用中的潜力了。或许你已经见过一些卷积神经网络（CNN）模型被用于图像/视频描述生成。如果我们能反向操作，根据描述文本生成图像，那岂不是太棒了？
- en: In this chapter, you will learn about the basics of word embeddings and how
    are they used in the NLP field. You will also learn how to design a text-to-image
    GAN model so that you can generate images based on one sentence of description
    text. Finally, you will understand how to stack two or more Conditional GAN models
    to perform text-to-image synthesis with much higher resolution with StackGAN and
    StackGAN++.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解词嵌入的基本知识及其在NLP领域中的应用。你还将学习如何设计一个文本到图像的GAN模型，从而根据一句描述文本生成图像。最后，你将了解如何堆叠两个或更多条件GAN模型，通过StackGAN和StackGAN++进行更高分辨率的文本到图像合成。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Text-to-image synthesis with GANs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GAN进行文本到图像的合成
- en: Generating photo-realistic images with StackGAN++
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用StackGAN++生成逼真的照片级图像
- en: Text-to-image synthesis with GANs
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GAN进行文本到图像的合成
- en: From [Chapter 4](3894df8d-1a40-418e-ac36-d9357abdfd6a.xhtml), *Building Your
    First GAN with PyTorch*, to [Chapter 8](f05fbf9f-30b6-41d4-b706-5f3ef0d6fff7.xhtml),
    *Training Your GANs to Break Different Models*, we have learned almost every basic
    application of GANs in computer vision, especially when it comes to image synthesis.
    You're probably wondering how GANs are used in other fields, such as text or audio
    generation. In this chapter, we will gradually move from CV to NLP by combining
    the two fields together and try to generate realistic images from description
    text. This process is called **text-to-image ****synthesis** (or text-to-image translation).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从[第4章](3894df8d-1a40-418e-ac36-d9357abdfd6a.xhtml)，*使用PyTorch构建你的第一个GAN*，到[第8章](f05fbf9f-30b6-41d4-b706-5f3ef0d6fff7.xhtml)，*训练GAN突破不同模型*，我们已经学习了几乎所有GAN在计算机视觉中的基本应用，特别是在图像合成方面。你可能会想，GAN在其他领域（如文本或音频生成）中是如何使用的。在本章中，我们将逐步从CV领域转向NLP领域，通过结合这两个领域，尝试从描述文本中生成逼真的图像。这个过程叫做**文本到图像合成**（或文本到图像翻译）。
- en: We know that almost every GAN model generates synthesized data by establishing
    a definite mapping from a certain form of input data to the output data. Therefore,
    in order to generate an image from a corresponding description sentence, we need
    to understand how to represent sentences with vectors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，几乎每个GAN模型都是通过在某种输入数据形式与输出数据之间建立明确的映射来生成合成数据。因此，为了根据对应的描述句子生成图像，我们需要了解如何用向量表示句子。
- en: Quick introduction to word embedding
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入简介
- en: It's rather easy to define an approach for transforming the words in a sentence
    into vectors. We can simply assign different values to all the possible words
    (for example, let 001 represent *I*, 002 represent *eat*, and 003 represent *apple*)
    so that the sentence can be uniquely represented by a vector (for example, *I
    eat apple* would become [001, 002, 003]). This is basically how words are represented
    in computers. However, languages are much more complicated and flexible than cold
    digits. Without knowing the meaning of words (for example, a noun or a verb, positive
    or negative), it is nearly impossible to establish the relationship between the
    words and understand the meaning of the sentence. Furthermore, it is very hard
    to find a synonym of a word based on hardcoded values since the distance between
    the values does not represent the similarity between the corresponding words.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一种方法将句子中的单词转换为向量其实是相当简单的。我们可以简单地为所有可能的单词分配不同的值（例如，让001表示*I*，002表示*eat*，003表示*apple*），这样就可以通过向量唯一表示句子（例如，*I
    eat apple*将变为[001, 002, 003]）。这基本上就是计算机中单词的表示方式。然而，语言比冷冰冰的数字要复杂和灵活得多。如果不了解单词的含义（例如，名词或动词，积极或消极），几乎不可能建立单词之间的关系并理解句子的含义。此外，由于硬编码的值之间的距离无法表示相应单词之间的相似性，因此很难基于硬编码的值找到一个单词的同义词。
- en: Methods that have been designed to map words, phrases, or sentences to vectors
    are called **word embeddings**. One of the most successful word embedding techniques
    is called **word2vec**. If you want to learn more about word2vec, feel free to
    check out the paper *word2vec Parameter Learning Explained*, [https://arxiv.org/pdf/1411.2738.pdf](https://arxiv.org/pdf/1411.2738.pdf),
    by Xin Rong.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 用于将单词、短语或句子映射到向量的方法被称为**词嵌入**。其中最成功的词嵌入技术之一叫做**word2vec**。如果你想了解更多关于word2vec的信息，可以随时查阅Xin
    Rong的论文《*word2vec 参数学习解释*》，[https://arxiv.org/pdf/1411.2738.pdf](https://arxiv.org/pdf/1411.2738.pdf)。
- en: The term **embedding** means projecting data to a different space so that it's
    easier to analyze. You may have seen this term being used in some old papers or
    articles about CNNs, where the output vector of a learned fully connected layer
    is used to visualize whether the models are trained properly.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**嵌入**这个术语意味着将数据映射到一个不同的空间，以便更容易分析。你可能在一些关于卷积神经网络（CNN）的旧论文或文章中见过这个术语，在这些论文中，经过训练的全连接层的输出向量被用来可视化模型是否已正确训练。'
- en: 'Word embeddings are mostly used to solve two types of problems in NLP:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入主要用于解决NLP中的两类问题：
- en: '**CBOW** (**Continuous Bag-of-Word**) models, which are used to predict a single
    word based on several other words in the context'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CBOW**（**连续词袋模型**），用于根据上下文中的若干个词预测单一目标词'
- en: Skip-Gram models, which are the opposite of CBOWs and are used to predict the
    context words based on the target word
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skip-Gram模型，和CBOW模型相反，用于根据目标词预测上下文词
- en: 'The following diagram provides us with an overview of the CBOW and Skip-Gram
    models:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 下图为我们提供了CBOW和Skip-Gram模型的概述：
- en: '![](img/9b9d6576-7fd3-4acb-b186-a7b7d208c6cb.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b9d6576-7fd3-4acb-b186-a7b7d208c6cb.png)'
- en: Two types of word embeddings. Image retrieved from Xin Rong, 2014
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 两种类型的词嵌入。图像来源：Xin Rong，2014
- en: Another common term in NLP is **language modeling**. Compared to word embeddings,
    language models predict the possibilities of sentences or, more specifically, the
    possibilities of words appearing at the next position in a sentence. Since language
    modeling takes the order of words into consideration, many language models are
    built upon word embeddings to get good results.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: NLP中的另一个常见术语是**语言建模**。与词嵌入相比，语言模型预测的是句子的可能性，或者更具体地说，预测下一个词在句子中出现的可能性。由于语言建模考虑了单词的顺序，许多语言模型是基于词嵌入构建的，从而获得更好的结果。
- en: Simply put, a learned word embedding is a vector that represents a sentence
    that is easier for machine learning algorithms to analyze and understand the meaning
    of the original sentence. Check out the official tutorial about word embedding
    to learn how to implement CBOW and Skip-Gram models in PyTorch: [https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，学习到的词嵌入是一个向量，它表示一个句子，使得机器学习算法更容易分析和理解原始句子的含义。请查看关于词嵌入的官方教程，了解如何在PyTorch中实现CBOW和Skip-Gram模型：[https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py)。
- en: Translating text to image with zero-shot transfer learning
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用零-shot迁移学习将文本转换为图像
- en: In [Chapter 8](f05fbf9f-30b6-41d4-b706-5f3ef0d6fff7.xhtml), *Training Your GANs
    to Break Different Models*, we learned about the basic steps we need to take in
    order to perform transfer learning in image classification tasks. Under more realistic
    circumstances, it becomes harder to transfer this learned knowledge to another
    domain because there can be many new forms of data that the pretrained model hasn't
    met before, especially when we try to generate images based on description text
    (or in a reverse process where we generate description text from given images).
    For example, if the model is only trained on white cats, it won't know what to
    do when we ask it to generate images of black cats. This is where zero-shot transfer
    learning comes into play.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](f05fbf9f-30b6-41d4-b706-5f3ef0d6fff7.xhtml)，*训练你的GAN以打破不同的模型*中，我们了解了在图像分类任务中进行迁移学习所需的基本步骤。在更为现实的情况下，将这种学习到的知识迁移到另一个领域变得更加困难，因为可能会有许多新的数据形式是预训练模型之前没有见过的，尤其是当我们尝试根据描述文本生成图像（或者反过来，从给定图像生成描述文本）时。例如，如果模型仅在白猫的图像上进行训练，当我们要求它生成黑猫的图像时，它将无法知道该怎么做。这就是零-shot迁移学习发挥作用的地方。
- en: Zero-shot learning
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零-shot学习
- en: '**Zero-shot learning** refers to a machine learning process where we need to
    predict new samples with labels that haven''t been seen before. It is often done
    by providing additional information to the pretraining process. For example, we
    can tell the model that the objects known as **white cats** have two properties:
    a color, that is, white, and the shape of a cat. This makes it easy for the model
    to know that replacing the white color with black would give us *black cats* when
    we ask for them.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**零样本学习**是指机器学习过程中我们需要预测以前未见过的标签的新样本。通常通过向预训练过程提供额外信息来完成。例如，我们可以告诉模型，所谓的**白猫**具有两个属性：颜色即白色，以及猫的形状。这使得模型可以轻松知道，当我们要求它们时，用黑色替换白色将给我们*黑猫*。'
- en: Similarly, the machine learning process where the new samples are only labeled
    once per class (or very few samples are labeled per class) is called **one-shot
    learning**.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，机器学习过程中，新样本仅一次标记每个类（或每个类很少标记几个样本）的过程称为**单样本学习**。
- en: 'In order to establish the zero-shot learning ability between text and image,
    we will use the word embedding model proposed by Scott Reed, Zeynep Akata, and
    Bernt Schiele, et al in their paper, *Learning Deep Representations of Fine-Grained
    Visual Descriptions*. Their model is designed for one purpose: finding the most
    matching images from a large collection based on a single query sentence.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在文本和图像之间建立零样本学习能力，我们将使用Scott Reed、Zeynep Akata和Bernt Schiele等人在其论文*Learning
    Deep Representations of Fine-Grained Visual Descriptions*中提出的词嵌入模型。他们的模型设计用于一个目的：基于单个查询句子从大型集合中找到最匹配的图像。
- en: 'The following image is an example of image search results from a single query
    sentence:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图片是通过单个查询句子获得的图像搜索结果的示例：
- en: '![](img/cb24ab6f-7baa-4120-888d-0b5566458c51.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb24ab6f-7baa-4120-888d-0b5566458c51.png)'
- en: Examples of image search results from a single query sentence on the CUB-200-2011
    dataset
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUB-200-2011数据集上，通过单个查询句子获得的图像搜索结果示例
- en: We won't dive into the implementation details of the word embedding method here
    and instead use the pretrained `char-CNN-RNN` results provided by the authors.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不会深入讨论词嵌入方法的实现细节，而是使用作者提供的预训练的`char-CNN-RNN`结果。
- en: GAN architecture and training
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAN架构与训练
- en: The design of the GAN model in this section is based on the text-to-image model
    proposed by Scott Reed, Zeynep Akata, and Xinchen Yan, et al in their paper, *Generative
    Adversarial Text to Image Synthesis*. Here, we will describe and define the architectures
    of the generator and discriminator networks and the training process.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中GAN模型的设计基于Scott Reed、Zeynep Akata和Xinchen Yan等人在其论文*Generative Adversarial
    Text to Image Synthesis*中提出的文本到图像模型。在这里，我们将描述和定义生成器和判别器网络的架构以及训练过程。
- en: The generator network has two inputs, including a latent noise vector, ![](img/1c8d67dc-79fd-4232-8a34-666a701583bd.png), and
    the embedding vector, ![](img/e747bae2-9b44-4376-b675-4e9729446fcd.png), of the
    description sentence. The embedding vector, ![](img/11b94c85-71d6-420b-9a62-79bfa784a8ba.png), has
    a length of 1,024, which is mapped by a fully-connected layer to a vector of 128\.
    This vector is concatenated with the noise vector, ![](img/11db02c9-d480-43ee-a5f7-913ae50abb22.png), to
    form a tensor with a size of `[B, 228, 1, 1]` (in which B represents the batch
    size and is omitted from now on). Five transposed convolution layers (with a kernel
    size of 4, a stride size of 2, and a padding size of 1) are used to gradually
    expand the size of feature map (while decreasing the channel width) to `[3, 64,
    64]`, which is the generated image after a `Tanh` activation function. Batch normalization
    layers and `ReLU` activation functions are used in the hidden layers.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络有两个输入，包括一个潜在的噪声向量，![](img/1c8d67dc-79fd-4232-8a34-666a701583bd.png)，和描述句子的嵌入向量，![](img/e747bae2-9b44-4376-b675-4e9729446fcd.png)，长度为1,024，由一个全连接层映射到一个长度为128的向量。这个向量与噪声向量![](img/11db02c9-d480-43ee-a5f7-913ae50abb22.png)连接起来形成一个大小为`[B,
    228, 1, 1]`的张量（其中B表示批量大小，现在被省略）。使用五个转置卷积层（核大小为4，步长为2，填充大小为1），逐步扩展特征图的大小（同时减小通道宽度）到`[3,
    64, 64]`，这是通过`Tanh`激活函数生成的图像。隐藏层中使用批量归一化层和`ReLU`激活函数。
- en: 'Let''s create a new file named `gan.py` to define the networks. Here is the
    code definition of the generator network:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个名为`gan.py`的新文件来定义网络。以下是生成器网络的代码定义：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The discriminator network also has two inputs, which are the generated/real
    image, ![](img/0436cf09-e622-40e4-8157-8609fe701657.png), and the embedding vector, ![](img/9f0ec161-c717-458c-b755-b6afd4802065.png).
    The input image, ![](img/59d4e74c-a099-4663-af65-b478c69c594d.png), is a tensor
    with a size of `[3, 64, 64]` and is mapped to `[512, 4, 4]` through four convolution
    layers. The discriminator network has two outputs and the `[512, 4, 4]` feature
    map is also the second output tensor. The embedding vector, ![](img/7fc07115-6a51-46f0-b573-b19b8a5b963f.png), is
    mapped to a vector with a length of 128 and expanded to a tensor of size `[128,
    4, 4]`, which is then concatenated with the image feature map. Finally, the concatenated
    tensor (with a size of `[640, 4, 4]`) is fed into another convolution layer that
    gives us the prediction value.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器网络也有两个输入，一个是生成的/真实图像，![](img/0436cf09-e622-40e4-8157-8609fe701657.png)，另一个是嵌入向量，![](img/9f0ec161-c717-458c-b755-b6afd4802065.png)。输入图像，![](img/59d4e74c-a099-4663-af65-b478c69c594d.png)，是一个大小为`[3,
    64, 64]`的张量，通过四个卷积层映射到`[512, 4, 4]`。判别器网络有两个输出，`[512, 4, 4]`特征图也是第二个输出张量。嵌入向量，![](img/7fc07115-6a51-46f0-b573-b19b8a5b963f.png)，被映射为一个长度为128的向量，并扩展为大小为`[128,
    4, 4]`的张量，随后与图像特征图进行拼接。最终，拼接后的张量（大小为`[640, 4, 4]`）被输入到另一个卷积层，得到预测值。
- en: 'The code definition of the discriminator network is as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器网络的代码定义如下：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The training process of both networks can be seen in the following diagram.
    We can see that training text-to-image GANs is very similar to the vanilla GAN,
    except that the intermediate outputs (the second output tensors) of the discriminator
    from the real and generated images are used to calculate the L1 loss, while the
    real/generated images are used to calculate the L2 loss:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 两个网络的训练过程可以在下图中看到。我们可以看到，训练文本到图像的 GAN 与普通的 GAN 非常相似，唯一的不同是，判别器从真实和生成图像中得到的中间输出（第二个输出张量）用于计算
    L1 损失，而真实/生成图像则用于计算 L2 损失：
- en: '![](img/39c793ee-d4b7-484b-9544-c55fee800106.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/39c793ee-d4b7-484b-9544-c55fee800106.png)'
- en: Training process of a text-to-image GAN, in which *x** represents the real image,
    *x* represents the generated image, *t* represents the text embedding vector,
    and *z* represents the latent noise vector. The dotted arrows coming out of the
    discriminator, *D*, represent the intermediate output tensors
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一个文本到图像的生成对抗网络（GAN）的训练过程，其中 *x* 代表真实图像，*x* 代表生成的图像，*t* 代表文本嵌入向量，*z* 代表潜在噪声向量。来自判别器
    *D* 的虚线箭头表示中间输出张量。
- en: As we introduce the following code, things are going to be presented in a somewhat
    non-linear fashion. This is to ensure that you understand the processes that are
    involved at each point.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍以下代码时，内容将以一种非线性的方式呈现。这是为了确保你能够理解每个环节涉及的过程。
- en: 'Let''s create a new file named `build_gan.py` and create a one-stop train/eval
    API, just like we did in some of the previous chapters. We will only show the
    crucial parts of the training process. You may fill in the blanks yourself as
    an exercise or refer to the full source code under the `text2image` folder in
    the code repository for this chapter:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个名为 `build_gan.py` 的新文件，并创建一个一站式的训练/评估 API，就像我们在之前的章节中做的那样。我们将只展示训练过程中的关键部分。你可以自行填充空白作为练习，或者参考代码仓库
    `text2image` 文件夹中的完整源代码：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let''s work on the training process (which is defined in `Model.train()`):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来进行训练过程的工作（该过程在`Model.train()`中定义）：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we use the Caltech-UCSD Birds-200-2011 (**CUB-200-2011**) dataset, which
    contains 11,788 annotated bird images. Instead of processing the bird images and
    training the word embedding vectors by ourselves, we will use the pretrained embeddings
    by the authors directly ([https://github.com/reedscot/icml2016](https://github.com/reedscot/icml2016)).
    In the GitHub repository ([https://github.com/aelnouby/Text-to-Image-Synthesis](https://github.com/aelnouby/Text-to-Image-Synthesis)),
    an HDF5 database file containing image files, embedding vectors, and original
    description text is kindly provided.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们使用的是 Caltech-UCSD 鸟类数据集（**CUB-200-2011**），该数据集包含11,788张标注的鸟类图像。我们不会自己处理鸟类图像和训练词向量，而是直接使用作者提供的预训练嵌入（[https://github.com/reedscot/icml2016](https://github.com/reedscot/icml2016)）。在
    GitHub 仓库（[https://github.com/aelnouby/Text-to-Image-Synthesis](https://github.com/aelnouby/Text-to-Image-Synthesis)）中，提供了一个包含图像文件、嵌入向量和原始描述文本的
    HDF5 数据库文件。
- en: Let's download the database file (which is around 5.7 GB in size) from the Google
    Drive link that's provided ([https://drive.google.com/open?id=1mNhn6MYpBb-JwE86GC1kk0VJsYj-Pn5j](https://drive.google.com/open?id=1mNhn6MYpBb-JwE86GC1kk0VJsYj-Pn5j))
    and put it in a folder (for example, `/media/john/DataAsgard/text2image/birds`).
    Let's also download the custom dataset class ([https://github.com/aelnouby/Text-to-Image-Synthesis/blob/master/txt2image_dataset.py](https://github.com/aelnouby/Text-to-Image-Synthesis/blob/master/txt2image_dataset.py))
    because it's a little bit tricky to get exported HDF5 database elements into PyTorch
    tensors correctly. This also means that we need to install the `h5py` library
    before running the script with `pip install h5py`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从提供的Google Drive链接中下载数据库文件（大小约为5.7 GB）（[https://drive.google.com/open?id=1mNhn6MYpBb-JwE86GC1kk0VJsYj-Pn5j](https://drive.google.com/open?id=1mNhn6MYpBb-JwE86GC1kk0VJsYj-Pn5j)），并将其放在一个文件夹中（例如，`/media/john/DataAsgard/text2image/birds`）。我们还需要下载自定义数据集类（[https://github.com/aelnouby/Text-to-Image-Synthesis/blob/master/txt2image_dataset.py](https://github.com/aelnouby/Text-to-Image-Synthesis/blob/master/txt2image_dataset.py)），因为将导出的HDF5数据库元素正确地转化为PyTorch张量有点复杂。这也意味着我们在运行脚本之前需要安装`h5py`库，使用命令`pip
    install h5py`进行安装。
- en: 'Finally, let''s create a `main.py` file and fill in the argument parsing code,
    as we have done many times already, and call `Model.train()` from it. Again, we
    omit most of the code in `main.py`. You can refer to the full source code in this
    chapter''s code repository if you need any help:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们创建一个`main.py`文件，并填写参数解析代码，正如我们已经做过多次一样，并从中调用`Model.train()`。再次强调，我们省略了`main.py`中的大部分代码。如果您需要任何帮助，可以参考本章的代码仓库中的完整源代码：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'It takes about 2 and a half hours to finish 200 epochs of training and costs
    about 1,753 MB of GPU memory with a batch size of 256\. Some of the results by
    the end of training are as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 完成200个epoch的训练大约需要2个半小时，使用256的batch size时，GPU内存消耗大约为1,753 MB。训练结束时，一些结果如下所示：
- en: '![](img/ca438617-3483-41df-bc7b-2fedd6e06e4d.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca438617-3483-41df-bc7b-2fedd6e06e4d.png)'
- en: Images generated by a text-to-image GAN on the CUB-200-2011 dataset
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文本到图像的GAN在CUB-200-2011数据集上生成的图像
- en: The method that we used in this section was proposed more than 3 years ago,
    and so the quality of the generated images is not as good as it should be in this
    day and age. Therefore, we will introduce you to StackGAN and StackGAN++ so that
    you can generate high-resolution results.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中使用的方法是3年前提出的，因此生成的图像质量不如今天应该达到的水平。因此，我们将向您介绍StackGAN和StackGAN++，以便您能够生成高分辨率的结果。
- en: Generating photo-realistic images with StackGAN++
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用StackGAN++生成逼真的照片级图像
- en: The generation of images from description text can be considered as a **Conditional
    GAN** (**CGAN**) process in which the embedding vector of the description sentence
    is used as the additional label information. Luckily for us, we already know how
    to use CGAN models to generate convincing images. Now, we need to figure out how
    to generate large images with CGAN.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从描述文本生成图像可以视为一个**条件GAN**（**CGAN**）过程，其中描述句子的嵌入向量作为附加标签信息。幸运的是，我们已经知道如何使用CGAN模型生成令人信服的图像。现在，我们需要弄清楚如何使用CGAN生成大尺寸图像。
- en: Do you remember how we used two generators and two discriminators to fill out
    the missing holes in images (image inpainting) in [Chapter 7](c9fec01a-2b58-4de3-a62d-da11928e5afe.xhtml),
    *Image Restoration with GANs*? It's also possible to stack two CGANs together
    so that we can get high-quality images. This is exactly what StackGAN does.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你还记得我们如何在[第7章](c9fec01a-2b58-4de3-a62d-da11928e5afe.xhtml)《使用GAN进行图像修复》中，利用两个生成器和两个判别器填补图像中的缺失部分（图像修复）吗？同样，我们也可以将两个CGAN堆叠在一起，从而获得高质量的图像。这正是StackGAN所做的。
- en: High-resolution text-to-image synthesis with StackGAN
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用StackGAN进行高分辨率文本到图像合成
- en: '**StackGAN** was proposed by Han Zhang, Tao Xu, and Hongsheng Li, et al in
    their paper, *StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative
    Adversarial Networks*.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**StackGAN**是由Han Zhang、Tao Xu和Hongsheng Li等人在他们的论文《*StackGAN：基于堆叠生成对抗网络的文本到逼真图像合成*》中提出的。'
- en: The embedding vector, ![](img/c7f5af9d-724c-4630-9d94-ddae4343edd5.png), of
    the description sentence is processed by the Conditioning Augmentation step to
    create a conditional vector, ![](img/93aef638-6af8-466f-a4e7-94b7a5037865.png).
    In Conditioning Augmentation, a pair of mean, ![](img/759a63f4-c271-44c0-9583-202625fd74fa.png), and
    standard deviation, ![](img/6c7dc87d-7ac5-40a3-bab8-1b6ac7c86251.png), vectors
    are calculated from the embedding vector, ![](img/5d858e81-a298-40db-a739-282de8b317b3.png), to
    generate the conditional vector, ![](img/6649db62-a8e5-4699-910f-bc54419ebfdc.png), based
    on the Gaussian distribution, ![](img/f96b7563-a795-4250-86c9-a307f98a1df3.png).
    This process lets us create much more unique conditional vectors from limited
    text embeddings and ensures that all the conditional variables obey the same Gaussian
    distribution. At the same time, ![](img/9dbac81e-46b0-4230-9c91-8857b5fb2b43.png) and ![](img/a60ba181-13b0-40aa-adcc-49d5bbcc7e25.png) are
    restrained so that they're not too far away from ![](img/6329474d-f9f9-4b0b-ae9b-bf551ef8809c.png).
    This is done by adding a Kullback-Leibler divergence (**KL divergence**) term
    to the generator's loss functions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 描述句子的嵌入向量，![](img/c7f5af9d-724c-4630-9d94-ddae4343edd5.png)，经过条件增强步骤处理，生成一个条件向量，![](img/93aef638-6af8-466f-a4e7-94b7a5037865.png)。在条件增强中，一对均值，![](img/759a63f4-c271-44c0-9583-202625fd74fa.png)，和标准差，![](img/6c7dc87d-7ac5-40a3-bab8-1b6ac7c86251.png)，向量是从嵌入向量，![](img/5d858e81-a298-40db-a739-282de8b317b3.png)，计算得出，用于生成基于高斯分布的条件向量，![](img/6649db62-a8e5-4699-910f-bc54419ebfdc.png)，![](img/f96b7563-a795-4250-86c9-a307f98a1df3.png)。这个过程使我们能够从有限的文本嵌入生成更多独特的条件向量，并确保所有的条件变量遵循相同的高斯分布。同时，![](img/9dbac81e-46b0-4230-9c91-8857b5fb2b43.png)和![](img/a60ba181-13b0-40aa-adcc-49d5bbcc7e25.png)被限制，使得它们不会偏离![](img/6329474d-f9f9-4b0b-ae9b-bf551ef8809c.png)太远。这是通过在生成器的损失函数中加入Kullback-Leibler散度（**KL散度**）项来实现的。
- en: 'A latent vector, ![](img/d95c20b8-4a36-4510-9505-ed6745508a7b.png) (which is
    sampled from ![](img/58bb6f57-35ad-4540-9427-61f6369f215c.png)), is combined with
    the conditional vector, ![](img/238c0643-d964-4c2d-8d3e-438eaf7112df.png), to
    serve as the input of the **Stage-I Generator**. The first generator network generates
    a low-resolution image with a size of 64 x 64\. The low-resolution image is passed
    to the **Stage-I Discriminator**, which also takes the embedding vector, ![](img/c681f0ec-8d8f-43ae-ad5a-adf43138327c.png), as
    input to predict the fidelity of the low-resolution image. The loss functions
    of the Stage-I Generator and Discriminator are as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一个潜在向量，![](img/d95c20b8-4a36-4510-9505-ed6745508a7b.png)（它是从![](img/58bb6f57-35ad-4540-9427-61f6369f215c.png)采样的），与条件向量，![](img/238c0643-d964-4c2d-8d3e-438eaf7112df.png)，结合，作为**Stage-I生成器**的输入。第一个生成器网络生成一个低分辨率的图像，大小为64
    x 64。低分辨率图像被传递到**Stage-I判别器**，后者同样将嵌入向量，![](img/c681f0ec-8d8f-43ae-ad5a-adf43138327c.png)，作为输入来预测低分辨率图像的真实性。Stage-I生成器和判别器的损失函数如下：
- en: '![](img/b3599625-7cd2-4d78-be35-7eecdeb8edda.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b3599625-7cd2-4d78-be35-7eecdeb8edda.png)'
- en: In the preceding equations, ![](img/27d8a610-f64a-41a2-a357-58a8ab0b3df6.png) in ![](img/93bbbd44-48d1-4c9b-a9d8-7561dc2de372.png) is
    the output of the Stage-I Generator ![](img/68201ed0-605a-4081-981c-29f3055abe4c.png),
    in which ![](img/6cb26ab9-a50b-4c09-b227-d64470257f64.png) is the conditional
    vector and ![](img/b9920f48-2f92-4098-b1ab-6e5d49abcaeb.png)represents the KL
    divergence.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，![](img/27d8a610-f64a-41a2-a357-58a8ab0b3df6.png)中的![](img/93bbbd44-48d1-4c9b-a9d8-7561dc2de372.png)是Stage-I生成器的输出，![](img/68201ed0-605a-4081-981c-29f3055abe4c.png)，其中，![](img/6cb26ab9-a50b-4c09-b227-d64470257f64.png)是条件向量，![](img/b9920f48-2f92-4098-b1ab-6e5d49abcaeb.png)表示KL散度。
- en: 'Then, the low-resolution image is fed into the **Stage-II Generator**. Again, the embedding
    vector, ![](img/c096dd4e-3d3c-4cd5-9266-26dcf2bb073b.png), is also passed to the
    second generator to help create the high-resolution image that''s 256 x 256 in
    size. The quality of the high-resolution image is judged by the **Stage-II Discriminator**, which
    also takes ![](img/71aff809-f328-4c93-8222-8f5e9486d639.png) as input. The loss
    functions in the second stage are similar to the first stage, as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，低分辨率图像被输入到**Stage-II生成器**。同样，嵌入向量，![](img/c096dd4e-3d3c-4cd5-9266-26dcf2bb073b.png)，也传递到第二个生成器，帮助生成大小为256
    x 256的高分辨率图像。高分辨率图像的质量由**Stage-II判别器**判断，后者同样接收![](img/71aff809-f328-4c93-8222-8f5e9486d639.png)作为输入。第二阶段的损失函数与第一阶段类似，如下所示：
- en: '![](img/86afb79c-8af9-46cf-82e8-aee5f5cd0487.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86afb79c-8af9-46cf-82e8-aee5f5cd0487.png)'
- en: In the preceding equations , ![](img/cc9a10ce-ed57-4f71-ad42-55d19c91d2b2.png) in ![](img/cd3dfe50-c9df-489a-a1cc-c0467eeb951a.png) is
    the output of the Stage-II Generator, ![](img/e9134e36-e6cb-41de-97b9-715bb863bff2.png),
    in which ![](img/88488533-71cb-47cb-b839-c2ea31bd15ce.png) is the output of the Stage-I
    Generator and ![](img/af026797-9233-4afb-80a2-d8258da7aa46.png) is the conditional
    vector.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，![](img/cc9a10ce-ed57-4f71-ad42-55d19c91d2b2.png) 在 ![](img/cd3dfe50-c9df-489a-a1cc-c0467eeb951a.png) 中是Stage-II生成器的输出，![](img/e9134e36-e6cb-41de-97b9-715bb863bff2.png)，其中![](img/88488533-71cb-47cb-b839-c2ea31bd15ce.png) 是Stage-I生成器的输出，![](img/af026797-9233-4afb-80a2-d8258da7aa46.png) 是条件向量。
- en: Some images that have been generated by StackGAN will be provided in the upcoming
    sections. If you are interested in trying out StackGAN, the authors of the paper
    have opensourced a PyTorch version here: [https://github.com/hanzhanggit/StackGAN-Pytorch](https://github.com/hanzhanggit/StackGAN-Pytorch).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由StackGAN生成的一些图像将在接下来的章节中提供。如果你有兴趣尝试StackGAN，论文的作者已经在这里开源了一个PyTorch版本：[https://github.com/hanzhanggit/StackGAN-Pytorch](https://github.com/hanzhanggit/StackGAN-Pytorch)。
- en: From StackGAN to StackGAN++
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从StackGAN到StackGAN++
- en: '**StackGAN++** (also called StackGAN v2) is an improved version of StackGAN
    and was proposed by Han Zhang, Tao Xu, and Hongsheng Li, et al in their paper,
    *StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks*.
    Compared to StackGAN, there are three main differences in the design of StackGAN++,
    which are as follows.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**StackGAN++**（也叫StackGAN v2）是StackGAN的改进版本，由Han Zhang、Tao Xu和Hongsheng Li等人在他们的论文《StackGAN++:
    Realistic Image Synthesis with Stacked Generative Adversarial Networks》中提出。与StackGAN相比，StackGAN++在设计上有三个主要的区别，具体如下：'
- en: '**Multi-scale image synthesis**: It uses a tree-like structure (as shown in
    the following diagram) in which each branch represents an individual generator
    network and the size of generated image increases as the tree becomes higher.
    The quality of the images that are generated by each branch is estimated by a
    different discriminator network.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多尺度图像合成**：它使用一种树形结构（如下图所示），其中每个分支代表一个独立的生成器网络，随着树的高度增加，生成图像的尺寸也会增大。每个分支生成的图像质量是由不同的判别器网络来估算的。'
- en: '**Employment of unconditional loss**: Besides using label information (calculated
    from text embedding) to estimate the fidelity of images, additional loss terms,
    where the images are the only inputs, are added to the loss function of every
    generator and discriminator (as shown in the following equation).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采用无条件损失**：除了使用标签信息（通过文本嵌入计算）来估算图像的保真度外，还向每个生成器和判别器的损失函数中添加了额外的损失项，这些损失项仅使用图像作为输入（如下式所示）。'
- en: 'The loss functions of the discriminator and generator at the ![](img/e5ab09e2-29b8-4f74-8d79-99967bc59a1d.png) th
    branch are defined as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器和生成器在 ![](img/e5ab09e2-29b8-4f74-8d79-99967bc59a1d.png) 第 th分支的损失函数定义如下：
- en: '![](img/eb599287-74c7-4877-ad25-1d1f92ea0dad.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb599287-74c7-4877-ad25-1d1f92ea0dad.png)'
- en: In the preceding equation, the first line in each loss function is called the
    conditional loss, and the second line is called the unconditional loss. They are
    calculated by the **JCU Discriminator**, which was illustrated in the previous
    diagram.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，每个损失函数的第一行叫做条件损失，第二行叫做无条件损失。它们是通过**JCU判别器**计算的，该判别器在之前的图中已有说明。
- en: '**Color-consistency restraints**: Since there can be several branches in the
    tree structure, it is important to ensure that the images that are generated by
    different branches are similar to each other. Therefore, a color-consistency regularization
    term is added to the generator''s loss function (with a scale factor, of course).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**颜色一致性约束**：由于树形结构中可能有多个分支，因此确保不同分支生成的图像彼此相似是很重要的。因此，一个颜色一致性正则化项被添加到生成器的损失函数中（当然，还会有一个比例因子）。'
- en: 'The color-consistency regularization is defined as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 颜色一致性正则化定义如下：
- en: '![](img/b9c0f454-d81e-4113-8967-9fd21a1a7d7b.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9c0f454-d81e-4113-8967-9fd21a1a7d7b.png)'
- en: In the preceding formula, ![](img/38e5f946-4502-4b72-b44c-931f0436ef6c.png) represents
    the batch size, while ![](img/5bea8296-a644-474c-86e8-41508819fe46.png) and ![](img/d76aef4f-8ff0-4d0d-8489-31b5077db110.png) represent
    the mean and covariance of the ![](img/5f6dd24c-3cd2-4375-8067-9fe9084b84f0.png) th
    image generated by the ![](img/abff9797-76f8-4e79-88e8-b77d97d73a58.png) th generator.
    This makes sure that the images that are generated by neighboring branches have
    similar color structures.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述公式中，![](img/38e5f946-4502-4b72-b44c-931f0436ef6c.png)代表批量大小，而![](img/5bea8296-a644-474c-86e8-41508819fe46.png)和![](img/d76aef4f-8ff0-4d0d-8489-31b5077db110.png)代表由第![](img/5f6dd24c-3cd2-4375-8067-9fe9084b84f0.png)个图像生成的第![](img/abff9797-76f8-4e79-88e8-b77d97d73a58.png)个生成器的均值和协方差。这确保由相邻分支生成的图像具有相似的颜色结构。
- en: Training StackGAN++ to generate images with better quality
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练StackGAN++生成更高质量的图像
- en: 'The authors of StackGAN++ have kindly open sourced the full source code here: [https://github.com/hanzhanggit/StackGAN-v2](https://github.com/hanzhanggit/StackGAN-v2).
    Follow these steps to train StackGAN++ on the CUB-200-2011 dataset. Make sure
    you have created a **Python 2.7** environment with PyTorch in Anaconda since there
    will be decoding errors from `pickle` when loading the pretrained text embeddings.
    You can follow the steps in [Chapter 2](4459c703-9610-43e7-9eda-496d63a45924.xhtml),
    *Getting Started with PyTorch 1.3*, to create a new environment:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: StackGAN++的作者已经很好地开源了完整的源代码：[https://github.com/hanzhanggit/StackGAN-v2](https://github.com/hanzhanggit/StackGAN-v2)。按照这些步骤在CUB-200-2011数据集上训练StackGAN++。确保你已经在Anaconda中创建了一个**Python
    2.7**环境，并安装了PyTorch，因为在加载预训练文本嵌入时会从`pickle`中出现解码错误。你可以参考[第2章](4459c703-9610-43e7-9eda-496d63a45924.xhtml)，*开始使用PyTorch
    1.3*，来创建一个新的环境：
- en: 'Install the prerequisites by running the following command in your Terminal:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的终端中通过运行以下命令安装先决条件：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Make sure you don't have `tensorboard` installed in your Python 2.7 environment
    since StackGAN++ calls `FileWriter` to write logging information to TensorBoard
    and `FileWriter` has been removed in the latest version of TensorBoard. If you
    don't want to uninstall TensorBoard, you can downgrade it by running `pip install
    tensorboard==1.0.0a6`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你的Python 2.7环境中没有安装`tensorboard`，因为StackGAN++调用`FileWriter`将日志信息写入TensorBoard，而`FileWriter`已在最新版本的TensorBoard中移除。如果你不想卸载TensorBoard，可以通过运行`pip
    install tensorboard==1.0.0a6`来降级它。
- en: 'Download the source code of StackGAN++:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载StackGAN++的源代码：
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Download the CUB-200-2011 dataset from [http://www.vision.caltech.edu/visipedia/CUB-200-2011.html](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)
    and put the `CUB_200_2011` folder in the `data/birds` directory, so that the images
    are located at paths such as `data/birds/CUB_200_2011/images/001.Black_footed_Albatross/Black_Footed_Albatross_0001_796111.jpg`.
    The compressed file that needs to be downloaded is about 1.1 GB in size.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[http://www.vision.caltech.edu/visipedia/CUB-200-2011.html](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)下载CUB-200-2011数据集，并将`CUB_200_2011`文件夹放置在`data/birds`目录下，以便图像位于类似于`data/birds/CUB_200_2011/images/001.Black_footed_Albatross/Black_Footed_Albatross_0001_796111.jpg`的路径中。需要下载的压缩文件大小约为1.1
    GB。
- en: Download the pretrained text embeddings from [https://drive.google.com/open?id=0B3y_msrWZaXLT1BZdVdycDY5TEE](https://drive.google.com/open?id=0B3y_msrWZaXLT1BZdVdycDY5TEE)
    and move the three folders in it to `data/birds`. Make sure that you rename the
    `text_c10` folder to `text`.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://drive.google.com/open?id=0B3y_msrWZaXLT1BZdVdycDY5TEE](https://drive.google.com/open?id=0B3y_msrWZaXLT1BZdVdycDY5TEE)下载预训练的文本嵌入，并将其中的三个文件夹移动到`data/birds`。确保将`text_c10`文件夹重命名为`text`。
- en: 'Navigate to the code folder and start the training process:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到代码文件夹并开始训练过程：
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You only need to make a few changes to the source code of StackGAN++ so that
    it can run under PyTorch 1.1; for example, you can replace all the `.data[0]`
    with `.item()` in `trainer.py`. There are also several deprecation warnings that
    we can fix. You can refer to the source code located under the `stackgan-v2` folder
    in this book's code repository for this chapter for more information.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需对StackGAN++的源代码进行少量修改，以便其可以在PyTorch 1.1下运行；例如，你可以在`trainer.py`中将所有的`.data[0]`替换为`.item()`。还有几个弃用警告需要我们修复。你可以参考本书本章的`stackgan-v2`文件夹中的源代码库获取更多信息。
- en: '(Optional) Test your trained model. Specify the model file in the `code/cfg/eval_birds.yml`
    file like so:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （可选）测试你训练好的模型。在`code/cfg/eval_birds.yml`文件中指定模型文件：
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, run the following script in your Terminal to begin the evaluation process:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在你的终端中运行以下脚本来开始评估过程：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The evaluation costs about 7,819 MB of GPU memory and takes 12 minutes to finish.
    The generated images will be located in the `output/birds_3stages_2019_07_16_23_57_11/Model/iteration220800/single_samples/valid`
    folder.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 评估大约需要7,819 MB的GPU内存，并且需要12分钟才能完成。生成的图像将位于`output/birds_3stages_2019_07_16_23_57_11/Model/iteration220800/single_samples/valid`文件夹中。
- en: 'It takes about 48 hours to finish 600 epochs of training on a GTX 1080Ti graphics
    card and costs about 10,155 MB of GPU memory. Here are some of the images that
    are generated by the end of the training process:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在GTX 1080Ti显卡上完成600个训练周期大约需要48小时，并且大约消耗10,155 MB的GPU内存。以下是训练结束时生成的部分图像：
- en: '![](img/06294c97-6264-40bf-8ac6-3771f30b8f00.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06294c97-6264-40bf-8ac6-3771f30b8f00.png)'
- en: Image generated by StackGAN++
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由StackGAN++生成
- en: While this process takes a very long time and a large amount of GPU memory,
    you can see that the results are very nice.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个过程需要非常长的时间和大量的GPU内存，但你可以看到生成的结果非常出色。
- en: Summary
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have learned how to generate low-resolution and high-resolution
    images based on description text.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们学习了如何根据描述文本生成低分辨率和高分辨率图像。
- en: In the next chapter, we will focus on directly generating sequence data, such
    as text and audio, with GANs.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将重点介绍如何使用GAN直接生成序列数据，如文本和音频。
- en: Further reading
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Rong X. (2014). *word2vec Parameter Learning Explained*. arXiv:1411.2738.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rong X.（2014）。*word2vec参数学习解析*。arXiv:1411.2738。
- en: Reed S, Akata Z, Schiele B, et. al. (2016). *Learning Deep Representations of
    Fine-Grained Visual Descriptions*. CVPR.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Reed S, Akata Z, Schiele B等（2016）。*深度学习细粒度视觉描述的表示方法*。CVPR。
- en: Reed S, Akata Z, Yan X, et al (2016). *Generative Adversarial Text to Image
    Synthesis*. ICML.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Reed S, Akata Z, Yan X等（2016）。*生成对抗文本到图像合成*。ICML。
- en: 'Zhang H, Xu T, Li H, et al (2017). *StackGAN: Text to Photo-realistic Image
    Synthesis with Stacked Generative Adversarial Networks*. ICCV.'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhang H, Xu T, Li H等（2017）。*StackGAN：基于堆叠生成对抗网络的文本到照片级真实图像合成*。ICCV。
- en: 'Zhang H, Xu T, Li H, et al (2018). *StackGAN++: Realistic Image Synthesis with
    Stacked Generative Adversarial Networks*. IEEE Trans. on Pattern Analysis and
    Machine Intelligence.'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhang H, Xu T, Li H等（2018）。*StackGAN++：基于堆叠生成对抗网络的真实图像合成*。IEEE模式分析与机器智能杂志。
