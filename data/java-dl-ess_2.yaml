- en: Chapter 2. Algorithms for Machine Learning – Preparing for Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 2 章. 机器学习算法 – 深度学习的准备
- en: In the previous chapter, you read through how deep learning has been developed
    by looking back through the history of AI. As you should have noticed, machine
    learning and deep learning are inseparable. Indeed, you learned that deep learning
    is the developed method of machine learning algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你通过回顾人工智能的历史，了解了深度学习是如何发展的。正如你应该已经注意到的，机器学习和深度学习是密不可分的。事实上，你学到了深度学习是机器学习算法的拓展方法。
- en: In this chapter, as a pre-exercise to understand deep learning well, you will
    see the mode details of machine learning, and in particular, you will learn the
    actual code for the method of machine learning, which is closely related to deep
    learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，作为理解深度学习的预备练习，你将看到机器学习模式的详细介绍，尤其是你将学习与深度学习密切相关的机器学习方法的实际代码。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: The core concepts of machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的核心概念
- en: An overview of popular machine learning algorithms, especially focusing on neural
    networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行的机器学习算法概览，特别是聚焦于神经网络
- en: 'Theories and implementations of machine learning algorithms related to deep
    learning: perceptrons, logistic regression, and multi-layer perceptrons'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与深度学习相关的机器学习算法的理论和实现：感知机、逻辑回归和多层感知机
- en: Getting started
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始入门
- en: We will insert the source code of machine learning and deep learning with Java
    from this chapter. The version of JDK used in the code is 1.8, hence Java versions
    greater than 8 are required. Also, IntelliJ IDEA 14.1 is used for the IDE. We
    will use the external library from [Chapter 5](ch05.html "Chapter 5. Exploring
    Java Deep Learning Libraries – DL4J, ND4J, and More"), *Exploring Java Deep Learning
    Libraries – DL4J, ND4J, and More*, so we are starting with a new Maven project.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章插入与 Java 相关的机器学习和深度学习源代码。代码使用的 JDK 版本为 1.8，因此需要 Java 8 及以上版本。此外，IDE 使用的是
    IntelliJ IDEA 14.1。我们将使用[第 5 章](ch05.html "第 5 章：探索 Java 深度学习库 – DL4J、ND4J 等")中的外部库，*探索
    Java 深度学习库 – DL4J、ND4J 等*，所以我们将从一个新的 Maven 项目开始。
- en: 'The root package name of the code used in this book is `DLWJ`, the initials
    of *Deep Learning with Java*, and we will add a new package or a class under `DLWJ`
    as required. Please refer to the screenshot below, which shows the screen immediately
    after the new project is made:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用的代码根包名为`DLWJ`，即*深度学习与 Java*（Deep Learning with Java）的首字母，我们将根据需要在`DLWJ`下添加新的包或类。请参考下面的截图，它显示的是新建项目后的界面：
- en: '![Getting started](img/B04779_02_45.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![开始入门](img/B04779_02_45.jpg)'
- en: There will be some names of variables and methods in the code that don't follow
    the Java coding standard. This is to improve your understanding together with
    some characters in the formulas to increase readability. Please bear this in mind
    in advance.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中会出现一些不符合 Java 编码标准的变量和方法名。这是为了配合公式中的一些字符，帮助提升你的理解并增强可读性。请提前注意这一点。
- en: The need for training in machine learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的训练需求
- en: You have already seen that machine learning is a method of pattern recognition.
    Machine learning reaches an answer by recognizing and sorting out patterns from
    the given learning data. It may seem easy when you just look at the sentence,
    but the fact is that it takes quite a long time for machine learning to sort out
    unknown data, in other words, to build the appropriate model. Why is that? Is
    it that difficult to just sort out? Does it even bother to have a "learning" phase
    in between?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经知道，机器学习是一种模式识别方法。机器学习通过识别和整理给定的学习数据中的模式来得出答案。仅从这句话看起来可能很简单，但事实是，机器学习要整理出未知数据，换句话说，要建立适当的模型，实际上需要花费相当长的时间。为什么会这样？仅仅整理一下就这么难吗？它为什么要有一个“学习”阶段？
- en: 'The answer is, of course, yes. It is extremely difficult to sort out data appropriately.
    The more complicated a problem becomes, the more it becomes impossible to perfectly
    classify data. This is because there are almost infinite patterns of categorization
    when you simply say "pattern classifier." Let''s look at a very simple example
    in the following graph:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 答案当然是肯定的。整理数据确实非常困难。问题越复杂，完美分类数据就越不可能。这是因为当你简单地说“模式分类器”时，几乎有无穷多种分类模式。让我们来看一个非常简单的例子，见下图：
- en: '![The need for training in machine learning](img/B04779_02_46.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习中的训练需求](img/B04779_02_46.jpg)'
- en: There are two types of data, circles and triangles, and the unknown data, the
    square. You don't know which group the square belongs to in the two-dimensional
    coordinate space, so the task is to find out which group the square belongs to.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的数据：圆形和三角形，以及未知数据——正方形。你不知道正方形在二维坐标空间中属于哪个类别，因此任务是找出正方形属于哪个类别。
- en: 'You might instantly know that there seems to be a boundary that separates two
    data types. And if you decide where to set this boundary, it looks like you should
    be able to find out to which group the square belongs. Well then, let''s decide
    the boundary. In reality, however, it is not so easy to clearly define this boundary.
    If you want to set a boundary, there are various lines to consider, as you can
    see in the following figure:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会立刻意识到，似乎有一条边界将这两种数据类型分开。如果你决定在何处设置这条边界，看起来你应该能够找出正方形属于哪个类别。那么，接下来我们来决定这条边界。然而，现实中，清晰地定义这条边界并不像想象的那么容易。如果你想设置边界，有各种各样的线需要考虑，如下图所示：
- en: '![The need for training in machine learning](img/B04779_02_47.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习中训练的必要性](img/B04779_02_47.jpg)'
- en: Additionally, depending on the placement of the boundary, you can see that the
    square might be allocated to a different group or pattern. Furthermore, it is
    also possible to consider that the boundary might be a nonlinear boundary.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，根据边界的位置，你可以看到正方形可能被分配到不同的类别或模式。此外，还可以考虑边界可能是一个非线性边界。
- en: In machine learning, what a machine does in training is choose the most likely
    boundary from these possible patterns. It will automatically learn how to sort
    out patterns when processing massive amounts of training data one after another.
    In other words, it adjusts the parameters of a mathematical model and eventually
    decides the boundary. The boundary decided by machine learning is called the **decision
    boundary** and is not necessarily a linear or nonlinear boundary. A decision boundary
    can also be a hyperplane if it classifies the data best. The more complicated
    the distribution of the data is, the more likely it is that the decision boundary
    would be nonlinear boundary or a hyperplane. A typical case is the multi-dimensional
    classification problem. We have already faced such difficulty by just setting
    a boundary in this simple problem, so it's not hard to imagine that it would be
    very time-consuming to solve a more complicated problem.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，机器在训练过程中所做的事情是从这些可能的模式中选择最有可能的边界。它会通过处理大量的训练数据逐步自动学习如何整理模式。换句话说，它调整数学模型的参数，并最终决定边界。机器学习决定的边界称为**决策边界**，不一定是线性或非线性边界。如果一个边界能够最好地分类数据，它也可以是一个超平面。数据分布越复杂，决策边界越有可能是非线性边界或超平面。一个典型的例子就是多维分类问题。我们在这个简单的问题中仅仅设置边界就已经遇到了困难，因此不难想象，解决一个更复杂的问题会非常耗时。
- en: Supervised and unsupervised learning
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习与无监督学习
- en: In the previous section, we saw that there could be millions of boundaries even
    for a simple classification problem, but it is difficult to say which one of them
    is the most appropriate. This is because, even if we could properly sort out patterns
    in the known data, it doesn't mean that unknown data can also be classified in
    the same pattern. However, you can increase the percentage of correct pattern
    categorization. Each method of machine learning sets a standard to perform a better
    pattern classifier and decides the most possible boundary—the decision boundary—to
    increase the percentage. These standards are, of course, greatly varied in each
    method. In this section, we'll see what all the approaches we can take are.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到即使是一个简单的分类问题，也可能有数百万条边界，但很难说哪一条最合适。这是因为，即使我们能正确地整理出已知数据中的模式，也并不意味着未知数据也能按照相同的模式分类。然而，你可以提高正确分类模式的比例。每种机器学习方法都会设定标准，以便执行更好的模式分类器，并决定最可能的边界——决策边界——以提高分类的准确率。当然，这些标准在每种方法中差异很大。在这一节中，我们将了解我们可以采取的所有方法。
- en: First, machine learning can be broadly classified into **supervised learning**
    and **unsupervised learning**. The difference between these two categories is
    the dataset for machine learning is labeled data or unlabeled data. With supervised
    learning, a machine uses labeled data, the combination of input data and output
    data, and mentions which pattern each type of data is to be classified as. When
    a machine is given unknown data, it will derive what pattern can be applied and
    classify the data based on labeled data, that is, the past correct answers. As
    an example, in the field of image recognition, when you input some images into
    a machine, if you prepare and provide a certain number of images of a cat, labeled
    `cat`, and the same number of images of a human, labeled `human`, for a machine
    to learn, it can judge by itself which group out of cat or human (or none of them)
    that an image belongs to. Of course, just deciding whether the image is a cat
    or a human doesn't really provide a practical use, but if you apply the same approach
    to other fields, you can create a system that can automatically tag who is who
    in a photo uploaded on social media. As you can now see, in supervised training,
    the learning proceeds when a machine is provided with the correct data prepared
    by humans in advance.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，机器学习可以大致分为**监督学习**和**无监督学习**。这两者之间的区别在于机器学习的数据集是标记数据还是未标记数据。在监督学习中，机器使用的是标记数据，即输入数据和输出数据的组合，并标明每种类型的数据应分类为哪种模式。当机器获得未知数据时，它会根据标记数据（也就是过去的正确答案）推导出可以应用的模式，并根据这些模式对数据进行分类。例如，在图像识别领域，当你将一些图像输入到机器中时，如果你准备并提供一定数量标记为`cat`的猫的图像和同样数量标记为`human`的人类图像，机器就可以判断该图像属于猫还是人类（或者都不是）。当然，仅仅判断图像是猫还是人类并没有实际应用，但如果你将相同的方法应用到其他领域，就可以创建一个自动标记社交媒体上传照片中人物的系统。正如你现在所看到的，在监督学习中，学习是通过提前由人类准备好的正确数据来进行的。
- en: On the other hand, with unsupervised learning, a machine uses unlabeled data.
    In this case, only input data is given. Then, what the machine learns is patterns
    and rules that the dataset includes and contains. The purpose of unsupervised
    learning is to grasp the structure of the data. It can include a process called
    **clustering**, which classifies a data constellation in each group that has a
    common character, or the process of extracting the correlation rule. For example,
    imagine there is data relating to a user's age, sex, and purchase trend for an
    online shopping website. Then, you might find out that the tastes of men in their
    20s and women in their 40s are close, and you want to make use of this trend to
    improve your product marketing. We have a famous story here—it was discovered
    from unsupervised training that a large number of people buy beer and diapers
    at the same time.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在无监督学习中，机器使用的是未标记的数据。在这种情况下，只有输入数据被提供。那么，机器学习的内容是数据集中包含和涵盖的模式和规则。无监督学习的目的是理解数据的结构。它可以包括一个叫做**聚类**的过程，将具有共同特征的数据分组，或者是提取关联规则的过程。例如，假设有一份与用户的年龄、性别和在线购物趋势相关的数据。那么，你可能会发现20多岁男性和40多岁女性的品味相似，你可能希望利用这一趋势来改善你的产品营销。这里有一个著名的故事——通过无监督学习发现，很多人会同时购买啤酒和尿布。
- en: You now know there are big differences between supervised learning and unsupervised
    learning, but that's not all. There are also different learning methods and algorithms
    for each learning method, respectively. Let's look at some representative examples
    in the following section.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在知道监督学习和无监督学习之间有很大的区别，但这还不是全部。每种学习方法和每种算法也有所不同。接下来我们将看看一些具有代表性的例子。
- en: Support Vector Machine (SVM)
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）
- en: 'You could say that SVM is the most popular supervised training method in machine
    learning. The method is still used for broad fields in the data mining industry.
    With SVM, data from each category located the closest to other categories is marked
    as the standard, and the decision boundary is determined using the standard so
    that the sum of the Euclidean distance from each marked data and the boundary
    is maximized. This marked data is called **support vectors**. Simply put, SVM
    sets the decision boundary in the middle point where the distance from every pattern
    is maximized. Therefore, what SVM does in its algorithm is known as **maximizing
    the margin**. The following is the figure of the concept of SVM:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，SVM是机器学习中最流行的监督学习方法。该方法仍然广泛应用于数据挖掘行业的各个领域。通过SVM，将每个类别中最接近其他类别的数据点作为标准标记，决策边界使用这些标准来确定，使得每个标记数据点与边界之间的欧几里得距离之和最大化。这个标记的数据点被称为**支持向量**。简单来说，SVM将决策边界设置在每个模式距离最大的位置。因此，SVM在其算法中执行的操作被称为**最大化间隔**。以下是SVM概念的图示：
- en: '![Support Vector Machine (SVM)](img/B04779_02_80.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机（SVM）](img/B04779_02_80.jpg)'
- en: 'If you only hear this statement, you might think "is that it?" but what makes
    SVM the most valuable is a math technique: the kernel trick, or the kernel method.
    This technique takes the data that seems impossible to be classified linearly
    in the original dimension and intentionally maps it to a higher dimensional space
    so that it can be classified linearly without any difficulties. Take a look at
    the following figure so you can understand how the kernel trick works:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只听到这个说法，你可能会想“就这么简单吗？”但使SVM最有价值的是一种数学技巧：核技巧，或者称为核方法。这个技巧将看似无法在线性分类的原始数据故意映射到更高维的空间，使其能够在线性上分类而不遇到任何困难。看看以下图示，你就能理解核技巧是如何工作的：
- en: '![Support Vector Machine (SVM)](img/B04779_02_56.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机（SVM）](img/B04779_02_56.jpg)'
- en: We have two types of data, represented by circles and triangles, and it is obvious
    that it would be impossible to separate both data types linearly in a two-dimensional
    space. However, as you can see in the preceding figure, by applying the kernel
    function to the data (strictly speaking, the feature vectors of training data),
    whole data is transformed into a higher dimensional space, that is, a three-dimensional
    space, and it is possible to separate them with a two-dimensional plane.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两种类型的数据，分别用圆圈和三角形表示，很明显在二维空间中无法线性地分开这两种数据类型。然而，正如你在前面的图示中看到的，通过将核函数应用于数据（严格来说是训练数据的特征向量），所有数据被转换到更高维的空间，即三维空间，并且可以用二维平面将它们分开。
- en: While SVM is useful and elegant, it has one demerit. Since it maps the data
    into a higher dimension, the number of calculations often increases, so it tends
    to take more time in processing as the calculation gets more complicated.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然支持向量机（SVM）既有用又优雅，但它也有一个缺点。由于它将数据映射到更高的维度，计算量往往增加，因此随着计算复杂性的增加，处理时间也往往更长。
- en: Hidden Markov Model (HMM)
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型（HMM）
- en: HMM is an unsupervised training method that assumes data follows the **Markov
    process**. The Markov process is a stochastic process in which a future condition
    is decided solely on the present value and is not related to the past condition.
    HMM is used to predict which state the observation comes from when only one observation
    is visible.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: HMM是一种假设数据遵循**马尔可夫过程**的无监督训练方法。马尔可夫过程是一种随机过程，其中未来的状态仅由当前值决定，而与过去的状态无关。HMM用于预测当仅能观察到一个状态时，观察结果来自哪个状态。
- en: The previous explanation alone may not help you fully understand how HMM works,
    so let's look at an example. HMM is often used to analyze a base sequence. You
    may know that a base sequence consists of four nucleotides, for example, A, T,
    G, C, and the sequence is actually a string of these nucleotides. You won't get
    anything just by looking through the string, but you do have to analyze which
    part is related to which gene. Let's say that if any base sequence is lined up
    randomly, then each of the four characters should be output by one-quarter when
    you cut out any part of the base sequence.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 仅凭前面的解释可能无法帮助你完全理解HMM的工作原理，所以我们来看看一个例子。HMM常用于分析一个基本序列。你可能知道，基本序列由四种核苷酸组成，例如A、T、G、C，实际上这个序列就是由这些核苷酸组成的字符串。仅仅查看这个字符串你什么也得不到，但你确实需要分析哪个部分与哪个基因相关。假设如果任何基本序列是随机排列的，那么当你切割基本序列的任何部分时，每个字符的出现频率应该是四分之一。
- en: However, if there is a regularity, for example, where C tends to come next to
    G or the combination of ATT shows up frequently, then the probability of each
    character being output would vary accordingly. This regularity is the probability
    model and if the probability of being output relies only on an immediately preceding
    base, you can find out genetic information (= state) from a base sequence (= observation)
    using HMM.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果存在规律性，例如C通常紧跟在G后面，或者ATT组合经常出现，那么每个字符输出的概率将相应地变化。这个规律性就是概率模型，如果输出的概率仅依赖于紧接前面的基础，你可以通过HMM从基础序列（=观察）中找出遗传信息（=状态）。
- en: Other than these bioinformatic fields, HMM is often used in fields where time
    sequence patterns, such as syntax analysis of **natural language processing**
    (**NLP**) or sound signal processing, are needed. We don't explore HMM deeper
    here because its algorithm is less related to deep learning, but you can reference
    a very famous book, *Foundations of statistical natural language processing*,
    from MIT Press if you are interested.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些生物信息学领域，HMM还常用于需要时间序列模式的领域，比如**自然语言处理**（**NLP**）的句法分析或声音信号处理。我们在这里不深入探讨HMM，因为它的算法与深度学习关系较少，但如果你感兴趣，可以参考MIT出版社的经典书籍，*统计自然语言处理基础*。
- en: Neural networks
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'Neural networks are a little different to the machine learning algorithms.
    While other methods of machine learning take an approach based on probability
    or statistics, neural networks are algorithms that imitate the structure of a
    human brain. A human brain is made of a neuron network. Take a look at the following
    figure to get an idea of this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络与机器学习算法稍有不同。虽然其他机器学习方法基于概率或统计学的方法，神经网络是模仿人脑结构的算法。人脑由神经元网络构成。看看下面的图，了解一下这种结构：
- en: '![Neural networks](img/B04779_02_50.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络](img/B04779_02_50.jpg)'
- en: One neuron is linked to the network through another neuron and takes electrical
    stimulation from the synapse. When that electricity goes above the threshold,
    it gets ignited and transmits the electrical stimulation to the next neuron linked
    to the network. Neural networks distinguish things based on how electrical stimulations
    are transmitted.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一个神经元通过另一个神经元与网络连接，并从突触接受电刺激。当电流超过阈值时，它会被点燃并将电刺激传递给下一个与网络连接的神经元。神经网络根据电刺激的传递方式来区分事物。
- en: Neural networks have originally been the type of supervised learning that represents
    this electrical stimulation with numbers. Recently, especially with deep learning,
    various types of neural networks algorithms have been introduced, and some of
    them are unsupervised learning. The algorithm increases the predictability by
    adjusting the weight of the networks through the process of learning. Deep learning
    is an algorithm based on neural networks. More details on neural networks will
    be explained later, with implementations.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络最初是通过数字表示这种电刺激的有监督学习类型。最近，尤其是深度学习的出现，介绍了各种类型的神经网络算法，其中一些是无监督学习。该算法通过学习过程调整网络的权重，从而提高预测能力。深度学习是一种基于神经网络的算法。更多关于神经网络的细节将在后续通过实现进行讲解。
- en: Logistic regression
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic regression is one of the statistical regression models of variables
    with the Bernoulli distribution. While SVM and neural networks are classification
    models, logistic regression is a regression model, yet it certainly is one of
    the supervised learning methods. Although logistic regression has a different
    base of thinking, as a matter of fact, it can be thought of as one of the neural
    networks when you look at its formula. Details on logistic regression will also
    be explained with implementations later.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是具有伯努利分布的变量的统计回归模型。虽然SVM和神经网络是分类模型，逻辑回归是回归模型，但它确实是有监督学习方法之一。尽管逻辑回归有不同的思维基础，实际上，当你查看其公式时，可以将其视为神经网络的一种。逻辑回归的详细内容将在后续通过实现进行讲解。
- en: As you can see, each machine learning method has unique features. It's important
    to choose the right algorithm based on what you would like to know or what you
    would would like to use the data for. You can say the same of deep learning. Deep
    learning has different methods, so not only should you consider which the best
    method among them is, but you should also consider that there are some cases where
    you should not use deep learning. It's important to choose the best method for
    each case.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，每种机器学习方法都有独特的特性。根据你希望了解什么，或者希望用数据做什么，选择正确的算法非常重要。深度学习也是如此。深度学习有不同的方法，因此不仅要考虑哪种方法最优，还要考虑在某些情况下不应使用深度学习。选择最适合每种情况的方法非常重要。
- en: Reinforcement learning
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习
- en: 'Just for your reference, there is another method of machine learning called
    **reinforcement learning**. While some categorize reinforcement learning as unsupervised
    learning, others declare that all three learning algorithms, supervised learning,
    unsupervised learning, and reinforcement learning, should be divided into different
    types of algorithms, respectively. The following image shows the basic framework
    of reinforcement learning:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 仅供参考，还有一种叫做**强化学习**的机器学习方法。尽管有些人将强化学习归类为无监督学习，但也有人认为，监督学习、无监督学习和强化学习这三种学习算法应该被分别划分为不同的算法类型。以下图示展示了强化学习的基本框架：
- en: '![Reinforcement learning](img/B04779_02_49.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习](img/B04779_02_49.jpg)'
- en: An agent takes an action based on the state of an environment and an environment
    will change based on the action. A mechanism with some sort of reward is provided
    to an agent following the change of an environment and the agent learns a better
    choice of act (decision-making).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 代理根据环境的状态采取行动，环境将根据行动发生变化。环境变化后，向代理提供某种奖励机制，代理学习更好的行动选择（决策）。
- en: Machine learning application flow
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习应用流程
- en: 'We have looked at the methods that machine learning has and how these methods
    recognize patterns. In this section, we''ll see which flow is taken, or has to
    be taken, by data mining using machine learning. A decision boundary is set based
    on the model parameters in each of the machine learning methods, but we can''t
    say that adjusting the model parameters is the only thing we have to care about.
    There is another troublesome problem, and it is actually the weakest point of
    machine learning: feature engineering. Deciding which features are to be created
    from raw data, that is, the analysis subject, is a necessary step in making an
    appropriate classifier. And doing this, which is the same as adjusting the model
    parameters, also requires a massive amount of trial and error. In some cases,
    feature engineering requires far more effort than deciding a parameter.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了机器学习的方法及这些方法如何识别模式。在本节中，我们将看到数据挖掘在使用机器学习时采取的流程，或者必须采取的流程。每种机器学习方法都会基于模型参数设置决策边界，但我们不能说调整模型参数是我们唯一需要关注的事情。还有另一个棘手的问题，它实际上是机器学习的弱点：特征工程。决定从原始数据中创建哪些特征，也就是分析对象，是构建适当分类器的必要步骤。而这一过程，和调整模型参数一样，也需要大量的反复试验。在某些情况下，特征工程所需的努力远超过决定一个参数。
- en: 'Thus, when we simply say "machine learning," there are certain tasks that need
    to be completed in advance as preprocessing to build an appropriate classifier
    to deal with actual problems. Generally speaking, these tasks can be summarized
    as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们简单地说“机器学习”时，实际上有一些任务需要提前完成，作为预处理，以构建一个适当的分类器来处理实际问题。一般来说，这些任务可以总结如下：
- en: Deciding which machine learning method is suitable for a problem
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定哪种机器学习方法适用于某个问题
- en: Deciding what features should be used
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定应该使用哪些特征
- en: Deciding which setting is used for model parameters
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定使用哪个模型参数设置
- en: Only when these tasks are completed does machine learning become valuable as
    an application.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 只有完成这些任务，机器学习才能作为一个应用变得有价值。
- en: 'So, how do you decide the suitable features and parameters? How do you get
    a machine to learn? Let''s first take a look at the following diagram as it might
    be easier for you to grasp the whole picture of machine learning. This is the
    summary of a learning flow:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何决定合适的特征和参数呢？如何让机器学习呢？我们先来看一下下面的图示，它可能会帮助你更容易理解机器学习的整体流程。这是学习流程的总结：
- en: '![Machine learning application flow](img/B04779_02_48.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习应用流程](img/B04779_02_48.jpg)'
- en: 'As you can see from the preceding image, the learning phase of machine learning
    can be roughly divided into these two steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，机器学习的学习阶段大致可以分为以下两个步骤：
- en: Training
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练
- en: Testing
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试
- en: Literally, model parameters are renewed and adjusted in the training phase and
    the machine examines the merit of a model in the test phase. We have no doubt
    that the research or experiment will hardly ever succeed with just one training
    and one test set. We need to repeat the process of training → test, training →
    test … until we get the right model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从字面上看，模型参数在训练阶段被更新和调整，机器在测试阶段检查模型的优点。我们毫不怀疑，仅凭一次训练和一次测试集，研究或实验几乎无法成功。我们需要反复进行训练
    → 测试，训练 → 测试……直到得到合适的模型。
- en: 'Let''s consider the preceding flowchart in order. First, you need to divide
    the raw data into two: a training dataset and a test dataset. What you need to
    be very careful of here is that the training data and the test data are separated.
    Let''s take an example so you can easily imagine what this means: you are trying
    to predict the daily price of S&P 500 using machine learning with historical price
    data. (In fact, predicting the prices of financial instruments using machine learning
    is one of the most active research fields.)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按顺序考虑前面的流程图。首先，你需要将原始数据分为两部分：训练数据集和测试数据集。在这里你需要特别注意的是，训练数据和测试数据必须是分开的。我们通过一个例子来帮助你更容易理解这是什么意思：你正在使用历史价格数据，通过机器学习来预测S&P
    500的每日价格。（事实上，使用机器学习预测金融工具的价格是一个最活跃的研究领域之一。）
- en: 'Given that you have historical stock price data from 2001 to 2015 as raw data,
    what would happen if you performed the training with all the data from 2001 to
    2015 and similarly performed the test for the same period? The situation would
    occur that even if you used simple machine learning or feature engineering, the
    probability of getting the right prediction would be 70%, or even higher at 80%
    or 90%. Then, you might think: *What a great discovery! The market is actually
    that simple! Now I can be a billionaire!*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你拥有从2001年到2015年的历史股票价格数据作为原始数据，那么如果你使用2001到2015年的所有数据进行训练，并且同样在相同的时间段进行测试，会发生什么情况呢？这种情况下，即使你使用的是简单的机器学习或特征工程，得到正确预测的概率可能高达70%，甚至更高，达到80%或90%。然后，你可能会想：“*这真是个大发现！市场其实就是这么简单！现在我可以成为亿万富翁了！*”
- en: But this would end as short-lived elation. The reality doesn't go that well.
    If you actually start investment management with that model, you wouldn't get
    the performance you were expecting and would be confused. This is obvious if you
    think about it and pay a little attention. If a training dataset and a test dataset
    are the same, you do the test with the data for which you already know the answer.
    Therefore, it is a natural consequence to get high precision, as you have predicted
    a correct answer using a correct answer. But this doesn't make any sense for a
    test. If you would like to evaluate the model properly, be sure to use data with
    different time periods, for example, you should use the data from 2001 to 2010
    for the training dataset and 2011 to 2015 for the test. In this case, you perform
    the test using the data you don't know the answer for, so you can get a proper
    prediction precision rate. Now you can avoid going on your way to bankruptcy,
    believing in investments that will never go well.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 但这最终只是短暂的欣喜。现实情况并没有那么顺利。如果你真的用这个模型开始进行投资管理，你就不会得到你预期的表现，甚至会感到困惑。如果你稍微想一下并留心观察，这一点就显而易见了。如果训练数据集和测试数据集相同，你是在用你已经知道答案的数据进行测试。因此，得到高精度是自然而然的结果，因为你是用正确的答案去预测正确的答案。但这对于测试来说毫无意义。如果你想正确评估模型，务必使用不同时间段的数据，例如，训练数据集可以使用2001到2010年的数据，而测试数据集使用2011到2015年的数据。在这种情况下，你是用你不知道答案的数据来进行测试，因此你可以得到一个合适的预测精度。这样，你就能避免在投资中走向破产，避免相信那些永远不会成功的投资。
- en: So, it is obvious that you should separate a training dataset and a test dataset
    but you may not think this is a big problem. However, in the actual scenes of
    data mining, the case often occurs that we conduct an experiment with the same
    data without such awareness, so please be extra careful. We've talked about this
    in the case of machine learning, but it also applies to deep learning.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，显然你应该分开训练数据集和测试数据集，但你可能不会认为这是什么大问题。然而，在实际的数据挖掘场景中，往往会发生我们在没有意识到的情况下，使用相同的数据进行实验的情况，所以请特别小心。我们在机器学习的情况下讨论过这个问题，但它同样适用于深度学习。
- en: If you divide a whole dataset into two datasets, the first dataset to be used
    is the training dataset. To get a better precision rate, we first need to think
    about creating features in the training dataset. This feature engineering partly
    depends on human experience or intuition. It might take a long time and a lot
    of effort before you can choose the features to get the best results. Also, each
    machine learning method has different types of data formats of features to be
    accepted because the theory of models and formulas are unique to each method.
    As an example, we have a model that can only take an integer, a model that can
    only take a non-negative number/value, and a model that can only take real numbers
    from 0 to 1\. Let's look back at the previous example of stock prices. Since the
    value of the price varies a lot within a broader range, it may be difficult to
    make a prediction with a model that can only take an integer.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将一个完整的数据集分成两个数据集，第一个用于训练的数据集称为训练数据集。为了获得更好的精度，我们首先需要考虑在训练数据集中创建特征。这种特征工程在一定程度上依赖于人的经验或直觉。在选择能够获得最佳结果的特征之前，可能需要花费大量的时间和精力。此外，每种机器学习方法接受的特征数据格式不同，因为每种方法的模型理论和公式都是独特的。举个例子，我们有一个只能接受整数的模型，一个只能接受非负数值的模型，以及一个只能接受从0到1之间的实数的模型。让我们回顾一下前面提到的股票价格的例子。由于价格的值在较宽的范围内波动较大，用一个只能接受整数的模型来进行预测可能会很困难。
- en: Additionally, we have to be careful to ensure that there is compatibility between
    the data and the model. We don't say we can't use a model that can take all the
    real numbers from 0 if you would like to use a stock price as is for features.
    For example, if you divide all the stock price data by the maximum value during
    a certain period, the data range can fit into 0-1, hence you can use a model that
    can only take real numbers from 0 to 1\. As such, there is a possibility that
    you can apply a model if you slightly change the data format. You need to keep
    this point in mind when you think about feature engineering. Once you create features
    and decide which method of machine learning to apply, then you just need to examine
    it.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要小心确保数据与模型之间的兼容性。我们并不是说不能使用一个可以接受所有0到1之间实数的模型，如果你想将股票价格本身作为特征。例如，如果你将某一时期的所有股票价格数据除以最大值，数据范围就可以适应0到1之间，因此你可以使用一个只能接受0到1之间实数的模型。因此，通过稍微改变数据格式，可能就可以应用某个模型。在进行特征工程时，你需要牢记这一点。一旦你创建了特征并决定了应用哪种机器学习方法，你只需要进行检验。
- en: In machine learning, features are, of course, important variables when deciding
    on the precision of a model; however, a model itself, in other words a formula
    within the algorithm, also has parameters. Adjusting the speed of learning or
    adjusting how many errors to be allowed are good examples of this. The faster
    the learning speed, the less time it takes to finish the calculation, hence it's
    better to be fast. However, making the learning speed faster means that it only
    provides solutions in brief. So, we should be careful not to lose our expected
    precision rates. Adjusting the permissible range of errors is effective for the
    case where a noise is blended in the data. The standard by which a machine judges
    "is this data weird?" is decided by humans.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，特征当然是决定模型精度的重要变量；然而，模型本身，换句话说，算法中的公式，也有参数。调整学习速度或调整允许的误差范围就是这种参数的典型例子。学习速度越快，计算完成所需的时间越短，因此快速的学习速度通常更好。然而，提高学习速度意味着它只提供简短的解决方案。因此，我们应该小心不要失去预期的精度。调整允许误差的范围对于数据中掺杂噪声的情况是有效的。机器判断“这个数据怪不怪？”的标准是由人类决定的。
- en: Each method, of course, has a set of peculiar parameters. As for neural networks,
    how many neurons there should be in one of the parameters is a good example. Also,
    when we think of the kernel trick in SVM, how we set the kernel function is also
    one of the parameters to be determined. As you can see, there are so many parameters
    that machine learning needs to define, and which parameter is best cannot be found
    out in advance. In terms of how we define model parameters in advance, there is
    a research field that focuses on the study of parameters.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 每种方法当然都有一套特有的参数。例如，对于神经网络，参数之一就是应该有多少个神经元。当我们考虑到支持向量机（SVM）中的核技巧时，如何设置核函数也是需要确定的参数之一。如你所见，机器学习需要定义的参数非常多，哪个参数最好是无法提前得出的。关于如何提前定义模型参数，有一个研究领域专门研究这些参数。
- en: Therefore, we need to test many combinations of parameters to examine which
    combination can return the best precision. Since it takes a lot of time to test
    each combination one by one, the standard flow is to test multiple models with
    different parameter combinations in concurrent processing and then compare them.
    It is usually the case that a range of parameters that should be set to some extent
    is decided, so it's not that the problem can't be solved within a realistic time
    frame.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要测试许多参数组合，以确定哪种组合能够返回最佳的精度。由于逐个测试每种组合需要大量时间，标准流程是在并发处理中测试多个具有不同参数组合的模型，然后进行比较。通常情况下，应该设置一定范围的参数已经确定，所以问题并非不能在现实时间范围内解决。
- en: When the model that can get good precision is ready in the training dataset,
    next comes the test step. The rough flow of the test is to apply the same feature
    engineering applied to the training dataset and the same model parameters respectively
    and then verify the precision. There isn't a particularly difficult step in the
    test. The calculation doesn't take time either. It's because finding a pattern
    from data, in other words optimizing a parameter in a formula, creates a calculation
    cost. However, once a parameter adjustment is done, then the calculation is made
    right away as it only applies the formula to new datasets. The reason for performing
    a test is, simply put, to examine whether a model is too optimized by the training
    dataset. What does this mean? Well, in machine learning, there are two patterns
    where a training set goes well but a test set doesn't.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当能够在训练数据集中获得良好精度的模型准备好时，接下来是测试步骤。测试的大致流程是分别应用到训练数据集和相同模型参数上的相同特征工程，然后验证精度。测试中没有特别困难的步骤。计算时间也不长。这是因为从数据中找出模式，换句话说是优化公式中的参数，会产生计算成本。但是，一旦参数调整完成，计算就会立即进行，因为它只是将公式应用到新数据集上。进行测试的原因很简单，就是检查模型是否对训练数据集进行了过度优化。这是什么意思？嗯，在机器学习中，有两种模式，一种是训练集表现良好但测试集表现不佳。
- en: The first case is incorrect optimization by classifying noisy data blended into
    a training dataset. This can be related to the adjustment of a permissible range
    of errors mentioned earlier in this chapter. Data in the world is not usually
    clean. It can be said that there is almost no data that can be properly classified
    into clean patterns. The prediction of stock prices is a good example again. Stock
    prices usually repeat moderate fluctuations from previous stock prices, but sometimes
    they suddenly surge or drop sharply. And, there is, or should be, no regularity
    in this irregular movement. Another case is if you would like to predict the yield
    of a crop for a country; the data of the year affected by abnormal weather should
    be largely different from the normal years' data. These examples are extreme and
    easy to understand, but most for a data in the real world also contains noises,
    making it difficult to classify data into proper patterns. If you just do training
    without adjusting the parameters of machine learning, the model forces it to classify
    the noise data into a pattern. In this case, data from the training dataset might
    be classified correctly, but since noise data in the training dataset is also
    classified and the noise doesn't exist in the test dataset, the predictability
    in a test should be low.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种情况是通过将噪声数据混合到训练数据集中而导致的错误优化。这可以与本章早些时候提到的误差容许范围调整有关。世界上的数据通常不干净。可以说，几乎没有数据可以被正确分类为清晰的模式。再举一个股票价格预测的例子。股票价格通常会从先前的股票价格中适度波动，但有时会突然激增或急剧下降。而且，这种不规律的运动中没有或者不应该有规律。另一种情况是，如果你想预测一个国家的作物产量；受异常天气影响的年份数据应该与正常年份的数据大不相同。这些例子是极端的且易于理解，但实际世界中的大多数数据也包含噪声，这使得将数据分类到适当的模式中变得困难。如果只是训练而不调整机器学习的参数，模型会强制将噪声数据分类为一种模式。在这种情况下，训练数据集中的数据可能被正确分类，但由于训练数据集中的噪声数据也被分类了，而测试数据集中不存在噪声，因此测试中的可预测性可能较低。
- en: The second case is incorrect optimizing by classifying data that is characteristic
    only in a training dataset. For example, let's think about making an app of English
    voice inputs. To build your app, you should prepare the data of pronunciation
    for various words as a training dataset. Now, let's assume you prepared enough
    voice data of British English native speakers and were able to create a high precision
    model that could correctly classify the pronunciation in the training dataset.
    The next step is a test. Since it's a test, let's use the voice data of American
    English native speakers for the means of providing different data. What would
    be the result then? You probably wouldn't get good precision. Furthermore, if
    you try the app to recognize the pronunciation of non-native speakers of English,
    the precision would be much lower. As you know, English has different pronunciations
    for different areas. If you don't take this into consideration and optimize the
    model with the training data set of British English, even though you may get a
    good result in the training set, you won't get a good result in the test set and
    it won't be useful for the actual application.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种情况是通过对仅在训练数据集中具有特征的数据进行分类来进行错误优化。例如，假设我们要制作一个英文语音输入的应用程序。为了构建这个应用程序，你应该准备一个关于不同单词发音的语音数据集作为训练数据集。现在，假设你准备了足够的英国英语母语者的语音数据，并且能够创建一个高精度的模型，能够正确地分类训练数据集中的发音。下一步是测试。既然是测试，我们就使用美国英语母语者的语音数据，提供不同的数据。那么结果会怎样呢？你可能不会得到好的精度。此外，如果你尝试让这个应用程序识别非英语母语者的发音，精度将会更低。正如你所知道的，英语在不同地区有不同的发音。如果你没有考虑到这一点，且只用英国英语的训练数据集进行模型优化，即使你在训练集上取得了好结果，在测试集上也无法得到好的结果，甚至在实际应用中也没有什么用处。
- en: These two problems occur because the machine learning model learns from a training
    dataset and fits into the dataset too much. This problem is literally called the
    **overfitting problem**, and you should be very careful to avoid it. The difficulty
    of machine learning is that you have to think about how to avoid this overfitting
    problem besides the feature engineering. These two problems, overfitting and feature
    engineering, are partially related because poor feature engineering would fail
    into overfitting.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个问题发生的原因是机器学习模型从训练数据集中学习并且对数据集的拟合过度。这个问题被称为**过拟合问题**，你应该非常小心避免它。机器学习的难点在于，除了特征工程外，你还需要思考如何避免这个过拟合问题。这两个问题——过拟合和特征工程——是部分相关的，因为糟糕的特征工程可能会导致过拟合。
- en: To avoid the problem of overfitting, there's not much to do except increase
    the amount of data or the number of tests. Generally, the amount of data is limited,
    so the methods of increasing the number of tests are often performed. The typical
    example is **K-fold cross-validation**. In K-fold cross-validation, all the data
    is divided into K sets at the beginning. Then, one of the datasets is picked as
    a test dataset and the rest, K-1, are put as training datasets. Cross-validation
    performs the verification on each dataset divided into K for K times, and the
    precision is measured by calculating the average of these K results. The most
    worrying thing is that both a training dataset and a test dataset may happen to
    have good precision by chance; however, the probability of this accident can be
    decreased in K-fold cross-validation as it performs a test several times. You
    can never worry too much about overfitting, so it's necessary that you verify
    results carefully.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免过拟合问题，除了增加数据量或增加测试次数外，几乎没有其他方法。一般来说，数据量是有限的，因此增加测试次数的方法经常被采用。一个典型的例子是**K折交叉验证**。在K折交叉验证中，所有数据在开始时被分成K个数据集。然后，选取其中一个数据集作为测试数据集，其余K-1个数据集作为训练数据集。交叉验证在每个分割成K的数据集上进行验证，并通过计算这K次结果的平均值来衡量精度。最令人担忧的是，训练数据集和测试数据集可能会偶然地都取得很好的精度；然而，通过K折交叉验证多次进行测试，可以减少这种偶然事件的概率。你永远不能对过拟合过于担心，因此必须仔细验证结果。
- en: Well, you have now read through the flow of training and test sets and learned
    key points to be kept in mind. These two mainly focus on data analysis. So, for
    example, if your purpose is to pull out the meaningful information from the data
    you have and make good use of it, then you can go through this flow. On the other
    hand, if you need an application that can cope with a further new model, you need
    an additional process to make predictions with a model parameter obtained in a
    training and a test set. As an example, if you would like to find out some information
    from a dataset of stock prices and analyze and write a market report, the next
    step would be to perform training and test sets. Or, if you would like to predict
    future stock prices based on the data and utilize it as an investment system,
    then your purpose would be to build an application using a model obtained in a
    training and a test set and to predict a price based on the data you can get anew
    every day, or from every period you set. In the second case, if you would like
    to renew the model with the data that is newly added, you need to be careful to
    complete the calculation of the model building by the time the next model arrives.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，您现在已经阅读了训练集和测试集的流程，并了解了需要记住的关键点。这两者主要集中在数据分析上。例如，如果您的目的是从现有的数据中提取有意义的信息并加以利用，那么您可以遵循这个流程。另一方面，如果您需要一个能够应对新模型的应用程序，那么您需要一个额外的过程，通过训练集和测试集得到的模型参数来进行预测。例如，如果您想从股价数据集中提取一些信息并撰写市场报告，那么下一步就是执行训练集和测试集。或者，如果您想基于数据预测未来的股价，并将其作为投资系统来使用，那么您的目标将是使用训练集和测试集获得的模型构建一个应用程序，并根据您每天或在您设定的每个周期内可以获得的数据来预测价格。在第二种情况下，如果您想使用新增的数据更新模型，那么您需要小心，在下一个模型到达之前完成模型构建的计算。
- en: Theories and algorithms of neural networks
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的理论与算法
- en: In the previous section, you saw the general flow of when we perform data analysis
    with machine learning. In this section, theories and algorithms of neural networks,
    one of the methods of machine learning, are introduced as a preparation toward
    deep learning.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，您看到了我们使用机器学习进行数据分析时的一般流程。本节将介绍神经网络的理论与算法，神经网络是机器学习的一种方法，为深度学习做准备。
- en: 'Although we simply say "neural networks", their history is long. The first
    published algorithm of neural networks was called **perceptron**, and the paper
    released in 1957 by Frank Rosenblatt was named *The Perceptron: A Perceiving and
    Recognizing Automaton (Project Para)*. From then on, many methods were researched,
    developed, and released, and now neural networks are one of the elements of deep
    learning. Although we simply say "neural networks," there are various types and
    we''ll look at the representative methods in order now.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们简单地说“神经网络”，但它们的历史悠久。神经网络的第一个公开算法叫做**感知机**，由弗兰克·罗森布拉特（Frank Rosenblatt）于1957年发布的论文《感知机：一种感知和识别自动机（Project
    Para）》中提出。从那时起，许多方法被研究、开发并发布，现在神经网络已成为深度学习的一个重要组成部分。虽然我们简单地说“神经网络”，但它有多种类型，我们现在将介绍一些代表性的方法。
- en: Perceptrons (single-layer neural networks)
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知机（单层神经网络）
- en: The perceptron algorithm is the model that has the simplest structure in the
    algorithms of neural networks and it can perform linear classification for two
    classes. We can say that it's the prototype of neural networks. It is the algorithm
    that models human neurons in the simplest way.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机算法是神经网络算法中结构最简单的模型，它可以进行二分类的线性分类。我们可以说它是神经网络的原型。它是以最简单的方式模拟人类神经元的算法。
- en: 'The following figure is a schematic drawing of the general model:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图为一般模型的示意图：
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_51.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![感知机（单层神经网络）](img/B04779_02_51.jpg)'
- en: Here, ![Perceptrons (single-layer neural networks)](img/B04779_02_61.jpg) shows
    the input signal, ![Perceptrons (single-layer neural networks)](img/B04779_02_62.jpg)
    shows the weight corresponding to each input signal, and ![Perceptrons (single-layer
    neural networks)](img/B04779_02_63.jpg) shows the output signal. ![Perceptrons
    (single-layer neural networks)](img/B04779_02_64.jpg) is the activation function.
    ![Perceptrons (single-layer neural networks)](img/B04779_02_65.jpg) shows, literally,
    the meaning of calculating the sum of data coming from the input. Please bear
    in mind that ![Perceptrons (single-layer neural networks)](img/B04779_02_61.jpg)
    applies a processing of nonlinear conversion with feature engineering in advance,
    that is, ![Perceptrons (single-layer neural networks)](img/B04779_02_61.jpg)is
    an engineered feature.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![感知机（单层神经网络）](img/B04779_02_61.jpg) 显示输入信号，![感知机（单层神经网络）](img/B04779_02_62.jpg)
    显示与每个输入信号对应的权重，![感知机（单层神经网络）](img/B04779_02_63.jpg) 显示输出信号。![感知机（单层神经网络）](img/B04779_02_64.jpg)
    是激活函数。![感知机（单层神经网络）](img/B04779_02_65.jpg) 显示，字面意思是计算来自输入的数据和。请记住，![感知机（单层神经网络）](img/B04779_02_61.jpg)
    在前期应用了特征工程的非线性转换处理，即，![感知机（单层神经网络）](img/B04779_02_61.jpg) 是一个工程化的特征。
- en: 'Then, the output of perceptron can be represented as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，感知机的输出可以表示如下：
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_01.jpg)![Perceptrons
    (single-layer neural networks)](img/B04779_02_02.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![感知机（单层神经网络）](img/B04779_02_01.jpg)![感知机（单层神经网络）](img/B04779_02_02.jpg)'
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_66.jpg) is called
    the step function. As shown in the equation, Perceptron returns the output by
    multiplying each factor of the feature vector by weight, calculating the sum of
    them, and then activating the sum with the step function. The output is the result
    estimated by Perceptron. During the training, you will compare this result with
    the correct data and feed back the error.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![感知机（单层神经网络）](img/B04779_02_66.jpg) 被称为阶跃函数。如方程所示，感知机通过将特征向量的每个因子乘以权重，计算它们的和，然后使用阶跃函数激活该和，从而返回输出。输出是感知机估算的结果。在训练过程中，您将把该结果与正确的数据进行比较，并反馈误差。'
- en: 'Let ![Perceptrons (single-layer neural networks)](img/B04779_02_67.jpg) be
    the value of the labeled data. Then, the formula can be represented as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 令 ![感知机（单层神经网络）](img/B04779_02_67.jpg) 为标记数据的值。那么，该公式可以表示如下：
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_03.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![感知机（单层神经网络）](img/B04779_02_03.jpg)'
- en: 'If some labeled data belongs to class 1, ![Perceptrons (single-layer neural
    networks)](img/B04779_02_68.jpg), we have ![Perceptrons (single-layer neural networks)](img/B04779_02_69.jpg).
    If it belongs to class 2, ![Perceptrons (single-layer neural networks)](img/B04779_02_70.jpg),
    we have ![Perceptrons (single-layer neural networks)](img/B04779_02_71.jpg). Also,
    if the input data is classified correctly, we get:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一些标记数据属于类别 1，![感知机（单层神经网络）](img/B04779_02_68.jpg)，我们有 ![感知机（单层神经网络）](img/B04779_02_69.jpg)。如果它属于类别
    2，![感知机（单层神经网络）](img/B04779_02_70.jpg)，我们有 ![感知机（单层神经网络）](img/B04779_02_71.jpg)。另外，如果输入数据被正确分类，我们得到：
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_04.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![感知机（单层神经网络）](img/B04779_02_04.jpg)'
- en: 'So, putting these equations together, we have the following equation of properly
    classified data:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将这些方程式组合在一起，我们得到以下正确分类数据的方程：
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_05.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![感知机（单层神经网络）](img/B04779_02_05.jpg)'
- en: 'Therefore, you can increase the predictability of Perceptron by minimizing
    the following function:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您可以通过最小化以下函数来提高感知机的可预测性：
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_06.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![感知机（单层神经网络）](img/B04779_02_06.jpg)'
- en: 'Here, ![Perceptrons (single-layer neural networks)](img/B04779_02_72.jpg) is
    called the error function. ![Perceptrons (single-layer neural networks)](img/B04779_02_73.jpg)
    shows the set of misclassification. To minimize the error function, gradient descent,
    or steepest descent, an optimization algorithm is used to find a local minimum
    of a function using gradient descent. The equation can be described as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![感知机（单层神经网络）](img/B04779_02_72.jpg) 被称为误差函数。![感知机（单层神经网络）](img/B04779_02_73.jpg)
    显示了误分类的集合。为了最小化误差函数，使用梯度下降或最速下降，这是一种优化算法，用于通过梯度下降寻找函数的局部最小值。该方程可以描述如下：
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_07.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![Perceptrons (single-layer neural networks)](img/B04779_02_07.jpg)'
- en: Here, ![Perceptrons (single-layer neural networks)](img/B04779_02_74.jpg) is
    the learning rate, a common parameter of the optimization algorithm that adjusts
    the learning speed, and ![Perceptrons (single-layer neural networks)](img/B04779_02_75.jpg)
    shows the number of steps of the algorithm. In general, the smaller the value
    of the learning rate, the more probable it is that the algorithm falls into a
    local minimum because the model can't override the old value much. If the value
    is too big, however, the model parameters can't converge because the values fluctuate
    too widely. Therefore, practically, the learning rate is set to be big at the
    beginning and then dwindle with each iteration. On the other hand, with perceptrons,
    it is proved that the algorithm converges irrespective of the value of the learning
    rate when the data set is linearly separable, and thus the value is set to be
    1.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![Perceptrons (single-layer neural networks)](img/B04779_02_74.jpg) 是学习率，这是优化算法的常见参数，用于调整学习速度；而![Perceptrons
    (single-layer neural networks)](img/B04779_02_75.jpg) 显示了算法的步骤数。通常，学习率的值越小，算法越可能陷入局部最小值，因为模型不能对旧值进行太多覆盖。然而，如果值太大，模型参数无法收敛，因为值波动过大。因此，实际操作中，学习率通常设置得较大，随着每次迭代逐渐减小。另一方面，使用感知器时，当数据集是线性可分的，已证明算法会收敛，无论学习率的值如何，因此该值被设置为1。
- en: 'Now, let''s look at an implementation. The package structure is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一个实现。包结构如下：
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_52.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![Perceptrons (single-layer neural networks)](img/B04779_02_52.jpg)'
- en: Let's have a look at the content of `Perceptrons.java` as shown in the previous
    image. We will look into the main methods one by one.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看前面图片中展示的`Perceptrons.java`的内容。我们将逐一查看主要方法。
- en: 'First, we define the parameters and constants that are needed for learning.
    As explained earlier, the learning rate (defined as `learningRate` in the code)
    can be 1:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义学习所需的参数和常量。如前所述，学习率（在代码中定义为`learningRate`）可以设为1：
- en: '[PRE0]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Needless to say, machine learning and deep learning need a dataset to be learned
    and classified. Here, since we would like to focus on implementations deeply related
    to the theory of perceptrons, a sample dataset is generated within the source
    code and is used for the training and test sets, the class called `GaussianDistribution`
    is defined, and it returns a value following the normal distribution or Gaussian
    distribution. As for the source code itself, we don''t mention it here as you
    can see it in `GaussianDistribution.java`. We set the dimensions of the learning
    data in `nIn = 2` and define two types of instances as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 不用多说，机器学习和深度学习需要一个数据集来进行学习和分类。在这里，由于我们希望深入关注与感知器理论密切相关的实现，源代码中生成了一个示例数据集，并用于训练和测试集，定义了一个名为`GaussianDistribution`的类，它返回遵循正态分布或高斯分布的值。至于源代码本身，我们在这里不提及，因为你可以在`GaussianDistribution.java`中看到。我们将学习数据的维度设置为`nIn
    = 2`，并定义了如下的两种类型的实例：
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can get the values that follow the normal distributions with a mean of `-2.0`
    and a variance of `1.0` by `g1.random()` and a mean of `2.0` and a variance of
    `1.0` by `g2.random()`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过`g1.random()`得到均值为`-2.0`、方差为`1.0`的正态分布值，通过`g2.random()`得到均值为`2.0`、方差为`1.0`的正态分布值。
- en: With these values, 500 data attributes are generated in class 1 obtained by
    `[ g1.random(), g2.random() ]` and another 500 generated in class 2 obtained by
    `[ g2.random(), g1.random() ]`. Also, please bear in mind that each value of the
    class 1 label is `1` and of the class 2 label is `-1`. Almost all the data turns
    out to be a value around `[-2.0, 2.0]` for class 1 and `[2.0, -2.0]` for class
    2; hence, they can be linearly separated, but some data can be blended near the
    other class as noise.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些值，通过`[ g1.random(), g2.random() ]`生成了类1的500个数据属性，另有500个数据通过`[ g2.random(),
    g1.random() ]`生成在类2中。此外，请记住，类1标签的每个值为`1`，类2标签的每个值为`-1`。几乎所有的数据的值都位于`[-2.0, 2.0]`（类1）和`[2.0,
    -2.0]`（类2）附近；因此，它们可以被线性分开，但有些数据可能会因噪声而与另一类混合。
- en: 'Now we have prepared the data, we can move on to building the model. The number
    of units in the input layer, `nIn`, is the argument used here to decide the model
    outline:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了数据，接下来可以开始构建模型。输入层的单元数`nIn`是这里用来决定模型框架的参数：
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s look at the actual `Perceptrons` constructor. The parameter of the perceptrons
    model is only the weight, `w`, of the network—very simple —as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下实际的`Perceptrons`构造函数。感知机模型的参数只有网络的权重`w`——非常简单——如下所示：
- en: '[PRE3]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The next step is finally the training. The iteration of learning continues
    until it reaches enough numbers of the learning set in advance or classifies all
    the training data correctly:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步终于是训练。学习的迭代将继续，直到达到预先设定的学习集数量，或者正确分类所有的训练数据：
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the `train` method, you can write down the gradient descent algorithm as
    we just explained. Here, the `w` parameter of the network is updated:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在`train`方法中，你可以写下我们刚刚解释的梯度下降算法。在这里，网络的`w`参数被更新：
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once you have done enough numbers of learning and finish, the next step is
    to perform the test. First, let''s check which class the test data is classified
    by in the well-trained model:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成足够数量的学习并结束，下一步是进行测试。首先，让我们检查在经过良好训练的模型中，测试数据被分类到了哪个类别：
- en: '[PRE6]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the `predict` method, simply activate the input through the network. The
    step function used here is defined in `ActivationFunction.java`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在`predict`方法中，只需通过网络激活输入。这里使用的阶跃函数定义在`ActivationFunction.java`中：
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Subsequently, we evaluate the model using the test data. You might need more
    explanation to perform this part.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们使用测试数据评估该模型。你可能需要更多的解释来执行这一部分。
- en: 'Generally, the performance of the method of machine learning is measured by
    the indicator of accuracy, precision, and recall based on the confusion matrix.
    The confusion matrix summarizes the results of a comparison of the predicted class
    and the correct class in the matrix and is shown as the following table:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，机器学习方法的性能是通过基于混淆矩阵的准确率、精确度和召回率来衡量的。混淆矩阵总结了预测类别与真实类别的比较结果，并以以下表格的形式显示：
- en: '|   | p_predicted | n_predicted |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|   | p_predicted | n_predicted |'
- en: '| --- | --- | --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **p_actual** | True positive (TP) | False negative (FN) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| **p_actual** | 真阳性（TP） | 假阴性（FN） |'
- en: '| **n_actual** | False positive (FP) | True negative (TN) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| **n_actual** | 假阳性（FP） | 真阴性（TN） |'
- en: 'The three indicators are shown below:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个指标如下所示：
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_08.jpg)![Perceptrons
    (single-layer neural networks)](img/B04779_02_09.jpg)![Perceptrons (single-layer
    neural networks)](img/B04779_02_10.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![感知机（单层神经网络）](img/B04779_02_08.jpg)![感知机（单层神经网络）](img/B04779_02_09.jpg)![感知机（单层神经网络）](img/B04779_02_10.jpg)'
- en: 'Accuracy shows the proportion of the data that is correctly classified for
    all the data, while precision shows the proportion of the actual correct data
    to the data predicted as positive, and recall is the proportion of the data predicted
    as positive to the actual positive data. Here is the code for this:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率显示的是所有数据中被正确分类的数据比例，而精确度显示的是实际正确数据与预测为正的所有数据的比例，召回率则是预测为正的数据与实际正数据的比例。这里是对应的代码：
- en: '[PRE8]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: When you compile `Perceptron.java` and run it, you can get 99.0% for accuracy,
    98.0% for precision, and 100% for recall. This means that actual positive data
    is classified correctly but that there has been some data wrongly predicted as
    positive when it is actually negative. In this source code, since the data set
    is for demonstration, K-fold cross-validation is not included. The dataset in
    the example above is programmatically generated and has little noise data. Therefore,
    accuracy, precision, and recall are all high because the data can be well classified.
    However, as mentioned above, you have to look carefully at results, especially
    when you have great results.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当你编译并运行`Perceptron.java`时，你可以获得99.0%的准确率，98.0%的精确度和100%的召回率。这意味着实际的正数据被正确分类，但也有一些数据被错误预测为正类，而实际上是负类。在这个源代码中，由于数据集是用于演示的，所以没有包括K折交叉验证。上面示例中的数据集是程序生成的，并且噪声数据很少。因此，准确率、精确度和召回率都很高，因为数据可以很好地分类。然而，如上所述，当你获得很好的结果时，仍然需要仔细检查结果。
- en: Logistic regression
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic regression is, as you can assume from the name, the regression model.
    But when you look at the formula, you can see that logistic regression is the
    linear separation model that generalizes perceptrons.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归顾名思义是回归模型。但当你查看公式时，你会发现逻辑回归是一个线性分离模型，它是对感知机的概括。
- en: 'Logistic regression can be regarded as one of the neural networks. With perceptrons,
    the step function is used for the activation function, but in logistic regression,
    the (logistic) sigmoid function is used. The equation of the sigmoid function
    can be represented as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归可以看作是神经网络的一种形式。使用感知机时，激活函数是阶跃函数，而在逻辑回归中，使用的是（逻辑）sigmoid 函数。sigmoid 函数的方程可以表示如下：
- en: '![Logistic regression](img/B04779_02_11.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![Logistic regression](img/B04779_02_11.jpg)'
- en: 'The graph of this function can be illustrated as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的图形可以如下所示：
- en: '![Logistic regression](img/B04779_02_58.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![Logistic regression](img/B04779_02_58.jpg)'
- en: 'The `sigmoid` function maps any values of a real number to a value from 0 to
    1\. Therefore, the output of the logistic regression can be regarded as the posterior
    probability for each class. The equations can be described as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid` 函数将任意实数值映射到0到1之间的值。因此，逻辑回归的输出可以被视为每个类别的后验概率。方程可以描述如下：'
- en: '![Logistic regression](img/B04779_02_12.jpg)![Logistic regression](img/B04779_02_13.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![Logistic regression](img/B04779_02_12.jpg)![Logistic regression](img/B04779_02_13.jpg)'
- en: 'These equations can be combined to make:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方程可以结合起来表示：
- en: '![Logistic regression](img/B04779_02_14.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![Logistic regression](img/B04779_02_14.jpg)'
- en: Here, ![Logistic regression](img/B04779_02_15.jpg) is the correct data. You
    may have noticed that the range of the data is different from the one of perceptrons.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![Logistic regression](img/B04779_02_15.jpg) 是正确的数据。你可能注意到，这些数据的范围与感知机的范围不同。
- en: 'With the previous equation, the `likelihood` function, which estimates the
    maximum likelihood of the model parameters, can be expressed as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的方程，`likelihood` 函数（估计模型参数的最大似然）可以表示如下：
- en: '![Logistic regression](img/B04779_02_16.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![Logistic regression](img/B04779_02_16.jpg)'
- en: 'Where:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '![Logistic regression](img/B04779_02_17.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![Logistic regression](img/B04779_02_17.jpg)'
- en: As you can see, not only the weight of the network but the bias ![Logistic regression](img/B04779_02_76.jpg)
    are also parameters that need to be optimized.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，网络的权重以及偏置 ![Logistic regression](img/B04779_02_76.jpg) 也是需要优化的参数。
- en: 'What we need to do now is maximize the likelihood function, but the calculation
    is worrying because the function has a mathematical product. To make the calculation
    easier, we take the logarithm (log) of the likelihood function. Additionally,
    we substitute the sign to turn the object to minimizing the negative log `likelihood`
    function. Since the log is the monotonic increase, the magnitude correlation doesn''t
    change. The equation can be represented as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要做的是最大化似然函数，但由于该函数具有数学乘积，计算会令人担忧。为了简化计算，我们对似然函数取对数（log）。此外，我们替换符号，将目标转化为最小化负对数
    `likelihood` 函数。由于对数是单调递增的，幅度关系不会改变。方程可以表示如下：
- en: '![Logistic regression](img/B04779_02_18.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![Logistic regression](img/B04779_02_18.jpg)'
- en: You can see the error function at the same time. This type of function is called
    a cross-entropy error function.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以同时看到误差函数。这种类型的函数被称为交叉熵误差函数。
- en: 'Similar to perceptrons, we can optimize the model by computing the gradients
    of the model parameters, ![Logistic regression](img/B04779_02_57.jpg) and ![Logistic
    regression](img/B04779_02_76.jpg). The gradients can be described as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于感知机，我们可以通过计算模型参数的梯度来优化模型，![Logistic regression](img/B04779_02_57.jpg) 和 ![Logistic
    regression](img/B04779_02_76.jpg)。梯度可以描述如下：
- en: '![Logistic regression](img/B04779_02_19.jpg)![Logistic regression](img/B04779_02_20.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![Logistic regression](img/B04779_02_19.jpg)![Logistic regression](img/B04779_02_20.jpg)'
- en: 'With these equations, we can update the model parameters as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些方程，我们可以按如下方式更新模型参数：
- en: '![Logistic regression](img/B04779_02_21.jpg)![Logistic regression](img/B04779_02_22.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![Logistic regression](img/B04779_02_21.jpg)![Logistic regression](img/B04779_02_22.jpg)'
- en: Theoretically, we have no problem using the equations just mentioned and implementing
    them. As you can see, however, you have to calculate the sum of all the data to
    compute the gradients for each iteration. This will hugely increase the calculation
    cost once the size of a dataset becomes big.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论上讲，我们使用刚才提到的方程并实现它们没有问题。然而，正如你所看到的，你必须计算所有数据的总和，以便在每次迭代时计算梯度。当数据集的规模变大时，这将极大增加计算成本。
- en: Therefore, another method is usually applied that partially picks up some data
    from the dataset, computes the gradients by calculating the sum only with picked
    data, and renews the parameters. This is called **stochastic gradient descent**
    (**SGD**) because it stochastically chooses a subset of the data. This subset
    of the dataset used for one renewal is called a **mini-batch**.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通常会采用另一种方法，即部分从数据集中选取一些数据，仅通过计算选取数据的和来计算梯度，并更新参数。这种方法被称为**随机梯度下降**（**SGD**），因为它随机选择数据的一个子集。用于一次更新的数据子集称为**小批量**。
- en: Tip
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: SGD using a mini-batch is sometimes called **mini-batch stochastic gradient
    descent** (**MSGD**). Online training that learns to randomly choose one data
    from the dataset is called SGD to distinguish one from the other. In this book,
    however, both MSGD and SGD are called SGD, as both become the same when the size
    of the mini-batch is 1\. Since learning by each data does increase the calculation
    cost, it's better to use mini-batches.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用小批量的SGD有时被称为**小批量随机梯度下降**（**MSGD**）。从数据集中随机选择一个数据进行在线学习的方式被称为SGD，以区分这两者。然而，在本书中，MSGD和SGD都称为SGD，因为当小批量大小为1时，它们是相同的。由于每个数据的学习会增加计算成本，使用小批量学习更为优越。
- en: In terms of the implementation of logistic regression, since it can be covered
    with multi-class logistic regression introduced in the next section, we won't
    write the code here. You can refer to the code of multi-class logistic regression
    in this section.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归的实现中，由于它可以通过下一节介绍的多类逻辑回归来覆盖，我们在这里不写代码。你可以参考本节中的多类逻辑回归代码。
- en: Multi-class logistic regression
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多类逻辑回归
- en: 'Logistic regression can also be applied to multi-class classification. In two-class
    classification, the activation function is the sigmoid function, and you can classify
    the data by evaluating the output value shifting from 0 to 1\. How, then, can
    we classify data when the number of classes is K? Fortunately, it is not difficult.
    We can classify multi-class data by changing the equation for the output to the
    K-dimensional class-membership probability vector, and we use the `softmax` function
    to do so, which is the multivariate version of the sigmoid function. The posterior
    probability of each class can be represented as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归也可以应用于多类分类。在二类分类中，激活函数是sigmoid函数，你可以通过评估输出值从0到1来对数据进行分类。那么，当类的数量为K时，我们如何分类数据呢？幸运的是，这并不困难。我们可以通过将输出的方程更改为K维的类别成员概率向量来分类多类数据，我们使用`softmax`函数来做到这一点，它是sigmoid函数的多元版本。每个类别的后验概率可以表示如下：
- en: '![Multi-class logistic regression](img/B04779_02_23.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![多类逻辑回归](img/B04779_02_23.jpg)'
- en: 'With this, the same as two-class cases, you can get the likelihood function
    and the negative log likelihood function as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，与二类情况相同，你可以得到如下的似然函数和负对数似然函数：
- en: '![Multi-class logistic regression](img/B04779_02_24.jpg)![Multi-class logistic
    regression](img/B04779_02_25.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![多类逻辑回归](img/B04779_02_24.jpg)![多类逻辑回归](img/B04779_02_25.jpg)'
- en: Here, ![Multi-class logistic regression](img/B04779_02_26.jpg) , ![Multi-class
    logistic regression](img/B04779_02_27.jpg). Also, ![Multi-class logistic regression](img/B04779_02_28.jpg)
    is the Kth element of the correct data vector, ![Multi-class logistic regression](img/B04779_02_29.jpg),
    which corresponds to the *n* *th* training data. If an input data belongs to the
    class *k*, the value of ![Multi-class logistic regression](img/B04779_02_28.jpg)
    is 1; the value is 0 otherwise.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![多类逻辑回归](img/B04779_02_26.jpg) ，![多类逻辑回归](img/B04779_02_27.jpg)。此外，![多类逻辑回归](img/B04779_02_28.jpg)
    是正确数据向量的第K个元素，![多类逻辑回归](img/B04779_02_29.jpg)，它对应于 *n* *th* 训练数据。如果输入数据属于类 *k*，那么![多类逻辑回归](img/B04779_02_28.jpg)的值为1；否则，值为0。
- en: 'Gradients of the loss function against the model parameters, the weight vector,
    and the bias, can be described as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数相对于模型参数、权重向量和偏置的梯度可以描述如下：
- en: '![Multi-class logistic regression](img/B04779_02_30.jpg)![Multi-class logistic
    regression](img/B04779_02_31.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![多类逻辑回归](img/B04779_02_30.jpg)![多类逻辑回归](img/B04779_02_31.jpg)'
- en: 'Now let''s look through the source code to better understand the theory. You
    can see some variables related to mini-batches besides the ones necessary for
    the model:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过查看源代码来更好地理解理论。除了模型所需的变量外，你还可以看到一些与小批量（mini-batch）相关的变量：
- en: '[PRE9]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following code is the process to shuffle training data so the data of each
    mini-batch is to be applied randomly to SGD:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是将训练数据进行随机打乱的过程，这样每个小批量的数据就可以随机应用于SGD：
- en: '[PRE10]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Since we can see the multi-class classification problem, we generate a sample
    dataset with three classes. In addition to mean values and variances used in perceptrons,
    we also use the dataset according to normal distribution with the mean of `0.0`
    and the variance of 1.0 for the training data and the test data for class 3\.
    In other words, each class''s data follows normal distributions with the mean
    of `[-2.0, 2.0]`, `[2.0, -1.0]` and `[0.0, 0.0]` and the variance of `1.0`. We
    defined the training data as the `int` type and the test data as the Integer type
    for the labeled data. This is to process the test data easier when evaluating
    the model. Also, each piece of labeled data is defined as an array because it
    follows multi-class classification:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可以看到多类分类问题，我们生成一个包含三类的样本数据集。除了在感知机中使用的均值和方差外，我们还使用了根据正态分布生成的数据集，训练数据和测试数据的均值为`0.0`，方差为1.0，属于第3类。换句话说，每一类的数据遵循正态分布，均值分别为`[-2.0,
    2.0]`、`[2.0, -1.0]`和`[0.0, 0.0]`，方差为`1.0`。我们将训练数据定义为`int`类型，测试数据定义为整数类型，以便在评估模型时更容易处理测试数据。此外，每个标注数据被定义为数组，因为它遵循多类分类：
- en: '[PRE11]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then we classify the training data into a mini-batch using `minibatchIndex`,
    which was defined earlier:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用之前定义的`minibatchIndex`将训练数据划分为一个小批量：
- en: '[PRE12]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we have prepared the data, let''s practically build a model:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了数据，接下来实际构建模型：
- en: '[PRE13]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The model parameters of logistic regression are `W`, weight of the network,
    and bias `b`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型的参数是`W`，网络的权重，以及偏置`b`：
- en: '[PRE14]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The training is done with each mini-batch. If you set `minibatchSize = 1`,
    you can make the training so-called online training:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 训练是通过每个小批量完成的。如果你设置`minibatchSize = 1`，那么训练就变成了所谓的在线训练：
- en: '[PRE15]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here, the learning rate gradually decreases so that the model can converge.
    Now, for the actual training `train` method, you can briefly divide it into two
    parts, as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，学习率逐渐减小，以便模型能够收敛。现在，针对实际的训练`train`方法，你可以将其简要地分为两个部分，如下所示：
- en: Calculate the gradient of `W` and `b` using the data from the mini-batch.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用小批量数据计算`W`和`b`的梯度。
- en: 'Update `W` and `b` with the gradients:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度更新`W`和`b`：
- en: '[PRE16]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: At the end of the `train` method, `return dY`, the error value of the predicted
    data and the correct data is returned. This is not mandatory for logistic regression
    itself but it is necessary in the machine learning and the deep learning algorithms
    introduced later.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在`train`方法的末尾，`return dY`返回预测数据和正确数据之间的误差值。虽然这对于逻辑回归本身并非强制要求，但对于后续介绍的机器学习和深度学习算法是必要的。
- en: Next up for the training is the test. The process of performing the test doesn't
    really change from the one for perceptrons.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是测试。执行测试的过程与感知机的过程基本相同。
- en: 'First, with the `predict` method, let''s predict the input data using the trained
    model:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用`predict`方法，通过训练好的模型预测输入数据：
- en: '[PRE17]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `predict` method and the `output` method called are written as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict`方法和`output`方法的调用方式如下所示：'
- en: '[PRE18]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: First, input data is activated with the `output` method. As you can see from
    the bottom of the output, the activation function uses the `softmax` function.
    `softmax` is defined in `ActivationFunction.java`, and with this function the
    array showing the probability of each class is returned, hence you just need to
    get the index within the array of the element that has the highest probability.
    The index represents the predicted class.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用`output`方法激活输入数据。如你从输出的底部可以看到，激活函数使用了`softmax`函数。`softmax`在`ActivationFunction.java`中定义，通过这个函数会返回一个显示每个类概率的数组，因此你只需要获取数组中概率最大的元素的索引。这个索引代表了预测的类别。
- en: 'Finally, let''s evaluate the model. Again, the confusion matrix is introduced
    for model evaluation, but be careful as you need to find the precision or recall
    for each class this time because we have multi-class classification here:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们评估模型。再次介绍混淆矩阵用于模型评估，但要小心，因为这次我们有多类分类问题，因此你需要为每个类分别计算精度或召回率：
- en: '[PRE19]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Multi-layer perceptrons (multi-layer neural networks)
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多层感知机（多层神经网络）
- en: 'Single-layer neural networks have a huge problem. Perceptrons or logistic regressions
    are efficient for problems that can be linearly classified but they can''t solve
    nonlinear problems at all. For example, they can''t even solve the simplest XOR
    problem seen in the figure here:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 单层神经网络存在一个巨大问题。感知机或逻辑回归对能够线性分类的问题有效，但它们根本无法解决非线性问题。例如，它们甚至无法解决图中所示的最简单的XOR问题：
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_53.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知机（多层神经网络）](img/B04779_02_53.jpg)'
- en: 'Since most of the problems in the real world are nonlinear, perceptrons and
    logistic regression aren''t applicable for practical uses. Hence, the algorithm
    was improved to correspond to nonlinear problems. These are multi-layer perceptrons
    (or **multi-layer neural networks**, **MLPs**). As you can see from the name,
    by adding another layer, called a hidden layer, between the input layer and the
    output layer, the networks have the ability to express various patterns. This
    is the graphical model of an MLP:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 由于现实世界中的大多数问题都是非线性的，感知机和逻辑回归无法用于实际应用。因此，算法被改进以应对非线性问题。这就是多层感知机（或**多层神经网络**，**MLPs**）。正如你从名字中看到的，通过在输入层和输出层之间添加另一个叫做隐藏层的层，网络具有表达各种模式的能力。这是MLP的图形模型：
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_54.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知机（多层神经网络）](img/B04779_02_54.jpg)'
- en: What is most important here is not to introduce the skip-layer connection. In
    neural networks, it is better for both theory and implementation to keep the model
    as having a feed-forward network structure. By sticking to these rules, and by
    increasing the number of hidden layers, you can approximate arbitrary functions
    without making the model too complicated mathematically.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是不要引入跳层连接。在神经网络中，保持模型具有前馈网络结构对理论和实现更有利。遵循这些规则，并通过增加隐藏层的数量，你可以在不使模型过于复杂的情况下，逼近任意函数。
- en: 'Now, let''s see how we compute the output. It looks complicated at first glance
    but it accumulates the layers and the scheme of the network''s weight or activation
    in the same way, so you simply have to combine the equation of each layer. Each
    output can be shown as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何计算输出。乍一看它似乎很复杂，但它按相同方式累积每一层的网络权重或激活方案，所以你只需将每一层的方程结合起来。每个输出可以如下表示：
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_32.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知机（多层神经网络）](img/B04779_02_32.jpg)'
- en: Here, ![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_77.jpg)
    is the activation function of the hidden layer and ![Multi-layer perceptrons (multi-layer
    neural networks)](img/B04779_02_78.jpg) is the output layer.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![多层感知机（多层神经网络）](img/B04779_02_77.jpg) 是隐藏层的激活函数，![多层感知机（多层神经网络）](img/B04779_02_78.jpg)
    是输出层。
- en: 'As has already been introduced, in the case of multi-class classification,
    the activation function of the output layer can be calculated efficiently by using
    the `softmax` function, and the error function is given as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，对于多类别分类，输出层的激活函数可以通过使用`softmax`函数高效计算，误差函数如下所示：
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_33.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知机（多层神经网络）](img/B04779_02_33.jpg)'
- en: As for a single layer, it's fine just to reflect this error in the input layer,
    but for the multi-layer, neural networks cannot learn as a whole unless you reflect
    the error in both the hidden layer and input layer.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单层网络，仅在输入层反映该误差即可，但对于多层网络，除非在隐藏层和输入层都反映误差，否则神经网络无法整体学习。
- en: 'Fortunately, in feed-forward networks, there is an algorithm known as `backpropagation`,
    which enables the model to propagate this error efficiently by tracing the network
    forward and backward. Let''s look at the mechanism of this algorithm. To make
    the equation more readable, we''ll think about the valuation of an error function
    in the online training, shown as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在前馈网络中，有一种被称为`反向传播`的算法，它使得模型能够通过前向和后向追踪网络来高效传播误差。让我们来看一下该算法的机制。为了使方程更易读，我们将思考在线训练中的误差函数的估算，如下所示：
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_34.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知机（多层神经网络）](img/B04779_02_34.jpg)'
- en: We can now think about just the gradient of this, ![Multi-layer perceptrons
    (multi-layer neural networks)](img/B04779_02_79.jpg). Since all the data in a
    dataset in most cases of practical application is independent and identically
    distributed, we have no problem defining it as we just mentioned.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以仅考虑它的梯度，![多层感知机（多层神经网络）](img/B04779_02_79.jpg)。由于在大多数实际应用中，数据集中的所有数据通常是独立同分布的，我们在定义时不会遇到问题，正如我们刚才所提到的那样。
- en: 'Each unit in the feed-forward network is shown as the sum of the weight of
    the network connected to the unit, hence the generalized term can be represented
    as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈网络中的每个单元显示为与该单元连接的网络权重的总和，因此可以表示为以下广义项：
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_35.jpg)![Multi-layer
    perceptrons (multi-layer neural networks)](img/B04779_02_36.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知机（多层神经网络）](img/B04779_02_35.jpg)![多层感知机（多层神经网络）](img/B04779_02_36.jpg)'
- en: 'Be careful, as ![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_61.jpg)
    here is not only the value of the input layer (of course, this can be the value
    of the input layer). Also, ![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_77.jpg)
    is the nonlinear activation function. The gradient of weights and the gradient
    of the bias can be shown as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 请小心，因为这里的![多层感知机（多层神经网络）](img/B04779_02_61.jpg)不仅仅是输入层的值（当然，这可以是输入层的值）。另外，![多层感知机（多层神经网络）](img/B04779_02_77.jpg)是非线性激活函数。权重的梯度和偏置的梯度可以如下所示：
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_37.jpg)![Multi-layer
    perceptrons (multi-layer neural networks)](img/B04779_02_38.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知机（多层神经网络）](img/B04779_02_37.jpg)![多层感知机（多层神经网络）](img/B04779_02_38.jpg)'
- en: 'Now, let the notation defined in the next equation be introduced:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，介绍下一个方程中定义的符号：
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_39.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知机（多层神经网络）](img/B04779_02_39.jpg)'
- en: 'Then, we get:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到：
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_40.jpg)![Multi-layer
    perceptrons (multi-layer neural networks)](img/B04779_02_41.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知机（多层神经网络）](img/B04779_02_40.jpg)![多层感知机（多层神经网络）](img/B04779_02_41.jpg)'
- en: 'Therefore, when we compare the equations, the output unit can be described
    as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们比较这些方程时，输出单元可以描述如下：
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_42.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知机（多层神经网络）](img/B04779_02_42.jpg)'
- en: 'Also, each unit of the hidden layer is:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，隐藏层的每个单元是：
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_43.jpg)![Multi-layer
    perceptrons (multi-layer neural networks)](img/B04779_02_44.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知机（多层神经网络）](img/B04779_02_43.jpg)![多层感知机（多层神经网络）](img/B04779_02_44.jpg)'
- en: Thus, the **backpropagation formula** is introduced. As such, delta is called
    the **backpropagated** error. By computing the `backpropagated` error, the weights
    and bias can be calculated. It may seem difficult when you look at the formula,
    but what it basically does is receive feedback on errors from a connected unit
    and renew the weight, so it's not that difficult.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，引入了**反向传播公式**。这样，delta被称为**反向传播**误差。通过计算`反向传播`误差，可以计算权重和偏置。看起来公式可能很难理解，但它基本上是在接收到来自连接单元的错误反馈后更新权重，因此并不难。
- en: 'Now, let''s look at an implementation with a simple XOR problem as an example.
    You will have better understanding when you read the source code. The structure
    of the package is as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们以一个简单的XOR问题为例来看一下实现。你在阅读源代码时会有更好的理解。该包的结构如下：
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_55.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知机（多层神经网络）](img/B04779_02_55.jpg)'
- en: The basic flow of the algorithm is written in `MultiLayerPerceptrons.java`,
    but the actual part of backpropagation is written in `HiddenLayer.java`. We use
    multi-class logistic regression for the output layer. Since there is no change
    in `LogisticRegression.java`, the code is not shown in this section. In `ActivationFunction.java`,
    derivatives of the sigmoid function and hyperbolic tangent are added. The hyperbolic
    tangent is also the activation function that is often used as an alternative to
    the sigmoid. Also, in `RandomGenerator.java`, the method to generate random numbers
    with a uniform distribution is written. This is to randomly initialize the weight
    of the hidden layer, and it is quite an important part because a model often falls
    into a local optimum and fails to classify the data depending on these initial
    values.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的基本流程写在`MultiLayerPerceptrons.java`中，但实际的反向传播部分写在`HiddenLayer.java`中。我们使用多类逻辑回归作为输出层。由于`LogisticRegression.java`没有变化，代码在本节中不展示。在`ActivationFunction.java`中，加入了sigmoid函数和双曲正切函数的导数。双曲正切函数也是一个常用的激活函数，常作为sigmoid的替代。此外，在`RandomGenerator.java`中，写了生成均匀分布随机数的方法。这是为了随机初始化隐藏层的权重，这部分非常重要，因为模型通常会根据这些初始值陷入局部最优解，从而导致无法正确分类数据。
- en: 'Let''s have a look at the content of `MultiLayerPerceptrons.java`. In `MultiLayerPereptrons.java`,
    differently defined classes are defined respectively for each layer: `HiddenLayer`
    class is used for the hidden layer and `LogisticRegression` class for the output
    layer. Instances of these classes are defined as `hiddenLayer` and `logisticLayer`,
    respectively:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下`MultiLayerPerceptrons.java`的内容。在`MultiLayerPerceptrons.java`中，每一层都分别定义了不同的类：`HiddenLayer`类用于隐藏层，`LogisticRegression`类用于输出层。这些类的实例分别定义为`hiddenLayer`和`logisticLayer`：
- en: '[PRE20]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The parameters of the MLP are the weights `W` and bias `b` of the hidden layer,
    `HiddenLayer`, and the output layer, `LogisticRegression`. Since the output layer
    is the same as the one previously introduced, we won''t look at the code here.
    The constructor of `HiddenLayer` is as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: MLP的参数是隐藏层`HiddenLayer`和输出层`LogisticRegression`的权重`W`和偏置`b`。由于输出层与之前介绍的相同，我们这里不再查看代码。`HiddenLayer`的构造函数如下：
- en: '[PRE21]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`W` is initialized, randomly matching the number of the units. This initialization
    is actually tricky as it makes you face the local minima problem more often if
    the initial values are not well distributed. Therefore, in a practical scene,
    it often happens that the model is tested with some random seeds. The activation
    function can be applied to either the sigmoid function or the hyperbolic tangent
    function.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`W`会被随机初始化，且与单元数相匹配。这个初始化过程实际上很复杂，因为如果初始值没有很好地分布，会让你更频繁地遇到局部最小值问题。因此，在实际场景中，模型通常会使用一些随机种子进行测试。激活函数可以使用sigmoid函数或双曲正切函数。'
- en: 'The training of the MLP can be given by forward propagation and backward propagation
    through the neural networks in order:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: MLP的训练可以通过前向传播和反向传播依次进行神经网络的传播：
- en: '[PRE22]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The part of `hiddenLayer.backward` gives the hidden layer backpropagation of
    the prediction error, `dY`, from a logistic regression. Be careful, as the input
    data of a logistic regression is also necessary for the backpropagation:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`hiddenLayer.backward`部分给出了隐藏层对来自逻辑回归的预测误差`dY`的反向传播。请注意，逻辑回归的输入数据也是反向传播所必需的：'
- en: '[PRE23]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You might think the algorithm is complex and difficult because the arguments
    seem complicated, but what we do here is almost the same as what we do with the
    `train` method of logistic regression: we calculate the gradients of `W` and `b`
    with the unit of the mini-batch and update the model parameters. That''s it. So,
    can an MLP learn the XOR problem? Check the result by running `MultiLayerPerceptrons.java`.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为这个算法很复杂且难以理解，因为参数看起来很复杂，但实际上我们所做的与逻辑回归的`train`方法几乎相同：我们计算`W`和`b`的梯度，并在小批量的单位上更新模型参数，仅此而已。那么，MLP能学习XOR问题吗？通过运行`MultiLayerPerceptrons.java`来检查结果。
- en: 'The result only outputs the percentages of the accuracy, precision, and recall
    of the model, but for example, if you dump the prediction data with the `predict`
    method of `LogisticRegression`, you can see how much it actually predicts the
    probability, as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 结果只输出模型的准确率、精确度和召回率的百分比，但例如，如果你使用`LogisticRegression`的`predict`方法导出预测数据，你可以看到它实际预测的概率，如下所示：
- en: '[PRE24]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We've just shown that MLPs can approximate the function of XOR. Moreover, it
    is proven that MLPs can approximate any functions. We don't follow the math details
    here, but you can easily imagine that the more units MLPs have, the more complicated
    functions they could express and approximate.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚展示了多层感知机（MLP）可以近似XOR的函数。此外，已证明MLP可以近似任何函数。我们在这里不深入数学细节，但你可以很容易地想象，MLP拥有的单元越多，它们能够表达和近似的函数就越复杂。
- en: Summary
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, as preparation for deep learning, we dug into neural networks,
    which are one of the algorithms of machine learning. You learned about three representative
    algorithms of single-layer neural networks: perceptrons, logistic regression,
    and multi-class logistic regression. We see that single-layer neural networks
    can''t solve nonlinear problems, but this problem can be solved with multi-layer
    neural networks—the networks with a hidden layer(s) between the input layer and
    output layer. An intuitive understanding of why MLPs can solve nonlinear problems
    says that the networks can learn more complicated logical operations by adding
    layers and increasing the number of units, and thus having the ability to express
    more complicated functions. The key to letting the model have this ability is
    the backpropagation algorithm. By backpropagating the error of the output to the
    whole network, the model is updated and adjusted to fit in the training data with
    each iteration, and finally optimized to approximate the function for the data.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，作为深度学习的准备，我们深入探讨了神经网络，这是机器学习的算法之一。你学习了单层神经网络的三种代表性算法：感知器、逻辑回归和多类逻辑回归。我们看到单层神经网络无法解决非线性问题，但这个问题可以通过多层神经网络来解决——即在输入层和输出层之间有一个或多个隐藏层的网络。对为什么MLP能够解决非线性问题的直观理解是，通过增加层数和单元数量，网络可以学习更复杂的逻辑运算，从而具备表达更复杂函数的能力。让模型具备这一能力的关键是反向传播算法。通过将输出的误差反向传播到整个网络，模型会在每次迭代中更新和调整，以便更好地拟合训练数据，最终优化以近似数据的函数。
- en: In the next chapter, you'll learn the concepts and algorithms of deep learning.
    Since you've now acquired a foundational understanding of machine learning algorithms,
    you'll have no difficulty learning about deep learning.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习深度学习的概念和算法。由于你现在已经掌握了机器学习算法的基础知识，因此你将毫不困难地学习深度学习。
