- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Deep Learning Models for Graphs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图的深度学习模型
- en: In recent years, the field of machine learning has witnessed a paradigm shift
    with the emergence of **graph neural networks** ( **GNNs** ) as powerful tools
    for addressing prediction tasks on graph-structured data. Here, we'll delve into
    the transformative potential of GNNs, highlighting their role as optimizable transformations
    capable of handling diverse graph attributes, such as nodes, edges, and global
    context while preserving crucial graph symmetries, particularly permutation invariances.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，随着**图神经网络**（**GNNs**）的出现，机器学习领域迎来了范式转变，GNN成为了处理图结构数据预测任务的强大工具。在这里，我们将深入探讨GNN的变革潜力，强调其作为可优化变换的作用，能够处理多样的图属性，如节点、边缘和全局上下文，同时保持重要的图对称性，尤其是排列不变性。
- en: The foundation of GNNs lies in the **message-passing neural network** ( **MPNN**
    ) framework. Through this framework, GNNs leverage a sophisticated mechanism for
    information exchange and aggregation across graph structures, enabling the model
    to capture intricate relationships and dependencies within the data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: GNN的基础在于**信息传递神经网络**（**MPNN**）框架。通过该框架，GNN利用一种复杂的机制，在图结构中进行信息交换和聚合，使得模型能够捕捉数据中的复杂关系和依赖性。
- en: One distinctive feature of GNNs is their adherence to a *graph-in, graph-out*
    architecture. This means that the model accepts a graph as input, equipped with
    information embedded in its nodes, edges, and global context. This inherent structure
    aligns with many real-world problems where data exhibits complex relationships
    and dependencies best represented as graphs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: GNN的一个显著特点是遵循*图输入，图输出*架构。这意味着模型接受一个图作为输入，该图包含嵌入在节点、边缘和全局上下文中的信息。这种固有的结构与许多现实世界的问题相吻合，数据通常展现出复杂的关系和依赖性，最适合通过图来表示。
- en: GNNs excel in their ability to perform a progressive embedding transformation
    on the input graph without altering its connectivity. This progressive transformation
    ensures that the model refines its understanding of the underlying patterns and
    structures within the data, contributing to enhanced predictive capabilities.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: GNN的优势在于其能够在不改变图的连接性的情况下，对输入图执行逐步的嵌入变换。这一逐步变换确保了模型不断优化对数据中潜在模式和结构的理解，从而提升预测能力。
- en: 'We''ll cover the following topics in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Message passing in graphs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图中的信息传递
- en: Decoding GNNs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码GNN
- en: '**Graph convolutional** **networks** ( **GCNs** )'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图卷积网络**（**GCNs**）'
- en: '**Graph Sample and** **Aggregation** ( **GraphSAGE** )'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图采样与聚合**（**GraphSAGE**）'
- en: '**Graph attention** **networks** ( **GATs** )'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图注意力网络**（**GATs**）'
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter requires a basic understanding of graphs and representation learning
    as covered in previous chapters. The code present in the chapter and on GitHub
    can be used directly on Google Colab with an additional installation of the **PyTorch
    Geometric** ( **PyG** ) package. The code examples for the book are available
    in its GitHub repository: [https://github.com/PacktPublishing/Applied-Deep-Learning-on-Graphs](https://github.com/PacktPublishing/Applied-Deep-Learning-on-Graphs)
    .'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求读者具备图和表示学习的基本理解，这些内容已在前几章中讲解。章节中的代码以及GitHub上的代码可以直接在Google Colab上使用，只需额外安装**PyTorch
    Geometric**（**PyG**）包。书中的代码示例可以在其GitHub仓库中找到：[https://github.com/PacktPublishing/Applied-Deep-Learning-on-Graphs](https://github.com/PacktPublishing/Applied-Deep-Learning-on-Graphs)。
- en: Message passing in graphs
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图中的信息传递
- en: Unlike traditional neural networks, GNNs need to account for the inherent structure
    of the graph, allowing nodes to exchange information and update their representations
    based on their local neighborhoods. This core mechanism is achieved through **message
    passing** , a process of iteratively passing messages between nodes and aggregating
    information from their neighbors.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的神经网络不同，GNN（图神经网络）需要考虑图的固有结构，使得节点能够交换信息并根据其局部邻域更新自身表示。这一核心机制是通过**信息传递**实现的，这是一个节点间迭代传递信息并聚合邻居信息的过程。
- en: GNNs operate on graph-structured data and use a message-passing mechanism to
    update node representations based on information from neighboring nodes. Let’s
    delve into the mathematical explanation of message passing in GNNs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: GNN在图结构数据上进行操作，并使用信息传递机制基于邻居节点的信息更新节点表示。让我们深入探讨GNN中信息传递的数学解释。
- en: Consider an undirected graph ![<mml:math  ><mml:mi>G</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/116.png)
    , where ![<mml:math  ><mml:mi>V</mml:mi></mml:math>](img/1.png) is the set of
    nodes and ![<mml:math  ><mml:mi mathvariant="normal"> </mml:mi><mml:mi>E</mml:mi></mml:math>](img/118.png)
    is the set of edges. Each node ![<mml:math  ><mml:mi>v</mml:mi></mml:math>](img/89.png)
    in ![<math ><mrow><mi>V</mi></mrow></math>](img/120.png) has an associated feature
    vector ![<mml:math  ><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/121.png)
    . The goal of a GNN is to learn a representation ![<mml:math  ><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/122.png)
    for each node ![<mml:math  ><mml:mi>v</mml:mi></mml:math>](img/89.png) that captures
    information from its neighborhood.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个无向图 ![<mml:math  ><mml:mi>G</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/116.png)
    ，其中 ![<mml:math  ><mml:mi>V</mml:mi></mml:math>](img/1.png) 是节点集合，![
- en: 'The basic message-passing operation in a GNN can be broken down into a series
    of steps:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络中的基本消息传递操作可以分解为一系列步骤：
- en: '**Aggregation** **of messages** :'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**消息聚合**：'
- en: For each node ![<mml:math  ><mml:mi>v</mml:mi></mml:math>](img/89.png) , gather
    information   from its neighbors.
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个节点 ![<mml:math  ><mml:mi>v</mml:mi></mml:math>](img/89.png)，从其邻居收集信息。
- en: Let ![<mml:math  ><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/125.png)
    represent the set of neighbors of node ![<mml:math  ><mml:mi>v</mml:mi></mml:math>](img/89.png)
    .
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 令 ![<mml:math  ><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/125.png)
    表示节点 ![<mml:math  ><mml:mi>v</mml:mi></mml:math>](img/89.png) 的邻居集合。
- en: 'The aggregated message ![<mml:math  ><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/127.png)
    for node ![<mml:math  ><mml:mi>v</mml:mi></mml:math>](img/128.png) is computed
    by aggregating information from its neighbors:'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 节点 ![<mml:math  ><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/127.png)
    的聚合消息是通过聚合其邻居的信息计算得出的：
- en: '![<mml:math   display="block"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi> </mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mi> </mml:mi><mml:mi>u</mml:mi><mml:mi> </mml:mi><mml:mi>ϵ</mml:mi><mml:mi> </mml:mi><mml:mi>N</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/129.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math   display="block"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi> </mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mi> </mml:mi><mml:mi>u</mml:mi><mml:mi> </mml:mi><mml:mi>ϵ</mml:mi><mml:mi> </mml:mi><mml:mi>N</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/129.png)'
- en: The aggregation function can vary (e.g., sum, mean, or attention-weighted sum).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合函数可以有所不同（例如，求和、均值或注意力加权和）。
- en: '**UPDATE function** :'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更新函数**：'
- en: Update the node representation ![<mml:math  ><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/122.png)
    based on the aggregated message ![<mml:math  ><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/131.png)
    and the current node   representation ![<mml:math  ><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/122.png)
    .
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于聚合的消息 ![<mml:math  ><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/122.png)
    和当前节点表示 ![<mml:math  ><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/122.png)，更新节点表示
    ![<mml:math  ><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/131.png)。
- en: 'The **UPDATE** function ![<math ><mrow><mrow><mi>u</mi><mi>p</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow></mrow></math>](img/133.png)
    ( ![<mml:math  ><mml:mo>∙</mml:mo></mml:math>](img/134.png) ) is a neural network
    layer that takes the aggregated message and the current node representation as
    input and produces the updated representation:'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**UPDATE** 函数 ![<math ><mrow><mrow><mi>u</mi><mi>p</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow></mrow></math>](img/133.png)
    ( ![<mml:math  ><mml:mo>∙</mml:mo></mml:math>](img/134.png) ) 是一个神经网络层，它将聚合的消息和当前节点表示作为输入，并生成更新后的表示：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/135.png)
    = ![<math ><mrow><mrow><mi>u</mi><mi>p</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>e</mi><mfenced
    open="(" close=")"><mrow><msub><mi>h</mi><mi>v</mi></msub><mo>,</mo><msub><mi>m</mi><mi>v</mi></msub></mrow></mfenced></mrow></mrow></math>](img/136.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/135.png)
    = ![<math ><mrow><mrow><mi>u</mi><mi>p</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>e</mi><mfenced
    open="(" close=")"><mrow><msub><mi>h</mi><mi>v</mi></msub><mo>,</mo><msub><mi>m</mi><mi>v</mi></msub></mrow></mfenced></mrow></mrow></math>](img/136.png)'
- en: The **UPDATE** function typically involves a neural network layer with learnable
    parameters.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**UPDATE** 函数通常涉及一个具有可学习参数的神经网络层。'
- en: 'These steps are iteratively applied for a fixed number of times or until convergence
    to refine the node representations. The overall process can be expressed in a
    few equations:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤会迭代应用固定次数，或直到收敛，以细化节点表示。整个过程可以通过几个公式表达：
- en: '![<mml:math   display="block"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mi> </mml:mi><mml:mi>u</mml:mi><mml:mi> </mml:mi><mml:mo>∈</mml:mo><mml:mi> </mml:mi><mml:mi>N</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/137.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math   display="block"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mi> </mml:mi><mml:mi>u</mml:mi><mml:mi> </mml:mi><mml:mo>∈</mml:mo><mml:mi> </mml:mi><mml:mi>N</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/137.png)'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/138.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/138.png)'
- en: These equations capture the essence of the message-passing mechanism in a GNN.
    The specific choices for the aggregation function, **UPDATE** function, and the
    number of iterations depend on the architecture of the GNN (e.g., GraphSAGE, GCN,
    **gated graph neural networks** ( **GGNNs** ), etc.).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方程捕捉了图神经网络（GNN）中信息传递机制的本质。聚合函数、**更新**函数以及迭代次数的具体选择取决于GNN的架构（例如，GraphSAGE、GCN、**门控图神经网络**（**GGNNs**）等）。
- en: 'For instance, in a simple **GraphSAGE** formulation, the aggregation function
    might be a mean operation, and the **UPDATE** function might be a simple neural
    network layer:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个简单的**GraphSAGE**公式中，聚合函数可能是均值操作，**更新**函数可能是一个简单的神经网络层：
- en: '![<mml:math   display="block"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced
    open="|" close="|" separators="|"><mml:mrow><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi> </mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi> </mml:mi><mml:mi>ϵ</mml:mi><mml:mi> </mml:mi><mml:mi>N</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi> </mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/139.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math   display="block"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced
    open="|" close="|" separators="|"><mml:mrow><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi> </mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi> </mml:mi><mml:mi>ϵ</mml:mi><mml:mi> </mml:mi><mml:mi>N</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi> </mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/139.png)'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>σ</mml:mi><mml:mi> </mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>W</mml:mi><mml:mi> </mml:mi><mml:mo>⋅</mml:mo><mml:mi> </mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi> </mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/140.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>σ</mml:mi><mml:mi> </mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>W</mml:mi><mml:mi> </mml:mi><mml:mo>⋅</mml:mo><mml:mi> </mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi> </mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/140.png)'
- en: Here, ![<mml:math  ><mml:mi>σ</mml:mi></mml:math>](img/141.png) is an activation   function,
    ![<mml:math  ><mml:mi>W</mml:mi></mml:math>](img/142.png) is a learnable weight
    matrix, and ![<mml:math  ><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow></mml:mfenced></mml:math>](img/143.png)
    is the concatenation operation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![<mml:math  ><mml:mi>σ</mml:mi></mml:math>](img/141.png) 是一个激活函数，![<mml:math  ><mml:mi>W</mml:mi></mml:math>](img/142.png)
    是一个可学习的权重矩阵，而![<mml:math  ><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow></mml:mfenced></mml:math>](img/143.png)
    是连接操作。
- en: '![Figure 4.1 – Message passing in graphs](img/B22118_04_1.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – 图中的信息传递](img/B22118_04_1.jpg)'
- en: Figure 4.1 – Message passing in graphs
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 图中的信息传递
- en: Through these iterative steps, message passing enables nodes to learn representations
    that capture not only their own intrinsic features but also the information from
    their connected neighbors and the overall structure of the graph. This allows
    GNNs to effectively model complex relationships and dependencies within graph-structured
    data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些迭代步骤，信息传递使得节点能够学习到不仅仅是自身的内在特征，还能从其连接的邻居以及图的整体结构中获取信息。这使得 GNN 能够有效地建模图结构数据中的复杂关系和依赖。
- en: Now, let’s try to understand how to formally define a GNN.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试理解如何正式定义 GNN。
- en: Decoding GNNs
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码 GNN
- en: A **GNN** is a neural network architecture designed to operate on graph-structured
    data. It learns a function that maps a graph and its associated features to a
    set of node-level, edge-level, or graph-level outputs. The following is a formal
    mathematical definition of a GNN.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**GNN** 是一种神经网络架构，旨在处理图结构数据。它学习一个函数，将图及其相关特征映射到一组节点级、边级或图级的输出。以下是 GNN 的正式数学定义。'
- en: Given a graph ![<mml:math  ><mml:mi>G</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/144.png)
    , where ![<mml:math  ><mml:mi>V</mml:mi></mml:math>](img/1.png) is the set of
    nodes and ![<mml:math  ><mml:mi>E</mml:mi></mml:math>](img/146.png) is the set
    of edges, let ![<mml:math  ><mml:mi>X</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>∈</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    open="|" close="|" separators="|"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math>](img/147.png)
    be the node feature matrix, where each row ![<mml:math  ><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math>](img/148.png)
    represents the features of node ![<math ><mrow><mrow><mi>v</mi><mspace width="0.25em"
    /><mo>∈</mo><mspace width="0.25em" /><mi>V</mi></mrow></mrow></math>](img/149.png)
    .
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个图 ![<mml:math  ><mml:mi>G</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/144.png)
    ，其中 ![<mml:math  ><mml:mi>V</mml:mi></mml:math>](img/1.png) 是节点集，![
- en: A GNN is a function ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>G</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced open="|"
    close="|" separators="|"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced open="|"
    close="|" separators="|"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">'</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math>](img/150.png)
    parameterized by learnable weights ![<mml:math  ><mml:mi>θ</mml:mi></mml:math>](img/151.png)
    , which maps the graph ![<mml:math  ><mml:mi>G</mml:mi></mml:math>](img/152.png)
    and its node features ![<mml:math  ><mml:mi>X</mml:mi></mml:math>](img/153.png)
    to a new set of node representations ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced open="|"
    close="|" separators="|"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">'</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math>](img/154.png)
    , where ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">'</mml:mi></mml:mrow></mml:msup></mml:math>](img/155.png)
    is the dimensionality of the output node representations.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: GNN 是一个函数 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>G</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced open="|"
    close="|" separators="|"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced open="|"
    close="|" separators="|"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">'</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math>](img/150.png)
    ，由可学习权重 ![<mml:math  ><mml:mi>θ</mml:mi></mml:math>](img/151.png) 参数化，它将图 ![<mml:math  ><mml:mi>G</mml:mi></mml:math>](img/152.png)
    及其节点特征 ![<mml:math  ><mml:mi>X</mml:mi></mml:math>](img/153.png) 映射到一组新的节点表示 ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced open="|"
    close="|" separators="|"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">'</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math>](img/154.png)
    ，其中 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">'</mml:mi></mml:mrow></mml:msup></mml:math>](img/155.png)
    是输出节点表示的维度。
- en: 'The function ![<mml:math  ><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:math>](img/156.png)
    is computed through a series of message passing and aggregation steps, typically
    organized into ![<mml:math  ><mml:mi>L</mml:mi></mml:math>](img/115.png) layers.
    At each layer ![<mml:math  ><mml:mi>l</mml:mi><mml:mi> </mml:mi><mml:mo>∈</mml:mo><mml:mi> </mml:mi><mml:mfenced
    open="{" close="}" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/158.png)
    , the node representations are updated as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数 ![<mml:math  ><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:math>](img/156.png)
    通过一系列的消息传递和聚合步骤计算，通常被组织为 ![<mml:math  ><mml:mi>L</mml:mi></mml:math>](img/115.png)
    层。在每一层 ![<mml:math  ><mml:mi>l</mml:mi><mml:mi> </mml:mi><mml:mo>∈</mml:mo><mml:mi> </mml:mi><mml:mfenced
    open="{" close="}" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/158.png)
    ，节点表示按照如下方式更新：
- en: '![<mml:math   display="block"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>U</mml:mi><mml:mi>P</mml:mi><mml:mi>D</mml:mi><mml:mi>A</mml:mi><mml:mi>T</mml:mi><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>A</mml:mi><mml:mi>G</mml:mi><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>:</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>u</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>∈</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/159.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math   display="block"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>U</mml:mi><mml:mi>P</mml:mi><mml:mi>D</mml:mi><mml:mi>A</mml:mi><mml:mi>T</mml:mi><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>A</mml:mi><mml:mi>G</mml:mi><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>:</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>u</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>∈</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/159.png)'
- en: 'Let’s break this down:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下：
- en: '![<mml:math  ><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mi>ϵ</mml:mi><mml:mi> </mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/160.png)
    is the representation of node ![<mml:math  ><mml:mi>v</mml:mi></mml:math>](img/95.png)
    at layer ![<mml:math  ><mml:mi> </mml:mi><mml:mi>l</mml:mi><mml:mo>,</mml:mo></mml:math>](img/162.png)
    with ![<mml:math  ><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/163.png)
    . Here, ![<mml:math  ><mml:mi> </mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>](img/164.png)
    represents a real-valued vector space of dimension ![<mml:math  ><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/165.png)
    .'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math  ><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mi>ϵ</mml:mi><mml:mi> </mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/160.png)
    是第 ![<mml:math  ><mml:mi>v</mml:mi></mml:math>](img/95.png) 节点在第 ![<mml:math  ><mml:mi> </mml:mi><mml:mi>l</mml:mi><mml:mo>,</mml:mo></mml:math>](img/162.png)
    层的表示，且满足条件 ![<mml:math  ><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/163.png)
    。其中，![<mml:math  ><mml:mi> </mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>](img/164.png)
    表示一个实值向量空间，维度为 ![<mml:math  ><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/165.png)
    。'
- en: The **UPDATE function** ![<math ><mrow><mrow><mi>U</mi><mi>P</mi><mi>D</mi><mi>A</mi><mi>T</mi><msup><mi>E</mi><mi>l</mi></msup><mo>:</mo><mspace
    width="0.25em" /><msup><mi mathvariant="double-struck">R</mi><mfenced open="("
    close=")"><msub><mi>d</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub></mfenced></msup><mspace
    width="0.25em" /><mi mathvariant="normal">Χ</mi><mspace width="0.25em" /><msup><mi
    mathvariant="double-struck">R</mi><mrow><mfenced open="(" close=")"><msub><mi>d</mi><mi>l</mi></msub></mfenced><mspace
    width="0.25em" /></mrow></msup><mo>→</mo><mspace width="0.25em" /><mspace width="0.25em"
    /><msup><mi mathvariant="double-struck">R</mi><mrow><mfenced open="(" close=")"><msub><mi>d</mi><mi>l</mi></msub></mfenced><mspace
    width="0.25em" /></mrow></msup></mrow></mrow></math>](img/166.png) is a learnable
    function that updates the node   representation based on its previous representation
    and the aggregated messages from its neighbors.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**UPDATE 函数** ![<math ><mrow><mrow><mi>U</mi><mi>P</mi><mi>D</mi><mi>A</mi><mi>T</mi><msup><mi>E</mi><mi>l</mi></msup><mo>:</mo><mspace
    width="0.25em" /><msup><mi mathvariant="double-struck">R</mi><mfenced open="("
    close=")"><msub><mi>d</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub></mfenced></msup><mspace
    width="0.25em" /><mi mathvariant="normal">Χ</mi><mspace width="0.25em" /><msup><mi
    mathvariant="double-struck">R</mi><mrow><mfenced open="(" close=")"><msub><mi>d</mi><mi>l</mi></msub></mfenced><mspace
    width="0.25em" /></mrow></msup><mo>→</mo><mspace width="0.25em" /><mspace width="0.25em"
    /><msup><mi mathvariant="double-struck">R</mi><mrow><mfenced open="(" close=")"><msub><mi>d</mi><mi>l</mi></msub></mfenced><mspace
    width="0.25em" /></mrow></msup></mrow></mrow></math>](img/166.png) 是一个可学习的函数，通过基于节点先前的表示和从邻居聚合的消息来更新节点表示。'
- en: The **aggregation (AGG) function** ![<mml:math  ><mml:mi>A</mml:mi><mml:mi>G</mml:mi><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>*</mml:mi></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:mi> </mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/167.png)
    is a permutation-invariant aggregation function that   combines the representations
    of the neighboring nodes. Common choices include sum, mean, and max.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合（AGG）函数 ![<mml:math  ><mml:mi>A</mml:mi><mml:mi>G</mml:mi><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>*</mml:mi></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:mi> </mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/167.png)
    是一种置换不变的聚合函数，结合了相邻节点的表示。常见选择包括求和、平均值和最大值。
- en: '![<mml:math  ><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/125.png)
    denotes the set of neighbors of node ![<mml:math  ><mml:mi>v</mml:mi></mml:math>](img/89.png)
    in the graph ![<math ><mrow><mi>G</mi></mrow></math>](img/170.png) .'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math  ><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/125.png)
    表示图 ![<math ><mrow><mi>G</mi></mrow></math>](img/170.png) 中节点 ![<math ><mrow><mi>v</mi></mrow></math>](img/89.png)
    的邻居集合。'
- en: After ![<mml:math  ><mml:mi> </mml:mi><mml:mi>L</mml:mi></mml:math>](img/171.png)
    layers of message passing   and aggregation, the final node representations are
    given by ![<mml:math  ><mml:mi>H</mml:mi><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/172.png)
    for all ![<mml:math  ><mml:mi>v</mml:mi><mml:mi> </mml:mi><mml:mo>∈</mml:mo><mml:mi> </mml:mi><mml:mi>V</mml:mi></mml:math>](img/173.png)
    .
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ![<mml:math  ><mml:mi> </mml:mi><mml:mi>L</mml:mi></mml:math>](img/171.png)
    层消息传递和聚合之后，最终的节点表示由 ![<mml:math  ><mml:mi>H</mml:mi><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/172.png)
    代表所有 ![<mml:math  ><mml:mi>v</mml:mi><mml:mi> </mml:mi><mml:mo>∈</mml:mo><mml:mi> </mml:mi><mml:mi>V</mml:mi></mml:math>](img/173.png)
    。
- en: The **UPDATE** function is typically implemented as neural networks, such as
    **multi-layer perceptrons** ( **MLPs** ) or attention mechanisms, with learnable
    parameters.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 更新函数通常实现为神经网络，如多层感知器（MLPs）或注意机制，具有可学习参数。
- en: 'For graph-level tasks, a **READOUT function** is applied to the final node
    representations to obtain a graph-level representation:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图级任务，会应用一个READOUT函数来获取最终的节点表示以获得图级表示：
- en: '![<mml:math   display="block"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:mi>A</mml:mi><mml:mi>D</mml:mi><mml:mi>O</mml:mi><mml:mi>U</mml:mi><mml:mi>T</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup><mml:mo>:</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>v</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>∈</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/174.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math   display="block"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:mi>A</mml:mi><mml:mi>D</mml:mi><mml:mi>O</mml:mi><mml:mi>U</mml:mi><mml:mi>T</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup><mml:mo>:</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>v</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>∈</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/174.png)'
- en: Here, **READOUT** is a permutation-invariant function that aggregates the node
    representations into a single vector, such as sum, mean, or a more complex pooling
    operation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**READOUT**是一个排列不变的函数，它将节点表示聚合为一个单一的向量，如求和、均值或更复杂的池化操作。
- en: The graph-level representation ![<mml:math  ><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:math>](img/175.png)
    can then be used for downstream tasks, such as graph classification or regression.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图级别的表示![<mml:math  ><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:math>](img/175.png)随后可以用于下游任务，如图分类或回归。
- en: This is a general formulation of GNNs, and there are many specific architectures
    that fall under this framework, such as GCNs, GraphSAGE, GATs, and MPNNs, each
    with their own variations of the **UPDATE** , **AGG** , and **READOUT** functions.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这是图神经网络（GNN）的一个通用公式框架，许多具体的架构都属于这一框架，例如GCN、GraphSAGE、GAT和MPNN等，每种架构都有自己不同的**UPDATE**、**AGG**和**READOUT**函数变体。
- en: Let’s understand how graph learning borrows the concept of convolution networks
    and leverages it to extract learnings from a graph.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解图学习如何借用卷积网络的概念，并利用它从图中提取学习。
- en: GCNs
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GCN
- en: '**GCNs** are a specific type of GNN that extend the concept of convolution
    to graph-structured data. GCNs learn node representations by aggregating information
    from neighboring nodes, allowing for the capture of both node features and graph
    structure.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**GCN**是GNN的一种特定类型，它将卷积的概念扩展到图结构化数据。GCN通过聚合来自邻近节点的信息来学习节点表示，从而能够捕捉节点特征和图结构。'
- en: 'In a GCN, the graph convolution operation at layer ![<mml:math  ><mml:mi>l</mml:mi></mml:math>](img/176.png)
    is defined as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在GCN中，第l层的图卷积操作![<mml:math  ><mml:mi>l</mml:mi></mml:math>](img/176.png)定义如下：
- en: '![<mml:math   display="block"><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mi>A</mml:mi><mml:mi
    mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mi
    mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mi
    mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/177.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math   display="block"><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mi>A</mml:mi><mml:mi
    mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mi
    mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mi
    mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/177.png)'
- en: 'Let’s break this down:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来分解一下：
- en: '![<mml:math  ><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    open="|" close="|" separators="|"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/178.png)
    is the matrix of node representations at layer ![<math ><mrow><mi>l</mi></mrow></math>](img/179.png)
    , with ![<mml:math  ><mml:mi>H</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>X</mml:mi></mml:math>](img/180.png)
    (input node features).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math  ><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    open="|" close="|" separators="|"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/178.png)
    是层 ![<math ><mrow><mi>l</mi></mrow></math>](img/179.png) 中节点表示的矩阵，其中 ![<mml:math  ><mml:mi>H</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>X</mml:mi></mml:math>](img/180.png)
    是输入节点特征。'
- en: '![<mml:math  ><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>I</mml:mi></mml:math>](img/181.png)
    is the adjacency matrix ![<mml:math  ><mml:mi>A</mml:mi></mml:math>](img/43.png)
    with added self-loops, where ![<mml:math  ><mml:mi>I</mml:mi></mml:math>](img/183.png)
    is the identity matrix.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math  ><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>I</mml:mi></mml:math>](img/181.png)
    是添加了自环的邻接矩阵 ![<mml:math  ><mml:mi>A</mml:mi></mml:math>](img/43.png)，其中 ![<mml:math  ><mml:mi>I</mml:mi></mml:math>](img/183.png)
    是单位矩阵。'
- en: '![<mml:math  ><mml:mover accent="true"><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/184.png)
    is the diagonal degree matrix of ![<mml:math  ><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/185.png)
    , with ![<mml:math  ><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/186.png)
    .'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math  ><mml:mover accent="true"><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/184.png)
    是 ![<mml:math  ><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/185.png)
    的对角度矩阵，其中 ![<mml:math  ><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/186.png)
    .'
- en: '![<mml:math  ><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mi> </mml:mi><mml:mo>∈</mml:mo><mml:mi> </mml:mi><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mi>x</mml:mi><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mi>​</mml:mi><mml:mi>​</mml:mi></mml:math>](img/187.png)
    is a learnable weight matrix for layer ![<mml:math  ><mml:mi>l</mml:mi></mml:math>](img/176.png)
    .'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math  ><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mi> </mml:mi><mml:mo>∈</mml:mo><mml:mi> </mml:mi><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mi>x</mml:mi><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mi>​</mml:mi><mml:mi>​</mml:mi></mml:math>](img/187.png)
    是一层的可学习权重矩阵 ![<mml:math  ><mml:mi>l</mml:mi></mml:math>](img/176.png)。'
- en: '![<mml:math  ><mml:mi>σ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow></mml:mfenced></mml:math>](img/189.png)
    is a non-linear activation   function, such as the ** rectified linear unit **
    ( ** ReLU ** ) function or sigmoid function.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math  ><mml:mi>σ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow></mml:mfenced></mml:math>](img/189.png)是一个非线性激活函数，例如**修正线性单元**（**ReLU**）函数或sigmoid函数。'
- en: The term ![<mml:math  ><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mi>A</mml:mi><mml:mi> </mml:mi><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:math>](img/190.png)
    is the   symmetrically normalized adjacency matrix, which ensures that the scale
    of the node representations remains consistent across layers.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 术语![<mml:math  ><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mi>A</mml:mi><mml:mi> </mml:mi><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:math>](img/190.png)是对称归一化邻接矩阵，它确保节点表示在不同层之间的一致性。
- en: Imagine a citation network, where each node stands for a scientific paper and
    each edge represents a citation connecting two papers. Each paper has a feature
    vector representing its content (e.g., bag-of-words). A GCN can be used to classify
    the papers into different categories (e.g., computer science, physics, biology)
    by learning node representations that capture both the content and the citation
    structure of the network.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个引用网络，其中每个节点代表一篇科学论文，每条边代表连接两篇论文的引用。每篇论文有一个特征向量，代表其内容（例如，词袋模型）。GCN可以通过学习节点表示来将论文分类到不同的类别（例如，计算机科学、物理学、生物学），这些节点表示同时捕捉了内容和引用网络的结构。
- en: 'Building on our mathematical understanding of the GCN, let’s look at a piece
    of sample code leveraging PyG:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们对GCN的数学理解，接下来我们来看一段使用PyG的示例代码：
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For this example, we import the necessary libraries and load the **Cora dataset**
    using the **Planetoid** class from PyG. The Cora dataset is a citation network
    dataset, where *nodes* represent scientific papers and *edges* represent citations
    between papers. The dataset contains 2,708 nodes, 10,556 edges, and 7 classes
    representing different research areas:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们导入必要的库，并使用PyG中的**Planetoid**类加载**Cora数据集**。Cora数据集是一个引用网络数据集，其中*节点*代表科学论文，*边*代表论文之间的引用。该数据集包含2,708个节点，10,556条边和7个类别，表示不同的研究领域：
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, we access the graph data using **dataset[0]** . We then print some statistics
    about the graph, including the number of nodes, edges, features, and classes:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过**dataset[0]**访问图数据。然后，我们打印一些关于图的统计信息，包括节点数、边数、特征数和类别数：
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that we understand what the data looks like, let’s put down the building
    blocks of the model:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了数据的结构，让我们来构建模型的基本模块：
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we define the GCN model. The model consists of two GCN layers ( **GCNConv**
    ) with a hidden layer in between. The **__init__** method initializes the layers
    with the specified input, hidden, and output dimensions. The **forward** method
    defines the forward pass of the model, where **x** and **edge_index** are passed
    through the GCN layers. ReLU activation and dropout are applied after the first
    layer, and **log-softmax** is applied to the output of the second layer:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义GCN模型。该模型由两层GCN（**GCNConv**）组成，中间有一个隐藏层。**__init__**方法使用指定的输入、隐藏和输出维度来初始化层。**forward**方法定义了模型的前向传播过程，其中**x**和**edge_index**会传递到GCN层。ReLU激活函数和dropout会在第一层之后应用，**log-softmax**则应用于第二层的输出：
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here, we set the model parameters based on the dataset. The input dimension
    ( **in_channels** ) is set to the number of node features in the dataset, the
    hidden dimension ( **hidden_channels** ) is set to **16** , and the output dimension
    ( **out_channels** ) is set to the number of classes in the dataset. We then create
    an instance of the GCN model with these parameters:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们根据数据集设置模型的参数。输入维度（**in_channels**）设置为数据集中的节点特征数，隐藏维度（**hidden_channels**）设置为**16**，输出维度（**out_channels**）设置为数据集中的类别数。然后，我们用这些参数创建一个GCN模型的实例：
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this part, we define the optimizer ( **Adam** ) and the loss function ( **negative
    log-likelihood loss (NLLLoss)** ) for training the model. We set the learning
    rate to **0.01** and the weight decay to **5e-4** .
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们定义了优化器（**Adam**）和损失函数（**负对数似然损失（NLLLoss）**）以训练模型。我们将学习率设置为**0.01**，权重衰减设置为**5e-4**。
- en: 'We then train the model for 200 epochs. In each epoch, we do the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们训练模型200个周期。在每个周期中，我们执行以下操作：
- en: Zero the gradients of the optimizer.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将优化器的梯度清零。
- en: Perform a forward pass of the model on the node features and edge index.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对节点特征和边索引执行前向传播。
- en: Compute the loss using the model’s output and the ground truth labels for the
    training nodes (specified by **data.train_mask** ).
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型输出和训练节点的真实标签（由**data.train_mask**指定）计算损失。
- en: Perform a backward pass to compute the gradients.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行反向传播以计算梯度。
- en: 'Update the model parameters using the optimizer:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用优化器更新模型参数：
- en: '[PRE6]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Finally, we evaluate the trained model on the test set. We set the model to
    evaluation mode using **model.eval()** . We perform a forward pass on the entire
    graph and obtain the predicted class labels using **max(dim=1)** . We then compute
    the accuracy by comparing the predicted labels with the ground truth labels for
    the test nodes (specified by **data.test_mask** ).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在测试集上评估训练后的模型。我们通过**model.eval()**将模型设置为评估模式。我们对整个图进行前向传播，并使用**max(dim=1)**获得预测的类别标签。然后，我们通过比较预测标签和测试节点的真实标签（由**data.test_mask**指定）来计算准确率。
- en: This code provides a basic implementation of GCN using PyG on the Cora dataset.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码提供了一个使用PyG在Cora数据集上实现GCN的基本实现。
- en: Note
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Make sure you have PyG installed before running this code. You can install it
    using **pip** **install torch-geometric** .
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行此代码之前，请确保已安装PyG。您可以使用**pip install torch-geometric**来安装它。
- en: Overall, this code creates an instance of the GCN model, trains it on the Cora
    dataset using the specified hyperparameters (hidden units, learning rate, epochs),
    and evaluates the trained model on the test set to measure its performance in
    terms of accuracy.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这段代码创建了一个GCN模型实例，使用指定的超参数（隐藏单元、学习率、周期数）在Cora数据集上进行训练，并在测试集上评估训练后的模型，以衡量其在准确度方面的表现。
- en: Using GCNs for different graph tasks
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用GCN执行不同的图任务
- en: 'GCNs can be utilized to learn and perform tasks at different levels in a graph.
    The following can be performed with GCNs:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: GCN可以用于在图中不同层次上学习和执行任务。以下是GCN可以执行的任务：
- en: '**Node-level tasks** : GCNs can be used for **node classification** , where
    the goal is to predict the label of each node in the graph. This is demonstrated
    in the previous example, where the GCN is used to classify nodes in the Cora citation
    network.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点级任务**：GCN可用于**节点分类**，目标是预测图中每个节点的标签。这在前面的示例中有所展示，其中GCN用于分类Cora引文网络中的节点。'
- en: '**Edge-level tasks** : GCNs can be adapted for **edge prediction** or **link**
    **prediction** tasks, where the goal is to predict the existence or attributes
    of edges in the graph. To do this, the node representations learned by the GCN
    can be used to compute edge scores or probabilities.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边级任务**：GCN可以适应**边预测**或**链接预测**任务，目标是预测图中边的存在性或属性。为此，可以使用GCN学习到的节点表示来计算边的得分或概率。'
- en: '**Graph-level tasks** : GCNs can be used for **graph classification** or **regression**
    tasks, where the goal is to predict a label or a continuous value for an entire
    graph. To achieve this, a pooling operation (e.g., global mean pooling or global
    max pooling) is applied to the node representations learned by the GCN to obtain
    a graph-level representation, which is then fed into a classifier or regressor.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图级任务**：GCN可用于**图分类**或**回归**任务，目标是预测整个图的标签或连续值。为此，需要对GCN学习到的节点表示应用池化操作（例如全局平均池化或全局最大池化），以获得图级表示，然后将其输入分类器或回归器。'
- en: GCNs are a powerful and widely used type of GNN that can effectively learn node
    representations by incorporating both node features and graph structure. They
    have shown strong performance on various graph-based tasks and can be adapted
    for node-level, edge-level, and graph-level problems.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: GCN是强大且广泛使用的图神经网络（GNN）类型，通过结合节点特征和图结构，能够有效地学习节点表示。它们在各种基于图的任务中表现出色，且可以适应节点级、边级和图级的问题。
- en: Over time, many optimizations over vanilla GCNs have been proposed and utilized
    in the industry. One such optimization, especially for scaling the graph learning
    process, is GraphSAGE.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，许多对原始GCN（图卷积网络）进行优化的方案相继被提出，并在行业中得到了应用。其中一种优化方法，尤其适用于扩展图学习过程的，是GraphSAGE。
- en: GraphSAGE
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GraphSAGE
- en: '**GraphSAGE** introduces a scalable and adaptive approach to graph representation
    learning, addressing some limitations of GCN and enhancing scalability. At its
    core, GraphSAGE employs a neighborhood sampling and aggregation strategy, diverging
    from the fixed-weight aggregation mechanism of GCN.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**GraphSAGE**引入了一种可扩展且自适应的图表示学习方法，解决了GCN的一些局限性，并增强了其可扩展性。GraphSAGE的核心在于采用了邻域采样与聚合策略，突破了GCN固定权重聚合机制的限制。'
- en: 'In GraphSAGE, the process of learning node representations involves iteratively
    sampling and aggregating information from local neighborhoods. Let ![<mml:math  ><mml:mi>G</mml:mi><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/191.png)
    be a graph with nodes ![<mml:math  ><mml:mi>V</mml:mi></mml:math>](img/1.png)
    and edges ![<mml:math  ><mml:mi>E</mml:mi></mml:math>](img/2.png) , and ![<mml:math  ><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/194.png)
    denote the embedding of node ![<mml:math  ><mml:mi>i</mml:mi></mml:math>](img/195.png)
    at layer ![<mml:math  ><mml:mi>l</mml:mi></mml:math>](img/176.png) . The update
    rule for GraphSAGE can be expressed as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在GraphSAGE中，学习节点表示的过程涉及反复从局部邻域中采样和聚合信息。设![<mml:math  ><mml:mi>G</mml:mi><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/191.png)是一个图，其中包含节点![<mml:math  ><mml:mi>V</mml:mi></mml:math>](img/1.png)和边![<mml:math  ><mml:mi>E</mml:mi></mml:math>](img/2.png)，且![<mml:math  ><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/194.png)表示节点![<mml:math  ><mml:mi>i</mml:mi></mml:math>](img/195.png)在第![<mml:math  ><mml:mi>l</mml:mi></mml:math>](img/176.png)层的嵌入。GraphSAGE的更新规则可以表示如下：
- en: '![<mml:math   display="block"><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi
    mathvariant="normal">​</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mi
    mathvariant="normal">​</mml:mi><mml:mo>,</mml:mo><mml:mo>∀</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/197.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math   display="block"><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi
    mathvariant="normal">​</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mi
    mathvariant="normal">​</mml:mi><mml:mo>,</mml:mo><mml:mo>∀</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/197.png)'
- en: Here, ![<mml:math  ><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/198.png)
    represents a dynamically sampled subset of neighbors for node ![<mml:math  ><mml:mi>i</mml:mi></mml:math>](img/199.png)
    at each iteration. This adaptability allows GraphSAGE to scale more efficiently
    compared to GCN, especially in scenarios where the graph is large or when computational
    resources are limited, maintaining scalability.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![<mml:math  ><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/198.png)表示节点
    ![<mml:math  ><mml:mi>i</mml:mi></mml:math>](img/199.png)在每次迭代中动态采样的邻居子集。这种适应性使得GraphSAGE比GCN在大规模图或计算资源有限的情况下更高效，保持了可扩展性。
- en: The structure for PyG code remains the same as for the GCN; we will just be
    using the **GraphSAGE** module from **torch_geometric.nn** .
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: PyG代码的结构与GCN相同；我们只是使用**torch_geometric.nn**中的**GraphSAGE**模块。
- en: 'To modify the previous code to use GraphSAGE instead of GCN, you need to make
    a few changes. Here are the lines you need to update:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要修改之前的代码以使用GraphSAGE而非GCN，你需要做一些更改。以下是需要更新的代码行：
- en: 'Replace the **import** statement for **GCNConv** with **SAGEConv** :'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将**GCNConv**的**import**语句替换为**SAGEConv**：
- en: '[PRE7]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Update the **GCN** model class to use **SAGEConv** layers instead of **GCNConv**
    :'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新**GCN**模型类，使用**SAGEConv**层代替**GCNConv**：
- en: '[PRE8]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Update the **model** creation line to use the **GraphSAGE** model instead of
    **GCN** :'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新**model**创建行，以使用**GraphSAGE**模型而不是**GCN**：
- en: '[PRE9]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With these changes, the code will now use the GraphSAGE model instead of GCN.
    The rest of the code, including loading the dataset, training, and evaluation,
    remains the same.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 做出这些更改后，代码将使用GraphSAGE模型而不是GCN。其余代码，包括数据集加载、训练和评估，保持不变。
- en: '![Figure 4.2 – GraphSAGE network leveraging neighborhood sampling](img/B22118_04_2.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – 利用邻域采样的GraphSAGE网络](img/B22118_04_2.jpg)'
- en: Figure 4.2 – GraphSAGE network leveraging neighborhood sampling
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 利用邻域采样的GraphSAGE网络
- en: At times, it is a good idea to assign different weights to neighbors based on
    their relevance to a particular node. We will now look at GATs, which borrow the
    concept of attention from language models.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，根据邻居与特定节点的相关性，为邻居分配不同的权重是一个好主意。我们现在来看一下GAT，它借用了语言模型中的注意力概念。
- en: GATs
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAT
- en: '**GATs** are an extension of GCNs that incorporate an attention mechanism to
    assign different weights to neighboring nodes based on their relevance. While
    GCNs apply a fixed aggregation function to combine the features of neighboring
    nodes, GATs allow for a more flexible and adaptive approach by learning the importance
    of each neighbor during the aggregation process. The core of GATs is the attention
    network.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**GAT** 是**GCN**的扩展，结合了注意力机制，根据邻居节点的相关性为其分配不同的权重。虽然**GCN**应用固定的聚合函数来组合邻居节点的特征，**GAT**通过在聚合过程中学习每个邻居的重要性，提供了更灵活和自适应的方法。**GAT**的核心就是注意力网络。'
- en: Attention networks
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力网络
- en: An **attention network** , often referred to as an **attention mechanism** or
    **attention model** , is a powerful concept in machine learning and artificial
    intelligence, particularly in the field of neural networks. It’s inspired by how
    human attention works – focusing on specific parts of input data while processing
    information.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意力网络**，通常被称为**注意力机制**或**注意力模型**，是机器学习和人工智能中的一个强大概念，特别是在神经网络领域。它的灵感来源于人类注意力的工作方式——在处理信息时专注于输入数据的特定部分。'
- en: An attention mechanism allows the model to dynamically focus on different parts
    of the input data, assigning varying degrees of importance or attention to each
    part. This enables the model to weigh the relevance of different inputs when making
    predictions or decisions.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制使模型能够动态地关注输入数据的不同部分，赋予每个部分不同程度的重要性或关注度。这使得模型在做出预测或决策时，可以权衡不同输入的相关性。
- en: Attention networks are commonly used in tasks involving sequential data, such
    as **natural language processing** ( **NLP** ) tasks such as machine translation,
    text summarization, and sentiment analysis. In these tasks, the model needs to
    process sequences of words or tokens and understand the contextual relationships
    between them. By using attention mechanisms, the model can effectively capture
    long-range dependencies and attend to relevant parts of the input sequence.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力网络通常用于涉及序列数据的任务，如**自然语言处理**（**NLP**）任务，包括机器翻译、文本摘要和情感分析。在这些任务中，模型需要处理一系列单词或标记，并理解它们之间的上下文关系。通过使用注意力机制，模型可以有效地捕捉长程依赖，并关注输入序列中相关的部分。
- en: 'Here’s how an attention mechanism is leveraged in graph learning:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在图学习中如何利用注意力机制：
- en: In GATs, the attention mechanism is used to compute attention coefficients between
    a node and its neighbors.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在GATs（图注意力网络）中，注意力机制用于计算节点与其邻居之间的注意力系数。
- en: Attention coefficients represent the importance of each neighbor’s features
    to the target node.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力系数表示每个邻居特征对目标节点的重要性。
- en: The attention mechanism is typically implemented using an MLP or a single-layer
    neural network.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制通常通过MLP（多层感知机）或单层神经网络来实现。
- en: Attention coefficients are computed based on the learned weights of the attention
    mechanism and the features of the target node and its neighbors.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力系数是基于学习到的注意力机制的权重以及目标节点和其邻居的特征来计算的。
- en: Let’s look at how we can compute the attention coefficient in a graph setting.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在图的设置中计算注意力系数。
- en: Attention coefficients computation
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力系数的计算
- en: 'For each node, the attention coefficients are computed for all its neighbors.
    The attention coefficient between node ![<mml:math  ><mml:mi>i</mml:mi></mml:math>](img/195.png)
    and its neighbor ![<math ><mrow><mi>j</mi></mrow></math>](img/201.png) is calculated
    as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个节点，计算其所有邻居的注意力系数。节点 ![<mml:math  ><mml:mi>i</mml:mi></mml:math>](img/195.png)
    和其邻居 ![<math ><mrow><mi>j</mi></mrow></math>](img/201.png) 之间的注意力系数按以下方式计算：
- en: '![<math  display="block"><mrow><mrow><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mfenced
    open="(" close=")"><mrow><mi>L</mi><mi>e</mi><mi>a</mi><mi>k</mi><mi>y</mi><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mfenced
    open="(" close=")"><mrow><msup><mi>W</mi><mi>T</mi></msup><mo>⋅</mo><mfenced open="["
    close="]"><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>∥</mo><msub><mi>h</mi><mi>j</mi></msub></mrow></mfenced></mrow></mfenced></mrow></mfenced></mrow><mfenced
    open="(" close=")"><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><mi>exp</mi><mfenced
    open="(" close=")"><mrow><mi>L</mi><mi>e</mi><mi>a</mi><mi>k</mi><mi>y</mi><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mfenced
    open="(" close=")"><mrow><msup><mi>W</mi><mi>T</mi></msup><mo>⋅</mo><mfenced open="["
    close="]"><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>|</mo><msub><mi>h</mi><mi>k</mi></msub></mrow></mfenced></mrow></mfenced></mrow></mfenced></mrow></mrow></mfenced></mfrac></mrow></mrow></math>](img/202.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![<math  display="block"><mrow><mrow><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mfenced
    open="(" close=")"><mrow><mi>L</mi><mi>e</mi><mi>a</mi><mi>k</mi><mi>y</mi><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mfenced
    open="(" close=")"><mrow><msup><mi>W</mi><mi>T</mi></msup><mo>⋅</mo><mfenced open="["
    close="]"><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>∥</mo><msub><mi>h</mi><mi>j</mi></msub></mrow></mfenced></mrow></mfenced></mrow></mfenced></mrow><mfenced
    open="(" close=")"><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><mi>exp</mi><mfenced
    open="(" close=")"><mrow><mi>L</mi><mi>e</mi><mi>a</mi><mi>k</mi><mi>y</mi><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mfenced
    open="(" close=")"><mrow><msup><mi>W</mi><mi>T</mi></msup><mo>⋅</mo><mfenced open="["
    close="]"><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>|</mo><msub><mi>h</mi><mi>k</mi></msub></mrow></mfenced></mrow></mfenced></mrow></mfenced></mrow></mrow></mfenced></mfrac></mrow></mrow></math>](img/202.png)'
- en: '![<mml:math  ><mml:mo>∥</mml:mo></mml:math>](img/203.png) is the concatenation
    operation, and ![<mml:math  ><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi></mml:math>](img/204.png)
    is the leaky ReLU activation function.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math  ><mml:mo>∥</mml:mo></mml:math>](img/203.png) 是连接操作，而 ![<mml:math  ><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi></mml:math>](img/204.png)
    是泄漏ReLU激活函数。'
- en: '![<mml:math  ><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/205.png)
    and ![<mml:math  ><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/206.png)
    are the learned node embeddings for nodes ![<mml:math  ><mml:mi>i</mml:mi></mml:math>](img/195.png)
    and ![<mml:math  ><mml:mi>j</mml:mi></mml:math>](img/208.png) , respectively.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math  ><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:msub></mml:math>](img/205.png)
    和 ![<mml:math  ><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:msub></mml:math>](img/206.png)
    分别表示节点 ![<mml:math  ><mml:mi>i</mml:mi></mml:math>](img/195.png) 和 ![<mml:math  ><mml:mi>j</mml:mi></mml:math>](img/208.png)
    的学习到的节点嵌入表示。'
- en: '![<mml:math  ><mml:mi>W</mml:mi></mml:math>](img/142.png) is a learnable attention
    weight vector that is shared across all nodes.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math  ><mml:mi>W</mml:mi></mml:math>](img/142.png) 是一个可学习的注意力权重向量，该向量在所有节点中共享。'
- en: The attention coefficients are normalized using the **softmax** function to
    ensure they sum up to 1 for each node.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力系数使用**softmax**函数进行归一化，以确保每个节点的系数和为1。
- en: Now that we understand how to compute attention coefficients, let’s see how
    these are utilized in the aggregation step of a GNN.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了如何计算注意力系数，接下来让我们看看这些系数是如何在 GNN 的聚合步骤中被利用的。
- en: Aggregation of neighbor features
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 邻居特征的聚合
- en: Once the attention coefficients are computed, the features of the neighboring
    nodes are aggregated using a weighted sum.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算出注意力系数，就使用加权和对邻居节点的特征进行聚合。
- en: 'The aggregated features for node ![<mml:math  ><mml:mi>i</mml:mi></mml:math>](img/195.png)
    are calculated as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 节点 ![<mml:math  ><mml:mi>i</mml:mi></mml:math>](img/195.png)的聚合特征计算如下：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mi>ϵ</mml:mi><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal"> </mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mi
    mathvariant="normal">*</mml:mi><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/211.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mi>ϵ</mml:mi><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal"> </mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mi
    mathvariant="normal">*</mml:mi><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/211.png)'
- en: Here, ![<mml:math  ><mml:mi mathvariant="normal"> </mml:mi><mml:mi>σ</mml:mi></mml:math>](img/212.png)
    is a non-linear activation function, such as ReLU. The aggregated features ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/213.png)
    represent the updated representation of node ![<mml:math  ><mml:mi mathvariant="normal"> </mml:mi><mml:mi>i</mml:mi></mml:math>](img/214.png)
    after considering the importance of its neighbors.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![<mml:math  ><mml:mi mathvariant="normal"> </mml:mi><mml:mi>σ</mml:mi></mml:math>](img/212.png)
    是一种非线性激活函数，如 ReLU。聚合特征 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/213.png)
    代表了节点 ![<mml:math  ><mml:mi mathvariant="normal"> </mml:mi><mml:mi>i</mml:mi></mml:math>](img/214.png)
    在考虑了邻居的重要性后的更新表示。
- en: '![  Figure 4.3 – Multi-head attention (with K = 3 heads) by node 1 on its neighborhood](img/B22118_04_3.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![  图 4.3 – 节点 1 对其邻居的多头注意力（K = 3 头）](img/B22118_04_3.jpg)'
- en: Figure 4.3 – Multi-head attention (with K = 3 heads) by node 1 on its neighborhood
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 – 由节点1在其邻域上执行的多头注意力（K = 3个头）
- en: To capture multiple aspects of the relationships between two nodes, we can leverage
    the concept of multi-head attention.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉节点之间关系的多个方面，我们可以利用多头注意力的概念。
- en: Multi-head attention
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头注意力
- en: In GNNs, **multi-head attention** can be applied to learn representations of
    nodes as an extension of vanilla attention. Each “head” can be seen as a different
    perspective or attention mechanism applied to a node’s neighborhood. By running
    multiple heads in parallel, the GNN can capture diverse aspects of the node’s
    local graph structure and feature space. This allows the model to aggregate information
    from neighboring nodes in multiple ways, enhancing its ability to learn informative
    node representations that incorporate various patterns and relationships within
    the graph.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在GNN中，**多头注意力**可以作为传统注意力的扩展，应用于学习节点的表示。每个“头”可以看作是应用于节点邻域的不同视角或注意力机制。通过并行运行多个头，GNN可以捕捉节点局部图结构和特征空间的多样化方面。这使得模型能够以多种方式聚合来自邻居节点的信息，从而增强其学习节点表示的能力，这些表示融入了图中多种模式和关系。
- en: 'There are several notable aspects to keep in mind for multi-head attention:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在多头注意力中，有几个需要注意的方面：
- en: GATs can employ multi-head attention to capture different aspects of the node
    relationships.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAT可以采用多头注意力来捕捉节点关系的不同方面。
- en: In multi-head attention, multiple attention mechanisms are used in parallel,
    each with its own set of learnable parameters.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多头注意力中，多个注意力机制并行使用，每个机制都有自己的一组可学习参数。
- en: The output features from each attention head are concatenated or averaged to
    obtain the final node representations.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个注意力头的输出特征会被连接或平均，以获得最终的节点表示。
- en: Multi-head attention allows the model to learn diverse patterns and capture
    different types of dependencies between nodes.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多头注意力使得模型能够学习多样的模式，并捕捉节点之间不同类型的依赖关系。
- en: At times, a single attention layer might be unable to capture complex relationships
    in a graph. Stacking multiple layers can help improve the learning space.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，单个注意力层可能无法捕捉图中复杂的关系。堆叠多个层可以帮助改善学习空间。
- en: Stacking GAT layers
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠GAT层
- en: 'We can also use multiple GAT layers, similar to GCNs:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用多个GAT层，类似于GCN：
- en: Like GCNs, GAT layers can be stacked to capture higher-order dependencies and
    learn more abstract representations of the graph.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与GCN类似，GAT层可以堆叠以捕捉更高阶的依赖关系，并学习图的更抽象表示。
- en: In each layer, the updated node representations from the previous layer serve
    as input to the next layer.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每一层中，来自前一层的更新节点表示作为输入传递到下一层。
- en: The final node representations obtained after multiple GAT layers can be used
    for downstream tasks such as node classification or graph classification.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个GAT层之后获得的最终节点表示可以用于下游任务，例如节点分类或图分类。
- en: GATs seamlessly combine the advantages of an **adaptive receptive field** and
    an **interpretable attention mechanism** , making them powerful tools for processing
    graph-structured data. The adaptive receptive field of GATs allows nodes to dynamically
    adjust their focus on relevant neighbors during information aggregation. Importantly,
    GATs provide interpretable attention coefficients, enabling a clear understanding
    of the model’s decision-making process. The transparency in attention weights
    allows for intuitive insights into which neighbors contribute significantly to
    a node’s representation, fostering interpretability and facilitating model debugging.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: GAT无缝地结合了**自适应感受野**和**可解释的注意力机制**的优点，使其成为处理图结构数据的强大工具。GAT的自适应感受野使得节点在信息聚合过程中可以动态调整其关注相关邻居的程度。更重要的是，GAT提供了可解释的注意力系数，使得我们能够清楚地理解模型的决策过程。注意力权重的透明性使我们能够直观地了解哪些邻居在节点表示中贡献显著，从而促进了模型的可解释性并便于调试。
- en: This combination of adaptability and interpretability makes GATs effective in
    capturing fine-grained local information while maintaining a global perspective,
    contributing to their success in various graph-based tasks.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这种适应性与可解释性的结合使得GAT在捕捉精细的局部信息的同时，保持全局视角，从而在各种基于图的任务中取得成功。
- en: 'Let’s look at the model code for GAT:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看GAT的模型代码：
- en: '[PRE10]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In this code, the GAT model class is defined using two **GATConv** layers. The
    first layer has multiple attention heads specified by the **heads** parameter,
    while the second layer has a single attention head. The activation function used
    is the **exponential linear unit** ( **ELU** ) function.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，GAT 模型类使用了两个**GATConv** 层来定义。第一层有多个由 **heads** 参数指定的注意力头，而第二层只有一个注意力头。使用的激活函数是
    **指数线性单元**（**ELU**）函数。
- en: Note that in the **__init__** method of the GAT class, we multiply **hidden_channels**
    by **heads** when specifying the input channels for the second **GATConv** layer.
    This is because the output of the first layer has **hidden_channels * heads dimensions**
    due to the multiple attention heads.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在 GAT 类的 **__init__** 方法中，我们在指定第二个 **GATConv** 层的输入通道时，将 **hidden_channels**
    乘以 **heads**。这是因为第一层的输出由于多个注意力头的原因，具有 **hidden_channels * heads 的维度**。
- en: Summary
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we provided a comprehensive overview of graph-based deep learning
    models, starting with the fundamental concept of message passing and then delving
    into specific GNN architectures such as GCNs, GraphSAGE, and GATs.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们提供了关于基于图的深度学习模型的全面概述，从信息传递的基本概念开始，然后深入探讨了特定的 GNN 架构，如 GCN、GraphSAGE 和
    GAT。
- en: Graph-based models rely on message passing, a key operation where nodes exchange
    information with neighbors to update their representations. GCNs perform convolutions
    on graphs, aggregating neighboring node information to learn node representations.
    GraphSAGE efficiently generates embeddings for large-scale graphs through neighborhood
    sampling. GATs integrate attention mechanisms, enabling nodes to assign varying
    importance weights to neighbors during message passing. These techniques enhance
    the capacity of graph-based models to capture complex relationships and patterns
    within data structures.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的模型依赖于信息传递，这是一个关键操作，其中节点与邻居交换信息，以更新它们的表示。GCN 在图上执行卷积，聚合邻居节点的信息来学习节点表示。GraphSAGE
    通过邻域采样高效地为大规模图生成嵌入。GAT 整合了注意力机制，使得节点在信息传递过程中能够为邻居分配不同的重要性权重。这些技术增强了基于图的模型捕捉数据结构中复杂关系和模式的能力。
- en: Building upon the foundational understanding of prevalent graph learning algorithms,
    we’ll explore the contemporary challenges confronting GNNs in the upcoming chapter.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在对流行的图学习算法有了基础理解后，我们将在接下来的章节中探讨 GNN 面临的当代挑战。
