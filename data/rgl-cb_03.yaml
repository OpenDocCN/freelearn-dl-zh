- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Regularization with Linear Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性模型的正则化
- en: A huge part of **machine learning** (**ML**) is made up of linear models. Although
    sometimes considered less powerful than their nonlinear counterparts (such as
    tree-based models or deep learning models), linear models do address many concrete,
    valuable problems. Customer churn and advertising optimization are just a couple
    of problems where linear models may be the right solution.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）的一个重要部分是由线性模型构成的。尽管有时它们被认为不如非线性模型（如基于树的模型或深度学习模型）强大，但线性模型确实能解决许多具体且有价值的问题。客户流失和广告优化就是线性模型可能是正确解决方案的两个问题。'
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下内容：
- en: Training a linear regression with scikit-learn
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用scikit-learn训练线性回归模型
- en: Regularizing with ridge regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用岭回归进行正则化
- en: Regularizing with lasso regression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用套索回归进行正则化
- en: Regularizing with elastic net regression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用弹性网回归进行正则化
- en: Training a logistic regression model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练逻辑回归模型
- en: Regularizing a logistic regression model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化逻辑回归模型
- en: Choosing the right regularization
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择正确的正则化方法
- en: By the end of this chapter, we will have learned how to use and regularize some
    of the most commonly used linear models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将学习如何使用并正则化一些最常用的线性模型。
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, besides loading data, you will learn how to fit and compute
    inferences with several linear models. In order to do so, the following libraries
    are required:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，除了加载数据外，您将学习如何使用多个线性模型进行拟合和推理计算。为此，以下库是必需的：
- en: NumPy
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: Matplotlib
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib
- en: Scikit-learn
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn
- en: Training a linear regression model with scikit-learn
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 训练线性回归模型
- en: Linear regression is one the most basic ML models we can use, but it is very
    useful. Most people used linear regression in high school without talking about
    ML, and still use it on a regular basis within spreadsheets. In this recipe, we
    will explain the basics of linear regression, and then train and evaluate a linear
    regression model using scikit-learn on the California housing dataset.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是我们可以使用的最基础的机器学习模型之一，但它非常有用。大多数人在高中时就使用过线性回归，虽然当时并没有讨论机器学习，而且他们在电子表格中也经常使用它。在这个教程中，我们将解释线性回归的基础知识，然后使用
    scikit-learn 在加利福尼亚房价数据集上训练并评估一个线性回归模型。
- en: Getting ready
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Linear regression is not a complicated model, but it is still useful to understand
    what is under the hood to get the best out of it.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归不是一个复杂的模型，但理解其底层原理仍然非常有用，这样才能从中获得最佳效果。
- en: The way linear regression works is pretty straightforward. Heading back to the
    real estate price example, if we consider a feature *x* such as the apartment
    surface and a label *y* such as the apartment price, a common solution would be
    to find *a* and *b* such that *y = ax +* *b*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的工作原理非常简单。回到房地产价格的例子，如果我们考虑一个特征 *x*，比如公寓的面积和一个标签 *y*，比如公寓价格，一个常见的解决方案是找到
    *a* 和 *b* 使得 *y = ax + b*。
- en: Unfortunately, this is not so simple in real life. There is usually no *a* and
    *b* that makes this equality always respected. It is more likely that we can define
    a function `h(x)` that aims to give a value as close as possible to *y*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，现实生活中并非如此简单。通常没有一个 *a* 和 *b* 使得这个等式始终成立。更可能的情况是，我们可以定义一个函数 `h(x)`，旨在给出尽可能接近
    *y* 的值。
- en: Also, we may have not just one feature *x*, but several features *x*1, *x*2,…,
    *x*n, representing apartment surface, location, floor, number of rooms, exponent
    features, and so on.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可能不仅仅有一个特征 *x*，而是有多个特征 *x*1、*x*2、...、*x*n，代表公寓面积、位置、楼层、房间数量、指数特征等等。
- en: 'By this logic, we would end up with a prediction *h(x)* that may look like
    the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这个逻辑，我们最终得到的预测 *h(x)* 可能会像下面这样：
- en: '![](img/Formula_03_001.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_001.jpg)'
- en: '![](img/Formula_03_002.png)is the weight associated to the feature ![](img/Formula_03_003.png)
    *x*j, and *b* is a bias term. This is just a generalization of the previous *y
    = ax + b* to *n* features. This formula allows a linear regression to predict
    virtually any real number.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/Formula_03_002.png) 是与特征 ![](img/Formula_03_003.png) *x*j 相关的权重，*b*
    是一个偏置项。这只是之前 *y = ax + b* 公式的一个推广，扩展到 *n* 个特征。这一公式允许线性回归预测几乎任何实数值。'
- en: The goal of our ML model is to find the set of *w* and *b* values that minimizes
    prediction errors on the training set. By this, we mean finding the parameters
    *w* and *b* so that *h(x)* and *y* are as close as possible.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的机器学习模型的目标是找到一组 *w* 和 *b* 的值，使得训练集上的预测误差最小。也就是说，我们要找到参数 *w* 和 *b*，使得 *h(x)*
    和 *y* 尽可能接近。
- en: 'One way to achieve that is to minimize the loss *L*, that can be defined here
    as a slightly modified **mean squared** **error** (**MSE**):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一目标的一种方法是最小化损失 *L*，这里可以将其定义为稍作修改的**均方** **误差**（**MSE**）：
- en: '![](img/Formula_03_004.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_004.jpg)'
- en: '![](img/Formula_03_005.png) is the ground truth of the sample *i* in the training
    set, and *m* is the number of samples in the training set.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/Formula_03_005.png) 是训练集中样本 *i* 的真实值，*m* 是训练集中的样本数量。'
- en: Note
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The loss is usually a representation of the difference between the ground truth
    and the predictions. Hence, minimizing the loss allows the model to predict values
    that are as close as possible to the ground truth.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 损失通常表示真实值与预测值之间的差异。因此，最小化损失可以让模型预测的值尽可能接近真实值。
- en: 'Minimizing this mean squared error would allow us to find the set of *w* and
    *b* values so that the prediction *h(x)* is as close as possible to the ground
    truth *y*. Schematically, this can be represented as finding the *w* that minimizes
    the loss, as shown in the following figure:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化均方误差可以帮助我们找到一组 *w* 和 *b* 的值，使得预测 *h(x)* 尽可能接近真实值 *y*。从示意图来看，这可以表示为找到最小化损失的
    *w*，如以下图所示：
- en: '![Figure 3.1 – Loss function as a function of a parameter theta, having a global
    minimum at the cross](img/B19629_03_01.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – 损失函数作为参数 theta 的函数，在交点处具有全局最小值](img/B19629_03_01.jpg)'
- en: Figure 3.1 – Loss function as a function of a parameter theta, having a global
    minimum at the cross
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – 损失函数作为参数 theta 的函数，在交点处具有全局最小值
- en: 'The next question is: how do we find the set of values to minimize the loss?
    There are several ways of solving this problem. One commonly used technique in
    ML is gradient descent.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个问题是：我们如何找到最小化损失的值集？解决这个问题有多种方法。一种在机器学习中常用的技术是梯度下降。
- en: What is gradient descent? In a few words, it is going down the curve to the
    minimum value in the preceding figure.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是梯度下降？简而言之，它是沿着曲线下降至前面图示中的最小值。
- en: 'How does this work? It’s a multi-step process:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这怎么运作？这是一个多步骤的过程：
- en: Start with random values of the parameters *w* and *b*. Random values are usually
    defined using normal distribution centered on zero. This is why having scaled
    features may help significantly for convergence.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从随机的 *w* 和 *b* 参数值开始。随机值通常使用以零为中心的正态分布来定义。因此，缩放特征可能有助于收敛。
- en: Compute the loss for the given data and current values of *w* and *b*. As defined
    earlier, we may use the mean squared error to compute the loss *L*.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算给定数据和当前 *w*、*b* 值的损失。如前所定义，我们可以使用均方误差来计算损失 *L*。
- en: 'The following figure is a good representation of the situation at this point:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表示了此时的情形：
- en: '![Figure 3.2 – The loss function with the global minimum at the red cross,
    and a possible random initial state at the blue cross](img/B19629_03_02.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – 损失函数在红色交点处具有全局最小值，蓝色交点处为可能的随机初始状态](img/B19629_03_02.jpg)'
- en: Figure 3.2 – The loss function with the global minimum at the red cross, and
    a possible random initial state at the blue cross
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 损失函数在红色交点处具有全局最小值，蓝色交点处为可能的随机初始状态
- en: 'Compute the loss gradient with respect to each parameter ![](img/Formula_03_006.png)
    . This is nothing more than the slope of the loss for a given parameter, which
    can be computed with the following equations:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算相对于每个参数的损失梯度 ![](img/Formula_03_006.png)。这不过是给定参数的损失的斜率，可以通过以下公式计算：
- en: '![](img/Formula_03_007.jpg)![](img/Formula_03_008.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_007.jpg)![](img/Formula_03_008.jpg)'
- en: Note
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: One may notice that the slope is expected to decrease as we get closer to the
    minimum. Indeed, as we get close to the minimum, the error tends to zero and so
    does the slope, based on these equations.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 可以注意到，随着接近最小值，斜率预计会减小。事实上，随着接近最小值，误差趋向于零，斜率也趋向于零，这基于这些公式。
- en: 'Apply gradient descent to parameters. Apply the gradient descent to parameters,
    with a user-defined learning rate *α*. This is computed using the following formulas:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对参数应用梯度下降。对参数应用梯度下降，使用用户定义的学习率 *α*。这可以通过以下公式计算：
- en: '![](img/Formula_03_009.jpg)![](img/Formula_03_010.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_009.jpg)![](img/Formula_03_010.jpg)'
- en: 'This allows us to take a step toward the minimum, as represented in the following
    figure:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我们朝着最小值迈出一步，如下图所示：
- en: '![Figure 3.3 – The gradient descent allows us to take one step down the loss
    function, allowing us to get closer to the global minimum](img/B19629_03_03.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3 – 梯度下降法允许我们沿着损失函数向下迈进一步，从而使我们更接近全局最小值](img/B19629_03_03.jpg)'
- en: Figure 3.3 – The gradient descent allows us to take one step down the loss function,
    allowing us to get closer to the global minimum
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – 梯度下降法允许我们沿着损失函数向下迈进一步，从而使我们更接近全局最小值。
- en: 'Iterate through *steps 2 to 4* until convergence or max iteration. This would
    allow us to reach the optimal parameters, as represented in the following figure:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代执行 *步骤 2 到 4*，直到收敛或达到最大迭代次数。这将使我们达到最优参数，如下图所示：
- en: '![Figure 3.4 – With enough iterations and a convex loss function, the parameters
    will converge to the global minimum](img/B19629_03_04.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – 在足够的迭代次数和凸损失函数下，参数将会收敛到全局最小值](img/B19629_03_04.jpg)'
- en: Figure 3.4 – With enough iterations and a convex loss function, the parameters
    will converge to the global minimum
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 在足够的迭代次数和凸损失函数下，参数将会收敛到全局最小值
- en: Note
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注：
- en: A learning rate *α* that is too large would miss the global minimum, or even
    diverge, while one that is too small would take forever to converge.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果学习率 *α* 太大，可能会错过全局最小值，甚至导致发散；而如果学习率太小，收敛速度会非常慢。
- en: 'To complete this recipe, the following libraries have to be installed: `numpy`,
    `matplotlib`, and `sklearn`. They can be installed with `pip` in the terminal,
    with the following command line:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个步骤，必须安装以下库：`numpy`、`matplotlib` 和 `sklearn`。你可以在终端使用 `pip` 安装，命令如下：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How to do it…
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: Fortunately, all this procedure is fully implemented in scikit-learn, and the
    only thing you need to do is fully reuse this library. Let’s now train a linear
    regression on the California housing dataset provided by scikit-learn.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，整个过程在 scikit-learn 中已经完全实现，你所需要做的就是充分利用这个库。现在我们来训练一个线性回归模型，使用 scikit-learn
    提供的加利福尼亚住房数据集。
- en: '`fetch_california_housing`: A function that allows us to load the dataset'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`fetch_california_housing`：一个可以加载数据集的函数'
- en: '`train_test_split`: A function that allows us to split the data'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`train_test_split`：一个可以将数据拆分的函数'
- en: '`StandardScaler`: A class that allows us to rescale the data'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`StandardScaler`：允许我们重新缩放数据的类'
- en: '`LinearRegression`: The class that contains the implementation of the linear
    regression'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`LinearRegression`：包含线性回归实现的类'
- en: 'Here is what the code looks like:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码示例：
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`X` with `X*X`:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`X` 与 `X*X`：'
- en: '[PRE2]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`train_test_split` function, with `test_size=0.2`, meaning we end up having
    80% of the data in the training set, and 20% in the test set, split at random.
    This is shown here:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`train_test_split` 函数，使用 `test_size=0.2`，意味着我们将数据随机拆分，80% 的数据进入训练集，20% 的数据进入测试集。如下所示：'
- en: '[PRE5]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Prepare the data**: Since we have only quantitative features here, the only
    preparation we need is rescaling. We can use the standard scaler of scikit-learn.
    We need to instantiate it, then fit it on the training set and transform the training
    set, and finally we transform the test set. Feel free to use any other rescaler:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**准备数据**：由于我们这里只有量化特征，因此唯一需要的准备工作是重新缩放。我们可以使用 scikit-learn 的标准缩放器。我们需要实例化它，然后在训练集上拟合并转换训练集，最后转换测试集。你可以自由使用任何其他的重缩放器：'
- en: '[PRE8]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Fit the model on the training set. The model must be instantiated, and here
    we use the default parameters, so nothing is specified. Once the model is instantiated,
    we can use the `.fit()` method on the training set:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集上拟合模型。模型必须先实例化，这里我们使用默认参数，因此没有指定任何内容。实例化模型后，我们可以使用 `.fit()` 方法对训练集进行拟合：
- en: '[PRE12]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Evaluate the model on both the training and test set. Here, we use the `.score()`
    method of the `LinearRegression` class, which provides `R2-score`, but you can
    use any other metric provided in `sklearn.metrics` that suits regression:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集和测试集上评估模型。这里，我们使用 `LinearRegression` 类的 `.score()` 方法，它提供了 `R2-score`，但你也可以使用
    `sklearn.metrics` 中提供的任何其他回归度量：
- en: '[PRE15]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here is the output:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As we can see, there is a significant difference between the train and test
    set’s scores, indicating model overfitting on the train set. To address this problem,
    regularization techniques will be proposed in the following recipes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，训练集和测试集的分数差异显著，表明模型在训练集上出现了过拟合。为了解决这个问题，接下来将介绍正则化技术。
- en: There’s more…
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'Once the model has been trained, we can access all the parameters *w* (here,
    16 values for 16 input features) as well as the intercept *b*, with the attributes,
    `.coef_`, and the `.intercept_` value of the `LinearRegression` object respectively:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们可以通过 `LinearRegression` 对象的属性 `.coef_` 和 `.intercept_` 分别访问所有的参数
    *w*（这里是16个值，对应16个输入特征）以及截距 *b*：
- en: '[PRE19]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here is the output:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '[PRE20]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If we plot these values, we will notice that their values range between approximately
    `-8` and `2` on this dataset:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们绘制这些值，我们会注意到它们在这个数据集中的值大约在 `-8` 和 `2` 之间：
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here is a visual representation of this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个可视化表示：
- en: '![Figure 3.5 – Learned values of each weight of the linear regression model.
    The range of the values is quite large, from -8 to 2](img/B19629_03_05.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – 线性回归模型中每个权重的学习值。值的范围相当大，从 -8 到 2](img/B19629_03_05.jpg)'
- en: Figure 3.5 – Learned values of each weight of the linear regression model. The
    range of the values is quite large, from -8 to 2
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 线性回归模型中每个权重的学习值。值的范围相当大，从 -8 到 2
- en: See also
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见其他
- en: 'To have a full understanding of how to use linear regression using scikit-learn,
    it is good practice to check the official documentation of the class: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要全面了解如何使用 scikit-learn 进行线性回归，最好参考该类的官方文档：[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml)。
- en: We now have a good understanding of how linear regression works, and we will
    see in the next section how to regularize it with penalization.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在对线性回归的工作原理有了较好的理解，接下来我们将看到如何通过惩罚进行正则化。
- en: Regularizing with ridge regression
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ridge 回归进行正则化
- en: A very common and useful way to regularize a linear regression is through penalization
    of the loss function. In this recipe, after reviewing what it means to add penalization
    to the loss function in the case of ridge regression, we will train a ridge model
    on the same California housing dataset as in the previous recipe, and see how
    it can improve the score thanks to regularization.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一种非常常见且有用的正则化线性回归的方法是通过惩罚损失函数。在本示例中，经过回顾将惩罚项添加到 ridge 回归的损失函数之后，我们将在与之前示例相同的加利福尼亚住房数据集上训练一个
    ridge 模型，看看它如何通过正则化提高评分。
- en: Getting ready
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'One way to make sure that a model’s parameters are not going to overfit is
    to keep them close to zero: if the parameters do not have the possibility to evolve
    freely, they are less likely to overfit.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 确保模型参数不会过拟合的一种方法是保持它们接近零：如果参数无法自由变化，它们就不太可能过拟合。
- en: 'To that end, ridge regression adds a new term (regularization term) to the
    loss ![](img/Formula_03_011.png):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，ridge 回归将一个新的项（正则化项）添加到损失函数中！[](img/Formula_03_011.png)：
- en: '![](img/Formula_03_012.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_012.jpg)'
- en: 'Where ![](img/Formula_03_013.png)is the *L2* norm of *w*:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/Formula_03_013.png) 是 *L2* 范数的 *w*：
- en: '![](img/Formula_03_014.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_014.jpg)'
- en: 'With this loss, we intuitively understand that high values of weights *w* are
    not possible, and thus overfitting is less likely. Also, *𝜆* is a hyperparameter
    (it can be fine-tuned) allowing us to control the regularization level:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个损失函数，我们可以直观地理解，较大的权重 *w* 是不可能的，因此过拟合的可能性较小。此外，*𝜆* 是一个超参数（可以微调），允许我们控制正则化的程度：
- en: A high value of *𝜆* means high regularization
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*𝜆* 的较高值表示较强的正则化'
- en: A value of *𝜆**=0* means no regularization, for example, regular linear regression
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*𝜆* = 0 表示没有正则化，例如普通的线性回归'
- en: 'The gradient descent formulas are slightly updated to the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的公式稍作更新，如下所示：
- en: '![](img/Formula_03_015.jpg)![](img/Formula_03_016.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_015.jpg)![](img/Formula_03_016.jpg)'
- en: 'Let’s now see how to use ridge regression with scikit-learn: the same libraries
    required in the previous recipe must be installed: `numpy`, `sklearn`, and `matplotlib`.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下如何使用 ridge 回归与 scikit-learn：与前一个示例中一样，需要安装相同的库：`numpy`、`sklearn` 和
    `matplotlib`。
- en: Also, we assume the data is already downloaded and prepared from the previous
    recipe. To download, split and prepare the data, refer to the previous recipe.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，我们假设数据已经通过上一示例下载并准备好。有关如何下载、拆分和准备数据的详细信息，请参考前一个示例。
- en: How to do it…
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Let’s assume we are reusing the same data from the previous recipe. We will
    just train and evaluate another model on this exact same data, including the feature
    engineering with the squared features. The related implementation in scikit-learn
    for ridge regression is the `Ridge` class, where the `alpha` class attribute is
    equivalent to the *𝜆* in the preceding equation. Let’s use it:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在重复使用前一个示例中的相同数据。我们将在完全相同的数据上训练和评估另一个模型，包括具有平方特征的特征工程。scikit-learn中岭回归的相关实现是`Ridge`类，其中`alpha`类属性等同于前述方程中的*𝜆*。让我们来使用它：
- en: 'Import the `Ridge` class from scikit-learn:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从scikit-learn中导入`Ridge`类：
- en: '[PRE22]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We then instantiate a ridge model. A regularization parameter of `alpha=5000`
    has been selected here, but every dataset may need a very specific hyperparameter
    value to perform best. Next, train the model on the training set (previously prepared)
    with the `.fit()` method, which is shown as follows:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们实例化一个岭回归模型。在这里选择了正则化参数`alpha=5000`，但每个数据集可能需要一个非常具体的超参数值来表现最佳。接下来，用`.fit()`方法在训练集上训练模型（事先准备好的），如下所示：
- en: '[PRE23]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We then evaluate the model. Here, we compute and display the R2-score provided
    by the `.score()` of the ridge class, but any other regression metric could be
    used:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们评估模型。在这里，我们计算并显示由岭回归类的`.score()`提供的R2分数，但也可以使用任何其他回归指标：
- en: '[PRE26]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here is the output:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE29]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We will notice that we are getting better results on the test set compared to
    the linear regression model (with no regularization) by allowing the R2-score
    on the test set to be slightly above `0.5`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会注意到，与线性回归模型（没有正则化）相比，我们在测试集上获得了更好的结果，允许测试集上的R2分数略高于`0.5`。
- en: There’s more…
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We can also print the weights and plot them, and compare those values to the
    ones of regular linear regression, as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以打印权重并绘制它们，将这些值与普通线性回归的值进行比较，如下所示：
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here is the output:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'For visualization, we can also plot these values with the following code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化，我们还可以使用以下代码绘制这些值：
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This code outputs the following:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码输出如下：
- en: '![Figure 3.6 – Learned values of each weight of the ridge regression model.
    Note that some are positive, some negative, and none are purely equal to zero.
    Also, the range is much smaller than without regularization](img/B19629_03_06.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图3.6 - 岭回归模型每个权重的学习值。请注意，一些是正数，一些是负数，并且没有一个完全等于零。此外，范围比无正则化时小得多](img/B19629_03_06.jpg)'
- en: Figure 3.6 – Learned values of each weight of the ridge regression model. Note
    that some are positive, some negative, and none are purely equal to zero. Also,
    the range is much smaller than without regularization
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 - 岭回归模型每个权重的学习值。请注意，一些是正数，一些是负数，并且没有一个完全等于零。此外，范围比无正则化时小得多
- en: The weight values are now ranging from `-0.2` to .`0.5`, which is indeed much
    smaller than with no penalization.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在权重值的范围是从`-0.2`到`.0.5`，确实比没有惩罚时要小得多。
- en: As expected, this adding regularization results in a test set R2-score that
    is much better and closer to the train set than without regularization.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，添加正则化导致测试集的R2分数比没有正则化时更好，且接近训练集的分数。
- en: See also
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'For more information about all the possible parameters and hyperparameters
    of the Ridge regression, you can take a look at the official scikit-learn documentation:
    [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 有关岭回归的所有可能参数和超参数的更多信息，您可以查看官方scikit-learn文档：[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml)。
- en: Regularizing with lasso regression
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用套索回归进行正则化
- en: '**Lasso** regression stands for **Least Absolute Shrinkage and Selection Operator**.
    This is a regularization method that is conceptually very close to ridge regression.
    In some cases, lasso regression outperforms ridge regression, which is why it’s
    useful to know what it does and how to use it. In this recipe, we will briefly
    explain what lasso regression is and then train a model using scikit-learn on
    the same California housing dataset.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**套索**回归代表**最小绝对值收缩和选择算子**。这是一个正则化方法，在某些情况下，套索回归优于岭回归，因此了解其作用和如何使用它非常有用。在这个示例中，我们将简要解释套索回归是什么，然后在相同的加利福尼亚房屋数据集上使用scikit-learn训练模型。'
- en: Getting ready
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Instead of using the L2-norm, lasso uses the L1-norm, so that the loss ![](img/Formula_03_017.png)
    is the following:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: lasso 使用 L1-norm，而不是 L2-norm，因此损失 ![](img/Formula_03_017.png) 公式如下：
- en: '![](img/Formula_03_018.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_018.jpg)'
- en: While ridge regression tends to decrease weights close to zero quite smoothly,
    lasso is more drastic. Lasso, having a much steeper loss, tends to set weights
    to zero quite quickly.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 ridge 回归倾向于平滑地将权重减少到接近零，lasso 的变化更加剧烈。由于 lasso 的损失曲线更陡峭，它倾向于迅速将权重设置为零。
- en: 'Just like the ridge regression recipe, we’ll use the same libraries and assume
    they are installed: `numpy`, `sklearn`, and `matplotlib`. Also, we’ll assume the
    data is already downloaded and prepared.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 ridge 回归的做法一样，我们将使用相同的库，并假设它们已经安装：`numpy`、`sklearn` 和 `matplotlib`。同时，我们假设数据已经下载并准备好。
- en: How to do it…
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'The scikit-learn implementation of lasso is available with the `Lasso` class.
    Like in the `Ridge` class, `alpha` is the term that allows control of this regularization:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 的 lasso 实现可以通过 `Lasso` 类获得。与 `Ridge` 类类似，`alpha` 是控制正则化强度的参数：
- en: The value of alpha is 0 means no regularization
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: alpha 值为 0 表示没有正则化
- en: A large value of alpha means high regularization
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: alpha 的大值意味着高强度的正则化
- en: 'Again, we will reuse the same, already prepared dataset that we used for linear
    regression and ridge regression:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用已经准备好的数据集，该数据集已经用于线性回归和 ridge 回归：
- en: 'Import the `Lasso` class from scikit-learn:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 scikit-learn 导入 `Lasso` 类：
- en: '[PRE33]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We instantiate a lasso model with a value of `alpha=0.2`, which provides pretty
    good results and low overfitting, as we will see right away. However, feel free
    to test other values, as each dataset may have its very unique optimal value.
    Next, train the model on the training set using the `.fit()` method of the `Lasso`
    class:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们实例化一个 `alpha=0.2` 的 lasso 模型，它能提供相当不错的结果和较低的过拟合，如我们接下来所见。然而，欢迎测试其他值，因为每个数据集可能有其独特的最佳值。接下来，使用
    `Lasso` 类的 `.fit()` 方法在训练集上训练该模型：
- en: '[PRE34]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Evaluate the lasso model on the training and test datasets, using the R2-score,
    implemented in the `.score()` method of the `Lasso` class:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.score()` 方法中的 R2-score 来评估训练集和测试集上的 lasso 模型，该方法在 `Lasso` 类中实现：
- en: '[PRE36]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This code outputs the following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码会输出以下内容：
- en: '[PRE39]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Here we see an improvement when compared to the linear regression with no penalization,
    and we have improved against the ridge regression too, having an R2-score of about
    `0.57` on the test set.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在与没有惩罚的线性回归进行比较时，我们可以看到有所改进，并且相较于 ridge 回归，我们在测试集上的 R2-score 约为 `0.57`。
- en: There’s more…
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'If we plot again for the weights instances, we now have the following values:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再次绘制权重实例图，我们现在得到以下值：
- en: '[PRE40]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This outputs the plot in *Figure 3**.14*:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出 *图 3**.14* 中的图表：
- en: '![Figure 3.7 – Learned values of each weight of the lasso model. Note that,
    unlike the ridge model, several weights are set to zero](img/B19629_03_07.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.7 – 每个 lasso 模型权重的学习值。注意，与 ridge 模型不同，多个权重被设置为零](img/B19629_03_07.jpg)'
- en: Figure 3.7 – Learned values of each weight of the lasso model. Note that, unlike
    the ridge model, several weights are set to zero
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – 每个 lasso 模型权重的学习值。注意，与 ridge 模型不同，多个权重被设置为零
- en: As expected, some values are set to 0, with an overall range of `-0.5` to `0.7`.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期所示，某些值被设置为 0，整体范围在 `-0.5` 到 `0.7` 之间。
- en: Lasso regularization allowed us to significantly improve performance on the
    test set in this case, while also reducing overfitting.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，lasso 正则化使我们在测试集上的表现得到了显著提升，同时也减少了过拟合。
- en: See also
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'Take a look at the official documentation for more information on the lasso
    class: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.xhtml).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 lasso 类的官方文档，了解更多信息：[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.xhtml).
- en: 'Another technique called group lasso can be useful for regularization; find
    out more about it here: [https://group-lasso.readthedocs.io/en/latest/](https://group-lasso.readthedocs.io/en/latest/).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种名为 group lasso 的技术可以用于正则化，了解更多信息请访问： [https://group-lasso.readthedocs.io/en/latest/](https://group-lasso.readthedocs.io/en/latest/).
- en: Regularizing with elastic net regression
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用弹性网回归进行正则化
- en: Elastic net regression, besides having a very fancy name, is nothing more than
    a combination of ridge and lasso penalization. It’s a regularization method that
    can be of help in some specific cases. Let’s have a look at what it means in terms
    of loss, and then train a model on the California housing dataset.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性网回归，除了有一个非常华丽的名字外，其实不过是岭回归和套索回归的组合。它是一种正则化方法，在一些特定情况下可能会有所帮助。让我们看看它在损失函数中的表现，然后在加利福尼亚住房数据集上训练一个模型。
- en: Getting ready
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: The idea with elastic net is to have both L1 and L2 regularization.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性网的思想是同时使用L1和L2正则化。
- en: 'This means that the loss ![](img/Formula_03_019.png) is the following:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着损失函数！[](img/Formula_03_019.png)是如下所示：
- en: '![](img/Formula_03_020.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_020.jpg)'
- en: The two hyperparameters, ![](img/Formula_03_021.png) and ![](img/Formula_03_022.png),
    can be fine-tuned.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个超参数，![](img/Formula_03_021.png)和![](img/Formula_03_022.png)，可以进行微调。
- en: We won’t go into detail on the equations for the gradient descent, since deriving
    them is straightforward as soon as ridge and lasso are clear.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会详细讨论梯度下降的公式，因为一旦理解了岭回归和套索回归，推导它们是非常直接的。
- en: To train a model, we again need the `sklearn` library, which we already installed
    in previous recipes. Also, we again assume that the California housing dataset
    is already downloaded and prepared.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型，我们再次需要`sklearn`库，这个库我们在之前的配方中已经安装过。此外，我们假设加利福尼亚住房数据集已经下载并准备好。
- en: How to do it…
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'In scikit-learn, elastic net is implemented in the `ElasticNet` class. However,
    instead of having two hyperparameters, ![](img/Formula_03_023.png) and ![](img/Formula_03_024.png),
    they are using two hyperparameters, `alpha` and `l1_ratio`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，弹性网在`ElasticNet`类中实现。然而，它们使用的是两个超参数`alpha`和`l1_ratio`，而不是两个超参数![](img/Formula_03_023.png)和![](img/Formula_03_024.png)：
- en: '![](img/Formula_03_025.png)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/Formula_03_025.png)'
- en: '![](img/Formula_03_026.png)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/Formula_03_026.png)'
- en: 'Let’s now apply this to our already prepared California housing dataset:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将其应用到已经准备好的加利福尼亚住房数据集上：
- en: 'Import the `ElasticNet` class from scikit-learn:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从scikit-learn中导入`ElasticNet`类：
- en: '[PRE41]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Instantiate an elastic net model. Values of `alpha=0.1` and `l1_ratio=0.5`
    have been chosen, but other values can be tested. Then train the model on the
    training set, using the `.fit()` method of the `ElasticNet` class:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个弹性网模型。选择了`alpha=0.1`和`l1_ratio=0.5`，但也可以测试其他值。然后使用`ElasticNet`类的`.fit()`方法，在训练集上训练模型：
- en: '[PRE42]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Evaluate the elastic net model on the training and test dataset, using the
    R2-score computed with the `.``score()` method:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.score()`方法计算的R2得分，在训练集和测试集上评估弹性网模型：
- en: '[PRE46]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Here is the output for it:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的输出：
- en: '[PRE51]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'In this case, the results are not an improvement upon lasso regularization:
    perhaps a better fine-tuning of the hyperparameters is required to achieve equivalent
    performance.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，结果并没有超越套索回归：可能需要对超参数进行更好的微调才能达到相同的性能。
- en: While more complicated to fine-tune because of having two hyperparameters, elastic
    net regression may offer more flexibility in regularization than the ridge or
    lasso regularizations. The use of hyperparameter optimization is the recommended
    method to find the right set of hyperparameters for any specific task.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管弹性网回归由于有两个超参数而在微调时更为复杂，但它可能比岭回归或套索回归在正则化上提供更多灵活性。推荐使用超参数优化方法来找到适合特定任务的超参数组合。
- en: Note
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In practice, elastic net regression is probably less widely used than the ridge
    and lasso regressions.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，弹性网回归可能不如岭回归和套索回归广泛使用。
- en: See also
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'The official documentation of scikit-learn for elastic net regression can be
    found here: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.xhtml).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的官方文档中，关于弹性网回归的详细信息可以在此找到：[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.xhtml)。
- en: Training a logistic regression model
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练逻辑回归模型
- en: Logistic regression is really close to linear regression conceptually. Once
    linear regression is fully understood, logistic regression is just a couple of
    tricks away. But unlike linear regression, logistic regression is most commonly
    used for classification tasks.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归在概念上与线性回归非常接近。一旦完全理解了线性回归，逻辑回归就只需要几步小技巧。然而，与线性回归不同，逻辑回归最常用于分类任务。
- en: Let’s first explain what logistic regression is, and then train a model on the
    breast cancer dataset using scikit-learn.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先解释什么是逻辑回归，然后使用 scikit-learn 在乳腺癌数据集上训练一个模型。
- en: Getting ready
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'Unlike linear regression, logistic regression’s output is limited to a range
    of `0` to `1`. The first idea is exactly the same as linear regression, having
    for each feature ![](img/Formula_03_028.png) a parameter ![](img/Formula_03_027.png):'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性回归不同，逻辑回归的输出范围限制在`0`到`1`之间。第一个思路与线性回归完全相同，为每个特征 ![](img/Formula_03_028.png)
    分配一个参数 ![](img/Formula_03_027.png)：
- en: '![](img/Formula_03_029.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_029.jpg)'
- en: 'There is one more step to limit the range to `0` to `1`, which is to apply
    the logistic function to this output *z*. As a reminder, the logistic function
    (also called the sigmoid function, although it’s a more generic function) is the
    following:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一步是将输出 *z* 限制在 `0` 到 `1` 之间，那就是对该输出应用逻辑函数。作为提醒，逻辑函数（也叫做 Sigmoid 函数，尽管它是一个更通用的函数）如下所示：
- en: '![](img/Formula_03_030.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_030.jpg)'
- en: 'The logistic function has an S-shape, with values ranging from `0` to `1`,
    with a value of `0.5` on *x = 0*, as shown here in *Figure 3**.8*:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑函数呈 S 形，其值从 `0` 到 `1`，在 *x = 0* 时取值为 `0.5`，如*图 3.8*所示：
- en: '![Figure 3.8 – Logistic function, ranging from 0 to 1 through 0.5 for x = 0](img/B19629_03_08.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.8 – 逻辑函数，x = 0 时从 0 到 1 范围内穿过 0.5](img/B19629_03_08.jpg)'
- en: Figure 3.8 – Logistic function, ranging from 0 to 1 through 0.5 for x = 0
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 – 逻辑函数，x = 0 时从 0 到 1 范围内穿过 0.5
- en: 'Finally, by applying the sigmoid function, we have the logistic regression
    prediction *h(x)* as the following:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过应用 Sigmoid 函数，我们得到了逻辑回归的预测 *h(x)*，如下所示：
- en: '![](img/Formula_03_031.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_031.jpg)'
- en: 'This ensures an output value *h(x)* in the range of `0` to `1`. But it does
    not yet allow us to have a classification. The final step is to apply a threshold
    (for example, `0.5`) to have a classification prediction:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了输出值 *h(x)* 在`0`到`1`的范围内。但它还没有让我们进行分类。最后一步是应用一个阈值（例如，`0.5`）来进行分类预测：
- en: '![](img/Formula_03_032.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_032.jpg)'
- en: 'As we did for linear regression, we now need to define a loss `L` that is to
    be minimized, in order to optimize the parameters ![](img/Formula_03_033.png)
    and *b*. The commonly used loss is the so-called **binary** **cross entropy**:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 和线性回归一样，我们现在需要定义一个损失函数 `L`，以便最小化它，进而优化参数 ![](img/Formula_03_033.png) 和 *b*。常用的损失函数是所谓的**二元**
    **交叉熵**：
- en: '![](img/Formula_03_034.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_034.jpg)'
- en: 'We can see four extreme cases here:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这里有四种极端情况：
- en: '*if y = 1 and h(x)**≃**1: L* *≃* *0*'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如果 y = 1 且 h(x)**≃**1：L* *≃* *0*'
- en: '*if y = 1 and h(x)**≃**0: L* *≃* *+∞*'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如果 y = 1 且 h(x)**≃**0：L* *≃* *+∞*'
- en: '*if y = 0 and h(x)**≃**0: L* *≃* *0*'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如果 y = 0 且 h(x)**≃**0：L* *≃* *0*'
- en: '*if y = 0 and h(x)**≃**1: L* *≃* *+∞*'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如果 y = 0 且 h(x)**≃**1：L* *≃* *+∞*'
- en: 'So now for us to have the expected behavior, which is, a loss that tends to
    0 indicating a highly accurate prediction, and that increases for a wrong prediction.
    This is represented in the following figure:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了实现我们期望的行为，即一个趋向于 0 的损失，表示高度准确的预测，并且在错误预测时损失增加。这个过程在下图中表示：
- en: '![Figure 3.9 – Binary cross entropy, minimizing the error for both y = 0 and
    y = 1](img/B19629_03_09.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.9 – 二元交叉熵，最小化 y = 0 和 y = 1 的误差](img/B19629_03_09.jpg)'
- en: Figure 3.9 – Binary cross entropy, minimizing the error for both y = 0 and y
    = 1
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 – 二元交叉熵，最小化 y = 0 和 y = 1 的误差
- en: Again, one way to optimize logistic regression is through gradient descent,
    the exact same way as for linear regression. As a matter of fact, the equations
    are exactly the same, even if we can’t prove it here.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，优化逻辑回归的一种方法是通过梯度下降，和线性回归完全相同。事实上，方程是完全相同的，尽管我们在这里无法证明这一点。
- en: To get ready for this recipe, all we need is to have the `scikit-learn` library
    installed.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 要准备这个步骤，我们只需要安装 `scikit-learn` 库。
- en: How to do it…
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Logistic regression is fully implemented in scikit-learn as the `LogisticRegression`
    class. Unlike linear regression in scikit-learn, logistic regression has regularization
    implemented directly into one single class. The following parameters are the ones
    to tweak in order to fine-tune regularization:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归在 scikit-learn 中完全实现为 `LogisticRegression` 类。与 scikit-learn 中的线性回归不同，逻辑回归将正则化直接集成到一个类中。以下参数是需要调整的，以便微调正则化：
- en: '`penalty`: This can be either `''l1''`, `''l2''`, `''elasticnet''`, or `''none''`'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`penalty`：这可以是 `''l1''`，`''l2''`，`''elasticnet''` 或 `''none''`'
- en: '`C`: This is a float, inverse to regularization strength; the smaller the value,
    the greater the regularization'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`C`：这是一个浮动值，与正则化强度成反比；值越小，正则化越强。'
- en: 'In this recipe, we will apply logistic regression with no regularization to
    the breast cancer dataset provided by scikit-learn. We will first load and prepare
    the data, and then train and evaluate the logistic regression model, as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将应用不使用正则化的逻辑回归模型，处理由scikit-learn提供的乳腺癌数据集。我们将首先加载并准备数据，然后训练并评估逻辑回归模型，具体步骤如下：
- en: 'Import the `load_breast_cancer` function, which will allow us to load the dataset,
    and the `LogisticRegression` class:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`load_breast_cancer`函数，它将允许我们加载数据集，并导入`LogisticRegression`类：
- en: '[PRE52]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Load the dataset using the `load_breast_cancer` function:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`load_breast_cancer`函数加载数据集：
- en: '[PRE54]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Split the dataset using the `train_test_split` function. We assume it has been
    imported in a previous recipe. Otherwise, you will need to import it with `from
    sklearn.model_selection import train_test_split`. We choose `test_size=0.2` so
    that we have a training size of 80% and a test size of 20%:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_test_split`函数拆分数据集。我们假设它已经在之前的示例中被导入。如果没有，你需要通过`from sklearn.model_selection
    import train_test_split`来导入。我们选择`test_size=0.2`，这样训练集占80%，测试集占20%：
- en: '[PRE56]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We then prepare the data. Since the dataset is only composed of quantitative
    data, we just apply rescaling using the standard scaler: we instantiate it, fit
    it on the training set, and transform this same training set with `fit_transform()`.
    Finally, we rescale the test set with `.transform()`. Again, we assume the standard
    scaler has been imported from a previous recipe, otherwise, we need to import
    it with `from sklearn.preprocessing` `import StandardScaler`:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们准备数据。由于数据集仅包含定量数据，我们只需要使用标准化方法进行重缩放：我们实例化标准化器，对训练集进行拟合，并用`fit_transform()`方法转换同一训练集。最后，我们使用`.transform()`对测试集进行重缩放。同样地，我们假设标准化器已经在之前的示例中被导入，否则需要通过`from
    sklearn.preprocessing import StandardScaler`导入：
- en: '[PRE59]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Then we instantiate the logistic regression, and we specify `penalty=''none''`
    here so that we don’t use any penalization of the loss for pedagogical reasons.
    Check out the next recipe to see how penalization works. Then we train the logistic
    regression model on the training set with the `.``fit()` method:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们实例化逻辑回归模型，并指定`penalty='none'`，这样就不对损失进行惩罚（为了教学目的）。查看下一个示例，了解惩罚项如何工作。接着我们使用`.fit()`方法在训练集上训练逻辑回归模型：
- en: '[PRE63]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Evaluate the model on both the training and the test set. The `.score()` method
    of the `LogisticRegression` class uses the accuracy metric, but any other metric
    can be used:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集和测试集上评估模型。`LogisticRegression`类的`.score()`方法使用准确率作为评估指标，但也可以使用其他指标：
- en: '[PRE67]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'This code outputs the following:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码输出如下内容：
- en: '[PRE70]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: We are facing rather strong overfitting here, with classification accuracy of
    100% on the training set but only about 94% on the test set. This is a good start,
    but in the next recipe, we will use regularization to help improve the test accuracy.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里面临较强的过拟合问题，训练集的分类准确率为100%，但测试集的准确率只有大约94%。这是一个好的开始，但在下一个示例中，我们将使用正则化来帮助提高测试准确率。
- en: Regularizing a logistic regression model
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对逻辑回归模型进行正则化
- en: 'Logistic regression uses the same trick as linear regression to add regularization:
    it adds penalization to the loss. In this recipe, we will first briefly explain
    how penalization affects the loss, and how to add regularization using scikit-learn
    on the breast cancer dataset that we prepared in the previous recipe.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归与线性回归使用相同的技巧来添加正则化：它将惩罚项加到损失函数中。在这个示例中，我们将简要解释惩罚项如何影响损失，并展示如何使用scikit-learn在之前准备的乳腺癌数据集上添加正则化。
- en: Getting ready
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Just like linear regression, it is very easy to add a regularization term to
    the loss `L`, either an L1- or L2-norm of the parameters *w*. For example, the
    loss with an L2-norm would be the following:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 就像线性回归一样，向损失函数`L`中添加正则化项非常简单，可以是L1或L2范数的参数*w*。例如，包含L2范数的损失函数如下所示：
- en: '![](img/Formula_03_035.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_035.jpg)'
- en: As we did for ridge regression, we’ve added a squared sum of the weights, with
    a hyperparameter in front of it. To keep as close as possible to the scikit-learn
    implementation, we will use 1/C instead of 𝜆 for the regularization hyperparameter,
    but the idea remains the same.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 和岭回归一样，我们在权重上加入了平方和，并在前面加了一个超参数。为了尽可能接近scikit-learn的实现，我们将使用1/C代替𝜆作为正则化超参数，但基本思想保持不变。
- en: 'In this recipe, we assume the following libraries are already installed from
    previous recipes: `sklearn` and `matplotlib`. Also, we assume the data from the
    breast cancer dataset is already loaded and prepared from the previous recipe,
    so that we can directly reuse it.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们假设之前的食谱中已经安装了以下库：`sklearn` 和 `matplotlib`。同时，我们假设乳腺癌数据集中的数据已经从前一个食谱中加载和准备好，因此我们可以直接重用它。
- en: How to do it…
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到…
- en: 'Let’s now try to improve the test accuracy we had on the previous recipe by
    adding L2 regularization:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过添加 L2 正则化来尝试提高我们在上一道食谱中的测试精度：
- en: 'Instantiate the logistic regression model. Here, we choose an L2 penalization
    and a regularization value of `C=0.1`. A lower value of `C` means greater regularization:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化逻辑回归模型。在这里，我们选择 L2 惩罚和正则化值 `C=0.1`。较低的 `C` 值意味着更强的正则化：
- en: '[PRE71]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Fit the logistic regression model on the training set, with the `.``fit()`
    method:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.fit()` 方法在训练集上拟合逻辑回归模型：
- en: '[PRE72]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Evaluate the model on both training and test set. We use the `.score()` method
    here, providing the accuracy score:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集和测试集上评估模型。我们在这里使用 `.score()` 方法，提供准确率得分：
- en: '[PRE74]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: As we can see here, adding L2 regularization allowed us to climb up to 98% accuracy
    on the test set, which is quite an improvement from about 94% without regularization.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这里看到的，添加 L2 正则化使我们在测试集上的精度达到了 98%，相比于没有正则化时的大约 94%，这是一项相当大的改进。
- en: Another reminder
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个提醒
- en: The best way to find the right regularization value for `C` is with hyperparameter
    optimization.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 找到 `C` 的最佳正则化值的最佳方法是进行超参数优化。
- en: 'Out of curiosity, we can plot the train and test accuracy here for several
    values of regularization strength:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 出于好奇，我们可以在这里绘制多个正则化强度值下的训练和测试精度：
- en: '[PRE81]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Here is the graph for it:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的图表：
- en: '![Figure 3.10 – Accuracy as a function of the C parameter, for both the training
    and test sets](img/B19629_03_10.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.10 – 精度作为 C 参数的函数，适用于训练集和测试集](img/B19629_03_10.jpg)'
- en: Figure 3.10 – Accuracy as a function of the C parameter, for both the training
    and test sets
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 – 精度作为 C 参数的函数，适用于训练集和测试集
- en: 'This plot can actually be read from right to left. We can see that as the value
    of `C` decreases (thus increasing regularization), the train accuracy keeps on
    decreasing, as the regularization gets higher and higher. On the other hand, decreasing
    `C` (thus increasing regularization) first allows us to improve the test results:
    the model is generalizing more and more. But at some point, adding regularization
    (decreasing `C` more) does not help any further, and even hurts the test accuracy.
    Indeed, adding too much regularization creates a high-bias model.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图实际上是从右到左读取的。我们可以看到，当 `C` 的值减小时（从而增加正则化），训练精度持续下降，因为正则化越来越强。另一方面，减小 `C`（即增加正则化）最初能够改善测试结果：模型的泛化能力越来越强。但在某个时刻，添加更多正则化（进一步减小
    `C`）并不会带来更多帮助，甚至会降低测试精度。事实上，过多的正则化会产生高偏差模型。
- en: There’s more…
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: The `LogisticRegression` implementation of scikit-learn allows us to not only
    use L2 penalization but also L1 and elastic net, just like linear regression,
    which allows us to have some flexibility on the best regularization for any given
    dataset and task.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '`LogisticRegression` 的 scikit-learn 实现不仅允许我们使用 L2 惩罚，还允许使用 L1 和弹性网，就像线性回归一样，这让我们可以根据任何给定的数据集和任务来选择最佳的正则化方式。'
- en: 'The official documentation can be checked for more details: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.xhtml).'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 可以查阅官方文档以获取更多细节：[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.xhtml)。
- en: Choosing the right regularization
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适的正则化
- en: 'Linear models share this regularization method with L1 and L2 penalization.
    The only difference in the implementation is the fact that linear regression has
    its own class for each regularization type, as mentioned here:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型与 L1 和 L2 惩罚共享这种正则化方法。实现中的唯一区别是线性回归有针对每种正则化类型的独立类，如下所述：
- en: '`LinearRegression` for no regularization'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LinearRegression` 无正则化'
- en: '`RidgeRegression` for L2 regularization'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RidgeRegression` 用于 L2 正则化'
- en: '`Lasso` for L1 regularization'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Lasso` 用于 L1 正则化'
- en: '`ElasticNet` for both L1 and L2'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ElasticNet` 用于 L1 和 L2'
- en: Logistic regression has an integrated implementation, passing `L1` or `L2` as
    the class parameter.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归有一个集成实现，可以将 `L1` 或 `L2` 作为类参数传递。
- en: Note
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: With `C` parameter for `L2` regularization for both the `SVC` classification
    class and the `SVR` regression class.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`SVC`分类类和`SVR`回归类，使用`C`参数进行`L2`正则化。
- en: 'But for linear regression as well as logistic regression, one question remains:
    should we use L1 or L2 regularization?'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 但是对于线性回归和逻辑回归，有一个问题依然存在：我们应该使用L1还是L2正则化？
- en: In this recipe, we will provide some practical tips about whether to use L1
    or L2 penalization in some cases and then we will perform a grid search on the
    breast cancer dataset using logistic regression to find the best regularization.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，我们将提供一些关于在某些情况下使用L1或L2惩罚的实用建议，然后我们将使用逻辑回归对乳腺癌数据集进行网格搜索，以找到最佳正则化方法。
- en: Getting ready
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: There is no absolute answer to what penalization is best. Most of the time,
    the only way to find out is by doing hyperparameter optimization. But in some
    cases, data itself or external constraints may give hints about which regularization
    to use. Let’s have a quick look. First, let’s compare L1 and L2 regularization,
    then let’s explore hyperparameter optimization.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最适合的惩罚方式，并没有绝对的答案。大多数情况下，唯一的办法是通过超参数优化来找出答案。但在某些情况下，数据本身或外部约束可能会提示我们选择哪种正则化方法。让我们快速看一下。首先，比较L1和L2正则化，然后探索超参数优化。
- en: L1 versus L2 regularization
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: L1与L2正则化
- en: L1 and L2 regularization have intrinsic differences that can sometimes help
    us make an educated guess upfront, and then save computational time. Let’s have
    a look at these differences.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: L1和L2正则化有内在的差异，这些差异有时可以帮助我们提前做出明智的判断，从而节省计算时间。让我们看看这些差异。
- en: As discussed, L1 regularization tends to set some weights to zero, and thus
    may allow feature selection. Thus, if we have a dataset with many features, it
    can be helpful to have this information, and then just remove those features in
    the future.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，L1正则化倾向于将某些权重设置为零，从而可能允许特征选择。因此，如果我们有一个包含许多特征的数据集，拥有这些信息将会很有帮助，未来可以删除这些特征。
- en: Also, L1 regularization uses the absolute value of weights in the loss, making
    it more robust to outliers compared to L2 regularization. If our dataset could
    contain outliers, it is to be considered.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，L1正则化使用权重的绝对值作为损失的一部分，使其相比L2正则化对异常值更加稳健。如果我们的数据集可能包含异常值，这是需要考虑的。
- en: Finally, in terms of computing speed, L2 regularization is adding a quadratic
    term and is as a consequence less computationally expensive than L1 regularization.
    If training speed is of concern, L2 regularization should be considered before
    L1.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在计算速度方面，L2正则化是通过添加一个二次项来实现的，因此比L1正则化计算开销更小。如果训练速度是一个关注点，应该优先考虑L2正则化。
- en: 'In a nutshell, we can consider the following cases to be ones where the choice
    can be made up front based on data or other constraints such as computation resources:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们可以考虑以下几种情况，这些情况可以根据数据或其他约束（如计算资源）在前期做出选择：
- en: 'The data contains numerous features, many of lesser importance: L1 regularization'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据包含许多特征，其中许多特征重要性较低：选择L1正则化
- en: 'The data contains many outliers: L1 regularization'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据包含许多异常值：L1正则化
- en: 'Training computation resources are of concern: L2 regularization'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果训练计算资源有限：选择L2正则化
- en: Hyperparameter optimization
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超参数优化
- en: A more practical and perhaps pragmatic approach is just to do hyperparameter
    optimization, with L1 or L2 being a hyperparameter (elastic net regression could
    be added too).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 更实际、或许更务实的方法是直接进行超参数优化，将L1或L2作为一个超参数（弹性网回归也可以加入）。
- en: We will use hyperparameter optimization with grid search as implemented by scikit-learn,
    optimizing a logistic regression model with both L1 and L2 regularization on the
    breast cancer dataset.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`scikit-learn`实现的网格搜索进行超参数优化，对乳腺癌数据集上的逻辑回归模型进行L1和L2正则化优化。
- en: For this recipe, we expect the `scikit-learn` library to be installed from a
    previous recipe. We also assume that the breast cancer data is already loaded
    and prepared from the *Training a logistic regression* *model* recipe.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个方案，我们假设在前一个方案中已经安装了`scikit-learn`库。我们还假设乳腺癌数据已经通过*训练逻辑回归* *模型*方案加载并准备好。
- en: How to do it…
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'We will perform a grid search on a given set of hyperparameters, more specifically,
    L1 and L2 penalization with several values of penalization, `C`:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在一组给定的超参数上执行网格搜索，更具体地说，就是在多个惩罚值下，进行L1和L2正则化的惩罚，其中`C`为惩罚参数：
- en: 'First, we need to import the `GridSearchCV` class from `sklearn`:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要从`sklearn`导入`GridSearchCV`类：
- en: '[PRE82]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Then we define the parameter grid. This is the space of hyperparameters that
    we want to test. Here, we will try both L1 and L2 penalization, and for each penalization,
    we will try the `C` values in `[0.01, 0.03, 0.06, 0.1, 0.3, 0.6]` as follows:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们定义参数网格。这是我们想要测试的超参数空间。在这里，我们将同时尝试L1和L2惩罚项，并且对于每个惩罚项，我们将尝试`C`值，范围是`[0.01,
    0.03, 0.06, 0.1, 0.3, 0.6]`，具体如下：
- en: '[PRE83]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Next, we instantiate the grid search object. Several parameters are passed
    here:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们实例化网格搜索对象。这里传入了几个参数：
- en: '`LogisticRegression`, for which we specify the solver to be `liblinear` (check
    the *See also* section for more information) so that it can handle both L1 and
    L2 penalization. We assume the class is already imported from a previous recipe;
    otherwise, you can import it with `from` `sklearn.linear_model` `import` `LogisticRegression`.'
  id: totrans-347
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LogisticRegression`，我们指定求解器为`liblinear`（更多信息请参见*另见*部分），这样它就能处理L1和L2惩罚项。我们假设该类已从之前的代码中导入；否则，您可以通过`from
    sklearn.linear_model import LogisticRegression`导入它。'
- en: '`scoring=''accuracy''`, but it can be any other relevant metric.'
  id: totrans-348
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scoring=''accuracy''`，但它也可以是任何其他相关的度量标准。'
- en: '`5` cross-validation folds here with `cv=5`, as it is pretty standard, but
    depending on the size of the dataset, other values may be just fine too:'
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里使用`cv=5`表示5折交叉验证，因为这是比较标准的设置，但根据数据集的大小，其他值也可能适用：
- en: '[PRE86]'
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Train the grid on the training data with the `.fit()` method. Then we can display
    the best hyperparameters found out of curiosity, using the `best_params_` attribute:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.fit()`方法在训练数据上训练网格。然后，我们可以出于好奇使用`best_params_`属性显示找到的最佳超参数：
- en: '[PRE92]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: In this case, the hyperparameters appear to be `C=0.06` with L2 penalization.
    We can now evaluate the model.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，最佳超参数是`C=0.06`，并使用L2惩罚。现在我们可以评估模型。
- en: 'Evaluate the model on both the training and the test sets, using the `.score()`
    method that computes accuracy. Using `.score()` or `.predict()` directly on the
    grid object will automatically compute the best model predictions:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.score()`方法对训练集和测试集上的模型进行评估，该方法计算准确率。直接在网格对象上使用`.score()`或`.predict()`将自动计算出最佳模型的预测结果：
- en: '[PRE97]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: In this case, this improved the performance on the test set, although it’s not
    always that easy. But the method remains the same and can be applied to any dataset.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，这提高了测试集上的性能，尽管这并不总是那么简单。但方法保持不变，可以应用于任何数据集。
- en: See also
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'The grid search official documentation can be found here: [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.xhtml).'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索的官方文档可以在这里找到：[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.xhtml)。
- en: 'More information about the solvers and penalizations available in scikit-learn
    can be found here: [https://scikit-learn.org/stable/modules/linear_model.xhtml#logistic-regression](https://scikit-learn.org/stable/modules/linear_model.xhtml#logistic-regression).'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 关于scikit-learn中可用的求解器和惩罚项的更多信息可以在这里找到：[https://scikit-learn.org/stable/modules/linear_model.xhtml#logistic-regression](https://scikit-learn.org/stable/modules/linear_model.xhtml#logistic-regression)。
