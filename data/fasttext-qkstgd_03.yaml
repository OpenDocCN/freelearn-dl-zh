- en: Creating Models Using FastText Command Line
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 FastText 命令行创建模型
- en: 'FastText has a powerful command line. In fact, you can call fastText a command-line-first
    library. Now, a lot of developers and researchers are not comfortable with the
    command line, and I would ask you to go through the examples in this chapter with
    greater attention. My hope is that by the end of this chapter, you will have some
    confidence in command-line file manipulations. The advantages of using the command
    line are as follows:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: FastText 拥有强大的命令行功能。事实上，可以将 fastText 称为一个“命令行优先”的库。现在，许多开发者和研究人员对命令行并不熟悉，我建议你在本章中更加专注于示例。我的希望是，在本章结束时，你能够对命令行文件操作有一定的信心。使用命令行的优势如下：
- en: Commands such as `cat`, `grep`, `sed`, and `awk` are quite old and their behavior
    is well-documented on the internet. Chances are high that, for any use case that
    you might have, you will easily get snippets on Stack Overflow/Google (or your
    colleague next door will know it).
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像`cat`、`grep`、`sed`和`awk`这样的命令已经有很长的历史，并且它们的行为在互联网上有很好的文档记录。很有可能，对于任何你可能遇到的使用场景，你都可以轻松在
    Stack Overflow/Google 上找到代码片段（或者隔壁的同事可能也知道）。
- en: Since they are generally implemented in the C language, they are very fast.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于它们通常是用 C 语言实现的，所以运行非常快速。
- en: The commands are very crisp and concise, which means there is not a lot of code
    to write and maintain.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些命令非常简洁明了，这意味着你不需要写很多代码，也不需要维护复杂的代码。
- en: 'We will take a look at how classification and word vector generation works
    in fastText. In this chapter, we will explore how to implement them using the
    command line:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将了解在 fastText 中如何进行分类和词向量生成。在这一章中，我们将探讨如何使用命令行来实现它们：
- en: Text classification using fastText
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 fastText 进行文本分类
- en: FastText word vectors
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FastText 词向量
- en: Creating word vectors
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建词向量
- en: Facebook word vectors
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Facebook 词向量
- en: Using pretrained word vectors
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练的词向量
- en: Text classification using fastText
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 fastText 进行文本分类
- en: To access the command line, open the Terminal on your Linux or macOS machines,
    or the command prompt (by typing `cmd` in Windows + *R* and hitting *Enter*) on
    Windows machines, and then type `fastText`. You should see some output coming
    out. If you are not seeing anything, or getting an error saying that the command
    not found, please take a look at the previous chapter on how to install fastText
    on your computer. If you are able to see some output, the output is a basic description
    of all the options. A description of the command line options for fastText can
    be found in the *Appendix* of this book.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问命令行，打开 Linux 或 macOS 机器上的终端，或在 Windows 机器上按下 Windows + *R* 输入`cmd`后按*Enter*打开命令提示符，然后输入`fastText`。你应该看到一些输出。如果什么都没看到，或者遇到错误提示“命令未找到”，请查看上一章关于如何在计算机上安装
    fastText 的内容。如果你能看到一些输出，说明成功运行，该输出是对所有选项的基本描述。fastText 的命令行选项描述可以在本书的*附录*中找到。
- en: All the methods and command line statements mentioned in this chapter will work
    on Linux and Mac machines. If you are a Windows user, focus more on the description
    and the logic of what is being done and follow the logic of the steps. A helpful
    guide on command line differences between Windows and Linux is mentioned in the
    *Appendix*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中提到的所有方法和命令行语句都适用于 Linux 和 Mac 机器。如果你是 Windows 用户，请更多关注描述和操作逻辑，按照步骤的逻辑进行操作。关于
    Windows 和 Linux 之间命令行差异的有用指南，请参见*附录*。
- en: 'In fastText, there are two primary use cases for the command line. These are
    the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在 fastText 中，命令行有两个主要的使用场景，分别如下：
- en: Text classification
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类
- en: Text representation
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本表示
- en: One of the core areas of focus for fastText is text classification. Text classification
    is a technique in which we learn to which set of categories the input text belongs.
    This is basically a supervised machine learning problem, so first and foremost,
    you will need a dataset that contains text and the corresponding labels.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: fastText 的核心重点之一是文本分类。文本分类是一种技术，通过它我们可以学习输入文本属于哪一类。基本上，这是一个监督学习问题，因此首先，你需要一个包含文本及相应标签的数据集。
- en: 'Roughly speaking, machine learning algorithms run some kind of optimization
    problem on a set of matrices and vectors. They do not really understand "raw text,"
    which means that you will need to set up a pipeline to convert the raw text into
    numbers. Here are the steps that can be followed to do that:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大致来说，机器学习算法会对一组矩阵和向量进行某种优化问题的处理。它们并不真正理解“原始文本”，这意味着你需要搭建一个管道，将原始文本转换为数字。以下是可以遵循的步骤：
- en: First, you need the data and hence for text classification you need a series
    of texts or documents that will be labeled. You convert them into a series of
    text-label pairs.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，你需要数据，因此对于文本分类，你需要一系列会被标注的文本或文档。你将它们转换成一系列的文本-标签对。
- en: The next step is called **tokenization**. Tokenization is the process of dividing
    the text into individual pieces or tokens. Tokenization is primarily done by understanding
    the word boundaries in the given text. Many languages in the world are space delimited.
    Examples of these are English and French. In some other cases, the word boundaries
    may not be clear, such as in the case of Mandarin, Tamil, and Urdu.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一步叫做**分词**。分词是将文本划分成独立的片段或词项的过程。分词主要通过理解给定文本中的词边界来完成。世界上许多语言是以空格分隔的，例如英语和法语。而在某些情况下，词边界可能并不清晰，例如在中文、泰米尔语和乌尔都语中。
- en: Once the tokenization is done, based on the process you may end up with a "bag
    of words," which is essentially a vector for the document/sentence telling you
    whether a specific word is there or not, and how many times. The columns in the
    matrix are all the set of words present, which is called the dictionary, and the
    rows are the count of the particular words in the document. This is called the
    **bag**-**of**-**words** approach.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦分词完成，依据这个过程，你可能会得到一个“词袋”，它本质上是一个向量，表示文档或句子中某个特定的词是否存在，以及它出现的次数。矩阵中的列是所有出现过的词集，称为字典，而行则表示文档中特定词的计数。这就是**词袋**-**模型**方法。
- en: Convert the bag of words into a TF-IDF matrix to reduce the weight of the common
    terms. TF-IDF has been used so that the terms that are common in the document
    do not have too much impact on the resultant matrix.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将词袋模型转换为TF-IDF矩阵，以减少常见术语的权重。使用TF-IDF是为了避免文档中常见的术语对结果矩阵产生过大影响。
- en: Now that you have the matrix, you can pass the matrix as input to a classification
    algorithm, which will essentially *train a model* on this input matrix. General
    algorithms that are quite popular in this stage are logistic regression, as well
    as algorithms such as XGBoost, random forest, and so on.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在你已经有了矩阵，可以将其作为输入传递给分类算法，这将基本上*训练一个模型*，在这个输入矩阵上进行训练。在这个阶段，比较常见的算法是逻辑回归，以及像XGBoost、随机森林等算法。
- en: 'Some of the additional steps that may need to be taken are the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要进行的其他步骤如下：
- en: Removal of stop words.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除停用词。
- en: Stemming or a heurestic removal of end of words. This process works mostly in
    English and related languages due to the prevalence of derivational affixes.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词干提取或启发式地去除词尾。这一过程主要适用于英语及相关语言，因为这些语言中派生词缀的使用非常普遍。
- en: Addition of n-grams to the model.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向模型中添加n-grams。
- en: Synonymous sets.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同义词集。
- en: Part of speech tagging.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词性标注。
- en: Text preprocessing
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本预处理
- en: 'Depending on the dataset, you may need to do some or all of these steps:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据集的不同，可能需要执行部分或所有这些步骤：
- en: Tokenize the text.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对文本进行分词。
- en: Convert the text into lowercase. This is only required for languages using Latin,
    Greek, Cyrillic, and Armenian scripts. Examples of such languages are English,
    French, German, and so on.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本转换为小写。这只对使用拉丁字母、希腊字母、西里尔字母和亚美尼亚字母的语言要求。像英语、法语、德语等语言都属于此类。
- en: Strip empty lines and their correspondences.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去除空行及其对应内容。
- en: Remove lines with XML tags (starting with `<`).
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除包含XML标签的行（以`<`开始）。
- en: These steps should be done in both cases, for sentence classification as well
    as the creation of word vectors.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤在两种情况中都应当执行，无论是句子分类还是词向量创建。
- en: English text and text using other Roman alphabets
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 英文文本及其他使用拉丁字母的文本。
- en: We will understand text processing using a sample dataset. In this chapter,
    the Yelp dataset is used. This is a popular dataset containing text reviews and
    the ratings given by users. In this dataset, you will find information about businesses
    in 11 metropolitan areas in four countries. If you download the data from the
    Kaggle link where it is shared, [https://www.kaggle.com/yelp-dataset/yelp-dataset/data](https://www.kaggle.com/yelp-dataset/yelp-dataset/data),
    there are various files we will see, but in our case we will only be interested
    in the review text provided by users in the `yelp_review.csv` file. As a challenge,
    we will try to see whether we can correctly predict the ratings or not.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个示例数据集来理解文本处理。在这一章中，使用的是 Yelp 数据集。这个数据集非常流行，包含了用户的文本评论和评分。在这个数据集中，您将找到来自四个国家的
    11 个大都市地区的商户信息。如果您从 Kaggle 的链接下载数据，[https://www.kaggle.com/yelp-dataset/yelp-dataset/data](https://www.kaggle.com/yelp-dataset/yelp-dataset/data)，您会看到各种文件，但在我们的例子中，我们只关心用户在
    `yelp_review.csv` 文件中提供的评论文本。作为挑战，我们将尝试预测评分是否正确。
- en: Downloading the data
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载数据
- en: 'Since this information is related to a particular business, and in case you
    are interested in downloading and playing with the data, please take a look at
    these steps before downloading the data:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些信息与特定商户相关，并且如果您有兴趣下载并操作数据，请在下载数据之前查看以下步骤：
- en: Please review the Yelp dataset webpage.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请查看 Yelp 数据集的网页。
- en: Please review, agree to, and respect Yelp's terms of use.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请查看、同意并遵守 Yelp 的使用条款。
- en: 'Download `yelp_review.csv` from Kaggle. The link for that is here: [https://www.kaggle.com/yelp-dataset/yelp-dataset/data](https://www.kaggle.com/yelp-dataset/yelp-dataset/data).'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Kaggle 下载 `yelp_review.csv`。下载链接在这里：[https://www.kaggle.com/yelp-dataset/yelp-dataset/data](https://www.kaggle.com/yelp-dataset/yelp-dataset/data)。
- en: 'This is the code:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这是代码：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Preprocessing the Yelp data
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理 Yelp 数据
- en: 'Take a look at the data. Always take a deep look at the data. The first line
    contains the headers:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 查看数据。始终仔细查看数据。第一行包含表头：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When you check the other lines, you will see that all the individual values
    are quotes. Also, the text field has new lines in many places. Since the strength
    of fastText is in text processing, we will only be taking the "stars" and the
    "text" fields, and will try to predict the ratings based on what is written in
    the text field.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当您查看其他行时，会发现所有的单个值都有引号。另外，文本字段中有许多地方有换行符。由于 fastText 在文本处理方面的优势，我们将只提取 "stars"
    和 "text" 字段，并尝试基于文本字段中的内容预测评分。
- en: You can use the following Python script to save the text and the ratings to
    another file, since the review text has a lot of new lines and we needed to remove
    the new lines from the text. You can keep them if you want and change the new
    line to another delimiter so that the file is fastText-compatible, but for our
    example we will remove the new lines from the text.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下 Python 脚本将文本和评分保存到另一个文件中，因为评论文本包含很多换行符，我们需要将其移除。您可以选择保留换行符，或者将换行符更改为其他分隔符，使文件与
    fastText 兼容，但在我们的示例中，我们将删除文本中的换行符。
- en: 'Here is the Python code to get only the relevant parts of the `.csv`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是获取 `.csv` 文件中相关部分的 Python 代码：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Save this in a file named `parse_yelp_dataset.py` and then run the following
    command:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 将此内容保存为名为 `parse_yelp_dataset.py` 的文件，然后运行以下命令：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Text normalization
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本标准化
- en: In this section, will take a look at some text normalization techniques that
    you can use.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些您可以使用的文本标准化技术。
- en: Removing stop words
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移除停用词
- en: The removal of stop words may or may not increase the performance of your model.
    So, keep two files, one with the stop words and one with the stop words stripped
    out. We will talk about how to check model performance in the *Model testing and
    evaluation* section.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 移除停用词可能会提高或不会提高您模型的性能。因此，请保留两个文件，一个包含停用词，另一个是移除停用词后的文件。我们将在 *模型测试与评估* 部分讨论如何检查模型性能。
- en: 'You can use the following script to remove the stop words. This is a Python
    script with dependencies such as `nltk`, so use it with your Anaconda installation.
    Please ensure that you have already downloaded the `nltk` `''english''` package
    before running the following script:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下脚本来移除停用词。这是一个带有 `nltk` 等依赖项的 Python 脚本，因此请在您的 Anaconda 安装环境中使用它。运行此脚本之前，请确保您已经下载了
    `nltk` 的 `'english'` 包：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Save the following code in a file named `remove_stop_words.py`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下代码保存为名为 `remove_stop_words.py` 的文件：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To run the file, you will need to pass the contents to the Python file. In the
    following explanations though, we are not really removing the stop words for the
    sake of brevity. You are of course encouraged to try both approaches.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行文件，您需要将文件内容传递给 Python 文件。不过，在以下解释中，为了简洁起见，我们并没有真正删除停用词。当然，您可以尝试这两种方法。
- en: Normalizing
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规范化
- en: 'Since we are dealing with English, it is recommended to first convert all uppercase
    letters to lowercase as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在处理英语，建议首先将所有大写字母转换为小写字母，如下所示：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Languages using the Latin, Greek, Cyrillic, and Armenian scripts are bicameral,
    which means that there are uppercase and lowercase letters. Examples of these
    are English, French, and German. Only in such languages should you be careful
    to convert all the text to lowercase. While processing a corpus for other languages,
    this step is not required.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用拉丁字母、希腊字母、西里尔字母和亚美尼亚字母的语言是双字母制的，这意味着有大写字母和小写字母。例如，英语、法语和德语就是这样的语言。只有在这些语言中，您才应小心将所有文本转换为小写字母。处理其他语言的语料库时，这一步是不需要的。
- en: Now, the start of the files already has all the labels. If we prefix the start
    of all sentences with `__label__`, it will add all the labels with the `__label__`
    text. This prefixing of the labels is necessary as the library takes in the whole
    text as the input, and there is no specific way to specify the input and the labels
    separately, as you might have seen in `scikit-learn` or other libraries. You can
    change the specific label prefix though, as you will see in the *A**ppendix*.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，文件的开头已经包含了所有标签。如果我们在所有句子的开头加上`__label__`，它将为所有标签添加`__label__`文本。这个标签前缀是必要的，因为库将整个文本作为输入，并且没有特定的方法来区分输入和标签，正如您在`scikit-learn`或其他库中看到的那样。您当然可以改变具体的标签前缀，正如您将在*附录*中看到的那样。
- en: 'So, to read the file in fastText and enable fastText to differentiate between
    normal text and label text, you will need to append `__label__` to the labels.
    One of the ways you can do that easily in the command line is shown here:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，要在 fastText 中读取文件并使 fastText 区分普通文本和标签文本，您需要在标签前加上`__label__`。以下是在命令行中轻松实现这一点的一种方式：
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Separate out and remove some of the punctuation that may be irrelevant:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 分离并移除一些可能不相关的标点符号：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Do not forget to keep checking how the data has been transformed after each
    transformation. On checking the data now, you can see that there is a comma at
    the beginning. There are also a lot of dots (`.`):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记在每次转换后继续检查数据是如何变化的。现在检查数据时，您会看到开头有一个逗号。还有很多点（`.`）：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Remove the commas and the dots. Keep in mind that fastText does not require
    the removal of punctuation and lowercasing all the letters; in fact, in some cases
    these may be important. Remember to take all the advice given here with a grain
    of salt and try all possible options you can think of. The ultimate aim is to
    train the best model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 删除逗号和点。请记住，fastText 不要求删除标点符号和将所有字母转为小写；事实上，在某些情况下，这些可能是重要的。记住，这里提供的所有建议都应该谨慎对待，并尝试您能想到的所有可能的选项。最终目标是训练出最佳的模型。
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Finally, remove all the consecutive spaces. Please note that in the following
    example, after all these transformations, the files are no longer in the `.csv`
    format, but that is fine for us because at the end of the day, `.csv` files are
    also text files and hence you should be fine with using `.txt` or any other textual
    format. As long as the files are text files with the contents in UTF-8 format,
    you should be good to go.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，删除所有连续的空格。请注意，在以下示例中，经过所有这些转换后，文件不再是 `.csv` 格式，但这对我们来说是可以的，因为归根结底，`.csv`
    文件也是文本文件，因此使用 `.txt` 或任何其他文本格式应该没有问题。只要文件是 UTF-8 格式的文本文件，您就可以继续使用。
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Shuffling all the data
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 打乱所有数据
- en: Shuffling the data before training the classifier is important. If the labels
    for the data are clustered, then the precision and recall, and hence the performance
    of the resulting model, will be low. This is because fastText uses stochastic
    gradient descent to learn the model. The training data from the files is processed
    in order. In our example, this is not the case as the labels of the same class
    are not together, but still it may be a good idea to keep this in mind and always
    shuffle before training.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练分类器之前打乱数据非常重要。如果数据的标签被聚集在一起，那么精度和召回率，进而模型的性能将较低。这是因为 fastText 使用随机梯度下降来学习模型。文件中的训练数据是按顺序处理的。在我们的例子中并非如此，因为同一类别的标签并不在一起，但仍然建议您记住这一点，并始终在训练前打乱数据。
- en: 'In *Nix systems, you have the shuffle command, as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Nix系统中，你可以使用shuffle命令，如下所示：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Sometimes, the shuffle command is quite slow and you may want to consider using
    the `perl` one-liner in the case of large files:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，shuffle命令可能会比较慢，在处理大文件时，你可以考虑使用`perl`单行命令：
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Dividing into training and validation
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 划分训练集和验证集
- en: 'Model performance evaluation should always be done on independent data. You
    should always separate out your whole dataset into train and test sets. But, dividing
    too much also reduces the amount of data that you have for training, so 80% is
    a good midpoint. You can divide the file into an 80-20 split using the following
    command:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 模型性能评估应始终在独立数据上进行。你应该始终将整个数据集分为训练集和测试集。但是，过度划分数据集会减少用于训练的数据量，因此80%的比例是一个不错的中间值。你可以使用以下命令将文件划分为80-20的比例：
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Model building
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型构建
- en: In this section, we take a look at how to go about the steps of model training
    and evaluation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨模型训练和评估的步骤。
- en: Model training
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练
- en: 'Now, you can start the training step:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以开始训练步骤：
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output will be shown when training, and at the end you should get an output
    similar to this:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 训练时将显示输出，最终你应该会看到类似以下的输出：
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If you now check the `result/yelp/` directory, you should be able to see two
    files with extensions `.vec` and `.bin`. The `.bin` file is the trained classifier.
    The `.vec` file has all the words with the vectors for the individual words. You
    can open the `.vec` file. It is just a text file. However, take care to open it
    using a lightweight text editor such as Sublime Text or Notepad++, as it will
    be a big file. Or, just use command line tools such as `head` or `tail`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在检查`result/yelp/`目录，你应该能看到两个扩展名为`.vec`和`.bin`的文件。`.bin`文件是训练好的分类器，而`.vec`文件包含了所有单词的词向量。你可以打开`.vec`文件，它只是一个文本文件。然而，注意使用轻量级文本编辑器，如Sublime
    Text或Notepad++，来打开它，因为它会是一个大文件。或者，也可以使用命令行工具，如`head`或`tail`。
- en: 'Here are some of the vectors created in our case. The first line has the dimensions
    of the vectors, which are (`1459620`, `100`) in our case. The next two lines are
    the vectors for `.` and `the`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们案例中创建的一些向量。第一行包含向量的维度，在我们案例中是（`1459620`，`100`）。接下来的两行是`.`和`the`的向量：
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Model testing and evaluation
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型测试与评估
- en: Now that you know how to create model files in fastText, you will need to test
    and check the performance of your model, and report its efficacy in real terms.
    This can be done using various performance measures.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道如何在fastText中创建模型文件，你需要测试并检查模型的性能，并以实际的方式报告其效能。这可以通过各种性能评估指标来完成。
- en: Precision and recall
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确度和召回率
- en: 'To test the accuracy of a classification model, two parameters that are very
    popular and are supported by fastText are precision and recall. Recall is the
    percentage of labels that are correctly recalled of all the labels that actually
    exist, and precision is the percentage of all the labels that were predicted correctly.
    These two parameter values can be checked using the following command:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试分类模型的准确性，有两个非常流行的参数，它们被fastText支持，那就是精确度（precision）和召回率（recall）。召回率是正确召回的标签占所有实际存在标签的百分比，而精确度是所有正确预测标签占总预测标签的百分比。可以使用以下命令检查这两个参数值：
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The precision and recall are currently at 68%. Let's optimize some parameters
    and see if we can make the model better.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 目前精确度和召回率为68%。让我们优化一些参数，看看是否能改进模型。
- en: Confusion matrix
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: Now that you have a model, you can also see the performance of the model with
    respect to the different labels using the confusion matrix. Along with precision
    and recall, confusion matrices give a good idea of the **true negatives** (**TN**)
    and the **false positives** (**FP**) as well. In an ideal world, all the diagonals
    have high values, while all the remaining cells have negligible values, but in
    a real scenario, you may need to chose if you are OK with having high FP values
    or high **false negative** (**FN**) values.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了一个模型，你还可以使用混淆矩阵查看模型在不同标签上的性能。与精确度和召回率一起，混淆矩阵能够很好地展示**真阴性**（**TN**）和**假阳性**（**FP**）。在理想情况下，所有对角线上的值都应该很高，而其余单元格的值应接近于零，但在实际情况下，你可能需要选择是否能接受较高的假阳性（FP）值，或者较高的**假阴性**（**FN**）值。
- en: 'To get the confusion matrix, you will need to do some post-processing. Separate
    the sentences from the labels. Then, using the predict command, you will be able
    to predict the label for each test line. A Python script is provided, which can
    be used to get the confusion matrix:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得混淆矩阵，你需要进行一些后处理。将句子与标签分开。然后，使用预测命令，你将能够预测每个测试行的标签。提供了一个Python脚本，可以用来获取混淆矩阵：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You can download the Python script from this gist: [https://gist.github.com/loretoparisi/41b918add11893d761d0ec12a3a4e1aa#file-fasttext_confusion_matrix-py](https://gist.github.com/loretoparisi/41b918add11893d761d0ec12a3a4e1aa#file-fasttext_confusion_matrix-py).
    Or, you can get it from the GitHub repository:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从这个gist下载Python脚本：[https://gist.github.com/loretoparisi/41b918add11893d761d0ec12a3a4e1aa#file-fasttext_confusion_matrix-py](https://gist.github.com/loretoparisi/41b918add11893d761d0ec12a3a4e1aa#file-fasttext_confusion_matrix-py)。或者，你也可以从GitHub仓库获取它：
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Hyperparameters
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数
- en: There are multiple hyperparameters that can be supplied when training the model
    to improve the model. Take a look at some of the hyperparameters here and their
    effects on model training.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，有多个超参数可以调整来提高模型性能。看看这里的一些超参数及其对模型训练的影响。
- en: Epoch
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 轮次
- en: 'By default, fastText takes a look at each data point five times. You can change
    this using the `-epoch` command. In the following example, we change the epoch
    parameter to 25 and see whether there is any improvement in our model:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，fastText会查看每个数据点五次。你可以使用`-epoch`命令来改变这一点。在下面的示例中，我们将epoch参数改为25，看看是否会改善我们的模型：
- en: '[PRE21]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The result of the model is 68.6% for precision and recall, which is only a
    0.1% improvement:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的结果是68.6%的精确度和召回率，只有0.1%的提升：
- en: '[PRE22]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Learning rate
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习率
- en: 'This may be because we already have a huge number of samples. Another important
    hyperparameter that we can change is the learning rate, using the `-lr` argument.
    The learning rate controls how "fast" the model updates during training. This
    parameter controls the size of the update that is applied to the parameters of
    the models. Changing the learning rate from 0.025 to 1.0 means that the updates
    that are applied to the model are 40 times larger. In our model, we could also
    see that the learning rate was becoming `0` at the end. This means that the model
    was not learning at all by the end. Lets try to make the learning rate as `1`
    and see what happens:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是因为我们已经有了大量的样本。我们可以改变的另一个重要超参数是学习率，使用`-lr`参数。学习率控制模型在训练过程中的“更新速度”。这个参数控制应用于模型参数的更新大小。将学习率从0.025改为1.0意味着应用于模型的更新将变大40倍。在我们的模型中，我们还可以看到，学习率在最后变成了`0`。这意味着到最后，模型完全没有学习。我们试着将学习率设为`1`，看看会发生什么：
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The result for this model was the same as before. There was no difference when
    changing the learning rate:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型的结果和之前一样。改变学习率时没有任何区别：
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: N-grams
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: N-gram
- en: 'We have one more hyperparameter that may have a huge influence on the performance
    of the model. By default, when creating word vectors for the model, unigrams are
    used. Unigrams are n-grams where *n* is 1\. N-grams can be best explained using
    the following diagram:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一个可能对模型性能产生巨大影响的超参数。默认情况下，在为模型创建词向量时，会使用单一词（unigrams）。单一词是n-gram，其中*n*为1。n-gram可以通过以下图示来解释：
- en: '![](img/00006.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00006.jpeg)'
- en: 'Source: [https://stackoverflow.com/a/45477420/5417164](https://stackoverflow.com/a/45477420/5417164)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://stackoverflow.com/a/45477420/5417164](https://stackoverflow.com/a/45477420/5417164)
- en: 'You also can fix the value of `N` in fastText using the `-wordNgrams` parameter:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用`-wordNgrams`参数在fastText中固定`N`的值：
- en: '[PRE25]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, the precision and recall are 71.8%, which is a 3.2% improvement. Lets
    try for some more:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，精确度和召回率为71.8%，提升了3.2%。我们再试试更多的调整：
- en: '[PRE26]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Making N = 3 resulted in a decrease in performance. So, let's keep the value
    of N as 2.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 将N设置为3导致了性能下降。所以，让我们将N的值保持为2。
- en: 'You can combine all the parameters to create the new model:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以结合所有这些参数来创建新的模型：
- en: '[PRE27]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Start with pretrained word vectors
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从预训练的词向量开始
- en: If the text corpus that you have is not huge, it is generally advised to start
    with some pretrained word vectors for the language that you are training the classifier
    for, or the classification results may be poor. How to create word vectors from
    your corpus is handled in depth in the next section.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你拥有的文本语料库不大，通常建议从为你训练分类器的语言预训练的词向量开始，否则分类结果可能较差。如何从你的语料库创建词向量将在下一节中深入讨论。
- en: '[PRE28]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In our case, there was not much improvement. The precision and recall increased
    marginally and stood at 68.5%.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，结果并没有显著改善。精度和召回率略有提高，达到了68.5%。
- en: Finding the best fastText hyperparameters
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找最佳的 fastText 超参数
- en: FastText has a lot of hyperparameters that you can optimize to find the right
    balance for your model. For a classifier, you can start with the loss functions,
    see whether changing the character n-grams makes sense, and see whether changing
    the learning rate and dimensions have any effect.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: FastText 有许多可以优化的超参数，以便找到模型的最佳平衡点。对于分类器，你可以从损失函数开始，查看改变字符n-grams是否有意义，看看改变学习率和维度是否会产生效果。
- en: A popular algorithm for implementing hyperparameter optimization is using the
    grid-search approach. Since your aim is to find a good model, you will have a
    training dataset and a test dataset. Let's say the training data is the `train.txt`
    file and the test data is the `test.txt` file. You are essentially solving an
    optimization problem (P), which is the function of the combination of weights
    in this case *w*^' and the hyperparameters, be in n-grams, learning rate or epochs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 实现超参数优化的一个流行算法是使用网格搜索方法。由于你的目标是找到一个好的模型，你将拥有一个训练数据集和一个测试数据集。假设训练数据是`train.txt`文件，测试数据是`test.txt`文件。你实际上是在解决一个优化问题（P），该问题是权重组合函数，在本例中是
    *w*^' 和超参数的组合，可能是n-grams、学习率或训练轮次。
- en: 'So, you understand that solving the optimization problem for a fixed set of
    values for the hyperparameters gives you a specific model. Since the optimal model
    (call it model*) is a function of the hyperparameters, we can write it as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你明白了，对于一组固定的超参数值，解决优化问题会得到一个特定的模型。由于最佳模型（可以称之为模型*）是超参数的函数，我们可以将其表示如下：
- en: '![](img/00007.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00007.jpeg)'
- en: Now, you can use this model* to predict on the training data to get the accuracy.
    Thus, the goal of hyperparameter optimization is to find the set of hyperparameters
    that gives the highest accuracy.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以使用这个模型*来预测训练数据的准确性。因此，超参数优化的目标是找到一组超参数，使得准确性最高。
- en: Note that this calculation of the best model is going to be quite expensive.
    There is no magic mantra, no magic formula to find the hyperparameters for the
    best model. Just taking one hyperparameter, the learning rate, would make the
    calculation impractical. This is a continuous variable and you would need to feed
    in each specific value, compute the model, and check the performance.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，计算最佳模型将是相当昂贵的。没有神奇的咒语，也没有魔法公式可以找到最佳模型的超参数。仅仅拿学习率作为超参数，就会让计算变得不切实际。学习率是一个连续变量，你需要输入每一个特定值，计算模型，并检查其性能。
- en: 'Therefore, we resort to a grid search: basically, picking a bunch of values
    for the hyperparameters based on a heurestic, and based on all the combinations
    of the values, feeding them into the calculation and picking the set of values
    with the best performance.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们采用网格搜索：基本上，根据启发式方法选择一组超参数值，并根据所有这些值的组合，将它们输入计算，最终选出性能最好的那一组值。
- en: This is called a grid search because the set of values that are considered,
    when plotted on a graph, look like a grid.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这叫做网格搜索，因为当考虑的值集在图表上绘制时，它们看起来像一个网格。
- en: 'How you can implement this is by defining an array of values for your hyperparameters:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过为你的超参数定义一个值数组来实现这一点：
- en: '[PRE29]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, we have a global variable where we will save the individual variables,
    as and when we find better models, and initialize them to be 0\. We will also
    have a global performance variable to store the present best performance and set
    it to 0 initially. In this case, since we are experimenting with three hyperparameters,
    we will have the variable as length 3, as you can see here:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个全局变量，用于保存每次找到的更好模型的单个变量，并将其初始化为0。我们还将有一个全局性能变量来存储当前最佳性能，并初始设置为0。在这个案例中，由于我们正在尝试三种超参数，因此我们将该变量的长度设置为3，如下所示：
- en: '[PRE30]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now comes the implementation of the `for` loops that will cycle through all
    the combinations of the values. Note that the depth of the `for` loop would be
    based on the number of hyperparameters that you are cycling through:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在开始实现`for`循环，循环将遍历所有值的组合。请注意，`for`循环的深度将取决于你正在循环的超参数数量：
- en: '[PRE31]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As you can probably guess, since we are checking for two values each for the
    three hyperparameters, the number of times training will happen is 2 x 2 x 2 =
    8\. So, if each training step takes, say, 5 minutes, that would mean that the
    total process will take 8 x 5 minutes or 40 minutes.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能猜到的，由于我们检查三个超参数的每个值都有两个选项，训练将进行 2 x 2 x 2 = 8 次。因此，如果每次训练步骤大约需要 5 分钟，那么整个过程将需要
    8 x 5 分钟，即 40 分钟。
- en: 'Now, let''s go to the mean. Here is the training step:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入平均值部分。以下是训练步骤：
- en: '[PRE32]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Once the training is done, then comes the test phase. We save the test data
    to a file so that we can compare the results later:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，接下来是测试阶段。我们将测试数据保存到文件中，以便稍后比较结果：
- en: '[PRE33]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now comes the comparison and saving the best models:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是比较并保存最佳模型：
- en: '[PRE34]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now, you can extend this script to bring in more hyperparameters as well. You
    can find the whole code in the repo in the file.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以扩展此脚本，加入更多超参数。你可以在仓库中的文件中找到完整代码。
- en: Model quantization
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型量化
- en: With the help of model quantization, the fastest models have the ability to
    fit on mobile and on small devices such as Raspberry Pi. Since the code is open
    source, there are Java and Swift libraries that can be used to load the quantized
    models and serve them in Android and iOS apps respectively.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 借助模型量化，最快的模型能够适应移动设备和小型设备，如 Raspberry Pi。由于代码是开源的，现有 Java 和 Swift 库可以用来加载量化后的模型，并分别在
    Android 和 iOS 应用中提供服务。
- en: The algorithm for compressing the fastText models was created with collaboration
    between fastText and the **Facebook AI Research** (**FAIR**) team. This results
    in the reduction of fastText models by a huge amount. FastText models that are
    of the range of hundreds of MB get reduced to around 1-2 MB.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩 fastText 模型的算法是 fastText 与 **Facebook AI Research** (**FAIR**) 团队合作创建的。这导致
    fastText 模型的体积大幅减少。原本为数百 MB 的 fastText 模型，经过压缩后仅为 1-2 MB。
- en: 'Implementing quantization can be done using the quantize argument. You will
    need to train a model with the normal route though:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 实现量化可以通过使用 quantize 参数来完成。不过，你仍然需要通过常规路线训练模型：
- en: '[PRE35]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Note that there is a huge difference between the quantized model and the unquantized
    one:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，量化模型与未量化模型之间存在巨大差异：
- en: '[PRE36]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The `.bin` file is about 466 MB, while the quantized model is just 1.6 MB.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`.bin` 文件大小约为 466 MB，而量化后的模型只有 1.6 MB。'
- en: Interestingly there seems to be a slight increase in precision and recall values.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，精度和召回率的数值似乎有所提升。
- en: '[PRE37]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: So you get almost no difference in performance and a good saving in space. You
    can now deploy this model in a smaller device. In the next chapter, we will discuss
    how this model quantization works. Also, in [Chapter 7](part0160.html#4OIQ00-05950c18a75943d0a581d9ddc51f2755),
    *Deploying Models to Web and Mobile*, we will discuss how you can package a quantized
    model as part of an Android app.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你几乎不会在性能上看到差异，同时节省了大量空间。你现在可以将此模型部署到更小的设备上。在下一章中，我们将讨论模型量化的工作原理。此外，在[第七章](part0160.html#4OIQ00-05950c18a75943d0a581d9ddc51f2755)，*将模型部署到
    Web 和移动设备*，我们将讨论如何将量化模型打包为 Android 应用的一部分。
- en: Unfortunately, quantization only works for supervised models for now, but this
    may change in the future, so keep your fastText installation updated.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，目前量化只适用于有监督模型，但未来可能会有所改变，所以请保持你的 fastText 安装版本更新。
- en: Understanding the model
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解模型
- en: 'Once the model is created, you can now see the parameters that were used while
    generating the model. This can be useful later when you are thinking deeper about
    your data and would like to change some model parameters, or for general documentation
    purposes:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型创建完成，你现在可以看到在生成模型时使用的参数。这在你深入思考数据并希望更改某些模型参数时，或作为一般文档记录时非常有用：
- en: '[PRE38]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The `dict` parameter gives information on the dictionary of words that was
    used in training. In the preceding training procedure, 1,459,625 words have been
    used, which can be seen as follows. `was` is used 8,272,495 times, `crinkle-also`
    is used only once in the whole set of sentences, and so on. It also gives information
    on whether the word is used as a word or a label. As you can see, the labels are
    listed at the end:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`dict` 参数提供了训练中使用的单词字典信息。在之前的训练过程中，使用了 1,459,625 个单词，具体可以见下文。`was` 被使用了 8,272,495
    次，`crinkle-also` 在整个句子集合中只使用了一次，等等。它还提供了单词是作为单词还是标签使用的信息。如你所见，标签位于列表的末尾：'
- en: '[PRE39]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The rows of the dump of input and output correspond to the parameters of the
    model. In our model, the first 1,459,620 rows of input are the vectors associated
    to the individual words, while the remaining 2 million rows are used to represent
    subwords. Those 2 million subwords were chosen to represent the overall meaning
    and can be understood from the bucket parameter in the output for the dump of
    the args as well. The rows of output are the vectors associated to the context
    or our labels. Usually, when learning unsupervised word representations, these
    are not kept after training:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和输出的行在数据转储中对应于模型的参数。在我们的模型中，前 1,459,620 行输入是与单个单词相关联的向量，而剩余的 200 万行则用于表示子词。这
    200 万个子词被选择来表示整体意义，并且可以通过数据转储中 `args` 的输出中的 bucket 参数理解。输出行是与上下文或我们的标签相关联的向量。通常，在学习无监督词表示时，这些向量在训练后不会保留下来：
- en: '[PRE40]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The transformations mentioned in this section can be seen in the `transformations.sh`
    file in the GitHub repository.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中提到的转换可以在 GitHub 仓库中的`transformations.sh`文件中查看。
- en: FastText word vectors
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FastText 词向量
- en: 'The second major focus of fastText is creating word embeddings for the input
    text. During training, fastText looks at the supplied text corpus and forms a
    high-dimensional vector space model, where it tries to encapsulate as much meaning
    as possible. The aim of creating the vectors space is that the vectors of similar
    words should be near to each other. In fastText, these word vectors are then saved
    in two files, similar to what you have seen in text classification: a `.bin` file
    and a `.vec` file.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: fastText 的第二个主要关注点是为输入文本创建词嵌入。在训练过程中，fastText 会查看提供的文本语料库，并形成一个高维向量空间模型，其中它尝试尽可能地封装语义。创建词向量空间的目标是让相似的单词向量尽可能接近。在
    fastText 中，这些词向量将保存在两个文件中，类似于你在文本分类中看到的：一个 `.bin` 文件和一个 `.vec` 文件。
- en: In this section, we will look at the creation and use of word vectors using
    the fastText command line.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过使用 fastText 命令行来创建和使用词向量。
- en: Creating word vectors
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建词向量
- en: We will now take a look at how to go about creating word vectors in fastText.
    You will probably be working with and building a solution for a specific domain,
    and in such a case, my advice would be to generate the raw text from the specific
    domain. But in cases where the raw text is not available to you, then you can
    use the help of Wikipedia, which is a huge collection of raw text in multiple
    languages.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍如何在 fastText 中创建词向量。你可能正在处理并为某个特定领域构建解决方案，在这种情况下，我的建议是从该特定领域生成原始文本。但如果原始文本不可用，你可以借助维基百科，它是一个包含多种语言的大型原始文本集合。
- en: Downloading from Wikipedia
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从维基百科下载
- en: To start with word vectors, you will need data or a text corpus. If you are
    lucky, you have the text corpus available to you. If you are not so lucky, which
    you will eventually be if you are interested in solving interesting problems in
    NLP, you will not have the data with you. In those cases, Wikipedia is your friend.
    The best thing about Wikipedia is that it is the best source of written text in
    more than 250 languages from around the world. Granted that is a minuscule number
    compared to the number of languages there are, but still that will probably be
    enough for most of your use cases. And if you are working in a language for which
    there are not enough Wikipedia resources, maybe you should raise awareness of
    how important Wikipedia is in your language community and ask the community to
    contribute to Wikipedia more. Once you know your target language, you can download
    the Wikipedia corpus using the `get-wikimedia.sh` file. You can get this file
    from the GitHub repository of fastText. A slightly updated version of the file
    can be copied from the *Appendix*.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用词向量，你需要数据或文本语料库。如果你幸运的话，已经有了文本语料库。如果你不那么幸运，尤其是在你有兴趣解决 NLP 中有趣问题时，可能没有数据可用。在这种情况下，维基百科是你的朋友。维基百科的最佳之处在于，它是全球超过
    250 种语言的书面文本的最佳来源。尽管这个数量相比于实际语言种类来说微不足道，但对于大多数用例来说，可能已经足够了。如果你正在处理某种语言，但维基百科资源不够丰富，也许你应该在你的语言社区中提高维基百科重要性的意识，并呼吁社区更多地贡献内容。一旦你确定了目标语言，就可以使用`get-wikimedia.sh`文件下载维基百科语料库。你可以从
    fastText 的 GitHub 仓库获取此文件。稍作更新的版本可以从*附录*中复制。
- en: Use the `get-wikimedia.sh` file to download the Wikipedia corpus.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`get-wikimedia.sh`文件下载维基百科语料库。
- en: 'You can get the list of all the languages that Wikipedia has articles on at
    this link: [https://meta.wikimedia.org/wiki/List_of_Wikipedias](https://meta.wikimedia.org/wiki/List_of_Wikipedias).
    At this link, the list of languages is given in this format:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个链接获取维基百科上所有语言的文章列表：[https://meta.wikimedia.org/wiki/List_of_Wikipedias](https://meta.wikimedia.org/wiki/List_of_Wikipedias)。在这个链接中，语言列表是以这种格式给出的：
- en: '![](img/00008.jpeg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00008.jpeg)'
- en: 'It is the third column that needs your attention. Note down the third value
    for your language of choice and run `bash get-wikimedia.sh`:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是第三列。记录下你选择的语言的第三个值，并运行`bash get-wikimedia.sh`：
- en: '[PRE41]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'You will receive a BZ2 file. To uncompress a BZ2 file, run the following command:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你将收到一个BZ2文件。要解压BZ2文件，运行以下命令：
- en: '[PRE42]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: If you open the file (be careful while doing this, the file is huge), you will
    find a lot of unnecessary stuff, such as HTML tags and links. So, you will need
    to clean the text with the `wikifil.pl` script, which was written by Matt Mahoney.
    This script is distributed with the fastText GitHub files.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打开这个文件（在操作时请小心，文件很大），你会发现很多不必要的内容，比如HTML标签和链接。所以，你需要用`wikifil.pl`脚本清理文本，这个脚本是Matt
    Mahoney编写的，并且随着fastText的GitHub文件一起分发。
- en: Text normalization
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本规范化
- en: 'If you have downloaded the English corpus, you can use the `wikifil.pl` script:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你下载了英文语料库，你可以使用`wikifil.pl`脚本：
- en: '[PRE43]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'In our case, we have the Japanese text and hence we will be using WikiExtractor
    ([https://github.com/attardi/wikiextractor](https://github.com/attardi/wikiextractor))
    to extract the text from the BZ2 file:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们有日文文本，因此我们将使用WikiExtractor（[https://github.com/attardi/wikiextractor](https://github.com/attardi/wikiextractor)）从BZ2文件中提取文本：
- en: '[PRE44]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'There are still a lot of tags and English words that are part of the tags.
    You will need to do some more processing and text cleaning to make the corpus
    ready for training:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然有很多标签和属于标签的一部分的英语单词。你需要进行一些额外的处理和文本清理，以便准备好语料库进行训练：
- en: '[PRE45]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Now, you can go ahead and start the training process. We will keep the English
    numbers.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以开始训练过程了。我们将保留英语数字。
- en: Create word vectors
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建单词向量
- en: 'Now, you can create the word vectors. Create the `result` directory:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以创建单词向量了。创建`result`目录：
- en: '[PRE46]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'There are two algorithms that are supported by fastText for creating word vectors,
    `skipgram` and `cbow`:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: fastText支持两种创建单词向量的算法，`skipgram`和`cbow`：
- en: '[PRE47]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: and
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '[PRE48]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Model evaluation
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估
- en: Model evaluation for word vectors is largely a manual process. In such cases,
    you can try some samples of the words and see if the model gives adequate results.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 单词向量的模型评估在很大程度上是一个手动过程。在这种情况下，你可以尝试一些单词样本，看看模型是否给出了充分的结果。
- en: Some of the methods that you can use for model evaluation are looking at the
    nearest neighbors of the model and looking at some word analogies. One popular
    method is through t-SNE visualizations. We will look at t-SNE visualizations in
    [Chapter 4](part0098.html#2TEN40-05950c18a75943d0a581d9ddc51f2755),
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用来评估模型的一些方法包括查看模型的最近邻以及查看一些单词的类比。一个常用的方法是通过t-SNE可视化。我们将在[第4章](part0098.html#2TEN40-05950c18a75943d0a581d9ddc51f2755)中讨论t-SNE可视化，
- en: '*Sentence Classification in FastText*.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '*FastText中的句子分类*。'
- en: Nearest neighbors
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最近邻
- en: 'To get the nearest neighbors of a given word, pass the `nn` argument, and then
    you will need to give the path to the BIN file. For brevity, we will show only
    three results here:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取给定单词的最近邻，传递`nn`参数，然后你需要提供BIN文件的路径。为了简洁起见，我们在这里只显示三个结果：
- en: '[PRE49]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Similar words to sleep (![](img/00013.jpeg)) give the previous results, which
    mean "return to the mainland"; "Hachinosu," which is a peak in Antarctica; and
    "national election." The results are random in our case and therefore not good.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于“sleep”的单词（![](img/00013.jpeg)）给出了先前的结果，意思是“返回本土”；“Hachinosu”，这是南极洲的一个山峰；以及“全国选举”。在我们的例子中，结果是随机的，因此不好。
- en: Word analogies
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词类比
- en: 'Word analogies are a good way of finding out whether the model is working or
    not. How analogies work is that two groups of words with similar relationships
    should be separated by similar distances in the vector space. So, when you provide
    words for man, woman, and king, the result should be queen, as in a good vector
    space, the distance between word vector denoting "man" and the word vector denoting
    "woman" should be close to the distance between the word vector denoting "king"
    and the word vector denoting "queen." The command shows 10, but here only the
    top three are shown:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 单词类比是判断模型是否有效的好方法。类比的原理是，具有相似关系的两个单词组在向量空间中应该彼此距离相似。因此，当你提供“man”（男人）、“woman”（女人）和“king”（国王）等单词时，结果应该是“queen”（女王），因为在一个好的向量空间中，表示“man”和表示“woman”的词向量之间的距离应接近表示“king”和表示“queen”的词向量之间的距离。命令显示了10个结果，但这里只显示了前三个：
- en: '[PRE50]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The meaning of the symbols in this code block are "cracking," "Monkey King,"
    and "King", which are not very helpful in our case.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码块中的符号含义分别是“cracking”（破解）、“Monkey King”（孙悟空）和“King”（国王），在我们的情况下并没有太大帮助。
- en: Other parameters when training
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练时的其他参数
- en: Similar to supervised learning, you can change various hyperparameters and see
    whether the models that are created work better. So, you can fix the minimal number
    of word occurrences with the `-minCount` parameter and the maximum length of word
    n-grams with the `-wordNgrams` parameter. You can change the number of buckets
    that fastText uses to hash the word and character n-grams to limit memory use.
    If you have a huge memory in your system, you can change this bucket parameter
    by passing a larger value than 2 million to see whether your model performance
    increases using the `-bucket` parameter. You can change the minimum and maximum
    length of character n-grams using the parameters `-minn` and `-maxn`. Change the
    sampling threshold using `-t`, change the learning rate using `-lr`, change the
    rate of updates for the learning rate using `-lrUpdateRate`, change the dimensions
    of the word vectors using `-dim`, change the size of the context window using
    `-ws`, change the number of epochs, which is the number of times each row is looked
    at during training, from the default 5 using `-epoch`, change the number of negatives
    sampled using `-neg`, and change the loss function that is used from the default
    **ns** (**negative sampling**) to softmax or **hs** (**hierarchical softmax**).
    The default number of threads used is 12, but generally people have four cores
    or eight cores, and hence you can change the number of threads to optimally use
    the cores using the `-thread` parameter.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于监督学习，你可以更改各种超参数，并查看生成的模型是否表现更好。因此，你可以使用`-minCount`参数固定最小的单词出现次数，使用`-wordNgrams`参数固定单词n-gram的最大长度。你还可以更改fastText用于哈希单词和字符n-gram的桶的数量，从而限制内存使用。如果你的系统内存很大，可以通过传递大于200万的值来更改这个桶参数，看看使用`-bucket`参数时模型的表现是否有所提高。你可以使用`-minn`和`-maxn`参数更改字符n-gram的最小和最大长度。使用`-t`更改采样阈值，使用`-lr`更改学习率，使用`-lrUpdateRate`更改学习率更新的频率，使用`-dim`更改词向量的维度，使用`-ws`更改上下文窗口的大小，使用`-epoch`更改训练时每一行查看的次数（默认5次），使用`-neg`更改采样的负样本数量，以及更改使用的损失函数，从默认的**ns**（**负采样**）更改为softmax或**hs**（**分层softmax**）。默认使用的线程数是12，但通常计算机有四核或八核，因此你可以使用`-thread`参数更改线程数以优化核心使用。
- en: Out of vocabulary words
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词汇表外单词
- en: 'FastText also supports out of vocabulary words. FastText is able to do that
    because it not only keeps track of word-level N-grams, but also character-level
    n-grams. So, things like "learn," "learns," and "learned" look similar to it.
    To get the vectors for out of vocabulary words, you have to use binary models,
    which means the model files with the `.bin` extension:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: FastText也支持处理词汇表外的单词。FastText之所以能够做到这一点，是因为它不仅跟踪单词级别的n-gram，还跟踪字符级别的n-gram。因此，像“learn”（学习）、“learns”（学习）和“learned”（学会）这些单词对于它来说看起来是相似的。要获得词汇表外单词的向量，你必须使用二进制模型，这意味着模型文件具有`.bin`扩展名：
- en: '[PRE51]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Facebook word vectors
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Facebook词向量
- en: Facebook has released a large number of pretrained word vectors based on Wikipedia
    and common crawling, which you can download from their website and use in your
    projects ([https://fasttext.cc/docs/en/pretrained-vectors.html](https://fasttext.cc/docs/en/pretrained-vectors.html)).
    The common crawl models are CBOW models, and the wiki models are skip-gram models.
    Vectors are of dimensions 300 and character n-grams of length 5 are used, and
    a window size of 5 and 10 negatives are used.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook发布了大量基于维基百科和常规爬虫的预训练词向量，你可以从他们的网站下载并在你的项目中使用（[https://fasttext.cc/docs/en/pretrained-vectors.html](https://fasttext.cc/docs/en/pretrained-vectors.html)）。常规爬虫模型是CBOW模型，而维基百科模型是skip-gram模型。词向量的维度是300，使用了长度为5的字符n-gram，窗口大小为5，负样本数为10。
- en: Using pretrained word vectors
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预训练的词向量
- en: 'You can use pretrained word vectors for your supervised learning task. This
    has been discussed briefly under supervised learning. An example command is shown
    as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在监督学习任务中使用预训练词向量。这在监督学习部分已简要讨论过。一个示例命令如下所示：
- en: '[PRE52]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: There are some things that need to be taken care of while using pretrained vectors.
    You can find them at [https://stackoverflow.com/questions/47692906/fasttext-using-pre-trained-word-vector-for-text-classification](https://stackoverflow.com/questions/47692906/fasttext-using-pre-trained-word-vector-for-text-classification).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用预训练词向量时需要注意一些事项。你可以在[https://stackoverflow.com/questions/47692906/fasttext-using-pre-trained-word-vector-for-text-classification](https://stackoverflow.com/questions/47692906/fasttext-using-pre-trained-word-vector-for-text-classification)找到相关信息。
- en: Machine translation
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器翻译
- en: 'Facebook has done a lot of work on neural machine translation in the form of
    MUSE. MUSE is a library for multilingual unsupervised and supervised word embeddings.
    MUSE uses and is built on top of fastText word embeddings. The word embeddings
    that you get with fastText are monolingual, and hence the vectors need to be aligned
    to effectively translate from one language to the other. As part of MUSE, see
    the following features:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook在神经机器翻译方面做了大量工作，MUSE就是其中之一。MUSE是一个多语言无监督和监督式词嵌入库。MUSE使用并基于fastText词嵌入。通过fastText得到的词嵌入是单语的，因此这些向量需要对齐才能有效地进行语言间翻译。作为MUSE的一部分，以下是一些功能：
- en: fastText Wikipedia supervised word embeddings for 30 languages were released.
    These word embeddings are aligned in a single vector space.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fastText发布了适用于30种语言的维基百科监督式词嵌入。这些词嵌入被对齐到一个单一的向量空间中。
- en: 110 large-scale ground truth bilingual dictionaries were also released so that
    you can train your own models.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还发布了110个大规模的双语词典，供你训练自己的模型。
- en: Summary
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you took a look at how you can combine the command line text
    transformation capabilities of the *Nix shell and the fastText library to implement
    a training, validation, and prediction pipeline. The commands that you explored
    in this chapter are not only versatile, they are fast as well. Having good mastery
    over the command line, along with a fastText app, should enable you to create
    fast prototypes and deploy them in a fast-paced environment. With this, the first
    part of the book is complete.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了如何结合*Nix shell的命令行文本转换能力*和fastText库，来实现一个训练、验证和预测的流水线。你在本章中探索的命令不仅功能强大，而且执行速度也很快。掌握好命令行操作，再配合fastText应用，应该能帮助你快速创建原型并在快节奏的环境中部署它们。至此，本书的第一部分已经完成。
- en: The next part of the book is about the theory and algorithms that have gone
    into making the package, with the next chapter being about unsupervised learning
    using fastText.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的下一部分将介绍构建这个包的理论和算法，下一章将介绍如何使用fastText进行无监督学习。
