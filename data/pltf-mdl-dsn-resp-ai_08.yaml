- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Fairness in Model Optimization
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型优化中的公平性
- en: '**Machine Learning** (**ML**) has made massive strides in recent years, with
    applications in everything from finance to healthcare. However, these systems
    can often be opaque and biased against certain groups of people. In order for
    ML to live up to its potential, we must ensure that it is fair and unbiased.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）在近年来取得了巨大的进步，应用范围从金融到医疗保健。然而，这些系统往往不透明，并且可能对某些群体存在偏见。为了让ML发挥其潜力，我们必须确保它是公平且无偏的。'
- en: In [*Chapter 7*](B18681_07.xhtml#_idTextAnchor146), we discussed the concept
    of fairness in the context of data generation. In this chapter, we will cover
    different optimization constraints and techniques that are essential to optimizing
    and obtaining fair ML models. The focus of this chapter is to enlighten you about
    new custom optimizers, unveiled by research that can serve to build fair supervised,
    unsupervised, and semi-supervised ML models. In a broader sense, you will learn
    the foundational steps to create and define model constraints that can be used
    by different optimizers during the training process. You will also gain an understanding
    of how to evaluate such constraint-based models with proper metrics, and the extra
    training overheads incurred when employing the optimization techniques, which
    will enable you to design your own algorithms.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第七章*](B18681_07.xhtml#_idTextAnchor146)中，我们讨论了数据生成中的公平性概念。在本章中，我们将介绍优化约束和技术，这些对于优化和获取公平的机器学习（ML）模型至关重要。本章的重点是让你了解由研究揭示的新型自定义优化器，这些优化器可用于构建公平的监督式、无监督式和半监督式ML模型。从更广泛的角度来看，你将学习创建和定义模型约束的基础步骤，这些约束可以在训练过程中被不同的优化器使用。你还将了解如何使用适当的指标评估这些基于约束的模型，以及使用优化技术时所带来的额外训练开销，这将帮助你设计自己的算法。
- en: 'In this chapter, the following topics will be covered:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The concept of fairness as applied to ML
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习中公平性概念的应用
- en: Explicit and implicit mitigation of fairness issues
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公平性问题的显性与隐性缓解
- en: Fairness constraints for a classification task
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类任务的公平性约束
- en: Fairness constraints for a regression task
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归任务的公平性约束
- en: Fairness constraints for a clustering task
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类任务的公平性约束
- en: Fairness constraints in reinforcement learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习中的公平性约束
- en: Fairness constraints for a recommendation task
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐任务的公平性约束
- en: Challenges of fairness
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公平性挑战
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter requires you to have Python 3.8 along with some Python packages:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求你具备Python 3.8及以下一些Python库：
- en: TensorFlow 2.7.0
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 2.7.0
- en: NumPy
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: Matplotlib
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib
- en: The notion of fairness in ML
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的公平性概念
- en: Fairness is a subjective term. It has different meanings depending on culture,
    time, race, and so on. We could write endlessly on the topic of fairness in society;
    however, in this chapter, our focus is fairness with respect to ML. In [*Chapter
    7*](B18681_07.xhtml#_idTextAnchor146), you became familiar with the concept of
    fairness and how data plays an important role in the fairness of your ML model.
    In this chapter, we will expand on the concept of fairness.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性是一个主观的术语。根据文化、时间、种族等不同，公平的含义各不相同。我们可以不厌其烦地讨论社会中的公平话题；然而，在本章中，我们关注的是与机器学习相关的公平性。在[*第七章*](B18681_07.xhtml#_idTextAnchor146)中，你已经熟悉了公平性的概念，以及数据在你ML模型的公平性中所扮演的重要角色。在本章中，我们将进一步扩展公平性的概念。
- en: 'As you might have already realized, fairness does not have a universal definition.
    For example, seen from the point of view of an organization, fairness can have
    three different dimensions (Cropanzano et al., 2001\. See *Further reading*):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经意识到的那样，公平性并没有统一的定义。例如，从组织的角度来看，公平可以有三个不同的维度（Cropanzano 等，2001年，见*进一步阅读*）：
- en: '**Distributive fairness**: The algorithm should be fair in allocating important
    resources, for example, in hiring.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分配公平性**：算法在分配重要资源时应当公平，例如在招聘中。'
- en: '**Interactional fairness**: The algorithm should be able to explain the rationale
    behind it, and the explanation provided by the algorithm should be perceived as
    fair by the people concerned, for example, why *A* should get a promotion over
    *B*.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**互动公平性**：算法应该能够解释其背后的原理，且算法提供的解释应该被相关人员认为是公平的，例如，为什么 *A* 应该比 *B* 获得晋升。'
- en: '**Procedural fairness**: The algorithm should not generate different results
    when used for different subgroups within an organization, for example, different
    recommendations for women than for men. If it does, it should be because of explainable
    societal or biological reasons.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**程序公平性**：算法在组织内对不同子群体的使用不应产生不同的结果，例如，女性和男性的推荐结果不应不同。如果有不同，应该是因为可以解释的社会或生物学原因。'
- en: These are demonstrated in *Figure 8**.1*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些内容在*图 8.1*中有所展示。
- en: '![Figure 8.1 – Different dimensions of fairness in an organization](img/Figure_8.01_B18681.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – 组织中公平性的不同维度](img/Figure_8.01_B18681.jpg)'
- en: Figure 8.1 – Different dimensions of fairness in an organization
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 组织中公平性的不同维度
- en: Bias and fairness are two terms that are often used interchangeably in the ML
    community. However, we feel that they should be differentiated. Taking a leaf
    out of Booth’s book ([https://dl.acm.org/doi/fullHtml/10.1145/3462244.3479897](https://dl.acm.org/doi/fullHtml/10.1145/3462244.3479897)),
    we define bias as a systematic error that alters the model’s decision based on
    specific input information (such as gender and geographical location). Often called
    measurement bias, this bias arises either because unnecessary information is added
    to an input feature space (**construct contamination**) or because the model is
    not capturing all the aspects of what you are measuring (**construct deficiency**).
    Here, **construct** refers to features that cannot be directly measured, and instead
    need to be inferred from measurements. *Figure 8**.2* shows different sources
    of bias.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见和公平性是机器学习领域常常交替使用的两个术语。然而，我们认为它们应当加以区分。借鉴 Booth 的观点（[https://dl.acm.org/doi/fullHtml/10.1145/3462244.3479897](https://dl.acm.org/doi/fullHtml/10.1145/3462244.3479897)），我们将偏见定义为一种系统性错误，它基于特定的输入信息（如性别和地理位置）改变模型的决策。通常被称为测量偏见，这种偏见的产生要么是因为向输入特征空间添加了不必要的信息（**构建污染**），要么是因为模型没有捕捉到你正在测量的所有方面（**构建缺失**）。这里的**构建**是指那些不能直接测量的特征，而需要通过测量来推断。*图
    8.2*展示了偏见的不同来源。
- en: '![Figure 8.2 – Sources of measurement (adapted from Booth et al., 2021)](img/Figure_8.02_B18681.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2 – 测量来源（改编自 Booth 等，2021）](img/Figure_8.02_B18681.jpg)'
- en: Figure 8.2 – Sources of measurement (adapted from Booth et al., 2021)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 测量来源（改编自 Booth 等，2021）
- en: A biased model may not always be a bad thing – it is possible for a model to
    be biased but fair (*Figure 8**.3*). As illustrated in the paper *Algorithmic
    Fairness*, by Pessach et al., it is possible for a model to consider both SAT
    scores and demographics for the admission selection process and allocate seats
    unequally to students based on whether they come from a privileged background
    or an unprivileged background, favoring those from underprivileged backgrounds.
    While such a model is *biased*, it is *fair* because a student from an underprivileged
    background will have had to overcome many more challenges to achieve that score
    and probably has higher potential.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有偏的模型不一定是坏事——模型可能是有偏的，但仍然是公平的（*图 8.3*）。正如 Pessach 等人在《算法公平性》一文中所阐述的那样，一个模型可以同时考虑
    SAT 成绩和人口统计信息来进行录取选择，并根据学生是否来自特权背景或非特权背景不平等地分配名额，倾向于那些来自贫困背景的学生。虽然这样的模型是*有偏的*，但它是*公平的*，因为来自贫困背景的学生为了取得该成绩必须克服更多挑战，并且可能具有更高的潜力。
- en: '![Figure 8.3 – Biased resource allocation to candidates based on their demographics
    and so on can result in a fair algorithm by giving tools to the underrepresented
    classes of society](img/Figure_8.03_B18681.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3 – 基于候选人的人口统计信息等进行的资源分配偏见可以通过为社会中代表性不足的群体提供工具来实现公平的算法](img/Figure_8.03_B18681.jpg)'
- en: Figure 8.3 – Biased resource allocation to candidates based on their demographics
    and so on can result in a fair algorithm by giving tools to the underrepresented
    classes of society
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 基于候选人的人口统计信息等进行的资源分配偏见可以通过为社会中代表性不足的群体提供工具来实现公平的算法
- en: Now that we have set the boundaries between fairness and bias, we will proceed
    to look at unfairness and methods to mitigate unfairness.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经划定了公平性和偏见之间的界限，接下来我们将探讨不公平性及其缓解方法。
- en: Unfairness mitigation methods
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不公平性缓解方法
- en: 'In recent years, researchers have developed various techniques to mitigate
    unfairness in AI models. These methods can be applied at different stages of model
    development:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，研究人员开发了各种技术来缓解 AI 模型中的不公平性。这些方法可以应用于模型开发的不同阶段：
- en: '**Preprocessing**: Preprocessing methods work by adjusting the training data
    distribution so that the sensitive groups are balanced.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理**：预处理方法通过调整训练数据的分布，使敏感群体平衡。'
- en: '**Postprocessing**: Postprocessing methods work by calibrating the predictions
    after the model has been trained.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后处理**：后处理方法通过在模型训练完成后对预测结果进行校准来工作。'
- en: '**In-processing**: These methods incorporate fairness directly into the model
    design.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理中**：这些方法将公平性直接纳入模型设计中。'
- en: In [*Chapter 7*](B18681_07.xhtml#_idTextAnchor146), we discussed preprocessing
    methods and techniques. In this chapter, we will discuss in-processing unfairness
    mitigation methods first, and later discuss different fairness constraints as
    applied to different ML tasks.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第七章*](B18681_07.xhtml#_idTextAnchor146)中，我们讨论了预处理方法和技术。在本章中，我们将首先讨论处理中不公平性缓解方法，随后讨论应用于不同机器学习任务的各种公平性约束。
- en: In-processing methods
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理中方法
- en: 'Using in-processing methods offers certain benefits:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用处理中方法具有一定的优势：
- en: It has been observed that ML models often amplify the bias present in data.
    Using in-processing techniques for unfairness mitigation can help in addressing
    this problem. This is because the problem of amplification is caused by the algorithm,
    and since in-processing methods take fairness into consideration alongside model
    optimization it can help reduce or eliminate the issue.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已经观察到，机器学习模型通常会放大数据中存在的偏见。使用处理中技术来缓解不公平性可以帮助解决这个问题。因为放大的问题是由算法引起的，而处理中方法在模型优化的同时考虑了公平性，可以帮助减少或消除这个问题。
- en: Fine-tuning pre-trained models has become a popular approach to customize modeling
    for different tasks with limited training data. Although transfer learning from
    pre-trained models has alleviated the need for large amounts of training data,
    the issue of bias in pre-trained models has become more critical as the pre-trained
    models are usually trained on biased data. By using techniques such as contrastive
    learning, it is possible to use in-processing methods to mitigate unfairness in
    pre-trained models.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调预训练模型已成为根据有限的训练数据定制不同任务建模的流行方法。尽管从预训练模型中进行迁移学习缓解了对大量训练数据的需求，但由于预训练模型通常是在有偏数据上训练的，预训练模型中的偏见问题变得更加关键。通过使用对比学习等技术，可以利用处理中方法来缓解预训练模型中的不公平性。
- en: Developing efficient in-processing methods is still a challenge. Existing algorithms
    are not sufficient, and we need to focus on developing new methods that can address
    fairness issues when sensitive attributes are not disclosed or available.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 开发高效的处理中方法仍然是一个挑战。现有的算法不足，我们需要专注于开发新的方法，以解决当敏感属性未披露或不可用时的公平性问题。
- en: 'In-processing unfairness mitigation methods can be further subdivided into
    two: **implicit methods** and **explicit methods**. In explicit methods, unfairness
    mitigation is achieved by incorporating the fairness metrics within the objective
    function using either fairness constraints or regularization parameters. Implicit
    methods, on the other hand, rely on modifying learning strategies of the deep
    learning model – for example, by including adversarial examples while training.
    Explicit methods are easy to implement and are often flexible. Let us now explore
    the ways to mitigate unfairness with the help of explicit methods.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 处理中不公平性缓解方法可以进一步细分为两类：**隐式方法**和**显式方法**。在显式方法中，通过在目标函数中引入公平性度量，使用公平性约束或正则化参数来实现不公平性缓解。另一方面，隐式方法依赖于修改深度学习模型的学习策略——例如，在训练过程中加入对抗样本。显式方法易于实现，且通常具有灵活性。现在让我们探索如何通过显式方法来缓解不公平性。
- en: Note
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In [*Chapter 7*](B18681_07.xhtml#_idTextAnchor146), we covered many fairness
    measurement metrics. In this chapter, we will call an algorithm fair if it is
    able to achieve good results for the fairness metrics it is designed for.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第七章*](B18681_07.xhtml#_idTextAnchor146)中，我们讨论了许多公平性衡量指标。在本章中，我们将称一个算法为公平的，如果它能够在其设计的公平性指标上取得良好的结果。
- en: 'In the rest of the chapter, we will discuss various techniques for unfairness
    mitigation. Specifically, we will tackle two specific fairness issues:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将讨论各种不公平性缓解的技术。具体来说，我们将处理两个特定的公平性问题：
- en: '**Disparate treatment**: Disparate treatment is when there is a difference
    in the ways individuals are treated based on their race, gender, or other protected
    characteristics. For example, if an algorithm is trained on data that is mostly
    about males, it may predict a higher salary rise for a male candidate and a lower
    salary rise for a female candidate despite them having the same number of years
    of experience. Most often, disparate treatment arises from biased data, or using
    sensitive information while training the model.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不同的待遇**：不同的待遇是指基于个人的种族、性别或其他受保护特征而对其进行不同对待的情况。例如，如果一个算法在主要是男性的数据上进行训练，它可能会为男性候选人预测更高的薪资增长，而为女性候选人预测较低的薪资增长，尽管他们拥有相同的工作经验年数。通常，不同的待遇源于偏见数据，或在训练模型时使用敏感信息。'
- en: '![Figure 8.4 – Example of disparate treatment by Google’s translate algorithm
    (adopted from Fitria, 2021)](img/Figure_8.04_B18681.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 – Google翻译算法的不公平待遇示例（摘自Fitria，2021）](img/Figure_8.04_B18681.jpg)'
- en: Figure 8.4 – Example of disparate treatment by Google’s translate algorithm
    (adopted from Fitria, 2021)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – Google翻译算法的不公平待遇示例（摘自Fitria，2021）
- en: '**Disparate impact**: Disparate impact is when the model treats certain groups
    differently, even when the model is not explicitly trained on corresponding sensitive
    attributes. For example, if an algorithm is trained on data that is mostly male,
    it may learn to associate male names with high-status jobs and female names with
    low-status jobs. This can happen when unknowingly, the model creates some proxy
    attributes that correlate with sensitive information.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不同的影响**：不同的影响是指模型在处理某些群体时存在差异，即使模型没有明确地在对应的敏感属性上进行训练。例如，如果一个算法在主要是男性的数据上进行训练，它可能会学会将男性名字与高职位关联，将女性名字与低职位关联。这种情况可能在模型无意中创建了一些与敏感信息相关的代理属性时发生。'
- en: Explicit unfairness mitigation
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 显式不公平性缓解
- en: As discussed in the previous section, explicit unfairness mitigation is achieved
    by adding regularizers or by including constraints within the loss function. In
    this section, we will expand on the explicit unfairness mitigation techniques
    and explore some of the recently proposed strategies for this.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所讨论的，通过在损失函数中添加正则化项或包括约束来显式地减轻不公平性。在本节中，我们将扩展关于显式不公平性缓解技术的内容，并探讨一些最近提出的策略。
- en: Fairness constraints for a classification task
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类任务的公平性约束
- en: 'Let us first build an understanding of classification tasks in ML. Consider
    *Y*, *X*, and *S* to be random variables, corresponding to the class/label, non-sensitive
    input features, and sensitive input features, respectively. Our training dataset,
    D, will then consist of instances of these random variables:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解一下机器学习中的分类任务。假设*Y*、*X*和*S*是随机变量，分别对应类别/标签、非敏感输入特征和敏感输入特征。我们的训练数据集D将由这些随机变量的实例组成：
- en: D = { (y, x, s) }
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: D = { (y, x, s) }
- en: 'The aim of the classification task is to find a model, M, defined by parameters,
    Θ, such that it is able to correctly predict the conditional probability of a
    class when sensitive and non-sensitive features are given, M[Y|X,S; Θ]. The model
    parameters are estimated by using the **Maximum Likelihood** **Estimator** (**MLE**):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 分类任务的目标是找到一个由参数Θ定义的模型M，使其能够在给定敏感和非敏感特征的情况下正确预测类别的条件概率M[Y|X,S; Θ]。模型参数通过使用**最大似然估计**（**MLE**）来估计：
- en: '![](img/Formula_08_006.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_006.jpg)'
- en: Let us see how we can modify this loss function to mitigate unfairness.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何修改这个损失函数以减轻不公平性。
- en: Adding a prejudice removal regularizer
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加偏见移除正则化器
- en: 'The paper *Fairness-Aware Classifier with Prejudice Remover Regularizer*, by
    Kamishima et al., introduced a prejudice removal regularizer term to the loss
    function, as shown in the following figure:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Kamishima等人提出的论文《公平意识分类器与偏见移除正则化器》引入了一个偏见移除正则化项到损失函数中，如下图所示：
- en: '![Figure 8.5 – Adding a prejudice removal regularizer term to the loss function](img/Figure_8.05_B18681.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5 – 向损失函数中添加偏见移除正则化项](img/Figure_8.05_B18681.jpg)'
- en: Figure 8.5 – Adding a prejudice removal regularizer term to the loss function
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – 向损失函数中添加偏见移除正则化项
- en: 'To measure the model fairness, `util` module contains the `CVS` function for
    calculating the score. To have a point of comparison, it is good to first build
    a simple classifier:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量模型的公平性，`util`模块包含了`CVS`函数来计算得分。为了有一个对比点，首先构建一个简单的分类器是很有益的：
- en: 'To begin, we create a function to create a simple four-layered neural network
    classifier. Between each layer, we have added dropout layers for regularization
    to avoid overfitting:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建一个函数来创建一个简单的四层神经网络分类器。在每一层之间，我们添加了丢弃层进行正则化，以避免过拟合：
- en: '[PRE0]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The preceding code makes use of the NNabla (imported as `nn`) library to build
    the network.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码利用了NNabla（以`nn`导入）库来构建网络。
- en: 'Now, if we create a simple classifier using the previous function and train
    it on the `Adult` dataset ([https://archive.ics.uci.edu/ml/datasets/Adult](https://archive.ics.uci.edu/ml/datasets/Adult)),
    the result shows the accuracy and CV2NB score:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，如果我们使用之前的函数创建一个简单的分类器，并在`Adult`数据集上进行训练（[https://archive.ics.uci.edu/ml/datasets/Adult](https://archive.ics.uci.edu/ml/datasets/Adult)），结果将显示准确率和CV2NB得分：
- en: '[PRE1]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'But now, if you include the prejudice removal regularizer term and retrain
    the same classifier with the modified loss function, you can see that the CV2NB
    score improves:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 但是现在，如果你包含偏见移除正则化项并使用修改后的损失函数重新训练相同的分类器，你会发现CV2NB得分有所提高：
- en: '![Figure 8.6 – Accuracy versus CV2NB score on adult dataset after adding prejudice
    removal regularizer term to the loss function](img/Figure_8.06_B18681.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.6 – 在成人数据集上添加偏见移除正则化项到损失函数后，准确率与CV2NB得分的对比](img/Figure_8.06_B18681.jpg)'
- en: Figure 8.6 – Accuracy versus CV2NB score on adult dataset after adding prejudice
    removal regularizer term to the loss function
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 – 在成人数据集上添加偏见移除正则化项到损失函数后，准确率与CV2NB得分的对比
- en: In the preceding figure, we can see that as η (the fairness parameter) increases,
    the CV2NB score also improves. The larger value of the fairness parameter enforces
    fairness. However, we can see that the cost of increased fairness is accuracy;
    a balance must be reached between the two. Also, you can see that a classifier
    with the prejudice removal regularizer had a CV2NB score of 0.16, which is higher
    compared to a classifier trained without using the prejudice removal regularizer
    (*Figure 8**.6*), thus proving that adding the regularizer creates a more fair
    model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们可以看到，随着η（公平性参数）的增加，CV2NB得分也有所提高。更大的公平性参数值会增强公平性。然而，我们可以看到，增加公平性的代价是准确性；二者之间必须找到一个平衡点。并且，你可以看到，带有偏见移除正则化项的分类器的CV2NB得分为0.16，相比于没有使用偏见移除正则化项的分类器（*图
    8.6*），得分更高，从而证明了添加正则化项可以创建一个更公平的模型。
- en: Modifying the objective
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修改目标
- en: 'A great deal of work in this area has been done by M. B. Zafar. Those interested
    in his complete work can refer to his GitHub repo: [https://github.com/mbilalzafar/fair-classification](https://github.com/mbilalzafar/fair-classification).
    We will consider two different classifiers: **support vector machines** and **logistic
    regressors**. The common thing between these classifiers is that both are based
    on finding a decision boundary that separates the classes. In support vector machines,
    the decision boundary is found by maximizing the distance between minimum support
    vectors, and in logistic regressors, the decision boundary is found by minimizing
    the log-loss function.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这方面的很多工作是由M. B. Zafar完成的。那些对他完整的工作感兴趣的人可以参考他的GitHub仓库：[https://github.com/mbilalzafar/fair-classification](https://github.com/mbilalzafar/fair-classification)。我们将考虑两种不同的分类器：**支持向量机**和**逻辑回归**。这两种分类器的共同点在于，它们都基于寻找一个决策边界来分隔类别。在支持向量机中，决策边界是通过最大化最小支持向量之间的距离来找到的，而在逻辑回归中，决策边界是通过最小化对数损失函数来找到的。
- en: 'For simplicity, we consider the case of a binary classifier. For a linear binary
    classifier, the decision boundary is defined as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们考虑二元分类器的情况。对于线性二元分类器，决策边界定义如下：
- en: ΘTX = 0
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ΘTX = 0
- en: Here, Θ refers to the coefficients of the hyperplane defining the decision boundary.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，Θ指的是定义决策边界的超平面系数。
- en: For this to work, fairness against disparate treatment is ensured by making
    sure that sensitive attributes (*S*) are not used in decision-making/prediction.
    Continuing with our approach, this means that S ∩ X = ∅; that is, they are disjointed.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其生效，针对不公平对待的公平性是通过确保敏感属性（*S*）不参与决策/预测来保证的。按照我们的做法，这意味着S ∩ X = ∅；也就是说，它们是互不相交的。
- en: 'For disparate impact, the 80% rule (more generally called the p-rule) is used
    as the fairness metric. Translating the 80% rule to the distance boundary, it
    means that if dθ(*x*) is the signed (or oriented) distance of feature vector *x*
    from the decision boundary, then the ratio of users with a specific sensitive
    feature having a positive signed distance and users without the sensitive feature
    having a positive signed distance is no less than 80:100\. Mathematically, for
    a binary sensitive attribute, s ∈ {0,1}, we can express the p-rule as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于差异影响，使用80%规则（更一般地称为p规则）作为公平性指标。将80%规则转换为距离边界，它意味着如果dθ(*x*)是特征向量*x*到决策边界的有符号（或定向）距离，则具有特定敏感特征的用户与没有该敏感特征的用户在正有符号距离上的比率不低于80:100。数学上，对于一个二元敏感属性，s
    ∈ {0,1}，我们可以将p规则表示如下：
- en: '![](img/Formula_08_013.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_013.jpg)'
- en: 'To mitigate the unfairness, Zafar et al. introduced the concept of decision
    boundary covariance. They defined it as the covariance between the user’s sensitive
    features and the signed distance of the user’s input features to the decision
    boundary. Mathematically, it would be as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解不公平，Zafar等人引入了决策边界协方差的概念。他们将其定义为用户的敏感特征与用户输入特征到决策边界的有符号距离之间的协方差。数学上，它可以表示为：
- en: '![](img/Formula_08_014.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_014.jpg)'
- en: 'Simplified, this expression is reduced to the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 简化后，这个表达式被缩减为以下形式：
- en: '![](img/Formula_08_015.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_015.jpg)'
- en: It is easy to include this in the classifier objective since this is a convex
    function. Let us put it into action. We created a synthetic dataset with two sensitive
    features and one non-sensitive feature. The following graph shows the generated
    data points.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是一个凸函数，所以很容易将其纳入分类器目标中。让我们来实践一下。我们创建了一个具有两个敏感特征和一个非敏感特征的合成数据集。以下图表显示了生成的数据点。
- en: '![Figure 8.7 – Synthetic data generated with two sensitive features and one
    non-sensitive feature (showing only 200 sample points)](img/Figure_8.07_B18681.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7 – 使用两个敏感特征和一个非敏感特征生成的合成数据（仅显示200个样本点）](img/Figure_8.07_B18681.jpg)'
- en: Figure 8.7 – Synthetic data generated with two sensitive features and one non-sensitive
    feature (showing only 200 sample points)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – 使用两个敏感特征和一个非敏感特征生成的合成数据（仅显示200个样本点）
- en: 'The details of the data generated by us are as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成的数据的详细信息如下：
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If we do not use any fairness constraints and train a simple classifier, the
    accuracy that we get is 87%, but the classifier follows only a 41% p-rule, as
    opposed to the desired 80%. Now, there are two strategies we can adopt:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不使用任何公平性约束，训练一个简单的分类器，那么得到的准确度是87%，但该分类器仅遵循41%的p规则，而不是期望的80%。现在，我们可以采取两种策略：
- en: '**Maximize accuracy under fairness constraints**: In this case, the constraints
    are put so that the p-rule is satisfied while maximizing the accuracy of the classifier.
    This is achieved by the following:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在公平性约束下最大化准确度**：在这种情况下，约束条件设置为使p规则得到满足，同时最大化分类器的准确度。通过以下方式实现：'
- en: Minimize L(D,Θ) subject to ![](img/Formula_08_017.png) and ![](img/Formula_08_018.png)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在满足![](img/Formula_08_017.png)和![](img/Formula_08_018.png)的条件下最小化L(D,Θ)
- en: Here, *c* refers to the covariance threshold. It specifies an upper bound on
    the covariance between sensitive features and the signed distance of the input
    features from the decision boundary. The covariance threshold provides a trade-off
    between accuracy and fairness.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*c*表示协方差阈值。它指定了敏感特征与输入特征到决策边界的有符号距离之间的协方差的上限。协方差阈值提供了准确度与公平性之间的权衡。
- en: As *c* decreases (c → 0), the classifier becomes fairer at the cost of accuracy.
    The following is the result of the classifier trained using the fairness constraints.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当*c*减小（c → 0）时，分类器在牺牲准确度的情况下变得更加公平。以下是使用公平性约束训练的分类器的结果。
- en: '![Figure 8.8 – Decision boundary of an unconstrained and constrained logistic
    classifier when trained on the synthetic data](img/Figure_8.08_B18681.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图8.8 – 在合成数据上训练的无约束和有约束的逻辑回归分类器的决策边界](img/Figure_8.08_B18681.jpg)'
- en: Figure 8.8 – Decision boundary of an unconstrained and constrained logistic
    classifier when trained on the synthetic data
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – 在合成数据上训练的无约束和有约束的逻辑回归分类器的决策边界
- en: You can see that the accuracy of the classifier reduced slightly (71%), but
    the p-rule value is 104% – this means the classifier is fair. The protected versus
    non-protected ratio in the positive class is 53:51\. The graph also shows the
    decision boundary for the original and constrained classifiers.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到分类器的准确率略微下降（71%），但p-rule值是104%——这意味着分类器是公平的。正类中的受保护与非受保护比例是53:51。图表还展示了原始分类器和受限分类器的决策边界。
- en: '**Maximize fairness under accuracy constraints**: In this case, we minimize
    the covariance decision boundary, subject to the constraint that the loss of the
    constrained classifier (L(D,Θ)) is less than the loss of the unconstrained classifier
    (L(D,Θ*)) – in other terms, the following:'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在准确性约束下最大化公平性**：在这种情况下，我们最小化协方差决策边界，前提是受限分类器的损失（L(D,Θ)）小于无约束分类器的损失（L(D,Θ*)）——换句话说，以下是：'
- en: Minimize ![](img/Formula_08_022.png) subject to ![](img/Formula_08_023.png)![](img/Formula_08_024.png).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化 ![](img/Formula_08_022.png) 约束条件为 ![](img/Formula_08_023.png)![](img/Formula_08_024.png)。
- en: Here, Li represents loss associated with the *i*-th user in the training set.
    The following graph shows the results obtained using these constraints.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，Li表示训练集中第*i*个用户的损失。下图展示了使用这些约束所获得的结果。
- en: '![Figure 8.9 – Decision boundary of an unconstrained and constrained logistic
    classifier when trained on the synthetic data](img/Figure_8.09_B18681.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图8.9 – 在合成数据上训练的无约束与有约束的逻辑回归分类器的决策边界](img/Figure_8.09_B18681.jpg)'
- en: Figure 8.9 – Decision boundary of an unconstrained and constrained logistic
    classifier when trained on the synthetic data
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 – 在合成数据上训练的无约束与有约束的逻辑回归分类器的决策边界
- en: We can see that now, while the p-rule has improved from 41% to 61%, the decrease
    in accuracy is not very significant. For businesses, this is important. You cannot
    launch a product that is fair but not accurate.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，现在p-rule从41%提高到了61%，而准确率的下降并不显著。对于企业来说，这一点很重要。你不能推出一个公平但不准确的产品。
- en: 'The preceding graphs were generated using the code provided by the Kamishima
    group with the following parameters:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表是使用Kamishima小组提供的代码生成的，使用了以下参数：
- en: '[PRE3]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This approach helps mitigate the disparate impact on classification tasks. The
    approach can be extended to many classification tasks.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有助于缓解分类任务中的不公平影响。该方法可以扩展到许多分类任务中。
- en: Fairness constraints for a regression task
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归任务的公平性约束
- en: 'In regression, the task changes from finding a decision boundary to finding
    a hyperplane that contains most of the data points. A classifier is good when
    the decision space is small and discrete, but more often, decision spaces are
    continuous. For example, instead of telling whether a person will default on a
    loan or not (a binary world), it is more useful for decision-makers to know the
    probability of a person defaulting on a loan (a continuous value between 0 and
    1). In these scenarios, regression is a better option. The major difference is
    that while in the case of classification the logit loss function is used, in the
    case of regression, the loss function is conventionally the **Mean Square Error**
    (**MSE**). This means that we can add a regularizer term, just as we did with
    the classifier, to get a fair regression. Indeed, this was attempted by Berk et
    al., and they proposed a general framework for fairness in both regression and
    classification tasks:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归中，任务的变化是从找到决策边界变为找到一个包含大多数数据点的超平面。当决策空间较小且离散时，分类器表现良好，但更常见的是决策空间是连续的。例如，与其判断一个人是否会违约（一个二元问题），更有意义的是让决策者知道一个人违约的概率（一个介于0和1之间的连续值）。在这些场景中，回归是更好的选择。主要的区别是，在分类的情况下使用的是对数损失函数，而在回归中，传统的损失函数是**均方误差**（**MSE**）。这意味着我们可以像分类器一样添加正则化项，从而得到一个公平的回归。实际上，Berk等人曾尝试过这种方法，并提出了一个适用于回归和分类任务的公平性通用框架：
- en: '![](img/Formula_08_026.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_026.jpg)'
- en: Here, *L* is the conventional loss function (for the classifier, the conventional
    loss is logit loss, and for regression, we conventionally use the MSE loss). They
    added a λ -weighted fairness loss term, fp. To take care of overfitting, there
    is the standard L2 regularization term. This method can be applied to any model
    with a convex optimization function.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*L*是常规的损失函数（对于分类器，常规损失是对数损失，对于回归，我们通常使用均方误差（MSE）损失）。他们添加了一个λ加权的公平性损失项fp。为了防止过拟合，还有标准的L2正则化项。这种方法可以应用于任何具有凸优化函数的模型。
- en: Using this approach, they were able to achieve both individual and group fairness.
    Let us see how.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，他们能够实现个体和群体的公平性。让我们看看具体是如何做到的。
- en: Individual fairness using a fairness penalty term
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用公平性惩罚项的个体公平性
- en: 'Continuing with our previous terminology, let *s* be a binary sensitive feature,
    s ∈ {0,1}. It tells us whether a user belongs to the sensitive group or not. Then,
    for individual fairness, the penalty term is defined as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 继续我们之前的术语，让*s*是一个二元敏感特征，s ∈ {0,1}。它告诉我们用户是否属于敏感群体。因此，对于个体公平性，惩罚项定义如下：
- en: '![](img/Formula_08_030.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_030.jpg)'
- en: Here, *d* is a non-negative function, which is a measure of the absolute difference
    between the outputs for sensitive and non-sensitive groups (|yi – yj|), *n*1 is
    the number of individuals belonging to the sensitive group, and *n*2 is the number
    of individuals not belonging to the sensitive group.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*d*是一个非负函数，表示敏感和非敏感群体之间输出的绝对差异（|yi – yj|），*n*1是属于敏感群体的个体数，*n*2是不属于敏感群体的个体数。
- en: We can see that every time the model treats the inputs corresponding to sensitive
    and non-sensitive differently, it is penalized.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，每当模型将敏感和非敏感输入区分对待时，都会受到惩罚。
- en: Group fairness using a fairness penalty term
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用公平性惩罚项的群体公平性
- en: 'Now, for group fairness, we would like, on average, for the two group samples
    to have similar predictions. This is achieved by defining the fairness penalty
    term as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于群体公平性，我们希望两个群体样本的预测平均相似。这通过以下公平性惩罚项定义：
- en: '![](img/Formula_08_034.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_034.jpg)'
- en: Now, to verify whether it really works for the regression case, we make use
    of the **communities and crime** dataset, available from the UCI ML repository
    ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)). The dataset
    contains features related to per-capita violent crime rates in different communities
    across the US. Here, the sensitive feature is the *race* of the individual and
    the task is to predict the crime rate.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了验证这种方法是否真的适用于回归案例，我们使用UCI ML存储库提供的**社区与犯罪**数据集（[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)）。数据集包含与美国不同社区的人均暴力犯罪率相关的特征。这里的敏感特征是个体的*种族*，任务是预测犯罪率。
- en: '*Figure 8**.10* shows fairness loss versus MSE loss for the dataset. Here,
    **single** refers to the case where only one model is built for the two sensitive
    groups and **separate** refers to where we have separate models.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8**.10*显示了数据集的公平损失与MSE损失。这里，**单一**指的是只为两个敏感群体建立一个模型的情况，而**分开**则是指我们有单独的模型。'
- en: '![Figure 8.10 – MSE loss versus fairness for the communities and crime dataset](img/Figure_8.10_B18681.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图8.10 – 社区与犯罪数据集的MSE损失与公平性](img/Figure_8.10_B18681.jpg)'
- en: Figure 8.10 – MSE loss versus fairness for the communities and crime dataset
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 – 社区与犯罪数据集的MSE损失与公平性
- en: We can see once again that there is a price to be paid for fairness in terms
    of accuracy. Thus, care must be taken by model builders when defining the type
    of fairness they care about, making a decision specific to their application,
    and they should decide on a proper balance between prediction accuracy and fairness.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次看到，在精度方面追求公平性是有代价的。因此，在定义关心的公平类型时，建模者必须谨慎，特定于其应用程序作出决策，并在预测精度与公平性之间找到适当的平衡。
- en: Fairness constraints for a clustering task
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类任务的公平性约束
- en: 'A clustering task involves grouping the given data points into clusters. Although
    there are supervised clustering algorithms, unsupervised clustering is more common.
    Here, the data points X = {xp ∈ RM,p = 1, … , N} are provided and the task is
    to assign them to *K* clusters. Let *M* be the cluster assignment vector: M =
    [m1, … , mN ] ∈ [0,1]NK. Now, say the data contains sensitive information, for
    example, *J* for different demographic groups. We can represent the sensitive
    information as a vector, Sj = [sj, p] ∈ {0,1}N , so if point *p* is assigned to
    group *j*, then sj, p = 1; otherwise, it is 0.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类任务涉及将给定的数据点分组成簇。虽然有监督聚类算法，但无监督聚类更为常见。在这里，提供数据点 X = {xp ∈ RM,p = 1, … , N}，任务是将它们分配到*K*个簇中。设*M*为簇分配向量：M
    = [m1, … , mN ] ∈ [0,1]NK。现在，假设数据包含敏感信息，例如，不同人口统计学群体的*J*。我们可以将敏感信息表示为向量，Sj = [sj,
    p] ∈ {0,1}N，因此如果点*p*分配给群体*j*，则sj, p = 1；否则为0。
- en: 'One of the most widely used clustering algorithms, K-means, clusters the given
    data points in *K* clusters by minimizing the sum of the distance between individual
    data points and the representative cluster centroids. We can add a fairness penalty.
    In the paper *Variational Fair Clustering*, the authors added the Kullback-Leibler
    divergence term between the required demographic proportions and the marginal
    probability (Pm=[P(j|m)]) of the demographics within cluster *m*. In terms of
    the cluster assignment vector and demographic information vector, the marginal
    probability can be expressed as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 是最广泛使用的聚类算法之一，它通过最小化各个数据点与代表性聚类中心之间的距离之和，将给定数据点聚类为 *K* 个簇。我们可以加入公平性惩罚。在论文《变分公平聚类》中，作者将所需人口比例和簇
    *m* 内人口的边际概率（Pm=[P(j|m)]) 之间的 Kullback-Leibler 散度项加入到了目标函数中。在簇分配向量和人口信息向量的层面上，边际概率可以表示如下：
- en: '![](img/Formula_08_040.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_040.jpg)'
- en: 'The *T* here denotes the mathematical transpose operator. The modified objective
    function is shown in the following figure:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 *T* 表示数学中的转置运算符。修改后的目标函数如下图所示：
- en: '![Figure 8.11 – Loss function with a fairness penalty for the clustering task](img/Figure_8.11_B18681.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.11 – 带有公平性惩罚的聚类任务损失函数](img/Figure_8.11_B18681.jpg)'
- en: Figure 8.11 – Loss function with a fairness penalty for the clustering task
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 – 带有公平性惩罚的聚类任务损失函数
- en: The interesting thing to note is that the fairness penalty is represented by
    a cross-entropy term between the required demographic proportion, *U*, and the
    respective marginal probability, *Pk*. This penalty can further be decomposed
    into two parts, one convex and the other concave. You are encouraged to read the
    original paper for complete mathematical proofs of bounds, convergence, and monotonicity
    guarantees ([https://arxiv.org/abs/1906.08207](https://arxiv.org/abs/1906.08207)).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，公平性惩罚通过所需人口比例 *U* 和相应边际概率 *Pk* 之间的交叉熵项表示。这个惩罚项可以进一步分解为两个部分，一个是凸的，另一个是凹的。鼓励你阅读原论文以获取完整的数学证明，包括界限、收敛性和单调性保证（[https://arxiv.org/abs/1906.08207](https://arxiv.org/abs/1906.08207)）。
- en: '*Table 8.1* lists the results of applying these fairness constraints on different
    datasets. The synthetic datasets are created according to the demographic breakdown;
    both are created with 450 data points and two demographic groups. In the synthetic
    dataset, both groups have an equal number of required proportions generated; however,
    in synthetic unequal, we created 310 and 140 data points for the two demographic
    groups:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 8.1* 列出了在不同数据集上应用这些公平性约束的结果。合成数据集是根据人口统计分布创建的；两个数据集都包含 450 个数据点和两个人口群体。在合成数据集中，两个群体的所需比例相等；然而，在合成不均衡数据集中，我们为两个群体分别创建了
    310 和 140 个数据点：'
- en: '| **Dataset** | **Clustering Energy** | **Fairness Error** | **Average Balance**
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **聚类能量** | **公平性误差** | **平均平衡** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Synthetic Unequal (number of samples = 450, J = 2, l = 10) | 159.75 | 0.00
    | 0.33 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 合成不均衡（样本数量 = 450，J = 2，l = 10） | 159.75 | 0.00 | 0.33 |'
- en: '| Synthetic (number of samples = 450, J = 2, l = 10) | 207.80 | 3.69 | 0.01
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 合成数据集（样本数量 = 450，J = 2，l = 10） | 207.80 | 3.69 | 0.01 |'
- en: '| Adult (number of samples = 32,561, J = 2, l = 10) | 9,507 | 0.27 | 0.48 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 成人（样本数量 = 32,561，J = 2，l = 10） | 9,507 | 0.27 | 0.48 |'
- en: '| Bank (number of samples = 41,108, J = 3, l = 10) | 8,443.88 | 0.69 | 0.15
    |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 银行（样本数量 = 41,108，J = 3，l = 10） | 8,443.88 | 0.69 | 0.15 |'
- en: '| Census II (number of samples = 2,458,285, J = 2, l = 10) | 922,558.03 | 26.22
    | 0.44 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Census II（样本数量 = 2,458,285，J = 2，l = 10） | 922,558.03 | 26.22 | 0.44 |'
- en: Table 8.1 – Table listing clustering objective, fairness error, and average
    balance on different datasets using K-means clustering with a fairness penalty
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.1 – 使用带有公平性惩罚的 K-means 聚类在不同数据集上列出聚类目标、公平性误差和平均平衡的表格
- en: '*Figure 8**.12* shows the clusters obtained on the synthetic and synthetic
    unequal datasets by applying K-means with a fairness penalty.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.12* 显示了在合成数据集和合成不均衡数据集上应用带有公平性惩罚的 K-means 聚类所获得的聚类结果。'
- en: "![Figure \uFEFF8.12 – Clusters on the two datasets using K-means with a fairness\
    \ penalty](img/Figure_8.12_B18681.jpg)"
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.12 – 使用带有公平性惩罚的 K-means 聚类在两个数据集上的聚类结果](img/Figure_8.12_B18681.jpg)'
- en: Figure 8.12 – Clusters on the two datasets using K-means with a fairness penalty
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12 – 使用带有公平性惩罚的 K-means 聚类在两个数据集上的聚类结果
- en: The same approach can also be applied to the K-median clustering algorithm and
    the Ncut clustering algorithm.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的方法也可以应用于 K-median 聚类算法和 Ncut 聚类算法。
- en: Fairness constraints for a reinforcement learning task
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习任务的公平性约束
- en: Reinforcement learning is quite different from the supervised learning used
    in regression and classification tasks and the unsupervised learning used in the
    task of clustering. What makes it different is that here, the algorithm has no
    idea of the desired output. Every choice it makes affects not only its present
    learning but also future outcomes. In reinforcement learning, the algorithm gets
    feedback from the environment; it is this feedback that helps it learn.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习与用于回归和分类任务的监督学习、以及用于聚类任务的无监督学习有很大的不同。它的不同之处在于，算法在这里并不知道期望的输出。它所做的每一个选择不仅影响当前学习，还会影响未来的结果。在强化学习中，算法从环境中获得反馈；正是这种反馈帮助它进行学习。
- en: To define the reinforcement learning task, we can say that the system consists
    of the agent (reinforcement learning agent or algorithm) and an environment. The
    agent can perceive the environment through state vectors, `s`, it can bring changes
    in the environment through actions, `a`, and the environment may give the agent
    feedback in terms of reward, `r` (*Figure 8**.13*). The goal of the agent is to
    find the optimal policy, π(s,a), that will maximize the rewards.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定义强化学习任务，我们可以说系统由代理（强化学习代理或算法）和环境组成。代理可以通过状态向量`s`感知环境，可以通过动作`a`对环境进行改变，环境可能通过奖励`r`反馈给代理（*图8.13*）。代理的目标是找到一个最优策略π(s,a)，以最大化奖励。
- en: "![Figure \uFEFF8.13 – Reinforcement learning framework](img/Figure_8.13_B18681.jpg)"
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图8.13 – 强化学习框架](img/Figure_8.13_B18681.jpg)'
- en: Figure 8.13 – Reinforcement learning framework
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 – 强化学习框架
- en: 'We can represent a simple reinforcement learning problem as a multi-armed bandit
    problem. The problem imagines a gambler playing at a row of slot machines. They
    have to decide the order in which the slot arms will be played so as to maximize
    their rewards. We can define the problem as having *N* slots, with the task of
    the gambler to choose one arm at each time step, *t*:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将一个简单的强化学习问题表示为一个多臂老虎机问题。这个问题假设一个赌徒在一排老虎机中进行赌博。他们需要决定每次玩哪个老虎机臂，以最大化他们的奖励。我们可以定义该问题有*N*个老虎机，赌徒的任务是在每个时间步*t*选择一个老虎机臂：
- en: '![](img/Formula_08_042.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_042.jpg)'
- en: 'At each time step, *t*, the gambler receives a reward, rt ∈ [0, 1]N, decided
    by a fixed distribution, E[rt(it)] = μ(it). However, the gambler is not aware
    of this distribution. The only thing they can observe is the reward, rt(it). The
    goal of the gambler is to choose the arms in a sequence that maximizes the total
    reward:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步*t*，赌徒会收到一个奖励，rt ∈ [0, 1]N，这个奖励由一个固定的分布决定，E[rt(it)] = μ(it)。然而，赌徒并不知道这个分布。它们唯一能观察到的是奖励rt(it)。赌徒的目标是选择一个臂的顺序，以最大化总奖励：
- en: '![](img/Formula_08_046.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_046.png)'
- en: 'The optimal reward probability of best the optimal slot arm is as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最优老虎机臂的最佳奖励概率如下：
- en: '![](img/Formula_08_047.png) where, ![](img/Formula_08_048.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_047.png) 其中，![](img/Formula_08_048.png)'
- en: 'To train our agent to play the role of the gambler, we can define our loss
    function as a regret function – representing the regret that the agent will have
    by not selecting the optimal slot arm at time step *t*:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的代理扮演赌徒的角色，我们可以将我们的损失函数定义为一个遗憾函数——表示代理在时间步*t*未选择最优老虎机臂所带来的遗憾：
- en: '![](img/Formula_08_049.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_049.jpg)'
- en: We can think of the multi-armed problem as a resource allocation problem – there
    are limited resources and they should be allocated for maximum performance. When
    the reinforcement learning agent starts, it has no idea what action would result
    in a good reward, and therefore most reinforcement learning agents use the technique
    called exploration and exploitation. The idea is that the agent initially explores
    all the possible actions by choosing random actions, learns from them (**exploration**),
    and later, when sufficient experience is gained, uses the action as guided by
    the learned policy (**exploitation**). One of the algorithms used to implement
    exploration/exploitation is the **ε greedy algorithm** (Mnih, 2015).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将多臂问题看作一个资源分配问题——有限的资源应该被分配以获得最大性能。当强化学习代理开始时，它不知道哪种动作会产生好的奖励，因此大多数强化学习代理使用一种叫做探索与利用的技术。其思想是，代理最初通过选择随机动作来探索所有可能的动作，从中学习（**探索**），然后当足够的经验积累后，使用由学到的策略指导的动作（**利用**）。用来实现探索/利用的算法之一是**ε贪心算法**（Mnih，2015）。
- en: There is always a trade-off between exploration and exploitation. In the ɛ greedy
    algorithm, the balance between exploration and exploitation is achieved by decreasing
    the exploration parameter, ɛ. Another way to achieve the balance between exploration
    and exploitation is using the **Upper Confidence Bound** (**UCB**). The UCB algorithm
    works by maintaining an estimate of the expected reward for each action. At each
    step, the algorithm selects the action with the highest estimated reward. However,
    the UCB algorithm also adds a term to each estimate that encourages exploration.
    This term is based on the uncertainty of the estimate, and it increases as the
    number of times an action has been taken decreases. As a result, the UCB algorithm
    strikes a balance between exploration and exploitation, and it often outperforms
    other algorithms in online settings.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 探索与利用之间总是存在权衡。在ɛ贪婪算法中，通过减小探索参数ɛ来实现探索与利用之间的平衡。另一种实现探索与利用平衡的方法是使用**上置信界限**（**UCB**）。UCB算法通过维护每个动作的预期奖励估计来工作。在每个步骤中，算法选择具有最高预期奖励的动作。然而，UCB算法还在每个估计中添加了一项鼓励探索的项。这个项基于估计的不确定性，并且随着某个动作被执行的次数减少而增加。因此，UCB算法在探索与利用之间达到了平衡，并且在在线环境中通常表现优于其他算法。
- en: In this section, we will apply fairness constraints to the multi-armed bandit-based
    resource allocation problem. The intention is to introduce the notion of fairness
    to resource distribution. Today, AI-based programs are making decisions in allocating
    resources such as medical care, loans, and subsidies. Can we ensure fairness in
    this distribution? Also, does fairness in distribution have an effect on collaboration?
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将应用公平性约束来解决基于多臂赌博机的资源分配问题。目的是将公平性概念引入资源分配中。如今，基于AI的程序在分配资源（如医疗、贷款和补贴）时做出决策。我们能否确保这一分配过程的公平性？另外，分配过程中的公平性是否对合作产生影响？
- en: One approach to deal with this issue was given by Claure et al. in the paper
    *Multi-Armed Bandits with Fairness Constraints for Distributing Resources to Human
    Teammates*. They put fair-use constraint limits on the number of times an individual
    may be assigned a resource, ensuring that all users get what they require. They
    modified the unconstrained UCB algorithm so as to estimate the expected reward
    of each arm by using the mean of its empirical rewards in the past and how often
    it has been pulled.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是由Claure等人在论文《*带有公平性约束的多臂赌博机，用于向人类队友分配资源*》中提出的。他们对个体分配资源的次数设置了公平使用约束限制，确保所有用户都能获得所需资源。他们修改了无约束的UCB算法，通过使用其过去的经验奖励均值以及每次被抽取的频率来估计每个臂的预期奖励。
- en: 'To evaluate their method, they considered the environment of a collaborative
    Tetris game, pairing teams of two people with the task of completing a game of
    Tetris together using their algorithm. The algorithm would decide at each time
    step which of the two players has control over the falling piece. They later asked
    the participants to describe whether they felt that the algorithm gave them turns
    fairly or not: that is their perception of the game’s fairness.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估他们的方法，他们考虑了一个协作俄罗斯方块游戏的环境，将两人小组配对，并要求他们使用该算法共同完成一局俄罗斯方块游戏。算法会在每个时间步骤决定由哪一位玩家控制下落的方块。然后，他们询问参与者是否觉得算法给他们分配轮次是公平的：即他们对游戏公平性的感知。
- en: 'The following table shows the participants’ perception for different constraint
    limits:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了不同约束限制下，参与者的感知：
- en: '| **Constraint Limits** | **Participants’ Perception** |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| **约束限制** | **参与者的感知** |'
- en: '| 25%, 33% | *“I felt the more competent player was given more turns, which
    makes sense but was why it felt unfair.”* |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 25%, 33% | *“我感觉更有能力的玩家被分配了更多的轮次，这很有道理，但也是我觉得不公平的原因。”* |'
- en: '| 50% | *“I think it was even, it made us take turns one after the other, enough
    that it made me feel I was making an equal contribution to the game.”* |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 50% | *“我认为这场比赛是公平的，它让我们轮流进行，每次都让我觉得自己在为比赛做出同等的贡献。”* |'
- en: Table 8.1 – Participants’ perception in the experiment for different constraint
    limits
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1 – 参与者在实验中对于不同约束限制的感知
- en: Their work shows that using their variation of the algorithm, it is possible
    to allocate resources fairly. This leads to higher trust among individuals without
    any decrease in performance.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的研究表明，使用他们变种的算法，资源可以公平地分配。这增加了个体之间的信任，而不会降低表现。
- en: Fairness constraints for recommendation systems
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推荐系统的公平性约束
- en: Recommender systems are used to make suggestions for products, services, potential
    friends, or content. They rely on feedback data and typically run using an ML
    algorithm. For example, an online bookstore may use a recommender system to classify
    books by genre and suggest similar books to customers who have purchased a book.
    Facebook uses a recommender system to suggest potential friends to users. YouTube
    uses a recommender system to suggest related videos to users who are watching
    a video. Generally, recommender systems are used to solve the **cold-start** problem,
    where it is difficult to make suggestions for new users or items with little data.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统用于为产品、服务、潜在朋友或内容提供建议。它们依赖于反馈数据，并通常通过机器学习算法运行。例如，一个在线书店可能使用推荐系统根据类别对书籍进行分类，并向购买过某本书的顾客推荐类似的书籍。Facebook使用推荐系统向用户推荐潜在的朋友。YouTube使用推荐系统向观看某个视频的用户推荐相关视频。一般来说，推荐系统用于解决**冷启动**问题，即在数据不足的情况下很难为新用户或新物品提供推荐。
- en: 'Recommender systems are classified into three types based on the information
    on which they are based: collaborative, content-based, and hybrid filtering. Collaborative
    filtering recommender systems rely on the collective intelligence of the community
    to make recommendations. Content-based recommender systems use information about
    the items themselves to make recommendations. Hybrid recommender systems combine
    both collaborative and content-based approaches to make recommendations. Each
    of these approaches has its own strengths and weaknesses, and the best recommender
    system for a particular application will depend on the specific requirements.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统根据其所依赖的信息类型分为三类：协同过滤、基于内容的过滤和混合过滤。协同过滤推荐系统依赖于社区的集体智慧来进行推荐。基于内容的推荐系统使用关于物品本身的信息来进行推荐。混合推荐系统结合了协同过滤和基于内容的两种方法进行推荐。这些方法各有优缺点，适合特定应用的最佳推荐系统将取决于具体需求。
- en: In recent years, recommender systems have become increasingly prevalent in many
    aspects of our lives. From online shopping to social media, these systems play
    a key role in helping us find the products, services, and content that we are
    looking for. However, these systems are not perfect. In particular, they can often
    suffer from a variety of biases that can lead to unfair recommendations. For example,
    a recommender system may inadvertently favor certain groups of users over others.
    This can result in unusable recommendations for some users and can even lead to
    discriminatory behavior. Therefore, it is essential to address the potential for
    unfairness in recommender systems. Fortunately, there has been recent progress
    in this area, and there are now a number of methods that can help to mitigate
    the problem. With continued research, there is the potential for recommender systems
    to be made more fair and inclusive for all users.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，推荐系统在我们生活的许多方面变得越来越普及。从在线购物到社交媒体，这些系统在帮助我们找到所需的产品、服务和内容方面发挥着关键作用。然而，这些系统并不完美。特别是，它们常常会受到各种偏见的影响，导致不公平的推荐。例如，一个推荐系统可能会无意中偏向某些用户群体。这可能导致一些用户得到无法使用的推荐，甚至可能导致歧视性行为。因此，解决推荐系统中的不公平问题至关重要。幸运的是，近年来这一领域已经取得了进展，现在已经有许多方法可以帮助减轻这一问题。随着持续的研究，推荐系统有望变得更加公平和包容，适用于所有用户。
- en: 'Recommender systems are multi-stake platforms, with three stakeholders: consumers,
    the supplier, and the platform:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统是一个多方平台，涉及三个利益相关方：消费者、供应商和平台：
- en: '**Consumers** are the users of the platform. They come for suggestions. They
    use the platform because they might be searching for something or having difficulty
    in deciding, say, in what to buy. They anticipate that the platform will provide
    a fair and objective suggestion.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消费者**是平台的用户。他们来寻求建议。因为他们可能在寻找某些东西，或在做决定时遇到困难，比如在购买什么时。消费者预期平台能够提供公平和客观的建议。'
- en: '**Suppliers** or providers are on the other side of the recommender system.
    They provide the service and, in return, gain some utility from the consumers.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**供应商**或提供者位于推荐系统的另一端。他们提供服务，并从消费者那里获得一定的收益。'
- en: Finally, there is the **platform** itself. It brings consumers and providers
    together and makes some benefit from it.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，是**平台**本身。它将消费者和供应商连接起来，并从中获益。
- en: 'Therefore, when we talk of fairness in recommender systems, we also need to
    clarify which type of fairness we are talking about. In general, we talk of two
    types of fairness, that is, provider fairness and consumer fairness:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们谈论推荐系统中的公平性时，我们也需要明确我们讨论的是哪种类型的公平性。通常，我们讨论两种类型的公平性，即提供者公平性和消费者公平性：
- en: '**Provider fairness** is concerned with the items that are being ranked or
    recommended. Here, the notion of fairness is that similar items or groups of items
    should be ranked or recommended in a similar way. As an example, consider two
    articles about the same subject. If one is ranked higher because it has more views,
    this would be unfair because it is giving more weight to popularity instead of
    quality. To fix this, the algorithm could look at other factors, such as recency,
    number of shares, and number of likes, to get a more accurate idea of quality.
    By taking these factors into account, the algorithm would be fairer to both articles.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供者公平性** 关注的是被排名或推荐的项目。在这里，公平性的概念是相似的项目或项目组应该以相似的方式被排名或推荐。例如，考虑两篇关于相同主题的文章。如果其中一篇因为拥有更多的浏览量而被排名更高，这就是不公平的，因为它赋予了流行度更多的权重，而不是质量。为了解决这个问题，算法可以考虑其他因素，如时效性、分享次数和点赞数，以更准确地评估质量。通过考虑这些因素，算法对两篇文章都会更加公平。'
- en: '**Consumer fairness** has a focus on the users who receive or use the data
    or service. A fair recommendation algorithm, in this case, would ensure that users
    or groups of users receive the same recommendations or rankings. Continuing with
    the example of the recommender system suggesting articles, if two readers have
    the same educational background and interest, the algorithm should recommend the
    same/similar articles to them, without considering sensitive attributes such as
    gender or age.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消费者公平性** 关注的是接收或使用数据或服务的用户。在这种情况下，一个公平的推荐算法应该确保用户或用户群体接收到相同的推荐或排名。继续以推荐系统推荐文章为例，如果两个读者拥有相同的教育背景和兴趣，算法应该向他们推荐相同或相似的文章，而不考虑性别或年龄等敏感属性。'
- en: One approach to achieving fairness in recommendation systems is to treat them
    as classification algorithms, with *recommendation* being a class. Another way
    is to treat recommendation systems as a ranking problem; here, the recommendation
    is a ranked list, *L*. We have already covered how fairness constraints can be
    added to classification tasks. In this section, we will discuss fairness constraints
    in ranking, based on the approach developed in the paper *Ranking with Fairness
    Constraints* by Celis et al. The basic idea is to put constraints on the number
    of items that can appear in the top *k* places from different groups.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 实现推荐系统公平性的一种方法是将其视为分类算法，其中 *推荐* 是一个类别。另一种方法是将推荐系统视为排序问题；在这里，推荐是一个有序列表 *L*。我们已经讨论了如何在分类任务中添加公平性约束。在本节中，我们将讨论基于
    Celis 等人论文 *Ranking with Fairness Constraints* 中提出的方法，如何在排序中添加公平性约束。基本思路是对不同组的项目在前
    *k* 个位置出现的数量施加约束。
- en: Let us start with defining the ranking problem. We have *m* items, and the goal
    is to output a list of *n* items (n ≪ m) in an order that is most valuable to
    a consumer or provider. We define a number, Wij, that captures the value that
    an item, *i*, *i* ∈ [m], contributes if placed at rank *j*. There is more than
    one way to get the value of Wij. We can define it as a **ranking maximization
    problem**. Here, the task is to assign each item a position such that the total
    value is maximized.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义排序问题开始。我们有 *m* 个项目，目标是输出一个 *n* 个项目的列表（n ≪ m），该列表对消费者或提供者最有价值。我们定义一个数字
    *Wij*，它表示如果第 *i* 个项目（*i* ∈ [m]）排在第 *j* 位时所贡献的价值。获取 *Wij* 的方法不止一种。我们可以将其定义为一个 **排序最大化问题**。在这里，任务是为每个项目分配一个位置，以最大化总价值。
- en: 'To ensure fairness, Celis et al. added constraints in the form of upper (*U*l,k)
    and lower (*L*l,k) bounds on the number of objects with property *l* that are
    allowed to appear in the top *k* position of the ranking. We can represent the
    position of items via a binary matrix, *x*, which would be an *n*x*m* matrix.
    The *x*ij element of this binary matrix tells us whether the *i*-th item is ranked
    at the *j*-th position. The constraints are put on the matrix:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保公平性，Celis 等人添加了约束条件，限制了具有属性 *l* 的对象在排名前 *k* 位中出现的数量，这些约束以上界 (*U*l,k) 和下界
    (*L*l,k) 的形式存在。我们可以通过二进制矩阵 *x* 来表示项目的位置，*x* 是一个 *n* x *m* 的矩阵。这个二进制矩阵的 *x*ij 元素告诉我们，第
    *i* 个项目是否排在第 *j* 位。约束条件被施加在矩阵上：
- en: '![](img/Formula_08_055.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_055.jpg)'
- en: 'Here, {1, 2, ... , p} is the set of properties, and Pl ⊑ [m] represents the
    set of items with property *l*. With this change, the ranking maximization will
    become the following:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，{1, 2, ... , p} 是属性的集合，Pl ⊑ [m] 表示具有属性 *l* 的项的集合。通过这一变化，排名最大化将变为以下形式：
- en: '![](img/Formula_08_058.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_058.jpg)'
- en: 'This is illustrated in the following figure:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了这一点：
- en: '![](img/Figure_8.14_B18681.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Figure_8.14_B18681.jpg)'
- en: 'Figure 8.14 – A sample matrix of W. The values represent the optimal unconstrained
    ranking (gray) and constrained ranking (orange). There is an upper constraint
    of 2 on the number of males in the top six positions (image adapted from the paper
    Fairness in rankings and recommendations: an overview)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14 – W 的样本矩阵。数值表示最佳无约束排名（灰色）和有约束排名（橙色）。在前六个位置的男性数量上有一个上限约束为2（图像改编自论文《排名与推荐中的公平性：概述》）
- en: Fairness in decision-making is an important issue that we have only begun to
    understand. For example, when there are many applicants for a job or loan, it
    can be difficult, if not impossible, to decide who should get what (job, loan
    or whatever decision is to be made) because all seem equally qualified. However,
    some candidates might deserve more consideration than others based on factors
    such as race, which could cause a societal disadvantage, even without those involved
    realizing it.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 决策中的公平性是一个我们才刚刚开始理解的重要问题。例如，当有很多求职者或贷款申请者时，如果每个人看起来都同样合适，决定谁应该获得职位、贷款或其他决策就变得非常困难，甚至是不可能的。然而，根据种族等因素，某些候选人可能比其他人更值得考虑，这可能会造成社会上的不利影响，即便当事人并未意识到这一点。
- en: Challenges of fairness
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 公平性的挑战
- en: The development of various methods for ML fairness has attracted increasing
    attention within the research community and we have made significant progress.
    However, there are still several challenges that need to be looked into. In this
    section, we briefly touch on different challenges that exist in building fair
    models.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 各种机器学习公平性方法的开发引起了研究界越来越多的关注，我们也取得了显著进展。然而，仍然存在一些需要进一步研究的挑战。在这一部分中，我们简要讨论了构建公平模型时存在的不同挑战。
- en: Missing sensitive attributes
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺失的敏感属性
- en: Fairness in ML models continues to be a challenge even if very few or even no
    sensitive attribute is known. Achieving fairness generally means ensuring that
    the resulting model is not biased against any particular group. This can be difficult
    to do when training data does not include information about individuals’ sensitive
    attributes. Most of the existing methods assume that sensitive attributes are
    explicitly known. However, with growing concern about privacy, and regulations
    such as GDPR, businesses are required to protect sensitive data.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 即使很少或者没有已知的敏感属性，机器学习模型中的公平性问题依然是一个挑战。实现公平性通常意味着确保生成的模型不会对任何特定群体存在偏见。当训练数据不包含个体敏感属性信息时，这一点尤其难以实现。目前大多数现有方法假设敏感属性是显式已知的。然而，随着隐私问题的日益关注，以及如GDPR等法规的出台，企业需要保护敏感数据。
- en: Multiple sensitive attributes
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多个敏感属性
- en: The techniques that we’ve covered in this chapter work with only one sensitive
    attribute. However, there is the possibility of there being more than one sensitive
    attribute, for example, gender and race. When there are multiple sensitive attributes
    in the data, a model that gives fair predictions for one sensitive attribute could
    give unfair predictions for other sensitive attributes. A model that is trained
    to be fair for gender can still be unfair for race. Multiple-attribute fairness
    is at present a relatively less-explored problem and should be explored in the
    future.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中介绍的技术仅适用于一个敏感属性。然而，数据中可能存在多个敏感属性，例如性别和种族。当数据中存在多个敏感属性时，一个对某个敏感属性做出公平预测的模型，可能会对其他敏感属性做出不公平的预测。一个训练为性别公平的模型，仍然可能对种族不公平。多属性公平性目前是一个相对较少研究的问题，未来应当进一步探索。
- en: Choice of fairness measurements
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公平性度量的选择
- en: We have seen, in this chapter, that the design of different techniques for mitigating
    unfairness in an algorithm depends on the desired fairness measurements. Selecting
    the right metrics to measure fairness is of utmost importance, and it depends
    on the specific circumstances being considered.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中已经看到，缓解算法不公平性的不同技术设计取决于所期望的公平性度量。选择正确的公平性度量标准至关重要，并且这取决于具体考虑的情境。
- en: Individual versus group fairness trade-off
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 个人公平性与群体公平性的权衡
- en: Group fairness is concerned with ensuring that the protected group (such as
    women) is represented fairly in the results of the ML model. Individual fairness
    is concerned with making sure that individuals with similar sensitive attributes
    (for example, race or gender) are treated similarly by the model. Group fairness
    and individual fairness have different goals. The existing unfairness mitigation
    algorithms often focus on only one of these goals. It is possible, therefore,
    that a model that is optimized for individual fairness might not be fair for a
    group and vice versa. However, there can be situations where both types of fairness
    are desirable.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 群体公平性关注确保受保护群体（如女性）在机器学习模型的结果中得到公平表现。个体公平性则关注确保具有相似敏感属性（如种族或性别）的个体在模型中获得类似对待。群体公平性和个体公平性有不同的目标。现有的不公平性缓解算法通常只关注其中一个目标。因此，一个为了个体公平性优化的模型可能对群体不公平，反之亦然。然而，也可能存在两种公平性都希望实现的情况。
- en: Interpretation and fairness
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释与公平性
- en: By utilizing interpretable ML techniques, we can gain a better understanding
    of our models and debug the model. This can serve as a tool to identify and remove
    bias and achieve fairness. For instance, in a sentiment analysis task, we can
    leverage model interpretation techniques to detect racial bias by examining the
    most significant features of each demographic group.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用可解释的机器学习技术，我们可以更好地理解我们的模型并进行调试。这可以作为识别和消除偏见并实现公平性的工具。例如，在情感分析任务中，我们可以通过分析每个群体的最重要特征，利用模型解释技术来检测种族偏见。
- en: Fairness versus model performance
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公平性与模型性能
- en: Fairness constraints typically limit the decision space of the ML model, resulting
    in a trade-off between fairness and model performance. There is a need for systematic
    and theoretical investigation of the relationship between fairness constraints
    and model performance.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性约束通常限制机器学习模型的决策空间，从而在公平性和模型性能之间产生权衡。需要对公平性约束和模型性能之间的关系进行系统的理论研究。
- en: Limited datasets
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有限的数据集
- en: 'There are very limited public datasets that can be used to study fairness.
    The following table lists some of the datasets available for benchmarking the
    fairness mitigating algorithm:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 用于研究公平性的数据集非常有限。下表列出了可用于基准测试公平性缓解算法的一些数据集：
- en: '| **Dataset** | **Description** | **Size** | **Field** |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **描述** | **大小** | **领域** |'
- en: '| German credit card dataset | Contains attributes such as personal status,
    gender, credit score, and housing status. It can be used to study fairness on
    gender- and credit-related issues. | 1,000 | Financial |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 德国信用卡数据集 | 包含个人状态、性别、信用评分和住房状况等属性。可用于研究与性别和信用相关的公平性问题。 | 1,000 | 金融 |'
- en: '| UCI adult dataset | It has attributes such as age, occupation, education,
    race, gender, marital status, and whether a person’s annual income is above $50K
    or not. | 48,842 | Social |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| UCI成人数据集 | 该数据集包含年龄、职业、教育、种族、性别、婚姻状况以及个人年收入是否超过50K美元等属性。 | 48,842 | 社会 |'
- en: '| Diversity in faces dataset | This is a large dataset of annotated facial
    images. Besides images, it also contains information about skin color, gender,
    and facial symmetry. | 1 million | Facial images |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 面部多样性数据集 | 这是一个大型注释面部图像数据集。除了图像外，它还包含有关肤色、性别和面部对称性的信息。 | 100万 | 面部图像 |'
- en: '| COMPAS dataset | This contains records for defendants from Broward County.
    It includes their jail and prison time and their demographic information. | 18,610
    | Social |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| COMPAS数据集 | 该数据集包含来自布劳沃德县被告的记录，包括他们的监禁时间和人口统计信息。 | 18,610 | 社会 |'
- en: '| Communities and crime dataset | This data from various communities in the
    US is from the US LEMAS survey and the FBI Unified Crime Report. | 1,994 | Social
    |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 社区和犯罪数据集 | 该数据来自美国各社区，来源于美国LEMAS调查和FBI统一犯罪报告。 | 1,994 | 社会 |'
- en: '| Pilot parliaments benchmark dataset | This contains the data of individuals
    in the national parliaments of three European countries (Iceland, Sweden, and
    Finland) and three African countries (South Africa, Senegal, and Rwanda). | 1,270
    | Facial images |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 议会代表基准数据集 | 该数据包含来自三个欧洲国家（冰岛、瑞典、芬兰）和三个非洲国家（南非、塞内加尔、卢旺达）国家议会成员的数据。 | 1,270
    | 面部图像 |'
- en: '| WinoBias dataset | A collection of texts designed to evaluate coreference
    resolution systems, containing sentences regarding 40 occupations, each described
    multiple times with different human pronouns, to uncover and address gender bias.
    | 3,160 | Coreference resolution |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| WinoBias数据集 | 一个用于评估共指解析系统的文本集合，包含关于40个职业的句子，每个职业使用不同的人称代词描述多次，旨在揭示和解决性别偏见。
    | 3,160 | 共指解析 |'
- en: '| Recidivism in juvenile justice dataset | This contains data about juvenile
    offenders who committed a crime between the years 2002 and 2010 in the Catalonia
    juvenile justice system. | 4,753 | Social |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 青少年司法系统再犯数据集 | 该数据集包含2002至2010年间，在加泰罗尼亚青少年司法系统中犯下罪行的青少年罪犯的数据。 | 4,753 | 社会
    |'
- en: Table 8.2 – A table of useful datasets for benchmarking fairness
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.2 – 用于公平性基准测试的有用数据集表
- en: These are some of the widely used datasets when studying fairness in algorithms.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是研究算法公平性时广泛使用的数据集。
- en: Summary
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered fairness constraints as applied to different ML
    tasks. We started with the classification task and saw how we can add a regularizer
    to the loss function to mitigate unfairness. The chapter also covered how we can
    modify the loss function (objective) and mitigate unfairness. After that, we worked
    on regression tasks. There, again, we saw how adding a regularizer term can ensure
    fair algorithms. We covered the penalty terms for both individual and group fairness.
    Then, we explored the term that can be added to cluster a task to make it fair.
    We also discussed reinforcement learning and saw how fairness constraints can
    be added to the regret function. The recommendation task was considered next,
    where we showed how adding fairness constraints in the form of upper and lower
    bounds can help in mitigating unfairness. We also discussed how the recommendation
    task is similar and different compared to the other tasks. Finally, we covered
    the challenges in fairness. We saw that there is still a lot of work needed in
    this area. Most fairness algorithms are currently in the nascent stage. There
    is a need to adopt fairness strategies in existing deep learning and ML frameworks
    so that they can be adopted widely. In the next chapter, we will talk about explainability
    in AI.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了应用于不同机器学习任务的公平性约束。我们从分类任务开始，讨论了如何在损失函数中添加正则化项以减少不公平性。本章还讨论了如何修改损失函数（目标函数）以缓解不公平性。随后，我们讨论了回归任务，同样，我们展示了如何通过添加正则化项来确保算法公平。我们讨论了个体公平性和群体公平性的惩罚项。接着，我们探讨了可以添加到聚类任务中的公平性项。我们还讨论了强化学习，并展示了如何将公平性约束添加到遗憾函数中。接下来是推荐任务，我们展示了如何通过上下界形式的公平性约束来缓解不公平性。我们还讨论了推荐任务与其他任务的异同。最后，我们讨论了公平性面临的挑战。我们看到，在这一领域仍有很多工作要做。目前，大多数公平性算法仍处于初始阶段。我们需要在现有的深度学习和机器学习框架中采纳公平性策略，以便广泛应用。在下一章中，我们将讨论人工智能中的可解释性。
- en: Further reading
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Three roads to organizational justice,* Cropanzano, R., Rupp, D. E., Mohler,
    C. J., and Schminke, M. (2001). ([https://www.emerald.com/insight/content/doi/10.1016/S0742-7301(01)20001-2/full/html](https://www.emerald.com/insight/content/doi/10.1016/S0742-7301(01)20001-2/full/html))'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*组织公平性的三条道路，* Cropanzano, R., Rupp, D. E., Mohler, C. J., 和 Schminke, M. (2001)。([https://www.emerald.com/insight/content/doi/10.1016/S0742-7301(01)20001-2/full/html](https://www.emerald.com/insight/content/doi/10.1016/S0742-7301(01)20001-2/full/html))'
- en: '*Bias and Fairness in Multimodal Machine Learning: A Case Study of Automated
    Video Interviews,* Booth, B. M., Hickman, L., Subburaj, S. K., Tay, L., Woo, S.
    E., and D’Mello, S. K. (2021, October). In *Proceedings of the 2021 International
    Conference on Multimodal Interaction* (pp. 268-277) ([https://dl.acm.org/doi/fullHtml/10.1145/3462244.3479897](https://dl.acm.org/doi/fullHtml/10.1145/3462244.3479897))'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多模态机器学习中的偏见与公平性：自动化视频面试的案例研究，* Booth, B. M., Hickman, L., Subburaj, S. K.,
    Tay, L., Woo, S. E., 和 D’Mello, S. K. (2021年10月)。收录于 *2021年国际多模态交互会议论文集* (第268-277页)
    ([https://dl.acm.org/doi/fullHtml/10.1145/3462244.3479897](https://dl.acm.org/doi/fullHtml/10.1145/3462244.3479897))'
- en: '*Algorithmic Fairness*. arXiv preprint arXiv:2001.09784, Pessach, D. and Shmueli,
    E. (2020). ([https://arxiv.org/pdf/2001.09784.pdf](https://arxiv.org/pdf/2001.09784.pdf))'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*算法公平性*。arXiv预印本arXiv:2001.09784，Pessach, D. 和 Shmueli, E. (2020)。([https://arxiv.org/pdf/2001.09784.pdf](https://arxiv.org/pdf/2001.09784.pdf))'
- en: '*Gender Bias in Translation Using Google Translate: Problems and Solution*.
    *Language Circle: Journal of Language and Literature*, 15(2), Fitria, T. N. (2021).
    ([https://journal.unnes.ac.id/nju/index.php/LC/article/download/28641/11534](https://journal.unnes.ac.id/nju/index.php/LC/article/download/28641/11534))'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用谷歌翻译中的性别偏见：问题与解决方案*。*语言圈：语言与文学期刊*，15(2)，Fitria, T. N. (2021)。([https://journal.unnes.ac.id/nju/index.php/LC/article/download/28641/11534](https://journal.unnes.ac.id/nju/index.php/LC/article/download/28641/11534))'
- en: '*Machine Learning: A Probabilistic Perspective*. MIT Press, Murphy, K. P. (2012).
    ([https://storage.googleapis.com/pub-tools-public-publication-data/pdf/38136.pdf](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/38136.pdf))'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习：一种概率视角*。MIT出版社，Murphy, K. P. (2012)。([https://storage.googleapis.com/pub-tools-public-publication-data/pdf/38136.pdf](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/38136.pdf))'
- en: '*Fairness-Aware Classifier with Prejudice Remover Regularizer*. In *Joint European
    conference on machine learning and knowledge discovery in databases* (pp. 35-50),
    Springer, Berlin, Heidelberg. Kamishima, T., Akaho, S., Asoh, H., and Sakuma,
    J. (2012, September). ([https://link.springer.com/content/pdf/10.1007/978-3-642-33486-3_3.pdf](https://link.springer.com/content/pdf/10.1007/978-3-642-33486-3_3.pdf))'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*具有偏见移除正则化器的公平感知分类器*。收录于*欧洲联合机器学习与数据库知识发现大会*（第35-50页），Springer，Berlin，Heidelberg。Kamishima,
    T., Akaho, S., Asoh, H., 和 Sakuma, J.（2012年9月）。([https://link.springer.com/content/pdf/10.1007/978-3-642-33486-3_3.pdf](https://link.springer.com/content/pdf/10.1007/978-3-642-33486-3_3.pdf))'
- en: '*Three naive Bayes approaches for discrimination-free classification*. In *Data
    mining and knowledge discovery*, 21(2), pp 277-292, Calders, T. and Verwer, S.
    (2010). ([https://link.springer.com/content/pdf/10.1007/s10618-010-0190-x.pdf](https://link.springer.com/content/pdf/10.1007/s10618-010-0190-x.pdf))'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*三种朴素贝叶斯方法用于无歧视分类*。收录于*数据挖掘与知识发现*，21(2)，第277-292页，Calders, T. 和 Verwer, S.（2010年）。([https://link.springer.com/content/pdf/10.1007/s10618-010-0190-x.pdf](https://link.springer.com/content/pdf/10.1007/s10618-010-0190-x.pdf))'
- en: '*Fairness Constraints: Mechanisms for Fair Classification*. In *Artificial
    intelligence and statistics* (pp. 962-970). PMLR, Zafar, M. B., Valera, I., Rogriguez,
    M. G., and Gummadi, K. P. (2017, April). ([https://proceedings.mlr.press/v54/zafar17a/zafar17a.pdf](https://proceedings.mlr.press/v54/zafar17a/zafar17a.pdf))'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*公平约束：公平分类机制*。收录于*人工智能与统计*（第962-970页），PMLR，Zafar, M. B., Valera, I., Rogriguez,
    M. G., 和 Gummadi, K. P.（2017年4月）。([https://proceedings.mlr.press/v54/zafar17a/zafar17a.pdf](https://proceedings.mlr.press/v54/zafar17a/zafar17a.pdf))'
- en: '*A Convex Framework for Fair Regression,* Berk, R., Heidari, H., Jabbari, S.,
    Joseph, M., Kearns, M., Morgenstern, J., ... and Roth, A. (2017). arXiv preprint
    arXiv:1706.02409\. ([https://arxiv.org/pdf/1706.02409](https://arxiv.org/pdf/1706.02409))'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*公平回归的凸框架*，Berk, R., Heidari, H., Jabbari, S., Joseph, M., Kearns, M., Morgenstern,
    J., ... 和 Roth, A.（2017年）。arXiv预印本arXiv:1706.02409。([https://arxiv.org/pdf/1706.02409](https://arxiv.org/pdf/1706.02409))'
- en: '*UCI machine learning repository*, 2013, Lichman, M. URL: [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*UCI机器学习库*，2013，Lichman, M. URL: [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)'
- en: '*Human-level control through deep reinforcement learning. Nature*, 518(7540),
    pp 529-533, Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare,
    M. G., ... and Hassabis, D. (2015). ([https://www.nature.com/articles/nature14236?wm=book_wap_0005](https://www.nature.com/articles/nature14236?wm=book_wap_0005))'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过深度强化学习实现人类级控制。《自然》*，518(7540)，第529-533页，Mnih, V., Kavukcuoglu, K., Silver,
    D., Rusu, A. A., Veness, J., Bellemare, M. G., ... 和 Hassabis, D.（2015年）。([https://www.nature.com/articles/nature14236?wm=book_wap_0005](https://www.nature.com/articles/nature14236?wm=book_wap_0005))'
- en: '*Fairness in rankings and recommendations: an overview*. *The VLDB Journal*,
    pp 1-28, Pitoura, E., Stefanidis, K., and Koutrika, G. (2021). ([https://link.springer.com/article/10.1007/s00778-021-00697-y](https://link.springer.com/article/10.1007/s00778-021-00697-y))'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*排名与推荐中的公平性：概述*。*VLDB期刊*，第1-28页，Pitoura, E., Stefanidis, K., 和 Koutrika, G.（2021年）。([https://link.springer.com/article/10.1007/s00778-021-00697-y](https://link.springer.com/article/10.1007/s00778-021-00697-y))'
- en: '*Ranking with Fairness Constraints*. arXiv preprint arXiv:1704.06840, Celis,
    L. E., Straszak, D., and Vishnoi, N. K. (2017). ([https://arxiv.org/pdf/1704.06840.pdf](https://arxiv.org/pdf/1704.06840.pdf))'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*带有公平性约束的排名*。arXiv预印本arXiv:1704.06840，Celis, L. E., Straszak, D., 和 Vishnoi,
    N. K.（2017年）。([https://arxiv.org/pdf/1704.06840.pdf](https://arxiv.org/pdf/1704.06840.pdf))'
