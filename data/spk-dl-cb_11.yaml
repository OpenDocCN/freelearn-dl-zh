- en: Creating and Visualizing Word Vectors Using Word2Vec
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Word2Vec创建和可视化词向量
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下内容：
- en: Acquiring data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取数据
- en: Importing the necessary libraries
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入必要的库
- en: Preparing the data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Building and training the model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和训练模型
- en: Visualizing further
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进一步可视化
- en: Analyzing further
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进一步分析
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Before training a neural network on text data and generating text using LSTM
    cells, it is important to understand how text data (such as words, sentences,
    customer reviews, or stories) is converted to word vectors first before it is
    fed into a neural network. This chapter will describe how to convert a text into
    a corpus and generate word vectors from the corpus, which makes it easy to group
    similar words using techniques such as Euclidean distance calculation or cosine
    distance calculation between different word vectors.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在对文本数据进行神经网络训练并使用LSTM单元生成文本之前，理解文本数据（例如词语、句子、客户评论或故事）如何首先转换为词向量，并输入到神经网络中是非常重要的。本章将描述如何将文本转换为语料库，并从中生成词向量，使得使用欧几里得距离计算或余弦距离计算等技术将相似词语进行分组变得容易。
- en: Acquiring data
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: The first step is to acquire some data to work with. For this chapter, we will
    require a lot of text data to convert it into tokens and visualize it to understand
    how neural networks rank word vectors based on Euclidean and Cosine distances.
    It is an important step in understanding how different words get associated with
    each other. This, in turn, can be used to design better, more efficient language
    and text-processing models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是获取一些数据来进行处理。在本章中，我们将需要大量的文本数据，将其转换为标记（tokens），并可视化以理解神经网络如何基于欧几里得距离和余弦相似度来对词向量进行排名。这是理解不同词语如何相互关联的重要步骤。反过来，这可以用于设计更好、更高效的语言和文本处理模型。
- en: Getting ready
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Consider the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑以下内容：
- en: The text data for the model needs to be in files of `.txt` format, and you must
    ensure that the files are placed in the current working directory. The text data
    can be anything from Twitter feeds, news feeds, customer reviews, computer code,
    or whole books saved in the `.txt` format in the working directory. In our case,
    we have used the *Game of Thrones* books as the input text to our model. However,
    any text can be substituted in place of the books, and the same model will work.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的文本数据需要以`.txt`格式保存文件，并且必须确保这些文件放在当前工作目录中。文本数据可以是任何内容，例如Twitter动态、新闻摘要、客户评论、计算机代码或保存在`.txt`格式中的完整书籍。在我们的例子中，我们使用了《权力的游戏》系列书籍作为模型的输入文本。然而，任何文本都可以代替书籍，且相同的模型仍然有效。
- en: Many classical texts are no longer protected under copyright. This means that
    you can download all of the text for these books for free and use them in experiments,
    such as creating generative models. The best place to get access to free books
    that are no longer protected by copyright is Project Gutenberg ([https://www.gutenberg.org/](https://www.gutenberg.org/)).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多经典的文本已经不再受到版权保护。这意味着你可以免费下载这些书籍的所有文本，并将它们用于实验，例如创建生成模型。获得不再受版权保护的免费书籍的最佳网站是古腾堡计划（[https://www.gutenberg.org/](https://www.gutenberg.org/)）。
- en: How to do it...
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The steps are as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'Begin by visiting the Project Gutenberg website and browsing for a book that
    interests you. Click on the book, and then click on UTF-8, which allows you to
    download the book in plain-text format. The link is shown in the following screenshot:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先访问古腾堡计划网站，浏览你感兴趣的书籍。点击书籍，然后点击UTF-8，这样你就可以以纯文本格式下载该书籍。链接如以下截图所示：
- en: '![](img/de2e19ef-378b-4782-b3b1-0393bdb35026.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de2e19ef-378b-4782-b3b1-0393bdb35026.png)'
- en: Project Gutenberg Dataset download page
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 古腾堡计划数据集下载页面
- en: 'After clicking on Plain Text UTF-8, you should see a page that looks like the
    following screenshot. Right click on the page and click on Save As... Next, rename
    the file to whatever you choose and save it in your working directory:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“Plain Text UTF-8”后，你应该会看到一个如下所示的页面。右键点击页面并选择“另存为...”，接下来，将文件重命名为你想要的名称，并保存到工作目录中：
- en: '![](img/44d32c31-04e8-4ee0-b4e6-2106b0afafc4.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/44d32c31-04e8-4ee0-b4e6-2106b0afafc4.png)'
- en: You should now see a `.txt` file with the specified filename in your current
    working directory.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你应该能在当前工作目录中看到一个`.txt`文件，文件名已按指定格式保存。
- en: Project Gutenberg adds a standard header and footer to each book; this is not
    part of the original text. Open the file in a text editor, and delete the header
    and the footer.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Project Gutenberg 在每本书的开头和结尾添加标准的页眉和页脚；这些并不是原始文本的一部分。请在文本编辑器中打开文件，删除页眉和页脚。
- en: How it works...
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The functionality is as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 该功能如下：
- en: Check for the current working directory using the following command: `pwd`.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令检查当前工作目录：`pwd`。
- en: 'The working directory can be changed using the `cd` command as shown in the
    following screenshot:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工作目录可以使用 `cd` 命令更改，如下所示截图：
- en: '![](img/4399df8e-ceb1-4d15-aa8f-abd4ab5b8e26.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4399df8e-ceb1-4d15-aa8f-abd4ab5b8e26.png)'
- en: Notice that, in our case, the text files are contained in a folder named `USF`,
    and, therefore, this is set as the working directory. You may similarly store
    one or more `.txt` files in the working directory for use as input to the model.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，在我们的案例中，文本文件存放在名为 `USF` 的文件夹中，因此这被设置为工作目录。你也可以类似地将一个或多个 `.txt` 文件存放在工作目录中，作为模型的输入。
- en: UTF-8 specifies the type of encoding of the characters in the text file. **UTF-8**
    stands for **Unicode Transformation Format**. The **8** means it uses **8-bit**
    blocks to represent a character.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: UTF-8 指定了文本文件中字符的编码类型。**UTF-8** 代表 **Unicode Transformation Format**。其中的 **8**
    表示它使用 **8位** 块来表示一个字符。
- en: UTF-8 is a compromise character encoding that can be as compact as ASCII (if
    the file is just plain-English text) but can also contain any Unicode characters
    (with some increase in file size).
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: UTF-8 是一种折衷的字符编码，它可以像 ASCII 一样紧凑（如果文件仅为纯英文文本），但也能包含任何 Unicode 字符（尽管文件大小会有所增加）。
- en: It is not necessary for the text file to be in a UTF-8 format, as we will use
    the codecs library at a later stage to encode all the text into the Latin1 encoding
    format.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本文件不需要是 UTF-8 格式，因为我们将在后期使用 codecs 库将所有文本编码为 Latin1 编码格式。
- en: There's more...
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'For more information about UTF-8 and Latin1 encoding formats, visit the following
    links:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 UTF-8 和 Latin1 编码格式的更多信息，请访问以下链接：
- en: '[https://en.wikipedia.org/wiki/UTF-8](https://en.wikipedia.org/wiki/UTF-8)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/UTF-8](https://en.wikipedia.org/wiki/UTF-8)'
- en: '[http://www.ic.unicamp.br/~stolfi/EXPORT/www/ISO-8859-1-Encoding.html](http://www.ic.unicamp.br/~stolfi/EXPORT/www/ISO-8859-1-Encoding.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.ic.unicamp.br/~stolfi/EXPORT/www/ISO-8859-1-Encoding.html](http://www.ic.unicamp.br/~stolfi/EXPORT/www/ISO-8859-1-Encoding.html)'
- en: See also
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'Visit the following link to understand the need for word vectors in neural
    networks better:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 访问以下链接以更好地理解神经网络中单词向量的需求：
- en: '[https://medium.com/deep-math-machine-learning-ai/chapter-9-1-nlp-word-vectors-d51bff9628c1](https://medium.com/deep-math-machine-learning-ai/chapter-9-1-nlp-word-vectors-d51bff9628c1)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://medium.com/deep-math-machine-learning-ai/chapter-9-1-nlp-word-vectors-d51bff9628c1](https://medium.com/deep-math-machine-learning-ai/chapter-9-1-nlp-word-vectors-d51bff9628c1)'
- en: 'Listed below are some other useful articles related to the topic of converting
    words to vectors:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列出了一些与将单词转换为向量相关的其他有用文章：
- en: '[https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/)'
- en: '[https://towardsdatascience.com/word-to-vectors-natural-language-processing-b253dd0b0817](https://towardsdatascience.com/word-to-vectors-natural-language-processing-b253dd0b0817)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/word-to-vectors-natural-language-processing-b253dd0b0817](https://towardsdatascience.com/word-to-vectors-natural-language-processing-b253dd0b0817)'
- en: Importing the necessary libraries
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入必要的库
- en: Before we begin, we require the following libraries and dependencies, which
    need to be imported into our Python environment. These libraries will make our
    tasks a lot easier, as they have readily available functions and models that can
    be used instead of doing that ourselves. This also makes the code more compact
    and readable.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我们需要以下库和依赖项，这些都需要导入到我们的 Python 环境中。这些库将使我们的任务变得更轻松，因为它们提供了现成的函数和模型，可以直接使用，而不需要我们自己实现。这样也使得代码更简洁、可读。
- en: Getting ready
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'The following libraries and dependencies will be required to create word vectors
    and plots and visualize the n-dimensional word vectors in a 2D space:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是创建单词向量、绘制图形以及在二维空间中可视化n维单词向量所需的库和依赖项：
- en: '`future`'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future`'
- en: '`codecs`'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`codecs`'
- en: '`glob`'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glob`'
- en: '`` `multiprocessing` ``'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`` `multiprocessing` ``'
- en: '`os`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`os`'
- en: '`` `pprint` ``'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`` `pprint` ``'
- en: '`re`'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`re`'
- en: '`nltk`'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nltk`'
- en: '`Word2Vec`'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Word2Vec`'
- en: '`sklearn`'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn`'
- en: '`numpy`'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`'
- en: '`matplotlib`'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib`'
- en: '`pandas`'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`'
- en: '`seaborn`'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seaborn`'
- en: How to do it...
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'The steps are as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'Type the following commands into your Jupyter notebook to import all the required
    libraries:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的 Jupyter notebook 中键入以下命令来导入所有必需的库：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You should see an output that looks like the following screenshot:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该看到如下截图中的输出：
- en: '![](img/cf709ffc-0cbc-419d-8b2c-b3b55f7c4daa.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf709ffc-0cbc-419d-8b2c-b3b55f7c4daa.png)'
- en: 'Next, import the `stopwords` and `punkt` libraries using the following commands:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令导入 `stopwords` 和 `punkt` 库：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output you see must look like the following screenshot:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你看到的输出应如下图所示：
- en: '![](img/0e52652d-b201-4877-8399-99a018016314.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e52652d-b201-4877-8399-99a018016314.png)'
- en: How it works...
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This section will describe the purpose of each library being used for this recipe.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述本配方中使用的每个库的目的。
- en: The `future` library is the missing link between Python 2 and Python 3\. It
    acts as a bridge between the two versions and allows us to use syntax from both
    versions.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`future` 库是 Python 2 和 Python 3 之间的桥梁。它充当这两个版本之间的桥梁，允许我们使用这两个版本的语法。'
- en: The `codecs` library will be used to perform the encoding of all words present
    in the text file. This constitutes our dataset.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`codecs` 库将用于执行文本文件中所有单词的编码处理。这构成了我们的数据集。'
- en: Regex is the library used to look up or search for a file really quickly. The
    `glob` function allows quick and efficient searching through a large database
    for a required file.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Regex 是用于快速查找或搜索文件的库。`glob` 函数允许快速高效地在大型数据库中搜索所需的文件。
- en: The `multiprocessing` library allows us to perform concurrency, which is a way
    of running multiple threads and having each thread run a different process. It
    is a way of making programs run faster by parallelization.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`multiprocessing` 库允许我们执行并发处理，这是通过运行多个线程并让每个线程运行不同的进程来实现的。这是一种通过并行化使程序运行更快的方法。'
- en: The `os` library allows easy interaction with the operating system, such as
    a Mac, Windows, and so on, and performs functions such as reading a file.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`os` 库允许与操作系统（如 Mac、Windows 等）轻松交互，并执行诸如读取文件等功能。'
- en: The `pprint` library provides a capability for pretty-printing arbitrary Python
    data structures in a form that can be used as input to the interpreter.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`pprint` 库提供了一种漂亮打印任意 Python 数据结构的能力，格式化后可以作为解释器的输入。'
- en: The `re` module provides regular expression matching operations similar to those
    found in Perl.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`re` 模块提供了类似于 Perl 中的正则表达式匹配操作。'
- en: NLTK is a natural language toolkit capable of tokenizing words in very short
    code. When fed in a whole sentence, the `nltk` function breaks up sentences and
    outputs tokens for each word. Based on these tokens, the words may be organized
    into different categories. NLTK does this by comparing each word with a huge database
    of pre-trained words called a **lexicon**.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NLTK 是一个自然语言工具包，能够用非常简短的代码对单词进行标记化。当输入一个完整的句子时，`nltk` 函数会将句子拆分并输出每个单词的标记。基于这些标记，单词可以被组织成不同的类别。NLTK
    通过将每个单词与一个庞大的预训练词库（称为**词典**）进行比较来实现这一点。
- en: '`Word2Vec` is Google''s model, trained on a huge dataset of word vectors. It
    groups semantically similar words close to one another. This will be the most
    important library for this section.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Word2Vec` 是谷歌的模型，训练于一个巨大的词向量数据集。它将语义相似的词汇彼此靠近。这将是本节最重要的库。'
- en: '`sklearn.manifold` allows the dimensionality reduction of the dataset by employing
    **t-distributed Stochastic Neighbor Embedding** (**t-SNE**) techniques. Since
    each word vector is multi-dimensional, we require some form of dimensionality
    reduction techniques to bring the dimensionality of these words down to a lower
    dimensional space so it can be visualized in a 2D space.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sklearn.manifold` 通过使用**t-分布随机邻居嵌入**（**t-SNE**）技术来实现数据集的降维。由于每个词向量是多维的，我们需要某种形式的降维技术将这些词的维度降低到较低的空间，以便可以在二维空间中进行可视化。'
- en: There's more...
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: '`Numpy` is a commonly used `math` library. `Matplotlib` is the `plotting` library
    we will utilize, and `pandas` provide a lot of flexibility in data handling by
    allowing easy reshaping, slicing, indexing, subsetting, and manipulation of data.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`Numpy` 是一个常用的 `math` 库。`Matplotlib` 是我们将使用的 `plotting` 库，`pandas` 通过允许轻松地重塑、切片、索引、子集化和操作数据，提供了很大的灵活性。'
- en: The `Seaborn` library is another statistical data visualization library that
    we require along with `matplotlib`. `Punkt` and `Stopwords` are two data-processing
    libraries that simplify tasks such as splitting a piece of text from a corpus
    into tokens (that is, via tokenization) and removing `stopwords`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`Seaborn`库是另一个统计数据可视化库，我们需要与`matplotlib`一起使用。`Punkt`和`Stopwords`是两个数据处理库，它们简化了任务，比如将语料库中的一段文本拆分为标记（即通过分词）并移除`stopwords`。'
- en: See also
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'For more information regarding some of the libraries utilized, visit the following
    links:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有关所使用的某些库的更多信息，请访问以下链接：
- en: '[https://docs.python.org/3/library/codecs.html](https://docs.python.org/3/library/codecs.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.python.org/3/library/codecs.html](https://docs.python.org/3/library/codecs.html)'
- en: '[https://docs.python.org/2/library/pprint.html](https://docs.python.org/2/library/pprint.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.python.org/2/library/pprint.html](https://docs.python.org/2/library/pprint.html)'
- en: '[https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html)'
- en: '[https://www.nltk.org/](https://www.nltk.org/)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.nltk.org/](https://www.nltk.org/)'
- en: '[https://www.tensorflow.org/tutorials/word2vec](https://www.tensorflow.org/tutorials/word2vec)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.tensorflow.org/tutorials/word2vec](https://www.tensorflow.org/tutorials/word2vec)'
- en: '[http://scikit-learn.org/stable/modules/manifold.html](http://scikit-learn.org/stable/modules/manifold.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/manifold.html](http://scikit-learn.org/stable/modules/manifold.html)'
- en: Preparing the data
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: A number of data-preprocessing steps are to be performed before the data is
    fed into the model. This section will describe how to clean the data and prepare
    it so it can be fed into the model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据输入模型之前，需要进行一系列数据预处理步骤。本节将描述如何清理数据并准备好输入模型。
- en: Getting ready
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: All the text from the `.txt` files is first converted into one big corpus. This
    is done by reading each sentence from each file and adding it to an empty corpus.
    A number of preprocessing steps are then executed to remove irregularities such
    as white spaces, spelling errors, `stopwords`, and so on. The cleaned text data
    has to then be tokenized, and the tokenized sentences are added to an empty array
    by running them through a loop.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 所有来自`.txt`文件的文本首先会被转换成一个大的语料库。通过从每个文件中读取每个句子并将其添加到一个空的语料库中来实现。接着，会执行一系列预处理步骤，以去除不规则项，如空格、拼写错误、`stopwords`等。然后，清理过的文本数据需要被分词，分词后的句子会通过循环添加到一个空的数组中。
- en: How to do it...
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The steps are as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'Type in the following commands to search for the `.txt` files within the working
    directory and print the names of the files found:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入以下命令，以在工作目录中查找`.txt`文件并打印找到的文件名：
- en: '[PRE2]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In our case, there are five books named `got1`, `got2`, `got3`, `got4`, and
    `got5` saved in the working directory.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，有五本书，分别名为`got1`、`got2`、`got3`、`got4`和`got5`，它们保存在工作目录中。
- en: 'Create a `corpus`, read each sentence starting with the first file, encode
    it, and add the encoded characters to a `corpus` using the following commands:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`corpus`，从第一个文件开始读取每个句子，对其进行编码，并将编码后的字符添加到`corpus`中，使用以下命令：
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Execute the code in the preceding steps, which should result in an output that
    looks like the following screenshot:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行前述步骤中的代码，最终输出应该类似于以下截图所示：
- en: '![](img/9dc0f8ab-814f-46ce-9efa-8946d4378b2f.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9dc0f8ab-814f-46ce-9efa-8946d4378b2f.png)'
- en: 'Load the English pickle `tokenizer` from `punkt` using the following command:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从`punkt`加载英语分词器`tokenizer`：
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`Tokenize` the entire `corpus` into sentences using the following command:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令将整个`corpus`分词为句子：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Define the function to split sentences into their constituent words as well
    as remove unnecessary characters in the following manner:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，将句子拆分为其组成的单词，并以以下方式移除不必要的字符：
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Add all the raw sentences where each word of the sentence is tokenized to a
    new array of sentences. This is done by using the following code:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有原始句子添加到一个新的句子数组中，每个句子的每个单词都已被分词。可以通过以下代码实现：
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Print a random sentence from the corpus to visually see how the `tokenizer`
    splits sentences and creates a word list from the result. This is done using the
    following commands:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印语料库中的一个随机句子，直观地看到`tokenizer`如何拆分句子并从结果中创建单词列表。可以使用以下命令来实现：
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Count the total tokens from the dataset using the following commands:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令统计数据集中的所有标记：
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: How it works...
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Executing the tokenizer and tokenizing all the sentences in the corpus should
    result in an output that looks like the one in the following screenshot:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 执行分词器并对语料库中的所有句子进行分词，应该会产生如下所示的输出：
- en: '![](img/1257f121-c354-454a-99af-54282e394815.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1257f121-c354-454a-99af-54282e394815.png)'
- en: 'Next, removing unnecessary characters, such as hyphens and special characters,
    are done in the following manner. Splitting up all the sentences using the user-defined
    `sentence_to_wordlist()` function produces an output as shown in the following
    screenshot:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，去除不必要的字符，如连字符和特殊字符，可以通过以下方式完成。使用用户定义的`sentence_to_wordlist()`函数拆分所有句子，生成如下所示的输出：
- en: '![](img/18472244-abe7-4029-a5aa-3ab539a44964.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18472244-abe7-4029-a5aa-3ab539a44964.png)'
- en: 'Adding the raw sentences to a new array named `sentences[]` produces an output
    as shown in the following screenshot:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始句子添加到一个名为`sentences[]`的新数组中，生成如下所示的输出：
- en: '![](img/c7d44661-9a51-4107-8661-8c09e6610aa4.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7d44661-9a51-4107-8661-8c09e6610aa4.png)'
- en: 'On printing the total number of tokens in the corpus, we notice that there
    are 1,110,288 tokens in the entire corpus. This is illustrated in the following
    screenshot:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当打印语料库中的总分词数时，我们注意到整个语料库中有1,110,288个分词。这在以下截图中得到了说明：
- en: '![](img/a535b630-52be-4d91-8b31-bf9626571f4d.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a535b630-52be-4d91-8b31-bf9626571f4d.png)'
- en: 'The functionality is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 功能如下：
- en: The pre-trained `tokenizer` from NLTK is used to tokenize the entire corpus
    by counting each sentence as a token. Every tokenized sentence is added to the
    variable `raw_sentences`, which stores the tokenized sentences.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用NLTK提供的预训练`tokenizer`对整个语料库进行分词，通过将每个句子作为一个分词进行计数。每个分词的句子都被添加到变量`raw_sentences`中，该变量存储了所有分词后的句子。
- en: In the next step, common stopwords are removed, and the text is cleaned by splitting
    each sentence into its words.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步，将去除常见的停用词，并通过将每个句子拆分成单词来清理文本。
- en: A random sentence along with its wordlist is printed to understand how this
    works. In our case, we have chosen to print the 50th sentence in the `raw_sentences` array.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机打印一个句子及其单词列表，以理解这一过程是如何工作的。在我们的例子中，我们选择打印`raw_sentences`数组中的第50个句子。
- en: The total number of tokens (in our case, sentences) in the sentences array are
    counted and printed. In our case, we see that 1,110,288 tokens are created by
    the `tokenizer`.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 统计并打印句子数组中的总分词数（在我们的例子中是句子）。在我们的案例中，我们看到`tokenizer`创建了1,110,288个分词。
- en: There's more...
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'More information about tokenizing paragraphs and sentences can be found by
    visiting the following links:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 有关段落和句子分词的更多信息，请访问以下链接：
- en: '[https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize](https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize](https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize)'
- en: '[https://stackoverflow.com/questions/37605710/tokenize-a-paragraph-into-sentence-and-then-into-words-in-nltk](https://stackoverflow.com/questions/37605710/tokenize-a-paragraph-into-sentence-and-then-into-words-in-nltk)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://stackoverflow.com/questions/37605710/tokenize-a-paragraph-into-sentence-and-then-into-words-in-nltk](https://stackoverflow.com/questions/37605710/tokenize-a-paragraph-into-sentence-and-then-into-words-in-nltk)'
- en: '[https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/](https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/](https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/)'
- en: See also
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'For more information about how regular expressions work, visit the following
    link:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 有关正则表达式工作原理的更多信息，请访问以下链接：
- en: '[https://stackoverflow.com/questions/13090806/clean-line-of-punctuation-and-split-into-words-python](https://stackoverflow.com/questions/13090806/clean-line-of-punctuation-and-split-into-words-python)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://stackoverflow.com/questions/13090806/clean-line-of-punctuation-and-split-into-words-python](https://stackoverflow.com/questions/13090806/clean-line-of-punctuation-and-split-into-words-python)'
- en: Building and training the model
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建和训练模型
- en: 'Once we have the text data in the form of tokens in an array, we are able to
    input it in the array format to the model. First, we have to define a number of
    hyperparameters for the model. This section will describe how to do the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有了以数组形式存储的分词数据，就可以将其输入到模型中。首先，我们需要为模型定义多个超参数。本节将介绍如何执行以下操作：
- en: Declare model hyperparameters
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 声明模型超参数
- en: Build a model using `Word2Vec`
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`Word2Vec`构建模型
- en: Train the model on the prepared dataset
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在准备好的数据集上训练模型
- en: Save and checkpoint the trained model
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存并检查训练过的模型
- en: Getting ready
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Some of the model hyperparameters that are to be declared include the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 需要声明的一些模型超参数包括以下内容：
- en: Dimensionality of resulting word vectors
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果词向量的维度
- en: Minimum word count threshold
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小词数阈值
- en: Number of parallel threads to run while training the model
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练模型时运行的并行线程数
- en: Context window length
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文窗口长度
- en: Downsampling (for frequently occurring words)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下采样（针对频繁出现的词）
- en: Setting a seed
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置种子
- en: Once the previously mentioned hyperparameters are declared, the model can be
    built using the `Word2Vec` function from the `Gensim` library.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦之前提到的超参数被声明，可以使用来自`Gensim`库的`Word2Vec`函数来构建模型。
- en: How to do it...
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The steps are as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'Declare the hyperparameters for the model using the following commands:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令声明模型的超参数：
- en: '[PRE10]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Build the model, using the declared hyperparameters, with the following lines
    of code:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用声明的超参数构建模型，代码如下：
- en: '[PRE11]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Build the model''s vocabulary using the tokenized sentences and iterating through
    all the tokens. This is done using the `build_vocab` function in the following
    manner:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用分词后的句子并遍历所有的词元来构建模型的词汇表。可以通过以下方式使用`build_vocab`函数来完成：
- en: '[PRE12]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Train the model using the following command:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令训练模型：
- en: '[PRE13]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Create a directory named trained, if it doesn''t already exist. Save and checkpoint
    the `trained` model using the following commands:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`trained`目录尚未存在，则创建该目录。使用以下命令保存并检查点`trained`模型：
- en: '[PRE14]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To load the saved model at any point, use the following command:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在任何时刻加载保存的模型，使用以下命令：
- en: '[PRE15]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: How it works...
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The functionality is as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 功能如下：
- en: The declaration of model parameters does not produce any output. It just makes
    space in the memory to store variables as model parameters. The following screenshot
    describes this process:![](img/b01b9b42-aaa9-41a6-91dd-550faff73ec6.png)
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型参数的声明不会产生任何输出，它仅仅是为存储模型参数的变量在内存中腾出空间。以下截图描述了这个过程：![](img/b01b9b42-aaa9-41a6-91dd-550faff73ec6.png)
- en: The model is built using the preceding hyperparameters. In our case, we have
    named the model `got2vec` ,but the model may be named as per your liking. The
    model definition is illustrated in the following screenshot:![](img/abe1bb95-6a89-45e2-8060-229f2759473c.png)
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型是使用前述的超参数构建的。在我们的案例中，我们将模型命名为`got2vec`，但模型的名称可以按您的喜好来命名。模型定义如下截图所示：![](img/abe1bb95-6a89-45e2-8060-229f2759473c.png)
- en: Running the `build_vocab` command on the model should produce an output as seen
    in the following screenshot:![](img/54a633e8-d76c-42a8-b40c-aea8e440a79d.png)
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模型上运行`build_vocab`命令后，应该会产生如下截图所示的输出：![](img/54a633e8-d76c-42a8-b40c-aea8e440a79d.png)
- en: Training the model is done by defining the parameters as seen in the following
    screenshot:![](img/a6116087-5efc-474e-9e00-69289485ade7.png)
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型通过定义如下截图中的参数来完成：![](img/a6116087-5efc-474e-9e00-69289485ade7.png)
- en: 'The above command produces an output as shown in the following screenshot:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述命令会产生如下截图所示的输出：
- en: '![](img/9d7dd945-1ed2-497f-9cd1-2e7daa05b4c7.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d7dd945-1ed2-497f-9cd1-2e7daa05b4c7.png)'
- en: 'The commands to save, checkpoint, and load the model produce the following
    output, as shown in the screenshot:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存、检查点和加载模型的命令会产生如下输出，如截图所示：
- en: '![](img/12a49260-afaf-484f-925c-cd73e1682646.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12a49260-afaf-484f-925c-cd73e1682646.png)'
- en: There's more...
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Consider the following:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑以下内容：
- en: In our case, we notice the `build_vocab` function identifies 23,960 different
    word types from a list of 1,110,288 words. However, this number will vary for
    different text corpora.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们注意到`build_vocab`函数从1,110,288个词中识别出23,960个不同的词类型。然而，这个数字会因文本语料的不同而有所变化。
- en: Each word is represented by a 300-dimensional vector since we have declared
    the dimensionality to be 300\. Increasing this number increases the training time
    of the model but also makes sure the model generalizes easily to new data.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个词被表示为一个300维的向量，因为我们已经将维度声明为300。增加这个数字会增加模型的训练时间，但也确保模型能更容易地推广到新数据。
- en: The downsampling rate of 1e![](img/4b993d4b-623a-44ac-8980-51f3d1ea5c11.png)3
    is found to be a good rate. This is specified to let the model know when to downsample
    frequently occurring words, as they are not of much importance when it comes to
    analysis. Examples of such words are this, that, those, them, and so on.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现1e![](img/4b993d4b-623a-44ac-8980-51f3d1ea5c11.png)3的下采样率是一个不错的比率。该值用来告诉模型何时对频繁出现的词进行下采样，因为这些词在分析中并不重要。此类词的例子包括：this、that、those、them等等。
- en: A seed is set to make results reproducible. Setting a seed also makes debugging
    a lot easier.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置一个种子以确保结果可重复。设置种子也能让调试变得更加容易。
- en: Training the model takes about 30 seconds using regular CPU computing since
    the model is not very complex.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型大约需要30秒，使用普通CPU计算，因为模型不复杂。
- en: The model, when check-pointed, is saved under the `trained` folder inside the
    working directory.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在检查点时被保存在工作目录中的`trained`文件夹下。
- en: See also
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'For more information on `Word2Vec` models and the Gensim library, visit the
    following link:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于`Word2Vec`模型和Gensim库的信息，请访问以下链接：
- en: '[https://radimrehurek.com/gensim/models/word2vec.html](https://radimrehurek.com/gensim/models/word2vec.html)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://radimrehurek.com/gensim/models/word2vec.html](https://radimrehurek.com/gensim/models/word2vec.html)'
- en: Visualizing further
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步可视化
- en: This section will describe how to squash the dimensionality of all the trained
    words and put it all into one giant matrix for visualization purposes. Since each
    word is a 300-dimensional vector, it needs to be brought down to a lower dimension
    for us to visualize it in a 2D space.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述如何将所有训练好的词的维度压缩，并将其放入一个巨大的矩阵中进行可视化。由于每个词是一个300维的向量，因此需要将其降维到较低的维度，以便我们在二维空间中可视化它。
- en: Getting ready
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Once the model is saved and checkpointed after training, begin by loading it
    into memory, as you did in the previous section. The libraries and modules that
    will be utilized in this section are:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型保存并检查点训练完成后，像上一节那样开始将模型加载到内存中。本节将使用的库和模块有：
- en: '`tSNE`'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tSNE`'
- en: '`pandas`'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`'
- en: '`Seaborn`'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Seaborn`'
- en: '`numpy`'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`'
- en: How to do it...
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The steps are as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'Squash the dimensionality of the 300-dimensional word vectors by using the
    following command:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令压缩300维词向量的维度：
- en: '[PRE16]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Put all the word vectors into one giant matrix (named `all_word_vectors_matrix`),
    and view it using the following commands:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有词向量放入一个巨大的矩阵（命名为`all_word_vectors_matrix`）中，并使用以下命令查看：
- en: '[PRE17]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Use the `tsne` technique to fit all the learned representations into a two-
    dimensional space using the following command:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tsne`技术通过以下命令将所有学习到的表示拟合到二维空间中：
- en: '[PRE18]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Gather all the word vectors, as well as their associated words, using the following
    code:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码收集所有的词向量以及它们关联的词：
- en: '[PRE19]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `X` and `Y` coordinates and associated words of the first ten points can
    be obtained using the following command:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用以下命令获取前十个点的`X`和`Y`坐标及其关联的词：
- en: '[PRE20]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Plot all the points using the following commands:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令绘制所有点：
- en: '[PRE21]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'A selected region of the plotted graph can be zoomed into for a closer inspection.
    Do this by slicing the original data using the following function:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以放大绘制图中的选定区域进行更细致的检查。通过以下函数切片原始数据来实现这一点：
- en: '[PRE22]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Plot the sliced data using the following command. The sliced data can be visualized
    as a zoomed-in region of the original plot of all data points:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令绘制切片后的数据。切片数据可以被视为原始图中所有数据点的放大区域：
- en: '[PRE23]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: How it works...
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The functionality is as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 功能如下：
- en: The t-SNE algorithm is a non-linear dimensionality reduction technique. Computers
    are easily able to interpret and process many dimensions during their computations.
    However, humans are only capable of visualizing two or three dimensions at a time.
    Therefore, these dimensionality reduction techniques come in very handy when trying
    to draw insights from data.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: t-SNE算法是一种非线性降维技术。计算机在处理计算时可以轻松处理多个维度。然而，人类只能一次性可视化两到三维。因此，在尝试从数据中提取洞见时，这些降维技术非常有用。
- en: On applying t-SNE to the 300-dimensional vectors, we are able to squash it into
    just two dimensions to plot it and view it.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将t-SNE应用于300维的词向量后，我们能够将其压缩成二维，进行绘图并查看。
- en: By specifying `n_components` as 2, we let the algorithm know that it has to
    squash the data into a two-dimensional space. Once this is done, we add all the
    squashed vectors into one giant matrix named `all_word_vectors_matrix`, which
    is illustrated in the following screenshot:![](img/325c8759-34c8-4a40-ad78-7e050945cfe2.png)
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将`n_components`指定为2，我们告诉算法必须将数据压缩到二维空间。一旦完成，我们将所有压缩后的向量加入到一个名为`all_word_vectors_matrix`的巨型矩阵中，如下图所示：![](img/325c8759-34c8-4a40-ad78-7e050945cfe2.png)
- en: The t-SNE algorithm needs to be trained on all these word vectors. The training
    takes about five minutes on a regular CPU.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: t-SNE算法需要对所有这些词向量进行训练。使用普通CPU训练大约需要五分钟。
- en: Once the t-SNE is finished training on all the word vectors, it outputs 2D vectors
    for each word. These vectors may be plotted as points by converting all of them
    into a data frame. This is done as shown in the following screenshot:![](img/e3554859-c59a-4361-929c-5aac54dcf654.png)
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦t-SNE在所有词向量上完成训练，它会输出每个单词的2D向量。这些向量可以通过将它们转换为数据框来作为点进行绘制。具体做法如下图所示：![](img/e3554859-c59a-4361-929c-5aac54dcf654.png)
- en: 'We see that the preceding code produces a number of points where each point
    represents a word along with its X and Y coordinates. On inspection of the first
    twenty points of the data frame, we see an output as illustrated in the following
    screenshot:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们看到，前面的代码生成了多个点，每个点代表一个单词及其X和Y坐标。检查数据框中的前二十个点时，输出如下图所示：
- en: '![](img/a3db5c42-0c48-4990-a657-2a8117e6df71.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3db5c42-0c48-4990-a657-2a8117e6df71.png)'
- en: On plotting all the points using the `all_word_vectors_2D` variable, you should
    see an output that looks similar to the one in the following screenshot:![](img/82c9e244-106f-4c01-825f-dbb00d5bd297.png)
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`all_word_vectors_2D`变量绘制所有点时，你应该会看到一个类似于以下截图的输出：![](img/82c9e244-106f-4c01-825f-dbb00d5bd297.png)
- en: The above command will produce a plot of all tokens or words generated from
    the entire text as shown in the following screenshot:![](img/7f14da02-37c8-42f0-a62c-019961507c8f.png)
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述命令将生成整个文本中所有标记或单词的图表，如下图所示：![](img/7f14da02-37c8-42f0-a62c-019961507c8f.png)
- en: We can use the `plot_region` function to zoom into a certain area of the plot
    so that we are able to actually see the words, along with their coordinates. This
    step is illustrated in the following screenshot:![](img/3c916ae2-d6a1-4d0b-b069-5a2a880562af.png)
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`plot_region`函数来缩放到图表的某个区域，这样就可以实际看到单词及其坐标。此步骤如以下截图所示：![](img/3c916ae2-d6a1-4d0b-b069-5a2a880562af.png)
- en: An enlarged or zoomed in area of the plot can be visualized by setting the `x_bounds` and
    `y_bounds`, values as shown in the following screenshot:![](img/a0214205-14a3-4165-ae29-22535d292dbb.png)
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过设置`x_bounds`和`y_bounds`值来放大或缩小图表的区域，如下图所示：![](img/a0214205-14a3-4165-ae29-22535d292dbb.png)
- en: A different region of the same plot can be visualized by varying the `x_bounds`
    and `y_bounds` values as shown in the following two screenshots:![](img/ab6375bb-8bea-4697-8109-3e05c12cee31.png)
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过更改`x_bounds`和`y_bounds`值来可视化同一图表的不同区域，如下图所示：![](img/ab6375bb-8bea-4697-8109-3e05c12cee31.png)
- en: '![](img/1041a93a-3c8e-42f2-a001-c0dea26cbc11.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1041a93a-3c8e-42f2-a001-c0dea26cbc11.png)'
- en: See also
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'The following additional points are of note:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些需要注意的附加要点：
- en: 'For more information on how the t-SNE algorithm works, visit the following
    link:'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解有关t-SNE算法如何工作的更多信息，请访问以下链接：
- en: '[https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm](https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm](https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm)'
- en: 'More information about cosine distance similarity and ranking can be found
    by visiting the following link:'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关余弦距离相似度和排序的更多信息，请访问以下链接：
- en: '[https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/)'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/)'
- en: 'Use the following link to explore the different functions of the `Seaborn`
    library:'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下链接来探索`Seaborn`库的不同功能：
- en: '[https://seaborn.pydata.org/](https://seaborn.pydata.org/)'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://seaborn.pydata.org/](https://seaborn.pydata.org/)'
- en: Analyzing further
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步分析
- en: This section will describe further analysis that can be performed on the data
    after visualization. For example, exploring cosine distance similarity between
    different word vectors.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述在可视化后可以对数据执行的进一步分析。例如，探索不同词向量之间的余弦距离相似度。
- en: Getting ready
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'The following link is a great blog on how cosine distance similarity works
    and also discusses some of the math involved:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接是一个很好的博客，讲解了余弦距离相似度的原理，并讨论了一些相关的数学内容：
- en: '[http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/)'
- en: How to do it...
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'Consider the following:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑以下内容：
- en: 'Various natural-language processing tasks can be performed using the different
    functions of `Word2Vec`. One of them is finding the most semantically similar
    words given a certain word (that is, word vectors that have a high cosine similarity
    or a short Euclidean distance between them). This can be done by using the `most_similar` function
    form `Word2Vec`, as shown in the following screenshot:![](img/9ac6fa2d-4380-4100-80be-ec37006e6efd.png)This
    screenshots  all the closest words related to the word `Lannister`:'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`Word2Vec`的不同功能，可以执行各种自然语言处理任务。其中一个任务是给定某个单词后，找出最语义相似的单词（即，具有高余弦相似度或较短欧几里得距离的词向量）。这可以通过使用`Word2Vec`的`most_similar`函数来实现，如以下截图所示：![](img/9ac6fa2d-4380-4100-80be-ec37006e6efd.png)该截图显示了与单词`Lannister`相关的所有最接近的单词：
- en: '![](img/30dd7d1c-bbed-4cef-9a87-377ff69d022a.png)This screenshot shows a list
    of all the words related to word `Jon`:![](img/ec9b90eb-4a33-49a9-ac48-75b56b4f65e6.png)'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/30dd7d1c-bbed-4cef-9a87-377ff69d022a.png)该截图显示了与单词`Jon`相关的所有词汇：![](img/ec9b90eb-4a33-49a9-ac48-75b56b4f65e6.png)'
- en: How it works...
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Consider the following:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑以下内容：
- en: 'There are various methods to measure the semantic similarity between words.
    The one we are using in this section is based on cosine similarity. We can also
    explore linear relationships between words by using the following lines of code:'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有多种方法可以测量单词之间的语义相似度。本节中我们使用的方法是基于余弦相似度的。我们还可以通过使用以下代码行来探索单词之间的线性关系：
- en: '[PRE24]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To find the cosine similarity of nearest words to a given set of words, use
    the following commands:'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要找出与给定一组单词最接近的单词的余弦相似度，可以使用以下命令：
- en: '[PRE25]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The preceding process is illustrated in the following screenshot:![](img/09338881-485f-4402-945c-c4ff5abc61b5.png)
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上述过程在以下截图中进行了说明：![](img/09338881-485f-4402-945c-c4ff5abc61b5.png)
- en: The results are as follows:![](img/0af641a8-cf30-4c33-8994-300ef137b545.png)
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果如下：![](img/0af641a8-cf30-4c33-8994-300ef137b545.png)
- en: As seen in this section, word vectors form the basis of all NLP tasks. It is
    important to understand them and the math that goes into building these models
    before diving into more complicated NLP models such as **recurrent neural networks** and **Long
    Short-Term Memory** (**LSTM**) cells.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如本节所示，词向量构成了所有自然语言处理（NLP）任务的基础。在深入研究更复杂的NLP模型，如**递归神经网络**和**长短期记忆**（**LSTM**）单元之前，理解词向量及其构建数学模型是非常重要的。
- en: See also
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'Further reading can be undertaken for a better understanding of the use of
    cosine distance similarity, clustering and other machine learning techniques used
    in ranking word vectors. Provided below are a few links to useful published papers
    on this topic:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解余弦距离相似度、聚类和用于排序词向量的其他机器学习技术，可以进一步阅读以下内容。以下是一些关于此主题的有用论文链接：
- en: '[https://s3.amazonaws.com/academia.edu.documents/32952068/pg049_Similarity_Measures_for_Text_Document_Clustering.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1530163881&Signature=YG6YjvJb2z0JjmfHzaYujA2ioIo%3D&response-content-disposition=inline%3B%20filename%3DSimilarity_Measures_for_Text_Document_Cl.pdf](https://s3.amazonaws.com/academia.edu.documents/32952068/pg049_Similarity_Measures_for_Text_Document_Clustering.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1530163881&Signature=YG6YjvJb2z0JjmfHzaYujA2ioIo%3D&response-content-disposition=inline%3B%20filename%3DSimilarity_Measures_for_Text_Document_Cl.pdf)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://s3.amazonaws.com/academia.edu.documents/32952068/pg049_Similarity_Measures_for_Text_Document_Clustering.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1530163881&Signature=YG6YjvJb2z0JjmfHzaYujA2ioIo%3D&response-content-disposition=inline%3B%20filename%3DSimilarity_Measures_for_Text_Document_Cl.pdf](https://s3.amazonaws.com/academia.edu.documents/32952068/pg049_Similarity_Measures_for_Text_Document_Clustering.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1530163881&Signature=YG6YjvJb2z0JjmfHzaYujA2ioIo%3D&response-content-disposition=inline%3B%20filename%3DSimilarity_Measures_for_Text_Document_Cl.pdf)'
- en: '[http://csis.pace.edu/ctappert/dps/d861-12/session4-p2.pdf](http://csis.pace.edu/ctappert/dps/d861-12/session4-p2.pdf)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://csis.pace.edu/ctappert/dps/d861-12/session4-p2.pdf](http://csis.pace.edu/ctappert/dps/d861-12/session4-p2.pdf)'
