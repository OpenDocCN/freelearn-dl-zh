- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Understanding Temporal Difference Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解时间差学习
- en: '**Temporal difference** (**TD**) learning is one of the most popular and widely
    used model-free methods. The reason for this is that TD learning combines the
    advantages of both the **dynamic programming** (**DP**) method and the **Monte
    Carlo** (**MC**) method we covered in the previous chapters.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**时间差**（**TD**）学习是最受欢迎且广泛使用的无模型方法之一。原因在于，TD学习结合了我们在前几章中介绍的**动态规划**（**DP**）方法和**蒙特卡洛**（**MC**）方法的优点。'
- en: We will begin the chapter by understanding how exactly TD learning is beneficial
    compared to DP and MC methods. Later, we will learn how to perform the prediction
    task using TD learning. Going forward, we will learn how to perform TD control
    tasks with an on-policy TD control method called SARSA and an off-policy TD control
    method called Q learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时，我们将了解与DP和MC方法相比，TD学习究竟有哪些好处。随后，我们将学习如何使用TD学习进行预测任务。接下来，我们将学习如何使用一种称为SARSA的基于策略TD控制方法和一种称为Q学习的离策略TD控制方法来执行TD控制任务。
- en: We will also learn how to find the optimal policy in the Frozen Lake environment
    using SARSA and the Q learning method. At the end of the chapter, we will compare
    the DP, MC, and TD methods.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将学习如何在Frozen Lake环境中使用SARSA和Q学习方法找到最优策略。在本章结束时，我们将比较DP、MC和TD方法。
- en: 'Thus, in this chapter, we will learn about the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们将学习以下主题：
- en: TD learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD学习
- en: TD prediction method
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD预测方法
- en: TD control method
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD控制方法
- en: On-policy TD control – SARSA
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于策略TD控制 – SARSA
- en: Off-policy TD control – Q learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离策略TD控制 – Q学习
- en: Implementing SARSA and Q learning to find the optimal policy
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现SARSA和Q学习以找到最优策略
- en: The difference between Q learning and SARSA
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q学习和SARSA的区别
- en: Comparing the DP, MC, and TD methods
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较DP、MC和TD方法
- en: TD learning
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TD学习
- en: The TD learning algorithm was introduced by Richard S. Sutton in 1988\. In the
    introduction of the chapter, we learned that the reason the TD method became popular
    is that it combines the advantages of DP and the MC method. But what are those
    advantages?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: TD学习算法由理查德·S·萨顿（Richard S. Sutton）于1988年提出。在本章的介绍中，我们了解到，TD方法之所以受到欢迎，是因为它结合了DP和MC方法的优点。那么，这些优点到底是什么呢？
- en: First, let's recap quickly the advantages and disadvantages of DP and the MC
    method.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们快速回顾一下DP和MC方法的优缺点。
- en: '**Dynamic programming**—The advantage of the DP method is that it uses the
    Bellman equation to compute the value of a state. That is, we have learned that
    according to the Bellman equation, the value of a state can be obtained as the
    sum of the immediate reward and the discounted value of the next state. This is
    called bootstrapping. That is, to compute the value of a state, we don''t have
    to wait till the end of the episode, instead, using the Bellman equation, we can
    estimate the value of a state just based on the value of the next state, and this
    is called bootstrapping.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态规划**——DP方法的优点是它使用贝尔曼方程来计算一个状态的价值。也就是说，我们已经了解到，根据贝尔曼方程，状态的价值可以通过即时奖励和下一个状态的折扣价值的总和来获得。这被称为自举（bootstrapping）。也就是说，计算一个状态的价值时，我们不必等到回合结束，相反，通过使用贝尔曼方程，我们可以仅根据下一个状态的价值来估算当前状态的价值，这就是所谓的自举。'
- en: 'Remember how we estimated the value function in DP methods (value and policy
    iteration)? We estimated the value function (the value of a state) as:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们如何在动态规划（DP）方法中估计价值函数吗（价值迭代和策略迭代）？我们将价值函数（一个状态的价值）估计为：
- en: '![](img/B15558_05_001.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_001.png)'
- en: As you may recollect, we learned that in order to find the value of a state,
    we didn't have to wait till the end of the episode. Instead, we bootstrap, that
    is, we estimate the value of the current state *V*(*s*) by estimating the value
    of the next state ![](img/B15558_05_002.png).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能记得的那样，我们学习到，为了找到一个状态的价值，我们不必等到回合结束。相反，我们进行自举，也就是说，我们通过估算下一个状态的价值来估算当前状态的价值
    *V*(*s*) ![](img/B15558_05_002.png)。
- en: However, the disadvantage of DP is that we can apply the DP method only when
    we know the model dynamics of the environment. That is, DP is a model-based method
    and we should know the transition probability in order to use it. When we don't
    know the model dynamics of the environment, we cannot apply the DP method.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，动态规划的缺点在于，只有在我们知道环境的模型动态时才能使用DP方法。也就是说，DP是一种基于模型的方法，我们应该知道转移概率才能使用它。当我们不知道环境的模型动态时，就无法应用DP方法。
- en: '**Monte Carlo method**—The advantage of the MC method is that it is a model-free
    method, which means that it does not require the model dynamics of the environment
    to be known in order to estimate the value and Q functions.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**蒙特卡罗方法**—MC 方法的优点在于它是一种无模型方法，这意味着它不需要已知环境的模型动态即可估算值函数和 Q 函数。'
- en: However, the disadvantage of the MC method is that in order to estimate the
    state value or Q value we need to wait until the end of the episode, and if the
    episode is long then it will cost us a lot of time. Also, we cannot apply MC methods
    to continuous tasks (non-episodic tasks).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，MC 方法的缺点在于，为了估算状态值或 Q 值，我们需要等到回合结束，而如果回合较长，这将消耗大量时间。此外，我们无法将 MC 方法应用于连续任务（非回合任务）。
- en: Now, let's get back to TD learning. The TD learning algorithm takes the benefits
    of the DP and the MC methods into account. So, just like in DP, we perform bootstrapping
    so that we don't have to wait until the end of an episode to compute the state
    value or Q value, and just like the MC method, it is a model-free method and so
    it does not require the model dynamics of the environment to compute the state
    value or Q value. Now that we have the basic idea behind the TD learning algorithm,
    let's get into the details and learn exactly how it works.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到 TD 学习。TD 学习算法考虑了 DP 和 MC 方法的优点。因此，像在 DP 中一样，我们执行自举（bootstrapping），这样我们就不必等到回合结束才能计算状态值或
    Q 值；像 MC 方法一样，它是一种无模型方法，因此在计算状态值或 Q 值时不需要环境的模型动态。现在我们已经了解了 TD 学习算法的基本理念，让我们深入细节，学习它是如何工作的。
- en: 'Similar to what we learned in *Chapter 4*, *Monte Carlo Methods*, we can use
    the TD learning algorithm for both the prediction and control tasks, and so we
    can categorize TD learning into:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们在*第 4 章*中学到的*蒙特卡罗方法*，我们可以将 TD 学习算法应用于预测任务和控制任务，因此我们可以将 TD 学习分为：
- en: TD prediction
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD 预测
- en: TD control
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD 控制
- en: We learned what the prediction and control methods mean in the previous chapter.
    Let's recap that a bit before going forward.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了预测方法和控制方法的含义。让我们在继续之前稍作回顾。
- en: In the prediction method, a policy is given as an input and we try to predict
    the value function or Q function using the given policy. If we predict the value
    function using the given policy, then we can say how good it is for the agent
    to be in each state if it uses the given policy. That is, we can say what the
    expected return an agent can get in each state if it acts according to the given
    policy.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测方法中，给定一个策略作为输入，我们尝试使用该策略预测值函数或 Q 函数。如果我们使用给定的策略预测值函数，那么我们就能知道，如果代理按照该策略行动，它在每个状态中的表现如何。换句话说，我们可以知道代理在每个状态下，如果遵循给定策略，它能获得的预期回报。
- en: In the control method, we are not given a policy as input, and the goal in the
    control method is to find the optimal policy. So, we initialize a random policy
    and then we try to find the optimal policy iteratively. That is, we try to find
    an optimal policy that gives us the maximum return.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制方法中，我们没有给定一个策略作为输入，控制方法的目标是找到最优策略。因此，我们初始化一个随机策略，然后我们尝试通过迭代找到最优策略。也就是说，我们尝试找到一个给我们最大回报的最优策略。
- en: First, let's see how to use TD learning to perform prediction task, and then
    we will learn how to use TD learning for the control task.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看如何使用 TD 学习来执行预测任务，然后我们将学习如何使用 TD 学习来执行控制任务。
- en: TD prediction
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TD 预测
- en: In the TD prediction method, the policy is given as input and we try to estimate
    the value function using the given policy. TD learning bootstraps like DP, so
    it does not have to wait till the end of the episode, and like the MC method,
    it does not require the model dynamics of the environment to compute the value
    function or the Q function. Now, let's see how the update rule of TD learning
    is designed, taking the preceding advantages into account.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TD 预测方法中，给定一个策略作为输入，我们尝试使用该策略估算值函数。TD 学习像 DP 一样进行自举（bootstrapping），因此它不需要等到回合结束，而像
    MC 方法一样，它不需要环境的模型动态来计算值函数或 Q 函数。现在，让我们看看 TD 学习的更新规则是如何设计的，考虑到前述的优点。
- en: 'In the MC method, we estimate the value of a state by taking its return:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MC 方法中，我们通过取回报来估算一个状态的值：
- en: '![](img/B15558_05_003.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_003.png)'
- en: 'However, a single return value cannot approximate the value of a state perfectly.
    So, we generate **N** episodes and compute the value of a state as the average
    return of a state across **N** episodes:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，单一的回报值无法完美地近似一个状态的值。因此，我们生成**N**个回合，并将一个状态的值计算为该状态在**N**个回合中的平均回报值：
- en: '![](img/B15558_04_013.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_013.png)'
- en: But with the MC method, we need to wait until the end of the episode to compute
    the value of a state and when the episode is long, it takes a lot of time. One
    more problem with the MC method is that we cannot apply it to non-episodic tasks
    (continuous tasks).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 但是使用 MC 方法时，我们需要等到回合结束后才能计算状态的值，而当回合较长时，这需要花费大量时间。MC 方法的另一个问题是，我们无法将其应用于非回合任务（连续任务）。
- en: 'So, in TD learning, we make use of bootstrapping and estimate the value of
    a state as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在 TD 学习中，我们利用自举（bootstrapping）并估计状态的值，如下所示：
- en: '![](img/B15558_05_005.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_005.png)'
- en: The preceding equation tells us that we can estimate the value of the state
    by only taking the immediate reward *r* and the discounted value of the next state
    ![](img/B15558_05_006.png). As you may observe from the preceding equation, similar
    to what we learned in DP methods (value and policy iteration), we perform bootstrapping
    but here we don't need to know the model dynamics.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的方程告诉我们，我们只需要考虑即时奖励 *r* 和下一个状态的折扣值 ![](img/B15558_05_006.png) 就能估计状态的值。如你从上述方程中所观察到的，与我们在
    DP 方法（值迭代和策略迭代）中学到的相似，我们进行了自举，但在这里我们无需了解模型动态。
- en: 'Thus, using TD learning, the value of a state is approximated as:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用 TD 学习，状态的值被近似为：
- en: '![](img/B15558_05_005.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_005.png)'
- en: However, a single value of ![](img/B15558_05_008.png) cannot approximate the
    value of a state perfectly. So, we can take a mean value and instead of taking
    an arithmetic mean, we can use the incremental mean.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，单一的 ![](img/B15558_05_008.png) 值不能完美地近似状态的值。因此，我们可以取一个均值，而不是算术平均数，我们可以使用增量均值。
- en: 'In the MC method, we learned how to use the incremental mean to estimate the
    value of the state and it given as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MC 方法中，我们学会了如何使用增量均值来估计状态的值，其公式如下：
- en: '![](img/B15558_05_009.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_009.png)'
- en: 'Similarly, here in TD learning, we can use the incremental mean and estimate
    the value of the state, as shown here:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在 TD 学习中，我们可以使用增量均值来估计状态的值，如下所示：
- en: '![](img/B15558_05_010.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_010.png)'
- en: 'This equation is called the TD learning update rule. As we can observe, the
    only difference between the TD learning and the MC method is that to compute the
    value of the state, in the MC method, we use the full return *R*, which is computed
    using the complete episode, whereas in the TD learning method, we use the bootstrap
    estimate ![](img/B15558_05_011.png) so that we don''t have to wait until the end
    of the episode to compute the value of the state. Thus, we can apply TD learning
    to non-episodic tasks as well. The following shows the difference between the
    MC method and TD learning:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程被称为 TD 学习更新规则。正如我们所观察到的，TD 学习与 MC 方法之间的唯一区别是：在 MC 方法中，我们使用完整回合计算得到的总回报 *R*
    来计算状态值，而在 TD 学习方法中，我们使用自举估计值 ![](img/B15558_05_011.png)，因此我们无需等到回合结束才能计算状态的值。这样，我们就可以将
    TD 学习应用于非回合任务。以下展示了 MC 方法和 TD 学习之间的区别：
- en: '![](img/B15558_05_01.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_01.png)'
- en: 'Figure 5.1: A comparison between MC and TD learning'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：MC 与 TD 学习的比较
- en: 'Thus, our TD learning update rule is:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的 TD 学习更新规则是：
- en: '![](img/B15558_05_010.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_010.png)'
- en: 'We learned that ![](img/B15558_05_011.png) is an estimate of the value of state
    *V*(*s*). So, we can call ![](img/B15558_05_011.png) the TD target. Thus, subtracting
    *V*(*s*) from ![](img/B15558_05_011.png) implies that we are subtracting the predicted
    value from the target value, and this is usually called the TD error. Okay, what
    about that ![](img/B15558_05_016.png)? It is basically the learning rate, also
    called the step size. That is:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，![](img/B15558_05_011.png) 是状态 *V*(*s*) 的一个估计值。因此，我们可以将 ![](img/B15558_05_011.png)
    称为 TD 目标。这样，从 ![](img/B15558_05_011.png) 中减去 *V*(*s*) 就意味着我们在从目标值中减去预测值，这通常称为
    TD 误差。好的，那么那个 ![](img/B15558_05_016.png) 呢？它基本上是学习率，也叫步长。即：
- en: '![](img/B15558_05_18.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_18.png)'
- en: 'Our TD learning update rule basically implies:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 TD 学习更新规则基本上意味着：
- en: '*Value of a state = value of a state + learning rate (reward + discount factor(value
    of next state) - value of a state)*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*状态的值 = 状态的值 + 学习率（奖励 + 折扣因子（下一个状态的值） - 状态的值）*'
- en: Now that we have seen the TD learning update rule and how TD learning is used
    to estimate the value of a state, in the next section, we will look into the TD
    prediction algorithm and get a clearer understanding of the TD learning method.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 TD 学习更新规则以及 TD 学习如何用于估计状态的值，在接下来的章节中，我们将深入探讨 TD 预测算法，并更清晰地理解 TD 学习方法。
- en: TD prediction algorithm
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TD 预测算法
- en: We learned that, in the prediction task, given a policy, we estimate the value
    function using the given policy. So, we can say what the expected return an agent
    can obtain in each state if it acts according to the given policy.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在预测任务中，给定一个策略后，我们根据该策略估计值函数。因此，我们可以说，在每个状态下，智能体如果按照给定策略行动，期望获得的回报是多少。
- en: 'We learned that the TD learning update rule is given as:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，TD 学习更新规则如下所示：
- en: '![](img/B15558_05_010.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_010.png)'
- en: Thus, using this equation, we can estimate the value function of the given policy.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过这个方程，我们可以估计给定策略的值函数。
- en: Before looking into the algorithm directly, for better understanding, first,
    let's manually calculate and see how exactly the value of a state is estimated
    using the TD learning update rule.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在直接查看算法之前，为了更好地理解，我们先手动计算并看看如何准确地使用 TD 学习更新规则估计状态的值。
- en: The upcoming sections are explained with manual calculations, for a better understanding,
    follow along with a pen and paper.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的章节通过手动计算进行解释，为了更好理解，建议拿起笔和纸跟着一起做。
- en: 'Let''s explore TD prediction with the Frozen Lake environment. We have learned
    that in the Frozen Lake environment, the goal of the agent is to reach the goal
    state **G** from the starting state **S** without visiting the hole states **H**.
    If the agent visits state **G**, we assign a reward of 1 and if it visits any
    other states, we assign a reward of 0\. *Figure 5.2* shows the Frozen Lake environment:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在“冻结湖泊”环境中探索 TD 预测。我们已经了解到，在“冻结湖泊”环境中，智能体的目标是从起始状态 **S** 到达目标状态 **G**，并避免经过洞穴状态
    **H**。如果智能体到达状态 **G**，我们给予奖励 1，如果经过其他状态，我们则给予奖励 0。*图 5.2* 显示了“冻结湖泊”环境：
- en: '![](img/B15558_05_02.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_02.png)'
- en: 'Figure 5.2: The Frozen Lake environment'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2：冻结湖泊环境
- en: We have four actions in our action space, which are *up*, *down*, *left*, and
    *right*, and we have 16 states from **S** to **G**. Instead of encoding the states
    and actions into numbers, for easier understanding, let's just keep them as they
    are. That is, let's just denote each action by the strings *up*, *down*, *left*,
    and *right*, and let's denote each state by their position in the grid. That is,
    the first state **S** is denoted by **(1,1)** and the second state **F** is denoted
    by **(1,2)** and so on to the last state **G**, which is denoted by **(4,4)**.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的动作空间有四个动作，分别是 *上*、*下*、*左* 和 *右*，而我们有从 **S** 到 **G** 的 16 个状态。为了更方便理解，暂时不将状态和动作编码成数字，我们直接使用它们的名称。也就是说，假设状态
    **S** 为 **(1,1)**，状态 **F** 为 **(1,2)**，依此类推，直到最后的状态 **G**，即 **(4,4)**。
- en: 'Now, let''s learn how to perform TD prediction in the Frozen Lake environment.
    We know that in the TD prediction method, we will be given a policy and we predict the
    value function (state value) using a given policy. Let''s suppose we are given
    the following policy. It basically tells us what action to perform in each state:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何在“冻结湖泊”环境中执行 TD 预测。我们知道，在 TD 预测方法中，给定一个策略后，我们根据该策略预测状态值函数（状态的价值）。假设我们给定了以下策略。它基本上告诉我们在每个状态下应执行的动作：
- en: '![](img/B15558_05_03.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_03.png)'
- en: 'Table 5.1: A policy'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.1：一个策略
- en: 'Now, we will see how to estimate the value function of the preceding policy
    using the TD learning method. Before going ahead, first, we initialize the values
    of all the states with random values, as shown here:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看到如何使用 TD 学习方法估计前述策略的值函数。在继续之前，我们首先将所有状态的值初始化为随机值，如下所示：
- en: '![](img/B15558_05_04.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_04.png)'
- en: 'Figure 5.3: Initialize the states with random values'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：用随机值初始化状态
- en: Say we are in state **(1,1)** and as per the given policy we take the *right*
    action and move to the next state **(1,2)**, and we receive a reward *r* of 0\.
    Let's keep the learning rate ![](img/B15558_05_016.png) as 0.1 and the discount
    factor ![](img/B15558_03_190.png) as 1 throughout this section. Now, how can we update
    the value of the state?
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们处于状态 **(1,1)**，根据给定的策略，我们执行 *右* 向的动作并移动到下一个状态 **(1,2)**，并获得奖励 *r* 为 0。我们将学习率
    ![](img/B15558_05_016.png) 设置为 0.1，折扣因子 ![](img/B15558_03_190.png) 设置为 1。现在，如何更新状态的值呢？
- en: 'Recall the TD update equation:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾 TD 更新方程：
- en: '![](img/B15558_05_010.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_010.png)'
- en: 'Substituting the value of state *V*(*s*) with *V*(1,1) and the next state ![](img/B15558_05_020.png)
    with *V*(1,2) in the preceding equation, we can write:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态 *V*(*s*) 中的 *V*(1,1) 和下一个状态 ![](img/B15558_05_020.png) 中的 *V*(1,2) 代入前面的方程中，我们可以得到：
- en: '![](img/B15558_05_021.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_021.png)'
- en: 'Substituting the reward *r* = 0, the learning rate ![](img/B15558_05_022.png),
    and the discount factor ![](img/B15558_05_023.png), we can write:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 将奖励 *r* = 0、学习率 ![](img/B15558_05_022.png) 和折扣因子 ![](img/B15558_05_023.png)
    代入，我们可以写成：
- en: '![](img/B15558_05_024.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_024.png)'
- en: 'We can get the state values from the value table shown earlier. That is, from
    the preceding value table, we can observe that the value of state **(1,1)** is
    0.9 and the value of the next state **(1,2)** is 0.6\. Substituting these values
    in the preceding equation, we can write:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从前面的价值表中获得状态值。也就是说，从前面的价值表中可以观察到状态**(1,1)**的值是0.9，下一状态**(1,2)**的值是0.6。将这些值代入前面的方程中，我们可以写成：
- en: '![](img/B15558_05_025.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_025.png)'
- en: 'Thus, the value of state **(1,1)** becomes:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，状态**(1,1)**的值变为：
- en: '![](img/B15558_05_026.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_026.png)'
- en: 'So, we update the value of state **(1,1)** as **0.87** in the value table,
    as *Figure 5.4* shows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们将状态**(1,1)**的值更新为**0.87**，如*图 5.4*所示：
- en: '![](img/B15558_05_05.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_05.png)'
- en: 'Figure 5.4: The value of state (1,1) is updated'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.4: 状态 (1,1) 的值被更新'
- en: 'Now we are in state **(1,2)**. We select the *right* action according to the
    given policy in state **(1,2)** and move to the next state **(1,3)** and receive
    a reward *r* of 0\. We can compute the value of the state as:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们处于状态**(1,2)**。根据给定的策略，在状态**(1,2)**中我们选择*右*动作，移动到下一个状态**(1,3)**并获得奖励 *r*
    为 0。我们可以计算状态的值如下：
- en: '![](img/B15558_05_010.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_010.png)'
- en: 'Substituting the value of state *V*(*s*) with *V*(1,2) and the next state ![](img/B15558_05_028.png)
    with *V*(1,3), we can write:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态 *V*(*s*) 的值替换为 *V*(1,2)，将下一个状态 ![](img/B15558_05_028.png) 替换为 *V*(1,3)，我们可以写成：
- en: '![](img/B15558_05_029.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_029.png)'
- en: 'Substituting the reward *r* = 0, the learning rate ![](img/B15558_05_030.png),
    and the discount factor ![](img/B15558_05_031.png), we can write:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 将奖励 *r* = 0、学习率 ![](img/B15558_05_030.png) 和折扣因子 ![](img/B15558_05_031.png)
    代入，我们可以写成：
- en: '![](img/B15558_05_032.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_032.png)'
- en: 'From the preceding value table, we can observe that the value of state **(1,2)**
    is 0.6 and the value of the next state **(1,3)** is 0.8, so we can write:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的价值表中，我们可以观察到状态**(1,2)**的值是0.6，下一状态**(1,3)**的值是0.8，因此我们可以写成：
- en: '![](img/B15558_05_033.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_033.png)'
- en: 'Thus, the value of state **(1,2)** becomes:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，状态**(1,2)**的值变为：
- en: '![](img/B15558_05_034.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_034.png)'
- en: 'So, we update the value of state **(1,2)** to **0.62** in the value table,
    as *Figure 5.5* shows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们将状态**(1,2)**的值更新为**0.62**，如*图 5.5*所示：
- en: '![](img/B15558_05_06.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_06.png)'
- en: 'Figure 5.5: The value of state (1,2) is updated'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.5: 状态 (1,2) 的值被更新'
- en: 'Now we are in state **(1,3)**. We select the *left* action according to our
    policy and move to the next state **(1,2)** and receive a reward *r* of 0\. We
    can compute the value of the state as:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们处于状态**(1,3)**。根据我们的策略，我们选择*左*动作，移动到下一个状态**(1,2)**并获得奖励 *r* 为 0。我们可以计算状态的值如下：
- en: '![](img/B15558_05_010.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_010.png)'
- en: 'Substituting the value of state *V*(*s*) with *V*(1,3) and the next state ![](img/B15558_05_020.png)
    with *V*(1,2), we have:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态 *V*(*s*) 的值替换为 *V*(1,3)，将下一个状态 ![](img/B15558_05_020.png) 替换为 *V*(1,2)，我们得到：
- en: '![](img/B15558_05_037.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_037.png)'
- en: 'Substituting the reward *r* = 0, the learning rate ![](img/B15558_05_038.png),
    and the discount factor ![](img/B15558_03_181.png), we can write:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 将奖励 *r* = 0、学习率 ![](img/B15558_05_038.png) 和折扣因子 ![](img/B15558_03_181.png)
    代入，我们可以写成：
- en: '![](img/B15558_05_040.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_040.png)'
- en: 'Note that we use the updated values in every step, that is, the value of state
    **(1,2)** is updated with 0.62 in the previous step, as shown in the preceding
    value table. So, we substitute *V*(1,2) with 0.62 and *V*(1,3) with 0.8:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在每一步都使用更新后的值，即状态**(1,2)**的值在上一阶段被更新为0.62，如前面的价值表所示。因此，我们将 *V*(1,2) 替换为
    0.62，将 *V*(1,3) 替换为 0.8：
- en: '![](img/B15558_05_041.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_041.png)'
- en: 'Thus, the value of state **(1,3)** becomes:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，状态**(1,3)**的值变为：
- en: '![](img/B15558_05_042.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_042.png)'
- en: 'So, we update the value of state **(1,3)** to **0.782** in the value table,
    as *Figure 5.6* shows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们将状态**(1,3)**的值更新为**0.782**，如*图 5.6*所示：
- en: '![](img/B15558_05_07.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_07.png)'
- en: 'Figure 5.6: The value of state (1,3) is updated'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.6: 状态 (1,3) 的值被更新'
- en: Thus, in this way, we compute the value of every state using the given policy.
    However, computing the value of the state just for one episode will not be accurate.
    So, we repeat these steps for several episodes and compute the accurate estimates
    of the state value (the value function).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过这种方式，我们使用给定的策略计算每个状态的值。然而，仅为一个回合计算状态的值是不准确的。因此，我们将这些步骤重复多次，计算状态值的准确估计（值函数）。
- en: 'The TD prediction algorithm is given as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: TD预测算法如下所示：
- en: Initialize a value function *V*(*s*) with random values. A policy ![](img/B15558_03_082.png)
    is given.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个带有随机值的值函数 *V*(*s*)。给定一个策略 ![](img/B15558_03_082.png)。
- en: 'For each episode:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个回合：
- en: Initialize state *s*
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化状态 *s*
- en: 'For each step in the episode:'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个步骤：
- en: Perform an action *a* in state *s* according to given policy ![](img/B15558_04_054.png),
    get the reward *r*, and move to the next state ![](img/B15558_05_045.png)
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据给定策略 ![](img/B15558_04_054.png) 在状态 *s* 中执行动作 *a*，获取奖励 *r*，并移动到下一个状态 ![](img/B15558_05_045.png)
- en: Update the value of the state to ![](img/B15558_05_010.png)
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将状态的值更新为 ![](img/B15558_05_010.png)：
- en: Update ![](img/B15558_05_047.png) (this step implies we are changing the next
    state ![](img/B15558_05_048.png) to the current state *s*)
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 ![](img/B15558_05_047.png) （这一步意味着我们将下一个状态 ![](img/B15558_05_048.png) 更新为当前状态
    *s*）
- en: If *s* is not the terminal state, repeat *steps 1* to *4*
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *s* 不是终止状态，则重复 *步骤 1* 到 *4*
- en: Now that we have learned how the TD prediction method predicts the value function
    of the given policy, in the next section, let's learn how to implement the TD prediction
    method to predict the value of states in the Frozen Lake environment.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了TD预测方法如何预测给定策略的值函数，在下一节中，让我们学习如何实现TD预测方法，预测冻结湖环境中状态的值。
- en: Predicting the value of states in the Frozen Lake environment
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在冻结湖环境中预测状态的值
- en: We have learned that in the prediction method, the policy is given as an input
    and we predict the value function using the given policy. So, let's initialize
    a random policy and predict the value function (state values) of the Frozen Lake
    environment using the random policy.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解，在预测方法中，策略作为输入给定，我们使用给定的策略预测值函数。所以，让我们初始化一个随机策略，并使用该随机策略预测冻结湖环境中的值函数（状态值）。
- en: 'First, let''s import the necessary libraries:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库：
- en: '[PRE0]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, we create the Frozen Lake environment using Gym:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用Gym创建冻结湖环境：
- en: '[PRE1]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the random policy, which returns the random action by sampling from
    the action space:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 定义随机策略，通过从动作空间中采样返回随机动作：
- en: '[PRE2]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s define the dictionary for storing the value of states, and we initialize
    the value of all the states to `0.0`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个字典来存储状态的值，并将所有状态的值初始化为 `0.0`：
- en: '[PRE3]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Initialize the discount factor ![](img/B15558_05_049.png) and the learning
    rate ![](img/B15558_05_050.png):'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化折扣因子 ![](img/B15558_05_049.png) 和学习率 ![](img/B15558_05_050.png)：
- en: '[PRE4]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Set the number of episodes and the number of time steps in each episode:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 设置回合数和每个回合中的时间步数：
- en: '[PRE5]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Compute the values of the states
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算状态的值
- en: Now, let's compute the value function (state values) using the given random
    policy.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用给定的随机策略计算值函数（状态值）。
- en: 'For each episode:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个回合：
- en: '[PRE6]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Initialize the state by resetting the environment:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重置环境来初始化状态：
- en: '[PRE7]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'For every step in the episode:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回合中的每个步骤：
- en: '[PRE8]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Select an action according to random policy:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 根据随机策略选择一个动作：
- en: '[PRE9]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Perform the selected action and store the next state information:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 执行选择的动作并存储下一个状态信息：
- en: '[PRE10]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Compute the value of the state as ![](img/B15558_05_010.png):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 计算状态的值，如 ![](img/B15558_05_010.png) 所示：
- en: '[PRE11]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Update the next state to the current state ![](img/B15558_05_052.png):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 更新下一个状态为当前状态 ![](img/B15558_05_052.png)：
- en: '[PRE12]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If the current state is the terminal state, then break:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果当前状态是终止状态，则跳出：
- en: '[PRE13]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: After all the iterations, we will have values of all the states according to
    the given random policy.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有迭代完成后，我们将根据给定的随机策略得到所有状态的值。
- en: Evaluating the values of the states
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估状态的值
- en: 'Now, let''s evaluate our value function (state values). First, let''s convert
    our value dictionary to a pandas data frame for more clarity:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们评估我们的值函数（状态值）。首先，让我们将值字典转换为 pandas 数据框以便更清晰地查看：
- en: '[PRE14]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Before checking the values of the states, let''s recollect that in Gym, all
    the states in the Frozen Lake environment will be encoded into numbers. Since
    we have 16 states, all the states will be encoded into numbers from 0 to 15 as
    *Figure 5.7* shows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查状态的值之前，让我们回顾一下在 Gym 中，所有 Frozen Lake 环境中的状态将被编码为数字。由于我们有 16 个状态，所有状态将被编码为数字
    0 到 15，如 *图 5.7* 所示：
- en: '![](img/B15558_05_08.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_08.png)'
- en: 'Figure 5.7: States encoded as numbers'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7：状态编码为数字
- en: 'Now, Let''s check the value of the states:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查状态的值：
- en: '[PRE15]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The preceding code will print:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将打印：
- en: '![](img/B15558_05_09.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_09.png)'
- en: 'Figure 5.8: Value table'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8：值表
- en: As we can observe, we now have the values of all the states. The value of state
    14 is high since we can reach goal state 15 from state 14 easily, and also, as
    we can see, the values of all the terminal states (hole states and the goal state)
    are zero.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所观察到的，现在我们已经得到了所有状态的值。状态 14 的值很高，因为我们可以轻松地从状态 14 达到目标状态 15，而且正如我们所见，所有终端状态（洞状态和目标状态）的值都是零。
- en: Note that since we have initialized a random policy, you might get varying results
    every time you run the previous code.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于我们初始化了一个随机策略，每次运行前面的代码时可能会得到不同的结果。
- en: Now that we have understood how TD learning can be used for prediction tasks,
    in the next section, we will learn how to use TD learning for control tasks.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了如何利用 TD 学习进行预测任务，在下一节中，我们将学习如何将 TD 学习应用于控制任务。
- en: TD control
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TD 控制
- en: 'In the control method, our goal is to find the optimal policy, so we will start
    off with an initial random policy and then we will try to find the optimal policy
    iteratively. In the previous chapter, we learned that the control method can be
    classified into two categories:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制方法中，我们的目标是找到最优策略，因此我们将从一个初始随机策略开始，并尝试迭代找到最优策略。在前一章中，我们学习到控制方法可以分为两类：
- en: On-policy control
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在策略控制
- en: Off-policy control
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离策略控制
- en: We learned what on-policy and off-policy control means in the previous chapter.
    Let's recap that a bit before going ahead. In the **on-policy control**, the agent
    behaves using one policy and tries to improve the same policy. That is, in the
    on-policy method, we generate episodes using one policy and improve the same policy
    iteratively to find the optimal policy. In the **off-policy control** method,
    the agent behaves using one policy and tries to improve a different policy. That
    is, in the off-policy method, we generate episodes using one policy and we try
    to improve a different policy iteratively to find the optimal policy.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前一章中学到了在策略和离策略控制的含义。在继续之前，让我们简要回顾一下。在 **在策略控制** 中，代理使用一个策略行为并尝试改进同一策略。也就是说，在在策略方法中，我们使用一个策略生成情节并迭代改进相同的策略以找到最优策略。在
    **离策略控制** 方法中，代理使用一个策略行为并尝试改进一个不同的策略。也就是说，在离策略方法中，我们使用一个策略生成情节，并尝试迭代改进一个不同的策略以找到最优策略。
- en: Now, we will learn how to perform control tasks using TD learning. First, we
    will learn how to perform on-policy TD control and then we will learn about off-policy
    TD control.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将学习如何使用 TD 学习执行控制任务。首先，我们将学习如何执行在策略 TD 控制，然后我们将学习关于离策略 TD 控制。
- en: On-policy TD control – SARSA
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在策略 TD 控制 - SARSA
- en: In this section, we will look into the popular on-policy TD control algorithm
    called **SARSA**, which stands for **State-Action-Reward-State-Action**. We know
    that in TD control our goal is to find the optimal policy. First, how can we extract
    a policy? We can extract the policy from the Q function. That is, once we have
    the Q function then we can extract policy by selecting the action in each state
    that has the maximum Q value.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究流行的在策略 TD 控制算法，称为 **SARSA**，其代表了**状态-动作-奖励-状态-动作**。我们知道，在 TD 控制中，我们的目标是找到最优策略。首先，我们如何从策略中提取策略？我们可以从
    Q 函数中提取策略。也就是说，一旦我们有了 Q 函数，我们就可以通过选择每个状态中具有最大 Q 值的动作来提取策略。
- en: 'Okay, how can we compute the Q function in TD learning? First, let''s recall
    how we compute the value function. In TD learning, the value function is computed
    as:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们如何在 TD 学习中计算 Q 函数？首先，让我们回顾一下如何计算值函数。在 TD 学习中，值函数计算为：
- en: '![](img/B15558_05_010.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_010.png)'
- en: 'We can just rewrite this update rule in terms of the Q function as:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简单地将这个更新规则重写为 Q 函数的形式：
- en: '![](img/B15558_05_054.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_054.png)'
- en: Now, we compute the Q function using the preceding TD learning update rule,
    and then we extract a policy from them. We can also call the preceding update
    rule as the SARSA update rule.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用前述的 TD 学习更新规则计算 Q 函数，然后从中提取策略。我们也可以将前述的更新规则称为 SARSA 更新规则。
- en: But wait! In the prediction method, we were given a policy as input, so we acted
    in the environment using that policy and computed the value function. But here,
    we don't have a policy as input. So how can we act in the environment?
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 但是等等！在预测方法中，我们给定了一个策略作为输入，因此我们在环境中根据该策略进行操作，并计算了价值函数。但在这里，我们没有输入策略。那么，我们如何在环境中行动呢？
- en: So, first we initialize the Q function with random values or with zeros. Then
    we extract a policy from this randomly initialized Q function and act in the environment.
    Our initial policy will definitely not be optimal as it is extracted from the
    randomly initialized Q function, but on every episode, we will update the Q function
    (Q values). So, on every episode, we can use the updated Q function to extract
    a new policy. Thus, we will obtain the optimal policy after a series of episodes.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将 Q 函数初始化为随机值或零。然后，我们从这个随机初始化的 Q 函数中提取策略并在环境中进行操作。我们的初始策略肯定不是最优的，因为它是从随机初始化的
    Q 函数中提取的，但每一轮后我们会更新 Q 函数（Q 值）。因此，在每一轮后，我们可以使用更新后的 Q 函数来提取新策略。这样，通过一系列回合后，我们将获得最优策略。
- en: One important point we need to note is that in the SARSA method, instead of
    making our policy act greedily, we use the epsilon-greedy policy. That is, in
    a greedy policy, we always select the action that has the maximum Q value. But,
    with the epsilon-greedy policy we select a random action with probability epsilon,
    and we select the best action (the action with the maximum Q value) with probability
    1-epsilon.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要注意的一个重要点是，在 SARSA 方法中，我们并不是让我们的策略贪婪地执行，而是使用 epsilon-贪婪策略。也就是说，在贪婪策略中，我们总是选择具有最大
    Q 值的操作。但是，使用 epsilon-贪婪策略时，我们以概率 epsilon 选择一个随机动作，并以概率 1-epsilon 选择最佳动作（即具有最大
    Q 值的动作）。
- en: Before looking into the algorithm directly, for a better understanding, first,
    let's manually calculate and see how exactly the Q function (Q value) is estimated
    using the SARSA update rule and how we can find the optimal policy.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在直接查看算法之前，为了更好地理解，我们首先手动计算并观察 Q 函数（Q 值）是如何通过 SARSA 更新规则估算的，以及我们如何找到最优策略。
- en: 'Let us consider the same Frozen Lake environment. Before going ahead, we initialize
    our Q table (Q function) with random values. *Figure 5.9* shows the Frozen Lake
    environment along with the Q table containing random values:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑相同的冰冻湖环境。在继续之前，我们将 Q 表（Q 函数）初始化为随机值。*图 5.9* 展示了冰冻湖环境以及包含随机值的 Q 表：
- en: '![](img/B15558_05_10.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_10.png)'
- en: 'Figure 5.9: The Frozen Lake environment and Q table with random values'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9：冰冻湖环境和带有随机值的 Q 表
- en: 'Suppose we are in state **(4,2)**. Now we need to select an action in this
    state. How can we select an action? We learned that in the SARSA method, we select
    an action based on the epsilon-greedy policy. With probability epsilon, we select
    a random action and with probability 1-epsilon we select the best action (the
    action that has the maximum Q value). Suppose we use a probability 1-epsilon and
    select the best action. So, in state **(4,2)**, we move *right* as it has the
    highest Q value compared to the other actions, as shown here:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们处于状态 **(4,2)**。现在我们需要在这个状态下选择一个动作。我们如何选择动作呢？我们知道，在 SARSA 方法中，我们根据 epsilon-贪婪策略选择动作。以概率
    epsilon，我们选择一个随机动作；以概率 1-epsilon，我们选择最佳动作（即具有最大 Q 值的动作）。假设我们使用概率 1-epsilon 选择最佳动作。因此，在状态
    **(4,2)** 中，我们选择 *右*，因为它相对于其他动作具有最高的 Q 值，如下所示：
- en: '![](img/B15558_05_11.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_11.png)'
- en: 'Figure 5.10: Our agent is in state (4,2)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10：我们的智能体处于状态 **(4,2)**
- en: 'Okay, so, we perform the *right* action in state **(4,2)** and move to the
    next state **(4,3)** as *Figure 5.11* shows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么，我们在状态 **(4,2)** 执行 *右* 操作并移动到下一个状态 **(4,3)**，正如 *图 5.11* 所示：
- en: '![](img/B15558_05_12.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_12.png)'
- en: 'Figure 5.11: We perform the action with the maximum Q value in state (4,2)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11：我们在状态 (4,2) 执行具有最大 Q 值的动作
- en: Thus, we moved *right* in state **(4,2)** to the next state **(4,3)** and received
    a reward *r* of 0\. Let's keep the learning rate ![](img/B15558_05_055.png) at
    0.1, and the discount factor ![](img/B15558_05_056.png) at 1\. Now, how can we
    update the Q value?
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在状态 **(4,2)** 向 *右* 移动到下一个状态 **(4,3)** 并获得了奖励 *r* 为 0。我们将学习率 ![](img/B15558_05_055.png)
    设置为 0.1，折扣因子 ![](img/B15558_05_056.png) 设置为 1。那么，我们该如何更新 Q 值呢？
- en: 'Let recall our SARSA update rule:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下 SARSA 更新规则：
- en: '![](img/B15558_05_054.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_054.png)'
- en: 'Substituting the state-action pair *Q*(*s*,*a*) with *Q*((4,2), right) and
    the next state ![](img/B15558_03_001.png) with **(4,3)** in the preceding equation,
    we can write:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态-动作对 *Q*(*s*,*a*) 替换为 *Q*((4,2), right) 和下一个状态 ![](img/B15558_03_001.png)
    替换为 **(4,3)**，代入前面的方程，我们可以写出：
- en: '![](img/B15558_05_059.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_059.png)'
- en: 'Substituting the reward *r* = 0, the learning rate ![](img/B15558_05_030.png),
    and the discount factor ![](img/B15558_05_061.png), we can write:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 将奖励 *r* = 0，学习率 ![](img/B15558_05_030.png)，折扣因子 ![](img/B15558_05_061.png) 代入方程，我们可以写出：
- en: '![](img/B15558_05_062.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_062.png)'
- en: 'From the previous Q table, we can observe that the Q value of *Q*((4,2), right)
    is **0.8**. Thus, substituting *Q*((4,2), right) with **0.8**, we can rewrite
    the preceding equation as:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的 Q 表中，我们可以观察到 *Q*((4,2), right) 的 Q 值为 **0.8**。因此，代入 *Q*((4,2), right) 为
    **0.8**，我们可以将前面的方程重写为：
- en: '![](img/B15558_05_063.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_063.png)'
- en: Okay, what about the term ![](img/B15558_05_064.png)? As you can see in the
    preceding equation, we have the term ![](img/B15558_05_064.png), which represents
    the Q value of the next state-action pair.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那项 ![](img/B15558_05_064.png) 怎么处理呢？如前面的方程所示，我们有一项 ![](img/B15558_05_064.png)，它表示下一个状态-动作对的
    Q 值。
- en: Because we have moved to the next state **(4,3)**, we need to select an action
    in this state in order to compute the Q value of the next state-action pair. So,
    we use our same epsilon-greedy policy to select the action. That is, we select
    a random action with a probability of epsilon, or we select the best action that
    has the maximum Q value with a probability of 1-epsilon.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们已经移动到下一个状态 **(4,3)**，所以我们需要在此状态下选择一个动作，以便计算下一个状态-动作对的 Q 值。因此，我们使用相同的 epsilon-greedy
    策略来选择动作。也就是说，我们以概率 epsilon 随机选择一个动作，或者以概率 1-epsilon 选择具有最大 Q 值的最佳动作。
- en: 'Suppose we use probability epsilon and select the random action. In state **(4,3)**,
    we select the *right* action randomly, as *Figure 5.12* shows. As you can see,
    although the *right* action does not have the maximum Q value, we selected it
    randomly with probability epsilon:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用概率 epsilon 并选择随机动作。在状态 **(4,3)**，我们随机选择 *right* 动作，如 *图 5.12* 所示。如你所见，尽管
    *right* 动作的 Q 值不是最大，但我们仍然以概率 epsilon 随机选择了它：
- en: '![](img/B15558_05_13.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_13.png)'
- en: 'Figure 5.12: We perform a random action in state (4,3)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12：我们在状态 (4,3) 执行了一个随机动作
- en: 'Thus, now our update rule becomes:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们的更新规则变为：
- en: '![](img/B15558_05_066.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_066.png)'
- en: 'From the preceding Q table, we can see that the Q value of **Q((4,3), right)**
    is **0.9**. Thus, substituting the value of **Q((4,3), right)** with **0.9**,
    we can rewrite the above equation as:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的 Q 表中，我们可以看到 **Q((4,3), right)** 的 Q 值为 **0.9**。因此，代入 **Q((4,3), right)**
    的值 **0.9**，我们可以将上面的方程重写为：
- en: '![](img/B15558_05_067.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_067.png)'
- en: 'Thus, our Q value becomes:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的 Q 值变为：
- en: '![](img/B15558_05_068.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_068.png)'
- en: Thus, in this way, we update the Q function by updating the Q value of the state-action
    pair in each step of the episode. After completing an episode, we extract a new
    policy from the updated Q function and uses this new policy to act in the environment.
    (Remember that our policy is always an epsilon-greedy policy). We repeat this
    steps for several episodes to find the optimal policy. The SARSA algorithm given
    in the following will help us understand this better.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过这种方式，我们在每一步骤中更新 Q 函数，通过更新状态-动作对的 Q 值来实现。完成一个回合后，我们从更新后的 Q 函数中提取出新的策略，并使用该新策略在环境中执行动作。（记住我们的策略始终是
    epsilon-greedy 策略）。我们重复这些步骤进行若干回合，以找到最优策略。下面给出的 SARSA 算法将帮助我们更好地理解这一点。
- en: 'The SARSA algorithm is given as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: SARSA 算法如下所示：
- en: Initialize a Q function *Q*(*s*, *a*) with random values
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 Q 函数 *Q*(*s*, *a*)，并赋予随机值
- en: 'For each episode:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个回合：
- en: Initialize state *s*
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化状态 *s*
- en: Extract a policy from *Q*(*s*, *a*) and select an action *a* to perform in state
    *s*
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *Q*(*s*, *a*) 中提取一个策略，并选择在状态 *s* 下执行的动作 *a*
- en: 'For each step in the episode:'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一步骤：
- en: Perform the action *a* and move to the next state ![](img/B15558_05_069.png)
    and observe the reward *r*
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作 *a* 并移动到下一个状态 ![](img/B15558_05_069.png)，然后观察奖励 *r*
- en: In state ![](img/B15558_05_069.png), select the action ![](img/B15558_05_071.png)
    using the epsilon-greedy policy
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在状态 ![](img/B15558_05_069.png) 下，使用 epsilon-greedy 策略选择动作 ![](img/B15558_05_071.png)
- en: Update the Q value to ![](img/B15558_05_072.png)
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Q 值更新为 ![](img/B15558_05_072.png)
- en: Update ![](img/B15558_05_052.png) and ![](img/B15558_05_074.png) (update the
    next state ![](img/B15558_05_075.png)-action ![](img/B15558_05_076.png) pair to
    the current state *s*-action *a* pair)
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 ![](img/B15558_05_052.png) 和 ![](img/B15558_05_074.png)（将下一个状态 ![](img/B15558_05_075.png)-动作
    ![](img/B15558_05_076.png) 对更新为当前状态 *s*-动作 *a* 对）
- en: If *s* is not a terminal state, repeat *steps 1* to *5*
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *s* 不是终止状态，则重复 *步骤 1* 至 *步骤 5*
- en: Now that we have learned how the SARSA algorithm works, in the next section,
    let's implement the SARSA algorithm to find the optimal policy.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了 SARSA 算法是如何工作的，那么在接下来的部分，我们将实现 SARSA 算法以找到最优策略。
- en: Computing the optimal policy using SARSA
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 SARSA 计算最优策略
- en: Now, let's implement SARSA to find the optimal policy in the Frozen Lake environment.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现 SARSA 来在 Frozen Lake 环境中找到最优策略。
- en: 'First, let''s import the necessary libraries:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库：
- en: '[PRE16]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, we create the Frozen Lake environment using Gym:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用 Gym 创建 Frozen Lake 环境：
- en: '[PRE17]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s define the dictionary for storing the Q value of the state-action pair
    and initialize the Q value of all the state-action pairs to `0.0`:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个字典来存储状态-动作对的 Q 值，并将所有状态-动作对的 Q 值初始化为 `0.0`：
- en: '[PRE18]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, let''s define the epsilon-greedy policy. We generate a random number from
    the uniform distribution and if the random number is less than epsilon, we select
    the random action, else we select the best action that has the maximum Q value:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义 epsilon-greedy 策略。我们从均匀分布中生成一个随机数，如果随机数小于 epsilon，则选择随机动作，否则选择具有最大
    Q 值的最佳动作：
- en: '[PRE19]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Initialize the discount factor ![](img/B15558_03_005.png), the learning rate
    ![](img/B15558_05_055.png), and the epsilon value:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化折扣因子 ![](img/B15558_03_005.png)，学习率 ![](img/B15558_05_055.png) 和 epsilon
    值：
- en: '[PRE20]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Set the number of episodes and number of time steps in the episode:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 设置回合数和每回合的时间步数：
- en: '[PRE21]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Compute the policy
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算策略
- en: 'For each episode:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一回合：
- en: '[PRE22]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Initialize the state by resetting the environment:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重置环境初始化状态：
- en: '[PRE23]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Select the action using the epsilon-greedy policy:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 epsilon-greedy 策略选择动作：
- en: '[PRE24]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'For each step in the episode:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个步骤，在这一回合中：
- en: '[PRE25]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Perform the selected action and store the next state information:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 执行所选动作并存储下一个状态信息：
- en: '[PRE26]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Select the action ![](img/B15558_05_079.png) in the next state ![](img/B15558_03_046.png)
    using the epsilon-greedy policy:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 epsilon-greedy 策略在下一个状态 ![](img/B15558_03_046.png) 中选择动作 ![](img/B15558_05_079.png)：
- en: '[PRE27]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Compute the Q value of the state-action pair as ![](img/B15558_05_072.png):'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 计算状态-动作对的 Q 值，公式为 ![](img/B15558_05_072.png)：
- en: '[PRE28]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Update ![](img/B15558_05_082.png) and ![](img/B15558_05_083.png) (update the
    next state ![](img/B15558_03_073.png)-action ![](img/B15558_05_076.png) pair to
    the current state *s*-action *a* pair):'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 更新 ![](img/B15558_05_082.png) 和 ![](img/B15558_05_083.png)（将下一个状态 ![](img/B15558_03_073.png)-动作
    ![](img/B15558_05_076.png) 对更新为当前状态 *s*-动作 *a* 对）：
- en: '[PRE29]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'If the current state is the terminal state, then break:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如果当前状态是终止状态，则跳出：
- en: '[PRE30]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note that on every iteration we update the Q function. After all the iterations,
    we will have the optimal Q function. Once we have the optimal Q function then
    we can extract the optimal policy by selecting the action that has the maximum
    Q value in each state.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在每次迭代中我们都会更新 Q 函数。经过所有迭代后，我们将得到最优的 Q 函数。一旦我们有了最优的 Q 函数，就可以通过选择每个状态下具有最大
    Q 值的动作来提取最优策略。
- en: Off-policy TD control – Q learning
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 脱策略 TD 控制 - Q 学习
- en: In this section, we will learn the off-policy TD control algorithm called Q
    learning. It is one of the very popular algorithms in reinforcement learning,
    and we will see that this algorithm keeps coming up in other chapters too. Q learning
    is an off-policy algorithm, meaning that we use two different policies, one policy
    for behaving in the environment (selecting an action in the environment) and the
    other for finding the optimal policy.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将学习一种称为 Q 学习的脱策略 TD 控制算法。它是强化学习中非常流行的算法之一，我们将看到这一算法在其他章节中也会出现。Q 学习是一种脱策略算法，这意味着我们使用两种不同的策略，一种策略用于在环境中行为（选择动作），另一种策略用于寻找最优策略。
- en: 'We learned that in the SARSA method, we select action *a* in state *s* using
    the epsilon-greedy policy, move to the next state ![](img/B15558_03_018.png),
    and update the Q value using the update rule shown here:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在 SARSA 方法中，我们使用 epsilon-greedy 策略在状态 *s* 中选择动作 *a*，然后转移到下一个状态 ![](img/B15558_03_018.png)，并使用此处显示的更新规则来更新
    Q 值：
- en: '![](img/B15558_05_054.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_054.png)'
- en: In the preceding equation, in order to compute the Q value of next state-action
    pair, ![](img/B15558_05_088.png), we need to select an action. So, we select the
    action using the same epsilon-greedy policy and update the Q value of the next
    state-action pair.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，为了计算下一个状态-动作对的Q值，![](img/B15558_05_088.png)，我们需要选择一个动作。因此，我们使用相同的epsilon-贪心策略选择动作，并更新下一个状态-动作对的Q值。
- en: But unlike SARSA, in Q learning, we use two different policies. One is the epsilon-greedy
    policy and the other is a greedy policy. To select an action in the environment
    we use an epsilon-greedy policy, but while updating the Q value of the next state-action
    pair we use a greedy policy.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 但是与SARSA不同，在Q学习中，我们使用两种不同的策略。一个是epsilon-贪心策略，另一个是贪心策略。为了在环境中选择一个动作，我们使用epsilon-贪心策略，但在更新下一个状态-动作对的Q值时，我们使用贪心策略。
- en: 'That is, we select action *a* in state *s* using the epsilon-greedy policy
    and move to the next state ![](img/B15558_05_048.png) and update the Q value using
    the update rule shown below:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们在状态*s*中使用epsilon-贪心策略选择动作*a*，并移动到下一个状态 ![](img/B15558_05_048.png)，然后使用下面的更新规则更新Q值：
- en: '![](img/B15558_05_054.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_054.png)'
- en: 'In the preceding equation, in order to compute the Q value of the next state-action
    pair, ![](img/B15558_05_091.png), we need to select an action. Here, we select
    the action using the greedy policy and update the Q value of the next state-action
    pair. We know that the greedy policy always selects the action that has the maximum
    value. So, we can modify the equation to:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，为了计算下一个状态-动作对的Q值，![](img/B15558_05_091.png)，我们需要选择一个动作。在这里，我们使用贪心策略选择动作，并更新下一个状态-动作对的Q值。我们知道，贪心策略总是选择具有最大Q值的动作。因此，我们可以将公式修改为：
- en: '![](img/B15558_05_092.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_092.png)'
- en: As we can observe from the preceding equation, the **max** operator implies
    that in state ![](img/B15558_05_048.png), we select the action ![](img/B15558_05_094.png)
    that has the maximum Q value.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的公式中可以观察到，**max** 操作符意味着在状态 ![](img/B15558_05_048.png) 中，我们选择具有最大Q值的动作 ![](img/B15558_05_094.png)。
- en: 'Thus, to sum up, in the Q learning method we select an action in the environment
    using the epsilon-greedy policy, but while computing the Q value of the next state-action
    pair we use the greedy policy. Thus, update rule of Q learning is given as:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总结一下，在Q学习方法中，我们使用epsilon-贪心策略在环境中选择动作，但在计算下一个状态-动作对的Q值时，我们使用贪心策略。因此，Q学习的更新规则如下：
- en: '![](img/B15558_05_092.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_092.png)'
- en: 'Let''s understand this better by manually calculating the Q value using our
    Q learning update rule. Let''s use the same Frozen Lake example. We initialize
    our Q table with random values. *Figure 5.13* shows the Frozen Lake environment,
    along with the Q table containing random values:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过手动计算Q值来更好地理解这一点，使用我们的Q学习更新规则。我们使用相同的冻结湖示例。我们将Q表初始化为随机值。*图 5.13* 显示了冻结湖环境，以及包含随机值的Q表：
- en: '![](img/B15558_05_14.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_14.png)'
- en: 'Figure 5.13: The Frozen Lake environment with a randomly initialized Q table'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13：带有随机初始化Q表的冻结湖环境
- en: Suppose we are in state **(3,2)**. Now, we need to select some action in this
    state. How can we select an action? We select an action using the epsilon-greedy
    policy. So, with probability epsilon, we select a random action and with probability
    1-epsilon we select the best action that has the maximum Q value.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们处于状态**(3,2)**。现在，我们需要选择该状态中的某个动作。我们如何选择动作呢？我们使用epsilon-贪心策略选择一个动作。因此，以概率epsilon，我们选择一个随机动作，而以概率1-epsilon，我们选择具有最大Q值的最佳动作。
- en: 'Say we use probability 1-epsilon and select the best action. So, in state **(3,2)**,
    we select the *down* action as it has the highest Q value compared to other actions
    in that state, as *Figure 5.14* shows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用概率1-epsilon选择最佳动作。因此，在状态**(3,2)** 中，我们选择*向下*动作，因为它相对于该状态中的其他动作具有最高的Q值，正如*图
    5.14* 所示：
- en: '![](img/B15558_05_15.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_15.png)'
- en: 'Figure 5.14: We perform the action with the maximum Q value in state (3,2)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14：我们在状态（3,2）中执行具有最大Q值的动作
- en: 'Okay, so, we perform the *down* action in state **(3,2)** and move to the next
    state **(4,2)**, as *Figure 5.15* shows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以，我们在状态**(3,2)** 中执行*向下*动作，并如*图 5.15* 所示移动到下一个状态**(4,2)**。
- en: '![](img/B15558_05_16.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_16.png)'
- en: 'Figure 5.15: We move down to state (4,2)'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15：我们移动到状态（4,2）
- en: Thus, we move *down* in state **(3,2)** to the next state **(4,2)** and receive
    a reward *r* of 0\. Let's keep the learning rate ![](img/B15558_05_016.png) as
    0.1, and the discount factor ![](img/B15558_03_035.png) as 1\. Now, how can we
    update the Q value?
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在状态 **(3,2)** 中向 *down* 方向移动，进入下一个状态 **(4,2)** 并获得奖励 *r* 为 0。我们将学习率 ![](img/B15558_05_016.png)
    设为 0.1，折扣因子 ![](img/B15558_03_035.png) 设为 1。现在，我们如何更新 Q 值呢？
- en: 'Let''s recall our Q learning update rule:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们的 Q 学习更新规则：
- en: '![](img/B15558_05_098.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_098.png)'
- en: 'Substituting the state-action pair *Q*(*s*,*a*) with *Q*((3,2), down) and the
    next state ![](img/B15558_03_018.png) with **(4,2)** in the preceding equation,
    we can write:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态-动作对 *Q*(*s*,*a*) 替换为 *Q*((3,2), down)，并将下一个状态 ![](img/B15558_03_018.png)
    替换为 **(4,2)**，我们可以将前面的方程写成：
- en: '![](img/B15558_05_100.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_100.png)'
- en: 'Substituting the reward, *r* = 0, the learning rate ![](img/B15558_05_101.png),
    and the discount factor ![](img/B15558_05_102.png), we can write:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 将奖励 *r* = 0、学习率 ![](img/B15558_05_101.png) 和折扣因子 ![](img/B15558_05_102.png)
    代入方程中，我们可以写出：
- en: '![](img/B15558_05_103.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_103.png)'
- en: 'From the previous Q table, we can observe that the Q value of *Q*((3,2), down)
    is **0.8**. Thus, substituting *Q*((3,2), down) with **0.8**, we can rewrite the
    preceding equation as:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的 Q 表中，我们可以观察到 *Q*((3,2), down) 的值为 **0.8**。因此，将 *Q*((3,2), down) 替换为 **0.8**，我们可以将前面的方程改写为：
- en: '![](img/B15558_05_104.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_104.png)'
- en: As we can observe, in the preceding equation we have the term ![](img/B15558_05_105.png),
    which represents the Q value of the next state-action pair as we moved to the
    new state **(4,2)**. In order to compute the Q value for the next state, first
    we need to select an action. Here, we select an action using the greedy policy,
    that is, the action that has maximum Q value.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在前面的方程中，我们有一个项 ![](img/B15558_05_105.png)，它表示我们在移动到新状态 **(4,2)** 后，下一状态-动作对的
    Q 值。为了计算下一个状态的 Q 值，首先我们需要选择一个动作。在这里，我们使用贪心策略选择动作，即选择具有最大 Q 值的动作。
- en: 'As *Figure 5.16* shows, the *right* action has the maximum Q value in state
    **(4,2)**. So, we select the *right* action and update the Q value of the next
    state-action pair:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 5.16* 所示，*right* 动作在状态 **(4,2)** 中具有最大 Q 值。因此，我们选择 *right* 动作，并更新下一个状态-动作对的
    Q 值：
- en: '![](img/B15558_05_17.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_17.png)'
- en: 'Figure 5.16: We perform the action with the maximum Q value in state (4,2)'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16：我们在状态 (4,2) 中执行具有最大 Q 值的动作
- en: 'Thus, now our update rule becomes:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们的更新规则变为：
- en: '![](img/B15558_05_106.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_106.png)'
- en: 'From the previous Q table, we can observe that the Q value of *Q*((4,2), right)
    is **0.8**. Thus, substituting the value of *Q*((4,2), right) with **0.8**, we
    can rewrite the above equation as:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的 Q 表中，我们可以观察到 *Q*((4,2), right) 的值为 **0.8**。因此，将 *Q*((4,2), right) 的值替换为
    **0.8**，我们可以将上述方程改写为：
- en: '![](img/B15558_05_107.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_107.png)'
- en: 'Thus, our Q value becomes:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的 Q 值变为：
- en: '![](img/B15558_05_108.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_108.png)'
- en: Similarly, we update the Q value for all state-action pairs. That is, we select
    an action in the environment using an epsilon-greedy policy, and while updating
    the Q value of the next state-action pair we use the greedy policy. Thus, we update
    the Q value for every state-action pair.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们为所有状态-动作对更新 Q 值。也就是说，我们在环境中选择动作时使用 epsilon-greedy 策略，而在更新下一个状态-动作对的 Q 值时使用贪心策略。这样，我们为每个状态-动作对更新
    Q 值。
- en: Thus, in this way, we update the Q function by updating the Q value of the state-action
    pair in each step of the episode. We will extract a new policy from the updated
    Q function on every step of the episode and uses this new policy. (Remember that
    we select an action in the environment using epsilon-greedy policy but while updating
    Q value of the next state-action pair we use the greedy policy). After several
    episodes, we will have the optimal Q function. The Q learning algorithm given
    in the following will help us to understand this better.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过这种方式，我们在每一步的训练过程中更新 Q 函数，更新状态-动作对的 Q 值。我们将在每一步从更新后的 Q 函数中提取新的策略，并使用这个新策略。（记住，我们在环境中选择动作时使用
    epsilon-greedy 策略，但在更新下一个状态-动作对的 Q 值时使用贪心策略。）经过若干回合后，我们将得到最优的 Q 函数。以下给出的 Q 学习算法将帮助我们更好地理解这一点。
- en: 'The Q learning algorithm is given as follows:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: Q 学习算法如下所示：
- en: Initialize a Q function *Q*(*s*, *a*) with random values
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个随机值的 Q 函数 *Q*(*s*, *a*)
- en: 'For each episode:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一回合：
- en: Initialize state *s*
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化状态 *s*
- en: 'For each step in the episode:'
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于回合中的每一步：
- en: Extract a policy from *Q*(*s*, *a*) and select an action *a* to perform in state
    *s*
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *Q*(*s*, *a*) 中提取策略，并选择一个动作 *a* 在状态 *s* 中执行
- en: Perform the action *a*, move to the next state ![](img/B15558_05_109.png), and
    observe the reward *r*
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作 *a*，转移到下一个状态 ![](img/B15558_05_109.png)，并观察奖励 *r*
- en: Update the Q value as ![](img/B15558_05_110.png)
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新Q值为 ![](img/B15558_05_110.png)
- en: Update ![](img/B15558_05_111.png) (update the next state ![](img/B15558_03_018.png)
    to the current state s)
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 ![](img/B15558_05_111.png)（将下一个状态 ![](img/B15558_03_018.png)更新为当前状态 *s*）
- en: If *s* is not a terminal state, repeat *steps 1* to *5*
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *s* 不是终止状态，重复 *步骤1* 到 *步骤5*
- en: Now that we have learned how the Q learning algorithm works, in the next section,
    let's implement Q learning to find the optimal policy.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了Q学习算法是如何工作的，在下一节中，让我们实现Q学习以找到最优策略。
- en: Computing the optimal policy using Q learning
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Q学习计算最优策略
- en: Now, let's implement Q learning to find the optimal policy in the Frozen Lake
    environment.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在Frozen Lake环境中实现Q学习，找到最优策略。
- en: 'First, let''s import the necessary libraries:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库：
- en: '[PRE31]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, we create the Frozen Lake environment using Gym:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用Gym创建Frozen Lake环境：
- en: '[PRE32]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let''s define the dictionary for storing the Q values of the state-action pairs,
    and initialize the Q values of all the state-action pairs to `0.0`:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先定义一个字典来存储状态-动作对的Q值，并将所有状态-动作对的Q值初始化为`0.0`：
- en: '[PRE33]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, let''s define the epsilon-greedy policy. We generate a random number from
    the uniform distribution, and if the random number is less than epsilon we select
    the random action, else we select the best action that has the maximum Q value:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义ε-贪婪策略。我们从均匀分布中生成一个随机数，如果该随机数小于ε，则选择一个随机动作，否则选择具有最大Q值的最佳动作：
- en: '[PRE34]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Initialize the discount factor ![](img/B15558_05_056.png), the learning rate
    ![](img/B15558_05_055.png), and the epsilon value:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化折扣因子 ![](img/B15558_05_056.png)、学习率 ![](img/B15558_05_055.png) 和ε值：
- en: '[PRE35]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Set the number of episodes and the number of time steps in the episode:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 设置回合数和每个回合中的时间步数：
- en: '[PRE36]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Compute the policy.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 计算策略。
- en: 'For each episode:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个回合：
- en: '[PRE37]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Initialize the state by resetting the environment:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重置环境来初始化状态：
- en: '[PRE38]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'For each step in the episode:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回合中的每一步：
- en: '[PRE39]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Select the action using the epsilon-greedy policy:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ε-贪婪策略选择动作：
- en: '[PRE40]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Perform the selected action and store the next state information:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 执行选择的动作并存储下一个状态的信息：
- en: '[PRE41]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Now, let's compute the Q value of the state-action pair as ![](img/B15558_05_092.png).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算状态-动作对的Q值为 ![](img/B15558_05_092.png)。
- en: 'First, select the action ![](img/B15558_05_116.png) that has the maximum Q
    value in the next state ![](img/B15558_03_046.png):'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，选择下一个状态 ![](img/B15558_03_046.png)中具有最大Q值的动作 ![](img/B15558_05_116.png)：
- en: '[PRE42]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, we can compute the Q value of the state-action pair as:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以计算状态-动作对的Q值为：
- en: '[PRE43]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Update ![](img/B15558_05_118.png) (update the next state ![](img/B15558_03_073.png)
    to the current state *s*):'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 更新 ![](img/B15558_05_118.png)（将下一个状态 ![](img/B15558_03_073.png)更新为当前状态 *s*）：
- en: '[PRE44]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'If the current state is the terminal state, then break:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 如果当前状态是终止状态，则中断：
- en: '[PRE45]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: After all the iterations, we will have the optimal Q function. Then we can extract
    the optimal policy by selecting the action that has the maximum Q value in each
    state.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 所有迭代完成后，我们将得到最优Q函数。然后，我们可以通过选择每个状态中具有最大Q值的动作来提取最优策略。
- en: The difference between Q learning and SARSA
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q学习和SARSA的区别
- en: Understanding the difference between Q learning and SARSA is very important.
    So, let's do a little recap on how Q learning and SARSA differ.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 理解Q学习和SARSA之间的区别非常重要。所以，让我们回顾一下Q学习和SARSA的不同之处。
- en: 'SARSA is an on-policy algorithm, meaning that we use a single epsilon-greedy
    policy for selecting an action in the environment and also to compute the Q value
    of the next state-action pair. The update rule of SARSA is given as follows:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: SARSA是一个基于策略的算法，这意味着我们使用一个单一的ε-贪婪策略来选择环境中的动作，并计算下一个状态-动作对的Q值。SARSA的更新规则如下所示：
- en: '![](img/B15558_05_054.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_054.png)'
- en: 'Q learning is an off-policy algorithm, meaning that we use an epsilon-greedy
    policy for selecting an action in the environment, but to compute the Q value
    of next state-action pair we use a greedy policy. The update rule of Q learning
    is given as follows:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习是一个基于策略外的算法，这意味着我们在环境中使用ε-贪婪策略来选择动作，但在计算下一个状态-动作对的Q值时，我们使用贪婪策略。Q学习的更新规则如下所示：
- en: '![](img/B15558_05_092.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_05_092.png)'
- en: Comparing the DP, MC, and TD methods
  id: totrans-363
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较DP、MC和TD方法
- en: So far, we have learned several interesting and important reinforcement learning
    algorithms, such as DP (value iteration and policy iteration), MC methods, and
    TD learning methods, to find the optimal policy. These are called the key algorithms
    in classic reinforcement learning, and understanding the differences between these
    three algorithms is very important. So, in this section, we will recap the differences
    between the DP, MC, and TD learning methods.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了几种有趣且重要的强化学习算法，例如 DP（值迭代和策略迭代）、MC 方法和 TD 学习方法，来寻找最优策略。这些被称为经典强化学习中的关键算法，理解这三种算法之间的区别非常重要。因此，在本节中，我们将回顾
    DP、MC 和 TD 学习方法之间的区别。
- en: '**Dynamic programming** (**DP**), that is, the value and policy iteration methods,
    is a model-based method, meaning that we compute the optimal policy using the
    model dynamics of the environment. We cannot apply the DP method when we don''t
    have the model dynamics of the environment.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态规划**（**DP**），即值迭代和策略迭代方法，是一种基于模型的方法，意味着我们使用环境的模型动态来计算最优策略。当我们没有环境的模型动态时，无法应用
    DP 方法。'
- en: We also learned about the **Monte Carlo** (**MC**) method. MC is a model-free
    method, meaning that we compute the optimal policy without using the model dynamics
    of the environment. But one problem we face with the MC method is that it is applicable
    only to episodic tasks and not to continuous tasks.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学习了**蒙特卡罗**（**MC**）方法。MC 是一种无模型方法，意味着我们在不使用环境模型动态的情况下计算最优策略。但我们在使用 MC 方法时遇到的一个问题是，它仅适用于回合任务，而不适用于连续任务。
- en: We learned about another interesting model-free method called **temporal difference**
    (**TD**) learning. TD learning takes advantage of both DP by bootstrapping and
    the MC method by being model free.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了一种有趣的无模型方法，称为**时间差分**（**TD**）学习。TD 学习结合了通过自举进行的动态规划（DP）和通过无模型方法进行的蒙特卡罗（MC）方法的优势。
- en: Many congratulations on learning about all the important reinforcement learning
    algorithms. In the next chapter, we will look into a case study called the multi-armed
    bandit problem.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你学习了所有重要的强化学习算法。在下一章中，我们将探讨一个案例研究，称为多臂赌博机问题。
- en: Summary
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started off the chapter by understanding what TD learning is and how it takes
    advantage of both DP and the MC method. We learned that, just like DP, TD learning
    bootstraps, and just like the MC method, TD learning is a model-free method.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过理解 TD 学习是什么以及它如何结合 DP 和 MC 方法的优势来开始本章。我们了解到，像 DP 一样，TD 学习也进行自举；像 MC 方法一样，TD
    学习是一种无模型方法。
- en: Later, we learned how to perform a prediction task using TD learning, and then
    we looked into the algorithm of the TD prediction method.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，我们学习了如何使用 TD 学习执行预测任务，然后我们研究了 TD 预测方法的算法。
- en: Going forward, we learned how to use TD learning for a control task. First,
    we learned about the on-policy TD control method called SARSA, and then we learned
    about the off-policy TD control method called Q learning. We also learned how
    to find the optimal policy in the Frozen Lake environment using the SARSA and
    Q learning methods.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们学习了如何使用 TD 学习进行控制任务。首先，我们学习了名为 SARSA 的在政策 TD 控制方法，然后我们学习了名为 Q 学习的离政策 TD
    控制方法。我们还学习了如何使用 SARSA 和 Q 学习方法在 Frozen Lake 环境中找到最优策略。
- en: We also learned the difference between SARSA and Q learning methods. We understood
    that SARSA is an on-policy algorithm, meaning that we use a single epsilon-greedy
    policy to select an action in the environment and also to compute the Q value
    of the next state-action pair, whereas Q learning is an off-policy algorithm,
    meaning that we use an epsilon-greedy policy to select an action in the environment
    but to compute the Q value of the next state-action pair we use a greedy policy.
    At the end of the chapter, we compared the DP, MC, and TD methods.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学习了 SARSA 和 Q 学习方法的区别。我们理解到，SARSA 是一种在政策算法，这意味着我们使用一个单一的 epsilon-greedy 策略在环境中选择一个动作，并计算下一个状态-动作对的
    Q 值；而 Q 学习是一种离政策算法，这意味着我们使用 epsilon-greedy 策略在环境中选择一个动作，但在计算下一个状态-动作对的 Q 值时使用贪心策略。章节末尾，我们对
    DP、MC 和 TD 方法进行了比较。
- en: In the next chapter, we will look into an interesting problem called the multi-armed
    bandit problem.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨一个有趣的问题，称为多臂赌博机问题。
- en: Questions
  id: totrans-375
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s evaluate our newly acquired knowledge by answering the following questions:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回答以下问题来评估我们新获得的知识：
- en: How does TD learning differ from the MC method?
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TD 学习与 MC 方法有何不同？
- en: What is the advantage of using the TD learning method?
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TD 学习方法有什么优势？
- en: What is TD error?
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是TD误差？
- en: What is the update rule of TD learning?
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TD学习的更新规则是什么？
- en: How does the TD prediction method work?
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TD预测方法是如何工作的？
- en: What is SARSA?
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是SARSA？
- en: How does Q learning differ from SARSA?
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q学习与SARSA有何不同？
- en: Further reading
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For further information, refer to the following link:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参考以下链接：
- en: '**Learning to Predict by the Methods of Temporal Differences** by *Richard
    S. Sutton*, available at [https://link.springer.com/content/pdf/10.1007/BF00115009.pdf](https://link.springer.com/content/pdf/10.1007/BF00115009.pdf)'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过时间差方法学习预测**，作者为*理查德·S·萨顿*，可在[https://link.springer.com/content/pdf/10.1007/BF00115009.pdf](https://link.springer.com/content/pdf/10.1007/BF00115009.pdf)查看。'
