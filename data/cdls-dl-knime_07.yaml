- en: '*Chapter 5:* Autoencoder for Fraud Detection'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第5章：* 用于欺诈检测的自编码器'
- en: 'At this point in the book, you should already know the basic math and concepts
    behind neural networks and some deep learning paradigms, as well as the most useful
    KNIME nodes for data preparation, how to build a neural network, how to train
    it and test it, and finally, how to evaluate it. We have built together, in [*Chapter
    4*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101), *Building and Training a
    Feedforward Neural Network*, two examples of fully connected feedforward neural
    networks: one to solve a multiclass classification problem on the Iris dataset
    and one to solve a binary classification problem on the Adult dataset.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到了本书的这个阶段，你应该已经掌握了神经网络背后的基本数学和概念，以及一些深度学习范式，还了解了数据准备中最有用的KNIME节点，如何构建神经网络、如何训练和测试它，最后如何评估它。我们在[*第4章*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101)《构建和训练前馈神经网络》中共同构建了两个全连接前馈神经网络的例子：一个用于解决Iris数据集的多分类问题，另一个用于解决Adult数据集的二分类问题。
- en: 'Those were two simple examples using quite small datasets, in which all the
    classes were adequately represented, with just a few hidden layers in the network
    and a straightforward encoding of the output classes. However, they served their
    purpose: to teach you how to assemble, train, and apply a neural network in KNIME
    Analytics Platform.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 那些是使用相对较小数据集的两个简单例子，所有类别都有充分的代表性，网络中仅有几层隐藏层，并且输出类别的编码也很直接。然而，它们达到了它们的目的：教你如何在KNIME分析平台中组装、训练和应用神经网络。
- en: Now, the time has come to explore more realistic examples and apply more complex
    neural architectures and more advanced deep learning paradigms in order to solve
    more complicated problems based sometimes on ill-conditioned datasets. In the
    following chapters, you will look at some of these more realistic case studies,
    requiring some more creative solutions than just a fully connected feedforward
    network for classification.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，时机已经成熟，开始探索更现实的例子，并应用更复杂的神经网络架构和更先进的深度学习范式，以解决有时基于条件较差数据集的更复杂问题。在接下来的章节中，你将看到一些更具现实性的案例研究，这些案例需要比单纯使用全连接前馈网络进行分类更具创意的解决方案。
- en: We will start with a binary classification problem with a dataset that has data
    from only one of the two classes. Here, the classic classification approach cannot
    work, since one of the two classes is missing from the training set. There are
    many problems of this kind, such as anomaly detection to predict mechanical failures
    or fraud detection to distinguish legitimate from fraudulent credit card transactions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个二分类问题开始，数据集只包含两个类别中的一个。在这里，经典的分类方法无法工作，因为训练集缺少其中一个类别。这类问题有很多，例如预测机械故障的异常检测或欺诈检测，用以区分合法和欺诈的信用卡交易。
- en: 'This chapter investigates an alternative neural approach to design a solution
    for this extreme situation in fraud detection: the **autoencoder** architecture.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了一种替代的神经网络方法，用于设计解决欺诈检测中极端情况的方案：**自编码器**架构。
- en: 'We will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Introducing Autoencoders
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍自编码器
- en: Why is Fraud Detection so hard?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么欺诈检测这么难？
- en: Building and Training the Autoencoder
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和训练自编码器
- en: Optimizing the Autoencoder Strategy
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化自编码器策略
- en: Deploying the Fraud Detector
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署欺诈检测器
- en: Introducing Autoencoders
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍自编码器
- en: 'In previous chapters, we have seen that neural networks are very powerful algorithms.
    The power of each network lies in its architecture, activation functions, and
    regularization terms, plus a few other features. Among the varieties of neural
    architectures, there is a very versatile one, especially useful for three tasks:
    detecting unknown events, detecting unexpected events, and reducing the dimensionality
    of the input space. This neural network is the **autoencoder**.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们看到神经网络是非常强大的算法。每个网络的力量来源于其架构、激活函数、正则化项，以及一些其他特性。在各种神经网络架构中，有一种非常通用的架构，特别适用于三项任务：检测未知事件、检测意外事件，以及降低输入空间的维度。这种神经网络就是**自编码器**。
- en: Architecture of the Autoencoder
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自编码器的架构
- en: The autoencoder (or **autoassociator**) is a multilayer feedforward neural network,
    trained to reproduce the input vector onto the output layer. Like many neural
    networks, it is trained using the gradient descent algorithm, or one of its modern
    variations, against [a loss funct](https://keras.io/losses/)ion, such as the **Mean
    Squared Error** (**MSE**). It can have as many hidden layers as desired. Regularization
    terms and other general parameters that are useful for avoiding overfitting or
    for improving the learning process can be applied here as well.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器（或**自关联器**）是一个多层前馈神经网络，训练的目标是将输入向量映射到输出层。像许多神经网络一样，它使用梯度下降算法或其现代变体进行训练，针对[损失函数](https://keras.io/losses/)，例如**均方误差**（**MSE**）。它可以有任意数量的隐藏层。也可以在这里应用正则化项和其他有助于避免过拟合或改善学习过程的通用参数。
- en: The only constraint on the architecture is that the number of input units must
    be the same as the number of output units, as the goal is to train the autoencoder
    to reproduce the input vector onto the output layer.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 架构的唯一约束是输入单元的数量必须与输出单元的数量相同，因为目标是训练自编码器将输入向量映射到输出层。
- en: 'The simplest autoencoder has only three layers: one input layer, one hidden
    layer, and one output layer. More complex structured autoencoders might include
    additional hidden layers:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的自编码器只有三层：一个输入层，一个隐藏层和一个输出层。更复杂结构的自编码器可能包含额外的隐藏层：
- en: '![Figure 5.1 – A simple autoencoder](img/B16391_05_001.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 一个简单的自编码器](img/B16391_05_001.jpg)'
- en: Figure 5.1 – A simple autoencoder
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 一个简单的自编码器
- en: Autoencoders can be used for many different tasks. Let's first see how an autoencoder
    can be used for dimensionality reduction.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器可以用于许多不同的任务。让我们首先看看自编码器如何用于降维。
- en: Reducing the Input Dimensionality with an Autoencoder
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自编码器减少输入的维度
- en: 'Let''s consider an autoencoder with a very simple architecture: one input layer
    with ![](img/Formula_B16391_03_252.png) units; one output layer, also with ![](img/Formula_B16391_05_002.png) units;
    and one hidden layer with ![](img/Formula_B16391_03_042.png) units. If ![](img/Formula_B16391_05_004.png),
    the autoencoder produces a compression of the input vector onto the hidden layer,
    reducing its dimensionality from ![](img/Formula_B16391_05_005.png) to ![](img/Formula_B16391_03_044.png).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个非常简单架构的自编码器：一个输入层，包含![](img/Formula_B16391_03_252.png)个单元；一个输出层，也包含![](img/Formula_B16391_05_002.png)个单元；以及一个隐藏层，包含![](img/Formula_B16391_03_042.png)个单元。如果![](img/Formula_B16391_05_004.png)，自编码器将输入向量压缩到隐藏层，降低其维度，从![](img/Formula_B16391_05_005.png)降到![](img/Formula_B16391_03_044.png)。
- en: 'In this case, the first part of the network, moving the data from a vector
    with size ![](img/Formula_B16391_05_007.png) to a vector with size ![](img/Formula_B16391_05_008.png),
    plays the role of the encoder. The second part of the network, reconstructing
    the input vector from a ![](img/Formula_B16391_05_009.png) space back into a ![](img/Formula_B16391_05_010.png)
    space, is the decoder. The compression rate is then ![](img/Formula_B16391_05_011.png).
    The larger the value of ![](img/Formula_B16391_03_029.png) and the smaller the
    value of ![](img/Formula_B16391_05_013.png), the higher the compression rate:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，网络的第一部分，将数据从大小为![](img/Formula_B16391_05_007.png)的向量转换为大小为![](img/Formula_B16391_05_008.png)的向量，充当编码器的角色。网络的第二部分，将输入向量从![](img/Formula_B16391_05_009.png)空间重建回![](img/Formula_B16391_05_010.png)空间，充当解码器。压缩率则为![](img/Formula_B16391_05_011.png)。值为![](img/Formula_B16391_03_029.png)越大，值为![](img/Formula_B16391_05_013.png)越小，压缩率越高：
- en: '![Figure 5.2 – Encoder and decoder subnetworks in a three-layer autoencoder](img/B16391_05_002.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 三层自编码器中的编码器和解码器子网络](img/B16391_05_002.jpg)'
- en: Figure 5.2 – Encoder and decoder subnetworks in a three-layer autoencoder
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 三层自编码器中的编码器和解码器子网络
- en: 'When using the autoencoder for **dimensionality reduction**, the full network
    is first trained to reproduce the input vector onto the output layer. Then, before
    deployment, it is split into two parts: the **encoder** (input layer and hidden
    layer) and the **decoder** (hidden layer and output layer). The two subnetworks
    are stored separately.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用自编码器进行**降维**时，首先训练完整的网络，将输入向量映射到输出层。然后，在部署之前，它被拆分为两部分：**编码器**（输入层和隐藏层）和**解码器**（隐藏层和输出层）。这两个子网络被分别存储。
- en: Tip
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: If you are interested in the output of the bottleneck layer, you can configure
    the **Keras Network Executor** node to output the middle layer. Alternatively,
    you can split the network within the **DL Python Network Editor** node by writing
    a few lines of Python code.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对瓶颈层的输出感兴趣，可以将**Keras网络执行器**节点配置为输出中间层。或者，你也可以在**DL Python网络编辑器**节点中，通过编写几行Python代码来拆分网络。
- en: During the deployment phase, in order to compress an input record, we just pass
    it through the encoder and save the output of the hidden layer as the compressed
    record. Then, in order to reconstruct the original vector, we pass the compressed
    record through the decoder and save the output values of the output layer as the
    reconstructed vector.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署阶段，为了压缩输入记录，我们只需将其传递通过编码器，并将隐藏层的输出保存为压缩记录。然后，为了重建原始向量，我们将压缩记录通过解码器并保存输出层的输出值作为重建向量。
- en: If a more complex structure is used for the autoencoder – for example, with
    more than one hidden layer – one of the hidden layers must work as the compressor
    output, producing the compressed record and separating the encoder from the decoder
    subnetwork.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果自编码器使用了更复杂的结构——例如，拥有多个隐藏层——那么其中一个隐藏层必须作为压缩器的输出，产生压缩记录并将编码器和解码器子网络分开。
- en: Now, the question when we talk about data compression is how faithfully can
    the original record be reconstructed? How much information is lost by using the
    output of the hidden layer instead of the original data vector? Of course, this
    all depends on how well the autoencoder performs and how large our error tolerance
    is.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们谈论数据压缩时，问题是原始记录能否被忠实地重建？使用隐藏层的输出而非原始数据向量，会丢失多少信息？当然，这一切都取决于自编码器的表现如何以及我们的误差容忍度有多大。
- en: During testing, when we apply the network to new data, we denormalize the output
    values and we calculate the chosen error metric – for example, the **Root Mean
    Square Error** (**RMSE**) – between the original input data and the reconstructed
    data on the whole test set. This error value gives us a measure of the quality
    of the reconstructed data. Of course, the higher the compression rate, the higher
    the reconstruction error. The problem thus becomes to train the network to achieve
    acceptable performance, as per our error tolerance.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试阶段，当我们将网络应用于新数据时，我们会对输出值进行去归一化，并计算所选的误差度量——例如，**均方根误差**（**RMSE**）——在整个测试集上计算原始输入数据与重建数据之间的差异。这个误差值为我们提供了重建数据质量的衡量标准。当然，压缩率越高，重建误差就越大。因此，问题变成了训练网络以达到根据我们的误差容忍度所能接受的性能。
- en: 'Let''s move on to the next application field of autoencoders: anomaly detection.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续讨论自编码器的下一个应用领域：异常检测。
- en: Detecting Anomalies Using an Autoencoder
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自编码器检测异常
- en: In most classification/prediction problems, we have a set of examples covering
    all event classes and based on this dataset, we train a model to classify events.
    However, sometimes, the event class we want to predict is so rare and unexpected
    that no (or almost no) examples are available at all. In this case, we do not
    talk about classification or prediction but about **anomaly detection**.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数分类/预测问题中，我们拥有一组覆盖所有事件类别的示例，并基于这个数据集训练一个模型来分类事件。然而，有时我们想要预测的事件类别是非常稀有和意外的，以至于根本没有（或几乎没有）任何示例可用。在这种情况下，我们不再谈论分类或预测，而是谈论**异常检测**。
- en: 'An anomaly can be any rare, unexpected, unknown event: a cardiac arrhythmia,
    a mechanical breakdown, a fraudulent transaction, or other rare, unexpected, unknown
    events. In this case, since no examples of anomalies are available in the training
    set, we need to use neural networks in a more creative way than for conventional,
    standard classification. The autoencoder structure lends itself to such creative
    usage, as required for the solution of an anomaly detection problem (see, for
    example, A.G. Gebresilassie, *Neural Networks for Anomaly (Outliers) Detection*,
    [https://blog.goodaudience.com/neural-networks-for-anomaly-outliers-detection-a454e3fdaae8](https://blog.goodaudience.com/neural-networks-for-anomaly-outliers-detection-a454e3fdaae8)).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 异常可以是任何稀有、意外、未知的事件：心律不齐、机械故障、欺诈交易或其他稀有、意外、未知的事件。在这种情况下，由于训练集中没有异常的示例，我们需要以比传统标准分类更具创造性的方式使用神经网络。自编码器结构非常适合这种创造性使用，正如解决异常检测问题所需要的那样（例如，参见A.G.
    Gebresilassie，*神经网络用于异常（离群点）检测*，[https://blog.goodaudience.com/neural-networks-for-anomaly-outliers-detection-a454e3fdaae8](https://blog.goodaudience.com/neural-networks-for-anomaly-outliers-detection-a454e3fdaae8)）。
- en: Since no anomaly examples are available, the autoencoder is trained only on
    non-anomaly examples. Let's call these examples of the "normal" class. On a training
    set full of "normal" data, the autoencoder network is trained to reproduce the
    input feature vector onto the output layer.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有异常示例可用，自编码器仅对非异常示例进行训练。我们将这些示例称为“正常”类。在充满“正常”数据的训练集上，自编码器网络被训练以将输入特征向量重构到输出层。
- en: The idea is that, when required to reproduce a vector of the "normal" class,
    the autoencoder is likely to perform a decent job because that is what it was
    trained to do. However, when required to reproduce an anomaly on the output layer,
    it will hopefully fail because it won't have seen this kind of vector throughout
    the whole training phase. Therefore, if we calculate the distance – any distance
    – between the original vector and the reproduced vector, we see a small distance
    for input vectors of the "normal" class and a much larger distance for input vectors
    representing an anomaly.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个思想是，当自编码器被要求重构“正常”类的向量时，它可能会表现得相当不错，因为这是它训练的目标。然而，当它被要求重构输出层中的异常时，它应该会失败，因为它在整个训练阶段没有见过这种类型的向量。因此，如果我们计算原始向量与重构向量之间的距离——任何一种距离——我们会发现对于“正常”类的输入向量，距离较小，而对于表示异常的输入向量，距离则大得多。
- en: 'Thus, by setting a threshold, ![](img/Formula_B16391_05_052.png), we should
    be able to detect anomalies with the following rule:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过设置一个阈值，![](img/Formula_B16391_05_052.png)，我们应该能够通过以下规则检测异常：
- en: IF ![](img/Formula_B16391_05_015.png) THEN ![](img/Formula_B16391_05_016.png)
    -> "normal"
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: IF ![](img/Formula_B16391_05_015.png) THEN ![](img/Formula_B16391_05_016.png)
    -> "正常"
- en: IF ![](img/Formula_B16391_05_017.png) THEN ![](img/Formula_B16391_05_018.png)
    -> "anomaly"
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: IF ![](img/Formula_B16391_05_017.png) THEN ![](img/Formula_B16391_05_018.png)
    -> "异常"
- en: Here, ![](img/Formula_B16391_05_019.png) is the reconstruction error for the
    input vector, ![](img/Formula_B16391_05_018.png), and ![](img/Formula_B16391_05_021.png)
    is the set threshold.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_B16391_05_019.png)是输入向量的重建误差，![](img/Formula_B16391_05_018.png)，以及![](img/Formula_B16391_05_021.png)是设定的阈值。
- en: This sort of solution has already been implemented successfully for fraud detection,
    as described in a blog post, *Credit Card Fraud Detection using Autoencoders in
    Keras -- TensorFlow for Hackers (Part VI*[*I)*, by Venelin Valkov (https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hacker](https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd)s-part-vii-20e0c85301bd).
    In this chapter, we will use the same idea to build a similar solution using a
    different autoencoder structure.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种解决方案已经成功地应用于欺诈检测，正如在一篇博客文章中所描述的，*使用Keras中的自编码器进行信用卡欺诈检测--黑客的TensorFlow（第六部分）*[*I)*，作者是Venelin
    Valkov（[https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hacker](https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd)s-part-vii-20e0c85301bd)）。在这一章中，我们将使用相同的思想，通过不同的自编码器结构构建类似的解决方案。
- en: Let's find out how the idea of an autoencoder can be used to detect fraudulent
    transactions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来了解自编码器的思想如何用于检测欺诈交易。
- en: Why is Detecting Fraud so Hard?
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么欺诈检测如此困难？
- en: '**Fraud detection** is a set of activities undertaken to prevent money or property
    from being obtained through false pretenses. Fraud detection is applied in many
    industries, such as banking or insurance. In banking, fraud may include forging
    checks or using stolen credit cards. For this example, we will focus on fraud
    in credit card transactions.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**欺诈检测**是一组旨在通过虚假手段防止财物被非法获取的活动。欺诈检测被应用于许多行业，如银行业或保险业。在银行业中，欺诈可能包括伪造支票或使用盗窃的信用卡。对于本示例，我们将重点讨论信用卡交易中的欺诈行为。'
- en: This kind of fraud, in credit card transactions, is a huge problem for credit
    card issuers as well as for the final payers. The European Central Bank reported
    that in 2016, the total number of card fraud cases using cards issued in the **Single
    Euro Payments Area** (**SEPA**) amounted to 17.3 million, and the total number
    of card transactions using cards issued in SEPA amounted to 74.9 billion ([https://www.ecb.europa.eu/pub/cardfraud/html/ecb.cardfraudreport201809.en.html#toc1](https://www.ecb.europa.eu/pub/cardfraud/html/ecb.cardfraudreport201809.en.html#toc1)).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这种信用卡交易中的欺诈问题，对于信用卡发行商和最终支付方来说都是一个巨大的问题。欧洲中央银行报告称，2016年，使用**单一欧元支付区**（**SEPA**）发行的卡的卡欺诈案件总数为1730万件，而使用SEPA发行的卡的卡交易总数为749亿笔（[https://www.ecb.europa.eu/pub/cardfraud/html/ecb.cardfraudreport201809.en.html#toc1](https://www.ecb.europa.eu/pub/cardfraud/html/ecb.cardfraudreport201809.en.html#toc1)）。
- en: However, the amount of fraud is not the only problem. From a data science perspective,
    fraud detection is also a very hard task to solve, because of the small amount
    of data available on fraudulent transactions. That is, often we have tons of data
    on legitimate credit card transactions and just a handful on fraudulent transactions.
    A classic approach (training, then applying a model) is not possible in this case
    since the examples for one of the two classes are missing.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，欺诈的数量并不是唯一的问题。从数据科学的角度来看，欺诈检测也是一个非常难解决的任务，因为关于欺诈交易的数据非常少。也就是说，我们通常有大量的合法信用卡交易数据，而关于欺诈交易的数据则寥寥无几。在这种情况下，经典的方法（训练，然后应用模型）是不可行的，因为其中一个类别的示例缺失。
- en: Fraud detection, however, can also be seen as anomaly detection. Anomaly detection
    is any event that is unexpected within a dataset. A fraudulent transaction is
    indeed an unexpected event and therefore we can consider it an anomaly in a dataset
    of legitimate *normal* credit card transactions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，欺诈检测也可以看作是异常检测。异常检测是数据集中任何意外事件。欺诈交易确实是一个意外事件，因此我们可以将其视为合法*正常*信用卡交易数据集中的异常。
- en: There are a few different approaches to fraud detection.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈检测有几种不同的方法。
- en: One option is the discriminative approach. Based on a training set with both
    classes, legitimate and fraudulent transactions, we build a model that distinguishes
    between data from the two classes. This could be a simple threshold-based rule
    or a supervised machine learning model. This is the classic approach based on
    a training set including enough examples from both classes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一种选择是判别方法。基于包含合法和欺诈交易的训练集，我们构建一个区分两类数据的模型。这可以是一个简单的基于阈值的规则，或者一个有监督的机器学习模型。这是基于包含两类数据的足够示例的经典方法。
- en: Alternatively, you can treat a fraud detection problem as outlier detection.
    In this case, you can use a clustering algorithm that leaves space for outliers
    (noise), such as **DBSCAN**; or you can use the **isolation forest technique**,
    which isolates outliers with just a few cuts with respect to legitimate data.
    Fraudulent transactions, though, must belong to the original dataset, to be isolated
    as outliers.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以将欺诈检测问题视为异常值检测。在这种情况下，你可以使用一种为异常值（噪声）留出空间的聚类算法，比如**DBSCAN**；或者使用**孤立森林技术**，它通过对合法数据进行少量切割来孤立异常值。然而，欺诈交易必须属于原始数据集，才能被孤立为异常值。
- en: A[nother app](https://en.wikipedia.org/wiki/Generative_model)roach, called the
    **generative approach**, involves using only legitimate transactions during the
    training phase. This allows us to reproduce the input vector onto the output layer.
    Once the model for the autoencoder has been trained, we use it during deployment
    to reproduce the input transaction. We then calculate the distance (or error)
    between the input values and the output values. If that distance falls below a
    given threshold, the transaction is likely to be legitimate; otherwise, it is
    flagged as a fraud candidate.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法，称为**生成方法**，在训练阶段只使用合法的交易。这使得我们能够将输入向量复制到输出层。一旦自编码器模型训练完成，我们将在部署过程中使用它来重现输入交易。然后我们计算输入值和输出值之间的距离（或误差）。如果该距离低于给定的阈值，则交易可能是合法的；否则，它将被标记为潜在欺诈交易。
- en: 'In this example, we will us[e the](https://www.kaggle.com/mlg-ulb/creditcardfraud)
    credit card dataset by Kaggle. This dataset contains credit card transactions
    from European cardholders in September 2013\. Fraudulent transactions have been
    labeled with `1`, while legitimate transactions are labeled with `0`. The dataset
    contains 284,807 transactions, but only 492 (0.2%) of them are fraudulent. Due
    to privacy reasons, principal components are used instead of the original transaction
    features. Thus, each credit card transaction is re[presented by 30 featu](https://en.wikipedia.org/wiki/Principal_component_analysis)res:
    28 principal components extracted from the original credit card data, the transaction
    time, and the transaction amount.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用[Kaggle上的](https://www.kaggle.com/mlg-ulb/creditcardfraud)信用卡数据集。该数据集包含2013年9月来自欧洲持卡人的信用卡交易记录。欺诈交易标记为`1`，合法交易标记为`0`。数据集包含284,807笔交易，但其中仅有492笔（0.2%）是欺诈交易。出于隐私原因，使用主成分代替原始的交易特征。因此，每笔信用卡交易由30个特征表示：28个从原始信用卡数据中提取的主成分、交易时间和交易金额。
- en: Let's proceed with the building, training, and testing of the autoencoder.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建、训练和测试自编码器。
- en: Building and Training the Autoencoder
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建和训练自编码器
- en: 'Let''s go into detail about the particular application we will build to tackle
    fraud detection with a neural autoencoder. Like all data science projects, it
    includes two separate applications: one to train and optimize the whole strategy
    on dedicated datasets, and one to set it in action to analyze real-world credit
    card transactions. The first application is implemented with the **training workflow**;
    the second application is implemented with the **deployment workflow**.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论一下我们将构建的特定应用，用以解决通过神经自编码器进行欺诈检测的问题。像所有的数据科学项目一样，它包括两个独立的应用：一个用于在专用数据集上训练和优化整个策略，另一个用于将其应用到实际的信用卡交易中进行分析。第一个应用通过**训练工作流**实现；第二个应用通过**部署工作流**实现。
- en: Tip
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Often, training and deployment are separate applications since they work on
    different data and have different goals.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，训练和部署是两个独立的应用，因为它们处理的数据不同，目标也不同。
- en: The training workflow uses a lab dataset to produce an acceptable model to implement
    the task, sometimes requiring a few different trials. The deployment workflow
    does not change the model or the strategy anymore; it just applies it to real-world
    transactions to get fraud alarms.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 训练工作流使用实验室数据集来生成一个可接受的模型，以实现任务，有时需要进行几次不同的尝试。部署工作流则不再改变模型或策略，它只是将模型应用于真实世界的交易中，以获取欺诈警报。
- en: 'In this section, we will focus on the training phase, including the following
    steps:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点关注训练阶段，包括以下步骤：
- en: '**Data Access**: Here, we read the lab data from the file, including all 28
    principal components, the transaction amount, and the corresponding time.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据访问**：在这里，我们从文件中读取实验室数据，包括所有28个主成分、交易金额和相应的时间。'
- en: '**Data Preparation**: The data comes already clean and transformed via **Principal
    Component Analysis** (**PCA**). What remains doing in this phase is to create
    all the data subsets required for the training, optimization, and testing of the
    neural autoencoder and the whole strategy.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据准备**：数据通过**主成分分析**（**PCA**）已经被清理和转换。本阶段需要做的就是创建所有训练、优化和测试神经自编码器及整个策略所需的数据子集。'
- en: '**Building the Neural Network**: An autoencoder is a feedforward neural network
    with as many inputs as outputs. Let''s then decide the number of hidden layers,
    the number of hidden neurons, and the activation functions in each layer, and
    then build it accordingly.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建神经网络**：自编码器是一种前馈神经网络，输入和输出的数量相同。接下来，我们决定隐藏层的数量、每层的隐藏神经元数量以及激活函数，并据此构建网络。'
- en: '**Training the Neural Autoencoder**: In this part, the autoencoder is trained
    on a training set of just legitimate transactions with one of the training algorithms
    (the optimizers), according to the selected training parameters, such as, at least,
    the loss function, the number of epochs, and the batch size.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练神经自编码器**：在这一部分，自编码器将在仅包含合法交易的训练集上进行训练，使用其中一种训练算法（优化器），根据选择的训练参数，如至少损失函数、迭代次数和批次大小等。'
- en: '**Rule for Fraud Alarms**: After the network has been trained and it is able
    to reproduce legitimate transactions on the output layer, we need to complete
    the strategy by calculating the distance between the input and output layers and
    by setting a threshold-based rule to trigger fraud alarms.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欺诈警报规则**：网络训练完成后，能够在输出层复现合法交易后，我们需要通过计算输入和输出层之间的距离，并设置基于阈值的规则来触发欺诈警报，从而完成策略。'
- en: '**Testing the whole Strategy**: The last step is to test the whole strategy
    performance. How many legitimate transactions are correctly recognized? How many
    fraud alarms are correctly triggered and how many are false alarms?'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试整体策略**：最后一步是测试整体策略的性能。多少合法交易被正确识别？多少欺诈警报被正确触发？有多少是误报？'
- en: Data Access and Data Preparation
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据访问和数据准备
- en: The credit card dataset from Kaggle comes already clean and transformed. We
    now need to create all the data subsets. Specifically, we need a training and
    a validation set for the training of the autoencoder. They must consist of only
    legitimate transactions. The training set is used to train the network and the
    validation set is used to monitor the performance of the autoencoder on unseen
    data during training.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Kaggle的信用卡数据集已经经过清理和转换。现在我们需要创建所有数据子集。具体来说，我们需要一个用于自编码器训练的训练集和一个验证集。它们必须仅包含合法交易。训练集用于训练网络，验证集用于在训练期间监控自编码器在未见数据上的性能。
- en: 'Then, we need an additional data subset, the threshold optimization set, to
    optimize the threshold, ![](img/Formula_B16391_05_052.png), in the rule-based
    fraud alarm generator. This last subset should include all fraudulent transactions,
    in addition to a number of legitimate transactions, as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要另一个数据子集，即阈值优化集，用于优化基于规则的欺诈警报生成器中的阈值值 ![](img/Formula_B16391_05_052.png)。这个最后的子集应包括所有欺诈交易，以及一些合法交易，具体如下：
- en: 2/3 of all legitimate transactions are dedicated to the autoencoder.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有合法交易的2/3都专用于自编码器。
- en: 90% of those legitimate transactions form the **training set**.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 那些合法交易的90%形成**训练集**。
- en: 10% form the **validation set**.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 10%来自**验证集**。
- en: 1/3 (96K) of all legitimate transactions and all 492 fraudulent transactions
    form the **threshold optimization set**, used to optimize the value of threshold
    ![](img/Formula_B16391_05_023.png).
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有合法交易中的1/3（96K）和所有492笔欺诈交易形成**阈值优化集**，用于优化基于阈值的欺诈警报生成器的阈值值 ![](img/Formula_B16391_05_023.png)。
- en: 'This all translates into one **Row Splitter** node to separate legitimate transactions
    from fraudulent transactions, one **Concatenate** node to add back the fraudulent
    transactions into the threshold optimization set, and a number of **Partitioning**
    nodes. All data extraction in the Partitioning nodes is performed at random:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些转化为一个**行分割器**节点，将合法交易与欺诈交易分开，一个**连接**节点将欺诈交易重新添加到阈值优化集中，以及若干**分区**节点。在分区节点中，所有数据的提取都是随机进行的。
- en: '![Figure 5.3 – The datasets used in the fraud detection process ](img/B16391_05_003.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3 – 用于欺诈检测过程的数据集](img/B16391_05_003.jpg)'
- en: Figure 5.3 – The datasets used in the fraud detection process
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – 用于欺诈检测过程的数据集
- en: Important note
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The training set, validation set, and threshold optimization set must be completely
    separated. No records can be shared across any of the subsets. This is to ensure
    a meaningful performance measure during evaluation and an independent optimization
    procedure.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集、验证集和阈值优化集必须完全分离。不能在任何子集之间共享记录。这是为了确保在评估期间有意义的性能度量和独立的优化过程。
- en: 'Next, all data in each subset must be normalized to fall in ![](img/Formula_B16391_05_024.png).
    Normalization is defined on the training set and applied to the other two subsets.
    The normalization parameters are also saved for the deployment workflow using
    the **Model Writer** node:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，必须对每个子集中的所有数据进行归一化，以确保它们落在 ![](img/Formula_B16391_05_024.png)范围内。归一化是在训练集上定义并应用于其他两个子集。归一化参数也会保存用于部署工作流的**模型写入器**节点：
- en: '![Figure 5.4 – The workflow implementing data preparation for fraud detection](img/B16391_05_004.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图5.4 – 实施欺诈检测数据准备的工作流程](img/B16391_05_004.jpg)'
- en: Figure 5.4 – The workflow implementing data preparation for fraud detection
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – 实施欺诈检测数据准备的工作流程
- en: The workflow in *Figure 5.4* shows how the creation of the different datasets
    and the normalization can be performed in KNIME Analytics Platform.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.4*显示了如何在KNIME Analytics平台中执行不同数据集的创建和归一化。'
- en: Building the Autoencoder
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建自编码器
- en: For this case study, we built an autoencoder with five hidden layers, with `30-40-20-8-20-40-30`
    units, and sigmoid as the activation function.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个案例研究，我们构建了一个具有五个隐藏层的自编码器，分别为`30-40-20-8-20-40-30`个单元，并且sigmoid作为激活函数。
- en: 'The neural network was built using the following (see *Figure 5.5*):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是通过以下方式构建的（参见*图5.5*）：
- en: The `Shape = 30`
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 形状 = 30
- en: Five **Keras Dense Layer** nodes to implement the hidden layers, using sigmoid
    as the activation function and 40, 20, 8, 20, and 40 units, respectively
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用五个**Keras Dense Layer**节点来实现隐藏层，使用sigmoid作为激活函数，分别是40、20、8、20和40个单元。
- en: 'The **Keras Dense Layer** node for the output layer, with 30 units and sigmoid
    as the activation function:'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Keras Dense Layer**节点用于输出层，具有30个单元和sigmoid作为激活函数：'
- en: '![Figure 5.5 – Structure of the neural autoencoder trained to reproduce credit
    card transactions from the input layer onto the output layer](img/B16391_05_005.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 5.5 – 训练以从输入层到输出层复制信用卡交易的神经自编码器的结构](img/B16391_05_005.jpg)'
- en: Figure 5.5 – Structure of the neural autoencoder trained to reproduce credit
    card transactions from the input layer onto the output layer
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 – 训练以从输入层到输出层复制信用卡交易的神经自编码器的结构
- en: Now that we've built the autoencoder, let's train and test it using the data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了自编码器，让我们使用数据来训练和测试它。
- en: Training and Testing the Autoencoder
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和测试自编码器
- en: 'To train and validate the network, we use the **Keras Network Learner** node,
    with the training set and the validation set at the input ports, and the following
    settings (*Figure 5.6*):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练和验证网络，我们使用**Keras Network Learner**节点，训练集和验证集分别放置在输入端口，并使用以下设置（*见图5.6*）：
- en: The number of epochs is set to `50`, the batch size for the training and validation
    set is set to `300`, [and](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)
    the **Adam** (an optimized ve[rsion of backpr](https://en.wikipedia.org/wiki/Backpropagation)opagation)
    training algorithm is used, in the **Options** tab.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: epochs数设置为`50`，训练和验证集的批次大小设置为`300`，并且使用了**Adam**（优化的反向传播的版本）训练算法，在**Options**标签中。
- en: The loss function is set to be the MSE in the **Target** tab.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数设置为**Target**标签中的MSE。
- en: The target and input features are the same in the **Input** tab and in the **Target**
    tab and are accepted as simple **Double** numbers.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入标签和目标标签中的目标和输入特征相同，并且被接受为简单的双精度数。
- en: 'In the **Loss** tab of the **Learning Monitor** view of the Keras Network Learner
    node, you can see two curves now: one is the mean loss (or error) per training
    sample in a batch (in red) and the other one is the mean loss per sample on the
    validation data (in blue).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在**Learning Monitor**视图的**Loss**标签中，您现在可以看到两条曲线：一条是批次中每个训练样本的平均损失（或错误）（红色），另一条是验证数据中每个样本的平均损失（蓝色）。
- en: 'At the end of the training phase, the final mean loss value fell in around
    [0.0012, 0016] for batches from the training set and in [0.0013, 0.0018] for batches
    from the validation set. The calculated loss is the mean reconstruction error
    for one batch, calculated by the following formula:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段结束时，来自训练集的最终平均损失值约为[0.0012, 0.0016]，来自验证集的批次约为[0.0013, 0.0018]。计算的损失是一个批次的平均重建误差，由以下公式计算：
- en: '![](img/Formula_B16391_05_025.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_05_025.png)'
- en: Here, ![](img/Formula_B16391_05_026.png) is the batch size, ![](img/Formula_B16391_05_002.png)
    is the number of units on the output layer, ![](img/Formula_B16391_05_028.png)
    is the output value of neuron *i* in the output layer for training sample *k*,
    and ![](img/Formula_B16391_05_029.png) is the corresponding target answer.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_B16391_05_026.png) 是批次大小，![](img/Formula_B16391_05_002.png)
    是输出层上的单元数，![](img/Formula_B16391_05_028.png) 是训练样本*k*中第*i*个神经元的输出值，![](img/Formula_B16391_05_029.png)
    是相应的目标答案。
- en: After training, the network is applied to the optimization set, using the **Keras
    Network Executor** node, and it is saved for deployment as a Keras file using
    the **Keras Network Writer** node.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，网络应用于优化集，使用**Keras Network Executor**节点，并将其保存为Keras文件以便部署，使用**Keras Network
    Writer**节点。
- en: '*Figure 5.6* shows the configuration for the **Options** tab in the Keras Network
    Executor node: all 30 input features are passed as **Double** numbers and the
    input columns are kept so that the reconstruction error can be calculated later
    on. The last layer is selected as the output and the values are exported as simple
    **Double** numbers:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.6* 显示了 Keras 网络执行器节点中 **选项** 标签的配置：所有 30 个输入特征被作为 **Double** 数字传递，并且输入列被保留，以便稍后计算重建误差。最后一层被选择为输出，值作为简单的
    **Double** 数字导出：'
- en: '![Figure 5.6 – The Keras Network Executor node and its configuration window](img/B16391_05_006.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – Keras 网络执行器节点及其配置窗口](img/B16391_05_006.jpg)'
- en: Figure 5.6 – The Keras Network Executor node and its configuration window
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – Keras 网络执行器节点及其配置窗口
- en: The next step is to calculate the distance between the original feature vector
    and the reproduced feature vector, and to apply a threshold, ![](img/Formula_B16391_05_030.png),
    to discover fraud candidates.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是计算原始特征向量和复现特征向量之间的距离，并应用阈值 ![](img/Formula_B16391_05_030.png)，以发现欺诈候选。
- en: Detecting Fraudulent Transactions
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 侦测欺诈交易
- en: When the model training is finished, the autoencoder has learned how to reproduce
    feature vectors representing legitimate transactions onto the output layer. How
    can we now spot suspicious transactions? If we have a new transaction, ![](img/Formula_B16391_05_016.png),
    how can we tell whether it is a suspicious or a legitimate one?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型训练完成后，自编码器已经学会了如何将代表合法交易的特征向量复现到输出层。那么我们如何发现可疑交易呢？如果有一个新的交易，![](img/Formula_B16391_05_016.png)，我们如何判断它是可疑的还是合法的呢？
- en: 'First, we run this new transaction, ![](img/Formula_B16391_05_032.png), through
    the autoencoder via the Keras Network Executor node. The reproduction of the original
    transaction is generated at the output layer. Now, a reconstruction error, ![](img/Formula_B16391_05_033.png),
    is calculated, as the distance between the original transaction vector and the
    reproduced one. A transaction is then considered a fraud candidate according to
    the following rule:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过 Keras 网络执行器节点，将这个新的交易 ![](img/Formula_B16391_05_032.png) 输入到自编码器中。原始交易的复现会在输出层生成。接下来，计算重建误差
    ![](img/Formula_B16391_05_033.png)，即原始交易向量与复现向量之间的距离。根据以下规则，交易会被视为欺诈候选：
- en: IF ![](img/Formula_B16391_05_034.png) THEN ![](img/Formula_B16391_05_018.png)
    -> "legitimate trx"
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ![](img/Formula_B16391_05_034.png) 那么 ![](img/Formula_B16391_05_018.png)
    -> "合法交易"
- en: IF ![](img/Formula_B16391_05_036.png) THEN ![](img/Formula_B16391_05_016.png)
    -> "fraud candidate trx"
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ![](img/Formula_B16391_05_036.png) 那么 ![](img/Formula_B16391_05_016.png)
    -> "欺诈候选交易"
- en: 'Here, ![](img/Formula_B16391_05_019.png) is the reconstruction error value
    for transaction ![](img/Formula_B16391_05_039.png) and *K* is a threshold. The
    MSE was also adopted for the reconstruction error:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_B16391_05_019.png) 是交易 ![](img/Formula_B16391_05_039.png)
    的重建误差值，*K* 是一个阈值。重建误差也采用了 MSE：
- en: '![](img/Formula_B16391_05_040.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_05_040.png)'
- en: Here, ![](img/Formula_B16391_05_041.png) is the *i*th feature of transaction
    ![](img/Formula_B16391_05_042.png), and ![](img/Formula_B16391_05_043.png) is
    the corresponding value on the output layer of the network.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_B16391_05_041.png) 是交易 ![](img/Formula_B16391_05_042.png)
    的 *i* 特征，![](img/Formula_B16391_05_043.png) 是网络输出层上对应的值。
- en: '![](img/Formula_B16391_05_044.png) is calculated via a `1` is the fraud candidate
    class and `0` is the legitimate transaction class. A `1`. Specificity is the ratio
    between the number of true legitimate transactions and all transactions that did
    not raise any alarm. Sensitivity, on the opposite side, measures the ratio of
    fraud alarms that actually hit a fraudulent transaction.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/Formula_B16391_05_044.png) 是通过 `1` 来计算的，`1` 是欺诈候选类，`0` 是合法交易类。一个 `1`。特异性是指真正的合法交易与所有没有触发任何警报的交易之间的比例。敏感性则相反，衡量实际触发欺诈交易的欺诈警报的比例。'
- en: 'Specificity produces a measure of the frauds we might have missed, while sensitivity
    produces a measure of the frauds we hit:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 特异性衡量我们可能错过的欺诈，而敏感性衡量我们发现的欺诈。
- en: '![Figure 5.7 – The rule implemented in the Rule Engine node, comparing reconstruction
    error with threshold](img/B16391_05_007.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7 – 在规则引擎节点中实现的规则，通过与阈值比较重建误差](img/B16391_05_007.jpg)'
- en: Figure 5.7 – The rule implemented in the Rule Engine node, comparing reconstruction
    error with threshold
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – 在规则引擎节点中实现的规则，通过与阈值比较重建误差
- en: Now that our model is trained and tested, it needs to be optimized.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的模型已经训练和测试，需要进行优化。
- en: Optimizing the Autoencoder Strategy
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化自编码器策略
- en: What is the best value to use for threshold ![](img/Formula_B16391_05_023.png)?
    In the last section, we adopted ![](img/Formula_B16391_05_048.png) based on our
    experience. However, is this the best value for ![](img/Formula_B16391_05_052.png)?
    Threshold ![](img/Formula_B16391_05_052.png), in this case, is not automatically
    optimized via the training procedure. It is just a static parameter external to
    the training algorithm. In KNIME Analytics Platform, it is also possible to optimize
    static parameters outside of the **Learner** nodes.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳阈值 ![](img/Formula_B16391_05_023.png) 是多少？在最后一节中，我们根据经验采用了 ![](img/Formula_B16391_05_048.png)。但是，对于
    ![](img/Formula_B16391_05_052.png) 来说，这是否是最佳值？在这种情况下，阈值 ![](img/Formula_B16391_05_052.png)
    并不会通过训练过程自动优化。它只是训练算法外部的静态参数。在KNIME Analytics平台上，也可以在**学习者**节点外部优化静态参数。
- en: Optimizing Threshold ![](img/Formula_B16391_05_051.png)
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化阈值 ![](img/Formula_B16391_05_051.png)
- en: 'Threshold ![](img/Formula_B16391_05_052.png) is defined on a separate subset
    of data, called the **optimization set**. There are two options here:'
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阈值 ![](img/Formula_B16391_05_052.png) 定义在名为**优化集**的数据的一个单独子集上。在这里有两个选择：
- en: If an optimization set with labeled fraudulent transactions is available, the
    value of threshold ![](img/Formula_B16391_05_052.png) is optimized against any
    accuracy measure for fraud detection.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果可用带有标记的欺诈交易优化集，则阈值 ![](img/Formula_B16391_05_052.png) 根据任何欺诈检测的准确性度量进行优化。
- en: If no labeled fraudulent transactions are available in the dataset, the value
    of threshold ![](img/Formula_B16391_05_052.png) is defined as a high percentile
    of the reconstruction errors on the optimization set.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据集中没有可用的标记欺诈交易，则阈值 ![](img/Formula_B16391_05_052.png) 的值被定义为优化集上重建误差的高百分位数。
- en: 'During the data preparation phase, we generated three data subsets: the training
    set and validation set for the Keras Network Learner node to train and validate
    the autoencoder, and one last subset, which we called the threshold optimization
    set. This final subset includes 1/3 of all the legitimate transactions and the
    handful of fraudulent transactions. We can use this subset to optimize the value
    of threshold ![](img/Formula_B16391_05_023.png) against the accuracy of the whole
    fraud detection strategy.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据准备阶段，我们生成了三个数据子集：用于Keras网络学习者节点训练和验证自编码器的训练集和验证集，以及最后一个子集，我们称之为阈值优化集。这个最终子集包括所有合法交易的三分之一和少量欺诈交易。我们可以使用这个子集来优化阈值
    ![](img/Formula_B16391_05_023.png)，以提高整个欺诈检测策略的准确性。
- en: To optimize a parameter means to find the value within a range that maximizes
    or minimizes a given measure. Based on our experience, we assume the value of
    *K* to be a positive number (> 0) and to lie below 0.02\. So, to optimize the
    value of threshold ![](img/Formula_B16391_05_023.png) means to find the value
    in ![](img/Formula_B16391_05_057.png) that maximizes the accuracy of the whole
    application.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 优化参数意味着在给定范围内找到最大化或最小化给定度量的值。根据我们的经验，我们假设 *K* 的值为正数（> 0），且在 0.02 以下。因此，优化阈值
    ![](img/Formula_B16391_05_023.png) 的值意味着在 ![](img/Formula_B16391_05_057.png) 中找到最大化整个应用程序准确性的值。
- en: The accuracy of the application is calculated via a Scorer (JavaScript) node,
    considering the results of the Rule Engine node as the predictions and comparing
    them with the original class (`0` = legitimate transaction, `1` = fraudulent transaction)
    in the optimization set.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序的准确性通过一个Scorer（JavaScript）节点计算，考虑规则引擎节点的结果作为预测，并将其与优化集中的原始类别（`0` = 合法交易，`1`
    = 欺诈交易）进行比较。
- en: The spanning of the value interval and the identification of the threshold value
    for the maximum accuracy is performed by an `loop start` node and a `loop end`
    node. In the optimization loop, these two nodes are the **Parameter Optimization
    Loop Start** node and the **Parameter Optimization Loop End** node.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个`loop start`节点和一个`loop end`节点执行值区间的跨度和最大准确性的阈值标识。在优化循环中，这两个节点分别是**参数优化循环起始**节点和**参数优化循环结束**节点。
- en: The **Parameter Optimization Loop Start** node spans parameter values in a given
    interval with a given step size. Interval ![](img/Formula_B16391_05_058.png) and
    step size ![](img/Formula_B16391_05_059.png) have been chosen here based on the
    range of the reconstruction error feature, as shown in the **Lower Bound** and
    **Upper Bound** cells in the **Spec** tab of the data table at the output port
    of the Math Formula node, named **MSE input-output distance**, after the Keras
    Network Executor node.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数优化循环起始**节点在给定的区间内以给定的步长遍历参数值。区间 ![](img/Formula_B16391_05_058.png) 和步长
    ![](img/Formula_B16391_05_059.png) 是根据重建误差特征的范围选择的，如数据表输出端口的**下限**和**上限**单元格所示，该表位于数学公式节点输出端，名为**MSE
    输入-输出距离**，紧随 Keras 网络执行器节点之后。'
- en: The **Parameter Optimization Loop End** node collects all results as flow variables,
    detects the best (maximum or minimum) value for the target measure, and exports
    it together with the parameter that generated it. In our case, the target measure
    is the accuracy, measured on the predictions from the Rule Engine node, which
    must be maximized against values for threshold ![](img/Formula_B16391_05_052.png).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数优化循环结束**节点将所有结果作为流变量收集，检测目标度量的最佳（最大或最小）值，并将其与生成该值的参数一起导出。在我们的案例中，目标度量是精度，通过规则引擎节点的预测进行测量，必须针对阈值
    ![](img/Formula_B16391_05_052.png) 进行最大化。'
- en: 'All nodes in between the loop start and the loop end make the body of the loop
    – that is, the part that gets repeated as many times as needed until the input
    interval of parameter values has all been covered. In the loop body, we add the
    additional constraint that the optimal accuracy should be found only for those
    parameters where the specificity and sensitivity are close in value. This is the
    goal of the metanode named `Coefficient 0/1`. Here, if the specificity and sensitivity
    are more than 10% apart, the coefficient is set to `0`, otherwise to `1`. This
    coefficient then multiplies the overall accuracy coming from the Scorer (JavaScript)
    node. In this way, the maximum accuracy is detected only for those cases where
    the specificity and sensitivity are close to each other:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 循环起始和循环结束之间的所有节点构成了循环的主体——即，直到输入的参数值区间完全覆盖为止，这部分会根据需要重复多次。在循环主体内，我们增加了额外的约束条件，即只有当特异性和灵敏度接近时，才会找到最优的精度。这是名为
    `Coefficient 0/1` 的元节点的目标。在这里，如果特异性和灵敏度相差超过 10%，系数被设置为 `0`，否则为 `1`。然后，这个系数将乘以来自评分器（JavaScript）节点的整体精度。通过这种方式，只有当特异性和灵敏度接近时，才能检测到最大精度：
- en: '![Figure 5.8 – The optimization loop](img/B16391_05_008.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8 – 优化循环](img/B16391_05_008.jpg)'
- en: Figure 5.8 – The optimization loop
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – 优化循环
- en: After extracting the optimal threshold, we transform it into a flow variable
    and pass it to the final rule implementation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在提取最优阈值后，我们将其转化为流变量，并将其传递给最终的规则实现。
- en: Wrapping up into a Component
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 包装成一个组件
- en: 'Now, this whole threshold optimization part seems to be a logical self-contained
    block. To keep our workflow clean and proper, we could wrap this block inside
    a metanode. Even better, we could make sure that the wrapping is close and tight
    via a stronger type of metanode: the **component**.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这整个阈值优化部分似乎是一个逻辑上自包含的模块。为了保持我们的工作流程简洁且规范，我们可以将这个模块包装在一个元节点内。更好的是，我们可以通过一种更强类型的元节点——**组件**，确保包装紧密且封闭。
- en: Tip
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: A metanode just collects and packages nodes. A component, on the other hand,
    collects and packages nodes together and, in addition, inherits the views of contained
    Widget and JavaScript nodes and the configuration windows of contained Configuration
    nodes. Even further, a component does not allow external flow variables to enter
    or internal flow variables to exit unless specifically defined.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 元节点仅仅是收集和打包节点。而组件则不仅收集和打包节点，还继承了包含的 Widget 和 JavaScript 节点的视图以及包含的配置节点的配置窗口。更进一步，组件不允许外部流变量进入或内部流变量退出，除非特别定义。
- en: 'A component is created in a similar way to a metanode. Just select the nodes
    to group together, right-click, and select **Create Component…**. When a component
    is created, its context (right-click) menu offers a number of commands to open,
    expand, modify via setup, and share it. To inspect the content of a component,
    just *Ctrl* + double-click the component. Once inside, you can see two nodes:
    **Component Input** and **Component Output**. In the configuration window of these
    two nodes, you can set the flow variables to import inside and export outside
    of the component, respectively.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 组件的创建方式类似于元节点。只需选择要分组的节点，右键单击并选择**创建组件…**。创建组件后，其上下文（右键）菜单提供了多个命令，用于打开、展开、通过设置进行修改和共享。要查看组件的内容，只需*Ctrl*
    + 双击组件。进入后，你可以看到两个节点：**组件输入**和**组件输出**。在这两个节点的配置窗口中，你可以分别设置流变量以导入和导出组件内外的数据。
- en: In the component we created, we set up the **Component Output** node to export
    the flow variable containing the value for the optimal threshold. This flow variable
    needs to exit the component to be used in the final rule for fraud detection.
    The final rule is implemented in a new Rule Engine node and the final predictions
    are evaluated against the original classes in a new Scorer (JavaScript) node.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们创建的组件中，我们设置了**组件输出**节点来导出包含最优阈值的流变量。这个流变量需要从组件中输出，以便在最终的欺诈检测规则中使用。最终的规则在一个新的规则引擎节点中实现，最终的预测将在一个新的评分器（JavaScript）节点中与原始类别进行比较。
- en: 'The final workflow to train and test the neural autoencoder using credit card
    transaction data and to implement the fraud detection rule with the optimal threshold
    is shown in *Figure 5.9*. The workflow, named `01_Autoencoder_for_Fraud_Detection_Training`,
    is downloadable from the KNIME Hub: [https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%205/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%205/):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的工作流用于训练和测试神经自编码器，使用信用卡交易数据，并实现带有最优阈值的欺诈检测规则，详见*图 5.9*。该工作流名为`01_Autoencoder_for_Fraud_Detection_Training`，可以从
    KNIME Hub 下载：[https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%205/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%205/)：
- en: '![Figure 5.9 – The workflow to train and test the autoencoder and to find the
    optimal threshold, K](img/B16391_05_009.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – 训练和测试自编码器并找到最优阈值 K 的工作流](img/B16391_05_009.jpg)'
- en: Figure 5.9 – The workflow to train and test the autoencoder and to find the
    optimal threshold, K
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 训练和测试自编码器并找到最优阈值 K 的工作流
- en: Now that we have found the best threshold, let's have a look at the performance
    of the autoencoder.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经找到了最佳阈值，让我们来看看自编码器的性能。
- en: Performance Metrics
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能指标
- en: In this section, we report the performance measures of this approach on the
    threshold optimization set after applying the fraud detection rule. The optimal
    threshold value was found to be ![](img/Formula_B16391_05_061.png) for an accuracy
    of 93.52%.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们报告了应用欺诈检测规则后，在阈值优化集上该方法的性能指标。最优阈值被发现为![](img/Formula_B16391_05_061.png)，准确率为93.52%。
- en: 'In *Figure 5.10*, you can see the **confusion matrix**, the class statistics
    based on it, and the general performance measures, all of them describing how
    well the fraud detector is performing on the optimization set:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 5.10*中，你可以看到**混淆矩阵**、基于此的类别统计以及一般的性能指标，所有这些都描述了欺诈检测器在优化集上的表现：
- en: '![Figure 5.10 – Performance metrics of the final fraud detector with optimized
    threshold K](img/B16391_05_010.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10 – 使用优化阈值 K 的最终欺诈检测器的性能指标](img/B16391_05_010.jpg)'
- en: Figure 5.10 – Performance metrics of the final fraud detector with optimized
    threshold K
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – 使用优化阈值 K 的最终欺诈检测器的性能指标
- en: 'Let''s consider class 1 (fraud) as the positive class. The high number of false
    positives (6,236) shows the weakness of this approach: it is prone to generating
    false positives. In other words, it tends to label perfectly legitimate transactions
    as fraud candidates. Now, there are case studies where false positives are not
    a huge problem, and this is one of those. In the case of a false positive, the
    price to pay is to send a message to the credit card owner about the current transaction.
    If the message turns out to be useless, the damage is not much compared to the
    possible risk. Of course, this tolerance does not apply to all case studies. A
    false positive in medical diagnosis carries a much heavier responsibility than
    a wrong fraud alarm in a credit card transaction.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 假设类别1（欺诈）为正类。大量的假阳性（6,236）显示了这种方法的弱点：它容易产生假阳性。换句话说，它倾向于将完全合法的交易错误标记为欺诈候选交易。现在，确实有些案例研究中假阳性不是一个大问题，这就是其中之一。在假阳性的情况下，付出的代价是向信用卡持有者发送当前交易的消息。如果这条消息没有用处，与可能的风险相比，损害并不大。当然，这种容忍度并不适用于所有案例研究。在医学诊断中，假阳性比信用卡交易中的错误欺诈警报责任要大得多。
- en: Important note
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The whole process could also be forced to lean more toward fraud candidates
    or legitimate transactions, by introducing an expertise-based bias in the definition
    of threshold *K*.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在定义阈值*K*时引入基于专业知识的偏差，整个过程也可以被迫倾向于欺诈候选交易或合法交易。
- en: In general, the autoencoder captures 87% of the fraudulent transactions and
    93% of the legitimate transactions in the validation set, for an overall accuracy
    of 85% and a Cohen's kappa of 0.112\. Considering the high imbalance between the
    number of normal and fraudulent transactions in the validation set (96,668 versus
    492), the results are still promising.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，自动编码器能够捕获验证集中87%的欺诈交易和93%的合法交易，总体准确率为85%，Cohen's kappa值为0.112。考虑到验证集中正常交易与欺诈交易的高度不平衡（96,668比492），结果仍然是有前景的。
- en: Notice that this false positive-prone approach is a desperate solution for a
    case study where no, or almost no, examples from one of the classes exist. A supervised
    classifier on a training set with labeled examples would probably reach better
    performances. But this is the data we have to deal with!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种容易产生假阳性的方式对于没有或几乎没有某一类别示例的案例研究来说是一个无奈的解决方案。在一个包含标注示例的训练集上，监督分类器可能会达到更好的表现。但这就是我们必须应对的数据！
- en: We have now trained the autoencoder and found the best threshold for our rule
    system. We will see, in the next section, how to deploy it in the real world on
    real data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经训练好了自动编码器，并找到了我们规则系统的最佳阈值。在下一部分中，我们将看到如何在现实世界的实际数据上部署它。
- en: Deploying the Fraud Detector
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署欺诈检测器
- en: At this point, we have an autoencoder network and a rule with acceptable performance
    for fraud detection. In this section, we will implement the **deployment** workflow.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经有了一个自动编码器网络和一个具有可接受性能的欺诈检测规则。在这一部分中，我们将实现**部署**工作流。
- en: The deployment workflow (*Figure 5.11*), like all deployment workflows, takes
    in new transaction data, passes it through the autoencoder, calculates the distance,
    applies the fraud detection rule, and finally, flags the input transaction as
    fraud or legitimate.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 部署工作流（*图5.11*），像所有部署工作流一样，接收新的交易数据，将其通过自动编码器，计算距离，应用欺诈检测规则，最后，将输入交易标记为欺诈或合法。
- en: 'This workflow, named `02_Autoencoder_for_Fraud_Detection_Deployment`, is downloadable
    from the KNIME Hub: [https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%205/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%205/):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工作流名为`02_Autoencoder_for_Fraud_Detection_Deployment`，可以从KNIME Hub下载：[https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%205/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%205/)：
- en: '![Figure 5.11 – The deployment workflow](img/B16391_05_011.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图5.11 – 部署工作流](img/B16391_05_011.jpg)'
- en: Figure 5.11 – The deployment workflow
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 – 部署工作流
- en: Let's have a look at the different parts of the workflow in detail.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看一下工作流的不同部分。
- en: Reading Network, New Transactions, and Normalization Parameters
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取网络、新交易和标准化参数
- en: In this workflow, first the autoencoder model is read from the previously saved
    Keras file, using the **Keras Network Reader** node.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个工作流中，首先通过**Keras网络读取器**节点，从之前保存的Keras文件中读取自动编码器模型。
- en: At the same time, data from some new credit card transactions are read from
    the file using the **File Reader** node. This particular file contains two new
    transactions.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，一些新的信用卡交易数据通过**文件读取器**节点从文件中读取。这个特定的文件包含了两笔新的交易。
- en: The transactions are normalized with the same parameters built on the training
    data and previously saved in the file named `normalizer model`. These normalization
    parameters are read from the file using the **Model Reader** node.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 交易数据使用与训练数据相同的参数进行标准化，这些参数之前保存在名为 `normalizer model` 的文件中。这些标准化参数通过**模型读取器**节点从文件中读取。
- en: The last file to read contains the value of the optimized threshold, *K*.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一份文件包含了优化后的阈值值，*K*。
- en: Applying the Fraud Detector
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用欺诈检测器
- en: Transaction data is fed into the autoencoder network and reproduced on the output
    layer with the Keras Network Executor node.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 交易数据被输入到自编码器网络中，并通过 Keras 网络执行器节点在输出层进行重建。
- en: Afterward, the MSEs between the original features and the reconstructed features
    for each transaction are calculated using the **Math Formula** node.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，通过**数学公式**节点计算每笔交易的原始特征和重建特征之间的均方误差（MSE）。
- en: The Rule Engine node applies the threshold, ![](img/Formula_B16391_05_052.png),
    as defined during the optimization phase, to detect possible fraud candidates.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 规则引擎节点应用阈值，![](img/Formula_B16391_05_052.png)，这是在优化阶段定义的，用来检测可能的欺诈候选。
- en: 'The following table shows the reconstruction errors for the two transactions
    and the consequent class assignment. The application (autoencoder and distance
    rule) defines the first transaction as legitimate and the second transaction as
    a fraud candidate:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了两笔交易的重建误差以及随之而来的分类结果。应用（自编码器和距离规则）将第一笔交易定义为合法，第二笔交易定义为欺诈候选：
- en: '![](img/B16391_05_012.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16391_05_012.jpg)'
- en: Figure 5.12 – Reconstruction errors and fraud class assignment for credit card
    transactions in the dataset used for deployment
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 – 用于部署的数据集中信用卡交易的重建误差和欺诈分类结果
- en: Taking Actions
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采取行动
- en: 'In the last part of the workflow, we need to take action:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在工作流的最后部分，我们需要采取行动：
- en: IF transaction is legitimate (class 0) => do nothing
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果交易是合法的（类别 0）=> 不做任何操作
- en: IF transaction is fraud candidate (class 1) => send message to owner to confirm
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果交易是欺诈候选（类别 1）=> 向卡主发送确认消息
- en: '**IF-THEN** conditions involving actions are implemented in KNIME Analytics
    Platform via switch blocks. Similar to loops, switch blocks have a start node
    and an end node. The end node in switch blocks is optional, however. The switch
    start node activates only one of the output ports, enabling de facto only one
    possible further path for the data flow. The switch end node collects the results
    from the different branches. The most versatile switch block is the **CASE switch**
    in all its declinations: for data, flow variables, or models.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**IF-THEN** 条件和行动通过 KNIME 分析平台中的开关块实现。类似于循环，开关块有一个起始节点和一个结束节点。然而，开关块中的结束节点是可选的。开关起始节点只会激活一个输出端口，从而仅启用一个可能的数据流路径。开关结束节点收集来自不同分支的结果。最通用的开关块是**CASE
    switch**，它有多种变体：适用于数据、流变量或模型。'
- en: The active port, and then the active branch, is controlled via the configuration
    window of the **Switch CASE Start** node. This configuration setting is usually
    controlled via a flow variable, whose values enable one or the other output each
    time.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 活动端口，然后是活动分支，通过**Switch CASE Start**节点的配置窗口进行控制。此配置设置通常通过流变量控制，流变量的值每次决定启用哪个输出。
- en: In our case, we have two branches. The upper branch is connected to port `0`,
    activated by class `0`, and performs nothing. The second branch is connected to
    port `1`, activated by class `1`, and sends an email to the owner of the credit
    card.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们有两个分支。上分支连接到端口 `0`，由类别 `0` 激活，不执行任何操作。第二个分支连接到端口 `1`，由类别 `1` 激活，并向信用卡持有者发送电子邮件。
- en: We conclude here the section on the implementation of the autoencoder-based
    strategy for fraud detection.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们结束了基于自编码器的欺诈检测策略的实现部分。
- en: Summary
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed approaches for building a fraud detector for credit
    card transactions in the desperate case when no, or almost no, examples of the
    fraud class are available. This solution trains a neural autoencoder to reproduce
    legitimate transactions from the input onto the output layer. Some postprocessing
    is necessary to set an alarm for the fraud candidate based on the reconstruction
    error.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了在几乎没有或完全没有欺诈类别样本的情况下，构建信用卡交易欺诈检测器的方法。这一解决方案训练一个神经自编码器，将合法交易从输入重建到输出层。需要一些后处理步骤，通过重建误差为潜在欺诈行为设置警报。
- en: In describing this solution, we have introduced the concept of training and
    deployment applications, components, optimization loops, and switch blocks.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述这个解决方案时，我们引入了训练与部署应用、组件、优化循环和开关块的概念。
- en: In the next chapter, we will discuss a special family of neural networks, so-called
    recurrent neural networks, and how they can be used to train neural networks for
    sequential data.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论一种特殊类型的神经网络，即所谓的循环神经网络，以及它们如何用于训练处理序列数据的神经网络。
- en: Questions and Exercises
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题与练习
- en: 'Check your level of understanding of the concepts presented in this chapter
    by answering the following questions:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 通过回答以下问题，检查你对本章概念的理解程度：
- en: What is the goal of an autoencoder during training?
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自编码器在训练过程中的目标是什么？
- en: a) To reproduce the input to the output
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 将输入重建到输出
- en: b) To learn an automatic encoding
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 学习自动编码
- en: c) To encode the training data
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 对训练数据进行编码
- en: d) To train a network that can distinguish between two classes
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 训练一个能够区分两个类别的网络
- en: What are common use cases for autoencoders?
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自编码器的常见应用场景有哪些？
- en: a) Time series prediction
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 时间序列预测
- en: b) Anomaly detection
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 异常检测
- en: c) Multiclass classification problems
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 多类别分类问题
- en: d) Regression problems
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 回归问题
- en: How can an autoencoder be used for dimensionality reduction?
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自编码器如何用于降维？
- en: a) By training a network with an output layer with less number than input layers
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 通过训练一个输出层神经元数量少于输入层的网络
- en: b) By training an autoencoder and extracting only the encoder
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 通过训练一个自编码器并仅提取编码器
- en: c) By building an autoencoder and extracting only the decoder
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 通过构建一个自编码器并仅提取解码器
- en: d) By building a network with more hidden neurons than the input and output
    layers
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 通过构建一个隐藏神经元比输入和输出层更多的网络
