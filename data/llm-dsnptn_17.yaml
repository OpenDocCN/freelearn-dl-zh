- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: Fairness and Bias Detection
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 公平性与偏见检测
- en: Fairness in LLMs involves ensuring that the model’s outputs and decisions do
    not discriminate against or unfairly treat individuals or groups based on protected
    attributes such as race, gender, age, or religion. It’s a complex concept that
    goes beyond just avoiding explicit bias.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM中，公平性涉及确保模型的结果和决策不会基于受保护属性（如种族、性别、年龄或宗教）歧视或不公平对待个人或群体。这是一个复杂的概念，而不仅仅是避免显性偏见。
- en: 'There are several definitions of fairness in machine learning:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中公平性的定义有几个：
- en: '**Demographic parity**: The probability of a positive outcome should be the
    same for all groups'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人口统计学平等性**：所有群体获得积极结果的概率应该是相同的'
- en: '**Equal opportunity**: The true positive rates should be the same for all groups'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平等机会**：所有群体的真正阳性率应该是相同的'
- en: '**Equalized odds**: Both true positive and false positive rates should be the
    same for all groups'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均衡机会**：所有群体的真正阳性和假阳性率应该是相同的'
- en: For LLMs, fairness often involves ensuring that the model’s language generation
    and understanding capabilities are equitable across different demographic groups
    and do not perpetuate or amplify societal bias.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型语言模型（LLM），公平性通常涉及确保模型的语言生成和理解能力在不同人口群体之间是公平的，并且不会持续或放大社会偏见。
- en: In this chapter, you’ll learn about different types of bias that can emerge
    in LLMs and techniques for detecting them.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解LLM中可能出现的不同类型的偏见以及检测它们的技巧。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Types of bias
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏见的类型
- en: Fairness metrics for LLM text generation and understanding
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM文本生成和理解公平性指标
- en: Detecting bias
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测偏见
- en: Debiasing strategies
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去偏见策略
- en: Fairness-aware training
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公平性意识训练
- en: Ethical considerations
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伦理考量
- en: Types of bias
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏见的类型
- en: 'LLMs can exhibit various types of bias:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: LLM可以表现出各种类型的偏见：
- en: '**Representation bias**: The underrepresentation or misrepresentation of certain
    groups in training data—for example, a facial recognition system trained primarily
    on lighter-skinned faces may exhibit significantly higher error rates when identifying
    individuals with darker skin tones, due to inadequate representation in the training
    set.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代表性偏见**：在训练数据中对某些群体的代表性不足或错误表示——例如，主要在浅色皮肤的面部上进行训练的面部识别系统在识别深色皮肤色调的人时可能表现出显著更高的错误率，这是由于训练集中代表性不足。'
- en: '**Linguistic bias**: The language used by AI systems to describe different
    groups—for instance, an AI system as may label men as “assertive” and women as
    “aggressive when referring to the same behaviors across genders, reinforcing subtle
    discriminatory patterns.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言偏见**：人工智能系统用于描述不同群体的语言——例如，一个AI系统可能会将男性标记为“自信”而将女性标记为“在性别间具有攻击性”，当描述相同的行为时，这会强化细微的歧视模式。'
- en: '**Allocation bias**: The unfair distribution of resources or opportunities
    based on model predictions, as seen when an automated hiring system systematically
    ranks candidates from certain universities higher, regardless of their qualifications,
    thereby disproportionately allocating interview opportunities to graduates from
    these institutions.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分配偏见**：基于模型预测的资源或机会的不公平分配，如自动化招聘系统系统性地将某些大学的候选人排名更高，而不管他们的资格如何，从而不成比例地将面试机会分配给这些机构的毕业生。'
- en: '**Quality of service bias**: Variations in model performance across different
    groups, as illustrated by a machine translation system that provides significantly
    more accurate translations for mainstream languages like English, Spanish, and
    Mandarin while delivering lower-quality translations for languages with fewer
    speakers or less representation in the training data.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务质量偏见**：模型在不同群体之间的性能差异，如一个机器翻译系统为英语、西班牙语和普通话等主流语言提供更准确的翻译，而为使用人数较少或训练数据中代表性较低的语言提供较低质量的翻译。'
- en: '**Stereotypical bias**: The reinforcement of societal stereotypes through language
    generation, as demonstrated when an AI writing assistant automatically suggests
    stereotypical career paths when completing stories about characters of different
    backgrounds – suggesting careers in sports or entertainment for characters from
    certain racial backgrounds while suggesting professional careers like doctors
    or lawyers for others.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**刻板印象偏见**：通过语言生成强化社会刻板印象，例如当人工智能写作助手在完成关于不同背景角色的故事时自动建议刻板印象的职业道路——为某些种族背景的角色建议体育或娱乐职业，而为其他人建议医生或律师等职业。'
- en: '**Explicit and implicit bias**: Explicit bias in LLMs arises from overt patterns
    in training data, such as stereotypes present in text sources, leading to clearly
    identifiable bias in outputs. Implicit bias, on the other hand, is more subtle
    and emerges from underlying statistical correlations in data, shaping responses
    in ways that may reinforce hidden bias without direct intention. While explicit
    bias can often be detected and mitigated through filtering or fine-tuning, implicit
    bias is harder to identify and requires deeper intervention, such as bias-aware
    training techniques and regular auditing of model outputs.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**显性和隐性偏见**: LLM 中的显性偏见源于训练数据中的明显模式，例如文本来源中存在的刻板印象，导致输出中明显可识别的偏见。另一方面，隐性偏见更为微妙，源于数据中的潜在统计相关性，以可能加强隐藏偏见的方式塑造响应，而没有直接意图。虽然显性偏见通常可以通过过滤或微调来检测和缓解，但隐性偏见更难识别，需要更深入的措施，例如偏见感知训练技术和对模型输出的定期审计。'
- en: '**Hidden bias**: Hidden bias in LLMs arises when training data, model design,
    or deployment choices subtly skew responses, reinforcing stereotypes or excluding
    perspectives. This can manifest in gendered language, cultural favoritism, or
    political slants, often due to overrepresented viewpoints in training data. Algorithmic
    processing can further amplify these biases, making responses inconsistent or
    skewed based on prompt phrasing. To mitigate this, diverse datasets, bias audits,
    and ethical fine-tuning are essential, ensuring models generate balanced and fair
    outputs while allowing user-aware adjustments within ethical constraints.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏偏见**: 当训练数据、模型设计或部署选择微妙地扭曲响应，加强刻板印象或排除观点时，LLM 中的隐藏偏见就会出现。这可以表现为性别语言、文化偏好或政治倾向，通常是由于训练数据中过度代表的观点。算法处理可以进一步放大这些偏见，使响应根据提示语句不一致或偏斜。为了缓解这种情况，需要多样化的数据集、偏见审计和道德微调，确保模型在道德约束内生成平衡和公平的输出，同时允许用户在道德约束内进行感知调整。'
- en: 'Here’s an example of how to check for representation bias in a dataset (we
    will just show one example to limit the size of this chapter):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个检查数据集中表示偏见的例子（我们将只展示一个例子以限制本章的篇幅）：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code analyzes the representation of gender-related terms in a corpus of
    texts, which can help identify potential gender bias in the dataset.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码分析了一个文本语料库中性别相关术语的表示，这有助于识别数据集中潜在的性别偏见。
- en: Fairness metrics for LLM text generation and understanding
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 文本生成和理解中的公平性指标
- en: Fairness metrics often focus on comparing model performance or outputs across
    different demographic groups.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性指标通常关注比较不同人口群体之间的模型性能或输出。
- en: 'Here are some examples:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些例子：
- en: '**Demographic parity difference for text classification**: This metric measures
    the difference in positive prediction rates between the most and least favored
    groups:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本分类中的人口统计学差异**: 该指标衡量了最被青睐的群体和最不受青睐的群体之间正预测率的差异：'
- en: '[PRE1]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The code defines a `demographic_parity_difference` function that computes the
    difference in demographic parity between groups defined by a protected attribute.
    It takes true labels (`y_true`), predicted labels (`y_pred`), and the protected
    attribute values as input. For each unique group in the protected attribute, it
    creates a Boolean mask to isolate the corresponding subset of predictions and
    computes the confusion matrix for that group. The demographic parity (DP) for
    each group is then calculated as the proportion of positive predictions—true or
    false—out of all predictions for that group, specifically using `(cm[1, 0] + cm[1,
    1]) / cm.sum()`, which corresponds to the number of actual positives (both misclassified
    and correctly classified) over the total. It stores these DP values and finally
    returns the maximum difference between them, indicating the disparity in treatment
    across groups. The example demonstrates this using dummy data, printing out the
    DP difference between groups `'A'` and `'B'`.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该代码定义了一个 `demographic_parity_difference` 函数，该函数计算由受保护属性定义的群体之间的人口统计学差异。它接受真实标签
    (`y_true`)、预测标签 (`y_pred`) 和受保护属性值作为输入。对于受保护属性中的每个唯一群体，它创建一个布尔掩码来隔离相应的预测子集，并计算该组的混淆矩阵。然后，每个群体的人口统计学差异（DP）被计算为该组所有预测中正预测（无论是误分类还是正确分类）的比例，具体使用
    `(cm[1, 0] + cm[1, 1]) / cm.sum()`，这对应于实际正数（无论是误分类还是正确分类）的数量除以总数。它存储这些 DP 值，并最终返回它们之间的最大差异，这表明了跨组之间的待遇差异。示例使用虚拟数据演示了这一点，打印出
    `'A'` 和 `'B'` 组之间的 DP 差异。
- en: '**Equal opportunity difference for text classification**: This metric measures
    the difference in true positive rates between the most and least favored groups:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本分类的平等机会差异**：此指标衡量最被青睐的群体和最不受青睐的群体之间真正阳性率的差异：'
- en: '[PRE2]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This code calculates the difference in true positive rates between groups defined
    by a protected attribute, measuring how equally the model correctly identifies
    positive cases across those groups.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此代码计算由受保护属性定义的群体之间的真正阳性率差异，衡量模型在那些群体中正确识别阳性案例的平等程度。
- en: Now that we’ve explored a couple of metrics for measuring fairness in model
    outputs and understanding capabilities, we’ll move on to learning techniques to
    actually detect bias in practice, building on these metrics to develop systematic
    testing approaches.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了几种衡量模型输出和推理能力的公平性指标，接下来我们将继续学习实际检测偏差的技术，基于这些指标来开发系统性的测试方法。
- en: Detecting bias
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测偏差
- en: 'Detecting bias in LLMs often involves analyzing model outputs across different
    demographic groups or for different types of inputs. Here are some techniques:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型语言模型（LLMs）中检测偏差通常涉及分析不同人口群体或不同类型输入下的模型输出。以下是一些技术：
- en: '**Word embeddings**: This code measures gender bias in word embeddings by comparing
    the projection of profession words onto the gender direction:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词嵌入**：此代码通过比较职业词汇在性别方向上的投影来衡量词嵌入中的性别偏差：'
- en: '[PRE3]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This code measures gender bias in word embeddings by first creating average
    vectors for male and female terms, calculating a gender direction vector between
    them, and then measuring how closely different profession words align with this
    gender axis through dot product calculations. The function returns professions
    sorted by their bias score, where positive values indicate male association and
    negative values indicate female association, allowing users to quantify gender
    stereotypes embedded in the language model.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此代码通过首先为男性和女性术语创建平均向量，计算它们之间的性别方向向量，然后通过点积计算来衡量不同职业词汇与该性别轴的接近程度，从而衡量词嵌入中的性别偏差。该函数按偏差分数对职业进行排序，其中正值表示男性关联，负值表示女性关联，使用户能够量化嵌入在语言模型中的性别刻板印象。
- en: '**Sentiment analysis**: You can analyze sentiment across different groups to
    detect potential bias:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**：您可以分析不同群体中的情感以检测潜在的偏差：'
- en: '[PRE4]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code analyzes sentiment bias across different demographic groups by using
    a pre-trained sentiment analysis model from the `transformers` library. It takes
    a list of texts and their corresponding group labels, processes each text through
    a sentiment analyzer, and tallies positive and negative sentiment counts for each
    group. The function then calculates a “positive ratio” for each group (the proportion
    of texts classified as positive), allowing comparison of sentiment distribution
    across different groups. In the example, it’s specifically examining potential
    gender bias by analyzing how identical statements about intelligence and leadership
    are classified when attributed to men versus women, which could reveal if the
    underlying language model treats identical qualities differently based on gender
    association.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此代码通过使用来自`transformers`库的预训练情感分析模型，分析不同人口群体中的情感偏差。它接受一个文本列表及其相应的群体标签，通过情感分析器处理每个文本，并计算每个群体的正面和负面情感计数。然后，该函数为每个群体计算“正面比率”（被分类为正面的文本比例），允许比较不同群体之间的情感分布。在示例中，它特别通过分析关于智力和领导力的相同陈述在归因于男性还是女性时如何被分类来检查潜在的性别偏差，这可能揭示底层语言模型是否根据性别关联对相同品质进行不同的处理。
- en: '**Coreference resolution**: You can analyze coreference resolution to detect
    potential occupation-gender bias:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指代消解**：您可以分析指代消解来检测潜在的职业-性别偏差：'
- en: '[PRE5]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The code defines an `analyze_coreference_bias` function that uses spaCy’s NLP
    pipeline to assess potential gender bias in text by analyzing how often specific
    gendered pronouns (like “he” and “she”) co-occur with certain occupations (e.g.,
    “doctor”, “nurse”). It initializes a spaCy language model and creates a nested
    dictionary to count occurrences of each gender-occupation pair, as well as a separate
    count for each gender. For each input text, it tokenizes the content, identifies
    if any of the predefined occupations and gendered pronouns appear, and if both
    are present, it increments the relevant counters. After processing all texts,
    it normalizes the occupation counts for each gender by the total number of gender
    mentions, effectively yielding a proportion that reflects the relative association
    of each occupation with each gender in the given dataset. The function returns
    this normalized result, which is then printed in the example usage.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码定义了一个`analyze_coreference_bias`函数，该函数使用spaCy的NLP管道通过分析特定性别化代词（如“他”和“她”）与某些职业（例如，“医生”，“护士”）共现的频率来评估文本中的潜在性别偏见。它初始化一个spaCy语言模型，创建一个嵌套字典来计算每个性别-职业对的频率，以及每个性别的单独计数。对于每个输入文本，它将内容分词，确定是否有任何预定义的职业和性别化代词出现，如果两者都存在，则增加相关计数器。处理完所有文本后，它通过性别提及的总数对每个性别的职业计数进行归一化，从而有效地得到一个反映给定数据集中每个职业与每个性别相对关联比例的结果。该函数返回这个归一化结果，然后在示例用法中打印出来。
- en: Next, we’ll build on this detection knowledge to explore practical strategies
    for reducing bias, helping us move from diagnosis to treatment.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将基于这种检测知识来探讨减少偏差的实际策略，帮助我们从诊断转向治疗。
- en: Debiasing strategies
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去除偏差策略
- en: 'Debiasing LLMs is an active area of research. Here are some strategies:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 去除偏差的LLM是一个活跃的研究领域。以下是一些策略：
- en: '**Data augmentation** (see [*Chapter 3*](B31249_03.xhtml#_idTextAnchor049)):
    In the following code, we augment the dataset by swapping gendered words, helping
    to balance gender representation:'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据增强**（见[*第3章*](B31249_03.xhtml#_idTextAnchor049)）：在以下代码中，我们通过交换性别化的词汇来增强数据集，帮助平衡性别代表性：'
- en: '[PRE6]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Bias fine-tuning**: In the following code, we fine-tune a language model
    to replace biased words with more neutral alternatives:'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差微调**：在以下代码中，我们微调一个语言模型，用更中性的替代词替换有偏见的词汇：'
- en: '[PRE7]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Fairness-aware training
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 公平性感知训练
- en: Fairness constraints in machine learning are mathematical formulations that
    quantify and enforce specific notions of fairness by ensuring that model predictions
    maintain desired statistical properties across different demographic groups. These
    constraints typically express conditions such as demographic parity (equal positive
    prediction rates across groups), equalized odds (equal true positive and false
    positive rates), or individual fairness (similar individuals receive similar predictions).
    They can be incorporated directly into model optimization as regularization terms
    or enforced as post-processing steps. By explicitly modeling these constraints,
    developers can mitigate algorithmic bias and ensure more equitable outcomes across
    protected attributes like race, gender, or age—balancing the traditional goal
    of accuracy with ethical considerations about how predictive systems impact different
    populations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的公平性约束是数学公式，通过确保模型预测在不同人口群体中保持所需的统计特性，来量化并强制执行特定的公平性概念。这些约束通常表达条件，如人口比例（组间相同的正面预测率）、均衡机会（相同的真正率和假正率）或个人公平（相似的个人收到相似的预测）。它们可以直接作为正则化项纳入模型优化，或作为后处理步骤强制执行。通过明确建模这些约束，开发者可以减轻算法偏差，并确保在受保护属性（如种族、性别或年龄）方面有更公平的结果——在准确性的传统目标与预测系统对不同群体影响的相关伦理考量之间取得平衡。
- en: 'Incorporating fairness constraints directly into the training process can help
    produce fairer models. Here’s a simplified example:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 将公平性约束直接纳入训练过程可以帮助产生更公平的模型。以下是一个简化的例子：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This code implements a neural network classifier that aims to be fair with respect
    to protected attributes such as race or gender. The `FairClassifier` class defines
    a simple two-layer neural network, while the `fair_loss` function combines standard
    classification loss with a fairness constraint that penalizes the model when predictions
    differ between demographic groups. The `train_fair_model` function handles the
    training loop, applying this combined loss to optimize the model parameters while
    balancing accuracy and fairness.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码实现了一个神经网络分类器，旨在对受保护属性（如种族或性别）保持公平。`FairClassifier`类定义了一个简单的两层神经网络，而`fair_loss`函数将标准分类损失与一个公平约束相结合，当预测在不同人口群体之间有差异时，对模型进行惩罚。`train_fair_model`函数处理训练循环，应用这种组合损失来优化模型参数，同时平衡准确性和公平性。
- en: By incorporating a fairness penalty term in the loss function (weighted by `lambda_fairness`),
    the model is explicitly trained to make similar predictions across different protected
    groups, addressing potential bias. This represents a “constraint-based” approach
    to fair machine learning, where the fairness objective is directly incorporated
    into the optimization process rather than applied as a post-processing step. The
    trade-off between task performance and fairness can be tuned through the `lambda_fairness`
    hyperparameter.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在损失函数中（按`lambda_fairness`加权）引入公平性惩罚项，模型被明确训练以在不同受保护群体之间做出相似的预测，从而解决潜在的偏见。这代表了一种“基于约束”的公平机器学习方法，其中公平性目标直接纳入优化过程，而不是作为后处理步骤应用。可以通过`lambda_fairness`超参数调整任务性能与公平性之间的权衡。
- en: Ethical considerations
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 道德考虑因素
- en: 'Developing fair and unbiased LLMs is not just a technical challenge but also
    an ethical imperative. Some key ethical considerations include the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 开发公平且无偏见的LLMs不仅是一个技术挑战，也是一个道德上的必要。以下是一些关键的道德考虑因素：
- en: '**Transparency**: Be open about the model’s limitations and potential bias.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明度**：对模型的局限性和潜在偏见保持开放。'
- en: '**Diverse development teams**: Ensure diverse perspectives in the development
    process to help identify and mitigate potential bias.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多元化的开发团队**：确保开发过程中的多元化视角，以帮助识别和减轻潜在的偏见。'
- en: '**Regular auditing**: Implement regular bias and fairness audits of your LLM
    throughout its life cycle.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定期审计**：在整个生命周期内定期对你的LLM进行偏见和公平性审计。'
- en: '**Contextual deployment**: Consider the specific context and potential impacts
    of deploying your LLM in different applications.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情境部署**：考虑在不同应用中部署你的LLM的具体情境和潜在影响。'
- en: '**Ongoing research**: Stay informed about the latest research in AI ethics
    and fairness and continuously work to improve your models.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续研究**：了解AI伦理和公平的最新研究，并持续努力改进你的模型。'
- en: '**User education**: Educate users about the capabilities and limitations of
    your LLM, including potential bias.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户教育**：教育用户关于你的LLM的能力和局限性，包括潜在的偏见。'
- en: '**Feedback mechanisms**: Implement robust feedback mechanisms to identify and
    address unfair or biased outputs in deployed models. Keep in mind that feedback
    loops can reinforce bias by amplifying patterns in data, leading to self-perpetuating
    errors. If an AI system’s outputs influence future inputs—whether in content recommendations,
    hiring, or risk assessments—small biases can compound over time, narrowing diversity,
    reinforcing stereotypes, and skewing decision-making.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反馈机制**：实施强大的反馈机制以识别和解决部署模型中的不公平或偏见输出。请记住，反馈循环可能会通过放大数据中的模式来加强偏见，导致自我延续的错误。如果AI系统的输出影响未来的输入——无论是在内容推荐、招聘还是风险评估中——小的偏见随着时间的推移会累积，缩小多样性，加强刻板印象，并扭曲决策。'
- en: 'Here’s an example of how you might implement a simple feedback system:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个示例，说明你可能如何实现一个简单的反馈系统：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This code sets up a simple SQLite database to store user feedback on model outputs,
    which can be regularly reviewed to identify potential biases or issues.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码设置了一个简单的SQLite数据库来存储用户对模型输出的反馈，这些反馈可以定期审查，以识别潜在的偏见或问题。
- en: Summary
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about fairness and bias in LLMs, focusing on understanding
    different fairness definitions, such as demographic parity, equal opportunity,
    and equalized odds. We explored the types of bias that can emerge in LLMs, including
    representation, linguistic, allocation, quality of service, and stereotypical,
    along with techniques for detecting and quantifying them through metrics such
    as demographic parity difference and equal opportunity difference.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了LLMs中的公平性和偏见，重点关注理解不同的公平性定义，例如人口统计学平等、平等机会和均衡机会。我们探讨了LLMs中可能出现的偏见类型，包括代表性、语言、分配、服务质量以及刻板印象，以及通过人口统计学平等差异和机会平等差异等指标检测和量化这些偏见的技术。
- en: We used practical coding examples to show you how to analyze bias. Debiasing
    strategies such as data augmentation, bias-aware fine-tuning, and fairness-aware
    training were also covered, providing actionable ways to mitigate bias. Finally,
    we gained insights into ethical considerations, including transparency, diverse
    development teams, regular auditing, and user feedback systems. These skills will
    help you detect, measure, and address bias in LLMs while building more equitable
    and transparent AI systems.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过实际编码示例向您展示了如何分析偏见。还涵盖了去偏策略，如数据增强、偏见感知微调和公平感知训练，提供了减轻偏见的具体方法。最后，我们获得了对伦理考量的见解，包括透明度、多元化的开发团队、定期审计和用户反馈系统。这些技能将帮助您在构建更公平和透明的AI系统时检测、衡量和解决LLMs中的偏见。
- en: Keep in mind that fairness metrics in LLMs often conflict because they prioritize
    different aspects of equitable treatment. For example, *demographic parity* (equal
    outcomes across groups) can clash with *equalized odds*, which ensures similar
    false positive and false negative rates across groups, particularly when base
    rates differ. Similarly, *calibration* (ensuring predicted probabilities reflect
    actual outcomes) can contradict *equalized odds*, as a model that is well calibrated
    might still have unequal error rates. Additionally, *individual fairness* (treating
    similar individuals similarly) can be at odds with *group fairness*, which enforces
    equity across demographic groups, sometimes requiring differential treatment.
    These conflicts highlight the challenge of balancing fairness objectives in AI
    models.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，LLMs中的公平性指标往往存在冲突，因为它们优先考虑公平待遇的不同方面。例如，*人口统计学平等*（各组之间结果平等）可能与*均衡机会*相冲突，后者确保各组之间假阳性率和假阴性率相似，尤其是在基础率不同的情况下。同样，*校准*（确保预测概率反映实际结果）可能与*均衡机会*相矛盾，因为一个校准良好的模型可能仍然具有不平等的错误率。此外，*个体公平性*（对类似个体进行类似处理）可能与*群体公平性*相冲突，后者强制在人口统计群体之间实现公平，有时需要差别化处理。这些冲突突显了在AI模型中平衡公平性目标所面临的挑战。
- en: As we move forward, the next chapter will explore advanced prompt engineering
    techniques for LLMs.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们继续前进，下一章将探讨针对大型语言模型（LLMs）的高级提示工程技巧。
