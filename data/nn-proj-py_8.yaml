- en: What's Next?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一步是什么？
- en: We did it! We have created six different neural network projects, each with
    their own unique architecture. In this final chapter, let's recap on what we have
    accomplished. We will also look at some of the recent advancements in neural networks
    and deep learning that were not covered in previous chapters. Finally, we will
    peer ahead and see what the future holds for neural networks and AI in general.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做到了！我们创建了六个不同的神经网络项目，每个项目都有其独特的架构。在本章的最后，我们将回顾我们所取得的成就。我们还将看看一些近期在神经网络和深度学习方面的进展，这些进展在前几章中并未涉及。最后，我们将展望未来，看看神经网络和人工智能的前景。
- en: 'Specifically, these are the topics that we''ll cover in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，以下是我们将在本章中讨论的主题：
- en: A recap of the different neural networks that we used in this book
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本书中使用的不同神经网络回顾
- en: A recap of key neural network concepts
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络关键概念回顾
- en: Cutting edge advancements in neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的前沿进展
- en: The limitations of neural networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的局限性
- en: The future of AI and machine learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能与机器学习的未来
- en: Keeping up with machine learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟上机器学习的步伐
- en: Favorite machine learning tools
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最喜爱的机器学习工具
- en: What will you create?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将创造什么？
- en: Putting it all together
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将一切整合在一起
- en: We have accomplished a lot in this book. Let's do a quick recap of the projects
    that we have built in each chapter, as well as the neural network architecture
    enabling them. This section also serves as a quick refresher for the key neural
    network concepts that we have covered in this book.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中已经完成了很多工作。让我们快速回顾一下每一章中所构建的项目，以及支持它们的神经网络架构。本节也为我们在本书中涉及的关键神经网络概念提供了一个快速复习。
- en: Machine Learning and Neural Networks 101
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习与神经网络基础
- en: In [Chapter 1](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml), *Machine Learning
    and Neural Networks 101,* we started off by building the simplest, one-layer neural
    network, known as the **perceptron**. At its core, the perceptron is simply a
    mathematical function that takes in a set of input, performs some mathematical
    computation, and outputs the result of the computation. For the perceptron, the
    mathematical computation is simply the multiplication of the weights with the
    inputs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml)，*机器学习与神经网络基础*，我们从构建最简单的单层神经网络——**感知器**开始。从本质上讲，感知器只是一个数学函数，它接收一组输入，执行一些数学运算，然后输出计算结果。对于感知器，数学运算就是将权重与输入进行相乘。
- en: Therefore, the right set of weights dictates how well our neural network performs.
    At the start, the weights of the neural network are initialized randomly. The
    process of tuning the weights of our neural network to maximize model performance
    is called **model training**. During training, the weights of the neural network
    are continuously tuned to minimize the **loss function**. The loss function is
    simply a mathematical function that allows us to quantify how well our neural
    networks are doing. The algorithm that we use to adjust our weights to minimize
    the loss function is known as **gradient descent**.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正确的权重集决定了我们的神经网络性能的好坏。一开始，神经网络的权重是随机初始化的。调整神经网络的权重以最大化模型性能的过程叫做**模型训练**。在训练过程中，神经网络的权重不断被调整，以最小化**损失函数**。损失函数只是一个数学函数，允许我们量化神经网络的表现。我们用来调整权重以最小化损失函数的算法称为**梯度下降**。
- en: We created our very first neural network from scratch, without using machine
    learning libraries such as Keras or scikit-learn. We applied our simple neural
    network on a toy example, where the neural network had to learn binary (that is,
    1 or 0) predictions. We used a sum-of-squares error as the loss function to train
    our neural network, where the error is 1 if the prediction is wrong and the error
    is 0 if the prediction is correct. We then summed up the error across each individual
    point, giving us the sum-of-squares error. We saw that our neural network was
    able to learn from the training examples, producing accurate predictions for the
    testing data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从零开始创建了我们的第一个神经网络，没有使用像Keras或scikit-learn这样的机器学习库。我们将这个简单的神经网络应用于一个玩具示例，在这个示例中，神经网络需要学习二元（即1或0）预测。我们使用平方和误差作为损失函数来训练我们的神经网络，其中如果预测错误，误差为1，如果预测正确，误差为0。然后，我们对每个个体数据点的误差进行求和，得到平方和误差。我们看到我们的神经网络能够从训练示例中学习，并为测试数据提供准确的预测。
- en: Having understood the concepts of a neural network, we then discussed the most
    important libraries in Python for neural networks and machine learning in general.
    We saw how pandas is essential when we are working with tabular data (that is,
    from CSV files) and how it can also be used for data visualization. More importantly,
    we talked about Keras, the essential library in Python for working with neural
    networks and deep learning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了神经网络的概念后，我们接着讨论了 Python 中最重要的神经网络和机器学习库。我们了解到，在处理表格数据（即来自 CSV 文件的数据）时，`pandas`
    是不可或缺的工具，并且它也可以用于数据可视化。更重要的是，我们讨论了 Keras，这是在 Python 中进行神经网络和深度学习工作时的核心库。
- en: We talked about the fundamental building blocks in Keras, which are the `layers`.
    There are several types of `layers` in Keras, but the most important ones are
    the `convolutional` and `dense` layers, which are the building blocks of all neural
    networks that we covered in the book.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了 Keras 中的基本构建模块——`层`。Keras 中有多种类型的`层`，但最重要的两种是`卷积层`和`全连接层`，它们是我们书中讨论的所有神经网络的构建模块。
- en: Predicting Diabetes with Multilayer Perceptrons
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知机预测糖尿病
- en: In [Chapter 2](81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml), *Predicting Diabetes
    with Multilayer Perceptrons,* we kicked off our first project by creating a neural
    network that can predict whether a patient is at risk of diabetes. Specifically,
    we used a neural network known as the MLP to perform this classification prediction.
    We used the Pima Indians Diabetes dataset for this problem. The dataset consists
    of 768 different data points, with eight measurements (that is, features) and
    one label for each data point.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 2 章](81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml)，*使用多层感知机预测糖尿病*，我们通过创建一个神经网络，开始了第一个项目，目标是预测患者是否有糖尿病风险。具体来说，我们使用了一个称为
    MLP（多层感知机）的神经网络来进行这种分类预测。我们使用了 Pima 印第安人糖尿病数据集来进行这一问题的研究。该数据集包含 768 个不同的数据点，每个数据点有
    8 个特征和一个标签。
- en: As part of the machine learning workflow,we had to do data preprocessing before
    using this dataset with our neural network. We had to impute missing values, perform
    data standardization, and split our dataset into a training and testing dataset.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 作为机器学习工作流的一部分，在使用该数据集进行神经网络训练之前，我们需要进行数据预处理。我们需要填补缺失值、进行数据标准化，并将数据集分为训练集和测试集。
- en: We also built our very first neural network using Keras in this chapter. We
    saw how we can use the `Sequential` class in Keras to construct a neural network
    layer by layer, just like stacking Lego blocks on top of one another. We also
    looked at the **ReLU** and **sigmoid activation** functions, which are the two
    commonly used activation functions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们还用 Keras 构建了我们的第一个神经网络。我们展示了如何使用 Keras 中的 `Sequential` 类，一层一层地构建神经网络，就像将乐高积木一个个堆叠起来一样。我们还研究了
    **ReLU** 和 **sigmoid 激活** 函数，这两种是最常用的激活函数。
- en: We evaluated the performance of our neural network by using metrics such as
    the **confusion matrix** and the **ROC curve**, which are important tools to help
    us understand the performance of our neural network.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用 **混淆矩阵** 和 **ROC 曲线** 等度量指标，评估了神经网络的表现，这些工具对于帮助我们理解神经网络的性能非常重要。
- en: Predicting Taxi Fares with Deep Feedforward Nets
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度前馈神经网络预测出租车费用
- en: In [Chapter 3](bf157365-e4d3-42ae-89f4-58c9047e6500.xhtml), *Predicting Taxi
    Fares with Deep Feedforward Nets*, we used a **deep feedforward neural network**
    in a **regression** **prediction** problem, where the task was to predict the
    dollar amount of a NYC taxi fare, based on features such as pick-up and drop-off
    locations. In this project, we had to work with a noisy dataset that included
    missing data and data anomalies. We saw how data visualization is essential to
    help us identify outliers in the dataset, and to discover important trends in
    the dataset.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 3 章](bf157365-e4d3-42ae-89f4-58c9047e6500.xhtml)，*使用深度前馈神经网络预测出租车费用*，我们在一个
    **回归** **预测** 问题中，使用了 **深度前馈神经网络**，任务是预测纽约市出租车费用的美元金额，基于拾取和放下地点等特征。在这个项目中，我们需要处理一个包含缺失数据和数据异常的嘈杂数据集。我们了解到，数据可视化对于帮助我们识别数据集中的异常值，以及发现数据集中的重要趋势至关重要。
- en: This project was also the first project where feature engineering was done.
    Using the existing features, we created other features that improved the accuracy
    of our neural network. Finally, we created and trained a deep feedforward neural
    network in Keras by using our dataset, which produced an impressive mean square
    error of 3.50.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目也是第一次进行特征工程的项目。利用现有特征，我们创建了其他特征，提升了神经网络的准确性。最后，我们使用自己的数据集，在 Keras 中创建并训练了一个深度前馈神经网络，最终得到了令人印象深刻的均方误差
    3.50。
- en: Cats Versus Dogs – Image Classification Using CNNs
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 猫狗大战 – 使用 CNN 进行图像分类
- en: In [Chapter 4](48f7db0c-7c74-42a2-82b7-a17c9f220423.xhtml), *Cats Versus Dogs –
    Image Classification Using CNNs*, we started our first neural network project
    in the domain of image recognition and computer vision. Specifically, we created
    a CNN that is able to classify images of cats and dogs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 4 章](48f7db0c-7c74-42a2-82b7-a17c9f220423.xhtml)，*猫狗大战 – 使用 CNN 进行图像分类*，我们开始了第一个图像识别和计算机视觉领域的神经网络项目。具体来说，我们创建了一个能够分类猫狗图像的
    CNN。
- en: We saw how digital images are essentially two-dimensional arrays (for grayscale
    images), with each array value representing the intensity of each pixel. CNNs are
    the go-to neural network architecture for most image recognition problems. The
    **filtering** and **convolution** operation in the CNN is used to identify important
    spatial features in the images, which makes it suitable for image recognition
    problems. CNNs have gone through several iterations and improvements over the
    years. LeNet first came to the scene in 1998, before more sophisticated neural
    networks such as the VGG16 and ResNet were developed in the 2010s.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到数字图像本质上是二维数组（对于灰度图像），每个数组值代表每个像素的强度。CNN 是解决大多数图像识别问题的首选神经网络架构。CNN 中的 **过滤**
    和 **卷积** 操作用于识别图像中的重要空间特征，这使得它非常适用于图像识别问题。多年来，CNN 经历了多次迭代和改进。LeNet 在 1998 年首次出现，随后更复杂的神经网络如
    VGG16 和 ResNet 在 2010 年代相继开发出来。
- en: We created our own CNN in Keras, and we used Keras's `ImageDataGenerator` and
    the `flow_from_directory` method to train a neural network when the dataset (images
    of cats and dogs) was too large to fit into memory in one go. The simple CNN that
    we created achieved an accuracy of 80%. We also used **transfer learning **to
    leverage on a pre-trained VGG16 neural network for the cats-and-dogs classification
    problem. This method showed the sophistication of the VGG16 model, achieving an accuracy
    of 90%.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Keras 中创建了自己的 CNN，并使用 Keras 的 `ImageDataGenerator` 和 `flow_from_directory`
    方法来训练神经网络，当数据集（猫狗图片）太大，无法一次性加载到内存时。我们创建的简单 CNN 达到了 80% 的准确率。我们还使用了 **迁移学习**，利用预训练的
    VGG16 神经网络来解决猫狗分类问题。这种方法展示了 VGG16 模型的复杂性，达到了 90% 的准确率。
- en: Removing Noise from Images Using Autoencoders
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自编码器去除图像噪声
- en: In [Chapter 5](16d0775b-23ec-456a-a57b-cba2e9da7570.xhtml), *Removing Noise
    from Images Using Autoencoders*, we looked at **autoencoders**, a special class
    of neural networks that learns a **latent representation** of the input. Autoencoders
    have an **encoder** component that compresses the input into a latent representation,
    and a **decoder** component that reconstructs the input using the latent representation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 5 章](16d0775b-23ec-456a-a57b-cba2e9da7570.xhtml)，*使用自编码器去除图像噪声*，我们研究了 **自编码器**，一种特殊的神经网络，它学习输入的
    **潜在表示**。自编码器有一个 **编码器** 部件，它将输入压缩为潜在表示，还有一个 **解码器** 部件，它使用潜在表示重建输入。
- en: In an autoencoder, the size of the hidden layer that's used for the latent representation
    is an important hyperparameter that needs to be tuned carefully. The size of the
    latent representation should be sufficiently small enough to represent a compressed
    representation of the input features, and also sufficiently *large* enough for
    the decoder to reconstruct the input without too much loss. We trained an autoencoder
    to compress MNIST images. We showed that by using a hidden layer size of 32 ×
    1, and we achieved a compression rate of 24.5, while ensuring that the reproduced
    images were similar to the original input images.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在自编码器中，用于潜在表示的隐藏层大小是一个重要的超参数，需要仔细调整。潜在表示的大小应该足够小，以表示输入特征的压缩表示，同时也要足够 *大*，使得解码器能够重建输入，而不会有太多损失。我们训练了一个自编码器来压缩
    MNIST 图像。我们通过使用 32 × 1 的隐藏层大小，达到了 24.5 的压缩率，同时确保重建的图像与原始输入图像相似。
- en: We also looked at using autoencoders for **image** **denoising**. By using a
    noisy image as the input and a clean image as the output, we can train an autoencoder
    to identify features of the image that do not belong to noise. Thus, we can apply
    the autoencoder to remove noise from images. Such autoencoders are known as **denoising
    autoencoders**. We trained and applied a denoising autoencoder to the noisy office
    documents dataset, which consists of scanned images of dirty office documents.
    Using deep convolutional layers within the denoising autoencoder, we managed to
    remove noise almost entirely from the office documents.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还研究了如何使用自编码器进行**图像** **去噪**。通过使用噪声图像作为输入，清晰图像作为输出，我们可以训练自编码器识别不属于噪声的图像特征。这样，我们就可以应用自编码器从图像中去除噪声。这样的自编码器被称为**去噪自编码器**。我们训练并应用了一个去噪自编码器，针对包含脏污办公文件扫描图像的去噪办公文件数据集。在去噪自编码器中使用深度卷积层后，我们成功地几乎完全去除了办公文件中的噪声。
- en: Sentiment Analysis of Movie Reviews Using LSTM
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LSTM进行电影评论情感分析
- en: In [Chapter 6](21ef7df7-5976-4e0d-bec5-d736ec571d94.xhtml), *Sentiment Analysis
    of Movie Reviews Using LSTM*, we looked at sentiment analysis, which is a sequential
    problem in the domain of **Natural Language Processing** (**NLP**).We saw that
    sentiment analysis is a challenging problem even for humans, because words convey
    different meanings in different contexts. RNNs are thought to be the best form
    of neural networks for tackling sequential problems such as sentiment analysis.
    However, conventional recurrent neural networks suffer from a long-term dependency
    problem, which makes it unsuitable for lengthy bodies of text.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第六章](21ef7df7-5976-4e0d-bec5-d736ec571d94.xhtml)《使用LSTM进行电影评论情感分析》中，我们探讨了情感分析，它是**自然语言处理**（**NLP**）领域中的一个序列问题。我们看到，情感分析对人类来说也是一个具有挑战性的问题，因为词语在不同语境中传递的意义不同。RNN被认为是处理情感分析等序列问题的最佳神经网络形式。然而，传统的递归神经网络存在长期依赖问题，这使得它不适用于处理长篇文本。
- en: A variation of recurrent neural networks, known as the LSTM network, was designed
    to overcome the long-term dependency problem. The intuition behind LSTMs is that
    because of its ability to assign weights to certain words, we can selectively
    forget words that are less important and remember words that are more important.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一种变种的递归神经网络，称为LSTM网络，旨在克服长期依赖问题。LSTM的直觉是，由于其能够为特定词语分配权重，我们可以有选择性地忘记不重要的词语，而记住更重要的词语。
- en: We also looked at how we can represent words as vectors using word embeddings.
    Word embeddings transform words into a lower-dimensional feature space, placing
    words that are similar near to one another, while words that are dissimilar are
    placed further away.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还研究了如何通过词嵌入将词语表示为向量。词嵌入将词语转化为较低维度的特征空间，将相似的词语放置在彼此靠近的位置，而将不相似的词语放置得更远。
- en: We created and trained an LSTM network in Keras for sentiment analysis on the
    IMDB movie reviews dataset, and we looked at some of the important hyperparameters
    to tune while training an LSTM net. In particular, we saw how the optimizer makes
    a significant difference in the performance of the LSTM network. Our final LSTM
    network achieves 85% accuracy in classifying the sentiment of IMDB movie reviews.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Keras中创建并训练了一个LSTM网络，用于对IMDB电影评论数据集进行情感分析，同时研究了一些在训练LSTM网络时需要调整的重要超参数。特别是，我们看到了优化器对LSTM网络性能的显著影响。我们最终的LSTM网络在分类IMDB电影评论情感时达到了85%的准确率。
- en: Implementing a Facial Recognition System with Neural Networks
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络实现面部识别系统
- en: In [Chapter 7](9223bc03-bd68-42df-93ff-32c5d0f7e246.xhtml), *Implementing a
    Facial Recognition System with Neural Networks*, we created a facial recognition
    system using Siamese neural networks. Siamese neural networks are a special class
    of neural networks, with a shared, conjoined component. Siamese neural networks
    accept a pair of images as input, and can be trained to output a distance that
    is inversely proportional to the similarity of the two images. This forms the
    idea behind using Siamese neural networks for facial recognition. If the two faces
    in the input pair of images belong to the same subject, then the distance output
    should be small, and vice versa. By training a Siamese neural network on positive
    pairs (faces that belong to the same subject) and negative pairs (faces that belong
    to different subjects), using contrastive loss, it will eventually learn to output
    an appropriate distance for positive and negative pairs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](9223bc03-bd68-42df-93ff-32c5d0f7e246.xhtml)《*使用神经网络实现人脸识别系统*》中，我们使用Siamese神经网络创建了一个人脸识别系统。Siamese神经网络是一类特殊的神经网络，具有共享的、联结的组件。Siamese神经网络接受一对图像作为输入，并可以训练输出与两张图像相似度成反比的距离。这构成了使用Siamese神经网络进行人脸识别的基本思路。如果输入图像对中的两张人脸属于同一人物，那么输出的距离应该很小，反之亦然。通过使用对比损失训练Siamese神经网络，输入正对（属于同一人物的人脸）和负对（属于不同人物的人脸），它最终会学会为正对和负对输出适当的距离。
- en: We also looked at face detection, which is an important precursor to facial
    recognition. Face detection is used to isolate and extract faces from raw images,
    which is then passed to our neural network for face recognition. Face detection
    is commonly done using the Viola-Jones algorithm, which uses Haar features to
    detect facial features in images. To create our facial recognition system, we
    combined OpenCV, which uses the video stream from a computer's webcam for face
    detection, and the Siamese neural network that we trained for face recognition.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了人脸检测，它是人脸识别的重要前提。人脸检测用于从原始图像中隔离和提取人脸，提取后的图像会传递给神经网络进行人脸识别。人脸检测通常使用Viola-Jones算法，它利用Haar特征来检测图像中的面部特征。为了创建我们的人脸识别系统，我们结合了OpenCV，它利用计算机摄像头的视频流进行人脸检测，以及我们训练的人脸识别Siamese神经网络。
- en: Cutting edge advancements in neural networks
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的前沿进展
- en: As we saw in the previous section, we covered a lot of material in this book.
    However, the possibilities of neural networks are truly boundless. There are other
    important types of neural networks that we have not yet discussed in this book.
    For completeness, we shall discuss them in this section. As you shall see, these
    neural networks are very different than what we have seen so far, and it should
    provide you with a new perspective.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中看到的，本书涉及了很多内容。然而，神经网络的可能性实际上是无穷无尽的。还有一些重要的神经网络类型，我们在本书中还没有讨论。为了完整起见，我们将在本节中讨论它们。正如你将看到的，这些神经网络与我们到目前为止看到的非常不同，它应该会为你提供一个全新的视角。
- en: Generative adversarial networks
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: '**Generative adversarial networks** (**GANs**) are a class of generative neural
    networks. To understand generative models, it''s important to contrast them against
    discriminativemodels. So far in this book, we have focused only on discriminative
    models. Discriminative models are concerned with learning the mapping of features
    to a label. For example, when we created a CNN to classify images of cats and
    dogs, the CNN is a discriminative model that learns the mapping of features (images)
    to a label (a cat or a dog).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成对抗网络**（**GANs**）是一类生成神经网络。要理解生成模型，重要的是将它们与判别模型进行对比。在本书之前的内容中，我们只关注了判别模型。判别模型关注学习特征到标签的映射。例如，当我们创建一个卷积神经网络（CNN）来分类猫狗图像时，CNN是一个判别模型，它学习特征（图像）到标签（猫或狗）的映射。'
- en: On the other hand, generative models are concerned with generating appropriate
    features, given the label. For example, given labeled images of cats and dogs,
    a generative model would learn to create the appropriate features for each label.
    In other words, a generative model learns to synthesize images of cats and dogs!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，生成模型关注于在给定标签的情况下生成适当的特征。例如，给定标注为猫和狗的图像，生成模型将学习为每个标签生成适当的特征。换句话说，生成模型学会合成猫狗的图像！
- en: GANs are one of the most exciting developments in AI in recent years. In fact,
    Yann LeCun called GANs *the most interesting idea in machine learning in the past
    10 years.* So, how do GANs work? Intuitively, GANs consist of two components—the
    generator and the discriminator. The role of the generator is to generate features
    (such as images), and the role of the discriminator is to evaluate how well the
    generated features represent the original features. When we train the GAN, we
    pit the generator against the discriminator (hence the term *adversarial* in GAN).
    Eventually, the generator will become so good that the discriminator can no longer
    differentiate the generated features from the original features and the GAN can
    now generate lifelike images.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: GAN是近年来人工智能领域最令人兴奋的发展之一。事实上，Yann LeCun曾称GAN是*过去10年中机器学习最有趣的想法*。那么，GAN是如何工作的呢？直观地讲，GAN由两个部分组成——生成器和判别器。生成器的作用是生成特征（例如图像），判别器的作用是评估生成的特征与原始特征的相似度。在训练GAN时，我们将生成器与判别器对立起来（这也是GAN中“*对抗*”一词的来源）。最终，生成器会变得如此强大，以至于判别器无法区分生成的特征与原始特征，GAN就能生成栩栩如生的图像。
- en: 'To have a sense of how good GANs have become, check out the following paper,
    which was released by researchers from NVIDIA:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解GAN的强大程度，请查看以下由NVIDIA研究人员发布的论文：
- en: '[https://arxiv.org/pdf/1812.04948.pdf](https://arxiv.org/pdf/1812.04948.pdf)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1812.04948.pdf](https://arxiv.org/pdf/1812.04948.pdf)'
- en: Within the paper, you'll see some samples of faces generated by GANs that are
    indistinguishable from real human faces. GANs have improved at such a frightening
    pace that we can now generate hyper-realistic human faces.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，你将看到一些由GAN生成的面孔样本，这些面孔与真实的人类面孔无法区分。GAN的进步速度令人惊讶，现在我们已经能够生成超真实的人类面孔。
- en: GANs have been applied to several interesting use cases. For example, researchers
    have found a way to use GANs in style transfer. In style transfer, GANs learn
    the artistic style of a given image, and apply it to another image. For example,
    we can use GANs to learn the artistic style of Vincent van Gough's famous *The
    Starry Night* painting, and apply it to any arbitrary image. Check out the GitHub
    repository at [https://github.com/jcjohnson/neural-style](https://github.com/jcjohnson/neural-style) for
    examples of style transfer.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: GAN已被应用于几个有趣的案例。例如，研究人员已找到一种将GAN应用于风格迁移的方法。在风格迁移中，GAN学习给定图像的艺术风格，并将其应用到另一幅图像上。例如，我们可以使用GAN来学习文森特·梵高著名的*星夜*画作的艺术风格，并将其应用于任何任意图像。可以访问[https://github.com/jcjohnson/neural-style](https://github.com/jcjohnson/neural-style)查看风格迁移的示例。
- en: Deep reinforcement learning
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习
- en: Reinforcement learning is a branch of machine learning that learns the best
    action to take in any given state to maximize future rewards. Reinforcement learning
    has been applied to games such as chess. In chess, the position of the pieces
    on the chessboard represents the state that we are in. The role of reinforcement
    learning in this case is to learn the best action to take (that is, the pieces
    to move) in any given state.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是机器学习的一个分支，旨在学习在任何给定状态下采取最佳行动，以最大化未来的奖励。强化学习已被应用于象棋等游戏。在象棋中，棋盘上棋子的布局代表了我们所处的状态。强化学习的作用是学习在任何给定状态下应采取的最佳行动（即，应该移动哪些棋子）。
- en: If we think of the best action to take in any arbitrary state being represented
    by a mathematical function (call it the action-value function), then we can use
    neural networks to learn this action-value function. Once the function has been
    learned, our neural network can be used to predict the best action to take in
    any given state—essentially, our neural network becomes an unbeatable chess player!
    The application of deep neural networks to reinforcement learning is known as
    deep reinforcement learning.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将任何任意状态下应采取的最佳行动表示为一个数学函数（称之为行动价值函数），那么我们可以使用神经网络来学习这个行动价值函数。一旦这个函数被学习出来，我们的神经网络就可以用来预测在任何给定状态下应该采取的最佳行动——本质上，我们的神经网络就成了一个无敌的棋手！将深度神经网络应用于强化学习的过程称为深度强化学习。
- en: Deep reinforcement learning has found much success in game-playing. In 2017,
    AlphaGo, an AI game-playing agent trained using deep reinforcement learning, managed
    to beat Ke Jie, one of the best Go players in the world. AlphaGo's victory sparked
    much fanfare and discussion over the future of AI.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习在游戏中取得了很多成功。2017年，使用深度强化学习训练的AI游戏玩家AlphaGo成功战胜了世界顶级围棋选手之一柯洁。AlphaGo的胜利引发了广泛的关注和讨论，尤其是关于人工智能未来的讨论。
- en: In 2018, deep reinforcement learning took another leap when OpenAI Five (a team
    of five neural networks) managed to beat amateur human players at Dota 2\. Dota
    2 is a multiplayer online game once thought to be beyond the realm of AI, due
    to the immense complexity and dynamism of the game. Professional Dota 2 players
    are celebrities in their own right, with legions of fans admiring the speed of
    thought and lighting-fast reactions possessed by the best Dota 2 players in the
    world. Today, OpenAI Five continuously pushes the boundary of what it can do in
    Dota 2\. OpenAI Five trains itself by playing 180 years' worth of games against
    itself every day. OpenAI Five views the the state of the game as an array of 20,000
    numbers, and from there, it decides on the best action to take.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，深度强化学习迎来了又一次飞跃，当时 OpenAI Five（由五个神经网络组成的团队）成功击败了 Dota 2 的业余玩家。Dota 2 是一款多人在线游戏，曾被认为是人工智能无法攻克的领域，原因是游戏的复杂性和动态性极高。职业
    Dota 2 玩家是公认的明星，全球顶尖 Dota 2 玩家因其思维敏捷和反应迅速而深受粉丝喜爱。如今，OpenAI Five 不断突破其在 Dota 2
    中的能力边界。OpenAI Five 每天通过自我对战进行训练，积累 180 年的游戏经验。OpenAI Five 将游戏状态视为由 20,000 个数字组成的数组，然后从中决定采取最佳行动。
- en: 'To learn more about OpenAI Five, as well as to try out an interactive demo,
    do visit OpenAI''s website:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 OpenAI Five 的信息，并尝试互动演示，请访问 OpenAI 的官网：
- en: '[https://blog.openai.com/openai-five/](https://blog.openai.com/openai-five/)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://blog.openai.com/openai-five/](https://blog.openai.com/openai-five/)'
- en: Beyond game-playing, deep reinforcement learning has made important contributions
    to autonomous vehicles as well. Autonomous vehicles abstract the world around
    them using computer vision algorithms. This abstraction represents the state that
    the vehicle is in. From there, deep reinforcement learning selects the best action
    to take (for example, accelerate, brake), according to its state.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 除了游戏玩法，深度强化学习也为自动驾驶汽车做出了重要贡献。自动驾驶汽车通过计算机视觉算法来抽象周围的世界。这种抽象表示了车辆当前的状态。然后，深度强化学习根据车辆的状态选择最佳行动（例如，加速或刹车）。
- en: Limitations of neural networks
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的局限性
- en: The possibilities of a neural network may seem boundless, but there are in fact
    limitations as to what neural networks and machine learning in general can achieve.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的潜力似乎是无限的，但实际上，神经网络和机器学习在一般情况下也有其局限性。
- en: First of all, neural networks have poor interpretability. In other words, neural
    networks often function as black-box algorithms, and it is difficult to *interpret*
    the results produced by a neural network. Take for example our project in [Chapter
    2](81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml), *Predicting Diabetes with Multilayer
    Perceptrons*, where we used a neural network to predict patients at risk of developing
    diabetes. The neural network takes in input, such as blood glucose level, blood
    pressure, age, and so on, and outputs a prediction of whether the patient is at
    risk of developing diabetes. Even though the neural network is able to make such
    a prediction with high accuracy, we do not actually know what are the factors
    that influence the predictions. This may be insufficient for a doctor, who may
    wish to create an intervention plan for the patient.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，神经网络的可解释性较差。换句话说，神经网络通常作为黑箱算法工作，难以*解释*神经网络产生的结果。以我们在[第二章](81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml)中的项目《使用多层感知机预测糖尿病》为例，我们使用神经网络预测患糖尿病风险的患者。神经网络输入数据，如血糖水平、血压、年龄等，并输出患者是否有糖尿病风险的预测。尽管神经网络能够高精度地做出预测，但我们实际上并不清楚哪些因素影响这些预测。这对于医生来说可能是不够的，因为他们可能希望为患者制定干预计划。
- en: When applied in a real-world setting, this lack of interpretability is a real
    concern for business users, who may be uncomfortable with deploying a black-box
    algorithm. Beyond model performance, business users would also like to know why
    the model works, and what factors influence a target variable that the business
    is concerned with.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，这种缺乏可解释性的问题对商业用户来说是一个真实的担忧，因为他们可能不愿意使用黑箱算法。除了模型的性能之外，商业用户还希望了解模型的工作原理，以及哪些因素影响着与业务相关的目标变量。
- en: Improving the interpretability of neural networks is one of the areas that researchers
    are working on. In particular, researchers are working on producing interpretable
    results when deep neural networks are applied to computer vision problems. To
    that end, some researchers have proposed to reduce the convolutional layers of
    a CNN to a graphical model, which represents the semantic hierarchy hidden inside
    the neural network.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 提高神经网络可解释性是研究人员正在努力的领域之一。特别是，研究人员正在致力于在深度神经网络应用于计算机视觉问题时，生成可解释的结果。为此，一些研究人员提出将CNN的卷积层减少到图形模型，表示神经网络内部隐藏的语义层次结构。
- en: The second limitation of neural networks is that they can be easily fooled when
    applied to image recognition. In [Chapter 4](48f7db0c-7c74-42a2-82b7-a17c9f220423.xhtml), *Cats
    Versus Dogs – Image Classification Using CNNs*, we achieved a high accuracy (90%)
    when CNNs were used to classify images of cats and dogs. While CNNs are considered
    state-of-the-art for image recognition, their Achilles' heel is that they can
    be easily fooled by a malicious agent.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的第二个局限性是，当应用于图像识别时，它们很容易被欺骗。在[第4章](48f7db0c-7c74-42a2-82b7-a17c9f220423.xhtml)，*猫与狗—使用卷积神经网络进行图像分类*中，我们在使用卷积神经网络（CNN）对猫和狗的图像进行分类时，达到了高精度（90%）。虽然CNN被认为是图像识别领域的最先进技术，但它们的致命弱点是容易受到恶意代理的欺骗。
- en: 'A recent study by Nguyen and others showed that because neural networks perceive
    images differently than humans, an image that is completely unrecognizable to
    humans can be used to fool neural networks, leading neural networks to produce
    erroneous predictions. For examples of these synthetic images that are unrecognizable
    to humans, but can be used to fool neural networks, check out the paper from Nguyen
    and others:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 近日，Nguyen等人的一项研究表明，由于神经网络感知图像的方式与人类不同，一张人类完全无法识别的图像可以用来欺骗神经网络，从而导致神经网络做出错误预测。有关这些对人类无法识别，但可以用来欺骗神经网络的合成图像的示例，请参阅Nguyen等人的论文：
- en: '[https://arxiv.org/pdf/1412.1897.pdf](https://arxiv.org/pdf/1412.1897.pdf)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1412.1897.pdf](https://arxiv.org/pdf/1412.1897.pdf)'
- en: Furthermore, researchers showed that by combining these synthetic images with
    an existing image in an imperceptible way to humans, neural networks can be fooled
    to produce a wrong prediction.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，研究人员表明，通过将这些合成图像以人类无法察觉的方式与现有图像结合，神经网络可以被欺骗，从而产生错误的预测。
- en: This finding has a significant impact on the feasibility of using neural networks
    in computer vision based security systems. A malicious agent could possibly provide
    a carefully handcrafted input image to the neural network, fooling it, and bypassing
    the security system.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这一发现对使用神经网络的计算机视觉安全系统的可行性产生了重大影响。恶意代理有可能向神经网络提供精心制作的输入图像，从而欺骗神经网络，绕过安全系统。
- en: It is clear that neural networks are far from perfect, and they are certainly
    not the magic solution to all our problems. However, there are reasons to be optimistic,
    as new breakthroughs are constantly being discovered every day that improve our
    understanding of neural networks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 很显然，神经网络远非完美，它们绝不是解决我们所有问题的魔法方案。然而，仍然有理由保持乐观，因为每天都有新的突破不断被发现，这些突破提高了我们对神经网络的理解。
- en: The future of artificial intelligence and machine learning
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能和机器学习的未来
- en: 'Next, let''s discuss the future of AI and machine learning in general. In my
    opinion, we will see the rise of the following key developments over the next
    few decades:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论一下人工智能和机器学习的未来。依我看，在未来几十年内，我们将看到以下几个关键发展的崛起：
- en: Artificial general intelligence
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工通用智能
- en: Automated machine learning
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化机器学习
- en: Artificial general intelligence
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工通用智能
- en: '**Artificial general intelligence** (**AGI**) is defined as *an artificial
    intelligence agent with the ability to perform any intellectual task that a human
    being can*. Some researchers have made the distinction of weak AI versus strong
    AI, with weak AI being used to describe the level of AI today. AI agents today
    are mostly concerned with performing a single task. For example, we train AI agents
    to predict whether a patient is at risk of diabetes, and another AI agent to classify
    images of cats and dogs. These AI agents are separate entities, and an AI agent
    that is trained to perform a certain task cannot be used to perform other tasks.
    This narrow view of AI is termed weak AI.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工通用智能**（**AGI**）被定义为*一种人工智能代理，具有执行任何人类能够完成的智力任务的能力*。一些研究者区分了弱人工智能与强人工智能，其中弱人工智能用来描述当今的AI水平。现在的AI代理主要专注于执行单一任务。例如，我们训练AI代理预测患者是否有糖尿病风险，另一个AI代理则用来分类猫和狗的图像。这些AI代理是独立的，训练来执行某一特定任务的AI代理不能用来执行其他任务。这种狭隘的AI视角被称为弱人工智能。  '
- en: 'On the other hand, strong AI refers to generalized AI agents that can perform
    any task. A strong AI agent could be something like a self-conscious, human-like
    AI assistant. At the moment, strong AI belongs to the realm of science fiction.
    In my opinion, the current machine learning algorithms at our disposal (for example,
    neural networks, decision trees) will not be sufficient to achieve AGI. In the
    words of Francois Chollet (the developer of Keras):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，强人工智能指的是能够执行任何任务的通用AI代理。一个强人工智能代理可能是一个自我意识的、类人化的AI助手。目前，强人工智能还属于科幻领域。在我看来，目前我们掌握的机器学习算法（例如神经网络、决策树）不足以实现AGI。正如**弗朗索瓦·肖莱**（Keras的开发者）所说：
- en: '*"You cannot achieve general intelligence simply by scaling up today''s deep
    learning techniques."'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*“仅仅通过扩大现有的深度学习技术，无法实现通用智能。”*  '
- en: - Francois Chollet*
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '- **弗朗索瓦·肖莱**  '
- en: It will take a significant breakthrough to attain AGI—the kind of breakthrough
    that once saw neural networks and deep learning define the *weak AI* that we know
    today.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '要实现AGI，需要一个重大的突破——类似于神经网络和深度学习曾经定义了我们今天所知的*弱人工智能*的那种突破。  '
- en: Automated machine learning
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '自动化机器学习  '
- en: Even though a data scientist has been termed *the sexiest job of the 21st century*,
    the reality is that most data scientists spend a disproportionate amount of time
    on time-consuming tasks such as data preprocessing and hyperparameter tuning.
    To address this issue, companies such as Google are developing tools to automate
    the machine learning process. Google has recently launched **AutoML**, a solution
    that uses neural nets to design neural nets. Google believes that AutoML can package
    the expertise that is currently possessed by data scientists and provide that
    expertise on demand as a service on the cloud.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管数据科学家被称为**21世纪最性感的职业**，但现实情况是，大多数数据科学家花费了大量的时间在一些费时的任务上，如数据预处理和超参数调优。为了应对这个问题，像谷歌这样的公司正在开发工具来自动化机器学习过程。谷歌最近推出了**AutoML**，这是一种使用神经网络设计神经网络的解决方案。谷歌认为，AutoML可以将目前数据科学家所拥有的专业知识进行打包，并通过云端按需提供这些专业知识作为服务。  '
- en: Of course, some data scientists have taken offense at the thought that they
    could one day be replaced by AI, and they claim that automated machine learning
    would never become a reality. My personal opinion is that the truth is somewhere
    in-between. Today, there are already libraries in Python that can help us automate
    some of the more time-consuming tasks, such as hyperparameter tuning. Such libraries
    can brute-force their way through a range of hyperparameters, selecting the set
    of hyperparameters that maximizes our results. There are even libraries in Python
    that can visualize a dataset automatically, plotting the most relevant graphs
    automatically. As such libraries become more mainstream, I believe that data scientists
    will spend less time on such time-consuming activities, and more time on other
    impactful tasks, such as model design and feature engineering.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '当然，一些数据科学家对他们可能会被AI取代的想法感到不满，并声称自动化机器学习永远不可能成为现实。我的个人观点是，真相介于两者之间。如今，已经有一些Python库可以帮助我们自动化一些更为耗时的任务，比如超参数调优。这些库可以通过暴力搜索一系列超参数，选择最大化结果的一组超参数。甚至有一些Python库可以自动可视化数据集，自动绘制最相关的图表。随着这些库的逐步普及，我认为数据科学家将花费更少的时间在这些繁琐的活动上，更多的时间将用于其他有影响力的任务，如模型设计和特征工程。  '
- en: Keeping up with machine learning
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '跟上机器学习的步伐  '
- en: The field of machine learning and AI is constantly evolving, and new knowledge
    is constantly being discovered. How do we keep ourselves updated in this ever-changing
    field? Personally, I keep myself updated by reading books, scientific journals,
    and practicing on real datasets.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习和人工智能领域在不断发展，新的知识不断被发现。我们如何在这个不断变化的领域保持更新呢？就个人而言，我通过阅读书籍、科学期刊以及在真实数据集上进行实践来保持更新。
- en: Books
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 书籍
- en: The fact that you are reading this book shows that you are committed to improving
    your knowledge! Unfortunately, we cannot cover every single topic of machine learning
    in this book. If you enjoyed this book, you may wish to refer to the catalog of
    books that Packt has. You will find that Packt has books on nearly every single
    topic in machine learning. The Packt team also ensures that the reader stays up
    to date with the latest developments by continuously publishing books on the latest
    technologies in machine learning.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在阅读这本书，说明你已经致力于提升自己的知识！但遗憾的是，我们无法在本书中涵盖所有机器学习的每个话题。如果你喜欢本书，你可能会想参考 Packt 的图书目录。你会发现
    Packt 在几乎每一个机器学习话题上都有书籍。Packt 团队还通过不断出版关于最新技术的书籍，确保读者能够与机器学习领域的最新发展保持同步。
- en: Packt's catalog can be found at [https://www.packtpub.com/all](https://www.packtpub.com/all).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Packt 的目录可以在 [https://www.packtpub.com/all](https://www.packtpub.com/all) 找到。
- en: Scientific journals
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 科学期刊
- en: 'AI researchers have always believed in openness. They believe that knowledge
    should be freely shared and that the best way to grow as a community is by sharing.
    Therefore, most of the cutting-edge scientific papers in AI and machine learning
    can be found freely online. In particular, most AI researchers share their findings
    on the following website:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能研究人员一直相信开放共享。他们认为知识应该自由共享，社区成长的最佳方式就是通过分享。因此，大多数前沿的人工智能和机器学习科学论文都可以在网上免费找到。尤其是，许多
    AI 研究人员会在以下网站分享他们的研究成果：
- en: '[https://arxiv.org](https://arxiv.org)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org](https://arxiv.org)'
- en: '**Arxiv** is an open-access depository for scientific journals. Most of the
    cutting-edge findings are shared freely on arxiv as soon as they are available.
    This results in rapid development, with ideas being built on top of one another
    iteratively.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**Arxiv** 是一个开放获取的科学期刊存储库。大多数前沿研究成果一经发布，便会自由分享在 arxiv 上。这促使了快速的发展，思想不断在上一个基础上迭代。'
- en: Practicing on real-world datasets
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在真实数据集上练习
- en: Lastly, as machine learning practitioners, it is important to keep our skills
    sharp by practicing often. **Kaggle** is a website that hosts data science competitions
    by using real-world datasets. There are different levels of competitions, so beginners
    and experts can find something suitable for their level of ability. The type of
    dataset also varies from tabular data, images (computer vision problems), and
    text (NLP problems).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，作为机器学习从业者，保持我们的技能锋利至关重要，定期练习是非常必要的。**Kaggle** 是一个举办数据科学竞赛的网站，使用真实的世界数据集。竞赛有不同的难度级别，初学者和专家都可以找到适合自己水平的内容。数据集的类型也各不相同，从表格数据到图像（计算机视觉问题），再到文本（自然语言处理问题）。
- en: Kernels are perhaps one of the most useful features of Kaggle. Through Kaggle
    kernels, users can share their code and methods openly. This ensures reproducible
    results, and, often, you will learn a technique that you did not know about. Kaggle
    also provides a free cloud environment to run your code, including GPU support.
    If you would like to put your skills to the test, after reading this book Kaggle
    is a great place to start.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Kernels 可能是 Kaggle 上最有用的功能之一。通过 Kaggle Kernels，用户可以公开分享他们的代码和方法。这确保了结果的可重复性，通常你会学到一些自己之前不知道的技巧。Kaggle
    还提供了一个免费的云环境来运行你的代码，包括 GPU 支持。如果你想挑战自己的技能，读完本书后，Kaggle 是一个很好的起点。
- en: Favorite machine learning tools
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最喜爱的机器学习工具
- en: 'In this book, I have used a lot of Python and Keras. Beyond that, there are
    also several machine learning tools that I consider to be useful:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我使用了大量的 Python 和 Keras。除此之外，还有一些我认为很有用的机器学习工具：
- en: '**Jupyter Notebook**:Jupyter notebooks are interactive notebooks that are often
    used during the early stages of machine learning projects. The advantage of using
    Jupyter Notebooks is that it allows us to write interactive code iteratively.
    Unlike a `.py` Python file, code can be executed in chunks, and output (for example,
    graphs) can be displayed in line with the code.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Jupyter Notebook**：Jupyter Notebook是交互式笔记本，通常在机器学习项目的早期阶段使用。使用Jupyter Notebook的优势在于，它允许我们迭代地编写交互式代码。与`.py`的Python文件不同，代码可以分块执行，输出（例如图表）可以与代码一起显示。'
- en: '**Google Colab**:Google Colab is a free cloud platform that allows us to write
    Jupyter Notebook code in the cloud. All changes are synced automatically, and
    teams can work collaboratively on the same notebook. The greatest advantage of
    Google Colab is that you can run code with GPU instances in the cloud, which are
    provided for free by Google! This means that we can train a deep neural network
    efficiently from anywhere in the world, even if we do not own a powerful GPU.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Colab**：Google Colab是一个免费的云平台，允许我们在云端编写Jupyter Notebook代码。所有的修改都会自动同步，团队成员可以在同一个笔记本上进行协作。Google
    Colab的最大优势是，你可以在云端使用GPU实例运行代码，而这些实例是Google免费提供的！这意味着我们可以从世界任何地方高效地训练深度神经网络，即使我们没有强大的GPU。'
- en: Summary
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we did a quick recap of all the different neural networks and
    key concepts that were covered in this book. We then looked at some cutting-edge
    developments in neural networks, including generative adversarial networks and
    deep reinforcement learning. Even though the potential of neural networks may
    seem boundless at times, it is important for us to remember that there are limitations
    to what the current state of neural networks can accomplish. Next, we surveyed
    the landscape of machine learning and AI in general, and we saw what AI could
    look like in the near future. We also offered some tips to readers on keeping
    up with the constantly evolving field of machine learning.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们快速回顾了本书中涉及的所有不同类型的神经网络和关键概念。接着，我们探讨了一些神经网络的前沿发展，包括生成对抗网络和深度强化学习。尽管神经网络的潜力有时看起来无穷无尽，但我们必须记住，当前神经网络的状态也有其局限性。接下来，我们概述了机器学习和人工智能的整体发展，并展示了人工智能在不久的未来可能呈现的样貌。我们还给读者提供了一些关于如何跟上机器学习这一不断发展的领域的建议。
- en: Finally, I would like to conclude this book by asking the following question—what
    will you create? We live in an age of highly advanced technology, where access
    to information is freely available. Whatever your current level, whether you are
    a seasoned machine learning veteran or a beginner in this field, you have all
    the resources that you need to succeed. I would like to encourage you to always
    be curious, and to always have a thirst for knowledge. Many of the discoveries
    in this field came from curious people like you and me. We can all contribute.
    What will you create?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想通过提问来结束本书——你将创造什么？我们生活在一个高度先进的科技时代，信息可以自由获取。无论你现在处于什么水平，无论你是经验丰富的机器学习专家还是刚入门的初学者，你都拥有成功所需的所有资源。我鼓励你保持好奇心，始终渴望知识。许多这个领域的发现来自像你我一样的好奇者。我们每个人都能做出贡献。你将创造什么？
