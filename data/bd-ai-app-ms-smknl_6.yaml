- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Adding Memories to Your AI Application
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将记忆添加到你的AI应用程序中
- en: In the previous chapter, we learned how to use planners to give our users the
    ability to ask our application to perform actions that we did not program explicitly.
    In this chapter, we are going to learn how to use external data, so that we can
    bring recent information and keep information between user sessions. For now,
    we are going to use small amounts of external data that our users may have given
    us by saving it to **memory**. Learning how to use memory will enable us to greatly
    expand the capabilities of AI models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何使用规划器让我们的用户能够要求我们的应用程序执行我们没有明确编程的动作。在本章中，我们将学习如何使用外部数据，这样我们就可以带来最新的信息并在用户会话之间保持信息。目前，我们将使用用户通过将其保存到**内存**中可能给我们提供的少量外部数据。学习如何使用内存将使我们能够极大地扩展AI模型的功能。
- en: This is a building block for the next chapter, in which we are going to learn
    techniques to use amounts of data that vastly exceed the context window of existing
    models. As you may remember, a *context window* is the maximum size of the input
    you can send to an AI service. By using memory, you can save a large amount of
    data and only send portions of the data in each call.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这是下一章的构建块，我们将学习使用超过现有模型上下文窗口的数据量的技术。如你所记得，**上下文窗口**是你可以向AI服务发送的输入的最大大小。通过使用内存，你可以保存大量数据，并在每次调用中只发送数据的一部分。
- en: We will start by understanding how LLMs convert words into meaning by using
    **embeddings** and then compare phrases with similar meanings to recall data from
    memory. Later in the chapter, we will see how to keep historical data in a chat
    application.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先了解LLMs如何通过使用**嵌入**将单词转换为意义，然后比较具有相似意义的短语以从记忆中召回数据。在章节的后面部分，我们将看到如何在聊天应用中保存历史数据。
- en: 'In this chapter, we’ll be covering the following topics :'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Creating embeddings for text data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为文本数据创建嵌入
- en: Storing data in memory and recovering it to use in your prompts
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在内存中存储数据并在需要时恢复以用于提示
- en: Using a plugin to keep track of a chat that your user is having with your application
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用插件跟踪用户与你的应用程序进行的聊天
- en: Using summarization to keep track of long chats
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用摘要来跟踪长聊天
- en: By the end of the chapter, you will have learned how to help your application
    remember information entered by the user and retrieve it when needed.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将学会如何帮助你的应用程序记住用户输入的信息，并在需要时检索它。
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To complete this chapter, you will need to have a recent, supported version
    of your preferred Python or C# development environment:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成本章，你需要拥有你首选的Python或C#开发环境的最新、受支持的版本：
- en: For Python, the minimum supported version is Python 3.10, and the recommended
    version is Python 3.11
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Python，最低支持的版本是Python 3.10，推荐版本是Python 3.11
- en: For C#, the minimum supported version is .NET 8
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于C#，最低支持的版本是.NET 8
- en: In this chapter, we will call OpenAI services. Given the amount that companies
    spend on training these LLMs, it’s no surprise that using these services is not
    free. You will need an **OpenAI API** key, either directly through **OpenAI**
    or **Microsoft**, via the **Azure** **OpenAI** service.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将调用OpenAI服务。鉴于公司在这类LLMs训练上的支出，使用这些服务不是免费的也就不足为奇了。你需要一个**OpenAI API**密钥，无论是直接通过**OpenAI**还是通过**Microsoft**的**Azure
    OpenAI**服务。
- en: If you are using .NET, the code for this chapter is at [https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch6](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch6).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用.NET，本章的代码位于[https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch6](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch6)。
- en: If you are using Python, the code for this chapter is at [https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch6](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch6).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用Python，本章的代码位于[https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch6](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch6)。
- en: 'You can install the required packages by going to the GitHub repository and
    using the following: `pip install -``r requirements.txt`.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过访问GitHub仓库并使用以下命令安装所需的包：`pip install -r requirements.txt`。
- en: Defining memory and embeddings
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义内存和嵌入
- en: LLMs provided by AI services such as OpenAI are **stateless**, meaning they
    don’t retain any memory of previous interactions. When you submit a request, the
    request itself contains all the information the model will use to respond. Any
    previous requests you submitted have already been forgotten by the model. While
    this stateless nature allows for many useful applications, some situations require
    the model to consider more context across multiple requests.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由OpenAI等AI服务提供的LLMs是**无状态的**，这意味着它们不保留任何先前交互的记忆。当你提交一个请求时，请求本身包含了模型将用于响应的所有信息。你之前提交的任何请求都已经由模型遗忘。虽然这种无状态的性质允许许多有用的应用，但某些情况需要模型在多个请求之间考虑更多的上下文。
- en: Despite their immense computing power, most LLMs can only work with small amounts
    of text, about one page at a time, although this has been increasing recently
    — the new GPT-4 Turbo, released in November 2023, can receive 128,000 tokens as
    input, which is about 200 pages of text. Sometimes, however, there are applications
    that require a model to consider more than 200 pages of text — for example, a
    model that answers questions about a large collection of academic papers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们具有巨大的计算能力，但大多数LLMs一次只能处理少量文本，大约一页，尽管最近这一数字有所增加——2023年11月发布的GPT-4 Turbo可以接收128,000个标记作为输入，这大约是200页文本。然而，有时有一些应用需要模型考虑超过200页的文本——例如，一个回答关于大量学术论文问题的模型。
- en: Memories are a powerful way to help Semantic Kernel work by providing more context
    for your requests. We add memory to Semantic Kernel by using a concept called
    **semantic memory search**, where textual information is represented by vectors
    of numbers called **embeddings**. Since the inception of computing, text has been
    converted into numbers to help computers compare different texts. For example,
    the ASCII table that converts letters into numbers was first published in 1963\.
    LLMs convert much more than a single character at a time, using embeddings.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆是帮助语义内核工作的强大方式，它通过为您的请求提供更多上下文来实现。我们通过使用一个称为**语义记忆搜索**的概念来向语义内核添加记忆，其中文本信息由称为**嵌入**的数字向量表示。自从计算机诞生以来，文本就被转换为数字，以帮助计算机比较不同的文本。例如，将字母转换为数字的ASCII表最早于1963年发布。LLMs一次转换的不仅仅是单个字符，而是使用嵌入。
- en: Embeddings take words and phrases as inputs and output a long list of numbers
    to represent them. The length of the list varies depending on the embedding model.
    Importantly, words and phrases with similar meanings are close to each other in
    a numerical sense; if one were to calculate a distance between the numeric components
    of the embeddings of two similar phrases, it would be smaller than the distance
    between two sentences with very different meanings.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入以单词和短语作为输入，并输出一个长长的数字列表来表示它们。列表的长度取决于嵌入模型。重要的是，含义相似的单词和短语在数值上彼此靠近；如果计算两个相似短语嵌入的数值组件之间的距离，它将小于两个含义非常不同的句子之间的距离。
- en: We will see a complete example in the *Embeddings in action* section, but for
    a quick example, the difference between the one-word phrases “*queen*” and “*king*”
    is much smaller than the difference between the one-word phrases “*camera*” and
    “*dog.*”
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*嵌入应用*部分看到一个完整的示例，但为了快速示例，单词短语“*queen*”和“*king*”之间的差异远小于单词短语“*camera*”和“*dog.*”之间的差异。
- en: Let’s take a deeper look.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解一下。
- en: How does semantic memory work?
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义记忆是如何工作的？
- en: 'Embeddings are a way of representing words or other data as vectors in a high-dimensional
    space. Embeddings are useful for AI models because they can capture the meaning
    and context of words or data in a way that computers can process. An embedding
    model takes a sentence, paragraph, or even some pages of text and outputs the
    corresponding embedding vector:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是将单词或其他数据表示为高维空间中的向量的方式。嵌入对AI模型很有用，因为它们可以以计算机可以处理的方式捕捉单词或数据的含义和上下文。嵌入模型接受一个句子、段落，甚至一些页面的文本，并输出相应的嵌入向量：
- en: '![Figure 6.1 – Embedding model](img/B21826_06_1.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – 嵌入模型](img/B21826_06_1.jpg)'
- en: Figure 6.1 – Embedding model
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 嵌入模型
- en: The current highest-performing embedding model that OpenAI makes available to
    users is called `text-embedding-3-large` and can convert an input of up to 8,191
    tokens (about 12 pages of text) into a vector of 3,072 dimensions represented
    by real numbers.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI目前提供给用户的当前最高性能的嵌入模型被称为`text-embedding-3-large`，可以将多达8,191个标记（大约12页文本）转换为3,072维的实数向量。
- en: OpenAI also makes additional embedding models available at different prices
    and performance points, such as `text-embedding-3-small` and `text-embedding-ada-2`.
    At the time of writing, the `text-embedding-3-small` model offers better performance
    than the `text-embedding-ada-2` model, and it’s five times cheaper.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI还提供了不同价格和性能点的额外嵌入模型，例如`text-embedding-3-small`和`text-embedding-ada-2`。在撰写本文时，`text-embedding-3-small`模型提供的性能优于`text-embedding-ada-2`模型，并且它便宜五倍。
- en: You, as a developer, can store text data (including user requests and responses
    provided by the AI service) as embedding vectors. It’s important to know that
    this will not necessarily make the data smaller. For a given embedding model,
    embedding vectors will always be the same length. For example, for the `text-embedding-3-large`
    model, the embedding vector length is always 3,072\. If you store the word “*No*”
    using this model, it will use a vector of 3,072 real numbers that would take 12,228
    bytes of memory, a lot more than the string “`No`”, which can usually be stored
    in two bytes. On the other hand, if you embed 12 pages of text, their embedding
    vector length will also be 3,072 and take 12,228 bytes of memory.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作为开发者，你可以将文本数据（包括AI服务提供的用户请求和响应）作为嵌入向量存储。重要的是要知道，这并不一定会使数据变小。对于给定的嵌入模型，嵌入向量始终是相同的长度。例如，对于`text-embedding-3-large`模型，嵌入向量长度始终是3,072。如果你使用这个模型存储单词“*No*”，它将使用一个包含3,072个实数的向量，这将占用12,228字节的内存，比字符串“`No`”多得多，后者通常可以存储在两个字节中。另一方面，如果你嵌入12页的文本，它们的嵌入向量长度也将是3,072，并占用12,228字节的内存。
- en: You can use embeddings to recall context that has been given to your application
    long before a request is made. For example, you could store all the chats you
    have had with a user in a database. If the user told you months ago that their
    favorite city is Paris in France, this information can be saved. Later, when the
    user asks what the biggest attraction is in the city they like the most, you can
    search for their favorite city in the database you created.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用嵌入来回忆在请求之前很久就给应用程序提供的上下文。例如，你可以在数据库中存储与用户的所有对话。如果用户几个月前告诉你他们最喜欢的城市是法国的巴黎，这条信息可以被保存。稍后，当用户询问他们最喜欢的城市中最大的景点是什么时，你可以在你创建的数据库中搜索他们的最爱城市。
- en: How are vector databases different from KernelContext?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库与KernelContext有何不同？
- en: In previous chapters, we used variables of the type `KernelContext` to pass
    information to functions. A `KernelContext` variable can be serialized to disk,
    and therefore, you could use it to store and remember things that your application
    has already been told.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们使用了`KernelContext`类型的变量来向函数传递信息。一个`KernelContext`变量可以被序列化到磁盘上，因此，你可以用它来存储和记住应用程序已经被告知的事情。
- en: The difference is that a `KernelContext` variable is a collection of key/value
    pairs. For each piece of information you store, you have to provide a key, and
    later, you have to use the same key to retrieve it. Vector databases, on the other
    hand, retrieve information by similarity, so you can retrieve a piece of information
    even if you don’t know the key used to store it.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 差异在于，`KernelContext`变量是一系列键/值对的集合。对于你存储的每条信息，你必须提供一个键，稍后，你必须使用相同的键来检索它。另一方面，向量数据库通过相似性来检索信息，因此即使你不知道存储它的键，你也可以检索到一条信息。
- en: Another difference is that, if you want, a vector database can return just the
    subset of information that is similar to the information you requested, while
    if you have a `KernelContext` variable, you need to keep all information available
    all the time, which may cause performance and capacity issues when you have a
    lot of information.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个不同之处在于，如果你想的话，向量数据库可以只返回与请求的信息相似的信息子集，而如果你有一个`KernelContext`变量，你需要始终保留所有信息可用，这在你有大量信息时可能会导致性能和容量问题。
- en: As the user chats with your application, you can store each command the user
    types in the chat as an embedding, its numerical representation. Then, when the
    user enters a new command, you can search through everything the user has ever
    typed before by comparing the embeddings of what the user just entered to things
    that they have entered before.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户与应用程序聊天时，你可以将用户在聊天中输入的每个命令作为嵌入存储，即其数值表示。然后，当用户输入新命令时，你可以通过比较用户刚刚输入的内容的嵌入与之前输入的内容来搜索用户之前输入的所有内容。
- en: Because the application is using embedding representations that encode meaning,
    the user may have said “*my favorite city is Paris*” months ago and may ask “*what’s
    the biggest attraction in the city I like the most*” now. A string search would
    not find a match between “*city I like the most*” and “*favorite city*,” but these
    two sentences will have embedding vectors that are close to each other, and a
    semantic search would return “*favorite city*” as a close match for “*city I like
    the most*.” In this case, a close match is exactly what you need.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因为应用程序使用编码意义的嵌入表示，用户可能几个月前说过“*我最喜欢的城市是巴黎*”，现在可能问“*我最喜欢的城市里最大的景点是什么*”。字符串搜索不会在“*我最喜欢的城市*”和“*最喜欢的城市*”之间找到匹配，但这两个句子将具有彼此接近的嵌入向量，语义搜索会返回“*最喜欢的城市*”作为“*我最喜欢的城市*”的接近匹配。在这种情况下，接近匹配正是你所需要的。
- en: Let’s see how to create embeddings with an example.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子看看如何创建嵌入。
- en: Embeddings in action
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入应用
- en: This subsection only has Python code, because OpenAI does not provide a C# API
    and we would need to call the REST API directly. This subsection will show embedding
    values to help you understand the embedding concepts, but you don’t need to implement
    the code to understand it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节只包含Python代码，因为OpenAI没有提供C# API，我们需要直接调用REST API。本小节将展示嵌入值以帮助你理解嵌入概念，但你不需要实现代码来理解它。
- en: 'To start, we will need to import some libraries. In Python, linear algebra
    calculations are in the `numpy` library, so we need to import that:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入一些库。在Python中，线性代数计算在`numpy`库中，因此我们需要导入它：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, we will generate embeddings for three sentences and compare them to one
    another.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将为三个句子生成嵌入，并将它们相互比较。
- en: 'First, we will write a function (`get_embedding`) that generates the embeddings:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将编写一个函数（`get_embedding`）来生成嵌入：
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding function is a straightforward function call, simply instantiating
    a connection to OpenAI and calling the `embeddings.create` method, using the `text-embedding-3-small`
    model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的函数是一个简单的函数调用，只是实例化了一个与OpenAI的连接，并调用了`embeddings.create`方法，使用`text-embedding-3-small`模型。
- en: Then, to compare how similar one embedding is to another, we will use a `0.0`
    and `1.0`, with `1.0` meaning they are very similar.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了比较一个嵌入与另一个嵌入的相似度，我们将使用`0.0`和`1.0`，其中`1.0`表示它们非常相似。
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The last step is to call the functions with the phrases we want to check:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是调用函数，使用我们想要检查的短语：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The three phrases are `"The king has been crowned"`, `"The queen has been crowned"`,
    and `"LinkedIn is a social media platform for professionals"`. We expect the first
    and second phrases to be similar, and both to be different from the third phrase.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 三个短语是`"The king has been crowned"`、`"The queen has been crowned"`和`"LinkedIn
    is a social media platform for professionals"`。我们预计前两个短语是相似的，并且两者都与第三个短语不同。
- en: 'As expected, this is what we get, remembering that the numbers go between `0.0`
    and `1.0`, with `0.0` meaning dissimilar and `1.0` meaning perfect match:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，这就是我们得到的结果，记住数字在`0.0`和`1.0`之间，其中`0.0`表示不相似，`1.0`表示完美匹配：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If you want to see the embeddings themselves, you can print them, remembering
    that they use 1,536 real numbers for the `text-embedding-3-small` model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看嵌入本身，你可以打印它们，记住它们使用1,536个实数来表示`text-embedding-3-small`模型。
- en: 'The following code prints the first 10 embedding values:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码打印了前10个嵌入值：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here’s the result:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是结果：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that we understand a little more about how embeddings work, let’s see how
    to use them with LLMs.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对嵌入的工作原理有了更多的了解，让我们看看如何使用它们与LLM结合。
- en: Using memory within chats and LLMs
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在聊天和LLM中使用内存
- en: As we have seen before, models have a size limit called a context window. The
    size limit includes both the prompt with the user request and the response. The
    default context window for a model such as GPT-3.5, for example, is 4,096 bytes,
    meaning that both your prompt, including the user request, and the answer that
    GPT-3.5 provides can have at most 4,096 bytes; otherwise, you will get an error,
    or the response will cut off in the middle.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所见，模型有一个称为上下文窗口的大小限制。大小限制包括用户请求的提示和响应。例如，GPT-3.5等模型的默认上下文窗口为4,096字节，这意味着你的提示（包括用户请求）和GPT-3.5提供的答案最多可以有4,096字节；否则，你会得到错误，或者响应会在中间被截断。
- en: If your application uses a lot of text data, for example, a 10,000-page operating
    manual, or allows people to search and ask questions about a database of hundreds
    of documents with each one having 50 pages, you need to find a way of including
    just the relevant portion of this large dataset with your prompt. Otherwise, the
    prompt alone could be larger than the context window, resulting in an error, or
    the remaining context window could be so short that there would be no space for
    the model to provide a good answer.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的应用程序使用大量文本数据，例如，一本10,000页的操作手册，或者允许人们搜索和询问包含数百份文档的数据库，每份文档有50页，你需要找到一种方法，只将相关部分的数据集包含在你的提示中。否则，提示本身可能比上下文窗口大，导致错误，或者剩余的上下文窗口可能太短，以至于没有空间让模型提供好的答案。
- en: One way in which you could work around this problem is by summarizing each page
    into a shorter paragraph and then generating an embedding vector for each summary.
    Instead of including all the pages in your prompt, you can use something such
    as cosine similarity to search for the relevant pages by comparing their embeddings
    with the request embeddings, and then include only the summaries of the relevant
    pages in the prompt, saving a lot of space.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法之一是将每一页总结成一个较短的段落，并为每个摘要生成一个嵌入向量。你不需要在提示中包含所有页面，可以使用余弦相似度等工具通过比较请求嵌入与嵌入向量来搜索相关页面，然后只将相关页面的摘要包含在提示中，这样可以节省很多空间。
- en: Another reason to use memory is to keep data between sessions, or even between
    prompts. For example, as we suggested in the *How does semantic memory work?*
    section, your user may have told you that their favorite city is Paris, and when
    they ask for a guide for their favorite city, you don’t need to ask again; you
    just need to search for their favorite city.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用内存的另一个原因是保持会话之间或提示之间的数据。例如，正如我们在 *语义记忆是如何工作的？* 部分中建议的，用户可能已经告诉过你他们最喜欢的城市是巴黎，当用户要求获取他们最喜欢的城市的指南时，你不需要再次询问；你只需要搜索他们的最喜欢的城市。
- en: To find the data in the memory that is relevant to our prompt, we could use
    something such as the cosine distance shown in the previous section. In practice,
    the Semantic Kernel SDK already provides you with a search function, so you don’t
    need to implement it yourself. In addition, you can use several third-party vector
    databases, each with its own search functions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到与我们的提示相关的内存中的数据，我们可以使用之前章节中展示的余弦距离等工具。在实践中，Semantic Kernel SDK 已经为你提供了搜索功能，因此你不需要自己实现它。此外，你还可以使用几个第三方向量数据库，每个数据库都有自己的搜索功能。
- en: 'Here’s a list of all databases that you can use out of the box:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个你可以直接使用的所有数据库的列表：
- en: '| **Database name** | **Python** | **C#** |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| **数据库名称** | **Python** | **C#** |'
- en: '| Azure Cosmos DB for MongoDB vCore |  | ✅ |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Azure Cosmos DB for MongoDB vCore |  | ✅ |'
- en: '| Azure AI Search | ✅ | ✅ |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| Azure AI Search | ✅ | ✅ |'
- en: '| Azure PostgreSQL Server | ✅ |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| Azure PostgreSQL Server | ✅ |  |'
- en: '| Chroma | ✅ | ✅ |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Chroma | ✅ | ✅ |'
- en: '| DuckDB | ✅ |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| DuckDB | ✅ |  |'
- en: '| Milvus |  | ✅ |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Milvus |  | ✅ |'
- en: '| MongoDB Atlas Vector Search | ✅ | ✅ |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| MongoDB Atlas Vector Search | ✅ | ✅ |'
- en: '| Pinecone | ✅ | ✅ |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Pinecone | ✅ | ✅ |'
- en: '| PostgreSQL | ✅ | ✅ |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| PostgreSQL | ✅ | ✅ |'
- en: '| Qdrant | ✅ |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Qdrant | ✅ |  |'
- en: '| Redis | ✅ |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Redis | ✅ |  |'
- en: '| SQLite | ✅ |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| SQLite | ✅ |  |'
- en: '| Weaviate | ✅ | ✅ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Weaviate | ✅ | ✅ |'
- en: Table 6.1 — Vector databases and compatibility with Semantic Kernel
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.1 — 向量数据库与 Semantic Kernel 的兼容性
- en: In addition to the databases listed, there’s another one, called `VolatileMemoryStore`,
    which represents the RAM of the machine you’re running your code on. That database
    is not persistent, and its contents are discarded when the code finishes running,
    but it’s fast and free and can be easily used during development.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 除了列出的数据库之外，还有一个叫做 `VolatileMemoryStore` 的数据库，它代表你运行代码的机器的RAM。这个数据库不是持久的，其内容在代码运行完成后将被丢弃，但它速度快且免费，可以在开发过程中轻松使用。
- en: Using memory with Microsoft Semantic Kernel
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Microsoft Semantic Kernel 的内存
- en: In the following example, we will store some information about the user and
    then use the `TextMemorySkill` core skill to retrieve it directly inside a prompt.
    Core skills are skills that come out of the box with Semantic Kernel. `TextMemorySkill`
    has functions to put text into memory and retrieve it.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将存储一些关于用户的信息，然后使用 `TextMemorySkill` 核心技能在提示中直接检索它。核心技能是Semantic Kernel自带的功能。`TextMemorySkill`
    具有将文本放入内存和检索文本的功能。
- en: In the following example, our use case will be of a user who tells us their
    favorite city and favorite activity. We will save those to memory and then we
    will retrieve them and provide an itinerary based on the saved information.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们的用例将是一个用户告诉我们他们的最喜欢的城市和最喜欢的活动。我们将将这些信息保存到内存中，然后检索它们，并根据保存的信息提供行程。
- en: We start by importing the libraries that we usually import, plus a few memory
    libraries that will be described later.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入我们通常导入的库，以及一些将在后面描述的内存库。
- en: Python
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE7]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: C#
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: C#
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that the memory functions in Python are asynchronous, so we must include
    the `asyncio` library. Also, at the time of writing, the memory functions in C#
    are marked as experimental, so you have to disable the experimental warnings with
    the `#``pragma` command.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Python 中的内存函数是异步的，因此我们必须包含 `asyncio` 库。此外，在撰写本文时，C# 中的内存函数被标记为实验性，因此您必须使用
    `#pragma` 命令禁用实验性警告。
- en: 'Now, let’s create a kernel:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个内核：
- en: Python
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: C#
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: C#
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Note that we added three items to our kernel:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们已经向我们的内核添加了三个项目：
- en: An embedding model, which will help us load things into memory. For C#, we can
    use `text-embedding-3-small`, but at the time of writing, even though Python can
    use `text-embedding-3-small` as we did in the previous section, the core Python
    plugins only work with model `text-embedding-ada-002`.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个嵌入模型，它将帮助我们加载内容到内存中。对于 C#，我们可以使用 `text-embedding-3-small`，但在撰写本文时，尽管 Python
    可以像上一节那样使用 `text-embedding-3-small`，但核心 Python 插件仅与模型 `text-embedding-ada-002`
    兼容。
- en: Memory storage; in this case, `VolatileMemoryStore`, which just stores data
    temporarily in your computer’s RAM.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存存储；在这种情况下，`VolatileMemoryStore`，它只是在您的计算机 RAM 中临时存储数据。
- en: A GPT model to generate the itinerary; we’re using GPT-4
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于生成行程的 GPT 模型；我们正在使用 GPT-4
- en: Also, note that in C#, the memory and the kernel are built in separate commands,
    while in Python, they are all built together.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，在 C# 中，内存和内核是分别构建的，而在 Python 中，它们是一起构建的。
- en: 'Now, let’s create a function that adds data to the memory:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个向内存添加数据的函数：
- en: Python
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: C#
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: C#
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The function to add data to memory simply calls `memory.save_information` in
    Python and `memory.SaveInformationAsync` in C#. You can keep different groups
    of information separate by using collections, but in our simple case, we’re just
    going to use `"generic"` for Python and `"default"` for C#, as those are the default
    collections for the plugins. The `id` parameter does not have to mean anything,
    but it must be unique by item. If you save multiple items using the same `id`
    parameter, the last saved item will overwrite the previous ones. It’s common to
    generate GUIDs to ensure some level of uniqueness, but if you are just going to
    add a few items manually, you can manually ensure that the ids are different.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 向内存添加数据的函数简单地调用 Python 中的 `memory.save_information` 和 C# 中的 `memory.SaveInformationAsync`。您可以使用集合将不同组的信息分开，但在我们的简单案例中，我们将使用
    `"generic"` 作为 Python 的默认集合，以及 `"default"` 作为 C# 的默认集合，因为这些是插件的默认集合。`id` 参数不必有任何意义，但它必须对每个项目是唯一的。如果您使用相同的
    `id` 参数保存多个项目，则最后保存的项目将覆盖前面的项目。通常生成 GUID 以确保一定程度的唯一性，但如果您只是手动添加少量项目，则可以手动确保 id
    不同。
- en: 'We can now create a function that generates a travel itinerary:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以创建一个生成旅行行程的函数：
- en: Python
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: C#
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: C#
- en: '[PRE14]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`TextMemoryPlugin` gives the ability to use `{{recall $question}}` to retrieve
    the contents of the memory inside a prompt without you needing to write any code.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`TextMemoryPlugin` 允许您在提示中使用 `{{recall $question}}` 来检索内存中的内容，而无需编写任何代码。'
- en: For example, assume that we loaded `My favorite city is Paris` in our memory.
    When we load the `$city` variable with `"What's my favorite city"` and write `{{$city}}
    {{recall $city}}` inside the prompt, Semantic Kernel will replace that line with
    `"What's my favorite city? My favorite city is Paris"` inside the prompt.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们已经将 `My favorite city is Paris` 加载到我们的内存中。当我们用 `"What's my favorite city"`
    加载 `$city` 变量并在提示中写入 `{{$city}} {{recall $city}}` 时，Semantic Kernel 将在提示中将该行替换为
    `"What's my favorite city? My favorite city is Paris"`。
- en: Storing data in memory
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在内存中存储数据
- en: Note that we didn’t use a meaningful key name when storing the data in memory
    (we used `"1"` and `"2"`). You also don’t need to classify the information before
    storing it. Some applications simply store everything as they comes, while others
    use a semantic function to ask Semantic Kernel whether the user input contains
    personalization information and store it in cases where it does.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在内存中存储数据时没有使用有意义的键名（我们使用了`"1"`和`"2"`）。您也不需要在存储之前对信息进行分类。一些应用程序只是简单地存储所有内容，而其他应用程序则使用语义函数询问Semantic
    Kernel用户输入是否包含个性化信息，并在这些情况下存储它。
- en: 'Now, let’s load the memory and call the prompt:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们加载内存并调用提示：
- en: Python
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: C#
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: C#
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the code, we load the information to memory using the `add_to_memory` function
    and immediately call our semantic function `f`. If you are using any memory store
    other than `VolatileMemoryStore`, you don’t need to implement these two steps
    in the same session. We will see an example of persisting memory in our **RAG**
    (**retrieval-augmented generation**) example in [*Chapter 7*](B21826_07.xhtml#_idTextAnchor132).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们使用`add_to_memory`函数将信息加载到内存中，并立即调用我们的语义函数`f`。如果您使用除`VolatileMemoryStore`之外的其他任何内存存储，您不需要在同一个会话中实现这两个步骤。我们将在[*第七章*](B21826_07.xhtml#_idTextAnchor132)中的**RAG**（**检索增强生成**）示例中看到一个持久化内存的例子。
- en: Results
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 结果
- en: 'Note that the model recalled that the user’s favorite city is Paris and that
    their favorite activity is going to museums:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，模型回忆起用户最喜欢的城市是巴黎，以及他们最喜欢的活动是去博物馆：
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that if you pay for a subscription to any of the vector database providers
    listed in *Table 6.1*, you can simply replace the `VolatileMemoryStore` constructor
    with their constructor; for example, if you are using Pinecone, you will use `Pinecone(apiKey)`,
    and the memory will be persisted in that database and available to the user the
    next time they run your application. We will see an example with Azure AI Search
    in [*Chapter 7*](B21826_07.xhtml#_idTextAnchor132).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果您为*表6.1*中列出的任何矢量数据库提供商支付订阅费，您可以直接用他们的构造函数替换`VolatileMemoryStore`构造函数；例如，如果您使用Pinecone，您将使用`Pinecone(apiKey)`，内存将保存在该数据库中，并在用户下次运行您的应用程序时可供用户使用。我们将在[*第七章*](B21826_07.xhtml#_idTextAnchor132)中看到一个使用Azure
    AI Search的示例。
- en: Now, let’s see how we can use memory in a chat with a user.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们如何在一个与用户的聊天中使用内存。
- en: Using memory in chats
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在聊天中使用内存
- en: Memory is typically used in chat-based applications. All the applications that
    we built in earlier chapters were *one-shot* — all the information required to
    complete a task is part of the request submitted by the user plus whatever modifications
    we make to the prompt in our own code, for example, by including the user-submitted
    prompt inside a variable in `skprompt.txt`, or modifying their prompt using string
    manipulation. All questions and answers that happened before are ignored. We say
    that the AI service is *stateless*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 内存通常用于基于聊天的应用程序。我们在前几章中构建的所有应用程序都是*单次*的——完成任务所需的所有信息都是用户提交的请求的一部分，以及我们在自己的代码中对提示所做的任何修改，例如，通过在`skprompt.txt`中的变量内包含用户提交的提示，或使用字符串操作修改他们的提示。所有之前发生的问题和答案都被忽略。我们说AI服务是*无状态的*。
- en: Sometimes, however, we want the AI service to remember requests that have been
    made before. For example, if I ask the app about the largest city in India by
    population, the application will respond that it is *Mumbai*. If I then ask “*how
    is the temperature there in the summer*,” I would expect the application to realize
    that I’m asking about the temperature in Mumbai, even though my second prompt
    does not include the name of the city.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时我们希望AI服务记住之前做出的请求。例如，如果我询问应用程序关于印度人口最多的城市，应用程序将回答它是*Mumbai*。如果然后我询问“*夏天那里的温度如何*”，我期望应用程序意识到我是在询问Mumbai的温度，即使我的第二个提示没有包含城市的名字。
- en: As we mentioned in [*Chapter 1*](B21826_01.xhtml#_idTextAnchor014), the brute-force
    solution is to simply repeat the whole history of the chat with every new request.
    Therefore, when the second request is submitted by the user, we could silently
    attach their first request and the response our AI service provided to it and
    then submit everything together again to the AI service.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第一章*](B21826_01.xhtml#_idTextAnchor014)中提到的，暴力解决方案是简单地重复整个聊天历史与每个新的请求。因此，当用户提交第二个请求时，我们可以静默地附加他们的第一个请求以及我们的AI服务提供的响应，然后再次将所有内容一起提交给AI服务。
- en: 'Let’s see how to do this next. We start with the usual imports:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何做到这一点。我们首先进行常规导入：
- en: Python
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: C#
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: C#
- en: '[PRE19]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that in C#, since several components of the Semantic Kernel package are
    still in pre-release, you need to disable the experimental warnings using a `#``pragma`
    directive.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在C#中，由于Semantic Kernel包的几个组件仍在预发布阶段，你需要使用`#pragma`指令禁用实验性警告。
- en: 'After importing the libraries, we create the kernel:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入库之后，我们创建内核：
- en: Python
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE20]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: C#
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: C#
- en: '[PRE21]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Our kernel just needs a chat completion service. I’m using GPT-4, but GPT-3.5
    also works. I am also adding `ConversationSummaryPlugin`, which will be used in
    the last subsection of this chapter, *Reducing history size with summarization*.
    We will explain it in detail later, but as the name implies, it summarizes conversations.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的内核只需要一个聊天完成服务。我正在使用GPT-4，但GPT-3.5也适用。我还添加了`ConversationSummaryPlugin`，它将在本章的最后一个小节*使用摘要减少历史大小*中使用。我们将在稍后详细解释它，但正如其名称所暗示的，它总结了对话。
- en: 'Now, let’s create the main chat function:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建主要的聊天函数：
- en: Python
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE22]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: C#
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: C#
- en: '[PRE23]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, let’s write the main loop of our program:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编写我们程序的主循环：
- en: Python
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: C#
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: C#
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The main loop of our program runs until the user enters the word `"exit"`. Otherwise,
    we submit the user request to the AI service, collect its answer, and add both
    to the `history` variable, which we also submit as part of our request.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们程序的主循环会一直运行，直到用户输入单词`"exit"`。否则，我们将用户请求提交给AI服务，收集其答案，并将两者都添加到`history`变量中，我们也将它作为请求的一部分提交。
- en: Although this solves the problem of always having the whole history, it becomes
    prohibitively expensive as prompts start to get larger and larger. When the user
    submits their request number *N*, the `history` variable contains their requests
    `1`, …, *N*-`1`, and the chatbot answers `1`, …, *N*-`1` along with it. For large
    *N*, in addition to being expensive, this can exceed the context window of the
    AI service and you will get an error.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这解决了总是需要整个历史的问题，但随着提示变得越来越大，这变得过于昂贵。当用户提交其请求编号*N*时，`history`变量包含他们的请求`1`，…，*N*-`1`，以及聊天机器人随附的答案`1`，…，*N*-`1`。对于大的*N*，除了成本高昂之外，这还可能超过AI服务的上下文窗口，你将得到一个错误。
- en: The solution is to only pass a summary of the history to the AI service. It’s
    fortunate that summarizing conversations is something that even older models can
    do very well. Let’s see how to easily do it with Semantic Kernel.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是只向AI服务传递历史摘要。幸运的是，总结对话是即使是较老模型也能做得很好的事情。让我们看看如何使用Semantic Kernel轻松完成它。
- en: Reducing history size with summarization
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用摘要减少历史大小
- en: If you want to reduce the prompt without losing much context, you can use the
    AI service to summarize what has already happened in the conversation. To do so,
    you can use the `SummarizeConversation` function of `ConversationSummaryPlugin`
    that we imported when creating the kernel. Now, instead of repeating the whole
    history in the prompt, the summary will have up to 1,000 tokens regardless of
    the conversation size, which should be plenty for most use cases. To summarize
    the history in the `$history` variable, simply call `{{ConversationSummaryPlugin.SummarizeConversation
    $history}}` in your prompt.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要在不丢失太多上下文的情况下减少提示，可以使用AI服务总结对话中已经发生的内容。为此，你可以使用我们在创建内核时导入的`ConversationSummaryPlugin`的`SummarizeConversation`函数。现在，不再需要在提示中重复整个历史，摘要将根据对话大小最多包含1,000个token，这对于大多数用例应该足够了。要总结`$history`变量中的历史，只需在提示中调用`{{ConversationSummaryPlugin.SummarizeConversation
    $history}}`即可。
- en: It is still possible to lose details after too much summarization. If you try
    to summarize 1,000 pages in 1,000 words, something will be lost. To prevent this
    problem, most applications have limits on how long conversations can go. For example,
    at the time of writing, Microsoft Copilot conversations have a limit of 20 interactions,
    and you must restart the conversation (with an empty memory) after that.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在过度总结之后，仍然可能丢失细节。如果你试图用1,000个单词总结1,000页的内容，某些东西将会丢失。为了防止这个问题，大多数应用程序都对对话的长度有限制。例如，在撰写本文时，Microsoft
    Copilot对话的限制为20次交互，在那之后你必须重新开始对话（带有空内存）。
- en: The change to the code is shown as follows; you just need to change the contents
    of the `prompt` variable. The change will add a conversation summary to the prompt,
    which will remind the LLM of everything that went on before. The summary will
    not be displayed to the user.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的更改如下所示；你只需更改`prompt`变量的内容。此更改将在提示中添加一个对话摘要，这将提醒LLM之前发生的一切。摘要将不会显示给用户。
- en: Python
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE26]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: C#
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: C#
- en: '[PRE27]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Results
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 结果
- en: '[PRE28]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Note that in the next question, I’ll use the word `there` to mean London. Since
    the history summary is included as part of the conversation, even though my next
    question doesn’t explicitly name London, the prompt that goes to the AI contains
    that information:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在下一个问题中，我将使用“那里”这个词来指代伦敦。由于历史摘要被包含在对话中，尽管我的下一个问题没有明确提到伦敦，但发送给AI的提示中包含了这个信息：
- en: '[PRE29]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Note that the answer to the previous question was correct. Shakespeare, Dickens,
    and Churchill all lived in London. Now, I’ll refer to Shakespeare just by its
    position on the list and to London simply as `that city`, and because we’re keeping
    track of history, the kernel will know that I mean Shakespeare and London:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，上一个问题的答案是正确的。莎士比亚、狄更斯和丘吉尔都住在伦敦。现在，我将通过其在列表中的位置来提及莎士比亚，并将伦敦简单地称为`那个城市`，因为我们正在追踪历史，内核将知道我指的是莎士比亚和伦敦：
- en: '[PRE30]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Again, the AI gets the answer correct. The `Henry V` play is actually set in
    London.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，AI给出了正确的答案。《亨利五世》这部戏剧实际上是在伦敦上演的。
- en: 'Let’s exit the chat:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们退出聊天：
- en: '[PRE31]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Summary
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to add and retrieve information from memory,
    and how to easily include the memory in your prompts. LLMs are stateless and limited
    by their prompt sizes, and in this chapter, we learned techniques to save information
    between sessions and reduce prompt sizes while still including relevant portions
    of the conversation in the prompt.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何添加和检索信息，以及如何轻松地将记忆包含在提示中。LLMs是无状态的，并且受到其提示大小的限制，在本章中，我们学习了在会话之间保存信息并减少提示大小，同时仍然在提示中包含相关对话部分的技术。
- en: In the next chapter, we will see how to use a vector database to retrieve a
    lot more information from memory and use a technique called **retrieval-augmented
    generation** (**RAG**) to organize and present that information in a useful way.
    This technique is often used in enterprise applications, as you trade off a little
    bit of the creativity offered by LLMs, but get back additional precision, the
    ability to show references, and the ability to use a lot of data that you own
    and have control over.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到如何使用向量数据库从记忆中检索更多信息，并使用称为**检索增强生成**（**RAG**）的技术以有用的方式组织和展示这些信息。这种技术通常用于企业应用，因为你在LLMs提供的创造性上做出一些妥协，但获得了额外的精确性、展示参考文献的能力以及使用你拥有和控制的大量数据的能力。
- en: For our application, we are going to load thousands of academic articles into
    a vector database and have Semantic Kernel search for a topic and summarize the
    research for us.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的应用程序，我们将把数千篇学术论文加载到向量数据库中，并让语义内核搜索一个主题，并为我们总结研究。
- en: 'Part 3: Real-World Use Cases'
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：现实世界用例
- en: In this part, we see how Semantic Kernel can be used in real-world problems.
    We learn how using the retrieval-augmented generation (RAG) technique can allow
    AI models to use large amounts of data, including very recent data that was not
    available when the AI service was trained. We conclude by learning how to use
    ChatGPT to distribute an application we wrote to hundreds of millions of users.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分，我们看到了语义内核如何在现实世界问题中得到应用。我们学习了如何使用检索增强生成（RAG）技术，使AI模型能够使用大量数据，包括在AI服务训练时不可用的非常最新的数据。我们通过学习如何使用ChatGPT将我们编写的应用程序分发给数亿用户来结束本部分。
- en: 'This part includes the following chapters:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包括以下章节：
- en: '[*Chapter 7*](B21826_07.xhtml#_idTextAnchor132), *Real-World Use Case – Retrieval-Augmented
    Generation*'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第七章*](B21826_07.xhtml#_idTextAnchor132)，*现实世界用例 – 检索增强生成*'
- en: '[*Chapter 8*](B21826_08.xhtml#_idTextAnchor143), *Real-World Use Case – Making
    Your Application Available on ChatGPT*'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第八章*](B21826_08.xhtml#_idTextAnchor143)，*现实世界用例 – 在ChatGPT上使你的应用程序可用*'
