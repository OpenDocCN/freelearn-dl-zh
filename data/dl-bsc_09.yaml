- en: 8\. Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8\. 深度学习
- en: Deep learning is a machine learning method based on deep neural networks. You
    can create a deep network by adding layers to the networks we've described so
    far. However, a deep network has problems. This chapter will describe the characteristics,
    problems, and possibilities of deep learning, as well as an overview of current
    deep learning practices.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一种基于深度神经网络的机器学习方法。通过向我们迄今为止描述的网络中添加层，你可以创建一个深度网络。然而，深度网络也存在一些问题。本章将介绍深度学习的特点、问题与可能性，以及当前深度学习实践的概述。
- en: Making a Network Deeper
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使网络更深
- en: Throughout this book, we have learned a lot about neural networks, including
    the various layers that constitute a neural network, effective techniques used
    in training, CNNs that are especially effective for handling images, and how to
    optimize parameters. These are all important techniques in deep learning. Here,
    we will integrate the techniques we have learned so far to create a deep network.
    Then, we will try our hand at handwritten digit recognition using the MNIST dataset.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们已经学习了很多关于神经网络的知识，包括构成神经网络的各种层、训练中使用的有效技巧、尤其适用于图像处理的卷积神经网络（CNN）以及如何优化参数。这些都是深度学习中的重要技巧。在这里，我们将整合到目前为止学到的技巧，创建一个深度网络。接着，我们将尝试使用MNIST数据集进行手写数字识别。
- en: Deeper Networks
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更深的网络
- en: First, we will create a CNN that has the network architecture shown in *Figure
    8.1*. This network is based on the VGG network, which will be described in the
    next section.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个具有*图8.1*中所示网络架构的CNN。这个网络基于VGG网络，下一节将对此进行描述。
- en: 'As shown in *Figure 8.1*, the network is deeper than the networks that we have
    implemented so far. All the convolution layers used here are small 3x3 filters.
    Here, the number of channels becomes larger as the network deepens (as the number
    of channels in a convolution layer increases from 16 in the first layer to 16,
    32, 32, 64, and 64). As you can see, pooling layers are inserted to reduce the
    spatial size of intermediate data gradually, while dropout layers are used for
    the latter fully connected layers:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图8.1*所示，网络比我们迄今为止实现的网络要深。这里使用的所有卷积层都是小的3x3卷积核。在这里，随着网络的加深，通道数变得更大（卷积层中的通道数从第一层的16增加到16、32、32、64和64）。如你所见，池化层被插入以逐渐减少中间数据的空间大小，同时丢弃层用于后续的全连接层：
- en: '![Figure 8.1: Deep CNN for handwritten digit recognition'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.1：用于手写数字识别的深度CNN'
- en: '](img/fig08_1.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig08_1.jpg)'
- en: 'Figure 8.1: Deep CNN for handwritten digit recognition'
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.1：用于手写数字识别的深度CNN
- en: 'This network uses the "He initializer" to initialize the weights, and Adam
    to update the weight parameters, resulting in the following characteristics:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络使用"He初始化器"来初始化权重，并使用Adam来更新权重参数，从而具有以下特点：
- en: Convolution layers which use small 3×3 filters
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用小的3×3卷积核的卷积层
- en: ReLU as the activation function
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU作为激活函数
- en: A dropout layer used after a fully connected layer
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在全连接层后使用的丢弃层
- en: Optimization is done by Adam
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化由Adam进行
- en: '"He initializer" for initial weight values'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"He初始化器"用于初始权重值'
- en: As these characteristics indicate, the network in *Figure 8.1* uses many neural
    network techniques that we have learned so far. Now, let's use this network for
    training. The result shows that the recognition accuracy of this network is 99.38%
    (final recognition accuracies vary slightly, but this network will generally exceed
    99%).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这些特点所示，*图8.1*中的网络使用了我们迄今为止学到的许多神经网络技巧。现在，我们来使用这个网络进行训练。结果表明，这个网络的识别准确率为99.38%（最终的识别准确率可能略有不同，但该网络通常会超过99%）。
- en: Note
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The source code that implemented the network shown in *Figure 8.1* is located
    at `ch08/deep_convnet.py`. The code for training is provided at `ch08/train_deepnet.py`.
    You can use this code to reproduce the training that will be conducted here. Training
    in a deep network takes a lot of time (probably more than half a day). This book
    provides trained weight parameters in `ch08/deep_conv_net_params.pkl`. The `deep_convnet.py`
    code file provides a feature for loading trained parameters. You can use it as
    required.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 实现*图8.1*中显示的网络的源代码位于`ch08/deep_convnet.py`。用于训练的代码在`ch08/train_deepnet.py`中提供。你可以使用这些代码来重现这里进行的训练。深度网络的训练需要大量时间（可能超过半天）。本书提供了在`ch08/deep_conv_net_params.pkl`中的训练权重参数。`deep_convnet.py`代码文件提供了一个加载训练参数的功能，你可以根据需要使用它。
- en: 'The error rate of the network shown in *Figure 8.1* is only 0.62%. Here, we
    can see what images were incorrectly recognized. *Figure 8.2* shows the recognition
    error examples:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图8.1*所示，网络的错误率仅为0.62%。在这里，我们可以看到哪些图像被错误地识别了。*图8.2*展示了识别错误的示例：
- en: '![Figure 8.2: Sample images that were recognized incorrectly – the upper left
    of each image shows the correct label, while the lower right shows the result
    of prediction by this network'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.2：识别错误的示例图像——每个图像的左上角显示正确标签，而右下角则显示该网络的预测结果'
- en: '](img/fig08_2.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig08_2.jpg)'
- en: 'Figure 8.2: Sample images that were recognized incorrectly – the upper left
    of each image shows the correct label, while the lower right shows the result
    of prediction by this network'
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.2：识别错误的示例图像——每个图像的左上角显示正确标签，而右下角则显示该网络的预测结果
- en: As shown in *Figure 8.2*, these images are difficult even for us humans to recognize.
    The upper-left image looks like a "0" (the correct answer is "6"), and the one
    next to it certainly seems to be a "5" (the correct answer is "3"). Generally,
    the distinctions between "1" and "7", "0" and "6", and "3" and "5" are difficult.
    These examples explain why they were recognized incorrectly.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图8.2*所示，这些图像即使对我们人类来说也很难识别。左上角的图像看起来像是“0”（正确答案是“6”），旁边的图像显然像是“5”（正确答案是“3”）。一般来说，“1”和“7”，“0”和“6”，“3”和“5”之间的区别很难区分。这些示例解释了为什么它们被错误识别。
- en: While this deep CNN is very precise, it recognized images incorrectly in the
    same way as humans would. This also shows us the large potential of a deep CNN.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个深度卷积神经网络（CNN）非常精确，但它以与人类相似的方式错误地识别了图像。这也向我们展示了深度卷积神经网络的巨大潜力。
- en: Improving Recognition Accuracy
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提高识别准确性
- en: 'The website called "What is the class of this image?" *(Rodrigo Benenson''s
    blog* "*Classification datasets results*" ([http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)))
    ranks the recognition accuracies for various datasets by the techniques published
    in the related literature (*Figure 8.3*):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 名为“这个图像属于什么类别？”的网站（*Rodrigo Benenson的博客* “*分类数据集结果*”（[http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)））根据相关文献中发布的技术对各种数据集的识别准确率进行了排名（见*图8.3*）：
- en: '![Figure 8.3: Ranking techniques for the MNIST dataset'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.3：MNIST数据集的排名技术'
- en: '](img/fig08_3.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig08_3.jpg)'
- en: 'Figure 8.3: Ranking techniques for the MNIST dataset'
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.3：MNIST数据集的排名技术
- en: Note
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '*Figure 8.3* is cited from reference, *Rodrigo Benenson''s blog* "*Classification
    datasets results*" ([http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html))
    as of June 2016.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.3*摘自参考文献，*Rodrigo Benenson的博客* “*分类数据集结果*”（[http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)），数据截至2016年6月。'
- en: 'In the ranking shown in *Figure 8.3*, keywords such as "neural networks," "deep,"
    and "convolutional" are noticeable. Many high-ranked techniques are CNN-based.
    As of June 2016, the highest recognition accuracy for the MNIST dataset is 99.79%
    (an error rate of 0.21%), and the technique is also CNN-based (*Li Wan, Matthew
    Zeiler, Sixin Zhang, Yann L. Cun, and Rob Fergus (2013): Regularization of Neural
    Networks using DropConnect. In Sanjoy Dasgupta & David McAllester, eds. Proceedings
    of the 30th International Conference on Machine Learning (ICML2013). JMLR Workshop
    and Conference Proceedings, 1058 – 1066*). The CNN that is used there is not very
    deep (two convolution layers and two fully connected layers).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图8.3*中显示的排名中，“神经网络”、“深度”和“卷积”等关键词十分显眼。许多排名靠前的技术基于CNN。截至2016年6月，MNIST数据集的最高识别准确率为99.79%（错误率为0.21%），该技术同样是基于CNN的（*Li
    Wan, Matthew Zeiler, Sixin Zhang, Yann L. Cun, 和 Rob Fergus（2013）：使用DropConnect对神经网络进行正则化。发表于Sanjoy
    Dasgupta & David McAllester编辑的《第30届国际机器学习会议（ICML2013）论文集》。JMLR工作坊和会议记录，1058-1066*）。这里使用的CNN并不深（包含两层卷积层和两层全连接层）。
- en: Note
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For the MNIST dataset, the highest accuracy can be obtained immediately, even
    if the network is not very deep. For a relatively simple problem such as handwritten
    digit recognition, the representation of the network does not need to be very
    high. Therefore, adding layers is not very beneficial. In the large-scale general
    object recognition process, adding layers greatly improves recognition accuracy
    because it is a complicated problem.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于MNIST数据集，即使网络不是很深，也可以立即获得最高的准确率。对于像手写数字识别这样相对简单的问题，网络的表示能力不需要非常高。因此，增加层数并不是很有益。在大规模的通用目标识别过程中，增加网络的层数可以大大提高识别精度，因为这是一个复杂的问题。
- en: By examining the aforementioned high-ranked techniques, we can find techniques
    and tips for further improving recognition accuracy. For example, we can see that
    ensemble learning, learning rate decay, and **data augmentation** contribute to
    the improvement of recognition accuracy. Data augmentation is a simple but particularly
    effective method for improving recognition accuracy.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查上述高排名的技术，我们可以找到一些进一步提高识别准确率的技术和技巧。例如，我们可以看到，集成学习、学习率衰减和**数据增强**都有助于提高识别准确率。数据增强是一种简单但特别有效的提高识别准确率的方法。
- en: 'Data augmentation uses an algorithm to expand input images (training images)
    artificially. As shown in *Figure 8.4*, it adds the images by slightly changing
    the input images with rotation or vertical/horizontal movement. This is especially
    effective when the number of images in the dataset is limited:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强使用一种算法来人工扩展输入图像（训练图像）。如*图8.4*所示，它通过旋转或垂直/水平移动稍微改变输入图像来增加图像。当数据集中的图像数量有限时，这种方法尤其有效：
- en: '![Figure 8.4: Sample data augmentation'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.4：样本数据增强'
- en: '](img/Figure_8.4.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_8.4.jpg)'
- en: 'Figure 8.4: Sample data augmentation'
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.4：样本数据增强
- en: You can use data augmentation to expand images in various ways, other than the
    modifications shown in *Figure 8.4*. For example, you can cut out part of an image
    (or crop) or reverse an image horizontally (called flipping, though this is only
    effective when the symmetry of the image does not need to be considered). For
    ordinary images, changing their appearance (e.g., by adding brightness and scaling
    them up or down, is also effective. If you can use data augmentation to increase
    the number of training images, you can improve the recognition accuracy by using
    deep learning. This may seem a simple trick, but it often brings good results.
    We will not implement data augmentation here. Since implementing this is easy,
    please try it for yourself if you are interested.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过多种方式使用数据增强来扩展图像，而不仅仅是*图8.4*中所示的修改。例如，你可以裁剪图像的一部分（或剪裁）或水平翻转图像（称为翻转，但这仅在图像的对称性不需要考虑时有效）。对于普通图像，改变它们的外观（例如，通过调整亮度和缩放它们）也是有效的。如果你可以通过数据增强增加训练图像的数量，你可以通过深度学习提高识别准确率。虽然这看起来是一个简单的技巧，但它通常能带来良好的结果。我们在这里不实现数据增强。由于实现这一点非常简单，如果你有兴趣，可以自己尝试。
- en: Motivation for a Deeper Network
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更深网络的动机
- en: There is still much that is not known about the importance of making a network
    deeper. Although theoretical findings are insufficient now, past research and
    experiments can explain some things (rather intuitively). This section will provide
    some data and explanations that support the importance of "making a network deeper."
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 关于让网络变深的重要性，仍有很多未解之谜。尽管目前的理论研究尚不足够，但过去的研究和实验可以在某种程度上解释一些事情（相对直观）。本节将提供一些数据和解释，支持“让网络变深”这一观点的重要性。
- en: First, the results from competitions surrounding large-scale image recognition
    such as ILSVRC show the importance of "making a network deeper" (please see the
    next section for details). They indicate that many of the recent high-ranked techniques
    are based on deep learning and that the networks tend to go deeper. The deeper
    the network, the better the recognition performance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，来自大规模图像识别竞赛（如ILSVRC）的结果显示了“让网络变深”的重要性（详细信息请参见下一节）。这些结果表明，许多近期排名靠前的技术都基于深度学习，并且网络往往会变得更深。网络越深，识别性能越好。
- en: One of the advantages of this is that you can reduce the number of parameters
    in the network. When a network is deeper, it can achieve similar (or higher) representation
    with fewer parameters. This is easy to understand when you consider the filter
    size in a convolution operation. *Figure 8.5* shows a convolution layer with a
    5x5 filter.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个优势是你可以减少网络中的参数数量。当网络变得更深时，可以用更少的参数实现相似（或更高）的表现。考虑到卷积操作中的滤波器大小，这一点很容易理解。*图
    8.5* 显示了一个具有 5x5 滤波器的卷积层。
- en: 'Please note the area of the input data each node of the output data is calculated
    in. Of course, each output node is based on the 5x5 area of the input data in
    the example shown in *Figure 8.5*. Now, let''s think about a case where 3x3 convolution
    operations are repeated twice, as shown in *Figure 8.6*. In this case, intermediate
    data is based on a 3x3 area for each output node. So, which area of the previous
    input data is the 3x3 area of intermediate data based on? When you look at *Figure
    8.6* carefully, you will notice that it is based on a 5×5 area. Thus, the output
    data of *Figure 8.6* "looks at" a 5×5 area of input data for calculation:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个输出数据节点计算时所依据的输入数据区域。当然，每个输出节点都基于示例中*图 8.5*所示的输入数据的 5x5 区域。那么，现在我们来思考一个
    3x3 卷积操作重复两次的情况，如*图 8.6*所示。在这种情况下，中间数据是基于每个输出节点的 3x3 区域。那么，前一个输入数据的哪个区域是中间数据的
    3x3 区域依据的呢？当你仔细查看*图 8.6*时，你会发现它是基于一个 5×5 区域的。因此，*图 8.6*中的输出数据“查看”了输入数据的 5×5 区域进行计算：
- en: '![Figure 8.5: Example of a 5x5 convolution operation'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.5：5x5 卷积操作示例'
- en: '](img/Figure_8.5.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_8.5.jpg)'
- en: 'Figure 8.5: Example of a 5x5 convolution operation'
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.5：5x5 卷积操作示例
- en: '![Figure 8:6: Example of when 3x3 convolution operations are repeated twice'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8:6：3x3 卷积操作重复两次的示例'
- en: '](img/Figure_8.6.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_8.6.jpg)'
- en: 'Figure 8:6: Example of when 3x3 convolution operations are repeated twice'
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8:6：3x3 卷积操作重复两次的示例
- en: The area of one 5x5 convolution operation is equivalent to that of two 3x3 convolution
    operations. The former uses 25 parameters (5x5), while the latter uses 18 parameters
    (2x3x3) in total. Thus, multiple convolution layers reduce the number of parameters.
    As the network gets deeper, the reduced number of parameters becomes larger. For
    example, when 3x3 convolution operations are repeated three times, the number
    of parameters is 27 in total. To "look at" the same area with one convolution
    operation, a 7x7 filter is required, which means that the number of parameters
    goes up to 49.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 5x5 卷积操作的区域相当于两个 3x3 卷积操作的区域。前者使用 25 个参数（5x5），而后者总共使用 18 个参数（2x3x3）。因此，多个卷积层减少了参数数量。随着网络加深，减少的参数数量会变得更大。例如，当
    3x3 卷积操作重复三次时，参数总数为 27。为了通过一次卷积操作“查看”相同的区域，需要一个 7x7 的滤波器，这意味着参数数量上升至 49。
- en: Note
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The advantage of making a network deeper by applying a small filter several
    times is that it can reduce the number of parameters and expand the **receptive
    field** (a local space area that changes neurons). When you add layers, an activation
    function, such as ReLU, is placed between convolution layers, resulting in an
    improved network representation. This is because the activation function applies
    a "nonlinear" force to the network. Multiple nonlinear functions enable more complicated
    expressions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过多次应用小卷积核使网络更深的优势在于它可以减少参数数量并扩展**感受野**（改变神经元的局部空间区域）。当你增加层数时，卷积层之间会放置一个激活函数，如
    ReLU，从而改进网络表示。这是因为激活函数对网络施加了“非线性”力量。多个非线性函数能够表达更复杂的特征。
- en: Training efficiency is another advantage of making a network deeper. A deeper
    network can reduce training data and conduct training quickly. You can understand
    this intuitively by remembering the description provided in *Visualizing a CNN*
    section in *Chapter 7*, *Convolutional Neural Networks*. In that section, you
    learned that the convolution layers in a CNN extract information hierarchically.
    In the front convolution layer, neurons react to simple shapes such as edges.
    As a layer becomes deeper, neurons react to hierarchically more complicated shapes,
    such as textures and object parts.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 训练效率是使网络更深的另一个优势。更深的网络可以减少训练数据并快速进行训练。你可以通过记住在*第七章*《卷积神经网络》中的*可视化 CNN*部分提供的描述来直观地理解这一点。在该部分，你了解到
    CNN 中的卷积层是分层提取信息的。在前面的卷积层中，神经元对简单的形状（如边缘）做出反应。随着层次的加深，神经元对更加复杂的形状做出反应，比如纹理和物体部分。
- en: With such a hierarchical structure of a network in mind, consider the problem
    of recognizing a "dog." To solve this problem in a shallow network, convolution
    layers must "understand" many characteristics of a dog at one time. There are
    various types of dogs, and what they look like varies, depending on the environment
    in which the image was shot. Therefore, understanding the characteristics of a
    dog requires varied training data and a lot of time in training.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以这样的网络层次结构为基础，考虑识别“狗”的问题。要在浅层网络中解决这个问题，卷积层必须同时“理解”狗的许多特征。狗有多种类型，它们的外貌因拍摄图像的环境不同而有所变化。因此，理解狗的特征需要多样的训练数据和大量的训练时间。
- en: However, you can divide the problem to learn hierarchically by making a network
    deeper. Then, the problem for each layer to learn becomes simpler. For example,
    the first layer can concentrate on learning edges. Thus, the network can learn
    efficiently with a small amount of training data. This is because the number of
    images that contain edges is larger than that of images of a dog, and the pattern
    of an edge is simpler than that of a dog.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你可以通过加深网络来将问题进行层次化学习。这样，每一层学习的任务变得更简单。例如，第一层可以集中学习边缘。因此，网络可以通过少量的训练数据高效学习。这是因为包含边缘的图像数量大于包含狗的图像数量，边缘的模式也比狗的模式简单。
- en: It is also important that you can pass information hierarchically by making
    a network deeper. For example, the layer next to the one that extracted edges
    can use edge information, so we can expect it to learn more advanced patterns
    efficiently. In short, by making a network deeper, you can divide the problem
    for each layer to learn into "simple problems that are easy to solve" so that
    you can expect efficient training.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是，通过加深网络，你可以层次化地传递信息。例如，提取边缘的下一层可以利用边缘信息，因此我们可以期望它能够高效地学习更高级的模式。简而言之，通过加深网络，你可以将每一层学习的问题分解成“容易解决的简单问题”，从而期望高效的训练。
- en: This is the explanation that supports the importance of "making a network deeper."
    Please note that deeper networks in recent years have been provided by new techniques
    and environments, such as big data and computer power, which enable correct training
    in a deep network.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是支持“加深网络”重要性的解释。请注意，近年来更深层的网络得益于新的技术和环境，如大数据和计算能力，这些都使得深度网络能够正确训练。
- en: A Brief History of Deep Learning
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习简史
- en: It is said that deep learning started to draw a lot of attention in the competition
    of large-scale image recognition due to the **ImageNet Large Scale Visual Recognition
    Challenge** (**ILSRVC**), which was held in 2012\. In the competition, a deep
    learning technique called AlexNet achieved an overwhelming win, overturning the
    traditional approaches to image recognition. Since deep learning launched a counterattack
    in 2012, it has always played the leading role in subsequent competitions. Here,
    we will look at the current trend of deep learning around the competition of large-scale
    image recognition, known as ILSVRC.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 据说深度学习之所以在大规模图像识别竞赛中引起广泛关注，是因为**ImageNet大规模视觉识别挑战赛**（**ILSVRC**）于2012年举行。在比赛中，一种名为AlexNet的深度学习技术取得了压倒性的胜利，颠覆了传统的图像识别方法。自2012年深度学习反击以来，它一直在随后的竞赛中扮演着主导角色。在这里，我们将探讨当前深度学习在大规模图像识别竞赛（即ILSVRC）中的趋势。
- en: ImageNet
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ImageNet
- en: 'ImageNet (*J. Deng, W. Dong, R. Socher, L.J. Li, Kai Li, and Li Fei-Fei (2009):
    ImageNet: A large-scale hierarchical image database*. In IEEE Conference on *Computer
    Vision and Pattern Recognition, 2009\. CVPR 2009\. 248 – 255\. DOI*: ([http://dx.doi.org/10.1109/CVPR.2009.5206848](http://dx.doi.org/10.1109/CVPR.2009.5206848)))
    is a dataset that contains more than 1 million images. As shown in *Figure 8.7*,
    it contains various types of images, and each image is associated with a label
    (class name). An image recognition competition called ILSVRC is held every year
    using this huge dataset:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet (*J. Deng, W. Dong, R. Socher, L.J. Li, Kai Li, 和Li Fei-Fei (2009)：ImageNet：一个大规模层次化图像数据库*。在IEEE计算机视觉与模式识别大会，2009年。CVPR
    2009。248–255。DOI*：([http://dx.doi.org/10.1109/CVPR.2009.5206848](http://dx.doi.org/10.1109/CVPR.2009.5206848)))
    是一个包含超过100万张图像的数据集。如*图8.7*所示，它包含了各种类型的图像，每张图像都与一个标签（类别名）相关联。每年都会使用这个庞大的数据集举行一个名为ILSVRC的图像识别比赛：
- en: '![Figure 8.7: Sample data in the large-scale ImageNet dataset'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.7：ImageNet大规模数据集中的示例数据'
- en: '](img/Figure_8.7.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_8.7.jpg)'
- en: 'Figure 8.7: Sample data in the large-scale ImageNet dataset'
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图8.7: 大规模ImageNet数据集中的示例数据'
- en: Note
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注
- en: '*Figure 8.7* is cited from reference, *J. Deng, W. Dong, R. Socher, L.J. Li,
    Kai Li, and Li Fei-Fei (2009): ImageNet: A large-scale hierarchical image database*.
    In IEEE Conference on *Computer Vision and Pattern Recognition, 2009\. CVPR 2009\.
    248 – 255\. DOI:* ([http://dx.doi.org/10.1109/CVPR.2009.5206848](http://dx.doi.org/10.1109/CVPR.2009.5206848)).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.7* 引自参考文献，*J. Deng, W. Dong, R. Socher, L.J. Li, Kai Li, 和 Li Fei-Fei (2009):
    ImageNet: A large-scale hierarchical image database*。收录于IEEE计算机视觉与模式识别会议 *Computer
    Vision and Pattern Recognition, 2009\. CVPR 2009\. 248 – 255\. DOI:* ([http://dx.doi.org/10.1109/CVPR.2009.5206848](http://dx.doi.org/10.1109/CVPR.2009.5206848))。'
- en: 'The ILSVRC competition provides some test items, and one of them is "classification"
    (in the "classification" division, 1,000 classes are classified to compete in
    recognition accuracy). *Figure 8.8* shows the results of the winning teams for
    ILSVRC''s classification division since 2010\. Here, a classification is regarded
    as "correct" if the top 5 predictions contain the correct class. The following
    bar graphs show the error rates:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ILSVRC比赛提供了一些测试项目，其中一个是“分类”（在“分类”项目中，1,000个类别通过识别准确率进行竞赛）。*图8.8*显示了自2010年以来ILSVRC分类项目的获胜队伍结果。在这里，如果前五个预测中包含正确的类别，则认为分类是“正确的”。以下条形图展示了错误率：
- en: '![Figure 8.8: The results of the winning teams in ILSVRC – the vertical axis
    shows error rates,'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.8: ILSVRC比赛获胜队伍的结果—纵坐标显示错误率，'
- en: while the horizontal axis shows years. Team names or technique names are shown
    in the
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，横坐标显示年份。队伍名称或技术名称显示在
- en: parentheses on the horizontal axis.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 横坐标上的括号。
- en: '](img/Figure_8.8.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_8.8.jpg)'
- en: 'Figure 8.8: The results of the winning teams in ILSVRC – the vertical axis
    shows error rates, while the horizontal axis shows years. Team names or technique
    names are shown in the parentheses on the horizontal axis.'
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图8.8: ILSVRC比赛获胜队伍的结果—纵坐标显示错误率，横坐标显示年份。队伍名称或技术名称显示在横坐标上的括号内。'
- en: Please note from the preceding graph that deep learning techniques have always
    been on top since 2012\. Actually, we can see that, in 2012, AlexNet significantly
    reduced the error rate. Since then, deep learning techniques have steadily improved
    in terms of accuracy. This was especially apparent with ResNet in 2015, which
    was a deep network with more than 150 layers and had reduced the error rate to
    3.5%. It is even said that this result exceeded the recognition capability of
    ordinary humans.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意从前面的图表中可以看出，自2012年以来，深度学习技术一直处于领先地位。实际上，我们可以看到，在2012年，AlexNet显著降低了错误率。从那时起，深度学习技术在准确性方面稳步提升。这一点在2015年的ResNet中尤为明显，ResNet是一个具有超过150层的深度网络，并且将错误率降至3.5%。甚至有人说，这一结果超过了普通人类的识别能力。
- en: Among the deep learning networks that have achieved great results for the past
    several years, VGG, GoogLeNet, and ResNet are the most famous. You will come across
    them at various places relevant to deep learning. I will introduce these three
    famous networks briefly next.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，取得显著成果的深度学习网络中，VGG、GoogLeNet和ResNet最为著名。你会在许多与深度学习相关的地方遇到它们。接下来我将简要介绍这三种著名网络。
- en: VGG
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VGG
- en: 'VGG is a "basic" CNN that consists of convolution layers and pooling layers.
    As shown in *Figure 8.9*, it can have as many as 16 (or 19) layers with weights
    (convolution layers and fully connected layers) to make itself deep and is sometimes
    called "VGG16" or "VGG19" based on the number of layers:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: VGG是一个“基础”卷积神经网络（CNN），由卷积层和池化层组成。如*图8.9*所示，它可以拥有最多16层（或19层）带权重的层（卷积层和全连接层），使得网络更深，并且根据层数的不同，有时被称为“VGG16”或“VGG19”：
- en: '![Figure 8.9: VGG'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.9: VGG'
- en: '](img/fig08_9.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig08_9.jpg)'
- en: 'Figure 8.9: VGG'
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图8.9: VGG'
- en: Note
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注
- en: '*Figure 8.9* is cited from reference, *Karen Simonyan and Andrew Zisserman
    (2014): Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv:1409.1556[cs]
    (September 2014)*.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.9* 引自参考文献，*Karen Simonyan 和 Andrew Zisserman (2014): Very Deep Convolutional
    Networks for Large-Scale Image Recognition. arXiv:1409.1556[cs] (2014年9月)*。'
- en: VGG contains consecutive convolution layers with a small 3x3 filter. As shown
    in the preceding image, two or four consecutive convolution layers and a pooling
    layer halve the size, and this process is repeated. Finally, the result is provided
    via fully connected layers.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: VGG包含连续的卷积层，并使用小的3x3滤波器。如前图所示，两个或四个连续的卷积层和一个池化层将尺寸减半，并且这一过程会重复进行。最终，结果通过全连接层给出。
- en: Note
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注
- en: VGG won second prize in the 2014 competition (GoogLeNet, which is described
    next, won in 2014). Its performance was not as good as the first-place network,
    but many engineers prefer to use VGG-based networks because they are very simple
    in structure and versatile.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: VGG 在 2014 年的比赛中获得了第二名（接下来的 GoogLeNet 获得了 2014 年的冠军）。它的性能不如第一名的网络，但许多工程师更喜欢使用基于
    VGG 的网络，因为其结构非常简单且具有广泛的适用性。
- en: GoogLeNet
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GoogLeNet
- en: '*Figure 8.10* shows the network architecture for GoogLeNet. The rectangles
    represent the various layers, such as convolution and pooling layers:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.10* 显示了 GoogLeNet 的网络架构。矩形表示各种层，例如卷积层和池化层：'
- en: '![Figure 8.10: GoogLeNet'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.10：GoogLeNet'
- en: '](img/fig08_10.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig08_10.jpg)'
- en: 'Figure 8.10: GoogLeNet'
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.10：GoogLeNet
- en: Note
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '*Figure 8.10* and *Figure 8.11* are cited from *Christian Szegedy et al. (2015):
    Going Deeper With Convolutions. In The IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR)*.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.10* 和 *图 8.11* 引自 *Christian Szegedy 等人 (2015)：通过卷积更深。发表于 IEEE 计算机视觉与模式识别大会（CVPR）*。'
- en: Its network architecture seems very complicated when you look at it, but it
    is basically the same as that of a CNN. What is distinctive about GoogLeNet is
    that the network not only has depth in the vertical direction but also in the
    horizontal direction (spread).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当你观察它时，网络架构似乎非常复杂，但本质上与 CNN 的架构相似。GoogLeNet 的独特之处在于，网络不仅在垂直方向上有深度，而且在水平方向上也有深度（扩展）。
- en: 'GoogLeNet has "width" in the horizontal direction. It is called an "inception
    architecture" and is based on the structure shown in *Figure 8.11*:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: GoogLeNet 在水平方向上有“宽度”。它被称为“inception 架构”，并基于*图 8.11*所示的结构：
- en: '![Figure 8.11: Inception architecture of GoogLeNet'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.11：GoogLeNet 的 Inception 架构'
- en: '](img/Figure_8.11.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_8.11.jpg)'
- en: 'Figure 8.11: Inception architecture of GoogLeNet'
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.11：GoogLeNet 的 Inception 架构
- en: As shown in *Figure 8.11*, the inception architecture applies multiple filters
    of different sizes (and pooling) and combines the results. Using this inception
    architecture as one building block (component) is the main characteristic of GoogLeNet.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 8.11*所示，Inception 架构应用了多种不同大小的滤波器（及池化）并将结果合并。将这一 Inception 架构作为一个构建模块（组件）是
    GoogLeNet 的主要特点。
- en: GoogLeNet uses convolution layers with a 1x1 filter in many places. This 1x1
    convolution operation reduces the size in the channel direction to reduce the
    number of parameters and accelerate processing.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: GoogLeNet 在许多地方使用 1x1 滤波器的卷积层。这种 1x1 卷积操作减少了通道方向上的大小，从而减少了参数的数量并加速了处理。
- en: ResNet
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ResNet
- en: 'ResNet (*Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun (2015): Deep
    Residual Learning for Image Recognition. arXiv:1512.03385[cs] (December 2015)*)
    is a network developed by a team at Microsoft. It is characterized by a "mechanism"
    that can make the network deeper than ever.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 'ResNet (*Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun (2015): 深度残差学习用于图像识别.
    arXiv:1512.03385[cs] (2015年12月)*) 是微软团队开发的网络。其特点是具有一种“机制”，能够使网络比以往更深。'
- en: Making a network deeper is important to improve its performance. However, when
    a network becomes too deep, deep learning fails and the final performance is often
    poor. To solve this problem, ResNet introduced a "skip architecture" (also called
    "shortcut" or "bypass"). By introducing this skip architecture, performance can
    be improved as the network becomes deeper (though there is a limit to permissible
    depth).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 增加网络深度对提高其性能非常重要。然而，当网络变得过深时，深度学习就会失败，最终性能通常较差。为了解决这个问题，ResNet 引入了“跳跃架构”（也称为“快捷方式”或“旁路”）。通过引入这一跳跃架构，随着网络深度的增加，性能可以得到提升（尽管深度有一定限制）。
- en: 'The skip architecture skips convolution layers in the input data to add the
    input data to the output, as shown in *Figure 8.12*:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 跳跃架构跳过输入数据中的卷积层，将输入数据添加到输出中，如*图 8.12*所示：
- en: '![Figure 8.12: Components of ResNet – the "weight layer" here indicates a convolution
    layer'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.12：ResNet 的组成——这里的“权重层”表示一个卷积层'
- en: '](img/fig08_12.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig08_12.jpg)'
- en: 'Figure 8.12: Components of ResNet – the "weight layer" here indicates a convolution
    layer'
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.12：ResNet 的组成——这里的“权重层”表示一个卷积层
- en: Note
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '*Figure 8.12* and *Figure 8.13* are cited from reference, *Kaiming He, Xiangyu
    Zhang, Shaoqing Ren, and Jian Sun (2015): Deep Residual Learning for Image Recognition.
    arXiv:1512.03385[cs] (December 2015)*.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.12* 和 *图 8.13* 引自文献，*Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
    Sun (2015): 深度残差学习用于图像识别. arXiv:1512.03385[cs] (2015年12月)*。'
- en: In *Figure 8.12*, the input, *x*, is connected to the output by skipping two
    consecutive convolution layers. The output of two convolution layers is originally
    *F(x)*, while the skip architecture changes it to *F(x) + x*.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图8.12*中，输入*x*通过跳过两个连续的卷积层与输出相连。两个卷积层的输出本来是*F(x)*，而跳跃架构则将其改为*F(x) + x*。
- en: Adopting this skip architecture enables efficient learning, even when the network
    is deep. This is because the skip architecture transmits signals without decay
    during backward propagation.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种跳跃架构使得即使网络很深，也能够高效地进行学习。这是因为跳跃架构在反向传播时传递信号而不发生衰减。
- en: Note
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The skip architecture only passes input data "as it is." In backward propagation,
    it also passes the gradients from the upper stream "as they are" to the lower
    stream without them being changed. Therefore, you don't need to be worried about
    the gradients becoming small (or too large) with the skip architecture. You can
    expect "meaningful gradients" to be transmitted to the front layers. You can also
    expect the skip architecture to alleviate a traditional gradient vanishing problem
    that reduces gradients as the network becomes deeper.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 跳跃架构只将输入数据“原样”传递。在反向传播时，它也将来自上游的梯度“原样”传递给下游，不会发生变化。因此，你不需要担心梯度变小（或过大）的问题。你可以预期“有意义的梯度”会传递到前面的层。你也可以预期跳跃架构能够缓解传统的梯度消失问题，避免随着网络加深，梯度逐渐减小。
- en: 'ResNet is based on the VGG network we described earlier and adopts the skip
    architecture to make the network deeper. *Figure 8.13* shows the result of this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet基于我们之前描述的VGG网络，并采用跳跃架构使网络更深。*图8.13*展示了这一结果：
- en: '![Figure 8.13: ResNet – blocks support 3x3 convolution layers. Its characteristic
    is the skip'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.13：ResNet – 块支持3x3卷积层。其特点是跳跃'
- en: architecture, which skips layers.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 架构，它跳过了某些层。
- en: '](img/fig08_13.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig08_13.jpg)'
- en: 'Figure 8.13: ResNet – blocks support 3x3 convolution layers. Its characteristic
    is the skip architecture, which skips layers.'
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.13：ResNet – 块支持3x3卷积层。其特点是跳跃架构，即跳过某些层。
- en: As shown in *Figure 8.13*, ResNet skips two convolution layers to make the network
    deeper. Experiments have shown that recognition accuracy continues to improve,
    even when the network contains 150 or more layers. In the ILSVRC competition,
    it achieved an amazing result of 3.5% in terms of error rate (the percentage of
    correct classes that were not included in the top 5 predictions).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图8.13*所示，ResNet跳过了两个卷积层，从而使得网络更深。实验表明，即使网络包含150层或更多层，识别精度仍然持续提高。在ILSVRC比赛中，它取得了3.5%的惊人结果（即错误率，表示正确类别未包含在前5个预测中的比例）。
- en: Note
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Weight data that's trained by using the huge ImageNet dataset is often used
    effectively. This is called **transfer learning**. Part of the trained weights
    is copied to another neural network for fine-tuning. For example, a network that
    has the same structure as VGG is provided. Trained weights are used as initial
    values, and fine-tuning is conducted for a new dataset. Transfer learning is especially
    effective when you have a few datasets at hand.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用巨大的ImageNet数据集训练的权重数据通常能够有效地使用。这被称为**迁移学习**。将部分训练好的权重复制到另一个神经网络中进行微调。例如，提供一个与VGG结构相同的网络。将训练好的权重作为初始值，并针对新数据集进行微调。当数据集较少时，迁移学习尤其有效。
- en: Accelerating Deep Learning
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加速深度学习
- en: Big data and large-scale networks require massive operations in deep learning.
    We have used CPUs for calculations so far, but CPUs alone are not sufficient to
    tackle deep learning. In fact, many deep learning frameworks support **Graphics
    Processing Units** (**GPUs**) to process a large number of operations quickly.
    Recent frameworks are starting to support distributed learning by using multiple
    GPUs or machines. This section describes accelerating calculations in deep learning.
    Our implementations of deep learning ended in section 8.1\. We will not implement
    the acceleration (such as support of GPUs) described here.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据和大规模网络要求在深度学习中进行大量操作。我们迄今为止使用了CPU进行计算，但仅仅依靠CPU不足以应对深度学习的需求。实际上，许多深度学习框架支持**图形处理单元**（**GPU**）来快速处理大量操作。最近的框架开始通过使用多个GPU或机器来支持分布式学习。本节描述了加速深度学习计算的方法。我们在第8.1节中结束了深度学习的实现。在这里，我们不会实现所描述的加速（例如GPU的支持）。
- en: Challenges to Overcome
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 面临的挑战
- en: 'Before discussing the acceleration of deep learning, let''s see what processes
    take time in deep learning. The pie charts in *Figure 8.14* show the time spent
    on each class in the forward processing of AlexNet:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论加速深度学习之前，我们先来看一下深度学习中哪些过程需要消耗时间。*图8.14*中的饼图展示了AlexNet在前向处理过程中各类操作所消耗的时间：
- en: '![Figure 8.14: Percentage of time that each layer spends in the forward processing
    of AlexNet – the left-hand chart shows GPU time, while the right-hand one shows
    CPU time'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.14：AlexNet前向处理过程中各层所花时间的百分比——左侧图表显示GPU时间，右侧图表显示CPU时间'
- en: '](img/Figure_8.14.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_8.14.jpg)'
- en: 'Figure 8.14: Percentage of time that each layer spends in the forward processing
    of AlexNet – the left-hand chart shows GPU time, while the right-hand one shows
    CPU time'
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.14：AlexNet前向处理过程中各层所花时间的百分比——左侧图表显示GPU时间，右侧图表显示CPU时间
- en: 'Here, "conv" indicates a convolution layer, "pool" indicates a pooling layer,
    "fc" indicates a fully connected layer, and "norm" indicates a normalization layer
    (cited from *Jia Yangqing (2014): Learning Semantic Image Representations at a
    Large Scale. PhD thesis, EECS Department, University of California, Berkeley,
    May 2014*, ([http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-93.html](http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-93.html))).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，“conv”表示卷积层，“pool”表示池化层，“fc”表示全连接层，“norm”表示归一化层（引用自*贾扬清（2014）：大规模学习语义图像表示。博士论文，加利福尼亚大学伯克利分校电子电气计算机系，2014年5月*，[http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-93.html](http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-93.html))）。
- en: As you can see, convolution layers spend a lot of time in AlexNet. Actually,
    the total processing time in convolution layers reaches 95% of GPU time and 89%
    of CPU time! Therefore, conducting fast and efficient operations in convolution
    layers is the main challenge of deep learning. *Figure 8.14* shows the results
    in the inference phase, but convolution layers spend a lot of time in the training
    phase as well.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，卷积层在AlexNet中花费了大量的时间。实际上，卷积层的总处理时间占GPU时间的95%和CPU时间的89%！因此，在卷积层进行快速高效的操作是深度学习的主要挑战。*图8.14*展示了推理阶段的结果，但卷积层在训练阶段也花费了大量时间。
- en: Note
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注释
- en: As explained in the *Convolution Layers* topic in *Chapter 7*, *Convolutional
    Neural Networks*, operations in convolution layers are basically "multiply-accumulate
    operations." Therefore, accelerating deep learning depends on how massive "multiply-accumulate
    operations" are calculated quickly and efficiently.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如在*第7章*中*卷积神经网络*部分所述，卷积层中的操作基本上是“乘加运算”。因此，加速深度学习的关键在于如何快速高效地进行大量的“乘加运算”。
- en: Using GPUs for Acceleration
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用GPU进行加速
- en: Originally, GPUs were exclusively used for graphics. Recently, they have been
    used for general numerical calculations, as well as graphics processing. Because
    GPUs can conduct parallel arithmetic operations quickly, GPU computing uses its
    overwhelming power for various purposes.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，GPU仅用于图形处理。最近，GPU不仅用于图形处理，还用于一般数值计算。由于GPU能够快速进行并行算术运算，GPU计算利用其强大的计算能力广泛应用于各种领域。
- en: 'Deep learning requires massive multiply-accumulate operations (or products
    of large matrices). GPUs are good at such massive parallel operations, while CPUs
    are good at continuous and complicated calculations. You can use a GPU to accelerate
    deep learning operations surprisingly compared to using only a CPU. *Figure 8.15*
    compares the time that AlexNet took for learning between a CPU and a GPU:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习需要大量的乘加运算（或大矩阵的乘积）。GPU擅长这种大规模并行运算，而CPU擅长连续的复杂计算。与仅使用CPU相比，使用GPU来加速深度学习运算的效果非常显著。*图8.15*对比了AlexNet在CPU和GPU上学习所花费的时间：
- en: '![Figure 8.15: Comparing the time that AlexNet took for learning between a
    "16-core Xeon CPU"'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.15：对比AlexNet在“16核Xeon CPU”和“Titan系列”GPU之间的学习时间'
- en: and a "Titan series" GPU
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 和一款“Titan系列”GPU
- en: '](img/Figure_8.15.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_8.15.jpg)'
- en: 'Figure 8.15: Comparing the time that AlexNet took for learning between a "16-core
    Xeon CPU" and a "Titan series" GPU'
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.15：对比AlexNet在“16核Xeon CPU”和“Titan系列”GPU之间的学习时间
- en: Note
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注释
- en: '*Figure 8.15* is cited from reference, *NVIDIA blog "NVIDIA Propels Deep Learning
    with TITAN X, New DIGITS Training System and DevBox"* ([https://blogs.nvidia.com/blog/2015/03/17/digits-devbox/](https://blogs.nvidia.com/blog/2015/03/17/digits-devbox/)).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.15*引用自参考文献，*NVIDIA博客 "NVIDIA通过TITAN X、新的DIGITS训练系统和DevBox推动深度学习"* ([https://blogs.nvidia.com/blog/2015/03/17/digits-devbox/](https://blogs.nvidia.com/blog/2015/03/17/digits-devbox/))。'
- en: As you can see, the CPU took more than 40 days, while the GPU took only 6 days.
    We can also see that using the cuDNN library, which is optimized for deep learning,
    accelerates the training further.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，CPU用了超过40天，而GPU只用了6天。我们还可以看到，使用针对深度学习优化的cuDNN库，进一步加速了训练过程。
- en: GPUs are mainly provided by two companies, NVIDIA and AMD. Although you can
    use both of their GPUs for general arithmetic operations, NVIDIA's GPUs are more
    "familiar" with deep learning. Actually, many deep learning frameworks can benefit
    only from NVIDIA's GPUs. This is because CUDA, which is an integrated development
    environment for GPU computing provided by NVIDIA, is used in deep learning frameworks.
    cuDNN, which can be seen in *Figure 8.15*, is a library that runs on CUDA in which
    the functions optimized for deep learning are implemented.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: GPU主要由两家公司提供，NVIDIA和AMD。尽管您可以使用两者的GPU进行一般的算术运算，但NVIDIA的GPU更“熟悉”深度学习。实际上，许多深度学习框架只能从NVIDIA的GPU中受益。这是因为NVIDIA提供的GPU计算集成开发环境CUDA被广泛用于深度学习框架中。*图8.15*中所示的cuDNN库运行在CUDA上，实现了针对深度学习优化的各种功能。
- en: Note
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意事项
- en: We used `im2col` to convert the operations in a convolution layer into the products
    of large matrices. Implementing this `im2col` method is suitable for GPUs. GPUs
    are good at calculating a large batch at a stretch rather than calculating small
    batches one by one. Using `im2col` to calculate the products of huge matrices
    makes it easy to exhibit a GPU's real power.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了`im2col`将卷积层中的操作转化为大矩阵的乘积。实现这一`im2col`方法适合GPU。GPU擅长一次性计算大批量数据，而不是逐个计算小批量数据。通过使用`im2col`计算巨大的矩阵乘积，可以轻松展现GPU的真正实力。
- en: Distributed Training
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布式训练
- en: You can accelerate deep learning operations by using a GPU, but a deep network
    still requires several days or weeks for training. As we have seen so far, deep
    learning involves lots of trial and error. You must try many things to create
    a good network. Naturally, you want to reduce the time required for training as
    much as possible. Then, scaling deep learning out or "distributed training" becomes
    important.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用GPU，您可以加速深度学习的操作，但深度网络仍然需要几天甚至几周的时间来完成训练。正如我们到目前为止所看到的，深度学习涉及大量的试错过程。为了创建一个好的网络，您必须尝试许多方法。自然，您希望尽可能减少训练所需的时间。于是，扩展深度学习或“分布式训练”变得至关重要。
- en: To further accelerate the calculations required for deep learning, you may want
    to distribute them among multiple GPUs or machines. Now, some deep learning frameworks
    support distributed training by multiple GPUs or machines. Among them, Google's
    TensorFlow and Microsoft's **Computational Network Toolkit** (**CNTK**) have been
    developed to focus on distributed training. Based on low-delay and high-throughput
    networks in huge data centers, distributed training by these frameworks achieves
    surprising results.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步加速深度学习所需的计算，您可能希望将计算任务分布到多个GPU或机器上。现在，一些深度学习框架支持通过多个GPU或机器进行分布式训练。其中，谷歌的TensorFlow和微软的**计算网络工具包**（**CNTK**）已被开发出来，专注于分布式训练。基于数据中心中低延迟和高吞吐量的网络，这些框架的分布式训练取得了令人惊讶的效果。
- en: How much can distributed training accelerate deep learning? The answer is that
    the larger the number of GPUs, the faster the training speed. In fact, 100 GPUs
    (a total of 100 GPUs installed on multiple machines) achieves a 56-fold speedup
    compared with one GPU. This means that training that usually takes 7 days is completed
    in only 3 hours, for example, and indicates the surprising effect of distributed
    training.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练能够加速深度学习的速度吗？答案是：GPU数量越多，训练速度越快。事实上，100个GPU（即安装在多台机器上的100个GPU）相比单个GPU，能实现56倍的加速。这意味着，通常需要7天才能完成的训练，使用100个GPU只需3小时，充分展示了分布式训练的惊人效果。
- en: '"How to distribute calculations" in distributed training is a very difficult
    problem. It contains many problems that are not easy to solve, such as communication
    and data synchronization between machines. You can leave such difficult problems
    to excellent frameworks such as TensorFlow. Here, we will not discuss the details
    of distributed training. For the technical details of distributed training, please
    see the technical paper (white paper) about TensorFlow (*Mart í n Abadi et al.
    (2016): TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed
    Systems. arXiv:1603.04467[cs] (March 2016)*).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练中的“如何分配计算”是一个非常难的问题。它包含了许多难以解决的问题，例如机器之间的通信和数据同步。你可以将这些难题交给优秀的框架，如TensorFlow。在这里，我们不会讨论分布式训练的细节。如需了解分布式训练的技术细节，请参阅有关TensorFlow的技术论文（白皮书）（*Martín
    Abadi 等人（2016）：TensorFlow：在异构分布式系统上进行大规模机器学习。arXiv:1603.04467[cs]（2016年3月）*）。
- en: Reducing the Bit Number for Arithmetic Precision
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 降低算术精度的位数
- en: Memory space and bus bandwidth, as well as computational complexity, can be
    bottlenecks in accelerating deep learning. For memory space, a large number of
    weight parameters and intermediate data must be stored in memory. For bus bandwidth,
    a bottleneck occurs when the data that flows through the GPU (or CPU) bus increases,
    exceeding a limit. In these cases, you want the bit number of the data flowing
    in the network to be as small as possible.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 内存空间和总线带宽，以及计算复杂度，可能成为加速深度学习的瓶颈。对于内存空间，必须在内存中存储大量的权重参数和中间数据。对于总线带宽，当通过GPU（或CPU）总线的数据量增加并超过限制时，会出现瓶颈。在这些情况下，你希望网络中流动的数据的位数尽可能小。
- en: A computer mainly uses 64- or 32-bit floating-point numbers to represent real
    numbers. Using many bits to represent a number reduces the influence of the error
    at numerical calculation but increases the processing cost and memory usage, placing
    a load on the bus bandwidth.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机主要使用64位或32位浮点数来表示实数。使用更多的位数表示数字可以减少数值计算中的误差影响，但会增加处理成本和内存使用量，并对总线带宽造成负担。
- en: From what we know about deep learning regarding numerical precision (how many
    bits are used to represent a numeric value), it does not need very high precision.
    This is one of the most important characteristics of a neural network due to its
    robustness. The robustness here means that, for example, the output result will
    not change in a neural network, even if the input images contain a small amount
    of noise. Think of it as a small influence on the output result because of the
    robustness, even if the data flowing in a network is "deteriorated."
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们对深度学习中数值精度（即表示数值所使用的位数）的了解，它并不需要非常高的精度。这是神经网络最重要的特性之一，因为它具有鲁棒性。这里的鲁棒性意味着，例如，即使输入图像中含有少量噪声，神经网络的输出结果也不会改变。可以把它理解为即使网络中的数据“退化”，鲁棒性也使得输出结果受影响较小。
- en: 'A computer usually uses 32-bit single-precision floating-point representations
    or 64-bit double-precision floating-point representations to represent a decimal.
    Experiments have shown that 16-bit `float`) are sufficient in deep learning (*Suyog
    Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan (2015): Deep
    learning with limited numerical precision. CoRR, abs/1502.02551 392 (2015)*).
    Actually, the Pascal architecture used for NVIDIA''s generation GPUs supports
    the operation of half-precision floating-point numbers. It is thought that the
    half format will be used as the standard in the future.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机通常使用32位单精度浮点数表示法或64位双精度浮点数表示法来表示十进制数。实验表明，16位`float`在深度学习中足够使用（*Suyog Gupta,
    Ankur Agrawal, Kailash Gopalakrishnan, 和 Pritish Narayanan (2015)：使用有限数值精度的深度学习。CoRR,
    abs/1502.02551 392 (2015)*）。实际上，NVIDIA的Pascal架构支持半精度浮点数的运算。人们认为未来半精度格式将成为标准。
- en: Note
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: NVIDIA's Maxwell generation of GPUs supported the storage of half-accuracy floating-point
    numbers (to maintain data), but it did not conduct 16-bit operations. The next-generation
    Pascal architecture conducts 16-bit operations as well. We can expect that only
    using half-accuracy floating-point numbers for calculations will accelerate processing
    so that it's around twice as fast as a previous-generation GPU.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA的Maxwell架构的GPU支持存储半精度浮点数（以保持数据），但并未进行16位运算。下一代Pascal架构则进行了16位运算。我们可以预期，仅使用半精度浮点数进行计算将加速处理，使其速度大约是上一代GPU的两倍。
- en: We haven't covered numerical precision in the preceding implementations of deep
    learning. Python generally uses 64-bit floating-point numbers. NumPy provides
    a 16-bit half-accuracy floating-point data type (however, it is used only for
    storage, not for operations). We can easily show that using NumPy's half-accuracy
    floating-point numbers do not reduce recognition accuracy. If you are interested,
    please see `ch08/half_float_network.py`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的深度学习实现中，我们没有涉及数值精度的问题。Python 通常使用 64 位浮点数。NumPy 提供了一种 16 位半精度浮点数据类型（但仅用于存储，而非计算）。我们可以很容易地证明，使用
    NumPy 的半精度浮点数不会降低识别准确性。如果您感兴趣，请参见 `ch08/half_float_network.py`。
- en: 'Some research has been conducted into reducing the bit number in deep learning.
    In recent research, a technique called a "binarized neural network" was proposed
    (*Matthieu Courbariaux and Yoshua Bengio (2016): Binarized Neural Networks: Training
    Deep Neural Networks with Weights and Activations Constrained to +1 or -1\. arXiv
    preprint arXiv:1602.02830 (2016)*). It represents the weights and intermediate
    data by 1 bit. Reducing the number of bits to accelerate deep learning is a topic
    we should keep our eyes on. It is especially important when we''re thinking of
    using deep learning for embedded devices.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '一些研究已经探讨了在深度学习中减少位数的问题。在最近的研究中，提出了一种名为“二值化神经网络”的技术（*Matthieu Courbariaux 和
    Yoshua Bengio (2016): 二值化神经网络：训练具有约束权重和激活值为 +1 或 -1 的深度神经网络。arXiv 预印本 arXiv:1602.02830
    (2016)*）。该技术用 1 位表示权重和中间数据。减少位数以加速深度学习是一个我们应该关注的话题，尤其是在考虑将深度学习用于嵌入式设备时。'
- en: Practical Uses of Deep Learning
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习的实际应用
- en: As an example of using deep learning, we have mainly discussed image classification,
    such as handwritten digit recognition, which is called "object recognition." However,
    we can apply deep learning to many problems other than object recognition. Deep
    learning demonstrates excellent performance for many problems, such as image recognition,
    sound (speech recognition), and natural language processing. This section will
    introduce what deep learning can do (its applications) in the computer vision
    field.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 作为使用深度学习的一个例子，我们主要讨论了图像分类，如手写数字识别，这被称为“物体识别”。然而，除了物体识别，我们还可以将深度学习应用于许多其他问题。深度学习在许多问题中表现出色，如图像识别、语音（语音识别）和自然语言处理。本节将介绍深度学习在计算机视觉领域的应用。
- en: Object Detection
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 物体检测
- en: Object detection identifies the positions of objects in images and classifies
    them. Object detection is more difficult than object recognition. While object
    recognition targets the entire image, object detection must identify the positions
    of classes in an image, and multiple objects may exist.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测识别图像中物体的位置并对其进行分类。物体检测比物体识别更具挑战性。物体识别的目标是识别整个图像，而物体检测则必须识别图像中各类物体的位置，并且可能存在多个物体。
- en: Some CNN-based techniques have been proposed for object detection. They demonstrate
    excellent performance, which indicates that deep learning is also effective for
    object detection.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一些基于 CNN 的技术已被提出用于物体检测。这些技术表现出色，表明深度学习在物体检测中同样有效。
- en: 'Among CNN-based object detection techniques, a technique called R-CNN (*Ross
    Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik (2014): Rich Feature
    Hierarchies for Accurate Object Detection and Semantic Segmentation. In 580 –
    587*) is famous. *Figure 8.16* shows the process flow of R-CNN:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '在基于 CNN 的物体检测技术中，一种名为 R-CNN（*Ross Girshick, Jeff Donahue, Trevor Darrell, 和
    Jitendra Malik (2014): 用于精确物体检测和语义分割的丰富特征层次结构. 第 580 – 587 页*）的技术非常著名。*图 8.16*
    展示了 R-CNN 的流程：'
- en: '![Figure 8.16: Process flow of R-CNN'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.16：R-CNN 流程图'
- en: '](img/Figure_8.16.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_8.16.jpg)'
- en: 'Figure 8.16: Process flow of R-CNN'
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.16：R-CNN 的流程图
- en: Note
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '*Figure 8.16* is cited from reference, *Ross Girshick, Jeff Donahue, Trevor
    Darrell, and Jitendra Malik (2014): Rich Feature Hierarchies for Accurate Object
    Detection and Semantic Segmentation. In 580 – 587*.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.16* 引用了参考文献，*Ross Girshick, Jeff Donahue, Trevor Darrell, 和 Jitendra Malik
    (2014): 用于精确物体检测和语义分割的丰富特征层次结构. 第 580 – 587 页*。'
- en: 'In *Figure 8.16*, note the *2\. Extract region proposals* and *3\. Compute
    CNN features* sections. The first technique detects the areas that seem to be
    objects (in some way) and then applies a CNN to the extracted areas to classify
    them. R-CNN converts an image into squares and uses **support vector machines**
    (**SVMs**) for classification. Its actual process flow is slightly complicated
    but mainly consists of the aforementioned processes: the extraction of candidate
    regions and to compute CNN features.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 8.16*中，请注意 *2. 提取区域提议* 和 *3. 计算 CNN 特征* 部分。第一个技术检测出看似物体的区域（以某种方式），然后对提取的区域应用
    CNN 进行分类。R-CNN 将图像转换为正方形，并使用 **支持向量机**（**SVMs**）进行分类。其实际流程略显复杂，但主要由上述过程组成：提取候选区域并计算
    CNN 特征。
- en: 'In the "Extract region proposals" process of R-CNN, candidates for objects
    are detected, and this is where various techniques that have been developed in
    computer vision can be used. In the paper about R-CNN, a technique called selective
    search is used. Recently, a technique called "Faster R-CNN" (*Shaoqing Ren, Kaiming
    He, Ross Girshick, and Jian Sun (2015): Faster R-CNN: Towards Real-Time Object
    Detection with Region Proposal Networks. In C. Cortes, N. D. Lawrence, D. D. Lee,
    M. Sugiyama, & R. Garnett, eds. Advances in Neural Information Processing Systems
    28\. Curran Associates, Inc., 91 – 99*) has been proposed. It even uses CNNs to
    extract region proposals. Faster R-CNN uses one CNN for the entire process, which
    enables fast processing.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在 R-CNN 的“提取区域候选”过程中，检测到目标候选区域，这时可以使用计算机视觉中已经开发的各种技术。在关于 R-CNN 的论文中，使用了一种叫做选择性搜索的技术。最近，提出了一种叫做“Faster
    R-CNN”的技术（*Shaoqing Ren, Kaiming He, Ross Girshick, 和 Jian Sun (2015)：Faster R-CNN：通过区域提议网络实现实时目标检测。发表于
    C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama 和 R. Garnett 编辑的《神经信息处理系统进展
    28》。Curran Associates, Inc., 91 – 99*）。它甚至使用 CNN 提取区域提议。Faster R-CNN 使用一个 CNN
    完成整个过程，从而实现快速处理。
- en: Segmentation
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分割
- en: Segmentation classifies an image on a pixel basis. It learns by using training
    data where objects are colored on a pixel basis and classifies all the pixels
    of an input image during inference. The neural networks we've implemented so far
    classify the entire image. So, how can we classify it on a pixel basis?
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 分割是基于像素对图像进行分类的技术。它通过使用标注了物体的像素训练数据进行学习，并在推理过程中对输入图像的所有像素进行分类。到目前为止，我们实现的神经网络对整个图像进行分类。那么，如何基于像素进行分类呢？
- en: 'The simplest method of performing segmentation with a neural network is to
    make a prediction for each pixel. For example, you can provide a network that
    classifies a pixel at the center of a rectangular area to make a prediction for
    all the pixels. As you can see, this requires as many forward processes as the
    number of pixels, thus taking a lot of time to complete (the problem being that
    convolution operations re-calculate many areas uselessly). To reduce such useless
    calculations, a technique called a **Fully Convolutional Network** (**FCN**) has
    been proposed (*Jonathan Long, Evan Shelhamer, and Trevor Darrell (2015): Fully
    Convolutional Networks for Semantic Segmentation. In The IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR)*). It classifies all the pixels in one forward
    process (see *Figure 8.20*).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络进行分割的最简单方法是对每个像素进行预测。例如，你可以提供一个网络，分类一个矩形区域中心的像素，并对所有像素进行预测。如你所见，这需要与像素数量相同的前向计算过程，因此完成起来需要大量时间（问题在于卷积操作会无用地重复计算很多区域）。为了减少这种无效计算，提出了一种叫做
    **全卷积网络**（**FCN**）的技术（*Jonathan Long, Evan Shelhamer 和 Trevor Darrell (2015)：用于语义分割的全卷积网络。发表于
    IEEE 计算机视觉与模式识别会议（CVPR）*）。它在一次前向计算中对所有像素进行分类（见*图 8.20*）。
- en: A FCN is a network that consists only of convolution layers. While an ordinary
    CNN contains fully connected layers, a FCN replaces fully connected layers with
    *convolution layers that play the same role*. In fully connected layers in a network
    that's used in object recognition, the space volume of the intermediate data is
    processed as nodes arranged in a line. On the other hand, in a network that consists
    only of convolution layers, the space volume can be maintained during processing
    until the last output.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: FCN 是一种仅由卷积层组成的网络。虽然普通的 CNN 包含全连接层，但 FCN 用*执行相同功能的卷积层*替换了全连接层。在用于目标识别的网络中的全连接层中，中间数据的空间体积被处理为线性排列的节点。另一方面，在一个仅由卷积层组成的网络中，空间体积可以在处理过程中保持，直到最后的输出。
- en: 'The main characteristic of a FCN is that the space size is expanded at the
    end. This expansion can enlarge shrunk intermediate data so that it''s the same
    size as the input image all at once. The expansion at the end of an FCN is an
    expansion by bi-linear interpolation (bi-linear expansion). An FCN uses deconvolution
    to conduct the bi-linear expansion (for details, see the paper (*Jonathan Long,
    Evan Shelhamer, and Trevor Darrell (2015): Fully Convolutional Networks for Semantic
    Segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)* about FCN).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 'FCN的主要特点是最后的空间尺寸会扩展。这个扩展可以使缩小的中间数据膨胀，从而一次性恢复到与输入图像相同的大小。FCN末尾的扩展是通过双线性插值（双线性扩展）实现的。FCN使用反卷积来进行双线性扩展（详细内容请参见论文《(*Jonathan
    Long, Evan Shelhamer, and Trevor Darrell (2015): Fully Convolutional Networks
    for Semantic Segmentation. In The IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*)》）。'
- en: Note
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: In a fully connected layer, the output is connected to all the inputs. You can
    also create a connection that's the same structure in a convolution layer. For
    example, a fully connected layer whose input data size is 32x10x10 (the number
    of channels is 32, the height is 10, and the width is 10) can be replaced with
    a convolution layer whose filter size is 32x10x10\. If the fully connected layer
    has 100 output nodes, the convolution layer can achieve completely the same processing
    by providing 100 of the 32x10x10 filters. In this way, a fully connected layer
    can be replaced with a convolution layer that conducts equivalent processing.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在全连接层中，输出连接到所有输入。你也可以在卷积层中创建一个结构相同的连接。例如，一个输入数据大小为32x10x10（通道数为32，高度为10，宽度为10）的全连接层可以用一个过滤器大小为32x10x10的卷积层来替代。如果全连接层有100个输出节点，卷积层可以通过提供100个32x10x10的过滤器完全实现相同的处理。这样，全连接层就可以被一个进行等效处理的卷积层替代。
- en: Generating Image Captions
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成图像描述
- en: There is some interesting research being conducted that combines natural language
    and computer vision. When an image is provided, the text explaining the image
    (the image caption) is automatically generated.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些有趣的研究正在进行，这些研究结合了自然语言和计算机视觉。当提供一张图像时，自动生成解释该图像的文本（即图像描述）。
- en: 'For example, an image of a motorcycle from a dirt bike competition could include
    the caption: "A person riding a motorcycle on a dirt road" (this text is automatically
    generated from the image). It is surprising that the system even "understands"
    that it is on a dirt road and that a person is riding a motorcycle.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一张来自越野摩托车比赛的摩托车图像可能会附带这样的描述：“一个人在泥路上骑摩托车”（该文本是从图像中自动生成的）。令人惊讶的是，系统甚至“理解”到它是在泥路上，并且有一个人骑着摩托车。
- en: A model called **Neural Image Caption** (**NIC**) is typically used to generate
    image captions for deep learning. NIC consists of a deep CNN and a **Recurrent
    Neural Network** (**RNN**) for handling natural language. An RNN has recursive
    connections and is often used for sequential data such as natural language and
    time-series data.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 一个名为**神经图像描述**（**NIC**）的模型通常用于生成图像描述，用于深度学习。NIC由一个深度卷积神经网络（**CNN**）和一个用于处理自然语言的**递归神经网络**（**RNN**）组成。RNN具有递归连接，常用于处理自然语言和时间序列数据等顺序数据。
- en: 'NIC uses CNN to extract the features from an image and passes them to the RNN.
    The RNN uses the features extracted by the CNN as initial values to generate a
    text "recursively." We will not discuss the technical details here. Basically,
    NIC has a simple architecture that combines two neural networks: a CNN and an
    RNN. It can generate surprisingly precise image captions. Handling various types
    of information, such as images and natural language, is called **multi-modal processing**.
    Multi-modal processing has gained a lot of attention in recent years:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: NIC使用CNN从图像中提取特征，并将其传递给RNN。RNN利用CNN提取的特征作为初始值，通过“递归”生成文本。我们在此不讨论技术细节。基本上，NIC有一个简单的架构，结合了两个神经网络：CNN和RNN。它能够生成惊人的精准图像描述。处理图像和自然语言等不同类型信息的能力被称为**多模态处理**。近年来，多模态处理受到了广泛关注：
- en: Note
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The R in RNN stands for recurrent. "Recurrent" indicates a neural network's
    recurrent network architecture. Because of the recurrent architecture, the RNN
    is affected by the information generated before it – in other words, it remembers
    past information. This is the main characteristic of an RNN. For example, after
    generating the word "I," it is affected by the word and generates the next word
    "am." Then, it is affected by the words "I am" that were previously generated
    and generates the word "sleeping." For continuous data such as natural language
    and time-series data, the RNN behaves as if it remembered past information.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: RNN中的R代表递归。“递归”表示神经网络的递归网络架构。由于递归架构，RNN受到之前生成的信息的影响——换句话说，它能记住过去的信息。这是RNN的主要特征。例如，在生成单词“I”后，它会受到该单词的影响，生成下一个单词“am”。然后，它会受到之前生成的“I
    am”这两个词的影响，生成单词“sleeping”。对于自然语言和时间序列数据等连续数据，RNN的行为就像记住了过去的信息一样。
- en: The Future of Deep Learning
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习的未来
- en: Deep learning is now being used in various fields, as well as in the traditional
    fields. This section describes the possibilities of deep learning and some research
    that shows the future of deep learning.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习现在正在被应用于各个领域，包括传统领域。 本节描述了深度学习的可能性以及一些展示深度学习未来的研究。
- en: Converting Image Styles
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换图像风格
- en: There is research being conducted that uses deep learning to "draw" a picture
    as an artist would. One popular use case of neural networks is to create a new
    image based on two provided images. One of them is called a "content image," while
    the other is called a "style image." A new image is created based on these two
    images.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 有一项研究正在进行，利用深度学习“绘制”图像，就像艺术家一样。神经网络的一个流行应用案例是根据两张提供的图像创建一张新图像。其中一张被称为“内容图像”，而另一张被称为“风格图像”。新图像是基于这两张图像创建的。
- en: 'In one example, you can specify Van Gogh''s painting style as the style that
    will be applied to the content image, deep learning draws a new picture, as specified.
    This research was published in the paper "A Neural Algorithm of Artistic Style"
    (*Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge (2015): A Neural Algorithm
    of Artistic Style. arXiv:1508.06576[cs, q-bio] (August 2015)*) and received a
    lot of attention all over the world as soon as it was published.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个示例中，你可以指定梵高的画风作为要应用于内容图像的风格，深度学习将按指定方式绘制一幅新图像。这项研究发表在论文《艺术风格的神经算法》中（*Leon
    A. Gatys, Alexander S. Ecker, 和 Matthias Bethge (2015)：艺术风格的神经算法。arXiv:1508.06576[cs,
    q-bio]（2015年8月）*），并在发布后立刻引起了全世界的关注。
- en: Roughly speaking, in the technique, the intermediate data in the network learn
    so that it approaches the intermediate data of the "content image." By doing so,
    the input image can be converted so that it is similar in shape to the content
    image. To absorb a style from the "style image," the concept of a style matrix
    is introduced. By training so that the gap of the style matrix is small, the input
    image can approach Van Gogh's style.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 粗略地说，在该技术中，网络中的中间数据会进行学习，从而接近“内容图像”的中间数据。通过这种方式，输入图像可以转换成类似内容图像的形状。为了吸收来自“风格图像”的风格，引入了风格矩阵的概念。通过训练使得风格矩阵的差距变小，输入图像可以接近梵高的风格。
- en: Generating Images
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成图像
- en: The preceding example of image style transfer required two images to generate
    a new image. On the other hand, some research has tried to generate new images
    without requiring any images (the technique trains by using many images beforehand
    but needs no images to "draw" a new image.) For example, you can use deep learning
    to generate the image of a "bedroom" from scratch
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 前述的图像风格迁移示例需要两张图像来生成一张新图像。另一方面，一些研究尝试在不需要任何图像的情况下生成新图像（该技术通过预先使用大量图像进行训练，但生成新图像时不需要任何图像）。例如，你可以使用深度学习从零开始生成一张“卧室”图像。
- en: They may seem to be real photographs, but they were newly generated by a DCGAN.
    The images that were generated by the DCGAN are images that nobody has ever seen
    (those that do not exist in the training data) and were newly created from scratch.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 它们看起来像真实的照片，但实际上是由DCGAN新生成的。这些由DCGAN生成的图像是没有人见过的图像（即那些不在训练数据中的图像），并且是从零开始新创作的。
- en: When a DCGAN generates images that look like real ones, it creates a model of
    the process where the images were generated. The model learns by using many images
    (such as those of bedrooms). After training finishes, you can use the model to
    generate new images.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 当DCGAN生成看起来像真实图像时，它创建了一个图像生成过程的模型。该模型通过使用许多图像（例如卧室的图像）进行学习。训练完成后，你可以使用该模型生成新的图像。
- en: 'DCGANs use deep learning. The main point of the DCGAN technique is that it
    uses two neural networks: a generator and a discriminator. The generator generates
    an image that seems real, while the discriminator determines whether it is real,
    that is, whether it was generated by the generator or whether it was really photographed.
    In this way, two networks are trained by making them compete against each other.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: DCGANs使用深度学习。DCGAN技术的关键点是它使用了两个神经网络：一个生成器和一个判别器。生成器生成看起来真实的图像，而判别器判断图像是否真实，即判断它是由生成器生成的，还是实际上是拍摄的。通过这种方式，两个网络通过相互竞争进行训练。
- en: The generator learns a more elaborate technique of creating fake images, while
    the discriminator grows like an appraiser who can detect fakes with higher precision.
    What is interesting is that in a technology called a **Generative Adversarial
    Network** (**GAN**), both of them grow through competition. Finally, the generator
    that has grown through competition can draw images that look real (or may grow
    even more).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器学习创建假图像的更复杂技术，而判别器则像一个评估师，可以更高精度地检测假图像。有趣的是，在一种被称为**生成对抗网络**（**GAN**）的技术中，它们通过竞争共同成长。最终，通过竞争成长起来的生成器能够绘制看起来真实的图像（或者可能会成长得更好）。
- en: Note
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The machine learning problems that we have seen so far are called **supervised
    learning** problems. They use a dataset that contains image data and labels in
    pairs, such as in handwritten digit recognition. Meanwhile, label data is not
    provided in the problem here. Only images (a set of images) are provided. This
    is called **unsupervised learning**. Unsupervised learning has been studied for
    a relatively long time (**Deep Belief Networks** and **Deep Boltzmann Machines**
    are famous), but it seems that these days, it is not being researched very actively.
    Since techniques using deep learning, such as DCGANs, are attracting more and
    more attention, it is expected that unsupervised learning will be developed further
    in the future.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止看到的机器学习问题被称为**监督学习**问题。它们使用包含图像数据和标签对的数据集，比如手写数字识别。而这里的问题中并没有提供标签数据。只提供图像（即一组图像）。这被称为**无监督学习**。无监督学习已经研究了相对较长时间（**深度置信网络**和**深度玻尔兹曼机**是著名的），但似乎现在它并没有被积极研究。由于像DCGANs这样的深度学习技术越来越受到关注，预计无监督学习将在未来得到进一步发展。
- en: Automated Driving
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动驾驶
- en: '"Automated driving" technology, in which a computer drives a car instead of
    a human, is likely to be realized soon. IT companies, universities, and research
    institutions, as well as car manufacturers, are competing to realize automated
    driving. This can only happen when various technologies such as path plan technology,
    which determines a traffic route, and sensing technology, including cameras and
    lasers, are combined. It is said that the technology used to recognize the surrounding
    environment properly is the most important. It is very difficult to recognize
    an environment that changes every moment of every day, as well as the cars and
    people that move around freely.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: “自动驾驶”技术，即计算机代替人类驾驶汽车，可能很快会实现。IT公司、大学、研究机构以及汽车制造商都在竞争实现自动驾驶。这只有在诸如路径规划技术（确定交通路线）和传感技术（包括摄像头和激光器）等各种技术结合的情况下才能实现。据说，用于正确识别周围环境的技术是最重要的。要识别一个每天每时每刻都在变化的环境，以及那些自由移动的汽车和人类，极其困难。
- en: If the system can properly recognize the travel area robustly and reliably,
    even in various environments, automated driving may be realized in the near future—a
    task for which deep learning should prove invaluable.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统能够在各种环境中稳健且可靠地识别出旅行区域，自动驾驶可能在不久的将来得以实现——这是深度学习应该证明其价值的重要任务。
- en: 'For example, a CNN-based network called SegNet (*Vijay Badrinarayanan, Kendall,
    and Roberto Cipolla (2015): SegNet: A Deep Convolutional Encoder-Decoder Architecture
    for Image Segmentation. arXiv preprint arXiv:1511.00561 (2015)*) can recognize
    the road environment accurately, as shown in *Figure 8.17*:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一种基于CNN的网络叫做SegNet（*Vijay Badrinarayanan, Kendall, 和 Roberto Cipolla（2015）：SegNet：一种用于图像分割的深度卷积编码解码架构。arXiv预印本arXiv:1511.00561（2015）*）可以准确识别道路环境，如*图8.17*所示：
- en: '![Figure 8.17: Example of segmenting an image by using deep learning – the
    road, cars, buildings, and sidewalks are recognized accurately'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.17：通过深度学习进行图像分割的示例 – 道路、汽车、建筑物和人行道被准确识别'
- en: '](img/fig08_17.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig08_17.jpg)'
- en: 'Figure 8.17: Example of segmenting an image by using deep learning – the road,
    cars, buildings, and sidewalks are recognized accurately'
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.17：通过深度学习进行图像分割的示例 – 道路、汽车、建筑物和人行道被准确识别
- en: Note
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '*Figure 8.17* is cited from reference, *SegNet Demo page* ([http://mi.eng.cam.ac.uk/projects/segnet/](http://mi.eng.cam.ac.uk/projects/segnet/)).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.17*来自参考文献，*SegNet演示页面*（[http://mi.eng.cam.ac.uk/projects/segnet/](http://mi.eng.cam.ac.uk/projects/segnet/)）。'
- en: Segmentation (pixel-level evaluation) is conducted for the input image, as shown
    in *Figure 8.17*. The result indicates that the road, buildings, sidewalks, trees,
    cars, and motorcycles are distinguished somewhat accurately. If deep learning
    improves the accuracy and speed of these recognition technologies from now on,
    automated driving may be put into practical use in the not too distant future.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对输入图像进行分割（像素级评估），如*图8.17*所示。结果表明，道路、建筑物、人行道、树木、汽车和摩托车得到了较为准确的区分。如果深度学习能够提升这些识别技术的准确性和速度，自动驾驶有可能在不久的将来实现实际应用。
- en: Deep Q-Networks (Reinforcement Learning)
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度Q网络（强化学习）
- en: There is a research field called **reinforcement learning** in which computers
    learn independently through trial and error, just as humans learn how to ride
    a bicycle, for example. This is different from "supervised learning," where a
    "supervisor" teaches face to face.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个研究领域叫做**强化学习**，计算机通过试错自我学习，就像人类学习如何骑自行车一样。这与“监督学习”不同，后者是由“监督者”面对面地进行教学。
- en: 'The basic framework of reinforcement learning is that an agent selects actions,
    depending on the situation of the environment, and its actions change the environment.
    After taking an action, the environment offers the agent some reward. The purpose
    of reinforcement learning is to determine the action policy of the agent so that
    it can obtain a better reward, as shown here:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的基本框架是，代理根据环境的情况选择行动，并且它的行动会改变环境。采取行动后，环境会给予代理一些奖励。强化学习的目的是确定代理的行动策略，使其能够获得更好的奖励，如下图所示：
- en: '![Figure 8.18: Basic framework of reinforcement learning – the agent learns
    independently'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.18：强化学习的基本框架 – 代理通过自我学习'
- en: to obtain a better reward
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 以获得更好的奖励
- en: '](img/Figure_8.18.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_8.18.jpg)'
- en: 'Figure 8.18: Basic framework of reinforcement learning – the agent learns independently
    to obtain a better reward'
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.18：强化学习的基本框架 – 代理通过自我学习获得更好的奖励
- en: The diagram in *Figure 8.18* shows the basic framework of reinforcement learning.
    Note that the reward is not labeled data, as it is in supervised learning. For
    example, in the video game "Super Mario Brothers," the exact quantity of rewards
    you gain by moving Mario to the right is not necessarily clear. In that case,
    the "prospective" reward must be determined by clear indicators such as the game
    scores (obtaining coins, defeating enemies, and so on) and game-over logic. In
    supervised learning, each action can be evaluated correctly by the "supervisor."
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.18*中的图示显示了强化学习的基本框架。请注意，奖励不是像监督学习中那样的标注数据。例如，在视频游戏《超级马里奥兄弟》中，移动马里奥向右所获得的奖励数量未必是明确的。在这种情况下，"预期"奖励必须通过清晰的指标（如游戏得分、获得金币、击败敌人、游戏结束逻辑等）来确定。在监督学习中，每个行动都可以由“监督者”正确评估。'
- en: 'A **Deep Q-Network** (**DQN**) is a reinforcement learning technique (*Volodymyr
    Mnih et al (2015): Human-level control through deep reinforcement learning. Nature
    518, 7540 (2015), 529 – 533*) that uses deep learning. It is based on the algorithm
    of reinforcement learning called Q-learning. Q-learning determines a function
    called the optimal action-value function to determine the optimal action. A DQN
    uses deep learning (CNNs) to approximate the function.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度Q网络**（**DQN**）是一种强化学习技术（*Volodymyr Mnih等人（2015）：通过深度强化学习实现人类水平的控制，《自然》518，7540（2015），529
    – 533*），它使用深度学习。它基于一种叫做Q学习的强化学习算法。Q学习确定一个叫做最优动作值函数的函数，用来确定最优动作。DQN使用深度学习（CNN）来逼近这个函数。'
- en: Some research has shown that DQNs can learn video games automatically to achieve
    more successful play than humans. As shown in *Figure 8.19*, a CNN, when used
    in a DQN, receives four consecutive frames of game images as input and outputs
    the "value" of the motion of the game controller (the movement of the joystick
    and the button operation).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究表明，DQNs可以自动学习视频游戏，取得比人类更成功的游戏成绩。如*图8.19*所示，当CNN用于DQN时，它接收四帧连续的游戏图像作为输入，并输出游戏控制器（操纵杆的移动和按钮操作）的“价值”。
- en: 'Traditionally, when a video game was learned by the network, the state of the
    game (such as the positions of the characters) was usually extracted and provided
    in advance. Meanwhile, the DQN receives only the images of a video game as input
    data, as shown in *Figure 8.19*. This is what is noteworthy in a DQN and highly
    improves its applicability. This is because you do not need to change the settings
    for each game, and you only need to provide game images to the DQN. In fact, DQNs
    have learned many games, such as "Pac-Man" and "Atari 2600" with the same configuration
    and achieved better results than humans:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，当网络学习视频游戏时，通常会提前提取并提供游戏的状态（如角色的位置）。与此同时，DQN仅接收视频游戏的图像作为输入数据，如*图8.19*所示。这是DQN值得注意的地方，并大大提高了其适用性。因为你不需要为每个游戏更改设置，只需要向DQN提供游戏图像。事实上，DQNs已经以相同的配置学习了许多游戏，如“吃豆人”和“雅达利2600”，并取得了比人类更好的结果：
- en: '![Figure 8.19: Using a Deep Q-Network to learn the operations of a video game.
    Here, the'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.19：使用深度Q网络学习视频游戏的操作。在这里， '
- en: network receives the images of a video game as an input and learns the operation
    of the game
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 网络接收视频游戏的图像作为输入，并学习游戏的操作。
- en: controller (joystick) through trial and error
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 通过反复试错学习控制器（操纵杆）的操作。
- en: '](img/fig08_19.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig08_19.jpg)'
- en: 'Figure 8.19: Using a Deep Q-Network to learn the operations of a video game.
    Here, the network receives the images of a video game as an input and learns the
    operation of the game controller (joystick) through trial and error'
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.19：使用深度Q网络学习视频游戏的操作。在这里，网络接收视频游戏的图像作为输入，并通过反复试错学习游戏控制器（操纵杆）的操作。
- en: Note
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '*Figure 8.17* is cited from reference, *Volodymyr Mnih et al. (2015): Human-level
    control through deep reinforcement learning. Nature 518, 7540 (2015), 529 – 533*.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.17*引用自参考文献，*Volodymyr Mnih等人（2015）：通过深度强化学习实现人类水平的控制，《自然》518，7540（2015），529
    – 533*。'
- en: Note
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The news that an AI called AlphaGo (*David Silver et al. (2016): Mastering
    the game of Go with deep neural networks and tree search. Nature 529, 7587 (2016),
    484 – 489*) beat the Go champion attracted much attention. Deep learning and reinforcement
    learning are also used in AlphaGo. It learned from 30 million game records created
    by professionals and played against itself many times to accumulate sufficient
    knowledge. Both AlphaGo and DQNs have been researched by Google''s DeepMind. We
    must keep an eye on their activities in the future.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一则名为AlphaGo的人工智能（*David Silver等人（2016）：通过深度神经网络和树搜索掌握围棋游戏，《自然》529，7587（2016），484
    – 489*）战胜围棋冠军的消息引起了广泛关注。AlphaGo也使用了深度学习和强化学习。它通过学习3000万条由专业人士创建的游戏记录，并多次与自己对战，积累了足够的知识。AlphaGo和DQNs都由谷歌的DeepMind进行研究。未来我们必须关注它们的活动。
- en: Summary
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we implemented a deep CNN and achieved an excellent recognition
    result exceeding 99% for handwritten digit recognition. We also discussed the
    motivation for making a network deeper and the current tendency toward deeper
    networks. We also looked at the trends and applications of deep learning, and
    the research is accelerating it, which will advance this technology into the future.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们实现了一个深度卷积神经网络（CNN），并取得了超过99%的优秀手写数字识别结果。我们还讨论了使网络更深的动机以及当前趋向于更深网络的趋势。我们还探讨了深度学习的趋势和应用，并且加速研究将推动这项技术进入未来。
- en: In the field of deep learning, there is much that is still unknown, and new
    research is being published all the time. Researchers and engineers around the
    world continue to research actively and will realize technologies that we cannot
    even imagine yet.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习领域，仍有许多未知的内容，新的研究成果不断发布。全球的研究人员和工程师们持续积极研究，并将实现我们甚至无法想象的技术。
- en: 'The following points were covered in this chapter:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下几点：
- en: Making a network deeper will improve performance for many deep learning problems.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使网络更深将提升许多深度学习问题的性能。
- en: In image recognition competitions, techniques using deep learning get a high
    ranking, and current networks are deeper than their predecessors
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图像识别比赛中，使用深度学习的技术获得了高排名，当前的网络比前代更深。
- en: Famous networks include VGG, GoogLeNet, and ResNet.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 著名的网络包括VGG、GoogLeNet和ResNet。
- en: GPUs, distributed training, and the reduction of bit accuracy can accelerate
    deep learning.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU、分布式训练和减少位精度可以加速深度学习。
- en: Deep learning (neural networks) can be used for object detection and segmentation,
    as well as for object recognition.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习（神经网络）可以用于物体检测和分割，以及物体识别。
- en: Applications that use deep learning include the generation of image captions,
    the generation of images, and reinforcement learning. These days, the use of deep
    learning for automated driving is also expected.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习的应用包括图像描述生成、图像生成和强化学习。如今，深度学习在自动驾驶中的应用也备受期待。
- en: Thank you for reading this book. We hope that you've gained a better understanding
    of deep learning and have found it an interesting journey.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您阅读本书。我们希望您对深度学习有了更好的理解，并且觉得这是一段有趣的旅程。
