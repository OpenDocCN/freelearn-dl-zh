- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Building Conversational AI Applications with Deep Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习构建对话式 AI 应用
- en: 'The art of conversation is considered a uniquely human trait. The ability of
    machines to have a dialog with humans has been a research topic for many years.
    Alan Turing proposed the now-famous Turing Test to see if a human could converse
    with another human and a machine through written messages, and identify each participant
    as machine or human correctly. In recent times, digital assistants such as Alexa
    by Amazon and Siri by Apple have made considerable strides in conversational AI.
    This chapter discusses different conversational agents and puts the techniques
    learned in the previous chapters into context. While there are several approaches
    to building conversational agents, we''ll focus on the more recent deep learning
    approaches and cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 对话的艺术被认为是人类独有的特质。机器与人类进行对话的能力已经成为多年来的研究课题。艾伦·图灵提出了现在著名的图灵测试，旨在检验人类能否通过书面信息与另一个人类和机器进行对话，并正确识别每个参与者是机器还是人类。近年来，像亚马逊的
    Alexa 和苹果的 Siri 这样的数字助手在对话式 AI 方面取得了显著进展。本章讨论了不同的对话式代理，并将前几章学到的技术置于实际背景中。虽然构建对话式代理有多种方法，但我们将重点讨论近年来的深度学习方法，并涵盖以下主题：
- en: Overview of conversational agents and their general architecture
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对话式代理及其通用架构概览
- en: An end-to-end pipeline for building a conversational agent
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建对话式代理的端到端流程
- en: The architecture of different types of conversational agents, such as
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型对话式代理的架构，例如
- en: Question-answering bots
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答机器人
- en: Slot-filling or task-oriented bots
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填槽型或任务导向型机器人
- en: General conversation bots
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用对话机器人
- en: We'll start with an overview of the general architecture of conversational agents.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从对话式代理的一般架构概述开始。
- en: Overview of conversational agents
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对话式代理概述
- en: 'A conversational agent interacts with people using speech or text. Facebook
    Messenger would be an example of a text-based agent while Alexa and Siri are examples
    of agents that interact through speech. In either case, the agent needs to understand
    the user''s intent and respond accordingly. Hence, the core part of the agent
    would be a **natural language understanding** (**NLU**) module. This module would
    interface with a **natural** **language generation** (**NLG**) module to supply
    a response back to the user. Voice agents differ from text-based agents in having
    an additional module that converts voice to text and vice versa. We can imagine
    the system having the following logical structure for a voice-activated agent:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对话式代理通过语音或文本与人进行互动。Facebook Messenger 是一个基于文本的代理示例，而 Alexa 和 Siri 是通过语音互动的代理示例。无论是哪种情况，代理都需要理解用户的意图并作出相应的回应。因此，代理的核心部分将是**自然语言理解**（**NLU**）模块。该模块将与**自然****语言生成**（**NLG**）模块进行交互，向用户提供回应。语音代理与基于文本的代理的不同之处在于，它们拥有一个额外的模块，用于将语音转换为文本，反之亦然。我们可以想象该系统对于语音激活代理的逻辑结构如下：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_09_01.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![一张手机的截图 自动生成的描述](img/B16252_09_01.png)'
- en: 'Figure 9.1: Conceptual architecture of a conversational AI system'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1：对话式 AI 系统的概念架构
- en: The main difference between a speech-based system and a text-based system is
    how the users communicate with the system. All the other parts to the right of
    the Speech Recognition and Generation section shown in *Figure 9.1* above are
    identical in both types of conversational AI systems.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 语音系统和文本系统之间的主要区别在于用户与系统的互动方式。上述*图 9.1*中“语音识别与生成”部分右侧的所有其他部分，在这两种类型的对话式 AI 系统中都是相同的。
- en: The user communicates with the agent using speech. The agent first converts
    speech to text. Many advancements have been made in the past few years in this
    area, and it is generally considered a solved problem for major languages like
    English.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 用户通过语音与代理进行交流。代理首先将语音转换为文本。在过去几年中，这一领域取得了许多进展，对于像英语这样的大语言，它通常被认为是一个已解决的问题。
- en: 'English is spoken in many countries across the globe, resulting in many different
    pronunciations and dialects. Consequently, companies like Apple develop various
    models for different accents, such as British English, Indian English, and Australian
    English. *Figure 9.2* below shows some English and French accents from the Siri
    control panel on an iPhone 11 running iOS 13.6\. French, German, and some other
    languages also have multiple variants. Another way to do this could be by putting
    an accent and language classification model as the first step and then processing
    the input through the appropriate speech recognition model:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 英语在全球许多国家都被使用，这导致了多种不同的发音和方言。因此，像苹果这样的公司开发了不同口音的模型，例如英式英语、印度英语和澳大利亚英语。下图 *Figure
    9.2* 显示了在运行 iOS 13.6 的 iPhone 11 上的 Siri 控制面板中，一些英语和法语口音。法语、德语以及其他一些语言也有多个变体。另一种方式是将口音和语言分类模型作为第一步，然后通过适当的语音识别模型处理输入：
- en: '![](img/B16252_09_02.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_09_02.png)'
- en: 'Figure 9.2: Language variants in Siri for speech recognition'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：Siri 中的语言变体用于语音识别
- en: For virtual assistants, there are specific models for wake word detection. The
    model's objective is to start the bot once it detects a wake word or phrase such
    as "OK Google." The wake word triggers the bot to listen to the utterances until
    the conversation is completed. Once the user's speech has been converted into
    words, it is easy to apply to various NLP techniques that we have seen in multiple
    chapters in this book. The breakdown of the elements shown inside the NLP box
    in *Figure 9.1* can be considered conceptual. Depending on the system and the
    task, these components may be different models or one end-to-end model. However,
    it is useful to think of the logical breakdown, as shown in the figure.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于虚拟助手，有特定的模型用于唤醒词检测。该模型的目标是在检测到唤醒词或短语（例如“OK Google”）时启动机器人。唤醒词会触发机器人监听用户的发言，直到对话完成。一旦用户的语音被转换为文本，就可以应用本书中多个章节提到的各种自然语言处理（NLP）技术。*Figure
    9.1* 中 NLP 框内显示的元素可以视为概念性的。根据系统和任务的不同，这些组件可能是不同的模型，或者是一个端到端的模型。然而，考虑如图所示的逻辑划分是很有帮助的。
- en: Understanding the user's commands and the intent is a crucial part. Intent identification
    is essential for general-purpose systems like Amazon's Alexa or Apple's Siri,
    which serve multiple purposes. Specific dialogue management systems may be invoked
    based on the intent identified. The dialog management may invoke APIs provided
    by a fulfillment system. In a banking bot, the command may be to get the latest
    balances, and the fulfillment may be a banking system that retrieves the latest
    balance. The dialogue manager would process the balance and use an NLG system
    to convert the balance into a proper sentence. Note that some of these systems
    are built on rules-based systems and others use end-to-end deep learning. A question-answering
    system is an example of an end-to-end deep learning system where dialog management,
    and NLU are a single unit.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 理解用户的命令和意图是至关重要的一部分。意图识别对于像亚马逊的 Alexa 或苹果的 Siri 这样的通用系统至关重要，因为它们具有多种功能。根据识别的意图，可能会调用特定的对话管理系统。对话管理可能会调用由履行系统提供的
    API。在银行聊天机器人中，命令可能是获取最新余额，而履行可能是一个从银行系统中提取最新余额的系统。对话管理器会处理余额并使用 NLG 系统将余额转换为适当的句子。请注意，这些系统中有些是基于规则的系统，而另一些则使用端到端深度学习。问答系统就是端到端深度学习系统的一个例子，在这种系统中，对话管理和自然语言理解（NLU）是一个整体。
- en: 'There are different types of conversational AI applications. The most common
    ones are:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 会话式人工智能应用有多种类型，其中最常见的包括：
- en: Task-oriented or slot-filling systems
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务导向或槽填充系统
- en: Question-answering
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答系统
- en: Machine reading comprehension
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器阅读理解
- en: Social or chit-chat bots
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社交或闲聊聊天机器人
- en: Each of these types is described in the following sections.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各节将描述这些类型。
- en: Task-oriented or slot-filling systems
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务导向或槽填充系统
- en: 'Task-oriented systems are purpose-built to satisfy a specific task. Some examples
    of tasks are ordering a pizza, getting the latest balance of a bank account, calling
    a person, sending a text message, turning a light on, and so on. Most of the capabilities
    exposed by virtual assistants can be classified into this category. Once the user''s
    intent has been identified, control is transferred to the model managing a specific
    intent to gather all the information to perform the task and manage the dialog
    with the user. NER and POS detection models form a crucial part of such systems.
    Imagine that the user needs to fill a form with some information, and the bot
    interacts with the user to find the required information to fulfill the task.
    Let''s take the example of ordering a pizza. The table below shows a simplified
    example of the choices in this process:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 任务导向系统是为了完成特定任务而专门构建的。任务的一些例子包括订披萨、获取银行账户的最新余额、打电话、发送短信、开灯等。大多数虚拟助手所暴露的功能可以归类为此类。一旦确定了用户的意图，控制权就会转交给管理特定意图的模型，收集完成任务所需的所有信息并与用户管理对话。命名实体识别（NER）和词性标注（POS）检测模型在这样的系统中扮演着至关重要的角色。假设用户需要填写一个包含一些信息的表单，而机器人与用户进行互动以获取完成任务所需的信息。我们以订披萨为例。下表展示了这个过程中的简化选择示例：
- en: '| Size | Crust | Toppings | Delivery | Quantity |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 底边 | 配料 | 外卖 | 数量 |'
- en: '| SmallMediumLargeXL | ThinRegularDeep dishGluten-free | CheeseJalapenoPineapplePepperoni
    | Take-outDelivery | 12… |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 小中大XL | 薄底常规深盘无麸质 | 芝士墨西哥辣椒菠萝意大利辣香肠 | 外卖送货 | 12… |'
- en: 'Here is a made-up example of a conversation with a bot:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个与机器人对话的虚构示例：
- en: '![Graphical user interface, text, application, chat or text message  Description
    automatically generated](img/B16252_09_03.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面、文本、应用程序、聊天或短信  描述自动生成](img/B16252_09_03.png)'
- en: 'Figure 9.3: A possible pizza-ordering bot conversation'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3：一个可能的披萨订购机器人对话示例
- en: The bot tracks the information needed and keeps marking the information it has
    received from the person as the conversation progresses. Once the bot has all
    the information needed to complete the task, it can execute the task. Note that
    some steps, such as confirming the order or the customer asking for options for
    toppings, have been excluded for brevity.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人跟踪所需信息，并随着对话的进展不断标记其已收到的信息。一旦机器人收集到完成任务所需的所有信息，它就可以执行任务。注意，为了简洁起见，一些步骤（如确认订单或客户要求披萨配料选项）已被省略。
- en: 'In today''s world, solutions like Dialogflow, part of Google Cloud, and LUIS,
    part of Azure, simplify building such conversational agents to just the configuration.
    Let''s see how a simple bot that implements a portion of the pizza-ordering task
    above can be implemented with Dialogflow. Note that this example has been kept
    small to simplify configuration and use the free tier of Dialogflow. The first
    step is to navigate to [https://cloud.google.com/dialogflow](https://cloud.google.com/dialogflow),
    which is the home page for this service. There are two version of Dialogflow –
    Essentials or ES, and CX. CX is the advanced version with a lot more features
    and controls. Essentials is a simplified version with a free tier that is perfect
    for a bot''s trial build. Scroll down on the page so that you can see the Dialogflow
    Essentials section and click on the **Go to console** link, as shown in *Figure
    9.4* below:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天的世界中，像 Dialogflow（Google Cloud 的一部分）和 LUIS（Azure 的一部分）这样的解决方案简化了构建此类对话代理的过程，只需配置即可。让我们看看如何使用
    Dialogflow 实现一个简单的机器人，完成上述披萨订购任务的一部分。注意，为了简化配置并使用 Dialogflow 的免费层，示例保持简小。第一步是访问
    [https://cloud.google.com/dialogflow](https://cloud.google.com/dialogflow)，这是该服务的主页。Dialogflow
    有两个版本——Essentials 或 ES 版和 CX 版。CX 是高级版，具有更多功能和控制。Essentials 是简化版，提供免费层，非常适合构建机器人的试用版。向下滚动页面，直到看到
    Dialogflow Essentials 部分，并点击 **进入控制台** 链接，如下图 *图 9.4* 所示：
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B16252_09_04.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含图形用户界面的图片  描述自动生成](img/B16252_09_04.png)'
- en: 'Figure 9.4: Dialogflow console access'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4：Dialogflow 控制台访问
- en: 'Clicking on the console may require the authorization of the service, and you
    may need to log in with your Google Cloud account. Alternatively, you may navigate
    to [dialogflow.cloud.google.com/#/agents](http://dialogflow.cloud.google.com/#/agents)
    to see a list of configured agents. This screen is shown in *Figure 9.5*:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 点击控制台可能需要服务授权，您可能需要使用您的 Google Cloud 账户登录。或者，您可以导航到 [dialogflow.cloud.google.com/#/agents](http://dialogflow.cloud.google.com/#/agents)
    查看已配置的代理列表。此屏幕如*图 9.5*所示：
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B16252_09_05.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，Teams，描述自动生成](img/B16252_09_05.png)'
- en: 'Figure 9.5: Agents configuration in Dialogflow'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：Dialogflow 中的代理配置
- en: 'A new agent can be created by clicking on the blue **CREATE AGENT** button
    on the top right. If you see a different interface, please check that you are
    using Dialogflow Essentials. You can also use this URL to get to the agents section:
    [https://dialogflow.cloud.google.com/#/agents](https://dialogflow.cloud.google.com/#/agents).
    This brings up the new agent configuration screen, shown in *Figure 9.6*:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过点击右上角的蓝色**创建代理**按钮来创建一个新代理。如果您看到不同的界面，请确保您正在使用 Dialogflow Essentials。您也可以使用以下
    URL 进入代理部分：[https://dialogflow.cloud.google.com/#/agents](https://dialogflow.cloud.google.com/#/agents)。这将打开新的代理配置屏幕，如*图
    9.6*所示：
- en: '![Graphical user interface, text, application, Teams  Description automatically
    generated](img/B16252_09_06.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，Teams，描述自动生成](img/B16252_09_06.png)'
- en: 'Figure 9.6: Creating a new agent'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：创建新代理
- en: 'Please note that this is not a comprehensive tutorial of Dialogflow, so we
    will be using several default values to illustrate the concept of building slot-filling
    bots. Hitting **CREATE** will build a new bot and load a screen, as shown in *Figure
    9.7*. The main part of building the bot is to define intent. The main intent of
    our bot is to order pizza. Before we create an intent, we will configure a few
    entities:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这不是 Dialogflow 的全面教程，因此我们将使用几个默认值来说明构建插槽填充机器人（slot-filling bot）的概念。点击**创建**将构建一个新机器人并加载屏幕，如*图
    9.7*所示。构建机器人的主要部分是定义意图。我们机器人的主要意图是订购披萨。在创建意图之前，我们将配置几个实体：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B16252_09_07.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，描述自动生成](img/B16252_09_07.png)'
- en: 'Figure 9.7: A barebones agent ready for configuration'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：一个准备配置的简化代理
- en: 'These entities are the slots that the bot will fill out in conversation with
    the user. In this case, we will define two entities – the crust of the pizza and
    the size of the pizza. Click on the **+** sign next to Entities on the left in
    the previous screenshot, and you''ll see the following screen:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实体是机器人在与用户对话时将填写的插槽。在此案例中，我们将定义两个实体——披萨底座和披萨大小。点击左侧“实体”（Entities）旁边的**+**符号，您将看到以下屏幕：
- en: '![Graphical user interface, application  Description automatically generated](img/B16252_09_08.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，描述自动生成](img/B16252_09_08.png)'
- en: 'Figure 9.8: Configuring options for the crust entity in Dialogflow'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8：在 Dialogflow 中配置披萨底座实体的选项
- en: 'The values on the left represent the values for the crust entity, and the multiple
    options or synonyms on the right are the terms the user can input or speak corresponding
    to each choice. We will configure four options corresponding to the table above.
    Another entity will be created for the size of the pizza. The configured entity
    looks like *Figure 9.9*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的值表示披萨底座（crust）实体的值，右侧的多个选项或同义词是用户可以输入或说出的与每个选择对应的术语。我们将配置四个选项，分别对应上表中的内容。另一个实体将用于配置披萨的大小。配置后的实体如*图
    9.9*所示：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B16252_09_09.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件，描述自动生成](img/B16252_09_09.png)'
- en: 'Figure 9.9: Configuration of the size entity'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9：大小实体的配置
- en: 'Now we are ready to build the intent. Click on the **+** sign next to the **Intents**
    section on the left navigation bar. We will name this intent `order`, as this
    intent will get the options for crust and size from the user. First, we need to
    specify a set of training phrases that will trigger this intent. Some examples
    of such training phrases can be "I would like to order pizza" or "Can I get a
    pizza?". *Figure 9.10* shows some of the configured training phrases for the intent:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始构建意图了。点击左侧导航栏中**意图**部分旁边的**+**号。我们将这个意图命名为`order`，因为这个意图将从用户那里获取外壳和尺寸选项。首先，我们需要指定一组训练短语，这些短语将触发这个意图。一些这样的训练短语示例如“我想点披萨”或“我能要一份披萨吗？”。*图
    9.10*显示了一些为该意图配置的训练短语：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B16252_09_10.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件 描述自动生成](img/B16252_09_10.png)'
- en: 'Figure 9.10: Training phrases that trigger the ordering intent'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10：触发订单意图的训练短语
- en: There is a lot of hidden machine learning and deep learning happening in this
    picture, simplified by Dialogflow. For example, the platform can process text
    input as well as speech. These training examples are indicative, and the actual
    phrasing does not need to match any of these expressions directly.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图中隐藏了大量的机器学习和深度学习，Dialogflow 对其进行了简化。例如，平台可以处理文本输入以及语音输入。这些训练示例是具有代表性的，实际的表达方式不需要与这些表达直接匹配。
- en: 'The next step is to define the parameters we need from the user. We add an
    action with two parameters – size and crust. Note that the **ENTITY** column links
    the parameter with the defined entities and their values. The **VALUE** column
    defines a variable name that can be used in future dialogue or for integration
    with APIs:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是定义我们从用户那里需要的参数。我们添加一个包含两个参数的操作——尺寸和外壳。请注意，**实体**列将参数与定义的实体及其值相关联。**值**列定义了一个变量名称，可在未来的对话或与
    API 集成时使用：
- en: '![Graphical user interface, application, table  Description automatically generated](img/B16252_09_11.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，表格 描述自动生成](img/B16252_09_11.png)'
- en: 'Figure 9.11: Required parameters for the order intent'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11：订单意图所需的参数
- en: 'For each parameter, we need to specify some prompts that the agent will use
    to ask the user for the information. *Figure 9.12* below shows some example prompts
    for the size parameter. You may choose to configure your phrasings for the prompts:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个参数，我们需要指定一些代理将用来询问用户信息的提示语。下面的*图 9.12*显示了一些尺寸参数的示例提示。您可以选择为提示配置自己的表达方式：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B16252_09_12.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件 描述自动生成](img/B16252_09_12.png)'
- en: 'Figure 9.12: Prompt options for the size parameter'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12：尺寸参数的提示选项
- en: 'The last step in configuring the intent is configuring a response once the
    information is collected. This configuration is done in the **Responses** section
    and is shown in *Figure 9.13*:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 配置意图的最后一步是收集信息后配置响应。该配置在**响应**部分完成，如*图 9.13*所示：
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B16252_09_13.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，Teams 描述自动生成](img/B16252_09_13.png)'
- en: 'Figure 9.13: Response configuration for the order intent'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13：订单意图的响应配置
- en: 'Note the use of `$size.original` and `$crust.original` in the response text.
    It uses the original terms used by the user while ordering when it repeats the
    order back. Finally, note that we set this intent as the end of the conversation
    as we have obtained all the information we needed to get. Our bot is ready to
    be trained and tested. Hit the blue **Save** button at the top of the page after
    you have configured the training phrases, action and parameters, and the responses.
    There is another section at the bottom called fulfilment. This allows connecting
    the intent with a web service to complete the intent. The bot can be tested using
    the right side. Note that though we configured only text, Dialogflow enables both
    text and voice interfaces. While we demonstrate the text interface here, you are
    encouraged to try the voice interface as well:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意响应文本中使用了`$size.original`和`$crust.original`。它使用用户在下单时使用的原始术语，在重复订单时将其复述。最后，注意我们将这个意图设置为对话的结束，因为我们已经获取了所需的所有信息。我们的机器人已经准备好进行训练和测试。在配置完训练短语、动作和参数，以及响应后，请点击页面顶部的蓝色**保存**按钮。页面底部还有一个名为"fulfilment"的部分，允许将意图与Web服务连接以完成该意图。可以通过右侧进行机器人测试。请注意，尽管我们仅配置了文本，Dialogflow同时支持文本和语音界面。这里我们演示的是文本界面，鼓励你也尝试语音界面：
- en: '![](img/B16252_09_14.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_09_14.png)'
- en: 'Figure 9.14: An example of dialog showing the response processing and the variable
    being set'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14：显示响应处理和变量设置的对话示例
- en: 'Cloud-based solutions have made it quite easy to build task-oriented conversational
    agents for general uses. However, building an agent for a specialized domain like
    medical uses may require custom builds. Let''s look at options for specific parts
    of such a system:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 基于云的解决方案使得为一般用途构建面向任务的对话代理变得相当容易。然而，为了医疗等特定领域构建代理可能需要定制化的构建。让我们来看一下构建这种系统特定部分的选项：
- en: '**Intent identification**: The simplest way to identify intent is to treat
    it as a classification problem. Given an utterance or input text, the model needs
    to classify it into several intents. Standard RNN-based architectures, like those
    seen in earlier chapters, can be used and adapted for this task.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**意图识别**：识别意图的最简单方法是将其视为一个分类问题。给定一个发声或输入文本，模型需要将其分类为几个意图。可以使用标准的基于RNN的架构，像前面章节中提到的那些，来适应这一任务。'
- en: '**Slot tagging**: Tagging slots used in a sentence to correspond to inputs
    can be treated as a sequence classification problem. This is similar to the approach
    used in the second chapter, where named entities were tagged in a sequence of
    text. Bi-directional RNN models are quite effective in this part.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**槽标记**：标记句子中使用的槽，以对应输入，可以将其视为一个序列分类问题。这与第二章中用于标记文本序列中命名实体的方法类似。双向RNN模型在这个部分非常有效。'
- en: 'Different models can be developed for these parts, or they can be combined
    in one end-to-end model with a dialog manager. Dialog state tracking systems can
    be built by using a set of rules generated by experts or by using CRFs (see *Chapter
    2*, *Understanding Sentiment in Natural Language with BiLSTMs*, for a detailed
    explanation). Recent approaches include a Neural Belief Tracker proposed by Mrkšić
    et al. in 2017 in their paper titled *Neural Belief Tracker: Data-Driven Dialogue
    State Tracking*. This system takes three inputs:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '这些部分可以开发不同的模型，也可以结合在一个端到端的模型中，并配备对话管理器。可以使用专家生成的一组规则，或使用CRFs（详见*第2章*，*使用BiLSTMs理解自然语言中的情感*）来构建对话状态跟踪系统。最近的研究方法包括Mrkšić等人在2017年提出的神经信念跟踪器，该方法在他们的论文《Neural
    Belief Tracker: Data-Driven Dialogue State Tracking》中有所介绍。该系统需要三个输入：'
- en: The last system output
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后的系统输出
- en: The last user utterance
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后的用户发言
- en: A slot-value pair from the possible candidates for slots
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个来自可能的候选槽的槽值对
- en: These three inputs are combined through the content model and semantic decoding
    model and fed to a binary decision (softmax) layer to produce a final output.
    Deep reinforcement learning is being used to optimize the dialog policy overall.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种输入通过内容模型和语义解码模型进行组合，并输入到二分类决策（softmax）层，以生成最终输出。深度强化学习正被用来优化整体对话策略。
- en: In the NLG part, the most common approach is to define a set of templates that
    can be dynamically populated. This approach was shown in the preceding figure
    *Figure 9.13*. Neural methods, such as semantically controlled LSTM, as proposed
    by Wen et al. in their paper *Semantically Conditioned LSTM-based Natural Language
    Generation for Spoken Dialogue Systems* in 2015, are being actively researched.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言生成（NLG）部分，最常见的方法是定义一组可以动态填充的模板。这个方法在前面的图示*图 9.13*中展示过。神经网络方法，如Wen等人在2015年发表的论文*基于语义控制的LSTM自然语言生成用于对话系统*中提出的语义控制LSTM，正在积极研究中。
- en: Now, let's move on to another interesting area of conversational agents – question-answering
    and machine reading comprehension.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入另一个有趣的对话代理领域——问答和机器阅读理解。
- en: Question-answering and MRC conversational agents
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问答和机器阅读理解（MRC）对话代理
- en: Bots can be trained to answer questions based on information contained in a
    **knowledge base** (**KB**). This setting is called the question-answering setting.
    Another related area is **machine reading comprehension** or **MRC**. In MRC,
    questions need to be answered with respect to a set of passages or documents provided
    with the query. Both of these areas are seeing a lot of startup activity and innovation.
    A very large number of business use cases can be enabled with both of these types
    of conversational agents. Passing the financial report to a bot and answering
    questions such as the increase in revenue given the financial report would be
    an example of MRC. Organizations have large digital caches of information, with
    new information pouring in every day. Building such agents empowers knowledge
    workers to process and parse large amounts of information quickly. Startups like
    Pryon are delivering conversational AI agents that merge, ingest, and adapt a
    myriad of structured and unstructured data into unified knowledge domains that
    end users can ask natural language questions as a way to discover information.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人可以通过基于**知识库**（**KB**）中包含的信息进行训练，来回答问题。这种设置称为问答设置。另一个相关领域是**机器阅读理解**或**MRC**。在MRC中，问题需要根据与查询一同提供的一组段落或文档进行回答。这两个领域都正看到大量的初创企业活动和创新。通过这两种类型的对话代理，可以实现大量商业应用场景。例如，将财务报告交给机器人，回答诸如“根据财务报告，收入增长多少”的问题，就是MRC的一个例子。组织拥有大量的数字化信息库，每天都有新信息涌入。构建这样的代理可以帮助知识工作者快速处理和解析大量信息。像Pryon这样的初创公司正在提供对话式AI代理，这些代理融合、摄取并适应各种结构化和非结构化数据，形成统一的知识领域，终端用户可以通过自然语言提问的方式来发现信息。
- en: KBs typically consist of subject-predicate-object triples. The subject and object
    are entities, while the predicate indicates a relationship between them. The KB
    can be represented as a knowledge graph, where objects and subjects are nodes
    connected by predicate edges. A big challenge is the maintenance of such knowledge
    bases and graphs in real life. Most deep NLP approaches are focused on determining
    whether a given subject-predicate-object triplet is true or not. The problem is
    reduced to a binary classification through this reformulation. There are several
    approaches, including the use of BERT models, which can solve the classification
    problem. The key here is to learn an embedding of the KB and then frame queries
    on top of this embedding. Dat Nguyen's survey paper, titled *A survey of embedding
    models of entities and relationships for knowledge graph completion*, provides
    an excellent overview of various topics for a deeper dive. We focus on MRC for
    the rest of this section now.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 知识库（KB）通常由主体-谓词-宾语三元组组成。主体和宾语是实体，而谓词表示它们之间的关系。KB可以表示为知识图谱，其中主体和宾语是节点，通过谓词边连接。一个大的挑战是如何在实际中维护这样的知识库和图谱。大多数深度NLP方法集中于判断给定的主体-谓词-宾语三元组是否为真。通过这种重新表述，问题被简化为二分类问题。有几种方法，包括使用BERT模型，这些方法可以解决分类问题。这里的关键是学习知识库的嵌入，然后在此嵌入之上构建查询。Dat
    Nguyen的调查论文*实体和关系嵌入模型用于知识图谱补全的综述*提供了各种主题的精彩概述，可以深入了解。接下来，我们将专注于MRC这一部分内容。
- en: MRC is a challenging task as the objective is to answer any set of questions
    about a given set of passages or documents. These passages are not known in advance
    and may be of variable length. The most common research dataset used for evaluating
    models is the **Stanford Question Answering Dataset** or **SQuAD**, as it is commonly
    called. The dataset has 100,000 questions for different Wikipedia articles. The
    objective of the model is to output the span of text from the article that answers
    the question. A more challenging dataset has been published by Microsoft based
    on Bing queries. This dataset is called the **MAchine Reading COmprehension**
    or **MARCO** dataset. This dataset has over 1 million anonymized questions, with
    over 8.8 million passages extracted from over 3.5 million documents. Some of the
    questions in this dataset may not be answerable based on the passages, which is
    not the case with the SQuAD dataset, which makes this a challenging dataset. The
    second challenging aspect of MARCO as compared to SQuAD is that MARCO requires
    the generation of an answer by combining information from multiple passages, whereas
    SQuAD requires marking the span from the given passage.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: MRC（机器阅读理解）是一个具有挑战性的任务，其目标是回答关于给定一组段落或文档的任何问题。这些段落在预先并不已知，且长度可能不同。最常用的评估模型的研究数据集是**斯坦福问答数据集**，也就是通常所说的**SQuAD**。该数据集包含了10万个问题，涉及不同的维基百科文章。模型的目标是输出文章中回答问题的文本跨度。微软基于必应查询发布了一个更具挑战性的数据集，称为**机器阅读理解（MAchine
    Reading COmprehension）**数据集，或简称**MARCO**。该数据集包含超过100万个匿名问题，涵盖超过880万个段落，提取自350多万个文档。该数据集中的一些问题可能无法根据段落内容找到答案，而SQuAD数据集则没有此问题，这使得MARCO成为一个具有挑战性的数据集。与SQuAD相比，MARCO的第二个挑战性在于它要求从多个段落中合成信息来生成答案，而SQuAD则要求标注给定段落中的文本跨度。
- en: 'BERT and its variants such as *ALBERT: A Lite BERT for Self-supervised Learning
    of Language Representations* published at ICLR 2020 form the basis of most competitive
    baselines today. BERT architecture is well suited to this task as it allows passing
    in two pieces of input text separated by a [SEP] token. The BERT paper evaluated
    their language model on a number of tasks, including performance on the SQuAD
    task. Question tokens formed the first part of the pair, and the passage/document
    formed the second part of the pair. The output tokens corresponding to the second
    part, the passage, are scored to represent whether the token represents the start
    of the span or the end of the span.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: BERT及其变体，例如在2020年ICLR上发布的*ALBERT：一种轻量级BERT用于自监督学习语言表示*，构成了今天大多数竞争性基准的基础。BERT架构非常适合这个任务，因为它允许传入由[SEP]标记分隔的两段输入文本。BERT论文评估了他们的语言模型在多个任务上的表现，包括在SQuAD任务上的表现。问题标记形成对中的第一部分，段落/文档形成第二部分。输出标记对应第二部分，即段落，根据是否表示跨度的起始或结束，给出评分。
- en: 'A high-level depiction of the architecture is shown in *Figure 9.15*:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 架构的高级图示见于*图9.15*：
- en: '![](img/B16252_09_15.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_09_15.png)'
- en: 'Figure 9.15: BERT fine-tuning approach for SQuAD question answering'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15：BERT微调方法用于SQuAD问答
- en: A multi-modal aspect of question answering is Visual QA, which was briefly introduced
    in *Chapter 7*, *Multi-modal Networks and Image Captioning with ResNets and Transformer*.
    Analogous architectures to the one proposed for image captioning, which can take
    images as well as text tokens, are used for solving this challenge.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 问答的一个多模态方面是视觉问答（Visual QA），在*第7章*，*多模态网络与图像描述（使用ResNets和Transformer）*中简要介绍过。与图像描述所提议的架构类似，能够处理图像和文本标记的架构被用于解决这一挑战。
- en: 'The setting for QA above is called single turn because the user presents a
    question with a passage from where the question needs to be answered. However,
    people have conversations with a back and forth dialog. Such a setting is called
    multi-turn dialog. A follow-up question may have context from a previous question
    or answer in the conversation. One of the challenges in a multi-turn dialog is
    coreference resolution. Consider the following dialog:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 上述QA设置称为单轮对话，因为用户提出一个问题，并且提供了一个需要回答问题的段落。然而，人们的对话是来回交替的。这种设置称为多轮对话。后续问题可能包含先前问题或答案的上下文。多轮对话中的一个挑战是指代消解。考虑以下对话：
- en: '**Person**: Can you tell me the balance in my account #XYZ?'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**用户**：您能告诉我账户#XYZ中的余额吗？'
- en: '**Bot**: Your balance is $NNN.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器人**：您的余额是$NNN。'
- en: '**Person**: Can you transfer $MM to account #ABC from *that* account?'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**用户**：您能从*那个*账户转账$MM到账户#ABC吗？'
- en: '"that" in the second instruction refers to account #XYZ, which was mentioned
    in the first question from the person. This is called coreference resolution.
    In a multi-turn conversation, resolving references could be quite complicated
    based on the distance between the references. Several strides have been made in
    this area with respect to general conversation bots, which we''ll cover next.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '第二个指令中的“that”指的是账户 #XYZ，这是在提问者的第一个问题中提到的。这称为共指解析。在多轮对话中，解析共指可能会根据引用之间的距离变得相当复杂。在这一领域，关于一般对话机器人已经取得了若干进展，接下来我们将讨论这一部分内容。'
- en: General conversational agents
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一般对话代理
- en: Seq2seq models provide the best inspiration for learning multi-turn general
    conversations. A useful mental model is that of machine translation. Similar to
    the machine translation problem, the response to the previous question can be
    thought of as a translation of that input into a different language – the response.
    Encoding more context into a conversation can be achieved by passing in a sliding
    window of the previous conversation turns instead of just the last question/statement.
    The term open-domain is often used to describe bots in this area as the domain
    of the conversation is not fixed. The bot should be able to discuss a wide variety
    of topics. There are several issues that are their own research topics.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Seq2seq 模型为学习多轮一般对话提供了最佳的灵感。一个有用的思维模型是机器翻译模型。与机器翻译问题类似，对前一个问题的回答可以被看作是将该输入翻译成另一种语言——即回答。通过传入前几轮对话的滑动窗口，而不仅仅是上一轮问题/陈述，可以将更多的上下文编码到对话中。术语“开放领域”常用来描述这一领域的机器人，因为对话的领域不是固定的。机器人应该能够讨论各种话题。这个领域有几个问题是独立的研究课题。
- en: Lack of personality or blandness is one such problem. The dialog is very dry.
    As an example, we have seen the use of a temperature hyperparameter to adjust
    the predictability of the response in previous chapters. Conversational agents
    have a high propensity to generate "I don't know" responses due to a lack of specificity
    in the dialog. A variety of techniques, including GANs, can be used to address
    this. The *Personalizing Dialogue Agents* paper authored by Zhang et al. from
    Facebook outlines some of the approaches used to address this problem.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏个性或过于平淡是其中一个问题。对话非常干涩。举个例子，我们在前几章中见过使用温度超参数来调整响应可预测性的方法。由于对话中的不具体性，对话代理有很大的倾向生成“我不知道”这样的回答。可以使用包括
    GAN 在内的多种技术来解决这个问题。Facebook 的张等人撰写的 *Personalizing Dialogue Agents* 论文概述了用来解决这一问题的一些方法。
- en: Two recent examples that highlight the state of the art of writing human-like
    comments come from Google and Facebook. Google published a paper titled *Towards
    a Human-like Open-Domain Chatbot*, with a chatbot named Meena with over 2.6 billion
    parameters. The core model is a seq2seq model using an **Evolved Transformer**
    (**ET**) block for encoding and decoding. The model architecture has one ET block
    in the encoder and 13 ET block in the decoder. ET block was discovered through
    **neural architecture search** (**NAS**) on top of the Transformer architecture.
    A new human evaluation metric called **Sensibleness and Specificity Average**
    (**SSA**) was proposed in the paper. The current literature has a variety of different
    metrics being proposed for the evaluation of such open-domain chatbots with little
    standardization.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最近有两个例子突出了类人评论生成的最新技术，分别来自 Google 和 Facebook。Google 发布了一篇名为 *Towards a Human-like
    Open-Domain Chatbot* 的论文，介绍了一个名为 Meena 的聊天机器人，拥有超过 26 亿个参数。核心模型是一个 seq2seq 模型，采用
    **演化 Transformer** (**ET**) 块进行编码和解码。模型架构中，编码器有一个 ET 块，解码器有 13 个 ET 块。ET 块是通过对
    Transformer 架构进行 **神经架构搜索** (**NAS**) 发现的。该论文提出了一种新的人工评估指标，称为 **合理性与特异性平均值** (**SSA*)。目前文献中提出了多种评估开放领域聊天机器人的不同指标，但缺乏统一的标准。
- en: Another example of an open-domain chatbot is described by Facebook on [https://ai.facebook.com/blog/state-of-the-art-open-source-chatbot/](https://ai.facebook.com/blog/state-of-the-art-open-source-chatbot/).
    This paper builds on several years of research and combines the work on personalization,
    empathy, and KBs into a blended model called BlenderBot. Similar to Google's research,
    different datasets and benchmarks are used to train this chatbot. The code for
    the bot has been shared on [https://parl.ai/projects/recipes/](https://parl.ai/projects/recipes/).
    ParlAI, by Facebook research, provides several models for chatbots through [https://github.com/facebookresearch/ParlAI](https://github.com/facebookresearch/ParlAI).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook 在[https://ai.facebook.com/blog/state-of-the-art-open-source-chatbot/](https://ai.facebook.com/blog/state-of-the-art-open-source-chatbot/)上描述了另一个开放域聊天机器人的例子。本文基于多年的研究，结合了个性化、同理心和知识库（KBs）方面的工作，形成了一个名为BlenderBot的混合模型。与谷歌的研究类似，使用了不同的数据集和基准来训练这个聊天机器人。该机器人的代码已分享在[https://parl.ai/projects/recipes/](https://parl.ai/projects/recipes/)。由Facebook研究团队开发的ParlAI通过[https://github.com/facebookresearch/ParlAI](https://github.com/facebookresearch/ParlAI)提供了多个聊天机器人模型。
- en: This is a very hot area of active research with a lot of action happening in
    it. Comprehensive coverage of this topic would take a book of its own. Hopefully,
    you have learned many techniques in this book that can be combined to build amazing
    conversational agents. Let's wrap up.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常热门的研究领域，很多工作正在其中展开。要全面覆盖这个话题将需要一本独立的书。希望你通过本书学到了许多可以结合使用的技术，来构建出令人惊艳的对话式智能体。让我们总结一下。
- en: Summary
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We discussed the various types of conversational agents, such as task-oriented,
    question-answering, machine reading comprehension, and general chit-chat bots.
    Building a conversational AI system is a very challenging task with many layers,
    and it is an area of active research and development. The material covered earlier
    in the book can also help in building various parts of chatbots.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了各种类型的对话式智能体，如任务导向型、问答型、机器阅读理解型和一般闲聊型机器人。构建一个对话式AI系统是一项非常具有挑战性的任务，涉及许多层面，并且是一个活跃的研究和开发领域。本书前面讨论的内容也有助于构建聊天机器人的各个部分。
- en: Epilogue
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语
- en: First, let me congratulate you on reaching the end of the book. I hope this
    book helped you get a grounding in advanced NLP models. The main challenge facing
    a book such as this is that it will likely be obsolete by the time it reaches
    the press. The key thing is that new developments are based on past developments;
    for example, the Evolved Transformer is based on the Transformer architecture.
    Knowledge of all the models presented in the book will give you a solid foundation
    and significantly cut down the amount of time you need to spend to understand
    a new development. A set of influential and important papers for each chapter
    have also been made available in the GitHub repository. I am excited to see what
    you will discover and build next!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，恭喜你读完了本书。我希望本书能帮助你打下坚实的高级NLP模型基础。这类书籍面临的主要挑战是，它可能在印刷出版时就已经过时。关键在于，新发展是基于过去的进展；例如，Evolved
    Transformer基于Transformer架构。了解本书中呈现的所有模型将为你提供坚实的基础，并显著减少你理解新发展所需的时间。每一章的有影响力和重要的论文也已在GitHub仓库中提供。我期待看到你接下来会发现和构建什么！
