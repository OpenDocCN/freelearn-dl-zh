- en: Building Multi-Agent Environments
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建多代理环境
- en: With our single-agent experiences under our belt, we can move on to the more
    complex but equally entertaining world of working in multi-agent environments,
    training multiple agents to work in the same environment in a co-operative or
    competitive fashion. This also opens up several new opportunities for training
    agents with adversarial self-play, cooperative self-play, competitive self-play,
    and more. The possibilities become endless here, and this may be the true holy
    grail of AI.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成单代理的经验后，我们可以进入更加复杂但同样有趣的多代理环境中，在该环境中训练多个代理以合作或竞争的方式工作。这也为训练具有对抗性自我对抗、合作性自我对抗、竞争性自我对抗等的新机会打开了大门。在这里，可能性变得无穷无尽，这也许就是AI的真正“圣杯”。
- en: 'In this chapter, we are going to cover several aspects of multi-agent training
    environments and the main section topics are highlighted here:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍多代理训练环境的多个方面，主要的章节主题如下所示：
- en: Adversarial and cooperative self-play
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗性和合作性自我对抗
- en: Competitive self-play
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 竞争性自我对抗
- en: Multi-brain play
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多大脑游戏
- en: Adding individuality with intrinsic rewards
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过内在奖励增加个体性
- en: Extrinsic rewards for individuality
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个体化的外部奖励
- en: This chapter assumes you have covered the three previous chapters and completed
    some exercises in each. In the next section, we begin to cover the various self-play
    scenarios.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章假设你已经完成了前三章并做了一些练习。在下一节中，我们将开始介绍各种自我对抗场景。
- en: It is best to start this chapter with a new clone of the ML-Agents repository.
    We do this as a way of cleaning up our environment and making sure no errant configuration
    was unintentionally saved. If you need help with this, then consult one of the
    earlier chapters.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最好从ML-Agents仓库的一个新克隆开始本章内容。我们这样做是为了清理我们的环境，确保没有不小心保存的错误配置。如果你需要帮助，可以查阅前面的章节。
- en: Adversarial and cooperative self-play
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对抗性和合作性自我对抗
- en: 'The term *self-play* can, of course, mean many things to many people, but in
    this case, we mean the brain is competing (adversarial) or cooperating with itself
    by manipulating multiple agents. In the case of ML-Agents, this may mean having
    a single brain manipulating multiple agents in the same environment. There is
    an excellent example of this in ML-Agents, so open up Unity and follow the next
    exercise to get this scene ready for multi-agent training:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 术语*自我对抗*当然对不同的人来说有不同的含义，但在此案例中，我们的意思是大脑通过操控多个代理来与自己进行竞争（对抗性）或合作。在ML-Agents中，这可能意味着一个大脑在同一环境中操控多个代理。在ML-Agents中有一个很好的示例，所以打开Unity并按照下一个练习准备好这个场景以进行多代理训练：
- en: Open the SoccerTwos scene from the Assets | ML-Agents | Examples | Soccer |
    Scenes folder. The scene is set to run, by default, in player mode, but we need
    to convert it back to learning mode.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Assets | ML-Agents | Examples | Soccer | Scenes文件夹中打开SoccerTwos场景。该场景默认设置为玩家模式运行，但我们需要将其转换回学习模式。
- en: Select and disable all the SoccerFieldTwos(1) to SoccerFieldTwos(7) areas. We
    won't use those yet.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择并禁用所有SoccerFieldTwos(1)到SoccerFieldTwos(7)区域。我们暂时不使用这些区域。
- en: Select and expand the remaining active SoccerFieldTwos object. This will reveal
    the play area with four agents, two marked RedStriker and BlueStriker and two
    marked RedGoalie and BlueGoalie.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择并展开剩余的活动SoccerFieldTwos对象。这将显示一个包含四个代理的游戏区，其中两个标记为RedStriker和BlueStriker，另外两个标记为RedGoalie和BlueGoalie。
- en: 'Inspect the agents and set each one''s brain to StrikerLearning or GoalieLearning
    as appropriate, as shown here:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查代理并将每个代理的大脑设置为StrikerLearning或GoalieLearning，具体设置请参见下图：
- en: '![](img/bad9504b-6031-4dc7-89a1-6a0fdeccacb9.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bad9504b-6031-4dc7-89a1-6a0fdeccacb9.png)'
- en: Setting the learning brains on the agents
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在代理上设置学习大脑
- en: 'We have four agents in this environment being controlled by brains that are
    both cooperating with and competing against each other. To be honest, this example
    is brilliant and demonstrates incredibly well the whole concept of cooperative
    and competitive self-play. If you are still struggling with some concepts, consider
    this diagram, which shows how this is put together:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个环境中，我们有四个代理，由大脑控制，这些大脑既在合作又在竞争。说实话，这个示例非常出色，极好地展示了合作性和竞争性自我对抗的概念。如果你还在努力理解一些概念，可以参考这个图示，它展示了如何将这些内容结合起来：
- en: '![](img/7d798d2c-a983-47a0-9e7f-123947814e6d.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d798d2c-a983-47a0-9e7f-123947814e6d.png)'
- en: The SoccerTwos brain architecture
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: SoccerTwos的大脑架构
- en: As we can see, we have two brains controlling four agents: two strikers and
    two goalies. The striker's job is to score against the goalie, and, of course,
    the goalie's job is to block goals.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如我们所见，我们有两个大脑控制四个代理：两个前锋和两个守门员。前锋的任务是进攻守门员，当然，守门员的任务是防守进球。
- en: 'Select the Academy and set the Soccer Academy | Brains | Control enabled for
    both brains, as shown:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择“Academy”并启用“Soccer Academy | Brains | Control”以控制两个大脑，如下所示：
- en: '![](img/bd97dcc4-28e4-4857-9f62-ae7795aeb202.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd97dcc4-28e4-4857-9f62-ae7795aeb202.png)'
- en: Setting the Brains to control in the Academy
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在Academy中设置大脑控制
- en: 'Also, note the Striker, Goalie Reward, and Punish settings at the bottom of
    the Soccer Academy component. It is important to also note the way the `reward`
    functions for each brain. The following are the `reward` functions described mathematically
    for this sample:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时，注意一下“Striker”、“Goalie Reward”和“Punish”设置，这些都位于“Soccer Academy”组件的底部。还需要注意的是，每个大脑的
    `reward` 函数是如何运作的。以下是该示例中 `reward` 函数的数学描述：
- en: '![](img/94d69f7e-74b3-4fd0-a481-a2a0c8a5cd95.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94d69f7e-74b3-4fd0-a481-a2a0c8a5cd95.png)'
- en: '![](img/e5e35e1a-b4c1-4fed-9ffd-7a88b020fdeb.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5e35e1a-b4c1-4fed-9ffd-7a88b020fdeb.png)'
- en: '![](img/d2aec05a-effa-4726-9bd9-3543994fc124.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d2aec05a-effa-4726-9bd9-3543994fc124.png)'
- en: '![](img/bb21059d-f367-4c9f-9c51-386c18dbd594.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb21059d-f367-4c9f-9c51-386c18dbd594.png)'
- en: That means, when a goal is scored, each of the four agents gets a reward based
    on its position and team. Thus, if red scored, the Red Striker would get a `+1`
    reward, the Blue Striker a `-0.1` reward, the Red Goalie a `+0.1` reward, and
    the poor Blue Goalie a `-1` reward. Now, you may think this could cause overlap,
    but remember that each agent's view of a state or an observation will be different.
    Thus, the reward will be applied to the policy for that state or observation.
    In essence, the agent is learning based on its current view of the environment,
    which will change based on which agent is sending that observation.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这意味着，当进球时，每个四个代理会根据其位置和队伍获得奖励。因此，如果红队进球，红队的前锋将获得 `+1` 奖励，蓝队前锋将获得 `-0.1` 奖励，红队守门员将获得
    `+0.1` 奖励，而可怜的蓝队守门员将获得 `-1` 奖励。现在，你可能会认为这可能会导致重叠，但请记住，每个代理对一个状态或观察的看法是不同的。因此，奖励将根据该状态或观察应用于该代理的策略。实质上，代理正在基于其当前对环境的看法进行学习，这种看法会根据哪个代理发送该观察而变化。
- en: Save the scene and project when you are done editing.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑完毕后保存场景和项目。
- en: That sets up our scene for multi-agent training using two brains and four agents,
    using both competitive and cooperative self-play. In the next section, we complete
    the external configuration and start training the scene.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们的场景设置了多代理训练，使用两个大脑和四个代理，既包括竞争性又包括合作性的自我对战。在接下来的部分中，我们完成外部配置并开始训练场景。
- en: Training self-play environments
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练自我对战环境
- en: Training these types of self-play environments opens up further possibilities
    for not only enhanced training possibilities but also for fun gaming environments.
    In some ways, these types of training environments can be just as much fun to
    watch, as we will see at the end of this chapter.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这种类型的自我对战环境不仅为增强训练提供了更多可能性，还为有趣的游戏环境开辟了新的可能性。在某些方面，这种类型的训练环境看起来和观看游戏一样有趣，正如我们将在本章结束时所看到的。
- en: 'For now, though, we are going to jump back and continue setting up the configuration
    we need to train our SoccerTwos multi-agent environment in the next exercise:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在，我们将回到前面，继续设置我们需要的配置，以便在下一步的练习中训练我们的SoccerTwos多代理环境：
- en: 'Open the `ML-Agents/ml-agents/config/trainer_config.yaml` file and inspect
    the `StrikerLearning` and `GoalieLearning` config sections, as shown:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 `ML-Agents/ml-agents/config/trainer_config.yaml` 文件，查看 `StrikerLearning`
    和 `GoalieLearning` 配置部分，如下所示：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The obvious thought is that the brains should have a similar configuration,
    and you may start that way, yes. However, note that even in this example the `batch_size`
    parameter is set differently for each brain.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显而易见的想法是大脑应该有类似的配置，你可能会从这种方式开始，没错。然而，请注意，即使在这个示例中，`batch_size` 参数对于每个大脑也设置得不同。
- en: 'Open a Python/Anaconda window and switch to your ML-Agents virtual environment
    and then launch the following command from the `ML-Agents/ml-agents` folder:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Python/Anaconda窗口，切换到ML-Agents虚拟环境，然后从 `ML-Agents/ml-agents` 文件夹中启动以下命令：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Press Play when prompted, and you should see the following training session
    running:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当提示时按下播放，你应该能看到以下训练会话正在运行：
- en: '![](img/d39dccad-9782-46dd-b092-b65d37365fab.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d39dccad-9782-46dd-b092-b65d37365fab.png)'
- en: The SoccerTwos scene running in training mode
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模式下运行的SoccerTwos场景
- en: As has been said, this can be a very entertaining sample to watch, and it trains
    surprisingly quickly.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，这可以是一个非常有趣的示例，观看时很有娱乐性，并且训练速度惊人地快。
- en: 'Open up the Python/Anaconda console after some amount of training, and note
    how you are getting stats on two brains now, StrikerLearning and GoalieLearning,
    as shown in the following screenshot:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在进行了一些训练后，打开 Python/Anaconda 控制台，并注意到现在你得到了两个大脑的统计信息，分别是 StrikerLearning 和 GoalieLearning，如下图所示：
- en: '![](img/38567196-c47a-438e-b8de-42f02e49ade0.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/38567196-c47a-438e-b8de-42f02e49ade0.png)'
- en: Console output showing stats from two brains
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台输出显示来自两个大脑的统计信息
- en: Note how StrikerLearning and GoalieLearning are returning opposite rewards to
    each other. This means, in order for these agents to be trained, they must balance
    their mean reward to 0 for both agents. As the agents train, you will notice their
    rewards start to converge to 0, the optimum reward for this example.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意到 StrikerLearning 和 GoalieLearning 互相返回相反的奖励。这意味着，为了训练这些代理，它们必须使两者的平均奖励都平衡到
    0。当代理们进行训练时，你会注意到它们的奖励开始收敛到 0，这是这个示例的最佳奖励。
- en: Let the sample run to completion. You can easily get lost watching these environments,
    so you may not even notice the time go by.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让示例运行至完成。观看这些环境时很容易迷失其中，因此你甚至可能没有注意到时间流逝。
- en: This example showed how we can harness the power of multi-agent training through
    self-play to teach two brains how to both compete and cooperate at the same time.
    In the next section, we look at multiple agents competing against one another
    in self-play.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例展示了我们如何通过自我游戏利用多代理训练的力量，同时教两个大脑如何同时进行竞争和合作。在接下来的部分，我们将看看多个代理如何在自我游戏中相互竞争。
- en: Adversarial self-play
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对抗性自我游戏
- en: 'In the previous example, we saw an example of both cooperative and competitive
    self-play where multiple agents functioned almost symbiotically. While this was
    a great example, it still tied the functionality of one brain to another through
    their reward functions, hence our observation of the agents being in an almost
    rewards-opposite scenario. Instead, we now want to look at an environment that
    can train a brain with multiple agents using just adversarial self-play. Of course,
    ML-Agents has such an environment, called Banana, which comprises several agents
    that randomly wander the scene and collect bananas. The agents also have a laser
    pointer, which allows them to disable an opposing agent for several seconds if
    they are hit. This is the scene we will look at in the next exercise:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们看到了一个既有合作又有竞争的自我游戏示例，其中多个代理几乎是共生地运作的。虽然这是一个很好的示例，但它仍然将一个大脑的功能与另一个大脑通过奖励函数联系起来，因此我们观察到代理们几乎处于奖励对立的情境中。相反，我们现在想要查看一个能够仅通过对抗性自我游戏来训练大脑与多个代理的环境。当然，ML-Agents
    就有这样一个环境，称为 Banana，它包括几个随机游走在场景中并收集香蕉的代理。这些代理还有一个激光指示器，如果击中对手，可以使其禁用几秒钟。接下来的练习中，我们将查看这个场景：
- en: Open the Banana scene from the Assets | ML-Agents | Examples | BananaCollectors
    | Scenes folder.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开位于 Assets | ML-Agents | Examples | BananaCollectors | Scenes 文件夹中的 Banana
    场景。
- en: Select and disable the additional training areas RLArea(1) to RLArea(3).
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择并禁用额外的训练区域 RLArea(1) 到 RLArea(3)。
- en: Select the five agents (Agent, Agent(1), Agent(2), Agent(3), Agent(4)) in the
    RLArea.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 RLArea 中的五个代理（Agent、Agent(1)、Agent(2)、Agent(3)、Agent(4)）。
- en: Swap the Banana Agent | Brain from BananaPlayer to BananaLearning.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Banana Agent | Brain 从 BananaPlayer 切换到 BananaLearning。
- en: Select the Academy and set the Banana Academy | Brains | Control property to
    Enabled.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择学院并将 Banana Academy | Brains | Control 属性设置为启用。
- en: 'Select the Banana Agent component (Script) in the editor, and open it in your
    code editor of choice. If you scroll down to the bottom, you can see the `OnCollisionEnter`
    method as shown:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在编辑器中选择 Banana Agent 组件（脚本），并在你选择的代码编辑器中打开。如果你向下滚动到页面底部，你会看到 `OnCollisionEnter`
    方法，如下所示：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Reading the preceding code, we can summarize our `reward` functions to the
    following:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读上述代码后，我们可以将 `reward` 函数总结为以下内容：
- en: '![](img/069ca155-ee9c-4ff0-a01a-2cff13ed6d22.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/069ca155-ee9c-4ff0-a01a-2cff13ed6d22.png)'
- en: '![](img/27024412-a985-427d-a522-a5586088562c.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/27024412-a985-427d-a522-a5586088562c.png)'
- en: This simply means the agents only receive a reward for eating bananas. Interestingly,
    there is no reward for disabling an opponent with a laser or by being disabled.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅仅意味着代理们只会因吃香蕉而获得奖励。有趣的是，禁用对手（使用激光或被禁用）并没有奖励。
- en: Save the scene and the project.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存场景和项目。
- en: 'Open a prepared Python/Anaconda console and start training with the following
    command:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开准备好的 Python/Anaconda 控制台，并使用以下命令开始训练：
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Press Play in the editor when prompted, and watch the action unfold as shown
    in the next screenshot:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当提示时，按下编辑器中的Play按钮，并观察接下来截图中展示的动作：
- en: '![](img/e5e1b177-d51f-40c2-abed-6be4a9549669.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5e1b177-d51f-40c2-abed-6be4a9549669.png)'
- en: The Banana Collector agents doing their work
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 香蕉收集器代理正在执行任务
- en: Let the scene run for as long as you like.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让场景运行尽可能长的时间。
- en: This scene is an excellent example of how agents learn to use a secondary game
    mechanic that returns no rewards, but, like the laser, is still used to immobilize
    adversarial collectors and obtain more bananas, all while only receiving rewards
    for eating only bananas. This example shows some of the true power of RL and how
    it can be used to find secondary strategies in order to solve problems. While
    this is a very entertaining aspect and fun to watch in a game, consider the grander
    implications of this. RL has been shown to optimize everything from networking
    to recommender systems using **adversarial self-play**, and it will be interesting
    to see what this method of learning is capable of accomplishing in the near future.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个场景是一个很好的例子，展示了代理如何学习使用一个不返回奖励的次要游戏机制，但像激光一样，它仍然被用来使对抗性的收集者无法动弹，从而获得更多的香蕉，同时仅仅因吃香蕉才获得奖励。这个例子展示了强化学习（RL）的真正力量，以及如何利用它来发现次要策略以解决问题。虽然这是一个非常有趣的方面，观看起来也很有趣，但请考虑其更深远的影响。研究表明，RL已经被用来优化从网络到推荐系统的所有内容，通过**对抗性自我游戏**，因此未来看强化学习这种学习方法能够达成什么目标将会非常有趣。
- en: Multi-brain play
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多脑模式游戏
- en: 'One of the truly great things about the ML-Agents kit is the ability to add
    multiple agents powered by multiple brains quickly. This in turns gives us the
    ability to build more complex game environments or scenarios with fun agents/AI
    to play both with and against. Let''s see how easy it is to convert our soccer
    example to let the agents all use individual brains:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ML-Agents工具包的一个真正伟大的特点是能够快速添加由多个大脑驱动的多个代理。这使我们能够构建更复杂的游戏环境或场景，拥有有趣的代理/人工智能，既可以与其互动也可以对抗。让我们看看将我们的足球示例转换为让所有代理使用独立大脑是多么容易：
- en: Open up the editor to the SoccerTwos scene we looked at earlier.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开我们之前查看的SoccerTwos场景的编辑器。
- en: Locate the `Brains` folder for the example at Assets | ML-Agents | Examples
    | Soccer | Brains.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定位到示例中的`Brains`文件夹，路径为Assets | ML-Agents | Examples | Soccer | Brains。
- en: 'Click the Create menu in the upper right corner of the window and from the
    Context menu, and select ML-Agents | Learning Brain:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击窗口右上角的Create菜单，在上下文菜单中选择ML-Agents | Learning Brain：
- en: '![](img/5529ce20-f7be-4dcf-910c-89567b1b8c23.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5529ce20-f7be-4dcf-910c-89567b1b8c23.png)'
- en: Creating a new learning brain
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的学习大脑
- en: Name the new brain `RedStrikerLearning`.  Create three more new brains named
    `RedGoalieLearning`, `BlueGoalieLearning`, and `BlueStrikerLearning` in the same
    folder.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新大脑命名为`RedStrikerLearning`。在同一文件夹中创建三个新的大脑，分别命名为`RedGoalieLearning`、`BlueGoalieLearning`和`BlueStrikerLearning`。
- en: 'Select RedStrikerLearning. Then select and drag the StrikerLearning brain and
    drop it into the Copy Brain Parameters from slot:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择RedStrikerLearning。然后选择并拖动StrikerLearning大脑，将其放入“从槽复制大脑参数”位置：
- en: '![](img/895d4ef3-1d95-41ae-90d8-6b2bd63ae8c5.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/895d4ef3-1d95-41ae-90d8-6b2bd63ae8c5.png)'
- en: Copying brain parameters from another brain
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 从另一个大脑复制大脑参数
- en: Do this for BlueStrikerLearning, copying parameters from StrikerLearning. Then
    do the same for the RedGoalieLearning and BlueGoalieLearning, copying parameters
    from GoalieLearning.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于BlueStrikerLearning，复制StrikerLearning的参数。然后对RedGoalieLearning和BlueGoalieLearning执行相同操作，复制GoalieLearning的参数。
- en: Select the RedAgent in the Hierarchy window and set the Agent Soccer | Brain
    to RedStrikerLearning. Do this for each of the other agents, matching the color
    with a position.  BlueGoalie **->** BlueGoalieLearning.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Hierarchy窗口中选择RedAgent，并将Agent Soccer | Brain设置为RedStrikerLearning。对其他每个代理执行相同操作，将颜色与位置匹配。BlueGoalie
    **->** BlueGoalieLearning。
- en: 'Select Academy and remove all the current Brains from the Soccer Academy |
    Brains list. Then add all the new brains we just created back into the list using
    the Add New button and set them to Control:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择Academy，并从Soccer Academy | Brains列表中移除当前所有的大脑。然后使用添加新按钮将所有我们刚创建的新大脑添加回列表，并设置为控制：
- en: '![](img/becf9e28-691c-454c-b52d-cbac98e3881e.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/becf9e28-691c-454c-b52d-cbac98e3881e.png)'
- en: Adding the new brains to Academy
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 将新的大脑添加到Academy
- en: Save the scene and the project. Now, we just swapped the example from using
    two concurrent brains in self-play mode to be individual agents on teams.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存场景和项目。现在，我们只是将示例从使用两个并行大脑的自我游戏模式切换为让代理们分别在不同的队伍中。
- en: 'Open a Python/Anaconda window set up for training and launch with it the following:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个设置好用于训练的Python/Anaconda窗口，并用以下内容启动：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let the training run and note how the agents start off playing just as well
    as they did previously. Take a look at the console output as well. You will see
    it now reports for four agents, but the agents are still somewhat symbiotic, as
    the red striker is opposite the blue goalie. However, they now train much more
    slowly, due in part to each brain seeing only half the observations now. Remember
    that we had both striker agents feeding to a single brain previously, and, as
    we learned, this additional input of state can expedite training substantially.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让训练运行并注意观察代理的表现，看看他们是否像之前一样发挥得那么好。同时也查看控制台输出。你会看到现在它为四个代理提供报告，但代理之间仍然有些共生关系，因为红色前锋与蓝色守门员相对。然而，现在他们的训练速度要慢得多，部分原因是每个大脑现在只能看到一半的观察数据。记得之前我们有两个前锋代理将数据输入到一个大脑，而正如我们所学，这种额外的状态输入可以显著加速训练。
- en: At this point, we have four agents with four individual brains playing a game
    of soccer. Of course, since the agents are still training symbiotically by sharing
    a reward function, we can't really describe them as individuals. Except, as we
    know, individuals who play on teams are often influenced by their own internal
    or intrinsic reward system. We will look at how the application of intrinsic rewards
    can make this last exercise more interesting in the next section.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们有四个代理，四个独立的大脑正在进行一场足球比赛。当然，由于代理仍通过共享奖励函数进行共生式训练，我们不能真正把它们称为独立个体。除了，如我们所知，队伍中的个体往往会受到其内在奖励系统的影响。我们将在下一部分中查看内在奖励的应用如何使最后的这个练习更加有趣。
- en: Adding individuality with intrinsic rewards
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过内在奖励增加个体性
- en: As we learned in [Chapter 9](ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml), *Rewards
    and Reinforcement Learning*, intrinsic reward systems and the concept of agent
    motivation is currently implemented as just **curiosity learning** in ML-Agents.
    This whole area of applying intrinsic rewards or motivation combined with RL has
    wide applications to gaming and interpersonal applications such as **servant agents**.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第9章](ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml)《奖励与强化学习》中学到的，内在奖励系统和代理动机的概念目前在ML-Agents中只是作为**好奇心学习**实现的。应用内在奖励或动机与强化学习结合的这一领域，在游戏和人际应用中有广泛的应用，例如**仆人代理**。
- en: 'In the next exercise, we are going to add intrinsic rewards to a couple of
    our agents and see what effect this has on the game. Open up the scene from the
    previous exercise and follow these steps:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将为一些代理添加内在奖励，并观察这对游戏产生什么影响。打开上一个练习的场景并按以下步骤操作：
- en: Open up the `ML-Agents/ml-agents/config/trainer_config.yaml` file in a text
    editor. We never did add any specialized configuration to our agents, but we are
    going to rectify that now and add some extra configurations.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`ML-Agents/ml-agents/config/trainer_config.yaml`文件，用文本编辑器进行编辑。我们之前没有为我们的代理添加任何专门的配置，但现在我们将纠正这一点并添加一些额外的配置。
- en: 'Add the following four new brain configurations to the file:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下四个新的大脑配置添加到文件中：
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Note how we have also enabled `use_curiosity: true` on the `BlueGoalieLearning`
    and `RedStrikerLearning` brains. You can copy and paste most of this from the
    original `GoalieLearning` and `StrikerLearning` brain configurations already in
    the file; just pay attention to the details.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '注意我们已经在`BlueGoalieLearning`和`RedStrikerLearning`大脑上启用了`use_curiosity: true`。你可以从文件中原有的`GoalieLearning`和`StrikerLearning`大脑配置中复制并粘贴大部分内容；只需注意细节即可。'
- en: Save the file when you are done editing.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑完成后保存文件。
- en: 'Open your Python/Anaconda console and start training with the following command:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你的Python/Anaconda控制台并使用以下命令开始训练：
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let the agents train for a while, and you will notice that, while they do appear
    to work more like individuals, their training ability is still subpar, while any
    improvement we do see in training is likely the cause of giving a couple of agents
    curiosity.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让代理训练一段时间，你会注意到，尽管它们确实表现得像更独立的个体，但它们的训练能力仍然较差，任何在训练中看到的进步很可能是因为给了几个代理好奇心奖励。
- en: This ability to add individuality to an agent with intrinsic rewards or motivation
    will certainly mature as DRL does for games and other potential applications and
    will hopefully provide other intrinsic reward modules that may not be entirely
    focused on learning. However, intrinsic rewards can really do much to encourage
    individuality, so in the next section, we introduce extrinsic rewards to our modified
    example.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 通过内在奖励或动机为代理添加个性化的能力，随着深度强化学习（DRL）在游戏和其他潜在应用中的发展，肯定会逐渐成熟，并希望能够提供其他不完全专注于学习的内在奖励模块。然而，内在奖励确实能鼓励个性化，因此，在下一节中，我们将为修改后的示例引入外在奖励。
- en: Another excellent application of transfer learning would be the ability to add
    intrinsic reward modules after agents have been trained on general tasks.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习的另一个优秀应用是，在代理已经完成一般任务的训练后，能够添加内在奖励模块。
- en: Extrinsic rewards for individuality
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 个性化的外在奖励
- en: We have looked extensively at external or extrinsic rewards for several chapters
    now and how techniques can be used to optimize and encourage them for agents.
    Now, it may seem like the easy way to go in order to modify an agent's behavior
    is by altering its extrinsic rewards or in essence its reward functions. However,
    this can be prone to difficulties, and this can often alter training performance
    for the worse, which is what we witnessed when we added **Curriculum Learning**
    (**CL**) to a couple of agents in the previous section. Of course, even if we
    make the training worse, we now have a number of techniques up our sleeves such
    as **Transfer Learning** (**TL**), also known as** Imitation Learning** (**IL**);
    **Curiosity**; and CL, to help us correct things.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在多个章节中广泛讨论了外部或外在奖励，以及如何使用技术来优化和鼓励它们对代理的作用。现在，看似通过修改代理的行为来调整其外在奖励或本质上的奖励函数，似乎是一种简便的方法。然而，这可能会带来困难，并且通常会导致训练表现变差，这就是我们在前一节中为几个代理添加**课程学习**（**CL**）时所观察到的情况。当然，即使训练变差，我们现在手头上有许多技巧，比如**迁移学习**（**TL**），也叫做**模仿学习**（**IL**）；**好奇心**；和
    CL，来帮助我们纠正问题。
- en: 'In the next exercise, we are going to look to add further individuality to
    our agents by adding additional extrinsic rewards. Open up the previous exercise
    example we were just working on and follow along:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将通过添加额外的外在奖励来为我们的代理增添更多个性。打开我们刚才正在操作的前一个练习示例并跟着做：
- en: From the menu, select Window | Asset Store.  This will take you to the Unity
    Asset Store, which is an excellent resource for helper assets. While most of these
    assets are paid, honestly, the price compared to comparable developer tools is
    minimal, and there are several free and very excellent assets that you can start
    using to enhance your training environments. The Asset Store is one of the best
    and worst things about Unity, so if you do purchase assets, be sure to read the
    reviews and forum posts. Any good asset will typically have its own forum if it
    is developer-focused, artistic assets much less so.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从菜单中选择窗口 | 资产商店。这将带你进入 Unity 资产商店，这是一个非常优秀的辅助资源库。虽然大多数这些资源是付费的，但老实说，与同类开发者工具相比，价格非常低廉，而且有几个免费的、非常优秀的资源，你可以开始使用它们来增强你的训练环境。资产商店是
    Unity 最好与最糟糕的地方之一，所以如果你购买了资源，记得查看评论和论坛帖子。任何好的资源通常都会有自己的开发者论坛，而艺术资源则较少。
- en: In the search bar, enter `toony tiny people` and press the *Enter* key or click
    the Search button. This will display the search results.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在搜索栏中输入 `toony tiny people` 并按 *Enter* 键或点击搜索按钮。这将显示搜索结果。
- en: We would like to thank **Polygon Blacksmith** for their support in allowing
    us to distribute their Toony Tiny People Demo asset with the book's source. Also,
    their collection of character assets is very well done and simple to use. The
    price is also at an excellent starting point for some of the larger asset packages
    if you decide you want to build a full game or enhanced demo.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢**Polygon Blacksmith**，感谢他们的支持，使我们能够将他们的 Toony Tiny People Demo 资源与本书的源文件一起分发。此外，他们的角色资源包做得非常好，且易于使用。如果你决定构建一个完整的游戏或增强版演示，他们的一些较大资源包的价格也是一个很好的起点。
- en: 'Select the result called Toony Tiny People Demo by Polygon Blacksmith and select
    it. It will appear as shown in this screenshot:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择名为 Toony Tiny People Demo 的结果，由 Polygon Blacksmith 提供，并点击选择它。它将显示在此截图中：
- en: '![](img/b1f3ba55-d605-4c21-828c-7cdf29d5f0a4.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b1f3ba55-d605-4c21-828c-7cdf29d5f0a4.png)'
- en: The Toony Tiny People Demo asset from Polygon Blacksmith
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Polygon Blacksmith 的 Toony Tiny People Demo 资源
- en: Click the red Download button and, after the asset has downloaded, the button
    will change to Import, as shown in the preceding screenshot. Click the Import
    button to import the assets.  When you are prompted by the Import dialog, make
    sure everything is selected and click Import.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击红色的下载按钮，下载完成后，按钮会变为导入，如前面的截图所示。点击导入按钮导入资产。当导入对话框弹出时，确保选中所有内容，然后点击导入。
- en: These types of low polygon or toon assets are perfect for making a simple game
    or simulation more entertaining and fun to watch. It may not seem like much, but
    you can spend a lot of time watching these training sims run, and it helps if
    they look appealing.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类型的低多边形或卡通资产非常适合让简单的游戏或模拟更具娱乐性和观看乐趣。虽然看起来不多，但你可以花费大量时间观看这些训练模拟的运行，若它们看起来更吸引人，那将大有帮助。
- en: Select and expand all the agent objects in Hierarchy. This includes RedStriker,
    BlueStriker, RedGoalie, and BlueGoalie.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择并展开层级中的所有代理对象。这包括 RedStriker、BlueStriker、RedGoalie 和 BlueGoalie。
- en: Open the Assets | TooyTinyPeople | TT_demo | prefabs folder in the Project window.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开项目窗口中的 Assets | TooyTinyPeople | TT_demo | prefabs 文件夹。
- en: 'Select and drag the TT_demo_Female prefab from the preceding folder and drop
    it into the RedStriker agent object in the Hierarchy window. Select the cube object
    just beneath the agent and disable it in the inspector. Continue to do this for
    the other agents according to the following list:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从前面的文件夹中选择并拖动 TT_demo_Female 预设体，并将其拖放到层级窗口中的 RedStriker 代理对象上。选择位于代理对象下方的立方体对象，并在检查器中禁用它。继续按以下列表对其他代理执行相同操作：
- en: TT_demo_female -> RedStriker
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: TT_demo_female -> RedStriker
- en: TT_demo_male_A -> BlueStriker
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: TT_demo_male_A -> BlueStriker
- en: TT_demo_police -> BlueGoalie
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: TT_demo_police -> BlueGoalie
- en: TT_demo_zombie -> RedGoalie
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: TT_demo_zombie -> RedGoalie
- en: 'This is further demonstrated in this screenshot:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点在下图中得到了进一步展示：
- en: '![](img/13987114-bc54-4b7a-9fc1-4b3716e8b4ec.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13987114-bc54-4b7a-9fc1-4b3716e8b4ec.png)'
- en: Setting the new agent bodies
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 设置新的代理模型
- en: 'Make sure to also reset the new agent model''s Transform Position and Orientation
    to `[0,0,0]`, as shown in the following screenshot:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保也将新代理模型的 Transform 位置和朝向重置为 `[0,0,0]`，如以下截图所示：
- en: '![](img/362c8dbb-4f85-4fc3-bbde-1b2f2d0de4f9.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/362c8dbb-4f85-4fc3-bbde-1b2f2d0de4f9.png)'
- en: Resetting the orientation and position of dragged prefabs
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 重置拖动的预设体的朝向和位置
- en: Save the scene and project.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存场景和项目。
- en: At this point, you can run the scene in training and watch the new agent models
    move around, but there isn't much point. The agents will still act the same, so
    what we need to do next is set additional extrinsic rewards based on some arbitrary
    personality, which we will define in the next section.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你可以在训练中运行场景，观看新的代理模型移动，但这并没有多大意义。代理的行为仍然是一样的，所以接下来我们需要做的是基于某些任意的个性设置额外的外在奖励，我们将在下一节定义这些个性。
- en: Creating uniqueness with customized reward functions
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过自定义奖励函数创建独特性
- en: We managed to have some success in making our agents unique by adding intrinsic
    rewards, although the results may have been not as unique as we would have liked.
    This means we now want to look at modifying the agents' extrinsic rewards in the
    hopes of making their behavior more unique and ultimately more entertaining for
    the game.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管结果可能没有我们预期的那样独特，我们通过加入内在奖励成功地使我们的代理变得更具个性。这意味着我们现在希望通过修改代理的外在奖励来使其行为更具个性，最终使游戏更具娱乐性。
- en: 'The best way for us to start doing that is to look at the `SoccerTwos` reward
    functions we described earlier; these are listed here, for reference:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始实现这一点的最佳方式是查看我们之前描述的 `SoccerTwos` 奖励函数；这些奖励函数在这里列出，供参考：
- en: '![](img/8360374e-f459-4fef-98dd-3b11264f0636.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8360374e-f459-4fef-98dd-3b11264f0636.png)'
- en: '![](img/e943d6d7-592f-48e8-8bf4-1ec5798f5f5b.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e943d6d7-592f-48e8-8bf4-1ec5798f5f5b.png)'
- en: '![](img/594438d5-58dc-40fe-b993-cdf625b439ae.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/594438d5-58dc-40fe-b993-cdf625b439ae.png)'
- en: '![](img/97c31523-231f-4211-aece-29c95edcb586.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97c31523-231f-4211-aece-29c95edcb586.png)'
- en: 'What we want to do now is apply some individualistic modification to the rewards
    function based on the current character. We will do this by simply chaining the
    functions with a modification based on the character type, as shown:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在想做的是基于当前角色对奖励函数进行一些个性化修改。我们将通过简单地将函数链与基于角色类型的修改进行组合来实现，如下所示：
- en: '![](img/03fb39c9-5222-466c-a90a-b30d4e4c340c.png)  or  ![](img/7be32220-52f5-4067-a9a1-d9731f2d59dc.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03fb39c9-5222-466c-a90a-b30d4e4c340c.png) 或 ![](img/7be32220-52f5-4067-a9a1-d9731f2d59dc.png)'
- en: '![](img/a2547f6b-3e83-4ce3-abb7-a9405f5ef26b.png)  or  ![](img/33de502b-245d-4659-9715-41af693970b9.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2547f6b-3e83-4ce3-abb7-a9405f5ef26b.png) 或 ![](img/33de502b-245d-4659-9715-41af693970b9.png)'
- en: '![](img/fd164fb9-7793-492c-a371-3bfac8a77c08.png) or  ![](img/b540589a-8861-490a-8476-253b634e98b7.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd164fb9-7793-492c-a371-3bfac8a77c08.png) 或 ![](img/b540589a-8861-490a-8476-253b634e98b7.png)'
- en: '![](img/dd76d52b-c033-4888-856b-f5e6384f4440.png) or  ![](img/ef696a36-6623-4c61-bb89-57805927aac0.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd76d52b-c033-4888-856b-f5e6384f4440.png) 或 ![](img/ef696a36-6623-4c61-bb89-57805927aac0.png)'
- en: All we are doing here with these reward functions is simply modifying the reward
    value by some personality modification. For the girl, we give her a bonus of 1.25
    x the rewards, reflecting that she may be excited. The boy is less excited, so
    we modify his rewards by .95 times, which reduces them slightly. The policeman,
    who is always calm and in control, remains constant with no rewards modifications. 
    Finally, we introduce a bit of a wildcard, the half-dead zombie. In order to characterize
    it as half-dead, we also decrease all of its rewards by half as well.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这些奖励函数中所做的只是通过某些个性修改来调整奖励值。对于女孩，我们给她 1.25 倍的奖励，反映出她可能很兴奋。男孩则不那么兴奋，因此我们将他的奖励调整为
    0.95 倍，稍微减少奖励。警察则始终冷静且掌控自如，奖励保持不变。最后，我们引入了一个变数——半死的僵尸。为了表现它是半死不活，我们还将它的奖励减少一半。
- en: You could, of course, modify these functions in any way you please, according
    to your game mechanics, but it is important to note that the effect of the personality
    modification you are applying could hinder training. Be sure to take a mental
    note of that as we get into training this example as well.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以根据游戏机制修改这些函数，但需要注意的是，你所应用的个性修改可能会妨碍训练。在我们开始训练这个示例时，务必记住这一点。
- en: A girl, a boy, a zombie, and a policeman enter the soccer field.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一个女孩，一个男孩，一个僵尸和一个警察走进了足球场。
- en: Now that we understand the new reward functions, we want to add to our example
    that it is time to open Unity and code them. This example will require some slight
    modifications to the C# files, but the code is quite simple and should be readily
    understood by any programmer with experience of a C-based language.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了新的奖励函数，我们想要在示例中添加一些内容，说明是时候打开 Unity 并编写代码了。这个示例将需要对 C# 文件做一些轻微的修改，但代码非常简单，任何有
    C 语言经验的程序员都应该能轻松理解。
- en: 'Open up Unity to the scene we were modifying in the previous example, and follow
    the next exercise:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 Unity，进入我们在上一个示例中修改的场景，并跟随下一个练习：
- en: Locate the RedStriker  agent in the Hierarchy window and select it.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层级窗口中找到 RedStriker 代理并选择它。
- en: From Inspector, click the gear icon beside the Agent Soccer component and, from
    the Context menu, select Edit Script.  This will open the script and solution
    in your editor.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Inspector 面板中，点击 Agent Soccer 组件旁边的齿轮图标，然后在上下文菜单中选择“编辑脚本”。这将会在你的编辑器中打开脚本和解决方案。
- en: 'Add a new `enum` called `PersonRole` at the top of the file right after the
    current `enum AgentRole` and as shown in the code:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在文件顶部的当前 `enum AgentRole` 后添加一个新的 `enum`，名为 `PersonRole`，如代码所示：
- en: '[PRE7]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This creates a new role, for, in essence, the personality we want to apply to
    each brain.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这创建了一个新的角色，实际上是我们希望应用到每个大脑的个性。
- en: 'Add another new variable to the class, as shown:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向类中添加另一个新变量，如下所示：
- en: '[PRE8]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'That adds the new `PersonRole` to the agent. Now we want to also add the new
    type to the setup by adding a single line to the `InitializeAgent`  method, shown
    here:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将 `PersonRole` 新角色添加到代理中。现在，我们还想通过向 `InitializeAgent` 方法添加一行代码来将新类型添加到设置中，如下所示：
- en: '[PRE9]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You should likely see an error now in the line. That is because we also need
    to add the new `personRole` property to `PlayerState`. Open the `PlayerState`
    class and add the property as shown:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你现在可能会看到一行错误。这是因为我们还需要将新的 `personRole` 属性添加到 `PlayerState` 中。打开 `PlayerState`
    类并按如下所示添加属性：
- en: '[PRE10]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You should now be in the `SoccerFieldArea.cs` file.  Scroll to the `RewardOrPunishPlayer`
    method and modify it as shown:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你现在应该已经进入了 `SoccerFieldArea.cs` 文件。滚动到 `RewardOrPunishPlayer` 方法，并按如下所示修改：
- en: '[PRE11]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'What we are doing here is injecting another reward function, `RewardOrPunishPerson`,
    in order to add our extrinsic personality rewards. Next, add a new `RewardOrPunishPerson` method,
    as shown:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这里做的是注入另一个奖励函数，`RewardOrPunishPerson`，以便添加我们外部的个性奖励。接下来，添加一个新的 `RewardOrPunishPerson`
    方法，如下所示：
- en: '[PRE12]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: That code does exactly what our earlier customized reward functions do. When
    you are done editing, save all your files and return to the Unity editor. If there
    are any errors or compiler warnings, they will be shown in the console. If you
    need to go back and fix any (red) error issues, do so.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码的功能与我们之前定制的奖励函数完全相同。编辑完成后，保存所有文件并返回到 Unity 编辑器。如果有任何错误或编译警告，它们将在控制台中显示。如果需要返回并修复任何（红色）错误，进行修正即可。
- en: As you can see, with very little code, we are able to add our extrinsic personality
    rewards. You could, of course, enhance this system in any number of ways and even
    make it more generic and parameter-driven. In the next section, we look to put
    all this together and get our agents training individually.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，凭借很少的代码，我们就能够添加外在的个性奖励。当然，你可以以任何方式增强这个系统，甚至让它更通用、参数化。在接下来的部分，我们将把所有这些内容整合起来，开始训练我们的代理。
- en: Configuring the agents' personalities
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置代理的个性
- en: 'With all the code set up, we can now continue back in the editor and set up
    the agents to match the personality we want to apply to them. Open up the editor
    again, and follow the next exercise to apply the personalities to the agents and
    start training:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 所有代码设置好后，我们现在可以继续回到编辑器，设置代理以匹配我们想要应用的个性。再次打开编辑器，按照接下来的练习将个性应用到代理上并开始训练：
- en: 'Select RedStriker in Hierarchy and set the Agent Soccer | Person Role parameter
    we just created to Girl, as shown:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层级视图中选择 RedStriker，并将我们刚刚创建的 Agent Soccer | Person Role 参数设置为 Girl，如下所示：
- en: '![](img/54e3b787-d936-453c-8103-5aa2c5fd599d.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54e3b787-d936-453c-8103-5aa2c5fd599d.png)'
- en: Setting the personalities on each of the agents
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个代理设置个性
- en: Update all the agents with the relevant personality that matches the model we
    assigned earlier: BlueStriker-> Boy, BlueGoalie -> Police, and RedGoalie -> Zombie, as
    shown in the preceding screenshot.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新所有代理，使其具备与我们之前分配的模型匹配的相关个性：BlueStriker -> 男孩，BlueGoalie -> 警察，RedGoalie ->
    僵尸，如前面的截图所示。
- en: Save the scene and project.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存场景和项目。
- en: Now, at this point, if you wanted it to be more detailed, you may want to go
    back and update each of the agent brain names to reflect their personalities,
    such as GirlStrikerLearning or PoliceGoalieLearning, and you can omit the team
    colors. Be sure to also add the new brain configuration settings to your `trainer_config.yaml`
    file.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在这一点上，如果你希望更详细些，你可能想回去更新每个代理的大脑名称以反映它们的个性，比如 GirlStrikerLearning 或 PoliceGoalieLearning，并且可以省略团队颜色。务必将新的大脑配置设置添加到你的
    `trainer_config.yaml` 文件中。
- en: 'Open your Python/Anaconda training console and start training with the following
    command:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你的 Python/Anaconda 训练控制台，并使用以下命令开始训练：
- en: '[PRE13]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, this can be very entertaining to watch, as you can see in the following
    screenshot:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，这会非常有趣，正如你在下面的截图中看到的：
- en: '![](img/c9e87060-970f-4b74-91d6-0add4c847e12.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9e87060-970f-4b74-91d6-0add4c847e12.png)'
- en: Watching individual personalities play soccer
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 观看不同个性在踢足球
- en: Note how we kept the team color cubes active in order to show which team each
    individual agent is on.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，我们保留了团队颜色的立方体，以显示每个代理所属的团队。
- en: Let the agents train for several thousand iterations and then open the console;
    note how the agents now look less symbiotic. In our example, they are still paired
    with each other, since we only applied a simple linear transformation to the rewards.
    You could, of course, apply more complex functions that are non-linear and not
    inversely related that describe some other motivation or personality for your
    agents.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让代理训练几千次迭代后，再打开控制台；注意代理们现在看起来不再那么共生了。在我们的示例中，它们仍然是成对出现的，因为我们仅对奖励应用了简单的线性变换。当然，你也可以应用更复杂的非线性函数，这些函数不是反相关的，可以描述代理的其他动机或个性。
- en: 'Finally, let''s open up TensorBoard and look at a better comparison of our
    multi-agent training. Open another Python/Anaconda console to the `ML-Agents/ml-agents`
    folder you are currently working in and run the following command:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们打开 TensorBoard，查看我们多代理训练的更好比较。在你当前工作的 `ML-Agents/ml-agents` 文件夹中，再次打开一个
    Python/Anaconda 控制台，并运行以下命令：
- en: '[PRE14]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Use your browser to open the TensorBoard interface and examine the results.
    Be sure to disable any extra results and just focus on the four brains in our
    current training run. The three main plots we want to focus on are shown merged
    together in this diagram:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用浏览器打开 TensorBoard 界面并检查结果。确保禁用所有额外的结果，只专注于我们当前训练中四个大脑的表现。我们要关注的三个主要图表已合并在这个图示中：
- en: '![](img/702bce1d-2467-4a1a-b68e-dbc70a022b67.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/702bce1d-2467-4a1a-b68e-dbc70a022b67.png)'
- en: TensorBoard Plots showing results of training four brains
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard绘图，显示四个大脑训练的结果
- en: As you can see from the TensorBoard results, the agents are not training very
    well. We could enhance that, of course, by adding additional training areas and
    feeding more observations in order to train the policy. However, if you look at
    the **Policy Loss** plot, the results show the agents' competition is causing
    minimal policy change, which is a bad thing this early in training. If anything,
    the zombie agent appears to be the agent learning the best from these results.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 从TensorBoard的结果中可以看出，代理的训练效果不佳。我们当然可以通过增加额外的训练区域并提供更多的观测值来改善这一点，从而训练策略。然而，如果你查看**策略损失**图，结果表明，代理之间的竞争导致了最小的策略变化，这在训练初期是一个不好的现象。如果有的话，僵尸代理似乎是从这些结果中学到最多的代理。
- en: There are plenty of other ways you can, of course, modify your extrinsic reward
    function in order to encourage some behavioral aspect in multi-agent training
    scenarios. Some of these techniques work well and some not so well. We are still
    in the early days of developing this tech and best practices still need to emerge.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以通过很多其他方式修改外部奖励函数，以鼓励在多智能体训练场景中某些行为方面的表现。这些技术中有些效果很好，有些则效果不佳。我们仍处于开发这项技术的初期阶段，最佳实践仍在逐步形成。
- en: In the next section, we look to further exercises you can work on in order to
    reinforce your knowledge of all the material we covered in this chapter.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，我们将探讨你可以做的进一步练习，以巩固我们在本章中所涵盖的所有内容。
- en: Exercises
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'As always, try at least one or two of the following exercises on your own for
    your own enjoyment and learning:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，尝试至少一个或两个以下练习，来获得个人的乐趣和学习：
- en: Open the BananaCollectors example Banana scene and run it in training mode.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开BananaCollectors示例中的Banana场景，并在训练模式下运行。
- en: Modify the BananaCollectors | Banana scene so that it uses five separate learning
    brains and then run it in training mode.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改BananaCollectors | Banana场景，使其使用五个独立的学习大脑，然后在训练模式下运行。
- en: Modify the reward functions in the last SoccerTwos exercise to use exponential
    or logarithmic functions.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改最后一个SoccerTwos练习中的奖励函数，使用指数或对数函数。
- en: Modify the reward function in the last SoccerTwos exercise to use non-inverse
    related and non-linear functions. This way, the mean modifying the positive and
    negative rewards is different for each personality.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改最后一个SoccerTwos练习中的奖励函数，使用非逆相关和非线性函数。这样，正负奖励的调整方式对于每个个性来说都是不同的。
- en: Modify the SoccerTwos scene with different characters and personalities. Model
    new rewards functions as well, and then train the agents.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改SoccerTwos场景，使用不同的角色和个性。也要建立新的奖励函数，然后训练代理。
- en: Modify the BananaCollectors example Banana scene to use the same personalities
    and custom reward functions as we did with the SoccerTwos example.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改BananaCollectors示例中的Banana场景，使其使用与SoccerTwos示例相同的个性和自定义奖励函数。
- en: Do exercise 3 with the BananaCollectors example.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用BananaCollectors示例做练习3。
- en: Do exercise 4 with the BananaCollectors example.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用BananaCollectors示例做练习4。
- en: Do exercise 5 with the BananaCollectors example.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用BananaCollectors示例做练习5。
- en: Build a new multi-agent environment using one of the current samples as a template
    or create your own.  This last exercise could very likely turn into your very
    own game.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用当前示例中的一个作为模板，或创建自己的，构建一个新的多智能体环境。这个最后的练习很有可能变成你自己的游戏。
- en: You may have noticed by now that as we progress through the book, the exercises
    become more time-consuming and difficult. Please try for your own personal benefit
    to complete at least a couple of the exercises.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，随着我们在书中的进展，练习变得更加耗时和困难。为了你自己的个人利益，请尽量完成至少几个练习。
- en: Summary
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored a world of possibilities with multi-agent training
    environments. We first looked at how we could set up environments using self-play,
    where a single brain may control multiple brains that both compete and cooperate
    with one another. Then we looked at how we could add personality with intrinsic
    rewards in the form of curiosity using the ML-Agents curiosity learning system. Next,
    we looked at how extrinsic rewards could be used to model an agent's personality
    and influence training. We did this by adding a free asset for style and then
    applied custom extrinsic rewards through reward function chaining. Finally, we
    trained the environment and were entertained by the results of the boy agent solidly
    thrashing the zombie; you will see this if you watch the training to completion.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了多智能体训练环境中的无限可能性。我们首先研究了如何通过自我对弈设置环境，在这种环境中，一个大脑可以控制多个大脑，它们既相互竞争又相互合作。接着，我们探讨了如何通过使用
    ML-Agents 好奇心学习系统，结合内在奖励的方式，增加个性化元素，激发智能体的好奇心。然后，我们研究了如何使用外在奖励来塑造智能体的个性并影响训练。我们通过添加免费的风格资产，并通过奖励函数链应用自定义的外部奖励来实现这一点。最后，我们训练了环境，并被男孩智能体彻底击败僵尸的结果逗乐；如果你观看完整的训练过程，你将看到这一点。
- en: In the next chapter, we will look at another novel application of DRL for debugging
    and testing already constructed games.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨深度强化学习（DRL）在调试和测试已构建游戏中的另一种新颖应用。
