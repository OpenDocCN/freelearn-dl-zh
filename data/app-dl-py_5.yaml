- en: Model Architecture
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型架构
- en: 'Building on fundamental concepts from C*hapter 4*, Introduction to Neural Networks
    and Deep Learning, we now move into a practical problem: can we predict Bitcoin
    prices using a deep learning model? In this chapter, we will learn how to build
    a deep learning model that attempts to do that. We will conclude this chapter
    by putting all of these components together and building a bare-bones yet complete
    fist version of a deep learning application.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 基于第4章《神经网络与深度学习介绍》的基本概念，我们现在进入一个实际问题：我们能否使用深度学习模型预测比特币价格？在本章中，我们将学习如何构建一个尝试进行此预测的深度学习模型。我们将通过将所有这些组件结合起来，构建一个简单但完整的深度学习应用程序的初步版本来结束本章。
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将能够：
- en: Prepare data for a deep learning model
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为深度学习模型准备数据
- en: Choose the right model architecture
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择正确的模型架构
- en: Use Keras, a TensorFlow abstraction library
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keras，这是一个 TensorFlow 抽象库
- en: Make predictions with a trained model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练好的模型进行预测
- en: Choosing the Right Model Architecture
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适的模型架构
- en: Deep learning is a filled undergoing intense research activity. Among other
    things, researchers are devoted to inventing new neural network architectures
    that can either tackle new problems or increase the performance of previously
    implemented architectures. In this section, we study both old and new architectures.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一个正在进行激烈研究活动的领域。研究人员致力于发明新的神经网络架构，这些架构可以解决新问题或提高之前实现的架构的性能。在本节中，我们将研究旧的和新的架构。
- en: Older architectures have been used to solve a large array of problems and are
    generally considered the right choice when starting a new project. Newer architectures
    have shown great successes in specific problems, but are harder to generalize.
    The latter are interesting as references of what to explore next, but are hardly
    a good choice when starting a project.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 旧的架构已经被广泛应用于解决各种问题，并且通常被认为是在开始新项目时的正确选择。较新的架构在特定问题上取得了巨大成功，但它们更难以推广。后者作为下一步探索的参考非常有趣，但在启动项目时并不是一个好的选择。
- en: Common Architectures
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见架构
- en: 'Considering the many architecture possibilities, there are two popular architectures
    that have often been used as starting points for a number of applications: **convolutional
    neural networks** (**CNNs**) and **recurrent neural networks** (**RNNs**). These
    are foundational networks and should be considered starting points for most projects.
    We also include descriptions of another three networks, due to their relevance
    in the field: **Long-short term memory** (**LSTM**) networks, an RNN variant;
    **generative adversarial networks** (**GANs**); and deep reinforcement learning.
    These latter architectures have shown great successes in solving contemporary
    problems, but are somewhat more difficult to use.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到众多架构的可能性，有两种流行的架构经常作为许多应用的起点：**卷积神经网络**（**CNNs**）和**递归神经网络**（**RNNs**）。这些是基础性网络，应该作为大多数项目的起点。我们还介绍了另外三种网络，因其在该领域的重要性：**长短期记忆**（**LSTM**）网络，RNN
    的变种；**生成对抗网络**（**GANs**）；以及深度强化学习。这些后者架构在解决当代问题时取得了巨大成功，但使用起来相对更为复杂。
- en: Convolutional Neural Networks
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Convolutional neural networks have gained notoriety for working with problems
    that have a grid-like structure. They were originally created to classify images,
    but have been used in a number of other areas, ranging from speech recognition
    to self-driving vehicles.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络因其在处理具有网格状结构的问题中表现出色而声名显赫。它们最初是为了分类图像而创建的，但已经在许多其他领域得到了应用，从语音识别到自动驾驶汽车。
- en: CNN's essential insight is to use closely related data as an element of the
    training process, instead of only individual data inputs. This idea is particularly
    effective in the context of images, where a pixel located to the right of another
    pixel is related to that pixel as well, given that they form part of a larger
    composition. In this case, that composition is what the network is training to
    predict. Hence, combining a few pixels together is better than using an individual
    pixel on its own.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 的核心思想是将紧密相关的数据作为训练过程的一个要素，而不仅仅是单独的数据输入。这个理念在图像处理中尤其有效，因为图像中位于另一个像素右侧的像素与该像素相关，因为它们是更大构图的一部分。在这种情况下，网络训练的目标就是预测该构图。因此，将几个像素组合在一起比仅使用单独的像素要好。
- en: 'The name **convolution** is given to the mathematical representation of this
    process:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积**这个名称是用来表示这个过程的数学表达式：'
- en: '![](img/8de2852a-e55b-420b-aed1-cbcdfa685677.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8de2852a-e55b-420b-aed1-cbcdfa685677.png)'
- en: 'Figure 1: Illustration of the convolution process Image source: Volodymyr Mnih,
    et al.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：卷积过程的插图 图像来源：Volodymyr Mnih等人。
- en: 'For more information refer, Human-level control through deep reinforcement
    learning. February 2015, Nature. Available at: [https://storage. googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参考《通过深度强化学习实现人类级别的控制》，2015年2月，《自然》杂志。可通过以下链接访问：[https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)。
- en: Recurrent Neural Networks
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: Convolutional neural networks work with a set of inputs that keep altering the
    weights and biases of the networks' respective layers and nodes. A known limitation
    of this approach is that its architecture ignores the sequence of these inputs
    when determining how to change the networks' weights and biases.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络通过一组输入来工作，这些输入会不断改变网络各层和节点的权重和偏置。这种方法的一个已知限制是，它的架构在决定如何改变网络的权重和偏置时忽略了这些输入的顺序。
- en: Recurrent neural networks were created precisely to address that problem. RNNs
    are designed to work with sequential data. This means that at every epoch, layers
    can be influenced by the output of previous layers. The memory of previous observations
    in a given sequence plays a role in the evaluation of posterior observations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络正是为了应对这个问题而创建的。RNNs旨在处理序列数据。这意味着在每个迭代中，层级可以受到前一层输出的影响。在给定序列中的先前观察的记忆在评估后续观察时起着重要作用。
- en: RNNs have had successful applications in speech recognition due to the sequential
    nature of that problem. Also, they are used for translation problems. Google Translate's
    current algorithm—called **Transformer**—uses an RNN to translate text from one
    language to another.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于语音识别问题具有序列性质，RNNs（循环神经网络）在该领域得到了成功的应用。此外，它们还用于翻译问题。谷歌翻译当前的算法——称为**Transformer**——使用RNN将文本从一种语言翻译成另一种语言。
- en: 'For more information refer, Transformer: A Novel Neural Network Architecture
    for Language Understanding, by Jakob Uszkoreit, Google Research Blog, August 2017\.
    Available at: [https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参考《Transformer：一种用于语言理解的新型神经网络架构》，作者：Jakob Uszkoreit，谷歌研究博客，2017年8月。可通过以下链接访问：[https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)。
- en: '![](img/fb6536c7-db6a-4a9a-8bdd-c24d002b1c65.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb6536c7-db6a-4a9a-8bdd-c24d002b1c65.png)'
- en: 'Figure 2: Illustration from distill.pub (https://distill.pub/2016/augmented-rnns/)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：来自distill.pub的插图（https://distill.pub/2016/augmented-rnns/）
- en: Figure 2 shows that words in English are related to words in French, based on
    where they appear in a sentence. RNNs are very popular in language translation
    problems.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图2显示了英语中的单词与法语中的单词之间的关系，这种关系取决于它们在句子中出现的位置。RNNs在语言翻译问题中非常流行。
- en: Long-short term memory networks are RNN variants created to address the vanishing
    gradient problem. The vanishing gradient problem is caused by memory components
    that are too distant from the current step and would receive lower weights due
    to their distance. LSTMs are a variant of RNNs that contain a memory component—called
    **forget gate**. That component can be used to evaluate how both recent and old
    elements affect the weights and biases, depending on where the observation is
    placed in a sequence.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆网络（LSTM）是为了解决梯度消失问题而创建的RNN变种。梯度消失问题是由于记忆组件距离当前步骤太远，导致它们因距离较远而获得较低的权重。LSTM是RNN的一种变体，包含一个叫做**忘记门**的记忆组件。该组件可用于评估近期和旧的元素如何影响权重和偏置，具体取决于观察在序列中的位置。
- en: For more details refer, The LSTM architecture was fist introduced by Sepp Hochreiter
    and Jürgen Schmidhuber in 1997\. Current implementations have had several modifiations.
    For a detailed mathematical explanation of how each component of an LSTM works,
    we suggest the article *Understanding LSTM Networks* by Christopher Olah, August
    2015, available at [http://colah.github.io/posts/2015- 08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多细节，请参见 Sepp Hochreiter 和 Jürgen Schmidhuber 于1997年首次提出的 LSTM架构。当前的实现版本已有多次修改。关于LSTM每个组件如何工作的详细数学解释，建议参考
    Christopher Olah 于2015年8月发布的文章《理解LSTM网络》，可访问：[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)。
- en: Generative Adversarial Networks
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: '**Generative adversarial networks** (**GANs**) were invented in 2014 by Ian
    Goodfellow and his colleagues at the University of Montreal. GANs suggest that,
    instead of having one neural network that optimizes weights and biases with the
    objective to minimize its errors, there should be two neural networks that compete
    against each other for that purpose.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成对抗网络** (**GANs**) 是由Ian Goodfellow及其在蒙特利尔大学的同事们于2014年发明的。GANs提出，应该有两个神经网络相互竞争，以此来优化权重和偏置，而不是仅有一个神经网络去最小化其错误。'
- en: 'For more details refer, Generative Adversarial Networks by Ian Goodfellow,et
    al, arXiv. June 10, 2014\. Available at: [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多细节，请参见 Ian Goodfellow 等人所著的《生成对抗网络》，发表于 arXiv. 2014年6月10日。可访问：[https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)。
- en: 'GANs have a network that generates new data (that is, "fake" data) and a network
    that evaluates the likelihood of the data generated by the fist network to be
    real or "fake". They compete because both learn: one learns how to better generate
    "fake" data, and the other learns how to distinguish if the data it is presented
    with is real or not. They iterate on every epoch until they both converge. That
    is the point when the network that evaluates generated data cannot distinguish
    between "fake" and real data any longer.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: GANs拥有一个生成新数据（即“假”数据）的网络和一个评估由第一个网络生成的数据是否真实的网络。它们相互竞争，因为它们都在学习：一个学习如何更好地生成“假”数据，另一个则学习如何区分数据是否为真实。它们在每一个迭代周期中不断优化，直到两者都收敛。此时，评估生成数据的网络无法再区分“假”数据和真实数据。
- en: GANs have been successfully used in fields where data has a clear topological
    structure. Its original implementation used a GAN to create synthetic images of
    objects, people's faces, and animals that were similar to real images of those
    things. This domain of image creation is where GANs are used the most frequently,
    but applications in other domains occasionally appear in research papers.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: GANs已成功应用于数据具有明确拓扑结构的领域。其最初的实现是使用GAN生成与真实图像相似的物体、人物面孔和动物的合成图像。图像生成是GAN应用最为广泛的领域，但在其他领域的应用也偶尔出现在研究论文中。
- en: '![](img/a256fdab-16a0-4f18-b2ab-907ddbecd12b.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a256fdab-16a0-4f18-b2ab-907ddbecd12b.png)'
- en: 'Figure 3: Image that shows the result of different GAN algorithms in changing
    people''s faces based on a given emotion. Source: StarGAN Project. Available at
    [https://github.com/yunjey/StarGAN](https://github.com/yunjey/StarGAN).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：展示不同GAN算法在根据给定情绪变化人物面孔的结果。来源：StarGAN项目。可访问：[https://github.com/yunjey/StarGAN](https://github.com/yunjey/StarGAN)。
- en: Deep Reinforcement Learning
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习
- en: The original DRL architecture was championed by DeepMind, a Google-owned artificial
    intelligence research organization based in the UK.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的深度强化学习（DRL）架构由Google旗下的人工智能研究机构DeepMind提出，该机构位于英国。
- en: The key idea of DRL networks is that they are unsupervised in nature and that
    they learn from trial-and-error, only optimizing for a reward function. That is,
    different than other networks (which use supervised approaches to optimize for
    how wrong the predictions are, compared to what is known to be right), DRL networks
    do not know of a correct way of approaching a problem. They are simply given the
    rules of a system and are then rewarded every time they perform a function correctly.
    This process, which takes a very large number of iterations, eventually trains
    networks to excel in a number of tasks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: DRL 网络的关键思想是它们本质上是无监督的，并且通过试错学习，只优化奖励函数。也就是说，与其他使用监督学习方法来优化预测错误（与已知正确答案相比）的网络不同，DRL
    网络并不知道处理问题的正确方式。它们只是被给定系统规则，并且每当它们正确执行一个任务时，就会获得奖励。这个过程需要大量的迭代，最终训练网络在多个任务中表现出色。
- en: 'For more information refer, Human-level control through deep reinforcement
    learning, by Volodymyr Mnih et al., February 2015, Nature. Available at: [https://storage.googleapis.com/deepmind-media/dqn/
    DQNNaturePaper.pdf](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多信息，请参见，Volodymyr Mnih 等人的《通过深度强化学习实现人类级别控制》，2015年2月，发表于《自然》杂志。可在以下地址获取：[https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)。
- en: 'Deep Reinforcement Learning models gained popularity after DeepMind created
    AlphaGo, a system that plays the game Go better than professional players. DeepMind
    also created DRL networks that learn how to play video games at a superhuman level,
    entirely on their own:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）模型在 DeepMind 创建 AlphaGo 后获得了广泛关注，AlphaGo 是一个在围棋游戏中超越职业选手的系统。DeepMind
    还创建了能够自学并以超人类水平玩视频游戏的 DRL 网络：
- en: '![](img/42cf84e8-ec2b-454b-9198-5e47b6bb8c13.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/42cf84e8-ec2b-454b-9198-5e47b6bb8c13.png)'
- en: 'Figure 4: Image that represents how the DQN algorithm works'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：表示 DQN 算法如何工作的图像
- en: 'For more information refer, DQN was created by DeepMind to beat Atari games.
    The algorithm uses a deep reinforcement learning solution to continuously increase
    its reward. Image source: [https://keon.io/ deep-q-learning/](https://keon.io/deep-q-learning/).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多信息，请参见，DQN 是 DeepMind 为了击败 Atari 游戏而创建的。该算法使用深度强化学习解决方案，不断提高其奖励。图片来源：[https://keon.io/deep-q-learning/](https://keon.io/deep-q-learning/)。
- en: '| **Architecture**  | **Data Structure** | **Successful Applications**  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| **架构** | **数据结构** | **成功应用** |'
- en: '| Convolutional neural networks (CNNs)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '| 卷积神经网络（CNNs）'
- en: '| Grid-like topological structure (that is, images)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '| 网格状拓扑结构（即，图像）'
- en: '| Image recognition and classification  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 图像识别和分类 |'
- en: '| Recurrent neural network (RNN) and long-short term memory (LSTM) networks  |
    Sequential data (that is, time-series data)  | Speech recognition, text generation,
    and translation  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 循环神经网络（RNN）和长短期记忆（LSTM）网络 | 序列数据（即，时间序列数据） | 语音识别、文本生成与翻译 |'
- en: '| Generative adversarial networks (GANs)  | Grid-like topological structure (that
    is, images)  | Image generation  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 生成对抗网络（GANs） | 网格状拓扑结构（即，图像） | 图像生成 |'
- en: '| Deep reinforcement learning (DRL)  | System with clear rules and a clearly
    defined reward function  | Playing video games and selfdriving vehicles  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 深度强化学习（DRL） | 具有明确规则和清晰定义奖励函数的系统 | 玩视频游戏和自动驾驶车辆 |'
- en: 'Table 1: Different neural network architectures have shown success in different
    filelds. The networks'' architecture is typically related to the structure of
    the problem at hand.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：不同的神经网络架构在不同领域取得了成功。网络的架构通常与当前问题的结构相关。
- en: Data Normalization
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据归一化
- en: 'Before building a deep learning model, one more step is necessary: data normalization.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建深度学习模型之前，还有一步是必须的：数据归一化。
- en: Data normalization is a common practice in machine learning systems. Particularly
    regarding neural networks, researchers have proposed that normalization is an
    essential technique for training RNNs (and LSTMs), mainly because it decreases
    the network's training time and increases the network's overall performance.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 数据归一化是机器学习系统中的常见做法。尤其在神经网络中，研究人员提出归一化是训练 RNN（和 LSTM）的一个关键技术，主要是因为它能减少网络的训练时间，并提高网络的整体性能。
- en: 'For more information refer, *Batch Normalization: Accelerating Deep Network Training
    by Reducing Internal Covariate Shift* by Sergey Ioffe et. al., arXiv,March 2015\.
    Available at: [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参考 Sergey Ioffe 等人在 arXiv 上发布的《批量归一化：通过减少内部协变量偏移加速深度网络训练》，2015年3月，网址：[https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)。
- en: Deciding on a normalization technique varies, depending on the data and the
    problem at hand. The following techniques are commonly used.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 决定采用哪种归一化技术取决于数据和具体问题。以下是常用的几种技术。
- en: Z-score
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Z-分数
- en: When data is normally distributed (that is, Gaussian), one can compute the distance
    between each observation as a standard deviation from its mean.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据呈正态分布（即高斯分布）时，可以计算每个观测值与其均值之间的标准差距离。
- en: 'This normalization is useful when identifying how distant data points are from
    more likely occurrences in the distribution. The Z-score is defiled by:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当识别数据点与分布中更可能发生的事件之间的距离时，这种归一化非常有用。Z-分数的定义为：
- en: '![](img/dea0c619-724b-4811-89d7-7b94ad031f1f.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dea0c619-724b-4811-89d7-7b94ad031f1f.png)'
- en: Here, ![](img/71b9e4fd-3854-4202-8792-5872dca200f2.png) is the ![](img/88fdc3b7-222c-40df-90fd-19de4ef61ac3.png)observation,
    ![](img/64f15854-2c09-4dc7-8ae2-66cbba67e794.png)the mean, and ![](img/e519bc15-4a32-4535-ad1a-f4803d95c3ea.png)the
    standard deviation of the series.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/71b9e4fd-3854-4202-8792-5872dca200f2.png) 是 ![](img/88fdc3b7-222c-40df-90fd-19de4ef61ac3.png)
    的观测值，![](img/64f15854-2c09-4dc7-8ae2-66cbba67e794.png) 是均值，![](img/e519bc15-4a32-4535-ad1a-f4803d95c3ea.png)
    是该序列的标准差。
- en: 'For more information refer, Standard score article (Z-score). Wikipedia.Available
    at: [https://en.wikipedia.org/wiki/Standard_score](https://en.wikipedia.org/wiki/Standard_score).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参考标准分数文章（Z-分数）。维基百科，网址：[https://en.wikipedia.org/wiki/Standard_score](https://en.wikipedia.org/wiki/Standard_score)。
- en: Point-Relative Normalization
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 点相对归一化
- en: 'This normalization computes the difference of a given observation in relation
    to the fist observation of the series. This kind of normalization is useful to
    identify trends in relation to a starting point. The point-relative normalization
    is defined by:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这种归一化计算给定观测值与序列第一个观测值之间的差异。这种归一化有助于识别相对于起始点的趋势。点相对归一化的定义为：
- en: '![](img/77bd28e6-aa6b-44ad-94b6-d4a342a0d074.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77bd28e6-aa6b-44ad-94b6-d4a342a0d074.png)'
- en: Here, ![](img/9d1a43fa-2b2e-4f4c-8b23-362390dae545.png)is the ![](img/ad642e62-6059-4768-b872-19f70caa4a88.png)observation
    and![](img/77b76bcb-3fbd-43e7-8c5d-ed149d7e7afb.png) is the fist observation of
    the series.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/9d1a43fa-2b2e-4f4c-8b23-362390dae545.png) 是 ![](img/ad642e62-6059-4768-b872-19f70caa4a88.png)
    的观测值，![](img/77b76bcb-3fbd-43e7-8c5d-ed149d7e7afb.png) 是该序列的第一个观测值。
- en: As suggested by Siraj Raval in his video, *How to Predict Stock Prices Easily
    Intro to Deep Learning* *#7*, available on YouTube at: [https://www.youtube.com/watch?v=ftMq5ps503w](https://www.youtube.com/watch?v=ftMq5ps503w).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 Siraj Raval 在他的视频《如何轻松预测股票价格 深度学习入门》第 *7* 集中所建议的，视频可在 YouTube 上观看：[https://www.youtube.com/watch?v=ftMq5ps503w](https://www.youtube.com/watch?v=ftMq5ps503w)。
- en: Maximum and Minimum Normalization
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大值和最小值归一化
- en: This normalization computes the distance between a given observation and the
    maximum and minimum values of the series. This normalization is useful when working
    with series in which the maximum and minimum values are not outliers and are important
    for future predictions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这种归一化计算给定观测值与序列的最大值和最小值之间的距离。当处理的序列中的最大值和最小值不是异常值并且对未来预测有重要作用时，这种归一化非常有用。
- en: 'This normalization technique can be applied with:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这种归一化技术可以应用于：
- en: '![](img/553451e1-0d79-4ddb-a484-4501ea4298dd.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/553451e1-0d79-4ddb-a484-4501ea4298dd.png)'
- en: Here, ![](img/0d60a1c3-779b-4517-a09f-5fc81d3f41d0.png)is the ![](img/4c8b2958-c7c3-4b0e-a6e9-88d3aeee18ca.png)observation,
    *O* represents a vector with all O values, and the functions min (O) and max (O)
    represent the minimum and maximum values of the series, respectively.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/0d60a1c3-779b-4517-a09f-5fc81d3f41d0.png) 是 ![](img/4c8b2958-c7c3-4b0e-a6e9-88d3aeee18ca.png)
    的观测值，*O* 代表包含所有 O 值的向量，min (O) 和 max (O) 函数分别表示该序列的最小值和最大值。
- en: During next Activity, Exploring the Bitcoin Dataset and Preparing Data for Model,
    we will prepare available Bitcoin data to be used in our LSTM mode. That includes
    selecting variables of interest, selecting a relevant period, and applying the
    preceding point-relative normalization technique.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个活动中，我们将探索比特币数据集并为模型准备数据。我们将准备可用的比特币数据以供 LSTM 模型使用。这包括选择感兴趣的变量，选择相关的时间段，并应用前述的点相对归一化技术。
- en: Structuring Your Problem
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化你的问题
- en: Compared to researchers, practitioners spend much less time determining which
    architecture to choose when starting a new deep learning project. Acquiring data
    that represents a given problem correctly is the most important factor to consider
    when developing these systems, followed by the understanding of the datasets inherent
    biases and limitations.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 与研究人员相比，实践者在开始一个新的深度学习项目时，花在确定选择哪种架构上的时间要少得多。获取能够正确代表给定问题的数据是开发这些系统时需要考虑的最重要因素，其次是理解数据集固有的偏差和局限性。
- en: 'When starting to develop a deep learning system, consider the following questions
    for reflection:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始开发深度学习系统时，考虑以下反思问题：
- en: '**Do I have the right data?** This is the hardest challenge when training a
    deep learning model. First, define your problem with mathematical rules. Use precise
    definitions and organize the problem in either categories (classification problems)
    or a continuous scale (regression problems). Now, how can you collect data about
    those metrics?'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我有正确的数据吗？** 这是训练深度学习模型时最难的挑战。首先，用数学规则定义你的问题。使用精确的定义，并将问题组织为类别（分类问题）或连续尺度（回归问题）。现在，如何收集关于这些度量的数据呢？'
- en: '**Do I have enough data?** Typically, deep learning algorithms have shown to
    perform much better in large datasets than in smaller ones. Knowing how much data
    is necessary to train a high-performance algorithm depends on the kind of problem
    you are trying to address, but aim to collect as much data as you can.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我有足够的数据吗？** 通常，深度学习算法在大数据集上的表现明显优于在小数据集上的表现。知道训练一个高性能算法需要多少数据，取决于你试图解决的问题类型，但尽量收集尽可能多的数据。'
- en: '**Can I use a pre-trained model?** If you are working on a problem that is
    a subset of a more general application—but within the same domain—consider using
    a pretrained model. Pretrained models can give you a head start on tackling the
    specific patterns of your problem, instead of the more general characteristics
    of the domain at large. A good place to start is the official TensorFlow repository 
    ([https://github.com/tensorflow/models](https://github.com/tensorflow/models)).'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我可以使用预训练模型吗？** 如果你正在处理的问题是更一般应用的一个子集——但在同一领域内——考虑使用预训练模型。预训练模型可以帮助你在解决问题时，专注于问题的具体模式，而不是领域的更一般特征。一个好的起点是官方的
    TensorFlow 仓库（[https://github.com/tensorflow/models](https://github.com/tensorflow/models)）。'
- en: '![](img/40b2f22f-501b-4608-b702-668971502661.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40b2f22f-501b-4608-b702-668971502661.png)'
- en: 'Figure 5: Decision-tree of key reflection questions to be made at the beginning
    of a deep learning project'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：深度学习项目开始时需要考虑的关键反思问题的决策树
- en: In certain circumstances, data may simply not be available. Depending on the
    case, it may be possible to use a series of techniques to effectively create more
    data from your input data. This process is known as **data augmentation** and
    has successful application when working with image recognition problems.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，数据可能根本无法获得。根据具体情况，可以使用一系列技术有效地从输入数据中生成更多数据。这个过程称为**数据增强**，在处理图像识别问题时具有成功的应用。
- en: A good reference is the article Classifying plankton with deep neural networks
    available at [http://benanne.github.io/2015/03/17/plankton.html](http://benanne.github.io/2015/03/17/plankton.html).
    The authors show a series of techniques for augmenting a small set of image data
    in order to increase the number of training samples the model has.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的参考是文章《使用深度神经网络分类浮游生物》，可以在[http://benanne.github.io/2015/03/17/plankton.html](http://benanne.github.io/2015/03/17/plankton.html)找到。作者展示了一系列技术，用于增强一小组图像数据，以增加模型的训练样本数量。
- en: Activity:Exploring the Bitcoin Dataset and Preparing Data for Model
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动：探索比特币数据集并为模型准备数据
- en: We will be using a public dataset originally retrieved from CoinMarketCap, a
    popular website that tracks different cryptocurrency statistics. The dataset has
    been provided alongside this chapter and will be used.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个公开的数据集，该数据集最初来自于 CoinMarketCap，一个追踪不同加密货币统计信息的流行网站。数据集已与本章一起提供，将会被使用。
- en: We will be exploring the dataset using Jupyter Notebooks. Jupyter Notebooks
    provide Python sessions via a web-browser that allows you to work with data interactively.
    They are a popular tool for exploring datasets. They will be used in activities
    throughout this book.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Jupyter Notebooks 探索数据集。Jupyter Notebooks 提供通过网页浏览器访问的 Python 会话，使你可以互动地处理数据。它们是探索数据集的流行工具，在本书的活动中会被使用。
- en: 'Using your terminal, navigate to the directory `Chapter_5/activity_3` and execute
    the following command to start a Jupyter Notebook instance:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用终端，导航到目录 `Chapter_5/activity_3` 并执行以下命令启动 Jupyter Notebook 实例：
- en: '[PRE0]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, open the URL provided by the application in your browser. You should be
    able to see a Jupyter Notebook page with a number of directories from your file
    system. You should see the following output:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在浏览器中打开应用程序提供的网址。你应该能够看到一个 Jupyter Notebook 页面，页面上显示了文件系统中的多个目录。你应该能看到以下输出：
- en: '![](img/6f2a7e42-7770-466a-b1bd-4d1232d1d6e9.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f2a7e42-7770-466a-b1bd-4d1232d1d6e9.png)'
- en: 'Figure 6: Terminal image after starting a Jupyter Notebook instance. Navigate
    to the URL show in a browser, and you should be able to see the Jupyter Notebook
    landing page.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：启动 Jupyter Notebook 实例后的终端图像。导航到浏览器中显示的网址，你应该能够看到 Jupyter Notebook 的登录页面。
- en: Now, navigate to the directories and click on the file Activity `Exploring_Bitcoin_
    Dataset.ipynb`. This is a Jupyter Notebook file that will be opened in a new browser
    tab. The application will automatically start a new Python interactive session
    for you.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，导航到目录并点击文件 Activity `Exploring_Bitcoin_ Dataset.ipynb`。这是一个 Jupyter Notebook
    文件，它将会在新的浏览器标签中打开。应用程序将自动为你启动一个新的 Python 交互式会话。
- en: '![](img/d520e1b2-f642-4877-9885-3a6d2bf9ab1b.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d520e1b2-f642-4877-9885-3a6d2bf9ab1b.png)'
- en: 'Figure 7: Landing page of your Jupyter Notebook instance'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：你的 Jupyter Notebook 实例的登录页面
- en: '![](img/0eeb900e-4592-4edd-b97b-2f3516e4b4c9.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0eeb900e-4592-4edd-b97b-2f3516e4b4c9.png)'
- en: 'Figure 8: Image of the Notebook Activity_Exploring_Bitcoin_Dataset.ipynb. You
    can now interact with that Notebook and make modifications.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：Notebook Activity_Exploring_Bitcoin_Dataset.ipynb 的图片。你现在可以与该 Notebook 进行交互并进行修改。
- en: After opening our Jupyter Notebook, let's now explore the Bitcoin data made
    available with this chapter.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 打开我们的 Jupyter Notebook 后，让我们现在探索本章提供的比特币数据。
- en: 'The dataset `data/bitcoin_historical_prices.csv` contains measurements of Bitcoin
    prices since early 2013\. The most recent observation is on November 2017—the
    dataset comes from `CoinMarketCap`, an online service that is updated daily. It
    contains eight variables, two of which (date and week) describe a time period
    of the data—these can be used as indices—and six others (`open`, `high`, `low`,
    `close`, `volume`, and `market_ capitalization`) that can be used to understand
    how the price and value of Bitcoin has changed over time:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 `data/bitcoin_historical_prices.csv` 包含自 2013 年初以来的比特币价格数据。最新的观测数据为 2017
    年 11 月——该数据集来自 `CoinMarketCap`，这是一个每天更新的在线服务。数据集包含八个变量，其中两个（日期和周数）描述了数据的时间周期——这些可以用作索引——另外六个（`open`、`high`、`low`、`close`、`volume`
    和 `market_capitalization`）则可以用来理解比特币价格和价值是如何随时间变化的：
- en: '| **Variable**  | **Description**  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| **变量**  | **描述**  |'
- en: '| `date`  | Date of the observation.  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| `date`  | 观察的日期。  |'
- en: '| `iso_week`  | Week number for a given year.  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| `iso_week`  | 给定年份的周数。  |'
- en: '| `open`  | Open value for a single Bitcoin coin.  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| `open`  | 单个比特币的开盘值。  |'
- en: '| `high`  | Highest value achieved during a given day period.  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| `high`  | 给定日期期间内的最高值。  |'
- en: '| `low`  | Lowest value achieved during a given day period.  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| `low`  | 给定日期期间内的最低值。  |'
- en: '| `close`  | Value at the close of the transaction day.  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| `close`  | 交易日结束时的值。  |'
- en: '| `volume`  | The total volume of Bitcoin that was exchanged during that day.  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| `volume`  | 当天交易的比特币总量。  |'
- en: '| `market_capitalization`  | Market capitalization, which is explained by Market
    Cap = Price *Circulating Supply.  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| `market_capitalization`  | 市值，计算公式为 市值 = 价格 * 流通供应量。  |'
- en: 'Table 2: Available variables (that is, columns) in the Bitcoin historical prices
    dataset'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：比特币历史价格数据集中可用的变量（即列）
- en: 'Using the open Jupyter Notebook instance, let''s now explore the time-series
    of two of those variables: `close` and `volume`. We will start with those time-series
    to explore price-fluctuation patterns.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用打开的 Jupyter Notebook 实例，现在让我们探索这两个变量的时间序列：`close` 和 `volume`。我们将从这些时间序列开始，探索价格波动模式。
- en: Navigate to the open instance of the Jupyter Notebook Activity `Exploring_Bitcoin_
    Dataset.ipynb`. Now, execute all cells under the header Introduction. This will
    import the required libraries and import the dataset into memory.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到已打开的 Jupyter Notebook 实例 Activity `Exploring_Bitcoin_ Dataset.ipynb`。现在，执行标题为
    Introduction 下的所有单元格。这将导入所需的库并将数据集导入内存。
- en: After the dataset has been imported into memory, move to the Exploration section.
    You will find a snippet of code that generates a time-series plot for the close
    variable. Can you generate the same plot for the `volume` variable?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集导入内存后，转到探索部分。你会看到一段生成 `close` 变量时间序列图的代码片段。你能为 `volume` 变量生成相同的图吗？
- en: '![](img/f68fa8a7-9123-44f9-93fd-afbe5ac1a1b3.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f68fa8a7-9123-44f9-93fd-afbe5ac1a1b3.png)'
- en: 'Figure 9: Time-series plot of the closing price for Bitcoin from the close
    variable. Reproduce this plot,but using the volume variable in a new cell below
    this one.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：比特币收盘价的时间序列图，数据来自 `close` 变量。请在下方的新单元格中重现该图，但使用 `volume` 变量。
- en: You will have most certainly noticed that both variables surge in 2017\. This
    reflects the current phenomenon that both the prices and value of Bitcoin have
    been continuously growing since the beginning of that year.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你一定已经注意到，2017年这两个变量都有大幅上升。这反映了一个当前现象，即比特币的价格和价值自2017年初以来持续增长。
- en: '![](img/6d7278be-375b-4533-b4b7-22f1cd2daf27.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d7278be-375b-4533-b4b7-22f1cd2daf27.png)'
- en: 'Figure 10: Closing price of Bitcoin coins in USD. Notice an early spike by
    late 2013 and early 2014\. Also, notice how the recent prices have skyrocketed
    since the beginning of 2017.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：比特币收盘价（以美元计）。注意到2013年底和2014年初的早期价格飙升。同时，也可以注意到自2017年初以来，最近的价格已经飙升。
- en: '![](img/c239d72f-ae1c-4050-b983-c461063fde59.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c239d72f-ae1c-4050-b983-c461063fde59.png)'
- en: 'Figure 11: The volume of transactions of Bitcoin coins (in USD) shows that
    starting in 2017, a trend starts in which a significantly larger amount of Bitcoin
    is being transacted in the market. The total daily volume varies much more than
    daily closing prices.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：比特币交易量（以美元计）显示，从2017年开始，市场上的比特币交易量显著增加。与每日收盘价相比，总体交易量的波动性要大得多。
- en: Also, we notice that for many years, Bitcoin prices did not fluctuate as much
    as in recent years. While those periods can be used by a neural network to understand
    certain patterns, we will be excluding older observations, given that we are interested
    in predicting future prices for not-too-distant periods. Let's filter the data
    for 2016 and 2017 only.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还注意到，很多年前，比特币的价格波动不如近几年那样剧烈。虽然这些早期的周期可以被神经网络用来理解某些模式，但考虑到我们关注的是预测未来不远的价格，我们将排除较旧的观察数据。让我们只筛选2016年和2017年的数据。
- en: 'Navigate to the Preparing Dataset for Model section. We will use the pandas
    API for filtering the data for the years 2016 and 2017\. Pandas provides an intuitive
    API for performing this operation:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到“准备数据集以供模型使用”部分。我们将使用 pandas API 筛选2016年和2017年的数据。Pandas 提供了一个直观的 API 来执行此操作：
- en: '[PRE1]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The variable `bitcoin_recent` now has a copy of our original bitcoin dataset,
    but filtered to the observations that are newer or equal to January 1, 2016.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 变量 `bitcoin_recent` 现在包含了我们原始比特币数据集的一个副本，但仅包含 2016年1月1日或之后的观测数据。
- en: As our final step, we now normalize our data using the point-relative normalization
    technique described in the *Data Normalization* section. We will only normalize
    two variables (close and volume), because those are the variables that we are
    working to predict.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，我们现在使用 *数据归一化* 部分描述的点相对归一化技术来归一化我们的数据。我们只归一化两个变量（close 和 volume），因为这两个是我们要预测的变量。
- en: In the same directory containing this chapter, we have placed a script called
    `normalizations.py`. That script contains the three normalization techniques described
    in this chapter. We import that script into our Jupyter Notebook and apply the
    functions to our series.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在包含本章节的同一目录下，我们放置了一个名为 `normalizations.py` 的脚本。该脚本包含了本章节中描述的三种归一化技术。我们将该脚本导入到
    Jupyter Notebook 中，并将函数应用于我们的系列。
- en: 'Navigate to the Preparing Dataset for Model section. Now, use the `iso_week`
    variable to group all the day observations from a given week using the pandas
    method `groupby()` . We can now apply the normalization function `normalizations.point_relative_
    normalization()` directly to the series within that week. We store the output
    of that normalization as a new variable in the same pandas dataframe using:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到“准备数据集以供模型使用”部分。现在，使用 `iso_week` 变量按周对所有日期观察进行分组，使用 pandas 方法 `groupby()`。然后，我们可以直接对该周内的系列应用归一化函数
    `normalizations.point_relative_normalization()`。我们将该归一化的输出存储为同一个 pandas 数据框中的新变量，方法如下：
- en: '[PRE2]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The variable `close_point_relative_normalization` now contains the normalized
    data for the variable `close`. Do the same with the variable `volume`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，变量 `close_point_relative_normalization` 包含了 `close` 变量的归一化数据。请对 `volume`
    变量执行相同的操作：
- en: '![](img/d8daa1c0-9449-464b-abeb-05250c0ac72c.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8daa1c0-9449-464b-abeb-05250c0ac72c.png)'
- en: 'Figure 12: Image of Jupyter Notebook focusing on section where the normalization
    function is applied.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：Jupyter Notebook的图像，聚焦于应用标准化函数的部分。
- en: The normalized close variable contains an interesting variance pattern every
    week. We will be using that variable to train our LSTM model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化的收盘变量包含每周有趣的方差模式。我们将使用该变量来训练我们的LSTM模型。
- en: '![](img/8412f325-783f-478b-8862-adc14683b416.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8412f325-783f-478b-8862-adc14683b416.png)'
- en: 'Figure 13: Plot that displays the series from the normalized variable close_point_relative_normalization.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：显示来自标准化变量close_point_relative_normalization的系列图。
- en: 'In order to evaluate how well our model performs, we need to test its accuracy
    versus some other data. We do that by creating two datasets: a training set and
    a test set. In this activity, we will use 80 percent of the dataset to train our
    LSTM model and 20 percent to evaluate its performance.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的模型表现如何，我们需要将其准确性与其他数据进行对比。我们通过创建两个数据集来实现这一点：一个训练集和一个测试集。在本次活动中，我们将使用80%的数据集来训练我们的LSTM模型，剩下的20%用于评估其表现。
- en: 'Given that the data is continuous and in the form of a time series, we use
    the last 20 percent of available weeks as a test set and the fist 80 percent as
    a training set:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于数据是连续的，并且以时间序列的形式存在，我们使用最后20%的可用周作为测试集，前80%作为训练集：
- en: '![](img/ad7ee5d6-370e-49b2-bad4-3871d4146d5b.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad7ee5d6-370e-49b2-bad4-3871d4146d5b.png)'
- en: 'Figure 14: Using weeks to create a training and a test set'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：使用周数创建训练集和测试集
- en: 'Finally, navigate to the Storing Output section and save the filtered variable
    to disk, as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，导航到“存储输出”部分，并将过滤后的变量保存到磁盘，如下所示：
- en: '[PRE3]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this section, we explored the Bitcoin dataset and prepared it for a deep
    learning model.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探索了比特币数据集，并为深度学习模型做好了准备。
- en: We learned that during the year 2017, the prices of Bitcoin skyrocketed. This
    phenomenon takes a long time to take place—and may be influenced by a number of
    external factors that this data alone doesn't explain (for instance, the emergence
    of other cryptocurrencies). We also used the point-relative normalization technique
    to process the Bitcoin dataset in weekly chunks. We do this to train an LSTM network
    to learn the weekly patterns of Bitcoin price changes so that it can predict a
    full week into the future. However, Bitcoin statistics show significant fluctuations
    on a weekly basis. Can we predict the price of Bitcoin in the future?
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在2017年，比特币的价格暴涨。这一现象需要较长时间才能发生——并且可能受到许多外部因素的影响，这些因素是仅凭这些数据无法解释的（例如，其他加密货币的出现）。我们还使用了点相对标准化技术来处理比特币数据集，并按周进行划分。我们这样做是为了训练LSTM网络学习比特币价格变化的每周模式，以便它可以预测未来一整周的价格。然而，比特币的统计数据显示其每周波动很大。我们能预测比特币未来的价格吗？
- en: What will those prices be seven days from now? We will be building a deep learning
    model to explore that question in our next section using Keras.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，七天后的价格将是多少？我们将在下一节中使用Keras构建一个深度学习模型来探索这个问题。
- en: Using Keras as a TensorFlow Interface
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras作为TensorFlow接口
- en: This section focuses on Keras. We are using Keras because it simplifies the
    TensorFlow interface into general abstractions. In the backend, the computations
    are still performed in TensorFlow—and the graph is still built using TensorFlow
    components—but the interface is much simpler. We spend less time worrying about
    individual components, such as variables and operations, and spend more time building
    the network as a computational unit. Keras makes it easy to experiment with different
    architectures and hyperparameters, moving more quickly towards a performant solution.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重点介绍Keras。我们使用Keras是因为它将TensorFlow的接口简化为通用抽象。在后台，计算仍然是在TensorFlow中进行的——图形仍然是使用TensorFlow组件构建的——但接口要简单得多。我们减少了对单独组件（如变量和操作）的关注，更多地关注构建网络作为一个计算单元。Keras使得实验不同的架构和超参数变得更加容易，从而更快地朝着高效的解决方案迈进。
- en: As of TensorFlow 1.4.0 (November 2017), Keras is now officially distributed
    with TensorFlow as `tf.keras`. This suggests that Keras is now tightly integrated
    with TensorFlow and that it will likely continue to be developed as an open source
    tool for a long period of time.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 从TensorFlow 1.4.0（2017年11月）开始，Keras现在正式与TensorFlow一起分发为`tf.keras`。这表明Keras现在与TensorFlow紧密集成，且它很可能会继续作为开源工具开发很长一段时间。
- en: Model Components
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型组件
- en: As we have seen in C*hapter 4*, *Introduction to Neural Networks and Deep Learning*,
    LSTM networks also have input, hidden, and output layers. Each hidden layer has
    an activation function which evaluates that layer's associated weights and biases.
    As expected, the network moves data sequentially from one layer to another and
    evaluates the results by the output at every iteration (that is, an epoch).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在《神经网络与深度学习简介》第4章中所看到的，LSTM网络也有输入、隐藏和输出层。每个隐藏层都有一个激活函数，用于评估该层的相关权重和偏差。如预期的那样，网络按顺序从一层传递数据到另一层，并通过每次迭代的输出来评估结果（即一个epoch）。
- en: 'Keras provides intuitive classes that represent each one of those components:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供直观的类来表示每一个这些组件：
- en: '| **Component**  | **Keras Class**  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| **组件**  | **Keras类**  |'
- en: '| High-level abstraction of a complete sequential neural network.  | `keras.models.Sequential()`  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 完整顺序神经网络的高级抽象。  | `keras.models.Sequential()`  |'
- en: '| Dense, fully-connected layer.  | `keras.layers.core.Dense()`  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 密集连接层。  | `keras.layers.core.Dense()`  |'
- en: '| Activation function.  | `keras.layers.core.Activation()`  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 激活函数。  | `keras.layers.core.Activation()`  |'
- en: '| LSTM recurrent neural network. This class contains components that are exclusive
    to this architecture, most of which are abstracted by Keras.  | `keras.layers.recurrent.LSTM()`  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| LSTM循环神经网络。这个类包含了专属于这个架构的组件，其中大部分被Keras抽象化。  | `keras.layers.recurrent.LSTM()`  |'
- en: 'Table 3: Description of key components from the Keras API. We will be using
    these components to build a deep learning model.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：Keras API中关键组件的描述。我们将使用这些组件来构建深度学习模型。
- en: Keras' `keras.models.Sequential()` component represents a whole sequential neural
    network. That Python class can be instantiated on its own, then have other components
    added to it subsequently.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的`keras.models.Sequential()`组件代表了一个完整的顺序神经网络。该Python类可以独立实例化，然后随后添加其他组件。
- en: 'We are interested in building an LSTM network because those networks perform
    well with sequential data—and time-series is a kind of sequential data. Using
    Keras, the complete LSTM network would be implemented as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对构建LSTM网络感兴趣，因为这些网络在处理顺序数据时表现良好——而时间序列是顺序数据的一种。使用Keras，完整的LSTM网络实现如下所示：
- en: '[PRE4]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Snippet 1*: LSTM implementation using Keras'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*Snippet 1*：使用Keras的LSTM实现'
- en: This implementation will be further optimized in *Chapter 6*, *Model Evaluation
    and Optimization*.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现将在《模型评估与优化》第6章中进一步优化。
- en: 'Keras abstraction allows for one to focus on the key elements that make a deep
    learning system more performant: what the right sequence of components is, how
    many layers and nodes to include, and which activation function to use. All of
    these choices are determined by either the order in which components are added
    to the instantiated `keras.models. Sequential()` class or by parameters passed
    to each component instantiation (that is, `Activation("linear")` ). The final
    model`.compile()` step builds the neural network using TensorFlow components.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的抽象化允许专注于使深度学习系统更高性能的关键元素：正确的组件序列是什么，包括多少层和节点，以及使用哪种激活函数。所有这些选择都由将组件添加到实例化的`keras.models.Sequential()`类的顺序或通过传递给每个组件实例化的参数（即`Activation("linear")`）确定。最终的`.compile()`步骤使用TensorFlow组件构建神经网络。
- en: 'After the network is built, we train our network using the `model.fit()` method.
    This will yield a trained model that can be used to make predictions:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 构建网络后，我们使用`model.fit()`方法来训练我们的网络。这将产生一个经过训练的模型，可以用来进行预测：
- en: '[PRE5]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Snippet* 2.1: Usage of `model.fit()`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*Snippet* 2.1：使用`model.fit()`的示例'
- en: The variables `X_train` and `Y_train` are, respectively, a set used for training
    and a smaller set used for evaluating the loss function (that is, testing how
    well the network predicts data).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 变量`X_train`和`Y_train`分别用于训练的一组数据和用于评估损失函数的较小数据集（即测试网络预测数据的效果）。
- en: 'Finally, we can make predictions using the `model.predict()` method:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用`model.predict()`方法进行预测：
- en: '[PRE6]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*Snippet* 2.2: Usage of `model.predict()`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*Snippet* 2.2：使用`model.predict()`的示例'
- en: 'The previous steps cover the Keras paradigm for working with neural networks.
    Despite the fact that different architectures can be dealt with in very different
    ways, Keras simplifies the interface for working with different architectures
    by using three components - network architecture, fit, and predict:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的步骤涵盖了Keras在处理神经网络时的范式。尽管不同的架构可以以非常不同的方式处理，但Keras通过使用三个组件——网络架构、拟合和预测，简化了处理不同架构的接口：
- en: '![](img/b5e7c37a-bb2b-4af7-91f9-5ad1c648a012.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5e7c37a-bb2b-4af7-91f9-5ad1c648a012.png)'
- en: 'Figure 15: The Keras neural network paradigm: A. design a neural network architecture,
    B. Train a neural network (or Fit), and C. Make predictions'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：Keras 神经网络范式：A. 设计神经网络架构，B. 训练神经网络（或拟合），C. 做出预测
- en: Keras allows for much greater control within each of those steps. However, its
    focus is to make it as easy as possible for users to create neural networks in
    as little time as possible. That means that we can start with a simple model,
    then add complexity to each one of the steps above to make that initial model
    perform better.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 在每个步骤中都允许更大的控制。然而，它的重点是尽可能简单地帮助用户在最短的时间内创建神经网络。这意味着我们可以从一个简单的模型开始，然后在上述每个步骤中添加复杂性，使初始模型的性能更好。
- en: We will take advantage of that paradigm during our upcoming activity and chapters.
    In the next activity, we will create the simplest LSTM network possible. Then,
    in C*hapter 6*, *Model Evaluation and Optimization*, we will continuously evaluate
    and alter that network to make it more robust and performant.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的活动和章节中利用这个范式。在下一个活动中，我们将创建最简单的 LSTM 网络。然后，在 *第六章*，*模型评估与优化*，我们将不断评估并修改该网络，使其更加强大和高效。
- en: Activity:Creating a TensorFlow Model Using Keras
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动：使用 Keras 创建 TensorFlow 模型
- en: In this activity, we will create an LSTM model using Keras.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将使用 Keras 创建一个 LSTM 模型。
- en: Keras serves as an interface for lower-level programs; in this case, TensorFlow.
    When we use Keras to design our neural network, that neural network is *compiled*
    as a TensorFlow computation graph.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 作为一个接口连接低层次的程序；在这个例子中是 TensorFlow。当我们使用 Keras 设计神经网络时，该神经网络会被 *编译* 为一个
    TensorFlow 计算图。
- en: 'Navigate to the open instance of the Jupyter Notebook `Activity_4_Creating_a_
    TensorFlow_Model_Using_Keras.ipynb`. Now, execute all cells under the header **Building
    a Model**. In that section, we build our fist LSTM model parametrizing two values:
    the input size of the training observation (1 equivalent for a single day) and
    the output size for the predicted period—in our case, seven days:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到打开的 Jupyter Notebook 实例 `Activity_4_Creating_a_ TensorFlow_Model_Using_Keras.ipynb`。现在，执行
    **构建模型** 下的所有单元格。在该部分，我们构建了第一个 LSTM 模型，设置了两个参数：训练观察值的输入大小（对于单个日期等同为 1）和预测期的输出大小——在我们的案例中是七天：
- en: '![](img/3cc954a0-193b-4adb-9fa2-fc4ccf80e5fb.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3cc954a0-193b-4adb-9fa2-fc4ccf80e5fb.png)'
- en: Use the Jupyter Notebook `Activity_4_Creating_a_TensorFlow_Model_Using_Keras.ipynb`
    to build the same model from the *Model Components* section, parametrizing the
    period length of input and of output to allow for experimentation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Jupyter Notebook `Activity_4_Creating_a_TensorFlow_Model_Using_Keras.ipynb`
    来构建与 *模型组件* 部分相同的模型，设置输入和输出的周期长度，以便进行实验。
- en: After the model is compiled, we proceed to storing it as an `h5 file` on disk.
    It is a good practice to store versions of your model on disk occasionally, so
    that you keep a version of the model architecture alongside its predictive capabilities.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型编译完成后，我们继续将其存储为磁盘上的 `h5 文件`。定期将模型版本存储到磁盘上是一个好习惯，这样你就可以将模型架构与其预测能力一起保存在硬盘上。
- en: 'Still on the same Jupyter Notebook, navigate to the header **Saving Model**.
    In that section, we will store the model as a file on disk with the following
    command:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然在同一个 Jupyter Notebook 中，导航到 **保存模型** 头部。在该部分，我们将使用以下命令将模型存储为磁盘上的文件：
- en: '[PRE7]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The model ''`bitcoin_lstm_v0.h5`'' hasn''t been trained yet. When saving a
    model without prior training, one effectively only saves the architecture of the
    model. That same model can later be loaded by using Keras'' `load_model()` function,
    as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 '`bitcoin_lstm_v0.h5`' 还没有经过训练。当在没有先前训练的情况下保存模型时，实际上只保存了模型的架构。该模型稍后可以通过 Keras
    的 `load_model()` 函数加载，如下所示：
- en: '[PRE8]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You may encounter the following warning when loading the Keras library: Using
    TensorFlow backend. Keras can be configured to use an other backend instead of
    TensorFlow (that is, Theano). In order to avoid this message, you can create a
    file called `keras.json` and configure its backend there. The correct configuration
    of that file depends on your system. Hence, it is recommended that you visit Keras''
    official documentation on the topic at [https://keras.io/backend/](https://keras.io/backend/)[.](https://keras.io/backend/)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当加载Keras库时，您可能会遇到以下警告：使用TensorFlow后端。Keras可以配置为使用其他后端而不是TensorFlow（即Theano）。为了避免此消息，您可以创建一个名为`keras.json`的文件，并在其中配置其后端。该文件的正确配置取决于您的系统。因此，建议您访问Keras官方文档，了解相关主题：[https://keras.io/backend/](https://keras.io/backend/)[.](https://keras.io/backend/)
- en: In this section, we have learned how to build a deep learning model using Keras,
    an interface for TensorFlow. We studied core components from Keras and used those
    components to build the fist version of our Bitcoin price-predicting system based
    on an LSTM model.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用Keras（TensorFlow的接口）构建深度学习模型。我们研究了Keras的核心组件，并使用这些组件基于LSTM模型构建了第一个比特币价格预测系统的版本。
- en: In our next section, we will discuss how to put all the components from this
    chapter together into a (nearly complete) deep learning system. That system will
    yield our very fist predictions, serving as a starting point for future improvements.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们接下来的章节中，我们将讨论如何将本章中的所有组件整合到一个（几乎完整的）深度学习系统中。该系统将产生我们第一次的预测，作为未来改进的起点。
- en: From Data Preparation to Modeling
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据准备到建模
- en: This section focuses on the implementation aspects of a Deep Learning system.
    We will use the Bitcoin data from, *Choosing the Right Model Architecture* and
    the Keras knowledge from, *Using Keras as a TensorFlow Interface* to put both
    of these components together. This section concludes the chapter by building a
    system that reads data from a disk and feeds it into a model as a single piece
    of software.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 本节专注于深度学习系统的实现方面。我们将使用在*选择正确的模型架构*一节中的比特币数据，以及在*使用Keras作为TensorFlow接口*一节中的Keras知识，将这两个组件结合起来。本节通过构建一个系统来结束本章，该系统从磁盘读取数据，并将其作为一个整体输入模型。
- en: Training a Neural Network
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: 'Neural networks can take long periods of time to train. Many factors affect
    how long that process may take. Among them, three factors are commonly considered
    the most important:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的训练可能需要较长时间。许多因素会影响该过程需要的时间。其中，有三个因素通常被认为是最重要的：
- en: The network's architecture
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络架构
- en: How many layers and neurons the network has
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络有多少层和神经元
- en: How much data there is to be used in the training process
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练过程的数据量
- en: Other factors may also greatly impact how long a network takes to train, but
    most of the optimization that a neural network can have when addressing a business
    problem comes from exploring those three.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 其他因素也可能大大影响网络的训练时间，但神经网络在解决业务问题时的优化大多数来源于探索这三点。
- en: 'We will be using the normalized data from our previous section. Recall that
    we have stored the training data in a file called `train_dataset.csv`. We will
    load that dataset into memory using pandas for easy exploration:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用上一节中的归一化数据。回想一下，我们已经将训练数据存储在一个名为`train_dataset.csv`的文件中。我们将使用pandas将该数据集加载到内存中，以便进行简便的探索：
- en: '[PRE9]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/a32c3f11-9d76-4325-8350-37b8eefd48eb.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a32c3f11-9d76-4325-8350-37b8eefd48eb.png)'
- en: 'Figure 17: Table showing the first five rows of the training dataset loaded
    from the `train_d–ataset.csv` file'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：显示从`train_dataset.csv`文件加载的训练数据集的前五行的表格
- en: We will be using the series from the variable `close_point_relative_normalization`,
    which is a normalized series of the Bitcoin closing prices—from the variable close—since
    the beginning of 2016.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自变量`close_point_relative_normalization`的系列数据，这是一组归一化的比特币收盘价序列（来自变量close），自2016年初以来的数据。
- en: The variable `close_point_relative_normalization` has been normalized on a weekly
    basis. Each observation from the week's period is made relative to the difference
    from the closing prices on the fist day of the period. This normalization step
    is important and will help our network train faster.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 变量`close_point_relative_normalization`是基于每周归一化的。每个观测值都相对于该周期间第一天的收盘价差异进行归一化。这个归一化步骤很重要，它将帮助我们的网络更快地训练。
- en: '![](img/b3c97bde-f62a-43c2-8f94-5377459c1fb9.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b3c97bde-f62a-43c2-8f94-5377459c1fb9.png)'
- en: 'Figure 18: Plot that displays the series from the normalized variable close_point_relative_normalization.
    This variable will be used to train our LSTM model.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：展示从归一化变量 close_point_relative_normalization 中绘制的时间序列图。这个变量将用于训练我们的 LSTM
    模型。
- en: Reshaping Time-Series Data
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整时间序列数据
- en: Neural networks typically work with vectors and tensors, both mathematical objects
    that organize data in a number of dimensions. Each neural network implemented
    in Keras will have either a vector or a tensor that is organized according to
    a specification as input. At fist, understanding how to reshape the data into
    the format expected by a given layer can be confusing. To avoid confusion, it
    is advised to start with a network with as little components as possible, then
    add components gradually. Keras' official documentation (under the section **Layers**)
    is essential for learning about the requirements for each kind of layer.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通常处理向量和张量，这些都是组织数据的数学对象，它们在多个维度上组织数据。在 Keras 中实现的每个神经网络都会根据规格组织一个向量或张量作为输入。一开始，理解如何将数据调整为给定层所期望的格式可能会令人困惑。为了避免混淆，建议从一个尽可能简单的网络开始，然后逐步添加组件。Keras
    的官方文档（在**层**部分）对学习每种层的要求非常重要。
- en: The Keras official documentation is available at [https://keras.io/ layers/core/](https://keras.io/layers/core/).
    That link takes you directly to the Layers section.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 官方文档可以在 [https://keras.io/layers/core/](https://keras.io/layers/core/)
    获取。该链接直接将您带到 Layers 部分。
- en: '`NumPy` is a popular Python library used for performing numerical computations.
    It is used by the deep learning community to manipulate vectors and tensors and
    prepare them for deep learning systems. In particular, the `numpy.reshape()` method
    is very important when adapting data for deep learning models. That model allows
    for the manipulation of NumPy arrays, which are Python objects analogous to vectors
    and tensors.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`NumPy` 是一个流行的 Python 库，用于执行数值计算。深度学习社区使用它来操作向量和张量，并将它们为深度学习系统做准备。特别是，`numpy.reshape()`
    方法在调整数据以适应深度学习模型时非常重要。该方法允许操作 NumPy 数组，这些数组是 Python 对象，类似于向量和张量。'
- en: We now organize the prices from the variable `close_point_relative_normalization`
    using the weeks of both 2016 and 2017\. We create distinct groups containing seven
    observations each (one for each day of the week) for a total of 77 complete weeks.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用 2016 和 2017 年的周数据来组织来自 `close_point_relative_normalization` 变量的价格。我们将数据分成不同的组，每组包含七个观察值（每个周的一天），总共有
    77 个完整的周。
- en: We do that because we are interested in predicting the prices of a week's worth
    of trading.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这样做是因为我们对预测一周的交易价格感兴趣。
- en: We use the ISO standard to determine the beginning and the end of a week. Other
    kinds of organizations are entirely possible. This one is simple and intuitive
    to follow, but there is room for improvement.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 ISO 标准来确定一周的开始和结束时间。其他类型的组织方式也是完全可能的。这种方式简单且直观，但仍有改进的空间。
- en: 'LSTM networks work with three-dimensional tensors. Each one of those dimensions
    represents an important property for the network. These dimensions are:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 网络使用三维张量。每个维度都代表网络的一个重要属性。这些维度是：
- en: '**Period length**: The period length, that is, how many observations there
    are on a period'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**周期长度**：周期长度，即每个周期中有多少观察值'
- en: '**Number of periods**: How many periods are available in the dataset'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**周期数量**：数据集中可用的周期数量'
- en: '**Number of features**: Number of features available in the dataset'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征数量**：数据集中可用的特征数量'
- en: Our data from the variable `close_point_relative_normalization` is currently
    a one dimensional vector—we need to reshape it to match those three dimensions.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从变量 `close_point_relative_normalization` 获取的数据目前是一个一维向量——我们需要将其调整为符合这三个维度的格式。
- en: We will be using a week's period. Hence, our period length is seven days (period
    length = 7). We have 77 complete weeks available in our data. We will be using
    the very last of those weeks to test our model against during its training period.
    That leaves us with 76 distinct weeks (number of periods = 76). Finally, we will
    be using a single feature in this network (number of features = 1)—we will include
    more features in future versions.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一周的时间周期。因此，我们的周期长度是七天（周期长度 = 7）。我们的数据中有 77 个完整的周。我们将在训练期间使用最后一个周进行模型测试。这使得我们剩下
    76 个不同的周（周期数量 = 76）。最后，我们将在这个网络中使用单一的特征（特征数量 = 1）——我们将在未来的版本中加入更多的特征。
- en: 'How can we reshape the data to match those dimensions? We will be using a combination
    of base Python properties and the `reshape()` from the `numpy library`. First,
    we create the 76 distinct week groups with seven days each using pure Python:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何重塑数据以匹配这些维度？我们将使用基础 Python 属性和来自 `numpy` 库的 `reshape()` 函数的组合。首先，我们用纯 Python
    创建 76 个不同的周组，每个周组有七天：
- en: '[PRE10]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Snippet 3: Python code snippet that creates distinct week groups'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段 3：创建不同周组的 Python 代码片段
- en: 'The resulting variable data is a variable that contains all the right dimensions.
    The Keras LSTM layer expects these dimensions to be organized in a specific order:
    number of features, number of observations, and period length.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 结果变量数据是一个包含所有正确维度的变量。Keras 的 LSTM 层期望这些维度按特定顺序组织：特征数量、观测数量和周期长度。
- en: 'Let''s reshape our dataset to match that format:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重塑数据集以匹配该格式：
- en: '[PRE11]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Snippet 4: Python code snippet that creates distinct week groups'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段 4：创建不同周组的 Python 代码片段
- en: Each Keras layer will expect its input to be organized in specific ways. However,
    Keras will reshape data accordingly in most cases. Always refer to the Keras documentation
    on layers ([https://keras.io/layers/ core/](https://keras.io/layers/core/)) before
    adding a new layer or if you encounter issues with the shape layers expect.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Keras 层都会期望其输入以特定的方式组织。然而，Keras 通常会根据需要重新塑形数据。每次添加新层或遇到维度问题时，都应参考 Keras 层的文档
    ([https://keras.io/layers/core/](https://keras.io/layers/core/))。
- en: '*Snippet 4* also selects the very last week of our set as a validation set
    (`via data[-1]` ). We will be attempting to predict the very last week in our
    dataset by using the preceding 76 weeks. The next step is to use those variables
    to fit our model:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码片段 4* 还将我们数据集中的最后一周选择为验证集（`via data[-1]`）。我们将尝试使用前 76 周的数据预测数据集中的最后一周。接下来的步骤是使用这些变量来拟合我们的模型：'
- en: '[PRE12]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '*Snippet 5*: Snippet shows how to train our model'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码片段 5*：展示如何训练我们的模型'
- en: 'LSTMs are computationally expensive models. They may take up to file minutes
    to train with our dataset in a modern computer. Most of that time is spent at
    the beginning of the computation, when the algorithm creates the full computation
    graph. The process gains speed after it starts training:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 是计算开销较大的模型。在现代计算机上，训练我们的数据集可能需要几分钟时间。大部分时间都花费在计算的开始阶段，当时算法创建完整的计算图。训练开始后，速度会逐渐提升：
- en: '![](img/01635e04-55ec-49fd-9de8-911f20ea04f7.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01635e04-55ec-49fd-9de8-911f20ea04f7.png)'
- en: 'Figure 19: Graph that shows the results of the loss function evaluated at each
    epoch'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19：显示在每个 epoch 评估的损失函数结果的图形
- en: This compares what the model predicted at each epoch, then compares with the
    real data using a technique called mean-squared error. This plot shows those results.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这比较了模型在每个 epoch 中的预测结果，然后使用均方误差技术与实际数据进行比较。此图显示了这些结果。
- en: 'At a glance, our network seems to perform very well: it starts with a very
    small error rate that continuously decreases. Now, what do our predictions tell
    us?'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一目了然，我们的网络表现得非常好：它从一个非常小的误差率开始，并持续减少。那么，我们的预测结果告诉我们什么呢？
- en: Making Predictions
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行预测
- en: After our network has been trained, we can now proceed to making predictions.
    We will be making predictions for a future week beyond our time period.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的网络训练完成后，我们现在可以进行预测。我们将对超出时间范围的未来一周进行预测。
- en: 'Once we have trained our model with model.fit(), making predictions is trivial:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们通过 `model.fit()` 训练了我们的模型，做出预测就变得非常简单：
- en: '[PRE13]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '*Snippet 6*: Making a prediction using the same data that we previously used
    for training'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码片段 6*：使用之前用于训练的相同数据进行预测'
- en: We use the same data for making predictions as the data used for training (`the
    X_train variable`). If we have more data available, we can use that instead—given
    that we reshape it to the format the LSTM requires.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与训练数据相同的数据进行预测（即 `X_train` 变量）。如果我们有更多数据可用，我们可以使用这些数据，只要我们将其重新塑形为 LSTM 所要求的格式。
- en: Overfitting
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过拟合
- en: 'When a neural network overfits to a validation set, it means that it learns
    patterns present in the training set, but is unable to generalize it to unseen
    data (for instance, the test set). During our next chapter, we will learn how
    to avoid over-fitting and create a system for both evaluating our network and
    increasing its performance:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经网络在验证集上发生过拟合时，意味着它学会了训练集中存在的模式，但无法将其推广到未见过的数据（例如，测试集）。在下一章中，我们将学习如何避免过拟合，并创建一个系统来评估我们的网络并提升其性能：
- en: '![](img/f4b1d78c-54be-448e-aa90-9d0cb95d865a.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4b1d78c-54be-448e-aa90-9d0cb95d865a.png)'
- en: 'Figure 20: After de-normalization, our LSTM model predicted that in late July
    2017, the prices of Bitcoin would increase from $2,200 to roughly $2,800, a 30
    percent increase in a single week'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图20：去归一化后，我们的LSTM模型预测2017年7月底，比特币的价格将从2200美元上涨到约2800美元，单周上涨30％
- en: Activity:Assembling a Deep Learning System
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动：组装深度学习系统
- en: 'In this activity, we bring together all the essential pieces for building a
    basic deep learning system: data, model, and prediction.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将所有构建基础深度学习系统的关键元素汇集在一起：数据、模型和预测。
- en: We will continue to use Jupyter Notebooks, and will use the data prepared in
    previous exercises (`data/train_dataset.csv`) as well as the model that we stored
    locally (`bitcoin_ lstm_v0.h5`).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用Jupyter Notebooks，并将使用之前练习中准备的数据（`data/train_dataset.csv`）以及我们本地存储的模型（`bitcoin_lstm_v0.h5`）。
- en: 'After starting a Jupyter Notebook instance, navigate to the Notebook called
    `Activity_5_Assembling_a_Deep_Learning_System.ipynb` and open it. Execute the
    cells from the header to load the required components and then navigate to the
    header **Shaping Data**:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Jupyter Notebook实例后，导航到名为`Activity_5_Assembling_a_Deep_Learning_System.ipynb`的Notebook并打开它。从标题开始执行单元格以加载所需的组件，然后导航到标题**数据整形**：
- en: '![](img/bc02ce10-84f8-40b3-886a-1720b68227ec.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc02ce10-84f8-40b3-886a-1720b68227ec.png)'
- en: 'Figure 21: Plot that displays the series from the normalized variable close_point_relative_normalization'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图21：展示归一化变量`close_point_relative_normalization`的时间序列图
- en: The `close_point_relative_normalization` variable will be used to train our
    LSTM model.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`close_point_relative_normalization`变量将用于训练我们的LSTM模型。'
- en: We will start by loading the dataset we prepared during our previous activities.
    We use pandas to load that dataset into memory.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从加载我们在之前活动中准备的数据集开始。我们使用pandas将该数据集加载到内存中。
- en: 'Load the training dataset into memory using pandas, as follows:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用pandas将训练数据集加载到内存中，如下所示：
- en: '[PRE14]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, quickly inspect the dataset by executing the following command:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，通过执行以下命令，快速检查数据集：
- en: '[PRE15]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As explained in this chapter, LSTM networks require tensors with three dimensions.
    These dimensions are: period length, number of periods, and number of features.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本章所解释的，LSTM网络需要三维张量。这些维度是：周期长度、周期数和特征数。
- en: Now, proceed to creating weekly groups, then rearrange the resulting array to
    match those dimensions.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，继续创建每周的分组，然后重新排列生成的数组以匹配这些维度。
- en: 'Feel free to use the provided function `create_groups()` to perform this operation:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随时使用提供的`create_groups()`函数来执行此操作：
- en: '[PRE16]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The default values for that function are 7 days. What would happen if you changed
    that number to a different value, for instance, 10?
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的默认值为7天。如果你将该数字更改为其他值，比如10，会发生什么呢？
- en: 'Now, make sure to the data into two sets: training and validation. We do this
    by assigning the last week from the Bitcoin prices dataset to the evaluation set.
    We then train the network to evaluate that last week.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，确保将数据分成两个集合：训练集和验证集。我们通过将比特币价格数据集的最后一周分配到评估集来实现这一点。然后，我们训练网络来评估这一最后一周的数据。
- en: 'Separate the last week of the training data and reshape it using `numpy.reshape()`
    . Reshaping is important, as the LSTM model only accepts data organized in this
    way:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 将训练数据的最后一周分离出来，并使用`numpy.reshape()`进行重塑。重塑非常重要，因为LSTM模型只接受这种格式的数据：
- en: '[PRE17]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Our data is now ready to be used in training. Now we load our previously saved
    model and train it with a given number of epochs.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据现在已经准备好用于训练。现在我们加载之前保存的模型，并用给定的epochs数量训练它。
- en: 'Navigate to the header **Load Our Model** and load our previously trained model:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到标题**加载我们的模型**，并加载我们之前训练的模型：
- en: '[PRE18]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And now, train that model with our training data `X_train` and `Y_validation`:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，用我们的训练数据`X_train`和`Y_validation`来训练该模型：
- en: '[PRE19]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Notice that we store the logs of the model in a variable called history. The
    model logs are useful for exploring specific variations in its training accuracy
    and to understand how well the loss function is performing:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将模型的日志存储在名为history的变量中。这些日志对于探索模型训练准确率的具体变化，以及理解损失函数的表现非常有用：
- en: '![](img/019c6080-5184-4600-a077-cc5315e19925.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/019c6080-5184-4600-a077-cc5315e19925.png)'
- en: 'Figure 22: Section of Jupyter Notebook where we load our earlier model and
    train it with new data'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图22：Jupyter Notebook部分，我们加载了之前的模型，并用新数据训练它
- en: Finally, let's make a prediction with our trained model.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们用训练好的模型进行预测。
- en: 'Using the same `data X_train`, call the following method:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用相同的`data X_train`，调用以下方法：
- en: '[PRE20]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The model immediately returns a list of normalized values with the prediction
    for the next seven days. Use the `denormalize()` function to turn the data into
    US Dollar values. Use the latest values available as a reference for scaling the
    predicted results:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型立即返回一个标准化值的列表，包含未来七天的预测数据。使用 `denormalize()` 函数将数据转化为美元值。请使用最新的可用值作为参考来调整预测结果：
- en: '[PRE21]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](img/020c46f7-38af-4919-a8b8-d7ce2ede143a.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](img/020c46f7-38af-4919-a8b8-d7ce2ede143a.png)'
- en: 'Figure 23: Section of Jupyter Notebook where we predict the prices of Bitcoin
    for the next seven days.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23：Jupyter Notebook 部分，展示了我们预测未来七天比特币价格的过程。
- en: Our predictions suggest a great price surge of about 30 percent.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的预测表明，比特币价格可能会大约上涨 30%。
- en: '![](img/b15b2b04-3e9c-4349-8c52-f6b4a8660e64.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b15b2b04-3e9c-4349-8c52-f6b4a8660e64.png)'
- en: 'Figure 24: Projection of Bitcoin prices for seven days in the future using
    the LSTM model we just built'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图 24：使用我们刚刚建立的 LSTM 模型预测未来七天的比特币价格走势。
- en: 'We combine both time-series in this graph: the real data (before the line)
    and the predicted data (after the line). The model shows variance similar to the
    patterns seen before and it suggests a price increase during the following seven
    days period.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这张图中结合了两个时间序列：真实数据（线之前）和预测数据（线之后）。该模型展示的方差与之前看到的模式类似，并且它暗示未来七天内可能会有价格上涨。
- en: 'After you are done experimenting, save your model with the following command:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成实验后，使用以下命令保存你的模型：
- en: '[PRE22]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We will save this trained network for future reference and compare its performance
    with other models.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保存这个训练好的网络以供将来参考，并与其他模型的表现进行比较。
- en: The network may have learned patterns from our data, but how can it do that
    with such a simple architecture and so little data? LSTMs are powerful tools for
    learning patterns from data. However, we will learn in our next sessions that
    they can also suffer from *overfitting*, a phenomenon common in neural networks
    in which they learn patterns from the training data that are useless when predicting
    real-world patterns. We will learn how to deal with that and how to improve our
    network to make useful predictions.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 网络可能已经从我们的数据中学习到了一些模式，但它是如何在如此简单的架构和如此少的数据下做到这一点的呢？LSTM 是一种从数据中学习模式的强大工具。然而，我们将在接下来的课程中学习到，它们也可能会遭遇
    *过拟合* 问题，这是神经网络中常见的现象，其中模型学习到了训练数据中的一些模式，但这些模式在预测现实世界的数据时并没有什么用处。我们将学习如何处理这一问题，并改进我们的网络以进行有用的预测。
- en: Summary
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we have assembled a complete deep learning system: from data
    to prediction. The model created in this activity needs a number of improvements
    before it can be considered useful. However, it serves as a great starting point
    from which we will continuously improve.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经组装了一个完整的深度学习系统：从数据到预测。此活动中创建的模型需要进行多次改进才能算得上有用。然而，它为我们提供了一个很好的起点，之后我们将不断改进。
- en: Our next chapter will explore techniques for measuring the performance of our
    model and will continue to make modifications until we reach a model that is both
    useful and robust.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一章将探索评估模型性能的技术，并继续进行修改，直到我们得到一个既有用又健壮的模型。
