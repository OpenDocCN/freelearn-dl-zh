- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Machine Learning Part 3 – Transformers and Large Language Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习第三部分——变换器与大语言模型
- en: In this chapter, we will cover the currently best-performing techniques in **natural
    language processing** (**NLP**) – **transformers** and **pretrained models**.
    We will discuss the concepts behind transformers and include examples of using
    transformers and **large language models** (**LLMs**) for text classification.
    The code for this chapter will be based on the TensorFlow/Keras Python libraries
    and the cloud services provided by OpenAI.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍当前表现最好的技术——**自然语言处理**（**NLP**）——**变换器**和**预训练模型**。我们将讨论变换器的概念，并提供使用变换器和**大语言模型**（**LLMs**）进行文本分类的示例。本章的代码将基于TensorFlow/Keras
    Python库以及OpenAI提供的云服务。
- en: The topics covered in this chapter are important because although transformers
    and LLMs are only a few years old, they have become state-of-the-art for many
    different types of NLP applications. In fact, LLM systems such as ChatGPT have
    been widely covered in the press and you have undoubtedly encountered references
    to them. You have probably even used their online interfaces. In this chapter,
    you will learn how to work with the technology behind these systems, which should
    be part of the toolkit of every NLP developer.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所讨论的主题非常重要，因为尽管变换器和大语言模型（LLMs）只有几年历史，但它们已经成为许多不同类型NLP应用的最前沿技术。事实上，像ChatGPT这样的LLM系统已被广泛报道，您无疑已经看到过它们的相关信息。您甚至可能已经使用过它们的在线接口。在本章中，您将学习如何使用这些系统背后的技术，这应该是每个NLP开发者工具箱的一部分。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Overview of transformers and large language models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器和大语言模型概述
- en: '**Bidirectional Encoder Representations from Transformers** (**BERT**) and
    its variants'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双向编码器表示从变换器**（**BERT**）及其变体'
- en: Using BERT – a classification example
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用BERT——一个分类示例
- en: Cloud-based LLMs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于云的大语言模型（LLMs）
- en: We’ll start by listing the technical resources that we’ll use to run the examples
    in this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先列出本章示例运行所需的技术资源。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The code that we will go over in this chapter makes use of a number of open
    source software libraries and resources. We have used many of these in earlier
    chapters, but we will list them here for convenience:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍的代码使用了多个开源软件库和资源。我们在前几章中已经使用了其中的许多，但为了方便起见，我们在这里列出它们：
- en: 'The Tensorflow machine learning libraries: `hub`, `text`, and `tf-models`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow机器学习库：`hub`、`text` 和 `tf-models`
- en: The Python numerical package, NumPy
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python数值计算包，NumPy
- en: The Matplotlib plotting and graphical package
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib绘图和图形包
- en: The IMDb movie reviews dataset
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IMDb电影评论数据集
- en: scikit-learn’s `sklearn.model_selection` to do the training, validation, and
    test split
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn的`sklearn.model_selection`用于进行训练、验证和测试数据集划分
- en: 'A BERT model from TensorFlow Hub: we’re using this one –`''small_bert/bert_en_uncased_L-4_H-512_A-8''`
    – but you can use any other BERT model you like, bearing in mind that larger models
    might take a long time to train'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自TensorFlow Hub的BERT模型：我们使用的是这个——`'small_bert/bert_en_uncased_L-4_H-512_A-8'`——但您可以使用任何其他BERT模型，只需注意较大的模型可能需要更长的训练时间
- en: Note that we have kept the models relatively small here so that they don’t require
    an especially powerful computer. The examples in this chapter were tested on a
    Windows 10 machine with an Intel 3.4 GHz CPU and 16 GB of RAM, without a separate
    GPU. Of course, more computing resources will speed up your training runs and
    enable you to use larger models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在这里使用的模型相对较小，因此不需要特别强大的计算机。 本章中的示例是在一台配备Intel 3.4 GHz CPU和16 GB内存、没有独立GPU的Windows
    10机器上测试的。当然，更多的计算资源将加速您的训练过程，并使您能够使用更大的模型。
- en: The next section provides a brief description of the transformer and LLM technology
    that we’ll be using.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节简要介绍了我们将使用的变换器和大语言模型（LLM）技术。
- en: Overview of transformers and LLMs
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器和大语言模型概述
- en: Transformers and LLMs are currently the best-performing technologies for **natural
    language understanding** (**NLU**). This does not mean that the approaches covered
    in earlier chapters are obsolete. Depending on the requirements of a specific
    NLP project, some of the simpler approaches may be more practical or cost-effective.
    In this chapter, you will get information about the more recent approaches that
    you can use to make that decision.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，变换器和大型语言模型（LLM）是**自然语言理解（NLU）**领域表现最好的技术。这并不意味着早期章节中介绍的方法已经过时。根据特定NLP项目的需求，某些简单的方法可能更实用或更具成本效益。在本章中，你将获得有关这些新方法的信息，帮助你做出决策。
- en: There is a great deal of information about the theoretical aspects of these
    techniques available on the internet, but here we will focus on applications and
    explore how these technologies can be applied to solving practical NLU problems.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些技术的理论方面的信息在互联网上有大量的资料，但在这里我们将专注于应用，探讨这些技术如何应用于解决实际的自然语言理解（NLU）问题。
- en: As we saw in [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184), **recurrent neural
    networks** (**RNNs**) have been a very effective approach in NLP because they
    don’t assume that the elements of input, specifically words, are independent,
    and so are able to take into account sequences of input elements such as the order
    of words in sentences. As we have seen, RNNs keep the memory of earlier inputs
    by using previous outputs as inputs to later layers. However, with RNNs, the effect
    of earlier inputs on the current input diminishes quickly as processing proceeds
    through the sequence.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第10章*](B19005_10.xhtml#_idTextAnchor184)中看到的，**递归神经网络（RNN）**在自然语言处理（NLP）中是一种非常有效的方法，因为它们不假设输入元素，特别是词语，是独立的，因此能够考虑输入元素的顺序，例如句子中词语的顺序。正如我们所看到的，RNN通过使用前面的输出作为后续层的输入，保持早期输入的记忆。然而，对于RNN，随着处理通过序列进行，早期输入对当前输入的影响迅速减弱。
- en: When longer documents are processed, because of the context-dependent nature
    of natural language, even very distant parts of the text can have a strong effect
    on the current input. In fact, in some cases, distant inputs can be more important
    than more recent parts of the input. But when the data is a long sequence, processing
    with an RNN means that the earlier information will not be able to have much impact
    on the later processing. Some initial attempts to address this issue include **long
    short-term memory** (**LSTM**), which allows the processor to maintain the state
    and includes forget gates, and **gated recurrent units** (**GRUs**), a new and
    relatively fast type of LSTM, which we will not cover in this book. Instead, we
    will focus on more recent approaches such as attention and transformers.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理较长文档时，由于自然语言的上下文依赖特性，即使是文本中很远的部分，也可能对当前输入产生强烈的影响。实际上，在某些情况下，远距离的输入可能比较近期的输入更为重要。但是，当数据是一个长序列时，使用递归神经网络（RNN）进行处理意味着较早的信息在处理过程中对后续处理的影响会迅速减弱。为了解决这个问题，最初的一些尝试包括**长短期记忆（LSTM）**，它允许处理器保持状态并包括遗忘门，以及**门控循环单元（GRU）**，这是一种新的且相对较快的LSTM类型，但我们在本书中不会讨论它们。相反，我们将重点讨论更近期的方法，如注意力机制和变换器。
- en: Introducing attention
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍注意力机制
- en: '**Attention** is a technique that allows a network to learn where to pay attention
    to the input.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意力机制**是一种技术，它使得网络可以学习在哪里关注输入。'
- en: Initially, attention was used primarily in machine translation. The processing
    was based on an encoder-decoder architecture where a sentence was first encoded
    into a vector and then decoded into the translation. In the original encoder-decoder
    idea, each input sentence was encoded into a fixed-length vector. It turned out
    that it was difficult to encode all of the information in a sentence into a fixed-length
    vector, especially a long sentence. This is because more distant words that were
    outside of the scope of the fixed-length vector were not able to influence the
    result.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，注意力机制主要应用于机器翻译。该处理基于编码器-解码器架构，首先将句子编码为向量，然后解码为翻译。在原始的编码器-解码器模型中，每个输入句子被编码为固定长度的向量。结果发现，将句子中的所有信息编码成固定长度的向量是困难的，尤其是长句子。这是因为，固定长度向量无法影响远距离的词语，这些词语超出了固定长度向量的范围。
- en: Encoding the sentence into a *set* of vectors, one per word, removed this limitation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 将句子编码为一组向量，每个词一个，消除了这一限制。
- en: As one of the early papers on attention states, “*The most important distinguishing
    feature of this approach from the basic encoder-decoder is that it does not attempt
    to encode a whole input sentence into a single fixed-length vector. Instead, it
    encodes the input sentence into a sequence of vectors and chooses a subset of
    these vectors adaptively while decoding the translation. This frees a neural translation
    model from having to squash all the information of a source sentence, regardless
    of its length, into a fixed-length vector.*” (Bahdanau, D., Cho, K., & Bengio,
    Y. (2014). *Neural machine translation by jointly learning to align and translate*.
    arXiv preprint arXiv:1409.0473.)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正如早期关于注意力的论文所述，“*这种方法与基本的编码器-解码器模型的最重要区别在于，它不试图将整个输入句子编码成一个固定长度的向量。相反，它将输入句子编码成一系列向量，并在解码翻译时自适应地选择这些向量的一个子集。这使得神经翻译模型不必将源句子的所有信息，无论其长度如何，都压缩成一个固定长度的向量。*”（Bahdanau,
    D., Cho, K., & Bengio, Y. (2014). *神经机器翻译通过联合学习对齐和翻译*。arXiv预印本 arXiv:1409.0473.）
- en: For machine translation applications, it is necessary both to encode the input
    text and to decode the results into the new language in order to produce the translated
    text. In this chapter, we will simplify this task by using a classification example
    that uses just the encoding part of the attention architecture.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器翻译应用，既需要对输入文本进行编码，也需要将结果解码为新语言，以生成翻译文本。在本章中，我们将通过使用一个仅使用注意力架构中的编码部分的分类示例来简化此任务。
- en: A more recent technical development has been to demonstrate that one component
    of the attention architecture, RNNs, was not needed in order to get good results.
    This new development is called **transformers**, which we will briefly mention
    in the next section, and then illustrate with an in-depth example.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的一个技术发展是证明了注意力架构的一个组成部分，即RNNs，并不是获得良好结果的必要条件。这个新的发展被称为**变压器**，我们将在下一节简要提到，并通过深入的示例来说明。
- en: Applying attention in transformers
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在变压器中应用注意力
- en: Transformers are a development of the attention approach that dispenses with
    the RNN part of the original attention systems. Transformers were introduced in
    the 2017 paper *Attention is all you need* (Ashish Vaswani, et al., 2017\. *Attention
    is all you need*. In the Proceedings of the 31st International Conference on Neural
    Information Processing Systems (NIPS’17). Curran Associates Inc., Red Hook, NY,
    USA, 6000-6010). The paper showed that good results can be achieved just with
    attention. Nearly all research on NLP learning models is now based on transformers.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器是注意力方法的一个发展，它摒弃了原始注意力系统中的RNN部分。变压器在2017年的论文《*Attention is all you need*》（Ashish
    Vaswani 等，2017年）中被提出。（*Attention is all you need*。第31届国际神经信息处理系统会议（NIPS’17）论文集，Curran
    Associates Inc.，纽约红钩，美国，6000-6010）。该论文展示了仅使用注意力就能获得良好的结果。现在几乎所有关于NLP学习模型的研究都基于变压器。
- en: A second important technical component of the recent dramatic increases in NLP
    performance is the idea of pretraining models based on large amounts of existing
    data and making them available to NLP developers. The next section talks about
    the advantages of this approach.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在自然语言处理（NLP）性能急剧提升的第二个重要技术组成部分是基于大量现有数据进行预训练模型，并将其提供给NLP开发者的思想。下一节将讨论这种方法的优势。
- en: Leveraging existing data – LLMs or pre-trained models
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用现有数据——大型语言模型（LLMs）或预训练模型
- en: So far, in this book, we’ve created our own text representations (vectors) from
    training data. In our examples so far, all of the information that the model has
    about the language is contained in the training data, which is a very small sample
    of the full language. But if models start out with general knowledge of a language,
    they can take advantage of vast amounts of training data that would be impractical
    for a single project. This is called the **pretraining** of a model. These pretrained
    models can be reused for many projects because they capture general information
    about a language. Once a pretrained model is available, it can be fine-tuned to
    specific applications by supplying additional data
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们已经从训练数据中创建了我们自己的文本表示（向量）。在我们到目前为止的示例中，模型所拥有的关于语言的所有信息都包含在训练数据中，而这些训练数据只是完整语言的一个非常小的样本。但如果模型一开始就具备了语言的通用知识，它们就可以利用大量的训练数据，这些数据对单个项目来说是不可行的。这被称为**模型预训练**。这些预训练模型可以被多个项目重用，因为它们捕获了关于语言的通用信息。一旦预训练模型可用，就可以通过提供额外的数据对其进行微调，以适应特定的应用。
- en: The next section will introduce one of the best-known and most important pretrained
    transformer models, BERT.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将介绍一个最著名且最重要的预训练变换器模型——BERT。
- en: BERT and its variants
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT及其变体
- en: As an example of an LLM technology based on transformers, we will demonstrate
    the use of BERT, a widely used state-of-the-art system. BERT is an open source
    NLP approach developed by Google that is the foundation of today’s state-of-the-art
    NLP systems. The source code for BERT is available at [https://github.com/google-research/bert](https://github.com/google-research/bert).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作为基于变换器（transformers）的LLM技术示例，我们将演示广泛使用的最先进系统BERT的使用。BERT是由谷歌开发的一个开源自然语言处理（NLP）方法，是当今最先进NLP系统的基础。BERT的源代码可以在[https://github.com/google-research/bert](https://github.com/google-research/bert)找到。
- en: BERT’s key technical innovation is that the training is bidirectional, that
    is, taking both previous and later words in input into account. A second innovation
    is that BERT’s pretraining uses a masked language model, where the system masks
    out a word in the training data and attempts to predict it.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的关键技术创新是其训练是双向的，即考虑输入中的前后词语。第二个创新是BERT的预训练使用了掩蔽语言模型，系统会在训练数据中掩盖一个词并尝试预测它。
- en: BERT also uses only the encoder part of the encoder-decoder architecture because,
    unlike machine translation systems, it focuses only on understanding; it doesn’t
    produce language.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: BERT仅使用编码器-解码器架构中的编码器部分，因为与机器翻译系统不同，它只关注理解，而不生成语言。
- en: Another advantage of BERT, unlike the systems we’ve discussed earlier in this
    book, is that the training process is unsupervised. That is, the text that it
    is trained on does not need to be annotated or assigned any meaning by a human.
    Because it is unsupervised, the training process can take advantage of the enormous
    quantities of text available on the web, without needing to go through the expensive
    process of having humans review it and decide what it means.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的另一个优势是，与本书中之前讨论的系统不同，它的训练过程是无监督的。也就是说，它训练所用的文本不需要人工标注或赋予任何意义。由于是无监督的，训练过程可以利用网络上大量的文本数据，而无需经过人工审核和判断其含义的昂贵过程。
- en: 'The initial BERT system was published in 2018\. Since then, the ideas behind
    BERT have been explored and expanded into many different variants. The different
    variants have various features that make them appropriate for addressing different
    requirements. Some of these features include faster training times, smaller models,
    or higher accuracy. *Table 11.1* shows a few of the common BERT variants and their
    specific features. Our example will use the original BERT system since it is the
    basis of all the other BERT versions:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 初始的BERT系统于2018年发布。从那时起，BERT背后的理念被探索并扩展为许多不同的变体。这些不同的变体有各种特性，使它们适用于解决不同的需求。这些特性包括更快的训练时间、更小的模型或更高的准确性。*表11.1*展示了几个常见BERT变体及其特定特性。我们的示例将使用原始BERT系统，因为它是所有其他BERT版本的基础：
- en: '| **Acronym** | **Name** | **Date** | **Features** |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| **缩写** | **名称** | **日期** | **特性** |'
- en: '| BERT | Bidirectional Encoder Representations from Transformer | 2018 | The
    original BERT system. |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| BERT | 基于变换器的双向编码表示 | 2018 | 原始的BERT系统。 |'
- en: '| BERT-Base |  |  | A number of models released by the original BERT authors.
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| BERT-Base |  |  | 原始BERT作者发布的多个模型。 |'
- en: '| RoBERTa | Robustly Optimized BERT pre-training approach | 2019 | In this
    approach, different parts of the sentences are masked in different epochs, which
    makes it more robust to variations in the training data. |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| RoBERTa | 强化优化的BERT预训练方法 | 2019 | 在该方法中，句子的不同部分在不同的epoch中被掩码，这使得它对训练数据中的变化更具鲁棒性。
    |'
- en: '| ALBERT | A Lite BERT | 2019 | A version of BERT that shares parameters between
    layers in order to reduce the size of models. |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| ALBERT | 轻量化BERT | 2019 | 一种BERT版本，通过在层间共享参数来减少模型的大小。 |'
- en: '| DistilBERT |  | 2020 | Smaller and faster than BERT with good performance
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| DistilBERT |  | 2020 | 比BERT更小更快，且性能良好 |'
- en: '| TinyBERT |  | 2019 | Smaller and faster than BERT-Base with good performance;
    good for resource-restricted devices. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| TinyBERT |  | 2019 | 比BERT-Base更小更快，且性能良好；适用于资源受限的设备。 |'
- en: Table 11.1 – BERT variations
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.1 – BERT变体
- en: The next section will go through a hands-on example of a BERT application.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将通过一个BERT应用的动手示例进行讲解。
- en: Using BERT – a classification example
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用BERT – 一个分类示例
- en: In this example, we’ll use BERT for classification, using the movie review dataset
    we saw in earlier chapters. We will start with a pretrained BERT model and *fine-tune*
    it to classify movie reviews. This is a process that you can follow if you want
    to apply BERT to your own data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用BERT进行分类，使用我们在前几章中看到的电影评论数据集。我们将从一个预训练的BERT模型开始，并对其进行*微调*以分类电影评论。如果你想将BERT应用于自己的数据，可以按照这个过程进行。
- en: Using BERT for specific applications starts with one of the pretrained models
    available from TensorFlow Hub ([https://tfhub.dev/tensorflow](https://tfhub.dev/tensorflow))
    and then fine-tuning it with training data that is specific to the application.
    It is recommended to start with one of the small BERT models, which have the same
    architecture as BERT but are faster to train. Generally, the smaller models are
    less accurate, but if their accuracy is adequate for the application, it isn’t
    necessary to take the extra time and computer resources that would be needed to
    use a larger model. There are many models of various sizes that can be downloaded
    from TensorFlow Hub.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用BERT进行特定应用从TensorFlow Hub上提供的预训练模型之一开始（[https://tfhub.dev/tensorflow](https://tfhub.dev/tensorflow)），然后通过特定应用的训练数据进行微调。建议从小型BERT模型开始，这些模型与BERT具有相同的架构，但训练速度更快。通常，小型模型的准确性较低，但如果它们的准确性足以满足应用需求，就不必花费额外的时间和计算资源去使用更大的模型。TensorFlow
    Hub上有许多不同大小的模型可以下载。
- en: BERT models can also be cased or uncased, depending on whether they take the
    case of text into account. Uncased models will typically provide better results
    unless the application is one where the case of the text is informative, such
    as **named entity recognition** (**NER**), where proper names are important.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: BERT模型可以是有大小写处理的（cased）或无大小写处理的（uncased），具体取决于它是否考虑文本的大小写。无大小写处理的模型通常会提供更好的结果，除非应用场景是大小写信息有意义的情况，如**命名实体识别**（**NER**），在这种情况下，专有名词很重要。
- en: 'In this example, we will work with the `small_bert/bert_en_uncased_L-4_H-512_A-8/1`
    model. It has the following properties, which are encoded in its name:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用`small_bert/bert_en_uncased_L-4_H-512_A-8/1`模型。它具有以下属性，这些属性已编码在它的名称中：
- en: Small BERT
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小型BERT。
- en: Uncased
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无大小写处理。
- en: 4 hidden layers (L-4)
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4个隐藏层（L-4）。
- en: A hidden size of 512
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层大小为512。
- en: 8 attention heads (A-8)
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 8个注意力头（A-8）。
- en: This model was trained on Wikipedia and BooksCorpus. This is a very large amount
    of text, but there are many pretrained models that were trained on much larger
    amounts of text, which we will discuss later in the chapter. Indeed, an important
    trend in NLP is developing and publishing models trained on larger and larger
    amounts of text.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是在维基百科和BooksCorpus上训练的。这是一个非常庞大的文本数据集，但也有许多经过更大规模文本训练的预训练模型，我们将在本章后面讨论这些模型。事实上，NLP领域的一个重要趋势是开发并发布基于越来越大量文本训练的模型。
- en: 'The example that will be reviewed here is adapted from the TensorFlow tutorial
    for text classification with BERT. The full tutorial can be found here:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里将回顾的例子改编自TensorFlow的BERT文本分类教程。完整的教程可以在这里找到：
- en: '[https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb#scrollTo=EqL7ihkN_862](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb#scrollTo=EqL7ihkN_862'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb#scrollTo=EqL7ihkN_862](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb#scrollTo=EqL7ihkN_862)'
- en: )
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'We’ll start by installing and loading some basic libraries. We will be using
    a Jupyter notebook (you will recall that the process of setting up a Jupyter notebook
    was covered in detail in [*Chapter 4*](B19005_04.xhtml#_idTextAnchor085), and
    you can refer to [*Chapter 4*](B19005_04.xhtml#_idTextAnchor085) for additional
    details if necessary):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始安装并加载一些基本库。我们将使用Jupyter Notebook（你可能还记得，Jupyter Notebook的设置过程在[*第4章*](B19005_04.xhtml#_idTextAnchor085)中详细介绍过，必要时可以参考[*第4章*](B19005_04.xhtml#_idTextAnchor085)获取更多细节）：
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Our BERT fine-tuned model will be developed through the following steps:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的BERT微调模型将通过以下步骤进行开发：
- en: Installing data.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装数据。
- en: Splitting the data into training, validation, and testing subsets.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拆分为训练集、验证集和测试集。
- en: Loading a BERT model from TensorFlow Hub.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从TensorFlow Hub加载BERT模型。
- en: Building a model by combining BERT with a classifier.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将BERT与分类器结合来构建模型。
- en: Fine-tuning BERT to create a model.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微调BERT以创建模型。
- en: Defining the loss function and metrics.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义损失函数和评估指标。
- en: Defining the optimizer and number of training epochs.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义优化器和训练周期数。
- en: Compiling the model.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译模型。
- en: Training the model.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。
- en: Plotting the results of the training steps over the training epochs.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制训练步骤结果在训练周期中的图表。
- en: Evaluating the model with the test data.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用测试数据评估模型。
- en: Saving the model and using it to classify texts.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存模型并用其分类文本。
- en: The following sections will go over each of these steps in detail.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的章节将详细介绍每个步骤。
- en: Installing the data
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装数据
- en: 'The first step is to install the data. We will use the NLTK movie review dataset
    that we installed in [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184). We will
    use the `tf.keras.utils.text_dataset_from_directory` utility to make a TensorFlow
    dataset from the movie review directory:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是安装数据。我们将使用在[*Chapter 10*](B19005_10.xhtml#_idTextAnchor184)中安装的NLTK电影评论数据集。我们将使用`tf.keras.utils.text_dataset_from_directory`实用程序从电影评论目录创建一个TensorFlow数据集：
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: There are 2,000 files in the dataset, divided into two classes, `neg` and `pos`.
    We print the class names in the final step as a check to make sure the class names
    are as expected. These steps can be used for any dataset that is contained in
    a directory structure with examples of the different classes contained in different
    directories with the class names as directory names.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有2,000个文件，分为两类，`neg`和`pos`。我们在最后一步打印类名，以检查类名是否符合预期。这些步骤可用于任何以不同目录包含不同类示例的目录结构的数据集，其中类名作为目录名。
- en: Splitting the data into training, validation, and testing sets
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据分割为训练、验证和测试集
- en: 'The next step is to split the dataset into training, validation, and testing
    sets. As you will recall from earlier chapters, the training set is used to develop
    the model. The validation set, which is kept separate from the training set, is
    used to look at the performance of the system on data that it hasn’t been trained
    on during the training process. In our example, we will use a common split of
    80% training 10% for validation, and 10% for testing. The validation set can be
    used at the end of every training epoch, to see how training is progressing. The
    testing set is only used once, as a final evaluation:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将数据集分割为训练、验证和测试集。正如您在前面章节中记得的那样，训练集用于开发模型。验证集与训练集分开，用于查看系统在训练过程中尚未训练的数据上的性能。在我们的示例中，我们将使用常见的80%训练、10%验证和10%测试的划分。验证集可以在每个训练周期结束时使用，以查看训练进展情况。测试集仅在最后进行一次使用，作为最终评估：
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Loading the BERT model
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载BERT模型
- en: The next step is to load the BERT model we will fine-tune in this example, as
    shown in the following code block. As discussed previously, there are many BERT
    models to select from, but this model is a good choice to start with.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是加载我们将在此示例中进行微调的BERT模型，如下代码块所示。如前所述，有许多BERT模型可供选择，但这个模型是一个很好的起点。
- en: 'We will also need to provide a preprocessor to transform the text inputs into
    numeric token IDs before their input to BERT. We can use the matching preprocessor
    provided by TensorFlow for this model:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要提供一个预处理器，将文本输入转换为BERT输入之前的数字标记ID。我们可以使用TensorFlow为该模型提供的匹配预处理器：
- en: '[PRE3]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The code here specifies the model we’ll use and defines some convenience variables
    to simplify reference to the model, the encoder, and the preprocessor.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此处的代码指定我们将使用的模型，并定义了一些方便的变量，以简化对模型、编码器和预处理器的引用。
- en: Defining the model for fine-tuning
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义微调模型
- en: 'The following code defines the model we will use. We can increase the size
    of the parameter to the `Dropout` layer if desired to make the model robust to
    variations in the training data:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码定义了我们将使用的模型。如果需要，可以增加`Dropout`层参数的大小，以使模型对训练数据的变化更加稳健：
- en: '[PRE4]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In *Figure 11**.1*, we can see a visualization of the model’s layers, including
    the text input layer, the preprocessing layer, the BERT layer, the dropout layer,
    and the final classifier layer. The visualization was produced by the last line
    in the code block. This structure corresponds to the structure we defined in the
    preceding code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Figure 11**.1*中，我们可以看到模型的层次结构可视化，包括文本输入层、预处理层、BERT层、dropout层和最终分类器层。可视化是由代码块中的最后一行生成的。这个结构对应于我们在前面代码中定义的结构：
- en: '![Figure 11.1 – Visualizing the model structure](img/B19005_11_01.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 11.1 – 可视化模型结构](img/B19005_11_01.jpg)'
- en: Figure 11.1 – Visualizing the model structure
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – 可视化模型结构
- en: Sanity checks such as this visualization are useful because, with larger datasets
    and models, the training process can be very lengthy, and if the structure of
    the model is not what was intended, a lot of time can be wasted in trying to train
    an incorrect model.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 像这样的合理性检查（例如可视化）非常有用，因为对于较大的数据集和模型，训练过程可能非常漫长。如果模型的结构不是预期的，那么花费大量时间训练错误的模型是非常浪费的。
- en: Defining the loss function and metrics
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义损失函数和评估指标
- en: 'We will use a cross-entropy function for the loss function. `losses.BinaryCrossEntropy`
    loss function:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用交叉熵函数作为损失函数。`losses.BinaryCrossEntropy`损失函数：
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: A classification application with several possible outcomes, such as an intent
    identification problem where we have to decide which of 10 intents to assign to
    an input, would use categorical cross-entropy. Similarly, since this is a binary
    classification problem, the metric should be `binary accuracy`, rather than simply
    `accuracy`, which would be appropriate for a multi-class classification problem.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有多个可能结果的分类应用，例如意图识别问题，其中我们必须决定将10个意图中的哪一个分配给输入，将使用类别交叉熵。同样，由于这是一个二分类问题，评估指标应为`binary
    accuracy`，而不是简单的`accuracy`，后者适用于多类分类问题。
- en: Defining the optimizer and the number of epochs
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义优化器和训练轮数
- en: 'The optimizer improves the efficiency of the learning process. We’re using
    the popular `Adam` optimizer here, and starting it off with a very small learning
    rate (`3e-5`), which is recommended for BERT. The optimizer will dynamically adjust
    the learning rate during training:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器提高了学习过程的效率。我们在这里使用流行的`Adam`优化器，并以非常小的学习率（`3e-5`）开始，这对于BERT是推荐的。优化器将在训练过程中动态调整学习率：
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that we have selected 15 epochs of training. For the first training run,
    we’ll try to balance the goals of training on enough epochs to get an accurate
    model and wasting time training on more epochs than needed. Once we get our results
    from the first training run, we can adjust the number of epochs to balance these
    goals.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们选择了15个训练轮次。在第一次训练时，我们会尽量平衡在足够的轮次上进行训练以获得准确的模型，同时避免浪费时间训练超过所需轮次的模型。一旦得到第一次训练的结果，我们可以调整训练轮次，以平衡这两个目标。
- en: Compiling the model
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编译模型
- en: 'Using the classifier model in the call to `def build_classifier_model()`, we
    can compile the model with the loss, metrics, and optimizer, and take a look at
    the summary. It’s a good idea to check the model before starting a lengthy training
    process to make sure the model looks as expected:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用在`def build_classifier_model()`中定义的分类器模型，我们可以使用损失函数、评估指标和优化器编译模型，并查看模型摘要。在开始漫长的训练过程之前，检查模型是否符合预期是一个好主意：
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The summary of the model will look something like the following (we will only
    show a few lines because it is fairly long):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 模型摘要大概会如下所示（我们将只展示几行，因为它相当长）：
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The output here just summarizes the first two layers – input and preprocessing.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的输出只是总结了前两层——输入和预处理。
- en: The next step is training the model.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤是训练模型。
- en: Training the model
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'In the following code, we start the training process with a call to `classifier_model.fit(_)`.
    We supply this method with parameters for the training data, the validation data,
    the verbosity level, and the number of epochs (which we set earlier), as shown
    in this code:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们通过调用`classifier_model.fit(_)`开始训练过程。我们为此方法提供训练数据、验证数据、输出详细程度和训练轮次（我们之前设置的）的参数，如下所示：
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that the `classifier_model.fit()` method returns a `history` object, which
    will include information about the progress of the complete training process.
    We will use the `history` object to create plots of the training process. These
    will provide quite a bit of insight into what happened during training, and we
    will use this information to guide our next steps. We will see these plots in
    the next section.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`classifier_model.fit()`方法返回一个`history`对象，其中包含完整训练过程中进展的信息。我们将使用`history`对象来绘制训练过程图。这些图将为我们提供关于训练期间发生情况的深入了解，我们将利用这些信息指导我们的下一步行动。在下一节中，我们将看到这些图。
- en: Training times for transformer models can be quite lengthy. The time taken depends
    on the size of the dataset, the number of epochs, and the size of the model, but
    this example should probably not take more than an hour to train on a modern CPU.
    If running this example takes significantly longer than that, you may want to
    try testing with a higher verbosity level (2 is the maximum) so that you can get
    more information about what is going on in the training process.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型的训练时间可能会非常长。所需时间取决于数据集的大小、训练的轮次（epochs）以及模型的大小，但这个例子在现代 CPU 上训练应该不会超过一小时。如果运行这个例子的时间明显超过这个时间，你可以尝试使用更高的详细程度（2
    是最大值）进行测试，这样你就可以获得更多关于训练过程中发生了什么的信息。
- en: At the end of this code block, we also see the results of processing the first
    epoch of training. We can see that the first epoch of training took `189` seconds.
    The loss was `0.7` and the accuracy was `0.54`. The loss and accuracy after one
    epoch of training are not very good, but they will improve dramatically as training
    proceeds. In the next section, we will see how to show the training progress graphically.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码块的最后，我们还看到了处理第一轮训练的结果。我们可以看到第一轮训练用了`189`秒。损失为`0.7`，准确率为`0.54`。经过一轮训练后的损失和准确率并不理想，但随着训练的进行，它们会显著改善。在下一节中，我们将看到如何通过图形化方式显示训练进度。
- en: Plotting the training process
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制训练过程
- en: 'After training is complete, we will want to see how the system’s performance
    changes over training epochs. We can see this with the following code:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练完成后，我们需要查看系统的性能如何随训练轮次变化。我们可以通过以下代码来观察：
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The preceding code defines some variables and gets the relevant metrics (`binary_accuracy`
    and `loss` for the training and validation data) from the model’s `history` object.
    We are now ready to plot the progress of the training process. As usual, we will
    use Matplotlib to create our plots:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码定义了一些变量，并从模型的`history`对象中获取了相关的指标（用于训练和验证数据的`binary_accuracy`和`loss`）。现在我们已经准备好绘制训练过程的进展图了。像往常一样，我们将使用
    Matplotlib 来创建我们的图表：
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In *Figure 11**.2*, we see the plot of the decreasing loss and increasing accuracy
    over time as the model is trained. The dashed lines represent the training loss
    and accuracy, and the solid lines represent the validation loss and accuracy:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 11.2*中，我们看到随着模型训练，损失逐渐减少，准确率逐步增加的图像。虚线代表训练损失和训练准确率，实线代表验证损失和验证准确率：
- en: '![Figure 11.2 – Accuracy and loss during the training process](img/B19005_11_02.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2 – 训练过程中的准确率和损失](img/B19005_11_02.jpg)'
- en: Figure 11.2 – Accuracy and loss during the training process
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 训练过程中的准确率和损失
- en: It is most typical for the validation accuracy to be less than the training
    accuracy, and for the validation loss to be greater than the training loss, but
    this will not necessarily be the case, depending on how the data is split between
    validation and training subsets. In this example, the validation loss is uniformly
    lower than the training loss and the validation accuracy is uniformly higher than
    the training accuracy. We can see from this plot that the system isn’t changing
    after the first fourteen epochs. In fact, its performance is almost perfect.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，验证准确率会低于训练准确率，验证损失会大于训练损失，但这并不一定是必然的，具体取决于验证和训练数据子集的划分方式。在这个例子中，验证损失始终低于训练损失，而验证准确率始终高于训练准确率。我们可以从这个图中看到，系统在前十四轮训练后没有变化。实际上，它的表现几乎完美。
- en: Consequently, it is clear that there isn’t any reason to train the system after
    this point. In comparison, look at the plots around epoch `4`. We can see that
    it would not be a good idea to stop training after four epochs because loss is
    still decreasing and accuracy is still increasing. Another interesting observation
    that we can see in *Figure 11**.2* around epoch `7` is that the accuracy seems
    to decrease a bit. If we had stopped training at epoch `7`, we couldn’t tell that
    accuracy would start to increase again at epoch `8`. For that reason, it’s a good
    idea to keep training until we either see the metrics level off or start to get
    consistently worse.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，很明显在这个点之后没有必要继续训练系统。相比之下，看看在第`4`轮附近的图像。我们可以看到，如果在四轮后停止训练并不是一个好主意，因为损失仍在减少，准确率仍在增加。在*图
    11.2*中，我们可以注意到在第`7`轮附近，准确率似乎有所下降。如果我们在第`7`轮就停止训练，我们就无法知道在第`8`轮准确率会再次开始上升。因此，最好继续训练，直到我们看到指标趋于平稳或开始持续变差。
- en: Now we have a trained model, and we’d like to see how it performs on previously
    unseen data. This unseen data is the test data that we set aside during the training,
    validation, and testing split.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个训练好的模型，我们想看看它在之前未见过的数据上的表现。这些未见过的数据是我们在训练、验证和测试拆分过程中预留出的测试数据。
- en: Evaluating the model on the test data
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在测试数据上评估模型
- en: 'After the training is complete, we can see how the model performs on the test
    data. This can be seen in the following output, where we can see that the system
    is doing very well. The accuracy is nearly 100% and the loss is near zero:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练完成后，我们可以看到模型在测试数据上的表现。这可以从以下输出中看到，在这里我们可以看到系统表现得非常好。准确度接近100%，而损失值接近零：
- en: '[PRE12]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This is consistent with the system performance during training that we saw in
    *Figure 11**.2*.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在*图11.2*中看到的训练过程中系统的表现是一致的。
- en: It looks like we have a very accurate model. If we want to use it later on,
    we can save it.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们有一个非常准确的模型。如果我们以后想要使用它，可以将其保存下来。
- en: Saving the model for inference
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存模型用于推理
- en: 'The final step is to save the fine-tuned model for later use – for example,
    if the model is to be used in a production system, or if we want to use it in
    further experiments. The code for saving the model can be seen here:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的步骤是保存微调后的模型，以便以后使用——例如，如果该模型要在生产系统中使用，或者我们想在进一步的实验中使用它。保存模型的代码如下：
- en: '[PRE13]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the code here, we show both saving the model and then reloading it from the
    saved location.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里的代码中，我们展示了如何保存模型并从保存的位置重新加载它。
- en: As we saw in this section, BERT can be trained to achieve very good performance
    by fine-tuning it with a relatively small (2,000-item) dataset. This makes it
    a good choice for many practical problems. Looking back at the example of classification
    with the multi-layer perceptron in [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184),
    we saw that the accuracy (as shown in *Figure 10**.4*) was never better than about
    80% for the validation data, even after 20 epochs of training. Clearly, BERT does
    much better than that.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本节中看到的，BERT可以通过在相对较小的（2,000个条目）数据集上进行微调，从而训练出非常好的表现。这使得它成为许多实际问题的一个不错选择。回顾在[*第10章*](B19005_10.xhtml#_idTextAnchor184)中使用多层感知器进行分类的示例，我们看到即使经过20轮训练，验证数据的准确度（如*图10.4*所示）也从未超过大约80%。显然，BERT的表现要好得多。
- en: Although BERT is an excellent system, it has recently been surpassed by very
    large cloud-based pretrained LLMs. We will describe them in the next section.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然BERT是一个非常优秀的系统，但它最近已被非常大的基于云的预训练LLM所超越。我们将在下一节中描述这些模型。
- en: Cloud-based LLMs
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于云的LLM
- en: Recently, there have been a number of cloud-based pretrained large language
    models that have shown very impressive performance because they have been trained
    on very large amounts of data. In contrast to BERT, they are too large to be downloaded
    and used locally. In addition, some are closed and proprietary and can’t be downloaded
    for that reason. These newer models are based on the same principles as BERT,
    and they have shown a very impressive performance. This impressive performance
    is due to the fact that these models have been trained with much larger amounts
    of data than BERT. Because they cannot be downloaded, it is important to keep
    in mind that they aren’t appropriate for every application. Specifically, if there
    are any privacy or security concerns regarding the data, it may not be a good
    idea to send it to the cloud for processing. Some of these systems are GPT-2,
    GPT-3, GPT-4, ChatGPT, and OPT-175B, and new LLMs are being published on a frequent
    basis.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，出现了一些基于云的预训练大型语言模型，它们因为在大量数据上进行训练而展现出了非常令人印象深刻的表现。与BERT相比，它们太大，无法下载并在本地使用。此外，一些模型是封闭和专有的，因而无法下载。这些更新的模型基于与BERT相同的原理，并且表现出了非常令人印象深刻的性能。这种令人印象深刻的表现是因为这些模型在比BERT更大规模的数据上进行了训练。由于它们无法下载，重要的是要记住，这些模型并不适用于所有应用。特别是，如果数据涉及任何隐私或安全问题，将数据发送到云端进行处理可能并不是一个好主意。部分系统如GPT-2、GPT-3、GPT-4、ChatGPT和OPT-175B，且新的LLM模型也在频繁发布。
- en: The recent dramatic advances in NLP represented by these systems are made possible
    by three related technical advances. One is the development of techniques such
    as attention, which are much more able to capture relationships among words in
    texts than previous approaches such as RNNs, and which scale much better than
    the rule-based approaches that we covered in [*Chapter 8*](B19005_08.xhtml#_idTextAnchor159).
    The second factor is the availability of massive amounts of training data, primarily
    in the form of text data on the World Wide Web. The third factor is the tremendous
    increase in computer resources available for processing this data and training
    LLMs.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统所代表的NLP领域的戏剧性进展得益于三个相关的技术突破。其一是注意力机制等技术的发展，这些技术比以往的RNN等方法更能捕捉文本中单词之间的关系，且比我们在[*第8章*](B19005_08.xhtml#_idTextAnchor159)中讨论的基于规则的方法有更好的扩展性。第二个因素是大量训练数据的可用性，主要是来自万维网的文本数据。第三个因素是可用计算资源的巨大增加，这些资源可以用来处理这些数据并训练LLM。
- en: So far in the systems we’ve discussed, all of the knowledge of a language that
    goes into the creation of a model for a specific application is derived from the
    training data. The process starts without knowing anything about the language.
    LLMs, on the other hand, come with models that have been *pretrained* through
    processing very large amounts of more or less generic text, and as a consequence
    have a basic foundation of information about the language. Additional training
    data can be used for *fine-tuning* the model so that it can handle inputs that
    are specific to the application. An important aspect of fine-tuning a model for
    a specific application is to minimize the amount of new data that is needed for
    fine-tuning. This is a cutting-edge area in NLP research and you may find references
    to training approaches called **few-shot learning**, which is learning to recognize
    a new class with only a few examples, or even **zero-shot learning**, which enables
    a system to identify a class without having seen any examples of that class in
    the training data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的所有系统中，创建特定应用模型所需的语言知识都来自训练数据。这个过程开始时对语言一无所知。另一方面，LLM（大规模语言模型）带有通过处理大量更为通用的文本进行*预训练*的模型，因此它们对语言有了基本的知识基础。可以使用额外的训练数据对模型进行*微调*，以便它能处理特定应用的输入。微调模型以适应特定应用的一个重要方面是尽量减少微调所需的新数据量。这是自然语言处理（NLP）研究中的前沿领域，你可能会看到一些训练方法的相关参考，例如**少样本学习**，即通过仅几个例子学习识别一个新类别，甚至**零样本学习**，它使得系统能够在没有见过任何该类别示例的情况下识别该类别。
- en: In the next section, we’ll take a look at one of the currently most popular
    LLMs, ChatGPT.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看看当前最流行的LLM之一——ChatGPT。
- en: ChatGPT
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatGPT
- en: 'ChatGPT ([https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/))
    is a system that can interact with users about generic information in a very capable
    way. Although at the time of writing, it is hard to customize ChatGPT for specific
    applications, it can be useful for other purposes than customized natural language
    applications. For example, it can very easily be used to generate training data
    for a conventional application. If we wanted to develop a banking application
    using some of the techniques discussed earlier in this book, we would need training
    data to provide the system with examples of how users might ask the system questions.
    Typically, this involves a process of collecting actual user input, which could
    be very time-consuming. ChatGPT could be used to generate training data instead,
    by simply asking it for examples. For example, for the prompt *give me 10 examples
    of how someone might ask for their checking balance*, ChatGPT responded with the
    sentences in *Figure 11**.3*:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT ([https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/))
    是一个能够与用户就通用信息进行非常有效互动的系统。虽然在写作时，定制ChatGPT以适应特定应用尚且困难，但它对于除定制自然语言应用之外的其他目的仍然有用。例如，它可以非常容易地用来生成常规应用的训练数据。如果我们想使用本书中前面讨论的一些技术开发一个银行应用程序，我们需要训练数据，以便系统能提供用户可能如何提问的示例。通常，这涉及收集实际的用户输入，而这一过程可能非常耗时。相反，可以使用ChatGPT来生成训练数据，只需向它请求示例即可。例如，对于提示*给我10个用户可能询问支票余额的示例*，ChatGPT给出的回答是*图11.3*中的句子：
- en: '![Figure 11.3 – GPT-3 generated training data for a banking application](img/B19005_11_03.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图11.3 – GPT-3为银行应用生成的训练数据](img/B19005_11_03.jpg)'
- en: Figure 11.3 – GPT-3 generated training data for a banking application
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 – 用于银行应用程序的GPT-3生成的训练数据
- en: Most of these seem like pretty reasonable queries about a checking account,
    but some of them don’t seem very natural. For that reason, data generated in this
    way always needs to be reviewed. For example, a developer might decide not to
    include the second to the last example in a training set because it sounds stilted,
    but overall, this technique has the potential to save developers quite a bit of
    time.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这些查询看起来大多数是关于支票账户的合理问题，但有些听起来并不太自然。出于这个原因，以这种方式生成的数据总是需要进行审查。例如，开发者可能决定不将倒数第二个例子包含在训练集中，因为它听起来生硬，但总体而言，这种技术有潜力为开发者节省大量时间。
- en: Applying GPT-3
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用GPT-3
- en: Another well-known LLM, GPT-3, can also be fine-tuned with application-specific
    data, which should result in better performance. To do this, you need an OpenAI
    key because using GPT-3 is a paid service. Both fine-tuning to prepare the model
    and using the fine-tuned model to process new data at inference time will incur
    a cost, so it is important to verify that the training process is performing as
    expected before training with a large dataset and incurring the associated expense.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个知名的LLM，GPT-3，也可以通过应用特定的数据进行微调，这样应该能带来更好的性能。为了实现这一点，你需要一个OpenAI的API密钥，因为使用GPT-3是收费服务。无论是为了准备模型进行微调，还是在推理时使用微调后的模型处理新数据，都将产生费用。因此，在使用大量数据集进行训练并承担相关费用之前，验证训练过程是否按预期执行非常重要。
- en: OpenAI recommends the following steps to fine-tune a GPT-3 model.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI推荐以下步骤来微调GPT-3模型。
- en: Sign up for an account at [https://openai.com/](https://openai.com/) and obtain
    an API key. The API key will be used to track your usage and charge your account
    accordingly.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[https://openai.com/](https://openai.com/)注册账户并获取API密钥。API密钥将用于追踪你的使用情况并相应地向你的账户收费。
- en: 'Install the OpenAI **command-line interface** (**CLI**) with the following
    command:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装OpenAI **命令行界面**（**CLI**）：
- en: '[PRE14]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This command can be used at a terminal prompt in Unix-like systems (some developers
    have reported problems with Windows or macOS). Alternatively, you can install
    GPT-3 to be used in a Jupyter notebook with the following code:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令可以在类Unix系统的终端提示符中使用（一些开发者报告称在Windows或macOS上有问题）。另外，你也可以安装GPT-3，在Jupyter
    notebook中使用以下代码：
- en: '[PRE15]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'All of the following examples assume that the code is running in a Jupyter
    notebook:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下所有示例都假设代码在Jupyter notebook中运行：
- en: 'Set your API key:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置你的API密钥：
- en: '[PRE16]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The next step is to specify the training data that you will use for fine-tuning
    GPT-3 for your application. This is very similar to the process of training any
    NLP system; however, GPT-3 has a specific format that must be used for training
    data. This format uses a syntax called JSONL, where every line is an independent
    JSON expression. For example, if we want to fine-tune GPT-3 to classify movie
    reviews, a couple of data items would look like the following (omitting some of
    the text for clarity):'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是指定你将用来微调GPT-3的训练数据。这与训练任何NLP系统的过程非常相似；然而，GPT-3有一个特定的格式，必须按照这个格式提供训练数据。这个格式使用一种叫做JSONL的语法，其中每一行都是一个独立的JSON表达式。例如，如果我们想要微调GPT-3来分类电影评论，几条数据项可能如下所示（为了清晰起见省略了一些文本）：
- en: '[PRE18]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Each item consists of a JSON dict with two keys, `prompt` and `completion`.
    `prompt` is the text to be classified, and `completion` is the correct classification.
    All three of these items are negative reviews, so the completions are all marked
    as `negative`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 每个项目由一个包含两个键的JSON字典组成，`prompt`和`completion`。`prompt`是待分类的文本，`completion`是正确的分类。所有这三个项目都是负面评论，因此所有的completion都标记为`negative`。
- en: 'It might not always be convenient to get your data into this format if it is
    already in another format, but OpenAI provides a useful tool for converting other
    formats into JSONL. It accepts a wide range of input formats, such as CSV, TSV,
    XLSX, and JSON, with the only requirement for the input being that it contains
    two columns with `prompt` and `completion` headers. *Table 11.2* shows a few cells
    from an Excel spreadsheet with some movie reviews as an example:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据已经是其他格式，可能不太方便将其转换为这种格式，但OpenAI提供了一个有用的工具，可以将其他格式转换为JSONL。它接受多种输入格式，如CSV、TSV、XLSX和JSON，唯一的要求是输入必须包含两个列，分别为`prompt`和`completion`。*表11.2*展示了一些来自Excel电子表格的单元格，其中包含一些电影评论作为示例：
- en: '| **prompt** | **completion** |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| **prompt** | **completion** |'
- en: '| kolya is one of the richest films i’ve seen in some time . zdenek sverak
    plays a confirmed old bachelor ( who’s likely to remain so ) , who finds his life
    as a czech cellist increasingly impacted by the five-year old boy that he’s taking
    care of … | positive |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Kolya是我近期看到的最丰富的电影之一。Zdenek Sverak饰演一位确认的老光棍（可能会保持这样的状态），他的生活作为一名捷克大提琴家，越来越受到他照顾的五岁男孩的影响……
    | 正面 |'
- en: '| this three hour movie opens up with a view of singer/guitar player/musician/composer
    frank zappa rehearsing with his fellow band members . all the rest displays a
    compilation of footage , mostly from the concert at the palladium in new york
    city , halloween 1979 … | positive |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 这部三小时的电影以歌手/吉他手/音乐家/作曲家Frank Zappa与他的乐队成员排练的画面开场。之后展示的是一系列的片段，主要来自1979年万圣节在纽约市的Palladium音乐厅的演唱会……
    | 正面 |'
- en: '| `strange days’ chronicles the last two days of 1999 in los angeles . as the
    locals gear up for the new millenium , lenny nero ( ralph fiennes ) goes about
    his business … | positive |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| `strange days` 讲述了1999年最后两天洛杉矶的故事。当当地人准备迎接新千年时，Lenny Nero（拉尔夫·费因斯）继续忙碌着他的工作……
    | 正面 |'
- en: Table 11.2 – Movie review data for fine-tuning GPT-3
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.2 – 用于微调GPT-3的电影评论数据
- en: 'To convert one of these alternative formats into JSONL, you can use the `fine_tunes.prepare_data`
    tool, as shown here, assuming that your data is contained in the `movies.csv`
    file:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 要将这些替代格式之一转换为JSONL格式，你可以使用`fine_tunes.prepare_data`工具，假设你的数据包含在`movies.csv`文件中，如下所示：
- en: '[PRE21]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `fine_tunes.prepare_data` utility will create a JSONL file of the data and
    will also provide some diagnostic information that can help improve the data.
    The most important diagnostic that it provides is whether or not the amount of
    data is sufficient. OpenAI recommends several hundred examples of good performance.
    Other diagnostics include various types of formatting information such as separators
    between the prompts and the completions.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`fine_tunes.prepare_data`工具将创建一个JSONL格式的数据文件，并提供一些诊断信息，这些信息有助于改进数据。它提供的最重要的诊断信息是数据量是否足够。OpenAI建议使用几百个具有良好表现的示例。其他诊断信息包括各种格式化信息，如提示与完成之间的分隔符。'
- en: 'After the data is correctly formatted, you can upload it to your OpenAI account
    and save the filename:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据格式正确后，你可以将其上传到你的OpenAI账户，并保存文件名：
- en: '[PRE22]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The next step is to create and save a fine-tuned model. There are several different
    OpenAI models that can be used. The one we’re using here, `ada`, is the fastest
    and least expensive, and does a good job on many classification tasks:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建并保存一个微调模型。可以使用多个不同的OpenAI模型。我们这里使用的`ada`是最快且最便宜的，并且在许多分类任务中表现良好：
- en: '[PRE23]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we can test the model with a new prompt:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以用一个新的提示来测试这个模型：
- en: '[PRE24]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In this example, since we are only using a few fine-tuning utterances, the results
    will not be very good. You are encouraged to experiment with larger amounts of
    training data.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，由于我们只使用了少量的微调语句，结果不会非常理想。鼓励你尝试使用更多的训练数据进行实验。
- en: Summary
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter covered the currently best-performing techniques in NLP – transformers
    and pretrained models. In addition, we have demonstrated how they can be applied
    to processing your own application-specific data, using both local pretrained
    models and cloud-based models.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了目前在自然语言处理（NLP）领域中表现最好的技术——变换器和预训练模型。此外，我们还展示了如何将它们应用于处理你自己特定应用的数据，使用本地预训练模型和基于云的模型。
- en: Specifically, you learned about the basic concepts behind attention, transformers,
    and pretrained models, and then applied the BERT pretrained transformer system
    to a classification problem. Finally, we looked at using the cloud-based GPT-3
    systems for generating data and for processing application-specific data.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，你了解了注意力机制、变换器（transformers）和预训练模型的基本概念，然后将BERT预训练变换器系统应用于分类问题。最后，我们探讨了如何使用基于云的GPT-3系统生成数据以及处理特定应用的数据。
- en: In [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217), we will turn to a different
    topic – unsupervised learning. Up to this point, all of our models have been *supervised*,
    which you will recall means that the data has been annotated with the correct
    processing result. Next, we will discuss applications of *unsupervised* learning.
    These applications include topic modeling and clustering. We will also talk about
    the value of unsupervised learning for exploratory applications and maximizing
    scarce data. It will also address types of partial supervision, including weak
    supervision and distant supervision.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第12章*](B19005_12.xhtml#_idTextAnchor217)中，我们将转向一个不同的主题——无监督学习。到目前为止，我们的所有模型都是*有监督的*，你会记得这意味着数据已经被标注了正确的处理结果。接下来，我们将讨论*无监督*学习的应用。这些应用包括主题建模和聚类。我们还将讨论无监督学习在探索性应用和最大化稀缺数据方面的价值。还将涉及部分监督的类型，包括弱监督和远程监督。
