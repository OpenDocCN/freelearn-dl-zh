- en: CNNs for Image Recognition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNNs用于图像识别
- en: In this chapter, you will learn to use **convolutional neural networks** (**CNNs**)
    for image recognition. Convolutional neural networks are a variation of neural
    networks that are particularly well-suited to image recognition because they take
    into account the relationship between data points in space.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何使用**卷积神经网络**（**CNNs**）进行图像识别。卷积神经网络是神经网络的一种变体，特别适合图像识别，因为它们考虑了空间中数据点之间的关系。
- en: We will cover how convolutional neural networks differ from the basic feedforward,
    fully connected neural network that we created in the last chapter. The main difference
    is that the hidden layers in a CNN are not all fully connected dense layers—CNNs
    include a number of special layers. One of these is the convolutional layer, which
    convolves a filter around the image space. The other special layer is a pooling
    layer, which reduces the size of the input and only persists particular values.
    We will go into more depth on these layers later in the chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍卷积神经网络如何与我们在上一章创建的基本前馈全连接神经网络有所不同。主要区别在于，CNN中的隐藏层并非全部是全连接的密集层——CNN包括一些特殊层。其中之一是卷积层，它在图像空间周围卷积一个滤波器。另一个特殊层是池化层，它减少输入的大小，并仅保留特定的值。我们将在本章稍后的部分深入探讨这些层。
- en: As we learn about these concepts, we will see why they are so critical for image
    recognition. When we think about classifying images, we know that we need to detect
    patterns among arrays of pixels and that the neighboring pixels are important
    for finding certain shapes. By learning more about the convolution layer, you
    will know how to adjust the filter or lens to detect different patterns depending
    on your image data. You will also learn how to adjust the pooling layer depending
    on the size of your data to help make your model run more efficiently.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们学习这些概念时，我们会看到它们为何在图像识别中如此重要。当我们思考图像分类时，我们知道需要在像素阵列中检测模式，并且邻近的像素对于寻找特定形状非常重要。通过了解卷积层，您将知道如何根据您的图像数据调整滤波器或镜头以检测不同的模式。您还将学习如何根据数据的大小调整池化层，以帮助使模型运行得更高效。
- en: 'Specifically, this chapter will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，本章将覆盖以下主题：
- en: Image recognition with shallow nets
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用浅层网络进行图像识别
- en: Image recognition with convolutional neural networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用卷积神经网络进行图像识别
- en: Enhancing the model with appropriate activation layers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过适当的激活层增强模型
- en: Choosing the most appropriate activation function
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择最合适的激活函数
- en: Selecting optimal epochs using dropout and early stopping
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用dropout和早期停止选择最佳epochs
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the code files of this chapter on the GitHub link at [https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R](https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R](https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R)上的GitHub链接找到本章的代码文件。
- en: Image recognition with shallow nets
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用浅层网络进行图像识别
- en: Image classifiers can be created without using deep-learning algorithms and
    methods. To demonstrate, let's use the **Fashion MNIST** dataset, which is an
    alternative to the MNIST handwriting dataset. The name MNIST stands for the **Modified
    National Institute of Standards and Technology** database, and as the name suggests,
    it is a modified version of the original dataset created by the National Institute
    of Standards and Technology. While MNIST is a series of hand-drawn numbers, Fashion
    MNIST uses small images of different types of clothing. The clothing in the dataset
    is labeled with one of ten categories. Fashion MNIST has nothing to do with the
    National Institute of Standards and Technology; however, the MNIST name carried
    over since it is well-known as a database to use for image recognition.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类器可以在不使用深度学习算法和方法的情况下创建。为了演示，让我们使用**Fashion MNIST**数据集，它是MNIST手写数据集的替代品。MNIST的名字代表**修改版国家标准与技术研究所**数据库，正如名称所示，它是由国家标准与技术研究所创建的原始数据集的修改版。MNIST是一系列手绘数字，而Fashion
    MNIST使用的是不同类型服装的小图像。数据集中的服装被标注为十个类别之一。Fashion MNIST与国家标准与技术研究所无关，但由于MNIST作为图像识别的数据库广为人知，所以名称得以延续。
- en: 'Since this dataset is not very large and each image is only 28 x 28 pixels,
    we can use a machine-learning algorithm, such as `RandomForest`, to train a classifier.
    We will train a very simple `RandomForest` model and achieve surprisingly good
    results; however, at the end of the chapter, we will discuss why these same results
    will not scale as the dataset gets larger and the individual images get larger.
    We will now code our image recognition model using traditional machine-learning
    methods:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个数据集不大，而且每个图像只有28 x 28像素，因此我们可以使用机器学习算法，例如`RandomForest`，来训练分类器。我们将训练一个非常简单的`RandomForest`模型，并取得令人惊讶的好结果；然而，在本章的最后，我们将讨论为什么这些结果在数据集变大以及单个图像变大时无法扩展。我们现在将使用传统的机器学习方法编写我们的图像识别模型：
- en: 'We will start by loading the `tidyverse` suite of packages, as shown in the
    following code. In this case, we only need `readr` for reading in the data; however,
    we will use other packages later. We will also load `randomForest` for training
    our model and `caret` for evaluating our model performance:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从加载`tidyverse`套件包开始，如下所示的代码。在此情况下，我们只需要`readr`来读取数据；不过，我们稍后还会使用其他包。我们还将加载`randomForest`来训练我们的模型，并加载`caret`来评估模型的表现：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The code here will not return any values to the console; however, within the
    RStudio environment, we will see a checkmark next to these packages in the Packages
    window indicating that they are ready to be used. Your Packages pane should look
    like the following image, which shows that two of three packages have been loaded:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的代码不会向控制台返回任何值；但是，在RStudio环境中，我们会看到在包窗口旁边出现一个勾选标记，表示这些包已准备好使用。你的Packages面板应显示如下图，表明三个包中的两个已经加载完毕：
- en: '![](img/ef79de23-1f1c-4dec-a4b8-363f52a0cf02.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef79de23-1f1c-4dec-a4b8-363f52a0cf02.png)'
- en: 'Next, we read in the train and test data for the Fashion MNIST dataset with
    the help of the following code:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将借助以下代码读取Fashion MNIST数据集的训练数据和测试数据：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This code will place two data objects in our environment called `fm` and `fm_test`.
    The Environment pane should look like the following screenshot:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将在我们的环境中创建两个数据对象，分别名为`fm`和`fm_test`。环境面板应显示如下截图：
- en: '![](img/ed554320-b06f-479e-9879-3d053ff5b72b.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed554320-b06f-479e-9879-3d053ff5b72b.png)'
- en: We will use `fm` to train our model. The data from `fm` will be used to compute
    weights for splits along this tree-based model. We will then use our model, which
    contains information on how the independent variable values relate to the target
    variables, to predict target variables for the `fm_test` data using the independent
    variable values.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`fm`来训练我们的模型。`fm`中的数据将用于计算沿树模型的分割权重。然后，我们将使用包含自变量值与目标变量关系的模型，利用自变量值预测`fm_test`数据的目标变量。
- en: 'Next, we will train our model. We set a seed for reproducibility so that we
    get the same quasirandom numbers every time we run the model, and as such, we
    always get the same results. We convert the label to a factor. The label, in this
    case, is an integer between `0` and `9`; however, we do not want the model to
    treat these values numerically. Instead, they should be treated as different categories.
    The remaining columns aside from the label are all pixel values. We use `~`. to
    denote that we will use all the remaining columns (all the pixel values) as independent
    variables for our model. We will grow 10 trees because this is simply an example
    that image classification can be done this way. Lastly, we will choose 5 variables
    at random during every split in our tree. We will train our `RandomForest` model
    in this way using the following code:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将训练我们的模型。我们设置一个种子值以确保结果可复现，这样每次运行模型时我们都会得到相同的伪随机数，因此每次结果都相同。我们将标签转换为因子。标签在这里是一个介于`0`和`9`之间的整数；然而，我们不希望模型将这些值当作数值处理。相反，它们应该被视为不同的类别。除标签外，剩余的列都是像素值。我们使用`~`来表示我们将使用所有剩余的列（所有像素值）作为模型的自变量。我们将生成10棵树，因为这只是一个示例，展示了图像分类可以通过这种方式完成。最后，我们将在每次分割时随机选择5个变量。我们将使用以下代码以这种方式训练我们的`RandomForest`模型：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When we execute the code, the model will run, which can take several minutes.
    During this time, we will be unable to execute any code in the console. We can
    see that the model is now in our environment. The following screenshot shows some
    of the details contained in the model object:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行代码时，模型将开始运行，可能需要几分钟。在此期间，我们将无法在控制台执行任何代码。我们可以看到模型现在已经进入我们的环境。以下截图显示了模型对象中的一些细节：
- en: '![](img/da2a3770-a604-46c2-a5d3-8504de4975dc.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/da2a3770-a604-46c2-a5d3-8504de4975dc.png)'
- en: We can use this model object to make predictions on new data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个模型对象来对新数据进行预测。
- en: 'We then use our model to make predictions on the test dataset and use the `ConfusionMatrix`
    function to evaluate performance. The following code will populate the vector
    of predicted values and then evaluate the accuracy of the predictions:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用模型对测试数据集进行预测，并使用`ConfusionMatrix`函数来评估性能。以下代码将填充预测值的向量，并评估预测的准确性：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code will create one last data object, which is a vector that
    holds the predicted values for each case based on the model being trained on the
    independent variables for that dataset. We also printed some output to our console
    with performance metrics. The output that you receive will look like the following
    screenshot:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将创建最后一个数据对象，它是一个向量，包含基于该数据集的独立变量训练的模型为每个案例预测的值。我们还在控制台打印了一些输出，包括性能指标。你收到的输出将如下所示：
- en: '![](img/290bca6f-c0fa-4d7d-867c-2bc193432f05.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/290bca6f-c0fa-4d7d-867c-2bc193432f05.png)'
- en: The metrics are based on comparing the actual target variables for the test
    dataset with the predicted values from modeling on the test data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标是通过将测试数据集的实际目标变量与从测试数据中建模得到的预测值进行比较来计算的。
- en: Surprisingly, this model produced decent results. We have achieved an accuracy
    of 84.6%. This shows that a simple approach can work for a dataset like this;
    however, as the data scales up, this type of model will have worse performance.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 出乎意料的是，这个模型产生了不错的结果。我们已经达到了84.6%的准确率。这表明，简单的方法可以适用于像这样的数据集；然而，随着数据量的增加，这种类型的模型表现会变得更差。
- en: To understand why, we should first explain how images are stored as data for
    modeling. When we view a grayscale image, we see lighter and darker areas. In
    fact, every pixel holds an integer from 0 for white to 255 for black and anywhere
    in between. These numbers are converted into tones so that we can visualize the
    image; however, for our purposes, we use these raw pixel values. When modeling
    with `RandomForest`, each pixel value is compared in isolation with all the other
    images; however, this is rarely ideal. Usually, we want to look for larger patterns
    of pixels within each image.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解原因，我们首先需要解释图像如何作为数据存储用于建模。当我们查看灰度图像时，我们看到的是明暗不同的区域。实际上，每个像素都包含一个从0到255之间的整数，0表示白色，255表示黑色，中间的值则代表不同的灰度。这些数字被转换成音调，以便我们能可视化图像；然而，为了我们的目的，我们直接使用这些原始的像素值。在使用`RandomForest`进行建模时，每个像素值都会与其他图像进行单独比较；然而，这通常不是最理想的做法。通常，我们希望在每个图像内寻找更大的像素模式。
- en: 'Let''s explore how to create a shallow neural network with just one layer.
    The hidden layer of the neural network will perform a calculation using all input
    values so that the entire image is considered. We are going to make this a simple
    binominal classification problem for illustration purposes and use a method to
    create our neural network that is similar to the method we used in the last chapter.
    If you completed that chapter, then this will likely look familiar. Completing
    the previous chapter is not a prerequisite as we will walk through all the steps
    here:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来探讨如何创建一个只有一层的浅层神经网络。神经网络的隐藏层将使用所有输入值进行计算，以便考虑整个图像。为了演示，我们将这个问题设定为一个简单的二项分类问题，并使用与上一章类似的方法来创建我们的神经网络。如果你完成了上一章的内容，那么这将可能看起来很熟悉。完成上一章的内容不是先决条件，因为我们将在这里逐步讲解所有步骤：
- en: 'Before starting, we will load two more libraries for the following code: the
    `neuralnet` package for training our model and the `Metrics` package for evaluation
    functions. In particular, we will use the AUC metric later to evaluate our model.
    Both of these libraries can be loaded by running the following lines of code:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开始之前，我们需要加载两个额外的库：`neuralnet`包用于训练我们的模型，`Metrics`包用于评估函数。特别是，我们稍后将使用AUC指标来评估我们的模型。通过运行以下代码行，可以加载这两个库：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This code will not cause anything to happen in the console; however, we will
    see checks by these packages in the Package pane indicating that these packages
    are ready to use. Your Packages pane will look like the following screenshot:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码不会在控制台中产生任何效果；但是，我们会在包面板中看到这些包的检查，表明它们已经准备好使用。你的包面板将显示如下截图：
- en: '![](img/eb517d20-8e51-418d-beb9-ff46289e7e4b.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb517d20-8e51-418d-beb9-ff46289e7e4b.png)'
- en: 'First, we will change the **target** column so that it is a simple binary response
    rather than include all ten categories. This is done so that we can keep this
    neural network very straightforward, as this is just to create a benchmark for
    comparing with our CNN later and to show how coding the two styles of neural networks
    differs. This filtering is accomplished by running the following lines of code:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将修改**target**列，使其成为一个简单的二元响应，而不是包含所有十个类别。这样做是为了保持这个神经网络的简洁，因为它只是为了创建一个基准，便于与后续的卷积神经网络（CNN）进行比较，并展示两种风格的神经网络编码方式的差异。这个筛选过程通过运行以下代码来实现：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After running this code, we will see that the size of our data objects has
    changed and reduced in size as a result of our filtering. You should see that
    your data objects have changed from having 60,000 and 10,000 observations respectively
    to 12,000 and 2,000, as shown in the following screenshot:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码后，我们将看到数据对象的大小发生了变化，经过筛选后数据变小了。你应该能看到数据对象的观察值从原来的60,000和10,000分别减少到12,000和2,000，如下图所示：
- en: '![](img/2d415b2a-5859-452d-a0da-928b777adca7.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d415b2a-5859-452d-a0da-928b777adca7.png)'
- en: With the data in this format, we are now able to proceed with writing our code
    as a binary response task.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种格式的数据，我们现在可以继续编写代码，作为一个二分类任务。
- en: 'Now, using the following code, we will remove the target variable from the
    test set and isolate it in a separate vector for evaluation later:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下代码，我们将从测试集中移除目标变量，并将其隔离到一个单独的向量中，以便稍后进行评估：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After running this code you will notice two changes: there is one less variable
    or column in the `fm_test` object and there is a new data object called `test_label`,
    which is a vector containing the values that were in the label column of the `fm_test`
    object. Your Environment pane should look like the following screenshot:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码后，你会注意到两个变化：`fm_test`对象中少了一个变量或列，并且新增了一个数据对象叫做`test_label`，它是一个包含`fm_test`对象中标签列值的向量。你的环境面板应该如下图所示：
- en: '![](img/b22efddd-8ddf-4e73-b21e-696ec062c3b4.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b22efddd-8ddf-4e73-b21e-696ec062c3b4.png)'
- en: We have made this change because we do not want the label in our test object.
    In this object, we need to treat the data as if we do not know the true classes
    so that we can try to predict the classes. We then use the labels from the vector
    later to evaluate how well we predicted the correct values.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之所以做出这个修改，是因为我们不希望在测试对象中包含标签。在这个对象中，我们需要将数据视为我们不知道真实类别的情况，这样我们可以尝试预测类别。然后，我们稍后会使用来自向量的标签来评估我们预测正确值的效果。
- en: 'Next, we will create the formula for our neural network. Using the `neuralnet`
    function from the `neuralnet` package, we need our formula to be formatted with
    the target variable on one side of a tilde (`~`) and all of our independent variables
    on the other side connected by plus (`+`) signs. In the following code, we collect
    all columns names into a vector `n` and then use `paste` to concatenate each term
    from this vector with a plus sign in between:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '接下来，我们将为我们的神经网络创建公式。使用`neuralnet`包中的`neuralnet`函数，我们需要将目标变量放在波浪号（`~`）的一侧，所有自变量则放在另一侧，并通过加号（`+`）连接。在以下代码中，我们将所有列名收集到一个向量`n`中，然后使用`paste`将这个向量中的每一项与加号连接起来：  '
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After running this code, we can see the changes in our Environment pane. We
    will see the vector `n` that contains all the column names and the `formula` object
    that has the dependent variable and independent variables placed together in the
    proper format. Your Environment pane should now look like the following screenshot:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码后，我们可以在环境面板中看到变化。我们将看到向量`n`，它包含了所有列名，还有`formula`对象，它将因变量和自变量按照正确的格式放在一起。你的环境面板现在应该如下图所示：
- en: '![](img/62fabd77-496c-4c49-8dd8-9e5123e58fd1.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62fabd77-496c-4c49-8dd8-9e5123e58fd1.png)'
- en: We ran the preceding code in order to create this `formula` object as it is
    a requirement for training a neural network using the `neuralnet` package.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行前面的代码是为了创建这个`formula`对象，因为这是使用`neuralnet`包训练神经网络的要求。
- en: 'After this, we can write the code to train our model. We will set a seed for
    reproducibility as we always do with modeling. We will include one hidden layer
    with the number of units set to approximately one-third the number of predictor
    variables. We will set the `linear.output` argument to `false` to denote that
    this will be a classification model. We will also set the activation function
    to `logistic` because this is a classification problem. We train our model in
    the way we described earlier using the following code:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们可以编写代码来训练我们的模型。我们将设置一个种子以确保可复现性，就像我们在建模时总是做的那样。我们将包含一个隐藏层，单位数设为大约预测变量数量的三分之一。我们将`linear.output`参数设置为`false`，表示这是一个分类模型。我们还将激活函数设置为`logistic`，因为这是一个分类问题。我们将使用以下代码按照之前描述的方式训练我们的模型：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After running the code, we now have a new object in our Environment pane that
    contains all the details gathered from training our model that can now be applied
    to make predictions on new data. Your Environment pane should contain a model
    object similar to the one shown in the following screenshot:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，我们现在在环境面板中有一个新对象，包含了所有从训练模型中获得的细节，这些细节现在可以用来对新数据进行预测。你的环境面板中应该包含一个类似下面截图所示的模型对象：
- en: '![](img/cb9fd321-7063-49a5-be27-e18439503605.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb9fd321-7063-49a5-be27-e18439503605.png)'
- en: Now that we have run this code, we have a model that we can use to make predictions
    on our test data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经运行了这段代码，我们有了一个模型，可以用它对我们的测试数据进行预测。
- en: 'Lastly, we can make our predictions and evaluate our results with the help
    of the following code:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以借助以下代码进行预测并评估我们的结果：
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Running this code will print the accuracy metric to the console. Your console
    should contain output just like the following image:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码将把准确度指标打印到控制台。你的控制台应该包含如下图像中的输出：
- en: '![](img/8f1b7f45-9822-4112-a552-62f676557d51.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f1b7f45-9822-4112-a552-62f676557d51.png)'
- en: Looking at this output, we see that we have a significant improvement already.
    Accuracy is now up to 97.487%. When the pixels were considered in concert, it
    did improve results. We should remember that this model only used two target variables,
    and the selection of these target variables could also be part of the reason for
    the significant increase. In any case, with larger images, it is not efficient
    to push all pixel values to an activation function. This is where convolutional
    neural networks come in to solve this problem. They are able to look at smaller
    groupings of pixel values to look for patterns. They also contain a means of reducing
    dimensionality.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个输出结果来看，我们已经有了显著的提升。准确度现在达到了97.487%。当像素值被联合考虑时，确实提高了结果。我们应该记住，这个模型只使用了两个目标变量，这些目标变量的选择可能也是准确度显著提升的原因之一。无论如何，在处理更大的图像时，将所有像素值输入激活函数并不高效。这就是卷积神经网络发挥作用的地方。它们能够通过观察较小的像素值组来寻找模式，并且还包含一种减少维度的方式。
- en: Let's now explore what separates convolutional neural networks from traditional
    neural networks.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来探索一下卷积神经网络与传统神经网络的区别。
- en: Image recognition with convolutional neural networks
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积神经网络进行图像识别
- en: Convolutional neural networks are a special form of neural network. In a traditional
    neural network, the input is passed to the model as vectors; however, for image
    data, it is more helpful to have the data arranged as matrices because we want
    to capture the relationship of the pixel values in two-dimensional space.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络是一种特殊形式的神经网络。在传统神经网络中，输入作为向量传递到模型中；然而，对于图像数据，将数据以矩阵的形式排列更为有利，因为我们希望捕捉像素值在二维空间中的关系。
- en: Convolutional neural networks are able to capture these two-dimensional relationships
    through the use of a filter that convolves over the image data. The filter is
    a matrix with constant values and dimensions that are smaller than the image data.
    The constant values are multiplied by the underlying values and the sum of the
    resulting products is passed through to an activation function.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络能够通过一个滤波器来捕捉这些二维关系，滤波器在图像数据上进行卷积。滤波器是一个矩阵，具有固定的值和比图像数据更小的维度。这些常数值与底层值相乘，结果的乘积之和将通过激活函数进行处理。
- en: The activation function step, which can also be considered a separate layer,
    evaluates whether a given pattern is present in an image. In a traditional neural
    network, the activation layer determines whether the calculated value from the
    input values exceeds a threshold and should be fed forward in the model. In a
    convolutional neural network, the activation layer operates in a very similar
    way; however, because it uses matrix multiplication, it is able to evaluate whether
    a two-dimensional shape is present in the data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数步骤，也可以视为一个独立的层，用于评估图像中是否存在某个特定模式。在传统神经网络中，激活层决定输入值计算出的结果是否超过阈值，并将其传递到模型中。在卷积神经网络中，激活层的工作方式非常相似；然而，由于它使用矩阵乘法，因此能够评估数据中是否存在二维形状。
- en: After the activation layer, the data is further processed by a pooling layer.
    The function of the pooling layer is to concentrate the signal captured in the
    previous step while also reducing the dimensionality of the data. The pooling
    layer will result in a matrix that is smaller than the input data. Often a 2 x
    2 pooling layer will be used that reduces the size of the input data by half.
    In this case, the values within every 2 x 2 section are pooled through some sort
    of aggregation. These values can be aggregated using any means, such as summing
    and averaging the values; however, in most cases, the max value is used and this
    value is passed to the pooling layer.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在激活层之后，数据会通过池化层进一步处理。池化层的功能是集中前一步骤捕获的信号，同时减少数据的维度。池化层将生成一个比输入数据更小的矩阵。通常会使用一个
    2 x 2 的池化层，这样可以将输入数据的大小缩小一半。在这种情况下，每个 2 x 2 区域内的值通过某种聚合方式进行汇聚。这些值可以通过任何方式进行聚合，例如对值求和或取平均；然而，在大多数情况下，采用最大值，并将该值传递到池化层。
- en: 'After the preceding method has been implemented, the processed and reduced
    image data is flattened and the vectors are then fed forward to essentially a
    traditional neural network as the last step. Let’s start here with just this final
    step, since we are familiar with using a traditional neural network to model from
    the previous code. Here, we will see two things: firstly, that we can train a
    model on image data using just this final step, and secondly, that the syntax
    is slightly different but generally recognizable when compared with training this
    type of model using the `neuralnet` package.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方法实现之后，处理并压缩后的图像数据会被扁平化，向前传递到一个基本上是传统神经网络的最后一步。让我们从这一最终步骤开始，因为我们已经熟悉使用传统神经网络进行建模。在这里，我们将看到两件事：首先，我们可以只通过最后这一步来训练图像数据模型；其次，尽管语法略有不同，但与使用
    `neuralnet` 包训练这种模型相比，整体结构是可以识别的。
- en: 'We will now code a neural network consisting of fully connected dense hidden
    layers using the `keras` package:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用 `keras` 包编写一个由全连接的密集隐藏层组成的神经网络：
- en: 'First, we will load the `keras` library and the Fashion MNIST dataset that
    comes with the package. This is accomplished by running the following code:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将加载 `keras` 库以及该包自带的 Fashion MNIST 数据集。通过运行以下代码来完成这一操作：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'When we run the preceding code, we will see that we get both the train and
    test data together in one list object. Your Environment pane should now look like
    the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行前面的代码时，我们会看到将训练数据和测试数据一起存储在一个列表对象中。你的环境面板现在应该如下所示：
- en: '![](img/1831ae17-0d4e-4668-807c-ef73dc73030e.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1831ae17-0d4e-4668-807c-ef73dc73030e.png)'
- en: 'Next, we can split the dataset into its component parts. It is conveniently
    set up so that it is easy to extract the training and test datasets and target
    variables. In the previous code, we used a version of the Fashion MNIST data that
    has already been preprocessed so that every pixel was in a separate column; however,
    in the following code, we will start with a large array of 28 x 28 matrices and
    demonstrate how to transform this data so that all pixel values for a given image
    are in the same row. The first step in the process is to separate out the four
    data objects from the list using the preceding code:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以将数据集拆分成各个部分。它被方便地设置为容易提取训练集、测试集和目标变量。在前面的代码中，我们使用的是已经预处理过的 Fashion MNIST
    数据集，每个像素值都在单独的列中；然而，在接下来的代码中，我们将从一个 28 x 28 的大矩阵数组开始，演示如何将这些数据转换，使得每个图像的所有像素值都在同一行。这个过程的第一步是使用前面的代码将列表中的四个数据对象分离出来：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'When you look at your Environment pane now, you will see the image data stored
    in 28 x 28 matrices and the target variables in vectors within an array. Your
    Environment pane will look like the following:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当您现在查看环境窗格时，您将看到图像数据存储在28 x 28的矩阵中，目标变量存储在数组中的向量内。您的环境窗格将如下所示：
- en: '![](img/f3af5e2f-a0a4-46d2-9e87-5350ca95db66.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f3af5e2f-a0a4-46d2-9e87-5350ca95db66.png)'
- en: With our data in this format, we could apply our convolutional filters, which
    we will do soon; however, at this point, we are going to code a dense, fully connected
    neural network, and in order to do so, we will need to get all the data to one
    row per image instead of a two-dimensional matrix per image. Since we need the
    data in both formats at different stages of coding a convolutional neural network,
    it is a straightforward conversion process that we will complete in step six.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这种格式的数据，我们可以应用我们的卷积滤波器，我们很快就会这样做；然而，此时我们将编写一个密集的全连接神经网络，为了这样做，我们需要将所有数据转换为每个图像一行，而不是每个图像的二维矩阵。由于我们在编码卷积神经网络的不同阶段需要两种格式的数据，这是一个简单的转换过程，我们将在第六步完成。
- en: 'Image data consists of a matrix or matrices of pixel values between `0` and
    `255`. To process this data with our neural network, we need to convert these
    values to floats between `0` and `1`. As shown in the following code, we will
    use the `normalize()` convenience function to achieve this result and the `range()`
    function to test whether the values are now between `0` and `1`:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像数据由介于`0`和`255`之间的像素值矩阵或矩阵组成。要用我们的神经网络处理这些数据，我们需要将这些值转换为介于`0`和`1`之间的浮点数。如下所示的代码，我们将使用`normalize()`便捷函数来实现这一结果，并使用`range()`函数来测试这些值现在是否在`0`和`1`之间：
- en: '[PRE12]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After running this code, we may not see a noticeable change in the Environment
    pane for our data objects; however, when we run the `range()` function, we can
    see that all values are now between `0` and `1`. The output to your console after
    running the `range()` function on the data object will look like this:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，我们可能看不到环境窗格中数据对象的明显变化；然而，当我们运行`range()`函数时，我们可以看到所有值现在都在`0`和`1`之间。在对数据对象运行`range()`函数后，您的控制台输出将如下所示：
- en: '![](img/571d3c33-fc60-4bf0-8940-fddec74afffa.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/571d3c33-fc60-4bf0-8940-fddec74afffa.png)'
- en: 'Now we can begin to train our model using the `keras` syntax. In the following
    code, we begin by declaring that we will be creating a sequential model, which
    means that data will pass through each subsequent layer in turn:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以开始使用`keras`语法来训练我们的模型。在下面的代码中，我们首先声明我们将创建一个序列模型，这意味着数据将依次通过每个后续层：
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Running the preceding code initiates the model object; however, it doesn''t
    contain any data yet. You can see this in your Environment pane, which will look
    like this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码会初始化模型对象；然而，它尚未包含任何数据。您可以在环境窗格中看到这一点，它将如下所示：
- en: '![](img/4029ead7-1740-456c-ba08-99a76b13cb69.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4029ead7-1740-456c-ba08-99a76b13cb69.png)'
- en: 'The steps we take next are the final steps of a convolutional neural network
    and represent the part of the process where the data is converted in such a way
    that it can be processed by a traditional neural network. The first step will
    be to define the layer that takes the data from a large array of matrices and
    transforms the data so that all values for a given image are in one single row.
    To do this, we use the `layer_flatten()` function and pass the matrix shape in
    as an argument, as shown here:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来我们要进行的步骤是卷积神经网络的最后步骤，代表了将数据转换为传统神经网络可以处理的方式的过程的一部分。第一步将是定义一个层，该层将从大量的矩阵中获取数据并将数据转换，使得给定图像的所有值都在一个单独的行中。为了做到这一点，我们使用`layer_flatten()`函数，并将矩阵形状作为参数传递，如下所示：
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The preceding code and the remainder of the steps that define the model will
    not cause a noticeable change in your R environment. This step adds a layer to
    the model object, though. This layer flattens our data so that the data for every
    image will be contained in a single row.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码和定义模型的其余步骤不会导致您的R环境中出现明显变化。此步骤向模型对象添加了一个层。该层将我们的数据展平，使得每个图像的数据都包含在一个单独的行中。
- en: 'We will include one hidden layer as we have done previously. The way this is
    done in `keras` is to use the `layer_dense` function. We then note how many units
    we would like our hidden layer to have and the activation function that should
    be used to decide whether the signal from a unit should feed forward. In this
    case, we select a unit count that is approximately one-third the size of our total
    number of independent variable columns and **rectified linear units** (**ReLU**)
    as our activation function:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将像之前一样包括一个隐藏层。在`keras`中实现这一点的方法是使用`layer_dense`函数。接着，我们指定隐藏层的单元数量，以及用于决定某个单元的信号是否应该传递的激活函数。在这种情况下，我们选择的单元数量大约是独立变量列总数的三分之一，并选择**整流线性单元**（**ReLU**）作为我们的激活函数：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Again, this step produces no output or noticeable change to the environment.
    This adds one dense, fully connected layer to the model. The layer will contain
    256 units or neurons. It will use the ReLU function to determine which signals
    get sent forward.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这一步也不会在环境中产生输出或显著变化。这将向模型中添加一个密集的全连接层，该层将包含256个单元或神经元，并将使用ReLU函数来确定哪些信号被传递。
- en: 'In the following code, we have 10 possible target classes, so we include 10
    units in our output layer, one for each class. Since this is a multinomial classification
    problem, we use the `softmax` function to compute the probabilities that any given
    set of image data belongs to one of the 10 classes of clothing:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下代码中，我们有10个可能的目标类别，因此我们在输出层中包含了10个单元，每个类别对应一个单元。由于这是一个多项分类问题，我们使用`softmax`函数来计算任何给定的图像数据属于10个服装类别中的一个的概率：
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This line of code will add the last layer to this current neural network. This
    will be our output layer, which is another dense, fully connected layer where
    units are equal to the number of target classes. The `softmax` function will be
    used at this step to determine the probability that a given set of data belongs
    to each of the possible target classes. This step again produces no output or
    change in the R environment.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这一行代码将向当前神经网络中添加最后一层。这将是我们的输出层，另一个密集的全连接层，其中单元数等于目标类别的数量。在这一步中，将使用`softmax`函数来确定给定数据集属于每个可能目标类别的概率。此步骤同样不会在R环境中产生输出或变化。
- en: 'Before moving on to the `compile` step, we need to convert our target vectors
    to a matrix, which can be done simply with the `to_categorical` functions using
    the following code:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在进入`compile`步骤之前，我们需要将目标向量转换为矩阵，这可以通过以下代码使用`to_categorical`函数简单实现：
- en: '[PRE17]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After running the code, we will see a change in our Environment pane. The `target`
    variable objects that were vectors are now matrices where every row contains a
    single value equal to `1` at the index point for the class to which it belongs.
    Your Environment pane will now look like this:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，我们将在环境窗格中看到变化。之前是向量的`target`变量对象现在变成了矩阵，每行包含一个值，等于该行所属类别的索引点处的`1`。您的环境窗格现在将如下所示：
- en: '![](img/bd5e7a69-cb31-46d1-84fa-83cf040811eb.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd5e7a69-cb31-46d1-84fa-83cf040811eb.png)'
- en: This step is a requirement for training a multinominal classification model
    with `keras`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步是使用`keras`训练多项分类模型的要求。
- en: 'The following code is used to define the arguments for the `compile` step.
    Here, we select the `optimizer`, `loss`, and `evaluation` metrics. The `optimizer`
    is the algorithm that calculates the error rate between the model results and
    the actual values to adjust weights. We define the compile portion of the model
    using the following code:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码用于定义`compile`步骤的参数。在这里，我们选择了`optimizer`、`loss`和`evaluation`指标。`optimizer`是计算模型结果与实际值之间误差率的算法，用于调整权重。我们通过以下代码定义模型的编译部分：
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the preceding code, we did the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们做了以下操作：
- en: We chose **Adaptive Moment Estimation** (**Adam**). The loss function is the
    formula that is used to calculate the error rate.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们选择了**自适应矩估计**（**Adam**）。损失函数是用于计算误差率的公式。
- en: When working with a multinominal classification problem like this one, the most
    suitable loss function is categorical crossentropy, which is another term for
    multiclass log loss.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理像这样的多项分类问题时，最合适的损失函数是类别交叉熵，它是多类对数损失的另一种说法。
- en: In order to use this loss function, the target variable must be stored as a
    matrix, which is why we performed that data type conversion in the previous step.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了使用这个损失函数，目标变量必须以矩阵形式存储，这也是我们在上一步进行数据类型转换的原因。
- en: The metrics argument stores the measurement for evaluating model performance,
    and we used categorical accuracy.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: metrics 参数存储用于评估模型性能的度量值，我们使用了类别准确率。
- en: 'The step to fit our model at this point requires just three arguments. We pass
    in the training dataset, the target variables of the training data, and the number
    of times we would like the model to run. As shown in the following code, we will
    choose `10` epochs at this point so that our model finishes running quickly; however,
    if you have time to run the model for longer, then you will likely get better
    results. Train a neural network on the train data with the following code:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，拟合模型的步骤只需要三个参数。我们传入训练数据集、训练数据的目标变量以及希望模型运行的次数。如下面的代码所示，我们此时选择 `10` 次周期，以便模型能快速运行；然而，如果您有时间让模型运行更长时间，您可能会得到更好的结果。使用以下代码在训练数据上训练神经网络：
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'When we run this code, we will get a printout to our console with the results
    from every epoch. The output to your console will look like this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这段代码时，控制台将打印出每个时期的结果。控制台输出将如下所示：
- en: '![](img/6662fc0f-2302-4c51-a4ee-9b98345b52ba.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6662fc0f-2302-4c51-a4ee-9b98345b52ba.png)'
- en: 'In addition to the console output, the preceding code also produces a plot,
    which is a graphical representation of the same information. In your **Viewer**
    pane, you will see a plot like this:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 除了控制台输出外，前面的代码还生成了一张图，这是相同信息的图形表示。在您的**Viewer**窗格中，您将看到类似这样的图：
- en: '![](img/521d7502-2d54-4344-ae13-f8f7efdc7556.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/521d7502-2d54-4344-ae13-f8f7efdc7556.png)'
- en: The two pieces of output show that our model improves a lot at first and then
    improves at a much slower pace after additional iterations of the model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这两部分输出显示，我们的模型最初提升很大，随后在模型额外迭代后提升速度变得很慢。
- en: 'After this, we can run our model on our test dataset and use the test target
    variables to evaluate our model, as shown in the following screenshot. We can
    calculate the loss and categorical accuracy metric and print out the categorical
    accuracy metric by running the following code:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此之后，我们可以在测试数据集上运行模型，并使用测试目标变量来评估我们的模型，如下截图所示。我们可以通过运行以下代码来计算损失和类别准确率度量，并打印出类别准确率度量：
- en: '[PRE20]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You should see the following printed out to your console:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该会看到以下内容打印到控制台：
- en: '![](img/1b1b03f4-e508-4c57-bd38-3025041eb042.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b1b03f4-e508-4c57-bd38-3025041eb042.png)'
- en: We have achieved a categorical accuracy of 88.66%. This is a decrease from our
    previous accuracy; however, keep in mind that the accuracy metric before pertained
    to a binary response and this describes predictions for all classes.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经达到了 88.66% 的类别准确率。这比我们之前的准确率有所下降；然而，请记住，之前的准确率度量是针对二元响应的，而这个度量描述的是所有类别的预测。
- en: 'Making predictions using the model is achieved through the `predict()` function.
    In the following code, `predict_classes` can be used to choose one of the 10 classes
    that are most likely based on the probability scores, while `preds` will calculate
    the probabilities:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型进行预测是通过 `predict()` 函数实现的。在以下代码中，`predict_classes` 可以用来选择基于概率分数最有可能的 10
    个类别中的一个，而 `preds` 将计算这些概率：
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After running the preceding code, we can see the difference in our **Environment**
    pane, which contains two new data objects. Your **Environment** pane will look
    like this:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码后，我们可以在**Environment**窗格中看到差异，其中包含两个新的数据对象。您的**Environment**窗格将如下所示：
- en: '![](img/cea0f813-d0e8-4ede-8c1a-66453262c63e.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cea0f813-d0e8-4ede-8c1a-66453262c63e.png)'
- en: We can see that `preds` is a large matrix with a probability value for each
    class, while `predicted_classes` is a vector where every value indicates the most
    probable class for each case.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，`preds` 是一个大型矩阵，包含每个类别的概率值，而 `predicted_classes` 是一个向量，其中每个值表示每个案例最可能的类别。
- en: 'Finally, we can use a confusion matrix to review our results. In order to run
    this code, we will need our test target labels back in vector format rather than
    in a matrix. To do this, we will just read in the test target file again, as shown
    here:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以使用混淆矩阵来回顾我们的结果。为了运行这段代码，我们需要将测试目标标签以向量格式返回，而不是矩阵格式。为此，我们只需再次读取测试目标文件，如下所示：
- en: '[PRE22]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Running the code will produce an evaluation metric output to print to our console.
    Your console will look like this:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码将生成一个评估度量输出并打印到控制台。您的控制台将如下所示：
- en: '![](img/d22457d1-da11-46f3-b2ce-10a6576b3e33.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d22457d1-da11-46f3-b2ce-10a6576b3e33.png)'
- en: Our accuracy score has actually decreased from 97.487% to 88.66%. This is in
    part due to limiting our model to 10 runs and also, again, because we are now
    building a classifier for 10 categories, whereas the other score was achieved
    with a binary classifier. At this point, improving the accuracy score is not the
    priority in any case. The preceding code is here to show how to code a neural
    network using the `keras` syntax.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的准确率实际上从97.487%下降到了88.66%。这部分是由于我们将模型的运行次数限制为10次，同时，也因为我们现在正在为10个类别构建分类器，而另一个得分是通过二分类器得到的。此时，改善准确率并不是首要任务。前面的代码只是用来展示如何使用`keras`语法编写神经网络。
- en: In the preceding code, during the compile stage, we chose an optimizer, loss
    function, and evaluation metric; however, we should note the other options that
    we could have selected and the difference between the available choices. We'll
    look at these in the following sections.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，在编译阶段，我们选择了一个优化器、损失函数和评估指标；然而，我们应该注意其他可选项以及它们之间的区别。我们将在接下来的部分中讨论这些内容。
- en: Optimizers
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化器
- en: 'Let''s look at the following optimizers:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看以下优化器：
- en: '**Stochastic gradient descent** (**SGD**): This is the simplest optimizer.
    For every weight, the model stores an error rate. Depending on the direction of
    the error, whether the predicted value is greater than or less than the true value,
    a small learning rate is applied to the weight to change the next round of results
    so that the predicted values are incrementally moving in the opposite direction
    as the error. This process is simple, but for deep neural networks, the fine adjustments
    at every iteration mean that it can take a long time for the model to converge.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降**（**SGD**）：这是最简单的优化器。对于每个权重，模型会存储一个错误率。根据错误的方向（预测值是大于还是小于真实值），会应用一个小的学习率来调整权重，从而使下一轮结果的预测值朝着与错误方向相反的方向逐步调整。这个过程很简单，但对于深度神经网络来说，每次迭代的微调意味着模型可能需要很长时间才能收敛。'
- en: '**Momentum**: Momentum itself is not an optimizer, but is something that can
    be incorporated with different optimizers. The idea of momentum is that it takes
    the decaying average of all the corrections that have occurred previously and
    uses these in combination with the correction calculated for the current move.
    For this, we can imagine momentum working just like it would if an object was
    rolled down a hill and then continued to roll up another hill using its own momentum.
    For every movement that it made, there would be a force pulling it back down the
    hill; however, for a period of time, the momentum to continue up the hill would
    be stronger than the force pulling it back down.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动量**：动量本身并不是一个优化器，而是可以与不同优化器结合使用的一个方法。动量的概念是，它会取所有之前发生的修正的衰减平均值，并将这些值与当前步伐的修正结合使用。为此，我们可以想象动量就像一个物体从山坡上滚下来，然后利用自身的动量继续滚上另一座山。每一次的移动都会有一个力将物体拉回山坡下；然而，在一段时间内，继续上山的动量会强于将物体拉下山坡的力。'
- en: '**Adagrad/Adadelta**: Adagrad improves on SGD through the use of a matrix of
    all previous errors per node rather than one error rate for all nodes; however,
    the use of this matrix of values means that the learning rate can be canceled
    out of the model runs for too long. Adadelta is the correction for the limitation
    of Adagrad. Adadelta uses momentum to solve the growing error rate matrix problem.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Adagrad/Adadelta**：Adagrad通过使用每个节点所有先前错误的矩阵来改进SGD，而不是为所有节点使用统一的错误率；然而，使用这个值矩阵意味着学习率可能会在模型运行时间过长时被取消。Adadelta是对Adagrad局限性的修正。Adadelta利用动量来解决增长的错误率矩阵问题。'
- en: '**RMSprop**: This is a separate correction to Adagrad. It is similar to Adadelta,
    however, with RMSprop, the learning rate is not only divided by a matrix of local
    per-node decaying average error rates, but also by a global decaying average of
    all squared error rates.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RMSprop**：这是对Adagrad的另一种修正。它与Adadelta类似，然而，RMSprop不仅通过每个节点的局部衰减平均错误率矩阵来除以学习率，还通过所有平方错误率的全局衰减平均来除以学习率。'
- en: '**Adam**: This algorithm further corrects and builds upon the algorithms before.
    Adam includes not only an exponentially decaying average of squared error rates,
    as seen in RMSprop, but also an exponentially decaying average of error rates
    (not squared), which is momentum. In this way, Adam is more or less RMSprop with
    momentum. The combination of the two means that there is a correction to momentum
    similar to friction, which decreases the effect of momentum and thereby leads
    to faster convergence, in many cases making Adam a popular optimizer at the time
    of writing.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Adam**：该算法进一步修正并构建在之前的算法基础上。Adam不仅包含RMSprop中看到的平方误差率的指数衰减平均值，还包含误差率（未平方）的指数衰减平均值，这就是动量。通过这种方式，Adam或多或少就是带有动量的RMSprop。两者的结合意味着存在类似于摩擦的动量修正，这减少了动量的效果，从而在许多情况下加速了收敛过程，使Adam成为撰写本文时非常流行的优化器。'
- en: Loss functions
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'With the preceding optimizers, you can select whichever one you like and try
    different algorithms to see which produces the best results. With the loss function,
    there are some choices that are more appropriate than others based on the problem
    being solved:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前述优化器时，你可以选择任意一个并尝试不同的算法，看看哪一个产生最好的结果。在损失函数方面，根据要解决的问题，有一些选择比其他选择更为合适：
- en: '`binary_crossentropy`: This loss function is used for classification problems
    where the requirement is to assign input data to one of two classes. This can
    also be used when assigning input data to more than one or two classes if it is
    possible for a given case to belong to more than one class. In this case, each
    target class is treated as a separate binary class (for each target class the
    given case belongs to or does not belong to). This can also be used for regression
    when the target values are between 0 and 1.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_crossentropy`：该损失函数用于分类问题，要求将输入数据分配给两个类别之一。如果输入数据可以属于多个类别而不仅仅是两个类别，这也可以使用。当一个案例可能属于多个类别时，每个目标类别将被视为一个独立的二元类别（针对每个目标类别，该案例是否属于该类别）。如果目标值在0和1之间，也可以用于回归问题。'
- en: '`categorical_crossentropy`: Categorical crossentropy is used with a multinominal
    classification problem where any given row of input data can only belong to one
    of the more than two classes. As we noted with binary crossentropy, in order to
    use categorical crossentropy, the target vector must be converted to a matrix
    so that there is a value of 1 for the index associated with the target class for
    every given row. If the target vector contains integers that are meant to be treated
    as integers and not categorical classes, then `sparse_categorical_crossentropy`
    can be used without performing the matrix conversion required for categorical
    crossentropy.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`categorical_crossentropy`：分类交叉熵用于多类别分类问题，其中每一行输入数据只能属于两个以上的类别之一。如我们在二元交叉熵中提到的，为了使用分类交叉熵，目标向量必须转换为矩阵，以便每一行中与目标类别关联的索引的值为1。如果目标向量包含应当被视为整数而非分类类别的整数，则可以使用`sparse_categorical_crossentropy`，无需执行分类交叉熵所需的矩阵转换。'
- en: 'MSE (**mean squared error**): Mean squared error is an appropriate choice for
    regression problems since the predicted values and true values can be any number.
    This loss function takes the square of the difference between the predicted values
    and the target values.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MSE (**均方误差**)：均方误差是回归问题的适当选择，因为预测值和真实值可以是任意数字。该损失函数计算预测值与目标值之间差异的平方。
- en: Evaluation metrics
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估指标
- en: 'The evaluation metric is used to measure model performance. While similar to
    the loss functions, it is not used for making corrections while training the model.
    It is only used after the model has been trained to evaluate performance:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标用于衡量模型的表现。虽然它与损失函数类似，但并不用于在训练过程中进行修正。它只在模型训练完成后用于评估性能：
- en: '**Accuracy**: This metric measures how often the correct class is predicted.
    By default, 0.5 is used as a threshold, which means that if the predicted probability
    is below 0.5, then the predicted class is 0; otherwise, it is 1\. The total number
    of cases where the predicted class matches the target class is divided by the
    total number of target variables.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确度**：该度量衡量预测正确类别的频率。默认情况下，使用0.5作为阈值，这意味着如果预测概率低于0.5，则预测类别为0；否则，预测类别为1。预测类别与目标类别匹配的案例总数除以目标变量的总数。'
- en: '**Cosine similarity**: Compares the similarity between two vectors by evaluating
    the similarity of terms in *n*-dimensional space. This is used often to evaluate
    the similarity of text data. For this, we can imagine a piece of text with the
    word cat four times and the word dog once, and another with the word cat four
    times and the word dog twice. In this case, with two dimensions, we could envision
    a line from the origin through the point where *y* = 4 and *x* = 1 for the first
    piece of text and another line through the point where *y* = 4 and *x* = 2 for
    the next piece of text. If we evaluate the angle between the two lines, we will
    arrive at the cosine value used to determine similarity. If the lines overlap,
    then the documents have a perfect similarity score of 1, and the larger the angle
    between the lines, the less similar and the lower the similarity score will be.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**余弦相似度**：通过评估* n *维空间中术语的相似性，比较两个向量之间的相似性。这通常用于评估文本数据的相似性。为此，我们可以想象一段文本中“cat”一词出现四次，“dog”一词出现一次，另一段文本中“cat”一词出现四次，“dog”一词出现两次。在这种情况下，在二维空间中，我们可以设想一条从原点到达*
    y * = 4 和* x * = 1的点的直线，代表第一段文本；而另一条从原点到达* y * = 4 和* x * = 2的点的直线，代表第二段文本。如果我们评估这两条直线之间的角度，就可以得出用于判断相似性的余弦值。如果两条直线重合，则文档具有完美的相似度分数
    1，角度越大，相似度越低，分数也会越低。'
- en: '**Mean absolute error**: The average value for all absolute errors. The absolute
    error is the difference between the predicted variable and the target variable.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均绝对误差**：所有绝对误差的平均值。绝对误差是预测变量与目标变量之间的差异。'
- en: '**Mean squared error**: The average value for all squared errors. The squared
    error is the square of the difference between the predicted variable and the target
    variable. Through squaring the errors, an increased penalty is applied to larger
    errors relative to mean absolute error.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差**：所有平方误差的平均值。平方误差是预测变量与目标变量之间差异的平方。通过对误差进行平方，相较于平均绝对误差，较大的误差将受到更大的惩罚。'
- en: '**Hinge**: To use the hinge evaluation metric, all target variables should
    be -1 or 1\. From here, the formula is to subtract the product of the predicted
    value and the target variable from 1 and then to use this value or 0, whichever
    is greater for evaluation. Results are evaluated as more correct the closer the
    metric value is to 0.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**铰链损失**：使用铰链评估指标时，所有目标变量应为-1或1。公式是从1中减去预测值与目标变量的乘积，然后使用该值和0中较大的一个来进行评估。结果越接近0，说明评估的正确性越高。'
- en: '**KL divergence**: This metric compares the distribution of true results with
    the distribution of predicted results and evaluates the similarity of the distribution.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KL 散度**：该指标比较真实结果的分布与预测结果的分布，并评估两者分布的相似性。'
- en: We have, so far, used a tree-based classifier and a traditional neural network
    to classify our image data. We have also reviewed the `keras` syntax and looked
    at our options for several functions within the modeling pipeline using this framework.
    Next, we will add the additional layers before the neural network that we just
    coded to create a convolutional neural network. For this special type of neural
    network, we will include a convolution layer and a pooling layer. A dropout layer
    is often also included; however, we will add this later as it serves a slightly
    different purpose than the convolution and pooling layers. When used together,
    these layers find more complex patterns in our data and also reduce the size of
    our data, which is especially important when working with large image files.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用了基于树的分类器和传统神经网络来对图像数据进行分类。我们还回顾了`keras`语法，并探讨了在使用该框架时，建模管道中几个函数的选择。接下来，我们将在刚刚编写的神经网络之前添加额外的层，来创建卷积神经网络。对于这种特殊类型的神经网络，我们将包括卷积层和池化层。通常还会包括一个丢弃层；不过我们稍后再添加它，因为它与卷积层和池化层的功能稍有不同。当这些层一起使用时，它们可以在数据中找到更复杂的模式，并且还可以减小数据的大小，这在处理大型图像文件时尤为重要。
- en: Enhancing the model with additional layers
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增强模型，添加额外的层
- en: 'In this section, we add two important layers: the convolution layer, and pooling
    layer:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们添加了两个重要的层：卷积层和池化层：
- en: 'Before beginning, we make one small change to the data structure. We will add
    a fourth dimension that is a constant value. We add the extra dimension using
    the following code:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开始之前，我们对数据结构进行了一些小的调整。我们将添加一个常数值作为第四个维度。我们使用以下代码添加这个额外的维度：
- en: '[PRE23]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'When we make this change, we can see the added dimension for these data objects
    in the Environment pane, which will look like the following image:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进行此更改时，可以在环境窗格中看到这些数据对象的新增维度，输出将类似于以下图像：
- en: '![](img/807acb3e-c10c-41d4-a945-8d26d60f87c1.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/807acb3e-c10c-41d4-a945-8d26d60f87c1.png)'
- en: We make this change to the structure because it is a requirement of modeling
    a CNN using `keras`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对结构进行此更改，是因为这是使用`keras`建模CNN的要求。
- en: 'As before, the first step in the modeling process is to establish that we will
    be building a sequential model by calling the `keras_model_sequential()` function
    with no arguments using the following code:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，建模过程的第一步是通过调用`keras_model_sequential()`函数并不传递任何参数来确定我们将构建一个顺序模型，代码如下：
- en: '[PRE24]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Running the preceding code will place the model object into our Environment
    pane, however, it contains no data at the moment and no output is printed to console.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码将把模型对象放入我们的环境窗格中，但目前它没有数据，也不会在控制台上输出任何内容。
- en: 'Next, we will add our convolving layer for a two-dimensional object. In the
    following code, we will decide on the number of filters or subsets of the data
    and the size of these subsets as well as the activation function used to determine
    whether the subset contains a certain pattern:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将为二维对象添加卷积层。在以下代码中，我们将决定滤波器的数量或数据子集的数量，以及这些子集的大小，并选择激活函数来判断子集是否包含某种模式：
- en: '[PRE25]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let''s looks at the preceding code in a little more detail:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下前面的代码：
- en: We chose to include 484 filters and a kernel size that is 7 pixels high and
    wide.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们选择包含484个滤波器，并且内核的大小为7个像素高和宽。
- en: We applied a filter the size of the kernel on the image and, using the constant
    values on the filter, the model determined whether it detects a pattern.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在图像上应用了与内核大小相同的滤波器，并且使用滤波器上的常量值，模型决定是否检测到模式。
- en: The stride value of 1 that is used in this example means that the kernel slides
    over the surface of the image by 1 pixel after every filter has been evaluated,
    though this number can be changed.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本例中使用的步幅值为1，这意味着每次滤波器评估后，内核会在图像表面滑动1个像素，尽管这个数字是可以改变的。
- en: 'After this, we will use a max pooling layer in the following code to create
    a new downsampled version of our data that represents the maximum values within
    the pool size after the convolving round:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们将在以下代码中使用最大池化层，创建数据的新下采样版本，该版本代表卷积轮次后池化大小内的最大值：
- en: '[PRE26]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This line of code causes no noticeable changes to the Environment pane and prints
    no output to the console. It adds a layer to our model that will reduce the size
    of our data to one-quarter of the original size.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码对环境窗格没有明显变化，也没有向控制台打印输出。它向我们的模型中添加了一个层，这个层将数据的大小减少到原始大小的四分之一。
- en: 'After this, the steps continue in the same way as we described in the previous
    example from the *Image recognition with convolutional neural networks* section.
    Let''s have a look at the following code:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此后，步骤将继续按照我们在*卷积神经网络图像识别*部分中描述的方式进行。让我们看一下以下代码：
- en: '[PRE27]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'After running the preceding code, we will get model diagnostic data printed
    to the console, which will look like the following image:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码后，我们将在控制台上看到模型诊断数据，输出将类似于以下图像：
- en: '![](img/60457c3a-0a74-4e39-b620-7048c639f12a.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60457c3a-0a74-4e39-b620-7048c639f12a.png)'
- en: 'You will also get a plot with the same data, which looks like the following
    image:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 您还将获得一个包含相同数据的图表，输出将类似于以下图像：
- en: '![](img/f4815302-74f6-4740-bdfc-b3e23911ce91.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4815302-74f6-4740-bdfc-b3e23911ce91.png)'
- en: 'The preceding code also produces performance metrics and prints them to the
    console. Your console will look like the following image:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码还生成了性能指标，并将其打印到控制台。您的控制台将显示如下图像：
- en: '![](img/b59da266-148f-460d-bcdf-e5c2a5bb9710.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b59da266-148f-460d-bcdf-e5c2a5bb9710.png)'
- en: By adding our convolution layer and our pooling layer, we have already increased
    our accuracy score from 88.66% to 90.51%, and we did so with five fewer rounds
    to prevent using a long-running model while learning.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加卷积层和池化层，我们已将准确度得分从88.66%提高到90.51%，而且我们减少了五轮，以避免使用长时间运行的模型进行学习。
- en: 'As discussed previously, deep learning occurs when there are many layers for
    the signal to pass through, so in the following method, we will see how we can
    continue to add more layers. Using this, you can continue to add as many layers
    as you need as you refine your model:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，深度学习发生在信号需要通过多个层时，因此在以下方法中，我们将看到如何继续添加更多的层。通过这种方法，您可以根据需要继续添加任意数量的层来优化您的模型：
- en: 'We start by redefining our model. We will state again that it is a sequential
    model in the following code:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从重新定义模型开始。在以下代码中，我们再次声明它是一个顺序模型：
- en: '[PRE28]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The preceding code will reset the model object; however, no noticeable changes
    will occur in the Environment pane and nothing will print to the console.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将重置模型对象；然而，在“环境”面板中不会出现明显的变化，也不会有任何内容打印到控制台。
- en: 'In the next step, we will use a convolving layer with 128 filters and then
    a pooling layer. In this case, we also add `padding = "same"` to prevent dimension
    reduction after the convolution layer. This is in order to make it possible to
    add additional layers. If we reduce the dimensionality of our data too quickly,
    then we will not be able to fit a filter of any kernel size on the data later.
    We define our first convolving layer and pooling layer using the following code:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一步中，我们将使用具有128个过滤器的卷积层，然后使用池化层。在此情况下，我们还添加了`padding = "same"`，以防止卷积层后维度发生缩减。这是为了使我们能够添加更多层。如果我们过快地减少数据的维度，那么以后就无法在数据上使用任何大小的过滤器。我们使用以下代码来定义第一个卷积层和池化层：
- en: '[PRE29]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The preceding code results in no noticeable changes to the Environment pane
    and nothing will print to the console.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码不会对“环境”面板产生明显变化，也不会在控制台中打印任何内容。
- en: 'As shown in the following code, we will add in another convolving layer with
    64 filters and a pooling layer. We add these two additional layers using the following
    code:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下代码所示，我们将添加另一个卷积层，使用64个过滤器，并添加一个池化层。我们使用以下代码添加这两个额外的层：
- en: '[PRE30]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The preceding code results in no noticeable changes to the Environment pane
    and nothing will print to the console.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码不会对“环境”面板产生明显变化，也不会在控制台中打印任何内容。
- en: 'We then add in one more convolving layer. This time, we will use 32 filters
    and a pooling layer, as shown in the following code. We add these two additional
    layers using the following code:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们再添加一个卷积层。这次，我们将使用32个过滤器和一个池化层，如以下代码所示。我们使用以下代码添加这两个额外的层：
- en: '[PRE31]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The preceding code results in no noticeable changes to the Environment pane
    and nothing will print to the console.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码不会对“环境”面板产生明显变化，也不会在控制台中打印任何内容。
- en: 'Finally, in the following code, we flatten the values and proceed in the same
    way as we have in the previous examples:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在以下代码中，我们将数据展平，并按照之前的示例继续操作：
- en: '[PRE32]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The preceding code produces model diagnostics in our console. Your console
    will look like the following image:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码会在控制台中生成模型诊断信息。您的控制台将显示以下图像：
- en: '![](img/42678786-19c3-41bd-9015-a133f90ad9b1.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/42678786-19c3-41bd-9015-a133f90ad9b1.png)'
- en: 'The preceding code also produces a plot. You will see a plot in your Viewer
    pane that looks like the following image:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码还生成了一个图表。您将在您的查看器面板中看到一个类似以下图像的图表：
- en: '![](img/7c853b96-fe00-4bee-9763-99c0241a4bcb.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c853b96-fe00-4bee-9763-99c0241a4bcb.png)'
- en: 'We also produce additional performance metrics and print these to our console.
    You will see the following output in your console:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还生成了额外的性能指标，并将其打印到我们的控制台。您将在控制台中看到以下输出：
- en: '![](img/159ab197-a120-4b36-b59d-88fffaa447d8.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/159ab197-a120-4b36-b59d-88fffaa447d8.png)'
- en: Our accuracy score remains almost the same at 89.97%. We could likely get the
    accuracy to improve with more filters per layer; however, this will significantly
    increase run time, so that is why we keep the filter count per layer as it is.
    We have seen here how to write code to create a CNN and a CNN with a deeper layer
    structure, and we noticed how it improved performance. We have so far used the
    ReLU activation function, which is a very popular and common activation function;
    however, we know there are other options, so next, we will write code to select
    a different activation function.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的准确率保持几乎不变，仍为89.97%。通过增加每层的过滤器数量，我们可能能够提高准确率；然而，这样做会显著增加运行时间，因此我们保持每层的过滤器数量不变。在这里，我们已经演示了如何编写代码来创建一个卷积神经网络（CNN）及其更深层结构，并注意到这种结构改善了性能。到目前为止，我们使用了ReLU激活函数，这是一种非常流行和常见的激活函数；然而，我们知道还有其他选项，因此接下来我们将编写代码来选择另一种激活函数。
- en: Choosing the most appropriate activation function
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择最合适的激活函数
- en: 'Using `keras`, you can use a number of different activation functions. Some
    of these have been discussed in previous chapters; however, there are some that
    have not been previously covered. We can begin by listing the ones we have already
    covered with a quick note on each function:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `keras`，你可以使用多种不同的激活函数。我们在前面的章节中已经讨论了一些激活函数；然而，还有一些是之前没有涉及到的。我们可以先列出已经覆盖的函数，并简要说明每个函数：
- en: '**Linear**: Also known as the identity function. Uses the value of *x*.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性**：也叫做恒等函数，使用 *x* 的值。'
- en: '**Sigmoid**: Uses 1 divided by 1 plus the exponent of negative *x*.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sigmoid**：使用 1 除以 1 加上负 *x* 的指数。'
- en: '**Hyperbolic tangent** (**tanh**): Uses the exponent of *x* minus the exponent
    of negative *x* divided by *x* plus the exponent of negative *x*. This has the
    same shape as the sigmoid function; however, the range along the *y*-axis goes
    from 1 to -1 instead of from 1 to 0.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双曲正切** (**tanh**)：使用 *x* 的指数减去负 *x* 的指数，再除以 *x* 加上负 *x* 的指数。这与 Sigmoid 函数具有相同的形状；然而，*y*
    轴上的范围是从 1 到 -1，而不是从 1 到 0。'
- en: '**Rectified Linear Units** (**ReLU**): Uses the value of *x* if *x* is greater
    than 0; otherwise, it assigns a value of 0 if *x* is less than or equal to 0.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**修正线性单元** (**ReLU**)：如果 *x* 大于 0，则使用 *x* 的值；否则，如果 *x* 小于或等于 0，则将值赋为 0。'
- en: '**Leaky ReLU**: Uses the same formula as ReLU; however, it applies a small
    alpha value when *x* is less than 0.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Leaky ReLU**：使用与 ReLU 相同的公式；然而，当 *x* 小于 0 时，它会应用一个小的 alpha 值。'
- en: '**Softmax**: Provides a probability for each possible target class.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Softmax**：为每个可能的目标类别提供一个概率。'
- en: 'Let''s look at all the functions that were not mentioned in previous chapters:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看在前几章中没有提到的所有函数：
- en: '**Exponential Linear Unit** (**ELU**): Uses the exponent of *x* - 1 multiplied
    by a constant alpha value if the value for *x* is less than 0'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指数线性单元** (**ELU**)：如果 *x* 小于 0，则使用 *x* 的指数减 1 乘以一个常数 alpha 值。'
- en: '**Scaled Exponential Linear Unit** (**SELU**): Uses the ELU function and then
    multiplies the result of the function by a constant scale value'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缩放指数线性单元** (**SELU**)：使用 ELU 函数，然后将该函数的结果乘以一个常数缩放值。'
- en: '**Thresholded ReLU**: Uses the same formula as ReLU; however, instead of using
    0 as the threshold for whether *x* is *x* or 0, it uses a user-defined value of
    theta to determine this threshold.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Thresholded ReLU**：使用与 ReLU 相同的公式；然而，它不是使用 0 作为 *x* 是否为 *x* 或 0 的阈值，而是使用用户定义的
    theta 值来确定这个阈值。'
- en: '**Parametric Rectified Linear Unit** (**PReLU**): The same as the formula for
    Leaky ReLu; however, it uses an array of values for alpha rather than a single
    value.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数化修正线性单元** (**PReLU**)：与 Leaky ReLU 的公式相同；然而，它使用一组值作为 alpha，而不是单一值。'
- en: '**Softplus**: Uses the log of the exponent of *x* plus 1'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Softplus**：使用 *x* 的指数加 1 的对数。'
- en: '**Softsign**: Uses *x* divided by the absolute value of *x* + 1'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Softsign**：使用 *x* 除以 *x* 的绝对值加 1。'
- en: '**Exponential**: Uses the exponent of *x*.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指数**：使用 *x* 的指数。'
- en: '**Hard Sigmoid**: Uses a modified and faster version of the sigmoid function.
    If *x* is less than -2.5, then the value is 0 and if *x* is greater than 2.5,
    then the value is 1; otherwise, it uses 0.2 * *x* + 0.5.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬 Sigmoid**：使用 Sigmoid 函数的修改版和更快版本。如果 *x* 小于 -2.5，则值为 0；如果 *x* 大于 2.5，则值为
    1；否则，使用 0.2 * *x* + 0.5。'
- en: 'So far, we have used the ReLU activation function by assigning the `relu` value to
    the activation argument within our layer function call. For some activation functions,
    such as the sigmoid function, we can simply swap out the `relu` value with the
    `sigmoid` value; however, more advanced activation functions require a separate
    activation function layer. Let''s switch the activation function from ReLu to
    Leaky ReLU in our code by removing the activation argument from the layer function
    call and adding a Leaky ReLU activation function:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经通过在层函数调用中将 `relu` 值赋给激活参数来使用 ReLU 激活函数。对于某些激活函数，如 Sigmoid 函数，我们可以简单地将
    `relu` 值替换为 `sigmoid` 值；然而，更复杂的激活函数需要一个单独的激活函数层。让我们通过从层函数调用中移除激活参数并添加一个 Leaky
    ReLU 激活函数，将激活函数从 ReLU 切换为 Leaky ReLU：
- en: '[PRE33]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding code prints model diagnostic data to our console. Your console
    will look like the following image:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码会将模型诊断数据打印到我们的控制台。你的控制台将显示如下图像：
- en: '![](img/bf800383-b76f-44e2-a9a5-46f2af9382ae.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf800383-b76f-44e2-a9a5-46f2af9382ae.png)'
- en: 'The preceding code also produces a plot with model performance data. You will
    see a plot like the following image in your Viewer pane:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码还会生成一个包含模型性能数据的图表。你将在查看器面板中看到类似下图的图表：
- en: '![](img/a6296662-30bb-4780-86ab-43046649d7a0.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6296662-30bb-4780-86ab-43046649d7a0.png)'
- en: 'The preceding code also produces performance metrics. These will print to your
    console and will look like the following image:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码还会生成性能指标。输出打印到控制台时，会显示如下图所示的内容：
- en: '![](img/51e4f360-e562-45b7-ac40-12e7717e4299.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/51e4f360-e562-45b7-ac40-12e7717e4299.png)'
- en: After switching our activation function, our accuracy score is 90.95%, which
    is very similar to the score we have been getting so far. In this case, switching
    our activation function did not improve performance; however, there will be times
    when this will be helpful, so it is important to know how to make this modification.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在切换激活函数后，我们的准确率为90.95%，这与我们之前的得分非常相似。在这种情况下，切换激活函数并没有提高性能；然而，有时候这种方法会有所帮助，因此了解如何进行这种修改非常重要。
- en: Selecting optimal epochs using dropout and early stopping
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用dropout和早停选择最佳epochs
- en: 'To avoid overfitting, we can use two techniques. The first is adding a dropout
    layer. The dropout layer will remove a subset of the layer output. This makes
    the data a little different at every iteration so that the model generalizes better and
    doesn''t fit the solution too specifically to the training data. In the preceding
    code, we add the dropout layer after the pooling layer:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免过拟合，我们可以使用两种技术。第一种是添加dropout层。dropout层将移除一部分层的输出。这使得每次迭代的数据略有不同，从而使模型更好地泛化，而不是过于专门地拟合训练数据。在前面的代码中，我们在池化层之后添加了dropout层：
- en: '[PRE34]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The preceding code prints model diagnostic data to our console. Your console
    will look like the following image:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码会将模型诊断数据输出到我们的控制台。你的控制台将显示如下图所示的内容：
- en: '![](img/9d0947cd-aa5b-416b-a197-f82689a0f31a.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d0947cd-aa5b-416b-a197-f82689a0f31a.png)'
- en: 'The preceding code also produces a plot with model performance data. You will
    see a plot like the following image in your Viewer pane:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码还会生成一个包含模型性能数据的图表。你将在查看面板中看到如下图所示的图表：
- en: '![](img/ec8e5b34-0189-4414-ade2-b328f32a771e.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec8e5b34-0189-4414-ade2-b328f32a771e.png)'
- en: 'The preceding code also produces some performance metrics. The output printed
    to your console will look like the following image:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码还会生成一些性能指标。输出到控制台的内容会显示如下图所示：
- en: '![](img/7db9be8b-473b-4e95-98ab-7cc34fd33542.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7db9be8b-473b-4e95-98ab-7cc34fd33542.png)'
- en: In this case, using this tactic caused our accuracy score to decrease slightly
    to 90.07%. This could be due to working with a dataset of such small images that
    the model suffered from the removal of data at every step. With larger datasets,
    this will likely help make models more efficient and generalize better.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，使用这一策略导致我们的准确率略微下降，降至90.07%。这可能是因为我们处理的数据集包含的是非常小的图像，模型在每次移除数据时受到了影响。对于更大的数据集，这种方法可能有助于提高模型效率并使其更好地泛化。
- en: The other tactic for preventing overfitting is using early stopping. Early stopping
    will monitor progress and stop the model from continuing to train on the data
    when the model no longer improves. In the following code, we have set the patience
    argument to `2`, which means that the evaluation metric must fail to improve for
    two epochs in a row.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 防止过拟合的另一种策略是使用早停。早停会监控进展，并在模型不再改进时停止继续训练数据。在以下代码中，我们将`patience`参数设置为`2`，这意味着评估指标必须连续两个epoch没有改进。
- en: 'In the following code, with epochs kept at `5`, the model will complete all
    five rounds; however, if you have time, feel free to run all the preceding code
    and then, when it is time to fit the model, increase the number of epochs to see
    when the early stopping function stops the model. We add early stopping functionality
    to our model using the following code:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，保持`5`个epochs，模型将完成所有五轮；然而，如果你有时间，可以运行之前的所有代码，然后在拟合模型时，将epochs的数量增加，观察早停函数何时停止模型。我们通过以下代码将早停功能添加到模型中：
- en: '[PRE35]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'From the preceding code, we can deduce the following:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以推断出以下内容：
- en: Using dropout and early stopping, we demonstrated methods for discovering the
    optimal number of epochs or rounds for our model.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用dropout和早停，我们展示了发现模型最佳epochs或轮次的方法。
- en: When thinking of the number of epochs we should use, the goal is twofold.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在考虑我们应该使用多少epochs时，目标是双重的。
- en: We want our model to run efficiently so that we are not waiting for our model
    to finish running, even though the additional rounds are not improving performance.
    We also want to avoid overfitting where a model is trained too specifically on
    the training data and is unable to generalize to new data.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望我们的模型能够高效运行，这样即使额外的训练轮次没有提高性能，我们也不需要等待模型完成训练。我们还希望避免过拟合问题，即模型在训练数据上训练得过于具体，无法泛化到新的数据。
- en: Dropout helps with the second issue by arbitrarily removing some data on each
    round. It also introduces randomness that prevents the model from learning too
    much about the whole dataset.
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout（丢弃法）通过在每一轮训练中随机移除部分数据来解决第二个问题。它还引入了随机性，防止模型对整个数据集学习过度。
- en: Early stopping helps with the first problem by monitoring performance and stopping
    the model when the model is no longer producing better results for a given length
    of time.
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提前停止（Early stopping）通过监控性能来解决第一个问题，当模型在一定时间内不再产生更好的结果时，停止模型训练。
- en: Using these two techniques together will allow you to find the epoch number
    that allows the model to run efficiently and also generalize well.
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这两种技术结合使用，可以帮助你找到一个合适的训练轮次，使模型高效运行并且具有较好的泛化能力。
- en: Summary
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started by showing how image classification models can be
    created using standard machine-learning techniques; however, this has limitations
    as the images get larger and more complex. We can use convolutional neural networks
    to combat this issue. Using this approach, we demonstrated how we could perform
    dimensionality reduction and make it more computationally efficient to train a
    classification model on image data. We built a model with one convolution and
    pooling layer and then showed how we could make the model even deeper by adding
    further layers. Lastly, we used dropout layers and early stopping to avoid overfitting
    our model. Using all of these tactics in concert, we are now able to build models
    for classifying any type of image data.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们首先展示了如何使用标准的机器学习技术创建图像分类模型；然而，随着图像尺寸的增大和复杂性的增加，这种方法会受到限制。为了解决这个问题，我们可以使用卷积神经网络（CNN）。通过这种方法，我们演示了如何进行降维处理，并提高训练图像数据分类模型的计算效率。我们构建了一个包含卷积层和池化层的模型，并展示了如何通过添加更多层来加深模型。最后，我们使用了丢弃法和提前停止策略来避免模型的过拟合。将这些方法结合使用，我们现在能够构建适用于任何类型图像数据的分类模型。
- en: In the next chapter, we will learn how to code a multilayer perceptron. The
    multilayer perceptron is a feedforward neural network that includes only dense,
    fully connected hidden layers. With fewer options to adjust, we will take a deeper
    dive into what is available for us to tune. In addition, we will use the MXNet
    package for the first time, aside from the brief introduction in [Chapter 2](b614d2ff-1494-4a53-bd21-3c5d108be87c.xhtml),
    *CNNs for Image Recognition*.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何编写一个多层感知器（Multilayer Perceptron）。多层感知器是一种前馈神经网络，只有稠密的完全连接的隐藏层。由于调节选项较少，我们将更深入地探讨可以调整的内容。此外，除了在[第2章](b614d2ff-1494-4a53-bd21-3c5d108be87c.xhtml)中的简要介绍外，我们将首次使用MXNet包，*卷积神经网络（CNN）用于图像识别*。
