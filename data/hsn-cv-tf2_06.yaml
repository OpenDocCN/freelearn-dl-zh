- en: Influential Classification Tools
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 影响力的分类工具
- en: After the deep learning breakthrough in 2012, research toward more refined classification
    systems based on **convolutional neural networks** (**CNNs**) gained momentum.
    Innovation is moving at a frantic pace nowadays, as more and more companies are
    developing smart products. Among the numerous solutions developed over the years
    for object classification, some have became famous for their contributions to
    computer vision. They have been derived and adapted for so many different applications
    that they have achieved must-know status, and so deserve their own chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2012 年深度学习突破之后，基于**卷积神经网络**（**CNNs**）的更精细的分类系统研究获得了动力。如今创新的速度越来越快，因为越来越多的公司在开发智能产品。在多年来为物体分类开发的众多解决方案中，有一些因为其对计算机视觉的贡献而变得非常著名。它们被衍生和改编用于如此多的不同应用，已达到必须了解的地位，因此值得拥有自己的一章。
- en: In parallel with the advanced network architectures introduced by these solutions,
    other methods have been explored to better prepare CNNs for their specific tasks.
    So, in the second part of this chapter, we will look at how the knowledge acquired
    by networks on specific use cases can be transferred to new applications for enhanced
    performance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 与这些解决方案所引入的先进网络架构并行，其他方法也被探索用来更好地为 CNN 做好特定任务的准备。因此，在本章的第二部分，我们将探讨如何将网络在特定使用案例中获得的知识转移到新的应用中，以提高性能。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: What instrumental architectures such as VGG, inception, and ResNet have brought
    to computer vision
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VGG、Inception 和 ResNet 等重要架构对计算机视觉带来了什么
- en: How these solutions can be reimplemented or directly reused for classification
    tasks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些解决方案如何能够重新实现或直接用于分类任务
- en: What transfer learning is, and how to efficiently repurpose trained networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是迁移学习，如何高效地重新利用已训练的网络
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Jupyter notebooks illustrating the concepts presented in this chapter can be
    found in the GitHub folder at [github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter04](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter04).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 解释本章概念的 Jupyter notebook 可以在 GitHub 文件夹中找到，地址为 [github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter04](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter04)。
- en: 'The only new package introduced in this chapter is `tensorflow-hub`. Installation
    instructions can be found at [https://www.tensorflow.org/hub/installation](https://www.tensorflow.org/hub/installation)
    (though it is a single-line command with `pip`: `pip install tensorflow-hub`).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章唯一新增的包是 `tensorflow-hub`。安装说明可以在 [https://www.tensorflow.org/hub/installation](https://www.tensorflow.org/hub/installation)
    找到（它是一个通过 `pip` 安装的单行命令：`pip install tensorflow-hub`）。
- en: Understanding advanced CNN architectures
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解先进的 CNN 架构
- en: Research in computer vision has been moving forward both through incremental
    contributions and large innovative leaps. Challenges organized by researchers
    and companies, inviting experts to submit new solutions in order to best solve
    a predefined task, have been playing a key role in triggering such instrumental
    contributions. The ImageNet classification contest (**ImageNet Large Scale Visual
    Recognition Challenge** *(***ILSVRC**); see [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml),
    *Computer Vision and Neural Networks*) is a perfect example. With its millions
    of images split into 1,000 fine-grained classes, it still represents a great challenge
    for daring researchers, even after the significant and symbolic victory of AlexNet
    in 2012.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉的研究一直在通过渐进式的贡献和重大的创新飞跃不断向前发展。由研究人员和公司组织的挑战赛，邀请专家提交新的解决方案，以便最佳地解决预定任务，这些挑战赛在推动这些关键贡献方面起到了重要作用。**ImageNet
    大规模视觉识别挑战赛** *(***ILSVRC**); 参见 [第1章](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)，*计算机视觉与神经网络*)
    就是一个典型的例子。尽管 2012 年 AlexNet 获得了具有重大意义的胜利，但它依然代表着一个巨大的挑战，挑战的图像库包含数百万张图片，分为 1,000
    个细粒度的类别，依然是勇敢的研究者们面临的巨大挑战。
- en: In this section, we will present some of the classic deep learning methods that
    followed AlexNet in tackling ILSVRC, covering the reasons leading to their development
    and the contributions they made.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些经典的深度学习方法，这些方法在 AlexNet 之后，针对 ILSVRC 持续进行改进，涵盖了导致这些方法发展的原因以及它们做出的贡献。
- en: VGG – a standard CNN architecture
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VGG – 一种标准的 CNN 架构
- en: The first network architecture we will present is **VGG** (or *VGGNet*), developed
    by the *Visual Geometry Group* from Oxford University. Though the group only achieved
    second place in the ILSVRC classification task in 2014, their method influenced
    many later architectures.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍的第一个网络架构是**VGG**（或*VGGNet*），由牛津大学的*视觉几何组*（Visual Geometry Group）开发。尽管该小组在2014年ILSVRC分类任务中仅获得第二名，但他们的方法影响了许多后来的架构。
- en: Overview of the VGG architecture
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VGG架构概述
- en: Looking at the motivation of the VGG authors, and then their contributions,
    we will present how the VGG architecture achieved higher accuracy with fewer parameters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 了解VGG作者的动机，再看他们的贡献，我们将展示VGG架构如何通过更少的参数实现更高的准确率。
- en: Motivation
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: 'AlexNet was a game changer, being the first CNN successfully trained for such
    a complex recognition task and making several contributions that are still valid
    nowadays, such as the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet是一次革命性的突破，它是第一个成功训练的CNN，用于如此复杂的识别任务，并作出了多个至今仍然有效的贡献，诸如以下几点：
- en: The use of a **rectified linear unit** (***ReLU***) as an activation function,
    which prevents the vanishing gradient problem (explained later in this chapter),
    and thus improving training (compared to using sigmoid or tanh)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**修正线性单元**（***ReLU***）作为激活函数，这有效避免了梯度消失问题（将在本章后面解释），从而提高了训练效果（与使用sigmoid或tanh相比）
- en: The application of **dropout** to CNNs (with all the benefits covered in [Chapter
    3](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml), *Modern Neural Networks*)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在CNN中应用**dropout**（详见[第3章](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml)，*现代神经网络*，其中覆盖了所有相关好处）
- en: The typical CNN architecture combining blocks of convolution and pooling layers,
    with dense layers afterward for the final prediction
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 典型的CNN架构结合了卷积层和池化层的块，最后通过全连接层进行最终预测
- en: The application of random transformations (image translation, horizontal flipping,
    and more) to synthetically augment the dataset (that is, augmenting the number
    of different training images by randomly editing the original samples—see [Chapter
    7](337ec077-c215-4782-b56c-beae4d94d718.xhtml), *Training on Complex and Scarce
    Datasets,* for more details)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对图像进行随机变换（图像平移、水平翻转等），以合成性地扩展数据集（即通过随机编辑原始样本来增加不同的训练图像数量——更多细节请参见[第7章](337ec077-c215-4782-b56c-beae4d94d718.xhtml)，*在复杂且稀缺的数据集上训练*）
- en: Still, even back then, it was clear that this prototype architecture had room
    for improvement. The main motivation of many researchers was to try going deeper
    (that is, building a network composed of a larger number of stacked layers), despite
    the challenges arising from this. Indeed, more layers typically means more parameters
    to train, making the learning process more complex. As we will describe in the
    next paragraph, however, Karen Simonyan and Andrew Zisserman from Oxford's VGG
    group tackled this challenge with success. The method they submitted to ILSVRC
    2014 reached a top-5 error of 7.3%, dividing the 16.4% error of AlexNet by more
    than two!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，即便在当时，很明显这个原型架构仍有改进的空间。许多研究者的主要动力是尝试让网络更深（也就是构建一个由更多层堆叠组成的网络），尽管这会带来一些挑战。事实上，更多的层通常意味着需要训练更多的参数，这使得学习过程变得更加复杂。然而，正如我们将在下一段中描述的那样，牛津大学VGG小组的Karen
    Simonyan和Andrew Zisserman成功地解决了这个挑战。他们提交给ILSVRC 2014的方法达到了7.3%的Top-5错误率，比AlexNet的16.4%错误率低了超过一半！
- en: '**Top-5 accuracy** is one of the main classification metrics of ILSVRC. It
    considers that a method has predicted properly if the correct class is among its
    five first guesses. Indeed, for many applications, it is fine to have a method
    that''s able to reduce a large number of class candidates to a lower number (for
    instance, to leave the final choice between the remaining candidates to an expert
    user). The top-5 metrics are a specific case of the more generic top-k metrics.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**Top-5准确率**是ILSVRC的主要分类指标之一。如果正确类别在前五个猜测之内，方法就被认为预测正确。事实上，对于许多应用程序来说，能够将大量类别候选缩小到较少的候选类别的方法是可以接受的（例如，可以将剩下的选择留给专家用户）。Top-5指标是更通用的top-k指标的特例。'
- en: Architecture
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: 'In their paper (*Very Deep Convolutional Networks for Large-Scale Image Recognition*,
    *ArXiv, 2014*), Simonyan and Zisserman presented how they developed their network
    to be deeper than most previous ones. They actually introduced six different CNN
    architectures, from 11 to 25 layers deep. Each network is composed of five blocks
    of several consecutive convolutions followed by a max-pooling layer and three
    final dense layers (with dropout for training). All the convolutional and max-pooling
    layers have `SAME` for padding. The convolutions have *s = 1* for stride, and
    are using the *ReLU* function for activation. All in all, a typical VGG network
    is represented in the following diagram:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的论文（*《用于大规模图像识别的非常深的卷积网络》，* *ArXiv, 2014*）中，Simonyan和Zisserman展示了他们如何将网络设计得比大多数以前的网络更深。实际上，他们介绍了六种不同的CNN架构，从11层到25层不等。每个网络由五个块组成，每个块包含几个连续的卷积层，后接最大池化层和三个最终的全连接层（训练时采用dropout）。所有的卷积层和最大池化层都使用`SAME`作为填充方式。卷积的步长是*s
    = 1*，并且使用*ReLU*函数作为激活函数。总体来说，一个典型的VGG网络在下图中表示：
- en: '![](img/444b8da7-80c0-47b2-a276-ff2fee8922f8.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/444b8da7-80c0-47b2-a276-ff2fee8922f8.png)'
- en: 'Figure 4.1: VGG-16 architecture'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：VGG-16架构
- en: The two most performant architectures, still commonly used nowadays, are called
    **VGG-16** and **VGG-19**. The numbers (16 and 19) represent the *depth* of these
    CNN architectures; that is, the number of *trainable* layers stacked together.
    For example, as shown in *Figure 4.1*, VGG-16 contains 13 convolutional layers
    and 3 dense ones, hence a depth of 16 (excluding the non-trainable operations;
    that is, the 5 max-pooling and 2 dropout layers). The same goes for VGG-19, which
    is composed of three additional convolutions. VGG-16 has approximately 138 million
    parameters, and VGG-19 has 144 million. Those numbers are quite high, although,
    as we will demonstrate in the following section, the VGG researchers took a new
    approach to keep these values in check despite the depth of their architecture.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 两个最具表现力的架构，时至今日仍被广泛使用，它们被称为**VGG-16**和**VGG-19**。这些数字（16和19）代表了这些CNN架构的*深度*；即，堆叠在一起的*可训练*层数。例如，如*图4.1*所示，VGG-16包含13个卷积层和3个全连接层，因此深度为16（不包括非可训练操作；即5个最大池化层和2个dropout层）。VGG-19也是如此，包含额外的三个卷积层。VGG-16大约有1.38亿个参数，而VGG-19有1.44亿个参数。这些数字相当高，尽管正如我们将在接下来的部分中展示的，VGG研究人员采取了一种新的方法来保持这些值的控制，尽管其架构深度较大。
- en: Contributions – standardizing CNN architectures
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贡献 – 标准化CNN架构
- en: In the following paragraphs, we will summarize the most significant contributions
    introduced by these researchers while further detailing their architecture.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的段落中，我们将总结这些研究人员介绍的最重要的贡献，并进一步详细说明他们的架构。
- en: Replacing large convolutions with multiple smaller ones
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用多个较小的卷积代替较大的卷积
- en: The authors began with a simple observation—a stack of two convolutions with
    *3* × *3* kernels has the same receptive field as a convolution with *5* × *5*
    kernels (refer to [Chapter 3](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml)*, Modern
    Neural Networks*, for the **effective receptive field** (**ERF**) formula).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作者从一个简单的观察开始——一堆两个*3* × *3*核的卷积与一个*5* × *5*核的卷积具有相同的感受野（请参考[第3章](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml)*,
    现代神经网络*，了解**有效感受野**（**ERF**）公式）。
- en: 'Similarly, three consecutive *3* × *3* convolutions result in a *7* × *7* receptive
    field, and five *3* × *3* operations result in an *11* × *11* receptive field.
    Therefore, while AlexNet has large filters (up to *11* × *11*), the VGG network
    contains more numerous but smaller convolutions for a larger ERF. The benefits
    of this change are twofold:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，三个连续的*3* × *3*卷积会得到*7* × *7*的感受野，而五个*3* × *3*操作会得到*11* × *11*的感受野。因此，尽管AlexNet使用了较大的滤波器（最大*11*
    × *11*），VGG网络却包含更多数量但较小的卷积，以获得更大的ERF。这一变化的好处有两个：
- en: '**It decreases the number of parameters**: Indeed, the *N* filters of an *11*
    × *11* convolution layer imply *11* × *11* × *D* × *N* = *121D**N* values to train
    just for their kernels (for an input of depth *D*), while five *3* × *3* convolutions
    have a total of *1* × *(3* × *3* × *D* × *N**)* + *4* × *(3* × *3* × *N* × *N**)*
    = *9**D**N* + *36**N²* weights for their kernels. As long as *N* < *3.6**D*, this
    means fewer parameters. For instance, for *N* = *2**D*, the number of parameters
    drops from *242**D²* to *153**D²* (refer to the previous equations). This makes
    the network easier to optimize, as well as much lighter (we invite you to look
    at the decrease for the replacements of the *7* × *7* and *5* × *5* convolutions).'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它减少了参数的数量**：实际上，*11* × *11*卷积层的*N*个滤波器意味着需要训练*11* × *11* × *D* × *N* = *121D**N*个值（其中*D*为输入的深度），而五个*3*
    × *3*卷积层则需要*1* × *(3* × *3* × *D* × *N**)* + *4* × *(3* × *3* × *N* × *N**)* =
    *9**D**N* + *36**N²*个权重（用于它们的滤波器）。只要*N* < *3.6**D*，这意味着参数会更少。例如，当*N* = *2**D*时，参数的数量从*242**D²*降到*153**D²*（参考前面的公式）。这使得网络更容易优化，而且更加轻量（我们邀请你查看*7*
    × *7*和*5* × *5*卷积替换后的下降情况）。'
- en: '**It increases the non-linearity**: Having a larger number of convolution layers—each
    followed by a *non-linear* activation function such as *ReLU*—increases the networks''
    capacity to learn complex features (that is, by combining more non-linear operations).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它增加了非线性**：拥有更多的卷积层——每一层后面跟着一个*非线性*激活函数，如*ReLU*——增加了网络学习复杂特征的能力（即，通过结合更多的非线性操作）。'
- en: Overall, replacing larger convolutions with small, consecutive ones allowed
    the VGG authors to effectively go deeper.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，用小的连续卷积替代更大的卷积，使得VGG的作者能够有效地加深网络。
- en: Increasing the depth of the feature maps
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增加特征图的深度
- en: Based on another intuition, the VGG authors doubled the depth of the feature
    maps for each block of convolutions (from 64 after the first convolution to 512).
    As each set is followed by a max-pooling layer with a *2* × *2* window size and
    a stride of 2, the depth doubles while the spatial dimensions are halved.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 基于另一种直觉，VGG的作者将每个卷积块的特征图深度加倍（从第一次卷积后的64增加到512）。由于每一组后面都有一个*2* × *2*的最大池化层，并且步幅为2，深度加倍，而空间维度则减半。
- en: This allows the encoding of spatial information into more and more complex and
    discriminative features for classification.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得空间信息能够编码为越来越复杂且具有区分度的特征，用于分类。
- en: Augmenting data with scale jittering
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用尺度抖动进行数据增强
- en: Simonyan and Zisserman also introduced a **data augmentation** mechanism that
    they named **scale jittering**. At each training iteration, they randomly scale
    the batched images (from 256 pixels to 512 pixels for their smaller side) before
    cropping them to the proper input size (*224* × *224* for their ILSVRC submission).
    With this random transformation, the network will be confronted with samples with
    different scales and will learn to properly classify them despite this scale jittering
    (refer to *Figure 4.2*). The network becomes more robust as a result, as it is
    trained on images covering a larger range of realistic transformations.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Simonyan和Zisserman还引入了一种他们称之为**数据增强**的机制，名为**尺度抖动**。在每次训练迭代中，他们随机缩放批处理图像（将其较小的一边从256像素缩放到512像素），然后将其裁剪到适当的输入尺寸（他们的ILSVRC提交使用*224*
    × *224*）。通过这种随机变换，网络将面临不同尺度的样本，并学习尽管存在尺度抖动，仍然正确分类这些样本（参见*图4.2*）。因此，网络变得更加稳健，因为它是在涵盖更多现实变换范围的图像上进行训练的。
- en: Data augmentation is the procedure of synthetically increasing the size of training
    datasets by applying random transformations to their images in order to create
    different versions. Details and concrete examples are provided in [Chapter 7](337ec077-c215-4782-b56c-beae4d94d718.xhtml),
    *Training on Complex and Scarce Datasets*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强是通过对图像应用随机变换，从而合成地增加训练数据集大小的过程，以创建不同版本的图像。详细信息和具体示例可参见[第7章](337ec077-c215-4782-b56c-beae4d94d718.xhtml)，*复杂和稀缺数据集的训练*。
- en: The authors also suggested applying random scaling and cropping at test time.
    The idea is to generate several versions of the query image this way and to feed
    them all to the network, with the intuition that it increases the chance of feeding
    content on a scale the network is particularly used to. The final prediction is
    obtained by averaging the results for each version.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们还建议在测试时应用随机缩放和裁剪。这个想法是通过这种方式生成查询图像的多个版本，并将它们全部输入网络，直觉是这样可以增加将内容以网络特别适应的尺度输入的机会。最终的预测是通过对每个版本的结果进行平均得到的。
- en: 'In their paper, they demonstrate how this process tends to also improve accuracy:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的论文中，他们展示了这一过程如何有助于提高准确性：
- en: '![](img/8a127497-cc9f-4714-b826-45b6fd79ada8.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a127497-cc9f-4714-b826-45b6fd79ada8.png)'
- en: 'Figure 4.2: Example of scale jittering. Notice that it is common to not preserve
    the aspect ratio of the content to further transform the images'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：尺度抖动的示例。注意，通常不会保持内容的纵横比，以进一步转换图像。
- en: The same principle was previously used by the AlexNet authors. During both training
    and testing, they were generating several versions of each image with different
    combinations of cropping and flipping transformations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这一原则之前被 AlexNet 的作者使用。在训练和测试过程中，他们为每张图像生成了多个版本，通过不同的裁剪和翻转变换的组合。
- en: Replacing fully connected layers with convolutions
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用卷积替代全连接层
- en: While the classic VGG architecture ends with several **fully connected** (**FC**)
    layers (such as AlexNet), the authors suggest an alternative version. In this
    version, the dense layers are replaced by convolutional ones.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然经典的 VGG 架构以多个**全连接**（**FC**）层（如 AlexNet）结束，但作者建议了一种替代版本。在这个版本中，全连接层被卷积层所替代。
- en: The first set of convolutions with larger kernels (*7* × *7* and *3* × *3*)
    reduces the spatial size of the feature maps to *1* × *1* (with no padding applied
    beforehand) and increases their depth to 4,096\. Finally, a *1* × *1* convolution
    is used with as many filters as classes to predict from (that is, *N* = 1,000
    for ImageNet). The resulting *1* × *1* × *N* vector is normalized with the `softmax`
    function, and then flattened into the final class predictions (with each value
    of the vector representing the predicted class probability).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第一组使用较大卷积核（*7* × *7* 和 *3* × *3*）的卷积操作将特征图的空间大小减少到 *1* × *1*（之前没有应用填充），并将其深度增加到
    4,096。最后，使用一个 *1* × *1* 的卷积，其滤波器数量等于要预测的类别数（即，*N* = 1,000 用于 ImageNet）。最终的 *1*
    × *1* × *N* 向量通过 `softmax` 函数进行归一化，然后将其展开成最终的类别预测（向量的每个值表示预测的类别概率）。
- en: '*1* × *1* convolutions are commonly used to change the depth of the input volume
    without affecting its spatial structure. For each spatial position, the new values
    are interpolated from all the depth values at that position.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*1* × *1* 卷积通常用于改变输入体积的深度，而不影响其空间结构。对于每个空间位置，新值是从该位置所有深度值的插值计算出来的。'
- en: Such a network without any dense layers is called a **fully convolutional network **(**FCN**).
    As mentioned in [Chapter 3](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml), *Modern
    Neural Networks*, and as has been highlighted by the VGG authors, FCNs can be
    applied to images of different sizes, with no need for cropping beforehand.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 没有任何全连接层的网络被称为**全卷积网络**（**FCN**）。正如在[第 3 章](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml)《现代神经网络》中提到的，并且正如
    VGG 作者所强调的那样，FCN 可以应用于不同大小的图像，而无需提前裁剪。
- en: Interestingly, to achieve the best accuracy for ILSVRC, the authors trained
    and used both versions (normal and FCN), once again averaging their results to
    obtain the final predictions. This technique is named **model averaging** and
    is frequently used in production.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，为了在 ILSVRC 中获得最佳准确率，作者同时训练并使用了两种版本（普通版和 FCN），再次通过平均它们的结果来获得最终的预测。这种技术被称为**模型平均**，在生产中经常使用。
- en: Implementations in TensorFlow and Keras
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 和 Keras 的实现
- en: Thanks to the efforts that the authors put into creating a clear architecture,
    VGG-16 and VGG-19 are among the simplest classifiers to reimplement. Example code
    can be found in the GitHub folder for this chapter, for educational purposes.
    However, in computer vision, as in many domains, it is always preferable not to
    reinvent the wheel and to instead reuse existing tools that are available. The
    following paragraphs present different preimplemented VGG solutions that you can
    directly adapt and reuse.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于作者在创建清晰架构方面的努力，VGG-16 和 VGG-19 是最容易重新实现的分类器之一。示例代码可以在本章的 GitHub 文件夹中找到，供教学用途。然而，在计算机视觉领域，像许多其他领域一样，通常建议不要重新发明轮子，而是重用现有的工具。以下段落展示了不同的预实现
    VGG 解决方案，您可以直接调整并重用。
- en: The TensorFlow model
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 模型
- en: While TensorFlow itself does not offer any official implementation of the VGG
    architectures, neatly implemented VGG-16 and VGG-19 networks are available in
    the `tensorflow/models` GitHub repository ([https://github.com/tensorflow/models](https://github.com/tensorflow/models)).
    This repository, maintained by TensorFlow contributors, contains numerous well-curated
    state-of-the-art or experimental models. It is often recommended that you should
    search this repository when looking for a specific network.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然TensorFlow本身没有提供VGG架构的官方实现，但在`tensorflow/models` GitHub库中可以找到精心实现的VGG-16和VGG-19网络（[https://github.com/tensorflow/models](https://github.com/tensorflow/models)）。这个由TensorFlow贡献者维护的库包含了许多精心策划的先进模型或实验性模型。通常建议在寻找特定网络时，应该搜索这个库。
- en: We invite our readers to have a look at the VGG code there (currently available
    at [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/vgg.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/vgg.py)),
    as it reimplements the FCN version we described earlier.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们邀请读者查看那里的VGG代码（目前可在[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/vgg.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/vgg.py)找到），它重新实现了我们之前描述的FCN版本。
- en: The Keras model
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras模型
- en: 'The Keras API has an official implementation of these architectures, accessible
    via its `tf.keras.applications` package (refer to the documentation at [https://www.tensorflow.org/api_docs/python/tf/keras/applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications)).
    This package contains several other well-known models and provides *pre trained*
    parameters for each (that is, parameters saved from prior training on a specific
    dataset). For instance, you can instantiate a VGG network with the following command:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Keras API提供了这些架构的官方实现，可以通过其`tf.keras.applications`包访问（请参见文档：[https://www.tensorflow.org/api_docs/python/tf/keras/applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications)）。该包包含了其他几个著名的模型，并为每个模型提供了*预训练*参数（即从先前在特定数据集上训练中保存的参数）。例如，你可以使用以下命令实例化一个VGG网络：
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With these default arguments, Keras instantiates the VGG-16 network and loads
    the persisted parameter values obtained after a complete training cycle on ImageNet.
    With this single command, we have a network ready to classify images into the
    1,000 ImageNet categories. If we would like to retrain the network from scratch
    instead, we should fix `weights=None` and Keras will randomly set the weights.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些默认参数，Keras会实例化VGG-16网络并加载在ImageNet上经过完整训练周期后保存的参数值。通过这一条命令，我们得到了一个准备好将图像分类为1,000个ImageNet类别的网络。如果我们想要重新从头训练网络，我们应该将`weights=None`，Keras将随机设置权重。
- en: In Keras terminology, the *top* layers correspond to the final consecutive dense
    layers. Therefore, if we set `include_top=False`, the VGG dense layers will be
    excluded, and the network's outputs will be the feature maps of the last convolution/max-pooling
    block. This can be useful if we want to reuse the pre trained VGG network to extract
    meaningful features (which can be applied to more advanced tasks), and not just
    for classification. The `pooling` function parameter can be used in those cases
    (that is, when `include_top=False`) to specify an optional operation to be applied
    to the feature maps before returning them (`pooling='avg'` or `pooling='max'`
    to apply a global average- or max- pooling).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras术语中，*top*层对应于最后一层的连续全连接层。因此，如果我们设置`include_top=False`，VGG的全连接层将被排除，网络的输出将是最后一个卷积/最大池化块的特征图。如果我们想要重用预训练的VGG网络来提取有意义的特征（这些特征可以应用于更高级的任务），而不仅仅是用于分类，这时可以使用`pooling`函数参数（即当`include_top=False`时）来指定在返回特征图之前对其进行的可选操作（`pooling='avg'`或`pooling='max'`，用于应用全局平均池化或最大池化）。
- en: GoogLeNet and the inception module
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GoogLeNet和inception模块
- en: Developed by researchers at Google, the architecture we will now present was
    also applied to ILSVRC 2014 and won first place for the classification task ahead
    of VGGNet. **GoogLeNet** (for *Google* and *LeNet*, as an homage to this pioneering
    network) is structurally very different from its linear challenger, introducing
    the notion of *inception blocks* (the network is also commonly called an **inception
    network**).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由谷歌研究人员开发的架构，我们现在将要介绍的这一架构也被应用于ILSVRC 2014，并在分类任务中超越了VGGNet，获得了第一名。**GoogLeNet**（源自*Google*和*LeNet*，向这一开创性网络致敬）在结构上与其线性对手有很大不同，引入了*inception
    blocks*的概念（该网络也通常被称为**inception network**）。
- en: Overview of the GoogLeNet architecture
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GoogLeNet 架构概述
- en: As we will see in the following section, the GoogLeNet authors, Christian Szegedy
    and others, approached the conception of a more efficient CNN from a very different
    angle than the VGG researchers (*Going Deeper with Convolutions*, Proceedings
    of the CVPR IEEE conference, 2014).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在接下来的章节中看到的，GoogLeNet 的作者 Christian Szegedy 等人，从一个与 VGG 研究人员截然不同的角度出发，构思了一个更高效的
    CNN（*深入卷积网络*，CVPR IEEE 会议论文，2014）。
- en: Motivation
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: While VGG's authors took AlexNet and worked on standardizing and optimizing
    its structure in order to obtain a clearer and deeper architecture, researchers
    at Google took a different approach. Their first consideration, as mentioned in
    the paper, was the optimization of the CNN computational footprint.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 VGG 的作者基于 AlexNet 并致力于标准化和优化其结构，以获得更清晰和更深的架构，谷歌的研究人员则采取了不同的方法。正如论文中提到的，他们的第一个考虑因素是优化
    CNN 的计算开销。
- en: Indeed, in spite of careful engineering (refer to VGG), the deeper CNNs are,
    the larger their number of trainable parameters and their number of computations
    per prediction become (it is costly with respect to memory and time). For instance,
    VGG-16 weighs approximately 93 MB (in terms of parameter storage), and the VGG
    submission for ILSVRC took two to three weeks to train on four GPUs. With approximately
    5 million parameters, GoogLeNet is 12 times lighter than AlexNet and 21 times
    lighter than VGG-16, and the network was trained within a week. As a result, GoogLeNet—and
    more recent inception networks—can even run on more modest machines (such as smartphones),
    which contributed to their lasting popularity.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，尽管经过精心设计（参见 VGG），CNN 越深，训练参数的数量和每次预测的计算量就越大（在内存和时间上都很昂贵）。例如，VGG-16 大约占 93
    MB（参数存储方面），而 VGG 在 ILSVRC 的提交需要两到三周的时间，在四个 GPU 上进行训练。GoogLeNet 拥有大约 500 万个参数，比
    AlexNet 轻 12 倍，比 VGG-16 轻 21 倍，而且该网络在一周内就能训练完成。因此，GoogLeNet——以及更近期的 inception
    网络——甚至可以在较为普通的机器（如智能手机）上运行，这也促进了它们的长期流行。
- en: We have to keep in mind that, despite this impressive reduction in the numbers
    of parameters and operations, GoogLeNet did win the classification challenge in
    2014 with a top-5 error of 6.7% (against 7.3% with VGG). This performance is the
    result of the second target of Szegedy and others—the conception of a network
    that was not only deeper but also larger, with blocks of parallel layers for *multiscale
    processing*. While we will detail this solution later in this chapter, the intuition
    behind it is simple. Building a CNN is a complex, iterative task. How do we know
    which layer (such as convolutional or pooling) should be added to the stack in
    order to improve the accuracy? How do we know which kernel size would work best
    for a given layer? After all, kernels of different sizes will not react to features
    of the same scale. How can we avoid such a trade-off? A solution, according to
    the authors, is to use the *inception modules* they developed, composed of several
    different layers working in parallel.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须记住，尽管在参数和操作数量上大幅减少，GoogLeNet 仍然在 2014 年的分类挑战中获胜，具有 6.7% 的 top-5 错误率（而 VGG
    为 7.3%）。这一表现是 Szegedy 等人第二个目标的结果——构建一个不仅更深而且更大的网络，具有并行层块用于 *多尺度处理*。虽然我们将在本章后面详细介绍这个解决方案，但其直觉其实很简单。构建一个
    CNN 是一项复杂的迭代任务。我们如何知道应该在堆叠中添加哪个层（如卷积层或池化层）以提高准确性？我们如何知道哪个卷积核尺寸最适合某一层？毕竟，不同尺寸的卷积核对不同尺度的特征反应不同。我们如何避免这种取舍？根据作者的说法，一个解决方案是使用他们开发的
    * inception 模块*，由多个不同的层并行工作组成。
- en: Architecture
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: As shown in *Figure 4.3*, GoogLeNet architecture is not as straightforward as
    the previous architectures we studied, although it can be analyzed region by region.
    The input images are first processed by a classic series of convolutional and
    max-pooling layers. Then, the information goes through a stack of nine inception
    modules. These modules (often called **subnetworks**; further detailed in *Figure
    4.4*), are blocks of layers stacked vertically and horizontally. For each module,
    the input feature maps are passed to four parallel sub-blocks composed of one
    or two different layers (convolutions with different kernel sizes and max-pooling).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 4.3*所示，GoogLeNet架构不像我们之前学习的架构那样简单，尽管可以按区域逐一分析。输入图像首先通过经典的卷积层和最大池化层系列进行处理。然后，信息经过九个Inception模块的堆叠。这些模块（通常称为**子网络**；详见*图
    4.4*）是垂直和水平方向堆叠的层块。对于每个模块，输入特征图会传递到由一个或两个不同层（具有不同卷积核大小的卷积和最大池化）组成的四个并行子块。
- en: 'The results of these four parallel operations are then concatenated together
    along the depth dimension and into a single feature volume:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个并行操作的结果会沿深度维度被串联在一起，形成一个单一的特征体积：
- en: '![](img/994f1060-1599-4d5a-96e6-c61b06fc087e.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/994f1060-1599-4d5a-96e6-c61b06fc087e.png)'
- en: 'Figure 4.3: GoogLeNet architecture. The inception modules are detailed in *Figure
    4.4*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：GoogLeNet架构。Inception模块详见*图 4.4*
- en: In the preceding figure, all the convolutional and max-pooling layers have `SAME` for
    padding. The convolutions have *s = 1* for stride if unspecified and are using
    the *ReLU* function for activation.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，所有的卷积层和最大池化层都使用了`SAME`填充。卷积层如果没有特别说明，*s = 1*（步幅为1），并使用*ReLU*函数作为激活函数。
- en: 'This network is composed of several layer blocks sharing a similar structure
    with parallel layers—the inception modules. For instance, the first inception
    module, represented in *Figure 4.3*, receives a feature volume of size *28* ×
    *28* × *192 *for input. Its first parallel sub-block, composed of a single *1*
    × *1* convolution output (*N* = 64 and *s* = 1), thus generates a *28* × *28* ×
    *64* tensor. Similarly, the second sub-module, composed of two convolutions, outputs
    a *28* × *28* × *128* tensor; and the two remaining ones output a *28* × *28* ×
    *32* and a *28* × *28* × *32* feature volume, respectively. Therefore, by stacking
    these four results together along the last dimension, the first inception module
    outputs a *28* × *28* × *256* tensor, which is then passed to the second module,
    and so on. In the following diagram, the naive solution is represented on the
    left, and the module used in GoogLeNet (that is, the inception module v1) is shown
    on the right (note that in GoogLeNet, the number of filters *N* increases the
    deeper the module is):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络由几个层块组成，这些层块共享相似的结构，并具有并行层——即Inception模块。例如，第一Inception模块，如*图 4.3*所示，接收一个大小为*28*
    × *28* × *192*的特征体积作为输入。它的第一个并行子块，由一个*1* × *1*卷积输出组成（*N* = 64，*s* = 1），因此生成一个*28*
    × *28* × *64*的张量。同样，第二个子模块，由两个卷积组成，输出一个*28* × *28* × *128*的张量；剩下的两个子模块分别输出一个*28*
    × *28* × *32*和*28* × *28* × *32*的特征体积。因此，通过将这四个结果沿最后一个维度堆叠，第一Inception模块输出一个*28*
    × *28* × *256*的张量，然后将其传递到第二模块，以此类推。在下面的图示中，左侧表示的是朴素解法，右侧显示的是GoogLeNet中使用的模块（即Inception模块v1）（请注意，在GoogLeNet中，随着模块的深度增加，滤波器数量*N*也会增加）：
- en: '![](img/79d097f6-6abf-4bc6-8e4f-48bf0a4c09c2.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79d097f6-6abf-4bc6-8e4f-48bf0a4c09c2.png)'
- en: 'Figure 4.4: Inception modules: naive versus actual'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：Inception模块：朴素解法与实际解法
- en: The features of the last module are average, pooled from *7* × *7* × *1,024*
    to *1* × *1* × *1,024*, and are finally densely converted into the prediction
    vector. As shown in *Figure 4.3*, the network is further composed of two auxiliary
    branches, also leading to predictions. Their purpose will be detailed in the next
    section.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个模块的特征通过平均池化从*7* × *7* × *1,024*池化到*1* × *1* × *1,024*，并最终密集地转换为预测向量。如*图
    4.3*所示，网络进一步由两个辅助分支组成，这些分支也指向预测。它们的目的将在下一节中详细说明。
- en: In total, GoogLeNet is a 22-layer deep architecture (counting the trainable
    layers only), with a total of more than 60 convolutional and FC layers. And yet,
    this much larger network has 12 times fewer parameters than AlexNet.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，GoogLeNet是一个22层深的架构（仅计算可训练的层），总共有超过60个卷积层和全连接层。尽管如此，这个更大的网络参数量比AlexNet少了12倍。
- en: Contributions – popularizing larger blocks and bottlenecks
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贡献 – 推广更大块和瓶颈结构
- en: The low number of parameters, as well as the network's performance, are the
    results of several concepts implemented by the GoogLeNet authors. We will cover
    the main ones in this section.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 低数量的参数以及网络的性能是 GoogLeNet 作者实现的多个概念的结果。我们将在本节中介绍这些主要概念。
- en: In this section, we will present only the key concepts differentiating the inception
    networks from the ones we introduced previously. Note that the GoogLeNet authors
    reapplied several other techniques that we have already covered, such as the prediction
    of multiple crops for each input image and the use of other image transformations
    during training.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们将仅介绍与之前介绍的网络不同的关键概念。请注意，GoogLeNet 的作者重新应用了我们已覆盖的几种其他技术，例如对每个输入图像进行多个裁剪的预测以及在训练期间使用其他图像变换。
- en: Capturing various details with inception modules
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 inception 模块捕获各种细节
- en: Introduced by Min Lin and others in their influential **Network in Network** (**NIN**)
    paper in 2013, the idea of having a CNN composed of sub-network modules was adapted
    and fully exploited by the Google team. As previously mentioned and shown in *Figure
    4.4*, the basic inception modules they developed are composed of four parallel
    layers—three convolutions with filters of size *1* × *1*, *3* × *3*, and *5* ×
    *5*, respectively, and one max-pooling layer with stride `1`. The advantages of
    this parallel processing, with the results concatenated together after, are numerous.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Min Lin 等人在其具有影响力的 **Network in Network** (**NIN**) 论文中于 2013 年提出，构建由子网络模块组成的
    CNN 的理念被 Google 团队采用并充分利用。如前所述并在*图 4.4*中展示，他们开发的基本 inception 模块由四个并行层组成——三个卷积层，滤波器大小分别为
    *1* × *1*、*3* × *3* 和 *5* × *5*，以及一个步长为 `1` 的最大池化层。此并行处理的优势，在结果拼接后，显而易见。
- en: As explained in the Motivation sub-section, this architecture allows for the
    multiscale processing of the data. The results of each inception module combine
    features of different scales, capturing a wider range of information. We do not
    have to choose which kernel size may be the best (such a choice would require
    several iterations of training and testing cycles), that is, the network learns
    by itself which convolutions to rely on more for each module.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在动机小节中所解释的，这种架构允许对数据进行多尺度处理。每个 inception 模块的结果结合了不同尺度的特征，捕获了更广泛的信息。我们不需要选择哪个卷积核大小可能是最好的（这样的选择需要进行多次训练和测试迭代），即网络会自己学习在每个模块中依赖哪些卷积。
- en: Additionally, while we presented how vertically stacking layers with non-linear
    activation functions positively affects a network's performance, this is also
    true for horizontal combinations. The concatenation of features mapped from different
    layers further adds to the non-linearity of the CNN.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，虽然我们展示了如何通过垂直堆叠具有非线性激活函数的层来积极影响网络性能，但对于水平组合，情况也是如此。来自不同层的特征拼接进一步增强了卷积神经网络（CNN）的非线性特性。
- en: Using 1 x 1 convolutions as bottlenecks
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 *1* × *1* 卷积作为瓶颈
- en: Though not a contribution *per se*, Szegedy et al. made the following technique
    notorious by efficiently applying it to their network.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这本身不是一种贡献，Szegedy 等人通过高效地将其应用于他们的网络，使得以下技术广为人知。
- en: As previously mentioned in the *Replacing fully connected layers with convolutions*
    section, *1* × *1* convolutional layers (with a stride of 1) are often used to
    change the overall depth of input volumes without affecting their spatial structures.
    Such a layer with *N* filters would take an input of shape *H* × *W* × *D* and
    return an interpolated *H* × *W* × *N* tensor. For each pixel in the input image,
    its *D* channel values will be interpolated by the layer (according to its filter
    weights) into *N* channel values.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如在*用卷积替代全连接层*部分中提到的，*1* × *1* 卷积层（步长为 1）常用于在不影响输入空间结构的情况下改变输入体积的整体深度。这样的层具有
    *N* 个滤波器，输入形状为 *H* × *W* × *D*，并返回一个插值后的 *H* × *W* × *N* 张量。对于输入图像中的每个像素，其 *D*
    通道的值将通过该层（根据其滤波器权重）被插值为 *N* 个通道值。
- en: This property can be applied to reduce the number of parameters required for
    larger convolutions by compressing the features' depth beforehand (using *N* <
    *D*). This technique basically uses *1* × *1* convolutions as **bottlenecks**
    (that is, as intermediary layers reducing the dimensionality and, thus, the number
    of parameters). Since activations in neural networks are often redundant or left
    unused, such bottlenecks usually barely affect the performance (as long as they
    do not drastically reduce the depth). Moreover, GoogLeNet has its parallel layers
    to compensate for the depth reduction. Indeed, in inception networks, bottlenecks
    are present in every module, before all larger convolutions and after max-pooling
    operations, as illustrated in *Figure 4.4*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这一特性可以应用于通过预先压缩特征深度（使用*N* < *D*）来减少更大卷积所需的参数数量。该技术基本上使用*1* × *1*的卷积作为**瓶颈**（即作为中间层，降低维度，从而减少参数数量）。由于神经网络中的激活通常是冗余的或未被使用，这样的瓶颈通常几乎不影响性能（只要它们不会大幅度减少深度）。此外，GoogLeNet有并行层来补偿深度的减少。实际上，在Inception网络中，每个模块中都有瓶颈，出现在所有较大卷积之前和最大池化操作之后，如*图4.4*所示。
- en: Given the *5* × *5* convolution in the first inception module (taking as input
    a *28* × *28* × *192* volume) for example, the tensor containing its filters would
    be of the dimension *5* × *5* × *192* × *32* in the naive version. This represents
    153,600 parameters just for this convolution. In the first version of the inception
    module (that is, with bottlenecks), a *1* × *1* convolution is introduced before
    the 5 × 5 one, with *N* = 16\. As a result, the two convolutions require a total
    of *1* × *1* × *192* × *16* + *5* × *5* × *16* × *32* = *15,872* trainable values
    for their kernels. This is 10 times fewer parameters than the previous version
    (just for this single *5* × *5* layer), for the same output size! Furthermore,
    as mentioned already, the addition of layers with a non-linear activation function
    (*ReLU*) further improves the networks' ability to grasp complex concepts.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以第一个Inception模块中的*5* × *5*卷积为例（输入为*28* × *28* × *192*的体积），其过滤器的张量在传统版本中将是*5*
    × *5* × *192* × *32*的维度。仅这个卷积就代表了153,600个参数。在Inception模块的第一个版本（即带瓶颈的版本）中，在5 ×
    5卷积之前引入了一个*1* × *1*的卷积，其中*N* = 16。结果，这两个卷积的总参数为*1* × *1* × *192* × *16* + *5*
    × *5* × *16* × *32* = *15,872*个可训练的核值。这比之前版本（仅对于这个单独的*5* × *5*层）少了10倍的参数，且输出大小相同！此外，正如前面所提到的，添加具有非线性激活函数（*ReLU*）的层进一步提高了网络抓取复杂概念的能力。
- en: We are presenting GoogLeNet as submitted to ILSVRC 2014 in this chapter. More
    commonly named **Inception V1**, this architecture has been refined by its authors
    since then. **Inception V2** and **Inception V3** contain several improvements,
    such as replacing the *5 × 5* and *7 × 7* convolutions by smaller ones (as done
    in VGG), improving the bottlenecks' hyperparameters to reduce the information
    loss, and adding *BatchNorm* layers.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本章展示的是GoogLeNet提交给ILSVRC 2014的版本。更常被称为**Inception V1**，该架构自那时以来已被作者进一步改进。**Inception
    V2**和**Inception V3**包含了几个改进，例如将*5 × 5*和*7 × 7*卷积替换为更小的卷积（如VGG中所做的），改进瓶颈的超参数以减少信息损失，并添加了*BatchNorm*层。
- en: Pooling instead of fully connecting
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用池化代替全连接
- en: Another solution used by the inception authors to reduce the number of parameters
    was to use an average-pooling layer instead of a fully connected one after the
    last convolutional block. With a *7* × *7* window size and stride of 1, this layer
    reduces the feature volume from *7* × *7* × *1,024* to *1* × *1* × *1,024* without
    any parameter to train. A dense layer would have added (*7* × *7* × *1,024*) ×
    1,024 = *51,380,224* parameters. Though the network loses a bit in expressiveness
    with this replacement, the computational gain is enormous (and the network already
    contains enough non-linear operations to capture the information it needs for
    the final prediction).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少参数的数量，Inception 的作者们采用了一个解决方案，即在最后一个卷积块之后使用平均池化层，而不是全连接层。通过使用*7* × *7*的窗口大小和步长为1，该层将特征体积从*7*
    × *7* × *1,024*减少到*1* × *1* × *1,024*，而无需训练任何参数。如果使用全连接层，则会增加（*7* × *7* × *1,024*）×
    1,024 = *51,380,224*个参数。尽管使用这个替代方法网络的表达能力有所降低，但计算上的提升是巨大的（并且网络已经包含了足够的非线性操作来捕捉最终预测所需的信息）。
- en: The last and only FC layer in GoogLeNet has *1,024* × *1,000* = *1,024,000*
    parameters, a fifth of the total number the network has!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: GoogLeNet中的最后一层全连接层（FC层）具有*1,024* × *1,000* = *1,024,000*个参数，占网络总参数的五分之一！
- en: Fighting vanishing gradient with intermediary losses
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过中间损失来应对梯度消失问题
- en: As briefly mentioned when introducing the architecture, GoogLeNet has two auxiliary
    branches at training time (removed after), also leading to predictions.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如在介绍架构时简要提到的，GoogLeNet在训练时有两个辅助分支（训练后移除），也用于生成预测。
- en: Their purpose is to improve the propagation of the loss through the network
    during training. Indeed, deeper CNNs are often plagued with **vanishing gradient**.
    Many CNN operations (for instance, *sigmoid*) have derivatives with small amplitudes
    (below one). Therefore, the higher the number of layers, the smaller the product
    of the derivatives becomes when backpropagating (as more values below one are
    multiplied together, the closer to zero the result will become). Often, the gradient
    simply vanishes/shrinks to zero when reaching the first layers. Since the gradient
    values are directly used to update the parameters, these layers won't effectively
    learn if the gradient is too small.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的目的是在训练过程中改善损失在网络中的传播。事实上，较深的CNN经常会遇到**梯度消失**问题。许多CNN操作（例如，*sigmoid*）的导数幅度较小（小于一）。因此，层数越高，反向传播时导数的乘积就会变得越小（因为更多小于一的值相乘，结果会更接近零）。通常，梯度在到达第一层时会消失或缩小为零。由于梯度值直接用于更新参数，如果梯度太小，这些层将无法有效学习。
- en: The opposite phenomenon—the **exploding gradient** problem—can also happen with
    deeper networks. When operations whose derivatives can take on larger magnitudes
    are used, their product during backpropagation can become so big that it makes
    the training unstable (with huge, erratic weight updates) or it can even sometimes
    overflow (`NaN` values).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个相反的现象——**梯度爆炸**问题——也可能发生在更深的网络中。当使用导数可能具有更大幅度的操作时，它们在反向传播时的乘积可能会变得非常大，以至于导致训练不稳定（权重更新剧烈且不稳定），甚至有时会溢出（`NaN`值）。
- en: The down-to-earth, yet effective, solution to this problem implemented here
    is to reduce the distance between the first layers and predictions, by introducing
    additional classification losses at various network depths. If the gradient from
    the final loss cannot flow properly to the first layers, these will still be trained
    to help with classification thanks to the closer intermediary losses. Incidentally,
    this solution also slightly improves the robustness of the layers affected by
    multiple losses, as they must learn to extract discriminative features that are
    not only useful to the main network, but also to the shorter branches.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这里实现的解决方案非常务实且有效：通过在不同的网络深度引入额外的分类损失，减少第一层与预测之间的距离。如果最终损失的梯度无法正常流向第一层，那么这些层仍然会通过更靠近的中间损失进行训练，从而有助于分类。顺便提一下，这个解决方案还略微提高了受多个损失影响的层的鲁棒性，因为它们必须学习提取既对主网络有用，又对较短分支有用的判别特征。
- en: Implementations in TensorFlow and Keras
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在TensorFlow和Keras中的实现
- en: While the inception architecture may look complex to implement at first glance,
    we already have most of the tools to do so. Moreover, several pretrained versions
    are also made available by TensorFlow and Keras.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Inception架构乍一看可能显得复杂，但我们已经拥有大部分实现它的工具。此外，TensorFlow和Keras也提供了几个预训练版本。
- en: Inception module with the Keras Functional API
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras功能API的Inception模块
- en: The networks we have implemented so far were purely sequential, with a single
    path from inputs to predictions. The inception model differs from those, with
    its multiple parallel layers and branches. This gives us the opportunity to demonstrate
    that such operational graphs are not much more difficult to instantiate with the
    available APIs. In the following section, we will write an inception module using
    the Keras Functional API (refer to the documentation at [https://keras.io/getting-started/sequential-model-guide/](https://keras.io/getting-started/sequential-model-guide/)).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们实现的网络完全是顺序结构，从输入到预测只有一条路径。与此不同，Inception模型有多个并行的层和分支。这使得我们有机会展示，这种操作图形在现有的API下并没有比其他网络更难实例化。在接下来的部分，我们将使用Keras功能API编写一个Inception模块（参见文档：[https://keras.io/getting-started/sequential-model-guide/](https://keras.io/getting-started/sequential-model-guide/)）。
- en: 'So far, we have mostly been using the Keras Sequential API, which is not well-adapted
    for multipath architectures (as its name implies). The Keras Functional API is
    closer to the TensorFlow paradigm, with Python variables for the layers being
    passed as parameters to the next ones to build a graph. The following code presents
    a simplistic model implemented with both APIs:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要使用的是 Keras Sequential API，而它并不太适合多路径架构（正如其名字所暗示的那样）。Keras Functional
    API 更接近 TensorFlow 的范式，通过将每一层的 Python 变量作为参数传递给下一层来构建图形。以下代码展示了使用这两种 API 实现的一个简化模型：
- en: '[PRE1]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With the functional API, a layer can easily be passed to multiple others, which
    is what we need for the parallel blocks of the inception modules. Their results
    can then be merged together using a `concatenate` layer (refer to the documentation
    at [https://keras.io/layers/merge/#concatenate_1](https://keras.io/layers/merge/#concatenate_1)).
    Therefore, the naive inception block presented in *Figure 4.4* can be implemented
    as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用功能性 API，可以轻松地将一个层传递给多个其他层，这正是我们需要用于 Inception 模块的并行块。它们的结果可以通过 `concatenate`
    层合并在一起（参见文档 [https://keras.io/layers/merge/#concatenate_1](https://keras.io/layers/merge/#concatenate_1)）。因此，*图
    4.4* 中展示的简单 Inception 块可以按如下方式实现：
- en: '[PRE2]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We will leave it to you to adapt this code to implement the proper modules for
    Inception V1 by adding the bottleneck layers.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将留给你自己来调整这段代码，以通过添加瓶颈层来实现 Inception V1 的正确模块。
- en: TensorFlow model and TensorFlow Hub
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 模型和 TensorFlow Hub
- en: Google offers several scripts and tutorials explaining how to directly use its
    inception networks, or how to retrain them for new applications. The directory
    dedicated to this architecture in the `tensorflow/models` Git repository ([https://github.com/tensorflow/models/tree/master/research/inception](https://github.com/tensorflow/models/tree/master/research/inception))
    is also rich and well-documented. Moreover, a pretrained version of Inception
    V3 is available on **TensorFlow Hub**, which gives us the opportunity to introduce
    this platform.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Google 提供了几种脚本和教程，解释如何直接使用其 Inception 网络，或者如何对其进行重新训练以适应新的应用。`tensorflow/models`
    Git 仓库中专门针对这一架构的目录（[https://github.com/tensorflow/models/tree/master/research/inception](https://github.com/tensorflow/models/tree/master/research/inception)）也非常丰富且文档详尽。此外，Inception
    V3 的预训练版本已在 **TensorFlow Hub** 上提供，这为我们介绍这一平台提供了机会。
- en: 'TensorFlow Hub is a repository of pretrained models. In a similar way to how
    Docker allows people to easily share and reuse software packages, removing the
    need to reconfigure distributions, TensorFlow Hub gives access to pretrained models
    so that people do not have to spend time and resources reimplementing and retraining.
    It combines a website ([https://tfhub.dev](https://tfhub.dev)) where people can
    search for specific models (depending, for example, on the target recognition
    task), and a Python package to easily download and start using these models. For
    instance, we can fetch and set up an Inception V3 network as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Hub 是一个预训练模型的仓库。类似于 Docker 允许人们轻松共享和重用软件包，避免重新配置分发，TensorFlow Hub
    提供了访问预训练模型的功能，让人们无需花时间和资源去重新实现和重新训练模型。它结合了一个网站（[https://tfhub.dev](https://tfhub.dev)），用户可以在其中搜索特定的模型（例如，基于目标识别任务），以及一个
    Python 包来轻松下载和开始使用这些模型。例如，我们可以按如下方式获取并设置 Inception V3 网络：
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Though this code is quite succinct, a lot is happening. A preliminary step was
    to browse the [tfhub.dev](https://tfhub.dev) website and decide on a model there.
    On the page presenting the selected model ([https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2](https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2);
    stored in `model_url`), we can read that the inception model we chose is defined
    as an **image feature vector** that expects *299 *× *299 *× *3* inputs, among
    other details. To use a TensorFlow Hub model, we need to know how to interface
    with it.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这段代码相当简洁，但实际上发生了很多事情。初步步骤是浏览 [tfhub.dev](https://tfhub.dev) 网站，并在其中选择一个模型。在展示所选模型的页面上（[https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2](https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2);
    存储在 `model_url` 中），我们可以看到所选的 Inception 模型被定义为 **图像特征向量**，它期望接收 *299* × *299* ×
    *3* 的输入，此外还有其他一些细节。要使用 TensorFlow Hub 模型，我们需要知道如何与其交互。
- en: The *image feature vector* type tells us that this network returns extracted
    features; that is, the results of the last convolutional block before the dense
    operations. With such a model, it is up to us to add the final layers (for instance,
    so that the output size corresponds to the number of considered classes).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*图像特征向量*类型告诉我们该网络返回的是提取的特征；也就是说，它返回的是在密集操作之前，最后一个卷积块的输出。使用这样的模型，我们可以自行添加最终层（例如，确保输出大小与考虑的类别数量相符）。'
- en: The latest versions of the TensorFlow Hub interface seamlessly with Keras, and
    a complete pretrained TensorFlow Hub model can be fetched and instantiated as
    a Keras layer thanks to `tensorflow_hub.KerasLayer(model_url, trainable, ...)`.
    Like any Keras layer, it can then be used inside larger Keras models or TensorFlow
    estimators.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Hub 接口的最新版本与 Keras 完美兼容，并且可以通过 `tensorflow_hub.KerasLayer(model_url,
    trainable, ...)` 将完整的预训练 TensorFlow Hub 模型提取并实例化为 Keras 层。像任何 Keras 层一样，它可以在更大的
    Keras 模型或 TensorFlow 估算器中使用。
- en: Though this may not seem as straightforward as using the Keras Applications
    API, TensorFlow Hub has an exotic catalog of models, which is destined to increase
    over time.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这可能看起来不像使用 Keras Applications API 那么直接，TensorFlow Hub 拥有一个异国情调的模型目录，并且预计随着时间的推移会不断增加。
- en: One of the Jupyter notebooks available in the Git repository is dedicated to
    TensorFlow Hub and its usage.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Git 仓库中提供的一个 Jupyter notebook 专门介绍了 TensorFlow Hub 及其使用方法。
- en: The Keras model
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras 模型
- en: As with VGG, Keras provides an implementation of Inception V3, optionally, with
    weights pretrained on ImageNet. `tf.keras.applications.InceptionV3()` (refer to
    the documentation at [https://keras.io/applications/#inceptionv3](https://keras.io/applications/#inceptionv3))
    has the same signature as the one presented for VGG.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 与 VGG 一样，Keras 提供了 Inception V3 的实现，并且可以选择使用在 ImageNet 上预训练的权重。`tf.keras.applications.InceptionV3()`（参见文档：[https://keras.io/applications/#inceptionv3](https://keras.io/applications/#inceptionv3)）的函数签名与
    VGG 的一致。
- en: We have mentioned AlexNet, the winning solution of ILSVRC 2012, as well as VGGNet
    and GoogLeNet, which prevailed during the 2014 edition. You might be wondering
    who won in 2013\. The challenge that year was dominated by the **ZFNet** architecture
    (named after its creators, Matthew Zeiler and Rob Fergus from New York University).
    If ZFNet is not covered in this chapter, it is because its architecture was not
    particularly innovative, and has not really been reused afterward.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到了 AlexNet，这是 2012 年 ILSVRC 的获胜解决方案，以及 VGGNet 和 GoogLeNet，它们在 2014 年的比赛中脱颖而出。你可能会想知道
    2013 年是谁赢得了比赛。那一年的挑战赛被 **ZFNet** 架构主导（以其创建者 Matthew Zeiler 和 Rob Fergus 为名，二人来自纽约大学）。如果本章没有介绍
    ZFNet，那是因为其架构并不特别创新，并且此后并未广泛使用。
- en: However, Zeiler and Fergus' significant contribution lay somewhere else—they
    developed and applied several operations to the visualization of CNNs (such as **unpooling**
    and **transposed convolution**, also known as **deconvolution**, which are both
    detailed in [Chapter 6](c4bb2429-f9f5-424d-8462-e376fd81f5a4.xhtml), *Enhancing
    and Segmenting Images*). Indeed, a common criticism of neural networks was that
    they behave like *black boxes*, and that no one can really grasp why and how they
    work so well. Zeiler and Fergus' work was an important first step toward opening
    up CNNs to reveal their inner processes (such as how they end up reacting to particular
    features and how they learn more abstract concepts as they go deeper.) Visualizing
    how each layer of their network reacted to specific images and contributed to
    the final prediction, the authors were able to optimize its hyperparameters and
    thus improve its performance (*Visualizing and Understanding Convolutional Networks*,
    Springer, 2014).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Zeiler 和 Fergus 的重要贡献在于他们在其他方面的工作——他们开发并应用了多个卷积神经网络（CNN）可视化操作（如**反池化**和**转置卷积**，也称为**反卷积**，这两者在[第
    6 章](c4bb2429-f9f5-424d-8462-e376fd81f5a4.xhtml)《增强与图像分割》中有详细介绍）。事实上，神经网络的一个常见批评是它们像*黑盒子*一样运行，没人真正理解它们为什么以及如何如此有效。Zeiler
    和 Fergus 的工作是揭示 CNN 内部过程的重要第一步（例如，它们是如何对特定特征作出反应的，以及它们如何随着层次加深学习更加抽象的概念）。通过可视化每一层网络对特定图像的反应及其对最终预测的贡献，作者能够优化超参数，从而提高模型性能（《卷积神经网络的可视化与理解》，Springer，2014）。
- en: Research toward understanding neural networks is still ongoing (for instance,
    with a multitude of recent work capturing and analyzing the *attention* of networks
    toward specific elements) and has already greatly helped to improve current systems.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对神经网络的理解研究仍在进行中（例如，最近的许多工作捕捉并分析了网络对特定元素的*注意力*），并且已经极大地帮助改进了当前的系统。
- en: ResNet – the residual network
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ResNet —— 残差网络
- en: The last architecture we will address in this chapter won the 2015 edition of
    ILSVRC. Composed of a new kind of module, the residual module, **ResNet** (**residual
    network**) provides an efficient approach to creating very deep networks, beating
    larger models such as Inception in terms of performance.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们将讨论的最后一个架构，在 2015 年的 ILSVRC 中获胜。它由一种新型模块——残差模块组成，**ResNet**（**残差网络**）提供了一种高效的方法来构建非常深的网络，在性能上超越了更大的模型，如
    Inception。
- en: Overview of the ResNet architecture
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ResNet 架构概述
- en: Developed by Kaiming He et al., researchers at Microsoft, the ResNet architecture
    is an interesting solution to learning problems affecting CNNs. Following the
    structure of previous sections, we will first clarify the author's targets and
    introduce their novel architecture (refer to *Deep Residual Learning for Image
    Recognition*, Proceedings of the CVPR IEEE conference, 2016).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 由微软研究员 Kaiming He 等人开发的 ResNet 架构，是一个有趣的解决方案，旨在解决影响 CNN 的学习问题。按照前述章节的结构，我们将首先明确作者的目标，并介绍他们的新颖架构（参见*深度残差学习用于图像识别*，CVPR
    IEEE 会议论文集，2016）。
- en: Motivation
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: Inception networks demonstrated that going larger is a valid strategy in image
    classification, as well as other recognition tasks. Nevertheless, experts still
    kept trying to increase networks in order to solve more and more complex tasks.
    However, the question *Is learning better networks as easy as stacking more layers?*,
    asked in the preamble of the paper written by He et al., is justified.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Inception 网络证明，增加网络规模是图像分类以及其他识别任务中的一种有效策略。然而，专家们仍然不断尝试通过增加网络的深度来解决越来越复杂的任务。然而，He
    等人在论文前言中提出的那个问题*“学习更好的网络就像堆叠更多的层一样简单吗？”*是有道理的。
- en: We know already that the deeper a network goes, the harder it becomes to train
    it. But besides the *vanishing/exploding gradient* problems (covered by other
    solutions already), He et al. pointed out another problem that deeper CNNs face—*performance
    degradation*. It all started with a simple observation—the accuracy of CNNs does
    not linearly increase with the addition of new layers. A degradation problem appears
    as the networks' depth increases. Accuracy starts saturating and even degrading.
    Even the training loss starts decreasing when negligently stacking too many layers,
    proving that the problem is not caused by overfitting. For instance, the authors
    compared the accuracy of an 18-layer-deep CNN with a 34-layer one, showing that
    the latter performs worse than the shallower version during and after training.
    In their paper, He et al. proposed a solution to build very deep and performant
    networks.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道，网络越深，训练就越困难。但除了*梯度消失/爆炸*问题（已有其他解决方案处理过），He 等人指出了深度卷积神经网络（CNN）面临的另一个问题——*性能退化*。这一切源于一个简单的观察——CNN
    的准确率并不会随着新层的增加而线性提高。随着网络深度的增加，出现了退化问题。准确率开始饱和甚至下降。即使是训练损失，在不加注意地堆叠过多层时，也开始下降，证明问题并不是由于过拟合引起的。例如，作者将
    18 层深的 CNN 和 34 层深的 CNN 进行了比较，结果显示后者在训练过程中以及训练后表现都不如较浅的版本。在他们的论文中，He 等人提出了解决方案，以构建非常深且具有良好性能的网络。
- en: With *model averaging* (applying ResNet models of various depths) and *prediction
    averaging* (over multiple crops of each input image), the ResNet authors reached
    a historically low 3.6% top-5 error rate for the ILSVRC challenge. This was the
    first time an algorithm beat humans on that dataset. Human performance had been
    measured by the challenge organizers, with the best human candidate reaching a
    5.1% error rate (refer to *ImageNet Large-Scale Visual Recognition Challenge*,
    Springer, 2015). Achieving super-human performance on such a task was a huge milestone
    for deep learning. We should, however, keep in mind that, while algorithms can
    expertly solve a specific task, they still do not have the human ability to extend
    that knowledge to others, or to grasp the context of the data they are to deal
    with.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过*模型平均*（应用不同深度的 ResNet 模型）和*预测平均*（对每个输入图像的多个裁剪结果进行预测平均），ResNet 作者在 ILSVRC 挑战中达到了历史最低的
    3.6% 的 top-5 错误率。这是首次有算法在该数据集上超过了人类。挑战组织者曾测量人类的表现，最佳人类候选者的错误率为 5.1%（参见*ImageNet
    大规模视觉识别挑战*，Springer，2015）。在这样的任务中实现超越人类的表现，是深度学习的一个巨大里程碑。然而，我们仍需牢记，尽管算法能够熟练地解决特定任务，但它们仍不具备将这些知识扩展到其他任务，或者理解所处理数据的上下文的能力。
- en: Architecture
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: 'Like Inception, ResNet has known several iterative improvements to its architecture,
    for instance, with the addition of bottleneck convolutions or the use of smaller
    kernels. Like VGG, ResNet also has several pseudo-standardized versions characterized
    by their depth: ResNet-18, ResNet-50, ResNet-101, ResNet-152, and others. Indeed,
    the winning ResNet network for ILSVRC 2015 vertically stacked 152 trainable layers
    (with a total of 60 million parameters), which was an impressive feat at that
    time:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 与Inception类似，ResNet也经历了多次架构的迭代改进，例如添加了瓶颈卷积或使用了更小的卷积核。与VGG类似，ResNet也有多个伪标准化版本，特点是其深度：ResNet-18、ResNet-50、ResNet-101、ResNet-152等。事实上，2015年ILSVRC的获胜ResNet网络堆叠了152个可训练层（总共有6000万个参数），这是当时一个令人印象深刻的成就：
- en: '![](img/1cdf0cd3-2fb2-4866-bd2c-76b2b0aa52bf.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1cdf0cd3-2fb2-4866-bd2c-76b2b0aa52bf.png)'
- en: 'Figure 4.5: Exemplary ResNet architecture'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4.5: 示例 ResNet 架构'
- en: In the preceding diagram, all the convolutional and max-pooling layers have
    `SAME` for padding, and for stride *s = 1* if unspecified. Batch normalization
    is applied after each *3* × *3* convolution (on the residual path, in gray), and
    *1* × *1* convolutions (on the mapping path in black) have no activation function
    (identity).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，所有卷积层和最大池化层的填充方式都是`SAME`，并且步幅未指定时为*s = 1*。每个*3* × *3*卷积后都应用批量归一化（在残差路径上，灰色部分），而*1*
    × *1*卷积（在映射路径上，黑色部分）没有激活函数（为恒等映射）。
- en: As we can see in *Figure 4.5*, the ResNet architecture is slimmer than the Inception
    architecture, though it is similarly composed of layer blocks with parallel operations.
    Unlike Inception, where each parallel layer non-linearly processes the input information,
    ResNet blocks are composed of one non-linear path, and one identity path. The
    former (represented by the thinner gray arrows in *Figure 4.5*) applies a couple
    of convolutions with batch normalization and *ReLU* activation to the input feature
    maps. The latter (represented by the thicker black arrows) simply forward the
    features without applying any transformation.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在*图 4.5*中看到的，ResNet架构比Inception架构更简洁，尽管它也由具有并行操作的层块组成。与Inception不同的是，每个并行层会非线性地处理输入信息，而ResNet的模块由一个非线性路径和一个恒等路径组成。前者（由*图
    4.5*中较细的灰色箭头表示）对输入特征图进行若干次卷积、批量归一化和*ReLU*激活。后者（由较粗的黑色箭头表示）则简单地转发特征而不进行任何转换。
- en: The last statement is not always true. As shown in *Figure 4.5*, *1* × *1* convolutions
    are applied in order to adapt the depth of the features, when the depth is increased
    in parallel by the non-linear branches. On those occasions, to avoid a large increase
    in the number of parameters, the spatial dimensionality is also reduced on both
    sides using a stride of *s* = 2.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一条陈述并不总是正确的。如*图 4.5*所示，当特征的深度通过非线性分支并行增加时，使用*1* × *1*卷积来调整特征的深度。在这种情况下，为了避免参数数量的急剧增加，空间维度在两侧也使用步幅*s*
    = 2进行减小。
- en: As in inception modules, the feature maps from each branch (that is, the transformed
    features and the original ones) are merged together before being passed to the
    next block. Unlike inception modules, however, this merging is not performed through
    depth concatenation, but through element-wise addition (a simple operation that
    does not require any additional parameters). We will cover, in the following section,
    the benefits of these residual blocks.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 与Inception模块一样，来自每个分支的特征图（即转换后的特征和原始特征）在传递到下一个模块之前会被合并在一起。然而，与Inception模块不同的是，这种合并并不是通过深度连接来实现的，而是通过逐元素加法（这是一种简单的操作，不需要额外的参数）。我们将在接下来的部分中探讨这些残差模块的优势。
- en: Note that, in most implementations, the last *3* × *3* convolution of each residual
    block is not followed directly by *ReLU* activation. Instead, the non-linear function
    is applied after merging with the identity branch is done.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在大多数实现中，每个残差块的最后一个*3* × *3*卷积后并不会直接跟随*ReLU*激活。相反，非线性函数是在与恒等路径合并之后才应用的。
- en: Finally, the features from the last block are average-pooled and densely converted
    into predictions, as in GoogLeNet.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，来自最后一个模块的特征经过平均池化并密集地转换成预测结果，如同在GoogLeNet中一样。
- en: Contributions – forwarding the information more deeply
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贡献 – 更深入地传递信息
- en: Residual blocks have been a significant contribution to machine learning and
    computer vision. In the following section, we will cover the reasons for this.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 残差模块对机器学习和计算机视觉作出了重要贡献。在接下来的部分，我们将探讨这一点的原因。
- en: Estimating a residual function instead of a mapping
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计残差函数而非映射
- en: As the ResNet authors pointed out, the degradation phenomenon would not happen
    if layers could easily learn **identity mapping** (that is, if a set of layers
    could learn weights so that their series of operations finally return the same
    tensors as the input layers).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 ResNet 的作者所指出的，如果层能够轻松地学习**恒等映射**（也就是说，如果一组层能够学习权重，使得它们的一系列操作最终返回与输入层相同的张量），则不会发生退化现象。
- en: Indeed, the authors argue that, when adding some layers on top of a CNN, we
    should at least obtain the same training/validation errors if these additional
    layers were able to converge to the identity function. They would learn to at
    least pass the result of the original network without degrading it. Since that
    is not the case—as we can often observe a degradation—it means that identity mapping
    is not easy to learn for CNN layers.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，作者们认为，当在 CNN 上添加一些层时，如果这些额外的层能够收敛到恒等函数，我们应该至少获得相同的训练/验证误差。它们至少会学习将原始网络的结果传递下去，而不会使其退化。由于我们经常可以观察到退化现象，这意味着
    CNN 层并不容易学习到恒等映射。
- en: 'This led to the idea of introducing residual blocks, with two paths:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这促使了引入残差块的想法，残差块有两条路径：
- en: One path further processes the data with some additional convolutional layers
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一条路径进一步处理数据，增加了一些卷积层
- en: One path performs the identity mapping (that is, forwarding the data with no
    changes)
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一条路径执行恒等映射（也就是，直接转发数据，不做任何更改）
- en: We may intuitively grasp how this can solve the degradation problem. When adding
    a residual block on top of a CNN, its original performance can at least be preserved
    by setting the weights of the processing branch to zero, leaving only the predefined
    identity mapping. The processing path will only be considered if it benefits loss
    minimization.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直观地理解这如何解决退化问题。当在 CNN 上添加一个残差块时，至少可以通过将处理分支的权重设置为零来保持原有的性能，从而只留下预定义的恒等映射。只有当处理路径有助于最小化损失时，它才会被考虑。
- en: The data forwarding path is usually called **skip** or **shortcut**. The processing
    one is commonly called **residual** **path**, since the output of its operations
    is then added to the original input, with the magnitude of the processed tensor
    being much smaller than the input one when the identity mapping is close to optimal
    (hence, the term *residual*). Overall, this residual path only introduces small
    changes to the input data, making it possible to forward patterns to deeper layers.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转发路径通常被称为**跳跃**路径或**快捷方式**。处理路径通常被称为**残差****路径**，因为其操作的输出会被加到原始输入上，当恒等映射接近最优时，处理后的张量的大小比输入的张量小得多（因此使用了*残差*这一术语）。总体来说，这条残差路径仅对输入数据引入微小的变化，使得它能够将模式传递到更深的层。
- en: In their paper, He et al. demonstrate that their architecture not only tackles
    the degradation problem, but their ResNet models achieve better accuracy than
    traditional ones for the same number of layers.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的论文中，何等人展示了他们的架构不仅解决了退化问题，而且他们的 ResNet 模型在相同层数下比传统模型的准确率更高。
- en: Going ultra-deep
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 极深网络
- en: It is also worth noting that residual blocks do not contain more parameters
    than traditional ones, as the skip and addition operations do not require any.
    They can, therefore, be efficiently used as building blocks for *ultra-deep* networks.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得注意的是，残差块比传统块不包含更多的参数，因为跳跃和加法操作不需要额外的参数。因此，它们可以高效地作为*超深*网络的构建块。
- en: Besides the 152-layer network applied to the ImageNet challenge, the authors
    illustrated their contributions by training an impressive 1,202-layer one. They
    reported no difficulty training such a massive CNN (although its validation accuracy
    was slightly lower than for the 152-layer network, allegedly because of overfitting).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 除了应用于 ImageNet 挑战的 152 层网络外，作者们还通过训练一个令人印象深刻的 1,202 层网络来展示他们的贡献。他们报告说，训练这样一个庞大的卷积神经网络（CNN）没有遇到困难（尽管其验证准确率略低于
    152 层网络，可能是因为过拟合）。
- en: More recent works have been exploring the use of residual computations to build
    deeper and more efficient networks, such as **Highway** networks (with a trainable
    switch value to decide which path should be used for each residual block) or **DenseNet**
    models (adding further skip connections between blocks).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 更近的研究开始探索利用残差计算来构建更深更高效的网络，例如**高速公路**网络（带有可训练的开关值来决定每个残差块应使用哪个路径）或**DenseNet**模型（在块之间添加更多的跳跃连接）。
- en: Implementations in TensorFlow and Keras
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 和 Keras 的实现
- en: As with previous architectures, we already have the tools needed to reimplement
    ResNet ourselves, while also having direct access to preimplemented/pretrained
    versions.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的架构一样，我们已经具备了重新实现 ResNet 所需的工具，并且可以直接访问预实现/预训练版本。
- en: Residual blocks with the Keras Functional API
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 函数式 API 实现残差块
- en: As practice, let's implement a basic residual block ourselves. As shown in *Figure
    4.5*, the residual path consists of two convolutional layers, each one followed
    by batch normalization. The *ReLU* activation function is applied directly after
    the first convolution. For the second, the function is only applied after merging
    with the other path. Using the Keras Functional API, the residual path can thus
    be implemented in a matter of five or six lines, as demonstrated in the following
    code.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，让我们自己实现一个基本的残差块。如*图 4.5*所示，残差路径由两个卷积层组成，每个卷积层后跟一个批量归一化层。*ReLU* 激活函数直接应用于第一个卷积层之后。对于第二个卷积层，激活函数仅在与另一条路径合并之后应用。使用
    Keras 函数式 API，残差路径可以通过五六行代码轻松实现，具体代码如下所示。
- en: The shortcut path is even simpler. It contains either no layer at all, or a
    single *1* × *1* convolution to reshape the input tensor when the residual path
    is altering its dimensions (for instance, when a larger stride is used).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 快捷路径甚至更简单。它要么没有任何层，要么只有一个 *1* × *1* 的卷积层，用于在残差路径改变输入张量的维度时（例如，当使用更大的步幅时）进行重塑。
- en: 'Finally, the results of the two paths are added together, and the *ReLU* function
    is applied to the sum. All in all, a basic residual block can be implemented as
    follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将两条路径的结果相加，并对和应用*ReLU*函数。总的来说，一个基本的残差块可以如下实现：
- en: '[PRE4]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A more elegant function is presented in one of the Jupyter notebooks. This notebook
    also contains a complete implementation of the ResNet architecture and a brief
    demonstration of a classification problem.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个 Jupyter notebook 中展示了一个更优雅的函数。该 notebook 还包含了 ResNet 架构的完整实现和一个分类问题的简要演示。
- en: The TensorFlow model and TensorFlow Hub
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 模型与 TensorFlow Hub
- en: Like the Inception networks, ResNet ones have their own official implementation
    provided in the `tensorflow/models` Git repository, as well as their own pretrained
    TensorFlow Hub modules.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 和 Inception 网络一样，ResNet 网络也有自己的官方实现，提供在 `tensorflow/models` Git 仓库中，并且有自己的预训练
    TensorFlow Hub 模块。
- en: We invite you to check out the official `tensorflow/models` implementation,
    as it offers several types of residual blocks from more recent research efforts.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们邀请你查看官方的 `tensorflow/models` 实现，因为它提供了多个来自最新研究的残差块类型。
- en: The Keras model
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras 模型
- en: Finally, Keras once again provides its own ResNet implementations—for instance,
    `tf.keras.applications.ResNet50()` (refer to the documentation at [https://keras.io/applications/#resnet50](https://keras.io/applications/#resnet50))—with
    the option to load parameters pretrained on ImageNet. These methods have the same
    signature as previously covered Keras applications.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Keras 再次提供了自己的 ResNet 实现——例如，`tf.keras.applications.ResNet50()`（请参考 [https://keras.io/applications/#resnet50](https://keras.io/applications/#resnet50)
    中的文档）——并可以加载在 ImageNet 上预训练的参数。这些方法的签名与之前介绍的 Keras 应用程序相同。
- en: The complete code for the usage of this Keras application is also provided in
    the Git repository.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Keras 应用程序的完整代码也已提供在 Git 仓库中。
- en: The list of CNN architectures presented in this chapter does not pretend to
    be exhaustive. It has been curated to cover solutions both instrumental to the
    computer vision domain and of pedagogical value.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中展示的 CNN 架构列表并不声称是详尽无遗的。它经过精心挑选，涵盖了计算机视觉领域中的重要解决方案和具有教学价值的内容。
- en: As research in visual recognition keeps moving forward at a fast pace, more
    advanced architectures are being proposed, building upon previous solutions (as
    Highway and DenseNet methods do for ResNet, for instance), merging them (as with
    the Inception-ResNet solution), or optimizing them for particular use cases (such
    as the lighter MobileNet, which was made to run on smartphones). It is, therefore,
    always a good idea to check what the state of the art has to offer (for example,
    on official repositories or research journals) before trying to reinvent the wheel.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 随着视觉识别领域的研究不断快速进展，越来越多的先进架构应运而生，这些架构是在之前的解决方案（例如 Highway 和 DenseNet 方法对 ResNet
    的改进）基础上提出的，或通过合并它们（如 Inception-ResNet 解决方案）而成，或为特定的使用场景进行了优化（如专为手机运行的轻量级 MobileNet）。因此，在尝试重新发明轮子之前，查看一下当前的技术前沿总是个不错的主意（例如，在官方仓库或研究期刊中）。
- en: Leveraging transfer learning
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用迁移学习
- en: This idea of reusing knowledge provided by others is not only important in computer
    science. The development of human technology over the millennia is the result
    of our ability to transfer knowledge from one generation to another, and from
    one domain to another. Many researchers believe that applying this guidance to
    machine learning could be one of the keys to developing more proficient systems
    that will be able to solve new tasks without having to relearn everything from
    scratch.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这种重新利用他人提供的知识的理念不仅在计算机科学中非常重要。人类技术的发展几千年来，正是源于我们将知识从一代传递到另一代、从一个领域传递到另一个领域的能力。许多研究人员认为，将这一理念应用于机器学习，可能是开发更高效系统的关键之一，使其能够解决新任务，而无需从头开始重新学习一切。
- en: Therefore, this section will present what **transfer learning** means for artificial
    neural networks, and how it can be applied to our models.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本节将介绍**迁移学习**对人工神经网络的意义，以及它如何应用于我们的模型。
- en: Overview
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: We will first introduce what transfer learning is and how it is performed in
    deep learning, depending on the use cases.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍什么是迁移学习，以及根据不同的应用场景，它是如何在深度学习中实现的。
- en: Definition
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义
- en: In the first part of this chapter, we presented several well-known CNNs, developed
    for the ImageNet classification challenge. We mentioned that these models are
    commonly repurposed for a broader range of applications. In the following pages,
    we will finally elaborate on the reasons behind this reconditioning and how it
    is performed.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一部分，我们介绍了几种为ImageNet分类挑战赛开发的著名CNN。我们提到这些模型通常被重新利用到更广泛的应用中。在接下来的章节中，我们将最终详细说明这种重新调适的原因及其实施方法。
- en: Human inspiration
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人类启示
- en: Like many developments in machine learning, transfer learning is inspired by
    our own human way of tackling complex tasks and gathering knowledge.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 就像许多机器学习的发展一样，迁移学习的灵感来源于我们人类处理复杂任务和获取知识的方式。
- en: As mentioned in the introduction of this section, the first inspiration is our
    ability as a species to transfer knowledge from one individual to another. Experts
    can efficiently transfer the precious knowledge they have gathered over the years
    to a large number of students through oral or written teaching. By harnessing
    the knowledge that has been accumulated and distilled generation after generation,
    human civilizations have been able to continuously refine and extend their technical
    abilities. Phenomena that took millennia for our ancestors to understand— such
    as human biology, the solar system, and more—became common knowledge.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如本节引言中所提到的，第一个灵感来源是我们作为一个物种将知识从一个个体转移到另一个个体的能力。专家可以通过口头或书面教学，迅速将他们多年来积累的宝贵知识传授给大量学生。通过利用一代又一代积累并提炼的知识，人类文明能够不断精炼并扩展其技术能力。我们祖先花费千年才理解的现象——如人类生物学、太阳系等——已成为常识。
- en: Furthermore, as individuals, we also have the ability to transfer some expertise
    from one task to another. For example, people mastering one foreign language have
    an easier time learning similar ones. Similarly, people who have been driving
    a car for some time already have knowledge of the rules of the road and some related
    reflexes, which are useful if they want to learn how to drive other vehicles.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作为个体，我们也具备将某些专长从一项任务转移到另一项任务的能力。例如，掌握一门外语的人更容易学习类似的语言。类似地，已经有一段时间驾驶汽车的人，已经掌握了交通规则和一些相关的反应技能，这些对他们学习驾驶其他车辆非常有帮助。
- en: These abilities to master complex tasks by building upon available knowledge,
    and to repurpose acquired skills to similar activities, are central to human intelligence.
    Researchers in machine learning dream of reproducing them.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这些通过利用已有知识来掌握复杂任务的能力，以及将获得的技能转用于相似活动的能力，是人类智慧的核心。机器学习领域的研究人员梦寐以求能够复制这些能力。
- en: Motivation
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: Unlike humans, most machine learning systems have been designed, so far, for
    single, specific tasks. Directly applying a trained model to a different dataset
    would yield poor results, especially if the data samples do not share the same
    semantic content (for instance, MNIST digit images versus ImageNet photographs)
    or the same image quality/distribution (for instance, a dataset of smartphone
    pictures versus a dataset of high-quality pictures). As CNNs are trained to extract
    and interpret specific features, their performance will be compromised if the
    feature distribution changes. Therefore, some transformations are necessary to
    apply networks to new tasks.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 与人类不同，迄今为止，大多数机器学习系统都是为单一特定任务设计的。直接将训练好的模型应用到不同的数据集上通常会得到较差的结果，尤其是当数据样本不共享相同的语义内容（例如，MNIST手写数字图像与ImageNet照片）或相同的图像质量/分布（例如，智能手机图片数据集与高质量图片数据集）时。由于CNN被训练来提取和解释特定特征，因此如果特征分布发生变化，它们的性能将会受到影响。因此，必须进行一些转换才能将网络应用于新任务。
- en: 'Solutions have been investigated for decades. In 1998, Sebastian Thrun and
    Lorien Pratt edited *Learning to Learn*, a book compiling the prevalent research
    stands on the topic. More recently, in their *Deep Learning* book ([http://www.deeplearningbook.org/contents/representation.html](http://www.deeplearningbook.org/contents/representation.html) on
    page 534, MIT Press), Ian Goodfellow, Yoshua Bengio, and Aaron Courville defined
    transfer learning as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案已经被研究了几十年。在1998年，Sebastian Thrun和Lorien Pratt编辑了*Learning to Learn*，一本汇编了该主题流行研究观点的书籍。最近，在他们的*Deep
    Learning*书中（[http://www.deeplearningbook.org/contents/representation.html](http://www.deeplearningbook.org/contents/representation.html)，MIT出版社，第534页），Ian
    Goodfellow、Yoshua Bengio和Aaron Courville将迁移学习定义如下：
- en: '[...] the situation where what has been learned in one setting (for example,
    distribution p[1]) is exploited to improve generalization in another setting (say,
    distribution p[2]).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[...] 这种情况是指在一个设置中学到的知识（例如，分布p[1]）被用来提高另一个设置中的泛化能力（例如，分布p[2]）。'
- en: It makes sense for researchers to suppose that, for example, some of the features
    a CNN is extracting to classify hand-written digits could be partially reused
    for the classification of hand-written texts. Similarly, a network that learned
    to detect human faces could be partially repurposed for the evaluation of facial
    expressions. Indeed, even though the inputs (full images for face detection versus
    cropped ones for the new task) and outputs (detection results versus classification
    values) are different, some of the network's layers are already trained to extract
    facial features, which is useful for both tasks.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 对研究人员来说，假设例如CNN提取的一些特征可以部分重用于分类手写文本是合理的，尤其是手写数字分类。类似地，一个学习了检测人脸的网络可以部分用于评估面部表情。事实上，尽管输入（用于人脸检测的完整图像与新任务的裁剪图像）和输出（检测结果与分类值）不同，但该网络的一些层已经被训练来提取面部特征，这对这两项任务都有帮助。
- en: In machine learning, a **task** is defined by the inputs provided (for example,
    pictures from smartphones) and the expected outputs (for example, prediction results
    for a specific set of classes). For instance, classification and detection on
    ImageNet are two different tasks with the same input images but different outputs.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，**任务**由提供的输入（例如，智能手机的图片）和期望的输出（例如，特定类别集的预测结果）定义。例如，ImageNet上的分类和检测是两个不同的任务，尽管它们使用相同的输入图像，但输出却不同。
- en: In some cases, algorithms can target similar tasks (for example, pedestrian
    detection) but have access to different sets of data (for example, CCTV images
    from different locations, or from cameras of different quality). These methods
    are thus trained on different **domains** (that is, data distributions).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，算法可以针对相似的任务（例如，行人检测），但使用不同的数据集（例如，不同位置的CCTV图像，或不同质量的摄像头图像）。因此，这些方法是在不同的**领域**（即，数据分布）上进行训练的。
- en: It is the goal of transfer learning to apply the knowledge either from one task
    to another or from one domain to another. The latter type of transfer learning
    is called **domain adaptation** and will be more specifically covered in [Chapter
    7](337ec077-c215-4782-b56c-beae4d94d718.xhtml), *Training on Complex and Scarce
    Datasets*.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习的目标是将知识从一个任务应用到另一个任务，或者从一个领域应用到另一个领域。后一种类型的迁移学习被称为**领域适应**，将在[第7章](337ec077-c215-4782-b56c-beae4d94d718.xhtml)中更具体地讨论，*在复杂和稀缺数据集上的训练*。
- en: Transfer learning is especially interesting when not enough data is available
    to properly learn the new task (that is, there are not enough image samples to
    estimate the distribution). Indeed, deep learning methods are data hungry; they
    require large datasets for their training. Such datasets—especially labeled ones
    for supervised learning—are often tedious, if not impossible, to gather. For example,
    experts building recognition systems to automate industries cannot go to every
    plant to take hundreds of pictures of every new manufactured product and its components.
    They often have to deal with much smaller datasets, which are not large enough
    for the CNNs to satisfactorily converge. Such limitations explain the efforts
    to reuse knowledge acquired on well-documented visual tasks for those other cases.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习在数据不足以充分学习新任务时尤为有趣（即，缺乏足够的图像样本来估计分布）。事实上，深度学习方法非常依赖数据；它们需要大量的数据集进行训练。这些数据集——特别是用于监督学习的标注数据集——通常非常繁琐，甚至不可能收集。例如，专家们在为工业自动化建立识别系统时，不能去每个工厂拍摄每个新制造的产品及其组件的数百张照片。他们通常不得不处理较小的数据集，而这些数据集对于CNN来说不足以令人满意地收敛。这些限制解释了为何要将已经在充分记录的视觉任务上获得的知识重用于其他任务的努力。
- en: With their millions of annotated images from a large number of categories, ImageNet—and,
    more recently, COCO—are particularly rich datasets. It is assumed that CNNs trained
    on those have acquired quite an expertise in visual recognition, hence the availability
    in Keras and TensorFlow Hub of standard models (Inception, ResNet-50, and others)
    already trained on these datasets. People looking for models to transfer knowledge
    from commonly use these.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet及最近的COCO是特别丰富的数据集，包含了来自大量类别的数百万张标注图像。假设在这些数据集上训练的CNN已获得了相当的视觉识别专业知识，因此，Keras和TensorFlow
    Hub提供了在这些数据集上已训练的标准模型（如Inception、ResNet-50等）。人们常常使用这些模型来进行知识迁移。
- en: Transferring CNN knowledge
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移CNN知识
- en: 'So, how can you transfer some knowledge from one model to another? Artificial
    neural networks have one advantage over human brains that facilitates this operation:
    they can be easily stored and duplicated. The expertise of a CNN is nothing but
    the values taken by its parameters after training—values that can easily be restored
    and transferred to similar networks.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何将某些知识从一个模型转移到另一个模型呢？人工神经网络相对于人脑有一个有利之处，那就是它们可以轻松存储和复制。CNN的专业知识无非是其参数在训练后得到的值——这些值可以很容易地恢复并转移到相似的网络中。
- en: Transfer learning for CNNs mostly consists of reusing the complete or partial
    architecture and weights of a performant network trained on a rich dataset to
    instantiate a new model for a different task. From this conditioned instantiation,
    the new model can then be *fine-tuned*; that is, it can be further trained on
    the available data for the new task/domain.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的迁移学习主要是通过重用在丰富数据集上训练的高性能网络的完整或部分架构和权重，来实例化一个用于不同任务的新模型。通过这种条件化的实例化，新模型可以进行*微调*；也就是说，它可以在可用数据上进一步训练，以适应新的任务或领域。
- en: As we highlighted in the previous chapters, the first layers of a network tend
    to extract low-level features (such as lines, edges, or color gradients), whereas
    final convolutional layers react to more complex notions (such as specific shapes
    and patterns). For classification tasks, the final pooling and/or fully connected
    layers then process these high-level feature maps (often called **bottleneck features**)
    to make their class predictions.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前几章中强调的那样，网络的第一层通常提取低级特征（如线条、边缘或颜色渐变），而最终的卷积层则反应更为复杂的概念（如特定的形状和模式）。对于分类任务，最终的池化层和/或全连接层处理这些高级特征图（通常称为**瓶颈特征**），从而做出类别预测。
- en: This typical setup and related observations led to various transfer learning
    strategies. Pretrained CNNs, with their final prediction layers removed, started
    being used as efficient *feature extractors*. When the new task is similar enough
    to the ones these extractors were trained for, they can directly be used to output
    pertinent features (the *image feature vector* models on TensorFlow Hub are available
    for that exact purpose). These features can then be processed by one or two new
    dense layers, which are trained to output the task-related predictions. To preserve
    the quality of the extracted features, the layers of the feature extractors are
    often *frozen* during this training phase; that is, their parameters are not updated
    during the gradient descent. In other cases, when the tasks/domains are less similar,
    some of the last layers of the feature extractors—or all of them—are *fine-tuned*;
    that is, trained along with the new prediction layers on the task data. These
    different strategies are further explained in the next paragraphs.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这一典型的设置和相关观察促成了各种迁移学习策略的提出。去掉最终预测层的预训练CNN，开始作为高效的*特征提取器*使用。当新任务与这些提取器训练的任务足够相似时，它们可以直接用来输出相关的特征（例如，TensorFlow
    Hub上的*图像特征向量*模型正是为了这个目的而存在）。然后，这些特征可以通过一两个新的全连接层进行处理，这些层经过训练后可以输出与任务相关的预测。为了保持提取特征的质量，特征提取器的层通常会在训练阶段被*冻结*；即，它们的参数在梯度下降过程中不会被更新。在其他情况下，当任务/领域不太相似时，特征提取器的最后几层——或者所有层——会被*微调*；也就是说，这些层与新的预测层一起在任务数据上进行训练。这些不同的策略将在接下来的段落中进一步解释。
- en: Use cases
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用场景
- en: In practice, which pretrained model should we reuse? Which layers should be
    frozen or fine-tuned? The answers to these questions depend on the similarity
    between the target task and the tasks that models have already been trained on,
    as well as the abundance of training samples for the new application.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们应该重用哪个预训练模型？哪些层应该被冻结或微调？这些问题的答案取决于目标任务与模型已训练任务之间的相似性，以及新应用的训练样本的丰富性。
- en: Similar tasks with limited training data
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相似任务，拥有有限的训练数据
- en: Transfer learning is especially useful when you want to solve a particular task
    and do not have enough training samples to properly train a performant model,
    but do have access to a larger and similar training dataset.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习尤其有用，当你想解决一个特定任务且没有足够的训练样本来有效训练一个高性能模型时，但你却能够访问到一个更大且相似的训练数据集。
- en: The model can be pretrained on this larger dataset until convergence (or, if
    available and pertinent, we can fetch an available pretrained model). Then, its
    final layers should be removed (when the target task is different, that is, its
    output differs from the pretraining task) and replaced with layers adapted to
    the target task. For example, imagine that we want to train a model to distinguish
    between pictures of bees and pictures of wasps. ImageNet contains images for these
    two classes, which could be used as a training dataset, but their number is not
    high enough for an efficient CNN to learn without overfitting. However, we could
    first train this network on the full ImageNet dataset to classify from the 1,000
    categories to develop broader expertise. After this pretraining, its final dense
    layers can be removed and replaced by layers configured to output predictions
    for our two target classes.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型可以在更大的数据集上进行预训练，直到收敛（或者，如果可用且相关，我们可以获取一个已经预训练的模型）。然后，当目标任务不同（即其输出与预训练任务不同）时，应移除其最终层，并用适应目标任务的层进行替换。例如，假设我们想训练一个模型来区分蜜蜂和黄蜂的图片。ImageNet中包含这两个类别的图片，可以用作训练数据集，但它们的数量不足以让一个高效的CNN在不发生过拟合的情况下进行学习。然而，我们可以首先在完整的ImageNet数据集上训练该网络，以便从1,000个类别中进行分类，从而发展更广泛的专业知识。在这个预训练之后，其最终的全连接层可以被移除，并替换为配置成输出我们两个目标类别预测的层。
- en: As we mentioned earlier, the new model can finally be prepared for its task
    by freezing the pretrained layers and by training only the dense ones on top.
    Indeed, since the target training dataset is too small, the model would end up
    overfitting if we do not freeze its feature extractor component. By fixing these
    parameters, we make sure that the network keeps the expressiveness it developed
    on the richer dataset.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，新模型最终可以通过冻结预训练的层，并仅训练其上方的全连接层来为其任务做准备。实际上，由于目标训练数据集过小，如果我们不冻结其特征提取组件，模型最终会过拟合。通过固定这些参数，我们确保网络保持它在更丰富数据集上所发展出的表达能力。
- en: Similar tasks with abundant training data
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相似任务，拥有丰富的训练数据
- en: The bigger the training dataset available for the target task, the smaller the
    chances of the network overfitting if we completely retrain it. Therefore, in
    such cases, people commonly unfreeze the latest layers of the feature extractor.
    In other words, the bigger the target dataset is, the more layers there are that
    can be safely fine-tuned. This allows the network to extract features that are
    more relevant to the new task, and thus to better learn how to perform it.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 可用于目标任务的训练数据集越大，完全重新训练网络时网络过拟合的可能性就越小。因此，在这种情况下，通常会解冻特征提取器的最新层。换句话说，目标数据集越大，越多的层可以安全地进行微调。这使得网络能够提取与新任务相关性更高的特征，从而更好地学习如何执行该任务。
- en: The model has already been through a first training phase on a similar dataset
    and is probably close to convergence already. Therefore, it is common practice
    to use a smaller learning rate for the fine-tuning phase.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 模型已经在一个相似的数据集上经历了第一次训练阶段，并且可能已经接近收敛。因此，在微调阶段使用较小的学习率是常见做法。
- en: Dissimilar tasks with abundant training data
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 具有丰富训练数据的不相似任务
- en: If we have access to a rich enough training set for our application, does it
    even make sense to use a pretrained model? This question is legitimate if the
    similarity between the original and target tasks is too low. Pretraining a model,
    or even downloading pretrained weights, can be costly. However, researchers demonstrated
    through various experiments that, in most cases, it is better to initialize a
    network with pretrained weights (even from a dissimilar use case) than with random
    ones.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有足够丰富的训练集来进行应用，是否还有必要使用预训练模型？如果原始任务与目标任务之间的相似性太低，这个问题是合理的。预训练模型，甚至下载预训练权重，可能会很昂贵。然而，研究人员通过各种实验表明，在大多数情况下，用预训练权重（即使来自不相似的使用案例）初始化网络，通常比用随机权重初始化更为有效。
- en: Transfer learning makes sense when the tasks or their domains share at least
    some basic similarities. For instance, images and audio files can both be stored
    as two-dimensional tensors, and CNNs (such as ResNet ones) are commonly applied
    to both. However, the models are relying on completely different features for
    visual and audio recognition. It would typically not benefit a model for visual
    recognition to receive the weights from a network trained for an audio-related
    task.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习在任务或其领域至少具有某些基本相似性时才有意义。例如，图像和音频文件都可以作为二维张量存储，并且卷积神经网络（如 ResNet）通常应用于两者。然而，模型在视觉和音频识别中依赖的是完全不同的特征。通常情况下，视觉识别模型从训练音频相关任务的网络中获取权重并不会带来好处。
- en: Dissimilar tasks with limited training data
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 具有有限训练数据的不相似任务
- en: Finally, what if the target task is so specific that training samples are barely
    available and using pretrained weights does not make much sense? First, it would
    be necessary to reconsider applying or repurposing a deep model. Training such
    a model on a small dataset would lead to overfitting, and a deep pretrained extractor
    would return features that are too irrelevant for the specific task. However,
    we can still benefit from transfer learning if we keep in mind that the first
    layers of CNNs react to low-level features. Instead of only removing the final
    prediction layers of a pretrained model, we can also remove some of the last convolutional
    blocks, which are too task-specific. A shallow classifier can then be added on
    top of the remaining layers, and the new model can finally be fine-tuned.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果目标任务非常具体，以至于几乎没有训练样本可用，而使用预训练权重没有太大意义该怎么办？首先，需要重新考虑是否应用或重新利用深度模型。在小数据集上训练这样的模型会导致过拟合，而深度预训练提取器返回的特征对于特定任务来说可能过于无关。然而，如果我们记住卷积神经网络（CNN）的前几层会响应低层次的特征，仍然可以从迁移学习中受益。我们不仅可以去掉预训练模型的最终预测层，还可以去掉一些过于特定于任务的最后几个卷积块。然后，可以在剩余层之上添加一个浅层分类器，最终对新模型进行微调。
- en: Transfer learning with TensorFlow and Keras
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 和 Keras 进行迁移学习
- en: To conclude this chapter, we will briefly cover how transfer learning can be
    performed with TensorFlow and Keras. We invite our readers to go through the related
    Jupyter notebook in parallel, to have transfer learning illustrated on classification
    tasks.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结本章内容，我们将简要介绍如何使用 TensorFlow 和 Keras 执行迁移学习。我们邀请读者并行阅读相关的 Jupyter 笔记本，通过分类任务演示迁移学习的过程。
- en: Model surgery
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型手术
- en: Indirectly, we have already presented how standard pretrained models provided
    through TensorFlow Hub and Keras applications can be fetched and easily transformed
    into feature extractors for new tasks. However, it is also common to reuse non-standard
    networks; for example, more specific state-of-the-art CNNs provided by experts,
    or custom models already trained for some previous tasks. We will demonstrate
    how any models can be edited for transfer learning.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 间接地，我们已经展示了如何通过TensorFlow Hub和Keras应用提供的标准预训练模型，轻松地将其提取为新任务的特征提取器。然而，重新使用非标准网络也很常见；例如，由专家提供的更具体的最新CNNs，或者已经针对某些先前任务进行过训练的自定义模型。我们将演示如何编辑任何模型以进行迁移学习。
- en: Removing layers
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 删除层
- en: 'The first task is to remove the final layers of the pretrained model to transform
    it into a feature extractor. As usual, Keras makes this operation quite easy.
    For `Sequential` models, the list of layers is accessible through the `model.layers` attribute.
    This structure has a `pop()` method, which removes the last layer of the model.
    Therefore, if we know the number of final layers we need to remove to transform
    a network into a specific feature extractor (for instance, two layers for a standard
    ResNet model), this can be done as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项任务是删除预训练模型的最终层，将其转换为特征提取器。像往常一样，Keras使这个操作非常简单。对于`Sequential`模型，层的列表可以通过`model.layers`属性访问。这个结构有一个`pop()`方法，可以删除模型的最后一层。因此，如果我们知道需要删除的最终层的数量以将网络转换为特定的特征提取器（例如，标准ResNet模型的两层），可以像下面这样做：
- en: '[PRE5]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In pure TensorFlow, editing an operational graph supporting a model is neither
    simple nor recommended. However, we have to keep in mind that unused graph operations
    are not executed at runtime. So, still having the old layers present in the compiled
    graph will not affect the computational performance of the new model, as long
    as they are not called anymore. Therefore, instead of removing layers, we simply
    need to pinpoint the last layer/operation of the previous model we want to keep.
    If we somehow lost track of its corresponding Python object, but know its name
    (for instance, by checking the graph in Tensorboard), its representative tensor
    can be recovered by looping over the layers of the model and checking their names:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在纯TensorFlow中，编辑支持模型的操作图既不简单也不推荐。然而，我们必须记住，在运行时未使用的图操作不会被执行。因此，即使在编译图中保留旧层，也不会影响新模型的计算性能，只要它们不再被调用。因此，我们只需确定我们想保留的先前模型的最后一层/操作，而不是删除层。如果我们不知道其对应的Python对象，但知道其名称（例如，通过TensorBoard检查图表），则可以通过循环遍历模型的层并检查它们的名称来恢复其代表张量：
- en: '[PRE6]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'However, Keras provides additional methods to simplify this process. Knowing
    the name of the last layer to keep (for instance, after printing the names with
    `model.summary()`), a feature extractor model can be built in a couple of lines:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Keras提供了额外的方法来简化这个过程。知道要保留的最后一层的名称（例如，在使用`model.summary()`打印名称后），可以在几行代码中构建一个特征提取器模型：
- en: '[PRE7]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Sharing its weights with the original model, this feature-extraction model is
    ready for use.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 将其权重与原始模型共享，这个特征提取模型已经准备好使用。
- en: Grafting layers
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嫁接层
- en: 'Adding new prediction layers on top of a feature extractor is straightforward
    (compared with previous examples with TensorFlow Hub), as it is just a matter
    of adding new layers on top of the corresponding model. For example, this can
    be done as follows, using the Keras API:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征提取器之上添加新的预测层相对简单（与以前的TensorFlow Hub示例相比），因为只需在相应模型的顶部添加新层。例如，可以使用Keras API如下完成这项工作：
- en: '[PRE8]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As we can see, through Keras, TensorFlow 2 makes it straightforward to shorten,
    extend, or combine models!
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，通过Keras，TensorFlow 2使缩短、扩展或组合模型变得简单！
- en: Selective training
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择性训练
- en: Transfer learning makes the training phase a bit more complex because we should
    first restore the pretrained layers and define which ones should be frozen. Thankfully,
    several tools are available that simplify these operations.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习使训练阶段变得更加复杂，因为我们首先应该恢复预训练层，并定义哪些层应该被冻结。幸运的是，有几个工具可用来简化这些操作。
- en: Restoring pretrained parameters
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 恢复预训练参数
- en: 'TensorFlow has some utility functions to warm-start estimators; that is, to
    initialize some of their layers with pretrained weights. The following snippet
    tells TensorFlow to use the saved parameters of a pretrained estimator for the
    new one for the layers sharing the same name:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 有一些实用函数可以为估算器进行热启动；即初始化其中一些层的预训练权重。以下代码片段告诉 TensorFlow 使用预训练估算器的保存参数来为具有相同名称的层的新估算器初始化：
- en: '[PRE9]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `WarmStartSettings` initializer takes an optional `vars_to_warm_start` parameter,
    which can also be used to provide the names of the specific variables (as a list
    or a regex) that you want to restore from the checkpoint files (refer to the documentation
    for more details at [https://www.tensorflow.org/api_docs/python/tf/estimator/WarmStartSettings](https://www.tensorflow.org/api_docs/python/tf/estimator/WarmStartSettings)).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`WarmStartSettings` 初始化器接受一个可选的 `vars_to_warm_start` 参数，该参数还可以用于提供您希望从检查点文件中恢复的特定变量的名称（作为列表或正则表达式）（有关更多详细信息，请参阅[https://www.tensorflow.org/api_docs/python/tf/estimator/WarmStartSettings](https://www.tensorflow.org/api_docs/python/tf/estimator/WarmStartSettings)
    的文档）。'
- en: 'With Keras, we can simply restore the pretrained model before its transformation
    for the new task:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Keras，我们可以在为新任务进行转换之前简单地恢复预训练模型：
- en: '[PRE10]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Although it is not exactly optimal to restore the complete model before removing
    some of its layers, this solution has the advantage of being concise.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在删除一些层之前完全恢复完整模型并不完全理想，但这种解决方案的优点在于简洁性。
- en: Freezing layers
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 冻结层
- en: 'In TensorFlow, the most versatile method for freezing layers consists of removing
    their `tf.Variable` attributes from the list of variables passed to the optimizer:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，冻结层的最通用方法是从传递给优化器的变量列表中删除它们的 `tf.Variable` 属性：
- en: '[PRE11]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In Keras, layers have a `.trainable` attribute, which can simply be set to
    `False` in order to freeze them:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，层具有 `.trainable` 属性，只需将其设置为 `False` 即可冻结它们：
- en: '[PRE12]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Again, for complete transfer learning examples, we invite you to go through
    the Jupyter notebooks.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，为了完整的迁移学习示例，我们邀请您查阅 Jupyter 笔记本。
- en: Summary
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Classification challenges, such as ILSVRC, are great playgrounds for researchers,
    leading to the development of more advanced deep learning solutions. In their
    own way, each of the architectures we detailed in this chapter became instrumental
    in computer vision and are still applied to increasingly complex applications.
    As we will see in the following chapters, their technical contributions inspired
    other methods for a wide range of visual tasks.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如 ILSVRC 等分类挑战是研究人员的良好试验场，导致了更先进的深度学习解决方案的发展。在这一章中详细介绍的每个架构都以其独特的方式成为计算机视觉中的重要组成部分，并仍然应用于越来越复杂的应用程序。正如我们将在接下来的章节中看到的那样，它们的技术贡献激发了其他方法，适用于广泛的视觉任务。
- en: Moreover, not only did we learn to reuse state-of-the-art solutions, but we
    also discovered how algorithms themselves can benefit from the knowledge acquired
    from previous tasks. With transfer learning, the performance of CNNs can be greatly
    improved for specific applications. This is especially true for tasks such as
    object detection, which will be the topic of our next chapter. Annotating datasets
    for object detection is more tedious than for image-level recognition, so methods
    usually have access to smaller training datasets. It is, therefore, important
    to keep transfer learning in mind as a solution to obtain efficient models.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们不仅学会了重用最先进的解决方案，还发现了算法本身如何从先前任务获取的知识中受益。通过迁移学习，CNN 的性能可以在特定应用中得到极大的改善。这对于诸如目标检测之类的任务尤为重要，这也将是我们下一章的主题。对于目标检测的数据集注释比图像级别识别更为繁琐，因此方法通常只能访问较小的训练数据集。因此，牢记迁移学习作为获取高效模型的解决方案非常重要。
- en: Questions
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Which TensorFlow Hub module can be used to instantiate an inception classifier
    for ImageNet?
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个 TensorFlow Hub 模块可以用于实例化 ImageNet 的 Inception 分类器？
- en: How can you freeze the first three residual macro-blocks of a ResNet-50 model
    from Keras applications?
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何冻结 Keras 应用程序中 ResNet-50 模型的前三个残差宏块？
- en: When is transfer learning not recommended?
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 何时不建议使用迁移学习？
- en: Further reading
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Hands-On Transfer Learning with Python* ([https://www.packtpub.com/big-data-and-business-intelligence/hands-transfer-learning-python](https://www.packtpub.com/big-data-and-business-intelligence/hands-transfer-learning-python)),
    by Dipanjan Sarkar, Raghav Bali, and Tamoghna Ghosh: This book covers transfer
    learning in more detail, while applying deep learning to domains other than computer
    vision.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 Python 进行实践迁移学习* ([https://www.packtpub.com/big-data-and-business-intelligence/hands-transfer-learning-python](https://www.packtpub.com/big-data-and-business-intelligence/hands-transfer-learning-python))，作者：Dipanjan
    Sarkar、Raghav Bali 和 Tamoghna Ghosh：本书更详细地介绍了迁移学习，同时将深度学习应用于计算机视觉以外的领域。'
