- en: Predictive Analytics with TensorFlow and Deep Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow和深度神经网络进行预测分析
- en: TensorFlow is an open source library developed by **Google Brain Team**. It
    is used in large-scale machine learning applications, such as neural networks,
    and for making numerical computations. Developers are able to create dataflow
    graphs using TensorFlow. These graphs show the movement of data. TensorFlow can
    be used to train and run deep neural networks for various applications such as
    image recognition, machine language translation, and natural language processing.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是由**Google Brain团队**开发的开源库。它用于大规模机器学习应用，如神经网络，并用于数值计算。开发者可以使用TensorFlow创建数据流图，这些图展示了数据的流动。TensorFlow可以用于训练和运行深度神经网络，用于图像识别、机器语言翻译和自然语言处理等各种应用。
- en: We already know that predictive analytics is about providing predictions about
    unknown events. We are going to use it here with TensorFlow.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道，预测分析是关于对未知事件做出预测。我们将在这里使用TensorFlow来实现它。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Predictions with TensorFlow
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow进行预测
- en: Regression with **Deep Neural networks** (**DNNs**)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**深度神经网络**（**DNNs**）进行回归
- en: Classification with DNNs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DNNs进行分类
- en: Predictions with TensorFlow
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow进行预测
- en: We will perform the `hello world` example of deep learning. This example is
    used to check and ensure that a model is working as intended. For this, we will
    use the MNIST dataset.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行深度学习的`hello world`示例。这个示例用于检查和确保模型按预期工作。为此，我们将使用MNIST数据集。
- en: Introduction to the MNIST dataset
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST数据集简介
- en: 'MNIST stands for **Mixed National Institute of Standards and Technology**,
    which has produced a handwritten digits dataset. This is one of the most researched
    datasets in machine learning, and is used to classify handwritten digits. This
    dataset is helpful for predictive analytics because of its sheer size, allowing
    deep learning to work its magic efficiently. This dataset contains 60,000 training
    images and 10,000 testing images, formatted as 28 x 28 pixel monochrome images.
    The following screenshot shows the images contained in this dataset:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST代表**混合国家标准与技术研究所**，它生成了一个手写数字数据集。这是机器学习中最常被研究的数据集之一，用于分类手写数字。这个数据集因其庞大的规模而有助于预测分析，使得深度学习能够高效地发挥作用。该数据集包含60,000个训练图像和10,000个测试图像，格式为28
    x 28像素的单色图像。以下截图显示了该数据集中的图像：
- en: '![](img/29266a1b-4e37-493c-b841-555f004034c3.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29266a1b-4e37-493c-b841-555f004034c3.png)'
- en: In the preceding screenshot, we can see that, for every handwritten digit, there
    is a corresponding true label; we can therefore use this dataset to build classification
    models. So we can use the image to classify each into one of the 10 digits from
    0 to 9.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，我们可以看到，对于每个手写数字，都有一个对应的真实标签；因此，我们可以使用这个数据集来构建分类模型。所以我们可以利用图像将每个手写数字分类为0到9之间的其中一个数字。
- en: Building classification models using MNIST dataset
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MNIST数据集构建分类模型
- en: 'Let''s take a look at the following steps and learn to build a classification
    model:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下以下步骤，学习如何构建一个分类模型：
- en: 'We have to import the libraries that we will use in this dataset. Use the following
    lines of code to import the `tensorflow`, `numpy`, and `matplotlib` libraries:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要导入在这个数据集中将使用的库。使用以下代码行导入`tensorflow`、`numpy`和`matplotlib`库：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will import the `fully_connected` function, which we will be used to build
    the layers of our network, from `tensorflow.contrib.layers`.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从`tensorflow.contrib.layers`导入`fully_connected`函数，用于构建我们网络的各层。
- en: Elements of the DNN model
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNN模型的元素
- en: 'Before running the model, we first have to determine the elements that we will
    use in building a multilayer perceptron model. Following are the elements that
    we will use in this model:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行模型之前，我们首先需要确定构建多层感知器模型时将使用的元素。以下是我们将在此模型中使用的元素：
- en: '**Architecture**: The model contains 728 neurons in the input layer. This is
    because we have 28 images and each image has 28 pixels. Here, each pixel is a
    feature in this case, so we have 728 pixels. We will have 10 elements in the output
    layer, and we will also use three hidden layers, although we could use any number
    of hidden layers. Here, we will use three hidden layers. The number of neurons
    we will use in each layer is 350 in the first layer, 200 in the second one, and
    100 in the last layer.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构**：模型的输入层包含728个神经元。这是因为我们有28张图像，每张图像有28个像素。在这里，每个像素都是一个特征，因此我们有728个像素。输出层将包含10个元素，我们还将使用三个隐藏层，尽管我们可以使用任意数量的隐藏层。这里，我们将使用三个隐藏层。每个层中神经元的数量为：第一层350个，第二层200个，最后一层100个。'
- en: '**Activation function**: We will use the ReLU activation function, as shown
    in the following code block:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**：我们将使用ReLU激活函数，如以下代码块所示：'
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If the input is negative, the function outputs `0`, and if the input is positive
    the function just outputs the same value as the input. So, mathematically, the
    ReLU function looks similar to this. The following screenshot shows the lines
    of code used for generating the graphical representation of the ReLU activation
    function:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入是负数，函数输出`0`；如果输入是正数，函数输出与输入相同的值。因此，从数学上讲，ReLU函数看起来类似于这个。下面的截图展示了用于生成ReLU激活函数图形表示的代码行：
- en: '![](img/e5bfc50a-2e28-44e0-8cf2-294a24807836.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5bfc50a-2e28-44e0-8cf2-294a24807836.png)'
- en: It gains the maximum between `0` and the input. This activation function will
    be used in every neuron of the hidden layers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 它计算输入值与`0`之间的最大值。这个激活函数将在每个隐藏层的神经元中使用。
- en: '**Optimizing ****algorithm**: The optimizing algorithm used here is the gradient
    descent with a learning rate of 0.01.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化算法**：这里使用的优化算法是带有学习率0.01的梯度下降法。'
- en: '**Loss function**: For the `loss` function, we will use the `cross_entropy`
    function, but as with other loss functions that we have used in this book, this
    function measures the distance between the actual values and the predictions that
    the model makes.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数**：对于`loss`函数，我们将使用`cross_entropy`函数，但与本书中使用的其他损失函数一样，该函数测量实际值与模型预测值之间的距离。'
- en: '**Weights initialization strategy**: For this, we will use the Xavier initializer, a
    method that actually comes with the `fully_connected` function from TensorFlow
    as a default.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重初始化策略**：为此，我们将使用Xavier初始化器，这是一种实际上作为`fully_connected`函数的默认值随TensorFlow一起提供的方法。'
- en: '**Regularization strategy**: We are not going to use any regularization strategy.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化策略**：我们不会使用任何正则化策略。'
- en: '**Training strategy**: We are going to use 20 epochs. The dataset will be presented
    to the network 20 times, and in every iteration, we will use a batch size of 80\.
    So, we will present the data to the network 80 points at a time and the whole
    dataset 20 times.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练策略**：我们将使用20个epoch。数据集将被展示给网络20次，每次迭代中我们将使用批量大小80。因此，我们将一次向网络提供80个数据点，并且整个数据集会展示20次。'
- en: Building the DNN
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建DNN
- en: Now we will import the dataset that we are going to use. The reason for using
    this dataset is that it is easily available. We are going to actually use this
    dataset and build a DNN model around it. In the next sections, we will see the
    steps involved in building a DNN model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将导入我们要使用的数据集。选择这个数据集的原因是它容易获得。我们将实际使用这个数据集，并围绕它构建一个DNN模型。在接下来的部分中，我们将看到构建DNN模型的步骤。
- en: Reading the data
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取数据
- en: 'Here, we read data in the cell. The following screenshot shows the lines of
    code used to read the data:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们在单元格中读取数据。以下截图展示了用于读取数据的代码行：
- en: '![](img/d3aaf54f-010c-4f34-a41a-d7f11fef7522.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3aaf54f-010c-4f34-a41a-d7f11fef7522.png)'
- en: Defining the architecture
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义架构
- en: 'We will use three hidden layers, with 256 neurons for the first layer, 128
    for the second, and 64 for the third one. The following code snippet shows the
    architecture for the classification example:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用三个隐藏层，第一个层有256个神经元，第二个层有128个神经元，第三个层有64个神经元。以下代码片段展示了分类示例的架构：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Placeholders for inputs and labels
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入和标签的占位符
- en: 'The values of different layers are the objects, and are also called placeholders
    for inputs and labels. These placeholders are used for feeding the data into the
    network. The following lines of code are used for showing placeholders for the
    inputs and labels:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 不同层的值是对象，也被称为输入和标签的占位符。这些占位符用于将数据输入网络。以下代码行用于显示输入和标签的占位符：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So we have a placeholder `X` for the features, which is the input layer, and
    we have a placeholder `y` for the target value. So this object will contain the
    actual true labels of the digits.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有一个占位符`X`用于特征，这是输入层，另外还有一个占位符`y`用于目标值。因此，这个对象将包含数字的真实标签。
- en: Building the neural network
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建神经网络
- en: 'For building DNNs, we use the `fully_connected` function for the first hidden
    layer. The input for this hidden layer is `x`, which is the data from the placeholder. `n_hidden1`
    is the number of neurons that we have in this hidden layer, which you will remember
    is 350 neurons. Now, this hidden layer 1 becomes the input for the hidden layer
    2, and `n_hidden2` is the number of neurons in this layer. Likewise, hidden layer
    2 becomes the input for the third hidden layer and we will use this number of
    neurons in this layer. Finally, the output layer, which we will call `logits`,
    is the fully connected layer that we use as input, hidden layer 3\. The following
    screenshot shows the lines of code used for building the neural network:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建DNN时，我们使用`fully_connected`函数作为第一个隐藏层。这个隐藏层的输入是`x`，即来自占位符的数据。`n_hidden1`是我们在该隐藏层中拥有的神经元数量，你会记得它是350个神经元。现在，隐藏层1将作为隐藏层2的输入，`n_hidden2`是这一层的神经元数量。同样，隐藏层2将作为第三隐藏层的输入，我们将在这一层使用相应数量的神经元。最后，输出层，我们称之为`logits`，是我们用作输入的完全连接层，隐藏层3。以下截图显示了用于构建神经网络的代码行：
- en: '![](img/02a2f19c-c256-420c-9373-11069daa5387.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02a2f19c-c256-420c-9373-11069daa5387.png)'
- en: We enter the output as 10 because we have 10 categories in our classification
    problem and we know that in the output layer we don't use any activation function.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将输出设置为10，因为我们的分类问题有10个类别，并且我们知道在输出层中我们不使用任何激活函数。
- en: The loss function
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'For our `loss` function, we use the cross-entropy function. TensorFlow provides
    us with many such functions. For example, in this case, we are using the `sparse_softmax_cross_entropy
    _with_logits` function because here we got `logits` from the network. So, in this
    function, we pass the actual labels. These are the true labels, which are `logits`—the
    results or the output of our network. The following screenshot shows the lines
    of code used for showing the use of the `reduce_mean` function with this cross-entropy
    for getting the loss:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的`loss`函数，我们使用了交叉熵函数。TensorFlow提供了许多此类函数。例如，在这种情况下，我们使用的是`sparse_softmax_cross_entropy_with_logits`函数，因为这里我们得到了来自网络的`logits`。所以，在这个函数中，我们传入了实际标签。这些是真实标签，即`logits`——我们网络的结果或输出。以下截图显示了用于显示`reduce_mean`函数与交叉熵一起计算损失的代码行：
- en: '![](img/73884877-d7ad-4b6c-90d8-2b054d336ead.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73884877-d7ad-4b6c-90d8-2b054d336ead.png)'
- en: Now, using this cross-entropy, we can calculate the loss as the mean of the
    vector that we will get here. So this is the `loss` function and the mean of the
    cross-entropy.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用这个交叉熵，我们可以将损失计算为我们在这里得到的向量的均值。这就是`loss`函数和交叉熵的均值。
- en: Defining optimizer and training operations
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义优化器和训练操作
- en: The goal of the optimizer is to minimize the loss, and it does this by adjusting
    the different weights that we have in all the layers of our network. The optimizer
    used here is the gradient descent with a learning rate of `0.01`. The following
    screenshot shows the lines of code used for defining the optimizer and also shows
    the training operations.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器的目标是最小化损失，它通过调整我们网络中所有层的不同权重来实现这一点。这里使用的优化器是带有`0.01`学习率的梯度下降。以下截图展示了定义优化器的代码行，并显示了训练操作。
- en: '![](img/2861b9b5-3425-4cbc-917c-34c6bebfb8fc.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2861b9b5-3425-4cbc-917c-34c6bebfb8fc.png)'
- en: Each time we run the training operation `training_op`, the optimizer will change
    the values of these weights a little bit. In doing so, it minimizes the loss,
    and the predictions and the actual values are as close as possible.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 每次运行训练操作`training_op`时，优化器都会稍微改变这些权重的值。通过这样做，它最小化了损失，使得预测值和实际值尽可能接近。
- en: Training strategy and valuation of accuracy of the classification
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类的训练策略和准确性评估
- en: 'Here we set the training strategy. We will use 20 epochs with a batch size
    of 80\. In all of these cells, we have build the computational graph that will
    be used in this program. The following screenshot shows the lines of code used
    for showing the training strategy and the couple of nodes for evaluating the accuracy
    of the classification:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们设置训练策略。我们将使用20个周期，每个批次大小为80。在这些单元中，我们已经构建了将在该程序中使用的计算图。以下截图展示了用于显示训练策略以及用于评估分类准确度的几个节点的代码：
- en: '![](img/c059d22e-2a90-4e34-94ba-d7f806e01d5a.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c059d22e-2a90-4e34-94ba-d7f806e01d5a.png)'
- en: Running the computational graph
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行计算图
- en: 'For actually running the computational graph, first we will initialize all
    the variables in our program. The following screenshot shows the lines of code
    used for running the computational graph:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实际运行计算图，首先我们需要初始化程序中的所有变量。以下截图展示了用于运行计算图的代码行：
- en: '![](img/dfff9daa-7bfb-4207-bb88-7419a4c8654a.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfff9daa-7bfb-4207-bb88-7419a4c8654a.png)'
- en: In line 3, we initialize all the variables in our program. Now, here, we don't
    have any variables explicitly. However, the variables that are inside are fully
    connected. The `fully_connected` function is where we have all the hidden layers
    that contain the weights. These are the variables which is why we must initialize
    the variables with the `global_ variables_initializer` object and run this node.
    For each epoch, we run this loop 20 times. Now, for each iteration that we have
    in the number of examples over the batch size, which is 80, we get the values
    for the features and the targets. So this will be 80 data points for each iteration.
    Then, we run the training operation and will pass as `x`; we will pass the feature
    values and here we will pass the target values. Remember, `x` and `y` are our
    placeholders. Then, we evaluate the accuracy of the training and then evaluate
    the accuracy in the testing dataset, and we get the testing dataset. We get from
    `mnist.test.images`, and so these are now the features and `test.labels` are the
    targets. Then, we print the two accuracies after these two loops are completed.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3行中，我们初始化了程序中的所有变量。现在，在这里，我们并没有显式定义任何变量。然而，内部的变量是完全连接的。`fully_connected`函数是我们定义所有隐藏层并包含权重的地方。这些就是变量，因此我们必须使用`global_variables_initializer`对象来初始化变量并运行此节点。对于每个周期，我们运行此循环20次。现在，对于每个迭代，我们遍历批次大小为80的所有示例，获取特征值和目标值。这意味着每次迭代都会有80个数据点。然后，我们运行训练操作并传递特征值作为`x`，目标值则传递给`y`。记住，`x`和`y`是我们的占位符。接着，我们评估训练的准确度，并评估测试数据集的准确度，我们会从`mnist.test.images`获取测试数据集。这些现在是特征，`test.labels`是目标。最后，在这两个循环完成后，我们打印出两个准确度。
- en: 'We then produce some individual predictions for the first 15 images in the
    testing dataset. After running this, we get the first epoch, with a training accuracy
    of 86 percent and a testing accuracy of 88-89 percent. The following screenshot
    shows the results of training and the testing results for different epochs:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接着我们为测试数据集中的前15张图片生成一些单独的预测。运行后，我们得到第一个周期，训练准确率为86%，测试准确率为88-89%。以下截图展示了不同周期的训练和测试结果：
- en: '![](img/16acb53f-7d10-4c38-9d16-273f6bd7b386.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16acb53f-7d10-4c38-9d16-273f6bd7b386.png)'
- en: 'The programs takes a little bit of time to run, but after 20 epochs, the testing
    accuracy is almost 97 percent. The following screenshot shows the actual labels
    and the predicted labels. These are the predictions the network made:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 程序运行需要一些时间，但经过20个周期后，测试准确率几乎达到了97%。下面的截图展示了实际标签和预测标签。这些是网络做出的预测：
- en: '![](img/affdeb3b-3f28-4b4b-a801-43210ca51816.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/affdeb3b-3f28-4b4b-a801-43210ca51816.png)'
- en: So we have built our first DNN model and we were able to classify handwritten
    digits using this program with almost 97 percent accuracy.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们已经构建了第一个DNN模型，并且能够用这个程序以接近97%的准确率对手写数字进行分类。
- en: Regression with Deep Neural Networks (DNN)
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络（DNN）的回归
- en: 'For regression with DNNs, we first have to import the libraries we will use
    here. We will import TensorFlow, pandas, NumPy, and matplotlib with the lines
    of code shown in the following screenshot:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用DNN的回归，我们首先需要导入将要使用的库。我们将导入TensorFlow、pandas、NumPy和matplotlib，以下截图展示了相关的代码：
- en: '![](img/d2622fc4-ef72-4d49-ac2e-025f5c814142.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d2622fc4-ef72-4d49-ac2e-025f5c814142.png)'
- en: We will use the `fully_ connected` function from the `tensorflow.contrib.layers` model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自`tensorflow.contrib.layers`模型的`fully_connected`函数。
- en: Elements of the DNN model
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNN模型的元素
- en: 'Before running the model, we first have to determine the elements that we will
    use in building a multilayer perceptron model, shown as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行模型之前，我们首先需要确定在构建多层感知机模型时将使用的元素，如下所示：
- en: '**Architecture:** The model contains 23 elements in the input layer, hence
    we have 25 features in this dataset. We have only one element in the output layer
    and we will use three hidden layers, although we could use any number of hidden
    layers. We will use 256 neurons for the first layer, 128 for the second, and 64
    for the third one. These are the powers of two.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构：** 模型的输入层包含23个元素，因此这个数据集有25个特征。输出层只有一个元素，我们将使用三个隐藏层，尽管我们可以使用任意数量的隐藏层。我们将为第一个隐藏层使用256个神经元，第二个隐藏层使用128个神经元，第三个隐藏层使用64个神经元。这些数字是2的幂。'
- en: '**Activation function:** We will choose the ReLu activation function.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数：** 我们将选择ReLu激活函数。'
- en: '**Optimizing a****lgorithm**: The optimization algorithm used here is the Adam
    optimizer. The Adam optimizer is one of the most popular optimizers as it is the
    best option for a lot of problems.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化算法：** 这里使用的优化算法是Adam优化器。Adam优化器是最受欢迎的优化器之一，因为它是许多问题的最佳选择。'
- en: '**Loss function**: We will use the mean squared error because we are doing
    a regression problem here and this is one of the optimal choices for the `loss`
    function.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数：** 我们将使用均方误差，因为我们正在做回归问题，而这是`loss`函数的最佳选择之一。'
- en: '**Weights initialization strategy:** For this, we will use the Xavier initializer,
    which comes as the default that with the `fully_connected` function from TensorFlow.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重初始化策略：** 为此，我们将使用Xavier初始化器，这是TensorFlow中的`fully_connected`函数的默认初始化方式。'
- en: '**Regularization strategy**: We are not going to use any regularization strategy.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化策略：** 我们不会使用任何正则化策略。'
- en: '**Training strategy**: We are going to use 40 epochs. We will present the dataset
    40 times to the network and, in every iteration, we will use batches of 50 data
    points each time we run the training operation. So, we will use 50 elements of
    the dataset.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练策略：** 我们将使用40个epoch。我们将在每次训练操作时将数据集展示给网络40次，并且每次迭代时，我们将使用每次50个数据点的批量。所以，我们将使用50个数据集元素。'
- en: Building the DNN
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建DNN
- en: First, we import the dataset that we will use. The reason behind using this
    dataset is that, it is easily available. The following are the steps involved
    in building a DNN model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入将要使用的数据集。选择这个数据集的原因是它容易获得。以下是构建DNN模型所涉及的步骤。
- en: Reading the data
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取数据
- en: 'We are going to read data in the cell and filter it to our preference. The
    following screenshot shows the lines of code used to read the data:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在该单元格中读取数据，并根据我们的需求进行过滤。下图展示了用于读取数据的代码行：
- en: '![](img/60d20624-6098-4ad9-8d91-a1b138c925b6.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60d20624-6098-4ad9-8d91-a1b138c925b6.png)'
- en: Objects for modeling
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模对象
- en: 'After importing the datasets, we prepare the objects for modeling. So we have
    training and testing here for `x` and for `y`. The following screenshot shows
    the lines of code used to prepare the objects for modelling:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 导入数据集后，我们为建模准备对象。因此，我们在这里为`x`和`y`分别准备了训练和测试数据。下图展示了用于准备建模对象的代码行：
- en: '![](img/e9278a94-5327-4ee6-bd04-3a925170420e.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9278a94-5327-4ee6-bd04-3a925170420e.png)'
- en: Training strategy
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练策略
- en: 'This is the training strategy with 40 epochs and a batch size of 50\. This
    is created with the following lines of code:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是带有40个epoch和50批次大小的训练策略。通过以下代码行创建：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Input pipeline for the DNN
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNN的输入管道
- en: 'Since this is an external dataset, we have to use a data input pipeline, and
    TensorFlow provides different tools for getting data inside the deep learning
    model. Here, we create a dataset object and an iterator object with the lines
    of code shown in the following screenshot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个外部数据集，我们必须使用数据输入管道，而TensorFlow提供了多种工具来将数据导入到深度学习模型中。在这里，我们通过以下代码行创建数据集对象和迭代器对象：
- en: '![](img/edc095ad-d63a-4b69-87c3-3d262b2e7345.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/edc095ad-d63a-4b69-87c3-3d262b2e7345.png)'
- en: First, we produce the dataset object. Then, we pass the whole training dataset
    to some placeholders that we will use. Then, we shuffle the data and divide or
    partition the training dataset into batches of 50\. Hence, the dataset object
    is prepared, containing all of the training samples partitioned into batches of
    size 50\. Next, we make an iterator object. Then, with the `get_next` method,
    we create a node called `next_element`, which provides the batches of 50 from
    the training examples.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们生成数据集对象。然后，我们将整个训练数据集传递给一些我们将要使用的占位符。接下来，我们打乱数据，并将训练数据集划分成50个一批的批次。因此，数据集对象已准备好，包含所有的训练样本，并分割成大小为50的批次。接着，我们创建一个迭代器对象。然后，通过`get_next`方法，我们创建一个名为`next_element`的节点，提供来自训练示例的50个批次。
- en: Defining the architecture
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义架构
- en: 'We use three hidden layers with 256 neurons for the first layer, 128 for the
    second, and 64 for the third one. The following code snippet shows the architecture
    for this procedure:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用三个隐藏层，第一个层有256个神经元，第二层有128个，第三层有64个。以下代码片段展示了这一过程的架构：
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Placeholders for input values and labels
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入值和标签的占位符
- en: 'The values of different layers are the objects, also called the placeholders,
    for inputs and labels. These placeholders are used for feeding the data into the
    network. The following lines of code shows the placeholders for inputs and labels:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 不同层的值是对象，也叫做占位符，用于输入和标签。这些占位符用于将数据传递到网络中。以下代码行展示了用于输入和标签的占位符：
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Building the DNN
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建DNN
- en: 'For building the following example, we first have to define the `DNN` function.
    This function will take `X_values` and output the predictions. For the first hidden
    layer, we use a `fully_ connected` function. The input for this hidden layer will
    be `X`, which is the data that comes from the placeholder, and `n_hidden1` is
    the number of neurons that we have in this hidden layer. Remember we have 350
    neurons in the first hidden layer. Now, the first hidden layer becomes the input
    for the second hidden layer, and `n_hidden2` is the number of neurons that we
    use in this second hidden layer. Likewise, this second hidden layer becomes the
    input for the third hidden layer and we use this number of neurons in this layer.
    Finally, we have the output layer, let''s call it `y_pred`, and this is a fully
    connected layer, with the third hidden layer as input. This is one output and
    this layer has no activation function. The following screenshot shows the lines
    of code used for building the neural network:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建以下示例，我们首先必须定义`DNN`函数。该函数将接收`X_values`并输出预测结果。对于第一个隐藏层，我们使用`fully_connected`函数。该隐藏层的输入是`X`，即来自占位符的数据，`n_hidden1`是该隐藏层中的神经元数量。记住，我们在第一个隐藏层中有350个神经元。现在，第一个隐藏层成为第二个隐藏层的输入，`n_hidden2`是我们在第二个隐藏层中使用的神经元数量。同样，第二个隐藏层成为第三个隐藏层的输入，我们在该层中使用的神经元数量也是如此。最后，我们有输出层，我们将其称为`y_pred`，它是一个全连接层，第三个隐藏层作为输入。这个输出层没有激活函数。以下截图展示了构建神经网络的代码行：
- en: '![](img/e509c688-e82a-44e0-a99b-1abce7126c60.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e509c688-e82a-44e0-a99b-1abce7126c60.png)'
- en: The loss function
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'We will use the `mean_squared _error` function—TensorFlow provides us with
    many such functions. We pass the observed values and the predicted values and
    this function calculates the mean squared error. The following screenshot shows
    the lines of code used for showing the `mean_squared _error` function:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`mean_squared_error`函数—TensorFlow为我们提供了许多类似的函数。我们传递观测值和预测值，该函数计算均方误差。以下截图展示了用于显示`mean_squared_error`函数的代码行：
- en: '![](img/cedfa97e-88ee-4564-9297-6299ce5e7652.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cedfa97e-88ee-4564-9297-6299ce5e7652.png)'
- en: Defining optimizer and training operations
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义优化器和训练操作
- en: The goal of the optimizer is to minimize the loss and it does this by adjusting
    the different weights that we have in all of the layers of our network. The optimizer
    used here is the Adam optimizer with a learning rate of 0.001.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器的目标是最小化损失，它通过调整我们网络中所有层的不同权重来实现这一目标。这里使用的优化器是Adam优化器，学习率为0.001。
- en: 'The following screenshot shows the lines of code used for defining the optimizer
    and also shows the training operations:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了用于定义优化器的代码行，同时也显示了训练操作：
- en: '![](img/d34af37c-db85-4d8d-82ec-1274c6805ea2.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d34af37c-db85-4d8d-82ec-1274c6805ea2.png)'
- en: 'The following screenshot shows some of the NumPy arrays that we created and
    will use for evaluation purposes:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了我们创建的一些NumPy数组，这些数组将用于评估目的：
- en: '![](img/88fcf3a8-6cf2-4475-b25e-d7ee0db78a4b.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88fcf3a8-6cf2-4475-b25e-d7ee0db78a4b.png)'
- en: Running the computational graph
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行计算图
- en: 'For actually running the computational graph, first we will initialize all
    of the variables in our program. The following screenshot shows the lines of code
    used for running the computational graph:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际运行计算图时，首先我们需要初始化程序中的所有变量。以下截图展示了用于运行计算图的代码行：
- en: '![](img/ea17818a-74ea-4811-ad41-4040102b4673.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea17818a-74ea-4811-ad41-4040102b4673.png)'
- en: The variables are the weights that are implicit in the `fully_connected` function.
    Then, for every epoch, we initialize the iterator object and pass the training
    dataset. Here, we have `batch_data`, we run this `next_ element` node, and we
    get batches of 50\. We can get the feature values and the labels, we can get the
    labels, and then we can run the training operation. When the object runs out of
    data, we get an error. In this case, when we get one of these errors, it means
    that we have used all of the training datasets. We then break from this `while`
    loop and proceed to the next epoch. Later, we produce some individual predictions
    so you can take a look at concrete predictions that this neural network makes.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变量是 `fully_connected` 函数中隐式的权重。然后，对于每一个周期，我们初始化迭代器对象并传入训练数据集。在这里，我们有 `batch_data`，运行这个
    `next_ element` 节点后，我们获得50个数据的批次。我们可以获取特征值和标签，获取标签后可以进行训练操作。当对象没有数据时，我们会遇到错误。此时，若遇到这种错误，意味着我们已经使用完了所有训练数据集。然后，我们跳出这个
    `while` 循环，进入下一个周期。稍后，我们会产生一些单独的预测结果，您可以查看该神经网络做出的具体预测。
- en: 'The following screenshot shows the behavior of the training and the testing
    MSE of all 40 epochs as we present the data to this network:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了在我们将数据输入到这个网络时，训练和测试的 MSE（均方误差）在40个周期中的变化情况：
- en: '![](img/e9121538-21c3-43f6-8fe0-59ecaa8da16e.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9121538-21c3-43f6-8fe0-59ecaa8da16e.png)'
- en: In the last tested MSE (epoch 40) we get the final value of the training and
    the testing MSE.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一次测试的 MSE（第40个周期）中，我们得到了训练和测试 MSE 的最终值。
- en: 'We get the actual predictions from the network and the values are relatively
    close. Here, we can see the predicted prices. For cheap diamonds, the network
    produced values that are relatively close. For very expensive diamonds, the network
    produced high values. Also, the predicted values are pretty close to the observed
    values. The following screenshot shows the actual and the predicted values that
    we got from the network:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从网络中获得了实际的预测结果，这些值相对较为接近。这里，我们可以看到预测的价格。对于便宜的钻石，网络生成的值相对较为接近。而对于非常昂贵的钻石，网络生成的值较高。此外，预测值与观察值也非常接近。以下截图展示了我们从网络中得到的实际值和预测值：
- en: '![](img/c37daf44-699d-4ec7-a142-2c8cfb057b90.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c37daf44-699d-4ec7-a142-2c8cfb057b90.png)'
- en: 'The following screenshot shows the graph of the training MSE with the testing
    MSE and the lines of code used to produce it:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了训练 MSE 与测试 MSE 的图表，以及生成该图表的代码行：
- en: '![](img/d41fa0ad-74c9-4232-b9e7-e8788788c194.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d41fa0ad-74c9-4232-b9e7-e8788788c194.png)'
- en: Classification with DNNs
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DNN 进行分类
- en: For understanding classification with DNNs, we first have to understand the
    concept of exponential linear unit function and the elements of the model.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解 DNN 的分类，我们首先需要理解指数线性单元函数的概念以及模型的各个元素。
- en: Exponential linear unit activation function
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指数线性单元激活函数
- en: 'The **Exponential Linear Unit** (**ELU**) function is a relatively recent modification
    to the ReLU function. It looks very similar to the ReLU function, but it has very
    different mathematical properties. The following screenshot shows the ELU function:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**指数线性单元**（**ELU**）函数是对 ReLU 函数的一个相对较新的修改。它与 ReLU 函数非常相似，但在数学性质上却有很大的不同。以下截图展示了
    ELU 函数：'
- en: '![](img/1e4604f6-28a4-4da0-9faa-de706a4da045.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1e4604f6-28a4-4da0-9faa-de706a4da045.png)'
- en: The preceding screenshot shows that, at `0`, we don't have a corner. In the
    case of the ReLU function, we have a corner. In this function, instead of a single
    value going to `0`, we have the ELU function slowly going to the negative alpha
    parameter.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的截图显示，在`0`时，我们没有出现拐角。对于 ReLU 函数，我们有一个拐角。在这个函数中，取值并不会直接趋向`0`，而是 ELU 函数会慢慢逼近负的
    alpha 参数。
- en: Classification with DNNs
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DNN 进行分类
- en: 'For classification with DNNs, we first have to import the libraries that we
    will use. Use the lines of code in the following screenshot to import the `tensorflow`,
    `pandas`, `numpy`, and `matplotlib` libraries:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 DNN（深度神经网络）的分类，我们首先需要导入将要使用的库。请使用以下截图中的代码行导入 `tensorflow`、`pandas`、`numpy`
    和 `matplotlib` 库：
- en: '![](img/e70a38d6-3326-48a1-a4e2-2a8156ff1188.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e70a38d6-3326-48a1-a4e2-2a8156ff1188.png)'
- en: We will also import the `train_test_split` function from `sklearn.model_selection`,
    `RobustScaler` from `sklearn.preprocessiong` , and `precision_score`, `recall_score`,
    and `accuracy_score` from `sklearn.metrics`. We also import the `fully_connected`
    function from `tensorflow.contrib.layers` to build the layers of our network.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将从 `sklearn.model_selection` 导入 `train_test_split` 函数，从 `sklearn.preprocessing`
    导入 `RobustScaler`，并从 `sklearn.metrics` 导入 `precision_score`、`recall_score` 和 `accuracy_score`。我们还将从
    `tensorflow.contrib.layers` 导入 `fully_connected` 函数，用于构建网络的各层。
- en: Elements of the DNN model
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNN 模型的元素
- en: Before running the model, we first have to determine the elements that we will
    use in
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行模型之前，我们首先必须确定我们将使用的元素
- en: 'building a multilayer perceptron model, shown as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个多层感知器模型，如下所示：
- en: '**Architecture**: The model contains 25 elements in the input layer because
    we have'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构**：该模型在输入层包含 25 个元素，因为我们有'
- en: 25 features in the dataset. We have two elements in the output layer and we
    will
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集中的 25 个特征。输出层中有两个元素，我们将
- en: also use three hidden layers, although we could use any number of hidden
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 还将使用三个隐藏层，尽管我们可以使用任意数量的隐藏层
- en: layers. We will use the same number of neurons in each layer, 200\. Here we
    use
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 层。我们将在每层使用相同数量的神经元，200。这里我们使用
- en: the powers of 2, which is an arbitrary choice.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2 的幂，这是一个任意选择。
- en: '**Activation function**: We will choose the ELU activation function, which
    was'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**：我们将选择 ELU 激活函数，已被'
- en: explained in the preceding chapter.
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前一章所述。
- en: '**Optimizing a****lgorithm**: The optimization algorithm used here is the Adam'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化算法**：这里使用的优化算法是 Adam'
- en: optimizer with a learning rate of 0.001.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习率为 0.001 的优化器。
- en: '**Loss function**: For the `loss` function, we will use the cross-entropy function.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数**：对于 `loss` 函数，我们将使用交叉熵函数。'
- en: '**Weights initialization strategy**: For this, we will use the Xavier initializer,
    a'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重初始化策略**：为此，我们将使用 Xavier 初始化器，'
- en: method that comes as default with the `fully_connected` function from
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方法，这是 `fully_connected` 函数的默认方法
- en: TensorFlow.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TensorFlow。
- en: '**Regularization strategy**: We are not going to use any regularization strategy.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化策略**：我们不打算使用任何正则化策略。'
- en: '**Training strategy**: We are going to use 40 epochs. So, we will present the
    dataset'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练策略**：我们将使用 40 个 epoch。因此，我们将呈现数据集'
- en: 40 times to the network, and in every iteration, we will use a batch size of
    100.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 40 次迭代，每次迭代我们将使用 100 的批量大小。
- en: Building the DNN
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 DNN
- en: Now, we import the dataset that we will use. The reason behind using this dataset
    is that it
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们导入我们将使用的数据集。使用这个数据集的原因是它
- en: is easily available. The following are the steps involved in building a DNN
    model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是轻松获取的。构建 DNN 模型的步骤如下。
- en: Reading the data
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取数据
- en: We are going to read the data in the cell. The following screenshot shows the
    lines of code used
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在该单元中读取数据。以下截图显示了用于读取数据的代码行
- en: 'to read the data:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 用于读取数据：
- en: '![](img/dfee505c-4e6a-46e0-aafc-a1a73825fa69.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfee505c-4e6a-46e0-aafc-a1a73825fa69.png)'
- en: Producing the objects for modeling
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成建模对象
- en: Now, we produce the objects used for modeling. We are going to use 10 percent
    for testing
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们生成用于建模的对象。我们将使用 10% 用于测试
- en: and 90 percent for training. The following screenshot shows the lines of code
    used for
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 90% 用于训练。以下截图显示了用于
- en: 'producing the objects for modeling:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 生成建模对象：
- en: '![](img/57d58fe3-2781-4cb0-b875-cbf1ef50ebcb.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/57d58fe3-2781-4cb0-b875-cbf1ef50ebcb.png)'
- en: Training strategy
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练策略
- en: This is the training strategy that we previously mentioned, 40 epochs and a
    batch size of
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们之前提到的训练策略，40 个 epoch 和批量大小为
- en: '100\. The following code block shows the parameters we set in this strategy:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 100。以下代码块显示了我们在此策略中设置的参数：
- en: '[PRE7]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Input pipeline for DNN
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNN 的输入流水线
- en: Now, we perform the same thing that we did with the regression example. We create
    a
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们执行与回归示例相同的操作。我们创建一个
- en: '`dataset` object and an iterator object. In the end, we have `next_element`.
    This will be a node in our computational graph that will give us 100 data points
    each time. Hence, we get'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`dataset` 对象和一个迭代器对象。最后，我们有 `next_element`。这将是我们计算图中的一个节点，每次给我们 100 个数据点。因此，我们得到'
- en: the batches. The following screenshot shows the lines of code used for producing
    an input
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 批次。以下截图显示了用于生成输入的代码行
- en: 'pipeline for the DNN:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: DNN 的流水线：
- en: '![](img/b2c44778-8a38-4d4a-ae49-bac788b96ee4.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2c44778-8a38-4d4a-ae49-bac788b96ee4.png)'
- en: Defining the architecture
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义架构
- en: We will use three hidden layers and 200 neurons for all three. The following
    code snippet
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用三个隐藏层，每个层有200个神经元。以下代码片段
- en: 'shows the architecture we will use in this example:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 显示了我们将在这个示例中使用的架构：
- en: '[PRE8]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Placeholders for inputs and labels
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入和标签的占位符
- en: The values of different layers are the objects, also called the placeholders,
    for inputs and
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 不同层的值是对象，也称为占位符，用于输入和
- en: labels. These placeholders are used for feeding the data into the network. The
    following
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 标签。这些占位符用于将数据输入网络。以下
- en: 'lines of code are used for showing placeholders for inputs and labels:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码行用于展示输入和标签的占位符：
- en: '[PRE9]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Building the neural network
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建神经网络
- en: For building deep neural networks, we will use the `DNN` function. We have three
    layers and we will use the ELU function as the activation function. You can get
    this function from
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 对于构建深度神经网络，我们将使用`DNN`函数。我们有三层，并将使用ELU函数作为激活函数。你可以从
- en: TensorFlow, `tf.nn.elu`, from which you can get a lot of functions that will
    help you build your deep learning models. The following screenshot shows the lines
    of code used for
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow，`tf.nn.elu`，它提供了许多可以帮助你构建深度学习模型的功能。以下截图显示了用于
- en: 'producing this function and for getting the output in the form of `logits`:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 生成此函数并以`logits`形式获取输出：
- en: '![](img/e5e00979-4492-4f24-9bbd-666d6c417cc6.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5e00979-4492-4f24-9bbd-666d6c417cc6.png)'
- en: The final layer is called the `logits` layer. We won't be using any activation
    function in
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层叫做`logits`层。我们不会在此层使用任何激活函数
- en: this layer.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层。
- en: The loss function
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'For the `loss` function, again, we are going to get `logits` from the DNN and
    then pass this `logits` to the `softmax_cross_entropy_with_logits`function from
    TensorFlow. We pass the true labels and `logits`, and then we can get the loss
    by using the `reduce_mean` function with `cross_entropy`. The following screenshot
    shows the lines of code used for showing the use of the `reduce_mean`function
    with `cross_entropy` for getting the loss:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`loss`函数，我们再次将从DNN中获取`logits`，然后将此`logits`传递给TensorFlow中的`softmax_cross_entropy_with_logits`函数。我们传入真实标签和`logits`，然后可以使用`reduce_mean`函数和`cross_entropy`来计算损失。以下截图显示了用于展示如何使用`reduce_mean`函数和`cross_entropy`来计算损失的代码行：
- en: '![](img/2c3d959e-568f-4e50-bf45-e28bcaf89b8c.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c3d959e-568f-4e50-bf45-e28bcaf89b8c.png)'
- en: Evaluation nodes
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估节点
- en: 'Now, for evaluation, we will calculate the probabilities of default and non-default
    variables; you can get the probabilities by applying a `softmax` function to `logits`.
    The following screenshot shows the `softmax` function:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，进行评估时，我们将计算违约和非违约变量的概率；你可以通过对`logits`应用`softmax`函数来获得概率。以下截图显示了`softmax`函数：
- en: '![](img/c08b33df-ddba-4203-8a20-c7c8d56f40cc.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c08b33df-ddba-4203-8a20-c7c8d56f40cc.png)'
- en: The `softmax` function is used for providing the probabilities for the different
    categories.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`softmax`函数用于为不同类别提供概率。'
- en: Optimizer and the training operation
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化器和训练操作
- en: The goal of the optimizer is to minimize loss, and it does this by adjusting
    the different weights that we have in all of the layers of our network.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器的目标是最小化损失，通过调整我们网络中各层的不同权重来实现。
- en: 'The following screenshot shows the lines of code used for defining the optimizer
    and shows the training operations:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了用于定义优化器并展示训练操作的代码行：
- en: '![](img/31bcbcf6-c2aa-4cd5-a759-3cf5b0a68f49.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/31bcbcf6-c2aa-4cd5-a759-3cf5b0a68f49.png)'
- en: In this case, the optimizer is, again, the Adam optimizer with a learning rate
    of `0.001`. The training operation is the operation in which the optimizer minimizes
    the loss.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，优化器再次使用Adam优化器，学习率为`0.001`。训练操作是优化器最小化损失的操作。
- en: Run the computational graph
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行计算图
- en: To actually run the computational graph, first we initialize all of the variables
    in our
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 要实际运行计算图，首先我们需要初始化所有变量
- en: program. The variables are the weights that are implicit in the `fully_connected`
    function. We run four epochs and for each epoch, we initialize our iterator object.
    We pass training `x` and training `y`, and then we run this loop. This loop will
    run as long as we have data in `next_elementelement`. So, we get the next 100
    elements and then, in the next iteration, the next 100 elements, and so on. In
    every iteration, we run the training operation. Now, what this training operation
    does is ask the optimizer to adjust the parameters and the weights, a little bit
    in order to make better predictions.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 程序。变量是`fully_connected`函数中隐含的权重。我们运行四个周期，在每个周期中初始化我们的迭代器对象。我们传入训练数据`x`和`y`，然后运行这个循环。只要`next_elementelement`中有数据，我们就会运行这个循环。因此，我们获取接下来的100个元素，然后在下一次迭代中获取下一个100个元素，以此类推。在每次迭代中，我们都会运行训练操作。训练操作的作用是要求优化器调整参数和权重，以便做出更好的预测。
- en: 'The following screenshot shows the lines of code used for running the computational
    graph:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了用于运行计算图的代码行：
- en: '![](img/3d0faf9a-3035-4af5-9cd1-a133bc2502cc.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d0faf9a-3035-4af5-9cd1-a133bc2502cc.png)'
- en: In the end, we can get the probabilities and we can use these for evaluation
    purposes.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们可以获得概率值，并可用于评估目的。
- en: Evaluating the model with a set threshold
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用设定阈值评估模型
- en: The `probabilities` object is produced to actually evaluate the model performance
    with
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`probabilities`对象实际上是用来评估模型性能的。'
- en: different classification thresholds. The classification threshold can be modified
    for a binary
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的分类阈值。分类阈值可以根据需要修改，用于二分类问题。
- en: classification problem and can be used for calculating the recall score, the
    precision, and the
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 分类问题，并可用于计算召回率、精确度和
- en: 'accuracy. On using a classification threshold of `0.16`, these are the metrics
    that we get in the testing dataset:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 精度。使用分类阈值`0.16`时，在测试数据集中得到的指标如下：
- en: '![](img/f23f3ebc-019f-4be1-a50d-dd44eaec977b.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f23f3ebc-019f-4be1-a50d-dd44eaec977b.png)'
- en: On calculating, we get a recall score of `82.53` percent, precision of `34.02`
    percent, and an accuracy of `60.7` percent.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 通过计算，我们得到了`82.53`的召回率，`34.02`的精确度，以及`60.7`的准确率。
- en: Summary
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to make predictions using TensorFlow. We studied
    the MNIST dataset and classification of models using this dataset. We came across
    the elements of DNN models and the process of building the DNN. Later, we progressed
    to study regression and classification with DNNs. We classified handwritten digits
    and learned more about building models in TensorFlow. This brings us to the end
    of this book! We learned how to use ensemble algorithms to produce accurate predictions.
    We applied various techniques to combine and build better models. We learned how
    to perform cross-validation efficiently. We also implemented various techniques
    to solve current issues in the domain of predictive analysis. And, the best part,
    we used the DNN models we built to solve classification and regression problems.
    This book has helped us implement various machine learning techniques to build
    advanced predictive models and apply them in the real world.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用TensorFlow进行预测。我们研究了MNIST数据集，并使用该数据集进行了模型分类。我们了解了DNN模型的各个元素及其构建过程。之后，我们深入研究了使用DNN进行回归和分类。我们对手写数字进行了分类，并学习了如何在TensorFlow中构建模型。这标志着本书的结束！我们学会了如何使用集成算法生成准确的预测。我们应用了多种技术来组合并构建更好的模型。我们还学习了如何高效地进行交叉验证。我们实施了多种技术来解决预测分析领域当前的问题。最重要的是，我们使用自己构建的DNN模型来解决分类和回归问题。本书帮助我们实现了多种机器学习技术，构建了先进的预测模型，并将其应用于现实世界。
