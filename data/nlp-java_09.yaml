- en: Chapter 6. String Comparison and Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章. 字符串比较与聚类
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下几种方案：
- en: Distance and proximity – simple edit distance
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 距离和接近度 – 简单编辑距离
- en: Weighted edit distance
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加权编辑距离
- en: The Jaccard distance
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaccard 距离
- en: The Tf-Idf distance
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tf-Idf 距离
- en: Using edit distance and language models for spelling correction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用编辑距离和语言模型进行拼写纠正
- en: The case restoring corrector
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大小写恢复修正器
- en: Automatic phrase completion
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动短语完成
- en: Single-link and complete-link clustering using edit distance
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用编辑距离的单链和完全链接聚类
- en: Latent Dirichlet allocation (LDA) for multitopic clustering
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配（LDA）用于多主题聚类
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: This chapter starts off by comparing strings using standard language neutral
    techniques. Then, we will use these techniques to build some commonly used applications.
    We will also look at clustering techniques based on distances between strings.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章从使用标准的语言中立技术来比较字符串开始。然后，我们将使用这些技术构建一些常用的应用程序。我们还将探讨基于字符串之间距离的聚类技术。
- en: For a string, we use the canonical definition that a string is a sequence of
    characters. So, clearly, these techniques apply to words, phrases, sentences,
    paragraphs, and so on, all of which you have learnt to extract in the previous
    chapters.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于字符串，我们使用标准定义，即字符串是字符的序列。所以，显然，这些技术适用于单词、短语、句子、段落等，你在前几章中已经学会了如何提取这些内容。
- en: Distance and proximity – simple edit distance
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 距离和接近度 – 简单编辑距离
- en: String comparison refers to techniques used to measure the similarity between
    two strings. We will use distance and proximity to specify how similar any two
    strings are. The more similar any two strings are, the lesser the distance between
    them, so the distance from a string to itself is 0\. An inverse measure is proximity,
    which means that the more similar any two strings are, the greater their proximity.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串比较是指用于衡量两个字符串相似度的技术。我们将使用距离和接近度来指定任意两个字符串的相似性。两个字符串的相似性越高，它们之间的距离就越小，因此，一个字符串与自身的距离为0。相反的度量是接近度，意味着两个字符串越相似，它们的接近度就越大。
- en: We will take a look at simple edit distance first. Simple edit distance measures
    distance in terms of how many edits are required to convert one string to the
    other. A common distance measure proposed by Levenshtien in 1965 allows deletion,
    insertion, and substitution as basic operations. Adding in transposition is called
    Damerau-Levenshtien distance. For example, the distance between `foo` and `boo`
    is 1, as we're looking at a substitution of `f` with `b`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先看看简单编辑距离。简单编辑距离通过衡量将一个字符串转换为另一个字符串所需的编辑次数来计算距离。Levenshtein在1965年提出的一种常见距离度量允许删除、插入和替换作为基本操作。加入字符交换后就称为Damerau-Levenshtein距离。例如，`foo`和`boo`之间的距离为1，因为我们是在将`f`替换为`b`。
- en: Note
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more on distance metrics, refer to the Wikipedia article on distance at
    [http://en.wikipedia.org/wiki/Distance](http://en.wikipedia.org/wiki/Distance).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有关距离度量的更多信息，请参考维基百科上的[距离](http://en.wikipedia.org/wiki/Distance)文章。
- en: 'Let''s look at some more examples of editable operations:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些可编辑操作的更多示例：
- en: '**Deletion**: `Bart` and `Bar`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**删除**：`Bart`和`Bar`'
- en: '**Insertion**: `Bar` and `Bart`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**插入**：`Bar`和`Bart`'
- en: '**Substitution**: `Bar` and `Car`'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**替换**：`Bar`和`Car`'
- en: '**Transposition**: `Bart` and `Brat`'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字符交换**：`Bart`和`Brat`'
- en: How to do it...
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'Now, we will run a simple example on edit distance:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将运行一个关于编辑距离的简单示例：
- en: 'Run the `SimpleEditDistance` class using the command line or your IDE:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用命令行或你的IDE运行`SimpleEditDistance`类：
- en: '[PRE0]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the command prompt, you will be prompted for two strings:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令提示符下，系统将提示你输入两个字符串：
- en: '[PRE1]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You will see the distance between the two strings with transposition allowed
    and with transposition not allowed.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将看到允许字符交换和不允许字符交换情况下两个字符串之间的距离。
- en: Play with some more examples to get a sense of how it works—try them first by
    hand and then verify that you got the optimal case.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多做一些示例来感受它是如何工作的——先手动尝试，然后验证你是否得到了最优解。
- en: How it works...
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'This is a very simple piece of code, and all it does is create two instances
    of the `EditDistance` class: one that allows transpositions, and the other that
    does not allow transpositions:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一段非常简单的代码，所做的只是创建两个`EditDistance`类的实例：一个允许字符交换，另一个不允许字符交换：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The remaining code will set up an I/O routing, apply the edit distances, and
    print them out:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的代码将设置输入/输出路由，应用编辑距离并输出结果：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If we wanted the proximity instead of distance, we would just use the `proximity`
    method instead of the `distance` method.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要的是接近度而不是距离，我们只需使用`proximity`方法，而不是`distance`方法。
- en: In simple `EditDistance`, all the editable operations have a fixed cost of 1.0,
    that is, each editable operation (deletion, substitution, insertion, and, if allowed,
    transposition) is counted with a cost of 1.0 each. So, in the example where we
    find the distance between `ab` and `ba`, there is one deletion and one insertion,
    both of which have a cost of 1.0\. Therefore, this makes the distance between
    `ab` and `ba` 2.0 if transposition is not allowed and 1.0 if it is. Note that
    typically, there will be more than one way to edit one string into the other.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单的`EditDistance`中，所有可编辑的操作都有固定的成本1.0，也就是说，每个可编辑的操作（删除、替换、插入，以及如果允许的话，交换）都被计为成本1.0。因此，在我们计算`ab`和`ba`之间的距离时，有一个删除操作和一个插入操作，两个操作的成本都是1.0。因此，如果不允许交换，`ab`和`ba`之间的距离为2.0；如果允许交换，则为1.0。请注意，通常将一个字符串编辑成另一个字符串的方法不止一种。
- en: Note
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'While `EditDistance` is quite simple to use, it is not simple to implement.
    This is what the Javadoc has to say about this class:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`EditDistance`使用起来非常简单，但实现起来却并不容易。关于这个类，Javadoc是这么说的：
- en: '*Implementation note: This class implements* *edit distance using dynamic programming
    in time O(n * m) where n and m are the length of the sequences being compared.
    Using a sliding window of three lattice slices rather than allocating the entire
    lattice at once, the space required is that for three arrays of integers as long
    as the shorter of the two character sequences being compared.*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*实现说明：该类使用动态规划实现编辑距离，时间复杂度为O(n * m)，其中n和m是正在比较的两个序列的长度。通过使用三个格子片段的滑动窗口，而不是一次性分配整个格子所需的空间，仅为三个整数数组的空间，长度为两个字符序列中较短的那个。*'
- en: In the following sections, we will see how to assign varying costs to each edit
    operation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将看到如何为每种编辑操作分配不同的成本。
- en: See also
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: Refer to the LingPipe Javadoc on `EditDistance` at [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/EditDistance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/EditDistance.html)
    for more details
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多详情请参阅LingPipe Javadoc中的`EditDistance`：[http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/EditDistance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/EditDistance.html)
- en: For more details on distance, refer to the Javadoc at [http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Distance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Distance.html)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多关于距离的详情，请参阅Javadoc：[http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Distance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Distance.html)
- en: For more details on proximity, refer to the Javadoc at [http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Proximity.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Proximity.html)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多关于接近度的详情，请参阅Javadoc：[http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Proximity.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Proximity.html)
- en: Weighted edit distance
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加权编辑距离
- en: Weighted edit distance is essentially a simple edit distance, except that the
    edits allow different costs to be associated with each kind of edit operation.
    The edit operations we identified in the previous recipe are substitution, insertion,
    deletion, and transposition. Additionally, there can be a cost associated with
    the exact matches to increase the weight for matching – this might be used when
    edits are required, such as a string-variation generator. Edit weights are generally
    scaled as log probabilities so that you can assign likelihood to an edit operation.
    The larger the weight, the more likely that edit operation is. As probabilities
    are between 0 and 1, log probabilities, or weights, will be between negative infinity
    and zero. For more on this refer to the Javadoc on the `WeightedEditDistance`
    class at [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 加权编辑距离本质上是一个简单的编辑距离，只不过编辑操作允许为每种操作分配不同的成本。我们在前面的示例中识别出的编辑操作包括替换、插入、删除和交换。此外，还可以为完全匹配分配成本，以提高匹配的权重——当需要进行编辑时，这可能会用于字符串变异生成器。编辑权重通常以对数概率的形式进行缩放，这样你就可以为编辑操作分配可能性。权重越大，表示该编辑操作越有可能发生。由于概率值介于0和1之间，因此对数概率或权重将在负无穷大到零之间。更多内容请参阅`WeightedEditDistance`类的Javadoc：[http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html)
- en: On the log scale, weighted edit distance can be generalized to produce exactly
    the same results as simple edit distance did in the previous recipe by setting
    the match weight to 0 and substituting, deleting, and inserting weights to -1
    and transposition weights to either -1 or negative infinity, if we want to turn
    transposition off.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数尺度上，加权编辑距离可以通过将匹配权重设置为0，将替换、删除和插入的权重设置为-1，且将置换权重设置为-1或负无穷（如果我们想关闭置换操作），以此方式将简单编辑距离的结果与前一个示例中的结果完全一样。
- en: We will look at weighted edit distance for spell checking and Chinese word segmentation
    in other recipes.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在其他示例中查看加权编辑距离在拼写检查和中文分词中的应用。
- en: In this section, we will use the `FixedWeightEditDistance` instance and create
    the `CustomWeightEditDistance` class that extends the `WeightedEditDistance` abstract
    class. The `FixedWeightEditDistance` class is initialized with weights for each
    edit operation. The `CustomWeightEditDistance` class extends `WeightedEditDistance`
    and has rules for each edit operation weights. The weight for deleting alphanumeric
    characters is -1, and for all other characters, that is, punctuation and spaces,
    it is is 0\. We will set insertion weights to be the same as deletion weights.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用`FixedWeightEditDistance`实例，并创建扩展了`WeightedEditDistance`抽象类的`CustomWeightEditDistance`类。`FixedWeightEditDistance`类通过为每个编辑操作初始化权重来创建。`CustomWeightEditDistance`类扩展了`WeightedEditDistance`，并为每个编辑操作的权重制定了规则。删除字母数字字符的权重是-1，对于所有其他字符，即标点符号和空格，则为0。我们将插入权重设置为与删除权重相同。
- en: How to do it...
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s expand on our previous example and look at a version that runs the simple
    edit distance as well as our weighted edit distance:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在前面的例子基础上扩展，并看一个同时运行简单编辑距离和加权编辑距离的版本：
- en: 'In your IDE run the `SimpleWeightedEditDistance class,` or in the command line,
    type:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的IDE中运行`SimpleWeightedEditDistance`类，或者在命令行中输入：
- en: '[PRE4]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the command line, you will be prompted for two strings: enter the examples
    shown here or choose your own:![How to do it...](img/4672OS_06_01.jpg)'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令行中，你将被提示输入两个字符串：输入此处显示的示例，或者选择你自己的：[如何操作...](img/4672OS_06_01.jpg)
- en: 'As you can see, there are two other distance measures being shown here: a fixed
    weight edit distance and a custom weight edit distance.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如你所见，这里显示了另外两种距离度量：固定权重编辑距离和自定义权重编辑距离。
- en: Play around with other examples, including punctuation and spaces.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试其他示例，包括标点符号和空格。
- en: How it works...
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'We will instantiate a `FixedWeightEditDistance` class with some weights that
    are, arbitrarily chosen:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实例化一个`FixedWeightEditDistance`类，并设置一些权重，这些权重是任意选择的：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this example, we set the delete, substitute, and insert weights to be equal.
    This is very similar to the standard edit distance, except that we modified the
    weights associated with the edit operations from 1 to 2\. Setting the transpose
    weight to negative infinity effectively turns off transpositions completely. Obviously,
    it's not necessary that the delete, substitute, and insert weights should be equal.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将删除、替换和插入的权重设置为相等。这与标准的编辑距离非常相似，唯一的区别是我们将编辑操作的权重从1修改为2。将置换权重设置为负无穷有效地完全关闭了置换操作。显然，删除、替换和插入的权重不必相等。
- en: 'We will also create a `CustomWeightEditDistance` class, which treats punctuations
    and whitespaces as matches, that is, zero cost for the insert and delete operations
    (for letters or digits, the cost remains -1). For substitutions, if the character
    is different only in case, the cost is zero; for all other cases, the cost is
    -1\. We will also turn off transposition by setting its cost to negative infinity.
    This will result in `Abc+` matching `abc-`:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将创建一个`CustomWeightEditDistance`类，它将标点符号和空格视为匹配项，也就是说，插入和删除操作的成本为零（对于字母或数字，成本仍为-1）。对于替换操作，如果字符仅在大小写上有所不同，则成本为零；对于所有其他情况，成本为-1。我们还将通过将其成本设置为负无穷来关闭置换操作。这将导致`Abc+`与`abc-`匹配：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This sort of custom weighted edit distance is particularly useful in comparing
    strings where minor formatting changes are encountered, such as gene/protein names
    that vary from `Serpin A3` to `serpina3` but refer to the same thing.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种自定义加权编辑距离特别适用于比较字符串，其中可能会遇到细微的格式更改，比如基因/蛋白质名称从`Serpin A3`变成`serpina3`，但它们指的却是同一样东西。
- en: See also
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: There is a T&T (Tsuruoka and Tsujii) specification for edit distance to compare
    protein names, refer to [http://alias-i.com/lingpipe/docs/api/com/aliasi/dict/ApproxDictionaryChunker.html#TT_DISTANCE](http://alias-i.com/lingpipe/docs/api/com/aliasi/dict/ApproxDictionaryChunker.html#TT_DISTANCE)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个 T&T（Tsuruoka 和 Tsujii）编辑距离规范用于比较蛋白质名称，参见 [http://alias-i.com/lingpipe/docs/api/com/aliasi/dict/ApproxDictionaryChunker.html#TT_DISTANCE](http://alias-i.com/lingpipe/docs/api/com/aliasi/dict/ApproxDictionaryChunker.html#TT_DISTANCE)
- en: More details on the `WeightedEditDistance` class can be found on the Javadoc
    page at [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关 `WeightedEditDistance` 类的更多细节，可以在 Javadoc 页面找到，网址为：[http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html)
- en: The Jaccard distance
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Jaccard 距离
- en: The Jaccard distance is a very popular and efficient way of comparing strings.
    The Jaccard distance operates at a token level and compares two strings by first
    tokenizing them and then dividing the number of common tokens by the total number
    of tokens. In the *Eliminate near duplicates with the Jaccard distance* recipe
    in [Chapter 1](ch01.html "Chapter 1. Simple Classifiers"), *Simple Classifiers*,
    we applied the distance to eliminate near-duplicate tweets. This recipe will go
    into a bit more detail and show you how it is computed.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Jaccard 距离是一种非常流行且高效的字符串比较方法。Jaccard 距离在标记级别进行操作，通过首先对两个字符串进行标记化，然后将共同标记的数量除以总的标记数量来比较两个字符串。在[第
    1 章](ch01.html "第 1 章：简单分类器")《简单分类器》中的*使用 Jaccard 距离消除近似重复项*示例中，我们应用该距离来消除近似重复的推文。本篇会更详细地介绍，并展示如何计算它。
- en: 'A distance of 0 is a perfect match, that is, the strings share all their terms,
    and a distance of 1 is a perfect mismatch, that is, the strings have no terms
    in common. Remember that proximity and distance are additive inverses, so proximity
    also ranges from 1 to 0\. Proximity of 1 is a perfect match, and proximity of
    0 is a perfect mismatch:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 距离为 0 是完美匹配，也就是说，两个字符串共享所有的词项，而距离为 1 是完美不匹配，也就是说，两个字符串没有共同的词项。请记住，接近度和距离是相互逆的，因此接近度的范围也是从
    1 到 0。接近度为 1 是完美匹配，接近度为 0 是完美不匹配：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The tokens are generated by `TokenizerFactory`, which is passed in during construction.
    For example, let's use `IndoEuropeanTokenizerFactory` and take a look at a concrete
    example. If `string1` is `fruit flies like a banana` and `string2` is `time flies
    like an arrow`, then the token set for `string1` would be `{'fruit', 'flies',
    'like', 'a', 'banana'}`, and the token set for `string2` would be `{'time', 'flies',
    'like', 'an', 'arrow'}`. The common terms (or the intersection) between these
    two token sets are `{'flies', 'like'}`, and the union of these terms is `{'fruit','
    flies', 'like', 'a', 'banana', 'time', 'an', 'arrow'}`. Now, we can calculate
    the Jaccard proximity by dividing the number of common terms by the total number
    of terms, that is, 2/8, which equals 0.25\. Thus, the distance is 0.75 (1 - 0.25).Obviously,
    the Jaccard distance is eminently tunable by modifying the tokenizer that the
    class is initialized with. For example, one could use a case-normalizing tokenizer
    so that `Abc` and `abc` would be considered equivalent. Similarly, a stemming
    tokenizer would consider the words `runs` and `run` to be equivalent. We will
    see a similar ability in the next distance metric, the Tf-Idf distance, as well.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 标记由 `TokenizerFactory` 生成，在构造时传入。例如，让我们使用 `IndoEuropeanTokenizerFactory`，并看一个具体示例。如果
    `string1` 是 `fruit flies like a banana`，`string2` 是 `time flies like an arrow`，那么
    `string1` 的标记集为 `{'fruit', 'flies', 'like', 'a', 'banana'}`，`string2` 的标记集为 `{'time',
    'flies', 'like', 'an', 'arrow'}`。这两个标记集之间的共同词项（或交集）是 `{'flies', 'like'}`，这些词项的并集是
    `{'fruit', 'flies', 'like', 'a', 'banana', 'time', 'an', 'arrow'}`。现在，我们可以通过将共同词项的数量除以词项的总数量来计算
    Jaccard 接近度，即 2/8，结果为 0.25。因此，距离是 0.75（1 - 0.25）。显然，通过修改类初始化时使用的标记器，Jaccard 距离是非常可调的。例如，可以使用一个大小写标准化的标记器，使得
    `Abc` 和 `abc` 被认为是等效的。同样，使用词干提取标记器时，`runs` 和 `run` 将被认为是等效的。我们将在下一个距离度量——Tf-Idf
    距离中看到类似的功能。
- en: How to do it...
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Here''s how to run the `JaccardDistance` example:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何运行 `JaccardDistance` 示例：
- en: 'In Eclipse, run the `JaccardDistanceSample` class, or in the command line,
    type:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Eclipse 中，运行 `JaccardDistanceSample` 类，或者在命令行中输入：
- en: '[PRE8]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As in the previous recipes, you will be prompted for two strings. The first
    string that we will use is `Mimsey Were the Borogroves`, which is an excellent
    sci-fi short-story title, and the second string `All mimsy were the borogoves,`
    is the actual line from *Jabberwocky* that inspired it:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与之前的示例一样，您将被要求输入两个字符串。我们将使用的第一个字符串是`Mimsey Were the Borogroves`，这是一个非常优秀的科幻短篇小说标题，第二个字符串`All
    mimsy were the borogoves,`是来自*Jabberwocky*的实际诗句，启发了这个标题：
- en: '[PRE9]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output contains the tokens and the distances using three different tokenizers.
    The `IndoEuropean` and `EnglishStopWord` tokenizers are pretty close and show
    that these two lines are far apart. Remember that the closer two strings are,
    the lesser is the distance between them. The character tokenizer, however, shows
    that these lines are closer to each other with characters as the basis of comparison.
    Tokenizers can make a big difference in calculating the distance between strings.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出包含使用三种不同分词器生成的标记和距离。`IndoEuropean`和`EnglishStopWord`分词器非常相似，显示这两行文本相距较远。记住，两个字符串越接近，它们之间的距离就越小。然而，字符分词器显示，这些行在以字符为比较基础的情况下距离较近。分词器在计算字符串间距离时可能会产生很大的差异。
- en: How it works...
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'The code is straightforward, and we will just cover the creation of the `JaccardDistance`
    objects. We will start with three tokenizer factories:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 代码很简单，我们只会讲解`JaccardDistance`对象的创建。我们将从三个分词器工厂开始：
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that `englishStopWordTf` uses a base tokenizer factory to construct itself.
    Refer to [Chapter 2](ch02.html "Chapter 2. Finding and Working with Words"), *Finding
    and Working with Words*, if there are questions on what is going on here.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`englishStopWordTf`使用基础分词器工厂构建自己。如果有任何疑问，参阅[第2章](ch02.html "第2章. 查找和处理词语")，*查找和处理词语*。
- en: 'Next, the Jaccard distance classes are constructed, given a tokenizer factory
    as an argument:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，构建Jaccard距离类，并将分词器工厂作为参数：
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The rest of the code is just our standard I/O loop and some print statements.
    That's it! On to more sophisticated measures of string distance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的代码只是我们标准的输入/输出循环和一些打印语句。就是这样！接下来是更复杂的字符串距离度量。
- en: The Tf-Idf distance
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tf-Idf距离
- en: A very useful distance metric between strings is provided by the `TfIdfDistance`
    class. It is, in fact, closely related to the distance metric from the popular
    open source search engine, Lucene/SOLR/Elastic Search, where the strings being
    compared are the query against documents in the index. Tf-Idf stands for the core
    formula that is **term frequency** (**TF**) times **inverse document frequency**
    (**IDF**) for terms shared by the query and the document. A very cool thing about
    this approach is that common terms (for example, `the`) that are very frequent
    in documents are downweighted, while rare terms are upweighted in the distance
    comparison. This can help focus the distance on terms that are actually discriminating
    in the document collection.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常有用的字符串间距离度量是由`TfIdfDistance`类提供的。它实际上与流行的开源搜索引擎Lucene/SOLR/Elastic Search中的距离度量密切相关，其中被比较的字符串是查询与索引中文档的比对。Tf-Idf代表核心公式，即**词频**（**TF**）乘以**逆文档频率**（**IDF**），用于查询与文档中共享的词。关于这种方法的一个非常酷的地方是，常见词（例如，`the`）在文档中出现频繁，因此其权重被下调，而稀有词则在距离比较中得到上调。这有助于将距离集中在文档集中真正具有区分性的词上。
- en: Not only does `TfIdfDistance` come in handy for search-engine-like applications,
    it can be very useful for clustering and for any problem that calls for document
    similarity without supervised training data. It has a desirable property; scores
    are normalized to a score between 0 and 1, and for a fixed document `d1` and varying
    length documents `d2`, do not overwhelm the assigned score. In our experience,
    the scores for different pairs of documents are fairly robust if you were trying
    to rank the quality of match for a pair of documents.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`TfIdfDistance`不仅对类似搜索引擎的应用非常有用，它对于聚类和任何需要计算文档相似度的问题也非常有用，而无需监督训练数据。它有一个理想的属性；分数被标准化为0到1之间的分数，并且对于固定的文档`d1`和不同长度的文档`d2`，不会使分配的分数过大。在我们的经验中，如果你想评估一对文档的匹配质量，不同文档对的分数是相当稳健的。'
- en: Note
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that there are a range of different distances called Tf-Idf distances.
    The one in this class is defined to be symmetric, unlike typical Tf-Idf distances
    that are defined for information-retrieval purposes.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，有一系列不同的距离被称为Tf-Idf距离。此类中的距离定义为对称的，不像典型的用于信息检索目的的Tf-Idf距离。
- en: There is a lot of information in the Javadoc that is well worth a good look.
    However, for the purposes of these recipes, all you need to know is that the Tf-Idf
    distance is useful for finding similar documents on a word-by-word basis.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Javadoc中有很多值得一看的信息。然而，针对这些食谱，你需要知道的是，Tf-Idf距离在逐字查找相似文档时非常有用。
- en: How to do it...
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'In the quest to keep things a little interesting, we will use our `TfIdfDistance`
    class to build a really simple search engine over tweets. We will perform the
    following steps:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让事情稍微有点趣味，我们将使用我们的`TfIdfDistance`类来构建一个非常简单的推文搜索引擎。我们将执行以下步骤：
- en: If you have not done it already, run the `TwitterSearch` class from [Chapter
    1](ch01.html "Chapter 1. Simple Classifiers"), *Simple Classifiers*, and get some
    tweets to play with, or go with our provided data. We will use the tweets found
    by running the `Disney World` query, and they are already in the `data` directory.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你还没有做过，运行[第1章](ch01.html "第1章 简单分类器")中的`TwitterSearch`类，*简单分类器*，并获取一些推文进行操作，或者使用我们提供的数据。我们将使用通过运行`Disney
    World`查询找到的推文，它们已经在`data`目录中。
- en: 'Type the following in the command line—this uses our defaults:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令行中输入以下内容——这使用我们的默认设置：
- en: '[PRE12]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Enter a query that has some likely word matches:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入一个可能有匹配单词的查询：
- en: '[PRE13]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: That's it. Try different queries and play around with the scores. Then, have
    a look at the source.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就是这样。尝试不同的查询，玩弄一下得分。然后，看看源代码。
- en: How it works...
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'This code is a very simple way to build a search engine rather than a good
    way to build one. However, it is a decent way to explore how the concept of string
    distance works in the search context. Later in the book, we will perform clustering
    based on the same distance metric. Start with the `main()` class in `src/com/lingpipe/cookbook/chapter6/TfIdfSearch.java`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码是构建搜索引擎的一种非常简单的方法，而不是一种好方法。然而，它是探索字符串距离概念在搜索上下文中如何工作的一个不错的方式。本书后续将基于相同的距离度量进行聚类。可以从`src/com/lingpipe/cookbook/chapter6/TfIdfSearch.java`中的`main()`类开始：
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This program can take command-line-supplied files for the searched data in
    the `.csv` format and a text file for use as the source of training data. Next,
    we will set up a tokenizer factory and `TfIdfDistance`. If you are not familiar
    with tokenizer factories, then refer to the *Modifying tokenizer factories* recipe
    in [Chapter 2](ch02.html "Chapter 2. Finding and Working with Words"), *Modifying
    Tokenizer Factories*, for an explanation:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序可以接受命令行传入的`.csv`格式的搜索数据文件和用作训练数据源的文本文件。接下来，我们将设置一个标记器工厂和`TfIdfDistance`。如果你不熟悉标记器工厂，可以参考[第2章](ch02.html
    "第2章 查找和处理单词")中的*修改标记器工厂*食谱，以获取解释：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, we will get the data that will be the IDF component by splitting the
    training text on ".", which approximates sentence detection—we could have done
    a proper sentence detection like we did in the *Sentence detection* recipe in
    [Chapter 5](ch05.html "Chapter 5. Finding Spans in Text – Chunking"), *Finding
    Spans in Text – Chunking*, but we chose to keep the example as simple as possible:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将通过按“.”分割训练文本来获取将作为IDF组件的数据，这种方式大致上是句子检测——我们本可以像在[第5章](ch05.html "第5章 查找文本中的范围
    - 分块")的*句子检测*食谱中那样进行正式的句子检测，但我们选择尽可能简单地展示这个例子：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Inside the `for` loop, there is `handle()`, which trains the class with knowledge
    of the token distributions in the corpus, with sentences being the document. It
    often happens that the concept of document is either smaller (sentence, paragraph,
    and word) or larger than what is typically termed `document`. In this case, the
    document frequency will be the number of sentences the token is in.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在`for`循环中，有`handle()`，它通过语料库中的标记分布训练该类，句子即为文档。通常情况下，文档的概念要么小于（句子、段落和单词），要么大于通常所称的`文档`。在这种情况下，文档频率将是该标记所在的句子数。
- en: 'Next, the documents that we are searching are loaded:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载我们要搜索的文档：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The console is set up to read in the query:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台设置为读取查询：
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, each document is scored against the query with `TfIdfDistance` and put
    into `ObjectToDoubleMap`, which keeps track of the proximity:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，每个文档将使用`TfIdfDistance`与查询进行评分，并放入`ObjectToDoubleMap`中，该映射用于跟踪相似度：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, `scoredMatches` is retrieved in the proximity order, and the first
    10 examples are printed out:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`scoredMatches`按相似度顺序被检索，并打印出前10个示例：
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: While this approach is very inefficient, in that, each query iterates over all
    the training data, does an explicit `TfIdfDistance` comparison, and stores it,
    it is not a bad way to play around with small datasets and comparison metrics.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法非常低效，因为每次查询都遍历所有训练数据，进行显式的`TfIdfDistance`比较并存储结果，但它对于玩转小数据集和比较度量指标来说并不是一种坏方法。
- en: There's more...
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: There are some subtleties worth highlighting about `TfIdfDistance`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些值得强调的`TfIdfDistance`的细节。
- en: Difference between supervised and unsupervised trainings
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有监督和无监督训练的区别
- en: When we train `TfIdfDistance`, there are some important differences in the use
    of training from the ones used in the rest of the book. The training done here
    is unsupervised, which means that no human or other external source has marked
    up the data for the expected outcome. Most of the recipes in this book that train
    use human annotated, or supervised, data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练`TfIdfDistance`时，在训练的使用上有一些重要的区别，这些区别与本书其他部分的使用不同。这里进行的训练是无监督的，这意味着没有人类或其他外部来源标记数据的预期结果。本书中大多数训练使用的是人类标注或监督数据。
- en: Training on test data is OK
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在测试数据上训练是可以的
- en: As this is unsupervised data, there is no requirement that the training data
    should be be distinct from the evaluation or production data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是无监督数据，因此没有要求训练数据必须与评估或生产数据不同。
- en: Using edit distance and language models for spelling correction
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用编辑距离和语言模型进行拼写纠正
- en: Spelling correction takes a user input text and provides a corrected form. Most
    of us are familiar with automatic spelling correction via our smart phones or
    editors such as Microsoft Word. There are obviously quite a few amusing examples
    of these on the Web where the spelling correction fails. In this example, we'll
    build our own spelling-correction engine and look at how to tune it.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 拼写纠正接收用户输入的文本并提供纠正后的形式。我们大多数人都熟悉通过智能手机或像Microsoft Word这样的编辑器进行的自动拼写纠正。网络上显然有很多有趣的例子，展示了拼写纠正失败的情况。在这个例子中，我们将构建自己的拼写纠正引擎，并看看如何调整它。
- en: 'LingPipe''s spelling correction is based on a noisy-channel model which models
    user mistakes and expected user input (based on the data). Expected user input
    is modeled by a character-language model, and mistakes (or noise) is modeled by
    weighted edit distance. The spelling correction is done using the `CompiledSpellChecker`
    class. This class implements the noisy-channel model and provides an estimate
    of the most likely message, given that the message actually received. We can express
    this through a formula in the following manner:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: LingPipe的拼写纠正基于噪声信道模型，该模型模拟了用户的错误和预期用户输入（基于数据）。预期用户输入通过字符语言模型进行建模，而错误（或噪声）则通过加权编辑距离建模。拼写纠正是通过`CompiledSpellChecker`类来完成的。该类实现了噪声信道模型，并根据实际收到的消息，提供最可能的消息估计。我们可以通过以下公式来表达这一点：
- en: '[PRE21]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In other words, we will first create a model of the intended message by creating
    an n-gram character-language model. The language model stores the statistics of
    seen phrases, that is, essentially, it stores counts of how many times the n-grams
    occurred. This gives us `P(intended)`. For example, `P(intended)` is how likely
    is the character sequence `the`. Next, we will create the channel model, which
    is a weighted edit distance and gives us the probability that the error was typed
    for that intended text. Again, for example, how likely is the error `teh` when
    the user intended to type `the`. In our case, we will model the likeliness using
    weighted edit distance where the weights are scaled as log probabilities. Refer
    to the *Weighted edit distance* recipe earlier in the chapter.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们首先通过创建一个n-gram字符语言模型来构建预期消息的模型。语言模型存储了已见短语的统计数据，本质上，它存储了n-gram出现的次数。这给我们带来了`P(intended)`。例如，`P(intended)`表示字符序列`the`的可能性。接下来，我们将创建信道模型，这是一个加权编辑距离，它给出了输入错误的概率，即用户输入的错误与预期文本之间的差距。再例如，用户本来打算输入`the`，但错误地输入了`teh`，这种错误的概率是多少。我们将使用加权编辑距离来建模这种可能性，其中权重按对数概率进行缩放。请参考本章前面的*加权编辑距离*配方。
- en: The usual way of creating a compiled spell checker is through an instance of
    `TrainSpellChecker`. The result of compiling the spell-checker-training class
    and reading it back in is a compiled spell checker. `TrainSpellChecker` creates
    the basic models, weighted edit distance, and token set through the compilation
    process. We will then need to set various parameters on the `CompiledSpellChecker`
    object.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个编译后的拼写检查器的常见方法是通过`TrainSpellChecker`实例。编译拼写检查训练类并将其读取回来后的结果就是一个编译过的拼写检查器。`TrainSpellChecker`通过编译过程创建了基本的模型、加权编辑距离和标记集。然后，我们需要在`CompiledSpellChecker`对象上设置各种参数。
- en: A tokenizer factory can be optionally specified to train token-sensitive spell
    checkers. With tokenization, input is further normalized to insert a single whitespace
    between all the tokens that are not already separated by a space in the input.
    The tokens are then output during compilation and read back into the compiled
    spell checker. The output of set of tokens may be pruned to remove any below a
    given count threshold. The thresholding doesn't make sense in the absence of tokens
    because we only have characters to count in the absence of tokens. Additionally,
    the set of known tokens can be used to constrain the set of alternative spellings
    suggested during spelling correction to include only tokens in the observed token
    set.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 可以选择性地指定一个分词工厂来训练对标记敏感的拼写检查器。通过分词，输入会进一步规范化，在所有未由空格分隔的标记之间插入单个空格。标记会在编译时输出，并在编译后的拼写检查器中读取回来。标记集的输出可能会被修剪，以删除任何低于给定计数阈值的标记。因为在没有标记的情况下我们只有字符，所以阈值在没有标记的情况下没有意义。此外，已知标记集可用于在拼写校正时限制替代拼写的建议，仅包括观察到的标记集中的标记。
- en: 'This approach to spell check has several advantages over a pure dictionary-based
    solution:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这种拼写检查方法相较于纯粹基于字典的解决方案有几个优点：
- en: The context is usefully modeled. `Frod` can be corrected to `Ford` if the next
    word is `dealership` and to `Frodo` if the next word is `Baggins`—a character
    from *The Lord of the Rings* trilogy.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个上下文得到了有效建模。如果下一个词是`dealership`，则`Frod`可以被纠正为`Ford`；如果下一个词是`Baggins`（《魔戒》三部曲中的角色），则可以纠正为`Frodo`。
- en: Spell checking can be sensitive to domains. Another big advantage of this approach
    over dictionary-based spell checking is that the corrections are motivated by
    data in the training corpus. So, `trt` will be corrected to `tort` in a legal
    domain, `tart` in a cooking domain, and `TRt` in a bioinformatics domain.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拼写检查可以对领域敏感。这个方法相较于基于字典的拼写检查还有一个大优点，那就是修正是基于训练语料库中的数据进行的。因此，在法律领域，`trt`将被纠正为`tort`，在烹饪领域，它将被纠正为`tart`，在生物信息学领域，它将被纠正为`TRt`。
- en: How to do it...
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s look at the steps involved in running spell checking:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下运行拼写检查的步骤：
- en: 'In your IDE, run the `SpellCheck` class, or in the command line, type the following—note
    that we are allocating 1 gigabyte of heap space with the `–Xmx1g` flag:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的IDE中，运行`SpellCheck`类，或者在命令行中输入以下命令—注意我们通过`–Xmx1g`标志分配了1GB的堆内存：
- en: '[PRE22]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Be patient; the spell checker takes a minute or two to train.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请耐心等待；拼写检查器需要一到两分钟的时间来训练。
- en: 'Now, let''s enter some misspelled words such as `beleive`:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们输入一些拼写错误的单词，例如`beleive`：
- en: '[PRE23]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As you can see, we got the best alternative to the input text as well as some
    other alternatives. They are sorted by the likelihood of being the best alternative.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如你所见，我们获得了最接近输入文本的最佳替代方案，以及一些其他替代方案。它们按最有可能是最佳替代方案的可能性排序。
- en: 'Now, we can play around with different input and see how well this spell checker
    does. Try multiple words in the input and see how it performs:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以尝试不同的输入，看看这个拼写检查器的表现如何。输入多个单词，看看它的效果：
- en: '[PRE24]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Also, try inputting some proper names to see how they get evaluated.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，尝试输入一些专有名词，看看它们是如何被评估的。
- en: How it works...
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Now, let''s look at what makes all this tick. We will start off by setting
    up `TrainSpellChecker`, which requires a `NGramProcessLM` instance, `TokenizerFactory`,
    and an `EditDistance` object that sets up weights for edit operations such as
    deletion, insertion, substitution, and so on:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下是什么让这一切运作起来。我们将从设置`TrainSpellChecker`开始，它需要一个`NGramProcessLM`实例、`TokenizerFactory`和一个`EditDistance`对象，用于设置编辑操作的权重，例如删除、插入、替换等：
- en: '[PRE25]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`NGramProcessLM` needs to know the number of characters to sample in its modeling
    of the data. Reasonable values have been supplied in this example for the weighted
    edit distance, but they can be played with to help with variations due to particular
    datasets:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`NGramProcessLM` 需要知道在建模数据时要采样的字符数量。此示例中已经为加权编辑距离提供了合理的值，但可以根据特定数据集的变化进行调整：'
- en: '[PRE26]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '`TrainSpellChecker` can now be constructed, and next, we will load 150,000
    lines of books from Project Gutenberg. In a search-engine context, this data will
    be the data in your index:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`TrainSpellChecker` 现在可以构建，接下来我们将从古腾堡计划中加载 150,000 行书籍。在搜索引擎的上下文中，这些数据将是你的索引中的数据：'
- en: '[PRE27]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we will add entries from a dictionary to help with rare words:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从字典中添加条目，以帮助处理罕见单词：
- en: '[PRE28]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we will compile `TrainSpellChecker` so that we can instantiate `CompiledSpellChecker`.
    Typically, the output of the `compileTo()` operation is written to disk, and `CompiledSpellChecker`
    is read and instantiated from the disk, but the in-memory option is being used
    here:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编译 `TrainSpellChecker`，以便我们可以实例化 `CompiledSpellChecker`。通常，`compileTo()`
    操作的输出会写入磁盘，并从磁盘读取并实例化 `CompiledSpellChecker`，但这里使用的是内存中的选项：
- en: '[PRE29]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note that there is also a way to deserialize to `TrainSpellChecker` in cases
    where more data might be added later. `CompiledSpellChecker` will not accept further
    training instances.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，还有一种方法可以将数据反序列化为 `TrainSpellChecker`，以便以后可能添加更多数据。`CompiledSpellChecker`
    不接受进一步的训练实例。
- en: '`CompiledSpellChecker` admits many fine-tuning methods that are not relevant
    during training but are relevant in use. For example, it can take a set of strings
    that are not to be edited; in this case, the single value is `lingpipe`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`CompiledSpellChecker` 接受许多微调方法，这些方法在训练期间不相关，但在使用时是相关的。例如，它可以接受一组不进行编辑的字符串；在这种情况下，单个值是
    `lingpipe`：'
- en: '[PRE30]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If these tokens are seen in the input, they will not be considered for edits.
    This can have a huge impact on the run time. The larger this set is, the faster
    the decoder will run. Configure the set of do-not-edit tokens to be as large as
    possible if execution speed is important. Usually, this is done by taking the
    object to counter-map from the compiled spell checker and saving tokens with high
    counts.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入中出现这些标记，它们将不会被考虑进行编辑。这会对运行时间产生巨大影响。这个集合越大，解码器的运行速度就越快。如果执行速度很重要，请将不编辑标记的集合配置得尽可能大。通常，这通过从已编译的拼写检查器中获取对象并保存出现频率较高的标记来实现。
- en: 'During training, the tokenizer factory was used to normalize data into tokens
    separated by a single whitespace. It is not serialized in the compile step, so
    if token sensitivity is needed in do-not-edit tokens, then it must be supplied:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，使用了分词器工厂将数据标准化为由单个空格分隔的标记。它不会在编译步骤中序列化，因此，如果需要在不编辑标记中保持标记敏感性，则必须提供：
- en: '[PRE31]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The nBest parameter is set for the number of hypotheses that will be considered
    in modifying inputs. Even though the `nBest` size in the output is set to 3, it
    is advisable to allow for a larger hypothesis space in the left-to-right exploration
    of best performing edits. Also, the class has methods to control what edits are
    allowed and how they are scored. See the tutorial and Javadoc for more about them.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`nBest` 参数设置了在修改输入时将考虑的假设数量。尽管输出中的 `nBest` 大小设置为 3，但建议在从左到右探索最佳编辑的过程中允许更大的假设空间。此外，类还有方法来控制允许的编辑以及如何评分。有关更多信息，请参阅教程和
    Javadoc。'
- en: 'Finally, we will do a console I/O loop to generate spelling variations:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将进行一个控制台 I/O 循环以生成拼写变化：
- en: '[PRE32]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Tip
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: We have included a dictionary in this model, and we will just feed the dictionary
    entries into the trainer like any other data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个模型中包含了一个字典，我们将像处理其他数据一样将字典条目输入到训练器中。
- en: It might be worthwhile to boost the dictionary by training each word in the
    dictionary more than once. Depending on the count of the dictionary, it might
    dominate or be dominated by the source training data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 通过多次训练字典中的每个单词，可能会使字典得到增强。根据字典的数量，它可能会主导或被源训练数据所主导。
- en: See also
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: The spelling-correction tutorial is more complete and covers evaluation at [http://alias-i.com/lingpipe/demos/tutorial/querySpellChecker/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/querySpellChecker/read-me.html)
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拼写修正教程更完整，涵盖了在 [http://alias-i.com/lingpipe/demos/tutorial/querySpellChecker/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/querySpellChecker/read-me.html)
    进行的评估
- en: The Javadoc for `CompiledSpellChecker` can be found at [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/CompiledSpellChecker.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/CompiledSpellChecker.html)
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CompiledSpellChecker` 的 Javadoc 可以在 [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/CompiledSpellChecker.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/CompiledSpellChecker.html)
    找到'
- en: More information on how spell checkers work is given in the textbook, *Speech
    and Language Processing*, *Jurafsky*, *Dan*, and *James H. Martin*, *2000*, *Prentice-Hall*
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多关于拼写检查器如何工作的内容，请参见教材《*Speech and Language Processing*》，*Jurafsky*、*Dan* 和
    *James H. Martin* 编著，*2000*，*Prentice-Hall*。
- en: The case restoring corrector
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大小写恢复校正器
- en: A case-restoring spell corrector, also called a truecasing corrector, only restores
    the case and does not change anything else, that is, it does not correct spelling
    errors. This is very useful when dealing with low-quality text from transcriptions,
    automatic speech-recognition output, chat logs, and so on, which contain a variety
    of case challenges. We typically want to enhance this text to build better rule-based
    or machine-learning systems. For example, news and video transcriptions (such
    as closed captions) typically have errors, and this makes it harder to use this
    data to train NER. Case restoration can be used as a normalization tool across
    different data sources to ensure that all the data is consistent.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 大小写恢复拼写校正器，也叫做真大小写校正器，只恢复大小写，不更改其他任何内容，也就是说，它不会纠正拼写错误。当处理转录、自动语音识别输出、聊天记录等低质量文本时，这非常有用，因为这些文本通常包含各种大小写问题。我们通常希望增强这些文本，以构建更好的基于规则或机器学习的系统。例如，新闻和视频转录（如字幕）通常存在错误，这使得使用这些数据训练命名实体识别（NER）变得更加困难。大小写恢复可以作为不同数据源之间的标准化工具，确保所有数据的一致性。
- en: How to do it...
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'In your IDE, run the `CaseRestore` class, or in the command line, type the
    following:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的 IDE 中运行 `CaseRestore` 类，或者在命令行中输入以下内容：
- en: '[PRE33]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, let''s type in some mangled-case or single-case input:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们输入一些错误大小写或单一大小写的文本：
- en: '[PRE34]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As you can see, the mangled case gets corrected. If we use more modern text,
    such as current newspaper data or something similar, this would be directly applicable
    to case-normalizing broadcast news transcripts or closed captions.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如你所见，大小写错误已经被纠正。如果我们使用更现代的文本，例如当前的报纸数据或类似内容，这将直接应用于广播新闻转录或字幕的大小写标准化。
- en: How it works...
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'The class works in a manner similar to the spelling correction in which we
    have a model specified by the language model and a channel model specified by
    the edit distance metric. The distance metric, however, only allows case changes,
    that is, case variants are zero cost, and all other edit costs are set to `Double.NEGATIVE_INFINITY`:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 该类的工作方式类似于拼写校正，我们有一个由语言模型指定的模型和一个由编辑距离度量指定的通道模型。然而，距离度量只允许大小写更改，也就是说，大小写变体是零成本的，所有其他编辑成本都被设置为
    `Double.NEGATIVE_INFINITY`：
- en: 'We will focus on what is different from the previous recipe rather than going
    over all the source. We will train the spell checker with some English text from
    Project Gutenberg and use the `CASE_RESTORING` edit distance from the `CompiledSpellChecker`
    class:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点讨论与前一个方法不同的部分，而不是重复所有源代码。我们将使用来自古腾堡计划的英文文本训练拼写检查器，并使用 `CompiledSpellChecker`
    类中的 `CASE_RESTORING` 编辑距离：
- en: '[PRE35]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Once again, by invoking the `bestAlternative` method, we will get the best
    estimate of case-restored text:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 再次通过调用 `bestAlternative` 方法，我们将获得最好的大小写恢复文本估计：
- en: '[PRE36]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: That's it. Case restoration is made easy.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。大小写恢复变得简单。
- en: See also
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: The paper by Lucian Vlad Lita et al., 2003, at [http://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf](http://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf)
    is a good reference on truecasing
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lucian Vlad Lita 等人于 2003 年的论文，[http://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf](http://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf)，是关于真大小写恢复的一个很好的参考资料。
- en: Automatic phrase completion
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动短语补全
- en: Automatic phrase completion is different from spelling correction, in that,
    it finds the most likely completion among a set of fixed phrases for the text
    entered so far by a user.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 自动短语补全与拼写校正不同，它是在用户输入的文本中，从一组固定短语中找到最可能的补全。
- en: 'Obviously, automatic phrase completion is ubiquitous on the Web, for instance,
    on[https://google.com](https://google.com). For example, if I type `anaz` as a
    query, Google pops up the following suggestions:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，自动短语补全在网络上无处不在，例如，在[https://google.com](https://google.com)上。例如，如果我输入 `anaz`
    作为查询，谷歌会弹出以下建议：
- en: '![Automatic phrase completion](img/4672OS_06_02.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![自动短语补全](img/4672OS_06_02.jpg)'
- en: Note that the application is performing spelling checking at the same time as
    completion. For instance, the top suggestion is **amazon**, even though the query
    so far is **anaz**. This is not surprising, given that the number of results reported
    for the phrases that start with **anaz** is probably very small.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，应用程序在完成补全的同时也在进行拼写检查。例如，即使查询到目前为止是**anaz**，但顶部的建议是**amazon**。这并不令人惊讶，因为以**anaz**开头的短语的结果数量可能非常少。
- en: Next, note that it's not doing word suggestion but phrase suggestion. Some of
    the results, such as **amazon prime** are two words.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，注意到它并不是进行单词建议，而是短语建议。比如一些结果，如**amazon prime**是由两个单词组成的。
- en: One important difference between autocompletion and spell checking is that autocompletion
    typically operates over a fixed set of phrases that must match the beginning to
    be completed. What this means is that if I type a query `I want to find anaz`,
    there are no suggested completions. The source of phrases for a web search is
    typically high-frequency queries from the query logs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 自动补全和拼写检查之间的一个重要区别是，自动补全通常是基于一个固定的短语集，必须匹配开头才能完成。这意味着，如果我输入查询`I want to find
    anaz`，就不会有任何推荐补全。网页搜索的短语来源通常是来自查询日志的高频查询。
- en: In LingPipe, we use the `AutoCompleter` class, which maintains a dictionary
    of phrases with counts and provides suggested completions based on prefix matching
    by weighted edit distance and phrase likelihood.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在LingPipe中，我们使用`AutoCompleter`类，它维护一个包含计数的短语字典，并通过加权编辑距离和短语似然性基于前缀匹配提供建议的补全。
- en: The autocompleter finds the best scoring phrases for a given prefix. The score
    of a phrase versus a prefix is the sum of the score of the phrase and the maximum
    score of the prefix against any prefix of the phrase. The score for a phrase is
    just its maximum likelihood probability estimate, that is, the log of its count
    divided by the sum of all counts.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 自动补全器为给定的前缀找到得分最高的短语。短语与前缀的得分是短语得分和该前缀与短语任何前缀匹配的最大得分之和。短语的得分就是其最大似然概率估计，即其计数的对数除以所有计数的总和。
- en: Google and other search engines most likely use their query counts as the data
    for the best scoring phrases. As we don't have query logs here, we'll use US census
    data about cities in the US with populations greater than 100,000\. The phrases
    are the city names, and their counts are their populations.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌和其他搜索引擎很可能将它们的查询计数作为最佳得分短语的数据。由于我们这里没有查询日志，因此我们将使用美国人口超过100,000的城市的美国人口普查数据。短语是城市名称，计数是它们的人口。
- en: How to do it...
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In your IDE, run the `AutoComplete` class, or in the command line, type the
    following command:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的IDE中，运行`AutoComplete`类，或者在命令行中输入以下命令：
- en: '[PRE37]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Enter some US city names and look at the output. For example, typing `new`
    will result in the following output:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入一些美国城市名称并查看输出。例如，输入`new`将产生以下输出：
- en: '[PRE38]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Typing city names that don''t exist in our initial list will not return any
    output:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入我们初始列表中不存在的城市名称将不会返回任何输出：
- en: '[PRE39]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: How it works...
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Configuring an autocompleter is very similar to configuring spelling, except
    that instead of training a language model, we will supply it with a fixed list
    of phrases and counts, an edit distance metric, and some configuration parameters.
    The initial portion of this code just reads a file and sets up a map of phrases
    to counts:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 配置自动补全器与配置拼写检查非常相似，不同之处在于，我们不是训练一个语言模型，而是提供一个固定的短语和计数列表、一个编辑距离度量以及一些配置参数。代码的初始部分只是读取一个文件，并设置一个短语到计数的映射：
- en: '[PRE40]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The next step is to configure the edit distance. This will measure how close
    a prefix of a target phrase is to the query prefix. This class uses a fixed weight
    edit distance, but any edit distance might be used in general:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是配置编辑距离。此操作将衡量目标短语的前缀与查询前缀的相似度。该类使用固定权重的编辑距离，但一般来说，可以使用任何编辑距离：
- en: '[PRE41]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'There are a few parameters to tune autocompletion: the edit distance and search
    parameters. The edit distance is tuned in exactly the same way as it is for spelling.
    The maximum number of results to return is more of an application''s decision
    than a tuning''s decision. Having said this, smaller result sets are faster to
    compute. The maximum queue size indicates how big the set of hypotheses can get
    inside the autocompleter before being pruned. Set `maxQueueSize` as low as possible
    while still performing adequately to increase speed:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些参数可以调整自动补全：编辑距离和搜索参数。编辑距离的调整方式与拼写检查完全相同。返回结果的最大数量更多是应用程序的决定，而不是调整的决策。话虽如此，较小的结果集计算速度更快。最大队列大小表示在被修剪之前，自动补全器内部假设集可以变得多大。在仍能有效执行的情况下，将`maxQueueSize`设置为尽可能小，以提高速度：
- en: '[PRE42]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: See also
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: Review the Javadoc for the `AutoCompleter` class at [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/AutoCompleter.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/AutoCompleter.html)
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看`AutoCompleter`类的Javadoc文档：[http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/AutoCompleter.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/AutoCompleter.html)
- en: Single-link and complete-link clustering using edit distance
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用编辑距离的单链和完全链聚类
- en: Clustering is the process of grouping a collection of objects by their similarities,
    that is, using some sort of distance measure. The idea behind clustering is that
    objects within a cluster are located close to each other, but objects in different
    clusters are farther away from each other. We can divide clustering techniques
    very broadly into hierarchical (or agglomerative) and divisional techniques. Hierarchical
    techniques start by assuming that every object is its own cluster and merge clusters
    together until a stopping criterion has been met.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是通过相似性将一组对象分组的过程，也就是说，使用某种距离度量。聚类的核心思想是，聚类内的对象彼此接近，而不同聚类的对象彼此较远。我们可以大致将聚类技术分为层次（或凝聚）和分治两种技术。层次技术从假设每个对象都是自己的聚类开始，然后合并聚类，直到满足停止准则。
- en: For example, a stopping criterion can be a fixed distance between every cluster.
    Divisional techniques go the other way and start by grouping all the objects into
    one cluster and split it until a stopping criterion has been met, such as the
    number of clusters.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个停止准则可以是每个聚类之间的固定距离。分治技术则恰好相反，首先将所有对象聚集到一个聚类中，然后进行拆分，直到满足停止准则，例如聚类的数量。
- en: We will review hierarchical techniques in the next few recipes. The two clustering
    implementations we will provide in LingPipe are single-link clustering and complete-link
    clustering; the resulting clusters form what is known as a partition of the input
    set. A set of sets is a partition of another set if each element of the set is
    a member of exactly one set of the partition. In mathematical terms, the sets
    that make up a partition are pair-wise disjoint, and the union is the original
    set.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的几个实例中回顾层次聚类技术。LingPipe中我们将提供的两种聚类实现是单链聚类和完全链聚类；所得的聚类形成输入集的所谓划分。若一组集合是另一个集合的划分，则该集合的每个元素恰好属于划分中的一个集合。从数学角度来说，构成划分的集合是成对不相交的，并且它们的并集是原始集合。
- en: A clusterer takes a set of objects as input and returns a set of sets of objects
    as output, that is, in code, `Clusterer<String>` has a `cluster` method, which
    operates on `Set<String>` and returns `Set<Set<String>>`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类器接收一组对象作为输入，并返回一组对象的集合作为输出。也就是说，在代码中，`Clusterer<String>`有一个`cluster`方法，作用于`Set<String>`并返回`Set<Set<String>>`。
- en: 'A hierarchical clusterer extends the `Clusterer` interface and also operates
    on a set of objects, but it returns `Dendrogram` instead of a set of sets of objects.
    A dendrogram is a binary tree over the elements being clustered, with distances
    attached to each branch, which indicates the distance between the two sub-branches.
    For the `aa`, `aaa`, `aaaaa`, `bbb`, `bbbb` strings, the single-link based dendrogram
    with `EditDistance` as the metric looks like this:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类器扩展了`Clusterer`接口，同样作用于一组对象，但返回的是`Dendrogram`（树状图），而不是一组对象的集合。树状图是一个二叉树，表示正在聚类的元素，其中每个分支附有距离值，表示两个子分支之间的距离。对于`aa`、`aaa`、`aaaaa`、`bbb`、`bbbb`这些字符串，基于单链的树状图并采用`EditDistance`作为度量看起来是这样的：
- en: '[PRE43]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The preceding dendrogram is based on single-link clustering, which takes the
    minimum distance between any two elements as the measure of similarity. So, when
    `{'aa','aaa'}` is merged with `{'aaaa'}`, the score is 2.0 by adding two `a` to
    `aaa`. Complete-link clustering takes the maximum distance between any two elements,
    which would be 3.0, with an addition of three `a` to `aa`. Single-link clustering
    tends to create highly separated clusters, whereas complete-link clustering tends
    to create more tightly centered clusters.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 上述树状图基于单链聚类，单链聚类将任何两个元素之间的最小距离作为相似性的度量。因此，当`{'aa','aaa'}`与`{'aaaa'}`合并时，得分为2.0，通过将两个`a`添加到`aaa`中。完全链接聚类则采用任何两个元素之间的最大距离，这将是3.0，通过将三个`a`添加到`aa`中。单链聚类倾向于形成高度分离的聚类，而完全链接聚类则倾向于形成更紧密的聚类。
- en: There are two ways to extract clusterings from dendrograms. The simplest way
    is to set a distance bound and maintain every cluster formed at less than or equal
    to this bound. The other way to construct a clustering is to continue cutting
    the highest distance cluster until a specified number of clusters is obtained.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 从树状图中提取聚类有两种方法。最简单的方法是设置一个距离上限，并保持所有在此上限或以下形成的聚类。另一种构建聚类的方法是继续切割最大距离的聚类，直到获得指定数量的聚类。
- en: In this example, we will look at single-link and complete-link clustering with
    edit distance as the distance metric. We will try to cluster city names by `EditDistance`,
    where the maximum distance is 4.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将研究使用`EditDistance`作为距离度量的单链和完全链接聚类。我们将尝试通过`EditDistance`对城市名称进行聚类，最大距离为4。
- en: How to do it…
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'In your IDE, run the `HierarchicalClustering`class, or in the command line,
    type the following:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的IDE中运行`HierarchicalClustering`类，或者在命令行中输入以下内容：
- en: '[PRE44]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is various clustering approaches to the same underlying set of `Strings`.
    In this recipe, we will intersperse the source and output. First, we will create
    our set of strings:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出是对同一基础集合`Strings`的各种聚类方法。在这个示例中，我们将交替展示源和输出。首先，我们将创建我们的字符串集合：
- en: '[PRE45]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, we will set up a single-link instance with `EditDistance` and create
    the dendrogram for the preceding set and print it out:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将设置一个使用`EditDistance`的单链实例，并为前面的集合创建树状图并打印出来：
- en: '[PRE46]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output will be as follows:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE47]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Next up, we will create and print out the complete-link treatment of the same
    set:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建并打印出相同集合的完全链接处理结果：
- en: '[PRE48]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This will produce the same dendrogram, but with different scores:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将产生相同的树状图，但具有不同的分数：
- en: '[PRE49]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next, we will produce the clusters where the number of clusters is being controlled
    for the single-link case:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将生成控制单链情况聚类数量的聚类：
- en: '[PRE50]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This will produce the following—it will be the same for the complete link,
    given the input set:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将产生如下结果——对于完全链接来说，给定输入集合时，它们将是相同的：
- en: '[PRE51]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The following code snippet is the complete-link clustering without a max distance:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码片段是没有最大距离的完全链接聚类：
- en: '[PRE52]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output will be:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出将是：
- en: '[PRE53]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Next, we will control the max distance:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将控制最大距离：
- en: '[PRE54]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The following is the effects of clustering limited by maximum distance for
    the complete-link case. Note that the single-link input here will have all elements
    in the same cluster at 3:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是通过最大距离限制的聚类效果，适用于完全链接的情况。请注意，这里的单链输入将在3的距离下将所有元素放在同一聚类中：
- en: '[PRE55]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: That's it! We have exercised a good portion of LingPipe's clustering API.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就是这样！我们已经演练了LingPipe聚类API的很大一部分。
- en: There's more…
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: Clustering is very sensitive to `Distance` that is used for the comparison of
    clusters. Consult the Javadoc for the 10 implementing classes for possible variations.
    `TfIdfDistance` can come in very handy to cluster language data.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类对用于比较聚类的`Distance`非常敏感。查阅Javadoc以获取10个实现类的可能变种。`TfIdfDistance`在聚类语言数据时非常有用。
- en: 'K-means (++) clustering is a feature-extractor-based clustering. This is what
    Javadoc says about it:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: K-means（++）聚类是一种基于特征提取的聚类方法。Javadoc是这样描述它的：
- en: '*K-means clustering* *may be viewed as an iterative approach to the minimization
    of the average square distance between items and their cluster centers…*'
  id: totrans-255
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*K-means 聚类* *可以视为一种迭代方法，旨在最小化项目与其聚类中心之间的平均平方距离……*'
- en: See also…
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见…
- en: For a detailed tutorial including details on evaluations, go over to [http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html)
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要查看详细的教程，包括评估的具体细节，请访问[http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html)
- en: Latent Dirichlet allocation (LDA) for multitopic clustering
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配 (LDA) 用于多主题聚类
- en: '**Latent Dirichlet allocation** (**LDA**) is a statistical technique to document
    clustering based on the tokens or words that are present in the document. Clustering
    such as classification generally assumes that categories are mutually exclusive.
    The neat thing about LDA is that it allows for documents to be in multiple topics
    at the same time, instead of just one category. This better reflects the fact
    that a tweet can be about *Disney* and *Wally World*, among other topics.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配** (**LDA**) 是一种基于文档中存在的标记或单词的文档聚类统计技术。像分类这样的聚类通常假设类别是互斥的。LDA的一个特点是，它允许文档同时属于多个主题，而不仅仅是一个类别。这更好地反映了一个推文可以涉及*迪士尼*和*沃利世界*等多个主题的事实。'
- en: The other neat thing about LDA, like many clustering techniques, is that it
    is unsupervised, which means that no supervised training data is required! The
    closest thing to training data is that the number of topics must be specified
    before hand.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: LDA的另一个有趣之处，就像许多聚类技术一样，是它是无监督的，这意味着不需要监督式训练数据！最接近训练数据的是必须提前指定主题的数量。
- en: LDA can be a great way to explore a dataset where you don't know what you don't
    know. It can also be difficult to tune, but generally, it does something interesting.
    Let's get a system working.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: LDA可以是探索你不知道的未知数据集的一个很好的方式。它也可能很难调整，但通常它会做出一些有趣的结果。让我们让系统运作起来。
- en: For each document, LDA assigns a probability of belonging to a topic based on
    the words in that document. We will start with documents that are converted to
    sequences of tokens. LDA uses the count of the tokens and does not care about
    the context or order in which those words appear. The model that LDA operates
    on for each document is called "a bag of words" to denote that the order is not
    important.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个文档，LDA根据该文档中的单词分配一个属于某个主题的概率。我们将从转换为标记序列的文档开始。LDA使用标记的计数，并不关心单词出现的上下文或顺序。LDA在每个文档上操作的模型被称为“词袋模型”，意味着顺序并不重要。
- en: The LDA model consists of a fixed number of topics, each of which is modeled
    as a distribution over words. A document under LDA is modeled as a distribution
    over topics. There is a Dirichlet prior on both the topic distributions over words
    and the document distributions over topics. Check out the Javadoc, referenced
    tutorial, and research literature if you want to know more about what is going
    on behind the scenes.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: LDA模型由固定数量的主题组成，每个主题都被建模为一个单词分布。LDA下的文档被建模为主题分布。对单词的主题分布和文档的主题分布都存在狄利克雷先验。如果你想了解更多幕后发生的事情，可以查看Javadoc、参考教程和研究文献。
- en: Getting ready
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will continue to work with the `.csv` data from tweets. Refer to [Chapter
    1](ch01.html "Chapter 1. Simple Classifiers"), *Simple Classifiers*, to know how
    to get tweets, or use the example data from the book. The recipe uses `data/gravity_tweets.csv`.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用来自推文的`.csv`数据。请参考[第一章](ch01.html "第一章. 简单分类器")，*简单分类器*，了解如何获取推文，或使用书中的示例数据。该配方使用`data/gravity_tweets.csv`。
- en: This recipe closely follows the tutorial at [http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html),
    which goes into much more detail than we do in the recipe. The LDA portion is
    at the end of the tutorial.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这个教程紧密跟随了[http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html)中的教程，该教程比我们在这个配方中所做的更为详细。LDA部分位于教程的最后。
- en: How to do it…
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到的…
- en: 'This section will be a source code review for `src/com/lingpipe/cookbook/chapter6/Lda.java`
    with some references to the `src/com/lingpipe/cookbook/chapter6/LdaReportingHandler.java`
    helping class that will get discussed as we use parts of it:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将对`src/com/lingpipe/cookbook/chapter6/Lda.java`进行源代码审查，并参考`src/com/lingpipe/cookbook/chapter6/LdaReportingHandler.java`辅助类，在使用其部分内容时进行讨论：
- en: 'The top of the `main()` method gets data from a standard `csv reader`:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`main()`方法的顶部从标准的`csv reader`获取数据：'
- en: '[PRE56]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Next up is a pile of configuration that we will address line by line. The `minTokenCount`
    filters all tokens that are seen less than five times in the algorithm. As datasets
    get bigger, this number can get larger. For 1100 tweets, we are assuming that
    at least five mentions will help reduce the overall noisiness of Twitter data:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是一堆我们将逐行处理的配置。`minTokenCount` 会过滤掉在算法中出现次数少于五次的所有标记。随着数据集的增大，这个数字可能会增大。对于
    1100 条推文，我们假设至少五次提及有助于减少 Twitter 数据的噪声：
- en: '[PRE57]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The `numTopics` parameter is probably the most critical configuration value,
    because it informs the algorithm about how many topics to find. Changes to this
    number can produce very different topics. You can experiment with it. By choosing
    10, we are saying that the 1100 tweets talk about 10 things overall. This is clearly
    wrong; maybe, 100 is closer to the mark. It is possible that the 1100 tweets had
    more than 1100 topics, since a tweet can be in more than one topic. Play around
    with it:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`numTopics` 参数可能是最关键的配置值，因为它告诉算法要找多少个主题。更改这个数字会产生非常不同的主题。你可以尝试调整它。选择 10 表示这
    1100 条推文大致涉及 10 个主题。但这显然是错误的，也许 100 会更接近实际情况。也有可能这 1100 条推文有超过 1100 个主题，因为一条推文可以出现在多个主题中。可以多尝试一下：'
- en: '[PRE58]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'According to the Javadoc, a rule of thumb for `documentTopicPrior` is to set
    it to 5 divided by the number of topics (or less if there are very few topics;
    0.1 is typically the maximum value used):'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据 Javadoc，`documentTopicPrior` 的经验法则是将其设置为 5 除以主题数量（如果主题非常少，则可以设置更小的值；0.1 通常是使用的最大值）：
- en: '[PRE59]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'A generally useful value for the `topicWordPrior` is as follows:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`topicWordPrior` 的一个通用实用值如下：'
- en: '[PRE60]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The `burninEpochs` parameter sets how many epochs to run before sampling. Setting
    this to greater than 0 has desirable properties, in that, it avoids correlation
    in the samples. The `sampleLag` controls how often the sample is taken after burning
    is complete, and `numSamples` controls how many samples to take. Currently, 2000
    samples will be taken. If `burninEpochs` were 1000, then 3000 samples would be
    taken with a sample lag of 1 (every time). If `sampleLag` was 2, then there would
    be 5000 iterations (1000 burnin, 2000 samples taken every 2 epochs for a total
    of 4000 epochs). Consult the Javadoc and tutorial for more about what is going
    on here:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`burninEpochs` 参数设置在采样之前运行多少个周期。将其设置为大于 0 会产生一些理想的效果，避免样本之间的相关性。`sampleLag`
    控制在烧入阶段完成后，采样的频率，`numSamples` 控制采样的数量。目前将进行 2000 次采样。如果 `burninEpochs` 为 1000，那么将会进行
    3000 次采样，样本间隔为 1（每次都采样）。如果 `sampleLag` 为 2，那么将会有 5000 次迭代（1000 次烧入，2000 次每 2 个周期采样，总共
    4000 个周期）。更多细节请参见 Javadoc 和教程：'
- en: '[PRE61]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Finally, `randomSeed` initializes the random process in `GibbsSampler`:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，`randomSeed` 初始化了 `GibbsSampler` 中的随机过程：
- en: '[PRE62]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '`SymbolTable` is constructed; this will store the mapping from strings to integers
    for efficient processing:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`SymbolTable` 被构造，它将存储字符串到整数的映射，以便进行高效处理：'
- en: '[PRE63]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'A tokenizer is next with our standard one:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是我们的标准分词器：
- en: '[PRE64]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Next, the configuration of LDA is printed out:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，打印 LDA 的配置：
- en: '[PRE65]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Then, we will create a matrix of documents and tokens that will be input to
    LDA and a report on how many tokens are present:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将创建一个文档和标记的矩阵，这些矩阵将作为输入传递给 LDA，并报告有多少标记：
- en: '[PRE66]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'A sanity check will follow by reporting a total token count:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 紧接着进行一个合理性检查，报告总的标记数量：
- en: '[PRE67]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'In order to get progress reports on the epochs/samples, a handler is created
    to deliver the desired news. It takes `symbolTable` as an argument to be able
    to recreate the tokens in reporting:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了获取有关周期/样本的进度报告，创建了一个处理程序来传递所需的消息。它将 `symbolTable` 作为参数，以便能够在报告中重新创建标记：
- en: '[PRE68]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The method that the search accesses in `LdaReportingHandler` follows:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索在 `LdaReportingHandler` 中访问的方法如下：
- en: '[PRE69]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'After all this setup, we will get to run LDA:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在完成所有设置之后，我们将开始运行 LDA：
- en: '[PRE70]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Wait, there''s more! However, we are almost done. We just need a final report:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等一下，还有更多内容！不过，我们快完成了。只需要一个最终报告：
- en: '[PRE71]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Finally, we will get to run this code. Type the following command:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将开始运行这段代码。输入以下命令：
- en: '[PRE72]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Have a look at a sample of the resulting output that confirms the configuration
    and the early reports from the search epochs:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看一下结果输出的样本，确认配置和搜索周期的早期报告：
- en: '[PRE73]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'After the epochs are done, we will get a report on the topics found. The first
    topic starts with a listing of words ordered by count. Note that the topic does
    not have a title. The topic `meaning` can be gleaned by scanning the words that
    have high counts and a high `Z` score. In this case, there is a word `movie` with
    a Z score of 4.0, `a` gets 6.0, and looking down the list, we see `good` with
    a score of 5.6\. The Z score reflects how nonindependent the word is from the
    topic with a higher score; this means that the word is more tightly associated
    with the topic. Look at the source for `LdaReportingHandler` to get the exact
    definition:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成时，我们将获得一个关于发现的主题的报告。第一个主题从按计数排序的单词列表开始。请注意，该主题没有标题。可以通过扫描具有高计数和高Z分数的单词来获取`meaning`主题。在这种情况下，有一个Z分数为4.0的单词`movie`，`a`得到了6.0，向下查看列表，我们看到`good`的得分为5.6。Z分数反映了该单词与具有较高分数的主题的非独立性，这意味着该单词与主题的关联更紧密。查看`LdaReportingHandler`的源代码以获取确切的定义。
- en: '[PRE74]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The preceding output is pretty awful, and the other topics don''t look any
    better. The next topic shows no more potential, but some obvious problems are
    arising because of tokenization:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前述输出相当糟糕，而其他主题看起来也不怎么样。下一个主题显示出了潜力，但由于标记化而出现了一些明显的问题：
- en: '[PRE75]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Donning our system tuner's hats, we will adjust the tokenizer to be the `new
    RegExTokenizerFactory("[^\\s]+")` tokenizer, which really cleans up the clusters,
    increases clusters to 25, and applies `Util.filterJaccard(tweets, tokFactory,
    .5)` to remove duplicates (1100 to 301). These steps were not performed one at
    a time, but this is a recipe, so we present the results of some experimentation.
    There was no evaluation harness, so this was a process that was made up of making
    a change, seeing if the output looks better and so on. Clusters are notoriously
    difficult to evaluate and tune on such an open-ended problem. The output looks
    a bit better.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 戴上我们系统调谐者的帽子，我们将调整分词器为`new RegExTokenizerFactory("[^\\s]+")`分词器，这真正清理了聚类，将聚类增加到25个，并应用`Util.filterJaccard(tweets,
    tokFactory, .5)`来去除重复项（从1100到301）。这些步骤并非一次执行，但这是一个配方，因此我们展示了一些实验结果。由于没有评估测试集，所以这是一个逐步调整的过程，看看输出是否更好等等。聚类在这样一个开放性问题上评估和调整是非常困难的。输出看起来好了一些。
- en: 'On scanning the topics, we get to know that there are still lots of low-value
    words that crap up the topics, but `Topic 18` looks somewhat promising, with a
    high Z score for `best` and `ever`:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在浏览主题时，我们发现仍然有许多低价值的词汇扰乱了主题，但`Topic 18`看起来有些有希望，其中`best`和`ever`的Z分数很高：
- en: '[PRE76]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Looking further into the output, we will see some documents that score high
    for `Topic 18`:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进一步查看输出，我们会看到一些在`Topic 18`上得分很高的文档：
- en: '[PRE77]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Both seem reasonable for a `best movie ever` topic. However, be warned that
    the other topics/document assignments are fairly bad.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于`best movie ever`主题，这两者看起来都是合理的。然而，请注意其他主题/文档分配相当糟糕。
- en: We can't really claim victory over this dataset in all honesty, but we have
    laid out the mechanics of how LDA works and its configuration. LDA has not been
    a huge commercial success, but it has produced interesting concept-level implementations
    for National Institutes of Health and other customers. LDA is a tuner's paradise
    with many ways to play with the resulting clustering. Check out the tutorial and
    Javadoc, and send us your success stories.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 诚实地说，我们不能完全宣称在这个数据集上取得了胜利，但我们已经阐明了LDA的工作原理及其配置。LDA在商业上并不是巨大的成功，但它为国家卫生研究院和其他客户提供了有趣的概念级别实现。LDA是一个调谐者的天堂，有很多方法可以对生成的聚类进行调整。查看教程和Javadoc，并向我们发送您的成功案例。
