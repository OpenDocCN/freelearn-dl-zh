- en: Getting Started with Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '深度学习入门  '
- en: In this chapter, we will explain some basic concepts of **Machine Learning**
    (**ML**) and **Deep Learning (DL)** that will be used in all subsequent chapters.
    We will start with a brief introduction to ML. Then we will move on to DL, which
    is one of the emerging branches of ML.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '在本章中，我们将解释一些基础的**机器学习**（**ML**）和**深度学习（DL）**概念，这些概念将在后续的所有章节中使用。我们将从简要介绍机器学习开始。接下来，我们将讲解深度学习，它是机器学习的一个新兴分支。  '
- en: We will briefly discuss some of the most well-known and widely used neural network
    architectures. Next, we will look at various features of deep learning frameworks
    and libraries. Then we will see how to prepare a programming environment, before
    moving on to coding with some open source, deep learning libraries such as **DeepLearning4J
    (DL4J)**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将简要讨论一些最著名和广泛使用的神经网络架构。接下来，我们将了解深度学习框架和库的各种特性。然后，我们将学习如何准备编程环境，在此基础上使用一些开源深度学习库，如**DeepLearning4J
    (DL4J)**进行编程。  '
- en: 'Then we will solve a very famous ML problem: the Titanic survival prediction.
    For this, we will use an Apache Spark-based **Multilayer Perceptron** (**MLP**)
    classifier to solve this problem. Finally, we''ll see some frequently asked questions
    that will help us generalize our basic understanding of DL. Briefly, the following
    topics will be covered:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '然后我们将解决一个非常著名的机器学习问题：泰坦尼克号生存预测。为此，我们将使用基于 Apache Spark 的**多层感知器**（**MLP**）分类器来解决这个问题。最后，我们将看到一些常见问题解答，帮助我们将深度学习的基本理解推广到更广泛的应用。简而言之，以下主题将被覆盖：  '
- en: A soft introduction to ML
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '机器学习的简单介绍  '
- en: Artificial Neural Networks (ANNs)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）
- en: Deep neural network architectures
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '深度神经网络架构  '
- en: Deep learning frameworks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '深度学习框架  '
- en: Deep learning from disasters—Titanic survival prediction using MLP
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '从灾难中学习深度学习——使用 MLP 进行泰坦尼克号生存预测  '
- en: Frequently asked questions (FAQ)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '常见问题解答（FAQ）  '
- en: A soft introduction to ML
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '机器学习的简单介绍  '
- en: ML approaches are based on a set of statistical and mathematical algorithms
    in order to carry out tasks such as classification, regression analysis, concept
    learning, predictive modeling, clustering, and mining of useful patterns. Thus,
    with the use of ML, we aim at improving the learning experience such that it becomes
    automatic. Consequently, we may not need complete human interactions, or at least
    we can reduce the level of such interactions as much as possible.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '机器学习方法基于一组统计和数学算法，以执行诸如分类、回归分析、概念学习、预测建模、聚类和挖掘有用模式等任务。因此，通过使用机器学习，我们旨在改善学习体验，使其变得自动化。结果，我们可能不需要完全的人类互动，或者至少我们可以尽可能减少这种互动的程度。  '
- en: Working principles of ML algorithms
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '机器学习算法的工作原理  '
- en: 'We now refer to a famous definition of ML by Tom M. Mitchell (*Machine Learning,
    Tom Mitchell, McGraw Hill*), where he explained what learning really means from
    a computer science perspective:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '我们现在引用 Tom M. Mitchell 的经典机器学习定义（《机器学习，Tom Mitchell，McGraw Hill》），他从计算机科学的角度解释了学习真正意味着什么：  '
- en: '"A computer program is said to learn from experience E with respect to some
    class of tasks T and performance measure P, if its performance at tasks in T,
    as measured by P, improves with experience E."'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '“如果一个计算机程序在经验 E 的基础上，在某些任务类别 T 和性能度量 P 的衡量下，其在任务 T 上的表现通过经验 E 得到提升，那么我们就说该程序从经验中学习。”  '
- en: 'Based on this definition, we can conclude that a computer program or machine
    can do the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '根据这个定义，我们可以得出结论：计算机程序或机器可以执行以下任务：  '
- en: Learn from data and histories
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '从数据和历史中学习  '
- en: Improve with experience
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '通过经验提升  '
- en: Iteratively enhance a model that can be used to predict outcomes of questions
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '迭代优化一个可以用于预测问题结果的模型  '
- en: 'Since they are at the core of predictive analytics, almost every ML algorithm
    we use can be treated as an optimization problem. This is about finding parameters
    that minimize an objective function, for example, a weighted sum of two terms
    like a cost function and regularization. Typically, an objective function has
    two components:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '由于它们是预测分析的核心，几乎我们使用的每个机器学习算法都可以视为一个优化问题。这涉及到找到最小化目标函数的参数，例如，像成本函数和正则化这样的加权和。通常，一个目标函数有两个组成部分：  '
- en: A regularizer, which controls the complexity of the model
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个正则化器，用来控制模型的复杂性  '
- en: The loss, which measures the error of the model on the training data.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '损失，衡量模型在训练数据上的误差。  '
- en: On the other hand, the regularization parameter defines the trade-off between
    minimizing the training error and the model's complexity in an effort to avoid
    overfitting problems. Now, if both of these components are convex, then their
    sum is also convex; it is non-convex otherwise. More elaborately, when using an
    ML algorithm, the goal is to obtain the best hyperparameters of a function that
    return the minimum error when making predictions. Therefore, using a convex optimization
    technique, we can minimize the function until it converges towards the minimum
    error.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，正则化参数定义了在最小化训练误差与模型复杂度之间的权衡，以避免过拟合问题。如果这两个组件都是凸的，那么它们的和也是凸的；否则，它是非凸的。更详细地说，在使用机器学习（ML）算法时，目标是获取能够在预测时返回最小误差的函数的最佳超参数。因此，使用凸优化技术，我们可以最小化该函数，直到它收敛到最小误差。
- en: 'Given that a problem is convex, it is usually easier to analyze the asymptotic
    behavior of the algorithm, which shows how fast it converges as the model observes
    more and more training data. The challenge of ML is to allow training a model
    so that it can recognize complex patterns and make decisions not only in an automated
    way but also as intelligently as possible. The entire learning process requires
    input datasets that can be split (or are already provided) into three types, outlined
    as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于问题是凸的，通常更容易分析算法的渐进行为，这展示了当模型观察到越来越多的训练数据时，它的收敛速度如何。机器学习的挑战在于使模型训练能够识别复杂模式，并且不仅能以自动化的方式做出决策，还能尽可能地智能地做出决策。整个学习过程需要输入数据集，这些数据集可以被拆分（或已经提供）为三种类型，具体如下：
- en: '**A training set** is the knowledge base coming from historical or live data
    used to fit the parameters of the ML algorithm. During the training phase, the
    ML model utilizes the training set to find optimal weights of the network and
    reach the objective function by minimizing the training error. Here, the **back-prop
    rule** (or another more advanced optimizer with a proper updater; we''ll see this
    later on) is used to train the model, but all the hyperparameters are need to
    be set before the learning process starts**.**'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集**是来自历史或实时数据的知识库，用于拟合机器学习算法的参数。在训练阶段，机器学习模型利用训练集来找到网络的最佳权重，并通过最小化训练误差来达到目标函数。在这里，使用**反向传播规则**（或其他更高级的优化器与适当的更新器；稍后会讨论）来训练模型，但所有超参数必须在学习过程开始之前设置**。**'
- en: '**A validation set** is a set of examples used to tune the parameters of an
    ML model. It ensures that the model is trained well and generalizes towards avoiding
    overfitting. Some ML practitioners refer to it as a **development set** or **dev
    set** as well.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证集**是一组用于调整机器学习模型参数的示例。它确保模型经过良好的训练，并能很好地泛化，从而避免过拟合。一些机器学习实践者也将其称为**开发集**或**dev集**。'
- en: '**A test set** is used for evaluating the performance of the trained model
    on unseen data. This step is also referred to as **model inferencing**. After
    assessing the final model on the test set (that is, when we''re fully satisfied
    with the model''s performance), we do not have to tune the model any further but
    the trained model can be deployed in a production-ready environment.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集**用于评估训练模型在未见过的数据上的表现。此步骤也称为**模型推理**。在对测试集中的最终模型进行评估后（即，当我们对模型的表现完全满意时），我们不需要进一步调整模型，训练好的模型可以部署到生产环境中。'
- en: A common practice is splitting the input data (after necessary pre-processing
    and feature engineering) into 60% for training, 10% for validation, and 20% for
    testing, but it really depends on use cases. Also, sometimes we need to perform
    up-sampling or down-sampling on the data based on the availability and quality
    of the datasets.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的做法是将输入数据（在必要的预处理和特征工程后）拆分为60%的训练数据，10%的验证数据和20%的测试数据，但这实际上取决于具体使用案例。此外，有时我们需要根据数据集的可用性和质量对数据进行上采样或下采样。
- en: 'Moreover, the learning theory uses mathematical tools that derive from probability
    theory and information theory. Three learning paradigms will be briefly discussed:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，学习理论使用的是源自概率论和信息论的数学工具。将简要讨论三种学习范式：
- en: Supervised learning
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有监督学习
- en: Unsupervised learning
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Reinforcement learning
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: 'The following diagram summarizes the three types of learning, along with the
    problems they address:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表总结了三种学习类型及其所解决的问题：
- en: '![](img/417d3a88-3435-4383-a62a-f387d8a98dac.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/417d3a88-3435-4383-a62a-f387d8a98dac.png)'
- en: Types of learning and related problems
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 学习类型及相关问题
- en: Supervised learning
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: '**Supervised learning** is the simplest and most well-known automatic learning
    task. It is based on a number of pre-defined examples, in which the category to
    which each of the inputs should belong is already known. *Figure 2* shows a typical
    workflow of supervised learning.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督学习**是最简单且最著名的自动学习任务。它基于一组预定义的示例，其中每个输入所属的类别已经知道。*图2*展示了监督学习的典型工作流程。'
- en: 'An actor (for example, an ML practitioner, data scientist, data engineer, ML
    engineer, and so on) performs **Extraction Transformation Load** (**ETL**) and
    the necessary feature engineering (including feature extraction, selection, and
    so on) to get the appropriate data having features and labels. Then he does the
    following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一位参与者（例如，机器学习实践者、数据科学家、数据工程师、机器学习工程师等）执行**提取转换加载**（**ETL**）及必要的特征工程（包括特征提取、选择等），以获得具有特征和标签的适当数据。然后他执行以下操作：
- en: Splits the data into training, development, and test sets
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拆分为训练集、开发集和测试集
- en: Uses the training set to train an ML model
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练集训练机器学习模型
- en: The validation set is used to validate the training against the overfitting
    problem and regularization
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证集用于验证训练是否过拟合以及正则化
- en: He then evaluates the model's performance on the test set (that is unseen data)
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，他在测试集上评估模型的表现（即未见过的数据）
- en: If the performance is not satisfactory, he can perform additional tuning to
    get the best model based on hyperparameter optimization
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果性能不令人满意，他可以进行额外的调优，以通过超参数优化获得最佳模型
- en: Finally, he deploys the best model in a production-ready environment
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，他将最佳模型部署到生产环境中
- en: '![](img/784a5592-403f-409c-8cbf-5cb2eba4deab.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/784a5592-403f-409c-8cbf-5cb2eba4deab.png)'
- en: Supervised learning in action
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的实际应用
- en: In the overall life cycle, there might be many actors involved (for example,
    a data engineer, data scientist, or ML engineer) to perform each step independently
    or collaboratively.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个生命周期中，可能会有多个参与者参与（例如，数据工程师、数据科学家或机器学习工程师），他们独立或协作执行每个步骤。
- en: The supervised learning context includes **classification** and **regression**
    tasks; classification is used to predict which class a data point is part of (**discrete
    value**), while regression is used to predict **continuous values**. In other
    words, a classification task is used to predict the label of the class attribute,
    while a regression task is used to make a numeric prediction of the class attribute.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的任务包括**分类**和**回归**；分类用于预测数据点属于哪个类别（**离散值**），而回归用于预测**连续值**。换句话说，分类任务用于预测类属性的标签，而回归任务则用于对类属性进行数值预测。
- en: In the context of supervised learning, **unbalanced data** refers to classification
    problems where we have unequal instances for different classes. For example, if
    we have a classification task for only two classes, **balanced data** would mean
    50% pre-classified examples for each of the classes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习的背景下，**不平衡数据**指的是分类问题，其中不同类别的实例数量不平衡。例如，如果我们有一个仅针对两个类别的分类任务，**平衡数据**意味着每个类别都有50%的预先分类示例。
- en: If the input dataset is a little unbalanced (for example, 60% data points for
    one class and 40% for the other class), the learning process will require for
    the input dataset to be split randomly into three sets, with 50% for the training
    set, 20% for the validation set, and the remaining 30% for the testing set.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入数据集稍微不平衡（例如，某一类占60%，另一类占40%），学习过程将要求将输入数据集随机拆分为三个子集，其中50%用于训练集，20%用于验证集，剩余30%用于测试集。
- en: Unsupervised learning
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In **unsupervised learning**, an input set is supplied to the system during
    the training phase. In contrast with supervised learning, the input objects are
    not labeled with their class. For classification, we assumed that we are given
    a training dataset of correctly labeled data. Unfortunately, we do not always
    have that advantage when we collect data in the real world.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在**无监督学习**中，训练阶段将输入集提供给系统。与监督学习不同，输入对象没有被标记其类别。对于分类任务，我们假设给定一个正确标记的数据集。然而，在现实世界中收集数据时，我们不总是拥有这种优势。
- en: For example, let's say you have a large collection of totally legal, not pirated,
    MP3 files in a crowded and massive folder on your hard drive. In such a case,
    how could we possibly group songs together if we do not have direct access to
    their metadata? One possible approach could be to mix various ML techniques, but
    clustering is often the best solution.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你在硬盘的一个拥挤且庞大的文件夹中有一大堆完全合法的、没有盗版的MP3文件。在这种情况下，如果我们无法直接访问它们的元数据，我们如何可能将歌曲归类呢？一种可能的方法是混合各种机器学习技术，但聚类往往是最好的解决方案。
- en: 'Now, what if you can build a clustering predictive model that helps automatically
    group together similar songs and organize them into your favorite categories,
    such as *country*, *rap*, *rock*, and so on? In short, unsupervised learning algorithms
    are commonly used in clustering problems. The following diagram gives us an idea
    of a clustering technique applied to solve this kind of problem:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，假设你能构建一个聚类预测模型，帮助自动将相似的歌曲分组，并将它们组织成你最喜欢的类别，如*乡村*、*说唱*、*摇滚*等。简而言之，无监督学习算法通常用于聚类问题。下图给我们展示了应用聚类技术解决此类问题的思路：
- en: '![](img/9e2bb782-2a63-4dfd-a242-dc8b18df8dc7.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e2bb782-2a63-4dfd-a242-dc8b18df8dc7.png)'
- en: Clustering techniques – an example of unsupervised learning
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类技术 —— 一种无监督学习的例子
- en: Although the data points are not labeled, we can still do the necessary feature
    engineering and grouping of a set of objects in such a way that objects in the
    same group (called a **cluster**) are brought together. This is not easy for a
    human. Rather, a standard approach is to define a similarity measure between two
    objects and then look for any cluster of objects that are more similar to each
    other than they are to the objects in the other clusters. Once we've done the
    clustering of the data points (that is, MP3 files) and the validation is completed,
    we know the pattern of the data (that is, what type of MP3 files fall in which
    group).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据点没有标签，但我们仍然可以进行必要的特征工程和对象分组，将属于同一组的对象（称为**聚类**）聚集在一起。这对于人类来说并不容易。标准方法是定义两个对象之间的相似度度量，然后寻找任何比其他聚类中的对象更相似的对象群集。一旦我们完成了数据点（即MP3文件）的聚类并完成验证，我们就能知道数据的模式（即，哪些类型的MP3文件属于哪个组）。
- en: Reinforcement learning
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: '**Reinforcement learning** is an artificial intelligence approach that focuses
    on the learning of the system through its interactions with the environment. In
    reinforcement learning, the system''s parameters are adapted based on the feedback
    obtained from the environment, which in turn provides feedback on the decisions
    made by the system. The following diagram shows a person making decisions in order
    to arrive at their destination.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**是一种人工智能方法，专注于通过与环境的交互来学习系统。在强化学习中，系统的参数会根据从环境中获得的反馈进行调整，而环境又会对系统做出的决策提供反馈。下图展示了一个人在做决策，以便到达目的地。'
- en: 'Let''s take an example of the route you take from home to work. In this case,
    you take the same route to work every day. However, out of the blue, one day you
    get curious and decide to try a different route with a view to finding the shortest
    path. This dilemma of trying out new routes or sticking to the best-known route
    is an example of **exploration versus exploitation**:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以你从家到工作地点的路线为例。在这种情况下，你每天都走相同的路线。然而，某天你突然好奇，决定尝试另一条路线，目的是寻找最短的路径。这个尝试新路线与坚持走最熟悉路线之间的两难困境，就是**探索与利用**的一个例子：
- en: '![](img/ed24ebcf-4492-4302-9226-33c35d8f9972.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed24ebcf-4492-4302-9226-33c35d8f9972.png)'
- en: An agent always tries to reach the destination
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一个智能体始终尝试到达目的地
- en: We can take a look at one more example in terms of a system modeling a chess
    player. In order to improve its performance, the system utilizes the result of
    its previous moves; such a system is said to be a system learning with reinforcement.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看一个更多的例子，假设有一个系统模拟一个棋手。为了提高其表现，系统利用先前动作的结果；这样的系统被称为强化学习系统。
- en: Putting ML tasks altogether
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将机器学习任务整合在一起
- en: 'We have seen the basic working principles of ML algorithms. Then we have seen
    what the basic ML tasks are and how they formulate domain-specific problems. Now
    let''s take a look at how can we summarize ML tasks and some applications in the
    following diagram:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了机器学习算法的基本工作原理。接下来我们了解了基本的机器学习任务，以及它们如何形成特定领域的问题。现在让我们来看一下如何总结机器学习任务以及一些应用，如下图所示：
- en: '![](img/899ceaf3-c710-4675-ae99-33c76cd6ac2f.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/899ceaf3-c710-4675-ae99-33c76cd6ac2f.png)'
- en: ML tasks and some use cases from different application domains
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 来自不同应用领域的机器学习任务及一些应用案例
- en: However, the preceding figure lists only a few use cases and applications using
    different ML tasks. In practice, ML is used in numerous use cases and applications.
    We will try to cover a few of those throughout this book.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，前面的图表只列出了使用不同机器学习任务的一些应用案例。在实践中，机器学习在许多应用场景中都有广泛应用。我们将在本书中尽量涵盖其中的一些案例。
- en: Delving into deep learning
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解深度学习
- en: Simple ML methods that were used in normal-size data analysis are not effective
    anymore and should be substituted by more robust ML methods. Although classical
    ML techniques allow researchers to identify groups or clusters of related variables,
    the accuracy and effectiveness of these methods diminish with large and high-dimensional
    datasets.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以往在常规数据分析中使用的简单机器学习方法已经不再有效，应该被更强大的机器学习方法所替代。尽管传统的机器学习技术允许研究人员识别相关变量的组或聚类，但随着大规模和高维数据集的增加，这些方法的准确性和有效性逐渐降低。
- en: Here comes deep learning, which is one of the most important developments in
    artificial intelligence in the last few years. Deep learning is a branch of ML
    based on a set of algorithms that attempt to model high-level abstractions in
    data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里出现了深度学习，它是近年来人工智能领域最重要的进展之一。深度学习是机器学习的一个分支，基于一套算法，旨在尝试对数据中的高级抽象进行建模。
- en: How did DL take ML into next level?
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习是如何将机器学习提升到一个新水平的？
- en: 'In short, deep learning algorithms are mostly a set of ANNs that can make better
    representations of large-scale datasets, in order to build models that learn these
    representations very extensively. Nowadays it''s not limited to ANNs, but there
    have been really many theoretical advances and software and hardware improvements
    that were necessary for us to get to this day. In this regard, Ian Goodfellow
    et al. (Deep Learning, MIT Press, 2016) defined deep learning as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，深度学习算法大多是一些人工神经网络（ANN），它们能够更好地表示大规模数据集，从而建立能够深入学习这些表示的模型。如今，这不仅仅局限于人工神经网络，实际上，理论上的进展以及软件和硬件的改进都是我们能够走到今天这一步的必要条件。在这方面，Ian
    Goodfellow 等人（《深度学习》，MIT出版社，2016年）将深度学习定义如下：
- en: '"Deep learning is a particular kind of machine learning that achieves great
    power and flexibility by learning to represent the world as a nested hierarchy
    of concepts, with each concept defined in relation to simpler concepts, and more
    abstract representations computed in terms of less abstract ones."'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: “深度学习是一种特定类型的机器学习，通过学习将世界表示为一个嵌套的概念层级结构，在这个层级中，每个概念都是相对于更简单的概念来定义的，更抽象的表示是通过较不抽象的表示来计算的，从而实现了巨大的能力和灵活性。”
- en: 'Let''s take an example; suppose we want to develop a predictive analytics model,
    such as an animal recognizer, where our system has to resolve two problems:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子；假设我们想开发一个预测分析模型，比如一个动物识别器，在这种情况下，我们的系统需要解决两个问题：
- en: To classify whether an image represents a cat or a dog
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于判断一张图片是猫还是狗
- en: To cluster images of dogs and cats.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于将狗和猫的图片进行聚类。
- en: If we solve the first problem using a typical ML method, we must define the
    facial features (ears, eyes, whiskers, and so on) and write a method to identify
    which features (typically nonlinear) are more important when classifying a particular
    animal.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用典型的机器学习方法来解决第一个问题，我们必须定义面部特征（如耳朵、眼睛、胡须等），并编写一个方法来识别在分类特定动物时哪些特征（通常是非线性的）更为重要。
- en: However, at the same time, we cannot address the second problem because classical
    ML algorithms for clustering images (such as **k-means**) cannot handle nonlinear
    features. Deep learning algorithms will take these two problems one step further
    and the most important features will be extracted automatically after determining
    which features are the most important for classification or clustering.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与此同时，我们无法解决第二个问题，因为用于图像聚类的传统机器学习算法（例如**k-means**）无法处理非线性特征。深度学习算法将把这两个问题提升一个层次，最重要的特征将在确定哪些特征对分类或聚类最为重要后自动提取。
- en: 'In contrast, when using a classical ML algorithm, we would have to provide
    the features manually. In summary, the deep learning workflow would be as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，当使用传统的机器学习算法时，我们必须手动提供这些特征。总结来说，深度学习的工作流程如下：
- en: A deep learning algorithm would first identify the edges that are most relevant
    when clustering cats or dogs. It would then try to find various combinations of
    shapes and edges hierarchically. This step is called ETL.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习算法首先会识别在聚类猫或狗时最相关的边缘。接着，它会尝试以层级方式找到各种形状和边缘的组合。这个步骤叫做ETL（提取、转换、加载）。
- en: After several iterations, hierarchical identification of complex concepts and
    features is carried out. Then, based on the identified features, the DL algorithm
    automatically decides which of these features are most significant (statistically)
    to classify the animal. This step is feature extraction.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过多次迭代后，复杂概念和特征的层级识别被执行。然后，基于已识别的特征，深度学习算法自动决定哪些特征在统计上对分类动物最为重要。这个步骤叫做特征提取。
- en: Finally, it takes out the label column and performs unsupervised training using
    **AutoEncoders** (**AEs**) to extract the latent features to be redistributed
    to k-means for clustering.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，算法去掉标签列，使用**自编码器**（**AEs**）进行无监督训练，以提取潜在特征，再将这些特征分配给k-means进行聚类。
- en: Then the clustering assignment hardening loss (CAH loss) and reconstruction
    loss are jointly optimized towards optimal clustering assignment. Deep Embedding
    Clustering (see more at [https://arxiv.org/pdf/1511.06335.pdf](https://arxiv.org/pdf/1511.06335.pdf))
    is an example of such an approach. We will discuss deep learning-based clustering
    approaches in [Chapter 11](b458baf5-6590-4e69-84d4-8c02a898dbca.xhtml), *Discussion,
    Current Trends, and Outlook*.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，聚类分配硬化损失（CAH损失）和重建损失被联合优化，以实现最优的聚类分配。深度嵌入聚类（详见[https://arxiv.org/pdf/1511.06335.pdf](https://arxiv.org/pdf/1511.06335.pdf)）就是这种方法的一个例子。我们将在[第11章](b458baf5-6590-4e69-84d4-8c02a898dbca.xhtml)中讨论基于深度学习的聚类方法，*讨论、当前趋势与展望*。
- en: Up to this point, we have seen that deep learning systems are able to recognize
    what an image represents. A computer does not see an image as we see it because
    it only knows the position of each pixel and its color. Using deep learning techniques,
    the image is divided into various layers of analysis.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们看到深度学习系统能够识别图像代表的是什么。计算机看图像的方式与我们不同，因为它只知道每个像素的位置和颜色。通过深度学习技术，图像被分解成多个分析层次。
- en: 'At a lower level, the software analyzes, for example, a grid of a few pixels
    with the task of detecting a type of color or various nuances. If it finds something,
    it informs the next level, which at this point checks whether or not that given
    color belongs to a larger form, such as a line. The process continues to the upper
    levels until you understand what is shown in the image. The following diagram
    shows what we have discussed in the case of an image classification system:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在较低的层次上，软件分析例如一小块像素网格，任务是检测某种颜色或其不同的色调。如果它发现了什么，它会通知下一层，下一层会检查该颜色是否属于更大的形态，比如一条线。这个过程会一直持续到更高层次，直到你理解图像展示的内容。下图展示了我们在图像分类系统中讨论的内容：
- en: '![](img/9ee85d48-6b19-4c74-bce0-196adfd06480.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ee85d48-6b19-4c74-bce0-196adfd06480.png)'
- en: A deep learning system at work on a dog versus cat classification problem
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理狗与猫分类问题时的深度学习系统工作原理
- en: 'More precisely, the preceding image classifier can be built layer by layer,
    as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，前述的图像分类器可以逐层构建，如下所示：
- en: '**Layer 1**: The algorithm starts identifying the dark and light pixels from
    the raw images'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第一层**：算法开始识别原始图像中的暗像素和亮像素。'
- en: '**Layer 2**: The algorithm then identifies edges and shapes'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第二层**：算法接着识别边缘和形状。'
- en: '**Layer 3**: It then learns more complex shapes and objects'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第三层**：接下来，算法识别更复杂的形状和物体。'
- en: '**Layer 4**: The algorithm then learns which objects define a human face'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第四层**：算法接着学习哪些物体定义了人脸。'
- en: Although this is a very simple classifier, software capable of doing these types
    of things is now widespread and is found in systems for recognizing faces, or
    in those for searching by an image on Google, for example. These pieces of software
    are based on deep learning algorithms.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这只是一个非常简单的分类器，但能够进行这些操作的软件如今已经非常普及，广泛应用于面部识别系统，或例如在Google中通过图像进行搜索的系统。这些软件是基于深度学习算法的。
- en: On the contrary, by using a linear ML algorithm, we cannot build such applications
    since these algorithms are incapable of handling nonlinear image features. Also,
    using ML approaches, we typically handle a few hyperparameters only. However,
    when neural networks are brought to the party, things become too complex. In each
    layer, there are millions or even billions of hyperparameters to tune, so much
    that the cost function becomes non-convex.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，通过使用线性机器学习（ML）算法，我们无法构建这样的应用程序，因为这些算法无法处理非线性的图像特征。而且，使用机器学习方法时，我们通常只处理少数几个超参数。然而，当神经网络进入这个领域时，事情变得复杂了。在每一层中，都有数百万甚至数十亿个超参数需要调整，以至于成本函数变得非凸。
- en: Another reason is that activation functions used in hidden layers are nonlinear,
    so the cost is non-convex. We will discuss this phenomenon in more detail in later
    chapters but let's take a quick look at ANNs.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个原因是隐藏层中使用的激活函数是非线性的，因此成本是非凸的。我们将在后续章节中更详细地讨论这一现象，但先快速了解一下人工神经网络（ANNs）。
- en: Artificial Neural Networks
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）
- en: ANNs work on the concept of deep learning. They represent the human nervous
    system in how the nervous system consists of a number of neurons that communicate
    with each other using axons.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）基于深度学习的概念。它们通过多个神经元之间的相互通信，代表了人类神经系统的工作方式，这些神经元通过轴突相互联系。
- en: Biological neurons
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生物神经元
- en: The working principles of ANNs are inspired by how a human brain works, depicted
    in *Figure 7*. The receptors receive the stimuli either internally or from the
    external world; then they pass the information into the biological *neurons* for
    further processing. There are a number of dendrites, in addition to another long
    extension called the **axon**.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）的工作原理受到人脑工作的启发，如*图7*所示。受体接收来自内部或外部世界的刺激；然后将信息传递给生物*神经元*进行进一步处理。除了另一个长的延伸部分称为**轴突**外，还有许多树突。
- en: 'Towards its extremity, there are minuscule structures called **synaptic terminals,**
    used to connect one neuron to the dendrites of other neurons. Biological neurons
    receive short electrical impulses called **signals** from other neurons, and in
    response, they trigger their own signals:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在其末端，有一些微小的结构称为**突触末端**，用于将一个神经元连接到其他神经元的树突。生物神经元接收来自其他神经元的短电流冲动，称为**信号**，并以此触发自己的信号：
- en: '![](img/3974bdf0-c8d9-4568-a829-fec048f9598c.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3974bdf0-c8d9-4568-a829-fec048f9598c.png)'
- en: Working principle of biological neurons
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元的工作原理
- en: We can thus summarize that the neuron comprises a cell body (also known as the
    soma), one or more **dendrites** for receiving signals from other neurons, and
    an **axon** for carrying out the signals generated by the neurons.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以总结出，神经元由一个细胞体（也称为胞体）、一个或多个**树突**用于接收来自其他神经元的信号，以及一个**轴突**用于传递神经元产生的信号。
- en: A neuron is in an active state when it is sending signals to other neurons.
    However, when it is receiving signals from other neurons, it is in an inactive
    state. In an idle state, a neuron accumulates all the signals received before
    reaching a certain activation threshold. This whole thing motivated researchers
    to introduce an ANN.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经元向其他神经元发送信号时，它处于激活状态。然而，当它接收来自其他神经元的信号时，它处于非激活状态。在空闲状态下，神经元会积累所有接收到的信号，直到达到一定的激活阈值。这一现象促使研究人员提出了人工神经网络（ANN）。
- en: A brief history of ANNs
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）的简史
- en: Inspired by the working principles of biological neurons, Warren McCulloch and
    Walter Pitts proposed the first artificial neuron model in 1943 in terms of a
    computational model of nervous activity. This simple model of a biological neuron,
    also known as an **artificial neuron (AN),** has one or more binary (on/off) inputs
    and one output only.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 受到生物神经元工作原理的启发，沃伦·麦卡洛克和沃尔特·皮茨于1943年提出了第一个人工神经元模型，作为神经活动的计算模型。这个简单的生物神经元模型，也称为**人工神经元（AN）**，有一个或多个二进制（开/关）输入，只有一个输出。
- en: 'An AN simply activates its output when more than a certain number of its inputs
    are active. For example, here we see a few ANNs that perform various logical operations.
    In this example, we assume that a neuron is activated only when at least two of
    its inputs are active:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一个人工神经元只有在它的输入中有超过一定数量的输入处于活动状态时，才会激活它的输出。例如，在这里我们看到几个执行各种逻辑运算的人工神经网络（ANNs）。在这个例子中，我们假设一个神经元只有在至少有两个输入处于活动状态时才会被激活：
- en: '![](img/6e2b1c75-5e3e-4236-8a64-38389a64f4cf.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e2b1c75-5e3e-4236-8a64-38389a64f4cf.png)'
- en: ANNs performing simple logical computations
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 执行简单逻辑计算的人工神经网络（ANNs）
- en: The example sounds too trivial, but even with such a simplified model, it is
    possible to build a network of ANs. Nevertheless, these networks can be combined
    to compute complex logical expressions too. This simplified model inspired John
    von Neumann, Marvin Minsky, Frank Rosenblatt, and many others to come up with
    another model called a **perceptron** back in 1957.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子听起来过于简单，但即使是如此简化的模型，也能构建一个人工神经网络。然而，这些网络也可以组合在一起计算复杂的逻辑表达式。这个简化模型启发了约翰·冯·诺依曼、马文·明斯基、弗兰克·罗森布拉特等人在1957年提出了另一个模型——**感知器**。
- en: 'The perceptron is one of the simplest ANN architectures we''ve seen in the
    last 60 years. It is based on a slightly different AN called a **Linear Threshold
    Unit** (**LTU**). The only difference is that the inputs and outputs are now numbers
    instead of binary on/off values. Each input connection is associated with a weight.
    The LTU computes a weighted sum of its inputs, then applies a step function (which
    resembles the action of an activation function) to that sum, and outputs the result:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器是过去60年里我们见过的最简单的人工神经网络架构之一。它基于一种略有不同的人工神经元——**线性阈值单元**（**LTU**）。唯一的区别是输入和输出现在是数字，而不是二进制的开/关值。每个输入连接都与一个权重相关联。LTU计算其输入的加权和，然后对该和应用一个阶跃函数（类似于激活函数的作用），并输出结果：
- en: '![](img/75a216da-3354-4b5f-bd0a-023f44330222.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75a216da-3354-4b5f-bd0a-023f44330222.png)'
- en: The left-side figure represents an LTU and the right-side figure shows a perceptron
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 左图表示一个线性阈值单元（LTU），右图显示一个感知器
- en: One of the downsides of a perceptron is that its decision boundary is linear.
    Therefore, they are incapable of learning complex patterns. They are also incapable
    of solving some simple problems like **Exclusive OR** (**XOR**). However, later
    on, the limitations of perceptrons were somewhat eliminated by stacking multiple
    perceptrons, called MLP.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器的一个缺点是其决策边界是线性的。因此，它们无法学习复杂的模式。它们也无法解决一些简单的问题，比如**异或**（**XOR**）。然而，后来通过堆叠多个感知器（称为MLP），感知器的局限性在某种程度上得到了消除。
- en: How does an ANN learn?
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络是如何学习的？
- en: 'Based on the concept of biological neurons, the term and the idea of ANs arose.
    Similarly to biological neurons, the artificial neuron consists of the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 基于生物神经元的概念，人工神经元的术语和思想应运而生。与生物神经元类似，人工神经元由以下部分组成：
- en: One or more incoming connections that aggregate signals from neurons
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个或多个输入连接，用于从神经元聚合信号
- en: One or more output connections for carrying the signal to the other neurons
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个或多个输出连接，用于将信号传递到其他神经元
- en: An **activation function**, which determines the numerical value of the output
    signal
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**激活函数**，用于确定输出信号的数值
- en: The learning process of a neural network is configured as an *iterative process*
    of *optimization* of the *weights* (see more in the next section). The weights
    are updated in each epoch. Once the training starts, the aim is to generate predictions
    by minimizing the loss function. The performance of the network is then evaluated
    on the test set.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的学习过程被配置为*迭代过程*，即对*权重*的*优化*（更多内容见下一节）。在每一轮训练中，权重都会被更新。一旦训练开始，目标是通过最小化损失函数来生成预测。然后，网络的性能将在测试集上进行评估。
- en: Now we know the simple concept of an artificial neuron. However, generating
    only some artificial signals is not enough to learn a complex task. Albeit, a
    commonly used supervised learning algorithm is the backpropagation algorithm,
    which is very commonly used to train a complex ANN.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了人工神经元的简单概念。然而，单单生成一些人工信号并不足以学习复杂的任务。尽管如此，一个常用的监督学习算法是反向传播算法，它被广泛用于训练复杂的人工神经网络。
- en: ANNs and the backpropagation algorithm
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络和反向传播算法
- en: The backpropagation algorithm aims to minimize the error between the current
    and the desired output. Since the network is feedforward, the activation flow
    always proceeds forward from the input units to the output units.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播算法旨在最小化当前输出与期望输出之间的误差。由于网络是前馈型的，激活流总是从输入单元向输出单元前进。
- en: 'The gradient of the cost function is backpropagated and the network weights
    get updated; the overall method can be applied to any number of hidden layers
    recursively. In such a method, the incorporation between two phases is important.
    In short, the basic steps of the training procedure are as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数的梯度会反向传播，网络权重会被更新；该方法可以递归地应用于任何数量的隐藏层。在这种方法中，两种阶段之间的结合是非常重要的。简而言之，训练过程的基本步骤如下：
- en: Initialize the network with some random (or more advanced XAVIER) weights
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用一些随机（或更先进的XAVIER）权重初始化网络
- en: For all training cases, follow the steps of forward and backward passes as outlined
    next
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于所有训练样本，按照接下来的步骤执行前向和后向传播
- en: Forward and backward passes
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向和后向传播
- en: In the forward pass, a number of operations are performed to obtain some predictions
    or scores. In such an operation, a graph is created, connecting all dependent
    operations in a top-to-bottom fashion. Then the network's error is computed, which
    is the difference between the predicted output and the actual output.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播中，会执行一系列操作来获得一些预测或评分。在这个操作中，创建一个图，将所有依赖操作按从上到下的方式连接起来。然后计算网络的误差，即预测输出与实际输出之间的差异。
- en: On the other hand, the backward pass is involved mainly with mathematical operations,
    such as creating derivatives for all differential operations (that is auto-differentiation
    methods), top to bottom (for example, measuring the loss function to update the
    network weights), for all the operations in the graph, and then using them in
    chain rule.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，反向传播主要涉及数学运算，比如为所有微分操作（即自动微分方法）创建导数，从上到下（例如，测量损失函数以更新网络权重），对图中的所有操作进行处理，然后应用链式法则。
- en: 'In this pass, for all layers starting with the output layer back to the input
    layer, it shows the network layer''s output with the correct input (error function).
    Then it adapts the weights in the current layer to minimize the error function.
    This is backpropagation''s optimization step. By the way, there are two types
    of auto-differentiation methods:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，对于所有层，从输出层到输入层，展示了网络层的输出与正确的输入（误差函数）。然后，调整当前层的权重以最小化误差函数。这是反向传播的优化步骤。顺便说一下，自动微分方法有两种类型：
- en: '**Reverse mode**: Derivation of a single output with respect to all inputs'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**反向模式**：关于所有输入的单个输出的导数'
- en: '**Forward mode**: Derivation of all outputs with respect to one input'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**前向模式**：关于一个输入的所有输出的导数'
- en: The backpropagation algorithm processes the information in such a way that the
    network decreases the global error during the learning iterations; however, this
    does not guarantee that the global minimum is reached. The presence of hidden
    units and the nonlinearity of the output function mean that the behavior of the
    error is very complex and has many local minimas.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播算法以这样的方式处理信息，使得网络在学习迭代过程中减少全局误差；然而，这并不保证能够到达全局最小值。隐藏单元的存在和输出函数的非线性意味着误差的行为非常复杂，具有许多局部最小值。
- en: This backpropagation step is typically performed thousands or millions of times,
    using many training batches, until the model parameters converge to values that
    minimize the cost function. The training process ends when the error on the validation
    set begins to increase, because this could mark the beginning of a phase overfitting.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这个反向传播步骤通常会执行成千上万次，使用许多训练批次，直到模型参数收敛到能最小化代价函数的值。当验证集上的误差开始增大时，训练过程结束，因为这可能标志着过拟合阶段的开始。
- en: Weights and biases
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重和偏置
- en: Besides the state of a neuron, synaptic weight is considered, which influences
    the connection within the network. Each weight has a numerical value indicated
    by *W[ij]*, which is the synaptic weight connecting neuron *i* to neuron *j*.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 除了神经元的状态外，还考虑突触权重，它影响网络中的连接。每个权重都有一个数值，表示为 *W[ij]*，它是连接神经元 *i* 和神经元 *j* 的突触权重。
- en: '**Synaptic weight**: This concept evolved from biology and refers to the strength
    or amplitude of a connection between two nodes, corresponding in biology to the
    amount of influence the firing of one neuron has on another.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**突触权重**：这个概念源自生物学，指的是两个节点之间连接的强度或幅度，在生物学中对应于一个神经元的激活对另一个神经元的影响程度。'
- en: 'For each neuron (also known as, unit) *i*, an input vector can be defined by
    *x[i]*= (*x[1]*, *x[2]*,...*x[n]*) and a weight vector can be defined by *w[i]*=
    (*w[i1]*, *w[i2]*,...*w[in]*). Now, depending on the position of a neuron, the
    weights and the output function determine the behavior of an individual neuron.
    Then during forward propagation, each unit in the hidden layer gets the following
    signal:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个神经元（也叫单元） *i*，可以定义一个输入向量 *x[i]* = (*x[1]*, *x[2]*, ... *x[n]*)，并且可以定义一个权重向量
    *w[i]* = (*w[i1]*, *w[i2]*, ... *w[in]*)。现在，根据神经元的位置，权重和输出函数决定了单个神经元的行为。然后，在前向传播过程中，隐藏层中的每个单元都会接收到以下信号：
- en: '![](img/e468c385-a1c8-4d70-a6c5-6ecf83d1cdb4.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e468c385-a1c8-4d70-a6c5-6ecf83d1cdb4.png)'
- en: 'Nevertheless, among the weights, there is also a special type of weight called
    *bias* unit *b.* Technically, bias units aren''t connected to any previous layer,
    so they don''t have true activity. But still, the bias *b* value allows the neural
    network to shift the activation function to the left or right. Now, taking the
    bias unit into consideration, the modified network output can be formulated as
    follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在权重中，还有一种特殊类型的权重叫做*偏置*单元*b*。从技术上讲，偏置单元并不与任何前一层连接，因此它没有真正的活动。但偏置*b*的值仍然可以让神经网络将激活函数向左或向右平移。现在，考虑到偏置单元后，修改后的网络输出可以表示如下：
- en: '![](img/ebd48a2a-9c42-4031-8d49-48b9a900f105.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ebd48a2a-9c42-4031-8d49-48b9a900f105.png)'
- en: 'The preceding equation signifies that each hidden unit gets the sum of inputs
    multiplied by the corresponding weight—summing junction. Then the resultant in
    the summing junction is passed through the activation function, which squashes
    the output as depicted in the following figure:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程表示，每个隐藏单元都会得到输入的总和乘以相应的权重——求和节点。然后，求和节点的结果通过激活函数，激活函数会压缩输出，如下图所示：
- en: '![](img/64811b83-4195-4c85-97db-d7a5c3cda41d.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64811b83-4195-4c85-97db-d7a5c3cda41d.png)'
- en: Artificial neuron model
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经元模型
- en: 'Now, a tricky question: how do we initialize the weights? Well, if we initialize
    all weights to the same value (for example, 0 or 1), each hidden neuron will get
    exactly the same signal. Let''s try to break it down:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有一个棘手的问题：我们该如何初始化权重？如果我们将所有的权重初始化为相同的值（例如0或1），每个隐藏神经元都会得到完全相同的信号。让我们试着分析一下：
- en: If all weights are initialized to 1, then each unit gets a signal equal to the
    sum of the inputs
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果所有的权重都初始化为1，那么每个单元将收到等于输入总和的信号
- en: If all weights are 0, which is even worse, every neuron in a hidden layer will
    get zero signal
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果所有的权重都为0，那么情况就更糟了，隐藏层中的每个神经元都会收到零信号
- en: For network weight initialization, Xavier initialization is nowadays used widely.
    It is similar to random initialization but often turns out to work much better
    since it can automatically determine the scale of initialization based on the
    number of input and output neurons.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于网络权重初始化，目前广泛使用Xavier初始化方法。它类似于随机初始化，但通常效果更好，因为它可以根据输入和输出神经元的数量自动确定初始化的规模。
- en: 'Interested readers should refer to this publication for detailed info: Xavier
    Glorot and Yoshua Bengio, *Understanding the difficulty of training deep feedforward
    neural networks*: proceedings of the 13^(th) international conference on **Artificial
    Intelligence and Statistics** (**AISTATS**) 2010, Chia Laguna Resort, Sardinia,
    Italy; Volume 9 of JMLR: W&CP.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣的读者可以参考这篇出版物以获取详细信息：Xavier Glorot 和 Yoshua Bengio，*理解训练深度前馈神经网络的难度*：2010年第13届国际人工智能与统计学会议（**AISTATS**）论文集，地点：意大利撒丁岛Chia
    Laguna度假村；JMLR卷9：W&CP。
- en: You may be wondering whether you can get rid of random initialization while
    training a regular DNN (for example, MLP or DBN). Well, recently, some researchers
    have been talking about random orthogonal matrix initializations that perform
    better than just any random initialization for training DNNs.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，是否可以在训练普通的DNN（例如MLP或DBN）时摆脱随机初始化。最近，一些研究人员提到过随机正交矩阵初始化，这种方法比任何随机初始化更适合用于训练DNN。
- en: When it comes to initializing the biases, we can initialize them to be zero.
    But setting the biases to a small constant value such as 0.01 for all biases ensures
    that all **Rectified Linear Unit** (**ReLU**) units can propagate some gradient.
    However, it neither performs well nor shows consistent improvement. Therefore,
    sticking with zero is recommended.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化偏置时，我们可以将其初始化为零。但将所有偏置设置为一个小的常数值，例如0.01，可以确保所有**修正线性单元**（**ReLU**）单元能够传播一些梯度。然而，这种方法并不表现良好，也没有展现出一致的改善。因此，推荐将其保持为零。
- en: Weight optimization
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重优化
- en: Before the training starts, the network parameters are set randomly. Then to
    optimize the network weights, an iterative algorithm called **Gradient Descent**
    (**GD**) is used. Using GD optimization, our network computes the cost gradient
    based on the training set. Then, through an iterative process, the gradient *G*
    of the error function *E* is computed*.*
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练开始之前，网络的参数是随机设置的。然后，为了优化网络权重，使用一种叫做**梯度下降法**（**GD**）的迭代算法。通过GD优化，我们的网络根据训练集计算代价梯度。然后，通过迭代过程，计算误差函数*E*的梯度*G*。
- en: 'In following graph, gradient **G** of error function ***E*** provides the direction
    in which the error function with current values has the steeper slope. Since the
    ultimate target is to reduce the network error, GD makes small steps in the opposite
    direction *-***G**. This iterative process is executed a number of times, so the
    error *E* would move down towards the global minima*.* This way, the ultimate
    target is to reach a point where **G = 0***,* where no further optimization is
    possible:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，误差函数***E***的梯度**G**提供了当前值的误差函数最陡坡度的方向。由于最终目标是减少网络误差，梯度下降法沿着相反方向*-***G**前进，采取小步走。这一迭代过程执行多次，使得误差*E*逐渐下降，向全局最小值*移动。这样，最终目标是达到**G
    = 0**的点，表示无法再进行优化：
- en: '![](img/bd401445-a7a4-4b48-87da-94f13c8d52d1.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd401445-a7a4-4b48-87da-94f13c8d52d1.png)'
- en: Searching for the minimum for the error function E; we move in the direction
    in which the gradient G of E is minimal
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索误差函数E的最小值时，我们朝着误差函数E的梯度G最小的方向移动
- en: The downside is that it takes too long to converge, which makes it impossible
    to meet the demand of handling large-scale training data. Therefore, a faster
    GD called **Stochastic Gradient Descent** (**SDG**) is proposed, which is also
    a widely used optimizer in DNN training. In SGD, we use only one training sample
    per iteration from the training set to update the network parameters.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点是收敛速度太慢，导致无法满足处理大规模训练数据的需求。因此，提出了一种更快的梯度下降法，称为**随机梯度下降法**（**SGD**），这也是深度神经网络（DNN）训练中广泛使用的优化器。在SGD中，每次迭代我们只使用来自训练集的一个训练样本来更新网络参数。
- en: I'm not saying SGD is the only available optimization algorithm, but there are
    so many advanced optimizers available nowadays, for example, Adam, RMSProp, ADAGrad,
    Momentum, and so on. More or less, most of them are either direct or indirect
    optimized versions of SGD.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我并不是说SGD是唯一可用的优化算法，但现在有很多先进的优化器可供选择，例如Adam、RMSProp、ADAGrad、Momentum等。或多或少，它们大多数都是SGD的直接或间接优化版本。
- en: By the way, the term **stochastic** comes from the fact that the gradient based
    on a single training sample per iteration is a stochastic approximation of the
    true cost gradient.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，术语**随机**源于基于每次迭代中单个训练样本的梯度是对真实代价梯度的随机近似。
- en: Activation functions
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'To allow a neural network to learn complex decision boundaries, we apply a
    non-linear activation function to some of its layers. Commonly used functions
    include Tanh, ReLU, softmax, and variants of these. More technically, each neuron
    receives as input signal the weighted sum of the synaptic weights and the activation
    values of the neurons connected. One of the most widely used functions for this
    purpose is the so-called **sigmoid function**. It is a special case of the logistic
    function, which is defined by the following formula:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让神经网络学习复杂的决策边界，我们在某些层上应用非线性激活函数。常用的函数包括Tanh、ReLU、softmax及其变种。从技术上讲，每个神经元接收的输入信号是与其连接的神经元的突触权重和激活值的加权和。为了实现这个目的，最广泛使用的函数之一是所谓的**Sigmoid函数**。它是逻辑函数的一个特例，定义如下公式：
- en: '![](img/60003fbe-ed08-424d-9478-582a826551bd.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60003fbe-ed08-424d-9478-582a826551bd.png)'
- en: The domain of this function includes all real numbers, and the co-domain is
    (*0, 1*). This means that any value obtained as an output from a neuron (as per
    the calculation of its activation state), will always be between zero and one.
    The sigmoid function, as represented in the following diagram, provides an interpretation
    of the saturation rate of a neuron, from not being active (*= 0*) to complete
    saturation, which occurs at a predetermined maximum value (*= 1*).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的定义域包括所有实数，值域为(*0, 1*)。这意味着神经元输出的任何值（根据其激活状态的计算）都将始终在零和一之间。如下图所示，Sigmoid函数提供了对神经元饱和度的解释，从不激活（*=
    0*）到完全饱和，发生在预定的最大值（*= 1*）时。
- en: 'On the other hand, a hyperbolic tangent, or **tanh**, is another form of the
    activation function. Tanh squashes a real-valued number to the range *[-1, 1]*.
    In particular, mathematically, tanh activation function can be expressed as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，双曲正切（**tanh**）是另一种激活函数。Tanh将实值数字压缩到范围*[-1, 1]*内。特别地，数学上，tanh激活函数可以表示为以下公式：
- en: '![](img/e72b887a-0d4b-4045-a62c-2db8cd4f3c4e.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e72b887a-0d4b-4045-a62c-2db8cd4f3c4e.png)'
- en: 'The preceding equation can be represented in the following figure:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 上述公式可以通过以下图形表示：
- en: '![](img/6a367bab-2c52-4abd-ad34-337accfb4b1a.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a367bab-2c52-4abd-ad34-337accfb4b1a.png)'
- en: Sigmoid versus tanh activation function
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 与 tanh 激活函数
- en: In general, in the last level of an **feedforward neural network** (**FFNN**),
    the softmax function is applied as the decision boundary. This is a common case,
    especially when solving a classification problem. In probability theory, the output
    of the softmax function is squashed as the probability distribution over *K* different
    possible outcomes. Nevertheless, the softmax function is used in various multiclass
    classification methods, such that the network's output is distributed across classes
    (that is, probability distribution over the classes) having a dynamic range between
    *-1* and *1* or *0* and *1*.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在**前馈神经网络（FFNN）**的最后一层，会应用 softmax 函数作为决策边界。这是一个常见的情况，尤其是在解决分类问题时。在概率论中，softmax
    函数的输出会被压缩为 *K* 种不同可能结果的概率分布。然而，softmax 函数也被用于多类别分类方法中，使得网络的输出在各类别之间分布（即，类别的概率分布），并且具有
    *-1* 到 *1* 或 *0* 到 *1* 之间的动态范围。
- en: For a regression problem, we do not need to use any activation function since
    the network generates continuous values—probabilities. However, I've seen people
    using the IDENTITY activation function for regression problems nowadays. We'll
    see this in later chapters.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，我们不需要使用任何激活函数，因为网络生成的是连续值——概率。然而，近年来我看到一些人开始在回归问题中使用**恒等（IDENTITY）**激活函数。我们将在后续章节中讨论这一点。
- en: To conclude, choosing proper activation functions and network weights initialization
    are two problems that make a network perform at its best and help to obtain good
    training. We'll discuss more in upcoming chapters; we will see where to use which
    activation function.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，选择合适的激活函数和网络权重初始化是两个决定网络性能的重要问题，能帮助获得良好的训练效果。我们将在后续章节中进一步讨论，探讨如何选择合适的激活函数。
- en: Neural network architectures
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络架构
- en: 'There are various types of architectures in neural networks. We can categorize
    DL architectures into four groups: **Deep Neural Networks** (**DNNs**), **Convolutional
    Neural Networks** (**CNNs**), **Recurrent Neural Networks** (**RNNs**), and **Emergent
    Architectures** (**EAs**).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络有多种架构类型。我们可以将深度学习架构分为四大类：**深度神经网络（DNN）**、**卷积神经网络（CNN）**、**递归神经网络（RNN）**和**涌现架构（EA）**。
- en: Nowadays, based on these architectures, researchers come up with so many variants
    of these for domain-specific use cases and research problems. The following sections
    of this chapter will give a brief introduction to these architectures. More detailed
    analysis, with examples of applications, will be the subject of later chapters
    of this book.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，基于这些架构，研究人员提出了许多针对特定领域的变种，以应对不同的研究问题。接下来的章节将简要介绍这些架构，更多的详细分析和应用实例将在本书的后续章节中讨论。
- en: Deep neural networks
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络
- en: DNNs are neural networks having complex and deeper architecture with a large
    number of neurons in each layer, and there are many connections. The computation
    in each layer transforms the representations in the subsequent layers into slightly
    more abstract representations. However, we will use the term DNN to refer specifically
    to the MLP, the **Stacked Auto-Encoder** (**SAE**), and **Deep Belief Networks**
    (**DBNs**).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: DNN（深度神经网络）是一种具有复杂和更深层次架构的神经网络，每层包含大量的神经元，并且连接众多。每一层的计算会将后续层的表示转化为更抽象的表示。然而，我们将使用“DNN”这一术语专指多层感知机（MLP）、**堆叠自编码器（SAE）**和**深度信念网络（DBN）**。
- en: 'SAEs and DBNs use AEs and **Restricted Boltzmann Machines** (**RBMs**) as building
    blocks of the architectures. The main difference between these and MLPs is that
    training is executed in two phases: unsupervised pre-training and supervised fine-tuning.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: SAE 和 DBN 使用 AE 和**受限玻尔兹曼机（RBM）**作为架构的构建模块。这些与 MLP 的主要区别在于，训练过程分为两个阶段：无监督预训练和有监督微调。
- en: '![](img/a0a9b905-3a15-43a7-8160-3a0c2a2f3d2a.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0a9b905-3a15-43a7-8160-3a0c2a2f3d2a.png)'
- en: SAE and DBN using AE and RBM respectively
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: SAE 和 DBN 分别使用 AE 和 RBM
- en: In unsupervised pre-training, shown in the preceding diagram, the layers are
    stacked sequentially and trained in a layer-wise manner, like an AE or RBM using
    unlabeled data. Afterwards, in supervised fine-tuning, an output classifier layer
    is stacked and the complete neural network is optimized by retraining with labeled
    data.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督预训练中，如前图所示，各层被按顺序堆叠，并以逐层的方式进行训练，类似于使用无标记数据的 AE 或 RBM。之后，在有监督的微调过程中，堆叠一个输出分类层，并通过使用标记数据重新训练整个神经网络来进行优化。
- en: Multilayer Perceptron
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知机
- en: 'As discussed earlier, a single perceptron is even incapable of approximating
    an XOR function. To overcome this limitation, multiple perceptrons are stacked
    together as MLPs, where layers are connected as a directed graph. This way, the
    signal propagates one way, from input layer to hidden layers to output layer,
    as shown in the following diagram:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，单一感知器甚至无法逼近XOR函数。为了克服这一限制，将多个感知器堆叠在一起形成MLP，其中各层作为有向图连接。通过这种方式，信号沿着一个方向传播，从输入层到隐藏层再到输出层，如下图所示：
- en: '![](img/25203e1f-35e2-409d-abb0-1059fbc94385.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/25203e1f-35e2-409d-abb0-1059fbc94385.png)'
- en: An MLP architecture having an input layer, two hidden layers, and an output
    layer
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有输入层、两个隐藏层和输出层的MLP架构
- en: 'Fundamentally, an MLP is one the most simple FFNNs having at least three layers:
    an input layer, a hidden layer, and an output layer. An MLP was first trained
    with a backpropogation algorithm in the 1980s.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，MLP是最简单的前馈神经网络（FFNN），至少有三层：输入层、隐藏层和输出层。MLP最早在1980年代使用反向传播算法进行训练。
- en: Deep belief networks
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度信念网络
- en: To overcome the overfitting problem in MLPs, the DBN was proposed by Hinton
    et al. It uses a greedy, layer-by-layer, pre-training algorithm to initialize
    the network weights through probabilistic generative models.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服MLP中的过拟合问题，Hinton等人提出了DBN。它使用贪心的逐层预训练算法，通过概率生成模型初始化网络权重。
- en: 'DBNs are composed of a visible layer and multiple layers—**hidden units**.
    The top two layers have undirected, symmetric connections in between and form
    an associative memory, whereas lower layers receive top-down, directed connections
    from the preceding layer. The building blocks of a DBN are RBMs, as you can see
    in the following figure, where several RBMs are *stacked* one after another to
    form DBNs:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: DBN由一个可见层和多个层—**隐藏单元**组成。顶部两层之间有无向对称连接，形成关联记忆，而较低层则从前一层接收自上而下的有向连接。DBN的构建模块是RBM，如下图所示，多个RBM一个接一个地*堆叠*在一起形成DBN：
- en: '![](img/67d0cd1e-9840-481f-b2a0-749cd099c0bf.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/67d0cd1e-9840-481f-b2a0-749cd099c0bf.png)'
- en: A DBN configured for semi-supervised learning
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 用于半监督学习的DBN配置
- en: 'A single RBM consists of two layers. The first layer is composed of visible
    neurons, and the second layer consists of hidden neurons. *Figure 16* shows the
    structure of a simple RBM, where the neurons are arranged according to a symmetrical
    bipartite graph:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 单个RBM由两层组成。第一层由可见神经元组成，第二层由隐藏神经元组成。*图16*展示了一个简单RBM的结构，其中神经元按照对称的二分图排列：
- en: '![](img/2c7b7adc-9112-4c9e-872e-20a69f6cde71.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c7b7adc-9112-4c9e-872e-20a69f6cde71.png)'
- en: RBM architecture
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: RBM架构
- en: In DBNs, an RBM is trained first with input data, called unsupervised pre-training,
    and the hidden layer represents the features learned using a greedy learning approach
    called supervised fine-tuning. Despite numerous successes, DBNs are being replaced
    by AEs.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在DBN中，首先用输入数据训练RBM，这被称为无监督预训练，隐藏层代表了通过称为监督微调的贪心学习方法学习到的特征。尽管DBN取得了众多成功，但它们正被AE所取代。
- en: Autoencoders
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动编码器
- en: An AE is a network with three or more layers, where the input layer and the
    output layer have the same number of neurons, and those intermediate (hidden layers)
    have a lower number of neurons. The network is trained to reproduce in the output,
    for each piece of input data, the same pattern of activity as in the input.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: AE是一个具有三层或更多层的网络，其中输入层和输出层的神经元数量相同，而中间的隐藏层神经元数量较少。网络被训练以在输出端重现每个输入数据的相同活动模式。
- en: 'Useful applications of AEs are data denoising and dimensionality reduction
    for data visualization. The following diagram shows how an AE typically works.
    It reconstructs the received input through two phases: an encoding phase, which
    corresponds to a dimensional reduction for the original input, and a decoding
    phase, which is capable of reconstructing the original input from the encoded
    (compressed) representation:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: AE的常见应用包括数据去噪和用于数据可视化的降维。下图展示了AE的典型工作原理。它通过两个阶段重建接收到的输入：编码阶段，通常对应于原始输入的降维；解码阶段，能够从编码（压缩）表示中重建原始输入：
- en: '![](img/281f46f7-6d2f-4ad6-9fa3-399aef7aa193.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/281f46f7-6d2f-4ad6-9fa3-399aef7aa193.png)'
- en: Encoding and decoding phases of an AE
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器（AE）的编码和解码阶段
- en: Convolutional neural networks
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: CNNs have achieved much and wide adoption in computer vision (for example, image
    recognition). In CNN networks, the connection scheme that defines the convolutional
    layer (conv) is significantly different compared to an MLP or DBN.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 已在计算机视觉（例如，图像识别）领域取得了巨大成功，并被广泛采用。在 CNN 网络中，定义卷积层（conv）的连接方案与 MLP 或 DBN 有显著不同。
- en: 'Importantly, a DNN has no prior knowledge of how the pixels are organized;
    it does not know that nearby pixels are close. A CNN''s architecture embeds this
    prior knowledge. Lower layers typically identify features in small areas of the
    image, while higher layers combine lower-level features into larger features.
    This works well with most natural images, giving CNNs a decisive head start over
    DNNs:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，DNN 对像素如何组织没有先验知识；它并不知道邻近的像素是接近的。CNN 的架构嵌入了这种先验知识。低层通常识别图像中小区域的特征，而高层则将低级特征组合成更大的特征。这在大多数自然图像中效果良好，使得
    CNN 在 DNN 之前占据了决定性优势：
- en: '![](img/2227d18b-1d82-4a75-9678-694029831dd2.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2227d18b-1d82-4a75-9678-694029831dd2.png)'
- en: A regular DNN versus a CNN
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 常规 DNN 与 CNN
- en: Take a close look at the preceding diagram; on the left is a regular three-layer
    neural network, and on the right, a CNN arranges its neurons in three dimensions
    (width, height, and depth). In a CNN architecture, a few convolutional layers
    are connected in a cascade style, where each layer is followed by a ReLUlayer,
    then a pooling layer, then a few more convolutional layers (+ReLU), then another
    pooling layer, and so on.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察前面的图示；左侧是一个常规的三层神经网络，右侧是一个将神经元以三维（宽度、高度和深度）排列的 CNN。在 CNN 架构中，几个卷积层以级联方式连接，每一层后面跟着一个
    ReLU 层，然后是池化层，再接几个卷积层（+ReLU），再接另一个池化层，依此类推。
- en: 'The output from each conv layer is a set of objects called feature maps that
    are generated by a single kernel filter. Then the feature maps can be used to
    define a new input to the next layer. Each neuron in a CNN network produces an
    output followed by an activation threshold, which is proportional to the input
    and not bound. This type of layer is called a convolutional layer. The following
    diagram is a schematic of the architecture of a CNN used for facial recognition:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 每个卷积层的输出是一组由单个内核滤波器生成的特征图。这些特征图随后可以作为下一层的新输入。CNN 网络中的每个神经元都会产生一个输出，并跟随一个激活阈值，该阈值与输入成正比且没有限制。这种类型的层称为卷积层。以下图示为用于面部识别的
    CNN 架构示意图：
- en: '![](img/209878a0-02ba-4912-80d8-d8662a4083b4.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/209878a0-02ba-4912-80d8-d8662a4083b4.png)'
- en: A schematic architecture of a CNN used for facial recognition
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 用于面部识别的 CNN 架构示意图
- en: Recurrent neural networks
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归神经网络
- en: 'A **recurrent neural network** **(RNN)** is a class of **artificial neural
    network** (**ANN**) where connections between units form a directed cycle. RNN
    architecture was originally conceived by Hochreiter and Schmidhuber in 1997\.
    RNN architectures have standard MLPs plus added loops (as shown in the following
    diagram), so they can exploit the powerful nonlinear mapping capabilities of the
    MLP; and they have some form of memory:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**递归神经网络**（**RNN**）是一类**人工神经网络**（**ANN**），其中单元之间的连接形成有向循环。RNN 架构最初由 Hochreiter
    和 Schmidhuber 于 1997 年提出。RNN 架构有标准的 MLP，并且加上了循环（如下面的图所示），因此它们能够利用 MLP 强大的非线性映射能力；并且具有某种形式的记忆：'
- en: '![](img/78c14ba6-ccda-4db1-92b6-cee4f27af9ab.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78c14ba6-ccda-4db1-92b6-cee4f27af9ab.png)'
- en: RNN architecture
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 架构
- en: The preceding image shows a a very basic RNN having an input layer, 2 recurrent
    layers and an output layer. However, this basic RNN suffers from gradient vanishing
    and exploding problem and cannot model the long-term depedencies. Therefore, more
    advanced architectures are designed to utilize sequential information of input
    data with cyclic connections among building blocks such as perceptrons. These
    architectures include **Long-Short-Term Memory** (**LSTM**), **Gated Recurrent
    Units** (**GRUs**), **Bidirectional-LSTM** and other variants.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图像展示了一个非常基础的 RNN，包含输入层、两个递归层和一个输出层。然而，这个基础的 RNN 存在梯度消失和爆炸问题，无法建模长期依赖性。因此，设计了更先进的架构，利用输入数据的顺序信息，并在各个构建模块（如感知器）之间使用循环连接。这些架构包括**长短期记忆网络**（**LSTM**）、**门控递归单元**（**GRUs**）、**双向
    LSTM**等变体。
- en: 'Consequently, LSTM and GR can overcome the drawbacks of regular RNNs: gradient
    vanishing/exploding problem and the long-short term dependency. We will look at
    these architectures in chapter 2.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，LSTM 和 GR 可以克服常规 RNN 的缺点：梯度消失/爆炸问题以及长期短期依赖问题。我们将在第二章中详细讨论这些架构。
- en: Emergent architectures
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新兴架构
- en: Many other emergent DL architectures have been suggested, such as **Deep SpatioTemporal
    Neural Networks** (**DST-NNs**), **Multi-Dimensional Recurrent Neural Networks**
    (**MD-RNNs**), and **Convolutional AutoEncoders** (**CAEs**).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 许多其他新兴的深度学习架构已经被提出，例如 **深度时空神经网络** (**DST-NNs**)、**多维递归神经网络** (**MD-RNNs**)，以及
    **卷积自编码器** (**CAEs**)。
- en: Nevertheless, there are a few more emerging networks, such as **CapsNets** (which
    is an improved version of a CNN, designed to remove the drawbacks of regular CNNs),
    RNN for image recognition, and **Generative Adversarial Networks** (**GANs**)
    for simple image generation. Apart from these, factorization machines for personalization
    and deep reinforcement learning are also being used widely.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有一些新兴的网络，如 **CapsNets**（CNN 的改进版本，旨在消除常规 CNN 的缺点）、用于图像识别的 RNN，以及用于简单图像生成的
    **生成对抗网络** (**GANs**)。除了这些，个性化的因式分解机和深度强化学习也被广泛应用。
- en: Residual neural networks
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 残差神经网络
- en: Since there are sometimes millions of billions of hyperparameters and other
    practical aspects, it's really difficult to train deeper neural networks. To overcome
    this limitation, Kaiming He et al. (see [https://arxiv.org/abs/1512.03385v1](https://arxiv.org/abs/1512.03385v1))
    proposed a residual learning framework to ease the training of networks that are
    substantially deeper than those used previously.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有时涉及数百万甚至数十亿个超参数和其他实际因素，训练更深的神经网络非常困难。为了解决这个问题，Kaiming He 等人（见 [https://arxiv.org/abs/1512.03385v1](https://arxiv.org/abs/1512.03385v1)）提出了一种残差学习框架，简化了训练比以前更深的网络。
- en: They also explicitly reformulated the layers as learning residual functions
    with reference to the layer inputs, instead of learning unreferenced functions.
    This way, these residual networks are easier to optimize and can gain accuracy
    from considerably increased depth.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还明确地将层次结构重新定义为学习参考层输入的残差函数，而不是学习无参考的函数。通过这种方式，这些残差网络更容易优化，并且可以从显著增加的深度中获得更高的准确性。
- en: The downside is that building a network by simply stacking residual blocks inevitably
    limits its optimization ability. To overcome this limitation, Ke Zhang et al.
    also proposed using a Multilevel Residual Network ([https://arxiv.org/abs/1608.02908](https://arxiv.org/abs/1608.02908)).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 不利的一面是，简单堆叠残差块来构建网络不可避免地会限制其优化能力。为了克服这一局限性，Ke Zhang 等人还提出了使用多层次残差网络（[https://arxiv.org/abs/1608.02908](https://arxiv.org/abs/1608.02908)）。
- en: Generative adversarial networks
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: GANs are deep neural net architectures that consist of two networks pitted against
    each other (hence the name "adversarial"). Ian Goodfellow et al. introduced GANs
    in a paper (see more at [https://arxiv.org/abs/1406.2661v1](https://arxiv.org/abs/1406.2661v1)).
    In GANs, the two main components are the **generator** **and discriminator**.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: GANs 是深度神经网络架构，由两个相互对抗的网络组成（因此得名“对抗”）。Ian Goodfellow 等人在一篇论文中介绍了 GANs（详情见 [https://arxiv.org/abs/1406.2661v1](https://arxiv.org/abs/1406.2661v1)）。在
    GANs 中，两个主要组件是 **生成器** 和 **判别器**。
- en: '![](img/2cf8b4f1-7163-4af1-aa4b-6066329d554a.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cf8b4f1-7163-4af1-aa4b-6066329d554a.png)'
- en: Working principle of Generative Adversarial Networks (GANs)
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GANs）的工作原理
- en: The Generator will try to generate data samples out of a specific probability
    distribution, which is very similar to the actual object. The discriminator will
    judge whether its input is coming from the original training set or from the generator
    part.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器将尝试从特定的概率分布中生成数据样本，这些样本与实际对象非常相似。判别器则会判断其输入是来自原始训练集还是来自生成器部分。
- en: Capsule networks
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 胶囊网络
- en: CNNs perform well at classifying images. However, if the images have rotation,
    tilt, or any other different orientation, then CNNs show relatively very poor
    performance. Even the pooling operation in CNNs cannot much help against the positional
    invariance.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 在图像分类方面表现优异。然而，如果图像有旋转、倾斜或其他不同的方向，CNNs 的表现会相对较差。即使是 CNN 中的池化操作，也无法在位置不变性方面提供太多帮助。
- en: This issue in CNNs has led us to the recent advancement of CapsNet through the
    paper titled *Dynamic Routing Between Capsules*(see more at [https://arxiv.org/abs/1710.09829](https://arxiv.org/abs/1710.09829))
    by Geoffrey Hinton et al.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 中的这一问题促使了 CapsNet 的最新进展，相关论文为 *胶囊之间的动态路由*（详情见 [https://arxiv.org/abs/1710.09829](https://arxiv.org/abs/1710.09829)），由
    Geoffrey Hinton 等人提出。
- en: Unlike a regular DNN, where we keep on adding layers, in CapsNets, the idea
    is to add more layers inside a single layer. This way, a CapsNet is a nested set
    of neural layers. We'll discuss more in [Chapter 11](b458baf5-6590-4e69-84d4-8c02a898dbca.xhtml),
    *Discussion, Current Trends, and Outlook*.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 与常规的 DNN 不同，在 CapsNets 中，核心思想是将更多的层添加到单一层内部。这样，CapsNet 就是一个嵌套的神经网络层集合。我们将在 [第
    11 章](b458baf5-6590-4e69-84d4-8c02a898dbca.xhtml) 中详细讨论，*讨论、当前趋势与展望*。
- en: DL frameworks and cloud platforms
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习框架和云平台
- en: In this section, we'll present some of the most popular deep learning frameworks.
    Then we will discuss some cloud based platforms where you can deploy/run your
    DL applications. In short, almost all of the libraries provide the possibility
    of using a graphics processor to speed up the learning process, are released under
    an open license, and are the result of university research groups.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些最流行的深度学习框架。然后，我们将讨论一些可以部署/运行深度学习应用程序的云平台。简而言之，几乎所有的库都提供了使用图形处理器加速学习过程的可能性，都是开源发布的，并且是大学研究小组的成果。
- en: Deep learning frameworks
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习框架
- en: '**TensorFlow** is mathematical software, and an open source software library
    for machine intelligence. The Google Brain team developed it in 2011 and open-sourced
    it in 2015\. The main features offered by the latest release of TensorFlow (v1.8
    during the writing of this book) are faster computing, flexibility, portability,
    easy debugging, a unified API, transparent use of GPU computing, easy use, and
    extensibility. Once you have constructed your neural network model, after the
    necessary feature engineering, you can simply perform the training interactively
    using plotting or TensorBoard.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow** 是一个数学软件，也是一个用于机器智能的开源软件库。由 Google Brain 团队于 2011 年开发，并在 2015
    年开源。TensorFlow 最新版本（本书写作时为 v1.8）提供的主要功能包括更快的计算速度、灵活性、可移植性、易于调试、统一的 API、透明的 GPU
    计算支持、易于使用和可扩展性。一旦你构建了神经网络模型，并进行了必要的特征工程后，你可以通过绘图或 TensorBoard 轻松地进行交互式训练。'
- en: '**Keras** is a deep learning library that sits atop TensorFlow and Theano,
    providing an intuitive API inspired by Torch. It is perhaps the best Python API
    in existence. DeepLearning4J relies on Keras as its Python API and imports models
    from Keras and through Keras from Theano and TensorFlow.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**Keras** 是一个深度学习库，位于 TensorFlow 和 Theano 之上，提供了一个直观的 API，灵感来源于 Torch。它可能是现存最好的
    Python API。DeepLearning4J 将 Keras 作为其 Python API，并通过 Keras 从 Theano 和 TensorFlow
    导入模型。'
- en: '**Theano** is also a deep learning framework written in Python. It allows using
    GPU, which is 24x faster than a single CPU. Defining, optimizing, and evaluating
    complex mathematical expressions is very straightforward in Theano.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**Theano** 也是一个用 Python 编写的深度学习框架。它允许使用 GPU，速度是单个 CPU 的 24 倍。在 Theano 中，定义、优化和评估复杂的数学表达式非常直接。'
- en: '**Neon** is a Python-based deep learning framework developed by Nirvana. Neon
    has a syntax similar to Theano''s high-level framework (for example, Keras). Currently,
    Neon is considered the fastest tool for GPU-based implementation, especially for
    CNNs. But its CPU-based implementation is relatively worse than most other libraries.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**Neon** 是一个基于 Python 的深度学习框架，由 Nirvana 开发。Neon 的语法类似于 Theano 的高级框架（例如 Keras）。目前，Neon
    被认为是基于 GPU 实现的最快工具，尤其适用于 CNN。但其基于 CPU 的实现相比大多数其他库较为逊色。'
- en: '**PyTorch** is a vast ecosystem for ML that offers a large number of algorithms
    and functions, including for DL and for processing various types of multimedia
    data, with a particular focus on parallel computing. Torch is a highly portable
    framework supported on various platforms, including Windows, macOS, Linux, and
    Android.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**PyTorch** 是一个庞大的机器学习生态系统，提供大量的算法和功能，包括深度学习和处理各种类型的多媒体数据，特别专注于并行计算。Torch 是一个高度可移植的框架，支持多种平台，包括
    Windows、macOS、Linux 和 Android。'
- en: '**Caffe**, developed primarily by **Berkeley Vision and Learning Center** (**BVLC**),
    is a framework designed to stand out because of its expression, speed, and modularity.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**Caffe** 由 **伯克利视觉与学习中心**（**BVLC**）主要开发，是一个因其表达性、速度和模块化而突出的框架。'
- en: '**MXNet** *(*[http://mxnet.io/](http://mxnet.io/)) is a deep learning framework
    that supports many languages, such as R, Python, C++, and Julia. This is helpful
    because if you know any of these languages, you will not need to step out of your
    comfort zone at all to train your deep learning models. Its backend is written
    in C++ and CUDA and it is able to manage its own memory in a way similar to Theano.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '**MXNet** *（[http://mxnet.io/](http://mxnet.io/)）是一个支持多种语言的深度学习框架，诸如 R、Python、C++
    和 Julia。这个特点很有帮助，因为如果你掌握了这些语言中的任何一种，你将无需走出舒适区就能训练你的深度学习模型。它的后端使用 C++ 和 CUDA 编写，能够像
    Theano 一样管理自己的内存。  '
- en: The **Microsoft Cognitive Toolkit** (**CNTK**) is a unified deep learning toolkit
    from Microsoft Research that makes it easy to train and combine popular model
    types across multiple GPUs and servers. CNTK implements highly efficient CNN and
    RNN training for speech, image, and text data. It supports cuDNN v5.1 for GPU
    acceleration.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**Microsoft Cognitive Toolkit**（**CNTK**）是微软研究院推出的统一深度学习工具包，旨在简化多 GPU 和服务器上的流行模型类型训练与组合。CNTK
    实现了高效的 CNN 和 RNN 训练，适用于语音、图像和文本数据。它支持 cuDNN v5.1 进行 GPU 加速。  '
- en: DeepLearning4J is one of the first commercial-grade, open source, distributed
    deep learning libraries written for Java and Scala. This also provides integrated
    support for Hadoop and Spark. DeepLearning4 is designed to be used in business
    environments on distributed GPUs and CPUs.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 'DeepLearning4J 是首批为 Java 和 Scala 编写的商业级开源分布式深度学习库之一。它还提供了对 Hadoop 和 Spark 的集成支持。DeepLearning4J
    旨在用于分布式 GPU 和 CPU 环境中的企业应用。  '
- en: DeepLearning4J aims to be cutting-edge and plug-and-play, with more convention
    than configuration, which allows for fast prototyping for non-researchers. The
    following libraries can be integrated with DeepLearning4 and will make your JVM
    experience easier whether you are developing your ML application in Java or Scala.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 'DeepLearning4J 旨在成为前沿技术且即插即用，更注重约定而非配置，这使得非研究人员能够快速原型开发。以下库可以与 DeepLearning4J
    集成，无论你是用 Java 还是 Scala 开发机器学习应用，都将让你的 JVM 使用体验更加便捷。  '
- en: ND4J is just like NumPy for JVM. It comes with some basic operations of linear
    algebra such as matrix creation, addition, and multiplication. ND4S, on the other
    hand, is a scientific computing library for linear algebra and matrix manipulation.
    It supports n-dimensional arrays for JVM-based languages.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 'ND4J 就像是 JVM 上的 NumPy，提供了诸如矩阵创建、加法和乘法等线性代数的基本操作。而 ND4S 是一个用于线性代数和矩阵操作的科学计算库，支持
    JVM 语言中的 n 维数组。  '
- en: 'To conclude, the following figure shows the last 1 year''s Google trends concerning
    the popularity of different DL frameworks:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 总结如下图所示，展示了过去一年来关于不同深度学习框架的 Google 趋势：
- en: '![](img/ea45c7d9-c0ff-44c7-8887-fec8cbdfaa15.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea45c7d9-c0ff-44c7-8887-fec8cbdfaa15.png)  '
- en: The trends of different DL frameworks. TensorFlow and Keras are most dominating.
    Theano is losing its popularity. On the other hand, DeepLearning4J is emerging
    for JVM.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '不同深度学习框架的趋势。TensorFlow 和 Keras 是最具主导性的框架，而 Theano 的人气正在下降。另一方面，DeepLearning4J
    正在成为 JVM 上的新兴选择。  '
- en: Cloud-based platforms for DL
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '基于云平台的深度学习  '
- en: Apart from the preceding libraries, there have been some recent initiatives
    for deep learning on the cloud. The idea is to bring deep learning capabilities
    to big data with millions of billions of data points and high-dimensional data.
    For example, **Amazon Web Services** (**AWS**), Microsoft Azure, Google Cloud
    Platform, and **NVIDIA GPU Cloud** (**NGC**) all offer machine and deep learning
    services that are native to their public clouds.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '除了前述的库之外，最近在云端也有一些深度学习的倡议。其理念是将深度学习能力引入到拥有数百万、数十亿数据点和高维数据的大数据中。例如，**Amazon
    Web Services**（**AWS**）、Microsoft Azure、Google Cloud Platform 和 **NVIDIA GPU Cloud**（**NGC**）都提供原生于其公共云平台的机器学习和深度学习服务。  '
- en: In October 2017, AWS released deep learning **Amazon Machine Images** (**AMIs**)
    for Amazon **Elastic Compute Cloud** (**EC2**) P3 instances. These AMIs come pre-installed
    with deep learning frameworks, such as TensorFlow, Gluon, and Apache MXNet, that
    are optimized for the NVIDIA Volta V100 GPUs within Amazon EC2 P3 instances.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '2017 年 10 月，AWS 发布了针对 Amazon **Elastic Compute Cloud**（**EC2**）P3 实例的深度学习 **Amazon
    Machine Images**（**AMIs**）。这些 AMI 预装了深度学习框架，如 TensorFlow、Gluon 和 Apache MXNet，且已针对
    Amazon EC2 P3 实例内的 NVIDIA Volta V100 GPU 进行了优化。  '
- en: The Microsoft Cognitive Toolkit is Azure's open source, deep learning service.
    Similar to AWS's offering, it focuses on tools that can help developers build
    and deploy deep learning applications.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '微软认知工具包是 Azure 的开源深度学习服务。与 AWS 提供的服务类似，它专注于可以帮助开发者构建和部署深度学习应用程序的工具。  '
- en: On the other hand, NGC empowers AI scientists and researchers with GPU-accelerated
    containers (see [https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/](https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/)).
    NGC features containerized deep learning frameworks such as TensorFlow, PyTorch,
    MXNet, and more that are tuned, tested, and certified by NVIDIA to run on the
    latest NVIDIA GPUs.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，NGC为AI科学家和研究人员提供了GPU加速的容器（见[https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/](https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/)）。NGC提供了如TensorFlow、PyTorch、MXNet等容器化的深度学习框架，这些框架经过NVIDIA的调优、测试和认证，可以在最新的NVIDIA
    GPU上运行。
- en: Now that we have a minimum of knowledge about available DL libraries, frameworks,
    and cloud-based platforms for running and deploying our DL applications, we can
    dive into coding. First, we will start by solving the famous Titanic survival
    prediction problem. However, we won't use the previously listed frameworks; we
    will be using the Apache Spark ML library. Since we will be using Spark along
    with other DL libraries, knowing a little bit of Spark would help us grasp things
    in the upcoming chapters.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对可用的深度学习库、框架和云平台有了基本的了解，能够运行和部署我们的深度学习应用程序，我们可以开始编写代码了。首先，我们将通过解决著名的泰坦尼克号生存预测问题来入手。不过，我们不会使用之前列出的框架；我们将使用Apache
    Spark ML库。由于我们将结合其他深度学习库使用Spark，了解一点Spark知识会帮助我们在接下来的章节中更好地掌握相关内容。
- en: Deep learning from a disaster – Titanic survival prediction
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习与灾难——泰坦尼克号生存预测
- en: In this section, we are going to solve the famous Titanic survival prediction
    problem available on Kaggle (see [https://www.kaggle.com/c/titanic/data](https://www.kaggle.com/c/titanic/data)).
    The task is to complete the analysis of what sorts of people are likely to survive
    using an ML algorithm.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解决Kaggle上的著名泰坦尼克号生存预测问题（见[https://www.kaggle.com/c/titanic/data](https://www.kaggle.com/c/titanic/data)）。任务是使用机器学习算法完成对哪些人群可能生还的分析。
- en: Problem description
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题描述
- en: 'Before diving into the coding, let''s see a short description of the problem.
    This paragraph is directly quoted from the Kaggle Titanic survival prediction
    page:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始编写代码之前，让我们先看看问题的简短描述。以下段落直接引用自Kaggle泰坦尼克号生存预测页面：
- en: '"The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.
    On April 15, 1912, during her maiden voyage, the Titanic sank after colliding
    with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational
    tragedy shocked the international community and led to better safety regulations
    for ships. One of the reasons that the shipwreck led to such loss of life was
    that there were not enough lifeboats for the passengers and crew. Although there
    was some element of luck involved in surviving the sinking, some groups of people
    were more likely to survive than others, such as women, children, and the upper
    class. In this challenge, we ask you to complete the analysis of what sorts of
    people were likely to survive. In particular, we ask you to apply the tools of
    machine learning to predict which passengers survived the tragedy."'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: “RMS泰坦尼克号沉没事件是历史上最臭名昭著的海难之一。1912年4月15日，泰坦尼克号在她的处女航中与冰山相撞沉没，造成2224名乘客和船员中1502人丧生。这场震惊国际社会的悲剧促使各国对船舶安全规定进行了改进。沉船导致如此大量的生命损失，其中一个原因是乘客和船员没有足够的救生艇。虽然幸存者在沉船过程中有一些运气成分，但某些群体比其他群体更有可能幸存，比如女性、儿童和上层阶级。在这个挑战中，我们要求你完成对哪些人群更可能生还的分析。特别是，我们要求你运用机器学习工具来预测哪些乘客在这场灾难中幸存下来。”
- en: 'Now, before going even deeper, we need to know about the data of the passengers
    traveling on the Titanic during the disaster so that we can develop a predictive
    model that can be used for survival analysis. The dataset can be downloaded from
    [https://github.com/rezacsedu/TitanicSurvivalPredictionDataset](https://github.com/rezacsedu/TitanicSurvivalPredictionDataset).
    There are two `.csv` files:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在深入之前，我们需要了解泰坦尼克号灾难中乘客的数据，以便我们可以开发出可用于生存分析的预测模型。数据集可以从[https://github.com/rezacsedu/TitanicSurvivalPredictionDataset](https://github.com/rezacsedu/TitanicSurvivalPredictionDataset)下载。数据集中有两个`.csv`文件：
- en: '**The training set** (`train.csv`): Can be used to build your ML models. This
    file also includes labels as the *ground truth* for each passenger for the training
    set.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集** (`train.csv`): 可用于构建你的机器学习模型。此文件还包括每位乘客的标签，作为训练集的*真实标签*。'
- en: '**The test set** (`test.csv`): Can be used to see how well your model performs
    on unseen data. However, for the test set, we do not provide the ground truth
    for each passenger.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集** (`test.csv`): 可以用来查看你的模型在未见数据上的表现。然而，对于测试集，我们没有为每个乘客提供实际结果。'
- en: 'In short, for each passenger in the test set, we have to use the trained model
    to predict whether they''ll survive the sinking of the Titanic. *Table 1* shows
    the metadata of the training set:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，对于测试集中的每个乘客，我们必须使用训练好的模型预测他们是否能在泰坦尼克号沉没中幸存。*表 1* 显示了训练集的元数据：
- en: '| **Variable** | **Definition** |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| **Variable** | **定义** |'
- en: '| `survival` | Two labels:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '| `survival` | 两个标签：'
- en: '*0 = No*'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*0 = 否*'
- en: '*1 = Yes*'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*1 = 是*'
- en: '|'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| `pclass` | This is a proxy for the **Socioeconomic Status** (**SES**) of
    a passenger and is categorized as upper, middle, and lower. In particular, *1
    = 1^(st)*, *2 = 2^(nd)*, *3 = 3^(rd).* |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| `pclass` | 这是乘客的**社会经济地位**（**SES**）的代理，分为上层、中层和下层。具体来说，*1 = 1^(st)*, *2 =
    2^(nd)*, *3 = 3^(rd)*。 |'
- en: '| `sex` | Male or female. |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| `sex` | 男性或女性。 |'
- en: '| `Age` | Age in years. |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| `Age` | 年龄（单位：年）。 |'
- en: '| `sibsp` | This signifies family relations as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '| `sibsp` | 这表示家庭关系，如下所示：'
- en: '*Sibling = brother, sister, stepbrother, stepsister*'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Sibling = 兄弟，姐妹，继兄，继姐*'
- en: '*Spouse = husband, wife (mistresses and fiancés were ignored)*'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Spouse = 丈夫，妻子（情妇和未婚夫未考虑在内）*'
- en: '|'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| `parch` | In the dataset, family relations are defined as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '| `parch` | 在数据集中，家庭关系定义如下：'
- en: '*Parent = mother, father*'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Parent = 母亲，父亲*'
- en: '*Child = daughter, son, stepdaughter, stepson*'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Child = 女儿，儿子，继女，继子*'
- en: Some children traveled only with a nanny, therefore *parch=0* for them. |
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 有些孩子是单独和保姆一起旅行的，因此对于他们来说，*parch=0*。
- en: '| `ticket` | Ticket number. |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| `ticket` | 票号。 |'
- en: '| `fare` | Passenger ticket fare. |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| `fare` | 乘客票价。 |'
- en: '| cabin | Cabin number. |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| cabin | 舱号。 |'
- en: '| `embarked` | Three ports:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '| `embarked` | 三个港口：'
- en: '*C = Cherbourg*'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*C = 瑟堡*'
- en: '*Q = Queenstown*'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Q = 皇后镇*'
- en: '*S = Southampton*'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S = 南安普顿*'
- en: '|'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Now the question would be: using this labeled data, can we draw some straightforward
    conclusions? Say that being a woman, being in first class, and being a child were
    all factors that could boost a passenger''s chances of survival during this disaster.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在问题是：使用这些标注数据，我们能否得出一些直接的结论？比如说，女性、头等舱以及儿童是能够提高乘客在这场灾难中幸存几率的因素。
- en: To solve this problem, we can start from the basic MLP, which is one of the
    oldest deep learning algorithms. For this, we use the Spark-based `MultilayerPerceptronClassifier`.
    At this point, you might be wondering why I am talking about Spark since it is
    not a DL library. However, Spark has an MLP implementation, which would be enough
    to serve our objective.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以从基本的 MLP 开始，MLP 是最古老的深度学习算法之一。为此，我们使用基于 Spark 的 `MultilayerPerceptronClassifier`。此时，你可能会想，既然
    Spark 不是深度学习库，为什么我要讲 Spark？不过，Spark 有一个 MLP 实现，这足以满足我们的目标。
- en: Then from the next chapter, we'll gradually start using more robust DNN by using
    DeepLearning4J, a JVM-based framework for developing deep learning applications.
    So let's see how to configure our Spark environment.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的章节中，我们将逐步开始使用更强大的 DNN，通过使用 DeepLearning4J，一个基于 JVM 的深度学习应用开发框架。所以我们来看看如何配置我们的
    Spark 环境。
- en: Configuring the programming environment
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置编程环境
- en: 'I am assuming that Java is already installed on your machine and the `JAVA_HOME`
    is set too. Also, I''m assuming that your IDE has the Maven plugin installed.
    If so, then just create a Maven project and add the project properties as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设你的机器上已经安装了 Java，且 `JAVA_HOME` 也已设置。还假设你的 IDE 安装了 Maven 插件。如果是这样，那么只需创建一个
    Maven 项目，并按照以下方式添加项目属性：
- en: '[PRE0]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the preceding tag, I specified Spark (that is, 2.3.0), but you can adjust
    it. Then add the following dependencies in the `pom.xml` file:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的标签中，我指定了 Spark（即 2.3.0），但你可以进行调整。然后在 `pom.xml` 文件中添加以下依赖项：
- en: '[PRE1]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then if everything goes smoothly, all the JAR files will be downloaded in the
    project home as Maven dependencies. Alright! Then we can start writing the code.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 然后如果一切顺利，所有 JAR 文件都会作为 Maven 依赖下载到项目目录下。好了！接下来我们可以开始编写代码了。
- en: Feature engineering and input dataset preparation
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程和输入数据集准备
- en: 'In this sub-section, we will see some basic feature engineering and dataset
    preparation that can be fed into the MLP classifier. So let''s start by creating
    `SparkSession`, which is the gateway to access Spark:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将看到一些基本的特征工程和数据集准备，它们可以输入到 MLP 分类器中。那么让我们从创建 `SparkSession` 开始，它是访问
    Spark 的门户：
- en: '[PRE2]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then let''s read the training set and see a glimpse of it:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 然后让我们读取训练集并看一看它的概况：
- en: '[PRE3]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A snapshot of the dataset can be seen as follows:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的快照如下所示：
- en: '![](img/1ac60d70-682f-4073-8ad9-5519b7be3bd8.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1ac60d70-682f-4073-8ad9-5519b7be3bd8.png)'
- en: A snapshot of the Titanic survival dataset
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 泰坦尼克号生存数据集快照
- en: Now we can see that the training set has both categorical as well as numerical
    features. In addition, some features are not important, such as `PassengerID`,
    `Ticket`, and so on. The same also applies to the `Name` feature unless we manually
    create some features based on the title. However, let's keep it simple. Nevertheless,
    some columns contain null values. Therefore, lots of consideration and cleaning
    are required.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到，训练集同时包含了分类特征和数值特征。此外，一些特征并不重要，例如`PassengerID`、`Ticket`等。同样，`Name`特征也不重要，除非我们手动基于标题创建一些特征。然而，我们还是保持简单。不过，一些列包含了空值，因此需要大量的考虑和清理。
- en: I ignore the `PassengerId`, `Name`, and `Ticket` columns. Apart from these,
    the `Sex` column is categorical, so I've encoded the passengers based on `male`
    and `female`. Then the `Embarked` column is encoded too. We can encode `S` as
    `0`, `C` as `1`, and `Q` as `2`.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我忽略了`PassengerId`、`Name`和`Ticket`列。除此之外，`Sex`列是分类变量，因此我根据`male`和`female`对乘客进行了编码。然后，`Embarked`列也被编码了。我们可以将`S`编码为`0`，`C`编码为`1`，`Q`编码为`2`。
- en: 'For this also, we can write user-defined-functions (also known as UDFs) called
    `normSex` and `normEmbarked` for `Sex` and `Embarked`, respectively. Let''s see
    their signatures:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一点，我们也可以编写名为`normSex`和`normEmbarked`的用户定义函数（UDF），分别对应`Sex`和`Embarked`。让我们看看它们的签名：
- en: '[PRE4]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Therefore, this UDF takes a `String` type and encodes as an integer. Now the
    `normSex` UDF also works similarly:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个UDF接收一个`String`类型，并将其编码为整数。`normSex` UDF的工作方式也类似：
- en: '[PRE5]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'So we can now select only useful columns but for the `Sex` and `Embarked` columns
    with the aforementioned UDFs:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们现在可以只选择有用的列，但对于`Sex`和`Embarked`列，我们需要使用上述UDF：
- en: '[PRE6]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](img/100ec195-8342-422b-9db2-22e4838df190.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/100ec195-8342-422b-9db2-22e4838df190.png)'
- en: Now we have been able to convert a categorical column into a numeric; however,
    as we can see, there are still null values. Therefore, what can we do? We can
    either drop the `null` values altogether or apply some `null` imputing techniques
    with the mean value of those particular columns. I believe the second approach
    is better.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经能够将一个分类列转换为数值型；然而，正如我们所看到的，仍然存在空值。那么，我们该怎么办呢？我们可以选择直接删除`null`值，或者使用一些`null`填补技术，用该列的均值进行填充。我认为第二种方法更好。
- en: 'Now, again for this null imputation, we can write UDFs too. However, for that
    we need to know some statistics about those numerical columns. Unfortunately,
    we cannot perform the summary statistics on DataFrame. Therefore, we have to convert
    the DataFrame into `JavaRDD<Vector>`. Well, we also ignore the `null` entries
    for calculating this:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，再次针对这个空值填补，我们也可以编写用户定义函数（UDF）。不过，为此我们需要了解一些关于数值列的统计信息。不幸的是，我们不能对DataFrame执行汇总统计。因此，我们必须将DataFrame转换为`JavaRDD<Vector>`。另外，我们在计算时也忽略了`null`值：
- en: '[PRE7]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now let''s compute the multivariate statistical `summary`. The `summary` statistical
    will be further used to calculate the `meanAge` and `meanFare` for the corresponding
    missing entries for these two features:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算多变量统计`summary`。`summary`统计将进一步用于计算这两个特征对应的缺失值的`meanAge`和`meanFare`：
- en: '[PRE8]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now let''s create two more UDFs for the null imputation on the `Age` and `Fare`
    columns:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为`Age`和`Fare`列创建两个UDF来进行空值填补：
- en: '[PRE9]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Therefore, we have defined a UDF, which fills in the `meanFare` values if the
    data has no entry. Now let''s create another UDF for the `Age` column:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们定义了一个UDF，如果数据没有条目，它会填充`meanFare`值。现在，让我们为`Age`列创建另一个UDF：
- en: '[PRE10]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we need to register the UDFs as follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要按照如下方式注册UDF：
- en: '[PRE11]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Therefore, let''s apply the preceding UDFs for `null` imputation:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们应用前面的UDF进行`null`填补：
- en: '[PRE12]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/e0d3af75-42f6-4d1b-9360-25daec84fbf3.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0d3af75-42f6-4d1b-9360-25daec84fbf3.png)'
- en: 'Great! We now can see that the `null` values are replaced with the mean value
    for the `Age` and `Fare` columns. However, still the numeric values are not scaled.
    Therefore, it would be a better idea to scale them. However, for that, we need
    to compute the mean and variance and then store them as a model to be used for
    later scaling:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们现在可以看到，`null`值已经被`Age`和`Fare`列的均值所替代。然而，数值仍然没有经过缩放。因此，最好对它们进行缩放。但是，为此，我们需要计算均值和方差，然后将它们存储为模型，以便以后进行缩放：
- en: '[PRE13]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then we need an encoder for the numeric values (that is, `Integer`; either
    `BINARY` or `Double`):'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要一个用于数值值的编码器（即`Integer`；可以是`BINARY`或`Double`）：
- en: '[PRE14]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then we can create a `VectorPair` consisting of the label (that is, `Survived`)
    and the features. Here the encoding is, basically, creating a scaled feature vector:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以创建一个`VectorPair`，由标签（即`Survived`）和特征组成。这里的编码基本上是创建一个缩放后的特征向量：
- en: '[PRE15]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the preceding code block, the `getScaledVector()` method does perform the
    scaling operation. The signature of this method can be seen as follows:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码块中，`getScaledVector()`方法执行了缩放操作。该方法的函数签名如下所示：
- en: '[PRE16]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Since we planned to use a Spark ML-based classifier (that is, an MLP implementation),
    we need to convert this RDD of the vector to an ML vector:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们计划使用基于Spark ML的分类器（即MLP实现），我们需要将这个RDD向量转换为ML向量：
- en: '[PRE17]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, let''s see how the resulting DataFrame looks:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看结果DataFrame的样子：
- en: '[PRE18]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](img/0fe08cf5-78c7-4260-bb72-51b3ac6fba75.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fe08cf5-78c7-4260-bb72-51b3ac6fba75.png)'
- en: 'Up to this point, we have been able to prepare our features. Still, this is
    an MLlib-based vector, so we need to further convert this into an ML vector:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经能够准备好特征了。不过，这仍然是一个基于MLlib的向量，因此我们需要进一步将其转换为ML向量：
- en: '[PRE19]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Fantastic! Now were'' almost done preparing a training set that can be consumed
    by the MLP classifier. Since we also need to evaluate the model''s performance,
    we can randomly split the training data for the training and test sets. Let''s
    allocate 80% for training and 20% for testing. These will be used to train the
    model and evaluate the model, respectively:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！现在我们几乎完成了准备一个可以供 MLP 分类器使用的训练集。由于我们还需要评估模型的性能，因此可以随机拆分训练数据为训练集和测试集。我们将80%分配给训练，20%分配给测试。它们将分别用于训练模型和评估模型：
- en: '[PRE20]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Alright. Now that we have the training set, we can perform training on an MLP
    model.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。现在我们已经有了训练集，可以对MLP模型进行训练了。
- en: Training MLP classifier
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练MLP分类器
- en: In Spark, an MLP is a classifier that consists of multiple layers. Each layer
    is fully connected to the next layer in the network. Nodes in the input layer
    represent the input data, whereas other nodes map inputs to outputs by a linear
    combination of the inputs with the node’s weights and biases and by applying an
    activation function.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，MLP是一个包含多层的分类器。每一层都与网络中的下一层完全连接。输入层的节点表示输入数据，而其他节点通过线性组合输入、节点的权重和偏置，并应用激活函数，将输入映射到输出。
- en: Interested readers can take a look at [https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier](https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣的读者可以查看[https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier](https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier)。
- en: So let's create the layers for the MLP classifier. For this example, let's make
    a shallow network considering the fact that our dataset is not that highly dimensional.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 那么让我们为MLP分类器创建层。对于这个示例，考虑到我们的数据集维度并不高，让我们构建一个浅层网络。
- en: 'Let''s assume that only 18 neurons in the first hidden layer and `8` neurons
    in the second hidden layer would be sufficient. Note that the input layer has
    `10` inputs, so we set `10` neurons and `2` neurons in the output layers since
    our MLP will predict only `2` classes. One thing is very important—the number
    of inputs has to be equal to the size of the feature vectors and the number of
    outputs has to be equal to the total number of labels:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 假设第一隐藏层只有18个神经元，第二隐藏层有`8`个神经元就足够了。请注意，输入层有`10`个输入，因此我们设置`10`个神经元，输出层设置`2`个神经元，因为我们的MLP只会预测`2`个类别。有一件事非常重要——输入的数量必须等于特征向量的大小，输出的数量必须等于标签的总数：
- en: '[PRE21]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then we instantiate the model with the trainer and set its parameters:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们用训练器实例化模型，并设置其参数：
- en: '[PRE22]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: So, as you can understand, the preceding `MultilayerPerceptronClassifier()`
    is the classifier trainer based on the MLP. Each layer has a sigmoid activation
    function except the output layer, which has the softmax activation. Note that
    Spark-based MLP implementation supports only minibatch GD and LBFGS optimizers.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所理解的，前面的`MultilayerPerceptronClassifier()`是基于MLP的分类器训练器。除了输出层使用softmax激活函数外，每一层都使用sigmoid激活函数。需要注意的是，基于Spark的MLP实现仅支持小批量梯度下降（minibatch
    GD）和LBFGS优化器。
- en: In short, we cannot use other activation functions such as ReLU or tanh in the
    hidden layers. Apart from this, other advanced optimizers are also not supported,
    nor are batch normalization and so on. This is a serious constraint of this implementation.
    In the next chapter, we will try to overcome this with DL4J.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们不能在隐藏层使用其他激活函数，如ReLU或tanh。除此之外，其他高级优化器也不被支持，批量归一化等也无法使用。这是该实现的一个严重限制。在下一章中，我们将尝试用DL4J克服这个问题。
- en: We have also set the convergence tolerance of iterations as a very small value
    so that it will lead to higher accuracy with the cost of more iterations. We set
    the block size for stacking input data in matrices to speed up the computation.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将迭代的收敛容差设置为非常小的值，这样可以通过更多的迭代获得更高的准确性。我们设置了块大小，以便在矩阵中堆叠输入数据，从而加速计算。
- en: If the size of the training set is large, then the data is stacked within partitions.
    If the block size is more than the remaining data in a partition, then it is adjusted
    to the size of this data. The recommended size is between 10 and 1,000, but the
    default block size is 128.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练集的大小很大，那么数据会在分区内堆叠。如果块大小超过了分区中剩余的数据量，那么它会调整为该数据的大小。推荐的块大小在10到1,000之间，但默认的块大小为128。
- en: 'Finally, we plan to iterate the training 1,000 times. So let''s start training
    the model using the training set:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计划将训练迭代1,000次。那么让我们开始使用训练集来训练模型：
- en: '[PRE23]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Evaluating the MLP classifier
  id: totrans-359
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估MLP分类器
- en: 'When the training is completed, we compute the prediction on the test set to
    evaluate the robustness of the model:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练完成后，我们计算测试集上的预测结果，以评估模型的鲁棒性：
- en: '[PRE24]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, how about seeing some sample predictions? Let''s observe both the true
    labels and the predicted labels:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何看一些样本预测呢？让我们观察真实标签和预测标签：
- en: '[PRE25]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](img/e47955e9-f0ad-4335-a487-19aaa4c43a47.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e47955e9-f0ad-4335-a487-19aaa4c43a47.png)'
- en: 'We can see that some predictions are correct but some of them are wrong too.
    Nevertheless, in this way, it is difficult to guess the performance. Therefore,
    we can compute performance metrics such as precision, recall, and f1 measure:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到一些预测是正确的，但也有一些是错误的。然而，以这种方式很难猜测性能。因此，我们可以计算精确度、召回率和F1值等性能指标：
- en: '[PRE26]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now let''s compute the classification''s `accuracy`, `precision`, `recall`,
    `f1` measure, and error on test data:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算分类的`准确度`、`精确度`、`召回率`、`F1`值以及测试数据上的错误率：
- en: '[PRE27]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Well done! We have been able to achieve a fair accuracy rate, that is, 78%.
    Still we can improve the with additional feature engineering. More tips will be
    given in the next section! Now, before concluding this chapter, let''s try to
    utilize the trained model to get the prediction on the test set. First, we read
    the test set and create the DataFrame:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 做得很好！我们已经能够达到一个相当高的准确率，即78%。不过，我们依然可以通过额外的特征工程进行改进。更多提示将在下一节给出！现在，在结束本章之前，让我们尝试利用训练好的模型对测试集进行预测。首先，我们读取测试集并创建DataFrame：
- en: '[PRE29]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Nevertheless, even if you see the test set, it has some null values. So let''s
    do null imputation on the `Age` and `Fare` columns. If you don''t prefer using
    UDF, you can create a MAP where you include your imputing plan:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使你查看测试集，你会发现其中有一些空值。所以我们需要对`Age`和`Fare`列进行空值填充。如果你不想使用UDF，你可以创建一个MAP，包含你的填充方案：
- en: '[PRE30]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then again, we create an RDD of `vectorPair` consisting of features and labels
    (target column):'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们再次创建一个包含特征和标签（目标列）的`vectorPair`的RDD：
- en: '[PRE31]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then we create a Spark DataFrame:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建一个Spark DataFrame：
- en: '[PRE32]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, let''s convert the MLib vectors to ML based vectors:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将MLib向量转换为基于ML的向量：
- en: '[PRE33]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, let''s perform the model inferencing, that is, create a prediction for
    the `PassengerId` column and show the sample `prediction`:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们执行模型推理，即为`PassengerId`列创建预测并展示示例`prediction`：
- en: '[PRE34]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](img/dadcc05d-8734-4479-85d1-7adeca1c1ac9.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dadcc05d-8734-4479-85d1-7adeca1c1ac9.png)'
- en: 'Finally, let''s write the result in a CSV file:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将结果写入CSV文件：
- en: '[PRE35]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Frequently asked questions (FAQs)
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见问题解答（FAQs）
- en: Now that we have solved the Titanic survival prediction problem with an acceptable
    level of accuracy, there are other practical aspects of this problem and of overall
    deep learning phenomena that need to be considered too. In this section, we will
    see some frequently asked questions that might be already in your mind. Answers
    to these questions can be found in *Appendix A*.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经以可接受的准确率解决了泰坦尼克号生存预测问题，但这个问题以及深度学习现象中其他实际问题也需要考虑。在这一节中，我们将看到一些你可能已经在想的常见问题。答案可以在*附录A*中找到。
- en: 'Draw an ANN using the original artificial neurons that compute the XOR operation:
    *A*⊕ *B*. Describe this problem formally as a classification problem. Why can''t
    simple neurons solve this problem? How does an MLP solve this problem by stacking
    multiple perceptrons?'
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用原始人工神经元绘制一个计算XOR操作的人工神经网络：*A*⊕ *B*。将这个问题正式描述为一个分类问题。为什么简单的神经元无法解决这个问题？多层感知器（MLP）是如何通过堆叠多个感知器来解决这个问题的？
- en: We have briefly seen the history of ANNs. What are the most significant milestones
    in the era of deep learning? Can we explain the timeline in a single figure?
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们简要回顾了人工神经网络的历史。那么在深度学习的时代，最重要的里程碑是什么？我们能否用一张图来解释时间线？
- en: Can I use another deep learning framework for solving this Titanic survival
    prediction problem more flexibly?
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我可以使用其他深度学习框架更灵活地解决这个泰坦尼克号生存预测问题吗？
- en: Can I use `Name` as a feature to be used in the MLP in the code?
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我可以在代码中使用`Name`作为MLP中的一个特征吗？
- en: I understand the number of neurons in the input and output layers. But how many
    neurons should I set for the hidden layers?
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我理解输入层和输出层的神经元数量。那么我应该为隐藏层设置多少个神经元？
- en: Can't we improve the predictive accuracy by the cross-validation and grid search
    technique?
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不能通过交叉验证和网格搜索技术来提高预测准确性吗？
- en: Summary
  id: totrans-393
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced some fundamental themes of DL. We started our
    journey with a basic but comprehensive introduction to ML. Then we gradually moved
    on to DL and different neural architectures. Then we got a brief overview of the
    most important DL frameworks. Finally, we saw some frequently asked questions
    related to deep learning and the Titanic survival prediction problem.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一些深度学习的基本主题。我们从对机器学习的基本但全面的介绍开始。然后，我们逐步过渡到深度学习和不同的神经网络结构。接着，我们对最重要的深度学习框架进行了简要概述。最后，我们看了一些与深度学习和泰坦尼克号生存预测问题相关的常见问题。
- en: In the next chapter, we'll begin our journey into DL by solving the Titanic
    survival prediction problem using MLP. Then'll we start developing an end-to-end
    project for cancer type classification using a recurrent LSTM network. A very-high-dimensional
    gene expression dataset will be used for training and evaluating the model.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始深入学习深度学习，通过使用多层感知器（MLP）解决泰坦尼克号生存预测问题。然后，我们将开始开发一个端到端的项目，用于使用循环LSTM网络进行癌症类型分类。我们将使用一个非常高维的基因表达数据集来训练和评估模型。
- en: Answers to FAQs
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见问题解答
- en: 'Answer to question 1: There are many ways to solve this problem:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 问题1的答案：解决这个问题的方法有很多：
- en: '*A* ⊕ *B= (A ∨ ¬ B)∨ (¬ A ∧ B)*'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*A* ⊕ *B= (A ∨ ¬ B)∨ (¬ A ∧ B)*'
- en: '*A* ⊕ *B = (A ∨ B) ∧ ¬(A ∨ B)*'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*A* ⊕ *B = (A ∨ B) ∧ ¬(A ∨ B)*'
- en: '*A* ⊕ *B = (A ∨ B) ∧ (¬ A ∨ ∧ B)*, and so on'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*A* ⊕ *B = (A ∨ B) ∧ (¬ A ∨ ∧ B)*，依此类推'
- en: 'If we go with the first approach, the resulting ANNs would look like this:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们采用第一种方法，得到的人工神经网络将如下所示：
- en: '![](img/499ef0aa-0177-40a6-8bf8-8aa808cfef1a.png)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![](img/499ef0aa-0177-40a6-8bf8-8aa808cfef1a.png)'
- en: 'Now from computer science literature, we know that only two input combinations
    and one output are associated with the XOR operation. With inputs (0, 0) or (1,
    1) the network outputs 0; and with inputs (0, 1) or (1, 0), it outputs 1\. So
    we can formally represent the preceding truth table as follows:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从计算机科学文献中，我们知道XOR操作仅与两个输入组合和一个输出相关联。对于输入（0, 0）或（1, 1），网络输出0；对于输入（0, 1）或（1,
    0），网络输出1。因此，我们可以正式地将前述真值表表示如下：
- en: '| **X0** | **X1** | **Y** |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| **X0** | **X1** | **Y** |'
- en: '| 0 | 0 | 0 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 |'
- en: '| 0 | 1 | 1 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 1 |'
- en: '| 1 | 0 | 1 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 1 |'
- en: '| 1 | 1 | 0 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0 |'
- en: 'Here, each pattern is classified into one of two classes that can be separated
    by a single line *L*. They are known as linearly separable patterns, as represented
    here:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个模式被分类为两个类之一，这两个类可以通过一条单独的直线*L*分开。它们被称为线性可分模式，如下所示：
- en: '![](img/19d184b5-45b8-431d-baa0-a389b9f93625.png)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19d184b5-45b8-431d-baa0-a389b9f93625.png)'
- en: 'Answer to question 2: The most significant progress in ANN and DL can be described
    in the following timeline. We have already seen how artificial neurons and perceptrons
    provided the base in 1943s and 1958s respectively. Then, XOR was formulated as
    a linearly non-separable problem in 1969 by Minsky et al. But later in 1974, Werbos
    et al. demonstrated the backpropagation algorithm for training the perceptron
    in 1974.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 问题2的答案：人工神经网络和深度学习的最重要进展可以通过以下时间线描述。我们已经看到，人工神经元和感知器分别在1943年和1958年为基础提供了支持。然后，1969年，Minsky等人将XOR问题公式化为一个线性不可分的问题。但是，后来在1974年，Werbos等人展示了用于训练感知器的反向传播算法。
- en: However, the most significant advancement happened in the 1980s, when John Hopfield
    et al. proposed the Hopfield Network in 1982\. Then, Hinton, one of the godfathers
    of neural networks and deep learning, and his team proposed the Boltzmann machine
    in 1985\. However, probably one of the most significant advances happened in 1986,
    when Hinton et al. successfully trained the MLP, and Jordan et. al. proposed RNNs.
    In the same year, Smolensky et al. also proposed an improved version of the RBM.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最重要的进展发生在 1980 年代，当时 John Hopfield 等人于 1982 年提出了 Hopfield 网络。然后，神经网络和深度学习的奠基人之一
    Hinton 和他的团队于 1985 年提出了玻尔兹曼机。然而，可能最重要的进展发生在 1986 年，当时 Hinton 等人成功训练了 MLP，而 Jordan
    等人提出了 RNN。同年，Smolensky 等人也提出了改进版的 RBM。
- en: In the 1990s, the most significant year was 1997\. Lecun et al. proposed LeNet
    in 1990, and Jordan et al. proposed RNN in 1997\. In the same year, Schuster et
    al. proposed an improved version of LSTM and an improved version of the original
    RNN, called **bidirectional RNN**.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在 1990 年代，最重要的一年是 1997 年。Lecun 等人于 1990 年提出了 LeNet，而 Jordan 等人则在 1997 年提出了 RNN。同年，Schuster
    等人提出了改进版的 LSTM 和改进版的原始 RNN，称为 **双向 RNN**。
- en: Despite significant advances in computing, from 1997 to 2005, we hadn't experienced
    much advancement, until Hinton struck again in 2006\. He and his team proposed
    a DBN by stacking multiple RBMs. Then in 2012, again Hinton invented dropout,
    which significantly improved regularization and overfitting in a DNN.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管计算能力有了显著的进展，但从 1997 年到 2005 年，我们并没有经历太多的突破，直到 2006 年 Hinton 再次提出了 DBN——通过堆叠多个
    RBM。然后在 2012 年，Hinton 又发明了 dropout，这大大改善了 DNN 中的正则化和过拟合问题。
- en: After that, Ian Goodfellow et al. introduced GANs, a significant milestone in
    image recognition. In 2017, Hinton proposed CapsNets to overcome the limitations
    of regular CNNs—so far one of the most significant milestones.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，Ian Goodfellow 等人引入了 GAN，这是图像识别领域的一个重要里程碑。2017 年，Hinton 提出了 CapsNets 来克服常规
    CNN 的局限性——迄今为止，这是最重要的里程碑之一。
- en: '**Answer to question 3**: Yes, you can use other deep learning frameworks described
    in the *Deep learning frameworks* section. However, since this book is about using
    Java for deep learning, I would suggest going for DeepLearning4J. We will see
    how flexibly we can create networks by stacking input, hidden, and output layers
    using DeepLearning4J in the next chapter.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 3 的答案**：是的，你可以使用*深度学习框架*部分中描述的其他深度学习框架。然而，由于本书是关于使用 Java 进行深度学习的，我建议使用
    DeepLearning4J。我们将在下一章中看到如何灵活地通过堆叠输入层、隐藏层和输出层来创建网络，使用 DeepLearning4J。'
- en: '**Answer to question 4**: Yes, you can, since the passenger''s name containing
    a different title (for example, Mr., Mrs., Miss, Master, and so on) could be significant
    too. For example, we can imagine that being a woman (that is, Mrs.) and being
    a junior (for example, Master.) could give a higher chance of survival.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 4 的答案**：是的，你可以，因为乘客的名字包含不同的称呼（例如，先生、夫人、小姐、少爷等等）也可能很重要。例如，我们可以想象，作为女性（即夫人）和作为一个年轻人（例如，少爷）可能有更高的生存机会。'
- en: 'Even, after watching the famous movie Titanic (1997), we can imagine that being
    in a relationship, a girl might have a good chance of survival since his boyfriend
    would try to save her! Anyway, this is just for imagination, so do not take it
    seriously. Now, we can write a user-defined function to encode this using Apache
    Spark. Let''s take a look at the following UDF in Java:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 甚至，在看完著名电影《泰坦尼克号》（1997）后，我们可以想象，如果一个女孩处于一段关系中，她可能有更好的生存机会，因为她的男朋友会尝试救她！不过，这只是想象而已，所以不要太当真。现在，我们可以编写一个用户定义的函数，使用
    Apache Spark 来编码这个过程。让我们来看一下以下的 Java 中的 UDF：
- en: '[PRE36]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, we can register the UDF. Then I had to register the preceding UDF as
    follows:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以注册 UDF。然后我必须按如下方式注册前面的 UDF：
- en: '[PRE37]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The resulting column would look like this:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 结果列看起来如下所示：
- en: '![](img/f2b16249-8d6f-45ec-9709-5f2b1f17060c.png)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2b16249-8d6f-45ec-9709-5f2b1f17060c.png)'
- en: '**Answer to question 5:** For many problems, you can start with just one or
    two hidden layers. This setting will work just fine using two hidden layers with
    the same total number of neurons (continue reading to get an idea about a number
    of neurons) in roughly the same amount of training time. Now let''s see some naïve
    estimation about setting the number of hidden layers:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 5 的答案**：对于许多问题，你可以从只有一到两个隐藏层开始。使用两个隐藏层（具有相同总神经元数量，稍后阅读时你会了解神经元数量）并且训练时间大致相同，这个设置就能很好地工作。现在让我们来看看关于设置隐藏层数量的一些简单估算：'
- en: '**0**: Only capable of representing linear separable functions'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**0**：只能表示线性可分函数'
- en: '**1**: Can approximate any function that contains a continuous mapping from
    one finite space to another'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1**：可以近似任何包含从一个有限空间到另一个有限空间的连续映射的函数'
- en: '**2**: Can represent an arbitrary decision boundary to arbitrary accuracy'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2**：可以以任意精度表示任意的决策边界'
- en: 'However, for a more complex problem, you can gradually ramp up the number of
    hidden layers, until you start overfitting the training set. Nevertheless, you
    can try increasing the number of neurons gradually until the network starts overfitting.
    This means the upper bound on the number of hidden neurons that will not result
    in overfitting is:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于更复杂的问题，你可以逐渐增加隐藏层的数量，直到开始过拟合训练集。不过，你也可以尝试逐步增加神经元的数量，直到网络开始过拟合。这意味着不会导致过拟合的隐藏神经元的上限是：
- en: '![](img/9b39b8c6-77fd-459a-83c9-27b442708c37.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b39b8c6-77fd-459a-83c9-27b442708c37.png)'
- en: 'In the preceding equation:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中：
- en: '*N[i]* = number of input neurons'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N[i]* = 输入神经元的数量'
- en: '*N[o]* = number of output neurons'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N[o]* = 输出神经元的数量'
- en: '*N[s]* = number of samples in training dataset'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N[s]* = 训练数据集中的样本数量'
- en: '*α* = an arbitrary scaling factor, usually *2-10*'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*α* = 任意的缩放因子，通常为*2-10*'
- en: Note that the preceding equation does not come from any research but from my
    personal working experience.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述方程并非来源于任何研究，而是来自我个人的工作经验。
- en: '**Answer to question 6:** Of course, we can. We can cross-validate the training
    and create a grid search technique for finding the best hyperparameters. Let''s
    give it a try.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题6的答案**：当然可以。我们可以对训练进行交叉验证，并创建网格搜索技术来寻找最佳超参数。让我们试试看。'
- en: 'First, we have the layers defined. Unfortunately, we cannot cross-validate
    layers. Probably, it''s either a bug or made intentionally by the Spark guys.
    So we stick to a single layering:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义了各层。不幸的是，我们无法对各层进行交叉验证。这可能是一个bug，或者是Spark团队故意为之。所以我们坚持使用单层结构：
- en: '[PRE38]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then we create the trainer and set only the layer and seed parameters:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建训练器，并只设置层和种子参数：
- en: '[PRE39]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We search through the MLP''s different hyperparameters for the best model:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在MLP的不同超参数中搜索最佳模型：
- en: '[PRE40]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We then set up the cross-validator and perform 10-fold cross-validation:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们设置交叉验证器，并执行10折交叉验证：
- en: '[PRE41]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Then we perform training using the cross-validated model:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用交叉验证后的模型进行训练：
- en: '[PRE42]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Finally, we evaluate the cross-validated model on the test set, as follows:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们对测试集上的交叉验证模型进行评估，如下所示：
- en: '[PRE43]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now we can compute and show the performance metrics, similar to our previous
    example:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以计算并显示性能指标，类似于我们之前的示例：
- en: '[PRE44]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
