- en: '<html:html><html:head><html:title>Building Chatbots and Agents with LlamaIndex</html:title></html:head>
    <html:body><html:div class="epub-source"><html:h1 class="H1---Chapter" id="_idParaDest-181">Building
    Chatbots and Agents with LlamaIndex</html:h1> <html:div id="_idContainer094"><html:p
    style="font-style:italic;">As this ebook edition doesn''t have fixed pagination,
    the page numbers below are hyperlinked for reference only, based on the printed
    edition of this book.</html:p> <html:p>This chapter provides an in-depth look
    at implementing chatbots and intelligent agents using the capabilities of LlamaIndex.
    We will explore the various chat engine modes available, from simple chatbots
    to more advanced context-aware and question- <html:strong class="bold">condensing
    engines</html:strong> . Then, we’ll dive into <html:strong class="bold">agent
    architectures</html:strong> , analyzing tools, <html:strong class="bold">reasoning
    loops</html:strong> , and parallel execution methods. You will gain practical
    knowledge so that you can build conversational interfaces powered by LLMs that
    can understand user needs and orchestrate responses or actions by utilizing tools
    and <html:span class="No-Break">data sources.</html:span></html:p> <html:p>Throughout
    this chapter, we’re going to cover the following <html:span class="No-Break">main
    topics:</html:span></html:p> <html:ul><html:li>Understanding chatbots <html:span
    class="No-Break">and agents</html:span></html:li> <html:li>Implementing agentic
    strategies in <html:span class="No-Break">our apps</html:span></html:li> <html:li>Hands-on
    – implementing conversation tracking <html:span class="No-Break">for PITS</html:span></html:li></html:ul>
    <html:a id="_idTextAnchor181"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Technical
    requirements</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-182">Technical requirements</html:h1> <html:div id="_idContainer094"><html:p>The
    following LlamaIndex integration packages will be required for the <html:span
    class="No-Break">sample code:</html:span></html:p> <html:ul><html:li><html:em
    class="italic">Database</html:em> <html:span class="No-Break"><html:em class="italic">Tool</html:em></html:span>
    <html:span class="No-Break">:</html:span> <html:a><html:span class="No-Break">https://pypi.org/project/llama-index-tools-database/</html:span></html:a></html:li>
    <html:li><html:em class="italic">OpenAI</html:em> <html:span class="No-Break"><html:em
    class="italic">Agent</html:em></html:span> <html:span class="No-Break">:</html:span>
    <html:a><html:span class="No-Break">https://pypi.org/project/llama-index-agent-openai/</html:span></html:a></html:li>
    <html:li><html:em class="italic">Wikipedia</html:em> <html:span class="No-Break"><html:em
    class="italic">Reader</html:em></html:span> <html:span class="No-Break">:</html:span>
    <html:a><html:span class="No-Break">https://pypi.org/search/?q=llama-index-readers-wikipedia</html:span></html:a></html:li>
    <html:li><html:em class="italic">LLM Compiler</html:em> <html:span class="No-Break"><html:em
    class="italic">Agent</html:em></html:span> <html:span class="No-Break">:</html:span>
    <html:a><html:span class="No-Break">https://pypi.org/project/llama-index-packs-agents-llm-compiler/</html:span></html:a></html:li></html:ul>
    <html:p>All the code samples in this chapter can be found in the <html:code class="literal">ch8</html:code>
    subfolder of this book’s GitHub <html:span class="No-Break">repository:</html:span>
    <html:a><html:span class="No-Break">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:a id="_idTextAnchor182"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Understanding
    chatbots and agents</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-183">Understanding chatbots and agents</html:h1> <html:div id="_idContainer094">chat_engine
    = index.as_chat_engine() response = chat_engine.chat("Hi, how are you?") chat_engine.chat_repl()
    chat_engine.reset() from llama_index.core.storage.chat_store import SimpleChatStore
    from llama_index.core.chat_engine import SimpleChatEngine from llama_index.core.memory
    import ChatMemoryBuffer try:     chat_store = SimpleChatStore.from_persist_path(
            persist_path="chat_memory.json"     ) except FileNotFoundError:     chat_store
    = SimpleChatStore() memory = ChatMemoryBuffer.from_defaults(     token_limit=2000,
        chat_store=chat_store,     chat_store_key="user_X"     ) chat_engine = SimpleChatEngine.from_defaults(memory=memory)
    while True:     user_message = input("You: ")     if user_message.lower() == ''exit'':
            print("Exiting chat")         break     response = chat_engine.chat(user_message)
        print(f"Chatbot: {response}") chat_store.persist(persist_path="chat_memory.json")
    from llama_index.core.chat_engine import SimpleChatEngine chat_engine = SimpleChatEngine.from_defaults()
    chat_engine.chat_repl() from llama_index.llms.openai import OpenAI llm = OpenAI(temperature=0.8,
    model="gpt-4") chat_engine = SimpleChatEngine.from_defaults(llm=llm) from llama_index.core
    import VectorStoreIndex, SimpleDirectoryReader docs = SimpleDirectoryReader(input_dir="files").load_data()
    index = VectorStoreIndex.from_documents(docs) chat_engine = index.as_chat_engine(
        chat_mode="context",     system_prompt=(         "You''re a chatbot, able
    to talk about "         "general topics, as well as answering specific "         "questions
    about ancient Rome."     ), ) chat_engine.chat_repl() retriever = index.as_retriever(retriever_mode=''default'')
    chat_engine = ContextChatEngine.from_defaults(     retriever=retriever     ) from
    llama_index.core import VectorStoreIndex, SimpleDirectoryReader from llama_index.core.chat_engine
    import CondenseQuestionChatEngine from llama_index.core.llms import ChatMessage
    documents = SimpleDirectoryReader("files").load_data() index = VectorStoreIndex.from_documents(documents)
    query_engine=index.as_query_engine() chat_history = [     ChatMessage(         role="user",
            content="Arch of Constantine is a famous"         "building in Rome"     ),
        ChatMessage(         role="user",         content="The Pantheon should not
    be "         "regarded as a famous building"     ), ] chat_engine = CondenseQuestionChatEngine.from_defaults(
        query_engine=query_engine,     chat_history=chat_history ) response = chat_engine.chat(
        "What are two of the most famous structures in ancient Rome?" ) print(response)
    The Colosseum and the Pantheon. The Colosseum and the Arch of Constantine are
    two famous buildings in ancient Rome. index.as_chat_engine(chat_mode="condense_question")
    index.as_chat_engine(chat_mode="condense_plus_context") <html:p>In the <html:a
    id="_idIndexMarker773"></html:a>modern business ecosystem, the role of <html:strong
    class="bold">chatbot systems</html:strong> is increasingly important. First appearing
    in the 1960s ( <html:a>https://en.wikipedia.org/wiki/ELIZA</html:a> ), chatbots
    have always fascinated both developers and technology users alike. <html:span
    class="No-Break"><html:em class="italic">Figure 8</html:em></html:span> <html:em
    class="italic">.1</html:em> shows the user interface of one of these <html:span
    class="No-Break">early systems:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 8.1 – The ELIZA chatbot interface</html:p> <html:p>While these
    <html:a id="_idIndexMarker774"></html:a>systems were rudimentary initially and
    seen as more of an experiment, with the advancement of NLP technologies, the experience
    they offer has become increasingly interesting and valuable <html:span class="No-Break">to
    users.</html:span></html:p> <html:p><html:strong class="bold">Chatbot-based support
    systems</html:strong> offer today’s consumers a self-service experience. For users,
    self-service support services have <html:a id="_idIndexMarker775"></html:a>two
    major advantages over <html:span class="No-Break">human support:</html:span></html:p>
    <html:ul><html:li>They are available 24/7, even outside normal <html:span class="No-Break">working
    hours</html:span></html:li> <html:li>The user does not have to <html:em class="italic">hold
    the line</html:em> to <html:span class="No-Break">access them</html:span></html:li></html:ul>
    <html:p>Even if there is some reluctance to use these systems at first, once they
    discover these advantages, users soon get used to interacting <html:span class="No-Break">with
    them.</html:span></html:p> <html:p>Don’t necessarily think of chatbots as a technology
    designed to replace human support and interaction entirely. Although they have
    made enormous progress in recent years, these technologies, while getting more
    and more advanced, still have <html:span class="No-Break">their shortcomings.</html:span></html:p>
    <html:p>Lacking real empathy and the human touch, even under ideal operating conditions,
    chatbot-based services are unlikely to replace human support completely. But that
    doesn’t mean they aren’t extremely valuable, both for organizations and <html:span
    class="No-Break">their users.</html:span></html:p> <html:p>Perhaps the greatest
    value they bring is when they work in a blended experience, where users can receive
    both human support and access to self-service platforms that are interfaced with
    chatbot technologies. Implemented strategically, these systems can vastly improve
    not only the support offered to end consumers but also the internal interactions
    between an <html:span class="No-Break">organization’s employees.</html:span></html:p>
    <html:p><html:strong class="bold">ChatOps</html:strong> , for example, is <html:a
    id="_idIndexMarker776"></html:a>a model <html:a id="_idIndexMarker777"></html:a>increasingly
    used by modern <html:span class="No-Break">organizations (</html:span> <html:a><html:span
    class="No-Break">https://www.ibm.com/blog/benefits-of-chatops/</html:span></html:a>
    <html:span class="No-Break">).</html:span></html:p> <html:p class="callout-heading">Definition</html:p>
    <html:p class="callout">ChatOps refers to the ability to integrate chat platforms
    with operational workflows, facilitating transparent collaboration among team
    members, processes, tools, and automated bots to enhance service dependability,
    accelerate recovery, and boost <html:span class="No-Break">collaborative productivity.</html:span></html:p>
    <html:p>Based on the <html:a id="_idIndexMarker778"></html:a>idea of <html:strong
    class="bold">conversation-driven collaboration</html:strong> , the ChatOps model
    combines <html:strong class="bold">DevOps</html:strong> principles ( <html:a>https://en.wikipedia.org/wiki/DevOps</html:a>
    ) by <html:a id="_idIndexMarker779"></html:a>simplifying and accelerating interactions
    between team members <html:span class="No-Break">using chatbots.</html:span></html:p>
    <html:p>Whether we use them for internal communication or in interactions with
    our users, chatbots can only be useful to the extent that they can solve real
    problems. This depends on how well they can understand the context of the interaction
    and how relevant the answers they <html:span class="No-Break">provide are.</html:span></html:p>
    <html:p><html:span class="No-Break"><html:em class="italic">Figure 8</html:em></html:span>
    <html:em class="italic">.2</html:em> provides <html:a id="_idIndexMarker780"></html:a>a
    visual representation of the <html:span class="No-Break">ChatOps model:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 8.2 – The ChatOps paradigm</html:p>
    <html:p>If, in the beginning, the main limitation of chatbots came from the <html:em
    class="italic">clumsy</html:em> way of interacting with the user, with the evolution
    of NLP technologies, the main shortcoming has become, more recently, the lack
    of integration with the organization’s <html:span class="No-Break">knowledge base.</html:span></html:p>
    <html:p>After all, what good is a natural communication experience if the answers
    given by the system aren’t useful in solving the <html:span class="No-Break">user’s
    requests?</html:span></html:p> <html:p>This brings us <html:span class="No-Break">to
    RAG.</html:span></html:p> <html:p>By now, I think it has become obvious that without
    being connected to an organization’s knowledge base, a chatbot can, at best, be
    considered a technology experiment. Even conversational engines based on powerful
    LLMs such as GPT-4 can, at best, provide generic answers that don’t always address
    the specific problems of each organization. Perhaps worse, not being anchored
    in validated documentation, they can <html:em class="italic">hallucinate</html:em>
    very convincingly, creating unpleasant or even potentially <html:span class="No-Break">dangerous
    experiences.</html:span></html:p> <html:p>As you’ve probably guessed by now, LlamaIndex
    <html:a id="_idIndexMarker781"></html:a>also offers RAG tools for implementing
    chatbot technologies. In this chapter, we will explore the options available to
    us and understand how we can implement very simple systems to advanced <html:span
    class="No-Break">chatbot mechanisms.</html:span></html:p> <html:p>But first, let’s
    see how this functionality is built <html:span class="No-Break">into LlamaIndex.</html:span></html:p>
    <html:a id="_idTextAnchor183"></html:a><html:h2 id="_idParaDest-184">Discovering
    ChatEngine</html:h2> <html:p>In the previous chapters, we <html:a id="_idIndexMarker782"></html:a>saw
    how we can build a query engine to run queries based on our data. This mechanism
    allows us to integrate multiple types of indexes, retrievers, node postprocessors,
    and response synthesizers at the same time, thus being able to access our proprietary
    data in multiple ways. Unfortunately, the <html:code class="literal">QueryEngine</html:code>
    class <html:a id="_idIndexMarker783"></html:a>does not provide any mechanism to
    keep the history of a conversation. That means each query is a separate interaction
    and there is no contextual memory to allow a <html:span class="No-Break">true</html:span>
    <html:span class="No-Break"><html:em class="italic">conversation</html:em></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>For that purpose, however,
    we have <html:strong class="bold">ChatEngine</html:strong> . Unlike query engines,
    <html:code class="literal">ChatEngine</html:code> allows us to have an actual
    conversation, giving us both the context of our proprietary data and the history
    of the chat. To simplify this concept even further, imagine a <html:code class="literal">QueryEngine</html:code>
    class <html:span class="No-Break">with memory.</html:span></html:p> <html:p>In
    its simplest form, a chat engine can be initialized just as easily, based on <html:span
    class="No-Break">an index:</html:span></html:p> <html:p>Once initialized, a chat
    engine can be queried using <html:span class="No-Break">various methods:</html:span></html:p>
    <html:ul><html:li><html:code class="literal">chat()</html:code> : This method
    initiates a synchronous chat session, processing the user’s message and returning
    the <html:span class="No-Break">response immediately.</html:span></html:li> <html:li><html:code
    class="literal">achat()</html:code> : This method is similar to <html:code class="literal">chat()</html:code>
    but executes the query asynchronously, allowing multiple requests to be processed
    simultaneously. This can be useful, for example, in a web or mobile application
    where we want to avoid blocking the main thread during <html:span class="No-Break">server
    queries.</html:span></html:li> <html:li><html:code class="literal">stream_chat()</html:code>
    : This method opens a streaming chat session, where responses can be returned
    as they are generated, for more dynamic interaction. This is particularly useful
    for long or complex responses that require significant processing time, allowing
    the user to start seeing parts of the response before all processing <html:span
    class="No-Break">is complete.</html:span></html:li> <html:li><html:code class="literal">astream_chat()</html:code>
    : This method is an asynchronous version of <html:code class="literal">stream_chat()</html:code>
    that allows us to handle streaming interactions in an <html:span class="No-Break">asynchronous
    context.</html:span></html:li></html:ul> <html:p>Another option is to <html:a
    id="_idIndexMarker784"></html:a>initiate a <html:strong class="bold">Read-Eval-Print</html:strong>
    ( <html:strong class="bold">REPL</html:strong> ) loop <html:span class="No-Break">with</html:span>
    <html:span class="No-Break"><html:code class="literal">ChatEngine</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p>A REPL chat is akin
    to a <html:a id="_idIndexMarker785"></html:a>ChatGPT interface, where a user sends
    a message or question, the LLM processes the input, generates a response, and
    then immediately displays it to the user. This loop continues for as long as the
    user keeps providing input, creating an <html:span class="No-Break">interactive
    conversation.</html:span></html:p> <html:p>To reset a chat conversation, you can
    use the <html:span class="No-Break">following command:</html:span></html:p> <html:p>This
    is useful when you want to clear the history and begin a new <html:span class="No-Break">conversation
    thread.</html:span></html:p> <html:p>So, the basics are <html:a id="_idIndexMarker786"></html:a>very
    straightforward. Next, let’s talk about the different <html:strong class="bold">built-in
    chat modes</html:strong> available <html:span class="No-Break">in LlamaIndex.</html:span></html:p>
    <html:a id="_idTextAnchor184"></html:a><html:h2 id="_idParaDest-185">Understanding
    the different chat modes</html:h2> <html:p>When <html:a id="_idIndexMarker787"></html:a>initializing
    a chat engine, we can use the <html:code class="literal">chat_mode</html:code>
    argument to invoke various chat engine types predefined in LlamaIndex. I will
    show you how each of these engines works. We will discuss them one by one and
    get a good understanding of the advantages and use cases best suited for each
    <html:span class="No-Break">of them.</html:span></html:p> <html:p>But first, let’s
    have a short introduction to how chat memory is managed <html:span class="No-Break">within
    LlamaIndex.</html:span></html:p> <html:h3>Understanding how chat memory works</html:h3>
    <html:p>The <html:code class="literal">ChatMemoryBuffer</html:code> class is a
    <html:a id="_idIndexMarker788"></html:a>specialized memory buffer that’s designed
    to store chat history efficiently while also managing the token limit imposed
    by different LLMs. This structure is important because we can pass it as an argument
    when initializing chat engines using the <html:code class="literal">memory</html:code>
    parameter. By saving and restoring this buffer from one session to another, we
    can implement persistence for <html:span class="No-Break">our conversations.</html:span></html:p>
    <html:p>There are two different storage options for the <html:span class="No-Break">chat
    store:</html:span></html:p> <html:ul><html:li>The default <html:code class="literal">SimpleChatStore</html:code>
    , which <html:a id="_idIndexMarker789"></html:a>stores the conversation <html:span
    class="No-Break">in memory</html:span></html:li> <html:li>The more <html:a id="_idIndexMarker790"></html:a>advanced
    <html:code class="literal">RedisChatStore</html:code> , which stores the chat
    history in a Redis database, eliminating the need to manually persist and load
    the <html:span class="No-Break">chat history</html:span></html:li></html:ul> <html:p>The
    <html:code class="literal">chat_store</html:code> attribute, which <html:a id="_idIndexMarker791"></html:a>is
    an <html:a id="_idIndexMarker792"></html:a>instance of the <html:code class="literal">BaseChatStore</html:code>
    class, is <html:a id="_idIndexMarker793"></html:a>used for the actual storage
    and retrieval of chat messages. This modular approach allows different storage
    implementations, such as a simple in-memory store or more complex <html:span class="No-Break">database-backed
    stores.</html:span></html:p> <html:p>We also have <html:a id="_idIndexMarker794"></html:a>the
    <html:code class="literal">chat_store_key</html:code> parameter, which is used
    to uniquely identify the chat session or conversation within the chat store. This
    is useful for retrieving the correct conversation history when there are multiple
    conversations stored in the <html:a id="_idIndexMarker795"></html:a>same chat
    store. Here’s a basic example of <html:strong class="bold">conversation history
    persistence</html:strong> <html:span class="No-Break">using</html:span> <html:span
    class="No-Break"><html:code class="literal">SimpleChatStore</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p>After importing the
    necessary libraries, we can try to load the previous conversation. If there is
    no previous conversation save file, we simply initialize an <html:span class="No-Break">empty</html:span>
    <html:span class="No-Break"><html:code class="literal">chat_store</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p>It’s now time to initialize
    our memory buffer by using <html:code class="literal">chat_store</html:code> as
    an argument. Although not needed here, for a more detailed illustration, we will
    also customize <html:code class="literal">token_limit</html:code> <html:span class="No-Break">and</html:span>
    <html:span class="No-Break"><html:code class="literal">chat_store_key</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p>OK; we have all <html:a
    id="_idIndexMarker796"></html:a>the necessary pieces. Let’s put them together
    into a <html:code class="literal">SimpleChatEngine</html:code> class and create
    a <html:span class="No-Break">chat loop:</html:span></html:p> <html:p>Once the
    user types <html:code class="literal">exit</html:code> and we break the loop,
    we use the <html:code class="literal">persist()</html:code> method to store the
    current conversation for <html:span class="No-Break">future sessions:</html:span></html:p>
    <html:p>In case you’re wondering why we haven’t used the <html:code class="literal">chat_repl()</html:code>
    method shown previously and created a chat loop instead, the answer is in the
    <html:span class="No-Break">following note.</html:span></html:p> <html:p class="callout-heading">Important
    note</html:p> <html:p class="callout">While the <html:code class="literal">chat()</html:code>
    , <html:code class="literal">achat()</html:code> , <html:code class="literal">stream_chat()</html:code>
    , and <html:code class="literal">astream_chat()</html:code> methods can benefit
    from loading and resuming previous conversations, by design, the <html:code class="literal">chat_repl()</html:code>
    method will reset the conversation history <html:span class="No-Break">during
    initialization.</html:span></html:p> <html:p><html:code class="literal">ChatMemoryBuffer</html:code>
    also plays an important role in ensuring that the conversation’s context remains
    within the token limits of the model being used. Among other parameters available
    for <html:code class="literal">ChatMemoryBuffer</html:code> , the <html:code class="literal">token_limit</html:code>
    attribute specifies the maximum number of tokens that can be stored in the memory
    buffer. This limit is essential to ensure we stay within the maximum context window
    size of the current LLM we <html:span class="No-Break">are using.</html:span></html:p>
    <html:p>When the <html:a id="_idIndexMarker797"></html:a>conversation exceeds
    the context limit, a sliding window method is applied. Older parts of the conversation
    are truncated to ensure that the most recent and relevant parts are retained and
    processed by the LLM within its <html:span class="No-Break">token constraints.</html:span></html:p>
    <html:p class="callout-heading">An analogy to better understand the sliding window
    method</html:p> <html:p class="callout">Imagine a conversation with an LLM as
    a train journey, where each piece of dialogue adds a carriage. However, the train
    can only be so long due to the tracks’ length limit, representing the model’s
    context window limit. To keep the journey going and add new carriages – in our
    case, messages – older ones need to be detached and left behind. This ensures
    the train can continue its journey, carrying the most recent and relevant parts
    of the conversation, while staying within the limits of the track. Just like in
    a train journey, where we might prioritize which carriages to keep based on their
    importance, the sliding window method prioritizes newer conversation parts, keeping
    the dialogue <html:span class="No-Break">flowing smoothly.</html:span></html:p>
    <html:p>Now that we understand how memory works, let’s talk about the different
    available <html:span class="No-Break">chat modes.</html:span></html:p> <html:h3>Simple
    mode</html:h3> <html:p>This is <html:a id="_idIndexMarker798"></html:a>the most
    <html:strong class="bold">basic chat engine</html:strong> available. It allows
    for a simple, direct conversation with the LLM, without any connection to our
    proprietary data. <html:span class="No-Break"><html:em class="italic">Figure 8</html:em></html:span>
    <html:em class="italic">.3</html:em> explains this chat <html:span class="No-Break">mode
    visually:</html:span></html:p> <html:p class="IMG---Caption" lang="en-US">Figure
    8.3 – SimpleChatEngine</html:p> <html:p>The user’s experience <html:a id="_idIndexMarker799"></html:a>in
    this mode is defined by the inherent capabilities and limitations of the LLM,
    such <html:a id="_idIndexMarker800"></html:a>as its context window size and <html:span
    class="No-Break">overall performance.</html:span></html:p> <html:p>To initialize
    this mode, we can use the <html:span class="No-Break">following code:</html:span></html:p>
    <html:p>If we want, we can customize the LLM using the <html:span class="No-Break"><html:code
    class="literal">llm</html:code></html:span> <html:span class="No-Break">argument:</html:span></html:p>
    <html:p>As you probably won’t be using this mode too much in your RAG designs,
    let’s talk about the more advanced options that <html:span class="No-Break">are
    available.</html:span></html:p> <html:h3>Context mode</html:h3> <html:p><html:code
    class="literal">ContextChatEngine</html:code> is <html:a id="_idIndexMarker801"></html:a>designed
    to enhance chat interactions by <html:a id="_idIndexMarker802"></html:a>leveraging
    our proprietary knowledge. It works by retrieving relevant text from an index
    based on the user’s input, integrating this retrieved information into the system
    prompt to provide context, and then generating a response with the help of <html:span
    class="No-Break">the LLM.</html:span></html:p> <html:p>Have a look at <html:span
    class="No-Break"><html:em class="italic">Figure 8</html:em></html:span> <html:em
    class="italic">.4</html:em> for a visual representation of this <html:span class="No-Break">chat
    mode:</html:span></html:p> <html:p class="IMG---Caption" lang="en-US">Figure 8.4
    – ContextChatEngine</html:p> <html:p>There are <html:a id="_idIndexMarker803"></html:a>several
    <html:a id="_idIndexMarker804"></html:a>parameters that <html:a id="_idIndexMarker805"></html:a>we
    can customize for this <html:span class="No-Break">chat engine:</html:span></html:p>
    <html:ul><html:li><html:code class="literal">retriever</html:code> : The actual
    retriever that’s used to retrieve relevant text from the index based on the user’s
    message. When the chat engine is initialized directly from the index, it will
    use the default retriever for that particular <html:span class="No-Break">index
    type</html:span></html:li> <html:li><html:code class="literal">llm</html:code>
    : An instance of an LLM, which will be used for <html:span class="No-Break">generating
    responses</html:span></html:li> <html:li><html:code class="literal">memory</html:code>
    : A <html:code class="literal">ChatMemoryBuffer</html:code> object, which is used
    to store and manage the <html:span class="No-Break">chat history</html:span></html:li>
    <html:li><html:code class="literal">chat_history</html:code> : This is an optional
    list of <html:code class="literal">ChatMessage</html:code> instances representing
    the history of the conversation. It can be used to maintain continuity in a conversation.
    This history includes all messages that have been exchanged in the chat session,
    including both user and chatbot messages. For instance, it can be used to continue
    a conversation from a certain point. A <html:code class="literal">ChatMessage</html:code>
    object contains <html:span class="No-Break">three attributes:</html:span> <html:ul><html:li><html:code
    class="literal">role</html:code> : This defaults <html:span class="No-Break">to</html:span>
    <html:span class="No-Break"><html:em class="italic">user</html:em></html:span></html:li>
    <html:li><html:code class="literal">content</html:code> : The <html:span class="No-Break">actual
    message</html:span></html:li> <html:li>Any optional arguments provided <html:span
    class="No-Break">via</html:span> <html:span class="No-Break"><html:code class="literal">additional_kwargs</html:code></html:span></html:li></html:ul></html:li>
    <html:li><html:code class="literal">prefix_messages</html:code> : A list of <html:code
    class="literal">ChatMessage</html:code> instances that may be used as predefined
    messages or prompts before the actual user message. This can be useful for setting
    a particular tone or context for <html:span class="No-Break">the chat</html:span></html:li>
    <html:li><html:code class="literal">node_postprocessors</html:code> : An optional
    list of <html:code class="literal">BaseNodePostprocessor</html:code> instances
    for further processing the nodes retrieved by the retriever. This can be used
    to <html:a id="_idIndexMarker806"></html:a>implement guardrails, scrub sensitive
    information from the <html:a id="_idIndexMarker807"></html:a>context, or make
    any other adjustments to the retrieved nodes <html:span class="No-Break">if required</html:span></html:li>
    <html:li><html:code class="literal">context_template</html:code> : A string template
    that can be used to format the prompt that feeds the context to <html:span class="No-Break">the
    LLM</html:span></html:li> <html:li><html:code class="literal">callback_manager</html:code>
    : An optional <html:code class="literal">CallbackManager</html:code> instance
    for managing callbacks during the chat process. This is useful for tracing and
    <html:span class="No-Break">debugging purposes</html:span></html:li> <html:li><html:code
    class="literal">system_prompt</html:code> : An optional string that’s used as
    a system prompt, providing initial context or instructions for <html:span class="No-Break">the
    chatbot</html:span></html:li> <html:li><html:code class="literal">service_context</html:code>
    : An optional <html:code class="literal">ServiceContext</html:code> instance,
    which can be used to make additional customizations to the <html:span class="No-Break">chat
    engine</html:span></html:li></html:ul> <html:p>To implement <html:code class="literal">ContextChatEngine</html:code>
    , we <html:a id="_idIndexMarker808"></html:a>must load our data and build an index,
    then optionally configure the chat engine with different parameters <html:span
    class="No-Break">as needed.</html:span></html:p> <html:p>Here’s a quick example
    based on our sample data files, which can be found in the <html:code class="literal">ch8/files</html:code>
    subfolder in this book’s <html:span class="No-Break">GitHub repository:</html:span></html:p>
    <html:p>In this <html:a id="_idIndexMarker809"></html:a>example, we initialized
    <html:code class="literal">chat_engine</html:code> from the index. Alternatively,
    we could have defined it standalone, providing a retriever as an argument, <html:span
    class="No-Break">like this:</html:span></html:p> <html:p>Overall, this <html:a
    id="_idIndexMarker810"></html:a>chat mode is particularly effective for queries
    that relate to the knowledge contained within our data, supporting both general
    conversations and more specific discussions based on the <html:span class="No-Break">indexed
    content.</html:span></html:p> <html:p>Because the engine first retrieves context
    from the index and uses it to generate responses, this approach makes the chat
    experience a lot more useful and natural for users seeking specific information
    from the <html:span class="No-Break">indexed data.</html:span></html:p> <html:h3>Condense
    question mode</html:h3> <html:p><html:code class="literal">CondenseQuestionChatEngine</html:code>
    streamlines <html:a id="_idIndexMarker811"></html:a>the <html:a id="_idIndexMarker812"></html:a>user
    interaction by first <html:strong class="bold">condensing the conversation</html:strong>
    and the latest user message into a standalone question with the help of the LLM.
    This standalone question, which tries to capture the essential elements of the
    conversation, is then sent to a query engine built on our proprietary data to
    generate <html:span class="No-Break">a response.</html:span></html:p> <html:p>The
    main benefit of using this approach is that it maintains the conversation focused
    on the topic, preserving the essential points of the entire dialogue throughout
    every interaction. And it <html:a id="_idIndexMarker813"></html:a>always responds
    in the context of our <html:span class="No-Break">proprietary data.</html:span></html:p>
    <html:p><html:span class="No-Break"><html:em class="italic">Figure 8</html:em></html:span>
    <html:em class="italic">.5</html:em> describes the operation of this particular
    <html:span class="No-Break">chat mode:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 8.5 – CondenseQuestionChatEngine</html:p> <html:p>The fact
    that the final response comes from our retrieved proprietary data and not directly
    from the LLM can also be a disadvantage sometimes. This chat mode may struggle
    with more general questions, such as inquiries about previous interactions, due
    to its reliance <html:a id="_idIndexMarker814"></html:a>on querying the knowledge
    base for <html:span class="No-Break">every response.</html:span></html:p> <html:p>Let’s
    look at some of <html:a id="_idIndexMarker815"></html:a>the key parameters <html:span
    class="No-Break">of</html:span> <html:span class="No-Break"><html:code class="literal">CondenseQuestionChatEngine</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">query_engine</html:code> : This is a <html:code class="literal">BaseQueryEngine</html:code>
    instance that’s used to query the condensed question. Any type of query engine
    may be used here, including complex constructs with <html:span class="No-Break">routing
    functionality</html:span></html:li> <html:li><html:code class="literal">condense_question_prompt</html:code>
    : This is a <html:code class="literal">BasePromptTemplate</html:code> instance
    that’s used for condensing the conversation and user message into a single, <html:span
    class="No-Break">standalone question</html:span></html:li> <html:li><html:code
    class="literal">Memory</html:code> : A <html:code class="literal">ChatMemoryBuffer</html:code>
    instance that’s used to manage and store the <html:span class="No-Break">chat
    history</html:span></html:li> <html:li><html:code class="literal">llm</html:code>
    : A language model instance for generating the <html:span class="No-Break">condensed
    question</html:span></html:li> <html:li><html:code class="literal">verbose</html:code>
    : A Boolean flag for printing verbose logs <html:span class="No-Break">during
    operation</html:span></html:li> <html:li><html:code class="literal">callback_manager</html:code>
    : An optional <html:code class="literal">CallbackManager</html:code> instance
    for <html:span class="No-Break">managing callbacks</html:span></html:li></html:ul>
    <html:p>To implement <html:a id="_idIndexMarker816"></html:a>this chat <html:a
    id="_idIndexMarker817"></html:a>engine, we typically initialize it with a query
    engine and, optionally, configure it with custom parameters. The conversation
    is condensed into a question using a predefined template that can be customized
    using the <html:code class="literal">condense_question_prompt</html:code> parameter.
    The resulting question is then sent to the <html:span class="No-Break">query engine.</html:span></html:p>
    <html:p>Here’s a brief <html:a id="_idIndexMarker818"></html:a><html:span class="No-Break">implementation
    example:</html:span></html:p> <html:p>In the first part of the code, we ingested
    our sample files, created an index, and then created a simple query engine. Next,
    we introduced a previous conversation context by creating a <html:a id="_idIndexMarker819"></html:a>chat
    history consisting of two <html:code class="literal">ChatMessage</html:code> objects.
    Specifically, we instructed the chat engine not to consider the Pantheon as a
    <html:span class="No-Break">famous building.</html:span></html:p> <html:p>Now,
    let’s create our chat engine and <html:span class="No-Break">query it:</html:span></html:p>
    <html:p>Let’s see what <html:a id="_idIndexMarker820"></html:a>happened in <html:span
    class="No-Break">the background:</html:span></html:p> <html:ol><html:li><html:code
    class="literal">CondenseQuestionChatEngine</html:code> took the user’s message,
    along with the provided chat history, and condensed them into a standalone question.
    This process involved using the LLM and <html:code class="literal">condense_question_prompt</html:code>
    to generate a question that encapsulates the essence of the conversation context
    and the user’s <html:span class="No-Break">latest query.</html:span></html:li>
    <html:li>Then, the engine forwarded this condensed question to the query engine,
    which searched the indexed data for <html:span class="No-Break">relevant information.</html:span></html:li>
    <html:li>The query engine, having access to the information from <html:code class="literal">VectorStoreIndex</html:code>
    , processed the question and returned an answer. This answer reflects the collective
    context of the previous conversation and the specific query about famous structures
    in <html:span class="No-Break">ancient Rome.</html:span></html:li></html:ol> <html:p>Without
    the added chat history, the output of the sample would have been similar to <html:span
    class="No-Break">the following:</html:span></html:p> <html:p>This is because the
    two buildings are explicitly mentioned in our <html:span class="No-Break">sample
    data.</html:span></html:p> <html:p>However, once we add the new conversational
    context, the output looks <html:span class="No-Break">like this:</html:span></html:p>
    <html:p>Another way of initializing this chat engine would be directly from the
    index, <html:span class="No-Break">like this:</html:span></html:p> <html:p>This
    <html:a id="_idIndexMarker821"></html:a>chat mode is particularly useful for complex
    <html:a id="_idIndexMarker822"></html:a>conversations where the context and nuances
    of previous exchanges play a crucial role in understanding and accurately responding
    to the latest query. It ensures that the chatbot remains aware of the conversation’s
    history, thus making the interaction more coherent and <html:span class="No-Break">contextually
    relevant.</html:span></html:p> <html:p>The next chat mode we’ll talk about uses
    a mix of two <html:span class="No-Break">other approaches.</html:span></html:p>
    <html:h3>Condense and context mode</html:h3> <html:p><html:code class="literal">CondensePlusContextChatEngine</html:code>
    offers <html:a id="_idIndexMarker823"></html:a>an even more comprehensive chat
    interaction <html:a id="_idIndexMarker824"></html:a>by combining the benefits
    of condensed questions and <html:span class="No-Break">context retrieval.</html:span></html:p>
    <html:p>While the previous chat engine we discussed is more straightforward and
    focuses on simplifying the conversation into a question for response generation,
    <html:code class="literal">CondensePlusContextChatEngine</html:code> takes an
    extra step to enrich the conversation with additional context from the indexed
    data, leading to more detailed and context-aware responses. The trade-off here
    is an increase in response generation time due to the additional step performed.
    Let’s explore how it works under the hood by looking at <html:span class="No-Break"><html:em
    class="italic">Figure 8</html:em></html:span> <html:span class="No-Break"><html:em
    class="italic">.6</html:em></html:span> <html:span class="No-Break">:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 8.6 – CondensePlusContextChatEngine</html:p>
    <html:p>First, this engine <html:a id="_idIndexMarker825"></html:a>condenses a
    conversation and the latest user message into a standalone question. Then, it
    retrieves relevant context from the index using this condensed question. Finally,
    it uses both the retrieved context and the condensed question to <html:a id="_idIndexMarker826"></html:a>generate
    a response with <html:span class="No-Break">the LLM.</html:span></html:p> <html:p>Here
    are some of the <html:a id="_idIndexMarker827"></html:a>key parameters <html:span
    class="No-Break">of</html:span> <html:span class="No-Break"><html:code class="literal">CondensePlusContextChatEngine</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">retriever</html:code> : Used to fetch context based on the <html:span
    class="No-Break">condensed question</html:span></html:li> <html:li><html:code
    class="literal">llm</html:code> : The LLM that’s used to generate the condensed
    question and the <html:span class="No-Break">final response</html:span></html:li>
    <html:li><html:code class="literal">memory</html:code> : A <html:code class="literal">ChatMemoryBuffer</html:code>
    instance for storing and managing <html:span class="No-Break">chat history</html:span></html:li>
    <html:li><html:code class="literal">context_prompt</html:code> : A prompt template
    for formatting the context in the <html:span class="No-Break">system prompt</html:span></html:li>
    <html:li><html:code class="literal">condense_prompt</html:code> : A prompt for
    condensing the conversation into a <html:span class="No-Break">standalone question</html:span></html:li>
    <html:li><html:code class="literal">system_prompt</html:code> : A prompt with
    instructions for <html:span class="No-Break">the chatbot</html:span></html:li>
    <html:li><html:code class="literal">skip_condense</html:code> : A Boolean flag
    to bypass the condensation step <html:span class="No-Break">if desired</html:span></html:li>
    <html:li><html:code class="literal">node_postprocessors</html:code> : An optional
    list of <html:code class="literal">BaseNodePostprocessors</html:code> for additional
    processing of <html:span class="No-Break">retrieved nodes</html:span></html:li>
    <html:li><html:code class="literal">callback_manager</html:code> : As usual, this
    can be used for <html:span class="No-Break">managing callbacks</html:span></html:li>
    <html:li><html:code class="literal">verbose</html:code> : A Boolean flag for enabling
    verbose logging <html:span class="No-Break">during operation</html:span></html:li></html:ul>
    <html:p>To build this <html:a id="_idIndexMarker828"></html:a>particular chat
    engine from an index, we can use the <html:span class="No-Break">following command:</html:span></html:p>
    <html:p>This chat mode is ideal in scenarios where both the context of the conversation
    and specific information from the indexed data are crucial for generating accurate
    and relevant responses. It enhances the chat experience by ensuring the responses
    are both contextually relevant and enriched with specific details from the <html:span
    class="No-Break">indexed content.</html:span></html:p> <html:p>OK. It’s time to
    discover the more advanced <html:span class="No-Break">chat modes.</html:span></html:p>
    <html:a id="_idTextAnchor185"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Implementing
    agentic strategies in our apps</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-186">Implementing agentic strategies in our apps</html:h1> <html:div
    id="_idContainer094">from llama_index.core.tools import FunctionTool def calculate_average(*values):
        """     Calculates the average of the provided values.     """     return
    sum(values) / len(values) average_tool = FunctionTool.from_defaults(     fn=calculate_average)
    pip install llama-index-tools-database from llama_index.tools.database import
    DatabaseToolSpec db_tools = DatabaseToolSpec(<db_specific_configuration>) tool_list
    = db_tools.to_tool_list() pip install llama-index-agent-openai from llama_index.tools.database
    import DatabaseToolSpec from llama_index.core.tools import FunctionTool from llama_index.agent.openai
    import OpenAIAgent from llama_index.llms.openai import OpenAI def write_text_to_file(text,
    filename):     """     Writes the text to a file with the specified filename.
        Args:         text (str): The text to be written to the file.         filename
    (str): File name to write the text into.     Returns: None     """     with open(filename,
    ''w'') as file:         file.write(text) save_tool = FunctionTool.from_defaults(fn=write_text_to_file)
    db_tools = DatabaseToolSpec(uri="sqlite:///files//database//employees.db") tools
    = [save_tool]+db_tools.to_tool_list() llm = OpenAI(model="gpt-4") agent = OpenAIAgent.from_tools(
        tools=tools,     llm=llm,     verbose=True,     max_function_calls=20 ) response
    = agent.chat(     "For each IT department employee with a salary lower "     "than
    the average organization salary, write an email,"     "announcing a 10% raise
    and then save all emails into "     "a file called ''emails.txt''") print(response)
    from llama_index.agent.react import ReActAgent agent = ReActAgent.from_tools(tools)
    from llama_index.core.tools.tool_spec.load_and_search.base import (     LoadAndSearchToolSpec)
    from llama_index.tools.database import DatabaseToolSpec from llama_index.agent.openai
    import OpenAIAgent from llama_index.llms.openai import OpenAI db_tools = DatabaseToolSpec(
        uri="sqlite:///files//database//employees.db") tool_list = db_tools.to_tool_list()
    tools=LoadAndSearchToolSpec.from_defaults( tool_list[0] ).to_tool_list() llm =
    OpenAI(model="gpt-4") agent = OpenAIAgent.from_tools(     tools=tools,     llm=llm,
        verbose=True ) response = agent.chat(     "Who has the highest salary in the
    Employees table?''") print(response) pip install llama-index-readers-wikipedia
    from llama_index.agent.openai import OpenAIAgent from llama_index.core.tools.ondemand_loader_tool
    import(     OnDemandLoaderTool) from llama_index.readers.wikipedia import WikipediaReader
    tool = OnDemandLoaderTool.from_defaults(     WikipediaReader(),     name="WikipediaReader",
        description="args: {''pages'': [<list of pages>],         ''query_str'': <query>}"
    ) agent = OpenAIAgent.from_tools(     tools=[tool],     verbose=True ) response
    = agent.chat(     "What were some famous buildings in ancient Rome?") print(response)
    pip install llama-index-packs-agents-llm-compiler from llama_index.tools.database
    import DatabaseToolSpec from llama_index.packs.agents_llm_compiler import LLMCompilerAgentPack
    db_tools = DatabaseToolSpec(     uri="sqlite:///files//database//employees.db")
    agent = LLMCompilerAgentPack(db_tools.to_tool_list()) response = agent.run(     "Using
    only the available tools, "     "List the HR department employee "     "with the
    highest salary " ) from llama_index.core.agent import AgentRunner from llama_index.agent.openai
    import OpenAIAgentWorker from llama_index.tools.database import DatabaseToolSpec
    db_tools = DatabaseToolSpec(     uri="sqlite:///files//database//employees.db"
    ) tools = db_tools.to_tool_list() step_engine = OpenAIAgentWorker.from_tools(
        tools,     verbose=True ) agent = AgentRunner(step_engine) input =  (     "Find
    the highest paid HR employee and write "     "them an email announcing a bonus"
    ) response = agent.chat(input) print(response) task = agent.create_task(input)
    step_output = agent.run_step(task.task_id) while not step_output.is_last:     step_output
    = agent.run_step(task.task_id) response = agent.finalize_response(task.task_id)
    print(response) <html:p><html:em class="italic">The name is Bot.</html:em> <html:span
    class="No-Break"><html:em class="italic">Chat Bot</html:em></html:span> <html:span
    class="No-Break">.</html:span></html:p> <html:p>At the <html:a id="_idIndexMarker829"></html:a>beginning
    of this chapter, we talked about the growing popularity of the ChatOps model.
    This model is based on the interaction between groups of human operators and AI
    agents, who can understand the context of discussions to provide answers to questions
    but also to perform certain functions, thus playing the role of virtual assistants
    for the group <html:span class="No-Break">they serve.</html:span></html:p> <html:p>You
    probably realize, however, that the chat engine models we have discussed so far
    can only answer questions and cannot execute functions or interact in ways other
    than read-only with <html:span class="No-Break">backend data.</html:span></html:p>
    <html:p>For these use cases, we <html:a id="_idIndexMarker830"></html:a><html:span
    class="No-Break">need</html:span> <html:span class="No-Break"><html:strong class="bold">agents</html:strong></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>The major difference
    between an agent and a simple chat engine is that an agent operates based <html:a
    id="_idIndexMarker831"></html:a>on a <html:strong class="bold">reasoning loop</html:strong>
    and has several tools at its disposal. After all, who would be Bond without the
    gadgets that Q <html:span class="No-Break">always provides?</html:span></html:p>
    <html:p>Unlike a simple chatbot, which can – at best – answer questions, either
    directly with the help of an LLM or by extracting proprietary data from a knowledge
    base, agents are much more powerful and can handle far more complex scenarios.
    This gives them a lot more utility in a business context, where human interactions
    augmented by AI are becoming <html:span class="No-Break">increasingly prevalent.</html:span></html:p>
    <html:p>Let’s understand the core components of an agent: the tools and the <html:span
    class="No-Break">reasoning loop.</html:span></html:p> <html:a id="_idTextAnchor186"></html:a><html:h2
    id="_idParaDest-187">Building tools and ToolSpec classes for our agents</html:h2>
    <html:p>We <html:a id="_idIndexMarker832"></html:a>briefly discussed tools in
    <html:a><html:span class="No-Break"><html:em class="italic">Chapter 6</html:em></html:span></html:a>
    , <html:em class="italic">Querying Our Data, Part 1 – Context Retrieval.</html:em>
    However, because the main topic of <html:a><html:span class="No-Break"><html:em
    class="italic">Chapter 6</html:em></html:span></html:a> was data querying, I only
    showed you how different query engines or retrievers can be wrapped in tools and
    then become components of a router. In many ways, you can think of a router as
    a very simple type of agent. It uses LLM reasoning to decide which query engine
    or retriever should be used, depending on their specified purpose and the actual
    <html:span class="No-Break">user query.</html:span></html:p> <html:p>But tools
    can be a lot <html:span class="No-Break">more useful.</html:span></html:p> <html:p>A
    tool can also be a wrapper for any kind of user-defined function, capable of reading
    or writing data, calling functions from external APIs, or executing any kind of
    code. This means that tools come in two <html:span class="No-Break">different
    flavors:</html:span></html:p> <html:ul><html:li><html:code class="literal">QueryEngineTool</html:code>
    : This <html:a id="_idIndexMarker833"></html:a>can encapsulate any existing query
    engine. This is the kind we covered during <html:a><html:span class="No-Break"><html:em
    class="italic">Chapter 6</html:em></html:span></html:a> and it can only provide
    read-only access to <html:span class="No-Break">our data</html:span></html:li>
    <html:li><html:code class="literal">FunctionTool</html:code> : This <html:a id="_idIndexMarker834"></html:a>enables
    any user-defined function to be transformed into a tool. This is a universal type
    of tool as it allows any type of operation to <html:span class="No-Break">be executed</html:span></html:li></html:ul>
    <html:p>Because we have already seen examples of how <html:code class="literal">QueryEngineTool</html:code>
    works, let’s focus on <html:span class="No-Break"><html:code class="literal">FunctionTool</html:code></html:span>
    <html:span class="No-Break">instead.</html:span></html:p> <html:p>Here’s an example
    of how we can <html:span class="No-Break">define one:</html:span></html:p> <html:p>To
    <html:a id="_idIndexMarker835"></html:a>enable agents to assimilate our functions
    as tools, they must contain descriptive docstrings, just like in the previous
    example. LlamaIndex relies on <html:a id="_idIndexMarker836"></html:a>these <html:strong
    class="bold">docstrings</html:strong> to provide agents with an <html:em class="italic">understanding</html:em>
    of the purpose and proper usage of a particular tool wrapping a <html:span class="No-Break">user-defined
    function.</html:span></html:p> <html:p class="callout-heading">Definition</html:p>
    <html:p class="callout">In Python, a docstring is a string literal that occurs
    as the first statement in a module, function, class, or method definition. It
    is used to document the purpose and usage of the code block it describes. Docstrings
    can be accessed from the code at runtime using the <html:code class="literal">__doc__</html:code>
    attribute on the object they describe, and they are also the primary way that
    documentation is generated <html:span class="No-Break">in Python.</html:span></html:p>
    <html:p>This description will be used by the reasoning loop of an agent to determine
    which particular tool is fit for solving a specific task, allowing the agent to
    decide the <html:span class="No-Break">execution path.</html:span></html:p> <html:p>However,
    competent agents are usually able to handle more than just <html:span class="No-Break">one
    tool.</html:span></html:p> <html:p>For this purpose, LlamaIndex also provides
    <html:a id="_idIndexMarker837"></html:a>the <html:code class="literal">ToolSpec</html:code>
    class. Akin to a collection of individual tools, <html:code class="literal">ToolSpec</html:code>
    specifies a full set of tools for a particular service. It’s like equipping our
    agent with a complete API for a particular type <html:span class="No-Break">of
    technology.</html:span></html:p> <html:p>We can build custom <html:code class="literal">ToolSpec</html:code>
    classes but there is also a growing number of them already available on LlamaHub:
    <html:a>https://llamahub.ai/?tab=tools</html:a> . They cover different types of
    service integrations, such as Gmail, Slack, SalesForce, Shopify, and <html:span
    class="No-Break">many others.</html:span></html:p> <html:p class="callout-heading">The
    LlamaHub agent tool repository</html:p> <html:p class="callout">The LlamaHub agent
    tool repository is a key addition to LlamaHub, providing a curated collection
    of tool specs that enable agents to interact with and extend the functionality
    of a range of services. This repository simplifies the agent design process for
    various APIs and includes numerous practical examples in its notebooks for easy
    integration <html:span class="No-Break">and use.</html:span></html:p> <html:p>Let’s
    take <html:a id="_idIndexMarker838"></html:a>the <html:code class="literal">DatabaseToolSpec</html:code>
    class available on LlamaHub as <html:span class="No-Break">an example.</html:span></html:p>
    <html:p>This <html:code class="literal">ToolSpec</html:code> class can be found
    here: <html:a>https://llamahub.ai/l/tools-database?from=tools</html:a> . First,
    let’s have a look at <html:span class="No-Break"><html:em class="italic">Figure
    8</html:em></html:span> <html:em class="italic">.7</html:em> to understand <html:span
    class="No-Break">its structure:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 8.7 – DatabaseToolSpec</html:p> <html:p>Built <html:a id="_idIndexMarker839"></html:a>on
    top of the <html:a id="_idIndexMarker840"></html:a>SQLAlchemy library ( <html:a>https://www.sqlalchemy.org/</html:a>
    ) this tool collection can access many types of databases while providing three
    <html:span class="No-Break">simple tools:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">list_tables</html:code> : A tool that lists the tables in the
    <html:span class="No-Break">database schema</html:span></html:li> <html:li><html:code
    class="literal">describe_tables</html:code> : A tool that describes the schema
    of <html:span class="No-Break">a table</html:span></html:li> <html:li><html:code
    class="literal">load_data</html:code> : A tool that accepts a SQL query as input
    and returns the <html:span class="No-Break">resulting data</html:span></html:li></html:ul>
    <html:p class="callout-heading">Quick note</html:p> <html:p class="callout">SQLAlchemy
    is a powerful and versatile toolkit for Python that allows developers to work
    with various databases, such as Microsoft SQL Server, OracleDB, MySQL, and others,
    in a more Pythonic way, abstracting away many of the complexities of database
    interaction and <html:span class="No-Break">query construction.</html:span></html:p>
    <html:p>Because this is not a LlamaIndex core component but comes as an integration
    package instead, it must be installed in our <html:span class="No-Break">environment
    first:</html:span></html:p> <html:p>Next, to initialize this <html:code class="literal">ToolSpec</html:code>
    , all we have to do is <html:span class="No-Break">import it:</html:span></html:p>
    <html:p>Then, we must configure our database access, <html:span class="No-Break">like
    this:</html:span></html:p> <html:p>Once <html:a id="_idIndexMarker841"></html:a>the
    <html:code class="literal">ToolSpec</html:code> class <html:a id="_idIndexMarker842"></html:a>has
    been built, if we want to initialize an agent with it, we have to convert it into
    a list of tools using the <html:code class="literal">to_tool_list()</html:code>
    method. This is because agents expect a list of tools as <html:span class="No-Break">an
    argument.</html:span></html:p> <html:p>Here’s how we can easily convert the <html:code
    class="literal">ToolSpec</html:code> class into a list of <html:span class="No-Break">tool
    objects:</html:span></html:p> <html:p>At this point, we can pass <html:code class="literal">tool_list</html:code>
    as an argument when initializing any type of agent. Our agent will now be capable
    of <html:em class="italic">understanding</html:em> the schema of the database
    and extracting any required information from its tables. You can find a full example
    of how to use this <html:code class="literal">ToolSpec</html:code> class later
    in this chapter in the <html:em class="italic">OpenAIAgent</html:em> section.
    Next, let’s see how reasoning <html:span class="No-Break">loops work.</html:span></html:p>
    <html:a id="_idTextAnchor187"></html:a><html:h2 id="_idParaDest-188">Understanding
    reasoning loops</html:h2> <html:p>Having so <html:a id="_idIndexMarker843"></html:a>many
    specialized tools already <html:a id="_idIndexMarker844"></html:a>available for
    our agents is a great advantage. But unfortunately, a box full of some of the
    best-quality instruments is not always enough. Our agents also need to know <html:em
    class="italic">when</html:em> to use each of <html:span class="No-Break">these
    tools.</html:span></html:p> <html:p>Specifically, the RAG applications we build
    need to decide – as autonomously as possible – which tool to use, depending on
    the specific user query and the dataset they are operating on. Any hard-coded
    solution will only deliver good results in a limited number of scenarios. This
    is where reasoning loops <html:span class="No-Break">come in.</html:span></html:p>
    <html:p>The reasoning loop is a fundamental aspect of agents, enabling them to
    intelligently decide which tools to use in different scenarios. This aspect is
    important because, in complex, real-world applications, the requirements can vary
    significantly and a static approach would limit the <html:span class="No-Break">agent’s
    effectiveness.</html:span></html:p> <html:p><html:span class="No-Break"><html:em
    class="italic">Figure 8</html:em></html:span> <html:em class="italic">.8</html:em>
    presents a visual representation of the reasoning <html:span class="No-Break">loop
    concept:</html:span></html:p> <html:p class="IMG---Caption" lang="en-US">Figure
    8.8 – The reasoning loop in an agent</html:p> <html:p>The <html:a id="_idIndexMarker845"></html:a>reasoning
    loop is responsible for the <html:a id="_idIndexMarker846"></html:a>decision-making
    process. It evaluates the context, understands the requirements of the task at
    hand, and then selects the appropriate tools from its arsenal to accomplish the
    task. This dynamic approach allows agents to adapt to various scenarios, making
    them versatile <html:span class="No-Break">and efficient.</html:span></html:p>
    <html:p>In LlamaIndex, the implementation of the reasoning loop is tailored to
    the type of agent. For instance, <html:code class="literal">OpenAIAgent</html:code>
    uses the Function API to make decisions, while <html:code class="literal">ReActAgent</html:code>
    relies on chat or text completion endpoints for its <html:span class="No-Break">reasoning
    process.</html:span></html:p> <html:p>This loop is not just about selecting the
    right tool, though; it’s also about determining the sequence in which the tools
    should be used and the specific parameters that should be applied. It’s the brain
    of the agent, orchestrating the tools to work together seamlessly, much like a
    skilled craftsman uses a combination of tools to create something greater than
    the sum of <html:span class="No-Break">its parts.</html:span></html:p> <html:p>This
    ability to intelligently interact with various tools and data sources, and read
    and modify data dynamically, sets agents apart from simpler chat engines and makes
    them invaluable in a business context where adaptability and intelligence <html:span
    class="No-Break">are key.</html:span></html:p> <html:p>The remaining types of
    chat modes that I’m going to describe over the next few pages are not simple chat
    engines but agents at their core. They all operate using a list of tools but implement
    the reasoning loop in <html:span class="No-Break">different ways.</html:span></html:p>
    <html:a id="_idTextAnchor188"></html:a><html:h2 id="_idParaDest-189">OpenAIAgent</html:h2>
    <html:p>This <html:a id="_idIndexMarker847"></html:a>specialized agent leverages
    the capabilities <html:a id="_idIndexMarker848"></html:a>of OpenAI models, particularly
    those supporting the function calling API. It works with OpenAI models that have
    been designed to support the function calling API. They can interpret and execute
    function calls as part of <html:span class="No-Break">their capabilities.</html:span></html:p>
    <html:p class="callout-heading">Quick note</html:p> <html:p class="callout">These
    models are designed to interpret prompts and context to determine when a function
    call is appropriate. They respond with outputs that adhere to the defined structure
    of the function, based on the patterns they’ve learned during training. For more
    information on this topic and a list of supported models, you may consult the
    official OpenAI <html:span class="No-Break">documentation:</html:span> <html:a><html:span
    class="No-Break">https://platform.openai.com/docs/guides/function-calling</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>The key advantage of
    this agent type is that the tool selection logic is implemented directly on the
    model itself. When a task is provided by the user to <html:strong class="bold">OpenAIAgent</html:strong>
    , along with any previous chat history, the function API will analyze the context
    and decide whether another tool needs to be invoked or if a final response can
    be returned. If it determines that another tool is required, the function API
    will output the name of that tool. <html:code class="literal">OpenAIAgent</html:code>
    will then execute the tool, passing the tool’s response back into the chat history.
    This cycle continues until the API returns a final message, indicating the reasoning
    loop <html:span class="No-Break">is complete.</html:span></html:p> <html:p><html:span
    class="No-Break"><html:em class="italic">Figure 8</html:em></html:span> <html:em
    class="italic">.9</html:em> explains this <html:span class="No-Break">process
    visually:</html:span></html:p> <html:p class="IMG---Caption" lang="en-US">Figure
    8.9 – The simplified workflow of OpenAIAgent</html:p> <html:p>With the <html:a
    id="_idIndexMarker849"></html:a>model handling the complex logic of tool <html:a
    id="_idIndexMarker850"></html:a>selection and chaining, <html:code class="literal">OpenAIAgent</html:code>
    is a great solution for tool orchestration. One tradeoff is less flexibility compared
    to other architectures as the tool selection logic is hard-coded into <html:span
    class="No-Break">the LLM.</html:span></html:p> <html:p>However, for many use cases,
    the pre-trained capabilities of the function API model are sufficient to enable
    effective tool orchestration and <html:span class="No-Break">task completion.</html:span></html:p>
    <html:p>Before proceeding to the next example, make sure you install the required
    <html:span class="No-Break">integration package:</html:span></html:p> <html:p>To
    <html:a id="_idIndexMarker851"></html:a>implement OpenAIAgent, we must define
    the available tools and then initialize the agent with these components, adding
    any other custom parameters we desire. The best way to explain how they work is
    through <html:span class="No-Break">an example.</html:span></html:p> <html:p>For
    the following example, we are using an SQLite database containing a single table
    called <html:em class="italic">Employees</html:em> . This table contains some
    randomly chosen salary data for 10 employees from different departments. <html:em
    class="italic">Table 8.1</html:em> displays the contents of the <html:span class="No-Break"><html:em
    class="italic">Employees</html:em></html:span> <html:span class="No-Break">table:</html:span></html:p>
    <html:table class="No-Table-Style _idGenTablePara-1" id="table001-3"><html:thead><html:tr
    class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span class="No-Break"><html:strong
    class="bold">ID</html:strong></html:span></html:p></html:td> <html:td class="No-Table-Style"><html:p><html:span
    class="No-Break"><html:strong class="bold">Name</html:strong></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break"><html:strong
    class="bold">Department</html:strong></html:span></html:p></html:td> <html:td
    class="No-Table-Style"><html:p><html:span class="No-Break"><html:strong class="bold">Salary</html:strong></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break"><html:strong
    class="bold">Email</html:strong></html:span></html:p></html:td></html:tr></html:thead>
    <html:tbody><html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p>1</html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Alice</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">IT</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">36420.77</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Alice_IT@org.com</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p>2</html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Karen</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Finance</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">57705.06</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Alice_Finance@org.com</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p>3</html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Helen</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">IT</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">52612.51</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Helen_IT@org.com</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p>4</html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Jackie</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Finance</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">61374.58</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Jack_Finance@org.com</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p>5</html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">David</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Finance</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">32242.72</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">David_Finance@org.com</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p>6</html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Cora</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">HR</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">62040.53</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Alice_HR@org.com</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p>7</html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Ingrid</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">IT</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">70821.96</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Alice_IT@org.com</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p>8</html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Jack</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">IT</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">57268.89</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Jack_IT@org.com</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p>9</html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Bob</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Finance</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">76868.23</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Bob_Finance@org.com</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break">10</html:span></html:p></html:td> <html:td class="No-Table-Style"><html:p><html:span
    class="No-Break">Bill</html:span></html:p></html:td> <html:td class="No-Table-Style"><html:p><html:span
    class="No-Break">HR</html:span></html:p></html:td> <html:td class="No-Table-Style"><html:p><html:span
    class="No-Break">74161.45</html:span></html:p></html:td> <html:td class="No-Table-Style"><html:p><html:span
    class="No-Break">Bob_HR@org.com</html:span></html:p></html:td></html:tr></html:tbody></html:table>
    <html:p class="IMG---Caption" lang="en-US">Table 8.1 – The sample Employees table
    from the Employees.db file</html:p> <html:p>The <html:a id="_idIndexMarker852"></html:a>database
    file itself can be found in the <html:code class="literal">ch8/files/database</html:code>
    subfolder of this book’s GitHub repository. Let’s have a <html:a id="_idIndexMarker853"></html:a>look
    at <html:span class="No-Break">the code:</html:span></html:p> <html:p>The first
    part is responsible for <html:span class="No-Break">the imports.</html:span></html:p>
    <html:p>Next, it’s time to define a simple function that’s going to become a custom
    tool for our agent. This simple tool will allow us to save files in the local
    folder. Notice the detailed docstring that we are providing to <html:span class="No-Break">the
    agent:</html:span></html:p> <html:p>Once the <html:a id="_idIndexMarker854"></html:a>function
    has been defined, we must wrap it into a new tool <html:span class="No-Break">called</html:span>
    <html:span class="No-Break"><html:code class="literal">save_tool</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>We also <html:a id="_idIndexMarker855"></html:a>initialize
    an entire <html:code class="literal">ToolSpec</html:code> class from the imported
    <html:code class="literal">DatabaseToolSpec</html:code> . We need these tools
    because the agent will have to read data from our SQLite database to solve <html:span
    class="No-Break">the task:</html:span></html:p> <html:p>Once we’ve created <html:code
    class="literal">db_tools</html:code> , we must join it with <html:code class="literal">save_tool</html:code>
    and put them into a single list called <html:code class="literal">tools</html:code>
    . We’ll use this list as an argument for initializing <html:span class="No-Break">the
    agent.</html:span></html:p> <html:p>Now, let’s build our agent. Notice that we’re
    not using the default LLM in this case; instead, we’re configuring our agent to
    use GPT-4 for <html:span class="No-Break">more accuracy:</html:span></html:p>
    <html:p>In the preceding <html:a id="_idIndexMarker856"></html:a>code, we initialized
    our agent using the list of tools we prepared. The <html:code class="literal">verbose</html:code>
    argument will make the agent display every execution step for better visibility
    of <html:a id="_idIndexMarker857"></html:a>the reasoning process. We also set
    <html:code class="literal">max_function_calls</html:code> to a larger value because,
    for complex tasks, the default value may not be enough to allow the agent to complete
    <html:span class="No-Break">the task.</html:span></html:p> <html:p class="callout-heading">A
    quick note on the max_function_calls parameter</html:p> <html:p class="callout">It
    may be <html:a id="_idIndexMarker858"></html:a>tempting to simply set this to
    a very large value to avoid exhausting the function calls and increase the chances
    for the agent to solve the task. Keep in mind, however, that every function call
    incurs costs, and sometimes, agents have the bad habit of entering infinite loops.
    I call them <html:em class="italic">rogue agents</html:em> when <html:a id="_idIndexMarker859"></html:a>they
    do that. Chances are that if your agent implementation requires a lot of LLM calls
    to solve even simple tasks, you’re probably doing something wrong when defining
    or describing the <html:span class="No-Break">underlying tools.</html:span></html:p>
    <html:p>Let’s continue with our code. It’s time to dispatch the task to <html:span
    class="No-Break">our agent:</html:span></html:p> <html:p>As you can see, the task
    we provided is relatively complex. Multiple steps will be required to solve it.
    As we are not providing too many details in the query, our agent will have to
    figure out the structure of the database and then craft a SQL query to extract
    the average salary in the organization and the list of employees from the IT department
    who are paid below <html:span class="No-Break">the average.</html:span></html:p>
    <html:p>Since the <html:code class="literal">verbose</html:code> argument is set
    to <html:code class="literal">True</html:code> , running this sample will show
    you the entire reasoning logic and steps performed by <html:span class="No-Break">the
    agent.</html:span></html:p> <html:p>Notice <html:a id="_idIndexMarker860"></html:a>how,
    in each step, the agent incorporates <html:a id="_idIndexMarker861"></html:a>outputs
    from the tools into its ongoing reasoning process. Once it has the list of employees,
    it will compose an email for each one. The final step of the task is to use our
    custom-created tool and save the results in a <html:span class="No-Break">local
    file.</html:span></html:p> <html:p>This is just a simple example. In a more complex
    implementation, instead of saving the text locally, for example, we <html:a id="_idIndexMarker862"></html:a>could
    import <html:code class="literal">GmailToolSpec</html:code> from LlamaHub and
    create email drafts that can be manually reviewed later and sent by the user.
    Unfortunately, that would have made the example much longer as <html:code class="literal">GmailToolSpec</html:code>
    requires stored credentials for the Google API, but I leave it to you to experiment
    with that <html:code class="literal">ToolSpec</html:code> class ( <html:a>https://llamahub.ai/l/tools-gmail?from=tools</html:a>
    ) and all the other tools available <html:span class="No-Break">on LlamaHub.</html:span></html:p>
    <html:p>The customizable <html:a id="_idIndexMarker863"></html:a>parameters of
    <html:code class="literal">OpenAIAgent</html:code> are <html:span class="No-Break">as
    follows:</html:span></html:p> <html:ul><html:li><html:code class="literal">tools</html:code>
    : A list of <html:code class="literal">BaseTool</html:code> instances that the
    agent can utilize during the chat session. These tools can range from specialized
    query engines to custom processing modules or collections of tools extracted from
    <html:span class="No-Break"><html:code class="literal">ToolSpec</html:code></html:span>
    <html:span class="No-Break">classes</html:span></html:li> <html:li><html:code
    class="literal">llm</html:code> : Any OpenAI model that supports the function
    calling API. The default model that’s used <html:span class="No-Break">is</html:span>
    <html:span class="No-Break"><html:code class="literal">gpt-3.5-turbo-0613</html:code></html:span></html:li>
    <html:li><html:code class="literal">memory</html:code> : Just like with any chat
    engine, this is a <html:code class="literal">ChatMemoryBuffer</html:code> instance
    that can be used for storing and managing the <html:span class="No-Break">chat
    history</html:span></html:li> <html:li><html:code class="literal">prefix_messages</html:code>
    : A list of <html:code class="literal">ChatMessage</html:code> instances that
    serve as pre-configured messages or prompts at the start of the <html:span class="No-Break">chat
    session</html:span></html:li> <html:li><html:code class="literal">max_function_calls</html:code>
    : The maximum number of function calls that can be made to the OpenAI model during
    a single chat interaction. The default <html:span class="No-Break">is 5</html:span></html:li>
    <html:li><html:code class="literal">default_tool_choice</html:code> : A string
    indicating the default choice of tool to be used when multiple tools are available.
    This is useful for coercing the agent into using a <html:span class="No-Break">specific
    tool</html:span></html:li> <html:li><html:code class="literal">callback_manager</html:code>
    : An optional <html:code class="literal">CallbackManager</html:code> instance
    for managing callbacks <html:a id="_idIndexMarker864"></html:a>during the chat
    process, aiding in tracing, <html:span class="No-Break">and debugging</html:span></html:li>
    <html:li><html:code class="literal">system_prompt</html:code> : An optional initial
    system prompt that provides context or instructions for <html:span class="No-Break">the
    agent</html:span></html:li> <html:li><html:code class="literal">verbose</html:code>
    : A Boolean flag to enable detailed logging <html:span class="No-Break">during
    operation</html:span></html:li></html:ul> <html:p>Overall, <html:code class="literal">OpenAIAgent</html:code>
    stands <html:a id="_idIndexMarker865"></html:a>out from other chat engines due
    to its ability to execute complex function calls, on top of contextually rich
    conversations. This makes it particularly suitable for scenarios where advanced
    functionalities, such as integrating external tools or processing user queries
    in more sophisticated ways, are required. <html:code class="literal">OpenAIAgent</html:code>
    provides a versatile and powerful platform for creating engaging and intelligent
    <html:span class="No-Break">chat experiences.</html:span></html:p> <html:p>But
    wait – there are other types of <html:span class="No-Break">agents too.</html:span></html:p>
    <html:a id="_idTextAnchor189"></html:a><html:h2 id="_idParaDest-190">ReActAgent</html:h2>
    <html:p>In contrast <html:a id="_idIndexMarker866"></html:a>to <html:code class="literal">OpenAIAgent</html:code>
    , <html:strong class="bold">ReActAgent</html:strong> uses <html:a id="_idIndexMarker867"></html:a>more
    generic text completion endpoints that can be driven by any LLM. It operates based
    on a <html:strong class="bold">ReAct</html:strong> loop within a chat mode built
    on top of a set <html:span class="No-Break">of tools.</html:span></html:p> <html:p>This
    loop <html:a id="_idIndexMarker868"></html:a>involves deciding whether to use
    any of the available tools, potentially using it and observing its output, and
    then deciding whether to repeat the process or provide a final response. This
    flexibility allows it to choose between using tools or relying solely on the LLM.
    However, this also means that its performance is heavily dependent on the quality
    of the LLM, often requiring more nuanced prompting to ensure accurate knowledge
    base queries, rather than relying on potentially inaccurate <html:span class="No-Break">model-generated
    responses.</html:span></html:p> <html:p>The input prompt for <html:code class="literal">ReActAgent</html:code>
    is carefully designed to guide the model in tool selection, using a format inspired
    by the ReAct paper by Yao, S., et al. (2022), <html:em class="italic">ReAct: Synergizing
    Reasoning and Acting in Language</html:em> <html:span class="No-Break"><html:em
    class="italic">Models</html:em></html:span> <html:span class="No-Break">(</html:span>
    <html:a><html:span class="No-Break">https://arxiv.org/abs/2210.03629</html:span></html:a>
    <html:span class="No-Break">).</html:span></html:p> <html:p>It presents a list
    of available tools and asks the model to select one and provide the required parameters
    in JSON format. This explicit prompt is critical to the agent’s decision-making
    process. After selecting a tool, the agent executes it and integrates the response
    into <html:a id="_idIndexMarker869"></html:a>the chat history. This cycle of prompting,
    execution, and response integration continues until a satisfactory response is
    achieved. For an overall visual representation of the workflow, you may review
    the diagram that was presented for <html:code class="literal">OpenAIAgent</html:code>
    in <html:span class="No-Break"><html:em class="italic">Figure 8</html:em></html:span>
    <html:span class="No-Break"><html:em class="italic">.9</html:em></html:span> <html:span
    class="No-Break">.</html:span></html:p> <html:p>Unlike <html:code class="literal">OpenAIAgent</html:code>
    , which uses a function calling API with a model capable of selecting and chaining
    together multiple tools, the <html:code class="literal">ReActAgent</html:code>
    class’s logic must be fully encoded through <html:span class="No-Break">its prompts.</html:span></html:p>
    <html:p><html:code class="literal">ReActAgent</html:code> uses <html:a id="_idIndexMarker870"></html:a>a
    predefined loop with a maximum number of iterations, along with strategic prompting,
    to mimic a reasoning loop. Nevertheless, with strategic prompt engineering, <html:code
    class="literal">ReActAgent</html:code> can achieve effective tool orchestration
    and chained execution, similar to the output of the OpenAI <html:span class="No-Break">Function
    API.</html:span></html:p> <html:p>The key difference is that whereas the logic
    of the OpenAI Function API is embedded in the model, <html:code class="literal">ReActAgent</html:code>
    relies on the structure of its prompts to induce the desired tool selection behavior.
    This approach offers considerable flexibility as it can adapt to different language
    model backends, allowing for different implementations <html:span class="No-Break">and
    applications.</html:span></html:p> <html:p>In this case, we have the usual customizable
    parameters that we discussed for <html:code class="literal">OpenAIAgent</html:code>
    : <html:code class="literal">tools</html:code> , <html:code class="literal">llm</html:code>
    , <html:code class="literal">memory</html:code> , <html:code class="literal">callback_manager</html:code>
    , <html:span class="No-Break">and</html:span> <html:span class="No-Break"><html:code
    class="literal">verbose</html:code></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p>In addition, <html:code class="literal">ReActAgent</html:code> comes with
    <html:a id="_idIndexMarker871"></html:a>a few <html:span class="No-Break">specific
    parameters:</html:span></html:p> <html:ul><html:li><html:code class="literal">max_iterations</html:code>
    : Similar to <html:code class="literal">max_function_calls</html:code> , this
    parameter sets the maximum number of iterations the ReAct loop can execute. This
    limit ensures that the agent does not enter an endless loop <html:span class="No-Break">of
    processing</html:span></html:li> <html:li><html:code class="literal">react_chat_formatter</html:code>
    : This formats the chat history into a structured list of <html:code class="literal">ChatMessages</html:code>
    , alternating between user and assistant roles, based on the provided tools, chat
    history, and reasoning steps. This helps maintain clarity and consistency in the
    <html:span class="No-Break">reasoning loop</html:span></html:li> <html:li><html:code
    class="literal">output_parser</html:code> : An optional instance of the <html:code
    class="literal">ReActOutputParser</html:code> class. This parser processes the
    outputs generated by the agent, helping in interpreting, and formatting <html:span
    class="No-Break">them appropriately</html:span></html:li> <html:li><html:code
    class="literal">tool_retriever</html:code> : An optional instance of <html:code
    class="literal">ObjectRetriever</html:code> for <html:code class="literal">BaseTool</html:code>
    . This <html:a id="_idIndexMarker872"></html:a>retriever can be used to dynamically
    fetch tools based on certain criteria. Similar to how we index nodes, there is
    also an option to create an <html:code class="literal">ObjectIndex</html:code>
    index to index a set of tools. This can be especially useful when we have to work
    with a large number of tools. You can find more information about this feature
    in the official <html:span class="No-Break">documentation:</html:span> <html:a><html:span
    class="No-Break">https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/usage_pattern.html#function-retrieval-agents</html:span></html:a></html:li>
    <html:li><html:code class="literal">context</html:code> : An optional string providing
    initial instructions for <html:span class="No-Break">the agent</html:span></html:li></html:ul>
    <html:p>Initializing <html:a id="_idIndexMarker873"></html:a>and using <html:code
    class="literal">ReActAgent</html:code> is done the same as with the OpenAI one,
    except this time, you won’t need to install any integration packages first – this
    type of agent is part of the core <html:span class="No-Break">LlamaIndex components:</html:span></html:p>
    <html:p>Overall, <html:code class="literal">ReActAgent</html:code> stands out
    for its flexibility as it can use any LLM to drive its unique ReAct loop, enabling
    it to smartly choose and use various tools. It’s like having a virtual assistant
    that not only answers questions but also intelligently decides when to consult
    external sources, making the conversation more contextually relevant and improving
    the <html:span class="No-Break">user experience.</html:span></html:p> <html:a
    id="_idTextAnchor190"></html:a><html:h2 id="_idParaDest-191">How do we interact
    with agents?</html:h2> <html:p>There <html:a id="_idIndexMarker874"></html:a>are
    two main methods that we can use to interact with an agent: <html:code class="literal">chat()</html:code>
    and <html:code class="literal">query()</html:code> . The first method utilizes
    stored conversation history to provide context-informed responses, making it <html:a
    id="_idIndexMarker875"></html:a>suitable for <html:span class="No-Break">ongoing
    dialogues.</html:span></html:p> <html:p>On the other hand, the former method operates
    in a stateless mode, treating each call independently without reference to past
    interactions. This is better suited for <html:span class="No-Break">standalone
    requests.</html:span></html:p> <html:a id="_idTextAnchor191"></html:a><html:h2
    id="_idParaDest-192">Enhancing our agents with the help of utility tools</html:h2>
    <html:p>To improve <html:a id="_idIndexMarker876"></html:a>the capabilities of
    the existing tools, LlamaIndex also provides two very useful so-called <html:em
    class="italic">utility tools</html:em> – <html:code class="literal">OnDemandLoaderTool</html:code>
    and <html:code class="literal">LoadAndSearchToolSpec</html:code> . They are universal
    and can be used with any type of agent to augment the standard tool functionality
    in <html:span class="No-Break">certain scenarios.</html:span></html:p> <html:p>One
    common <html:a id="_idIndexMarker877"></html:a>issue when interacting with an
    API is that we might receive a very long response in return. Our agents may not
    always be able to handle such <html:span class="No-Break">large outputs.</html:span></html:p>
    <html:p>Problems may arise because they may overflow the context window of the
    LLM or sometimes, key context may be diluted by a large amount of data, decreasing
    the accuracy of the agent’s <html:span class="No-Break">reasoning logic.</html:span></html:p>
    <html:p>A good way to understand this issue is by looking at our previous example
    for <html:code class="literal">OpenAIAgent</html:code> . In that case, we used
    a collection of tools called <html:code class="literal">DatabaseToolSpec</html:code>
    to retrieve data from our sample <html:em class="italic">Employees</html:em> table.
    If you’ve run that particular agent with the <html:code class="literal">Verbose</html:code>
    parameter set to <html:code class="literal">True</html:code> , then you’ve probably
    noticed that the outputs produced by the <html:code class="literal">load_data</html:code>
    tool are in the form of LlamaIndex document objects, as we can see in <html:span
    class="No-Break"><html:em class="italic">Figure 8</html:em></html:span> <html:span
    class="No-Break"><html:em class="italic">.10</html:em></html:span> <html:span
    class="No-Break">:</html:span></html:p> <html:p class="IMG---Caption" lang="en-US">Figure
    8.10 – Sample output for the OpenAIAgent code example</html:p> <html:p>This means
    that whenever the agent calls the <html:code class="literal">load_data</html:code>
    tool, using a SQL query to interrogate the database, instead of simply receiving
    the output of the query, it gets a whole document <html:a id="_idIndexMarker878"></html:a>in
    return – along with a bunch of additional data, such as the ID of the document,
    metadata fields, hashes, and so on. The agent has to extract the actual query
    results from that data using the LLM, hence the aforementioned <html:span class="No-Break">potential
    issues.</html:span></html:p> <html:p>So, what if we want to extract <html:em class="italic">only</html:em>
    the result of the query, without all the additional data on top of it? That is
    the job <html:span class="No-Break">of</html:span> <html:span class="No-Break"><html:code
    class="literal">LoadAndSearchToolSpec</html:code></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:h3>Understanding the LoadAndSearchToolSpec utility</html:h3> <html:p>This
    <html:a id="_idIndexMarker879"></html:a>utility tool is designed to help the <html:a
    id="_idIndexMarker880"></html:a>agent handle large volumes of data from API endpoints,
    as demonstrated in <html:span class="No-Break"><html:em class="italic">Figure
    8</html:em></html:span> <html:span class="No-Break"><html:em class="italic">.11</html:em></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 8.11 – Visualization of a direct API call versus interaction
    via LoadAndSearchToolSpec</html:p> <html:p>It takes an existing tool and generates
    two separate tools: one for loading and indexing data – by default, using a vector
    index – and another for conducting searches on this indexed data. The agent will
    now use the <html:em class="italic">Load</html:em> tool to ingest the data, and,
    similar to a caching mechanism, it will store it in an index. In the next step,
    the agent will use the <html:em class="italic">Search</html:em> tool to extract
    only the needed information using a built-in <html:span class="No-Break">query
    engine.</html:span></html:p> <html:p>Let’s see how that translates into code.
    We will adapt the previous <html:code class="literal">OpenAIAgent</html:code>
    example so that it <html:span class="No-Break">uses</html:span> <html:span class="No-Break"><html:code
    class="literal">LoadAndSearchToolSpec</html:code></html:span> <html:span class="No-Break">:</html:span></html:p>
    <html:p>Once we <html:a id="_idIndexMarker881"></html:a>finished with the imports,
    we <html:a id="_idIndexMarker882"></html:a>initialized our <html:code class="literal">DatabaseToolSpec</html:code>
    utility, which points to the same sample SQLite database as in the previous example.
    However, this time, we didn’t add any additional tools since we’ll only run a
    simple query. For that reason, we only pass the first tool from <html:code class="literal">ToolSpec</html:code>
    – that is, <html:code class="literal">tool_list[0]</html:code> – as an argument
    to <html:code class="literal">LoadAndSearchToolSpec</html:code> . That’s the <html:code
    class="literal">load_data</html:code> function, by the way. We don’t need the
    other two functions available in the database’s <html:code class="literal">ToolSpec</html:code>
    <html:span class="No-Break">this time.</html:span></html:p> <html:p>From this
    point on, the code is very <html:span class="No-Break">much straightforward:</html:span></html:p>
    <html:p>If you look at the output – presented in <html:span class="No-Break"><html:em
    class="italic">Figure 8</html:em></html:span> <html:em class="italic">.12</html:em>
    – you’ll notice the reduced amount of data the agent has to deal with <html:span
    class="No-Break">this time:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 8.12 – Sample agent output when LoadAndSearchToolSpec is used</html:p>
    <html:p>Instead <html:a id="_idIndexMarker883"></html:a>of receiving an entire
    document <html:a id="_idIndexMarker884"></html:a>as a response, the first call
    returns just a confirmation message that the data has been loaded and indexed,
    while the second extracts the final response using a query. We’ll talk about another
    utility <html:span class="No-Break">tool next.</html:span></html:p> <html:h3>Understanding
    OnDemandLoaderTool</html:h3> <html:p>Another <html:a id="_idIndexMarker885"></html:a>important
    utility is <html:code class="literal">OnDemandLoaderTool</html:code> . This <html:a
    id="_idIndexMarker886"></html:a>utility is designed to make the process of loading,
    indexing, and querying data seamless and efficient within an agent’s workflow,
    particularly when dealing with large volumes of data from <html:span class="No-Break">various
    sources.</html:span></html:p> <html:p>It simplifies the process of using data
    loaders for agents by allowing them to trigger the loading, indexing, and querying
    of data through a single <html:span class="No-Break">tool call.</html:span></html:p>
    <html:p>The normal approach in a RAG workflow would be to ingest all data at the
    start of our application, then chunk it, index it, and build a query engine on
    it. But that may not always be the most <html:span class="No-Break">efficient
    method.</html:span></html:p> <html:p>Let’s say we have a large number of data
    sources. Ingesting and indexing all of them during startup would take a very long
    time, negatively affecting the user experience. And what if the user asks a question
    that cannot be answered by the agent based on the ingested data sources alone?
    That’s where a feature like this <html:span class="No-Break">becomes useful.</html:span></html:p>
    <html:p><html:code class="literal">OnDemandLoaderTool</html:code> is especially
    useful in scenarios where data requirements are dynamic and unpredictable. Instead
    of pre-loading a vast amount of data at startup, which may not all be relevant
    to the user’s current needs, this tool enables an agent to fetch, index, and query
    data on demand. This approach significantly enhances efficiency as it allows the
    agent to focus only on the relevant data at any given time, rather than handling
    large datasets that may not be <html:span class="No-Break">immediately necessary.</html:span></html:p>
    <html:p>How does it work? It <html:a id="_idIndexMarker887"></html:a>takes any
    existing data loader and wraps it into a tool that can be used by the <html:a
    id="_idIndexMarker888"></html:a>agent as required. Before running the code, make
    sure you install the Wikipedia <html:span class="No-Break">integration package:</html:span></html:p>
    <html:p>Here’s the sample code. We’ll start with <html:span class="No-Break">the
    imports:</html:span></html:p> <html:p>Next, let’s define an on-demand tool for
    our agent, based <html:span class="No-Break">on</html:span> <html:span class="No-Break"><html:code
    class="literal">WikipediaReader</html:code></html:span> <html:span class="No-Break">:</html:span></html:p>
    <html:p>Notice how I provided usage instructions in the description argument.
    These should help the agent better <html:em class="italic">understand</html:em>
    how to properly use the tool, although it might still take a few tries to get
    it right. Now, it’s time to initialize <html:span class="No-Break">the agent:</html:span></html:p>
    <html:p class="callout-heading">Important side note</html:p> <html:p class="callout">One
    big advantage of using this approach is that once data has been loaded into the
    index, it’s also cached. Therefore, subsequent queries on the same topic will
    <html:span class="No-Break">run faster.</html:span></html:p> <html:p>In <html:a
    id="_idIndexMarker889"></html:a>addition, <html:code class="literal">OnDemandLoaderTool</html:code>
    can <html:a id="_idIndexMarker890"></html:a>be chained together with other, regular
    tools, allowing the agent to handle more <html:span class="No-Break">complex scenarios.</html:span></html:p>
    <html:p>With that, we’ve covered the basics. Now, let’s have a look at more advanced
    types <html:span class="No-Break">of agents.</html:span></html:p> <html:a id="_idTextAnchor192"></html:a><html:h2
    id="_idParaDest-193">Using the LLMCompiler agent for more advanced scenarios</html:h2>
    <html:p>I saved the best <html:span class="No-Break">for last.</html:span></html:p>
    <html:p>While <html:a id="_idIndexMarker891"></html:a>they tend to perform well
    in many scenarios, both OpenAI and ReAct agents have <html:a id="_idIndexMarker892"></html:a>some
    drawbacks. Because current LLMs are not very good at long-term planning, they
    can sometimes get stuck in an infinite loop without finding the desired solution.
    At other times, their attention can be distracted by certain outputs they receive
    during execution, and this can cause them to stop before solving the <html:span
    class="No-Break">given task.</html:span></html:p> <html:p>But probably the biggest
    drawback of these types of agents is their serialized way of working. In other
    words, the execution of the steps is done in sequence. These agents wait for the
    output generated by one step to trigger the next step. This is a very inefficient
    approach in many practical scenarios. Often, a series of steps can be executed
    in parallel, significantly improving application performance and user experience.
    Based on these premises, I will now present a more advanced form <html:span class="No-Break">of
    agent.</html:span></html:p> <html:p>Inspired by the paper by Kim, S., et al. (2023),
    <html:em class="italic">An LLM Compiler for Parallel Function Calling</html:em>
    ( <html:a>https://arxiv.org/abs/2312.04511</html:a> ), this agent implementation
    offers outstanding performance and scalability. The concept is based on the ability
    of LLMs to execute multiple functions in parallel and draws inspiration from classical
    compilers to efficiently orchestrate <html:span class="No-Break">multi-function
    execution.</html:span></html:p> <html:p>The <html:strong class="bold">LLMCompiler
    agent</html:strong> orchestrates these parallel function calls using a three-part
    system that plans, dispatches, and executes tasks, resulting in faster and more
    accurate multi-function calls compared to sequential methods. Just as compilers
    transform and optimize <html:a id="_idIndexMarker893"></html:a>code to run efficiently,
    LLMCompiler transforms natural language queries into optimized sequences of function
    calls that can be executed in parallel when dependencies allow. This makes calling
    multiple tools with LLMs faster, cheaper, and potentially more accurate. An additional
    advantage is that it works with any kind of LLM, including both open source and
    closed <html:span class="No-Break">source models.</html:span></html:p> <html:p>Under
    the hood, an LLMCompileraAgent has three <html:span class="No-Break">main components:</html:span></html:p>
    <html:ul><html:li><html:strong class="bold">LLM planner</html:strong> : Formulates
    <html:a id="_idIndexMarker894"></html:a>execution strategies and dependencies
    from user input <html:span class="No-Break">and examples</html:span></html:li>
    <html:li><html:strong class="bold">Task-fetching unit</html:strong> : Sends <html:a
    id="_idIndexMarker895"></html:a>and updates function-calling tasks based on <html:span
    class="No-Break">the dependencies</html:span></html:li> <html:li><html:strong
    class="bold">Executor</html:strong> : Executes <html:a id="_idIndexMarker896"></html:a>tasks
    in parallel using <html:span class="No-Break">associated tools</html:span></html:li></html:ul>
    <html:p><html:span class="No-Break"><html:em class="italic">Figure 8</html:em></html:span>
    <html:em class="italic">.13</html:em> explains <html:a id="_idIndexMarker897"></html:a>the
    structure of the LLMCompiler <html:span class="No-Break">agent visually:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 8.13 – An overview of the LLMCompiler
    agent’s architecture</html:p> <html:p>The <html:em class="italic">LLM planner</html:em>
    determines the order of function calls and their interdependencies according to
    <html:a id="_idIndexMarker898"></html:a>user input. Next, the <html:em class="italic">task-fetching
    unit</html:em> initiates parallel execution of these functions, replacing variables
    with the outputs from prior tasks. The <html:em class="italic">executor</html:em>
    then carries out these function calls with the relevant tools. Combined, these
    elements enhance the efficiency of parallel function calling <html:span class="No-Break">in
    LLMs.</html:span></html:p> <html:p>The <html:strong class="bold">directed acyclic
    graph</html:strong> ( <html:strong class="bold">DAG</html:strong> ) of <html:a
    id="_idIndexMarker899"></html:a>tasks is a key data structure created by the <html:em
    class="italic">LLM planner</html:em> from user inputs and examples. This planning
    graph captures task dependencies <html:a id="_idIndexMarker900"></html:a>and enables
    optimized parallel <html:span class="No-Break">execution (</html:span> <html:a><html:span
    class="No-Break">https://en.wikipedia.org/wiki/Directed_acyclic_graph</html:span></html:a>
    <html:span class="No-Break">).</html:span></html:p> <html:p>The DAG facilitates
    the simultaneous execution of tasks that do not depend on each other. Should one
    task rely on the completion of another, the prerequisite task must finish before
    the dependent task can commence. Independent tasks, on the other hand, are capable
    of being executed concurrently without any <html:span class="No-Break">dependency
    constraints.</html:span></html:p> <html:p class="callout-heading">Quick note</html:p>
    <html:p class="callout">While OpenAI has already introduced parallel function
    calling into their API, the LLMCompiler is still superior in its approach because
    it manifests fault tolerance in case of wrong LLM decisions and can replan, depending
    on the <html:span class="No-Break">outputs generated.</html:span></html:p> <html:p>To
    understand how we can <html:a id="_idIndexMarker901"></html:a>implement an agent
    using the LLMCompiler, let’s have a look at a simple example. But first, to run
    the example, you’ll need to install the necessary <html:span class="No-Break">integration
    package:</html:span></html:p> <html:p>Here’s <html:span class="No-Break">the code:</html:span></html:p>
    <html:p>After importing <html:code class="literal">LLMCompilerAgentPack</html:code>
    and <html:code class="literal">DatabaseToolSpec</html:code> , we initialized the
    <html:a id="_idIndexMarker902"></html:a>database tools and used the tool list
    to initialize the agent. It’s now time to interact with the agent, this time using
    the <html:span class="No-Break"><html:code class="literal">run()</html:code></html:span>
    <html:span class="No-Break">method:</html:span></html:p> <html:p><html:span class="No-Break"><html:em
    class="italic">Figure 8</html:em></html:span> <html:em class="italic">.14</html:em>
    shows the output of the <html:span class="No-Break">preceding code:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 8.14 – Sample output of the
    LLMCompiler agent</html:p> <html:p>Looking <html:a id="_idIndexMarker903"></html:a>at
    the output, we can see both the execution plan generated by the agent and the
    actual steps performed. Quite neat, <html:span class="No-Break">isn’t it?</html:span></html:p>
    <html:p>In conclusion, LLMCompiler-based agents represent a leap forward in addressing
    the limitations of serial execution found in traditional agents, pushing the boundaries
    of what’s possible in terms of chatbot implementations and <html:span class="No-Break">user
    interaction.</html:span></html:p> <html:a id="_idTextAnchor193"></html:a><html:h2
    id="_idParaDest-194">Using the low-level Agent Protocol API</html:h2> <html:p>Taking
    <html:a id="_idIndexMarker904"></html:a>inspiration from <html:a id="_idIndexMarker905"></html:a>the
    <html:strong class="bold">Agent Protocol</html:strong> ( <html:a>https://agentprotocol.ai/</html:a>
    ) and several research papers, the LlamaIndex community also created a <html:a
    id="_idIndexMarker906"></html:a>more granular way to control the agents. This
    provides enhanced control and flexibility for executing user queries. It enables
    users to manage the agent’s actions with finer detail, facilitating the development
    of more sophisticated <html:span class="No-Break">agentic systems.</html:span></html:p>
    <html:p>The entire concept is based on two main components, <html:code class="literal">AgentRunner</html:code>
    and <html:code class="literal">AgentWorker</html:code> , and works as described
    in <html:span class="No-Break"><html:em class="italic">Figure 8</html:em></html:span>
    <html:span class="No-Break"><html:em class="italic">.15</html:em></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 8.15 – The AgentRunner and AgentWorker orchestration model</html:p>
    <html:p>We use <html:strong class="bold">agent runners</html:strong> to <html:a
    id="_idIndexMarker907"></html:a>orchestrate tasks and store conversational memory.
    <html:strong class="bold">Agent workers</html:strong> control <html:a id="_idIndexMarker908"></html:a>the
    execution of each task step by step without storing the state themselves. The
    agent runner manages the overall process and integrates <html:span class="No-Break">the
    results.</html:span></html:p> <html:p>In terms of <html:a id="_idIndexMarker909"></html:a>benefits,
    there are multiple reasons to use agents like this. Firstly, it allows for a clear
    separation of concerns: agent runners manage the task’s overall orchestration
    and memory, while agent workers focus only on executing specific steps of a task.
    This division enhances the maintainability and scalability of <html:span class="No-Break">the
    system.</html:span></html:p> <html:p>Moreover, the architecture promotes enhanced
    visibility and control over the agent’s decision-making process. We can observe
    and intervene at each step, with very good insight into the agent’s operation.
    This is particularly useful for debugging and refining our <html:span class="No-Break">agent’s
    behavior.</html:span></html:p> <html:p>Another key benefit is the flexibility
    it provides. We can tailor the behavior of agents according to the specific needs
    of the application. We can modify or extend the functionality of agent workers,
    or integrate custom logic within the agent runner, making the system highly adaptable.
    This setup also supports modular development. We can build or update individual
    components without affecting the entire system, facilitating easier updates <html:span
    class="No-Break">and iterations.</html:span></html:p> <html:p>Here’s a sample
    <html:a id="_idIndexMarker910"></html:a>implementation that takes one of our previous
    examples and applies this more granular approach. We’ll implement <html:code class="literal">OpenAIAgent</html:code>
    in a low-level fashion by using <html:code class="literal">AgentRunner</html:code>
    <html:span class="No-Break">and</html:span> <html:span class="No-Break"><html:code
    class="literal">OpenAIAgentWorker</html:code></html:span> <html:span class="No-Break">:</html:span></html:p>
    <html:p>Here, we’ve <html:a id="_idIndexMarker911"></html:a>imported the necessary
    components and prepared the tool list for the agent. We’re using the same <html:code
    class="literal">employees.db</html:code> database as before. Next, we’ll define
    the <html:span class="No-Break">agent worker:</html:span></html:p> <html:p>It’s
    time to initialize our agent runner and prepare the input that will contain <html:span
    class="No-Break">the task:</html:span></html:p> <html:p>There are two distinct
    methods to engage with our agent now. Let’s take <html:span class="No-Break">a
    look.</html:span></html:p> <html:h3>Option A – the end-to-end interaction, using
    the chat() method</html:h3> <html:p>The <html:code class="literal">chat()</html:code>
    method <html:a id="_idIndexMarker912"></html:a>offers a seamless, end-to-end interaction,
    executing the task without requiring intervention at each <html:span class="No-Break">reasoning
    step:</html:span></html:p> <html:p>It’s very straightforward: just two lines of
    code, at which point we wait for the agent to solve the task and provide a final
    response when all the steps <html:span class="No-Break">are completed.</html:span></html:p>
    <html:h3>Option B – the step-by-step interaction, using the create_task() method</html:h3>
    <html:p>For more <html:a id="_idIndexMarker913"></html:a>granular control, we
    could leverage the agent runner and use a step-by-step method that allows us to
    create a task, run each step individually, and then finalize <html:span class="No-Break">the
    response:</html:span></html:p> <html:p>In the first part, we created a new task
    for the agent runner and executed the first step of the task. Because this method
    provides manual control of the execution of each step, we have to manually implement
    a loop in our code. We will repeatedly call <html:code class="literal">run_step()</html:code>
    until the output indicates all steps <html:span class="No-Break">are complete:</html:span></html:p>
    <html:p>The previous loop will run until the last step is completed. Then, it’s
    time to synthesize and display the <html:span class="No-Break">final answer:</html:span></html:p>
    <html:p>This <html:a id="_idIndexMarker914"></html:a>allows us to execute and
    observe each reasoning step individually. The <html:code class="literal">create_task()</html:code>
    method initializes a new task, <html:code class="literal">run_step()</html:code>
    executes each step, returning an output, and <html:code class="literal">finalize_response()</html:code>
    generates the final response once all steps <html:span class="No-Break">are complete.</html:span></html:p>
    <html:p>Overall, this option is particularly useful when you need to monitor the
    agent’s decisions closely or when you want to step in at certain points to guide
    the process or to <html:span class="No-Break">handle exceptions.</html:span></html:p>
    <html:p>Now, it’s time to apply this fresh knowledge and add some chat features
    to our <html:span class="No-Break">PITS project.</html:span></html:p> <html:a
    id="_idTextAnchor194"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Hands-on
    – implementing conversation tracking for PITS</html:title></html:head> <html:body><html:div
    class="epub-source"><html:h1 id="_idParaDest-195">Hands-on – implementing conversation
    tracking for PITS</html:h1> <html:div id="_idContainer094">import os import json
    import streamlit as st from openai import OpenAI from llama_index.core import
    load_index_from_storage from llama_index.core import StorageContext from llama_index.core.memory
    import ChatMemoryBuffer from llama_index.core.tools import QueryEngineTool, ToolMetadata
    from llama_index.agent.openai import OpenAIAgent from llama_index.core.storage.chat_store
    import SimpleChatStore from global_settings import INDEX_STORAGE, CONVERSATION_FILE
    def load_chat_store():     try:         chat_store = SimpleChatStore.from_persist_path(
                CONVERSATION_FILE         )     except FileNotFoundError:         chat_store
    = SimpleChatStore()     return chat_store def display_messages(chat_store, container):
        with container:         for message in chat_store.get_messages(key="0"):             with
    st.chat_message(message.role):                 st.markdown(message.content) def
    initialize_chatbot(user_name, study_subject,                        chat_store,
    container, context):     memory = ChatMemoryBuffer.from_defaults(         token_limit=3000,
            chat_store=chat_store,         chat_store_key="0"     )     storage_context
    = StorageContext.from_defaults(         persist_dir=INDEX_STORAGE     )     index
    = load_index_from_storage(         storage_context, index_id="vector"     )     study_materials_engine
    = index.as_query_engine(         similarity_top_k=3     )     study_materials_tool
    = QueryEngineTool(         query_engine=study_materials_engine,         metadata=ToolMetadata(
                name="study_materials",             description=(                 f"Provides
    official information about "                 f"{study_subject}. Use a detailed
    plain "                 f"text question as input to the tool."             ),
            )     )     agent = OpenAIAgent.from_tools(         tools=[study_materials_tool],
            memory=memory,         system_prompt=(             f"Your name is PITS,
    a personal tutor. Your "             f"purpose is to help {user_name} study and
    "             f"better understand the topic of: "             f"{study_subject}.
    We are now discussing the "             f"slide with the following content: {context}"
            )     )     display_messages(chat_store, container)     return agent def
    chat_interface(agent, chat_store, container):     prompt = st.chat_input("Type
    your question here:")     if prompt:         with container:             with
    st.chat_message("user"):                 st.markdown(prompt)             response
    = str(agent.chat(prompt))             with st.chat_message("assistant"):                 st.markdown(response)
            chat_store.persist(CONVERSATION_FILE) <html:p>In this <html:a id="_idIndexMarker915"></html:a>practical
    section, we’ll use some of our newfound knowledge to further improve our personal
    tutoring project. Like any professional tutor, eager to teach students and answer
    their questions, PITS should have a proper conversational engine at its core.
    It should be able to understand the topic, be aware of the current context, and
    keep track of the entire interaction with the student. Because the learning process
    will probably take place through multiple sessions, PITS must be able to persist
    the entire conversation and resume the interaction when a new session is initiated.
    We’ll implement all these features in <html:code class="literal">coversation_engine.py</html:code>
    . This <html:a id="_idIndexMarker916"></html:a>module is not meant to be used
    directly in our app architecture. Instead, it will provide three callable functions
    that we will later import and use in <html:a id="_idIndexMarker917"></html:a>the
    <html:span class="No-Break"><html:code class="literal">training_UI.py</html:code></html:span>
    <html:span class="No-Break">module:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">load_chat_store</html:code> : This <html:a id="_idIndexMarker918"></html:a>function
    is responsible for retrieving the chatbot conversation from previous sessions.
    We’re using a generic <html:code class="literal">chat_store_key="0"</html:code>
    key. In a multi-user scenario, this key could be used to store chat conversations
    for different users in the same <html:span class="No-Break">chat store.</html:span></html:li>
    <html:li><html:code class="literal">initialize_chatbot</html:code> : This <html:a
    id="_idIndexMarker919"></html:a>function is responsible for loading the training
    material vector index from storage, defining a query engine tool on the index,
    and then initializing <html:code class="literal">OpenAIAgent</html:code> using
    this tool as an argument. It also provides the agent with a system prompt that
    contains context information describing the purpose of the agent, the username
    and study topic, as well as the current slide content. The function returns the
    initialized agent, which will then be used by <html:code class="literal">chat_interface</html:code>
    to implement the <html:span class="No-Break">actual conversation.</html:span></html:li>
    <html:li><html:code class="literal">chat_interface</html:code> : This <html:a
    id="_idIndexMarker920"></html:a>function implements the ongoing conversation by
    taking <html:a id="_idIndexMarker921"></html:a>the user input and generating an
    answer from the agent. It also persists the conversation after each interaction.
    If the user ends the current session, on resume, the conversation will be continued
    from <html:span class="No-Break">that point.</html:span></html:li></html:ul> <html:p>Once
    implemented in the main training interface, this chat should look similar to what’s
    shown in <html:span class="No-Break"><html:em class="italic">Figure 8</html:em></html:span>
    <html:span class="No-Break"><html:em class="italic">.16</html:em></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 8.16 – Screenshot from the PITS training UI</html:p> <html:p>Let’s
    have a look at the code. The first part contains all the <html:span class="No-Break">necessary
    imports:</html:span></html:p> <html:p>You’ll notice <html:a id="_idIndexMarker922"></html:a>that
    in the first part of the code, we imported a lot of components. The <html:code
    class="literal">os</html:code> and <html:code class="literal">json</html:code>
    modules will be used for the chat persistence feature. The specific LlamaIndex
    elements will be used to implement the agent with all its <html:span class="No-Break">required
    components.</html:span></html:p> <html:p>We also imported the <html:code class="literal">INDEX_STORAGE</html:code>
    and <html:code class="literal">CONVERSATION_FILE</html:code> locations from the
    <html:code class="literal">global_settings.py</html:code> module. Because the
    chat conversation will be implemented using Streamlit, we also have to import
    the <html:span class="No-Break"><html:code class="literal">streamlit</html:code></html:span>
    <html:span class="No-Break">library.</html:span></html:p> <html:p>Next, let’s
    have a look at the <html:code class="literal">load_chat_store</html:code> function,
    which is responsible for resuming the previous conversation by loading the chat
    history from the local storage file specified <html:span class="No-Break">by</html:span>
    <html:span class="No-Break"><html:code class="literal">CONVERSATION_FILE</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p>As we can <html:a id="_idIndexMarker923"></html:a>see,
    the <html:code class="literal">load_chat_store</html:code> function tries to retrieve
    the conversation history from the local storage file. If the storage file does
    not exist, a new empty <html:code class="literal">chat_store</html:code> is created.
    The function <html:span class="No-Break">returns</html:span> <html:span class="No-Break"><html:code
    class="literal">chat_store</html:code></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p>The next function is responsible for displaying the entire conversation
    history in the <html:span class="No-Break">Streamlit interface:</html:span></html:p>
    <html:p>The <html:code class="literal">display_messages</html:code> function takes
    a chat store and a Streamlit container as arguments. It extracts all messages
    from the chat store using <html:code class="literal">get_messages()</html:code>
    . The function iterates over and displays each message from the chat store, assigning
    appropriate roles – <html:em class="italic">user</html:em> or <html:em class="italic">assistant</html:em>
    – <html:span class="No-Break">to each.</html:span></html:p> <html:p>The messages
    are displayed in the Streamlit container using Streamlit’s <html:code class="literal">chat_message()</html:code>
    method, which has the advantage of automatically adding a corresponding icon for
    <html:span class="No-Break">each role.</html:span></html:p> <html:p>The next function
    is responsible for initializing the agent. This function takes <html:span class="No-Break">five
    arguments:</html:span></html:p> <html:ul><html:li><html:code class="literal">user_name</html:code>
    : The name of the user – to enable a more <html:span class="No-Break">personal
    experience.</html:span></html:li> <html:li><html:code class="literal">study_subject</html:code>
    : The topic covered by the <html:span class="No-Break">study materials.</html:span></html:li>
    <html:li><html:code class="literal">chat_store</html:code> : Used to initialize
    the <html:span class="No-Break">conversation history.</html:span></html:li> <html:li><html:code
    class="literal">container</html:code> : This is the Streamlit container where
    the chat conversation will be displayed. It’s not used by this function itself
    and instead passed further to the <html:span class="No-Break"><html:code class="literal">display_messages</html:code></html:span>
    <html:span class="No-Break">function.</html:span></html:li> <html:li><html:code
    class="literal">context</html:code> : This is the content of the current slide
    being displayed in the training interface. This context will be fed into the agent’s
    system prompt to ground any answer on the current context of <html:span class="No-Break">the
    user.</html:span></html:li></html:ul> <html:p>Let’s see <html:a id="_idIndexMarker924"></html:a>the
    first part of <html:span class="No-Break">the function:</html:span></html:p> <html:p>Here,
    we have defined a <html:code class="literal">ChatMemoryBuffer</html:code> object
    for the agent, specifying the <html:code class="literal">chat_store</html:code>
    attribute containing the conversation history. We used the same <html:code class="literal">chat_store_key</html:code>
    as before. This is important to allow the agent to correctly retrieve the <html:span
    class="No-Break">chat history.</html:span></html:p> <html:p>Next, we’ll prepare
    the tools for <html:span class="No-Break">the agent:</html:span></html:p> <html:p>Here,
    we first retrieved our vector index by using a <html:code class="literal">StorageContext</html:code>
    object and the <html:code class="literal">load_index_from_storage()</html:code>
    method. We had to specify the <html:em class="italic">ID</html:em> of the index
    – <html:em class="italic">vector</html:em> – because in our case, the storage
    contains more than <html:span class="No-Break">one index.</html:span></html:p>
    <html:p>After loading <html:a id="_idIndexMarker925"></html:a>the index, we created
    a simple query engine configured with <html:code class="literal">similarity_top_k=3</html:code>
    and then created a <html:code class="literal">QueryEngineTool</html:code> utility,
    providing a proper description in its metadata so that the agent can <html:em
    class="italic">understand</html:em> its purpose and usage. The top-k similarity
    parameter is set to <html:code class="literal">3</html:code> to retrieve the three
    most relevant pieces of information from <html:span class="No-Break">the index.</html:span></html:p>
    <html:p>The next part will <html:span class="No-Break">initialize</html:span>
    <html:span class="No-Break"><html:code class="literal">OpenAIAgent</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p>In the preceding code,
    we initialized <html:code class="literal">OpenAIAgent</html:code> while providing
    <html:code class="literal">QueryEngineTool</html:code> , <html:code class="literal">memory</html:code>
    , and <html:code class="literal">system_prompt</html:code> as arguments. This
    prompt is used to provide the LLM with background information to contextualize
    its responses, ensuring they are relevant to the current discussion topic and
    the user’s <html:span class="No-Break">study needs.</html:span></html:p> <html:p>As
    you can <html:a id="_idIndexMarker926"></html:a>see, I’ve tried to keep the code
    as simple as possible. Many things could be improved in this implementation. After
    initializing the agent, we call <html:code class="literal">display_messages</html:code>
    to display the <html:span class="No-Break">existing conversation.</html:span></html:p>
    <html:p>Our last function is responsible for handling the actual conversation.
    It takes <html:span class="No-Break">three arguments:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">agent</html:code> : The agent engine that will be used to run
    <html:span class="No-Break">the chat</html:span></html:li> <html:li><html:code
    class="literal">chat_store</html:code> : The <html:code class="literal">chat_store</html:code>
    argument that will be used to persist <html:span class="No-Break">the conversation</html:span></html:li>
    <html:li><html:code class="literal">container</html:code> : The Streamlit container
    where the messages will <html:span class="No-Break">be displayed</html:span></html:li></html:ul>
    <html:p>Let’s have a look at <html:span class="No-Break">the code:</html:span></html:p>
    <html:p>This <html:code class="literal">chat_interface</html:code> function <html:a
    id="_idIndexMarker927"></html:a>displays a chat input widget using Streamlit’s
    <html:code class="literal">chat_input()</html:code> method. Upon <html:a id="_idIndexMarker928"></html:a>receiving
    input, it does <html:span class="No-Break">the following:</html:span></html:p>
    <html:ul><html:li>Adds the user’s question to the chat interface in the <html:span
    class="No-Break">specified container</html:span></html:li> <html:li>Calls the
    chat method of <html:code class="literal">OpenAIAgent</html:code> to process the
    question and generate <html:span class="No-Break">a response</html:span></html:li>
    <html:li>Adds the chatbot’s response to the chat interface in the <html:span class="No-Break">specified
    container</html:span></html:li> <html:li>Persists the new conversation to <html:code
    class="literal">CONVERSATION_FILE</html:code> using the chat store’s persist method
    to ensure continuity <html:span class="No-Break">across sessions</html:span></html:li></html:ul>
    <html:p>That’s it for now. We’ll talk about more of the features of PITS in the
    next <html:span class="No-Break">few chapters.</html:span></html:p> <html:a id="_idTextAnchor195"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Summary</html:title></html:head>
    <html:body><html:div class="epub-source"><html:h1 id="_idParaDest-196">Summary</html:h1>
    <html:div id="_idContainer094"><html:p>This chapter provided an in-depth exploration
    of building chatbots and agents with LlamaIndex. We covered <html:code class="literal">ChatEngine</html:code>
    for conversation tracking and different built-in chat modes, such as simple, context,
    condense question, and condense <html:span class="No-Break">plus context.</html:span></html:p>
    <html:p>Then, we explored different agent architectures and strategies using <html:code
    class="literal">OpenAIAgent</html:code> , <html:code class="literal">ReActAgent</html:code>
    , and the more advanced LLMCompiler agent. Key concepts such as tools, tool orchestration,
    reasoning loops, and parallel execution <html:span class="No-Break">were explained.</html:span></html:p>
    <html:p>We concluded this chapter with a hands-on implementation of conversation
    tracking for the PITS <html:span class="No-Break">tutoring application.</html:span></html:p>
    <html:p>Overall, you should now have a comprehensive understanding of leveraging
    LlamaIndex capabilities to create useful and engaging <html:span class="No-Break">conversational
    interfaces.</html:span></html:p> <html:p>Throughout the next chapter, we’ll discover
    how to customize our RAG pipeline and provide a straightforward guide to deploying
    it with Streamlit. We’ll also explore advanced tracing methods for seamless debugging
    and unravel strategies for evaluating <html:span class="No-Break">our applications.</html:span></html:p></html:div></html:div></html:body></html:html>
    <html:html><html:head><html:title>Part 4: Customization, Prompt Engineering, and
    Final Words</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-197" lang="en-US">Part 4: Customization, Prompt Engineering, and
    Final Words</html:h1> <html:div id="_idContainer096"><html:p>In the final part
    of this book, we explore customizing RAG components for robust, production-ready
    applications, covering tracing and evaluation methods as well as deployment with
    platforms such as Streamlit. We also discover techniques for effective prompt
    engineering and understand how prompts can enhance a RAG workflow. We conclude
    with reflections on the transformative potential of RAG and AI, emphasizing continuous
    learning, community engagement, and ethical considerations, alongside a forward-looking
    perspective on the role of technology and responsible development in shaping <html:span
    class="No-Break">the future.</html:span></html:p> <html:p>This part has the <html:span
    class="No-Break">following chapters:</html:span></html:p> <html:ul><html:li><html:em
    class="italic">Chapter 9</html:em> , <html:em class="italic">Customizing and Deploying
    Our LlamaIndex Project</html:em></html:li> <html:li><html:em class="italic">Chapter
    10</html:em> , <html:em class="italic">Prompt Engineering Guidelines and Best
    Practices</html:em></html:li> <html:li><html:a><html:em class="italic">Chapter
    11</html:em></html:a> , <html:em class="italic">Conclusion and Additional Resources</html:em></html:li></html:ul></html:div></html:div></html:body></html:html>'
  prefs: []
  type: TYPE_NORMAL
