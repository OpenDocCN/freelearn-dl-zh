- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Applying Unsupervised Learning Approaches
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用无监督学习方法
- en: In earlier chapters, such as [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107),
    we discussed the fact that supervised learning requires annotated data, where
    a human annotator makes a decision about how a **natural language processing**
    (**NLP**) system should analyze it – that is, a human has *annotated* it. For
    example, with the movie review data, a human has looked at each review and decided
    whether it is positive or negative. We also pointed out that this annotation process
    can be expensive and time-consuming.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，如[*第5章*](B19005_05.xhtml#_idTextAnchor107)，我们讨论了监督学习需要标注数据的事实，其中人工标注员决定**自然语言处理**（**NLP**）系统应如何分析数据——也就是说，数据是由人*标注*的。例如，在电影评论数据中，人工标注员查看每个评论并决定它是正面还是负面。我们还指出，这一标注过程既昂贵又耗时。
- en: In this chapter, we will look at techniques that don’t require annotated data,
    thereby saving this time-consuming step in data preparation. Although unsupervised
    learning will not be suitable for every NLP problem, it is very useful to have
    an understanding of the general area so that you can decide how to incorporate
    it into your NLP projects.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究不需要标注数据的技术，从而节省了数据准备中的这一耗时步骤。虽然无监督学习并不适用于每一个NLP问题，但了解这一领域的基本知识非常有用，这样你就能决定如何将其纳入到你的NLP项目中。
- en: At a deeper level, we will discuss applications of unsupervised learning, such
    as topic modeling, including the value of unsupervised learning for exploratory
    applications and maximizing scarce data. We will also cover label generation in
    unsupervised classification and mention some approaches to make the most of limited
    labeled data, with techniques for partial supervision.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在更深层次上，我们将讨论无监督学习的应用，如主题建模，包括无监督学习在探索性应用和最大化稀缺数据方面的价值。我们还将讨论无监督分类中的标签生成，并提及一些方法来最大化有限标签数据的利用，结合部分监督的技术。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: What is unsupervised learning?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是无监督学习？
- en: Topic modeling using clustering techniques and label derivation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用聚类技术和标签推导的主题建模
- en: Making the most of data with partial supervision
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大化部分监督下的数据利用
- en: What is unsupervised learning?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是无监督学习？
- en: The applications that we worked with in earlier chapters were based on data
    that was manually categorized by human annotators. For example, each review in
    the movie review corpus that we have used several times was read by a human annotator
    and assigned a category, *positive* or *negative*, based on the human’s opinion.
    The review-category pairs were then used to train models, using the machine learning
    algorithms that we previously learned about to categorize new reviews. This whole
    process is called **supervised learning** because the training process is, in
    effect, *supervised* by the training data. The training data labeled by humans
    is referred to as the *gold standard* or *ground truth.*
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们处理的应用都是基于人工标注的数据。例如，我们多次使用的电影评论语料库中的每条评论都由人工标注员阅读，并根据标注员的意见将其分配到*正面*或*负面*类别。然后，评论-类别对被用来训练模型，使用我们之前学习的机器学习算法对新评论进行分类。整个过程称为**监督学习**，因为训练过程实际上是由训练数据进行*监督*的。由人类标注的数据被称为*黄金标准*或*真实标签*。
- en: Supervised approaches have some disadvantages, however. The most obvious disadvantage
    is the cost of developing the ground-truth data because of the cost of human annotators.
    Another consideration is the possibility that the manual annotations from different
    annotators, or even the same annotator at different times, will be inconsistent.
    Inconsistent annotation can also occur if the data labels themselves are subjective
    or not clear-cut, which makes it harder for the annotators to agree on the correct
    annotation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，监督学习方法也存在一些缺点。最明显的缺点是开发真实标签数据的成本，因为需要人工标注员的费用。另一个需要考虑的问题是，不同标注员，甚至同一标注员在不同时间进行的人工标注，可能会存在不一致的情况。如果数据标签本身是主观的或不明确的，也可能导致标注不一致，这使得标注员难以就正确的标注达成一致。
- en: For many applications, supervised approaches are the only option, but there
    are other applications, which we will be exploring in this chapter, where **unsupervised
    techniques** are useful.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多应用，监督学习方法是唯一的选择，但在本章中，我们将探索其他应用，其中**无监督技术**是有用的。
- en: These unsupervised applications don’t require labeled training data because
    what we want to learn from the natural language data doesn’t require any human
    judgment. Rather, it can be found by just examining the raw text, which can be
    done by an algorithm. These kinds of applications include grouping documents by
    similarity and computing the similarity of documents. In particular, we will be
    looking at **clustering**, which is the process of putting data, documents in
    particular, into similar groups. Finding similar groups of documents is often
    a first step in the process of developing a classification application, before
    the categories of documents are known. Once the clusters are identified, there
    are additional techniques that can help find human-readable labels for the clusters,
    although in some cases, it is easy to identify how the clusters should be labeled
    by manual inspection of the clusters. We will look at tools to find cluster labels
    later in this chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些无监督应用不需要标注的训练数据，因为我们想要从自然语言数据中学习的内容并不需要任何人工判断。相反，这些内容可以通过仅仅检查原始文本来发现，而这一过程可以通过算法来完成。这类应用包括按相似度对文档进行分组和计算文档的相似度。特别地，我们将关注**聚类**，即将数据，特别是文档，分组到相似的类别中。寻找相似的文档组通常是开发分类应用过程中的第一步，在文档的类别尚未确定之前。一旦聚类被识别出来，还可以使用一些附加技术来帮助找到人类可读的标签，尽管在某些情况下，通过手动检查聚类，能够轻松确定聚类应如何标记。我们将在本章后面介绍找到聚类标签的工具。
- en: In addition to clustering, another important application of unsupervised learning
    is the training process for the **large language models** (**LLMs**) that we covered
    in [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193). Training LLMs doesn’t require
    any supervision because the training process only looks at words in the context
    of other words. However, we will not cover the training process for LLMs in this
    book because this is a very computationally intensive process and requires expensive
    computational resources that are not available to the vast majority of developers.
    In addition, LLM training is not needed for most practical applications, since
    existing LLMs that have already been trained are widely available.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 除了聚类，另一项重要的无监督学习应用是我们在[*第11章*](B19005_11.xhtml#_idTextAnchor193)中介绍的**大语言模型**（**LLMs**）的训练过程。训练LLMs不需要任何监督，因为训练过程仅查看单词在其他单词的上下文中的含义。然而，本书中不会涉及LLM的训练过程，因为这是一个计算密集型的过程，并且需要昂贵的计算资源，这些资源对绝大多数开发者来说并不具备。此外，大多数实际应用并不需要LLM训练，因为现有的已训练LLM已被广泛提供。
- en: In this chapter, we will illustrate in detail a practical NLP problem where
    unsupervised learning can be very useful – *topic modeling*. In topic modeling,
    we start with a collection of text items, such as documents or user inputs in
    chatbot applications, but we don’t have a pre-determined set of categories. Instead,
    we use the words in the texts themselves to find semantic similarities among the
    texts that enable us to group them into categories, or topics.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将详细阐述一个实际的自然语言处理问题，在该问题中，无监督学习非常有用——*主题建模*。在主题建模中，我们从一组文本项开始，例如文档或聊天机器人应用中的用户输入，但我们没有预先确定的类别集。相反，我们使用文本中的单词本身来发现文本之间的语义相似性，从而将它们分组为不同的类别或主题。
- en: We will start by reviewing a few general considerations about semantically grouping
    similar texts.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先回顾一些关于语义上将相似文本分组的基本考虑因素。
- en: Topic modeling using clustering techniques and label derivation
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用聚类技术和标签推导的主题建模
- en: We’ll start our exploration of topic modeling by looking at some considerations
    relating to grouping semantically similar documents in general, and then we’ll
    look at a specific example.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过先讨论一些关于一般性语义上相似文档分组的考虑因素开始探索主题建模，然后我们将看一个具体的例子。
- en: Grouping semantically similar documents
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义上相似文档的分组
- en: Like most of the machine learning problems we’ve discussed so far, the overall
    task generally breaks down into two sub-problems, representing the data and performing
    a task based on the representations. We’ll look at these two sub-problems next.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 和我们迄今为止讨论的大多数机器学习问题一样，整体任务通常可以分解为两个子问题：数据表示和基于表示执行任务。接下来，我们将探讨这两个子问题。
- en: Representing the data
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据表示
- en: The data representations we’ve looked at so far were reviewed in [*Chapter 7*](B19005_07.xhtml#_idTextAnchor144).
    These approaches included the simple **bag of words** (**BoW**) variants, **term
    frequency - inverse document frequency** (**TF-IDF**), and newer approaches, including
    **Word2Vec**. Word2Vec is based on word vectors, which are vectors that represent
    words in isolation, without taking into account the context in which they occur.
    A newer representation, used in the **BERT** system that we discussed in the previous
    chapter, takes into account the contexts of words in a sentence or document to
    create numerical word representations, or *embeddings*. We will use BERT embeddings
    in this chapter to uncover similarities among documents.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止查看的数据表示方法在[*第七章*](B19005_07.xhtml#_idTextAnchor144)中有所回顾。这些方法包括简单的**词袋**（**BoW**）变体、**词频-逆文档频率**（**TF-IDF**）以及较新的方法，包括**Word2Vec**。Word2Vec基于词向量，词向量是代表单独单词的向量，不考虑单词出现的上下文。一个较新的表示方法，使用于我们在上一章讨论的**BERT**系统，考虑了单词在句子或文档中的上下文，以创建数值化的单词表示，或称为*嵌入*。在本章中，我们将使用BERT嵌入来揭示文档之间的相似性。
- en: Working with data representations
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理数据表示
- en: This section will discuss two aspects of processing embeddings. First, we will
    discuss grouping similar texts into clusters, and then we will discuss visualizing
    the clusters.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将讨论处理嵌入的两个方面。首先，我们将讨论将相似文本分组到簇中，然后讨论如何可视化这些簇。
- en: Clustering – grouping similar items
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 聚类——分组相似项
- en: '**Clustering** is the main NLP task we’ll talk about in this chapter. Clustering
    is the name for a wide variety of algorithms that attempt to group data items
    together, based on similarities in the data representation. Clustering can be
    used with any dataset, whether or not the data items are text-based, as long as
    there is a numerical way of representing their similarity. In this chapter, we
    will review a practical set of tools to perform clustering, but you should be
    aware that there are many other options, and undoubtedly, there will be many new
    ones as technology moves forward. Two common approaches to clustering are k-means
    and HDBSCAN:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类**是我们在本章将讨论的主要自然语言处理（NLP）任务。聚类是指一类旨在根据数据表示中的相似性将数据项分组的各种算法。聚类可以应用于任何数据集，无论数据项是否是基于文本的，只要有数值化的方式表示它们的相似性。在本章中，我们将回顾一组实用的聚类工具，但你应该意识到还有许多其他选择，随着技术的发展，毫无疑问会有更多新的聚类方法。两种常见的聚类方法是k-means和HDBSCAN：'
- en: '**k-means**: The k-means algorithm, which we reviewed in [*Chapter 6*](B19005_06.xhtml#_idTextAnchor134),
    is a very common clustering approach in which data points are initially randomly
    assigned to one of k clusters, the means of the clusters are computed, and the
    distance of the data points from the centers of the clusters is minimized through
    an iterative process. The value of k, or the number of clusters, is a hyperparameter
    that is selected by the developer. It can be considered to be the most *useful*
    number of clusters for the application. The k-means algorithm is often used because
    it is efficient and easy to implement, but other clustering algorithms – in particular,
    HDBSCAN – can yield better results.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**k-means**：我们在[*第六章*](B19005_06.xhtml#_idTextAnchor134)回顾过的k-means算法是一种非常常见的聚类方法，其中数据点最初会被随机分配到k个簇中，计算簇的均值，并通过迭代过程最小化数据点与簇中心之间的距离。k的值，即簇的数量，是由开发者选择的超参数。它可以视为该应用中最*有用*的簇数。k-means算法之所以常用，是因为它高效且易于实现，但其他聚类算法——尤其是HDBSCAN——可以产生更好的结果。'
- en: '**HDBSCAN**: HDBSCAN is another popular clustering algorithm and stands for
    **Hierarchical Density-Based Spatial Clustering of Applications with Noise**.
    HDBSCAN takes into account the density of the data points within the clusters.
    Because of this, it is able to find oddly sized and differently sized clusters.
    It can also detect outliers or items that don’t fit well into a cluster, while
    k-means forces every item into a cluster.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HDBSCAN**：HDBSCAN是另一种流行的聚类算法，代表**基于密度的层次空间聚类与噪声应用**。HDBSCAN考虑了簇内数据点的密度。因此，它能够找到大小不一且形状各异的簇。它还能够检测到离群值或那些不适合某个簇的项，而k-means则会强制每个项都被分配到一个簇中。'
- en: Visualizing the clusters
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可视化簇
- en: '**Visualization** is very important in unsupervised approaches, such as clustering,
    because it allows us to see what groups of similar data items look like and helps
    us judge whether or not the grouping result is useful. While clusters of similar
    items can be represented in any number of dimensions, we are only able to effectively
    visualize clusters in, at most, three dimensions. Consequently, in practice, dimensionality
    reduction is needed to reduce the number of dimensions. We will use a tool called
    **Uniform Manifold Approximation and Projection** (**UMAP**) for dimensionality
    reduction.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**可视化**在无监督学习方法中非常重要，比如聚类，因为它允许我们看到相似数据项的分组方式，并帮助我们判断分组结果是否有用。尽管相似项的聚类可以用任意维度表示，但我们最多只能有效地在三维空间中可视化聚类。因此，在实践中，需要进行降维处理以减少维度的数量。我们将使用一种叫做**统一流形近似与投影**（**UMAP**）的工具进行降维。'
- en: In the next section, we will use clustering and visualization to illustrate
    a specific application of unsupervised learning called topic modeling. The general
    problem that can be solved using topic modeling is that of classifying documents
    into different topics. The unique characteristic of this technique is that, unlike
    the classification examples we saw in earlier chapters, we don’t know what the
    topics are at the outset. Topic modeling can help us identify groups of similar
    documents, even if we don’t know what the eventual categories will be.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用聚类和可视化来说明无监督学习的一个特定应用——主题建模。主题建模可以解决的普遍问题是将文档分类到不同的主题中。这种技术的独特之处在于，与我们在前几章看到的分类示例不同，起初我们并不知道主题是什么。主题建模可以帮助我们识别相似文档的组，即使我们不知道最终的分类会是什么。
- en: In this example, we will use BERT transformer embeddings to represent the documents
    and HDBSCAN for clustering. Specifically, we will use the BERTopic Python library
    found at [https://maartengr.github.io/BERTopic/index.html](https://maartengr.github.io/BERTopic/index.html).
    The BERTopic library is customizable, but we will stick with the default settings
    for the most part in our example.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用BERT变换器嵌入来表示文档，并使用HDBSCAN进行聚类。具体来说，我们将使用位于[https://maartengr.github.io/BERTopic/index.html](https://maartengr.github.io/BERTopic/index.html)的BERTopic
    Python库。BERTopic库是可定制的，但在我们的示例中，我们将大部分使用默认设置。
- en: 'The data we’ll look at is a well-known dataset called `20 newsgroups`, which
    is a collection of 20,000 newsgroup documents from 20 different internet newsgroups.
    This is a popular dataset that is often used in text processing. The data is email
    messages of varying lengths that are posted to newsgroups. Here’s one example
    of a short message from this dataset, with the email headers removed:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要查看的数据是一个著名的数据集，叫做`20 newsgroups`，它是来自20个不同互联网新闻组的20,000个新闻组文档集合。这是一个常用于文本处理的流行数据集。数据由不同长度的电子邮件消息组成，这些消息发布到新闻组中。以下是该数据集中的一条短消息示例，已去除电子邮件头部：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `20 newsgroups` dataset can be imported from the scikit-learn datasets
    or downloaded from the following website: [http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`20 newsgroups`数据集可以从scikit-learn的datasets中导入，或者从以下网站下载：[http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/)。'
- en: Dataset citation
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集引用
- en: 'Ken Lang, *Newsweeder: Learning to filter netnews*, 1995, *Proceedings of the
    Twelfth International Conference on Machine* *Learning*, 331–339'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 'Ken Lang，*Newsweeder: 学习过滤网新闻*，1995年，*第十二届国际机器学习会议论文集*，331–339'
- en: In the following sections, we will go over topic modeling in detail using the
    `20 newsgroups` dataset and the BERTopic package. We will create embeddings, construct
    the model, generate proposed labels for the topics, and visualize the resulting
    clusters. The last step will be to show the process of finding topics for new
    documents using our model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将使用`20 newsgroups`数据集和BERTopic包详细讲解主题建模。我们将创建嵌入，构建模型，为主题生成建议标签，并可视化生成的聚类。最后一步是展示如何使用我们的模型为新文档找到主题。
- en: Applying BERTopic to 20 newsgroups
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将BERTopic应用于20个新闻组
- en: 'The first step in this application is to install BERTopic and import the necessary
    libraries in a Jupyter notebook, as shown here:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个应用的第一步是在Jupyter笔记本中安装BERTopic并导入必要的库，如下所示：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Embeddings
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入
- en: 'The next step is to prepare the data representations, or embeddings, shown
    in the following code block. Because this is a slow process, it is useful to set
    `show_progress_bar ()` to `True` so that we can ensure that the process is moving
    along, even if it takes a long time:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是准备数据表示或嵌入，如以下代码块所示。由于这是一个较慢的过程，设置`show_progress_bar()`为`True`是很有用的，这样即使过程很慢，我们也能确保进程正在进行。
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Instead of the earlier BERT word embeddings that we worked with in [*Chapter
    11*](B19005_11.xhtml#_idTextAnchor193) in this exercise, we will work with sentence
    embeddings using **Sentence** **Bert** (**SBERT**).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用**Sentence** **Bert**（**SBERT**）进行句子嵌入，而不是我们在[*第11章*](B19005_11.xhtml#_idTextAnchor193)中使用的早期BERT词嵌入。
- en: SBERT produces one embedding per sentence. We will use a package called `SentenceTransformers`,
    available from Hugging Face, and the `all-MiniLM-L6-v2` model, which is recommended
    by BERTopic. However, many other transformer models can be used. These include,
    for example, the `en_core_web_trf` spaCy model, or the `distilbert-base-cased`
    Hugging Face model. BERTopic provides a guide to a wide selection of other models
    that you can use at [https://maartengr.github.io/BERTopic/getting_started/embeddings/embeddings.html](https://maartengr.github.io/BERTopic/getting_started/embeddings/embeddings.html).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: SBERT为每个句子生成一个嵌入。我们将使用一个名为`SentenceTransformers`的包，它来自Hugging Face，且推荐使用`all-MiniLM-L6-v2`模型，BERTopic也推荐使用该模型。不过，许多其他的transformer模型也可以使用。例如，`en_core_web_trf`的spaCy模型或`distilbert-base-cased`的Hugging
    Face模型。BERTopic还提供了一个指南，介绍了您可以使用的其他模型，详细内容请见[https://maartengr.github.io/BERTopic/getting_started/embeddings/embeddings.html](https://maartengr.github.io/BERTopic/getting_started/embeddings/embeddings.html)。
- en: 'We can see what the actual embeddings look like in the following output:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下输出中看到实际的嵌入效果：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Using the `corpus_embeddings.view()` method shown here, we can see a summary
    of the embeddings, which are an array of arrays of floating point numbers. Looking
    directly at the embeddings isn’t particularly useful itself, but it gives you
    something of a sense of what the actual data looks like.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`corpus_embeddings.view()`方法，如下所示，我们可以查看嵌入的摘要，它们是浮动数字的数组的数组。直接查看嵌入本身并不特别有用，但它可以让你对实际数据的样子有个大致的了解。
- en: Constructing the BERTopic model
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建BERTopic模型
- en: 'Once the embeddings have been computed, we can build the BERTopic model. The
    BERTopic model can take a large number of parameters, so we won''t show them all.
    We will show some useful ones, but there are many more, and you can consult the
    BERTopic documentation for additional ideas. The BERTopic model can be constructed
    very simply, with just the documents and the embeddings as parameters, as shown
    here:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦嵌入计算完成，我们就可以构建BERTopic模型。BERTopic模型可以接受大量参数，因此我们不会展示所有参数。我们将展示一些有用的参数，但还有很多其他的参数，您可以查阅BERTopic文档以获取更多的思路。BERTopic模型可以非常简单地构建，只需要文档和嵌入作为参数，如下所示：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This simple model has defaults for the parameters that will usually lead to
    a reasonable result. However, to illustrate some of the flexibility that’s possible
    with BERTopic, we’ll show next how the model can be constructed with a richer
    set of parameters, with the code shown here:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的模型具有一些默认的参数，通常会产生合理的结果。然而，为了展示BERTopic的一些灵活性，我们接下来将展示如何使用更丰富的参数集来构建模型，代码如下所示：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding code, we start by defining several useful models. The first
    one is `CountVectorizer`, which we saw in [*Chapter 7*](B19005_07.xhtml#_idTextAnchor144)
    and [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184), where we used it to vectorize
    text documents to formats such as BoW. Here, we will use the vectorizer to remove
    stopwords after dimensionality reduction and clustering so that they don’t end
    up being included in topic labels.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们首先定义了几个有用的模型。第一个是`CountVectorizer`，我们在[*第7章*](B19005_07.xhtml#_idTextAnchor144)和[*第10章*](B19005_10.xhtml#_idTextAnchor184)中看到过，在那里我们用它将文本文档向量化成如BoW这样的格式。在这里，我们将使用该向量化器在降维和聚类后移除停用词，以确保它们不会被包含到主题标签中。
- en: '**Stopwords** should not be removed from documents before preparing embeddings
    because the transformers have been trained on normal text, including stopwords,
    and the models will be less effective without the stopwords.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备嵌入之前，**停用词**不应从文档中删除，因为transformer模型是基于包含停用词的正常文本进行训练的，缺少停用词会导致模型效果下降。
- en: The vectorizer model parameters indicate that the model is constructed with
    English stopwords and that only words that occur in fewer than 95% of the documents
    and more than 1% are included. This is to exclude extremely common and extremely
    rare words from the model, which are unlikely to be helpful to distinguish topics.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化模型的参数表示该模型是使用英语停用词构建的，并且仅包括在少于95%的文档中出现且超过1%的单词。这是为了排除极其常见和极其罕见的单词，这些单词不太可能对区分主题有所帮助。
- en: 'The second model we will define is the HDBSCAN model, which is used for clustering.
    Some of the parameters include the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义的第二个模型是HDBSCAN模型，用于聚类。其一些参数包括以下内容：
- en: The `min_cluster_size` parameter is the minimum number of documents that we
    want to be in a cluster. Here, we’ve selected '`30'` as the minimum cluster size.
    This parameter can vary, depending on the problem you’re trying to solve, but
    you might want to consider a larger number if the number of documents in the dataset
    is large.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_cluster_size`参数是我们希望在聚类中包含的文档的最小数量。在这里，我们选择了`30`作为最小聚类大小。此参数可以根据你要解决的问题进行调整，但如果数据集中的文档数量较大，可能需要选择更大的数字。'
- en: '`prediction_data` is set to `True` if we want to be able to predict the topics
    of new documents after the model is trained.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_data` 设置为`True`，如果我们希望在模型训练后能够预测新文档的主题。'
- en: 'The next model is the **uniform manifold approximation and projection** (**UMAP**)
    model, which is used for dimensionality reduction. Besides making it easier to
    visualize multidimensional data, UMAP also makes it easier to cluster. Some of
    its parameters include the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个模型是**均匀流形近似与投影**（**UMAP**）模型，用于降维。除了更容易可视化多维数据外，UMAP还使得聚类变得更加容易。它的一些参数包括以下内容：
- en: '`n-neighbors`: This constrains the size of the area that UMAP will look at
    when learning the structure of the data.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n-neighbors`：此参数限制了UMAP在学习数据结构时会观察的区域大小。'
- en: '`n-components`: This determines the dimensionality of the reduced dimension
    space we will use.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n-components`：此参数决定了我们将使用的降维空间的维度。'
- en: '`low_memory`: This determines system memory management behavior. If the parameter
    is `False`, the algorithm will use a faster and more memory-intensive approach.
    If running out of memory is a problem with large datasets, you can set `low_memory`
    to `True`.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_memory`：此参数决定系统内存管理的行为。如果参数为`False`，算法将使用更快速但更占用内存的方法。如果在处理大数据集时内存不足是个问题，可以将`low_memory`设置为`True`。'
- en: 'With these preliminary models defined, we can move on to defining the BERTopic
    model itself. The parameters that are set in this example are the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了这些初步模型后，我们可以继续定义BERTopic模型本身。此示例中设置的参数如下：
- en: The models for the three tools we’ve already defined – the vectorizer model,
    the UMAP model, and the HDBSCAN model.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经定义的三个工具模型——向量化模型、UMAP模型和HDBSCAN模型。
- en: '`nr_topics`: If we have an idea of how many topics we want to find, this parameter
    can be set to that number. In this case, it is set to `auto`, and HDBSCAN will
    find a good estimate of the number of topics.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nr_topics`：如果我们已经知道想要找到多少个主题，则可以将此参数设置为该数量。在这种情况下，它设置为`auto`，HDBSCAN将根据数据估算出合适的主题数量。'
- en: '`top_n_words`: When generating labels, BERTopic should consider only the *n*
    most frequent words in a cluster.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_n_words`：在生成标签时，BERTopic应仅考虑聚类中出现频率最高的*前n*个单词。'
- en: '`min_topic_size`: The minimum number of documents that should be considered
    to form a topic.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_topic_size`：应考虑形成一个主题的文档的最小数量。'
- en: '`calculate_probabilities`: This calculates the probabilities of all topics
    per document.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`calculate_probabilities`：此参数计算每个文档所有主题的概率。'
- en: Running the code in this section will create clusters from the original documents.
    The number and size of the clusters will vary, depending on the parameters that
    were set in the generation of the model. You are encouraged to try different settings
    of the parameters and compare the results. As you think about the goals of your
    application, consider whether some of the settings seem to yield more or less
    helpful results.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 运行本节代码将从原始文档中创建聚类。聚类的数量和大小将有所不同，具体取决于生成模型时设置的参数。鼓励你尝试不同的参数设置并比较结果。在思考应用目标时，考虑一些设置是否会产生更多或更少有用的结果。
- en: Up to this point, we have used clusters of similar documents, but they are not
    labeled with any categories. It would be more useful if the clusters had names.
    Let’s see how we can get names or labels for the clusters that relate to what
    they are about. The next section will review ways to label the clusters.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用的是相似文档的簇，但这些簇并没有被标注上任何类别。如果这些簇有名字会更有用。接下来我们将看看如何为这些簇获得与其内容相关的名称或标签。下一节将回顾标记簇的几种方法。
- en: Labeling
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标记
- en: Once we have the clusters, there are various approaches to labeling them with
    topics. A manual approach, where you just look at the documents and think about
    what a good label might be, should not necessarily be ruled out. However, there
    are automatic approaches that can suggest topics based on the words that occur
    in the documents in the various clusters.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到了簇，就有多种方法可以为它们标上主题。手动方法，你只需查看文档并思考一个合适的标签是什么，这种方法并不一定要排除。然而，也有一些自动方法可以根据文档中出现的词语建议主题。
- en: BERTopic uses a method to suggest topic labels called class-based `tf-idf`,
    or `c-tf-idf`. You will recall that TF-IDF was discussed in earlier chapters as
    a document vectorization approach that identifies the most diagnostic terms for
    a document class. It does this by taking into account the frequency of a term
    in a document, compared to its frequency in the overall document collection. Terms
    that are frequent in one document but not overall in the dataset are likely to
    be indicative that a document should be assigned to a specific category.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: BERTopic使用一种方法来建议主题标签，称为基于类别的`tf-idf`，或`c-tf-idf`。你应该记得，TF-IDF在前几章中已作为一种文档向量化方法讨论，它可以识别文档类别中最具诊断性的术语。它通过比较术语在文档中的频率与其在整个文档集合中的频率来实现这一点。那些在某个文档中频繁出现，但在整个数据集中并不常见的术语，很可能表明该文档应该被分配到特定类别。
- en: Class-based TF-IDF takes this intuition a step further by treating each cluster
    as if it were a single document. It then looks at the terms that occur frequently
    in a cluster, but not overall in the entire dataset, to identify words that are
    useful to label a topic. Using this metric, labels can be generated for the topics
    using the most diagnostic words for each cluster. We can see in *Table 12.1* the
    labels generated for the 10 most frequent topics in a dataset, along with the
    number of documents in each topic.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 基于类别的TF-IDF通过将每个簇视为一个单独的文档，进一步拓展了这一直觉。然后，它会查看在某个簇中频繁出现的术语，但在整个数据集中不常出现的术语，以识别有助于标记主题的词语。利用这个指标，可以为每个簇生成最具诊断性的词语作为标签。我们可以在*表12.1*中看到，为数据集中前10个最频繁的主题生成的标签，以及每个主题中的文档数量。
- en: Note that the first topic in the table, *-1*, is a catchall topic that includes
    the documents that don’t fall into any topic.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 注意表格中的第一个主题，*-1*，是一个涵盖所有不属于任何其他主题的文档的通用主题。
- en: '|  | **Topic** | **Count** | **Name** |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | **主题** | **数量** | **名称** |'
- en: '| 0 | -1 | 6,928 | `-``1_maxaxaxaxaxaxaxaxaxaxaxaxaxaxax_dont_know_like` |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 0 | -1 | 6,928 | `-``1_maxaxaxaxaxaxaxaxaxaxaxaxaxaxax_dont_know_like` |'
- en: '| 1 | 0 | 1,820 | `0_game_team_games_players` |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 1,820 | `0_game_team_games_players` |'
- en: '| 2 | 1 | 1,569 | `1_space_launch_nasa_orbit` |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | 1,569 | `1_space_launch_nasa_orbit` |'
- en: '| 3 | 2 | 1,313 | `2_car_bike_engine_cars` |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2 | 1,313 | `2_car_bike_engine_cars` |'
- en: '| 4 | 3 | 1,168 | `3_image_jpeg_window_file` |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 3 | 1,168 | `3_image_jpeg_window_file` |'
- en: '| 5 | 4 | 990 | `4_armenian_armenians_people_turkish` |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 4 | 990 | `4_armenian_armenians_people_turkish` |'
- en: '| 6 | 5 | 662 | `5_drive_scsi_drives_ide` |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 5 | 662 | `5_drive_scsi_drives_ide` |'
- en: '| 7 | 6 | 636 | `6_key_encryption_clipper_chip` |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 6 | 636 | `6_key_encryption_clipper_chip` |'
- en: '| 8 | 7 | 633 | `7_god_atheists_believe_atheism` |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 7 | 633 | `7_god_atheists_believe_atheism` |'
- en: '| 9 | 8 | 427 | `8_cheek___` |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 8 | 427 | `8_cheek___` |'
- en: '| 10 | 9 | 423 | `9_israel_israeli_jews_arab` |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 9 | 423 | `9_israel_israeli_jews_arab` |'
- en: Table 12.1 – The top 10 topics and automatically generated labels
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.1 – 前10个主题及自动生成的标签
- en: Visualization
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化
- en: Visualization is extremely useful in unsupervised learning, because deciding
    how to proceed in the development process often depends on intuitions that can
    be assisted by looking at results in different ways.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化在无监督学习中极为有用，因为决定如何在开发过程中进行下一步通常依赖于可以通过不同方式查看结果来帮助做出直觉判断。
- en: 'There are many ways of visualizing the results of topic modeling. One useful
    visualization can be obtained with a BERTopic method, `model.visualize_barchart()`,
    which shows the top topics and their top words, as shown in *Figure 12**.1*. For
    example, looking at the top words in `space_launch_nasa_orbit`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以可视化主题建模的结果。一种有用的可视化方法是通过BERTopic方法`model.visualize_barchart()`获得的，它显示了最重要的主题及其关键词，如*图12**.1*所示。例如，查看`space_launch_nasa_orbit`中的重要词汇：
- en: "![Figure 12.1\uFEFF – The top seven topics and their most significant words](img/B19005_12_01.jpg)"
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图12.1 – 前七个主题及其最重要的词汇](img/B19005_12_01.jpg)'
- en: Figure 12.1 – The top seven topics and their most significant words
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 – 前七个主题及其最重要的词汇
- en: 'Another important visualization technique that is often used in clustering
    is representing each item in a cluster as a dot, representing their similarities
    by the distance between the dots, and assigning different colors or markers to
    the clusters. This clustering can be produced with a BERTopic method, `visualize_documents()`,
    as shown in the following code with the minimal `docs` and `embeddings` parameters:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种在聚类中常用的重要可视化技术是将每个聚类中的项目表示为一个点，通过点之间的距离来表示它们的相似性，并为不同的聚类分配不同的颜色或标记。这个聚类可以通过BERTopic方法`visualize_documents()`生成，如以下代码所示，使用最小的`docs`和`embeddings`参数：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Additional parameters can be set to configure a cluster plot in various ways,
    as documented in the BERTopic documentation. For example, you can set the display
    to show or hide the cluster labels, or to only show the top topics.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 可以设置额外的参数来以各种方式配置聚类图，具体内容可参见BERTopic文档。例如，你可以设置显示或隐藏聚类标签，或者仅显示最重要的主题。
- en: The clustering for the top seven topics in the `20 newsgroups` data can be seen
    in *Figure 12**.2*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`20 newsgroups`数据集中的前七个主题的聚类结果可以在*图12**.2*中看到。'
- en: "![Figure 12.2\uFEFF – The clustered documents for the top seven topics in the\
    \ 20 newsgroups dataset with their generated labels](img/B19005_12_02.jpg)"
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图12.2 – `20 newsgroups`数据集中前七个主题的聚类文档及其生成的标签](img/B19005_12_02.jpg)'
- en: Figure 12.2 – The clustered documents for the top seven topics in the 20 newsgroups
    dataset with their generated labels
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 – `20 newsgroups`数据集中前七个主题的聚类文档及其生成的标签
- en: There are seven topics displayed in *Figure 12**.2*, corresponding to the seven
    topics in *Figure 12**.1*, each labeled with its automatically generated label.
    In addition, there are documents in *Figure 12**.2* shown in light gray that were
    not assigned to any cluster. These are the documents shown as belonging to topic
    *-1* in *Table 12.1*.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12**.2*中显示了七个主题，对应于*图12**.1*中的七个主题，每个主题都有自动生成的标签。此外，*图12**.2*中的文档以浅灰色显示，表示这些文档没有被分配到任何聚类中。它们在*表12.1*中作为主题`-1`显示。'
- en: One insight we can gain from the cluster display is the fact that there are
    many unassigned documents, which means that we may want to increase the number
    of topics to look for (by increasing the `nr_topic`s parameter to `model()`).
    Another insight is that some of the topics – for example, `1_space_launch_nasa`
    – seem to be split over several clusters. This means that it might be more meaningful
    for them to be considered as separate topics. As in the case of unclassified documents,
    we can investigate this possibility by increasing the number of topics to look
    for.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从聚类展示中，我们可以得到一个见解，那就是有许多未分配的文档，这意味着我们可能需要增加要查找的主题数量（通过将`nr_topic`参数增加到`model()`）。另一个见解是，一些主题——例如，`1_space_launch_nasa`——似乎被分配到了多个聚类中。这意味着将它们视为单独的主题可能会更有意义。像未分类文档的情况一样，我们可以通过增加要查找的主题数量来调查这一可能性。
- en: We have shown two of the most useful BERTopic visualizations, but there are
    many more. You are encouraged to consult the BERTopic documentation for other
    ideas.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了两种最有用的BERTopic可视化图，但还有更多。建议你参考BERTopic文档，探索其他想法。
- en: Finding the topic of a new document
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查找新文档的主题
- en: 'Once the clustering is complete, the model can be used to find the topics of
    new documents, similar to a classification application. This can be done with
    the `model.transform()` method, as shown here:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦聚类完成，模型就可以用来找到新文档的主题，类似于分类应用。可以使用`model.transform()`方法完成，如下所示：
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The predicted topics for the two documents in `new_docs` are `-1` (no topic)
    and `3`, which stands for `3_space_launch_orbit_nasa`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`new_docs`中的两个文档的预测主题分别是`-1`（无主题）和`3`，表示`3_space_launch_orbit_nasa`。'
- en: After clustering and topic labeling
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在聚类和主题标注后
- en: Recall that the most common application of clustering is to explore the data,
    often in preparation to develop a supervised classification application. We can
    look at the clustering results and decide, for example, that some of the clusters
    are very similar and close to each other. In that case, it might be difficult
    to distinguish documents in those topics from each other, even after supervised
    training, and it would be a good idea to combine those clusters into a single
    topic. Similarly, if we find very small clusters, we would probably get more reliable
    results if the small clusters are combined with similar large clusters.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，聚类最常见的应用是探索数据，通常是为了开发监督分类应用做准备。我们可以查看聚类结果，并决定，例如，某些聚类非常相似并且彼此靠近。在这种情况下，即使经过监督训练，也很难将这些主题中的文档区分开来，因此将这些聚类合并为一个主题可能是个好主意。同样，如果我们发现一些非常小的聚类，通常将这些小聚类与相似的大聚类合并，会得到更可靠的结果。
- en: We can also modify labels to make them more helpful or informative. For example,
    topic *1* in *Table 12.1* is `1_space_launch_nasa_orbit`. You might decide that
    `space` would be a simpler label that is, nevertheless, just as informative as
    the automatically generated label.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以修改标签，使其更加有用或富有信息。例如，*表12.1*中的主题*1*是`1_space_launch_nasa_orbit`。你可能会决定使用`space`作为一个更简单的标签，它仍然和自动生成的标签一样具有信息量。
- en: After making any adjustments to the clusters and labels that you find helpful,
    the result will be a supervised dataset, just like the supervised datasets we
    worked with, such as the movie reviews. You can use this as you would use any
    supervised dataset in NLP applications.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在对你认为有帮助的聚类和标签进行调整后，结果将是一个监督数据集，就像我们之前使用的监督数据集，例如电影评论数据集一样。你可以像使用任何监督数据集一样使用它，在NLP应用中加以利用。
- en: While unsupervised topic modeling can be a very useful technique, it is also
    possible to take advantage of data that is even partially supervised. We will
    summarize some of the ideas that the NLP research community has explored to use
    partially annotated data in the next section.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管无监督的主题建模是一种非常有用的技术，但也可以利用部分监督的数据。我们将在下一节总结NLP研究社区探索的一些思想，看看如何利用部分标注的数据。
- en: Making the most of data with weak supervision
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大化利用弱监督数据
- en: In between completely supervised and unsupervised learning are several approaches
    to partial supervision, where only x data is supervised. Like unsupervised approaches,
    the goal of these techniques is to make the most of supervised data, which can
    be expensive to obtain. One advantage of partial supervision over unsupervised
    approaches is that unsupervised results don’t automatically have useful labels.
    The labels have to be supplied, either manually or through some of the techniques
    we saw earlier in this chapter. In general, with weak supervision, the labels
    are supplied based on the subset of the data that is supervised.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全监督和无监督学习之间，有几种部分监督的方法，其中只有x数据是受监督的。像无监督方法一样，这些技术的目标是最大化利用监督数据，而监督数据通常很难获取。部分监督相较于无监督方法的一个优势是，无监督结果自动生成的标签并不总是有用的。标签必须通过手动或本章早些时候提到的一些技术来提供。通常情况下，弱监督下的标签是基于已监督数据的子集来提供的。
- en: This is an active research area, and we will not go into it in detail. However,
    it is useful to know what the general tactics of weak supervision are so that
    you will be able to apply them as they relate to specific tasks, depending on
    the kind of labeled data that is available.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个活跃的研究领域，我们不会深入讨论。然而，了解弱监督的一般策略是有益的，这样你就能根据可用的标注数据，将这些策略应用到具体任务中。
- en: 'Some tactics for weak supervision include the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督的一些策略包括以下内容：
- en: Incomplete supervision, where only partial data has ground-truth labels
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不完全监督，其中只有部分数据具有真实标签
- en: Inexact supervision, where the data has coarse-grained labels
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不精确监督，其中数据有粗粒度的标签
- en: Inaccurate supervision, where some of the labels might be incorrect
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不准确监督，其中一些标签可能是错误的
- en: Semi-supervised learning, where some predefined labels are provided to help
    push a model toward known classes
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半监督学习，其中提供了一些预定义的标签，帮助将模型推向已知类别
- en: These approaches are worth considering in applications where full annotation
    is too expensive or takes too long. In addition, they can also be useful in situations
    where unsupervised learning would be problematic because there are predefined
    labels that are required by other parts of the overall application, such as a
    database.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在那些完整标注过于昂贵或耗时的应用中，这些方法值得考虑。此外，在那些无监督学习可能会遇到问题的情况下，它们也可以发挥作用，特别是当其他部分的整体应用（例如数据库）需要预定义标签时。
- en: Summary
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about the basic concepts of unsupervised learning.
    We also worked through a specific application of unsupervised learning, topic
    modeling, using a BERT-based tool called BERTopic. We used the BERTopic package
    to identify clusters of semantically similar documents and propose labels for
    the clusters based on the words they contain, without needing to use any supervised
    annotations of the cluster topics.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了无监督学习的基本概念。我们还通过使用一个基于BERT的工具——BERTopic，具体演示了无监督学习的一个应用：主题建模。我们使用BERTopic包来识别语义相似文档的聚类，并根据它们包含的词汇为这些聚类提议标签，而无需使用任何监督标注的聚类主题。
- en: In the next chapter, [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226), we will
    address the question of measuring how good our results are using quantitative
    techniques. Quantitative evaluation is useful in research applications to compare
    results to those from previous research, and it is useful in practical applications
    to ensure that the techniques being used meet the application’s requirements.
    Although evaluation was briefly discussed in earlier chapters, [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226)
    will discuss it in depth. It will include segmenting data into training, validation,
    and test data, evaluation with cross-validation, evaluation metrics such as precision
    and recall, the area under the curve, ablation studies, statistical significance,
    inter-annotator agreement, and user testing.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，[*第13章*](B19005_13.xhtml#_idTextAnchor226)，我们将讨论如何使用定量技术衡量我们结果的好坏。定量评估在研究应用中很有用，可以将结果与以往的研究进行比较，并且在实际应用中可以确保所使用的技术符合应用的要求。虽然在前面的章节中简要讨论过评估，[*第13章*](B19005_13.xhtml#_idTextAnchor226)将深入探讨这一主题。内容将包括将数据划分为训练集、验证集和测试集，使用交叉验证进行评估，评估指标（如精确度和召回率）、曲线下面积、消融研究、统计显著性、标注者间一致性和用户测试。
