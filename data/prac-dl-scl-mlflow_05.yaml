- en: '*Chapter 3*: Tracking Models, Parameters, and Metrics'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 3 章*：跟踪模型、参数和指标'
- en: Given that MLflow can support multiple scenarios through the life cycle of DL
    models, it is common to use MLflow's capabilities incrementally. Usually, people
    start with MLflow tracking since it is easy to use and can handle many scenarios
    for reproducibility, provenance tracking, and auditing purposes. In addition,
    tracking the history of a model from cradle to sunset not only goes beyond the
    data science experiment management domain but is also important for model governance
    in the enterprise, where business and regulatory risks need to be managed for
    using models in production. While the precise business values of tracking models
    in production are still evolving, the need for tracking a model's entire life
    cycle is unquestionable and growing. For us to be able to do this, we will begin
    this chapter by setting up a full-fledged local MLflow tracking server.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 MLflow 可以支持深度学习模型生命周期中的多种场景，通常会逐步使用 MLflow 的功能。通常，人们从 MLflow 跟踪开始，因为它易于使用，能够处理许多可复现性、来源追踪和审计目的的场景。此外，从模型的“摇篮到日落”跟踪其历史，不仅超越了数据科学实验管理领域，而且对于企业中的模型治理也至关重要，因为在生产中使用模型时，需要管理业务和监管风险。虽然在生产中跟踪模型的精确商业价值仍在发展中，但对跟踪模型整个生命周期的需求是不可置疑且日益增长的。为了实现这一点，我们将从本章开始，搭建一个完整的本地
    MLflow 跟踪服务器。
- en: We will then take a deep dive into how we can track a model, along with its
    parameters and metrics, using MLflow's tracking and registry APIs. By the end
    of this chapter, you should feel comfortable using MLflow's tracking and registry
    APIs for various reproducibility and auditing purposes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将深入探讨如何使用 MLflow 的跟踪和注册 API 跟踪模型及其参数和指标。在本章结束时，你应该能够熟练使用 MLflow 的跟踪和注册
    API 进行各种可复现性和审计目的。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主要内容：
- en: Setting up a full-fledged local MLflow tracking server
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搭建一个完整的本地 MLflow 跟踪服务器
- en: Tracking model provenance
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪模型来源
- en: Tracking model metrics
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪模型指标
- en: Tracking model parameters
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪模型参数
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following are the requirements you will need to follow the instructions
    provided in this chapter:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是你需要遵循本章提供的指令所需的要求：
- en: 'Docker Desktop: [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/).'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Desktop：[https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)。
- en: 'PyTorch `lightning-flash`: 0.5.0.: [https://github.com/PyTorchLightning/lightning-flash/releases/tag/0.5.0](https://github.com/PyTorchLightning/lightning-flash/releases/tag/0.5.0).'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PyTorch `lightning-flash`: 0.5.0： [https://github.com/PyTorchLightning/lightning-flash/releases/tag/0.5.0](https://github.com/PyTorchLightning/lightning-flash/releases/tag/0.5.0)。'
- en: 'VS Code with the Jupyter Notebook extension: [https://github.com/microsoft/vscode-jupyter/wiki/Setting-Up-Run-by-Line-and-Debugging-for-Notebooks](https://github.com/microsoft/vscode-jupyter/wiki/Setting-Up-Run-by-Line-and-Debugging-for-Notebooks).'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有 Jupyter Notebook 扩展的 VS Code：[https://github.com/microsoft/vscode-jupyter/wiki/Setting-Up-Run-by-Line-and-Debugging-for-Notebooks](https://github.com/microsoft/vscode-jupyter/wiki/Setting-Up-Run-by-Line-and-Debugging-for-Notebooks)。
- en: 'The following GitHub URL for the code for this chapter: [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章代码的 GitHub 链接：[https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03)。
- en: 'WSL2: If you are a Microsoft Windows user, it is recommended to install WSL2
    to run the Bash scripts provided in this book: [https://www.windowscentral.com/how-install-wsl2-windows-10](https://www.windowscentral.com/how-install-wsl2-windows-10).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WSL2：如果你是 Microsoft Windows 用户，建议安装 WSL2 以运行本书中提供的 Bash 脚本：[https://www.windowscentral.com/how-install-wsl2-windows-10](https://www.windowscentral.com/how-install-wsl2-windows-10)。
- en: Setting up a full-fledged local MLflow tracking server
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 搭建一个完整的本地 MLflow 跟踪服务器
- en: 'In [*Chapter 2*](B18120_02_ePub.xhtml#_idTextAnchor027), *Getting Started with
    MLflow for Deep Learning*, we gained hands-on experience working with a local
    filesystem-based MLflow tracking server and inspecting the components of the MLflow
    experiment. However, there are limitations with a default local filesystem-based
    MLflow server as the model registry functionality is not available. The benefit
    of having a model registry is that we can register the model, version control
    the model, and prepare for model deployment into production. Therefore, this model
    registry will bridge the gap between offline experimentation and an online deployment
    production scenario. Thus, we need a full-fledged MLflow tracking server with
    the following stores to track the complete life cycle of a model:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第二章*](B18120_02_ePub.xhtml#_idTextAnchor027)，*使用MLflow进行深度学习入门*中，我们通过实践操作，使用了基于本地文件系统的MLflow跟踪服务器，并检查了MLflow实验的组件。然而，基于默认本地文件系统的MLflow服务器存在一些限制，因为模型注册功能不可用。拥有模型注册的好处在于我们可以注册模型、进行版本控制，并为模型部署到生产环境做准备。因此，模型注册将填补离线实验与在线生产部署之间的空白。因此，我们需要一个功能齐全的MLflow跟踪服务器，并且该服务器需要以下存储来跟踪模型的完整生命周期：
- en: '**Backend store**: A relational database backend is needed to support MLflow''s
    storage of metadata (metrics, parameters, and many others) about the experiment.
    This also allows the query capability of the experiment to be used. We will use
    a MySQL database as a local backend store.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后端存储**：需要一个关系型数据库后端来支持MLflow存储关于实验的元数据（如度量、参数等）。这还允许查询实验的功能。我们将使用MySQL数据库作为本地后端存储。'
- en: '**Artifact store**: An object store that can store arbitrary types of objects,
    such as serialized models, vocabulary files, figures, and many others. In a production
    environment, a popular choice is the AWS S3 store. We will use **MinIO** ([https://min.io/](https://min.io/)),
    a multi-cloud object store, as a local artifact store, which is fully compatible
    with the AWS S3 store API but can run on your laptop without you needing to access
    the cloud.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工件存储**：一个可以存储任意类型对象的对象存储，比如序列化模型、词汇文件、图表等。在生产环境中，常用的选择是AWS S3存储。我们将使用**MinIO**（[https://min.io/](https://min.io/)），一个多云对象存储，作为本地工件存储，它完全兼容AWS
    S3存储API，但可以在本地电脑上运行，无需访问云端。'
- en: 'To make this local setup as easy as possible, we will use the `docker-compose`
    ([https://docs.docker.com/compose/](https://docs.docker.com/compose/)) tool with
    one line of command to start and stop a local full-fledged MLflow tracking server,
    as described in the following steps. Note that Docker Desktop ([https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/))
    must be installed and running on the machine before you can follow these steps.
    Docker helps build and share containerized applications and microservices. The
    following steps will launch the local MLflow tracking server inside your local
    Docker container:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尽可能简化本地设置，我们将使用`docker-compose`（[https://docs.docker.com/compose/](https://docs.docker.com/compose/)）工具，只需一行命令即可启动和停止本地全功能的MLflow跟踪服务器，具体步骤如下。请注意，必须先在机器上安装并运行Docker
    Desktop（[https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)），才能执行这些步骤。Docker有助于构建和分享容器化应用程序和微服务。以下步骤将在本地Docker容器中启动本地MLflow跟踪服务器：
- en: 'Check out the `chapter03` code repository for your local development environment:
    [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03).'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看`chapter03`代码库以便配置本地开发环境：[https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03)。
- en: Change directory to the `mlflow_docker_setup` subfolder, which can be found
    under the `chapter03` folder.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入`mlflow_docker_setup`子文件夹，该文件夹位于`chapter03`文件夹下。
- en: 'Run the following command:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令：
- en: '[PRE0]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If the command is successful, you should see an output similar to the following
    on your screen:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果命令成功执行，屏幕上应显示类似以下内容的输出：
- en: '![Figure 3.1 – A local full-fledged MLflow tracking server is up and running'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.1 – 本地全功能的MLflow跟踪服务器已启动并运行'
- en: '](img/B18120_03_001.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_03_001.jpg)'
- en: Figure 3.1 – A local full-fledged MLflow tracking server is up and running
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – 本地全功能的MLflow跟踪服务器已启动并运行
- en: 'Go to `http://localhost/` to see the MLflow UI web page. Then, click the **Models**
    tab in the UI (*Figure 3.2*). Note that this tab would not work if you only had
    a local filesystem as the backend store for the MLflow tracking server. Hence,
    the MLflow UI''s backend is now running on the Docker container service you just
    started, not a local filesystem. Since this is a brand-new server, there are no
    registered models yet:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问`http://localhost/`，你应该能看到MLflow UI网页。然后，点击UI中的**Models**标签（*图 3.2*）。请注意，如果你的MLflow追踪服务器仅使用本地文件系统作为后端存储，该标签将无法使用。因此，MLflow
    UI的后端现在正在运行在你刚启动的Docker容器服务上，而不是本地文件系统。由于这是一个全新的服务器，目前还没有注册的模型：
- en: '![Figure 3.2 – MLflow model registry UI'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.2 – MLflow 模型注册 UI'
- en: '](img/B18120_03_002.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_03_002.jpg)'
- en: Figure 3.2 – MLflow model registry UI
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – MLflow 模型注册 UI
- en: 'Go to `http://localhost:9000/`,and the following screen (*Figure 3.3*) should
    appear for the MinIO artifact store web UI. Enter `minio` for `minio123` for `.env`
    file, under the `mlflow_docker_setup` folder:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问`http://localhost:9000/`，此时应该会显示MinIO工件存储Web UI的页面（*图 3.3*）。在`.env`文件中，输入`minio`作为用户名，`minio123`作为密码，位于`mlflow_docker_setup`文件夹下：
- en: '![Figure 3.3 – MinIO Web UI login page and browser page after logging in'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.3 – MinIO Web UI 登录页面和登录后的浏览器页面'
- en: '](img/B18120_03_003.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_03_003.jpg)'
- en: Figure 3.3 – MinIO Web UI login page and browser page after logging in
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – MinIO Web UI 登录页面和登录后的浏览器页面
- en: 'At this point, you should have a full-fledged local MLflow tracking server
    running successfully! If you want to stop the server, simply type the following
    command:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你应该已经成功启动了一个完整的本地MLflow追踪服务器！如果你想停止服务器，只需输入以下命令：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Docker-based MLflow tracking server will stop. We are now ready to use this
    local MLflow server to track model provenance, parameters, and metrics.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Docker的MLflow追踪服务器将停止。我们现在准备使用这个本地MLflow服务器来追踪模型的来源、参数和指标。
- en: Tracking model provenance
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 追踪模型来源
- en: '**Provenance** tracking for digital artifacts has been long studied in the
    literature. For example, when you''re using a piece of patient diagnosis data
    in the biomedical industry, people usually want to know where it comes from, what
    kind of processing and cleaning has been done to the data, who owns the data,
    and other history and lineage information about the data. The rise of ML/DL models
    for industrial and business scenarios in production makes provenance tracking
    a required functionality. The different granularities of provenance tracking are
    critical for operationalizing and managing not just the data science offline experimentation,
    but also before/during/after the model is deployed in production. So, what needs
    to be tracked for provenance?'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**来源**追踪在数字工件方面已经在文献中得到广泛研究。例如，在生物医学行业，当你使用一份患者诊断数据时，通常希望知道这些数据的来源，数据经过了哪些处理和清洗，数据的所有者是谁，以及其他有关数据的历史和谱系信息。随着工业和商业场景中ML/DL模型的兴起，来源追踪已成为一项必备功能。不同粒度的来源追踪对操作和管理不仅仅是数据科学离线实验至关重要，而且对模型在生产环境中部署前/中/后的管理也至关重要。那么，来源追踪需要追踪哪些内容呢？'
- en: Understanding the open provenance tracking framework
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解开放来源追踪框架
- en: 'Let''s look at a general provenance tracking framework to understand the big
    picture of why provenance tracking is a major effort. The following diagram is
    based on the **Open Provenance Model Vocabulary Specification** ([http://open-biomed.sourceforge.net/opmv/ns.html](http://open-biomed.sourceforge.net/opmv/ns.html)):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下一个通用的来源追踪框架，以理解为什么来源追踪是一个重大努力。以下图示基于**开放来源模型词汇规范**（[http://open-biomed.sourceforge.net/opmv/ns.html](http://open-biomed.sourceforge.net/opmv/ns.html)）：
- en: '![Figure 3.4 – An open provenance tracking framework'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.4 – 一个开放的来源追踪框架'
- en: '](img/B18120_03_004.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_03_004.jpg)'
- en: Figure 3.4 – An open provenance tracking framework
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 一个开放的来源追踪框架
- en: 'In the preceding diagram, there are three important items:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述图示中，有三个重要的项目：
- en: '**Artifacts**: Things that are produced or used by processes (**A1** and **A2**).'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工件**：由过程生产或使用的事物（**A1** 和 **A2**）。'
- en: '**Processes**: Actions that are performed by using or producing artifacts (**P1**
    and **P2**).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过程**：通过使用或生产工件来执行的操作（**P1** 和 **P2**）。'
- en: '**Causal relationships**: Edges or relationships between artifacts and processes,
    such as *used*, *wasGeneratedBy*, and *wasDerivedFrom* in the preceding diagram
    (**R1**, **R2**, and **R3**).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**因果关系**：在工件和过程之间的边或关系，如前述图示中的*used*、*wasGeneratedBy*和*wasDerivedFrom*（**R1**、**R2**和**R3**）。'
- en: 'Intuitively, this **open provenance model** (**OPM**) framework allows us to
    ask the following 5W1H (five Ws and one H) questions, as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，**开放来源模型** (**OPM**) 框架允许我们提出以下 5W1H（五个 W 和一个 H）问题，如下所示：
- en: '![Figure 3.5 – Types of provenance questions'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.5 – 来源问题的类型](img/B18120_03_005.jpg)'
- en: '](img/B18120_03_005.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_03_005.jpg)'
- en: Figure 3.5 – Types of provenance questions
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 来源问题的类型
- en: Having a systematic provenance framework and a set of questions will help us
    learn how to track model provenance and provide answers to these questions. This
    will motivate us when we implement MLflow model tracking in the next section.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个系统化的来源框架和一组问题将帮助我们学习如何追踪模型来源并提供这些问题的答案。这将激励我们在下一节实现 MLflow 模型跟踪时。
- en: Implementing MLflow model tracking
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现 MLflow 模型跟踪
- en: 'We can use an MLflow tracking server to answer most of these types of provenance
    questions if we implement both MLflow logging and registry for the DL model we
    use. First, let''s review what MLflow provides in terms of model provenance tracking.
    MLflow provides two sets of APIs for model provenance:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们为所使用的深度学习模型实现了 MLflow 日志记录和注册功能，我们可以使用 MLflow 跟踪服务器回答这些来源问题中的大多数。首先，让我们回顾
    MLflow 在模型来源跟踪方面提供的功能。MLflow 提供了两套用于模型来源的 API：
- en: '**Logging API**: This allows each run of the experiment or a model pipeline
    to log the model artifact into the artifact store.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日志记录 API**：这允许每次实验或模型管道运行时将模型工件记录到工件存储中。'
- en: '**Registry API**: This allows a centralized location to track the version of
    the model and the stages of the model''s life cycle (**None**, **Archived**, **Staging**,
    or **Production**).'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注册表 API**：这允许在一个集中位置跟踪模型的版本和模型生命周期的各个阶段（**无**、**已归档**、**暂存**或**生产**）。'
- en: Difference between Model Logging and Model Registry
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型日志记录和模型注册表之间的区别
- en: Although every run of the experiment needs to be logged and the model needs
    to be saved in the artifact store, not every instance of the model needs to be
    registered in the model registry. That's because, for many early exploratory model
    experimentations, the model might not be good. Thus, it is not necessarily registered
    to track the version. Only when a model has good offline performance and becomes
    a candidate for promoting to production do we need to register it in the model
    registry to go through the model promotion process.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管每次实验都需要进行日志记录，并且模型需要保存在工件存储中，但并非每个模型实例都需要在模型注册表中注册。这是因为对于许多早期的探索性模型实验来说，模型可能不好。因此，不一定要注册以跟踪版本。只有当模型在离线测试中表现良好并成为推向生产环境的候选模型时，才需要将其注册到模型注册表，以便通过模型提升流程。
- en: Although MLflow's official API documentation separates logging and registry
    into two components, we will refer to them together as model tracking functionality
    in MLflow in this book.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管 MLflow 的官方 API 文档将日志记录和注册表分成了两个组件，但在本书中我们将它们统称为 MLflow 的模型跟踪功能。
- en: 'We already saw MLflow''s auto-logging for the DL model we built in [*Chapter
    2*](B18120_02_ePub.xhtml#_idTextAnchor027), *Getting Started with MLflow for Deep
    Learning*. Although auto-logging is powerful, there are two issues with the current
    version:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在 [*第 2 章*](B18120_02_ePub.xhtml#_idTextAnchor027)《使用 MLflow 进行深度学习入门》中看到过
    MLflow 的自动日志记录功能。尽管自动日志记录功能非常强大，但当前版本存在两个问题：
- en: It does not automatically register the model to the model registry.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不会自动将模型注册到模型注册表。
- en: It does not work out of the box for the logged model to work directly with the
    original input data (in our case, an English sentence) if you just follow MLflow's
    suggestion to use the `mlflow.pyfunc.load_model` API to load the logged model.
    This is a limitation that's probably due to the experimental nature of the current
    auto-logging APIs in MLflow.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你仅按照 MLflow 的建议使用 `mlflow.pyfunc.load_model` API 来加载已记录的模型，它不能直接与原始输入数据（在我们的案例中是英文句子）一起工作。这是由于
    MLflow 当前自动日志记录 API 的实验性质，可能是一个局限性。
- en: 'Let''s walk through an example to review MLflow''s capabilities and auto-logging''s
    limitations and how we can solve them:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例来回顾 MLflow 的功能以及自动日志记录的局限性，并讨论我们如何解决这些问题：
- en: 'Set up the following environment variables in your Bash terminal, where your
    MinIO and MySQL-based Docker component is running:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Bash 终端中设置以下环境变量，其中你的 MinIO 和基于 MySQL 的 Docker 组件正在运行：
- en: '[PRE2]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` are using the same
    values that were defined in the `.env` file, under the `mlflow_docker_setup` folder.
    This is done to make sure that we are using the MLflow server that we set up previously.
    Since these environmental variables are session-based, we can also set up the
    following environment variables in the notebook''s code, as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`AWS_ACCESS_KEY_ID` 和 `AWS_SECRET_ACCESS_KEY` 使用的是在 `.env` 文件中定义的相同值，该文件位于
    `mlflow_docker_setup` 文件夹下。这是为了确保我们使用的是之前设置的 MLflow 服务器。由于这些环境变量是基于会话的，我们还可以在笔记本的代码中设置以下环境变量，如下所示：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding three lines of code can be found in this chapter''s notebook
    file, just after importing the required Python packages. Before you execute the
    notebook, make sure that you run the following commands to initialize the virtual
    environment, `dl_model`, which now has additional required packages defined in
    the `requirements.txt` file:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 前面三行代码可以在本章的笔记本文件中找到，紧接在导入所需的 Python 包之后。在执行笔记本之前，请确保运行以下命令来初始化虚拟环境 `dl_model`，该环境现在包含
    `requirements.txt` 文件中定义的额外必需包：
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If you set up the `dl_model` virtual environment in the previous chapters, you
    can skip the first line on creating a virtual environment called `dl_model`. However,
    you still need to activate `dl_model` as the currently active virtual environment
    and then run `pip install -r requirements.txt` to install all the required Python
    packages. Once the `dl_model` virtual environment has been set up successfully,
    you may proceed to the next step.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在之前的章节中设置了 `dl_model` 虚拟环境，你可以跳过创建名为 `dl_model` 的虚拟环境的第一行。然而，你仍然需要激活 `dl_model`
    作为当前活动的虚拟环境，然后运行 `pip install -r requirements.txt` 来安装所有所需的 Python 包。一旦 `dl_model`
    虚拟环境成功设置，你可以继续执行下一步。
- en: 'To follow along with this model tracking implementation, check out the `dl_model_tracking.ipynb`
    notebook file in VS Code by going to this chapter''s GitHub repository: [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model_tracking.ipynb](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model_tracking.ipynb).'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要跟随此模型跟踪实现，请查看 VS Code 中的 `dl_model_tracking.ipynb` 笔记本文件，方法是访问本章的 GitHub 仓库：[https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model_tracking.ipynb](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model_tracking.ipynb)。
- en: 'Note that, in the fourth cell of the `dl_model_tracking.ipynb` notebook, we
    need to point it to the correct and new MLflow tracking URI that we just set up
    in the Docker and define a new experiment, as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在 `dl_model_tracking.ipynb` 笔记本的第四个单元格中，我们需要将其指向我们刚刚在 Docker 中设置的正确的新 MLflow
    跟踪 URI，并定义一个新的实验，如下所示：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will still use the auto-logging capabilities provided by MLflow but we will
    assign the run with a variable name, `dl_model_tracking_run`:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们仍然会使用 MLflow 提供的自动日志记录功能，但我们将给运行分配一个变量名，`dl_model_tracking_run`：
- en: '[PRE6]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`dl_model_tracking_run` allows us to get the `run_id` parameter and other metadata
    about this run programmatically, as we will see in the next step. Once this code
    cell has been executed, we will have a trained model logged in the MLflow tracking
    server with all the required parameters and metrics. However, the model hasn''t
    been registered yet. We can find the logged experiment in the MLflow web UI, along
    with all the relevant parameters and metrics, at http://localhost/#/experiments/1/runs/37a3fe9b6faf41d89001eca13ad6ca47\.
    You can find the model artifacts in the `http://localhost:9000/minio/mlflow/1/37a3fe9b6faf41d89001eca13ad6ca47/artifacts/model/`
    to see the storage UI, as shown here:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`dl_model_tracking_run` 允许我们以编程方式获取 `run_id` 参数和此运行的其他元数据，正如我们在下一步中将看到的那样。一旦执行了这段代码单元，我们将在
    MLflow 跟踪服务器中记录一个训练好的模型，并包含所有所需的参数和指标。然而，模型还没有注册。我们可以在 MLflow 的 Web UI 中找到记录的实验以及所有相关的参数和指标，地址是
    http://localhost/#/experiments/1/runs/37a3fe9b6faf41d89001eca13ad6ca47。你可以在 `http://localhost:9000/minio/mlflow/1/37a3fe9b6faf41d89001eca13ad6ca47/artifacts/model/`
    找到模型工件并查看存储 UI，如下所示：'
- en: '![Figure 3.6 – Model artifacts logged In the MinIO storage backend'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.6 – 模型工件记录在 MinIO 存储后端'
- en: '](img/B18120_03_006.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_03_006.jpg)'
- en: Figure 3.6 – Model artifacts logged In the MinIO storage backend
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 模型工件记录在 MinIO 存储后端
- en: The folder structure is similar to what we saw in [*Chapter 2*](B18120_02_ePub.xhtml#_idTextAnchor027),
    *Getting Started with MLflow for Deep Learning*, when we used a plain filesystem
    to store the model artifacts. However, here, we are using a **MinIO** bucket to
    store these model artifacts.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 文件夹结构与我们在[*第 2 章*](B18120_02_ePub.xhtml#_idTextAnchor027)《*使用 MLflow 开始深度学习*》中看到的类似，当时我们使用普通文件系统来存储模型工件。然而，在这里，我们使用
    **MinIO** 桶来存储这些模型工件。
- en: 'Retrieve the `run_id` parameter from `dl_model_tracking_run`, as well as other
    metadata, as follows:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `dl_model_tracking_run` 中检索 `run_id` 参数以及其他元数据，如下所示：
- en: '[PRE7]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This will print out something like the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出类似以下内容：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Retrieve the logged model by defining the logged model URI. This will allow
    us to reload the logged model at this specific location:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过定义已记录模型的 URI 来检索已记录的模型。这将允许我们在特定位置重新加载已记录的模型：
- en: '[PRE9]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Use `mlflow.pytorch.load_model` and the following `logged_model` URI to load
    the model back into memory and make a new prediction for a given input sentence,
    as follows:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `mlflow.pytorch.load_model` 和以下 `logged_model` URI 将模型加载回内存，并为给定的输入句子做出新预测，如下所示：
- en: '[PRE10]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will output a model prediction label, as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出一个模型预测标签，如下所示：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: mlflow.pytorch.load_model versus mlflow.pyfunc.load_model
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: mlflow.pytorch.load_model 与 mlflow.pyfunc.load_model
- en: By default, and in the MLflow experiment tracking page's artifact section, if
    you have a logged model, MLflow will recommend using `mlflow.pyfunc.load_model`
    to load back a logged model for prediction. However, this only works for inputs
    such as a pandas DataFrame, NumPy array, or tensor; this does not work for an
    NLP text input. Since auto-logging for PyTorch lightning uses `mlflow.pytorch.log_model`
    to save the model, the correct way to load a logged model back is to use `mlflow.pytorch.load_model`,
    as we have shown here. This is because MLflow's default design is to use `mlflow.pyfunc.load_model`
    with standardization and a known limitation that can only accept input formats
    in terms of numbers. For text and image data, it requires a tokenization step
    as a preprocessing step. However, since the PyTorch model we saved here already
    performs tokenization as part of the serialized model, we can use the native `mlflow.pytorch.load_model`
    to directly load the model that accepts text as inputs.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，在 MLflow 实验跟踪页面的工件部分，如果您有已记录的模型，MLflow 会建议使用 `mlflow.pyfunc.load_model`
    来加载已记录的模型进行预测。然而，这仅适用于像 pandas DataFrame、NumPy 数组或张量这样的输入；不适用于 NLP 文本输入。由于 PyTorch
    Lightning 的自动记录使用 `mlflow.pytorch.log_model` 来保存模型，正确的加载已记录模型的方法是使用 `mlflow.pytorch.load_model`，正如我们在这里展示的那样。这是因为
    MLflow 的默认设计是使用 `mlflow.pyfunc.load_model`，并且有一个已知的限制，要求输入格式必须是数值类型。对于文本和图像数据，它需要在预处理步骤中进行分词。然而，由于我们在此保存的
    PyTorch 模型已经作为序列化模型的一部分执行了分词步骤，我们可以使用原生的 `mlflow.pytorch.load_model` 来直接加载接受文本输入的模型。
- en: With that, we have successfully logged the model and loaded the model back to
    make a prediction. If we think this model is performing well enough, then we can
    register it.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们就成功记录了模型并将其加载回来进行预测。如果我们认为这个模型表现足够好，那么我们可以将其注册。
- en: 'Let''s register the model by using the `mlflow.register_model` API:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用 `mlflow.register_model` API 来注册模型：
- en: '[PRE12]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will produce the following output:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '![Figure 3.7 – Model registration success message'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.7 – 模型注册成功消息'
- en: '](img/B18120_03_007.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_03_007.jpg)'
- en: Figure 3.7 – Model registration success message
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – 模型注册成功消息
- en: This shows that the model has been successfully registered as version 1 in the
    model registry, under the name `nlp_dl_model`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明模型已成功注册为版本 1，并存储在模型注册表中，模型名称为 `nlp_dl_model`。
- en: 'We can also find this registered model in the MLflow web UI by clicking `http://localhost/#/models/nlp_dl_model/versions/1`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过点击 `http://localhost/#/models/nlp_dl_model/versions/1` 在 MLflow Web UI
    中找到这个已注册的模型：
- en: '![Figure 3.8 – MLflow tracking server web UI showing the newly registered model'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.8 – MLflow 跟踪服务器 Web UI 显示新注册的模型'
- en: '](img/B18120_03_008.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_03_008.jpg)'
- en: Figure 3.8 – MLflow tracking server web UI showing the newly registered model
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 – MLflow 跟踪服务器 Web UI 显示新注册的模型
- en: By default, a newly registered model's stage is **None**, as shown in the preceding
    screenshot.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，新注册模型的阶段为 **None**，如前述截图所示。
- en: By having a model registered with a version number and stage label, we have
    laid the foundation for deployment to staging (also known as pre-production) and
    then production. We will discuss how to perform model deployment based on registered
    models later in this book.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为模型注册版本号和阶段标签，我们为部署到暂存（也称为预生产）和生产环境奠定了基础。我们将在本书的后面讨论基于已注册模型进行模型部署的方法。
- en: 'At this point, we have solved the two issues we raised at the beginning of
    this section regarding the limitations of auto-logging:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经解决了本节开头关于自动记录限制的两个问题：
- en: How to load a logged DL PyTorch model using the `mlflow.pytorch.load_model`
    API instead of the `mlflow.pyfunc.load_model` API
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 `mlflow.pytorch.load_model` API 而不是 `mlflow.pyfunc.load_model` API 加载已记录的
    DL PyTorch 模型
- en: How to register a logged DL PyTorch model using the `mlflow.register_model`
    API
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 `mlflow.register_model` API 注册已记录的 DL PyTorch 模型
- en: Choices of MLflow DL Model Logging APIs
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MLflow DL 模型记录 API 的选择
- en: 'For DL models, the auto-logging for PyTorch only works for `mlflow.pyfunc.log_model`
    to log the model, especially when we need to have multi-step DL model pipelines.
    We will implement such custom MLflow model flavors later in this book. If you
    don''t want to use auto-logging for PyTorch, then you can directly use `mlflow.pytorch.log_model`.
    PyTorch''s auto-logging uses `mlflow.pytorch.log_model` inside its implementation
    (see the official MLflow open source implementation here: [https://github.com/mlflow/mlflow/blob/290bf3d54d1e5ce61944455cb302a5d6390107f0/mlflow/pytorch/_pytorch_autolog.py#L314](https://github.com/mlflow/mlflow/blob/290bf3d54d1e5ce61944455cb302a5d6390107f0/mlflow/pytorch/_pytorch_autolog.py#L314)).'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 DL 模型，PyTorch 的自动记录仅适用于 `mlflow.pyfunc.log_model` 用于记录模型，特别是当我们需要多步骤 DL 模型管道时。我们将在本书的后面实现这样的自定义
    MLflow 模型风格。如果你不想使用 PyTorch 的自动记录，那么可以直接使用 `mlflow.pytorch.log_model`。PyTorch
    的自动记录在其实现中使用了 `mlflow.pytorch.log_model`（请参阅官方 MLflow 开源实现：[https://github.com/mlflow/mlflow/blob/290bf3d54d1e5ce61944455cb302a5d6390107f0/mlflow/pytorch/_pytorch_autolog.py#L314](https://github.com/mlflow/mlflow/blob/290bf3d54d1e5ce61944455cb302a5d6390107f0/mlflow/pytorch/_pytorch_autolog.py#L314)）。
- en: 'If we don''t want to use auto-logging, then we can use MLflow''s model logging
    API directly. This also gives us an alternative way to simultaneously register
    the model in one call. You can use the following line of code to both log and
    register the trained model:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不想使用自动记录，那么可以直接使用 MLflow 的模型记录 API。这还为我们提供了一种同时注册模型的替代方法。您可以使用以下代码行来记录和注册训练好的模型：
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note that this line of code does not log any parameters or metrics of the model.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此代码行不记录模型的任何参数或度量值。
- en: 'With that, we have not only logged many experiments and models in the tracking
    server for offline experimentation but also registered performant models for production
    deployment in the future with version control and provenance tracking. We can
    now answer some of the provenance questions that we posted at the beginning of
    this chapter:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们不仅在跟踪服务器中记录了许多实验和模型以进行离线实验，而且还注册了性能优越的模型，以便将来能够进行版本控制和溯源跟踪，并将其部署到生产环境中。我们现在可以回答本章开头提出的一些溯源问题：
- en: '![Figure 3.9 – Answers to model provenance questions'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.9 – 模型溯源问题的答案'
- en: '](img/B18120_03_009.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_03_009.jpg)'
- en: Figure 3.9 – Answers to model provenance questions
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 – 模型溯源问题的答案
- en: The why and where provenance questions are yet to be fully answered but will
    be done so later in this book. This is because the why provenance question for
    the production model can only be tracked and logged when the model is ready for
    deployment, where we need to add comments and reasons to justify the model's deployment.
    The where provenance question can be answered fully when we have a multiple-step
    model pipeline. However, here, we only have a single-step pipeline, which is the
    simplest case. A multi-step pipeline contains explicitly separate modulized code
    to specify which step performs what functionality so that we can easily change
    the detailed implementation of any of the steps without changing the flow of the
    pipeline. In the next two sections, we will investigate how we can track metrics
    and the parameters of models without using auto-logging.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么和在哪里的问题尚未完全解答，但将在本书后续部分中解答。这是因为生产模型的“为什么”问题只有在模型准备部署时才能被跟踪和记录，此时我们需要添加注释和理由来证明模型的部署合理性。至于“在哪里”的问题，当我们有多步骤模型流水线时可以完全解答。然而，在这里，我们只有一个单步骤流水线，这是最简单的情况。一个多步骤流水线包含明确分离的模块化代码，用于指定每个步骤执行什么功能，这样我们就可以轻松更改任何步骤的详细实现，而不改变流水线的流程。在接下来的两个部分中，我们将探讨如何在不使用自动日志记录的情况下跟踪模型的指标和参数。
- en: Tracking model metrics
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跟踪模型指标
- en: 'The default metric for the text classification model in the PyTorch `lightning-flash`
    package is **Accuracy**. If we want to change the metric to **F1 score** (a harmonic
    mean of precision and recall), which is a very common metric for measuring a classifier''s
    performance, then we need to change the configuration of the classifier model
    before we start the model training process. Let''s learn how to make this change
    and then use MLflow''s non-auto-logging API to log the metrics:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch `lightning-flash`包中的文本分类模型的默认指标是**准确率**。如果我们想将指标更改为**F1分数**（精度和召回率的调和平均数），这是衡量分类器性能的常用指标，那么我们需要在开始模型训练过程之前更改分类器模型的配置。让我们学习如何进行此更改，并使用MLflow的非自动日志记录API来记录指标：
- en: 'When defining the classifier variable, instead of using the default metric,
    we will pass a metric function called `torchmetrics.F1` as a variable, as follows:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在定义分类器变量时，我们将传递一个名为`torchmetrics.F1`的度量函数作为变量，而不是使用默认的度量函数，如下所示：
- en: '[PRE14]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This uses the built-in metrics function of `torchmetrics`, the `F1` module,
    along with the number of classes in the data we need to classify as a parameter.
    This makes sure that the model is trained and tested using this new metric. You
    will see an output similar to the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这使用了`torchmetrics`的内置度量函数`F1`模块，并将数据中需要分类的类数作为参数。这确保了模型在训练和测试时使用了这个新指标。您将看到类似以下的输出：
- en: '[PRE15]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This shows that the model training and testing were using the F1 score as the
    metric, not the default accuracy metric. For more information on how you can use
    `torchmetrics` for customized metrics, please consult its documentation site:
    [https://torchmetrics.readthedocs.io/en/latest/](https://torchmetrics.readthedocs.io/en/latest/).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明模型的训练和测试使用的是F1分数作为指标，而不是默认的准确率指标。有关如何使用`torchmetrics`自定义指标的更多信息，请参考其文档网站：[https://torchmetrics.readthedocs.io/en/latest/](https://torchmetrics.readthedocs.io/en/latest/)。
- en: 'Now, if we want to log all the metrics to the MLflow tracking server, including
    the training, validation, and testing metrics, we need to get all the current
    metrics by calling the trainer''s callback function, as follows:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，如果我们想将所有指标记录到MLflow跟踪服务器中，包括训练、验证和测试指标，我们需要通过调用训练器的回调函数来获取所有当前的指标，如下所示：
- en: '[PRE16]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, we need to cast all the metric values to `float` to make sure that they
    are compatible with the MLflow `log_metrics` API:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将所有度量值转换为`float`，以确保它们与MLflow的`log_metrics` API兼容：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we can call MLflow''s `log_metrics` to log all the metrics in the tracking
    server:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以调用MLflow的`log_metrics`来记录所有在跟踪服务器中的指标：
- en: '[PRE18]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You will see the following metrics after using the F1 score as the classifier''s
    metric, which will be logged in MLflow''s tracking server:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用F1分数作为分类器指标后，您将看到以下指标，这些指标将被记录在MLflow的跟踪服务器中：
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Using MLflow's `log_metrics` API gives us more control with additional lines
    of code, but if we are satisfied with its auto-logging capabilities, then the
    only thing we need to change is what metric we want to use for the model training
    and testing processes. In this case, we only need to define a new metric to use
    when declaring a new DL model (that is, use the F1 score instead of the default
    accuracy metric).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MLflow的`log_metrics` API让我们通过额外的代码行获得更多控制，但如果我们对其自动日志记录功能满意，那么我们只需要改变我们想要在模型训练和测试过程中使用的度量。此时，我们只需要在声明新的深度学习模型时定义一个新的度量（即使用F1分数而不是默认的准确率度量）。
- en: 'If you want to track multiple model metrics simultaneously, such as the F1
    score, accuracy, precision, and recall, then the only thing you need to do is
    define a Python list of metrics you want to compute and track, as follows:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您想同时跟踪多个模型度量，例如F1分数、准确率、精确度和召回率，那么您需要做的就是定义一个包含您想要计算和跟踪的度量的Python列表，如下所示：
- en: '[PRE20]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, in the model initialization statement, instead of passing a single metric
    to the `metrics` parameter, you can just pass the `list_of_metrics` Python list
    that we just defined, above the `metrics` parameter, as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在模型初始化语句中，您可以不传递单个度量到`metrics`参数，而是传递我们刚刚定义的`list_of_metrics` Python列表，如下所示：
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'No more changes need to be made to the rest of the code. So, in the `dl_model-non-auto-tracking.ipynb`
    notebook (https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model-non-auto-tracking.ipynb),
    you will notice that the preceding line is commented out by default. However,
    you can uncomment it and then comment out the previous one:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的代码无需再做任何更改。因此，在`dl_model-non-auto-tracking.ipynb`笔记本（https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model-non-auto-tracking.ipynb）中，您会注意到前一行默认被注释掉。然而，您可以取消注释它，然后注释掉前面的那一行：
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, when you run the rest of the notebook, you will get the model testing
    reports, along with the following metrics, in the notebook''s output:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当您运行笔记本的其余部分时，您将在笔记本输出中获得模型测试报告，附带以下度量：
- en: '[PRE23]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: You may notice that the numbers for accuracy, F1, precision, and recall are
    the same. This is because, by default, `torchmetrics` uses a `torchmetrics` does
    not support a `none` method, which computes the metric for each class and returns
    the metric for each class, even for a binary classification model. So, this does
    not produce a single scalar number. However, you can always call scikit-learn's
    metrics API to compute an F1-score or other metrics based on the binary average
    method by passing two lists of values. Here, we can use `y_true` and `y_predict`,
    where `y_true` is the list of ground truth label values and `y_predict` is the
    list of model predicted label values. This can be a good exercise for you to try
    out as this is a common practice for all ML models, not special treatment for
    a DL model.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到准确率、F1分数、精确度和召回率的数字是相同的。这是因为，默认情况下，`torchmetrics`使用一个不支持`none`方法的`torchmetrics`，该方法为每个类计算度量，并返回每个类的度量，即使是二分类模型。因此，这不会生成一个单一的标量值。然而，您总是可以调用scikit-learn的度量API，通过传递两个值列表来根据二元平均方法计算F1分数或其他度量。在这里，我们可以使用`y_true`和`y_predict`，其中`y_true`是地面真实标签值的列表，而`y_predict`是模型预测标签值的列表。这可以作为一个很好的练习，供您尝试，因为这是所有机器学习模型的常见做法，而不仅仅是深度学习模型的特殊处理。
- en: Tracking model parameters
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跟踪模型参数
- en: As we have already seen, there are lots of benefits of using auto-logging in
    MLflow, but if we want to track additional model parameters, we can either use
    MLflow to log additional parameters on top of what auto-logging records, or directly
    use MLflow to log all the parameters we want without using auto-logging at all.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，使用MLflow的自动日志记录有许多好处，但如果我们想要跟踪额外的模型参数，我们可以使用MLflow在自动日志记录记录的基础上记录额外的参数，或者直接使用MLflow记录我们想要的所有参数，而不使用自动日志记录。
- en: 'Let''s walk through a notebook without using MLflow auto-logging. If we want
    to have full control of what parameters will be logged by MLflow, we can use two
    APIs: `mlflow.log_param` and `mlflow.log_params`. The first one logs a single
    pair of key-value parameters, while the second logs an entire dictionary of key-value
    parameters. So, what kind of parameters might we be interested in tracking? The
    following answers this:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在不使用MLflow自动日志记录的情况下走一遍笔记本。如果我们想要完全控制MLflow记录哪些参数，可以使用两个API：`mlflow.log_param`和`mlflow.log_params`。第一个用于记录单个键值对参数，而第二个用于记录整个键值对参数的字典。那么，我们可能会感兴趣跟踪哪些类型的参数呢？以下是答案：
- en: '`log_params` API to record them in the experiment.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `log_params` API 在实验中记录它们。
- en: '**Model parameters**: These parameters are learned during the model training
    process. For a DL model, these usually refer to the neural network weights that
    are learned during training. We don''t need to log these weight parameters individually
    since they are already in the logged DL model.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型参数**：这些参数是在模型训练过程中学习到的。对于深度学习模型，这通常指的是在训练过程中学习到的神经网络权重。我们不需要单独记录这些权重参数，因为它们已经包含在已记录的深度学习模型中。'
- en: 'Let''s log these hyperparameters using MLflow''s `log_params` API, as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 MLflow 的 `log_params` API 来记录这些超参数，代码如下：
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Note that here, we log the maximal number of epochs, the trainer's first optimizer's
    name, the optimizer's default parameters, and the overall classifier's hyperparameters
    (`classifier_model.hparams`). The one-line piece of code `mlflow.log_params(params)`
    logs all the key-value parameters in the `params` dictionary to the MLflow tracking
    server. If you see the following hyperparameters in the MLflow tracking server,
    then it means it works!
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这里，我们记录了最大轮数、训练器的第一个优化器名称、优化器的默认参数以及整体分类器的超参数（`classifier_model.hparams`）。那行代码
    `mlflow.log_params(params)` 将 `params` 字典中的所有键值参数记录到 MLflow 跟踪服务器。如果你在 MLflow
    跟踪服务器中看到以下超参数，说明它已经生效！
- en: '![Figure 3.10 – MLflow tracking server web UI showing the logged model hyperparameters'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.10 – MLflow 跟踪服务器的 Web UI 显示已记录的模型超参数'
- en: '](img/B18120_03_010.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_03_010.jpg)'
- en: Figure 3.10 – MLflow tracking server web UI showing the logged model hyperparameters
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 – MLflow 跟踪服务器的 Web UI 显示已记录的模型超参数
- en: Notice that this list of parameters is more than what the auto-logger logs as
    we added additional hyperparameters to log in the experiment. If you want to log
    any other customized parameters, you can follow the same pattern in your experiment.
    The complete notebook, without the use of auto-logging, can be checked out in
    this chapter's GitHub repository at [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model-non-auto-tracking.ipynb](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model-non-auto-tracking.ipynb).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个参数列表比自动记录器记录的要多，因为我们在实验中添加了额外的超参数。如果你想记录其他自定义的参数，可以按照相同的模式在你的实验中进行。完整的笔记本（没有使用自动记录）可以在本章的
    GitHub 仓库中查看，链接为 [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model-non-auto-tracking.ipynb](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model-non-auto-tracking.ipynb)。
- en: If you have reached this point in this chapter, then you have successfully implemented
    an MLflow tracking model and its metrics and parameters!
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经读到这一部分，说明你已经成功实现了一个 MLflow 跟踪模型以及其度量和参数！
- en: Summary
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we set up a local MLflow development environment that has full
    support for backend storage and artifact storage using MySQL and the MinIO object
    store. This will be very useful for us when we develop MLflow-supported DL models
    in this book. We started by presenting the open provenance tracking framework
    and asked model provenance tracking questions that are of interest. We worked
    on addressing the issues of auto-logging and successfully registered a trained
    model by loading a trained model from a logged model in MLflow for prediction
    using the `mlflow.pytorch.load_model` API. We also experimented on how to directly
    use MLflow's `log_metrics`, `log_params`, and `log_model` APIs without auto-logging,
    which gives us more control and flexibility over how we can log additional or
    customized metrics and parameters. We were able to answer many of the provenance
    questions by performing model provenance tracking, as well as by providing a couple
    of the questions that require further study of using MLflow to track multi-step
    model pipelines and their deployment.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们设置了一个本地的 MLflow 开发环境，完全支持使用 MySQL 和 MinIO 对象存储进行后端存储和工件存储。这在我们本书中开发 MLflow
    支持的深度学习模型时非常有用。我们首先介绍了开放的溯源跟踪框架，并提出了有关模型溯源的相关问题。我们解决了自动记录的问题，并成功地通过 `mlflow.pytorch.load_model`
    API 从已记录的模型中加载一个训练好的模型进行预测，完成了训练模型的注册工作。我们还尝试了如何在没有自动记录的情况下直接使用 MLflow 的 `log_metrics`、`log_params`
    和 `log_model` API，这使我们能够更好地控制和灵活地记录额外的或自定义的度量和参数。通过执行模型溯源跟踪，我们能够回答许多溯源问题，并提出了几个需要进一步研究的问题，例如使用
    MLflow 跟踪多步骤模型管道及其部署。
- en: We will continue our learning journey in the next chapter and learn how to perform
    code and data tracking using MLflow, which will give us additional power to answer
    data and code-related provenance questions.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将继续我们的学习旅程，学习如何使用MLflow进行代码和数据跟踪，这将为我们提供更多的能力，以回答与数据和代码相关的来源问题。
- en: Further reading
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解本章涉及的更多主题，请查看以下资源：
- en: 'MLflow Docker setup reference: [https://github.com/sachua/mlflow-docker-compose](https://github.com/sachua/mlflow-docker-compose)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLflow Docker设置参考：[https://github.com/sachua/mlflow-docker-compose](https://github.com/sachua/mlflow-docker-compose)
- en: 'MLflow PyTorch autologging implementation: [https://github.com/mlflow/mlflow/blob/master/mlflow/pytorch/_pytorch_autolog.py](https://github.com/mlflow/mlflow/blob/master/mlflow/pytorch/_pytorch_autolog.py)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLflow PyTorch自动日志记录实现：[https://github.com/mlflow/mlflow/blob/master/mlflow/pytorch/_pytorch_autolog.py](https://github.com/mlflow/mlflow/blob/master/mlflow/pytorch/_pytorch_autolog.py)
- en: 'MLflow PyTorch model logging, loading, and registry documentation: [https://www.mlflow.org/docs/latest/python_api/mlflow.pytorch.html](https://www.mlflow.org/docs/latest/python_api/mlflow.pytorch.html)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLflow PyTorch模型日志记录、加载和注册文档：[https://www.mlflow.org/docs/latest/python_api/mlflow.pytorch.html](https://www.mlflow.org/docs/latest/python_api/mlflow.pytorch.html)
- en: 'MLflow parameters and metrics logging documentation: [https://www.mlflow.org/docs/latest/python_api/mlflow.html](https://www.mlflow.org/docs/latest/python_api/mlflow.html)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLflow参数和指标日志记录文档：[https://www.mlflow.org/docs/latest/python_api/mlflow.html](https://www.mlflow.org/docs/latest/python_api/mlflow.html)
- en: 'MLflow model registry documentation: [https://www.mlflow.org/docs/latest/model-registry.html](https://www.mlflow.org/docs/latest/model-registry.html)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLflow模型注册文档：[https://www.mlflow.org/docs/latest/model-registry.html](https://www.mlflow.org/docs/latest/model-registry.html)
- en: 'Digging into big provenance (with SPADE): [https://queue.acm.org/detail.cfm?id=3476885](https://queue.acm.org/detail.cfm?id=3476885)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入了解大规模来源（使用SPADE）：[https://queue.acm.org/detail.cfm?id=3476885](https://queue.acm.org/detail.cfm?id=3476885)
- en: 'How to utilize `torchmetrics` and `lightning-flash`: [https://www.exxactcorp.com/blog/Deep-Learning/advanced-pytorch-lightning-using-torchmetrics-and-lightning-flash](https://www.exxactcorp.com/blog/Deep-Learning/advanced-pytorch-lightning-using-torchmetrics-and-lightning-flash)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用`torchmetrics`和`lightning-flash`：[https://www.exxactcorp.com/blog/Deep-Learning/advanced-pytorch-lightning-using-torchmetrics-and-lightning-flash](https://www.exxactcorp.com/blog/Deep-Learning/advanced-pytorch-lightning-using-torchmetrics-and-lightning-flash)
- en: Why are precision, recall, and F1 score equal when using micro averaging in
    a multi-class problem? [https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/](https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/)
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么在多类问题中使用微平均时，精度、召回率和F1分数相等？[https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/](https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/)
