- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Selecting Libraries and Tools for Natural Language Understanding
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择自然语言理解的库和工具
- en: This chapter will get you set up to process natural language. We will begin
    by discussing how to install Python, and then we will discuss general software
    development tools such as JupyterLab and GitHub. We will also review major Python
    **natural language processing** (**NLP**) libraries, including the **Natural Language
    Toolkit** (**NLTK**), **spaCy**, and **TensorFlow/Keras**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将帮助你设置处理自然语言的环境。我们将首先讨论如何安装 Python，然后讨论一些常用的开发工具，如 JupyterLab 和 GitHub。我们还将回顾几个主要的
    Python **自然语言处理**（**NLP**）库，包括 **自然语言工具包**（**NLTK**）、**spaCy** 和 **TensorFlow/Keras**。
- en: '**Natural language understanding** (**NLU**) technology has benefited from
    a wide assortment of very capable, freely available tools. While these tools are
    very powerful, there is no one library that can do all of the NLP tasks needed
    for all applications, so it is important to understand what the strengths of the
    different libraries are and how to combine them.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言理解**（**NLU**）技术受益于一系列非常强大且免费的工具。虽然这些工具非常强大，但没有一个库可以完成所有应用所需的所有 NLP 任务，因此理解不同库的优缺点以及如何将它们结合起来非常重要。'
- en: Making the best use of these tools will greatly accelerate any NLU development
    project. These tools include the Python language itself, development tools such
    as JupyterLab, and a number of specific natural language libraries that can perform
    many NLU tasks. It is equally important to know that because these tools are widely
    used by many developers, active online communities such as Stack Overflow ([https://stackoverflow.com/](https://stackoverflow.com/))
    have developed. These are great resources for getting answers to specific technical
    questions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最好地利用这些工具将大大加速任何自然语言理解（NLU）开发项目。这些工具包括 Python 语言本身、开发工具如 JupyterLab，以及许多可以执行多种
    NLU 任务的特定自然语言库。 同样重要的是要知道，由于这些工具被许多开发人员广泛使用，像 Stack Overflow（[https://stackoverflow.com/](https://stackoverflow.com/)）这样的活跃在线社区已经发展起来。这些都是解决特定技术问题的绝佳资源。
- en: 'This chapter will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Installing Python
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 Python
- en: Developing software—JupyterLab and GitHub
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发软件—JupyterLab 和 GitHub
- en: Exploring the libraries
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索这些库
- en: Looking at an example
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看一个示例
- en: Since there are many online resources for using tools such as Python, JupyterLab,
    and GitHub, we will only briefly outline their usage here in order to be able
    to spend more time on NLP.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有许多在线资源可以使用 Python、JupyterLab 和 GitHub 等工具，因此我们这里只会简要概述它们的使用，以便能够将更多时间集中在
    NLP 上。
- en: Note
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'For simplicity, we will illustrate the installation of the libraries in the
    base system. However, you may wish to install the libraries in a virtual environment,
    especially if you are working on several different Python projects. The following
    link may be helpful for installing a virtual environment: [https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简便起见，我们将演示如何在基础系统中安装这些库。然而，你可能希望在虚拟环境中安装这些库，特别是当你正在进行多个 Python 项目时。以下链接可能有助于安装虚拟环境：[https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/)。
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To run the examples in this chapter, you will need the following software:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本章中的示例，你需要以下软件：
- en: Python 3
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3
- en: '`pip` or `conda` (preferably `pip`)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pip` 或 `conda`（建议使用 `pip`）'
- en: JupyterLab
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JupyterLab
- en: NLTK
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLTK
- en: spaCy
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: spaCy
- en: Keras
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras
- en: The next sections will go over the process of installing these packages, which
    should be installed in the order in which they are listed here.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的章节将介绍安装这些软件包的过程，建议按照这里列出的顺序安装它们。
- en: Installing Python
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 Python
- en: 'The first step in setting up your development environment is to install Python.
    If you have already installed Python on your system, you can skip to the next
    section, but do make sure that your Python installation includes Python 3, which
    is required by most NLP libraries. You can check your Python version by entering
    the following command in a command-line window, and the version will be displayed:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 设置开发环境的第一步是安装 Python。如果你已经在系统上安装了 Python，可以跳到下一节，但请确保你的 Python 安装包括 Python 3，因为大多数
    NLP 库都需要 Python 3。你可以通过在命令行窗口输入以下命令来检查你的 Python 版本，版本号将会显示出来：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that if you have both Python 2 and Python 3 installed, you may have to
    run the `python3 –version` command to check the Python 3 version. If you don’t
    have Python 3, you’ll need to install it. Some NLP libraries require not just
    Python 3 but Python 3.7 or greater, so if your version of Python is older than
    3.7, you’ll need to update it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果你同时安装了 Python 2 和 Python 3，你可能需要运行 `python3 –version` 命令来检查 Python 3 的版本。如果你没有安装
    Python 3，你将需要安装它。一些 NLP 库不仅要求 Python 3，还要求 Python 3.7 或更高版本，因此，如果你的 Python 版本低于
    3.7，你将需要更新它。
- en: 'Python runs on almost any operating system that you choose to use, including
    Windows, macOS, and Linux. Python can be downloaded for your operating system
    from [http://www.python.org](http://www.python.org). Download the executable installer
    for your operating system and run the installer. When Python is installed, you
    can check the installation by running the preceding command on your command line
    or terminal. You will see the version you’ve just installed, as shown in the following
    command-line output:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Python 可以在你选择的几乎所有操作系统上运行，包括 Windows、macOS 和 Linux。你可以从 [http://www.python.org](http://www.python.org)
    下载适用于你的操作系统的 Python。下载适合操作系统的可执行安装程序并运行。当 Python 安装完成后，你可以通过在命令行或终端中运行上述命令来检查安装情况。你将看到你刚安装的版本，如以下命令行输出所示：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This installs Python, but you will also need to install add-on libraries for
    NLP. Installing libraries is done with the auxiliary programs `pip` and `conda`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这将安装 Python，但你还需要安装 NLP 的附加库。安装库的操作是通过辅助程序 `pip` 和 `conda` 完成的。
- en: '`pip` and `conda` are two cross-platform tools that can be used for installing
    Python libraries. We will be using them to install several important natural language
    and `pip` in this book, but you can also use `conda` if it’s your preferred Python
    management tool. `pip` is included by default with Python versions 3.4 and newer,
    and since you’ll need 3.7 for the NLP libraries, `pip` should be available in
    your Python environment. You can check the version with the following command:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`pip` 和 `conda` 是两个跨平台工具，可以用来安装 Python 库。在本书中，我们将使用它们来安装几种重要的自然语言处理库和 `pip`，但如果你更喜欢使用
    `conda` 作为 Python 管理工具，也是可以的。`pip` 默认包含在 Python 3.4 及更高版本中，而由于你将需要 3.7 版本来安装 NLP
    库，`pip` 应该已经在你的 Python 环境中可用。你可以使用以下命令检查版本：'
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You should see the following output:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the next section, we will discuss the development environment we will be
    using: JupyterLab.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论我们将使用的开发环境：JupyterLab。
- en: Developing software – JupyterLab and GitHub
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发软件 – JupyterLab 和 GitHub
- en: 'The development environment can make all the difference in the efficiency of
    the development process. In this section, we will discuss two popular development
    resources: JupyterLab and GitHub. If you are familiar with other Python **interactive
    development environments** (**IDEs**), then you can go ahead and use the tools
    that you’re familiar with. However, the examples discussed in this book will be
    shown in a JupyterLab environment.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 开发环境对开发过程的效率至关重要。在这一节中，我们将讨论两个流行的开发资源：JupyterLab 和 GitHub。如果你熟悉其他 Python **交互式开发环境**（**IDE**），你可以继续使用你熟悉的工具。然而，本书中的示例将在
    JupyterLab 环境中展示。
- en: JupyterLab
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JupyterLab
- en: JupyterLab is a cross-platform coding environment that makes it easy to experiment
    with different tools and techniques without requiring a lot of setup time. It
    operates in a browser environment but doesn’t require a cloud server—a local server
    is sufficient.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: JupyterLab 是一个跨平台的编码环境，使得你可以在无需大量设置时间的情况下，轻松地实验不同的工具和技术。它在浏览器环境中运行，但不需要云服务器——本地服务器就足够了。
- en: 'Installing JupyterLab is done with the following `pip` command:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 JupyterLab 可以通过以下 `pip` 命令完成：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once JupyterLab is installed, you can run it using the following command:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了 JupyterLab，你可以使用以下命令运行它：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This command should be run in a command line in the directory where you would
    like to keep your code. The command will launch a local server, and the Jupyter
    environment will appear in a browser window, as shown in *Figure 4**.1*:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令应在你希望存放代码的目录中的命令行中运行。该命令将启动一个本地服务器，并且 Jupyter 环境将在浏览器窗口中出现，如 *图 4.1* 所示：
- en: '![Figure 4.1 – JupyterLab user interface on startup](img/B19005_04_01.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – JupyterLab 启动时的用户界面](img/B19005_04_01.jpg)'
- en: Figure 4.1 – JupyterLab user interface on startup
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – JupyterLab 启动时的用户界面
- en: 'The environment shown in *Figure 4**.1* includes three types of content, as
    follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4.1* 中显示的环境包括三种内容，如下所示：'
- en: '**Notebook**—Contains your coding projects'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**笔记本**—包含你的编程项目'
- en: '**Console**—Gives you access to command-line or terminal functions from directly
    within the Jupyter notebook'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制台**—让你可以直接在Jupyter笔记本中访问命令行或终端功能'
- en: '`start` command was run'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start`命令已运行'
- en: When you click on the **Python 3** icon under **Notebook**, you’ll get a new
    notebook showing a coding cell, and you’ll be ready to start coding in Python.
    We’ll return to the JupyterLab environment and start coding in Python in the *Looking
    at an example* section later in this chapter and again in [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当你点击**Python 3**图标下的**Notebook**时，你将得到一个新的笔记本，显示一个代码单元，你就可以开始用Python编写代码。我们将在本章稍后的*查看示例*部分返回到JupyterLab环境，并开始用Python编程，[*第五章*](B19005_05.xhtml#_idTextAnchor107)中也会有相关内容。
- en: GitHub
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GitHub
- en: Many of you are probably already familiar with GitHub, a popular open source
    code repository system ([https://github.com](https://github.com)). GitHub provides
    very extensive capabilities for storing and sharing code, developing code branches,
    and documenting code. The core features of GitHub are currently free.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你们中的许多人可能已经熟悉GitHub，这是一个流行的开源代码仓库系统（[https://github.com](https://github.com)）。GitHub提供了非常广泛的功能，用于存储和共享代码、开发代码分支以及文档化代码。GitHub的核心功能目前是免费的。
- en: The code examples used in this book can be found at [https://github.com/PacktPublishing/Natural-Language-Understanding-with-Python](https://github.com/PacktPublishing/Natural-Language-Understanding-with-Python).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用的代码示例可以在[https://github.com/PacktPublishing/Natural-Language-Understanding-with-Python](https://github.com/PacktPublishing/Natural-Language-Understanding-with-Python)找到。
- en: The next step is to learn about several important libraries, including NLTK,
    spaCy, and Keras, which we will be using extensively in the following chapters.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是学习几个重要的库，包括我们将在接下来的章节中广泛使用的NLTK、spaCy和Keras。
- en: Exploring the libraries
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索库
- en: In this section, we will review several of the major Python libraries that are
    used in NLP; specifically, NLTK, spaCy, and Keras. These are very useful libraries,
    and they can perform most basic NLP tasks. However, as you gain experience with
    NLP, you will also find additional NLP libraries that may be appropriate for specific
    tasks as well, and you are encouraged to explore those.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾几个在自然语言处理（NLP）中使用的主要Python库；具体来说是NLTK、spaCy和Keras。这些都是非常有用的库，能够执行大多数基础的NLP任务。然而，随着你在NLP领域经验的积累，你还会发现一些额外的NLP库，可能适用于特定任务，鼓励你去探索这些库。
- en: Using NLTK
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用NLTK
- en: NLTK ([https://www.nltk.org/](https://www.nltk.org/)) is a very popular open
    source Python library that greatly reduces the effort involved in developing natural
    language applications by providing support for many frequently performed tasks.
    NLTK also includes many corpora (sets of ready-to-use natural language texts)
    that can be used for exploring NLP problems and testing algorithms.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK（[https://www.nltk.org/](https://www.nltk.org/)）是一个非常流行的开源Python库，它通过提供对许多常见任务的支持，极大地减少了开发自然语言应用程序的工作量。NLTK还包括许多语料库（即一套可以直接使用的自然语言文本），这些语料库可以用于探索NLP问题和测试算法。
- en: In this section, we will go over what NLTK can do, and then discuss the NLTK
    installation process.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍NLTK的功能，并讨论NLTK的安装过程。
- en: As we discussed in [*Chapter 3*](B19005_03.xhtml#_idTextAnchor059), many distinct
    tasks can be performed in an NLU pipeline as the processing moves from raw words
    to a final determination of the meaning of a document. NLTK can perform many of
    these tasks. Most of these functions don’t provide results that are directly useful
    in themselves, but they can be very helpful as part of a pipeline.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第三章*](B19005_03.xhtml#_idTextAnchor059)中讨论的那样，在自然语言理解（NLU）管道中可以执行许多不同的任务，处理从原始单词到最终确定文档含义的过程。NLTK可以执行其中许多任务。这些功能大多数本身并不会直接提供有用的结果，但它们可以作为管道的一部分，提供很大的帮助。
- en: 'Some of the basic tasks that are needed in nearly all natural language projects
    can easily be done with NLTK. For example, texts to be processed need to be broken
    down into words before processing. We can do this with NLTK’s `word_tokenize`
    function, as shown in the following code snippet:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在几乎所有自然语言项目中，一些基本任务可以轻松通过NLTK完成。例如，待处理的文本需要在处理前被拆分成单词。我们可以使用NLTK的`word_tokenize`函数来实现这一点，如下所示的代码片段所示：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The result will be an array of words:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是一个单词数组：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note that the word `we''d` is separated into two components, `we` and `''d`,
    because it is a contraction that actually represents two words: *we* and *would*.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，单词`we'd`被分为两个部分，`we`和`'d`，因为它是一个缩写，实际上代表了两个词：*we*和*would*。
- en: 'NLTK also provides some functions for basic statistics such as counting word
    frequencies in a text. For example, continuing from the text we just looked at,
    *we’d like to book a flight from Boston to London*, we can use the NLTK `FreqDist()`
    function to count how often each word occurs:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 还提供了一些用于基本统计的函数，比如计算文本中单词频率。例如，从我们刚才看到的文本 *我们想要预定从波士顿到伦敦的航班* 开始，我们可以使用
    NLTK 的 `FreqDist()` 函数来计算每个单词出现的频率：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this example, we imported the `FreqDist()` function from NLTK’s `probability`
    package and used it to count the frequencies of each word in the text. The result
    is a Python dict where the keys are the words and the values are how often the
    words occur. The word `to` occurs twice, and each of the other words occurs once.
    For such a short text, the frequency distribution is not particularly insightful,
    but it can be very helpful when you’re looking at larger amounts of data. We will
    see the frequency distribution for a large corpus in the *Looking at an example*
    section later in this chapter.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们从 NLTK 的 `probability` 包中导入了 `FreqDist()` 函数，并用它来统计文本中每个单词的频率。结果是一个
    Python 字典，其中键是单词，值是单词出现的次数。单词 `to` 出现了两次，其他每个单词出现了一次。对于如此简短的文本，频率分布并没有特别的启示性，但在查看更大量数据时，它会非常有用。我们将在本章稍后的
    *查看示例* 部分看到一个大语料库的频率分布。
- en: 'NLTK can also do `nltk.pos_tag(tokenized_text)` function is used for POS tagging:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 还可以执行 `nltk.pos_tag(tokenized_text)` 函数来进行词性标注：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Similarly, NLTK provides functions for parsing texts. Recall that parsing was
    discussed in [*Chapter 3*](B19005_03.xhtml#_idTextAnchor059). As discussed in
    [*Chapter 1*](B19005_01.xhtml#_idTextAnchor016), NLTK also provides functions
    for creating and applying **regular** **expressions** (**regexes**).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，NLTK 提供了用于解析文本的函数。回想一下，[*第 3 章*](B19005_03.xhtml#_idTextAnchor059)讨论了解析。正如[*第
    1 章*](B19005_01.xhtml#_idTextAnchor016)中所讨论的，NLTK 还提供了创建和应用 **正则** **表达式** (**regexes**)
    的函数。
- en: These are some of the most useful capabilities of NLTK. The full set of NLTK
    capabilities is too large to list here, but we will be reviewing some of these
    other capabilities in [*Chapter 6*](B19005_06.xhtml#_idTextAnchor134) and [*Chapter
    8*](B19005_08.xhtml#_idTextAnchor159).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是 NLTK 最有用的功能之一。NLTK 的完整功能集合过于庞大，无法在这里列出，但我们将在 [*第 6 章*](B19005_06.xhtml#_idTextAnchor134)
    和 [*第 8 章*](B19005_08.xhtml#_idTextAnchor159) 中回顾这些其他功能。
- en: Installing NLTK
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 NLTK
- en: 'NLTK requires Python 3.7 or greater. The installation process for Windows is
    to run the following command in a command window:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 需要 Python 3.7 或更高版本。在 Windows 上的安装过程是运行以下命令：
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'For a Mac or Unix environment, run the following command in a terminal window:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Mac 或 Unix 环境，在终端窗口中运行以下命令：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the next section, we will go over another popular NLU library, spaCy, and
    explain what it can do. As with NLTK, we will be using spaCy extensively in later
    chapters.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍另一个流行的 NLU 库，spaCy，并解释它能做什么。与 NLTK 一样，我们将在后续章节中广泛使用 spaCy。
- en: Using spaCy
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 spaCy
- en: spaCy is another very popular package that can do many of the same NLP tasks
    as NLTK. Both toolkits are very capable. spaCy is generally faster, and so is
    more suitable for deployed applications. Both toolkits support many languages,
    but not all NLU tasks are supported for all languages, so in making a choice between
    NLTK and spaCy, it is important to consider the specific language requirements
    for that application.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 是另一个非常流行的包，可以执行与 NLTK 相似的许多 NLP 任务。两个工具包都非常强大。spaCy 通常更快，因此更适合用于已部署的应用程序。两个工具包都支持多种语言，但并非所有
    NLU 任务都支持所有语言，因此在选择 NLTK 和 spaCy 时，考虑特定应用的语言需求非常重要。
- en: As with NLTK, spaCy can perform many basic text-processing functions. Let's
    check it out!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 与 NLTK 一样，spaCy 也可以执行许多基本的文本处理功能。让我们来看看吧！
- en: 'The code to set up tokenization in spaCy is very similar to the code for NLTK,
    with a slightly different function name. The result is an array of words, where
    each element is one token. Note that the `nlp` object is initialized with an `en_core_web_sm`
    model that tells it to use the statistics from a particular set of web-based data,
    `en_core_web_sm`:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在 spaCy 中设置分词的代码与 NLTK 中的代码非常相似，只有函数名称略有不同。结果是一个包含单词的数组，每个元素都是一个标记。请注意，`nlp`
    对象是使用 `en_core_web_sm` 模型初始化的，该模型告诉它使用特定网页数据集的统计信息，`en_core_web_sm`：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can also calculate statistics such as the frequency of the words that occur
    in the text:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以计算文本中单词出现频率等统计数据：
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The only difference between spaCy and NLTK is that NLTK uses the `FreqDist`
    function and spaCy uses the `Counter` function. The result, a Python dict with
    the words as keys and the frequencies as values, is the same for both libraries.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 和 NLTK 之间唯一的区别是，NLTK 使用 `FreqDist` 函数，而 spaCy 使用 `Counter` 函数。结果——一个 Python
    字典，单词作为键，频率作为值——在这两个库中是相同的。
- en: 'Just as with NLTK, we can perform POS tagging with spaCy:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 和 NLTK 一样，我们也可以使用 spaCy 执行词性标注：
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This results in the following POS assignments:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下的词性标注结果：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Unfortunately, NLTK and spaCy use different labels for the different parts of
    speech. This is not necessarily a problem, because there is no *correct* or *standard*
    set of parts of speech, even for one language. However, it’s important for the
    parts of speech to be consistent within an application, so developers should be
    aware of this difference and be sure not to confuse the NLTK and spaCy parts of
    speech.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，NLTK 和 spaCy 使用不同的标签来表示不同的词性。这不一定是一个问题，因为即使对于同一种语言，也没有*正确的*或*标准的*词性集合。然而，在一个应用程序中，词性的一致性是非常重要的，所以开发者需要意识到这种差异，并确保不要混淆
    NLTK 和 spaCy 的词性。
- en: Another very useful capability that spaCy has is **named entity recognition**
    (**NER**). NER is the task of identifying references to specific persons, organizations,
    locations, or other entities that occur in a text. NER can be either an end in
    itself or it can be part of another task. For example, a company might be interested
    in finding when their products are mentioned on Facebook, so NER for their products
    would be all that they need. On the other hand, a company might be interested
    in finding out if their products are mentioned in a positive or negative way,
    so in that case, they would want to perform both NER and **sentiment** **analysis**
    (**SA**).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 还有另一个非常有用的功能，那就是**命名实体识别**（**NER**）。NER 是识别文本中提到的特定人物、组织、地点或其他实体的任务。NER
    可以作为一个独立的任务，也可以是另一个任务的一部分。例如，一家公司可能会对自己产品在 Facebook 上的提及感兴趣，那么他们只需要进行产品的 NER。另一方面，如果一家公司想知道他们的产品是以积极还是消极的方式被提及，他们就需要同时执行
    NER 和**情感分析**（**SA**）。
- en: 'NER can be performed in most NLP libraries; however, it is particularly easy
    to do in spaCy. Given a document, we just have to request rendering of the document
    using the `ent` style, as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 NLP 库都可以执行命名实体识别（NER）；然而，在 spaCy 中特别容易做到。给定一个文档，我们只需要请求使用 `ent` 样式渲染文档，如下所示：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The rendered result shows that the `boston` and `new york` named entities are
    assigned a **geopolitical entity** (**GPE**) label, as shown in *Figure 4**.2*:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 渲染结果显示，`boston` 和 `new york` 这两个命名实体被赋予了**地理政治实体**（**GPE**）标签，如 *图 4.2* 所示：
- en: '![Figure 4.2 – NER for “we’d like to book a flight from Boston to New York”](img/B19005_04_02.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – 对于“我们想预订从波士顿到纽约的航班”的命名实体识别（NER）](img/B19005_04_02.jpg)'
- en: Figure 4.2 – NER for “we’d like to book a flight from Boston to New York”
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 对于“我们想预订从波士顿到纽约的航班”的命名实体识别（NER）
- en: 'Parsing or the analysis of syntactic relationships among the words in a sentence
    can be done very easily with almost the same code, just by changing the value
    of the `style` parameter to `dep` from `ent`. We’ll see an example of a syntactic
    parse later on in *Figure 4**.6*:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 解析或分析句子中单词之间的句法关系非常容易，只需将 `style` 参数的值从 `ent` 改为 `dep`，几乎可以使用相同的代码。稍后我们将在 *图
    4.6* 中看到一个句法解析的示例：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Installing spaCy is done with the following `pip` command:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 spaCy 可以通过以下 `pip` 命令完成：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The next library we will look at is the Keras ML library.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们要看的库是 Keras ML 库。
- en: Using Keras
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Keras
- en: Keras ([https://keras.io/](https://keras.io/)) is another popular Python NLP
    library. Keras is much more focused on ML than NLTK or spaCy and will be the go-to
    library for NLP **deep learning** (**DL**) applications in this book. It’s built
    on top of another package called TensorFlow ([https://www.tensorflow.org/](https://www.tensorflow.org/)),
    which was developed by Google. Because Keras is built on TensorFlow, TensorFlow
    functions can be used in Keras.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Keras ([https://keras.io/](https://keras.io/)) 是另一个流行的 Python NLP 库。Keras 比
    NLTK 或 spaCy 更加专注于机器学习（ML），并将在本书中作为 NLP **深度学习**（**DL**）应用的首选库。它建立在另一个名为 TensorFlow
    ([https://www.tensorflow.org/](https://www.tensorflow.org/)) 的包之上，TensorFlow 是由
    Google 开发的。由于 Keras 是基于 TensorFlow 构建的，因此可以在 Keras 中使用 TensorFlow 的函数。
- en: Since Keras focuses on ML, it has limited capabilities for preprocessing text.
    For example, unlike NLTK or spaCy, it does not support POS tagging or parsing
    directly. If these capabilities are needed, then it’s best to preprocess the text
    with NLTK or spaCy. Keras does support tokenization and removal of extraneous
    tokens such as punctuation and HTML markup.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Keras 专注于机器学习（ML），它在文本预处理方面的功能有限。例如，与 NLTK 或 spaCy 不同，它不直接支持词性标注（POS tagging）或句法分析（parsing）。如果需要这些功能，最好使用
    NLTK 或 spaCy 来进行文本预处理。Keras 支持分词和去除多余的标记，例如标点符号和 HTML 标记。
- en: Keras is especially strong for text-processing applications using **neural networks**
    (**NN**). This will be discussed in much more detail in [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184).
    Although Keras includes few high-level functions for performing NLP functions
    such as POS tagging or parsing in one step, it does include capabilities for training
    POS taggers from a dataset and then deploying the tagger in an application.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 特别适用于使用**神经网络**（**NN**）的文本处理应用。这将在 [*第 10 章*](B19005_10.xhtml#_idTextAnchor184)
    中详细讨论。尽管 Keras 包含的高阶功能较少，不能一步完成像词性标注（POS tagging）或句法分析（parsing）这样的 NLP 功能，但它确实提供了从数据集训练词性标注器并将其部署到应用中的能力。
- en: 'Since Keras is included in TensorFlow, Keras is automatically installed when
    TensorFlow is installed. It is not necessary to install Keras as an additional
    step. Thus, the following command is sufficient:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Keras 已包含在 TensorFlow 中，安装 TensorFlow 时会自动安装 Keras。因此，不需要额外安装 Keras。只需执行以下命令即可：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Learning about other NLP libraries
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解其他 NLP 库
- en: There are quite a few other Python libraries that include NLP capabilities and
    that can be useful in some cases. These include PyTorch ([https://pytorch.org/](https://pytorch.org/))
    for processing based on **deep neural networks** (**DNN**), scikit-learn ([https://scikit-learn.org/stable/](https://scikit-learn.org/stable/)),
    which includes general ML functions, and Gensim ([https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)),
    for topic modeling, among others. However, I would recommend working with the
    basic packages that we’ve covered here for a few projects at first until you get
    more familiar with NLP. If you later have a requirement for additional functionality,
    a different language, or faster processing speed than what the basic packages
    provide, you can explore some of these other packages at that time.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 还有不少其他 Python 库也包括 NLP 功能，并且在某些情况下非常有用。这些库包括 PyTorch（[https://pytorch.org/](https://pytorch.org/)），它基于**深度神经网络**（**DNN**）进行处理；scikit-learn（[https://scikit-learn.org/stable/](https://scikit-learn.org/stable/)），它包含一般的机器学习（ML）功能；以及
    Gensim（[https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)），用于主题建模等等。然而，我建议你首先使用我们这里介绍的基本包来做几个项目，直到你对
    NLP 更加熟悉。如果以后你有对额外功能、不同语言或比基本包更快的处理速度的需求，再去探索这些其他的包。
- en: In the next topic, we will discuss how to choose among NLP libraries. It’s good
    to keep in mind that choosing libraries isn’t an all-or-none process—libraries
    can easily be mixed and matched if one library has strengths that another one
    doesn’t.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个话题中，我们将讨论如何在 NLP 库中进行选择。值得记住的是，选择库并不是一个“非此即彼”的过程——如果一个库有其他库没有的优点，可以轻松地将不同的库混合使用。
- en: Choosing among NLP libraries
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 NLP 库中进行选择
- en: The libraries discussed in the preceding sections are all very useful and powerful.
    In some cases, they have overlapping capabilities. This raises the question of
    selecting which libraries to use in a particular application. Although all of
    the libraries can be combined in the same application, it reduces the complexity
    of applications if fewer libraries are used.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 前面讨论的库都非常有用且强大。在某些情况下，它们的功能有所重叠。这就提出了一个问题：如何在特定应用中选择使用哪些库。尽管所有这些库都可以在同一个应用中组合使用，但如果使用较少的库，可以降低应用的复杂性。
- en: NLTK is very strong in corpus statistics and rule-based linguistic preprocessing.
    For example, some useful corpus statistics include counting words, counting parts
    of speech, counting pairs of words (bigrams), and tabulating words in context
    (concordances). spaCy is fast, and its displaCy visualization library is very
    helpful in gaining insight into processing results. Keras is very strong in DL.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 在语料库统计和基于规则的语言学预处理中非常强大。例如，一些有用的语料库统计包括计数单词、计数词性、计数词对（bigrams），以及列出上下文中的单词（concordances）。spaCy
    速度很快，其 displaCy 可视化库对于了解处理结果非常有帮助。Keras 在深度学习（DL）方面非常强大。
- en: During the lifetime of a project, it is often useful to start with tools that
    help you quickly get a good overall picture of the data, such as NLTK and spaCy.
    This initial analysis will be very helpful for selecting the tools that are needed
    for full-scale processing and deployment. Since training DL models using tools
    such as Keras can be very time-consuming, doing some preliminary investigation
    with more traditional approaches will help narrow down the possibilities that
    need to be investigated in order to select a DL approach.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目的生命周期中，通常有必要从一些工具开始，帮助你快速获得数据的整体概览，例如 NLTK 和 spaCy。这一初步分析将有助于选择需要的工具，以进行全面的处理和部署。由于使用
    Keras 等工具训练深度学习模型可能非常耗时，因此使用更传统的方法进行一些初步调查将有助于缩小需要进一步调查的范围，从而选择合适的深度学习方法。
- en: Learning about other packages useful for NLP
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解其他对 NLP 有用的包
- en: 'In addition to packages that directly support NLP, there are also a number
    of other useful general-purpose open source Python packages that provide tools
    for generally managing data, including natural language data. These include the
    following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 除了直接支持自然语言处理（NLP）的包外，还有许多其他有用的通用开源 Python 包，提供了用于一般数据管理的工具，包括自然语言数据。这些包包括以下内容：
- en: '**NumPy**: NumPy ([https://numpy.org/](https://numpy.org/)) is a powerful package
    that includes many functions for the numerical calculations that we’ll be working
    with in [*Chapter 9*](B19005_09.xhtml#_idTextAnchor173), [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184),[*Chapter
    11*](B19005_11.xhtml#_idTextAnchor193), and [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NumPy**: NumPy（[https://numpy.org/](https://numpy.org/)）是一个强大的包，包含许多用于数值计算的函数，我们将在[*第
    9 章*](B19005_09.xhtml#_idTextAnchor173)、[*第 10 章*](B19005_10.xhtml#_idTextAnchor184)、[*第
    11 章*](B19005_11.xhtml#_idTextAnchor193)和[*第 12 章*](B19005_12.xhtml#_idTextAnchor217)中使用这些函数'
- en: '**pandas**: pandas ([https://pandas.pydata.org/](https://pandas.pydata.org/))
    provides general tools for data analysis and manipulation, including natural language
    data, especially data in the form of tables'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pandas**: pandas（[https://pandas.pydata.org/](https://pandas.pydata.org/)）提供了用于数据分析和处理的一般工具，包括自然语言数据，特别是表格形式的数据'
- en: '**scikit-learn**: scikit-learn is a powerful package for ML, including text
    processing ([https://scikit-learn.org/stable/](https://scikit-learn.org/stable/))'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**scikit-learn**: scikit-learn 是一个强大的机器学习包，包括文本处理功能（[https://scikit-learn.org/stable/](https://scikit-learn.org/stable/)）'
- en: 'There are also several visualization packages that will be very helpful for
    graphical representations of data and for processing results. Visualization is
    important in NLP development because it can often give you a much more comprehensible
    representation of results than a numerical table. For example, visualization can
    help you see trends, pinpoint errors, and compare experimental conditions. We’ll
    be using visualization tools throughout the book, but especially in [*Chapter
    6*](B19005_06.xhtml#_idTextAnchor134). Visualization tools include generic tools
    for representing different kinds of numerical results, whether they have to do
    with NLP or not, as well as tools specifically designed to represent natural language
    information such as parses and NER results. Visualization tools include the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 还有几个可视化包，对于数据的图形表示和处理结果非常有帮助。可视化在 NLP 开发中非常重要，因为它通常能比数字表格提供更易理解的结果表示。例如，可视化可以帮助你发现趋势、定位错误并比较实验条件。我们将在全书中使用可视化工具，尤其是在[*第
    6 章*](B19005_06.xhtml#_idTextAnchor134)中。可视化工具包括用于表示不同类型数字结果的通用工具，无论这些结果是否与 NLP
    相关，也包括专门设计用于表示自然语言信息（如句法分析和命名实体识别结果）的工具。可视化工具包括以下内容：
- en: '**Matplotlib**: Matplotlib ([https://matplotlib.org/](https://matplotlib.org/))
    is a popular Python visualization library that’s especially good at creating plots
    of data, including NLP data. If you’re trying to compare the results of processing
    with several different techniques, plotting the results can often provide insights
    very quickly about how well the different techniques are working, which can be
    helpful for evaluation. We will be returning to the topic of evaluation in [*Chapter
    13*](B19005_13.xhtml#_idTextAnchor226)*.*'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Matplotlib**: Matplotlib（[https://matplotlib.org/](https://matplotlib.org/)）是一个流行的
    Python 可视化库，尤其擅长创建数据图表，包括 NLP 数据。如果你尝试比较使用几种不同技术处理的结果，绘制结果图表通常可以迅速提供有关这些技术效果的洞察，这对评估非常有帮助。我们将在[*第
    13 章*](B19005_13.xhtml#_idTextAnchor226)中再次讨论评估主题。'
- en: '**Seaborn**: Seaborn ([https://seaborn.pydata.org/](https://seaborn.pydata.org/))
    is based on Matplotlib. It enables developers to produce attractive graphs representing
    statistical information.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Seaborn**：Seaborn（[https://seaborn.pydata.org/](https://seaborn.pydata.org/)）基于Matplotlib，能够帮助开发者绘制出代表统计信息的美观图表。'
- en: '**displaCy**: displaCy is part of the spaCy tools, and is especially good at
    representing natural language results such as POS tags, parses, and named entities,
    which we discussed in [*Chapter 3*](B19005_03.xhtml#_idTextAnchor059).'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**displaCy**：displaCy是spaCy工具的一部分，特别擅长表示自然语言结果，如词性标注、句法分析和命名实体，这些我们在[*第3章*](B19005_03.xhtml#_idTextAnchor059)中讨论过。'
- en: '**WordCloud**: WordCloud ([https://amueller.github.io/word_cloud/](https://amueller.github.io/word_cloud/))
    is a specialized library for visualizing word frequencies in a corpus, which can
    be useful when word frequencies are of interest. We’ll see an example of a word
    cloud in the next section.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WordCloud**：WordCloud（[https://amueller.github.io/word_cloud/](https://amueller.github.io/word_cloud/)）是一个专门用于可视化语料库中单词频率的库，当单词频率很重要时非常有用。我们将在下一部分看到一个词云的示例。'
- en: Up to this point, we’ve reviewed the technical requirements for our software
    development environment as well as the NLP libraries that we’ll be working with.
    In the next section, we’ll put everything together with an example.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经回顾了软件开发环境的技术要求以及我们将要使用的NLP库。在接下来的部分，我们将通过一个示例将所有内容结合起来。
- en: Looking at an example
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查看一个示例
- en: To illustrate some of these concepts, we’ll work through an example using JupyterLab
    where we explore an SA task for movie reviews. We’ll look at how we can apply
    the NLTK and spaCy packages to get some ideas about what the data is like, which
    will help us plan further processing.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这些概念，我们将在JupyterLab中做一个示例，探索电影评论的情感分析（SA）任务。我们将看看如何应用NLTK和spaCy包来了解数据的特征，这将帮助我们规划后续的处理步骤。
- en: The corpus (or dataset) that we’ll be looking at is a popular set of 2,000 movie
    reviews, classified as to whether the writer expressed a positive or negative
    sentiment about the movie ([http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要查看的数据集是一个流行的2,000条电影评论集，按评论者是否表达了对电影的积极或消极情感进行分类（[http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)）。
- en: Dataset citation
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集引用
- en: '*Bo Pang* and *Lillian Lee*, *Seeing stars: Exploiting class relationships
    for sentiment categorization with respect to rating scales*, *Proceedings of the*
    *ACL*, *2005*.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*Bo Pang* 和 *Lillian Lee*，*Seeing stars: Exploiting class relationships for
    sentiment categorization with respect to rating scales*, *Proceedings of the*
    *ACL*, *2005*。'
- en: This is a good example of the task of SA, which was introduced in [*Chapter
    1*](B19005_01.xhtml#_idTextAnchor016).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这是情感分析（SA）任务的一个很好的示例，我们在[*第1章*](B19005_01.xhtml#_idTextAnchor016)中已经介绍过。
- en: Setting up JupyterLab
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置JupyterLab
- en: 'We’ll be working with JupyterLab, so let’s start it up. As we saw earlier,
    you can start JupyterLab by simply typing the following command into a command
    (Windows) or terminal (Mac) window:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用JupyterLab，所以让我们启动它。正如我们之前看到的，你可以通过在命令（Windows）或终端（Mac）窗口中输入以下命令来启动JupyterLab：
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This will start a local web server and open a JupyterLab window in a web browser.
    In the JupyterLab window, open a new notebook by selecting **File** | **New**
    | **Notebook**, and an untitled notebook will appear (you can rename it at any
    time by selecting **File** | **Rename Notebook**).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动一个本地Web服务器，并在Web浏览器中打开JupyterLab窗口。在JupyterLab窗口中，通过选择**文件** | **新建** |
    **笔记本**来打开一个新的笔记本，随后会出现一个未命名的笔记本（你可以随时通过选择**文件** | **重命名笔记本**来更改其名称）。
- en: 'We’ll start by importing the libraries that we’ll be using in this example,
    as shown next. We’ll be using the NLTK and spaCy NLP libraries, as well as some
    general-purpose libraries for numerical operations and visualization. We’ll see
    how these are used as we go through the example:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从导入我们将在这个示例中使用的库开始，如下所示。我们将使用NLTK和spaCy的NLP库，以及一些用于数值运算和可视化的通用库。随着示例的展开，我们将看到这些库的使用方法：
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Enter the preceding code into a JupyterLab cell and run the cell. Running this
    cell (**Run** | **Run Selected Cells**) will import the libraries and give you
    a new code cell.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在JupyterLab单元格中输入上述代码并运行。运行此单元格（**运行** | **运行选定单元格**）将导入库，并为你创建一个新的代码单元格。
- en: 'Download the movie review data by typing `nltk.download()` into the new code
    cell. This will open a new **NLTK Downloader** window, as shown in *Figure 4**.3*:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在新的代码单元格中输入`nltk.download()`来下载电影评论数据。这将打开一个新的**NLTK下载器**窗口，如*图4.3*所示：
- en: "![Figure 4.3 – NLTK \uFEFFDownloader window](img/B19005_04_03.jpg)"
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3 – NLTK 下载窗口](img/B19005_04_03.jpg)'
- en: Figure 4.3 – NLTK Downloader window
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – NLTK 下载窗口
- en: On the `movie_reviews`. Click the **Download** button to download the corpus.
    You can select **File** | **Change Download Directory** if you want to change
    where the data is downloaded. Click **File** | **Exit** from the downloader window
    to exit from the **Download** window and return to the JupyterLab interface.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在`movie_reviews`上。点击**下载**按钮下载语料库。如果你想改变数据下载的路径，可以选择**文件** | **更改下载目录**。点击**文件**
    | **退出**退出下载窗口并返回到JupyterLab界面。
- en: 'If you take a look at the directory where you downloaded the data, you will
    see two directories: `neg` and `pos`. The directories contain negative and positive
    reviews, respectively. This represents the annotation of the reviews; that is,
    a human annotator’s opinion of whether the review was positive or negative. This
    directory structure is a common approach for representing text classification
    annotations, and you’ll see it in many datasets. The `README` file in the `movie_reviews`
    folder explains some details on how the annotation was done.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看你下载数据的目录，你会看到两个文件夹：`neg`和`pos`。这些文件夹分别包含负面和正面评论。这代表了评论的标注，即人工标注员对于评论是正面还是负面的判断。这种目录结构是文本分类标注的常见方式，你将在许多数据集中看到它。`movie_reviews`文件夹中的`README`文件解释了标注的具体细节。
- en: If you look at some of the movie reviews in the corpus, you’ll see that the
    correct annotation for a text is not always obvious.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看语料库中的一些电影评论，你会发现，文本的正确标注并不总是显而易见的。
- en: 'The next code block shows importing the reviews and printing one sentence from
    the corpus:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个代码块显示了导入评论并打印语料库中的一个句子：
- en: '[PRE22]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Since `movie_reviews` is an NLTK corpus, a number of corpora methods are available,
    including listing the sentences as an array of individual sentences. We can also
    select individual sentences from the corpus by number, as shown in the previous
    code block, where we selected and printed sentence number nine in the corpus.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`movie_reviews`是一个NLTK语料库，提供了多种语料库方法，包括将句子列出为单独的句子数组。我们还可以按编号选择语料库中的单个句子，如前面的代码块所示，我们选择并打印了语料库中的第九个句子。
- en: You can see that the sentences have been tokenized, or separated into individual
    words (including punctuation marks). This is an important preparatory step in
    nearly all NLP applications.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到句子已经被分词，或者说被拆分成了单个单词（包括标点符号）。这是几乎所有自然语言处理应用中的重要准备步骤。
- en: Processing one sentence
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理一个句子
- en: Now, let’s do some actual NLP processing for this sample sentence. We’ll use
    the spaCy library to perform POS tagging and rule-based parsing and then visualize
    the results with the displaCy library.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们对这个样本文本进行实际的自然语言处理。我们将使用spaCy库进行词性标注和基于规则的解析，然后使用displaCy库可视化结果。
- en: 'We first need to create an `nlp` object based on web data, `en_core_web_sm`,
    which is a basic small English model. There are larger models available, but they
    take longer to load, so we will stick with the small model here for brevity. Then,
    we use the `nlp` object to identify the parts of speech and parse this sentence,
    as shown in the following code block:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要创建一个基于网页数据的`nlp`对象，`en_core_web_sm`，它是一个基本的小型英语模型。虽然也有更大的模型可用，但它们加载速度较慢，因此为了简便起见，我们在这里使用小型模型。接着，我们使用`nlp`对象识别词性并解析这个句子，如以下代码块所示：
- en: '[PRE23]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In the `displacy.render` command, we have requested a dependency parse (`styles=''dep''`).
    This is a type of analysis we’ll go into in more detail in [*Chapter 8*](B19005_08.xhtml#_idTextAnchor159).
    For now, it’s enough to say that it’s one common approach to showing how the words
    in a sentence are related to each other. The resulting dependency parse is shown
    in *Figure 4**.4*:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在`displacy.render`命令中，我们请求了一个依赖解析（`styles='dep'`）。这是一种我们将在[**第8章**](B19005_08.xhtml#_idTextAnchor159)中详细探讨的分析方法。目前，简单来说，它是展示句子中各单词之间关系的常见方法。最终的依赖解析如*图
    4.4*所示：
- en: '![Figure 4.4 – Dependency parse for “they get in an accident”](img/B19005_04_04.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4 – “他们发生了事故”的依赖解析](img/B19005_04_04.jpg)'
- en: Figure 4.4 – Dependency parse for “they get in an accident”
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – “他们发生了事故”的依赖解析
- en: Now that we’ve loaded the corpus and looked at a few examples of the kinds of
    sentences it contains, we will look at some of the overall properties of the corpus.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载了语料库并查看了其中的一些句子示例，接下来我们将查看语料库的一些整体属性。
- en: Looking at corpus properties
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看语料库属性
- en: While corpora have many properties, some of the most interesting and insightful
    properties are word frequencies and POS frequencies, which we will be reviewing
    in the next two sections. Unfortunately, there isn’t space to explore additional
    corpus properties in detail, but looking at word and POS frequencies should get
    you started.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然语料库有许多属性，但一些最有趣且富有洞察力的属性是单词频率和词性频率，我们将在接下来的两节中回顾这些内容。不幸的是，由于篇幅限制，我们无法详细探讨其他语料库属性，但查看单词和词性频率应该能帮助你入门。
- en: Word frequencies
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单词频率
- en: 'In this section, we will look at some properties of the full corpus. For example,
    we can look at the most frequent words using the following code:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看整个语料库的一些属性。例如，我们可以使用以下代码查看最频繁的单词：
- en: '[PRE24]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As seen in the preceding code, we start by collecting the words in the `movie_review`
    corpus by using the `words()` method of the corpus object. We then count the words
    with NLTK’s `FreqDist()` function. At the same time, we are lowercasing the words
    and ignoring non-alpha words such as numbers and punctuation. Then, for clarity
    in the visualization, we’ll restrict the words we’ll look at to the most frequent
    25 words. You may be interested in trying different values of `top_words` in the
    code block to see how the graph looks with more and fewer words.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，我们首先通过使用语料库对象的`words()`方法来收集`movie_review`语料库中的单词。然后，我们使用NLTK的`FreqDist()`函数来统计单词的频率。与此同时，我们将单词转换为小写，并忽略掉数字和标点符号等非字母单词。接着，为了让可视化更清晰，我们将查看的单词限制为最频繁的25个单词。你可能会对在代码块中尝试不同的`top_words`值感兴趣，以查看图表在更多或更少单词情况下的效果。
- en: 'When we call `plt.show()`, the distribution of word frequencies is displayed.
    This can be seen in *Figure 4**.5*:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们调用`plt.show()`时，单词频率分布会被显示出来。这可以在*图 4**.5*中看到：
- en: '![Figure 4.5 – Visualizing the most frequent words in the movie review corpus](img/B19005_04_05.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.5 – 可视化电影评论语料库中的最频繁单词](img/B19005_04_05.jpg)'
- en: Figure 4.5 – Visualizing the most frequent words in the movie review corpus
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – 可视化电影评论语料库中的最频繁单词
- en: As *Figure 4**.5* shows, the most frequent word, not surprisingly, is **the**,
    which is about twice as frequent as the second most common word, **a**.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 正如*图 4**.5*所示，最频繁出现的单词毫无意外是**the**，其出现频率约是第二常见单词**a**的两倍。
- en: 'An alternative visualization of word frequencies that can also be helpful is
    a `all_fdist`, and displaying it with Matplotlib:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种也可能有帮助的单词频率可视化方法是`all_fdist`，并使用Matplotlib进行显示：
- en: '[PRE25]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The resulting word cloud is shown in *Figure 4**.6*. We can see that very frequent
    words such as **the** and **a** appear in very large fonts in comparison to other
    words:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的词云展示在*图 4**.6*中。我们可以看到，像**the**和**a**这样非常频繁的单词，相较于其他单词，显示得非常大：
- en: '![Figure 4.6 – A word cloud for the top 25 words in the movie review corpus](img/B19005_04_06.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6 – 电影评论语料库中前25个单词的词云](img/B19005_04_06.jpg)'
- en: Figure 4.6 – A word cloud for the top 25 words in the movie review corpus
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – 电影评论语料库中前25个单词的词云
- en: Notice that nearly all of the frequent words are words generally used in most
    English texts. The exception is **film**, which is to be expected in a corpus
    of movie reviews. Since most of these frequent words occur in the majority of
    texts, their occurrence won’t enable us to distinguish different categories of
    texts. If we’re dealing with a classification problem such as SA, we should consider
    removing these common words from texts before we try to train an SA classifier
    on this corpus. These kinds of words are called **stopwords**, and their removal
    is a common preprocessing step. We will discuss stopword removal in detail in
    [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107)*.*
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，几乎所有频繁出现的单词都是在大多数英语文本中常用的词汇。唯一的例外是**film**，这在电影评论语料库中是可以预料到的。由于这些频繁出现的单词出现在大多数文本中，它们的出现不会帮助我们区分不同类型的文本。如果我们处理的是情感分析（SA）等分类问题，我们应该考虑在尝试对该语料库进行情感分析分类器训练之前，从文本中去除这些常见单词。这些单词被称为**停用词**，它们的移除是常见的预处理步骤。我们将在[*第5章*](B19005_05.xhtml#_idTextAnchor107)*.*中详细讨论停用词移除。
- en: POS frequencies
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词性频率
- en: We can also look at the most frequent parts of speech with the following code.
    To reduce the complexity of the graph, we’ll restrict the display to the 18 most
    common parts of speech. After we tag the words, we loop through the sentences,
    counting up the occurrences of each tag. Then, the list of tags is sorted with
    the most frequent tags first.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用以下代码查看最常见的词性。为了减少图表的复杂性，我们将显示限制为 18 个最常见的词性。标注完单词之后，我们会遍历句子，统计每个标签的出现次数。然后，标签列表会按出现频率从高到低排序。
- en: The parts of speech used in the NLTK POS tagging code are the widely used Penn
    Treebank parts of speech, documented at [https://www.cs.upc.edu/~nlp/SVMTool/PennTreebank.html](https://www.cs.upc.edu/~nlp/SVMTool/PennTreebank.html).
    This tagset includes 36 tags overall. Previous work on NLP has found that the
    traditional English parts of speech (noun, verb, adjective, adverb, conjunction,
    interjection, pronoun, and preposition) are not fine-grained enough for computational
    purposes, so additional parts of speech are normally added. For example, different
    forms of verbs, such as *walk*, *walks*, *walked*, and *walking*, are usually
    assigned different parts of speech. For example, *walk* is assigned the *VB*—or
    *Verb base* form—POS, but *walks* is assigned the *VBZ*—or *Verb, third - person
    singular present*—POS. Traditionally, these would all be called *verbs*.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK POS 标注代码中使用的词性标签是广泛使用的 Penn Treebank 词性标签，文档可以参考[https://www.cs.upc.edu/~nlp/SVMTool/PennTreebank.html](https://www.cs.upc.edu/~nlp/SVMTool/PennTreebank.html)。这个标签集总共有
    36 个标签。之前的自然语言处理研究发现，传统的英语词性（名词、动词、形容词、副词、连词、感叹词、代词和介词）对于计算目的来说不够精细，因此通常会添加额外的词性标签。例如，不同形式的动词，如*walk*、*walks*、*walked*
    和 *walking*，通常会被分配不同的词性。例如，*walk* 会被分配为 *VB* ——即动词原形——而 *walks* 则会被分配为 *VBZ* ——即动词第三人称单数现在时——词性。传统上，这些都被称为*动词*。
- en: 'First, we’ll extract the sentences from the corpus, and then tag each word
    with its part of speech. It’s important to perform POS tagging on an entire sentence
    rather than just individual words, because many words have multiple parts of speech,
    and the POS assigned to a word depends on the other words in its sentence. For
    example, *book* at the beginning of *book a flight* can be recognized and tagged
    as a verb, while in *I read the book*, *book* can be tagged as a noun. In the
    following code, we display the frequencies of the parts of speech in this corpus:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们会从语料库中提取句子，并对每个单词进行词性标注。重要的是，要对整个句子进行词性标注，而不仅仅是单独的单词，因为许多单词有多种词性，而分配给单词的词性取决于它所在句子的其他单词。例如，*book*
    在 *book a flight* 这个句子开头时可以被识别并标注为动词，而在 *I read the book* 中，*book* 则可以被标注为名词。在以下代码中，我们展示了该语料库中词性的频率：
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In the movie review corpus, we can see that by far the most common tag is *NN*
    or *common noun*, followed by *IN* or *preposition or coordinating conjunction*,
    and *DT* or *determiner*.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在电影评论语料库中，我们可以看到，最常见的标签是 *NN* 或 *普通名词*，其次是 *IN* 或 *介词或并列连词*，以及 *DT* 或 *限定词*。
- en: 'The result, again using the Matplotlib and Seaborn libraries, is shown graphically
    in *Figure 4**.7*:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，再次使用 Matplotlib 和 Seaborn 库，通过图形展示在*图 4.7*中：
- en: '![Figure 4.7 – Visualizing the most frequent parts of speech in the movie review
    corpus](img/B19005_04_07.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7 – 可视化电影评论语料库中最常见的词性](img/B19005_04_07.jpg)'
- en: Figure 4.7 – Visualizing the most frequent parts of speech in the movie review
    corpus
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – 可视化电影评论语料库中最常见的词性
- en: We can look at other text properties such as the distribution of the lengths
    of the texts, and we can compare the properties of the positive and negative reviews
    to see if we can find some properties that distinguish the two categories. Do
    positive and negative reviews have different average lengths or different distributions
    of parts of speech? If we notice some differences, then we can make use of them
    in classifying new reviews.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看其他文本属性，比如文本长度的分布，比较正面和负面评论的属性，看是否能找到一些区分这两类评论的特征。正面和负面评论的平均长度是否不同？它们的词性分布有差异吗？如果我们注意到一些差异，那么我们可以在分类新的评论时加以利用。
- en: Summary
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the major development tools and Python libraries
    that are used in NLP application development. We discussed the JupyterLab development
    environment and the GitHub software repository system. The major libraries that
    we covered were NLTK, spaCy, and Keras. Although this is by no means an exhaustive
    list of NLP libraries, it’s sufficient to get a start on almost any NLP project.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了NLP应用开发中使用的主要开发工具和Python库。我们讨论了JupyterLab开发环境和GitHub软件仓库系统。我们介绍的主要库有NLTK、spaCy和Keras。虽然这不是一个详尽无遗的NLP库列表，但足以启动几乎任何NLP项目。
- en: We covered installation and basic usage for the major libraries, and we provided
    some suggested tips on selecting libraries. We summarized some useful auxiliary
    packages, and we concluded with a simple example of how the libraries can be used
    to do some NLP tasks.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了主要库的安装和基本使用，并提供了一些选择库的建议技巧。我们总结了一些有用的辅助包，并以一个简单的示例结束，展示了这些库如何用于执行一些NLP任务。
- en: The topics discussed in this chapter have given you a basic understanding of
    the most useful Python packages for NLP, which you will be using for the rest
    of the book. In addition, the discussion in this chapter has given you a start
    on understanding the principles for selecting tools for future projects. We have
    achieved our goal of getting you set up with tools for processing natural language,
    along with an illustration of some simple text processing using NLTK and spaCy
    and visualization with Matplotlib and Seaborn.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的内容使你对最有用的Python包在NLP中的应用有了基本了解，这些包将在本书的其余部分中使用。此外，本章的讨论也帮助你开始理解如何为未来的项目选择工具的原则。我们已经实现了帮助你设置自然语言处理工具的目标，并通过使用NLTK和spaCy进行简单文本处理以及使用Matplotlib和Seaborn进行可视化来进行说明。
- en: In the next chapter, we will look at how to identify and prepare data for processing
    with NLP techniques. We will discuss data from databases, the web, and other documents,
    as well as privacy and ethics considerations. For readers who don’t have access
    to their own data or who wish to compare their results to those of other researchers,
    this chapter will also discuss generally available corpora. It will then go on
    to discuss preprocessing steps such as tokenization, stemming, stopword removal,
    and lemmatization.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论如何识别和准备数据，以便使用NLP技术进行处理。我们将讨论来自数据库、网络和其他文档的数据，以及隐私和伦理考虑。对于没有自己数据的读者或希望将结果与其他研究人员的结果进行比较的读者，本章还将讨论一些通用的语料库。接着，我们将讨论一些预处理步骤，如分词、词干提取、去除停用词和词形还原。
