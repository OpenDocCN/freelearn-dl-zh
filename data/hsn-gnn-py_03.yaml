- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Creating Node Representations with DeepWalk
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DeepWalk 创建节点表示
- en: '**DeepWalk** is one of the first major successful applications of **machine
    learning** (**ML**) techniques to graph data. It introduces important concepts
    such as embeddings that are at the core of GNNs. Unlike traditional neural networks,
    the goal of this architecture is to produce **representations** that are then
    fed to other models, which perform downstream tasks (for example, node classification).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**DeepWalk** 是**机器学习**（**ML**）技术在图数据中最早且最成功的应用之一。它引入了像嵌入（embeddings）这样的重要概念，这些概念是图神经网络（GNN）核心的一部分。与传统的神经网络不同，该架构的目标是生成**表示**，这些表示会被传递给其他模型，后者执行下游任务（例如，节点分类）。'
- en: 'In this chapter, we will learn about the DeepWalk architecture and its two
    major components: `gensim` library on a **natural language processing** (**NLP**)
    example to understand how it is supposed to be used.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习 DeepWalk 架构及其两个主要组成部分：在**自然语言处理**（**NLP**）的例子中使用 `gensim` 库来理解它应该如何使用。
- en: 'Then, we will focus on the DeepWalk algorithm and see how performance can be
    improved using **hierarchical softmax** (**H-Softmax**). This powerful optimization
    of the softmax function can be found in many fields: it is incredibly useful when
    you have a lot of possible classes in your classification task. We will also implement
    random walks on a graph before wrapping things up with an end-to-end supervised
    classification exercise on Zachary’s Karate Club.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将专注于 DeepWalk 算法，看看如何使用**层次化 softmax**（**H-Softmax**）提高性能。这个强大的 softmax
    函数优化在许多领域都可以找到：当您的分类任务中有大量可能的类别时，它非常有用。我们还将在图上实现随机游走，最后通过一个关于 Zachary’s Karate
    Club 的端到端监督分类练习来总结。
- en: By the end of this chapter, you will master Word2Vec in the context of NLP and
    beyond. You will be able to create node embeddings using the topological information
    of the graphs and solve classification tasks on graph data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将掌握 Word2Vec 在 NLP 及其他领域中的应用。您将能够使用图的拓扑信息创建节点嵌入，并在图数据上解决分类任务。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Introducing Word2Vec
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 Word2Vec
- en: DeepWalk and random walks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepWalk 和随机游走
- en: Implementing DeepWalk
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现 DeepWalk
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: All the code examples from this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter03](https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter03).
    Installation steps required to run the code on your local machine can be found
    in the *Preface* section of this book.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有代码示例可以在 GitHub 上找到：[https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter03](https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter03)。运行代码所需的安装步骤可以在本书的*前言*部分找到。
- en: Introducing Word2Vec
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Word2Vec
- en: 'The first step to comprehending the DeepWalk algorithm is to understand its
    major component: Word2Vec.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 DeepWalk 算法的第一步是理解其主要组成部分：Word2Vec。
- en: Word2Vec has been one of the most influential deep-learning techniques in NLP.
    Published in 2013 by Tomas Mikolov et al. (Google) in two different papers, it
    proposed a new technique to translate words into vectors (also known as **embeddings**)
    using large datasets of text. These representations can then be used in downstream
    tasks, such as sentiment classification. It is also one of the rare examples of
    patented and popular ML architecture.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 是自然语言处理（NLP）中最具影响力的深度学习技术之一。由 Tomas Mikolov 等人（谷歌）于 2013 年在两篇不同的论文中发布，它提出了一种新技术，通过使用大量的文本数据集将单词转换为向量（也称为**嵌入**）。这些表示可以在下游任务中使用，如情感分类。它也是少数几种已获得专利且广受欢迎的机器学习架构之一。
- en: 'Here are a few examples of how Word2Vec can transform words into vectors:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些 Word2Vec 如何将单词转换为向量的例子：
- en: '![](img/Formula_B19153_03_001.jpg)![](img/Formula_B19153_03_002.jpg)![](img/Formula_B19153_03_003.jpg)![](img/Formula_B19153_03_004.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_03_001.jpg)![](img/Formula_B19153_03_002.jpg)![](img/Formula_B19153_03_003.jpg)![](img/Formula_B19153_03_004.jpg)'
- en: 'We can see in this example that, in terms of the Euclidian distance, the word
    vectors for *king* and *queen* are closer than the ones for *king* and *woman*
    (4.37 versus 8.47). In general, other metrics, such as the popular **cosine similarity**,
    are used to measure the likeness of these words. Cosine similarity focuses on
    the angle between vectors and does not consider their magnitude (length), which
    is more helpful in comparing them. Here is how it is defined:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以看到，按照欧几里得距离，*king*（国王）和*queen*（女王）的词向量距离比*king*（国王）和*woman*（女人）更近（4.37
    与 8.47）。通常，其他度量方法，如流行的**余弦相似度**，用于衡量这些单词的相似度。余弦相似度关注的是向量之间的角度，而不考虑它们的大小（长度），这在比较向量时更为有用。其定义如下：
- en: '![](img/Formula_B19153_03_005.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_03_005.jpg)'
- en: 'One of the most surprising results of Word2Vec is its ability to solve analogies.
    A popular example is how it can answer the question “*man is to woman, what king
    is to ___?*” It can be calculated as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 最令人惊讶的结果之一是其解决类比问题的能力。一个流行的例子是它如何回答问题：“*man is to woman, what king
    is to ___?*”（男人与女人的关系，国王与 ___ 的关系？）它可以按如下方式计算：
- en: '![](img/Formula_B19153_03_006.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_03_006.jpg)'
- en: This is not true with any analogy, but this property can bring interesting applications
    to perform arithmetic operations with embeddings.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不适用于所有类比，但这一特性可以为使用嵌入进行算术操作带来有趣的应用。
- en: CBOW versus skip-gram
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CBOW 与跳字模型的对比
- en: 'A model must be trained on a pretext task to produce these vectors. The task
    itself does not need to be meaningful: its only goal is to produce high-quality
    embeddings. In practice, this task is always related to predicting words given
    a certain context.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 模型必须在一个预设任务上进行训练，以生成这些向量。任务本身不需要具有实际意义：其唯一目标是生成高质量的嵌入。在实践中，这个任务通常与根据特定上下文预测单词相关。
- en: 'The authors proposed two architectures with similar tasks:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提出了两种架构，执行相似任务：
- en: '**The continuous bag-of-words (CBOW) model**: This is trained to predict a
    word using its surrounding context (words coming before and after the target word).
    The order of context words does not matter since their embeddings are summed in
    the model. The authors claim to obtain better results using four words before
    and after the one that is predicted.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续袋词（CBOW）模型**：这个模型通过上下文（目标词前后出现的单词）来预测一个单词。由于上下文单词的嵌入在模型中会被求和，所以上下文单词的顺序并不重要。作者称，通过使用目标词前后各四个单词，能够获得更好的结果。'
- en: '**The continuous skip-gram model**: Here, we feed a single word to the model
    and try to predict the words around it. Increasing the range of context words
    leads to better embeddings but also increases the training time.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续跳字模型**：在这里，我们将一个单词输入模型，并尝试预测其周围的单词。增加上下文单词的范围可以得到更好的嵌入，但也会增加训练时间。'
- en: 'In summary, here are the inputs and outputs of both models:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，以下是两种模型的输入和输出：
- en: '![Figure 3.1 – CBOW and skip-gram architectures](img/B19153_03_001.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – CBOW 和跳字模型架构](img/B19153_03_001.jpg)'
- en: Figure 3.1 – CBOW and skip-gram architectures
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – CBOW 和跳字模型架构
- en: 'In general, the CBOW model is considered faster to train, but the skip-gram
    model is more accurate thanks to its ability to learn infrequent words. This topic
    is still debated in the NLP community: a different implementation could fix issues
    related to CBOW in some contexts.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，CBOW 模型被认为训练速度较快，但跳字模型更准确，因为它能够学习不常见的单词。这个话题在 NLP 社区中仍然存在争议：不同的实现方式可能会解决某些情况下
    CBOW 的问题。
- en: Creating skip-grams
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建跳字模型
- en: 'For now, we will focus on the skip-gram model since it is the architecture
    used by DeepWalk. Skip-grams are implemented as pairs of words with the following
    structure: ![](img/Formula_B19153_03_007.png), where ![](img/Formula_B19153_03_008.png)
    is the input and ![](img/Formula_B19153_03_009.png) is the word to predict. The
    number of skip grams for the same target word depends on a parameter called **context
    size**, as shown in *Figure 3**.2*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将专注于跳字模型，因为它是 DeepWalk 使用的架构。跳字模型以单词对的形式实现，结构如下：![](img/Formula_B19153_03_007.png)，其中
    ![](img/Formula_B19153_03_008.png) 是输入，![](img/Formula_B19153_03_009.png) 是待预测的单词。同一目标词的跳字对数依赖于一个叫做**上下文大小**的参数，如
    *图 3.2* 所示：
- en: '![Figure 3.2 – Text to skip-grams](img/B19153_03_002.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – 跳字模型](img/B19153_03_002.jpg)'
- en: Figure 3.2 – Text to skip-grams
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 跳字模型
- en: The same idea can be applied to a corpus of text instead of a single sentence.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的思路可以应用于文本语料库，而不仅仅是单个句子。
- en: In practice, we store all the context words for the same target word in a list
    to save memory. Let’s see how it’s done with an example on an entire paragraph.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，我们将相同目标词的所有上下文词汇存储在一个列表中，以节省内存。让我们通过一个完整段落的例子来看看这是如何做到的。
- en: 'In the following example, we create skip-grams for an entire paragraph stored
    in the `text` variable. We set the `CONTEXT_SIZE` variable to `2`, which means
    we will look at the two words before and after our target word:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们为存储在 `text` 变量中的整个段落创建了 skip-grams。我们将 `CONTEXT_SIZE` 变量设置为 `2`，意味着我们将查看目标词前后各两个词：
- en: 'Let’s start by importing the necessary libraries:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们开始导入必要的库：
- en: '[PRE0]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we need to set the `CONTEXT_SIZE` variable to `2` and bring in the text
    we want to analyze:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们需要将 `CONTEXT_SIZE` 变量设置为 `2`，并引入我们想要分析的文本：
- en: '[PRE1]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we create the skip-grams thanks to a simple `for` loop to consider every
    word in `text`. A list comprehension generates the context words, stored in the
    `skipgrams` list:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通过一个简单的 `for` 循环来创建 skip-grams，考虑 `text` 中的每个词。列表推导式生成上下文词汇，并将其存储在 `skipgrams`
    列表中：
- en: '[PRE2]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, use the `print()` function to see the skip-grams we generated:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用 `print()` 函数查看我们生成的 skip-grams：
- en: '[PRE3]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This produces the following output:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会产生如下输出：
- en: '[PRE4]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: These two target words, with their corresponding context, work to show what
    the inputs to Word2Vec look like.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个目标词及其对应的上下文，展示了 Word2Vec 输入数据的样子。
- en: The skip-gram model
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Skip-gram 模型
- en: The goal of Word2Vec is to produce high-quality word embeddings. To learn these
    embeddings, the training task of the skip-gram model consists of predicting the
    correct context words given a target word.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 的目标是生成高质量的词向量。为了学习这些词向量，skip-gram 模型的训练任务包括给定目标词，预测正确的上下文词汇。
- en: 'Imagine that we have a sequence of ![](img/Formula_B19153_03_010.png) words
    ![](img/Formula_B19153_03_011.png). The probability of seeing the word ![](img/Formula_B19153_03_012.png)
    given the word ![](img/Formula_B19153_03_0121.png) is written ![](img/Formula_B19153_03_014.png).
    Our goal is to maximize the sum of every probability of seeing a context word
    given a target word in an entire text:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个词汇序列 ![](img/Formula_B19153_03_010.png) 和 ![](img/Formula_B19153_03_011.png)。给定词语
    ![](img/Formula_B19153_03_0121.png)，看到词语 ![](img/Formula_B19153_03_012.png) 的概率写作
    ![](img/Formula_B19153_03_014.png)。我们的目标是最大化在整篇文本中，看到每个上下文词汇给定目标词汇的概率之和：
- en: '![](img/Formula_B19153_03_015.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_03_015.jpg)'
- en: Where ![](img/Formula_B19153_03_016.png) is the size of the context vector.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/Formula_B19153_03_016.png) 是上下文向量的大小。
- en: Note
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Why do we use a log probability in the previous equation? Transforming probabilities
    into log probabilities is a common technique in ML (and computer science in general)
    for two main reasons.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们在之前的方程式中使用对数概率？将概率转换为对数概率是机器学习（以及计算机科学一般）中常见的技术，主要有两个原因。
- en: 'Products become additions (and divisions become subtractions). Multiplications
    are more computationally expensive than additions, so it’s faster to compute the
    log probability:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 加法变成了加法（除法变成了减法）。乘法的计算开销比加法大，因此计算对数概率更快：
- en: '![](img/Formula_B19153_03_017.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_03_017.jpg)'
- en: The way computers store very small numbers (such as 3.14e-128) is not perfectly
    accurate, unlike the log of the same numbers (-127.5 in this case). These small
    errors can add up and bias the final results when events are extremely unlikely.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机存储非常小的数字（如 3.14e-128）并不完全准确，而同样数字的对数（在此案例中为 -127.5）却非常精确。这些小的误差会累积，并在事件极不可能发生时影响最终结果。
- en: On the whole, this simple transformation allows us to gain speed and accuracy
    without changing our initial objective.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这个简单的转换让我们在不改变初始目标的情况下，获得了更快的速度和更高的准确性。
- en: 'The basic skip-gram model uses the softmax function to calculate the probability
    of a context word embedding ![](img/Formula_B19153_03_018.png) given a target
    word embedding ![](img/Formula_B19153_03_019.png):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的 skip-gram 模型使用 softmax 函数来计算给定目标词向量 ![](img/Formula_B19153_03_019.png) 的上下文词向量
    ![](img/Formula_B19153_03_018.png) 的概率：
- en: '![](img/Formula_B19153_03_020.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_03_020.jpg)'
- en: 'Where ![](img/Formula_B19153_03_021.png) is the vocabulary of size ![](img/Formula_B19153_03_022.png).
    This vocabulary corresponds to the list of unique words the model tries to predict.
    We can obtain this list using the `set` data structure to remove duplicate words:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/Formula_B19153_03_021.png) 是大小为 ![](img/Formula_B19153_03_022.png)
    的词汇表。这个词汇表对应模型尝试预测的唯一词汇列表。我们可以使用 `set` 数据结构去除重复的词汇来获取这个列表：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This gives us the following output:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这会给我们如下输出：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now that we have the size of our vocabulary, there is one more parameter we
    need to define: ![](img/Formula_B19153_03_023.png), the dimensionality of the
    word vectors. Typically, this value is set between 100 and 1,000\. In this example,
    we will set it to 10 because of the limited size of our dataset.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了词汇表的大小，还有一个参数需要定义： ![](img/Formula_B19153_03_023.png)，即词向量的维度。通常，这个值设置在
    100 到 1,000 之间。在这个示例中，由于数据集的大小有限，我们将其设置为 10。
- en: 'The skip-gram model is composed of only two layers:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Skip-gram 模型由仅两个层组成：
- en: A **projection layer** with a weight matrix ![](img/Formula_B19153_03_024.png),
    which takes a one-hot encoded-word vector as an input and returns the corresponding
    ![](img/Formula_B19153_03_025.png)-dim word embedding. It acts as a simple lookup
    table that stores embeddings of a predefined dimensionality.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 **投影层**，其权重矩阵为 ![](img/Formula_B19153_03_024.png)，该层以独热编码的词向量作为输入，并返回相应的
    ![](img/Formula_B19153_03_025.png)-维词嵌入。它充当一个简单的查找表，存储预定义维度的嵌入。
- en: A **fully connected layer** with a weight matrix ![](img/Formula_B19153_03_026.png),
    which takes a word embedding as input and outputs ![](img/Formula_B19153_03_027.png)-dim
    logits. A softmax function is applied to these predictions to transform logits
    into probabilities.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 **全连接层**，其权重矩阵为 ![](img/Formula_B19153_03_026.png)，该层以词嵌入作为输入并输出 ![](img/Formula_B19153_03_027.png)-维的
    logits。对这些预测应用 softmax 函数，以将 logits 转换为概率。
- en: Note
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'There is no activation function: Word2Vec is a linear classifier that models
    a linear relationship between words.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 没有激活函数：Word2Vec 是一个线性分类器，它建模单词之间的线性关系。
- en: 'Let’s call ![](img/Formula_B19153_03_028.png) the one-hot encoded-word vector
    the *input*. The corresponding word embedding can be calculated as a simple projection:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 ![](img/Formula_B19153_03_028.png) 称为独热编码的词向量，作为 *输入*。相应的词嵌入可以通过简单的投影来计算：
- en: '![](img/Formula_B19153_03_029.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_03_029.jpg)'
- en: 'Using the skip-gram model, we can rewrite the previous probability as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 skip-gram 模型，我们可以将先前的概率重写为以下形式：
- en: '![](img/Formula_B19153_03_030.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_03_030.jpg)'
- en: 'The skip-gram model outputs a ![](img/Formula_B19153_03_031.png)-dim vector,
    which is the conditional probability of every word in the vocabulary:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Skip-gram 模型输出一个 ![](img/Formula_B19153_03_031.png)-维向量，这是词汇表中每个单词的条件概率：
- en: '![](img/Formula_B19153_03_032.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_03_032.jpg)'
- en: During training, these probabilities are compared to the correct one-hot encoded-target
    word vectors. The difference between these values (calculated by a loss function
    such as the cross-entropy loss) is backpropagated through the network to update
    the weights and obtain better predictions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，这些概率会与正确的独热编码目标词向量进行比较。通过损失函数（如交叉熵损失）计算的这些值之间的差异会通过网络进行反向传播，以更新权重并获得更好的预测结果。
- en: 'The entire Word2Vec architecture is summarized in the following diagram, with
    both matrices and the final softmax layer:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 整个 Word2Vec 架构在以下图示中进行了总结，包含了矩阵和最终的 softmax 层：
- en: '![Figure 3.3 – The Word2Vec architecture](img/B19153_03_003.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3 – Word2Vec 架构](img/B19153_03_003.jpg)'
- en: Figure 3.3 – The Word2Vec architecture
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – Word2Vec 架构
- en: 'We can implement this model using the `gensim` library, which is also used
    in the official implementation of DeepWalk. We can then build the vocabulary and
    train our model based on the previous text:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `gensim` 库来实现这个模型，`gensim` 也被官方用于 DeepWalk 的实现。然后我们可以根据前面的文本构建词汇表并训练我们的模型：
- en: 'Let’s begin by installing `gensim` and importing the `Word2Vec` class:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们安装 `gensim` 并导入 `Word2Vec` 类：
- en: '[PRE7]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We initialize a skip-gram model with a `Word2Vec` object and an `sg=1` parameter
    (skip-gram = 1):'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 `Word2Vec` 对象和 `sg=1` 参数（skip-gram = 1）初始化一个 skip-gram 模型：
- en: '[PRE8]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It’s a good idea to check the shape of our first weight matrix. It should correspond
    to the vocabulary size and the word embeddings’ dimensionality:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查我们第一个权重矩阵的形状是个好主意。它应当与词汇表的大小以及词嵌入的维度相对应：
- en: '[PRE9]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This produces the following output:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会生成以下输出：
- en: '[PRE10]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we train the model for `10` epochs:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们训练模型 `10` 轮：
- en: '[PRE11]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, we can print a word embedding to see what the result of this training
    looks like:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以打印一个词嵌入，以查看该训练的结果是怎样的：
- en: '[PRE12]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This gives us the following output:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '[PRE13]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: While this approach works well with small vocabularies, the computational cost
    of applying a full softmax function to millions of words (the vocabulary size
    ) is too costly in most cases. This has been a limiting factor in developing accurate
    language models for a long time. Fortunately for us, other approaches have been
    designed to solve this issue.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种方法在小词汇量的情况下效果很好，但将完整 softmax 函数应用于数百万个词汇（词汇量）的计算成本在大多数情况下是太高昂的。长期以来，这一点一直是开发准确语言模型的一个限制因素。幸运的是，已经设计出其他方法来解决这个问题。
- en: Word2Vec (and DeepWalk) implements one of these techniques, called H-Softmax.
    Instead of a flat softmax that directly calculates the probability of every word,
    this technique uses a binary tree structure where leaves are words. Even more
    interestingly, a Huffman tree can be used, where infrequent words are stored at
    deeper levels than common words. In most cases, this dramatically speeds up the
    word prediction by a factor of at least 50.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec（以及 DeepWalk）实现了其中一种技术，称为 H-Softmax。这种技术不是直接计算每个词的概率的平坦 softmax，而是使用一个二叉树结构，其中叶子是单词。更有趣的是，可以使用哈夫曼树，其中罕见的单词存储在比常见单词更深的层级。在大多数情况下，这显著加快了至少
    50 倍的单词预测速度。
- en: H-Softmax can be activated in `gensim` using `hs=1`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `gensim` 中可以通过设置 `hs=1` 来激活 H-Softmax。
- en: 'This was the most difficult part of the DeepWalk architecture. But before we
    can implement it, we need one more component: how to create our training data.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 DeepWalk 结构中最困难的部分。但在我们实施之前，我们需要一个额外的组件：如何创建我们的训练数据。
- en: DeepWalk and random walks
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DeepWalk 和随机游走
- en: Proposed in 2014 by Perozzi et al., DeepWalk quickly became extremely popular
    among graph researchers. Inspired by recent advances in NLP, it consistently outperformed
    other methods on several datasets. While more performant architectures have been
    proposed since then, DeepWalk is a simple and reliable baseline that can be quickly
    implemented to solve a lot of problems.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Perozzi 等人在 2014 年提出了 DeepWalk，迅速在图研究领域中广受欢迎。受到近年来在自然语言处理中的进展的启发，DeepWalk 在多个数据集上一直优于其他方法。尽管此后提出了更高性能的架构，DeepWalk
    是一个可以快速实现并解决许多问题的简单可靠基准。
- en: 'The goal of DeepWalk is to produce high-quality feature representations of
    nodes in an unsupervised way. This architecture is heavily inspired by Word2Vec
    in NLP. However, instead of words, our dataset is composed of nodes. This is why
    we use random walks to generate meaningful sequences of nodes that act like sentences.
    The following diagram illustrates the connection between sentences and graphs:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: DeepWalk 的目标是以无监督方式生成节点的高质量特征表示。这种结构受到 NLP 中 Word2Vec 的启发。然而，我们的数据集由节点组成，而不是单词。这就是为什么我们使用随机游走生成像句子一样的有意义节点序列。以下图示说明了句子和图之间的关系：
- en: '![Figure 3.4 – Sentences can be represented as graphs](img/B19153_03_004.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – 句子可以被表示为图形](img/B19153_03_004.jpg)'
- en: Figure 3.4 – Sentences can be represented as graphs
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 句子可以被表示为图形
- en: Random walks are sequences of nodes produced by randomly choosing a neighboring
    node at every step. Thus, nodes can appear several times in the same sequence.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 随机游走是通过在每一步随机选择一个相邻节点来产生的节点序列。因此，节点可以在同一序列中出现多次。
- en: Why are random walks important? Even if nodes are randomly selected, the fact
    that they often appear together in a sequence means that they are close to each
    other. Under the **network homophily** hypothesis, nodes that are close to each
    other are similar. This is particularly the case in social networks, where people
    are connected to friends and family.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么随机游走很重要？即使节点是随机选择的，它们经常在序列中一起出现的事实意味着它们彼此接近。根据**网络同质性**假设，彼此接近的节点是相似的。这在社交网络中尤为明显，人们通过朋友和家人连接在一起。
- en: 'This idea is at the core of the DeepWalk algorithm: when nodes are close to
    each other, we want to obtain high similarity scores. On the contrary, we want
    low scores when they are farther apart.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是 DeepWalk 算法的核心：当节点彼此接近时，我们希望获得高相似度分数。相反，当它们相距较远时，我们希望得到低分数。
- en: 'Let’s implement a random walk function using a `networkx` graph:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `networkx` 图实现一个随机游走函数：
- en: 'Let’s import the required libraries and initialize the random number generator
    for reproducibility:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们导入所需的库并为了可重复性初始化随机数生成器：
- en: '[PRE14]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We generate a random graph thanks to the `erdos_renyi_graph` function with
    a fixed number of nodes (`10`) and a predefined probability of creating an edge
    between two nodes (`0.3`):'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过使用固定节点数目（`10`）和预定义的两节点之间创建边的概率（`0.3`）来生成一个随机图。
- en: '[PRE15]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We plot this random graph to see what it looks like:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们绘制这个随机图来看它的样子：
- en: '[PRE16]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This produces the following graph:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图形：
- en: '![Figure 3.5 – Random graph](img/B19153_03_005.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – 随机图](img/B19153_03_005.jpg)'
- en: Figure 3.5 – Random graph
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 随机图
- en: 'Let’s implement random walks with a simple function. This function takes two
    parameters: the starting node (`start`) and the length of the walk (`length`).
    At every step, we randomly select a neighboring node (using `np.random.choice`)
    until the walk is complete:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们用一个简单的函数来实现随机游走。这个函数有两个参数：起始节点（`start`）和游走的长度（`length`）。每一步，我们随机选择一个邻近的节点（使用`np.random.choice`），直到游走完成：
- en: '[PRE17]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we print the result of this function with the starting node as `0` and
    a length of `10`:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们用起始节点`0`和长度为`10`来打印这个函数的结果：
- en: '[PRE18]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This produces the following list:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将生成以下列表：
- en: '[PRE19]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We can see that certain nodes, such as `0` and `9`, are often found together.
    Considering that it is a homophilic graph, it means that they are similar. It
    is precisely the type of relationship we’re trying to capture with DeepWalk.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，某些节点，如`0`和`9`，经常一起出现。考虑到这是一个同质性图，意味着它们是相似的。这正是我们希望通过DeepWalk捕捉的关系类型。
- en: Now that we have implemented Word2Vec and random walks separately, let’s combine
    them to create DeepWalk.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经分别实现了Word2Vec和随机游走，让我们将它们结合起来创建DeepWalk。
- en: Implementing DeepWalk
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现DeepWalk
- en: Now that we have a good understanding of every component in this architecture,
    let’s use it to solve an ML problem.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对架构中的每个组件有了很好的理解，让我们用它来解决一个机器学习问题。
- en: The dataset we will use is Zachary’s Karate Club. It simply represents the relationships
    within a karate club studied by Wayne W. Zachary in the 1970s. It is a kind of
    social network where every node is a member, and members who interact outside
    the club are connected.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据集是Zachary的空手道俱乐部。它简单地表示1970年代Wayne W. Zachary研究的一个空手道俱乐部内的关系。这是一个社交网络，每个节点是一个成员，而在俱乐部外部互动的成员是相互连接的。
- en: 'In this example, the club is divided into two groups: we would like to assign
    the right group to every member (node classification) just by looking at their
    connections:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，俱乐部被分成了两组：我们希望通过查看每个成员的连接，将正确的组分配给每个成员（节点分类）：
- en: 'Let’s import the dataset using `nx.karate_club_graph()`:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用`nx.karate_club_graph()`导入数据集：
- en: '[PRE20]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we need to convert string class labels into numerical values (Mr. Hi
    = `0`, `Officer` = `1`):'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要将字符串类型的类别标签转换为数值（Mr. Hi = `0`，`Officer` = `1`）：
- en: '[PRE21]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let’s plot this graph using our new labels:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用新的标签绘制这个图：
- en: '[PRE22]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![Figure 3.6 – Zachary’s Karate Club](img/B19153_03_006.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – Zachary’s Karate Club](img/B19153_03_006.jpg)'
- en: Figure 3.6 – Zachary’s Karate Club
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – Zachary’s Karate Club
- en: 'The next step is to generate our dataset, the random walks. We want to be as
    exhaustive as possible, which is why we will create `80` random walks of a length
    of `10` for every node in the graph:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是生成我们的数据集，即随机游走。我们希望尽可能全面，这就是为什么我们会为图中的每个节点创建`80`个长度为`10`的随机游走：
- en: '[PRE23]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s print a walk to verify that it is correct:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打印一个游走来验证它是否正确：
- en: '[PRE24]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This is the first walk that was generated:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是生成的第一次随机游走：
- en: '[PRE25]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The final step consists of implementing Word2Vec. Here, we use the skip-gram
    model previously seen with H-Softmax. You can play with the other parameters to
    improve the quality of the embeddings:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是实现Word2Vec。在这里，我们使用先前看到的带有H-Softmax的skip-gram模型。你可以调整其他参数来提高嵌入的质量：
- en: '[PRE26]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The model is then simply trained on the random walks we generated.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，模型只需要在我们生成的随机游走上进行训练。
- en: '[PRE27]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now that our model is trained, let’s see its different applications. The first
    one allows us to find the most similar nodes to a given one (in terms of cosine
    similarity):'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们的模型已经训练完成，让我们看看它的不同应用。第一个应用是允许我们找到与给定节点最相似的节点（基于余弦相似度）：
- en: '[PRE28]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This produces the following output for `Nodes that are the most similar to`
    `node 0`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：`与node 0最相似的节点`：
- en: '[PRE29]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Another important application is calculating the similarity score between two
    nodes. It can be performed as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的应用是计算两个节点之间的相似度分数。可以如下进行：
- en: '[PRE30]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This code directly gives us the cosine similarity between two nodes:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码直接给出了两个节点之间的余弦相似度：
- en: '[PRE31]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can plot the resulting embeddings using **t-distributed stochastic neighbor
    embedding** (**t-SNE**) to visualize these high-dimensional vectors in 2D:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用**t分布随机邻居嵌入**（**t-SNE**）绘制结果嵌入图，将这些高维向量可视化为2D：
- en: 'We import the `TSNE` class from `sklearn`:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从`sklearn`导入`TSNE`类：
- en: '[PRE32]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We create two arrays: one to store the word embeddings and the other one to
    store the labels:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建两个数组：一个用于存储词嵌入，另一个用于存储标签：
- en: '[PRE33]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we train the t-SNE model with two dimensions (`n_components=2`) on the
    embeddings:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用两维（`n_components=2`）的t-SNE模型对嵌入进行训练：
- en: '[PRE34]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Finally, let’s plot the 2D vectors produced by the trained t-SNE model with
    the corresponding labels:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们绘制训练好的t-SNE模型生成的二维向量，并标注对应的标签：
- en: '[PRE35]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![Figure 3.7 – A t-SNE plot of the nodes](img/B19153_03_007.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图3.7 – 节点的t-SNE图](img/B19153_03_007.jpg)'
- en: Figure 3.7 – A t-SNE plot of the nodes
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 – 节点的t-SNE图
- en: 'This plot is quite encouraging since we can see a clear line that separates
    the two classes. It should be possible for a simple ML algorithm to classify these
    nodes with enough examples (training data). Let’s implement a classifier and train
    it on our node embeddings:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表相当鼓舞人心，因为我们可以看到一个清晰的分界线将两个类别分开。一个简单的机器学习算法应该能够在足够的样本（训练数据）下对这些节点进行分类。让我们实现一个分类器，并在我们的节点嵌入上进行训练：
- en: 'We import a Random Forest model from `sklearn`, which is a popular choice when
    it comes to classification. The accuracy score is the metric we’ll use to evaluate
    this model:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从`sklearn`导入了一个随机森林模型，这是分类任务中的一个流行选择。我们将使用准确率作为评估此模型的指标：
- en: '[PRE36]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We need to split the embeddings into two groups: training and test data. A
    simple way of doing it is to create masks as follows:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要将嵌入分成两组：训练数据和测试数据。一种简单的方法是创建如下的掩码：
- en: '[PRE37]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Next, we train the Random Forest classifier on the training data with the appropriate
    labels:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们在训练数据上使用适当的标签训练随机森林分类器：
- en: '[PRE38]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Finally, we evaluate the trained model on the test data based on its accuracy
    score:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们基于准确率对训练好的模型在测试数据上进行评估：
- en: '[PRE39]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This is the final result of our classifier:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是我们分类器的最终结果：
- en: '[PRE40]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Our model obtains an accuracy score of 95.45%, which is pretty good considering
    the unfavorable train/test split we gave it. There is still room for improvement,
    but this example showed two useful applications of DeepWalk:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型获得了95.45%的准确率，考虑到我们给出的不利训练/测试数据分割，结果相当不错。虽然仍有改进空间，但这个例子展示了DeepWalk的两个有用应用：
- en: '*Discovering similarities between nodes* using embeddings and cosine similarity
    (unsupervised learning)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用嵌入和余弦相似度发现节点之间的相似性*（无监督学习）'
- en: '*Using these embeddings as a dataset* for a supervised task such as node classification'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将这些嵌入作为数据集*用于节点分类等监督任务'
- en: As we are going to see in the following chapters, the ability to learn node
    representations offers a lot of flexibility to design deeper and more complex
    architectures.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在接下来的章节中将看到的，学习节点表示的能力为设计更深层次、更复杂的架构提供了很大的灵活性。
- en: Summary
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about DeepWalk architecture and its major components.
    Then, we transformed graph data into sequences using random walks to apply the
    powerful Word2Vec algorithm. The resulting embeddings can be used to find similarities
    between nodes or as input to other algorithms. In particular, we solved a node
    classification problem using a supervised approach.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了DeepWalk的架构及其主要组件。然后，我们通过随机游走将图数据转化为序列，应用强大的Word2Vec算法。生成的嵌入可以用来查找节点之间的相似性，或者作为其他算法的输入。特别是，我们使用监督学习方法解决了一个节点分类问题。
- en: In [*Chapter 4*](B19153_04.xhtml#_idTextAnchor054), *Improving Embeddings with
    Biased Random Walks in Node2Vec*, we will introduce a second algorithm based on
    Word2Vec. The difference with DeepWalk is that the random walks can be biased
    towards more or less exploration, which directly impacts the embeddings that are
    produced. We will implement this algorithm on a new example and compare its representations
    with those obtained using DeepWalk.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B19153_04.xhtml#_idTextAnchor054)《使用节点2Vec中的带偏随机游走改进嵌入》中，我们将介绍基于Word2Vec的第二种算法。与DeepWalk的区别在于，随机游走可以倾向于更多或更少的探索，这直接影响到生成的嵌入。我们将在一个新示例中实现此算法，并将其表示与使用DeepWalk获得的表示进行比较。
- en: Further reading
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[1] B. Perozzi, R. Al-Rfou, and S. Skiena, *DeepWalk*, Aug. 2014\. DOI: 10.1145/2623330.2623732\.
    Available at [https://arxiv.org/abs/1403.6652](B19153_03.xhtml#_idTextAnchor052).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] B. Perozzi, R. Al-Rfou, 和 S. Skiena, *DeepWalk*, 2014年8月\. DOI: 10.1145/2623330.2623732\.
    访问地址：[https://arxiv.org/abs/1403.6652](B19153_03.xhtml#_idTextAnchor052).'
- en: 'Part 2: Fundamentals'
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：基础知识
- en: In this second part of the book, we will delve into the process of constructing
    node representations using graph learning. We will start by exploring traditional
    graph learning techniques, drawing on the advancements made in natural language
    processing. Our aim is to understand how these techniques can be applied to graphs
    and how they can be used to build node representations.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第二部分，我们将深入探讨使用图学习构建节点表示的过程。我们将从探索传统的图学习技术开始，借鉴自然语言处理领域的进展。我们的目标是理解这些技术如何应用于图结构，以及如何利用这些技术构建节点表示。
- en: We will then move on to incorporating node features into our models and explore
    how they can be used to build even more accurate representations. Finally, we
    will introduce two of the most fundamental GNN architectures, the **Graph Convolutional
    Network** (**GCN**) and the **Graph Attention Network** (**GAT**). These two architectures
    are the building blocks of many state-of-the-art graph learning methods and will
    provide a solid foundation for the next part.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来将讨论如何将节点特征融入到我们的模型中，并探索它们如何用于构建更准确的表示。最后，我们将介绍两种最基本的 GNN 架构，即**图卷积网络**（**GCN**）和**图注意力网络**（**GAT**）。这两种架构是许多最先进的图学习方法的基石，并为接下来的内容提供了坚实的基础。
- en: By the end of this part, you will have a deeper understanding of how traditional
    graph learning techniques, such as random walks, can be used to create node representations
    and develop graph applications. Additionally, you will learn how to build even
    more powerful representations using GNNs. You will be introduced to two key GNN
    architectures and learn how they can be used to tackle various graph-based tasks.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 到本部分结束时，您将更深入地理解如何使用传统的图学习技术，如随机游走，来创建节点表示并开发图应用。此外，您还将学习如何使用 GNN 构建更强大的表示。您将接触到两种关键的
    GNN 架构，并了解它们如何用于解决各种基于图的任务。
- en: 'This part comprises the following chapters:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包括以下章节：
- en: '[*Chapter 3*](B19153_03.xhtml#_idTextAnchor041)*, Creating Node Representations
    with DeepWalk*'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第三章*](B19153_03.xhtml#_idTextAnchor041)*, 使用 DeepWalk 创建节点表示*'
- en: '[*Chapter 4*](B19153_04.xhtml#_idTextAnchor054)*, Improving Embeddings with
    Biased Random Walks in Node2Vec*'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第四章*](B19153_04.xhtml#_idTextAnchor054)*, 在 Node2Vec 中通过带偏随机游走改进嵌入*'
- en: '[*Chapter 5*](B19153_05.xhtml#_idTextAnchor064)*, Including Node Features with
    Vanilla Neural Networks*'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第五章*](B19153_05.xhtml#_idTextAnchor064)*, 使用普通神经网络包含节点特征*'
- en: '[*Chapter 6*](B19153_06.xhtml#_idTextAnchor074)*, Introducing Graph Convolutional
    Networks*'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第六章*](B19153_06.xhtml#_idTextAnchor074)*, 介绍图卷积网络*'
- en: '[*Chapter 7*](B19153_07.xhtml#_idTextAnchor082)*, Graph Attention Networks*'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第七章*](B19153_07.xhtml#_idTextAnchor082)*, 图注意力网络*'
