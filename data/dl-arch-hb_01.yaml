- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Deep Learning Life Cycle
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习生命周期
- en: In this chapter, we will explore the intricacies of the deep learning life cycle.
    Sharing similar characteristics to the machine learning life cycle, the deep learning
    life cycle is a framework as much as it is a methodology that will allow a deep
    learning project idea to be insanely successful or to be completely scrapped when
    it is appropriate. We will grasp the reasons why the process is cyclical and understand
    some of the life cycle’s initial processes on a deeper level. Additionally, we
    will go through some high-level sneak peeks of the later processes of the life
    cycle that will be explored at a deeper level in future chapters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨深度学习生命周期的复杂性。深度学习生命周期与机器学习生命周期具有相似的特点，它既是一个框架，也是一个方法论，能够让一个深度学习项目要么取得巨大的成功，要么在适当的时候完全废弃。我们将掌握为什么这个过程是循环的，并深入理解生命周期的一些初步过程。此外，我们还将简单预览生命周期后续过程的高层次内容，这些内容将在未来的章节中深入探讨。
- en: 'Comprehensively, this chapter will help you do the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 综合来看，本章将帮助你完成以下内容：
- en: Understand the similarities and differences between the deep learning life cycle
    and its machine learning life cycle counterpart
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解深度学习生命周期与机器学习生命周期之间的相似性和差异性
- en: Understand where domain knowledge fits in a deep learning project
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解领域知识在深度学习项目中的作用
- en: Understand the few key steps in planning a deep learning project to make sure
    it can tangibly create real-world value
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解规划深度学习项目的几个关键步骤，确保它能够切实创造现实世界的价值
- en: Grasp some deep learning model development details at a high level
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在高层次上掌握一些深度学习模型开发的细节
- en: Grasp the importance of model interpretation and the variety of deep learning
    interpretation techniques at a high level
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在高层次上掌握模型解释的重要性以及深度学习解释技术的多样性
- en: Explore high-level concepts of model deployments and their governance
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索模型部署的高层次概念及其治理
- en: Learn to choose the necessary tools to carry out the processes in the deep learning
    life cycle
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学会选择必要的工具来执行深度学习生命周期中的过程
- en: 'We’ll cover this material in the following sections:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在以下章节中讲解这些内容：
- en: Machine learning life cycle
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习生命周期
- en: The construction strategy of a deep learning life cycle
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习生命周期的构建策略
- en: The data preparation stage
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备阶段
- en: Deep learning model development
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型开发
- en: Delivering model insights
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供模型洞察
- en: Managing risks
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风险管理
- en: Technical requirements
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter includes some practical implementations in the **Python** programming
    language. To complete it, you need to have a computer with the following libraries
    installed:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括一些**Python**编程语言的实用实现。要完成本章内容，你需要一台安装了以下库的计算机：
- en: '`pandas`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`'
- en: '`matplotlib`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib`'
- en: '`seaborn`'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seaborn`'
- en: '`tqdm`'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tqdm`'
- en: '`lingua`'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lingua`'
- en: 'The code files are available on GitHub: [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_1](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_1).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 代码文件可在GitHub上找到：[https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_1](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_1)。
- en: Understanding the machine learning life cycle
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解机器学习生命周期
- en: '**Deep learning** is a subset of the wider machine learning category. The main
    characteristic that sets it apart from other machine learning algorithms is the
    foundational building block called **neural networks**. As deep learning has advanced
    tremendously since the early 2000s, it has made many previously unachievable feats
    possible through its machine learning counterparts. Specifically, deep learning
    has made breakthroughs in recognizing complex patterns that exist in complex and
    unstructured data such as text, images, videos, and audio. Some of the successful
    applications of deep learning today are face recognition with images, speech recognition
    from audio data, and language translation with textual data.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习**是更广泛的机器学习类别的一个子集。它与其他机器学习算法的主要区别在于其基础构建块——**神经网络**。随着深度学习自2000年代初以来的巨大进步，它通过其机器学习对等物实现了许多以前无法完成的壮举。特别是，深度学习在识别复杂模式方面取得了突破，这些模式存在于复杂且非结构化的数据中，例如文本、图像、视频和音频。如今，深度学习的一些成功应用包括图像中的人脸识别、音频数据中的语音识别和文本数据中的语言翻译。'
- en: 'Machine learning, on the other hand, is a subset of the wider artificial intelligence
    category. Its algorithms, such as tree-based models and linear models, which are
    not considered to be deep learning models, still serve a wide range of use cases
    involving tabular data, which is the bulk of the data that’s stored by small and
    big organizations alike. This tabular data may exist in multiple structured databases
    and can span from 1 to 10 years’ worth of historical data that has the potential
    to be used for building predictive machine learning models. Some of the notable
    predictive applications for machine learning algorithms are fraud detection in
    the finance industry, product recommendations in e-commerce, and predictive maintenance
    in the manufacturing industry. *Figure 1**.1* shows the relationships between
    deep learning, machine learning, and artificial intelligence for a clearer visual
    distinction between them:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，机器学习是更广泛的人工智能类别的一个子集。其算法，如基于树的模型和线性模型，虽然不被视为深度学习模型，但仍然适用于涉及表格数据的广泛应用场景，而表格数据是大多数小型和大型组织所存储的数据。这些表格数据可能存在于多个结构化数据库中，数据跨度可能达到1到10年的历史数据，且这些数据具有用于构建预测性机器学习模型的潜力。一些机器学习算法的显著预测应用包括金融行业中的欺诈检测、电子商务中的产品推荐以及制造业中的预测性维护。*图
    1**.1* 显示了深度学习、机器学习和人工智能之间的关系，以便更清晰地区分它们：
- en: '![Figure 1.1 – Artificial intelligence relationships](img/B18187_01_001.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1 – 人工智能关系](img/B18187_01_001.jpg)'
- en: Figure 1.1 – Artificial intelligence relationships
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – 人工智能关系
- en: 'Now that we know what deep learning and machine learning are in a nutshell,
    we are ready for a glimpse of the machine learning life cycle, as shown in *Figure
    1**.2*:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们大致了解了深度学习和机器学习的概念，我们准备好一窥机器学习生命周期，如*图 1**.2*所示：
- en: '![Figure 1.2 – Deep learning/machine learning life cycle](img/B18187_01_002.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.2 – 深度学习/机器学习生命周期](img/B18187_01_002.jpg)'
- en: Figure 1.2 – Deep learning/machine learning life cycle
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – 深度学习/机器学习生命周期
- en: 'As advanced and complex the deep learning algorithm is compared to other machine
    learning algorithms, the guiding methodologies that are needed to ensure success
    in both domains are unequivocally the same. The machine learning life cycle involves
    six stages that interact with each other in different ways:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习算法相比其他机器学习算法尽管更加先进和复杂，但确保这两个领域成功所需的指导方法无疑是相同的。机器学习生命周期涉及六个阶段，这些阶段之间以不同的方式相互作用：
- en: '**Planning**'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**规划**'
- en: '**Data Preparation**'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据准备**'
- en: '**Model Development**'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型开发**'
- en: '**Deliver** **Model Insights**'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**交付** **模型洞察**'
- en: '**Model Deployment**'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型部署**'
- en: '**Model Governance**'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型治理**'
- en: '*Figure 1**.2* shows these six stages and the possible stage transitions depicted
    with arrows. Typically, a machine learning project will iterate between stages,
    depending on the business requirements. In a deep learning project, most of the
    innovative predictive use cases require manual data collection and data annotation,
    which is a process that lies in the realm of the **Data Preparation** stage. As
    this process is generally time-consuming, especially when the data itself is not
    readily available, a go-to solution would be to start with an acceptable initial
    number of data and transition into the **Model Development** stage and, subsequently,
    to the **Deliver Model Insight** stage to make sure results from the ideas are
    sane.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1**.2* 展示了这六个阶段以及通过箭头表示的可能阶段转换。通常，一个机器学习项目会根据业务需求在各个阶段之间反复迭代。在深度学习项目中，大多数创新性的预测性应用场景需要手动数据收集和数据标注，这一过程属于**数据准备**阶段。由于这个过程通常非常耗时，尤其是当数据本身不容易获得时，一种解决方案是从一个可以接受的初始数据量开始，然后过渡到**模型开发**阶段，随后进入**交付模型洞察**阶段，以确保从想法中得到的结果是合理的。'
- en: After the initial validation process, depending again on business requirements,
    practitioners would then decide to transition back into the **Data Preparation**
    stage and continue to iterate through these stages cyclically in different data
    size milestones until results are satisfactory toward both the model development
    and business metrics. Once it gets approval from the necessary stakeholders, the
    project then goes into the **Model Deployment** stage, where the built machine
    learning model will be served to allow its predictions to be consumed. The final
    stage is **Model Governance**, where practitioners carry out tasks that manage
    the risk, performance, and reliability of the deployed machine learning model.
    Model deployment and model governance both deserve more in-depth discussion and
    will be introduced in separate chapters closer to the end of this book. Whenever
    any of the key metrics fail to maintain themselves to a certain determined confidence
    level, the project will fall back into the **Data Preparation** stage of the cycle
    and repeat the same flow all over again.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在初步验证过程之后，依照业务需求，实践者将决定是否重新进入**数据准备**阶段，并根据不同数据量的里程碑，周期性地迭代这些阶段，直到模型开发和业务指标的结果都令人满意。获得必要的利益相关者批准后，项目将进入**模型部署**阶段，届时构建的机器学习模型将被提供，以便其预测结果得以使用。最后阶段是**模型治理**，在这一阶段，实践者执行管理已部署机器学习模型的风险、性能和可靠性的任务。模型部署和模型治理都值得更深入的讨论，并将在本书后期的章节中单独介绍。每当任何关键指标未能维持在某一确定的置信度水平时，项目将返回到**数据准备**阶段，并重新开始相同的流程。
- en: The ideal machine learning project flows through the stages cyclically for as
    long as the business application needs it. However, machine learning projects
    are typically susceptible to a high probability of failure. According to a survey
    conducted by Dimensional Research and Alegion, covering around 300 machine learning
    practitioners from 20 different business industries, 78% of machine learning projects
    get held back or delayed at some point before deployment. Additionally, Gartner
    predicted that 85% of machine learning projects will fail ([https://venturebeat.com/2021/06/28/why-most-ai-implementations-fail-and-what-enterprises-can-do-to-beat-the-odds/](https://venturebeat.com/2021/06/28/why-most-ai-implementations-fail-and-what-enterprises-can-do-to-beat-the-odds/)).
    By expecting the unexpected, and anticipating failures before they happen, practitioners
    can likely circumvent potential failure factors early down the line in the planning
    stage. This also brings us to the trash icon bundled together in *Figure 1**.2*.
    Proper projects with a good plan typically get discarded only at the **Deliver
    Model Insights** stage, when it’s clear that the proposed model and project can’t
    deliver satisfactory results.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的机器学习项目应根据业务需求，按周期性阶段流动。然而，机器学习项目通常容易遭遇高概率的失败。根据Dimensional Research和Alegion进行的一项调查，涵盖了来自20个不同业务行业的约300名机器学习从业者，78%的机器学习项目在部署前的某个阶段被拖延或受阻。此外，Gartner预测，85%的机器学习项目将失败（[https://venturebeat.com/2021/06/28/why-most-ai-implementations-fail-and-what-enterprises-can-do-to-beat-the-odds/](https://venturebeat.com/2021/06/28/why-most-ai-implementations-fail-and-what-enterprises-can-do-to-beat-the-odds/)）。通过预见意外并在它们发生之前预判失败，实践者可以在规划阶段及早避免潜在的失败因素。这也引出了*图1**.2*中与垃圾图标捆绑在一起的概念。一个良好的项目，通常只有在**交付模型洞察**阶段被舍弃，那时已明确提出的模型和项目无法交付令人满意的结果。
- en: Now that we’ve covered an overview of the machine learning life cycle, let’s
    dive into each of the stages individually, broken down into sections, to help
    you discover the key tips and techniques that are needed the complete each stage
    successfully. These stages will be discussed in an abstract format and are not
    a concrete depiction of what you should ultimately be doing for your project since
    all projects are unique and strategies should always be evaluated on a case-by-case
    basis.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经概述了机器学习生命周期，让我们逐个阶段深入探讨，并将其分解为多个部分，帮助你发现完成每个阶段所需的关键技巧和方法。这些阶段将以抽象的方式进行讨论，并不是对你在项目中最终应采取的实际步骤的具体描述，因为每个项目都是独特的，策略应始终根据具体情况进行评估。
- en: Strategizing the construction of a deep learning system
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策划构建深度学习系统
- en: A deep learning model can only realize real-world value by being part of a system
    that performs some sort of operation. Bringing deep learning models from research
    papers to actual real-world usage is not an easy task. Thus, performing proper
    planning before conducting any project is a more reliable and structured way to
    achieve the desired goals. This section will discuss some considerations and strategies
    that will be beneficial when you start to plan your deep learning project toward
    success.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型只有成为执行某种操作的系统的一部分，才能实现实际的价值。将深度学习模型从研究论文带到实际的现实世界应用并非易事。因此，在进行任何项目之前进行适当的规划，是实现目标的更可靠和结构化的方式。本节将讨论一些在规划深度学习项目以走向成功时，可能会有帮助的考虑因素和策略。
- en: Starting the journey
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启程
- en: Today, deep learning practitioners tend to focus a lot on the algorithmic model-building
    part of the process. It takes a considerable amount of mental strength to not
    get hooked on the hype of **state-of-the-art** (**SOTA**) research-focused techniques.
    With crazy techniques such as **pixtopix**, which is capable of generating high-resolution
    realistic color images from just sketches or image masks, and **natural language
    processing** (**NLP**) techniques such as **GPT-3**, a 175-billion parameters
    text generation model from OpenAI, and GPT-4, a multimodal text generation model
    that is a successor to GPT-3 and its sub-models, that are capable of generating
    practically anything you ask it to in a text format that ranges from text summarization
    to generating code, why wouldn’t they?!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，深度学习从业者往往过于专注于模型构建过程中的算法部分。要避免被**最先进**（**SOTA**）的研究技术吸引，需要相当大的心理耐力。像**pixtopix**这样的疯狂技术，它能够从草图或图像掩模中生成高分辨率的真实彩色图像，和像**自然语言处理**（**NLP**）技术中的**GPT-3**（一个拥有1750亿参数的文本生成模型，由OpenAI开发）和GPT-4（GPT-3及其子模型的继任者，一个多模态文本生成模型，能够生成从文本摘要到代码生成的任何文本格式内容）一样，为什么不让它们成为主流呢？！
- en: Jokes aside, to become a true deep learning architect, we need to come to a
    consensus that any successful machine learning or deep learning project starts
    with the business *problem* and not from the shiny new research paper you just
    read online complete with a public GitHub repository. The planning stage often
    involves many business executives who are not savvy about the details of machine
    learning algorithms and often, the same set of people wouldn’t care about it at
    all. These algorithms are daunting for business-focused stakeholders to understand
    and, when added on top of the tough mental barriers of the adoption of artificial
    intelligence technologies itself, it doesn’t make the project any more likely
    to be adopted.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 开个玩笑，要成为一名真正的深度学习架构师，我们需要达成共识：任何成功的机器学习或深度学习项目都是从业务*问题*出发的，而不是从你刚在网上读到的、附带公共GitHub仓库的闪亮新研究论文开始的。规划阶段通常会涉及许多对机器学习算法细节不太了解的业务高管，而且通常，同一批人根本不关心这些内容。对于专注于业务的利益相关者来说，这些算法令人望而却步，再加上人工智能技术本身采纳所面临的巨大心理障碍，这并不会让项目更容易被采纳。
- en: Evaluating deep learning’s worthiness
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估深度学习的价值
- en: Deep learning shines the most in handling unstructured data. This includes image
    data, text data, audio data, and video data. This is largely due to the model’s
    ability to automatically learn and extract complex, high-level features from the
    raw data. In the case of images and videos, deep learning models can capture spatial
    and temporal patterns, recognizing objects, scenes, and activities. With audio
    data, deep learning can understand the nuances of speech, noise, and various sound
    elements, making it possible to build applications such as speech recognition,
    voice assistants, and audio classification systems. For text data, deep learning
    models can capture the context, semantics, and syntax, enabling NLP tasks such
    as sentiment analysis, machine translation, and text summarization.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在处理非结构化数据方面最为突出。这包括图像数据、文本数据、音频数据和视频数据。这在很大程度上得益于模型自动学习并从原始数据中提取复杂、高层次特征的能力。在图像和视频的案例中，深度学习模型能够捕捉空间和时间模式，识别物体、场景和活动。对于音频数据，深度学习能够理解语音、噪音和各种声音元素的细微差别，使得构建语音识别、语音助手和音频分类系统等应用成为可能。对于文本数据，深度学习模型能够捕捉上下文、语义和语法，从而支持如情感分析、机器翻译和文本摘要等自然语言处理（NLP）任务。
- en: This means that if this data exists and is utilized by your company in its business
    processes, there may be an opportunity to solve a problem with the help of deep
    learning. However, never overcomplicate problems just so you can solve them with
    deep learning. Equating this to something more relatable, you wouldn’t use a huge
    sledgehammer to get a nail into wood. It could work and you might get away with
    it, but you’d risk bending the nail or injuring yourself while using it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果您的公司在其业务流程中存在并利用这些数据，可能有机会借助深度学习来解决一个问题。然而，切勿仅仅为了使用深度学习而过度复杂化问题。用一个更容易理解的比喻来说，你不会用一把巨大的大锤去把钉子钉进木头。虽然可能有效，而且你或许能成功，但你可能会把钉子弄弯，甚至可能在使用过程中伤到自己。
- en: Once a problem has been identified, evaluate the business value of solving it.
    Not all problems are born the same and they can be ranked based on their business
    impact, value, complexity, risks, costs, and suitability for deep learning. Generally,
    you’d be looking for high impact, high value, low complexity, low risks, low cost,
    and high suitability to deep learning. Trade-offs between these metrics are expected
    but simply put, make sure the problem you’ve discovered is worth solving at all
    with deep learning. A general rule of thumb is to always resort to a simpler solution
    for a problem, even if it ends up abandoning the usage of deep learning technologies.
    Simple approaches tend to be more reliable, less costly, less prone to risks,
    and faster to fruition.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦识别出一个问题，就要评估解决它的商业价值。并非所有问题都是一样的，它们可以根据业务影响、价值、复杂度、风险、成本和深度学习的适用性进行排序。通常，你应该寻找高影响、高价值、低复杂度、低风险、低成本并且适合深度学习的问题。虽然这些指标之间存在权衡，但简而言之，要确保你发现的问题值得用深度学习来解决。一个普遍的经验法则是，面对问题时，总是优先选择更简单的解决方案，即使这意味着放弃使用深度学习技术。简单的方法往往更可靠、成本更低、风险更小，且更快见效。
- en: Consider a problem where a solution is needed to remove background scenes in
    a video feed and leave only humans or necessary objects untouched so that a more
    suitable background scene can be overlaid as a background instead. This is a common
    problem in the professional filmmaking industry in all film genres today.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个问题，在视频流中需要移除背景场景，只保留人类或必要的物体不受影响，以便可以将更合适的背景场景叠加为背景。这是当今所有电影类型中，专业电影制作行业常见的问题。
- en: '**Semantic segmentation**, which is the task of assigning a label to every
    pixel of an image in the width and height dimensions, is a method that is needed
    to solve such a problem. In this case, the task needs to assign labels that can
    help identify which pixels need to be removed. With the advent of many publicly
    available semantic segmentation datasets, deep learning has been able to advance
    considerably in the semantic segmentation field, allowing itself to achieve a
    very satisfactory fine-grained understanding of the world, enough so that it can
    be applied in the industry of autonomous driving and robot navigation most prominently.
    However, deep learning is not known to be 100% error-free and almost always has
    some error, even in the controlled evaluation dataset. In the case of human segmentation,
    for example, the model would likely result in the most errors in the fine hair
    areas. Most filmmakers aim for perfect depictions of their films and require that
    every single pixel gets removed appropriately without fail since a lot of money
    is spent on the time of the actors hired for the film. Additionally, a lot of
    time and money would be wasted in manually removing objects that could be otherwise
    simply removed if the scene had been shot with a green screen. This is an example
    of a case where we should not overcomplicate the problem. A green screen is all
    you need to solve the problem described: specifically, the rare chromakey green
    color. When green screens are prepped properly in the areas where the desired
    imagery will be overlaid digitally, image processing techniques alone can remove
    the pixels that are considered to be in the small light intensity range centered
    on the chromakey green color and achieve semantic segmentation effectively with
    a rule-based solution. The green screen is a simpler solution that is cost-effective,
    foolproof, and fast to set up.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**语义分割**是指在图像的宽度和高度维度上为每个像素分配标签的任务，这是一种解决此类问题所需的方法。在这种情况下，任务需要分配标签，以帮助识别哪些像素需要被移除。随着许多公开可用的语义分割数据集的出现，深度学习在语义分割领域取得了显著进展，使其能够实现对世界的非常满意的细粒度理解，足以广泛应用于自动驾驶和机器人导航等行业。然而，深度学习并不以100%无误著称，几乎总是存在一些误差，即使在受控的评估数据集上也是如此。例如，在人体分割的情况下，模型可能会在细小的发丝区域产生最多的错误。大多数电影制作人追求电影的完美呈现，并要求每一个像素都被恰当且无误地移除，因为电影演员的时间非常昂贵。此外，若场景没有使用绿幕拍摄，那么手动去除物体将浪费大量时间和金钱，而这些物体本可以通过绿幕简单去除。这是一个不应过度复杂化问题的例子。要解决上述问题，你只需要一个绿幕：具体来说，罕见的色度键绿色。当绿幕在需要数字化叠加图像的区域妥善准备时，图像处理技术本身可以去除被认为属于色度键绿色范围内的小光强度范围的像素，并通过基于规则的解决方案有效地实现语义分割。绿幕是一种更简单、经济、万无一失且快速设置的解决方案。'
- en: 'That was a mouthful! Now, let’s go through a simpler problem. Consider a problem
    where we want to automatically and digitally identify when it rains. In this use
    case, it is important to understand the actual requirements and goals of identifying
    the rain: is it sufficient to detect rain exactly when it happens? Or do we need
    to identify whether rain will happen in the near future? What will we use the
    information of rain events for? These questions will guide whether deep learning
    is required or not. We, as humans, know that rain can be predicted by visual input
    by either looking at the presence of raindrops falling or looking at cloud conditions.
    However, if the use case is sufficient to detect rain when it happens, and the
    goal of detecting rain is to determine when to water the plants, a simpler approach
    would be to use an electronic sensor to detect the presence of water or humidity.
    Only when you want to estimate whether it will rain in the future, let’s say in
    15 minutes, does deep learning make more sense to be applied as there are a lot
    of interactions between meteorological factors that can affect rainfall. Only
    by brainstorming each use case and analyzing all potential solutions, even outside
    of deep learning, can you make sure deep learning brings tangible business value
    compared to other solutions. Do not just apply deep learning because you want
    to.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是一口气说完！现在，我们来看看一个更简单的问题。假设我们需要自动化地识别下雨的情况。在这个应用场景中，理解识别下雨的实际需求和目标是非常重要的：仅仅在下雨发生的瞬间检测到雨是否足够吗？还是我们需要识别雨是否会在不久的未来发生？我们将如何使用降雨事件的信息？这些问题将帮助我们判断是否需要使用深度学习。我们作为人类，知道通过视觉输入可以预测雨，比如通过观察降水的存在或者云层的状况。然而，如果应用场景仅仅需要在下雨时进行检测，并且识别下雨的目标是决定何时给植物浇水，那么一个更简单的方法就是使用电子传感器来检测水的存在或湿度。只有当你想预测未来是否会下雨，比如说在15分钟内，深度学习才更有意义，因为有很多气象因素之间的相互作用可能会影响降雨情况。只有通过对每个应用场景进行头脑风暴，分析所有潜在的解决方案，甚至是深度学习之外的方案，才能确保深度学习相较于其他解决方案带来切实的商业价值。不要因为想用深度学习就随便应用它。
- en: At times, when value isn’t clear when you’re directly considering a use case,
    or when value is clear but you have no idea how to execute it, consider finding
    reference projects from companies in the same industry. Companies in the same
    industry have a high chance of wanting to optimize the same processes or solve
    the same pain points. Similar reference projects can serve as a guide to designing
    a deep learning system and can serve as proof that the use case being considered
    is worthy of the involvement of deep learning technologies. Of course, not everybody
    has access to details like this, but you’d be surprised what Google can tell you
    these days. Even if there isn’t a similar project being carried out for direct
    reference, you would likely be able to pivot upon the other machine learning project
    references that already have a track record of bringing value to the same industry.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，当你直接考虑某个应用场景时，价值不清晰，或者即使价值很明确，但你却不知道如何执行时，可以考虑寻找同行业公司的一些参考项目。同一行业的公司通常会有很高的可能性优化相同的流程或解决相同的痛点。类似的参考项目可以作为设计深度学习系统的指南，并且能够证明正在考虑的应用场景值得使用深度学习技术。当然，并不是每个人都能获得这种详细信息，但你会惊讶于谷歌现在能告诉你什么。即使没有类似的项目可以作为直接参考，你仍然有可能从其他已经在同一行业内带来价值的机器学习项目中获得灵感。
- en: Admittedly, rejecting deep learning at times would be a hard pill to swallow
    considering that most practitioners get paid to implement deep learning solutions.
    However, dismissing it earlier will allow you to focus your time on more valuable
    problems that would be more useful to solve with deep learning and prevent the
    risk of undermining the potential of deep learning in cases where simpler solutions
    can outperform deep learning. Criteria for deep learning worthiness should be
    evaluated on a case-by-case basis and as a practitioner, the best advice to follow
    is to simply practice common sense. Spend a good amount of time going through
    the problem exploration and the worthiness evaluation process. The last thing
    you want is to spend a painstaking amount of time preparing data, building a deep
    learning model, and delivering very convincing model insights only to find out
    that the label you are trying to predict does not provide enough value for the
    business to invest further.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 不得不承认，有时拒绝深度学习确实是一个难以接受的决定，因为大多数从业者都是靠实现深度学习解决方案来获得报酬的。然而，及早放弃深度学习将使你能将时间集中在更有价值的问题上，这些问题更适合通过深度学习来解决，并且能够避免在某些情况下，简单的解决方案胜过深度学习时，削弱深度学习潜力的风险。深度学习是否值得采用的标准应当根据具体情况进行评估，作为一名从业者，最好的建议是遵循常识。花足够的时间进行问题探索和价值评估。你最不想发生的事情，就是花费大量时间准备数据、构建深度学习模型，并提供非常有说服力的模型见解，结果却发现你试图预测的标签对于业务没有足够的价值，无法进一步投资。
- en: Defining success
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义成功
- en: Ever heard sentences like “*My deep learning model just got 99% accuracy on
    my validation dataset!”*? Data scientists often make the mistake of determining
    the success of a machine learning project just by using validation metrics they
    use to evaluate their machine learning models during the model development process.
    Model-building metrics such as accuracy, precision, or recall are important metrics
    to consider in a machine learning project but unless they add business values
    and connect to the business objectives in some way, they rarely mean anything.
    A project can achieve a good accuracy score but still fail to achieve the desired
    business goals. This can happen in cases when no proper success metrics have been
    defined early and subsequently cause a wrong label to be used in the data preparation
    and model development stages. Furthermore, even when the model metric positively
    impacts business processes directly, there is a chance that the achievement won’t
    be communicated effectively to business stakeholders and the worst case not considered
    to be successful when reported as-is.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 曾经听过类似这样的句子吗：“*我的深度学习模型在验证数据集上达到了99%的准确率！*”？数据科学家经常犯一个错误，那就是仅仅通过验证指标来判断一个机器学习项目的成功，这些验证指标通常是在模型开发过程中用于评估机器学习模型的表现。像准确率、精确度或召回率这样的模型构建指标在机器学习项目中确实很重要，但除非这些指标能够为业务带来价值，并以某种方式与业务目标相连接，否则它们往往毫无意义。一个项目可能取得了不错的准确率，但仍然未能实现预期的业务目标。这种情况通常出现在没有在早期定义出合适的成功指标，从而导致在数据准备和模型开发阶段使用了错误的标签。此外，即使模型指标对业务流程产生了积极影响，也有可能无法有效地传达给业务相关方，最糟糕的情况是，当报告按原样呈现时，模型的成功并未得到认可。
- en: Success metrics, when defined early, act as the machine learning project’s guardrails
    and ensure that the project goals are aligned with the business goals. One of
    the guardrails is that a success metric can help guide the choice of a proper
    label that can at inference time, tangibly improve the business processes or otherwise
    create value in the business. First, let’s make sure we are aligned with what
    a label means, which is a value that you want the machine learning model to predict.
    The purpose of a machine learning model is to assign these labels automatically
    given some form of input data, and thus during the data preparation and model
    development stages, a label needs to be chosen to serve that purpose. Choosing
    the wrong label can be catastrophic to a deep learning project as sometimes, when
    data is not readily available, it means the project has to start all over again
    from the data preparation stage. Labels should always be indirectly or directly
    attributed to the success metric.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 成功指标在早期定义时，充当着机器学习项目的保护线，确保项目目标与业务目标一致。保护线之一是，成功指标可以帮助指导选择一个适当的标签，该标签在推理时能切实改善业务流程，或以其他方式为业务创造价值。首先，让我们确保我们对标签的含义有统一的理解，标签是你希望机器学习模型预测的一个值。机器学习模型的目的是根据某些形式的输入数据自动分配这些标签，因此在数据准备和模型开发阶段，需要选择一个标签来服务于这一目的。选择错误的标签可能会对深度学习项目造成灾难性后果，因为有时候，当数据无法直接获取时，意味着项目必须从数据准备阶段重新开始。标签应始终间接或直接与成功指标相关联。
- en: 'Success metrics, as the name suggests, can be plural, and range from time-based
    success definitions or milestones to the overall project success, and from intangible
    to tangible. It’s good practice to generally brainstorm and document all the possible
    success criteria from a low level to a high level. Another best practice is to
    make sure to always define tangible success metrics alongside intangible metrics.
    Intangible metrics generate awareness, but tangible metrics make sure things are
    measurable and thus make them that much more attainable. A few examples of intangible
    and hard-to-measure metrics are as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 成功指标，顾名思义，可以是复数，范围从基于时间的成功定义或里程碑到整体项目成功，从无形到有形。最佳实践是通常进行头脑风暴并记录所有可能的成功标准，从低层次到高层次。另一个最佳实践是确保始终与无形指标一起定义有形成功指标。无形指标能产生意识，但有形指标确保事物是可衡量的，从而使其更加可达。以下是一些无形且难以衡量的指标示例：
- en: Increasing customer satisfaction
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高客户满意度
- en: Increasing employee performance
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高员工表现
- en: Improving shareholder outlook
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改善股东前景
- en: 'Metrics are ways to measure something and are tied to goals to seal the deal.
    Goals themselves can be intangible, similar to the few examples listed previously,
    but so long as it is tied to tangible metrics, the project is off to a good start.
    When you have a clear goal, ask yourself in what way the goal can be proven to
    be achieved, demonstrated, or measured. A few examples of tangible success metrics
    for machine learning projects that could align with business goals are as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 指标是衡量事物的方式，并与目标紧密相关，以确保交易的达成。目标本身可以是无形的，类似于前面列出的几个示例，但只要它与有形指标挂钩，项目就有了良好的开端。当你有了明确的目标时，问问自己，如何证明目标已经实现、展示或衡量。以下是一些可能与业务目标对齐的机器学习项目的有形成功指标示例：
- en: Increase the time customers spend, which can be a proxy for customer delight
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加客户的停留时间，这可以作为客户满意度的一个代理指标
- en: Increase company revenue, which can be a proxy for employee performance
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加公司收入，这可以作为员工表现的一个代理指标
- en: Increase the **click-through rate** (**CTR**), which can be a proxy for the
    effectiveness of targeted marketing campaigns
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加**点击率**（**CTR**），这可以作为定向营销活动有效性的一个代理指标
- en: Increase the **customer lifetime value** (**CLTV**), which can be a proxy for
    long-term customer satisfaction and loyalty
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加**客户生命周期价值**（**CLTV**），这可以作为长期客户满意度和忠诚度的一个代理指标
- en: Increase conversion rate, which can be a proxy for the success of promotional
    campaigns and website user experience
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高转化率，这可以作为促销活动和网站用户体验成功的一个代理指标
- en: This concept is not new nor limited to just machine learning projects – just
    about any single project carried out for a company as every single real-world
    project needs to be aligned with the business goal. Many foundational project
    management techniques can be applied similarly to machine learning projects, and
    spending time gaining some project management skills out of the machine learning
    field would be beneficial and transferable to machine learning projects. Additionally,
    as machine learning is considered to be a software-based technology, software
    project management methodologies also apply.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念并不新颖，也不仅限于机器学习项目——几乎每个为公司进行的项目都需要与业务目标对齐。许多基础的项目管理技术同样适用于机器学习项目，花时间学习一些项目管理技能对机器学习项目来说是有益的，并且这些技能可以迁移到机器学习项目中。此外，由于机器学习被认为是一项基于软件的技术，软件项目管理方法论同样适用。
- en: A final concluding thought to take away is that machine learning systems are
    not about how advanced your machine learning models are, but instead about how
    humans and machine intelligence can work together to achieve a greater good and
    create value.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后需要记住的一点是，机器学习系统不仅仅关乎你的机器学习模型有多先进，而是关于人类与机器智能如何协作，以实现更大的目标并创造价值。
- en: Planning resources
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规划资源
- en: Deep learning often involves neural network architectures with a large set of
    parameters, otherwise called weights. These architecture’s sizes can go from holding
    a few parameters up to holding hundreds of billions of parameters. For example,
    an OpenAI GPT-3 text generation model holds 175 billion neural network parameters,
    which amounts to around 350 GB in computer storage size. This means that to run
    GPT-3, you need a machine with a **random access memory** (**RAM**) size of at
    least 350 GB!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习通常涉及拥有大量参数（即权重）的神经网络架构。这些架构的大小可以从仅包含几个参数到包含数百亿个参数。例如，OpenAI的GPT-3文本生成模型包含1750亿个神经网络参数，存储大小约为350
    GB。这意味着，要运行GPT-3，你需要一台至少有350 GB **随机存取内存**（**RAM**）的机器！
- en: Deep learning model frameworks such as PyTorch and TensorFlow have been built
    to work with devices called **graphics processing units** (**GPUs**), which offer
    tremendous neural network model training and inference speedups. Off-the-shelf
    GPU devices commonly have a GPU RAM of 12 GB and are nowhere near the requirements
    needed to load a GPT-3 model in GPU mode. However, there are still methods to
    partition big models into multiple GPUs and run the model on GPUs. Additionally,
    some methods can allow for distributed GPU model training and inference to support
    larger data batch sizes at any one usage point. GPUs are not considered cheap
    devices and can cost anywhere from a few hundred bucks to hundreds of thousands
    from the most widely used GPU brand, Nvidia. With the rise of cryptocurrency technologies,
    the availability of GPUs is also reduced significantly due to people buying them
    immediately when they are in stock. All these emphasize the need to plan computing
    resources for training and inferencing deep learning models beforehand.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型框架，如PyTorch和TensorFlow，已经被构建为可以与被称为**图形处理单元**（**GPU**）的设备一起使用，这些设备能提供巨大的神经网络模型训练和推理加速。市面上的GPU设备通常配备12
    GB的GPU内存，远远不足以在GPU模式下加载GPT-3模型。然而，仍然有方法将大型模型划分到多个GPU上，并在GPU上运行该模型。此外，某些方法还可以支持分布式GPU模型训练和推理，以支持在任何一个使用点更大的数据批量大小。GPU并不是便宜的设备，最广泛使用的GPU品牌Nvidia的价格从几百美元到数十万美元不等。随着加密货币技术的兴起，GPU的可用性显著下降，因为人们一有库存就会立即购买它们。所有这些都突显了在深度学习模型训练和推理之前规划计算资源的重要性。
- en: It is important to align your model development and deployment needs to your
    computing resource allocation early in the project. Start by gauging the range
    of sizes of deep learning architectures that are suitable for the task at hand
    either by browsing research papers or websites that provide a good summary of
    techniques, and setting aside computing resources for the model development process.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目初期，将你的模型开发和部署需求与计算资源分配对齐是非常重要的。首先，通过浏览研究论文或提供技术总结的优质网站，评估适合当前任务的深度学习架构的规模范围，并为模型开发过程预留计算资源。
- en: Tip
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: '[paperswithcode.com](http://paperswithcode.com) provides summaries of a wide
    variety of techniques grouped by a wide variety of tasks!'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[paperswithcode.com](http://paperswithcode.com) 提供了按各种任务分类的广泛技术总结！'
- en: When computing resources are not readily available, make sure you always make
    purchase plans early, especially if it involves GPUs. But what if a physical machine
    is not desired? An alternative to using computing resources is to use paid cloud
    computing resource providers you can access online easily from anywhere in the
    world. During the model development stage, one of the benefits of having more
    GPUs with more RAM allocated is that it can allow you to train models faster by
    either using a larger data batch size during training or allowing the capability
    to train multiple models at any one time. It is generally fine to also use CPU-only
    deep learning model training, but the model training time would just inevitably
    be much longer.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当计算资源不可随时使用时，确保你总是提前制定购买计划，尤其是涉及到GPU时。但如果不想使用物理机器怎么办？使用计算资源的另一种选择是使用付费云计算资源提供商，您可以轻松地在世界任何地方在线访问。在模型开发阶段，分配更多GPU和更大RAM的一个好处是，可以通过在训练过程中使用更大的数据批次，或者允许同时训练多个模型，从而加速模型训练。通常也可以使用仅CPU进行深度学习模型训练，但模型训练时间会不可避免地更长。
- en: 'The GPU and CPU-based computing resources that are required during training
    are often considered overkill to be used during inference time when they are deployed.
    Different applications have different deployment computing requirements and the
    decision on what resource specification to allocate can be gauged by asking yourself
    the following three questions:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中所需的基于GPU和CPU的计算资源，通常在推理时使用时被认为是过剩的。不同的应用程序有不同的部署计算需求，决定分配哪些资源规格可以通过以下三个问题来评估：
- en: How often are the inference requests made?
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理请求的频率是多少？
- en: Many inference requests in a short period might signal the need to have more
    than one inference service up in multiple computing devices in parallel
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短时间内的多个推理请求可能意味着需要在多个计算设备上并行运行多个推理服务
- en: What is the average amount of samples that are requested for a prediction at
    any one time?
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次请求预测时，平均请求的样本数量是多少？
- en: Device RAM requirements should match batch size expectations
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设备的RAM需求应与批处理大小预期相匹配
- en: How fast do you need a reply?
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要多快的回复？
- en: GPUs are needed if it’s seconds or a faster response time requirement
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果响应时间要求为秒级或更快，则需要GPU
- en: CPUs can do the job if you don’t care about the response time
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你不在乎响应时间，CPU也能完成任务
- en: Resource planning is not restricted to just computing resource planning – it
    also expands to human resource planning. Assumptions for the number of deep learning
    engineers and data scientists working together in a team would ultimately affect
    the choices of software libraries and tools used in the model development process.
    The analogy of choosing these tools will be introduced in future sections.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 资源规划不仅仅局限于计算资源规划，还扩展到人力资源规划。假设团队中有多少名深度学习工程师和数据科学家共同工作，最终将影响模型开发过程中所使用的软件库和工具的选择。选择这些工具的类比将在未来的章节中介绍。
- en: The next step is to prepare your data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是准备数据。
- en: Preparing data
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'Data is to machine learning models as is the fuel to your car, the electricity
    to your electronic devices, and the food for your body. A machine learning model
    works by trying to capture the relationships between the provided input and output
    data. Similar to how human brains work, a machine learning model will attempt
    to iterate through collected data examples and slowly build a memory of the patterns
    required to map the provided input data to the provided target output data. The
    data preparation stage consists of methods and processes required to prepare ready-to-use
    data to build a machine learning model that includes the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 数据对于机器学习模型，就像燃料对于汽车、电力对于电子设备、食物对于身体一样。机器学习模型通过尝试捕捉提供的输入数据与输出数据之间的关系来工作。类似于人脑的工作方式，机器学习模型将尝试遍历收集到的数据示例，并慢慢建立记忆，以便将提供的输入数据映射到提供的目标输出数据。数据准备阶段包括为构建机器学习模型准备好可以直接使用的数据的方法和过程，其中包括以下内容：
- en: Acquisition of raw input and targeted output data
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始输入和目标输出数据的获取
- en: Exploratory data analysis of the acquired data
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取数据的探索性数据分析
- en: Data pre-processing
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理
- en: We will discuss each of these topics in the following subsections.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在以下小节中讨论这些话题。
- en: Deep learning problem types
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习问题类型
- en: Deep learning can be broadly categorized into two problem types, namely **supervised
    learning** and **unsupervised learning**. Both of these problem types involve
    building a deep learning model that is capable of making informed predictions
    as outputs, given well-defined data inputs.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习可以大致分为两类问题类型，即**有监督学习**和**无监督学习**。这两种问题类型都涉及构建一个能够根据明确的数据输入做出有根据的预测输出的深度学习模型。
- en: Supervised learning is a problem type where labels are involved that act as
    the source of truth to learn from. Labels can exist in many forms and can be broken
    down into two problem types, namely **classification** and **regression**. Classification
    is the process where a specific discrete class is predicted among other classes
    when given input data. Many more complex problems derive from the base classification
    problem types, such as **instance segmentation**, **multilabel classification**,
    and **object detection**. Regression, on the other hand, is the process where
    a continuous numerical value is predicted when given input data. Likewise, complex
    problem types can be derived from the base regression problem type, such as **multi-regression**
    and **image bounding** **box regression**.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**有监督学习**是一种涉及标签的问题类型，标签作为学习的真实来源。标签可以有多种形式，可以细分为两种问题类型，即**分类**和**回归**。分类是指在给定输入数据时，预测特定离散类别的问题。许多更复杂的问题来自于基础的分类问题类型，如**实例分割**、**多标签分类**和**目标检测**。而回归则是指在给定输入数据时，预测连续数值的问题。同样，复杂问题类型也可以从基础的回归问题类型中派生出来，如**多重回归**和**图像边界框回归**。'
- en: Unsupervised learning, on the other hand, is a problem type where there aren’t
    any labels involved and the goals can vary widely. Anomaly detection, clustering,
    and feature representation learning are the most common problem types that belong
    to the unsupervised learning category.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**无监督学习**是一种不涉及标签的问题类型，其目标可以有很大的差异。异常检测、聚类和特征表示学习是最常见的属于无监督学习类别的问题类型。
- en: We will go through these two problem types separately for deep learning in [*Chapter
    8*](B18187_08.xhtml#_idTextAnchor125), *Exploring Supervised Deep Learning*, and
    [*Chapter 9*](B18187_09.xhtml#_idTextAnchor149), *Exploring Unsupervised* *Deep
    Learning*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第8章*](B18187_08.xhtml#_idTextAnchor125)《探索有监督深度学习》和[*第9章*](B18187_09.xhtml#_idTextAnchor149)《探索无监督深度学习》中分别讲解这两种问题类型。
- en: Next, let’s learn about the things you should consider when acquiring data.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们了解在获取数据时应该考虑的事项。
- en: Acquiring data
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取数据
- en: Acquiring data in the context of deep learning usually involves unstructured
    data, which includes image data, video data, text data, and audio data. Sometimes,
    data can be readily available and stored through some business processes in a
    database but very often, it has to be collected manually from the environment
    from scratch. Additionally, very often, labels for this data are not readily available
    and require manual annotation work. Along with the capability of deep learning
    algorithms to process and digest highly complex data comes the need to feed it
    more data compared to its machine learning counterparts. The requirement to perform
    data collection and data annotation in high volumes is the main reason why deep
    learning is considered to have a high barrier of entry today.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的背景下，获取数据通常涉及非结构化数据，包括图像数据、视频数据、文本数据和音频数据。有时候，数据可以通过某些业务流程在数据库中预先存储并随时使用，但更多时候，它必须从头开始从环境中手动收集。此外，通常这些数据的标签并不容易获得，需要手动标注。随着深度学习算法能够处理和消化高度复杂数据的能力，它也需要比传统机器学习方法更多的数据。执行大量数据收集和数据标注的需求是深度学习今天被认为具有高门槛的主要原因。
- en: Don’t rush into choosing an algorithm quickly in a machine learning project.
    Spend a quality amount of time formally defining the features that can be acquired
    to predict the target variable. Get help from domain experts during the process
    and brainstorm potential predictive features that relate to the target variable.
    In actual projects, it is common to spend a big portion of your time planning
    and acquiring the data while making sure the acquired data is fit for a machine
    learning model’s consumption and subsequently spending the rest of the time in
    model building, model deployment, and model governance. A lot of research has
    been done into handling bad-quality data during the model development stage but
    most of these techniques aren’t comprehensive and are limited in ways that they
    can cover up the inherent quality of the data. Displaying ignorance in quality
    assurance during the data acquisition stage and showing enthusiasm only in the
    data science portion of the workflow is a strong indicator that the project would
    be doomed to failure right from the inception stage.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习项目中，不要急于快速选择一个算法。花一些时间正式定义可以用来预测目标变量的特征。在这个过程中，寻求领域专家的帮助，并集思广益，寻找与目标变量相关的潜在预测特征。在实际项目中，通常会花费大量时间规划和获取数据，同时确保获得的数据适合机器学习模型使用，之后才会将剩余时间用于模型构建、部署和治理。虽然关于如何处理模型开发阶段的低质量数据已经进行了大量研究，但大多数技术并不全面，且在一定程度上只能掩盖数据固有的质量问题。如果在数据采集阶段对质量保证表现出无知，只在数据科学部分表现出热情，那么项目从一开始就注定会失败。
- en: 'Formulating a data acquisition strategy is a daunting task when you don’t know
    what it means to have good-quality data. Let’s go through a few pillars of data
    quality you should consider for your data in the context of actual business use
    cases and machine learning:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 制定数据采集策略是一项艰巨的任务，特别是当你不知道什么才算是高质量数据时。让我们来看一下，在实际商业应用和机器学习的背景下，你应该考虑的一些数据质量的关键要素：
- en: '**Representativeness**: How representative is the data concerning the real-world
    data population?'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代表性**：数据在多大程度上能代表真实世界数据集？'
- en: '**Consistency**: How consistent are the annotation methods? Does the same pattern
    match the same label or are there some inconsistencies?'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性**：注释方法有多一致？相同的模式是否匹配相同的标签，还是存在一些不一致之处？'
- en: '**Comprehensiveness**: Are all variations of a specific label covered in the
    collected dataset?'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全面性**：收集的数据集中是否涵盖了特定标签的所有变体？'
- en: '**Uniqueness**: Does the data contain a lot of duplicated or similar data?'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独特性**：数据中是否包含大量重复或相似的数据？'
- en: '**Fairness**: Is the collected data biased toward any specific labels or data
    groups?'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公平性**：收集的数据是否对某些特定标签或数据群体存在偏见？'
- en: '**Validity**: Does the data contain invalid fields? Do the data inputs match
    up with their labels? Is there missing data?'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有效性**：数据中是否包含无效字段？数据输入是否与其标签匹配？是否存在缺失数据？'
- en: Let’s look at each of these in detail.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看这些要素。
- en: Representativeness
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代表性
- en: Data should be collected in a way that it mimics what data you will receive
    during model deployment as much as possible. Very often in research-based deep
    learning projects, researchers collect their data in a closed environment with
    controlled environmental variables. One of the reasons researchers prefer collecting
    data from a controlled environment is that they can build stabler machine learning
    models and generally try to prove a point. Eventually, when the research paper
    is published, you see amazing results that were applied using handpicked data
    to impress. These models, which are built on controlled data, fail miserably when
    you apply them to random uncontrolled real-world examples. Don’t get me wrong
    – it’s great to have these controlled datasets available to contribute toward
    a stabler machine learning model at times, but having uncontrolled real-world
    examples as a main part of the training and evaluation datasets is key to achieving
    a generalizable model.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 数据应尽可能模仿在模型部署期间将接收到的数据方式进行收集。在基于研究的深度学习项目中，研究人员通常会在一个封闭的环境中收集数据，并控制环境变量。研究人员偏好从控制环境中收集数据的原因之一是他们可以建立更稳定的机器学习模型，并通常试图证明某个观点。最终，当研究论文发布时，你会看到使用精心挑选的数据得出的惊人结果，旨在给人留下深刻印象。这些基于控制数据建立的模型，在应用到随机的、无法控制的现实世界示例时，往往会惨遭失败。别误会——拥有这些控制数据集在某些时候有助于建立更稳定的机器学习模型，但将无法控制的现实世界示例作为训练和评估数据集的主要部分，是实现通用模型的关键。
- en: Sometimes, the acquired training data has an expiry date and does not stay representative
    forever. This scenario is called data drift and will be discussed in more detail
    in the *Managing risk* section closer to the end of this chapter. The representativeness
    metric for data quality should also be evaluated based on the future expectations
    of the data the model will receive during deployment.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，获取的训练数据有保质期，并不会永远保持代表性。这个场景被称为数据漂移，将在本章末尾的*管理风险*部分中详细讨论。数据质量的代表性指标还应根据模型在部署期间将接收的数据的未来预期进行评估。
- en: Consistency
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一致性
- en: Data labels that are not consistently annotated make it harder for machine learning
    models to learn from them. This happens when the domain ideologies and annotation
    strategies differ among multiple labelers and are just not defined properly. For
    example, “Regular” and “Normal” mean the same thing, but to the machine, it’s
    two completely different classes; so are “Normal” and “normal” with just a capitalization
    difference!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 标注不一致的数据标签使得机器学习模型更难从中学习。当多个标注者之间的领域理念和标注策略存在差异且没有正确定义时，就会发生这种情况。例如，“Regular”和“Normal”意思相同，但对机器而言，它们是两个完全不同的类别；同样，“Normal”和“normal”仅有大小写差异！
- en: Practice formalizing a proper strategy for label annotation during the planning
    stage before carrying out the actual annotation process. Cleaning the data for
    simple consistency errors is possible post-data annotation, but some consistency
    errors can be hard to detect and complex to correct.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行实际标注过程之前，练习在规划阶段制定合适的标签标注策略。对于一些简单的一致性错误，在数据标注后可以进行清理，但有些一致性错误可能难以检测且复杂难以修正。
- en: Comprehensiveness
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全面性
- en: Machine learning thrives in building a decisioning mechanism that is robust
    to multiple variations and views of any specific label. Being capable and accomplishing
    it are two different things. One of the prerequisites of decisioning robustness
    is that the data that’s used for training and evaluation itself has to be comprehensive
    enough to provide coverage for all possible variations of each provided label.
    How can comprehensiveness be judged? Well, that depends on the complexity of the
    labels and how varied they can present themselves naturally when the model is
    deployed. More complex labels naturally require more samples and less complex
    labels require fewer samples.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习致力于建立一个对任何特定标签的多种变体和视角都能保持稳健的决策机制。具备这一能力和成功实现它是两回事。决策稳健性的先决条件之一是用于训练和评估的数据本身必须足够全面，以涵盖每个标签的所有可能变体。如何判断全面性呢？这取决于标签的复杂性以及它们在模型部署时自然呈现的变体有多大。更复杂的标签自然需要更多的样本，而较简单的标签则需要更少的样本。
- en: A good point to start with, in the context of deep learning, is to have at least
    100 samples for each label and experiment with building a model and deriving model
    insights to see if there are enough samples for the model to generalize on unseen
    variations of the label. When the model doesn’t produce convincing results, that’s
    when you need to cycle back to the data preparation stage again to acquire more
    data variations of any specific label. The machine learning life cycle is inherently
    a cyclical process where you will experiment, explore, and verify while transitioning
    between stages to obtain the answers you need to solve your problems, so don’t
    be afraid to execute these different stages cyclically.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的背景下，一个好的起点是每个标签至少拥有100个样本，并进行模型构建和推导模型见解的实验，以查看是否有足够的样本使模型能够对标签的未见变体进行泛化。当模型未能产生令人信服的结果时，您需要回到数据准备阶段，收集更多特定标签的数据变体。机器学习生命周期本质上是一个循环过程，您将在不同阶段之间实验、探索并验证，以获得解决问题所需的答案，因此不要害怕循环执行这些不同的阶段。
- en: Uniqueness
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独特性
- en: While having complete and comprehensive data is beneficial to build a machine
    learning model that is robust to data variations, having duplicated versions of
    the same data variation in the acquired dataset risks creating a biased model.
    A biased model makes biased decisions that can be unethical and illegal and sometimes
    renders such decisions meaningless. Additionally, the amount of data acquired
    for any specific label is rendered meaningless when all of them are duplicated
    or very similar to each other.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然拥有完整和全面的数据有助于构建一个能应对数据变化的稳健机器学习模型，但在获取的数据集中重复相同数据变体的版本则有可能导致模型产生偏见。一个有偏见的模型会做出偏颇的决策，这些决策可能不道德、违法，甚至有时完全没有意义。此外，任何特定标签的获取数据量在所有数据都重复或非常相似时变得毫无意义。
- en: Machine learning models are generally trained on a subset of the acquired data
    and then evaluated on other subsets of the data to verify the model’s performance
    on unseen data. When the part of the dataset that is not unique gets placed in
    the evaluation partition of the acquired dataset by chance, the model risks reporting
    scores that are biased against the duplicated data inputs.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型通常是在获取的部分数据上进行训练，然后在其他数据子集上进行评估，以验证模型在未见数据上的表现。当获取的数据集中的非独特部分偶然被放入评估分区时，模型可能会报告对重复数据输入存在偏见的评分。
- en: Fairness
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 公平性
- en: Does the acquired dataset represent minority groups properly? Is the dataset
    biased toward the majority groups in the population? There can be many reasons
    why a machine learning model turns out to be biased, but one of the main causes
    is data representation bias. Making sure the data is represented fairly and equitably
    is an ethical responsibility of all machine learning practitioners. There are
    a lot of types of bias, so this topic will have its own section and will be introduced
    along with methods of mitigating it in [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196),
    *Exploring Bias* *and Fairness*.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 获取的数据集是否适当地代表了少数群体？该数据集是否对人口中的多数群体存在偏向？机器学习模型出现偏见的原因可能有很多，但其中一个主要原因是数据表示偏见。确保数据公平、公正地表示是所有机器学习从业者的道德责任。偏见的类型有很多，因此这个话题将有自己的一章，并将在[*第13章*](B18187_13.xhtml#_idTextAnchor196)《探索偏见与公平》中介绍，*偏见*
    *和公平*的缓解方法。
- en: Validity
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有效性
- en: Are there outliers in the dataset? Is there missing data in the dataset? Did
    you accidentally add a blank audio or image file to the properly collected and
    annotated dataset? Is the annotated label for the data input considered a valid
    label? These are some of the questions you should ask when considering the validity
    of your dataset.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中是否存在异常值？数据集中是否缺失数据？您是否不小心将一个空白的音频或图像文件添加到了正确收集和标注的数据集中？数据输入的标注标签是否被认为是有效标签？这些是考虑数据集有效性时应该问的一些问题。
- en: Invalid data is useless for machine learning models and some of these complicate
    the pre-processing required for them. The reasons for invalidity can range from
    simple human errors to complex domain knowledge mistakes. One of the methods to
    mitigate invalid data is to separate validated and unvalidated data. Include some
    form of automated or manual data validation process before a data sample gets
    included in the validated dataset category. Some of this validation logic can
    be derived from business processes or just common sense. For example, if we are
    taking age as input data, there are acceptable age ranges and there are age ranges
    that are just completely impossible, such as 1,000 years old. Having simple guardrails
    and verifying these values early when collecting them makes it possible to correct
    them then and there to get accurate and valid data. Otherwise, these data will
    likely be discarded when it comes to the model-building stage. Maintaining a structured
    framework to validate data ensures that the majority of the data stays relevant
    and usable by machine learning models and free from simple mistakes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 无效数据对机器学习模型来说是无用的，其中一些会使所需的预处理变得复杂。无效性的原因可以从简单的人工错误到复杂的领域知识错误不等。减少无效数据的一种方法是将已验证的数据和未验证的数据分开。在数据样本进入已验证数据集类别之前，包含某种形式的自动化或人工数据验证过程。这些验证逻辑可以来源于业务流程或常识。例如，如果我们把年龄作为输入数据，存在可接受的年龄范围，也有一些完全不可能的年龄范围，如1000岁。设立简单的防护措施并在收集这些数据时验证这些值，可以立即纠正它们，从而获得准确且有效的数据。否则，在模型构建阶段，这些数据很可能会被丢弃。维护一个结构化的框架来验证数据可以确保大多数数据保持相关且可供机器学习模型使用，并避免简单的错误。
- en: As for more complex invalidity, such as errors in the domain ideology, domain
    experts play a big part in making sure the data stays sane and logical. Always
    make sure you include domain experts when defining the data inputs and outputs
    in the discussion about how data should be collected and annotated for model development.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的无效数据，例如领域意识形态中的错误，领域专家在确保数据保持合理和逻辑性方面发挥着重要作用。在讨论如何收集和标注数据以进行模型开发时，始终确保包括领域专家来定义数据的输入和输出。
- en: Making sense of data through exploratory data analysis (EDA)
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过探索性数据分析（EDA）理解数据
- en: After the acquisition of data, it is crucial to analyze the data to inspect
    its characteristics, patterns that exist, and the general quality of the data.
    Knowing the type of data you are dealing with allows you to plan a strategy for
    the subsequent model-building stage. Plot distribution graphs, calculate statistics,
    and perform univariate and multivariate analysis to understand the inherent relationships
    between the data that can help further ensure the validity of the data. The methods
    of analysis for different variable types are different and can require some form
    of domain knowledge beforehand. In the following subsections, we will be practically
    going through **exploratory data analysis** (**EDA**) for text-based data to get
    a sense of the benefits of carrying out an EDA task.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 数据获取后，分析数据以检查其特征、存在的模式以及数据的一般质量是至关重要的。了解你处理的数据类型可以帮助你为后续的模型构建阶段规划策略。绘制分布图、计算统计数据，并进行单变量和多变量分析，以理解数据之间的内在关系，这可以进一步确保数据的有效性。不同变量类型的分析方法不同，并且可能需要事先具备某种领域知识。在以下子部分中，我们将通过**探索性数据分析**（**EDA**）实际处理基于文本的数据，以了解进行EDA任务的好处。
- en: Practical text EDA
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实际文本EDA
- en: 'In this section, we will be manually exploring and analyzing a text-specific
    dataset using Python code, with the motive of building a deep learning model later
    in this book using the same dataset. The dataset we use will predict the categories
    of an item on an Indian e-commerce website based on its textual description. This
    use case will be useful to automatically group advertised items for user recommendation
    usage and can help increase purchasing volume on the e-commerce website:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Python代码手动探索和分析一个特定于文本的数据集，目的是在本书的后续部分使用相同的数据集构建深度学习模型。我们使用的数据集将基于印度电商网站上商品的文本描述预测商品的类别。这个用例将有助于自动将广告商品进行分组，以供用户推荐使用，并可以帮助提高电商网站的购买量：
- en: 'Let’s start by defining the libraries that we will use in a notebook. We will
    be using `pandas` for data manipulation and structuring, `matplotlib` and `seaborn`
    for plotting graphs, `tqdm` for visualizing iteration progress, and `lingua` for
    text language detection:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们先定义在笔记本中将使用的库。我们将使用`pandas`进行数据处理和结构化，`matplotlib`和`seaborn`用于绘制图表，`tqdm`用于可视化迭代进度，`lingua`用于文本语言检测：
- en: '[PRE0]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, let’s load the text dataset using `pandas`:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们使用`pandas`加载文本数据集：
- en: '[PRE1]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`pandas` has some convenient functions to visualize and describe the loaded
    dataset; let’s use them. Let’s start by visualizing three rows of the raw data:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`pandas`有一些方便的函数来可视化和描述加载的数据集；我们来使用它们。我们从可视化原始数据的三行开始：'
- en: '[PRE2]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will display the following figure in your notebook:'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将在你的笔记本中显示以下图形：
- en: '![Figure 1.3 – Visualizing the text dataset samples](img/B18187_01_003.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.3 – 可视化文本数据集样本](img/B18187_01_003.jpg)'
- en: Figure 1.3 – Visualizing the text dataset samples
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 – 可视化文本数据集样本
- en: 'Next, let’s describe the dataset by visualizing the data column-based statistics:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们通过可视化数据列的统计信息来描述数据集：
- en: '[PRE3]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This will display the following figure in your notebook:'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将在你的笔记本中显示以下图形：
- en: '![Figure 1.4 – Showing the statistics of the dataset](img/B18187_01_004.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.4 – 显示数据集的统计信息](img/B18187_01_004.jpg)'
- en: Figure 1.4 – Showing the statistics of the dataset
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 – 显示数据集的统计信息
- en: 'With these visualizations, it’s obvious that the description of the dataset
    aligns with what exists in the dataset, where the **category** column contains
    four unique class categories paired with the text description data column named
    **description** with evidence showing that they are strings. One important insight
    from the describing function is that there are duplicates in the text description.
    We can remove duplicates by taking the first example row among all the duplicates,
    but we also have to make sure that the duplicates have the same category, so let’s
    do that:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过这些可视化，可以明显看出，数据集的描述与数据集中的内容一致，其中**category**列包含四个独特的类别，与名为**description**的文本描述数据列配对，且有证据表明它们是字符串。从描述函数中获得的一个重要见解是，文本描述中存在重复项。我们可以通过取所有重复项中的第一个示例行来删除重复项，但我们还必须确保重复项属于相同类别，因此我们来做一下：
- en: '[PRE4]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s check the data types of the columns:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查一下各列的数据类型：
- en: '[PRE5]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will display the following figure in your notebook:'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将在你的笔记本中显示以下图形：
- en: '![Figure 1.5 – Showing the data types of the dataset columns](img/B18187_01_005.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.5 – 显示数据集各列的数据类型](img/B18187_01_005.jpg)'
- en: Figure 1.5 – Showing the data types of the dataset columns
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5 – 显示数据集各列的数据类型
- en: 'When some samples aren’t inherently a string data type, such as empty data
    or maybe numbers data, pandas automatically use an `Object` data type that categorizes
    the entire column as data types that are unknown to pandas. Let’s check for empty
    values:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当某些样本本身不是字符串数据类型时，比如空数据或数字数据，pandas会自动使用`Object`数据类型，这会将整个列分类为pandas无法识别的数据类型。让我们检查空值：
- en: '[PRE6]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This gives us the following output:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '![Figure 1.6 – Checking empty values](img/B18187_01_006.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.6 – 检查空值](img/B18187_01_006.jpg)'
- en: Figure 1.6 – Checking empty values
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6 – 检查空值
- en: 'It looks like the description column has one empty value, as expected. This
    might be rooted in a mistake when acquiring the data or it might truly be empty.
    Either way, let’s remove that row as we can’t do anything to recover it and convert
    the columns into strings:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看起来描述列有一个空值，正如预期的那样。这可能是获取数据时的错误，或者它可能确实为空。无论如何，让我们删除这一行，因为我们无法恢复它，并将列转换为字符串：
- en: '[PRE7]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Earlier, we discovered four unique categories. Let’s make sure we have a decent
    amount of samples for each category by visualizing its distribution:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之前，我们发现了四个独特的类别。让我们通过可视化其分布来确保每个类别都有足够的样本：
- en: '[PRE8]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This will result in the following figure:'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将显示以下图形：
- en: '![Figure 1.7 – A graph showing category distribution](img/B18187_01_007.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.7 – 显示类别分布的图表](img/B18187_01_007.jpg)'
- en: Figure 1.7 – A graph showing category distribution
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7 – 显示类别分布的图表
- en: Each category has a good amount of data samples and doesn’t look like there
    are any anomaly categories.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 每个类别都有相当数量的数据样本，并且看起来没有异常类别。
- en: 'The goal here is to predict the category of the selling item through the item’s
    description on the Indian e-commerce website. From that context, we know that
    Indian citizens speak Hindi, so the dataset might not contain only English data.
    Let’s try to estimate and verify the available languages in the dataset using
    an open sourced language detector tool called Lingua. Lingua uses both rule-based
    and machine learning model-based methods to detect more than 70 text languages
    that work great for short phrases, single words, and sentences. Because of that,
    Lingua has a better runtime and memory performance. Let’s start by initializing
    the language detector instance from the `lingua` library:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里的目标是通过印度电子商务网站上的商品描述预测商品类别。从这个背景来看，我们知道印度公民讲印地语，因此数据集可能不仅包含英语数据。让我们尝试使用一个名为Lingua的开源语言检测工具来估计和验证数据集中可用的语言。Lingua结合了基于规则的方法和基于机器学习模型的方法，能够检测70多种文本语言，非常适合短语、单词和句子的检测。由于这个原因，Lingua在运行时和内存性能上表现更佳。让我们从`lingua`库初始化语言检测实例开始：
- en: '[PRE9]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we will randomly sample a small portion of the dataset to detect language
    as the detection algorithm takes time to complete. Using a 10% fraction of the
    data should allow us to adequately understand the data:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将随机抽取数据集的一小部分进行语言检测，因为检测算法需要时间完成。使用数据的10%作为样本应能充分理解数据：
- en: '[PRE10]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, let’s visualize the distribution of the language:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们可视化语言的分布：
- en: '[PRE11]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will show the following graph plot:'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将显示以下图形：
- en: '![Figure 1.8 – Text language distribution](img/B18187_01_008.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图1.8 – 文本语言分布](img/B18187_01_008.jpg)'
- en: Figure 1.8 – Text language distribution
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8 – 文本语言分布
- en: 'Interestingly, Lingua detected some anomalous samples that aren’t English.
    The anomalous languages look like they might be mistakes made by Lingua. Hindi
    is also detected among them; this is more convincing than the other languages
    as the data is from an Indian e-commerce website. Let’s check these samples out:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有趣的是，Lingua检测到了一些异常样本，它们不是英语。异常语言看起来可能是Lingua检测错误的结果。印地语也被检测到了，这比其他语言更有说服力，因为数据来自印度的电子商务网站。让我们来看一下这些样本：
- en: '[PRE12]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will show the following text:'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将显示以下文本：
- en: '![Figure 1.9 – Visualizing Hindi text](img/B18187_01_009.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图1.9 – 可视化印地语文本](img/B18187_01_009.jpg)'
- en: Figure 1.9 – Visualizing Hindi text
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.9 – 可视化印地语文本
- en: It looks like there is a mix of Hindi and English here. How about another language,
    such as French?
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看起来这里混合了印地语和英语。那其他语言呢，比如法语？
- en: '[PRE13]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This will show the following text:'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将显示以下文本：
- en: '![Figure 1.10 – Visualizing French text](img/B18187_01_010.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图1.10 – 可视化法语文本](img/B18187_01_010.jpg)'
- en: Figure 1.10 – Visualizing French text
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10 – 可视化法语文本
- en: It looks like **potpourri** was the focused word here as this is a borrowed
    French word, but the text is still generally English.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看起来**potpourri**是这里的重点词，因为这是一个借用的法语词汇，但文本总体上还是英语。
- en: 'Since the list of languages does not include languages that do not use space
    as a separator between logical word units, let’s attempt to gauge the distribution
    of words by using a space-based word separation. Word counts and character counts
    can affect the parameters of a deep learning neural network, so it will be useful
    to understand these values during EDA:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于语言列表中不包括不使用空格作为逻辑词单元分隔符的语言，我们尝试通过基于空格的词分隔来衡量词语的分布。词计数和字符计数会影响深度学习神经网络的参数，因此在EDA过程中理解这些值会非常有用：
- en: '[PRE14]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will show the following bar plot:'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将显示以下条形图：
- en: '![Figure 1.11 – Word count distribution](img/B18187_01_011.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图1.11 – 词计数分布](img/B18187_01_011.jpg)'
- en: Figure 1.11 – Word count distribution
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.11 – 词计数分布
- en: 'From the exploration and analysis of the text data, we can deduce a couple
    of reasons that will help set up the model type and structure we should use during
    the model development stage:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对文本数据的探索和分析，我们可以推测出几个原因，这些原因有助于在模型开发阶段确定我们应该使用的模型类型和结构：
- en: The labels are decently sampled with 5,000-11,000 worth of samples per label,
    making them suitable for deep learning algorithms.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签样本采样得当，每个标签有5,000-11,000个样本，这使得它们适用于深度学习算法。
- en: The original data is not clean, has missing data, and duplicates but is fixable
    through manual processing. Using it as-is for model development would have the
    potential of creating a biased model.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始数据不干净，存在缺失数据和重复项，但可以通过手动处理修复。直接使用原始数据进行模型开发可能会导致模型产生偏差。
- en: The dataset has more than one language but mostly English text; this will allow
    us to make appropriate model choices during the model development stage.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集包含多种语言，但主要是英文文本；这将帮助我们在模型开发阶段做出合适的模型选择。
- en: An abundance of samples has fewer than 1,000 words, and some samples have 1,000-8,000
    words. In some non-critical use cases, we can safely cap the number of words to
    around 1,000 words so that we can build a model with better memory and runtime
    performance.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数样本少于1,000个单词，一些样本有1,000-8,000个单词。在一些非关键性应用场景中，我们可以安全地将单词数限制在大约1,000个单词，以便我们可以构建一个具有更好内存和运行时性能的模型。
- en: The preceding practical example should provide a simple experience of performing
    EDA that will be sufficient to understand the benefit and importance of running
    an in-depth EDA before going into the model development stage. Similar to the
    practical text EDA, we prepared a practical EDA sample workflow for other datasets
    that includes audio, image, and video datasets in our Packt GitHub repository
    that you should explore to get your hands dirty.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的实际示例应该提供了一个简单的EDA体验，足以让你理解在进入模型开发阶段之前执行深入EDA的好处和重要性。与实际文本EDA类似，我们在Packt GitHub仓库中为其他数据集准备了一个实际的EDA样本工作流，包括音频、图像和视频数据集，你可以探索这些内容，亲自动手实践。
- en: A major concept to grasp in this section is the importance of EDA and the level
    of curiosity you should display to uncover the truth about your data. Some methods
    are generalizable to other similar datasets, but treating any specific EDA workflow
    as a silver bullet blinds you to the increasing research people are contributing
    to this field. Ask questions about your data whenever you suspect something of
    it and attempt to uncover the answers yourself by doing manual or automated inspections
    however possible. Be creative in obtaining these answers and stay hungry in learning
    new ways you can figure out key information on your data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中需要掌握的一个主要概念是EDA的重要性，以及你应该展现出多大的好奇心来揭示数据的真相。一些方法可以推广到其他类似的数据集，但将任何特定的EDA工作流视为灵丹妙药，会使你忽视人们在这一领域不断贡献的研究。每当你怀疑数据中存在某些问题时，就要对数据提出疑问，并尽可能通过手动或自动检查来揭示答案。在获得这些答案时要富有创造力，并保持对学习新方法的渴望，探索如何在数据中发现关键信息。
- en: In this section, we have methodologically and practically gone through EDA processes
    for different types of data. Next, we will explore what it takes to prepare the
    data for actual model consumption.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已通过方法论和实践步骤讲解了不同类型数据的EDA过程。接下来，我们将探讨为实际模型使用准备数据需要做什么。
- en: Data pre-processing
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Data pre-processing involves data cleaning, data structuring, and data transforming
    so that a deep learning model will be capable of using the processed data for
    model training, evaluation, and inferencing during deployment. The processed data
    should not only be prepared just for the machine learning model to accept but
    should generally be processed in a way that optimizes the learning potential and
    increases the metric performance of the machine learning model.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理包括数据清理、数据结构化和数据转换，以便深度学习模型能够使用处理后的数据进行模型训练、评估和部署推理。处理后的数据不仅仅是为了让机器学习模型接受，还应该以一种优化学习潜力并提高机器学习模型指标性能的方式进行处理。
- en: Data cleaning is a process that aims to increase the quality of the data acquired.
    An EDA process is a prerequisite to figuring out anything wrong with the dataset
    before some form of data cleaning can be done. Data cleaning and EDA are often
    executed iteratively until a satisfactory data quality level is achieved. Cleaning
    can be as simple as duplicate values removal, empty values removal, or removing
    values that don’t make logical sense, either in terms of common sense or through
    business logic. These are concepts that we explained earlier, where the same risks
    and issues are applied.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理是一个旨在提高数据质量的过程。EDA过程是识别数据集问题的前提，只有在进行数据清理之前，才能发现数据中的任何问题。数据清理和EDA通常是迭代执行的，直到达到令人满意的数据质量水平。清理可能很简单，比如删除重复值、删除空值或删除不合逻辑的值，无论是从常识角度还是从业务逻辑角度来看。这些都是我们之前解释过的概念，同样的风险和问题也适用。
- en: 'Data structuring, on the other hand, is a process that orchestrates the data
    ingest and loading process from the stored data that is cleaned and verified of
    its quality. This process determines how data should be loaded from a source or
    multiple of them and fed into the deep learning model. Sounds simple enough, right?
    This could be very simple if this is a small, single CSV dataset where there wouldn’t
    be any performance or memory issues. In reality, this could be very complex in
    cases where data might be partitioned and stored in different sources due to storage
    limitations. Here are some concrete factors you’d need to consider in this process:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 数据结构化是一个协调数据摄取和加载过程的过程，涉及从存储的数据中加载已经清理并验证过质量的数据。这个过程决定了数据应如何从一个或多个源加载，并输入到深度学习模型中。听起来很简单，对吧？如果这是一个小型的单一CSV数据集，可能不会出现性能或内存问题，这会非常简单。然而，实际上，在数据可能因存储限制而被分割并存储在不同源的情况下，这个过程可能非常复杂。以下是一些你需要在此过程中考虑的具体因素：
- en: Do you have enough RAM in your computer to process your desired batch size to
    supply data for your model? Make sure you also take your model size into account
    so that you won’t get memory overloads and **Out of Memory** (**OOM**) errors!
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的计算机有足够的内存来处理你所需的批次大小，以供模型使用吗？确保同时考虑模型大小，以免出现内存过载和**内存溢出**（**OOM**）错误！
- en: Is your data from different sources? Make sure you have permission to access
    these data sources.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的数据来自不同的来源吗？确保你拥有访问这些数据源的权限。
- en: Is the speed latency when accessing these sources acceptable? Consider moving
    this data to a better hardware resource that you can access with higher speeds,
    such as a **solid-state drive** (**SSD**) instead of a **hard disk drive** (**HDD**),
    and from a remote network-accessible source to a direct local hardware source.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问这些源时的速度延迟可接受吗？考虑将数据转移到一个更好的硬件资源上，这样你就能以更高的速度访问，例如使用**固态硬盘**（**SSD**）代替**硬盘驱动器**（**HDD**），并从远程网络可访问的源转移到直接的本地硬件源。
- en: Do you even have enough local storage to store this data? Make sure you have
    enough storage to store this data, don’t overload the storage and risk performance
    slowdowns or worse, computer breakdowns.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你甚至有足够的本地存储空间来存储这些数据吗？确保你有足够的存储空间来保存这些数据，不要超载存储空间，以免影响性能，甚至导致计算机崩溃。
- en: Optimize the data loading and processing process so that it is fast. Store and
    cache outputs of data processes that are fixed so that you can save time that
    can be used to recompute these outputs.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化数据加载和处理过程，使其更加高效。存储和缓存固定的数据处理输出，这样你可以节省时间，避免重新计算这些输出。
- en: Make sure the data structuring process is deterministic, even when there are
    processes that need randomness. **Randomly deterministic** is when the randomness
    can be reproduced in a repeat of the cycle. Determinism helps make sure that the
    results that have been obtained can be reproduced and make sure model-building
    methods can be compared fairly and reliably.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保数据结构化过程是确定性的，即使有些过程需要随机性。**随机确定性**指的是当随机性在循环重复时可以被复现。确定性有助于确保已获得的结果可以被复现，并确保模型构建方法可以公正且可靠地进行比较。
- en: Log data so that you can debug the process when needed.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录数据，以便在需要时调试过程。
- en: Data partitioning methods. Make sure a proper cross-validation strategy is chosen
    that’s suitable for your dataset. If a time-based feature is included, consider
    whether you should construct a time-based partitioning method where the training
    data consists of earlier time examples and the evaluation data is in the future.
    If not, a stratified partitioning method would be your best bet.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据划分方法。确保选择适合你数据集的正确交叉验证策略。如果包含时间特征，考虑是否应该构建基于时间的划分方法，其中训练数据来自早期时间的示例，而评估数据则来自未来。如果没有，分层划分方法将是最佳选择。
- en: Different deep learning frameworks, such as `keras` on top of TensorFlow, `Catalyst`
    on top of PyTorch, `fast ai` on top of PyTorch, `pytorch lightning` on top of
    PyTorch, and `ignite` on top of PyTorch.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的深度学习框架，例如基于TensorFlow的`keras`，基于PyTorch的`Catalyst`，基于PyTorch的`fast ai`，基于PyTorch的`pytorch
    lightning`，以及基于PyTorch的`ignite`。
- en: 'Finally, data transformation is a process that applies unique data variable-specific
    pre-processing to transform the raw cleaned data into more a representable, usable,
    and learnable format. An important factor to consider when attempting to execute
    the data structuring and transformation process is the type of deep learning model
    you intend to use. Any form of data transformation is often dependent on the deep
    learning architecture and dependent on the type of inputs it can accept. The most
    widely known and common deep learning model architectures are invented to tackle
    specific data types, such as convolutional neural networks for image data, transformer
    models for sequence-based data, and basic multilayer perceptrons for tabular data.
    However, deep learning models are considered to be flexible algorithms that can
    twist and bend to accept data of different forms and sizes, even in multimodal
    data conditions. Through collaboration with domain experts from the past few years,
    deep learning experts have been able to build creative forms of deep learning
    architectures that can handle multiple data modalities and even multiple unstructured
    data modalities that succeeded in learning cross-modality patterns. Here are two
    examples:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，数据转换是一个应用独特数据变量特定预处理的过程，用以将原始清理后的数据转化为更具表现力、可用和可学习的格式。在执行数据结构化和转换过程时，一个重要的因素是你打算使用的深度学习模型类型。任何形式的数据转换通常依赖于深度学习架构，并且依赖于它能接受的输入类型。最广为人知和常见的深度学习模型架构是为了解决特定的数据类型问题而发明的，例如卷积神经网络用于图像数据，变换器模型用于基于序列的数据，基础的多层感知器用于表格数据。然而，深度学习模型被认为是灵活的算法，可以适应不同形式和大小的数据，即使在多模态数据条件下也能处理。通过过去几年与领域专家的合作，深度学习专家已经能够构建出能够处理多种数据模态甚至多个非结构化数据模态的创意深度学习架构，这些架构在学习跨模态模式方面取得了成功。以下是两个例子：
- en: '*Robust Self-Supervised Audio-Visual Speech Recognition*, by Meta AI (formerly
    Facebook) ([https://arxiv.org/pdf/2201.01763v2.pdf):](https://arxiv.org/pdf/2201.01763v2.pdf):)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*强健的自监督音视频语音识别*，由Meta AI（前Facebook）发布（[https://arxiv.org/pdf/2201.01763v2.pdf](https://arxiv.org/pdf/2201.01763v2.pdf)）'
- en: This tackled the problem of speech recognition in the presence of multiple speeches
    by building a deep learning transformer-based model that can take in both audio
    and visual data called AV-HuBERT
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这解决了在多重发言的情况下进行语音识别的问题，构建了一个基于深度学习变换器的模型，可以同时接受音频和视觉数据，称为AV-HuBERT。
- en: Visual data acted as supplementary data to help the deep learning model discern
    which speaker to focus on.
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉数据作为补充数据，帮助深度学习模型分辨应该关注哪个说话者。
- en: It achieved the latest state-of-the-art results on the LRS3-TED visual and audio
    lip reading dataset
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在LRS3-TED视觉和音频唇语识别数据集上达到了最新的最先进的结果
- en: '*Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence
    Learning Framework*, by DAMO Academy and Alibaba Group ([https://arxiv.org/pdf/2202.03052v1.pdf):](https://arxiv.org/pdf/2202.03052v1.pdf):)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过简单的序列到序列学习框架统一架构、任务和模态*，由达摩院和阿里巴巴集团发布（[https://arxiv.org/pdf/2202.03052v1.pdf](https://arxiv.org/pdf/2202.03052v1.pdf)）'
- en: They built a model that took in text and image data and published a pre-trained
    model
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们构建了一个模型，该模型可以处理文本和图像数据，并发布了一个预训练模型
- en: At achieved state-of-the-art results on an image captioning task on the COCO
    captions dataset
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在COCO字幕数据集上完成了图像标注任务，并达到了最先进的结果
- en: 'With that being said, data transformations are mainly differentiated into two
    parts: **feature engineering** and **data scaling**. Deep learning is widely known
    for its feature engineering capabilities, which replace the need to manually craft
    custom features from raw data for learning. However, this doesn’t mean that it
    always makes sense to not perform any feature engineering. Many successful deep
    learning models have utilized engineered forms of features as input.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，数据转换主要分为两部分：**特征工程**和**数据缩放**。深度学习以其特征工程能力而闻名，特征工程替代了从原始数据中手动构建定制特征的需求。然而，这并不意味着始终没有进行特征工程的意义。许多成功的深度学习模型利用了经过工程处理的特征作为输入。
- en: Now that we know what data pre-processing entails, let’s discuss and explore
    different data pre-processing techniques for unstructured data, both theoretically
    and practically.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们知道了数据预处理的内容，让我们理论上和实践上讨论和探索不同的非结构化数据预处理技术。
- en: Text data pre-processing
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本数据预处理
- en: 'Text data can be in different languages and exist in different domains, ranging
    from description data to informational documents and natural human text comments.
    Some of the most common text data pre-processing methods that are used for deep
    learning are as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据可以是不同语言的，并存在于不同领域，从描述性数据到信息文档和自然人类文本评论。以下是一些最常用的文本数据预处理方法，它们被广泛应用于深度学习：
- en: '**Stemming**: A process that removes the suffix of words in an attempt to reduce
    words into their base form. This promotes the cross-usage of the same features
    for different forms of the same word.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词干提取**：一种去除单词后缀的过程，试图将单词还原为其基础形式。这有助于促进不同形式的同一单词的特征跨使用。'
- en: '**Lemmatization**: A process that reduces a word into its base form that produces
    real English words. Lemmatization has many of the same benefits as stemming but
    is considered better due to the linguistically valid word reduction outputs it
    produces.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词形还原**：一种将单词简化为其基础形式的过程，从而生成真实的英语单词。词形还原具有与词干提取相似的许多好处，但由于其生成的语言学有效的单词简化输出，被认为是更好的选择。'
- en: '**Text tokenization**, by **Byte Pair Encoding** (**BPE**): Tokenization is
    a process that splits text into different parts that will be encoded and used
    by the deep learning models. BPE is a sub-word-based text-splitting algorithm
    that allows common words to be outputted as a single token but rare words get
    split into multiple tokens. These split tokens can reuse representations from
    matching sub-words. This is to reduce the vocabulary that can exist at any one
    time, reduce the amount of out-of-vocabulary tokens, and allow token representations
    to be learned more efficiently.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本标记化**，通过**字节对编码**（**BPE**）：标记化是将文本分割成不同部分的过程，这些部分将被编码并供深度学习模型使用。BPE是一种基于子词的文本分割算法，它允许常见词汇作为单个标记输出，而稀有词汇则被分割成多个标记。这些分割后的标记可以复用来自匹配子词的表示。这样做是为了减少同时存在的词汇量，减少超出词汇表范围的标记，并使标记表示的学习更加高效。'
- en: 'One uncommon pre-processing method that will be useful to build more generalizable
    text deep learning models is text data augmentation. Text data augmentation can
    be done in a few ways:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一种不常见的预处理方法，但对于构建更具泛化能力的文本深度学习模型非常有用的是文本数据增强。文本数据增强可以通过几种方式进行：
- en: '**Replacing verbs with their synonyms**: This can be done by using the set
    of synonym dictionaries from the NLTK library’s WordNet English lexical database.
    The obtained augmented text will maintain the same meaning with verb synonym replacement.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用同义词替换动词**：这可以通过使用来自NLTK库的WordNet英语词汇数据库中的同义词词典集来完成。获得的增强文本将通过动词同义词替换保持相同的含义。'
- en: '**Back translation**: This involves translating text into another language
    and back to the original language using translation services such as Google or
    using open sourced translation models. The obtained back-translated text will
    be in a slightly different form.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回译**：这涉及将文本翻译成另一种语言，再翻译回原始语言，使用像Google这样的翻译服务或开源的翻译模型。获得的回译文本将会以略微不同的形式呈现。'
- en: Audio data pre-processing
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 音频数据预处理
- en: Audio data is essentially sequence-based data and, in some cases, multiple sequences
    exist. One of the most commonly used pre-processing methods for audio is raw audio
    data transformed into different forms of spectrograms using **Short-Time Fourier
    Transform** (**STFT**), which is a process that converts audio from the time domain
    into the frequency domain. A spectrogram audio conversion allows audio data to
    be broken down and represented in a range of frequencies instead of a single waveform
    representation that is a combination of the signals from all audio frequencies.
    These spectrograms are two-dimensional data and thus can be treated as an image
    and fed into convolutional neural networks. Data scaling methods such as log scaling
    and log-mel scaling are also commonly applied to these spectrograms to further
    emphasize frequency characteristics.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 音频数据本质上是基于序列的数据，在某些情况下，可能存在多个序列。音频最常用的预处理方法之一是将原始音频数据转换为不同形式的频谱图，使用**短时傅里叶变换**（**STFT**），这是一种将音频从时域转换为频域的过程。频谱图音频转换使得音频数据能够在多个频率范围内被分解和表示，而不是像单一波形那样表示所有音频频率的信号组合。这些频谱图是二维数据，因此可以作为图像处理，并输入卷积神经网络。数据缩放方法，如对数缩放和对数梅尔缩放，通常也会应用于这些频谱图，以进一步强调频率特征。
- en: Image data pre-processing
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像数据预处理
- en: 'Image data augmentation is a type of image-based feature engineering technique
    that is capable of increasing the comprehensiveness potential of the original
    data. A best practice for applying this technique is to structure the data pipeline
    to apply image augmentations randomly during the training process by batch instead
    of providing a fixed augmented set of data for the deep learning model. Choosing
    the type of image augmentation requires some understanding of the business requirements
    of the use case. Here are some examples where it doesn’t make sense to apply certain
    augmentations:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图像数据增强是一种基于图像的特征工程技术，能够增加原始数据的全面性潜力。应用此技术的最佳实践是构建数据流水线，在训练过程中按批次随机应用图像增强，而不是为深度学习模型提供固定的增强数据集。选择图像增强类型需要理解使用案例的业务需求。以下是一些不适合应用某些增强方法的示例：
- en: When the orientation of the image affects the validity of the target label,
    orientation modification types of augmentation such as rotation and image flipping
    wouldn’t be suitable
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当图像的方向影响目标标签的有效性时，方向修改类的数据增强方法，如旋转和图像翻转，不适用
- en: When the color of the image affects the validity of the target label, color
    modification types of augmentation such as grayscale, channel shuffle, hue saturation
    shift, and RGB shift aren’t suitable
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当图像的颜色影响目标标签的有效性时，颜色修改类的数据增强方法，如灰度、通道混洗、色调饱和度变化和RGB偏移并不适用
- en: After excluding obvious augmentations that won’t be suitable, a common but effective
    method to figure out the best set of augmentations list is iterative experiments
    and model comparisons.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在排除显然不适用的数据增强方法后，找出最佳增强方法列表的一个常见但有效的方法是通过迭代实验和模型比较。
- en: Developing deep learning models
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发深度学习模型
- en: Let’s start with a short recap of what deep learning is. Deep learning’s core
    foundational building block is a neural network. A neural network is an algorithm
    that was made to simulate the human brain. Its building blocks are called neurons,
    which mimic the billions of neurons the human brain contains. Neurons, in the
    context of neural networks, are objects that store simple information called weights
    and biases. Think of these as the memory of the algorithm.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简短回顾一下什么是深度学习。深度学习的核心基础构建块是神经网络。神经网络是一种旨在模拟人类大脑的算法。它的构建块叫做神经元，模拟了人类大脑中数十亿个神经元。在神经网络的上下文中，神经元是存储简单信息（称为权重和偏置）的对象。把它们看作是算法的记忆。
- en: 'Deep learning architectures are essentially neural network architectures that
    have three or more neural network layers. Neural network layers can be categorized
    into three high-level groups – the input layer, the hidden layer, and the output
    layer. The input layer is the simplest layer group and whose functionality is
    to pass the input data to subsequent layers. This layer group does not contain
    biases and can be considered passive neurons, but the group still contains weights
    in its connections to neurons from subsequent layers. The hidden layer comprises
    neurons that contain biases and weights in their connections to neurons from subsequent
    layers. Finally, the output layer comprises neurons that relate to the number
    of classes and problem types and contains bias. A best practice when counting
    neural network layers is to exclude the input layer when doing so. So, a neural
    network with one input layer, one hidden layer, and one output layer is considered
    to be a two-layer neural network. The following figure shows a basic neural network,
    called a **multilayer perceptron** (**MLP**), with a single input layer, a single
    hidden layer, and a single output layer:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习架构本质上是具有三层或更多神经网络层的神经网络架构。神经网络层可以分为三大类——输入层、隐藏层和输出层。输入层是最简单的层组，其功能是将输入数据传递给后续层。该层组不包含偏置，可以看作是被动神经元，但它的连接中仍包含到后续层神经元的权重。隐藏层由包含偏置的神经元组成，并且与后续层的神经元之间有权重连接。最后，输出层由与类别数量和问题类型相关的神经元组成，并包含偏置。计算神经网络层时的最佳实践是排除输入层。因此，具有一个输入层、一个隐藏层和一个输出层的神经网络被视为一个两层神经网络。下图展示了一个基本的神经网络，也叫做**多层感知器**（**MLP**），它有一个输入层、一个隐藏层和一个输出层：
- en: '![Figure 1.12 – A simple deep learning architecture, also called an MLP](img/B18187_01_012.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.12 – 一个简单的深度学习架构，也叫做MLP](img/B18187_01_012.jpg)'
- en: Figure 1.12 – A simple deep learning architecture, also called an MLP
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12 – 一个简单的深度学习架构，也叫做MLP
- en: Being a subset of the wider machine learning category, deep learning models
    are capable of learning patterns from the data through a loss function and an
    optimizer algorithm that optimizes the loss function. A loss function defines
    the error made by the model so that its memory (weights and biases) can be updated
    to perform better in the next iteration. An optimizer algorithm is an algorithm
    that decides the strategy to update the weights given the loss value.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 作为更广泛机器学习类别的一个子集，深度学习模型能够通过损失函数和优化算法从数据中学习模式，优化算法用于优化损失函数。损失函数定义了模型的误差，以便更新其记忆（权重和偏差），使其在下一次迭代中表现得更好。优化算法是一种决定根据损失值更新权重的策略的算法。
- en: With this short recap, let’s dive into a summary of the common deep learning
    model families.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个简短的回顾，让我们深入总结一下常见的深度学习模型家族。
- en: Deep learning model families
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习模型家族
- en: 'These layers can come in many forms as researchers have been able to invent
    new layer definitions to tackle new problem types and almost always comes with
    a non-linear activation function that allows the model to capture non-linear relationships
    between the data. Along with the variation of layers come many different deep
    learning architecture families that are meant for different problem types. A few
    of the most common and widely used deep learning models are as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层可以有多种形式，因为研究人员已经能够发明新的层定义来应对新的问题类型，并几乎总是配备有非线性激活函数，使得模型能够捕捉数据之间的非线性关系。随着层的变化，出现了许多不同的深度学习架构家族，专门针对不同的问题类型。一些最常见和广泛使用的深度学习模型如下：
- en: '**MLP** for tabular data types. This will be explored in [*Chapter 2*](B18187_02.xhtml#_idTextAnchor040),
    *Designing Deep* *Learning Architectures*.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多层感知机**（**MLP**）用于表格数据类型。将在[*第2章*](B18187_02.xhtml#_idTextAnchor040)中详细探讨，*设计深度学习架构*。'
- en: '**Convolutional neural network** for image data types. This will be explored
    in [*Chapter 3*](B18187_03.xhtml#_idTextAnchor051), *Understanding Convolutional*
    *Neural Networks*.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积神经网络**用于图像数据类型。将在[*第3章*](B18187_03.xhtml#_idTextAnchor051)中详细探讨，*理解卷积神经网络*。'
- en: '**Autoencoders** for anomaly detection, data compression, data denoising, and
    feature representation learning. This will be explored in [*Chapter 5*](B18187_05.xhtml#_idTextAnchor085),
    *Understanding Autoencoders*.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自编码器**用于异常检测、数据压缩、数据去噪和特征表示学习。将在[*第5章*](B18187_05.xhtml#_idTextAnchor085)中详细探讨，*理解自编码器*。'
- en: '**Gated recurrent unit** (**GRU**), **Long Short-Term Memory** (**LSTM**),
    and **Transformers** for sequence data types. These will be explored in [*Chapter
    4*](B18187_04.xhtml#_idTextAnchor068), *Understanding Recurrent Neural Networks*,
    and [*Chapter 6*](B18187_06.xhtml#_idTextAnchor092), *Understanding Neural Network*
    *Transformers*, respectively.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**门控循环单元**（**GRU**）、**长短期记忆网络**（**LSTM**）和**变压器**用于序列数据类型。这些将在[*第4章*](B18187_04.xhtml#_idTextAnchor068)中详细探讨，*理解循环神经网络*，以及[*第6章*](B18187_06.xhtml#_idTextAnchor092)中详细探讨，*理解神经网络变压器*。'
- en: These architectures will be the focus of *Chapters 2 to 6*, where we will discuss
    their methodology and go through some practical evaluation. Next, let’s discover
    the problem types we can tackle in deep learning.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这些架构将是*第2到第6章*的重点，我们将讨论它们的方法论，并进行一些实际评估。接下来，让我们探索深度学习中可以解决的问题类型。
- en: The model development strategy
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型开发策略
- en: Today, deep learning models are easy to invent and create due to the advent
    of deep learning frameworks such as PyTorch and TensorFlow, along with their high-level
    library wrappers. Which framework you should choose at this point is a matter
    of preference regarding their interfaces as both frameworks are matured with years
    of improvement work done. Only when there is a pressing need for a very custom
    function to tackle a unique problem type will you need to choose the framework
    that can execute what you need. Once you’ve chosen your deep learning framework,
    the deep model creation, training, and evaluation process is pretty much covered
    all around.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，由于深度学习框架如PyTorch和TensorFlow的出现，以及它们的高级库封装，深度学习模型的发明和创建变得容易。此时，你应该选择哪个框架，取决于它们的接口偏好，因为这两个框架都经过多年改进，已经成熟。只有在迫切需要一个非常自定义的功能来应对独特问题时，才需要选择能够执行你所需要的框架。一旦选择了深度学习框架，深度模型的创建、训练和评估过程基本上就涵盖了所有内容。
- en: 'However, model management functions do not come out of the box from these frameworks.
    Model management is an area of technology that allows teams, businesses, and deep
    learning practitioners to reliably, quickly, and effectively build models, evaluate
    models, deliver model insights, deploy models to production, and govern models.
    Model management can sometimes be referred to as **machine learning operations**
    (**MLOps**). You might still be wondering why you’d need such functionalities,
    especially if you’ve been building some deep learning models off Kaggle, a platform
    that hosts data and machine learning problems as competitions. So, here are some
    factors that drive the need to utilize these functionalities:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些框架并不直接提供模型管理功能。模型管理是一个技术领域，它使团队、企业和深度学习从业者能够可靠、快速和高效地构建模型、评估模型、交付模型洞察、将模型部署到生产环境并管理模型。模型管理有时也被称为**机器学习运维**（**MLOps**）。你可能仍然在想，为什么需要这样的功能，尤其是如果你曾在
    Kaggle 这样的平台上构建过深度学习模型，Kaggle 提供数据和机器学习问题作为竞赛。那么，以下是一些推动你需要利用这些功能的因素：
- en: 'It is cumbersome to compare models manually:'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动比较模型很繁琐：
- en: Manually typing performance data in an Excel sheet to keep track of model performance
    is slow and unreliable
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动在 Excel 表格中输入性能数据以追踪模型性能既慢又不可靠
- en: 'Model artifacts are hard to keep track of:'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型产物很难追踪：
- en: A model has many artifacts, such as its trained weights, performance graphs,
    feature importance, and prediction explanations
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个模型有许多产物，比如它的训练权重、性能图表、特征重要性和预测解释
- en: It is also cumbersome to compare model artifacts
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较模型产物也很繁琐
- en: 'Model versioning is needed to make sure model-building experiments are not
    repeated:'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要进行模型版本管理，以确保模型构建实验不会被重复：
- en: Overriding the top-performing model with the most reliable model insights is
    the last thing you want to experience
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用最可靠的模型洞察覆盖表现最好的模型，是你最不希望经历的事情
- en: Versioning should depend on the data partitioning method, model settings, and
    software library versions
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本控制应该依赖于数据分区方法、模型设置和软件库版本
- en: It is not straightforward to deploy and govern models
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署和管理模型并不简单
- en: Depending on the size of the team involved in the project and how often components
    need to be reused, different software and libraries would fit the bill. These
    software and libraries are split into paid and free (usually open sourced) categories.
    **Metaflow**, an open sourced software, is suitable for bigger data science teams
    where there are many chances of components needing to be reused across other projects
    and **MLFlow** (open sourced software) would be more suitable for small or single-person
    teams. Other notable model management tools are **Comet** (paid), **Weights &
    Biases** (paid), **Neptune** (paid), and **Algorithmia** (paid).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 根据参与项目的团队规模以及组件需要重复使用的频率，适合使用不同的软件和库。这些软件和库分为付费和免费的（通常是开源的）类别。**Metaflow**，一个开源软件，适用于数据科学团队规模较大的情况，其中组件需要在其他项目中频繁复用，而**MLFlow**（开源软件）更适合小型团队或单人团队。其他值得注意的模型管理工具包括**Comet**（付费）、**Weights
    & Biases**（付费）、**Neptune**（付费）和**Algorithmia**（付费）。
- en: With that, we have provided a brief overview of deep learning model development
    methodology and strategy; we will dive deeper into model development topics in
    the next few chapters. But before that, let’s continue with an overview of the
    topic of delivering model insights.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们已经简要概述了深度学习模型开发的方法和策略；接下来我们将在章节中深入探讨模型开发的相关话题。但在此之前，让我们继续概览模型洞察的交付主题。
- en: Delivering model insights
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交付模型洞察
- en: Model metric performance, when used exclusively for model comparisons and the
    model choosing process, is often not the most effective way to reliably obtain
    the true best model. When people care about the decisions that can potentially
    be made by the machine learning model, they typically require more information
    and insights to eventually put their trust in the ability of the model to make
    decisions. Ultimately, when models are not trusted, they don’t get deployed. However,
    trust doesn’t just depend on insights of the model. Building trust in a model
    involves ensuring accurate, reliable, and unbiased predictions that align with
    domain expertise and business objectives, while providing stakeholders with insights
    into the model’s performance metrics, decision-making logic, and rationale behind
    its predictions. Addressing potential biases and demonstrating fairness are crucial
    for gaining confidence in the model’s dependability. This ongoing trust-building
    process extends beyond initial deployment, as the model must consistently exhibit
    sound decision-making, justify predictions, and maintain unbiased performance.
    By fostering trust, the model becomes a valuable and reliable tool for real-world
    applications, leading to increased adoption and utilization across various domains
    and industries.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 模型性能指标，在仅用于模型比较和选择的情况下，通常不是可靠地获取最佳模型的最有效方法。当人们关注机器学习模型可能做出的决策时，他们通常需要更多的信息和洞察力，以最终将信任寄托于模型的决策能力。最终，当模型不被信任时，它们就不会被部署。然而，信任不仅仅依赖于模型的洞察力。建立对模型的信任涉及确保准确、可靠和无偏的预测，这些预测与领域专业知识和商业目标一致，同时向利益相关者提供有关模型性能指标、决策逻辑及其预测背后理由的洞察。解决潜在偏见并展示公平性是赢得对模型可靠性信心的关键。这个持续的信任建立过程超越了初始部署，因为模型必须始终展现合理的决策过程、证明预测的合理性，并保持无偏的表现。通过建立信任，模型成为一个有价值且可靠的工具，能在现实应用中发挥作用，推动各个领域和行业的广泛采用和利用。
- en: Deliver model insights that matter to the business. Other than delivering model
    insights with the obvious goal of ensuring model trust and eliminating trust issues,
    actual performance metrics are equally important. Make sure you translate model
    metrics into layman’s business metrics whenever possible to effectively communicate
    the potential positive impact that the model can bring to the business. Success
    metrics, which are defined earlier in the planning phase, should be reported with
    actual values at this stage.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 提供对业务有价值的模型洞察。除了为确保模型信任并消除信任问题而提供模型洞察外，实际的性能指标同样重要。尽可能将模型指标转化为通俗易懂的商业指标，以有效地传达模型可能对业务带来的积极影响。成功指标应在规划阶段定义，并在此阶段报告实际值。
- en: The process of inducing trust in a model doesn’t stop after the model gets deployed.
    Similar to how humans are required to explain their decisions in life, machine
    learning models (if expected to replace humans to automate the decisioning process)
    are also required to do so. This process is called prediction explanations. In
    some cases, model decisions are expected to be used as a reference where there
    is a human in the loop that acts as a domain expert to verify decisions before
    the decisions are carried out. Prediction explanations are almost always a necessity
    in these conditions where the users of the model are interested in why the model
    made its predictions instead of using the predictions directly.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型部署后，建立对模型的信任并不会停止。类似于人类在生活中需要解释自己的决策，机器学习模型（如果预期替代人类来自动化决策过程）也需要如此。这一过程被称为预测解释。在某些情况下，模型的决策被期望作为参考，其中会有一个人类专家参与验证决策，确保决策正确后再执行。预测解释在这种情况下几乎总是必需的，因为模型的使用者希望了解模型为何做出预测，而不是直接使用预测结果。
- en: 'Model insights also allow you to improve a model’s performance. Remember that
    the machine learning life cycle is naturally an iterative process. Some concrete
    examples of where this condition could happen are as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 模型洞察力也可以帮助你提升模型的表现。记住，机器学习生命周期本质上是一个迭代过程。以下是一些可能出现这种情况的具体示例：
- en: You realize that the model is biased against a particular group and either go
    back to the data acquisition stage to acquire more data from the less represented
    group or change to a modeling technique that is robust to bias
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你意识到模型对某一特定群体存在偏见，并决定回到数据获取阶段，增加来自该群体的数据，或者更换为对偏见具有鲁棒性的建模技术。
- en: You realize that the model performs badly in one class and go back to the model
    development stage to use a different deep learning model loss function that can
    focus on the badly performing class
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你意识到模型在某一类上的表现不好，然后回到模型开发阶段，使用一个不同的深度学习模型损失函数，专注于表现不佳的那一类。
- en: Deep learning models are known to be a black box model. However, in reality,
    today, there have been many published research papers on deep learning explanation
    methods that have allowed deep learning to break the boundaries of a black box.
    We will dive into the different ways we can interpret and provide insights for
    deep learning models in *Part 2* of this book.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型被认为是一个黑箱模型。然而，实际上，今天已有许多关于深度学习解释方法的研究论文，这些方法使得深度学习突破了黑箱的局限性。我们将在本书的*第二部分*深入探讨我们可以如何解释并为深度学习模型提供洞见。
- en: Now that we have more context of the processes involved in the deep learning
    life cycle, in the next section, we will discuss risks that can exist throughout
    the life cycle of the project that you need to worry about.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对深度学习生命周期中的相关过程有了更多的了解，在接下来的部分，我们将讨论你需要关注的、可能贯穿整个项目生命周期的风险。
- en: Managing risks
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 风险管理
- en: Deep learning systems are exposed to a multitude of risks starting early, from
    inception to system adoption. Usually, most people who are assigned to work on
    a deep learning project are responsible only for a specific stage of the machine
    learning life cycle, such as model development or data preparation. This practice
    can be detrimental when the work of one stage in the life cycle generates problems
    at a later stage, which often happens in cases where the team members involved
    have little sense of the bigger picture in play. Risks involved in a deep learning
    system generally involve interactions between stages in the machine learning life
    cycle and follow the concept of *Garbage in, garbage out*. Making sure everyone
    working on building the system has a sense of accountability for what eventually
    gets outputted from the entirety of the system instead of just the individual
    stages is one of the foundational keys to managing risks in a machine learning
    system.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习系统从一开始就暴露于多种风险中，从构思到系统采纳。通常，分配到深度学习项目中的大多数人仅负责机器学习生命周期的某个特定阶段，如模型开发或数据准备。当生命周期的一个阶段的工作在后期产生问题时，尤其是当相关团队成员缺乏对整个大局的理解时，这种做法可能会带来不利影响。深度学习系统中涉及的风险通常与机器学习生命周期各个阶段之间的交互有关，并遵循*垃圾进，垃圾出*的概念。确保所有参与构建系统的人都对系统最终输出的结果负责，而不仅仅是各个独立阶段的结果，这是管理机器学习系统风险的基础要素之一。
- en: But what are the risks? Let’s start with something that can be handled way before
    anything tangible is made – something that happens when a use case is evaluated
    for worthiness.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，风险是什么呢？让我们从一个可以在任何实际成果产生之前就能处理的问题开始——也就是在评估用例的价值时发生的事情。
- en: Ethical and regulatory risks
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理和监管风险
- en: Deep learning can be applied in practically any industry but some of the hardest
    industries to get deep learning adopted in are the highly regulated industries,
    such as the medical and financial industries. The regulations that are imposed
    in these industries ultimately determine what a deep learning system can or cannot
    do in the industry. Regulations are mostly introduced by governments and most
    commonly involve ethical and legal considerations. In these highly regulated industries,
    it is common to experience audits being conducted monthly, weekly, or even daily
    to make sure the companies are compliant with the regulations imposed. One of
    the main reasons certain industries are regulated more aggressively is that the
    repercussions of any actions in these industries bear a heavy cost to the well-being
    of the people or the country. Deep learning systems need to be built in a way
    that they will be compliant with these regulations to avoid the risk of facing
    decommission, the risk of not getting adopted at all, and, worst of all, the risk
    of getting a huge fine by regulatory officials.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习几乎可以应用于任何行业，但一些最难以在其中推广深度学习的行业是高度监管的行业，如医疗和金融行业。这些行业的法规最终决定了深度学习系统在行业中能做什么或不能做什么。这些法规大多由政府引入，通常涉及伦理和法律的考量。在这些高度监管的行业中，常常会出现每月、每周甚至每天进行的审计，以确保公司符合所施加的法规。某些行业之所以受到更加严格的监管，主要是因为这些行业的任何行为所带来的后果会对人民或国家的福祉造成巨大的成本。深度学习系统需要以符合这些法规的方式进行构建，以避免面临停用的风险，无法被采纳的风险，甚至最严重的，面临监管官员巨额罚款的风险。
- en: At times, deep learning models can perform way better than their human counterparts
    but, on the other side, no deep learning model is perfect. Humans make mistakes,
    everybody knows that for a fact, but another reality we need to realize is that
    machines undoubtedly also make mistakes. The highest risk is when humans trust
    the machines so much that they give 100% trust in them to make decisions! So,
    how can we account for these mistakes and who will be responsible for them?
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，深度学习模型的表现可以远超其人类同行，但另一方面，没有任何深度学习模型是完美的。人类会犯错，大家都知道这一点，但我们需要意识到的另一个现实是，机器无疑也会犯错。最大的风险是，当人类过度信任机器，甚至完全依赖机器做决策时！那么，我们该如何考虑这些错误，并且谁将为这些错误负责呢？
- en: Let’s see what we humans do in situations without machines. When the stakes
    are high, important decisions always go through a hierarchy of approvals before
    a decision can be final. These hierarchies of approvals signal the need to make
    important decisions accountable and reliable. The more approvals we get for a
    decision, the more we can say that we are confident in making that important decision.
    Some examples of important decisions that commonly require a hierarchy of approvals
    include the decision to hire an employee in a company, the decision on insurance
    premiums to charge, or the decision of whether to invest money into a certain
    business. With that context in mind, deep learning systems need to have similar
    workflows for obtaining approvals when a deep learning model makes a certain predictive
    decision in high-stakes use cases. These workflows can include any form of insight,
    and an explanation of the predictions can be used to make it easier for domain
    experts to judge the validity of the decisions. Adding a human touch makes the
    deep learning system many times more ethical and trustable enough to be part of
    the high-stake decision workflow.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在人类没有机器的情况下，我们会怎么做。当风险较高时，重要的决策总是需要经过一系列的审批层级，才能最终做出决定。这些审批层级表明了重要决策需要有责任和可靠性。在决策过程中获得的审批越多，我们就越能说自己在做出这一重要决策时充满信心。一些常见的需要审批层级的重要决策包括：在公司中决定雇佣一名员工、决定向客户收取的保险费用，或者决定是否将资金投入到某个企业。考虑到这一点，深度学习系统在面对高风险应用时，需要具备类似的审批工作流程，以便在深度学习模型做出某个预测决策时获得批准。这些工作流程可以包括任何形式的洞察力，解释预测结果可以帮助领域专家更容易地判断决策的有效性。增加人类的参与使得深度学习系统变得更加符合伦理，并且足够值得信赖，能够成为高风险决策流程的一部分。
- en: Let’s take a medical industry use case – for instance, a use case to predict
    lung disease through X-ray scans. From an ethical standpoint, it is not right
    that a deep learning model can have the complete power to strip a person’s hope
    of life by predicting extremely harmful diseases such as end-stage lung cancer.
    If the predicted extreme disease is misdiagnosed, patients may have grieved unnecessarily
    or spent an unnecessary amount of money on expensive tests to verify the claims.
    Having an approval workflow in the deep learning system allows doctors to use
    these results as an assistive method would solve the ethical concerns of using
    an automated decisioning system.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个医疗行业的使用案例为例——例如，通过X光扫描预测肺病的使用案例。从伦理角度来看，深度学习模型完全有能力通过预测极其危害健康的疾病，如晚期肺癌，剥夺一个人的生存希望，这显然是错误的。如果预测到的极端疾病被误诊，患者可能会不必要地感到悲痛，或花费不必要的金钱进行昂贵的检查来验证结果。在深度学习系统中设立审批工作流程，允许医生将这些结果作为辅助方法使用，将解决使用自动决策系统时可能产生的伦理问题。
- en: Business context mismatch
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 业务背景不匹配
- en: Reiterating the previous point in the *Defining success* section, aligning the
    desired deep learning input data and target label choice to the business context
    and how the target predictions are consumed makes deep learning systems adoptable.
    The risk involved here is when the business value is either not properly defined
    or not matched properly by the deep learning system. Even when the target is appropriate
    for the business context, how these predictions are consumed holds the key to
    deriving value. Failing to match the business context in any way simply risks
    the rejection of the systems.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 重申在*定义成功*部分提到的观点，将所需的深度学习输入数据和目标标签选择与业务背景对齐，并了解这些目标预测如何被使用，是深度学习系统能否被采纳的关键所在。这里的风险在于，当业务价值未被恰当地定义，或者深度学习系统未能正确匹配时，可能会导致系统的失败。即使目标适合业务背景，如何消费这些预测才是获取价值的关键。如果在任何方面未能与业务背景匹配，系统可能会被拒绝。
- en: Understanding the business context involves understanding who the targeted user
    groups are and who they are not. A key step in this process is documenting and
    building user group personas. User personas hold information about their workflows,
    history, statistics, and concerns that should provide the context needed to build
    a system that aligns with the specific needs of potential users. Don’t be afraid
    to conduct market research and make proper validation of the targeted user needs
    in the process of building these user personas. After all, it takes a lot of effort
    to build a deep learning system and it would not be great to waste time building
    a system that nobody wants to use.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 理解业务背景涉及了解目标用户群体是谁，以及他们不包括谁。这个过程中的一个关键步骤是记录和构建用户群体画像。用户画像包含了关于他们的工作流程、历史、统计数据和关注点的信息，这些信息应当提供构建一个符合潜在用户具体需求的系统所需的背景。不要害怕进行市场调研，并在构建用户画像的过程中对目标用户需求进行适当验证。毕竟，构建一个深度学习系统需要付出大量努力，如果浪费时间构建一个没人愿意使用的系统，那就得不偿失。
- en: Data collection and annotation risks
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据收集和标注的风险
- en: Your machine learning model will only be as good as the quality of your data.
    Deep learning requires a substantially larger quantity of data compared to machine
    learning. Additionally, very often for new deep learning use cases, data is not
    available off the shelf and must be manually collected and meticulously annotated.
    Making sure that the data is collected and annotated in a way that ensures quality
    is upheld is a very hard job.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 你的机器学习模型的效果将取决于数据的质量。与机器学习相比，深度学习需要大量更多的数据。此外，通常对于新的深度学习使用案例，数据不是现成的，需要手动收集并精心标注。确保数据收集和标注的方式能够保证质量，是一项非常艰巨的任务。
- en: 'The strategies and methods that are used to collect and annotate data for deep
    learning vary across use cases. Sometimes, the data collection and annotation
    process can happen at the same time. This can happen either when there are natural
    labels or when a label is prespecified before data is collected:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 用于收集和标注数据以供深度学习使用的策略和方法因使用案例不同而有所差异。有时，数据收集和标注过程可以同时进行。这种情况可以发生在自然标签存在时，或者在数据收集之前已经预先指定标签时：
- en: '**Natural labels** are labels that usually come naturally after some time passes.
    For example, this can be the gender of a baby through an ultrasound image; another
    use case is the price of a house as a label with the range of images of the property
    as the input data.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然标签** 是在一段时间后通常会自然产生的标签。例如，这可以是通过超声波图像确定婴儿的性别；另一个用例是房屋价格作为标签，其输入数据为该房产的一系列图像。'
- en: '**Prespecified labels** are labels where the input data is collected with predetermined
    labels. For example, this can be the gender label of some speech data that was
    collected in a controlled environment just for building a machine learning model
    or the age of a person in a survey before taking a facial photo shot to build
    a machine learning model.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预先指定的标签** 是在收集输入数据时预先确定的标签。例如，这可以是在受控环境中收集的一些语音数据的性别标签，仅用于构建机器学习模型，或者在拍摄面部照片以构建机器学习模型之前的某项调查中的个人年龄。'
- en: These two types of labels are relatively safe from risks since the labels are
    likely to be accurate.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种类型的标签相对来说风险较低，因为标签很可能是准确的。
- en: A final method of data collection and data annotation that poses a substantial
    amount of risk is when the labels are post-annotated after the data is collected.
    **Post annotation** requires some form of domain knowledge about the characteristics
    of the desired label and does not always result in a 100% truth value due to human
    evaluation errors. Unconscious labeling errors happen when the labeler simply
    makes an error by accident. Conscious labeling errors, on the other hand, happen
    when the labeler actively decides to make a wrong label decision with intent.
    This intent can be from a strong or loosely rooted belief tied to a certain pattern
    in the data, or just a plain error made purposely for some reason. The risk being
    presented here is the potential of the labels being mistakenly annotated. Fortunately,
    there are a few strategies that can be executed to eliminate these risks; these
    will be introduced in the following paragraphs.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 一种最终的数据收集和数据注释方法，具有相当大的风险，是在收集数据后进行标注。**后注释** 需要对所需标签的特征具有某种形式的领域知识，并且由于人为评估错误，不总是导致100%的真实值。无意识的标注错误发生在标注者仅仅因为偶然而导致错误。另一方面，有意识的标注错误发生在标注者有意决定以错误的标签决策。这种意图可能源于数据中某种模式所根深蒂固的或宽泛的信念，或者仅仅是为某种原因故意制造的明显错误。这里所呈现的风险是标签可能被错误地注释。幸运的是，有几种策略可以执行以消除这些风险；这些将在以下段落中介绍。
- en: Unconscious labeling errors are hard to circumvent as humans are susceptible
    to a certain amount of error. However, different people generally have varying
    levels of focus and selective choice of the labelers could be a viable option
    for some. If the labelers are hired with financial remuneration just to annotate
    the labels for your data, another viable option is to periodically provide data
    that is already labeled in secret in between providing data that needs labeling
    for the labelers to label the data. This strategy allows you to evaluate the performance
    of individual labelers and provide compensation according to the accuracy of their
    annotation work (yes, we can use metrics that are used to evaluate machine learning
    models too). As a positive side effect, labelers also would be incentivized to
    perform better in their annotation work.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人类易于出现一定程度的错误，无意识的标注错误很难避免。然而，不同的人通常有不同程度的注意力和选择性标签者的选择可能对某些人来说是一种可行的选择。如果标签者仅因金融补偿而被聘请来标注您的数据标签，另一个可行的选择是在提供需要标记数据的数据之间周期性地提供已经标记的秘密数据以供标签者标记。这种策略允许您评估单个标签者的表现，并根据其注释工作的准确性提供补偿（是的，我们也可以使用用于评估机器学习模型的指标）。作为积极的副作用，标签者还将被激励在他们的注释工作中表现更好。
- en: Conscious labeling errors are the most dangerous risk here due to their potential
    to affect the quality of the entire dataset. When a wrong consensus on the pattern
    of data associated with any target label is used for the entire labeling process,
    the legitimacy of the labels won’t be discovered until the later stages of the
    machine learning life cycle. Machine learning models are only equipped with techniques
    to learn the patterns required to map the input data to the provided target labels
    and will likely be able to perform very well, even in conditions where the labels
    are incorrect. So long as there is a pattern that the labeler enforced during
    the labeling process, no matter right or wrong, machine learning models will do
    their best to learn the patterns needed to perform the exact input-to-output mapping.
    So, how do we mitigate this risk? Making sure ideologies are defined properly
    with the help of domain experts plays an important part in the mitigation process,
    but this strategy by itself holds varying levels of effectiveness, depending on
    the number of domain experts involved, the expertise level of the domain experts
    themselves, the number of labelers, and the varying degrees of anomalies or quality
    of the collected data.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 有意识的标注错误是这里最危险的风险，因为它们可能影响整个数据集的质量。当对与任何目标标签相关的数据模式达成错误共识，并应用于整个标注过程中时，标签的合法性直到机器学习生命周期的后期才会被发现。机器学习模型仅具备学习所需模式的技术，以便将输入数据映射到提供的目标标签，即使标签是错误的，它们也很可能表现得很好。只要标注者在标注过程中强加了某种模式，不论对错，机器学习模型都会尽力学习所需的模式，以执行准确的输入到输出映射。那么，我们如何减轻这种风险呢？确保在领域专家的帮助下正确定义意识形态，在减轻过程中起着重要作用，但这一策略本身的有效性因涉及的领域专家数量、专家的专业水平、标注者数量以及收集数据的异常程度或质量而有所不同。
- en: Even domain experts can be wrong at times. Let’s take doctors as an example
    – how many times have you heard about a doctor giving wrong prescriptions and
    wrong medical assessments? How about the times when you misheard someone’s speech
    and had to guess what the other person just said? In the last example, the domain
    expert is you, and the domain is language speech comprehension. Additionally,
    when the number of labelers exceeds one person and forms a labeling team, one
    of the most prominent risks that can happen is a mismatch between different preferred
    domain ideologies to label the data. Sometimes, this happens because of the different
    variations of a certain label that can exist in a digital format or the existence
    of confusing patterns that can deter the analytical capabilities of the labeler.
    Sometimes, this can also happen due to the inherent bias the labeler or the domain
    expert has toward a certain label or input data. Subsequently, bias in the data
    creates bias in the model and creates ethical issues that demote trust in the
    decisions of a machine learning model. When there is a lack of trust in a machine
    learning model, the project will fail to be adopted and will lose its business
    value. The topic of bias, fairness, and trust will be discussed more extensively
    in [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196), *Exploring Bias and Fairness*,
    which will elaborate on its origins and ways to mitigate it.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是领域专家有时也会犯错。以医生为例——你有多少次听说过医生开错药方或做出错误的医学评估？你有多少次误听别人说话后不得不猜测对方说了什么？在最后一个例子中，领域专家就是你，而领域是语言听力理解。此外，当标注者超过一个人并形成标注团队时，一个最突出的风险就是不同偏好的领域意识形态之间的标注数据不匹配。有时，这种情况发生是因为某些标签在数字格式中可能存在不同的变体，或者存在让标注者分析能力受阻的混乱模式。有时，这也可能是由于标注者或领域专家对某个标签或输入数据的固有偏见。随后，数据中的偏见会导致模型中的偏见，并引发伦理问题，从而削弱对机器学习模型决策的信任。当对机器学习模型缺乏信任时，项目将无法被采纳，并失去其商业价值。关于偏见、公平和信任的主题将在[*第13章*](B18187_13.xhtml#_idTextAnchor196)中更广泛地讨论，该章将详细阐述偏见的来源及其缓解方式。
- en: Data with wrong labels that still have correct labels are called noisy labels.
    Fortunately, there are methods in deep learning that can help you work your way
    around noisy labels, such as weakly supervised learning. However, remember that
    it’s always a good idea to fix the issue at the source, which is when the data
    is collected and annotated, rather than after. Let’s dive into another strategy
    we can use to make the labeling process less risky. The data labeling process
    for deep learning projects usually involves using a software tool that allows
    the specific desired labels to be annotated. Software tools make annotation work
    faster and easier, and using a labeling collaboration software tool can make annotation
    work more error-proof. A good collaboration-based labeling tool will allow labelers
    to align their findings of the data with each other in some way, which will promote
    a common ideology when labeling. For instance, automatically alerting the entire
    labeler team when an easily misinterpreted pattern is identified can help prevent
    more data from being mislabeled and make sure all the previously related data
    gets rereviewed.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 标签错误但仍有正确标签的数据称为噪声标签。幸运的是，深度学习中有一些方法可以帮助你绕过噪声标签，例如弱监督学习。然而，请记住，最好的做法是从源头上解决问题，即在数据收集和标注时解决，而不是事后再修正。让我们深入探讨另一种可以使标注过程更安全的策略。深度学习项目的数据标注过程通常涉及使用软件工具来标注特定所需的标签。软件工具使得标注工作更快、更轻松，使用标注协作软件工具可以让标注工作更加防错。一个好的协作标注工具会让标注人员以某种方式对齐他们的数据发现，从而促进标注时的一致性。例如，当识别到容易误解的模式时，自动通知整个标注团队，可以帮助防止更多数据被错误标注，并确保之前相关的数据得到重新审查。
- en: As a final point here, not as a risk, unlabeled data presents huge untapped
    potential for machine learning. Although it lacks specific labels to achieve a
    particular goal, there are inherent relationships between the data that can be
    learned. In [*Chapter 9*](B18187_09.xhtml#_idTextAnchor149), *Exploring Unsupervised
    Deep Learning*, we will explore how to use unsupervised learning to leverage this
    data as a foundation for subsequent supervised learning, which forms the workflow
    more widely known as semi-supervised learning.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 最后需要提到的是，未标记数据并不是一个风险，而是机器学习中巨大的潜力。尽管它缺乏具体的标签来实现特定目标，但数据之间存在固有的关系是可以学习的。在[*第9章*](B18187_09.xhtml#_idTextAnchor149)《探索无监督深度学习》中，我们将探讨如何利用无监督学习将这些数据作为后续监督学习的基础，这就是更广为人知的半监督学习工作流程。
- en: Next, we will go through the next risk related to security in the consumption
    of data for machine learning models.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论与机器学习模型使用数据时相关的另一个安全风险。
- en: Data security risk
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据安全风险
- en: Security, in the context of machine learning, relates to preventing the unauthorized
    usage of data, protecting the privacy of data, and preventing events or attacks
    that are unwanted relating to the usage of the data. The risk here is when data
    security is compromised and can result in failure to comply with regulatory standards,
    the downfall of the model’s performance, or the corruption and destruction of
    business processes. In this section, we will go through four types of data security
    risk, namely sensitive data handling, data licensing, software licensing, and
    adversarial attacks.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的背景下，安全性与防止数据被未经授权使用、保护数据隐私以及防止与数据使用相关的非预期事件或攻击有关。这里的风险是当数据安全受到破坏时，可能会导致未能遵守监管标准、模型性能的下降，或商业流程的破坏与损毁。在本节中，我们将介绍四种数据安全风险，分别是敏感数据处理、数据许可、软件许可和对抗性攻击。
- en: Sensitive data handling
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 敏感数据处理
- en: Some data is more sensitive than others and can be linked to regulatory risks.
    Sensitive data gives rise to data privacy regulations that govern the usage of
    personal data in different jurisdictions. Specifics of regulations vary but they
    generally revolve around lawfully protecting the rights of the usage of personal
    data and requiring consent for any actions taken on these types of data along
    with terms required when using such data. Examples of such regulations are the
    **General Data Protection Regulation** (**GDPR**), which covers the European Union,
    the **Personal Data Protection Act** (**PDPA**), which covers Singapore, the **California
    Consumer Privacy Act** (**CCPA**), which covers only the state of California in
    the United States, and the **Consumer Data Protection Act** (**CDPA**), which
    covers the state of Virginia in the United States. This means that you can’t just
    collect data that is categorized as personal, annotate it, build a model, and
    deploy it without adhering to these regulations as doing so would be considered
    a crime in some of these jurisdictions. Other than requesting consent, one of
    the common methods that’s used to mitigate this risk is to anonymize data so that
    the data can’t be identified by any single person. However, anonymization has
    to be done reliably so that the key general information is maintained while reliably
    removing any possibility of reconstructing the identity of a person. Making sure
    sensitive and personal data is handled properly goes a long way in building trust
    in the decisions of a machine learning model. Always practice extreme caution
    in handling sensitive and personal data to ensure the longevity of your machine
    learning project.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据比其他数据更敏感，并且可能与监管风险相关。敏感数据引发了数据隐私法规，这些法规管理在不同法域中个人数据的使用。法规的具体内容有所不同，但通常围绕合法保护个人数据使用权利，并要求对这些数据采取的任何行动都必须获得同意，同时在使用此类数据时需要遵循相关条款。这些法规的例子包括涵盖欧盟的**通用数据保护条例**（**GDPR**）、涵盖新加坡的**个人数据保护法**（**PDPA**）、仅涵盖美国加利福尼亚州的**加利福尼亚消费者隐私法**（**CCPA**）以及涵盖美国弗吉尼亚州的**消费者数据保护法**（**CDPA**）。这意味着，你不能仅仅收集被归类为个人的数据显示、标注数据、建立模型并部署，而不遵守这些法规，因为在某些法域中，这样做会被视为犯罪。除了请求同意外，常用的一种方法是通过匿名化数据来减轻这种风险，使得数据无法被任何单个个体识别。然而，匿名化必须可靠地进行，以确保关键的通用信息得以保留，同时可靠地消除任何重建个人身份的可能性。确保敏感和个人数据得到妥善处理，在建立机器学习模型的决策信任方面有着重要作用。在处理敏感和个人数据时，务必保持极高的谨慎，以确保你的机器学习项目的长期可持续性。
- en: Data and software licensing
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据与软件许可
- en: Particularly for deep learning projects, a lot of data is required to build
    a good quality model. The availability of publicly available datasets helps shorten
    the time and complexity of a project by partially removing the cost and time needed
    to collect and label data. However, most datasets, like software, have licenses
    associated with them. These licenses govern how the data associated with the license
    can be used. The most important criterion of data license about machine learning
    models for business problems is whether the data allows for commercial usage or
    not. As most business use cases are considered to be commercial use cases due
    to profits derived from them, datasets with a license that prevents commercial
    usage cannot be used. Examples of data licenses with terms that prevent commercial
    usage are all derivatives of the **Creative Commons Attribution-NonCommercial**
    (**CC-BY-NC**) license. Similarly, open sourced software also poses a risk to
    deep learning projects. Always make sure you triple-check the licenses before
    using any publicly available data or software for your project. Using data or
    code that has terms that prevent commercial usage in your commercial business
    project puts your project at risk of being fined or sued for license infringement.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其对于深度学习项目来说，构建一个高质量的模型需要大量数据。公开可用的数据集的存在有助于通过部分消除收集和标注数据所需的成本和时间，从而缩短项目的时间和复杂性。然而，大多数数据集与软件一样，都有相关的许可协议。这些许可协议规定了与其关联的数据可以如何使用。关于商业问题的机器学习模型，数据许可的最重要标准是数据是否允许商业使用。由于大多数商业用例因其产生的利润而被视为商业用例，因此具有禁止商业使用条款的数据集不能使用。防止商业使用的许可条款的一个例子是所有衍生自**创作共用署名-非商业性使用**（**CC-BY-NC**）许可的条款。同样，开源软件也会对深度学习项目构成风险。在使用任何公开可用的数据或软件之前，一定要确保三次检查其许可协议。将具有防止商业使用条款的数据或代码用于商业项目，会使项目面临因许可侵权而被罚款或起诉的风险。
- en: Adversarial attacks
  id: totrans-308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对抗攻击
- en: 'When an application of machine learning is widely known, it exposes itself
    to targeted attacks meant to maliciously derail and manipulate the decisions of
    a model. This brings us to a type of attack that can affect a deployed deep learning
    model, called an **adversarial attack**. Adversarial attacks are a type of attack
    that involves manipulating the data input in a certain way to affect the predictions
    from the machine learning model. The most common adversarial attacks are caused
    by adversarial data examples that are modified from the actual input data in a
    way that they appear to maintain their legitimacy as input data but are capable
    of skewing the model’s prediction. The level of risk of such an attack varies
    across different use cases and depends on how much a user can interact with the
    system that eventually passes the input data to the machine learning model. One
    of the most widely known adversarial examples for a deep learning model is an
    optimized image that looks like random color noise and, when used to perturb pixel
    values of another image by overlaying its own pixel values, generates an adversarial
    example. The original image that’s obtained after the perturbation looks as though
    it’s untouched visually to a human but is capable of producing erroneous misclassification.
    The following figure shows an example of this perturbation as part of a tutorial
    from [*Chapter 14*](B18187_14.xhtml#_idTextAnchor206), *Analyzing Adversarial
    Performance*. This figure depicts the result of a neural network called ResNet50,
    trained to classify facial identity, that, when given a facial image, correctly
    predicted the right identity. However, when added together with another noise
    image array that was strategically and automatically generated just with access
    to the predicted class probabilities, the model mistakenly predicts the identity
    of the facial image, even when the combined image looks visually the same as the
    original image:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 当机器学习应用广泛时，它会暴露出自身容易受到针对性攻击的风险，这些攻击旨在恶意地破坏和操控模型的决策。这引出了一个可能影响已部署深度学习模型的攻击类型，称为**对抗性攻击**。对抗性攻击是一种攻击方式，通过特定的方式操控数据输入，从而影响机器学习模型的预测。最常见的对抗性攻击是通过对实际输入数据进行修改，生成对抗性数据示例。这些修改后的数据看起来仍然符合输入数据的合法性，但却能扭曲模型的预测。此类攻击的风险程度在不同的使用场景中有所不同，具体取决于用户与系统的交互程度，最终将输入数据传递给机器学习模型。对于深度学习模型来说，最广为人知的对抗性示例之一是经过优化的图像，它看起来像是随机的色彩噪声。当这种图像用于扰动另一张图像的像素值，通过覆盖其像素值时，就会生成一个对抗性示例。扰动后的原始图像从视觉上看起来与人眼几乎无差，但却能导致错误的误分类。下图展示了这种扰动的示例，摘自[*第14章*](B18187_14.xhtml#_idTextAnchor206)，*分析对抗性表现*。该图展示了一个神经网络ResNet50的结果，ResNet50经过训练可以分类人脸身份，当给定人脸图像时，它能够正确预测身份。然而，当该图像与另一个噪声图像数组结合在一起时，这个噪声图像数组是通过访问预测类别概率自动生成的，模型错误地预测了人脸图像的身份，即使合成后的图像与原始图像在视觉上完全相同：
- en: '![Figure 1.13 – Example of a potential image-based adversarial attack](img/B18187_01_013.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.13 – 潜在的基于图像的对抗性攻击示例](img/B18187_01_013.jpg)'
- en: Figure 1.13 – Example of a potential image-based adversarial attack
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.13 – 潜在的基于图像的对抗性攻击示例
- en: Theoretically, adversarial attacks can be made using any type of data and are
    not limited to images. As the stakes of machine learning use cases grow higher,
    it’s more likely that attackers are willing to invest in a research team that
    produces novel adversarial examples.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，对抗性攻击可以使用任何类型的数据进行，并不限于图像。随着机器学习应用场景的风险增大，攻击者更有可能投入资金组建研究团队，以产生新的对抗性示例。
- en: Some of the adversarial examples and the methods that generate them rely on
    access to the deep learning model itself so that a targeted adversarial example
    can be made. This means that it is not remotely possible for a potential attacker
    to be able to succeed in confusing a model created by someone else unless they
    have access to the model. However, many businesses utilize publicly available
    pre-trained models as-is and apply transfer learning methods to reduce the amount
    of work needed to satisfy a business use case. Any publicly available pre-trained
    models will also be available to the attackers, allowing them to build targeted
    adversarial examples for individual models. Examples of such pre-trained models
    include all the publicly available ImageNet pre-trained convolutional neural network
    models and weights.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 一些对抗样本及其生成方法依赖于对深度学习模型本身的访问，以便制作定向对抗样本。这意味着，潜在的攻击者除非能够访问该模型，否则几乎不可能成功地混淆别人创建的模型。然而，许多企业直接使用公开可用的预训练模型，并应用迁移学习方法，以减少满足业务用例所需的工作量。任何公开可用的预训练模型也同样对攻击者开放，允许他们为个别模型构建定向对抗样本。这类预训练模型的例子包括所有公开可用的ImageNet预训练卷积神经网络模型及其权重。
- en: So, how do we attempt to mitigate this risk?
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何尝试减轻这种风险呢？
- en: One of the methods we can use to mitigate the risk of an adversarial attack
    is to train deep learning models with examples from the known set of methods from
    public research. By training with the known adversarial examples, the model will
    learn how to ignore the adversarial information through the learning process and
    be unfazed by such examples during validation and inference time. Evaluating different
    variations of the adversarial examples also helps set expectations on when the
    model will fail.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用来减轻对抗性攻击风险的方法之一，是通过使用公共研究中的已知方法集的示例来训练深度学习模型。通过使用已知的对抗样本进行训练，模型将学会在学习过程中忽略这些对抗信息，并在验证和推理阶段不受此类样本的影响。评估不同变体的对抗样本还帮助我们设定期望，了解模型何时会失败。
- en: In this section, we discussed an in-depth take on the security issues when dealing
    with and consuming data for machine learning purposes. In [*Chapter 14*](B18187_14.xhtml#_idTextAnchor206),
    *Analyzing Adversarial Performance*, we will go through a more in-depth practical
    evaluation of adversarial attack technologies for different data modalities and
    how to mitigate them in the context of deep learning. Next, we will dive into
    another category of risk that is at the core of the model development process.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们深入探讨了处理和使用数据进行机器学习时的安全问题。在[*第14章*](B18187_14.xhtml#_idTextAnchor206)，*分析对抗性表现*中，我们将深入实践评估针对不同数据模态的对抗性攻击技术，并探讨如何在深度学习的背景下减轻这些攻击。接下来，我们将深入探讨模型开发过程中的另一个核心风险类别。
- en: Overfitting and underfitting
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合
- en: During the model development process, which is the process of training, validating,
    and testing a machine learning model, one of the most foundational risks to handle
    is overfitting a model and underfitting a model.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型开发过程中，即训练、验证和测试机器学习模型的过程中，最基础的风险之一就是过拟合和欠拟合。
- en: '**Overfitting** is an event where a machine learning model becomes so biased
    toward the provided training dataset examples and learned patterns that it can
    only exclusively distinguish examples that belong in the training dataset while
    failing to distinguish any examples in the validation and testing dataset.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '**过拟合**是指机器学习模型对提供的训练数据集样本和已学习的模式产生过度偏向，以至于它只能区分训练数据集中的样本，而无法区分验证集和测试集中的任何样本。'
- en: '**Underfitting**, on the other hand, is an event where a machine learning model
    fails to capture any patterns of the provided training, validation, and testing
    dataset.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '**欠拟合**则是指机器学习模型未能捕捉到提供的训练、验证和测试数据集中的任何模式。'
- en: Learning a generalizable pattern to output mapping capability is the key to
    building a valuable and usable machine learning model. However, there is no silver
    bullet to achieving a nicely fitted model, and very often, it requires iterating
    between the data preparation, model development, and deliver model insights stages.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 学习可泛化的模式输出映射能力是构建有价值且可用的机器学习模型的关键。然而，达成一个良好拟合的模型并没有银弹，通常需要在数据准备、模型开发和模型交付洞察阶段之间反复迭代。
- en: 'Here are some tips to prevent overfitting and ensure generalization in the
    context of deep learning:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些防止过拟合并确保深度学习中泛化的建议：
- en: Augment your data in expectation of the eventual deployment conditions
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测最终部署条件时，增加数据增强
- en: Collect enough data to cover every single variation possible of your targeted
    label
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集足够的数据，覆盖目标标签的每一种可能变异
- en: Collaborate with domain experts and understand key indicators and patterns
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与领域专家合作，了解关键指标和模式
- en: Use cross-validation methods to ensure the model gets evaluated fairly on unseen
    data
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用交叉验证方法，确保模型在未见数据上进行公平评估
- en: Use simpler neural network architectures whenever possible
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能使用更简单的神经网络架构
- en: 'Here are some tips to prevent underfitting:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些防止欠拟合的建议：
- en: Evaluate a variety of different models
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估各种不同的模型
- en: Collaborate with domain experts and understand key indicators and patterns
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与领域专家合作，了解关键指标和模式
- en: Make sure the data is clean and has as low an error as possible
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保数据清洁，尽可能减少错误
- en: Make sure there is enough data
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保数据足够充足
- en: Start with a small set of data inputs when building your model and build your
    way up to more complex data to ensure models can fit appropriately on your data
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在构建模型时，从少量数据输入开始，逐步增加更复杂的数据，以确保模型能在数据上恰当地拟合
- en: In this section, we have discussed issues while building a model. Next, we will
    be discussing a type of risk that will affect the built model after it has ben
    trained.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了构建模型时的问题。接下来，我们将讨论一种在模型训练完成后会影响模型的风险。
- en: Model consistency risk
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型一致性风险
- en: One of the major traits of the machine learning process is that it is cyclical.
    Models get retrained constantly in hopes of finding better settings. These settings
    can be a different data partitioning method, a different data transformation method,
    a different model, or the same model with different model settings. Often, the
    models that are built need to be compared against each other fairly and equitably.
    Model consistency is the one feature that ensures that a fair comparison can be
    made between different model versions and performance metrics. Every single process
    before a model is obtained is required to be consistent so that when anybody tries
    to execute the same processes with the same settings, the same model should be
    obtainable and reproducible. We should reiterate that even when some processes
    require randomness when building the model, it needs to be randomly deterministic.
    This is needed so that the only difference in the setup is the targeted settings
    and nothing else.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习过程的一个主要特征是它是循环的。模型会不断重新训练，旨在寻找更好的设置。这些设置可能是不同的数据划分方法、不同的数据转换方法、不同的模型，或是相同的模型但具有不同的模型设置。通常，构建的模型需要相互公平且公正地进行比较。模型一致性是确保不同模型版本和性能指标之间能够进行公平比较的唯一特征。在获得模型之前的每个过程都要求保持一致性，以便当任何人尝试用相同的设置执行相同的过程时，应该能够获得和再现相同的模型。我们需要重申，即使在构建模型时某些过程需要随机性，它也需要是可随机确定的。这是必要的，以确保设置中的唯一差异是目标设置，而不是其他任何因素。
- en: Model consistency doesn’t stop with just the reproducibility of a model – it
    extends to the consistency of the predictions of a model. The predictions should
    be the same when predicted using a different batch size setting, and the same
    data input should always produce the same predictions. Inconsistencies in model
    predictions are a major red flag that signals that anything the model produces
    is not representative of what you will get during deployment and any derived model
    insights would be misinformation.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 模型一致性不仅仅体现在模型的可重现性上——它还扩展到模型预测的一致性。当使用不同的批量大小设置进行预测时，预测结果应该是相同的，而且相同的数据输入应始终产生相同的预测结果。模型预测中的不一致性是一个重大的警示信号，表明模型所产生的任何结果都无法代表在部署时的实际表现，任何由模型得出的见解都可能是误导信息。
- en: To combat model consistency risk, always make sure your code produces consistent
    results by seeding the random number generator whenever possible. Always validate
    model consistency either manually or automatically in the model development stage
    after you build the model.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对模型一致性风险，始终确保通过尽可能地种子化随机数生成器，使代码产生一致的结果。在模型开发阶段，构建模型后，始终手动或自动验证模型的一致性。
- en: Model degradation risk
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型退化风险
- en: When you have built a model, verified it, and demonstrated its impact on the
    business, you then take it into the model deployment stage and position it for
    use. One mistake is to think that this is the end of the deep learning project
    and that you can take your hands off and just let the model do its job. Sadly,
    most machine learning models degrade, depending on the level of generalization
    your model achieved during the model development stage on the data available in
    the wild. A common scenario that happens when a model gets deployed is that, initially,
    the model’s performance and the characteristics of the data received during deployment
    stay the same, and change over time. Time has the potential to change the conditions
    of the environment and anything in the world. Machines grow old, seasons change,
    and people change, and expecting that conditions and variables surrounding the
    machine learning model will change can allow you to make sure models stay relevant
    and impactful to the business.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 当你构建了一个模型、验证了它，并展示了它对业务的影响后，你接着将其进入模型部署阶段并准备好投入使用。一个常见的错误是认为这就意味着深度学习项目的结束，你可以把手放开，任由模型自我运行。遗憾的是，大多数机器学习模型会退化，这取决于你在模型开发阶段实现的泛化程度，以及在实际环境中可用数据的情况。一个常见的场景是，当模型被部署时，最初模型的表现和部署过程中接收到的数据特征保持一致，并随着时间推移发生变化。时间有可能改变环境条件以及世界上的任何事物。机器老化，季节变化，人也会改变，预见到这些条件和变量的变化可以帮助你确保模型在业务中保持相关性和影响力。
- en: How a model can degrade can be categorized into three categories, namely **data
    drift**, **concept drift**, and **model drift**. Conceptually, drift can be associated
    with a boat slowly drifting away from its ideal position. In the case of machine
    learning projects, instead of a boat drifting away, it’s the data or model drifting
    away from its original perceived behavior or pattern. Let’s briefly go through
    these types of drift.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 模型退化的方式可以分为三类，分别是**数据漂移**、**概念漂移**和**模型漂移**。从概念上讲，漂移可以与一艘船缓慢漂离其理想位置相联系。在机器学习项目中，不是船漂离，而是数据或模型从其原始的感知行为或模式中漂离。我们简要地介绍一下这几种漂移类型。
- en: Data drift is a form of degradation that is associated with the input data the
    model needs to provide a prediction value. When a deployed model experiences data
    drift, it means that the received that’s data during deployment doesn’t belong
    to the inherent distribution of the data that was trained and validated by the
    machine learning model. If there were ways to validate the model on the new data
    supplied during deployment, data drift would potentially cause a shift in the
    original expected metric performance obtained during model validation. An example
    of data drift in the context of a deep learning model can be a use case requiring
    the prediction of human actions outdoors. In this use case, data drift would be
    that the original data that was collected consisted of humans in summer clothing
    during the summer season, and due to winter, the people are wearing winter clothing
    instead.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 数据漂移是一种退化现象，涉及到模型所需提供预测值的输入数据。当一个部署后的模型经历数据漂移时，意味着在部署过程中接收到的数据不属于机器学习模型在训练和验证过程中所用数据的固有分布。如果有方法验证在部署过程中提供的新数据上的模型，数据漂移可能会导致模型验证期间原始预期指标性能的变化。一个深度学习模型中数据漂移的例子可以是在户外预测人类行为的用例。在这个用例中，数据漂移表现为原始数据是在夏季收集的，数据中包括穿夏季服装的人，而由于冬季的到来，人们则穿上了冬季服装。
- en: Concept drift is a form of degradation that is associated with the change in
    how the input data interacts with the target output data. In the planning stage,
    domain experts and machine learning practitioners collaborate to define input
    variables that can affect the targeted output data. This defined input and output
    setup will subsequently be used for building a machine learning model. Sometimes,
    however, not all of the context that can affect the targeted output data is included
    as input variables due to either the availability of that data or just the lack
    of domain knowledge. This introduces a dependence on the conditions of the missing
    context associated with the data collected for machine learning. When the conditions
    of the missing context drift away from the base values that exist in the training
    and validation data, concept drift occurs, rendering the original concept irrelevant
    or shifted. In simpler terms, this means that the same input doesn’t map to the
    same output from the training data anymore. In the context of deep learning, we
    can take a sentiment classification use case that is based on textual data. Let’s
    say that a comment or speech can be a negative, neutral, or positive sentiment
    based on both the jurisdiction and the text itself. This means that in some jurisdictions,
    people have different thresholds on what is considered negative, neutral, or positive
    and grade things differently. Training the model with only textual data wouldn’t
    allow itself to generalize across jurisdictions accurately and thus would face
    concept drift any time it gets deployed in another jurisdiction.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 概念漂移是一种与输入数据如何与目标输出数据交互变化相关的退化形式。在规划阶段，领域专家和机器学习从业者合作定义可能影响目标输出数据的输入变量。这个定义好的输入和输出设置随后将用于构建机器学习模型。然而，有时并不是所有可能影响目标输出数据的上下文都会作为输入变量包含在内，这可能是由于数据的可获得性或缺乏领域知识的原因。这就引入了依赖于缺失上下文条件的情况，这些缺失的上下文与为机器学习收集的数据相关。当缺失的上下文条件与训练和验证数据中存在的基本值发生漂移时，就会发生概念漂移，导致原始概念变得无关或发生偏移。简而言之，这意味着相同的输入不再映射到训练数据中的相同输出。在深度学习的背景下，我们可以以一个基于文本数据的情感分类用例为例。假设一个评论或讲话根据司法管辖区和文本本身可以是负面、中立或正面情感。这意味着在某些司法管辖区，人们对什么被视为负面、中立或正面的阈值不同，并且对事物的评分方式也不同。仅使用文本数据训练模型将无法准确地跨司法管辖区进行泛化，因此每当模型在另一个司法管辖区部署时，它都会面临概念漂移。
- en: Lastly, model drift is a form of degradation associated with operational metrics
    and easily measurable metrics. Some of the factors of degradation that can be
    categorized into model drift are model latency, model throughput, and model error
    rates. Model drift metrics are generally easy to measure and track compared to
    the other two types of drift.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，模型漂移是一种与操作指标和易于测量的指标相关的退化形式。可以归类为模型漂移的退化因素包括模型延迟、模型吞吐量和模型错误率。与其他两种漂移类型相比，模型漂移的指标通常更容易衡量和跟踪。
- en: One of the best workflows for mitigating these risks is by tracking the metrics
    under all of these three types of drift and having a clear path for a new machine
    learning model to be built, which is a process I call **drift reset**. Now that
    we’ve covered a brief overview of model degradation, in [*Chapter 16*](B18187_16.xhtml#_idTextAnchor238),
    *Governing Deep Learning Models*, and [*Chapter 17*](B18187_17.xhtml#_idTextAnchor247),
    *Managing Drift Effectively in a Dynamic Environment*, we will go more in-depth
    on these risks and discuss practically how to mitigate this risk in the context
    of deep learning.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解这些风险的最佳工作流程之一是跟踪这三种类型漂移下的指标，并为新机器学习模型的构建提供清晰路径，这个过程我称之为**漂移重置**。现在我们已经简要概述了模型退化，在[*第16章*](B18187_16.xhtml#_idTextAnchor238)《深度学习模型的管理》和[*第17章*](B18187_17.xhtml#_idTextAnchor247)《在动态环境中有效管理漂移效应》中，我们将更深入地讨论这些风险，并实际探讨如何在深度学习的背景下缓解这一风险。
- en: Summary
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned what it takes to complete a deep learning project
    successfully in a repeatable and consistent manner at a somewhat mid to high-level
    view. The topics we discussed in this chapter were structured to be more comprehensive
    at the earlier stages of the deep learning life, which covers the planning and
    data preparation stages.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学到了如何以一种可重复、一致的方式成功完成深度学习项目，虽然这是从中高层次的视角来看。我们在本章讨论的主题被结构化为深度学习生命周期早期阶段的更全面内容，这包括规划和数据准备阶段。
- en: In the following chapters, we will explore the mid to final stages of the life
    cycle more comprehensively. This involves everything after the data preparation
    stage, which includes model development, model insights, model deployment, and,
    finally, model governance.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将更全面地探讨生命周期的中后期阶段。这涉及数据准备阶段之后的所有内容，包括模型开发、模型洞察、模型部署以及最后的模型治理。
- en: In the next chapter, we will start to explore common and widely used deep learning
    architectures more extensively.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将更深入地探索常见和广泛使用的深度学习架构。
- en: Further reading
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Bowen Shi, Wei-Ning Hsu, Abdelrahman Mohamed. *Robust Self-Supervised Audio-Visual
    Speech Recognition*, 2022\. Toyota Technological Institute at Chicago, Meta AI:
    [https://arxiv.org/pdf/2201.01763v2.pdf](https://arxiv.org/pdf/2201.01763v2.pdf).'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 石博文、徐维宁、阿卜杜勒拉赫曼·穆罕默德。*强大的自监督视听语音识别*，2022年。芝加哥丰田技术学院，Meta AI：[https://arxiv.org/pdf/2201.01763v2.pdf](https://arxiv.org/pdf/2201.01763v2.pdf)。
- en: 'Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma,
    Chang Zhou, Jingren Zhou, Hongxia Yang. *Unifying Architectures, Tasks, and Modalities
    Through a Simple Sequence-to-Sequence Learning Framework*. DAMO Academy, Alibaba
    Group: [https://arxiv.org/pdf/2202.03052v1.pdf](https://arxiv.org/pdf/2202.03052v1.pdf).'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王鹏、杨安、门锐、林俊阳、白帅、李志康、马建新、周畅、周靖仁、杨红霞。*通过简单的序列到序列学习框架统一架构、任务和模态*。达摩院，阿里巴巴集团：[https://arxiv.org/pdf/2202.03052v1.pdf](https://arxiv.org/pdf/2202.03052v1.pdf)。
