- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Advanced Computer Vision Applications
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级计算机视觉应用
- en: In [*Chapter 4*](B19627_04.xhtml#_idTextAnchor107), we introduced **convolutional
    networks** (**CNNs**) for computer vision and some of the most popular and best-performing
    CNN models. In this chapter, we’ll continue with more of the same, but at a more
    advanced level. Our *modus operandi* so far has been to provide simple classification
    examples to support your theoretical knowledge of **neural networks** (**NNs**).
    In the universe of computer vision tasks, classification is fairly straightforward
    as it assigns a single label to an image. This also makes it possible to manually
    create large, labeled training datasets. In this chapter, we’ll introduce **transfer
    learning** (**TL**), a technique that will allow us to transfer the knowledge
    of pre-trained NNs to a new and unrelated task. We’ll also see how TL makes it
    possible to solve two interesting computer vision tasks – object detection and
    semantic segmentation. We can say that these tasks are more complex compared to
    classification because the model has to obtain a more comprehensive understanding
    of the image. It has to be able to detect different objects as well as their positions
    in the image. At the same time, the task’s complexity allows for more creative
    solutions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B19627_04.xhtml#_idTextAnchor107)中，我们介绍了用于计算机视觉的**卷积网络**（**CNN**）以及一些最受欢迎和表现最好的CNN模型。在本章中，我们将继续探讨类似的内容，但会深入到更高级的层次。到目前为止，我们的*操作方式*一直是提供简单的分类示例，以支持你对**神经网络**（**NN**）的理论知识。在计算机视觉任务的宇宙中，分类是相对直接的，因为它为图像分配一个单一的标签。这也使得手动创建大型标签化训练数据集成为可能。在本章中，我们将介绍**迁移学习**（**TL**），一种技术，它将使我们能够将预训练的神经网络的知识迁移到一个新的、无关的任务中。我们还将看到，迁移学习如何使得解决两个有趣的计算机视觉任务成为可能——目标检测和语义分割。我们可以说，这些任务相对于分类更为复杂，因为模型需要对图像有更全面的理解。它不仅要能够检测出不同的物体，还要知道它们在图像中的位置。同时，这些任务的复杂性也为更具创意的解决方案提供了空间。
- en: Finally, we’ll introduce a new class of algorithms called generative models,
    which will help us generate new images.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将介绍一种新型算法，称为生成模型，它将帮助我们生成新的图像。
- en: 'This chapter will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: '**Transfer** **learning** (**TL**)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迁移** **学习**（**TL**）'
- en: Object detection
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测
- en: Semantic segmentation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义分割
- en: Image generation with diffusion models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用扩散模型生成图像
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We’ll implement the example in this chapter using Python, PyTorch, Keras, and
    Ultralytics YOLOv8 ([https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics)).
    If you don’t have an environment set up with these tools, fret not – the example
    is available as a Jupyter notebook on Google Colab. You can find the code examples
    in this book’s GitHub repository: [https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter05](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter05).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中使用Python、PyTorch、Keras和Ultralytics YOLOv8（[https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics)）实现示例。如果你没有配置好这些工具的环境，不用担心——示例可以在Google
    Colab上的Jupyter notebook中找到。你可以在本书的GitHub仓库中找到代码示例：[https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter05](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter05)。
- en: Transfer learning (TL)
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习（TL）
- en: So far, we’ve trained small models on toy datasets, where the training took
    no more than an hour. But if we want to work with large datasets, such as ImageNet,
    we will need a much bigger network that trains for a lot longer. More importantly,
    large datasets are not always available for the tasks we’re interested in. Keep
    in mind that besides obtaining the images, they have to be labeled, and this could
    be expensive and time-consuming. So, what does a humble engineer do when they
    want to solve a real ML problem with limited resources? Enter TL.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经在玩具数据集上训练了小型模型，训练时间不超过一个小时。但如果我们想要处理大规模数据集，比如ImageNet，我们将需要一个更大的网络，且训练时间会更长。更重要的是，大规模数据集并不总是能满足我们感兴趣任务的需求。请记住，除了获取图像之外，它们还需要被标注，而这可能是既昂贵又费时的。那么，当工程师想用有限资源解决实际的机器学习问题时，应该怎么办呢？这时，迁移学习（TL）就派上用场了。
- en: TL is the process of applying an existing trained ML model to a new, but related,
    problem. For example, we can take a network trained on ImageNet and repurpose
    it to classify grocery store items. Alternatively, we could use a driving simulator
    game to train an NN to drive a simulated car, and then use the network to drive
    a real car (but don’t try this at home!). TL is a general ML concept that applies
    to all ML algorithms – we’ll also use TL in [*Chapter 8*](B19627_08.xhtml#_idTextAnchor220).
    But in this chapter, we’ll talk about TL in CNNs. Here’s how it works.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习（TL）是将一个已经训练好的机器学习（ML）模型应用于一个新的但相关的问题的过程。例如，我们可以将一个在 ImageNet 上训练过的网络重新用于分类杂货店物品。或者，我们可以使用一个驾驶模拟游戏来训练神经网络（NN）驾驶一辆模拟汽车，然后用这个网络来驾驶真实的汽车（但请不要在家尝试！）。迁移学习是一个适用于所有机器学习算法的通用概念——我们将在[*第
    8 章*](B19627_08.xhtml#_idTextAnchor220)中也使用迁移学习。但在本章中，我们将讨论卷积神经网络（CNN）中的迁移学习。它是如何工作的，下面解释。
- en: We start with an existing pre-trained net. The most common scenario is to take
    a network pre-trained with ImageNet, but it could be any dataset. PyTorch, **TensorFlow**
    (**TF**), and Keras all have popular ImageNet pre-trained neural architectures
    that we can use. Alternatively, we can train our network with a dataset of our
    choice.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个现有的预训练网络开始。最常见的场景是使用一个在 ImageNet 上预训练的网络，但它也可以是任何数据集。PyTorch、**TensorFlow**（**TF**）和
    Keras 都提供了流行的 ImageNet 预训练神经网络架构，我们可以使用。或者，我们也可以选择一个数据集来训练自己的网络。
- en: 'In [*Chapter 4*](B19627_04.xhtml#_idTextAnchor107), we mentioned how the **fully
    connected** (**FC**) layers at the end of a CNN act as translators between the
    network’s language (the abstract feature representations learned during training)
    and our language, which is the class of each sample. You can think of TL as a
    translation to another language. We start with the network’s features, which is
    the output of the last convolutional or pooling layer. Then, we translate them
    to a different set of classes for the new task. We can do this by removing the
    last layers of an existing pre-trained network and replacing them with a different
    set of layers, which represents the classes of the new problem. Here is a diagram
    of the TL scenario:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 4 章*](B19627_04.xhtml#_idTextAnchor107)中，我们提到过 CNN 最后的 **全连接层**（**FC**）如何作为网络语言（训练过程中学到的抽象特征表示）与我们的语言（每个样本的类别）之间的转换器。你可以将迁移学习看作是对另一种语言的翻译。我们从网络的特征开始，这些特征是最后一个卷积层或池化层的输出。然后，我们将它们翻译成新任务的不同类别。我们可以通过去除现有预训练网络的最后几层，并用一组新的层替换它们，这些新层代表了新问题的类别。以下是迁移学习场景的示意图：
- en: '![Figure 5.1 – A TL scenario, where we replace the last layer(s) of a  pre-trained
    network and repurpose it for a new problem](img/B19627_05_1.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 一个迁移学习场景，其中我们替换了一个预训练网络的最后一层，并将其重新用于新的问题](img/B19627_05_1.jpg)'
- en: Figure 5.1 – A TL scenario, where we replace the last layer(s) of a
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 一个迁移学习（TL）场景，其中我们替换了一个
- en: pre-trained network and repurpose it for a new problem
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练网络，并将其重新用于新的问题
- en: 'However, we cannot do this mechanically and expect the new network to work
    because we still have to train the new layer with data related to the new task.
    We have two options to do this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们不能机械地进行这种操作并期望新网络能够正常工作，因为我们仍然需要用与新任务相关的数据来训练新层。我们有两种方式可以做到这一点：
- en: '**Use the original part of the network as a feature extractor and only train
    the new layer(s)**: First, we feed the network a training batch of the new data
    and propagate it forward and backward to see the network’s output and error gradients.
    This part works just like regular training would. But during the weight updates
    phase, we lock the weights of the original network and only update the weights
    of the new layers. This is the recommended approach when we have limited training
    data for the new problem. By locking most of the network weights, we prevent overfitting
    on the new data.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用网络的原始部分作为特征提取器，只训练新的层**：首先，我们将新的数据批次输入网络，进行前向和反向传播，查看网络的输出和误差梯度。这部分的工作方式就像常规训练一样。但在权重更新阶段，我们会锁定原始网络的权重，只更新新层的权重。这是当我们对新问题的数据有限时推荐的做法。通过锁定大部分网络权重，我们可以防止在新数据上过拟合。'
- en: '**Fine-tune the whole network**: We train the whole network and not just the
    newly added layers at the end. It is possible to update all the network weights,
    but we can also lock some of the weights in the first layers. The idea here is
    that the initial layers detect general features – not related to a specific task
    – and it makes sense to reuse them. On the other hand, the deeper layers may detect
    task-specific features and it would be better to update them. We can use this
    method when we have more training data and don’t need to worry about overfitting.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调整个网络**：我们训练整个网络，而不仅仅是最后添加的层。可以更新所有网络权重，但我们也可以锁定一些第一层的权重。这里的想法是，初始层用于检测一般特征——与特定任务无关——因此重复使用它们是合理的。另一方面，较深的层可能会检测任务特定的特征，因此更新它们会更好。当我们拥有更多训练数据并且不需要担心过拟合时，可以使用这种方法。'
- en: Before we continue, let’s note that TL is not limited to classification-to-classification
    problems. As we’ll see later in this chapter, we can use pre-trained CNN as a
    backbone NN for object detection and semantic segmentation tasks. With that, let’s
    see how to implement TL in practice.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们需要指出，迁移学习不仅限于分类到分类的问题。正如我们在本章后面看到的，我们可以使用预训练的卷积神经网络（CNN）作为目标检测和语义分割任务的主干神经网络。现在，让我们看看如何在实践中实现迁移学习。
- en: Transfer learning with PyTorch
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PyTorch进行迁移学习
- en: 'In this section, we’ll apply an advanced ImageNet pre-trained network on the
    CIFAR-10 images. We’ll implement both types of TL. It’s preferable to run this
    example on GPU:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将应用一个先进的ImageNet预训练网络到CIFAR-10图像上。我们将实现两种类型的迁移学习。最好在GPU上运行这个示例：
- en: 'To define the training dataset, we have to consider a few things:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要定义训练数据集，我们需要考虑几个因素：
- en: Use mini-batch with size 50.
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用大小为50的mini-batch。
- en: The CIFAR-10 images are 32×32, while the ImageNet network expects 224×224 input.
    As we are using an ImageNet-based network, we’ll upsample the 32×32 CIFAR images
    to 224×224 using `transforms.``Resize`.
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CIFAR-10图像的大小为32×32，而ImageNet网络期望输入为224×224。由于我们使用的是基于ImageNet的网络，我们将使用`transforms.``Resize`将32×32的CIFAR图像上采样到224×224。
- en: Standardize the CIFAR-10 data using the ImageNet mean and standard deviation,
    because this is what the network expects.
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ImageNet的均值和标准差来标准化CIFAR-10数据，因为网络期望的是这种格式。
- en: Add minor data augmentation (flip).
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加轻微的数据增强（翻转）。
- en: 'We can do all this with the following code:'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码完成所有这些操作：
- en: '[PRE0]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Follow the same steps with the validation data (except for the data augmentation):'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照相同的步骤使用验证数据（除了数据增强之外）：
- en: '[PRE1]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Choose a device – preferably a GPU with a fallback on CPU:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个设备——最好是GPU，如果没有可退回到CPU：
- en: '[PRE2]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To train and validate the model, we’ll use the `train_model(model, loss_function,
    optimizer, data_loader)` and `test_model(model, loss_function, data_loader)` functions.
    We first implemented them in [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079),
    so we will not repeat the implementation here (it is available in the source code
    example on GitHub).
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了训练和验证模型，我们将使用`train_model(model, loss_function, optimizer, data_loader)`和`test_model(model,
    loss_function, data_loader)`函数。我们在[*第3章*](B19627_03.xhtml#_idTextAnchor079)中首先实现了这些函数，因此这里不再重复实现（完整的源代码示例可以在GitHub上找到）。
- en: 'Define the first TL scenario, where we use the pre-trained network as a feature
    extractor:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义第一个迁移学习场景，其中我们将预训练的网络用作特征提取器：
- en: We’ll use a popular network, `epochs` and evaluate the network accuracy after
    each epoch.
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用一个流行的网络，`epochs`，并在每个epoch后评估网络的准确度。
- en: Use the `plot_accuracy` accuracy function, which plots the validation accuracy
    on a `matplotlib` graph. We won’t include the full implementation here, but it
    is available on GitHub.
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`plot_accuracy`准确度函数，它在`matplotlib`图表上绘制验证准确度。我们不会在这里包含完整的实现，但可以在GitHub上找到。
- en: 'The following is the `tl_feature_extractor` function, which implements all
    this:'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是`tl_feature_extractor`函数，它实现了所有这些：
- en: '[PRE3]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Implement the fine-tuning approach with the `tl_fine_tuning` function. This
    function is similar to `tl_feature_extractor`, but now, we’ll train the whole
    network:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tl_fine_tuning`函数实现微调方法。此函数与`tl_feature_extractor`类似，但现在我们将训练整个网络：
- en: '[PRE4]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can run the whole thing in one of two ways:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过两种方式运行整个过程：
- en: Call `tl_fine_tuning(epochs=5)` to use the fine-tuning approach for five epochs.
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`tl_fine_tuning(epochs=5)`来使用微调方法训练五个epoch。
- en: Call `tl_feature_extractor(epochs=5)` to train the network with the feature
    extractor approach for five epochs.
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`tl_feature_extractor(epochs=5)`来使用特征提取方法训练网络五个epoch。
- en: With a network as a feature extractor, we’ll get about 81% accuracy, while with
    fine-tuning, we’ll get 89%. But if we run the fine-tuning for more epochs, the
    network will start overfitting. Next, let’s see the same example but with Keras.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用网络作为特征提取器时，我们的准确率大约为81%，而通过微调后，准确率可以达到89%。但如果我们在更多的训练周期中进行微调，网络将开始出现过拟合。接下来，我们来看一个相同的例子，但使用的是Keras。
- en: Transfer learning with Keras
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Keras进行迁移学习
- en: 'In this section, we’ll implement the two TL scenarios again, but this time
    using Keras and TF. In this way, we can compare the two libraries. Again, we’ll
    use the `MobileNetV3Small` architecture. In addition to Keras, this example also
    requires the TF Datasets package ([https://www.tensorflow.org/datasets](https://www.tensorflow.org/datasets)),
    a collection of various popular ML datasets. Let’s start:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将再次实现这两种迁移学习（TL）场景，但这次使用的是Keras和TF。通过这种方式，我们可以比较这两个库。我们仍然使用`MobileNetV3Small`架构。除了Keras外，本示例还需要TF
    Datasets包（[https://www.tensorflow.org/datasets](https://www.tensorflow.org/datasets)），它是一个包含各种流行机器学习数据集的集合。让我们开始：
- en: Note
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This example is partially based on [https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例部分基于[https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb)。
- en: 'Define the mini-batch and input image sizes (the image size is determined by
    the network architecture):'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义小批量和输入图像大小（图像大小由网络架构决定）：
- en: '[PRE5]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Load the CIFAR-10 dataset with the help of TF datasets. The `repeat()` method
    allows us to reuse the dataset for multiple epochs:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TF数据集的帮助加载CIFAR-10数据集。`repeat()`方法允许我们在多个周期中重复使用数据集：
- en: '[PRE6]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Define the `train_format_sample` and `test_format_sample` functions, which
    will transform the initial images into suitable CNN inputs. These functions play
    the same roles that the `transforms.Compose` object plays, which we defined in
    the *Implementing transfer learning with PyTorch* section. The input is transformed
    as follows:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`train_format_sample`和`test_format_sample`函数，这些函数会将初始图像转换为适合CNN输入的格式。这些函数扮演了与我们在*使用PyTorch实现迁移学习*一节中定义的`transforms.Compose`对象相同的角色。输入转换如下：
- en: The images are resized to 224×224, which is the expected network input size
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像会被调整为224×224的大小，这是网络预期的输入尺寸
- en: Each image is standardized by transforming its values so that it’s in the (-1;
    1) interval
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每张图片都会通过转换其值来进行标准化，使其处于（-1；1）区间内
- en: The labels are converted into one-hot encodings
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签被转换为独热编码
- en: The training images are randomly flipped horizontally and vertically
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练图像会随机地水平和垂直翻转
- en: 'Let’s look at the actual implementation:'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们看看实际的实现：
- en: '[PRE7]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next is some boilerplate code that assigns these transformers to the train/test
    datasets and splits them into mini-batches:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是一些模板代码，将这些转换器分配到训练/测试数据集，并将其拆分成小批量：
- en: '[PRE8]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Define the feature extraction model:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义特征提取模型：
- en: Use Keras for the pre-trained network and model definition since it is an integral
    part of TF
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于Keras是TF的核心部分，因此使用Keras来定义预训练网络和模型
- en: Load the `MobileNetV3Small` pre-trained net, excluding the final FC layers
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载`MobileNetV3Small`预训练网络，排除最后的全连接层
- en: Call `base_model.trainable = False`, which freezes all the network weights and
    prevents them from training
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用`base_model.trainable = False`，这会冻结所有网络权重，防止它们被训练
- en: Add a `GlobalAveragePooling2D` operation, followed by a new and trainable FC
    trainable layer at the end of the network
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一个`GlobalAveragePooling2D`操作，然后在网络的末端添加一个新的、可训练的全连接层
- en: 'The following code implements this:'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下代码实现了这一点：
- en: '[PRE9]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Define the fine-tuning model. The only difference it has from the feature extraction
    is that we only freeze some of the bottom pre-trained network layers (as opposed
    to all of them). The following is the implementation:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义微调模型。它与特征提取的唯一区别是，我们只冻结一些底层的预训练网络层（而不是全部层）。以下是实现代码：
- en: '[PRE10]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Implement the `train_model` function, which trains and evaluates the models
    that are created by either the `build_fe_model` or `build_ft_model` function.
    The `plot_accuracy` function is not implemented here but is available on GitHub:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现`train_model`函数，它用于训练和评估由`build_fe_model`或`build_ft_model`函数创建的模型。`plot_accuracy`函数在此未实现，但可在GitHub上找到：
- en: '[PRE11]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can run either the feature extraction or fine-tuning TL using the following
    code:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码运行特征提取或微调迁移学习：
- en: '`train_model(build_ft_model())`'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_model(build_ft_model())`'
- en: '`train_model(build_fe_model())`'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_model(build_fe_model())`'
- en: With a network as a feature extractor, we’ll get about 82% accuracy, while with
    fine-tuning, we’ll get 89% accuracy. The results are similar to the PyTorch example.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用网络作为特征提取器时，我们可以获得约82%的准确率，而通过微调后，准确率可达到89%。这些结果与PyTorch示例类似。
- en: Next, let’s turn our attention to object detection – a task we can solve with
    the help of TL.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们关注物体检测——这是一个我们可以通过TL来解决的任务。
- en: Object detection
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体检测
- en: '**Object detection** is the process of finding object instances of a certain
    class, such as people, cars, and trees, in images or videos. Unlike classification,
    object detection can detect multiple objects as well as their location in the
    image.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**物体检测**是指在图像或视频中找到某一类别的物体实例，例如人、车、树木等。与分类不同，物体检测不仅可以检测多个物体，还可以识别它们在图像中的位置。'
- en: 'An object detector would return a list of detected objects with the following
    information for each object:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测器会返回一份包含每个物体以下信息的检测对象列表：
- en: The class of the object (person, car, tree, and so on).
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体的类别（例如：人、车、树木等）。
- en: A probability (or objectness score) in the [0, 1] range, which conveys how confident
    the detector is that the object exists in that location. This is similar to the
    output of a regular binary classifier.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个概率值（或物体性得分），范围在[0, 1]之间，表示检测器对该位置存在物体的信心。这类似于常规二分类器的输出。
- en: The coordinates of the rectangular region of the image where the object is located.
    This rectangle is called a **bounding box**.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像中物体所在矩形区域的坐标。这个矩形被称为**边界框**。
- en: 'We can see the typical output of an object-detection algorithm in the following
    figure. The object type and objectness score are above each bounding box:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下图中看到物体检测算法的典型输出。物体类型和物体性得分位于每个边界框的上方：
- en: '![Figure 5.2 – The output of an object detector. Source: https://en.wikipedia.org/wiki/File:2011_FIA_GT1_Silverstone_2.jpg](img/B19627_05_2.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 物体检测器的输出。来源：[https://en.wikipedia.org/wiki/File:2011_FIA_GT1_Silverstone_2.jpg](https://en.wikipedia.org/wiki/File:2011_FIA_GT1_Silverstone_2.jpg)](img/B19627_05_2.jpg)'
- en: 'Figure 5.2 – The output of an object detector. Source: [https://en.wikipedia.org/wiki/File:2011_FIA_GT1_Silverstone_2.jpg](https://en.wikipedia.org/wiki/File:2011_FIA_GT1_Silverstone_2.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 物体检测器的输出。来源：[https://en.wikipedia.org/wiki/File:2011_FIA_GT1_Silverstone_2.jpg](https://en.wikipedia.org/wiki/File:2011_FIA_GT1_Silverstone_2.jpg)
- en: Next, let’s outline the different approaches to solving an object detection
    task.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将概述解决物体检测任务的不同方法。
- en: Approaches to object detection
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 物体检测方法
- en: 'In this section, we’ll outline three approaches:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将概述三种方法：
- en: '**Classic sliding window**: Here, we’ll use a regular classification network
    (classifier). This approach can work with any type of classification algorithm,
    but it’s relatively slow and error-prone:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经典滑动窗口**：在这里，我们将使用常规分类网络（分类器）。这种方法可以与任何类型的分类算法一起使用，但它相对较慢且容易出错。'
- en: '**Build an image pyramid**: This is a combination of different scales of the
    same image (see the following figure). For example, each scaled image can be two
    times smaller than the previous one. In this way, we’ll be able to detect objects
    regardless of their size in the original image.'
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建图像金字塔**：这是将同一图像的不同尺度组合在一起（见下图）。例如，每个缩放后的图像可以比前一个小两倍。通过这种方式，我们能够检测到原始图像中不同尺寸的物体。'
- en: '**Slide the classifier across the whole image**: We’ll use each location of
    the image as an input to the classifier, and the result will determine the type
    of object that is in the location. The bounding box of the location is just the
    image region that we used as input.'
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在整个图像上滑动分类器**：我们将图像的每个位置作为输入传递给分类器，结果将确定该位置的物体类型。位置的边界框就是我们用作输入的图像区域。'
- en: '**Multiple overlapping bounding boxes for each object**: We’ll use some heuristics
    to combine them into a single prediction.'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每个物体的多个重叠边界框**：我们将使用一些启发式方法将它们合并为一个单一的预测。'
- en: 'Here is a figure showing the sliding window approach:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一张展示滑动窗口方法的图示：
- en: '![Figure 5.3 – Sliding window plus image pyramid object detection](img/B19627_05_3.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 滑动窗口加图像金字塔物体检测](img/B19627_05_3.jpg)'
- en: Figure 5.3 – Sliding window plus image pyramid object detection
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 滑动窗口加图像金字塔物体检测
- en: '**Two-stage detection methods**: These methods are very accurate but relatively
    slow. As its name suggests, this involves two steps:'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**两阶段检测方法**：这些方法非常准确，但相对较慢。顾名思义，它涉及两个步骤：'
- en: A special type of CNN, called a **Region Proposal Network** (**RPN**), scans
    the image and proposes several possible bounding boxes, or **regions of interest**
    (**RoI**), where objects might be located. However, this network doesn’t detect
    the type of object, but only whether an object is present in the region.
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种特殊类型的 CNN，称为**区域提议网络** (**RPN**)，扫描图像并提出多个可能的边界框，或**兴趣区域** (**RoI**)，用于检测物体可能的位置。然而，该网络并不检测物体的类型，仅仅是判断该区域是否包含物体。
- en: The RoI is sent to the second stage for object classification, which determines
    the actual object in each bounding box.
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 RoI 送到第二阶段进行物体分类，从而确定每个边界框中的实际物体。
- en: '**One-stage (or one-shot) detection methods**: Here, a single CNN produces
    both the object type and the bounding box. These approaches are usually faster
    but less accurate than the two-stage methods.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一阶段（或单次）检测方法**：在这种方法中，单个 CNN 同时输出物体类型和边界框。这些方法通常比两阶段方法速度更快，但准确度较低。'
- en: In the next section, we’ll introduce **YOLO** – an accurate and efficient one-stage
    detection algorithm.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将介绍**YOLO**——一种精确高效的一阶段检测算法。
- en: Object detection with YOLO
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 YOLO 进行物体检测
- en: 'YOLO is one of the most popular one-stage detection algorithms. The name is
    an acronym for the popular motto “You only live once”, which reflects the one-stage
    nature of the algorithm. Since its original release, there have been multiple
    YOLO versions, with different authors. For the sake of clarity, we’ll list all
    versions here:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO 是最受欢迎的一阶段检测算法之一。其名称来源于流行的格言“你只活一次（You Only Live Once）”，这反映了算法的一阶段特性。自其首次发布以来，YOLO
    已经历了多个版本，不同的作者参与其中。为了便于理解，我们将在此列出所有版本：
- en: '*You Only Look Once: Unified, Real-Time Object Detection* ([https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640)),
    by Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*你只看一次：统一的实时物体检测* ([https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640))，作者：Joseph
    Redmon、Santosh Divvala、Ross Girshick 和 Ali Farhadi。'
- en: '*YOLO9000: Better, Faster, Stronger* ([https://arxiv.org/abs/1612.08242](https://arxiv.org/abs/1612.08242)),
    by Joseph Redmon and Ali Farhadi.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*YOLO9000: 更好、更快、更强* ([https://arxiv.org/abs/1612.08242](https://arxiv.org/abs/1612.08242))，作者：Joseph
    Redmon 和 Ali Farhadi。'
- en: '*YOLOv3: An Incremental Improvement* ([https://arxiv.org/abs/1804.02767](https://arxiv.org/abs/1804.02767),
    [https://github.com/pjreddie/darknet](https://github.com/pjreddie/darknet)), by
    Joseph Redmon and Ali Farhadi.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*YOLOv3: 增量式改进* ([https://arxiv.org/abs/1804.02767](https://arxiv.org/abs/1804.02767),
    [https://github.com/pjreddie/darknet](https://github.com/pjreddie/darknet))，作者：Joseph
    Redmon 和 Ali Farhadi。'
- en: '*YOLOv4: Optimal Speed and Accuracy of Object Detection* ([https://arxiv.org/abs/2004.10934](https://arxiv.org/abs/2004.10934),
    [https://github.com/AlexeyAB/darknet](https://github.com/AlexeyAB/darknet)), by
    Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*YOLOv4: 物体检测的最佳速度与精度* ([https://arxiv.org/abs/2004.10934](https://arxiv.org/abs/2004.10934),
    [https://github.com/AlexeyAB/darknet](https://github.com/AlexeyAB/darknet))，作者：Alexey
    Bochkovskiy、Chien-Yao Wang 和 Hong-Yuan Mark Liao。'
- en: '**YOLOv5** and **YOLOv8** ([https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5),
    [https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics)),
    by Ultralitics ([https://ultralytics.com/](https://ultralytics.com/)). V5 and
    v8 have no official paper.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**YOLOv5** 和 **YOLOv8** ([https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5),
    [https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics))，由
    Ultralitics 提供 ([https://ultralytics.com/](https://ultralytics.com/))。V5 和 v8
    没有正式论文。'
- en: '*YOLOv6 v3.0: A Full-Scale Reloading* ([https://arxiv.org/abs/2301.05586](https://arxiv.org/abs/2301.05586),
    [https://github.com/meituan/YOLOv6](https://github.com/meituan/YOLOv6)), by Chuyi
    Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng Cheng, Bo Zhang, Zaidan Ke, Xiaoming
    Xu, and Xiangxiang Chu.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*YOLOv6 v3.0: 全面重新加载* ([https://arxiv.org/abs/2301.05586](https://arxiv.org/abs/2301.05586),
    [https://github.com/meituan/YOLOv6](https://github.com/meituan/YOLOv6))，作者：Chuyi
    Li、Lulu Li、Yifei Geng、Hongliang Jiang、Meng Cheng、Bo Zhang、Zaidan Ke、Xiaoming Xu
    和 Xiangxiang Chu。'
- en: '*YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time
    object detectors* ([https://arxiv.org/abs/2207.02696](https://arxiv.org/abs/2207.02696),
    Mark L[https://github.com/WongKinYiu/yolov7](https://github.com/WongKinYiu/yolov7)),
    by Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan iao.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*YOLOv7: 可训练的“免费赠品”集合创造了实时物体检测的新技术前沿* ([https://arxiv.org/abs/2207.02696](https://arxiv.org/abs/2207.02696),
    Mark L[https://github.com/WongKinYiu/yolov7](https://github.com/WongKinYiu/yolov7))，作者：Chien-Yao
    Wang、Alexey Bochkovskiy 和 Hong-Yuan Mark Liao。'
- en: Note
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'v3 is the last version, released by the original authors of the algorithm.
    v4 is a fork of v3 and was endorsed by the main author of v1-v3, Joseph Redmon
    ([https://twitter.com/pjreddie/status/1253891078182199296](https://twitter.com/pjreddie/status/1253891078182199296)).
    On the other hand, v5 is an independent implementation, inspired by YOLO. This
    sparked a controversy regarding the name of v5\. You can follow some of the discussion
    at https://github.com/AlexeyAB/darknet/issues/5920, where Alexey Bochkovskiy,
    the author of v4, has also posted. The authors of v5 have also addressed the controversy
    here: [https://blog.roboflow.com/yolov4-versus-yolov5/](https://blog.roboflow.com/yolov4-versus-yolov5/).
    Regardless of this discussion, v5 and v8 have proven to work and are popular detection
    algorithms in their own right.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: v3是最后一个由算法的原作者发布的版本。v4是v3的一个分支，由v1-v3的主要作者Joseph Redmon（[https://twitter.com/pjreddie/status/1253891078182199296](https://twitter.com/pjreddie/status/1253891078182199296)）支持发布。另一方面，v5是一个独立的实现，灵感来自于YOLO。这引发了关于v5名称的争议。你可以查看一些讨论，访问[https://github.com/AlexeyAB/darknet/issues/5920](https://github.com/AlexeyAB/darknet/issues/5920)，其中v4的作者Alexey
    Bochkovskiy也进行了发帖。v5的作者也在这里解决了争议：[https://blog.roboflow.com/yolov4-versus-yolov5/](https://blog.roboflow.com/yolov4-versus-yolov5/)。不管这些讨论如何，v5和v8已经证明其有效性，并且它们在各自的领域中是受欢迎的检测算法。
- en: We’ll discuss the YOLO properties shared among all versions and we’ll point
    out some of the differences.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论所有版本共享的YOLO特性，并指出其中的一些差异。
- en: 'Let’s start with the YOLO architecture:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从YOLO架构开始：
- en: '![Figure 5.4 – The YOLO architecture](img/B19627_05_4.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – YOLO架构](img/B19627_05_4.jpg)'
- en: Figure 5.4 – The YOLO architecture
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – YOLO架构
- en: 'It contains the following components:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含以下组件：
- en: '**Backbone**: This is a CNN model that’s responsible for extracting features
    from the input image. These features are then passed to the next components for
    object detection. Usually, the backbone is an ImageNet pre-trained CNN, similar
    to the advanced models we discussed in [*Chapter 4*](B19627_04.xhtml#_idTextAnchor107).'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主干网络**：这是一个CNN模型，负责从输入图像中提取特征。这些特征随后会传递给下一个组件进行目标检测。通常，主干网络是一个在ImageNet上预训练的CNN，类似于我们在[*第4章*](B19627_04.xhtml#_idTextAnchor107)中讨论的高级模型。'
- en: The backbone is an example of TL – we take a CNN trained for classification
    and repurpose it for object detection. The different YOLO versions use different
    backbones. For example, v3 uses a special fully convolutional CNN called DarkNet-53
    with 53 layers. Subsequent YOLO versions introduce various improvements to this
    architecture, while others use their own unique backbone.
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 主干网络是迁移学习（TL）的一个例子——我们将一个用于分类的CNN拿来重新用于目标检测。不同版本的YOLO使用不同的主干网络。例如，v3使用一个名为DarkNet-53的特殊全卷积CNN，它有53层。随后的YOLO版本在此架构上进行了一些改进，而其他版本则使用了自己独特的主干网络。
- en: '**Neck**: This is an intermediate part of the model that connects the backbone
    to the head. It concatenates the output at different stages of the backbone feature
    maps before sending the combined result to the next component (the head). This
    is an alternative to the standard approach, where we would just send the output
    of the last backbone convolution for further processing. To understand the need
    for the neck, let’s recall that our goal is to create a precise bounding box around
    the edges of the detected object. The object itself might be big or small, relative
    to the image. However, the receptive field of the deeper layers of the backbone
    is large because it aggregates the receptive fields of all preceding layers. Hence,
    the features detected at the deeper layers encompass large parts of the input
    image. This runs contrary to our goal of fine-grained object detection, regardless
    of the object’s size. To solve this, the neck combines the feature maps at different
    backbone stages, which makes it possible to detect objects at different scales.
    However, the feature maps at each backbone stage have different dimensions and
    cannot be combined directly. The neck applies different techniques, such as upsampling
    or downsampling, to equalize these dimensions, so that they can be concatenated.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**颈部**：这是模型的中间部分，连接主干网络和头部。它在将组合结果发送到下一个组件（头部）之前，将主干特征图在不同阶段的输出进行串联。这是标准方法的替代方案，标准方法是仅发送最后一个主干卷积的输出以进行进一步处理。为了理解颈部的必要性，让我们回顾一下我们的目标是围绕检测到的物体边缘创建一个精确的边界框。物体本身的大小可能相对图像来说很大或很小。然而，主干的深层接收域较大，因为它汇聚了所有前面层的接收域。因此，深层检测到的特征包含了输入图像的大部分。这与我们的精细物体检测目标相悖，无论物体的大小如何。为了解决这个问题，颈部在不同主干阶段结合特征图，从而使得不同尺度的物体都能被检测到。然而，每个主干阶段的特征图维度不同，不能直接组合。颈部应用不同的技术，例如上采样或下采样，以平衡这些维度，使它们能够串联。'
- en: '**Head**: This is the final component of the model, which outputs the detected
    objects. Each detected object is represented by its bounding box coordinates and
    its class.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**头部**：这是模型的最终组件，输出检测到的物体。每个检测到的物体通过其边界框坐标和类别来表示。'
- en: 'With that, we have gained a bird’s-eye view of the YOLO architecture. But it
    doesn’t answer some inconvenient (yet intriguing) questions, such as how the model
    detects multiple objects on the same image, or what happens if two or more objects
    overlap and one is only partially visible. To find the answers to these questions,
    let’s introduce the following diagram, which consists of two overlapping objects:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些步骤，我们已经获得了YOLO架构的概览。但这并没有回答一些不太方便（但又令人好奇）的问题，例如模型如何在同一图像上检测多个物体，或者当两个或更多物体重叠且其中一个仅部分可见时会发生什么。为了找到这些问题的答案，我们引入下面的示意图，其中包含两个重叠的物体：
- en: '![Figure 5.5 – An object detection YOLO example with two overlapping objects
    and their bounding boxes](img/B19627_05_5.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图5.5 – 一个物体检测YOLO示例，包含两个重叠的物体及其边界框](img/B19627_05_5.jpg)'
- en: Figure 5.5 – An object detection YOLO example with two overlapping objects and
    their bounding boxes
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 – 一个物体检测YOLO示例，包含两个重叠的物体及其边界框
- en: 'These are the steps that YOLO implements to detect them:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这是YOLO实现物体检测的步骤：
- en: 'Split the input image into a grid of *S×S* cells (the preceding diagram uses
    a 3×3 grid):'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入图像分割成*S×S*个单元格（前面的示意图使用了一个3×3的网格）：
- en: The center of a cell represents the center of a region where an object might
    be located.
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个单元格的中心代表一个区域的中心，该区域可能包含一个物体。
- en: The model can detect both objects that span multiple cells and ones that lie
    entirely within the cell. Each object is associated with a single cell, even if
    it covers multiple cells. In this case, we’ll associate the object with the cell,
    where the center of its bounding box lies. For example, the two objects in the
    diagram span multiple cells, but they are both assigned to the central cell because
    their centers lie in it.
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型可以检测跨越多个单元格的物体，也可以检测完全位于单元格内的物体。每个物体都与一个单元格相关联，即使它跨越了多个单元格。在这种情况下，我们将物体与其边界框中心所在的单元格关联。例如，图中的两个物体跨越了多个单元格，但它们都分配给中央单元格，因为它们的中心位于其中。
- en: A cell can contain multiple objects (*1-to-n* relationship) or no objects at
    all. We’re only interested in the cells with objects.
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个单元格可以包含多个物体（*1对n*关系）或完全没有物体。我们只关注包含物体的单元格。
- en: 'The model outputs multiple possible detected objects for each grid cell. Each
    detected object is represented by the following array of values: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/320.png).
    Let’s discuss them:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该模型为每个网格单元输出多个可能的检测物体。每个检测物体由以下值数组表示：![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/320.png)。我们来讨论它们：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/321.png)
    describes the object bounding box. ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/322.png)
    are the coordinates of the center of the box concerning the whole image. They
    are normalized in the [0, 1] range. For example, if the image size is 100×100
    and the center of the bounding box is located at [40, 70], then'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/321.png)
    描述了物体的边界框。![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/322.png)
    是边界框中心相对于整张图像的坐标。它们被归一化到[0, 1]的范围内。例如，如果图像大小为100×100，且边界框中心位于[40, 70]的位置，那么'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mn>0.4,0.7</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/323.png).
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/324.png)
    represent the normalized bounding box height and width concerning the whole image.
    If the bounding box’s size is 80×50, then ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mn>0.8,0.5</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/325.png)
    for the same 100×100 image. In practice, a YOLO implementation usually includes
    helper methods, which will allow us to obtain the absolute coordinates of the
    bounding boxes.'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mn>0.4,0.7</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/323.png).
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/324.png)
    表示相对于整个图像的归一化边界框高度和宽度。如果边界框的尺寸是 80×50，那么 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mn>0.8,0.5</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/325.png)
    适用于相同的 100×100 图像。在实际操作中，YOLO 实现通常包括帮助方法，允许我们获取边界框的绝对坐标。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math>](img/326.png)
    is an objectness score, which represents the confidence of the model (in the [0,
    1] range) that an object is present in the cell. If ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math>](img/327.png)
    is closer to 1, then the model is confident that an object is present and vice
    versa.'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math>](img/326.png)
    是一个物体性分数，表示模型对单元格中是否存在物体的置信度（范围为 [0, 1]）。如果 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math>](img/327.png)
    趋近于 1，则表示模型确信单元格中存在物体，反之亦然。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/328.png)
    is a one-hot encoding of the class of the detected object. For example, if we
    have bicycle, flower, person, and fish classes, and the current object is a person,
    its encoding will be [0, 0, 1, 0].'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/328.png)
    是检测到的物体类别的独热编码。例如，如果我们有自行车、花、人物和鱼类，并且当前物体是人物，则它的编码将是 [0, 0, 1, 0]。'
- en: 'So far, we’ve demonstrated that the model can detect multiple objects on the
    same image. Next, let’s focus on the trickier case with multiple objects in the
    same cell. YOLO has an elegant solution to this problem in the form of **anchor
    boxes** (also known as **priors**). To understand this concept, we’ll start with
    the following diagram, which shows the grid cell (square, uninterrupted line)
    and two anchor boxes – vertical and horizontal (dashed lines):'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经展示了模型可以检测同一图像上的多个物体。接下来，让我们聚焦于一个更复杂的情况——同一单元格中有多个物体。YOLO在这个问题上提供了一个优雅的解决方案——**锚框**（也称为**先验框**）。为了理解这个概念，我们从下图开始，图中展示了网格单元（方形，实线）和两个锚框——垂直和水平（虚线）：
- en: '![Figure 5.6 – A grid cell (a square, uninterrupted line) with two anchor boxes
    (dashed lines)](img/B19627_05_6.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – 一个网格单元（方形，实线）与两个锚框（虚线）](img/B19627_05_6.jpg)'
- en: Figure 5.6 – A grid cell (a square, uninterrupted line) with two anchor boxes
    (dashed lines)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – 一个网格单元（方形，实线）与两个锚框（虚线）
- en: For each cell, we’ll have multiple candidate anchor boxes with different scales
    and aspect ratios. If we have multiple objects in the same cell, we’ll associate
    each object with a single anchor box. If an anchor box doesn’t have an associated
    object, it will have an objectness score of zero (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/329.png)).
    We can detect as many objects as there are anchor boxes per cell. For example,
    our example 3×3 grid with two anchor boxes per cell can detect a total of 3*3*2
    = 18 objects. Because we have a fixed number of cells (*S×S*) and a fixed number
    of anchor boxes per cell, the size of the network output doesn’t change with the
    number of detected objects. Instead, we’ll output results for all possible anchor
    boxes, but we’ll only consider the ones with an objectness score of ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mn>1</mml:mn></mml:math>](img/330.png).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个单元格，我们会有多个候选锚框，具有不同的尺度和纵横比。如果同一单元格内有多个物体，我们将每个物体与一个单独的锚框关联。如果某个锚框没有关联物体，它的物体性得分将为零
    (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/329.png))。我们可以检测到每个单元格内的锚框数量相等的物体。例如，我们的
    3×3 网格，每个单元格有两个锚框，可以检测到总共 3*3*2 = 18 个物体。因为我们有固定数量的单元格 (*S×S*) 和每个单元格固定数量的锚框，网络输出的大小不会随着检测到的物体数量而变化。相反，我们会输出所有可能锚框的结果，但我们只会考虑那些物体性得分为![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mn>1</mml:mn></mml:math>](img/330.png)的锚框。
- en: 'The YOLO algorithm uses the **Intersection over Union** (**IoU**) technique
    both during training and inference to improve its performance:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: YOLO算法在训练和推理过程中都使用**交并比**（**IoU**）技术来提高性能：
- en: '![Figure 5.7 – Intersection over Union (IoU)](img/B19627_05_7.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7 – 交并比（IoU）](img/B19627_05_7.jpg)'
- en: Figure 5.7 – Intersection over Union (IoU)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 – 交并比（IoU）
- en: IoU is the ratio between the area of the intersection and the area of the union
    of the detected object bounding box and the ground truth (or another object’s)
    bounding box.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: IoU是检测到的对象的边界框与真实标签（或其他对象）的边界框交集的面积与并集的面积之比。
- en: During training, we can compute the IoU between the anchor boxes and ground
    truth bounding boxes. Then, we can assign each ground truth object to its highest
    overlapping anchor box to generate labeled training data. In addition, we can
    compute the IoU between the detected bounding box and the ground truth (label)
    box. The higher value of IoU indicates a better overlap between ground truth and
    prediction. This can help us evaluate the detector.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们可以计算锚框与真实框之间的IoU。然后，我们可以将每个真实对象分配给其与之重叠度最高的锚框，从而生成标注的训练数据。此外，我们还可以计算检测到的边界框与真实框（标签框）之间的IoU。IoU值越高，表示真实值与预测值的重叠越好。这可以帮助我们评估检测器。
- en: 'During inference, the output of the model includes all possible anchor boxes
    for each cell, regardless of whether an object is present in them. Many of the
    boxes will overlap and predict the same object. We can filter the overlapping
    objects with the help of IoU and **non-maximum suppression** (**NMS**). Here’s
    how it works:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，模型的输出包括每个单元格的所有可能的锚框，无论其中是否存在对象。许多框会重叠并预测相同的对象。我们可以通过IoU和**非最大抑制**（**NMS**）来过滤这些重叠的对象。它是如何工作的：
- en: Discard all bounding boxes with an objectness score of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo><</mml:mo><mml:mn>0.6</mml:mn></mml:math>](img/331.png).
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 丢弃所有对象性分数低于![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo><</mml:mo><mml:mn>0.6</mml:mn></mml:math>](img/331.png)的边界框。
- en: Pick the box with the highest objectness score, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math>](img/326.png),
    from the remaining boxes.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择剩余框中具有最高对象性分数的框，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math>](img/326.png)。
- en: Discard all boxes with IoU >= 0.5 with the box we selected in the previous step.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 丢弃所有与我们在上一步中选择的框的IoU >= 0.5的框。
- en: Now that we are (hopefully) familiar with YOLO, let’s learn how to use it in
    practice.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们（希望）已经熟悉YOLO，接下来让我们学习如何在实际中使用它。
- en: Using Ultralytics YOLOv8
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Ultralytics YOLOv8
- en: 'In this section, we’ll demonstrate how to use the YOLOv8 algorithm, developed
    by Ultralytics. For this example, you’ll need to install the `ultralytics` Python
    package. Let’s start:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示如何使用由Ultralytics开发的YOLOv8算法。对于此示例，您需要安装`ultralytics` Python包。让我们开始：
- en: 'Import the YOLO module. We’ll load a pre-trained YOLOv8 model:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入YOLO模块。我们将加载一个预训练的YOLOv8模型：
- en: '[PRE12]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Use `model` to detect the objects on a Wikipedia image:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`model`在Wikipedia图片上检测对象：
- en: '[PRE13]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`results` is a list, composed of a single instance of the `ultralytics.yolo.engine.results.Results`
    class. The instance contains the list of detected objects: their bounding boxes,
    classes, and objectness scores.'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`results`是一个列表，包含一个`ultralytics.yolo.engine.results.Results`类的实例。该实例包含检测到的对象列表：它们的边界框、类别和对象性分数。'
- en: 'We can display the results with the help of the `results[0].plot()` method,
    which overlays the detected object on the input image. The result of this operation
    is the first image, we introduced at the start of the *Introduction to object*
    *detection* section:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过`results[0].plot()`方法来显示结果，它会将检测到的对象叠加在输入图像上。这个操作的结果就是我们在*目标检测简介*部分开始时展示的第一张图像：
- en: '[PRE14]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This concludes our introduction to the YOLO family of single-shot object detection
    models. Next, we’ll focus on a popular example of a two-shot detection algorithm.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对YOLO系列单次检测模型的介绍。接下来，我们将重点讲解一个流行的两次检测算法的示例。
- en: Object detection with Faster R-CNN
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Faster R-CNN进行目标检测
- en: 'In this section, we’ll discuss the **Faster R-CNN** (*Faster R-CNN: Towards
    Real-Time Object Detection with Region Proposal Networks*, [https://arxiv.org/abs/1506.01497](https://arxiv.org/abs/1506.01497))
    two-stage object detection algorithm. It is an evolution of the earlier two-stage
    detectors, **Fast R-CNN** (*Fast R-CNN*, [https://arxiv.org/abs/1504.08083](https://arxiv.org/abs/1504.08083))
    and **R-CNN** (*Rich feature hierarchies for accurate object detection and semantic*
    *segmentation*, [https://arxiv.org/abs/1311.2524](https://arxiv.org/abs/1311.2524)).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将讨论**Faster R-CNN**（*Faster R-CNN: Towards Real-Time Object Detection
    with Region Proposal Networks*，[https://arxiv.org/abs/1506.01497](https://arxiv.org/abs/1506.01497)）的两阶段目标检测算法。它是早期两阶段检测器的演变，**Fast
    R-CNN**（*Fast R-CNN*，[https://arxiv.org/abs/1504.08083](https://arxiv.org/abs/1504.08083)）和**R-CNN**（*Rich
    feature hierarchies for accurate object detection and semantic* *segmentation*，[https://arxiv.org/abs/1311.2524](https://arxiv.org/abs/1311.2524)）。'
- en: 'The general structure of the Faster R-CNN model is outlined in the following
    diagram:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN模型的一般结构在下图中概述：
- en: '![Figure 5.8 – The structure of Faster R-CNN. Source: https://arxiv.org/abs/1506.01497](img/B19627_05_8.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图5.8 – Faster R-CNN的结构。来源：[https://arxiv.org/abs/1506.01497](https://arxiv.org/abs/1506.01497)](img/B19627_05_8.jpg)'
- en: 'Figure 5.8 – The structure of Faster R-CNN. Source: [https://arxiv.org/abs/1506.01497](https://arxiv.org/abs/1506.01497)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 – Faster R-CNN的结构。来源：[https://arxiv.org/abs/1506.01497](https://arxiv.org/abs/1506.01497)
- en: Let’s keep this figure in mind while we explain the algorithm. Like YOLO, Faster
    R-CNN starts with a backbone classification network trained on ImageNet, which
    serves as a base for the different modules of the model. Originally, the authors
    of the paper experimented with classic backbone architectures, such as **VGG-16**
    (*Very Deep Convolutional Networks for Large-Scale Image Recognition*, [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556))
    and **ZFNet** (*Visualizing and Understanding Convolutional Networks*, [https://arxiv.org/abs/1311.2901](https://arxiv.org/abs/1311.2901)).
    Today, the model is available with more contemporary backbones, such as ResNet
    and MobileNet.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释算法时，我们要记住这个图。像YOLO一样，Faster R-CNN首先使用一个在ImageNet上训练的主干分类网络，它作为模型不同模块的基础。最初，论文的作者尝试了经典的主干架构，如**VGG-16**（*Very
    Deep Convolutional Networks for Large-Scale Image Recognition*，[https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)）和**ZFNet**（*Visualizing
    and Understanding Convolutional Networks*，[https://arxiv.org/abs/1311.2901](https://arxiv.org/abs/1311.2901)）。如今，该模型已提供更多现代化的主干，如ResNet和MobileNet。
- en: Unlike YOLO, Faster R-CNN doesn’t have a neck module and only uses the feature
    maps of the last backbone convolutional layer as input to the next components
    of the algorithm. More specifically, the backbone serves as a backbone (get it?)
    to the two other components of the model (hence two-stage) – the **region proposal
    network** (**RPN**) and the detection network. Let’s discuss the RPN first.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 与YOLO不同，Faster R-CNN没有颈部模块，只使用最后一个主干卷积层的特征图作为输入，供算法的下一个组件使用。更具体地说，主干网络作为模型其他两个组件（因此是两阶段）的支撑——**区域提议网络**（**RPN**）和检测网络。我们先来讨论一下RPN。
- en: The region proposal network
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 区域提议网络
- en: 'In the first stage, the RPN takes an image (of any size) as input and outputs
    a set of rectangular RoI, where an object might be located. The RoI is equivalent
    to the bounding box in YOLO. The RPN itself is created by taking the first *p*
    convolutional layers of the backbone model (see the preceding diagram). Once the
    input image is propagated to the last shared convolutional layer, the algorithm
    takes the feature map of that layer and slides another small network over each
    location of the feature map. The small network outputs whether an object is present
    at any of the *k* anchor boxes (the concept of anchor box is the same as in YOLO),
    as well as the coordinates of its potential bounding box. This is illustrated
    on the left-hand side image of the following diagram, which shows a single location
    of the RPN sliding over a single feature map of the last convolutional layer:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一阶段，RPN将图像（任意大小）作为输入，并输出一组矩形RoI，表示可能存在物体的位置。RoI相当于YOLO中的边界框。RPN本身是通过采用主干模型的前*p*个卷积层（参见前面的图示）来创建的。一旦输入图像传播到最后一个共享的卷积层，算法就会取该层的特征图，并在特征图的每个位置滑动另一个小型网络。这个小型网络输出是否在任意一个*k*个锚框中存在物体（锚框的概念与YOLO中的相同），以及其潜在的边界框坐标。下图左侧的图像展示了RPN在最后一个卷积层的特征图上滑动的一个位置：
- en: "![Figure 5.9 – \uFEFFRPN proposals over a single location\uFEFF. Source: https://arxiv.org/abs/1506.01497](img/B19627_05_9.jpg)"
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图5.9 – 单一位置的RPN提议。来源：https://arxiv.org/abs/1506.01497](img/B19627_05_9.jpg)'
- en: 'Figure 5.9 – RPN proposals over a single location. Source: https://arxiv.org/abs/1506.01497'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 – 单一位置的RPN提议。来源：https://arxiv.org/abs/1506.01497
- en: The small network takes an *n×n* region at the same location across all input
    feature maps as input
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 小型网络将跨所有输入特征图，在相同位置的*n×n*区域作为输入。
- en: '(*n = 3* according to the paper). For example, if the final convolutional layer
    has 512 feature maps, the small network’s input size at one location is 512*3*3
    = 4608\. The 512 3×3 feature maps are flattened to a 4,608-dimensional vector.
    It serves as input to a fully connected layer, which maps it to a lower dimensional
    (usually 512) vector. This vector itself serves as input to the following two
    parallel fully connected layers:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: （*n = 3*，根据论文）。例如，如果最终卷积层有512个特征图，那么在某个位置上，小型网络的输入大小为512*3*3 = 4608。512个3×3的特征图被展平为一个4608维的向量。这个向量作为输入传递给一个全连接层，该层将其映射到一个较低维度（通常为512）的向量。这个向量本身作为输入传递给接下来的两个并行的全连接层：
- en: A classification layer with *2k* units organized into *k* 2-unit binary softmax
    outputs. Like YOLO, the output of each softmax represents the objectness score
    (in the [0, 1] range) of whether an object exists in each of the *k* anchor boxes.
    During training, an object is assigned to an anchor box based on the IoU formula
    in the same way as in YOLO.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有*2k*单元的分类层，这些单元组织成*k*个2单元的二元softmax输出。像YOLO一样，每个softmax的输出表示在每个*k*个锚框中是否存在物体的目标性分数（在[0,
    1]范围内）。在训练过程中，物体是根据IoU公式分配给锚框的，和YOLO中的方式一样。
- en: A regression layer with *4k* units organized into *k* 4-unit RoI arrays. Like
    YOLO, the first array elements represent the coordinates of the RoI center in
    the [0:1] range relative to the whole image. The other two elements represent
    the height and width of the region, relative to the whole image (again, similar
    to YOLO).
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个回归层，具有*4k*单元，组织成*k*个4单元的RoI数组。像YOLO一样，第一个数组元素表示RoI中心的坐标，范围为[0:1]，相对于整个图像。其他两个元素表示区域的高度和宽度，相对于整个图像（再次类似YOLO）。
- en: The authors of the paper experimented with three scales and three aspect ratios,
    resulting in nine possible anchor boxes over each location. The typical *H×W*
    size of the final feature map is around 2,400, which results in 2,400*9 = 21,600
    anchor boxes.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者实验了三种尺度和三种长宽比，结果在每个位置上得到了九种可能的锚框。最终特征图的典型*H×W*大小大约是2,400，这样就得到了2,400*9
    = 21,600个锚框。
- en: RPN as a cross-channel convolution
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: RPN作为跨通道卷积
- en: In theory, we slide the small network over the feature map of the last convolutional
    layer. However, the small network weights are shared along all locations. Because
    of this, the sliding can be implemented as a cross-channel convolution. Therefore,
    the network can produce output for all anchor boxes in a single image pass. This
    is an improvement over Fast R-CNN, which requires a separate network pass for
    each anchor box.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，我们将小型网络滑动到最后一层卷积的特征图上。然而，小型网络的权重在所有位置之间共享。因为这个原因，滑动可以被实现为跨通道卷积。因此，网络可以在一次图像传递中为所有锚框生成输出。这是对Fast
    R-CNN的改进，后者需要为每个锚框单独进行网络传递。
- en: 'The RPN is trained with backpropagation and stochastic gradient descent (what
    a surprise!). The weights of the shared convolutional layers are initialized with
    the pre-trained weights of the backbone network and the rest are initialized randomly.
    The samples of each mini-batch are extracted from a single image. Each mini-batch
    contains an equal number of positive (objects) and negative (background) anchor
    boxes. There are two kinds of anchors with positive labels: the anchor/anchors
    with the highest IoU overlap with a ground truth box and an anchor that has an
    IoU overlap of higher than 0.7 with any ground truth box. If the IoU ratio of
    an anchor is lower than 0.3, the box is assigned a negative label. Anchors that
    are neither positive nor negative do not participate in the training.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: RPN通过反向传播和随机梯度下降进行训练（真是令人惊讶！）。共享卷积层的权重使用预训练的骨干网络权重进行初始化，其余的权重则是随机初始化的。每个小批量的样本都从一张图片中提取。每个小批量包含相同数量的正样本（物体）和负样本（背景）锚框。有两种正标签的锚框：与真实框IoU重叠最高的锚框，和与任何真实框的IoU重叠超过0.7的锚框。如果锚框的IoU比率低于0.3，则该框被分配为负标签。既不是正标签也不是负标签的锚框不会参与训练。
- en: 'As the RPN has two output layers (classification and regression), the training
    uses the following composite cost function with classification (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math>](img/333.png))
    and regression (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math>](img/334.png))
    parts:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 由于RPN具有两个输出层（分类和回归），因此训练使用以下复合代价函数，其中包含分类 (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math>](img/333.png))
    和回归 (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math>](img/334.png))
    部分：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>L</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfenced
    open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced
    open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal"> </mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>+</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>λ</mml:mi><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal"> </mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/335.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>L</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfenced
    open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced
    open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal"> </mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>+</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>λ</mml:mi><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal"> </mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/335.png)'
- en: 'Let’s discuss its components:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论它的组成部分：
- en: '*i*: The index of the anchor in the mini-batch.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*i*：小批量中锚点的索引。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/336.png):
    The classification output, which represents the predicted objectness score of
    an anchor, *i*, being an object or background. ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/337.png)
    is the target data for the same (0 or 1).'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/336.png)：分类输出，表示锚点
    *i* 是物体还是背景的预测物体性得分。![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/337.png)
    是相同目标的实际数据（0 或 1）。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/338.png):
    The regression output vector with size 4, which represents the RoI parameters.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/338.png)：回归输出向量，大小为
    4，表示 RoI 参数。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/339.png):
    The target vector for the same.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/339.png)：相同目标的目标向量。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math>](img/333.png):
    A cross-entropy loss for the classification layer.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math>](img/333.png)：分类层的交叉熵损失。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math>](img/341.png):
    A normalization term, equal to the mini-batch size.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math>](img/341.png)：归一化项，等于小批量大小。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math>](img/342.png):
    The regression loss, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math>](img/343.png),
    where *R* is the mean absolute error ([https://en.wikipedia.org/wiki/Mean_absolute_error](https://en.wikipedia.org/wiki/Mean_absolute_error)).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math>](img/342.png)：回归损失，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math>](img/343.png)，其中
    *R* 是平均绝对误差（[https://en.wikipedia.org/wiki/Mean_absolute_error](https://en.wikipedia.org/wiki/Mean_absolute_error)）。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math>](img/344.png):
    A normalization term equal to the total number of anchor locations (around 2400).'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math>](img/344.png)：一个归一化项，等于锚点位置的总数（大约
    2400）。'
- en: '*λ*: This helps combine the classification and regression components of the
    cost function. Since ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mn>2400</mml:mn></mml:math>](img/345.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>256</mml:mn></mml:math>](img/346.png),
    *λ* is set to 10 to preserve the balance between the two losses.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*λ*：这有助于将分类和回归组件结合到代价函数中。由于![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mn>2400</mml:mn></mml:math>](img/345.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>256</mml:mn></mml:math>](img/346.png)，*λ*
    被设置为 10，以保持两者损失的平衡。'
- en: Now that we’ve discussed the RPN, let’s focus on the detection network.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了 RPN，让我们集中注意力于检测网络。
- en: Detection network
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检测网络
- en: Let’s go back to the diagram that was shown at the beginning of the *Object
    detection with Faster R-CNN* section. Recall that in the first stage, the RPN
    has already generated the RoI coordinates and their objectness scores. The detection
    network is a regular classifier, which determines the class of objects in the
    current RoI. Both the RPN and the detection network share their first convolutional
    layers, borrowed from the backbone network. In addition, the detection network
    incorporates the proposed regions from the RPN, along with the feature maps of
    the last shared layer.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到在*基于 Faster R-CNN 的目标检测*部分开头展示的图示。回想一下，在第一阶段，RPN 已经生成了 RoI 坐标及其目标性分数。检测网络是一个常规分类器，用于确定当前
    RoI 中物体的类别。RPN 和检测网络共享它们的第一卷积层，这些层借用了背骨网络。此外，检测网络还整合了来自 RPN 的提议区域以及最后共享层的特征图。
- en: 'But how do we combine the backbone feature maps and the proposed regions in
    a unified input format? We can do this with the help of **RoI pooling**, which
    is the first layer of the second part of the detection network:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我们如何将背骨特征图和提议的区域以统一的输入格式结合起来呢？我们可以通过**RoI 池化**来实现，这也是检测网络第二部分的第一层：
- en: '![Figure 5.10 – An example of 2×2 RoI pooling with a 10×7 feature map and a
    5×5 RoI (bold rectangle)](img/B19627_05_10.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10 – 一个 2×2 RoI 池化示例，使用 10×7 的特征图和一个 5×5 的 RoI（粗体矩形）](img/B19627_05_10.jpg)'
- en: Figure 5.10 – An example of 2×2 RoI pooling with a 10×7 feature map and a 5×5
    RoI (bold rectangle)
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – 一个 2×2 RoI 池化示例，使用 10×7 的特征图和一个 5×5 的 RoI（粗体矩形）
- en: To understand how RoI pooling works, let’s assume that we have a single 10×7
    feature map and a single RoI. As we learned in the *Region proposal network* section,
    a RoI is defined by its center coordinates, width, and height. The RoI pooling
    first converts these parameters into actual coordinates on the feature map. In
    this example, the region size is *h×w = 5×5*. The RoI pooling is further defined
    by its output height and width, *H* and *W*. In this example, *H×W = 2×2*, but
    in practice, the values could be larger, such as 7×7\. The operation splits the
    *h×w* RoI into a grid of subregions with different sizes (displayed in the figure
    with different background colors). Once this is done, each subregion is downsampled
    to a single output cell by taking the maximum value of that region. In other words,
    RoI pooling can transform inputs with arbitrary sizes into a fixed-size output
    window. In this way, the transformed data can propagate through the network in
    a consistent format.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解 RoI 池化的工作原理，假设我们有一个 10×7 的特征图和一个 RoI。如同在*区域提议网络*部分中所学，RoI 是由其中心坐标、宽度和高度定义的。RoI
    池化首先将这些参数转换为特征图上的实际坐标。在这个示例中，区域大小是 *h×w = 5×5*。RoI 池化进一步通过其输出的高度和宽度 *H* 和 *W*
    来定义。在这个例子中，*H×W = 2×2*，但在实际中，这些值可能更大，比如 7×7。该操作将 *h×w* 的 RoI 分割成大小不同的子区域网格（图中通过不同的背景颜色显示）。完成此操作后，每个子区域通过获取该区域的最大值来下采样为单个输出单元。换句话说，RoI
    池化可以将任意大小的输入转换为固定大小的输出窗口。这样，转换后的数据可以以一致的格式通过网络传播。
- en: 'As we mentioned in the *Object detection with Faster R-CNN* section, the RPN
    and the detection network share their initial layers. However, they start their
    lives as separate networks. The training alternates between the two in a four-step
    process:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*基于 Faster R-CNN 的目标检测*部分中提到的，RPN 和检测网络共享它们的初始层。然而，它们一开始是作为独立的网络存在的。训练在两者之间交替进行，采用四步过程：
- en: Train the RPN, which is initialized with the ImageNet weights of the backbone.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练 RPN，它使用背骨的 ImageNet 权重进行初始化。
- en: Train the detection network, using the proposals from the freshly trained RPN
    from *step 1*. The training also starts with the weights of the ImageNet backbone.
    At this point, the two networks don’t share weights.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练检测网络，使用来自*步骤 1* 中刚训练好的 RPN 的提议。训练也从 ImageNet 背骨网络的权重开始。此时，两个网络并不共享权重。
- en: Use the detection network shared layers to initialize the weights of the RPN.
    Then, train the RPN again, but freeze the shared layers and fine-tune the RPN-specific
    layers only. The two networks share their weights now.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用检测网络共享的层来初始化 RPN 的权重。然后，再次训练 RPN，但冻结共享层，只微调特定于 RPN 的层。现在，两个网络共享它们的权重。
- en: Train the detection network by freezing the shared layers and fine-tuning the
    detection-net-specific layers only.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过冻结共享层并仅微调特定于检测网络的层来训练检测网络。
- en: Now that we’ve introduced Faster R-CNN, let’s discuss how to use it in practice
    with the help of a pre-trained PyTorch model.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了Faster R-CNN，接下来让我们讨论如何使用预训练的PyTorch模型来实际应用它。
- en: Using Faster R-CNN with PyTorch
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用PyTorch进行Faster R-CNN
- en: 'In this section, we’ll use a pre-trained PyTorch Faster R-CNN model with a
    ResNet50 backbone for object detection. PyTorch has out-of-the-box support for
    Faster R-CNN, which makes it easy for us to use. This example is implemented with
    PyTorch. In addition, it uses the `torchvision` and `opencv-python` packages.
    We will only include the relevant parts of the code, but you can find the full
    version in this book’s GitHub repository. Let’s start:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用一个带有ResNet50骨干网的预训练PyTorch Faster R-CNN模型进行物体检测。PyTorch原生支持Faster
    R-CNN，这使得我们很容易使用它。本示例已使用PyTorch实现。此外，它还使用了`torchvision`和`opencv-python`包。我们只会包括代码的相关部分，但你可以在本书的GitHub仓库中找到完整版本。让我们开始：
- en: 'Load the pre-trained model with the latest available weights. Ensure this by
    using the `DEFAULT` option:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最新的权重加载预训练模型。确保使用`DEFAULT`选项：
- en: '[PRE15]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We are going to use the model for inference and not for training, so we’ll
    enable the `eval()` mode:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用模型进行推断而不是训练，因此我们将启用`eval()`模式：
- en: '[PRE16]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Use `opencv-python` to read the RGB image located at `image_file_path`. We’ll
    omit the code, which downloads the image from this book’s repository if it doesn’t
    already exist locally:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`opencv-python`读取位于`image_file_path`的RGB图像。如果图像文件在本地不存在，我们会省略从本书仓库下载图像的代码：
- en: '[PRE17]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here, `img` is a three-dimensional `numpy` array of integers.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，`img`是一个三维的`numpy`整数数组。
- en: 'Implement the single-step image pre-processing pipeline. It transforms the
    `img` `numpy` array into `torch.Tensor`, which will serve as input to the model:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现单步图像预处理管道。它将`img` `numpy`数组转换为`torch.Tensor`，该Tensor将作为模型的输入：
- en: '[PRE18]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Run the detection model:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行检测模型：
- en: '[PRE19]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here, `detected_objects` is a dictionary with three items:'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，`detected_objects`是一个包含三个项目的字典：
- en: '`boxes`: A list of bounding boxes, represented by their top-left and bottom-right
    pixel coordinates'
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boxes`：一个边界框的列表，由它们的左上角和右下角像素坐标表示'
- en: '`labels`: A list of labels for each detected object'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`：每个检测到的物体的标签列表'
- en: '`scores`: A list of objectness scores for each detected object'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores`：每个检测到的物体的物体性得分列表'
- en: 'Use the initial `img` array and `detected_objects` as parameters for the `draw_bboxes`
    function, which overlays the bounding boxes and their labels on the original input
    image (the implementation of `draw_bboxes` is available in the full example):'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用初始的`img`数组和`detected_objects`作为`draw_bboxes`函数的参数，该函数会在原始输入图像上叠加边界框和它们的标签（`draw_bboxes`的实现可以在完整示例中找到）：
- en: '[PRE20]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Display the result with `opencv-python`:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`opencv-python`显示结果：
- en: '[PRE21]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output image looks like this:'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出图像如下所示：
- en: '![Figure 5.11 – Object detection with Faster R-CNN](img/B19627_05_11.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.11 – 使用Faster R-CNN进行物体检测](img/B19627_05_11.jpg)'
- en: Figure 5.11 – Object detection with Faster R-CNN
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – 使用Faster R-CNN进行物体检测
- en: We’re now familiar with two of the most popular object detection algorithms.
    In the next section, we’ll focus on the next major computer vision task, called
    **image segmentation**.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经熟悉了两种最流行的物体检测算法。在下一部分，我们将专注于下一个主要的计算机视觉任务，称为**图像分割**。
- en: Introducing image segmentation
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍图像分割
- en: 'Image segmentation is the process of assigning a class label (such as person,
    bicycle, or animal) to each pixel of an image. You can think of it as classification
    but on a pixel level – instead of classifying the entire image under one label,
    we’ll classify each pixel separately. The output of an image segmentation operation
    is known as a **segmentation mask**. It is a tensor with the same dimensions as
    the original input image, but instead of color, each pixel is represented by the
    class of object, to which it belongs. There are two types of segmentation:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割是将类标签（如人、自行车或动物）分配给图像中每个像素的过程。你可以将其视为像素级别的分类——而不是将整个图像分类为一个标签，我们会分别对每个像素进行分类。图像分割操作的输出被称为**分割掩码**。它是一个与原始输入图像具有相同维度的Tensor，但每个像素不是用颜色表示，而是用它所属于的物体类别来表示。图像分割有两种类型：
- en: '**Semantic segmentation**: This assigns a class to each pixel but doesn’t differentiate
    between object instances. For example, the middle image in the following figure
    shows a semantic segmentation mask, where the pixels of each separate vehicle
    have the same value. Semantic segmentation can tell us that a pixel is part of
    a vehicle but cannot make a distinction between two vehicles.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义分割**：这种方法为每个像素分配一个类别，但不会区分物体实例。例如，下面图中的中间图像展示了一种语义分割掩码，其中每辆车的像素值是相同的。语义分割可以告诉我们某个像素属于某个物体，但不能区分不同的物体。'
- en: '**Instance segmentation**: This assigns a class to each pixel and differentiates
    between object instances. For example, the image on the right in the following
    figure shows an instance segmentation mask, where each vehicle is segmented as
    a separate object.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实例分割**：这种方法为每个像素分配一个类别，并区分不同的物体实例。例如，下面图中的右侧展示了一种实例分割掩码，每辆车都被分割为独立的物体。'
- en: 'The following figure shows an example of semantic and instance segmentation:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了语义分割和实例分割的例子：
- en: '![Figure 5.12 – Left: input image; middle: semantic segmentation mask; right:
    instance segmentation mask. Source: http://sceneparsing.csail.mit.edu/](img/B19627_05_12.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图5.12 – 左：输入图像；中：语义分割掩码；右：实例分割掩码。来源：http://sceneparsing.csail.mit.edu/](img/B19627_05_12.jpg)'
- en: 'Figure 5.12 – Left: input image; middle: semantic segmentation mask; right:
    instance segmentation mask. Source: http://sceneparsing.csail.mit.edu/'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 – 左：输入图像；中：语义分割掩码；右：实例分割掩码。来源：http://sceneparsing.csail.mit.edu/
- en: To train a segmentation algorithm, we’ll need a special type of ground truth
    data, where the labels of each image are the segmented version of the image.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练分割算法，我们需要一种特殊类型的真实数据，其中每张图像的标签是图像的分割版本。
- en: The easiest way to segment an image is by using the familiar sliding-window
    technique, which we described in the *Approaches to object detection* section
    – that is, we’ll use a regular classifier, and we’ll slide it in either direction
    with stride 1\. After we get the prediction for a location, we’ll take the pixel
    that lies in the middle of the input region, and we’ll assign it to the predicted
    class. Predictably, this approach is very slow because of the large number of
    pixels in an image (even a 1,024×1,024 image has more than 1 million pixels).
    Thankfully, there are faster and more accurate algorithms, which we’ll discuss
    in the following sections.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 分割图像最简单的方法是使用我们在*物体检测方法*部分中描述的常见滑动窗口技术——即，我们使用一个常规分类器，并以步幅1在任一方向上滑动它。当我们得到某个位置的预测时，我们将取位于输入区域中心的像素，并将其分配给预测的类别。可以预见，这种方法非常慢，因为图像中像素的数量非常庞大（即便是1,024×1,024的图像，也有超过100万个像素）。幸运的是，还有更快速和更准确的算法，我们将在接下来的部分中讨论。
- en: Semantic segmentation with U-Net
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用U-Net的语义分割
- en: 'The first approach to segmentation we’ll discuss is called **U-Net** (*U-Net:
    Convolutional Networks for Biomedical Image Segmentation*, [https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)).
    The name comes from the visualization of the network architecture:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的第一个分割方法被称为**U-Net**（*U-Net：用于生物医学图像分割的卷积网络*，[https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)）。这个名称来源于网络架构的可视化：
- en: '![Figure 5.13 – The U-Net architecture. Source: https://arxiv.org/abs/1505.04597](img/B19627_05_13.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图5.13 – U-Net架构。来源：https://arxiv.org/abs/1505.04597](img/B19627_05_13.jpg)'
- en: 'Figure 5.13 – The U-Net architecture. Source: https://arxiv.org/abs/1505.04597'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 – U-Net架构。来源：https://arxiv.org/abs/1505.04597
- en: 'U-Net is a type of **fully convolutional network** (**FCN**), called so because
    it contains only convolutional layers and doesn’t use any fully connected layers
    at its output. An FCN takes the whole image as input and outputs its segmentation
    map in a single pass. To better understand this architecture, let’s clarify the
    figure notations first:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net是一种**全卷积网络**（**FCN**），之所以如此命名，是因为它仅包含卷积层，并且在输出端不使用任何全连接层。FCN将整个图像作为输入，并在一次传递中输出其分割图。为了更好地理解这个架构，我们首先来澄清一下图示符号：
- en: The horizontal dark blue arrows correspond to 3×3 cross-channel convolutions
    with ReLU activation. The single light blue arrow at the end of the model represents
    a 1×1 bottleneck convolution to reduce the number of channels.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平深蓝色箭头表示3×3的跨通道卷积，并应用ReLU激活函数。模型末端的单一浅蓝色箭头代表1×1的瓶颈卷积，用于减少通道数。
- en: All feature maps are denoted with blue boxes. The number of feature maps is
    on top of the box, and the feature map’s size is at the lower-left edge of the
    box.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有特征图都用蓝色框表示。特征图的数量显示在框的顶部，特征图的大小显示在框的左下角。
- en: The horizontal gray arrows represent copy and crop operation (more on that later).
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平灰色箭头表示复制和裁剪操作（稍后会详细介绍）。
- en: The red vertical arrows represent 2×2 max pooling operations.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 红色竖直箭头表示2×2最大池化操作。
- en: The vertical green arrows represent 2×2 up-convolutions (or transposed convolutions;
    see [*Chapter 4*](B19627_04.xhtml#_idTextAnchor107)).
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 竖直绿色箭头表示2×2上卷积（或转置卷积；参见[*第4章*](B19627_04.xhtml#_idTextAnchor107)）。
- en: 'We can separate the U-Net model into two virtual components (in reality, this
    is just a single network):'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将U-Net模型分为两个虚拟组件（实际上，这只是一个单一的网络）：
- en: '**Encoder**: The first part of the network (the left part of the *U*) is similar
    to a regular CNN but without the fully connected layers at the end. Its role is
    to learn highly abstract representations of the input image (nothing new here).
    The input image itself can be an arbitrary size, so long as the input feature
    maps of every max pooling operation have even (and not odd) dimensions. Otherwise,
    the output segmentation mask will be distorted. By default, the input size is
    572×572\. From there, it continues like a regular CNN with alternating convolutional
    and max pooling layers. The encoder consists of four identical blocks of two consecutive
    valid (unpadded)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**：网络的第一部分（*U*的左侧）类似于常规CNN，但末尾没有全连接层。其作用是学习输入图像的高度抽象表示（这没有什么新意）。输入图像本身可以是任意大小，只要每次最大池化操作的输入特征图具有偶数（而非奇数）维度。否则，输出的分割掩模将被扭曲。默认情况下，输入大小为572×572。接下来，它像常规CNN一样，交替进行卷积和最大池化层。编码器由四个相同的模块组成，每个模块包含两个连续的有效（未填充）卷积。'
- en: cross-channel 3×3 convolutions with stride 1, optional batch normalization,
    ReLU activations, and a 2×2 max pooling layer. Each downsampling step doubles
    the number of feature maps. The final encoder convolution ends with 1,024 28×28
    feature maps.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 具有步幅1的3×3跨通道卷积，可选的批量归一化、ReLU激活，以及2×2最大池化层。每个下采样步骤都会使特征图数量翻倍。最终的编码器卷积结束时会得到1,024个28×28的特征图。
- en: '**Decoder**: The second part of the network (the right part of the *U*) is
    symmetrical to the encoder. The decoder takes the innermost 28×28 encoder feature
    maps and simultaneously upsamples and converts them into a 388×388 segmentation
    map. It contains four identical upsampling blocks:'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**：网络的第二部分（*U*的右侧）与编码器对称。解码器接收最内层的28×28编码器特征图，并同时进行上采样，将其转换为388×388的分割图。它包含四个相同的上采样模块：'
- en: The upsampling works with 2×2 transposed cross-channel convolutions with stride
    2.
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上采样使用2×2转置交叉通道卷积，步幅为2。
- en: The output of each upsampling step is concatenated with the cropped high-resolution
    feature maps of the corresponding encoder step (gray horizontal arrows). The cropping
    is necessary because of the loss of border pixels in every unpadded encoder and
    decoder convolution.
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个上采样步骤的输出与相应编码器步骤的裁剪高分辨率特征图（灰色横向箭头）进行拼接。裁剪是必要的，因为每次未填充的编码器和解码器卷积都会丢失边缘像素。
- en: Each transposed convolution is followed by two regular convolutions to smooth
    the expanded representation.
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个转置卷积后跟随两个常规卷积，以平滑扩展后的表示。
- en: The upsampling steps halve the number of feature maps. The final output uses
    a 1×1 bottleneck convolution to map the 64-component feature map tensor to the
    desired number of classes (light blue arrow). The authors of the paper have demonstrated
    the binary segmentation of medical images of cells.
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上采样步骤会将特征图数量减半。最终输出使用1×1瓶颈卷积将64个分量的特征图张量映射到所需的类别数量（浅蓝色箭头）。论文的作者展示了医学图像中细胞的二分类分割。
- en: The network’s output is a softmax over each pixel of the segmentation mask –
    that is, the output contains as many independent softmax operations as the number
    of pixels. The softmax output for one pixel determines the pixel class. U-Net
    is trained like a regular classification network. However, the cost function is
    a combination of the cross-entropy losses of the softmax outputs over all pixels.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的输出是对每个像素的分割掩模进行softmax处理——也就是说，输出中包含与像素数相等的独立softmax操作。某一像素的softmax输出决定了该像素的类别。U-Net像常规分类网络一样进行训练。然而，损失函数是所有像素的softmax输出的交叉熵损失的组合。
- en: 'We can see that because of the unpadded convolutions of the network, the output
    segmentation map is smaller than the input image (388 versus 572). However, the
    output map is not a rescaled version of the input image. Instead, it has a one-to-one
    scale compared to the input, but only covers the central part of the input tile.
    This is illustrated in the following diagram:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，由于网络采用了无填充卷积，输出的分割图比输入图像小（388 对比 572）。然而，输出图并不是输入图像的缩放版本。相反，它与输入图具有一对一的比例，但仅覆盖输入图块的中心部分。这在下图中得到了说明：
- en: '![Figure 5.14 – An overlap-tile strategy for segmenting large images. Source:
    https://arxiv.org/abs/1505.04597](img/B19627_05_14.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.14 – 用于分割大图像的重叠平铺策略。来源：[https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)](img/B19627_05_14.jpg)'
- en: 'Figure 5.14 – An overlap-tile strategy for segmenting large images. Source:
    https://arxiv.org/abs/1505.04597'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14 – 用于分割大图像的重叠平铺策略。来源：[https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)
- en: The unpadded convolutions are necessary so that the network doesn’t produce
    noisy artifacts at the borders of the segmentation map. This makes it possible
    to segment images with arbitrary large sizes using the so-called overlap-tile
    strategy. The input image is split into overlapping input tiles, like the one
    shown on the left of the preceding figure. The segmentation map of the small light
    area in the image on the right requires the large light area (one tile) on the
    left image as input.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 无填充卷积是必要的，这样网络在分割图的边缘不会产生噪声伪影。这使得使用所谓的重叠平铺策略对任意大小的图像进行分割成为可能。输入图像被分割成重叠的输入图块，如前图左侧所示。右侧图像中小的亮区的分割图需要左侧图像中的大亮区（一个图块）作为输入。
- en: The next input tile overlaps with the previous one in such a way that their
    segmentation maps cover adjacent areas of the image. To predict the pixels in
    the border region of the image, the missing context is extrapolated by mirroring
    the input image.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个输入图块与前一个图块重叠，以使它们的分割图覆盖图像的相邻区域。为了预测图像边缘区域的像素，缺失的上下文通过镜像输入图像来推断。
- en: We’re not going to implement a code example with U-Net, but you can check out
    [https://github.com/mateuszbuda/brain-segmentation-pytorch](https://github.com/mateuszbuda/brain-segmentation-pytorch)
    for U-Net brain MRI image segmentation.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会实现 U-Net 的代码示例，但你可以查看 [https://github.com/mateuszbuda/brain-segmentation-pytorch](https://github.com/mateuszbuda/brain-segmentation-pytorch)
    来了解 U-Net 在大脑 MRI 图像分割中的应用。
- en: Instance segmentation with Mask R-CNN
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Mask R-CNN 进行实例分割
- en: 'Mask R-CNN ([https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870))
    is an extension of Faster R-CNN for instance segmentation. Faster R-CNN has two
    outputs for each candidate object: bounding box parameters and class labels. In
    addition to these, Mask R-CNN adds a third output – an FCN that produces a binary
    segmentation mask for each RoI. The following diagram shows the structure of Mask
    R-CNN:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN ([https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870))
    是 Faster R-CNN 在实例分割方面的扩展。Faster R-CNN 为每个候选目标提供两个输出：边界框参数和类别标签。除了这些，Mask R-CNN
    增加了第三个输出——一个 FCN，为每个 RoI 生成二进制分割掩码。下图展示了 Mask R-CNN 的结构：
- en: '![Figure 5.15 – Mask R-CNN structure](img/B19627_05_15.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.15 – Mask R-CNN 结构](img/B19627_05_15.jpg)'
- en: Figure 5.15 – Mask R-CNN structure
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 – Mask R-CNN 结构
- en: The segmentation and detection paths both use the RoI predictions of the RPN
    but are otherwise independent and *parallel* to each other. The segmentation path
    produces *I* *m×m* segmentation masks, one for each of the *I* RoIs. Since the
    detection path handles the classification of the object, the segmentation mask
    is *binary* and independent of the object class. The segmented pixels are automatically
    assigned to the class produced by the detection path. This is opposed to other
    algorithms, such as U-Net, where the segmentation is combined with classification
    and an individual softmax is applied at each pixel. At training or inference,
    only the mask related to the predicted object of the classification path is considered;
    the rest are discarded.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 分割路径和检测路径都使用 RPN 的 RoI 预测，但除此之外，它们是独立的，且是*并行*的。分割路径生成 *I* *m×m* 分割掩码，每个 RoI
    对应一个。由于检测路径负责处理目标的分类，因此分割掩码是*二进制*的，并且与目标类别无关。分割后的像素会自动被分配到检测路径所预测的类别中。这与其他算法（如
    U-Net）不同，后者将分割与分类结合，在每个像素上应用个别的 softmax。在训练或推理时，仅考虑与分类路径中预测目标相关的掩码，其余的会被丢弃。
- en: 'Mask R-CNN replaces the RoI max pooling operation with a more accurate RoI
    align layer. The RPN outputs the anchor box center and its height and width as
    four floating-point numbers. Then, the RoI pooling layer translates them into
    integer feature map cell coordinates (quantization). Additionally, the division
    of the RoI to *H×W* bins (the same size as the RoI pooling regions) also involves
    quantization. The RoI example from the *Object detection with Faster R-CNN* section
    shows that the bins have different sizes (3×3, 3×2, 2×3, 2×2). These two quantization
    levels can introduce misalignment between the RoI and the extracted features.
    The following diagram shows how RoI alignment solves this problem:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN 用更精确的 RoI align 层替换了 RoI 最大池化操作。RPN 输出锚框的中心及其高度和宽度，作为四个浮点数。然后，RoI
    池化层将其转换为整数特征图单元格坐标（量化）。此外，将 RoI 划分为 *H×W* 网格（与 RoI 池化区域大小相同）也涉及量化。从*使用 Faster
    R-CNN 进行目标检测*章节中的 RoI 示例可以看出，这些网格的大小不同（3×3、3×2、2×3、2×2）。这两个量化级别可能会导致 RoI 与提取的特征之间的不对齐。下图展示了
    RoI align 如何解决这个问题：
- en: '![Figure 5.16 – RoI align example. Source: https://arxiv.org/abs/1703.06870](img/B19627_05_16.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.16 – RoI align 示例。来源：https://arxiv.org/abs/1703.06870](img/B19627_05_16.jpg)'
- en: 'Figure 5.16 – RoI align example. Source: https://arxiv.org/abs/1703.06870'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 – RoI align 示例。来源：https://arxiv.org/abs/1703.06870
- en: The dashed lines represent the feature map cells. The region with solid lines
    in the middle is a 2×2 RoI overlaid on the feature map. Note that it doesn’t match
    the cells exactly. Instead, it is located according to the RPN prediction without
    quantization. In the same way, a cell of the RoI (the black dots) doesn’t match
    one particular cell of the feature map. The **RoI align** operation computes the
    value of a RoI cell with a bilinear interpolation of its adjacent cells. In this
    way, RoI align is more accurate than RoI pooling.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 虚线表示特征图的单元格。中间实线区域是覆盖在特征图上的 2×2 RoI。注意，它与单元格并不完全匹配，而是根据 RPN 预测的位置来定位的，没有进行量化。同样，RoI
    的单元格（黑点）也不与特定的特征图单元格对齐。**RoI align** 操作通过双线性插值计算 RoI 单元格的值，涉及其相邻单元格。这样，RoI align
    比 RoI pooling 更精确。
- en: At training, a RoI is assigned a positive label if it has IoU with a ground
    truth box of at least 0.5, and negative otherwise. The mask target is the intersection
    between a RoI and its associated ground truth mask. Only the positive RoIs participate
    in the segmentation path training.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练中，如果一个 RoI 与地面真值框的 IoU 大于或等于 0.5，则为其分配正标签，否则为负标签。掩膜目标是 RoI 与其关联的地面真值掩膜的交集。只有正
    RoI 会参与分割路径的训练。
- en: Using Mask R-CNN with PyTorch
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 PyTorch 的 Mask R-CNN
- en: 'In this section, we’ll use a pre-trained PyTorch Mask R-CNN model with a ResNet50
    backbone for instance segmentation. Like Faster R-CNN, PyTorch has out-of-the-box
    support for Mask R-CNN. The program structure and the requirements are the same
    as the ones in the *Using Faster R-CNN with PyTorch* section. We will only include
    the relevant parts of the code, but you can find the full version in this book’s
    GitHub repository. Let’s start:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用一个预训练的 PyTorch Mask R-CNN 模型，搭载 ResNet50 主干网络进行实例分割。与 Faster R-CNN
    类似，PyTorch 原生支持 Mask R-CNN。程序结构和要求与*使用 PyTorch 的 Faster R-CNN*章节中的相同。我们将只包含相关的代码部分，完整版本可以在本书的
    GitHub 仓库中找到。让我们开始：
- en: 'Load the pre-trained model with the latest available weights, which you can
    ensure by using the `DEFAULT` option:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载预训练模型并使用最新的权重，你可以通过选择 `DEFAULT` 选项来确保这一点：
- en: '[PRE22]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We are going to use the model for inference and not for training, so we’ll
    enable the `eval()` mode:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用模型进行推理而不是训练，因此我们将启用 `eval()` 模式：
- en: '[PRE23]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Use `opencv-python` to read the RGB image located at `image_file_path`. We’ll
    omit the code, which downloads the image from this book’s repository if it doesn’t
    already exist locally:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `opencv-python` 读取位于 `image_file_path` 的 RGB 图像。如果本地没有图像，我们将省略从本书的仓库下载图像的代码：
- en: '[PRE24]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Here, `img` is a three-dimensional `numpy` array of integers.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，`img` 是一个三维的 `numpy` 整型数组。
- en: 'Implement the single-step image pre-processing pipeline. It transforms the
    `img` `numpy` array into `torch.Tensor`, which will serve as input to the model:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现单步图像预处理管道。它将 `img` `numpy` 数组转换为 `torch.Tensor`，作为模型的输入：
- en: '[PRE25]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Run the detection model:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行检测模型：
- en: '[PRE26]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here, `segmented_objects` is a dictionary with four items: `boxes`, `labels`,
    `scores`, and `masks`. The first three are the same as in Faster R-CNN. `masks`
    is a tensor with a shape of `[number_of_detected_objects, 1, image_height, image_width]`.
    We have one binary segmentation mask that covers the entire image for each detected
    object. Each such mask has zeroes at all pixels, except the pixels where the object
    is detected with a value of 1.'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，`segmented_objects`是一个包含四个项的字典：`boxes`、`labels`、`scores`和`masks`。前三项与Faster
    R-CNN中的相同。`masks`是一个形状为`[number_of_detected_objects, 1, image_height, image_width]`的张量。对于每个检测到的物体，我们有一个覆盖整个图像的二进制分割掩码。每个这样的掩码在所有像素中都是零，除了物体被检测到的像素，其值为1。
- en: 'Use the initial `img` array and `segmented_objects` as parameters for the `draw_segmentation_masks`
    function. It overlays the bounding boxes, the segmentation masks, and the labels
    of the detected objects on the original input image (the implementation of `draw_segmentation_masks`
    is available in the full example):'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用初始的`img`数组和`segmented_objects`作为`draw_segmentation_masks`函数的参数。它将检测到的物体的边界框、分割掩码和标签叠加到原始输入图像上（`draw_segmentation_masks`的实现可以在完整示例中找到）：
- en: '[PRE27]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Display the result with `opencv`:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`opencv`显示结果：
- en: '[PRE28]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output image looks like this:'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出图像如下所示：
- en: '![Figure 5.17 – Instance segmentation with Mask R-CNN](img/B19627_05_17.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.17 – 使用Mask R-CNN进行实例分割](img/B19627_05_17.jpg)'
- en: Figure 5.17 – Instance segmentation with Mask R-CNN
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17 – 使用Mask R-CNN进行实例分割
- en: We’ve now discussed object detection and semantic segmentation. In the next
    section, we’ll discuss how to use CNNs to generate new images, instead of simply
    processing existing ones.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经讨论了目标检测和语义分割。在下一节中，我们将讨论如何使用CNN生成新的图像，而不仅仅是处理现有的图像。
- en: Image generation with diffusion models
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用扩散模型生成图像
- en: 'So far, we’ve used NNs as **discriminative models**. This simply means that,
    given input data, a discriminative model will **map** it to a certain label (in
    other words, a classification). A typical example is the classification of MNIST
    images in one of ten digit classes, where the NN maps input data features (pixel
    intensities) to the digit label. We can also say this in another way: a discriminative
    model gives us the probability of *y* (class), given *x* (input). In the case
    of MNIST, this is the probability of the digit when given the pixel intensities
    of the image. In the next section, we’ll introduce NNs as generative models.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用神经网络作为**判别模型**。这仅仅意味着，在给定输入数据的情况下，判别模型将**映射**它到某个标签（换句话说，就是分类）。一个典型的例子是将MNIST图像分类到十个数字类别之一，其中神经网络将输入数据特征（像素强度）映射到数字标签。我们也可以用另一种方式说：判别模型给出的是*y*（类）给定*x*（输入）的概率。在MNIST的例子中，就是在给定图像的像素强度时，识别数字的概率。在下一节中，我们将介绍神经网络作为生成模型的应用。
- en: Introducing generative models
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍生成模型
- en: A **generative model** learns the distribution of data. In a way, it is the
    opposite of the discriminative model we just described. It predicts the probability
    of the input sample, given its class, *y* – ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/347.png).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成模型**学习数据的分布。从某种程度上来说，它是我们刚刚描述的判别模型的对立面。它预测给定类*y*时输入样本的概率 – ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/347.png)。'
- en: For example, a generative model will be able to create an image based on textual
    description. Most often, *y* is tensor, rather than scalar. This tensor exists
    in the so-called **latent space** (or **latent feature space**), and we’ll refer
    to it as the **latent representation** (or **latent space representation**) of
    the original data, which itself exists in its own **feature space**. We can think
    of the latent representation as a **compressed** (or simplified) version of the
    original feature space. The digit-to-class case serves as an extreme example of
    this paradigm – after all, we’re compressing an entire image into a single digit.
    For the latent representation to work, it will have to capture the most important
    hidden properties of the original data and discard the noise.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个生成模型可以根据文本描述生成图像。通常，*y* 是张量，而不是标量。这个张量存在于所谓的**潜在空间**（或**潜在特征空间**）中，我们将其称为原始数据的**潜在表示**（或**潜在空间表示**），而原始数据本身存在于其自己的**特征空间**中。我们可以将潜在表示视为原始特征空间的**压缩**（或简化）版本。数字到类别的例子是这种范式的极端示例——毕竟，我们是在将整个图像压缩成一个数字。为了使潜在表示有效，它必须捕捉原始数据最重要的隐藏特征，并去除噪声。
- en: Because of its relative simplicity, we can reasonably expect that we have some
    knowledge of the structure and properties of the latent space. This is opposed
    to the feature space, which is complex beyond our comprehension. Therefore, if
    we know the reverse mapping from the latent space to the feature space, we could
    generate different feature space representations (that is, images) based on different
    latent representations. More importantly, we can influence the output image properties
    by modifying (in a conscious way) the initial latent representation.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其相对简单性，我们可以合理地期望我们对潜在空间的结构和属性有所了解。这与特征空间不同，后者复杂到超出我们的理解。因此，如果我们知道从潜在空间到特征空间的反向映射，我们就可以基于不同的潜在表示生成不同的特征空间表示（即图像）。更重要的是，我们可以通过有意识地修改初始潜在表示来影响输出图像的属性。
- en: 'To illustrate this, let’s imagine that we’ve managed to create a reverse mapping
    between latent vectors with *n=3* elements and full-fledged images of vehicles.
    Each vector element represents one vehicle property, such as length, height, and
    width (as shown in the following diagram):'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，假设我们成功地创建了一个反向映射，将具有*n=3*元素的潜在向量与完整的车辆图像关联起来。每个向量元素表示一个车辆属性，例如长度、高度和宽度（如下面的图示所示）：
- en: '![Figure 5.18 – An example of feature space-latent space and latent space-feature
    space mapping](img/B19627_05_18.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![图5.18 – 特征空间-潜在空间与潜在空间-特征空间映射示例](img/B19627_05_18.jpg)'
- en: Figure 5.18 – An example of feature space-latent space and latent space-feature
    space mapping
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.18 – 特征空间-潜在空间与潜在空间-特征空间映射示例
- en: Say that the average vehicle length is four meters. Instead of a discrete value,
    we can represent this property as a **normal** (**Gaussian**) **distribution**
    ([https://en.wikipedia.org/wiki/Normal_distribution](https://en.wikipedia.org/wiki/Normal_distribution))
    with a mean of 4, making the latent space continuous (the same applies to the
    other properties). Then, we can choose to sample new values for each element from
    the ranges of their distributions. They will form a new latent vector (in this
    case, a **latent variable**), which we can use as a seed to generate new images.
    For example, we can create longer and lower vehicles
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 假设平均车辆长度为四米。我们可以将这个属性表示为一个**正态**（**高斯**）**分布**（[https://en.wikipedia.org/wiki/Normal_distribution](https://en.wikipedia.org/wiki/Normal_distribution)），均值为4，从而使潜在空间变得连续（同样适用于其他属性）。然后，我们可以选择从每个属性的分布范围内采样新的值。它们将形成一个新的潜在向量（在这种情况下是一个**潜在变量**），我们可以将其作为种子来生成新的图像。例如，我们可以生成更长和更低的车辆。
- en: (as illustrated previously).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: （如前所示）。
- en: Note
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The second edition of this book included a whole chapter on NN-based generative
    models, where we discussed two particular architectures: **variational autoencoders**
    (**VAE**, *Auto-Encoding Variational Bayes*, [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114))
    and **generative adversarial networks** (**GAN**, [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)).
    At the time, these were the state-of-the-art generative models for images. Since
    then, they’ve been surpassed by a new class of algorithms called **diffusion models**.
    As we have to move with the times, in this edition, we’ll omit VAEs and GANs,
    and we’ll focus on diffusion models instead.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的第二版增加了关于基于神经网络的生成模型的整章内容，其中我们讨论了两个特别的架构：**变分自编码器**（**VAE**，*Auto-Encoding
    Variational Bayes*，[https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)）和**生成对抗网络**（**GAN**，[https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)）。当时，这些是用于图像生成的最先进的生成模型。从那时起，它们被一种新型算法——**扩散模型**所超越。为了与时俱进，本版中我们将省略VAE和GAN，重点介绍扩散模型。
- en: Denoising Diffusion Probabilistic Models
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去噪扩散概率模型
- en: '**Diffusion models** are a particular class of generative models, first introduced
    in 2015 (*Deep Unsupervised Learning using Nonequilibrium Thermodynamics*, [https://arxiv.org/abs/1503.03585](https://arxiv.org/abs/1503.03585)).
    In this section, we’ll focus on **Denoising Diffusion Probabilistic Models** (**DDPM**,
    [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)), which form
    the foundation of some of the most impressive generative tools such as **Stable**
    **Diffusion** ([https://github.com/CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion)).'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '**扩散模型**是一类特殊的生成模型，首次在2015年提出（*深度无监督学习与非平衡热力学*，[https://arxiv.org/abs/1503.03585](https://arxiv.org/abs/1503.03585)）。在本节中，我们将重点介绍**去噪扩散概率模型**（**DDPM**，[https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)），它们构成了许多令人印象深刻的生成工具的基础，比如**稳定扩散**（[https://github.com/CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion)）。'
- en: 'DDPM follows a similar pattern to the generative models we’ve already discussed:
    it starts with a latent variable and uses it to generate a full-fledged image.
    The DDPM training algorithm is split into two parts:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: DDPM遵循与我们已讨论过的生成模型类似的模式：它从一个潜在变量开始，并使用它生成完整的图像。DDPM的训练算法分为两个部分：
- en: '**Forward diffusion**: This starts with an initial image and then gradually
    adds random **Gaussian noise** ([https://en.wikipedia.org/wiki/Gaussian_noise](https://en.wikipedia.org/wiki/Gaussian_noise))
    to it through a series of small steps until the final (latent) representation
    is pure noise.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正向扩散**：从初始图像开始，然后通过一系列小步骤逐渐向其中添加随机的**高斯噪声**（[https://en.wikipedia.org/wiki/Gaussian_noise](https://en.wikipedia.org/wiki/Gaussian_noise)），直到最终（潜在）表示变成纯噪声。'
- en: '**Reverse diffusion**: This is the opposite of the forward process. It starts
    with pure noise and gradually tries to restore the original image.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向扩散**：这是正向过程的反向过程。它从纯噪声开始，并逐渐尝试恢复原始图像。'
- en: 'The following diagram illustrates the forward (top) and reverse (bottom) diffusion
    processes:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了正向（顶部）和反向（底部）扩散过程：
- en: '![Figure 5.19 – The forward (bottom) and reverse (top) diffusion processes.
    Source: https://arxiv.org/abs/2006.11239](img/B19627_05_19.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.19 – 正向（底部）和反向（顶部）扩散过程。来源：https://arxiv.org/abs/2006.11239](img/B19627_05_19.jpg)'
- en: 'Figure 5.19 – The forward (bottom) and reverse (top) diffusion processes. Source:
    https://arxiv.org/abs/2006.11239'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.19 – 正向（底部）和反向（顶部）扩散过程。来源：https://arxiv.org/abs/2006.11239
- en: 'Let’s discuss it in detail:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论一下：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/348.png):
    The initial image from the original feature space, represented as a tensor.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/348.png)：来自原始特征空间的初始图像，表示为张量。'
- en: '*T*: The number of steps in the forward and reverse processes. Originally,
    the authors used *T=1000*. More recently, *T=4000* has been proposed (*Improved
    Denoising Diffusion Probabilistic Models*). Each forward or reverse step adds
    or removes small amounts of noise.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*T*：正向和反向过程中的步骤数。最初，作者使用了*T=1000*。最近，提出了*T=4000*（*改进的去噪扩散概率模型*）。每个正向或反向步骤都会添加或去除少量噪声。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math>](img/349.png):
    The final result of the forward diffusion, which represents pure noise. We can
    think of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math>](img/349.png)
    as a peculiar latent representation of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/351.png).
    The two tensors have the same dimensions, unlike the example we discussed in the
    *Introducing generative* *models* section.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math>](img/349.png)：前向扩散的最终结果，表示纯噪声。我们可以把![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math>](img/349.png)看作![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/351.png)的一个特殊潜在表示。这两个张量具有相同的维度，与我们在*引入生成*
    *模型*部分讨论的例子不同。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png)
    (note the lowercase *t*): The noise-augmented tensor at an intermediate step,
    *t*. Again, it has the same dimensions as ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/348.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math>](img/349.png).'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png)（注意小写的*t*）：在一个中间步骤中的噪声增强张量，*t*。它的维度与![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/348.png)和![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math>](img/349.png)相同。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>q</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/355.png):
    This is the **probability density function** (**PDF**) of the forward diffusion
    process at an intermediate step, *t*. PDF sounds scary, but it isn’t. It simply
    means that we add small amounts of Gaussian noise to the already noisy tensor,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/356.png),
    to produce a new, noisier, tensor, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/357.png)
    (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/357.png)
    is conditioned on ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/359.png)).
    The forward diffusion doesn’t involve ML or NNs and has no learnable parameters.
    We just add noise and that’s it. Still, it represents a mapping from the original
    feature space to the latent representation space.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>q</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/355.png)：这是**前向扩散过程**在一个中间步骤*t*的**概率密度函数**（**PDF**）。PDF听起来很复杂，但其实并不是。它的意思就是我们给已经有噪声的张量添加少量的高斯噪声，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/356.png)，生成一个新的、更有噪声的张量，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/357.png)（![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/357.png)是依赖于![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/359.png))。前向扩散过程不涉及机器学习或神经网络，也没有可学习的参数。我们只是加了噪声，仅此而已。然而，它表示的是从原始特征空间到潜在表示空间的映射。'
- en: 'Note that we need to know ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/360.png)
    to produce ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/361.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/362.png)
    for ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/360.png)
    and so on – that is, we need all tensors ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/364.png)
    to produce ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/365.png).
    Thankfully, the authors have proposed an optimization that allows us to derive
    the value of any ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png)
    using only the initial tensor, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/348.png):'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们需要知道 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/360.png)
    来生成 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/361.png)，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/362.png)，以此类推——也就是说，我们需要所有的张量
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/364.png)
    来生成 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/365.png)。幸运的是，作者提出了一种优化方法，使我们能够仅使用初始张量
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/348.png)
    来推导出任何 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png)
    的值。
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mover
    accent="false"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:msqrt><mml:mi
    mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msqrt><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mover
    accent="false"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:msqrt><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi mathvariant="bold">ϵ</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi mathvariant="bold">ϵ</mml:mi><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/368.png)(1)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mover
    accent="false"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:msqrt><mml:mi
    mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msqrt><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mover
    accent="false"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:msqrt><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi mathvariant="bold">ϵ</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi mathvariant="bold">ϵ</mml:mi><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/368.png)(1)'
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/369.png)
    is a coefficient, which changes on a pre-defined schedule, but generally increases
    with *t*. **ϵ** is the Gaussian random noise tensor with the same size as ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png).
    The square root ensures that the new ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/357.png)
    will still follow a Gaussian distribution. We can see that ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/357.png)
    is a mixture of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/373.png)
    and **ϵ** and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/374.png)
    determines the balance between the two. If ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>t</mi><mo>→</mo><mn>0</mn></mrow></mrow></math>](img/375.png),
    then ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/376.png)
    will have more weight. The more ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>t</mi><mo>→</mo><mi>T</mi></mrow></mrow></math>](img/377.png),
    the more the noise, **ϵ**, will prevail. Because of this optimization, we don’t
    have a real multi-step forward diffusion process. Instead, we generate the desired
    noisy representation at step *t*, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png),
    in a single operation.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/369.png)
    是一个系数，它会根据预定的时间表发生变化，但通常随着 *t* 的增加而增大。**ϵ** 是与![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png)同样大小的高斯随机噪声张量。平方根确保新的![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/357.png)仍然遵循高斯分布。我们可以看到，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/357.png)是![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/373.png)和**ϵ**的混合，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/374.png)决定了两者之间的平衡。如果![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>t</mi><mo>→</mo><mn>0</mn></mrow></mrow></math>](img/375.png)，那么![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/376.png)将占据更多权重。当![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>t</mi><mo>→</mo><mi>T</mi></mrow></mrow></math>](img/377.png)时，噪声**ϵ**将占据主导地位。由于这种优化，我们并没有进行真正的多步前向扩散过程。相反，我们在一步操作中生成所需的噪声表示，位于步骤
    *t* 的![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png)。
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/379.png):
    This is the PDF of the reverse diffusion process at an intermediate step, *t-1*.
    This is the opposite function of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>q</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/380.png).
    It is a mapping from the latent space to the original feature space – that is,
    we start from the pure noise tensor, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math>](img/381.png),
    and we gradually try to remove the noise until we reach the original image in
    *T* steps. The reverse diffusion is a lot more challenging compared to simply
    adding noise to an image, as in the forward phase. This is the primary reason
    to split the denoising process into multiple steps with small amounts of noise
    in the first place. Our best chance is to train an NN with the hope that it will
    learn a reasonable approximation of the actual mapping between the latent and
    the original feature spaces. Therefore, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:math>](img/382.png)
    is an NN, where the *θ* index indicates its weights. The authors have proposed
    a **U-Net** type of network. It takes the noisy tensor, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/365.png),
    as input and outputs its approximation of the noise (that is, only the noise and
    not the image itself) that was added to the original image, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:math>](img/384.png).
    The input and output tensors have the same dimensions. DDPM was released later
    than the original U-Net, so their NN architecture uses some improvements that
    were introduced in the meantime. These include residual blocks, **group normalization**
    (an alternative to batch normalization), and **attention** (*Attention Is All
    You Need*, [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)),
    [https://arxiv.org/abs/1803.08494](https://arxiv.org/abs/1803.08494)), and **attention**
    (*Attention Is All You* *Need*, [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)).'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/379.png)：这是反向扩散过程在中间步骤
    *t-1* 时的PDF。这是![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>q</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/380.png)的对立函数。它是从潜在空间映射到原始特征空间的过程——也就是说，我们从纯噪声张量![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math>](img/381.png)开始，逐渐去除噪声，直到达到原始图像，整个过程需要
    *T* 步。与在前向阶段中仅将噪声添加到图像相比，反向扩散要复杂得多。这也是将去噪过程分成多个步骤，并在初期引入少量噪声的主要原因。我们的最佳机会是训练一个神经网络（NN），希望它能学习到潜在空间与原始特征空间之间实际映射的合理近似。因此，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:math>](img/382.png)
    是一个神经网络，其中 *θ* 索引表示其权重。作者们提出了一种 **U-Net** 类型的网络。它以噪声张量![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/365.png)为输入，并输出它对原始图像中加入的噪声（即仅噪声，而非图像本身）的近似值，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:math>](img/384.png)。输入和输出张量具有相同的维度。DDPM比原始的U-Net稍晚发布，因此它们的神经网络架构在这期间引入了一些改进。这些改进包括残差块、**组归一化**（一种批量归一化的替代方法）、以及
    **注意力机制** (*Attention Is All You Need*, [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))，[https://arxiv.org/abs/1803.08494](https://arxiv.org/abs/1803.08494))
    和 **注意力机制** (*Attention Is All You* *Need*, [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))。'
- en: 'Next, let’s focus on the DDPM training, which is displayed on the left in the
    following figure:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们聚焦于DDPM训练，以下图中的左侧所示：
- en: '![Figure 5.20 – DDPM training (left); DDPM sampling (right). Source: https://arxiv.org/abs/2006.11239](img/B19627_05_20.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.20 – DDPM 训练（左）；DDPM 采样（右）。来源：https://arxiv.org/abs/2006.11239](img/B19627_05_20.jpg)'
- en: 'Figure 5.20 – DDPM training (left); DDPM sampling (right). Source: https://arxiv.org/abs/2006.11239'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.20 – DDPM 训练（左）；DDPM 采样（右）。来源：https://arxiv.org/abs/2006.11239
- en: 'A single training episode involves the following steps (line 1):'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完整的训练过程包括以下步骤（第1行）：
- en: Start with a random sample (image), ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/348.png),
    from the training set (line 2).
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从训练集中的随机样本（图像）开始，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/348.png)（第2行）。
- en: Sample the random noise step, *t*, in the range [1:*T*] (line 3).
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在区间[1:*T*]内采样随机噪声步长，*t*（第3行）。
- en: Sample the random noise tensor, **ϵ**, from a Gaussian distribution (line 4).
    Within the NN itself, the step, *t*, is embedded in the values of **ϵ** using
    **sinusoidal position embeddings**. Don’t worry if you don’t understand the concept
    of positional embeddings. We’ll discuss it in detail in [*Chapter 7*](B19627_07.xhtml#_idTextAnchor202),
    as it was first introduced in that context. All we need to know now is that the
    step number, *t*, is implicitly encoded in the elements of **ϵ**, in a way that
    allows the model to use this information. The step-adjusted noise is denoted with
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">ϵ</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:math>](img/386.png)
    in the preceding diagram.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从高斯分布中采样随机噪声张量，**ϵ**（第4行）。在神经网络本身中，步长 *t* 被通过 **正弦位置嵌入**方式嵌入到**ϵ**的值中。如果你不理解位置嵌入的概念，不必担心。我们将在[*第7章*](B19627_07.xhtml#_idTextAnchor202)中详细讨论它，因为它最早是在该上下文中引入的。现在我们需要知道的是，步长
    *t* 被隐式编码在**ϵ**的元素中，方式使得模型能够利用这些信息。步长调整的噪声在前图中以![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">ϵ</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:math>](img/386.png)表示。
- en: Produce a corrupt image tensor, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/387.png),
    conditioned on the initial image, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/388.png),
    and based on the sampled noise step, *t*, and the random noise, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">ϵ</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:math>](img/389.png).
    To do this, we’ll use formula (1), which we introduced earlier in this section.
    Thanks to it, this single step constitutes the entire forward diffusion phase
    (line 5).
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于初始图像，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/387.png)，生成一个损坏的图像张量，该图像以初始图像和根据采样的噪声步长，*t*，以及随机噪声，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi mathvariant="bold">ϵ</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:math>](img/389.png)为条件。为此，我们将使用前面在本节中介绍的公式（1）。感谢它，这一步构成了整个正向扩散阶段（第5行）。
- en: Perform a single gradient descent step and weight update. The training uses
    the **mean squared error** (**MSE**). It measures the difference between the sampled
    noise, **ϵ** (line 4), and the noise predicted by the model, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:math>](img/390.png)
    (line 5). The loss equation seems deceptively simple. The paper’s authors made
    a long chain of transformations and assumptions to reach this simple result. This
    is one of the main contributions of the paper.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行一次梯度下降步骤和权重更新。训练过程中使用**均方误差**（**MSE**）。它衡量采样噪声**ϵ**（第4行）与模型预测的噪声之间的差异，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:math>](img/390.png)（第5行）。损失方程看起来
    deceptively 简单。论文的作者做了长链的变换和假设，才得出这个简单的结果。这是论文的主要贡献之一。
- en: 'Once the model has been trained, we can use it to sample new images based on
    random initial tensors, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math>](img/349.png).
    We can do this with the following procedure (preceding diagram, right):'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型经过训练，我们就可以使用它基于随机初始张量，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math>](img/349.png)，来采样新的图像。我们可以通过以下过程实现这一点（前面的图示，右侧）：
- en: Sample the initial random latent tensor, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math>](img/349.png),
    from a Gaussian distribution (line 1).
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从高斯分布中采样初始随机潜变量张量，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math>](img/349.png)（第1行）。
- en: 'Repeat the next steps *T* times (line 2):'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复接下来的步骤*T*次（第2行）：
- en: Sample random noise tensor, **z**, from a Gaussian distribution (line 3). We
    do this for all reverse steps, except for the final one.
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从高斯分布中采样随机噪声张量，**z**（第3行）。我们为所有反向步骤执行此操作，除了最后一步。
- en: Use the trained U-Net model to predict the noise, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:math>](img/384.png),
    at step *t*. Subtract this noise from the current sample, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/357.png),
    to produce the new, less noisy, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/395.png)
    (line 4). The scheduling coefficient, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/396.png),
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练好的U-Net模型预测噪声，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:math>](img/384.png)，在步骤*t*时。将此噪声从当前样本中减去，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/357.png)，得到新的、较少噪声的，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/395.png)（第4行）。调度系数，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/396.png)，
- en: also takes part in this formula, as it did in the forward phase. The formula
    also preserves the mean and the variance of the original distribution.
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该公式也参与了此过程，正如在前向阶段一样。该公式还保留了原始分布的均值和方差。
- en: The final denoising step produces the generated image.
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终的去噪步骤生成了图像。
- en: This concludes our introduction to DDPMs for now. However, we’ll revisit them
    in [*Chapter 9*](B19627_09.xhtml#_idTextAnchor236), but in the context of Stable
    Diffusion.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分是我们对DDPM的介绍，暂时到此为止。然而，我们将在[*第9章*](B19627_09.xhtml#_idTextAnchor236)中再次回顾它们，但会在稳定扩散的背景下进行讨论。
- en: Summary
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed some advanced computer vision tasks. We started
    with TL, a technique that makes it possible to bootstrap our experiments with
    the help of pre-trained models. We also introduced object detection and semantic
    segmentation models, which benefit from TL. Finally, we focused on generative
    models and DDPM in particular.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了一些高级计算机视觉任务。我们从TL开始，这是一种通过预训练模型帮助启动实验的技术。我们还介绍了对象检测和语义分割模型，这些模型受益于TL。最后，我们重点介绍了生成模型，特别是DDPM。
- en: In the next chapter, we’ll introduce language modeling and recurrent networks.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍语言建模与递归网络。
- en: 'Part 3:'
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：
- en: Natural Language Processing and Transformers
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理与变换器
- en: We’ll start this part with an introduction to natural language processing, which
    will serve as a backdrop for our discussion on recurrent networks and transformers.
    Transformers will be the main focus of this section because they represent one
    of the most significant deep learning advances in recent years. They are the foundation
    of **large language models** (**LLM**), such as ChatGPT. We’ll discuss their architecture
    and their core element – the attention mechanism. Then, we’ll discuss the properties
    of LLMs. Finally, we’ll focus on some advanced LLM applications, such as text
    and image generation, and learn how to build LLM-centered applications.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分将以自然语言处理的介绍开始，为我们关于递归网络和变换器的讨论提供背景。变换器将是本节的主要焦点，因为它们代表了近年来深度学习领域的重大进展之一。它们是**大型语言模型**（**LLM**）的基础，例如ChatGPT。我们将讨论它们的架构以及它们的核心元素——注意力机制。接着，我们将讨论LLM的特性。最后，我们将重点介绍一些高级LLM应用，如文本和图像生成，并学习如何构建以LLM为核心的应用。
- en: 'This part has the following chapters:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包括以下章节：
- en: '[*Chapter 6*](B19627_06.xhtml#_idTextAnchor185), *Natural Language Processing
    and Recurrent Neural Networks*'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第6章*](B19627_06.xhtml#_idTextAnchor185)，*自然语言处理与递归神经网络*'
- en: '[*Chapter 7*](B19627_07.xhtml#_idTextAnchor202), *The Attention Mechanism and
    Transformers*'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B19627_07.xhtml#_idTextAnchor202)，*注意力机制与变换器*'
- en: '[*Chapter 8*](B19627_08.xhtml#_idTextAnchor220), *Exploring Large Language
    Models in Depth*'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B19627_08.xhtml#_idTextAnchor220)，*深入探索大型语言模型*'
- en: '[*Chapter 9*](B19627_09.xhtml#_idTextAnchor236), *Advanced Applications of
    Large Language Models*'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B19627_09.xhtml#_idTextAnchor236)，*大型语言模型的高级应用*'
