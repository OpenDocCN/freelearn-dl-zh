- en: Human Activity Recognition using Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用循环神经网络进行人类活动识别
- en: A **recurrent neural network** (**RNN**) is a class of artificial neural network
    where connections between units form a directed cycle. RNNs make use of information
    from the past. That way, they can make predictions for data with high temporal
    dependencies. This creates an internal state of the network that allows it to
    exhibit dynamic temporal behavior.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNN**）是一类人工神经网络，其中单元之间的连接形成一个有向循环。RNN利用过去的信息，这样它们就能够对具有高时间依赖性的数据进行预测。这会创建一个网络的内部状态，使其能够表现出动态的时间行为。'
- en: An RNN takes many input vectors to process them and output other vectors. Compared
    to a classical approach, using an RNN with **Long Short-Term Memory** cells (**LSTMs**)
    requires no, or very little, feature engineering. Data can be fed directly into
    the neural network, which acts like a black box, modeling the problem correctly.
    The approach here is rather simple in terms of how much data is preprocessed.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: RNN接受多个输入向量进行处理并输出其他向量。与传统方法相比，使用带有**长短期记忆**单元（**LSTM**）的RNN几乎不需要，或者只需极少的特征工程。数据可以直接输入到神经网络中，神经网络像一个黑盒子一样，正确地建模问题。这里的方法在预处理数据的多少上相对简单。
- en: 'In this chapter, we will see how to develop a machine learning project using
    RNN implementation, called LSTM for **human activity recognition** (**HAR**),
    using the smartphones dataset. In short, our ML model will be able to classify
    the type of movement from six categories: walking, walking upstairs, walking downstairs,
    sitting, standing, and lying down.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到如何使用RNN实现开发机器学习项目，这种实现称为LSTM，用于**人类活动识别**（**HAR**），并使用智能手机数据集。简而言之，我们的机器学习模型将能够从六个类别中分类运动类型：走路、走楼梯、下楼梯、坐着、站立和躺下。
- en: 'In a nutshell, we will learn the following topics throughout this end-to-end
    project:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在这个从头到尾的项目中，我们将学习以下主题：
- en: Working with recurrent neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用循环神经网络
- en: Long term dependencies and drawbacks of RNN
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN的长期依赖性和缺点
- en: Developing an LSTM model for human activity recognition
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发用于人类活动识别的LSTM模型
- en: Tuning LSTM and RNN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调优LSTM和RNN
- en: Summary
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要
- en: Working with RNNs
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RNN
- en: In this section, we will first provide some contextual information about RNNs.
    Then, we will highlight some potential drawbacks of classical RNNs. Finally, we
    will see an improved variation of RNNs called LSTM to address the drawbacks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先将提供一些关于RNN的背景信息。然后，我们将强调传统RNN的一些潜在缺点。最后，我们将看到一种改进的RNN变体——LSTM，来解决这些缺点。
- en: Contextual information and the architecture of RNNs
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN的背景信息及其架构
- en: 'Human beings don''t start thinking from scratch; the human mind has so-called
    **persistence of memory**, the ability to associate the past with recent information.
    Traditional neural networks, instead, ignore past events. For example, in a movie
    scenes classifier, it''s not possible for a neural network to use a past scene
    to classify current ones. RNNs were developed to try to solve this problem:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人类不会从零开始思考；人类思维有所谓的**记忆持久性**，即将过去的信息与最近的信息关联起来的能力。而传统的神经网络则忽略了过去的事件。例如，在电影场景分类器中，神经网络无法使用过去的场景来分类当前的场景。RNN的出现是为了解决这个问题：
- en: '![](img/19903995-1d34-4460-af08-79d4dab2a55f.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19903995-1d34-4460-af08-79d4dab2a55f.png)'
- en: 'Figure 1: RNNs have loops'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：RNN具有循环结构
- en: 'In contrast to conventional neural networks, RNNs are networks with a loop
    that allows the information to be persistent (*Figure 1*). In a neural network
    say, **A**: at some time **t**, input **x[t]** and outputs a value **h[t]**. So
    from *Figure 1*, we can think of an RNN as multiple copies of the same network,
    each passing a message to a successor. Now, if we unroll the previous network,
    what will we receive? Well, the following figure gives you some insight:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统神经网络不同，RNN是带有循环的网络，允许信息保持（*图1*）。在一个神经网络中，比如**A**：在某个时刻**t**，输入**x[t]**并输出一个值**h[t]**。因此，从*图1*来看，我们可以把RNN看作是多个相同网络的副本，每个副本将信息传递给下一个副本。如果我们展开之前的网络，会得到什么呢？嗯，下面的图给出了些许启示：
- en: '![](img/ce3d30f1-828f-4530-932c-9af098e2e36e.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce3d30f1-828f-4530-932c-9af098e2e36e.png)'
- en: 'Figure 2: An unrolled representation of the same RNN represented in Figure
    1'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：图1中表示的同一RNN的展开表示
- en: 'However, the preceding unrolled figure does not provide detailed information
    about RNNs. Rather, an RNN is different from a traditional neural network because
    it introduces a transition weight **W** to transfer information between times.
    RNNs process a sequential input one at a time, updating a kind of vector state
    that contains information about all past elements of the sequence. The following
    figure shows a neural network that takes as input a value of **X(t)**, and then
    outputs a value **Y(t)**:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，前面的展开图并没有提供关于 RNN 的详细信息。相反，RNN 与传统神经网络不同，因为它引入了一个过渡权重**W**，用于在时间之间传递信息。RNN
    一次处理一个顺序输入，更新一种包含所有过去序列元素信息的向量状态。下图展示了一个神经网络，它将**X(t)**的值作为输入，然后输出**Y(t)**的值：
- en: '![](img/c664553f-02b8-4a9a-a128-625acf326972.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c664553f-02b8-4a9a-a128-625acf326972.png)'
- en: 'Figure 3: An RNN architecture can use the previous states of the network to
    its advantage'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：一个 RNN 架构可以利用网络的先前状态来发挥优势
- en: As shown in *Figure 1*, the first half of the neural network is characterized
    by the function *Z (t) = X (t) * W[in]*, and the second half of the neural network
    takes the form *Y(t)= Z(t) * W[out]*. If you prefer, the whole neural network
    is just the function *Y (t) = (X (t) * W*[in]*) * W*[out].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 1*所示，神经网络的前半部分通过函数*Z(t) = X(t) * W[in]* 表示，神经网络的后半部分则呈现为*Y(t)= Z(t) * W[out]*。如果你愿意，整个神经网络就是函数*Y(t)
    = (X(t) * W*[in]*) * W*[out]*。
- en: 'At each time *t*, calls the learned model, this architecture does not take
    into account knowledge about the previous runs. It''s like predicting stock market
    trends by only looking at data from the current day. A better idea would be to
    exploit overarching patterns from a week''s worth or months worth of data:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间点*t*，调用已学习的模型，这种架构没有考虑之前运行的知识。这就像只看当天数据来预测股市趋势。一个更好的方法是利用一周或几个月的数据中的总体模式：
- en: '![](img/eb70b26d-db1e-4a79-9670-e0d38d037b61.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb70b26d-db1e-4a79-9670-e0d38d037b61.png)'
- en: 'Figure 4: An RNN architecture where all the weights in all the layers have
    to be learned with time'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：一个 RNN 架构，其中所有层中的所有权重都需要随着时间学习
- en: A more explicit architecture can be found in *Figure 4*, where the temporally
    shared weights **w2** (for the hidden layer) must be learned in addition to **w1**
    (for the input layer) and **w3** (for the output layer).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更为明确的架构可以在*图 4*中找到，其中时间共享的权重**w2**（用于隐藏层）必须在**w1**（用于输入层）和**w3**（用于输出层）之外进行学习。
- en: Incredibly, over the last few years, RNNs have been used for a variety of problems,
    such as speech recognition, language modeling, translation, and image captioning.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 难以置信的是，在过去的几年里，RNN 已被用于各种问题，如语音识别、语言建模、翻译和图像描述。
- en: RNN and the long-term dependency problem
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 和长期依赖问题
- en: 'RNNs are very powerful and popular too. However, often, we only need to look
    at recent information to perform the present task rather than information that
    was stored a long time ago. This is frequent in NLP for language modeling. Let''s
    see a common example:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 非常强大，也很流行。然而，我们通常只需要查看最近的信息来执行当前任务，而不是很久以前存储的信息。这在自然语言处理中（NLP）进行语言建模时尤为常见。让我们来看一个常见的例子：
- en: '![](img/907bff1c-57b0-4927-9a45-0a2922d87fea.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/907bff1c-57b0-4927-9a45-0a2922d87fea.png)'
- en: 'Figure 5: If the gap between the relevant information and the place that its
    needed is small, RNNs can learn to use past information'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：如果相关信息与所需位置之间的间隙较小，RNN 可以学会利用过去的信息
- en: Suppose a language model is trying to predict the next word based on the previous
    words. As a human being, if we try to predict the last word in *the sky is blue*,
    without further context, it's most likely the next word that we will predict is
    *blue*. In such cases, the gap between the relevant information and the place
    is small. Thus, RNNs can learn to use past information easily.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个语言模型试图基于先前的单词预测下一个单词。作为人类，如果我们试图预测*the sky is blue*中的最后一个词，在没有进一步上下文的情况下，我们最可能预测下一个词是*blue*。在这种情况下，相关信息与位置之间的间隙较小。因此，RNN
    可以轻松学习使用过去的信息。
- en: 'But consider a longer sentence: *Asif grew up in Bangladesh... He studied in
    Korea... He speaks fluent Bengali* where we need more context. In this sentence,
    most recent information advises us that the next word will probably be the name
    of a language. However, if we want to narrow down which language, we need the
    context of *Bangladesh* from previous words:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 但是考虑一个更长的句子：*Asif在孟加拉国长大……他在韩国学习……他讲一口流利的孟加拉语*，我们需要更多的上下文。在这个句子中，最新的信息告诉我们，下一个单词可能是某种语言的名称。然而，如果我们想要缩小是哪种语言，我们需要从前面的词汇中得到*孟加拉国*的上下文：
- en: '![](img/44e0a9aa-4a4d-48df-87e7-a352f6f8b440.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/44e0a9aa-4a4d-48df-87e7-a352f6f8b440.png)'
- en: 'Figure 6: If the gap between the relevant information and the place that its
    needed is bigger, RNNs can''t learn to use past information'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：当相关信息和所需位置之间的间隔较大时，RNNs无法学会使用过去的信息
- en: Here, the gap is bigger so RNNs become unable to learn the information. This
    is a serious drawback of RNN. However, along comes LSTM to the rescue.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，信息间的间隙更大，因此RNNs变得无法学习到这些信息。这是RNN的一个严重缺点。然而，LSTM出现并拯救了这一局面。
- en: LSTM networks
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM网络
- en: One type of RNN model is an **LSTM**. The precise implementation details of
    LSTM are not within the scope of this book. An LSTM is a special RNN architecture,
    which was originally conceived by Hochreiter and Schmidhuber in 1997\. This type
    of neural network has been recently rediscovered in the context of deep learning,
    because it is free from the problem of vanishing gradients, and offers excellent
    results and performance. LSTM-based networks are ideal for prediction and classification
    of temporal sequences, and are replacing many traditional approaches to deep learning.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一种RNN模型是**LSTM**。LSTM的具体实现细节不在本书的范围内。LSTM是一种特殊的RNN架构，最初由Hochreiter和Schmidhuber在1997年提出。这种类型的神经网络最近在深度学习领域被重新发现，因为它避免了梯度消失问题，并提供了出色的结果和性能。基于LSTM的网络非常适合时间序列的预测和分类，正在取代许多传统的深度学习方法。
- en: 'It''s a hilarious name, but it means exactly what it sounds. The name signifies
    that short-term patterns aren''t forgotten in the long-term. An LSTM network is
    composed of cells (LSTM blocks) linked to each other. Each LSTM block contains
    three types of gate: input gate, output gate, and forget gate, respectively, that
    implement the functions of writing, reading, and resetting the cell memory. These
    gates are not binary, but analogical (generally managed by a sigmoidal activation
    function mapped in the range (0, 1), where 0 indicates total inhibition, and 1
    shows total activation).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个名字很有趣，但它的意思正如其字面所示。这个名字表明短期模式在长期内不会被遗忘。LSTM网络由相互连接的单元（LSTM块）组成。每个LSTM块包含三种类型的门控：输入门、输出门和遗忘门，分别实现写入、读取和重置单元记忆的功能。这些门控不是二进制的，而是模拟的（通常由一个sigmoid激活函数管理，映射到范围(0,
    1)，其中0表示完全抑制，1表示完全激活）。
- en: 'If you consider the LSTM cell as a black box, it can be used very much like
    a basic cell, except it will perform much better; training will converge faster,
    and it will detect long-term dependencies in the data. So how does an LSTM cell
    work? The architecture of a basic LSTM cell is shown in *Figure 7*:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你把LSTM单元看作一个黑箱，它的使用方式与基本单元非常相似，唯一不同的是它的表现会更好；训练过程会更快收敛，而且它能够检测数据中的长期依赖关系。那么，LSTM单元是如何工作的呢？一个基本的LSTM单元架构如*图7*所示：
- en: '![](img/68fcf3fa-5376-443f-94b4-41670eda6887.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/68fcf3fa-5376-443f-94b4-41670eda6887.png)'
- en: 'Figure 7: Block diagram of an LSTM cell'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：LSTM单元的框图
- en: 'Now, let''s see the mathematical notation behind this architecture. If we don''t
    look at what''s inside the LSTM box, the LSTM cell itself looks exactly like a
    regular memory cell, except that its state is split into two vectors, **h(t)**
    and **c(t)**:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这个架构背后的数学符号。如果我们不看LSTM箱子内部的内容，LSTM单元本身看起来就像一个普通的存储单元，唯一的区别是它的状态被分成两个向量，**h(t)**
    和 **c(t)**：
- en: '**c** is a cell'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**c** 是单元'
- en: '**h(t)** is the short-term state'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**h(t)** 是短期状态'
- en: '**c(t)** is the long-term state'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**c(t)** 是长期状态'
- en: Now let's open the box! The key idea is that the network can learn what to store
    in the long-term state, what to throw away, and what to read from it. As the long-term
    state **c[(t-1)]** traverses the network from left to right, you can see that
    it first goes through a forget gate, dropping some memories, and then it adds
    some new memories via the addition operation (which adds the memories that were
    selected by an input gate). The resulting **c(t)** is sent straight out, without
    any further transformation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们打开这个盒子！关键思想是，网络可以学习在长期状态中存储什么，丢弃什么，以及从中读取什么。当长期状态**c[(t-1)]**从左到右遍历网络时，你会看到它首先经过一个遗忘门，丢弃一些记忆，然后通过加法操作（将输入门选择的记忆添加进去）加入一些新的记忆。最终得到的**c(t)**直接输出，不经过进一步的转换。
- en: So, at each timestamp, some memories are dropped and some memories are added.
    Moreover, after the addition operation, the long-term state is copied and passed
    through the **tanh** function, and then the result is filtered by the output gate.
    This produces the short-term state **h(t)** (which is equal to the cell's output
    for this time step **y(t)**). Now let's look at where new memories come from and
    how the gates work. First, the current input vector **x(t)** and the previous
    short-term state **h(t-1)** are fed to four different fully connected layers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在每个时间戳，某些记忆被丢弃，某些记忆被添加。此外，在加法操作后，长期状态会被复制并通过**tanh**函数，然后结果会被输出门过滤。这产生了短期状态**h(t)**（即该时间步的单元输出**y(t)**）。现在让我们看看新记忆是从哪里来的，以及门是如何工作的。首先，将当前输入向量**x(t)**和前一个短期状态**h(t-1)**送入四个不同的全连接层。
- en: 'The presence of these gates allows LSTM cells to remember information for an
    indefinite time; if the input gate is below the activation threshold, the cell
    will retain the previous state, and if the current state is enabled, it will be
    combined with the input value. As the name suggests, the forget gate resets the
    current state of the cell (when its value is cleared to 0), and the output gate
    decides whether the value of the cell must be carried out or not. The following
    equations are used to do the LSTM computations of a cell''s long-term state, its
    short-term state, and its output at each time step for a single instance:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些门的存在使得LSTM单元可以在无限时间内记住信息；如果输入门低于激活阈值，单元将保留之前的状态；如果当前状态被启用，它将与输入值结合。如其名所示，遗忘门会重置单元的当前状态（当其值清零时），而输出门决定是否必须执行单元的值。以下方程用于执行LSTM计算，得到单元在每个时间步的长期状态、短期状态和输出：
- en: '![](img/72f5567e-1bd5-4ab9-abbd-452789a2b46b.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/72f5567e-1bd5-4ab9-abbd-452789a2b46b.png)'
- en: In the preceding equation, *W[xi]*, *W[xf]*, *W[xo]*, and *W[xg]* are the weight
    matrices of each of the four layers for their connection to the input vector *x[(t)]*.
    On the other hand, *W[hi]*, *W[hf]*, *W[ho]*, and *W[hg]* are the weight matrices
    of each of the four layers for their connection to the previous short-term state
    *h[(t-1)]*. Finally, *b[i]*, *b[f]*, *b[o]*, and *b[g]* are the bias terms for
    each of the four layers.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，*W[xi]*、*W[xf]*、*W[xo]*和*W[xg]*是每一层的权重矩阵，用于连接输入向量*x[(t)]*。另一方面，*W[hi]*、*W[hf]*、*W[ho]*和*W[hg]*是每一层的权重矩阵，用于连接前一个短期状态*h[(t-1)]*。最后，*b[i]*、*b[f]*、*b[o]*和*b[g]*是每一层的偏置项。
- en: Now that we know all that, how do both RNN and the LSTM network work? It's time
    to do some hands-on. We will start implementing an MXNet and Scala-based LSTM
    model for HAR.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了这些，那么RNN和LSTM网络是如何工作的呢？是时候动手实践了。我们将开始实现一个基于MXNet和Scala的LSTM模型来进行HAR。
- en: Human activity recognition using the LSTM model
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LSTM模型的人类活动识别
- en: The **Human Activity Recognition** (**HAR**) database was built from the recordings
    of 30 study participants performing **activities of daily living** (**ADL**) while
    carrying a waist-mounted smartphone with embedded inertial sensors. The objective
    is to classify activities into one of the six activities performed.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**人类活动识别**（**HAR**）数据库是通过记录30名研究参与者执行**日常生活活动**（**ADL**）时佩戴带有惯性传感器的腰部智能手机的活动数据构建的。目标是将活动分类为执行的六种活动之一。'
- en: Dataset description
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集描述
- en: The experiments have been carried out with a group of 30 volunteers within an
    age bracket of 19 - 48 years. Each person accomplished six activities, namely
    walking, walking upstairs, walking downstairs, sitting, standing, and laying by
    wearing a Samsung Galaxy S II smartphone on their waist. Using the accelerometer
    and gyroscope, the author captured 3-axial linear acceleration and 3-axial angular
    velocity at a constant rate of 50 Hz.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 实验是在一组30名志愿者中进行的，年龄范围为19至48岁。每个人完成了六项活动，即走路、走楼梯、下楼、坐着、站立和躺下，佩戴的设备是三星Galaxy S
    II智能手机，固定在腰部。通过加速度计和陀螺仪，作者以50 Hz的恒定速率捕获了三轴线性加速度和三轴角速度。
- en: Only two sensors, that is, accelerometer and gyroscope, were used. The sensor
    signals were pre-processed by applying noise filters and then sampled in fixed-width
    sliding windows of 2.56 sec and 50% overlap. This gives 128 readings/window. The
    gravitational and body motion components from the sensor acceleration signal were
    separated via a Butterworth low-pass filter into body acceleration and gravity.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用了两个传感器，即加速度计和陀螺仪。传感器信号通过应用噪声滤波器进行预处理，然后在2.56秒的固定宽度滑动窗口中采样，重叠50%。这意味着每个窗口有128个读数。通过Butterworth低通滤波器将来自传感器加速度信号的重力和身体运动分量分离为身体加速度和重力。
- en: 'For more information, please refer to this paper: Davide Anguita, Alessandro
    Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset
    for *Human Activity Recognition Using Smartphones*. *21st European Symposium on
    Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN*
    2013\. Bruges, Belgium 24-26 April 2013.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参考以下论文：Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra 和
    Jorge L. Reyes-Ortiz。 *使用智能手机的人体活动识别的公开数据集*。*第21届欧洲人工神经网络、计算智能与机器学习研讨会，ESANN*
    2013。比利时布鲁日，2013年4月24日至26日。
- en: For simplicity, the gravitational force is assumed to have only a few but low-frequency
    components. Therefore, a filter of 0.3 Hz cut-off frequency was used. From each
    window, a feature vector was found by calculating variables from the time and
    frequency domain.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为简便起见，假设重力只包含少数但低频的分量。因此，使用了0.3 Hz截止频率的滤波器。从每个窗口中，通过计算时间和频域的变量，得出了一个特征向量。
- en: 'The experiments have been video recorded to label the data manually. The obtained
    dataset has been randomly partitioned into two sets, where 70% of the volunteers
    were selected for generating the training data and 30% the test data. Now, when
    I explore the dataset, both the training and test set have the following file
    structure:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 实验已通过视频录制，手动标注数据。获得的数据集已随机划分为两个集合，其中70%的志愿者用于生成训练数据，30%用于生成测试数据。现在，当我浏览数据集时，训练集和测试集具有以下文件结构：
- en: '![](img/88720740-0311-4da6-b5f1-01ee9d3670a0.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88720740-0311-4da6-b5f1-01ee9d3670a0.png)'
- en: 'Figure 8: HAR dataset file structure'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：HAR数据集文件结构
- en: 'For each record in the dataset, the following is provided:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的每个记录提供以下内容：
- en: Triaxial acceleration from the accelerometer and the estimated body acceleration
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自加速度计的三轴加速度和估计的身体加速度
- en: Triaxial angular velocity from the gyroscope sensor
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自陀螺仪传感器的三轴角速度
- en: A 561-feature vector with time and frequency domain variables
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含时间和频域变量的561特征向量
- en: Its activity label
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的活动标签
- en: An identifier of the subject who carried out the experiment
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行实验的主体的标识符
- en: 'Now we know the problem that needs to be addressed, it''s time to explore the
    technology and related challenges. Well, as I already stated, we will be using
    an MXNet-based LSTM implementation. One question you may ask is: why aren''t we
    using H2O or DeepLearning4j? Well, the answer is that both of them either do not
    have LSTM-based implementation, or cannot be applied to solve this problem.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道需要解决的问题，是时候探索技术和相关挑战了。正如我之前所说，我们将使用基于MXNet的LSTM实现。你可能会问：为什么我们不使用H2O或DeepLearning4j？嗯，答案是这两者要么没有基于LSTM的实现，要么无法应用于解决这个问题。
- en: Setting and configuring MXNet for Scala
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置和配置Scala中的MXNet
- en: Apache MXNet is a flexible and efficient library for deep learning. Building
    a high-performance deep learning library requires many system-level design decisions.
    In this design note, we share the rationale for the specific choices made when
    designing MXNet. We imagine that these insights may be useful to both deep learning
    practitioners and builders of other deep learning systems.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Apache MXNet是一个灵活高效的深度学习库。构建一个高性能的深度学习库需要做出许多系统级设计决策。在这篇设计说明中，我们分享了在设计MXNet时所做的具体选择的理由。我们认为这些见解可能对深度学习实践者以及其他深度学习系统的构建者有所帮助。
- en: 'For this project, we will be needing different packages and libraries: Scala,
    Java, OpenBLAS, ATLAS, OpenCV, and overall, MXNet. Now let''s start configuring
    these tools one by one. For Java and Scala, I am assuming that you already have
    Java and Scala configured. Now the next task is to install build tools and `git`
    since we will be using the MXNet from the GitHub repository. To do this, just
    execute the following commands on Ubuntu:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们将需要不同的包和库：Scala、Java、OpenBLAS、ATLAS、OpenCV，最重要的，还有MXNet。现在让我们一步步地开始配置这些工具。对于Java和Scala，我假设你已经配置好了Java和Scala。接下来的任务是安装构建工具和`git`，因为我们将使用来自GitHub仓库的MXNet。只需要在Ubuntu上执行以下命令：
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we need to install OpenBLAS and ATLAS. These are required for linear algebra
    operations performed by MXNet. To install these, just execute the following command:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要安装OpenBLAS和ATLAS。这些库是MXNet进行线性代数运算时所必需的。要安装它们，只需执行以下命令：
- en: '[PRE1]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We also need to install OpenCV for image processing. Let''s install it by executing
    the following command:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要安装OpenCV进行图像处理。让我们通过执行以下命令来安装它：
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we need to generate the prebuilt MXNet binary. To do this, we need
    to clone and build MXNet for Scala:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要生成预构建的MXNet二进制文件。为此，我们需要克隆并构建MXNet的Scala版本：
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, if the preceding steps went smoothly, a prebuilt-binary for MXNet will
    be generated in `/home/$user_name/mxnet/scala-package/assembly/linux-x86_64-cpu`
    (or `linux-x86_64-gpu` with GPU configured on Linux, and `osx-x86_64-cpu` on macOS).
    Take a look at the following screenshot of the CPU on Ubuntu:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果前面的步骤顺利进行，MXNet的预构建二进制文件将在`/home/$user_name/mxnet/scala-package/assembly/linux-x86_64-cpu`（如果配置了GPU，则为`linux-x86_64-gpu`，在macOS上为`osx-x86_64-cpu`）中生成。请看一下Ubuntu上的CPU截图：
- en: '![](img/037aaeb6-0dec-465d-8936-594577f7e5d9.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/037aaeb6-0dec-465d-8936-594577f7e5d9.png)'
- en: 'Figure 9: MXNet pre-built binary generated'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：生成的MXNet预构建二进制文件
- en: 'Now, the next task before you start writing your Scala code on Eclipse (or
    IntelliJ) as a Maven (or SBT) project, is including this JAR in the build path.
    Additionally, we need some extra dependency for Scala plots and `args4j`:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，开始编写Scala代码之前（在Eclipse或IntelliJ中作为Maven或SBT项目），下一步任务是将这个JAR文件包含到构建路径中。此外，我们还需要一些额外的依赖项来支持Scala图表和`args4j`：
- en: '[PRE4]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Well done! All set and we're ready to go! Let's start coding.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好！一切准备就绪，我们可以开始编码了！
- en: Implementing an LSTM model for HAR
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现一个用于HAR的LSTM模型
- en: 'The overall algorithm (`HumanAR.scala`) has the following workflow:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 整体算法（`HumanAR.scala`）的工作流程如下：
- en: Loading the data
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载数据
- en: Defining hyperparameters
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义超参数
- en: Setting up the LSTM model using imperative programming and the hyperparameters
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用命令式编程和超参数设置LSTM模型
- en: Applying batch wise training, that is, picking batch size data, feeding it to
    the model, then at some iterations evaluating the model and printing the batch
    loss and the accuracy
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用批处理训练，即选择批量大小的数据，将其输入模型，然后在若干次迭代中评估模型，打印批次损失和准确率
- en: Output the chart for the training and test errors
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出训练和测试误差的图表
- en: 'The preceding steps can be followed and constructed by way of a pipeline:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的步骤可以通过管道方式进行跟踪和构建：
- en: '![](img/9db9b808-6ecd-4cbf-aa7e-6ee6a098d27d.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9db9b808-6ecd-4cbf-aa7e-6ee6a098d27d.png)'
- en: 'Figure 10: MXNet pre-built binary generated'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：生成的MXNet预构建二进制文件
- en: Now let's start the implementation step-by-step. Make sure that you understand
    each line of code then import the given project in Eclipse or SBT.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们一步一步地开始实现。确保你理解每一行代码，然后将给定的项目导入到Eclipse或SBT中。
- en: Step 1 - Importing necessary libraries and packages
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤1 - 导入必要的库和包
- en: 'Let''s start coding now. We start from the very beginning, that is, by importing
    libraries and packages:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在开始编码吧。我们从最基础的开始，也就是导入库和包：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Step 2 - Creating MXNet context
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤2 - 创建MXNet上下文
- en: 'Then we create an MXNet context for CPU-based computation. Since I am doing
    it by CPU, I instantiated for the CPU. Feel free to use the GPU if you have already
    configured it by providing the device ID:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们为基于CPU的计算创建一个MXNet上下文。由于我是在使用CPU，所以我为CPU实例化了它。如果你已经配置了GPU，可以通过提供设备ID来使用GPU：
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Step 3 - Loading and parsing the training and test set
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3步 - 加载和解析训练集与测试集
- en: 'Now let''s load the dataset. I am assuming that you copied your dataset to
    the `UCI_HAR_Dataset/` directory. Then, also place the other data files as described
    previously:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们加载数据集。我假设你已经将数据集复制到`UCI_HAR_Dataset/`目录下。然后，还需要将其他数据文件放在之前描述的地方：
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now it''s time to load the training and test set separately. To do this I wrote
    two methods called `loadData()` and `loadLabels()` that are in the `Utils.scala`
    file. These two methods and their signatures will be provided soon:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候分别加载训练集和测试集了。为此，我写了两个方法，分别是`loadData()`和`loadLabels()`，它们位于`Utils.scala`文件中。这两个方法及其签名稍后会提供：
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `loadData()` method loads and maps the data from each `.txt` file based
    on  the input signal type defined by the `INPUT_SIGNAL_TYPES` array in the `Array[Array[Array[Float]]]`
    format:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`loadData()`方法加载并映射来自每个`.txt`文件的数据，基于`INPUT_SIGNAL_TYPES`数组中定义的输入信号类型，格式为`Array[Array[Array[Float]]]`：'
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As stated earlier, the `INPUT_SIGNAL_TYPES` contains some useful constants:
    those are separate, normalized input features for the neural network:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`INPUT_SIGNAL_TYPES`包含了一些有用的常量：它们是神经网络的独立、归一化输入特征：
- en: '[PRE10]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'On the other hand, `loadLabels()` is also a user-defined method that is used
    to load only the labels in the training as well as the test set:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`loadLabels()`也是一个用户定义的方法，用于仅加载训练集和测试集中的标签：
- en: '[PRE11]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The labels are defined in another array as shown in the following code:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 标签在另一个数组中定义，如以下代码所示：
- en: '[PRE12]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Step 4 - Exploratory analysis of the dataset
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4步 - 数据集的探索性分析
- en: 'Now let''s see some statistics about the number of training series (as described
    earlier, this is with 50% overlap between each series), number of test series,
    number of timesteps per series, and number of input parameters per timestep:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一些关于训练系列数量的统计数据（如前所述，每个系列之间有50%的重叠）、测试系列数量、每个系列的时间步数以及每个时间步的输入参数数量：
- en: '[PRE13]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果是：
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Step 5 - Defining internal RNN structure and LSTM hyperparameters
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5步 - 定义内部RNN结构和LSTM超参数
- en: 'Now, let''s define the internal neural network structure and hyperparameters
    for the LSTM network:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义LSTM网络的内部神经网络结构和超参数：
- en: '[PRE15]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Step 6 - LSTM network construction
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6步 - LSTM网络构建
- en: 'Now, let''s set up an LSTM model with the preceding parameters and structure:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用前述参数和结构来设置LSTM模型：
- en: '[PRE16]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In the preceding line, `setupModel()` is the method that does the trick. The
    `getSymbol()` method actually constructs the LSTM cell. We will see its signature,
    too, later on. It accepts sequence length, number of input, number of hidden layers,
    number of labels, batch size, number of LSTM layers, dropout MXNet context, and
    constructs an LSTM model of type using the case class `LSTMModel`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述行中，`setupModel()`是完成此任务的方法。`getSymbol()`方法实际上构建了LSTM单元。稍后我们将看到它的签名。它接受序列长度、输入数量、隐藏层数量、标签数量、批次大小、LSTM层数量、丢弃率MXNet上下文，并使用`LSTMModel`的case类构建LSTM模型：
- en: '[PRE17]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now here''s the signature of the `setupModel()`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这是`setupModel()`的方法签名：
- en: '[PRE18]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the preceding method, we obtained a symbolic model for the deep RNN using
    the `getSymbol()` method that can be seen as follows. I have provided detailed
    comments and believe that will be enough to understand the workflow of the code:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述方法中，我们通过`getSymbol()`方法获得了深度RNN的符号模型，如下所示。我已经提供了详细的注释，并认为这些足以理解代码的工作流程：
- en: '[PRE19]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In summary, the algorithm uses 128 LSTM cells in parallel, and I concatenated
    all 128 cells and fed them to the output activation layer. Let''s concatenate
    the cells, outputs:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，该算法使用128个LSTM单元并行工作，我将这128个单元连接起来并送入输出激活层。让我们来连接这些单元，输出结果：
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then we connect them to an output layer that corresponds to the 6 label:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将它们连接到一个输出层，该层对应6个标签：
- en: '[PRE21]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the preceding code segment, `LSTMState` and `LSTMParam` are two case classes
    that used to define the state of each LSTM cell and the latter accepts the parameters
    needed to construct an LSTM cell. final case class `LSTMState(c: Symbol, h: Symbol)`
    final case class `LSTMParam(i2hWeight: Symbol, i2hBias: Symbol, h2hWeight: Symbol,
    h2hBias: Symbol)`.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '在前面的代码片段中，`LSTMState`和`LSTMParam`是两个案例类，用于定义每个LSTM单元的状态，后者接受构建LSTM单元所需的参数。最终案例类`LSTMState(c:
    Symbol, h: Symbol)`，`LSTMParam(i2hWeight: Symbol, i2hBias: Symbol, h2hWeight:
    Symbol, h2hBias: Symbol)`。'
- en: 'Now it''s time to discuss the most important step, which is LSTM cell construction.
    We will use some diagrams and legends as shown in the following diagram:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是讨论最重要的步骤——LSTM单元构建的时候了。我们将使用一些图示和图例，如下图所示：
- en: '![](img/fb82303a-6dd0-4ddc-b518-940435467dcb.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb82303a-6dd0-4ddc-b518-940435467dcb.png)'
- en: 'Figure 11: Legends used to describe LSTM cell in the following'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：在接下来的内容中描述LSTM单元所用的图例
- en: 'The repeating module in an LSTM contains four interacting layers as shown in
    the following figure:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM中的重复模块包含四个相互作用的层，如下图所示：
- en: '![](img/47c687ee-3a27-4670-bc63-8f825d976de1.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47c687ee-3a27-4670-bc63-8f825d976de1.png)'
- en: 'Figure 12: Inside an LSTM cell, that is the, repeating module in an LSTM contains
    four interacting layers'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：在LSTM单元内部，即LSTM中的重复模块包含四个相互作用的层
- en: 'An LSTM cell is defined by its stats and parameters, as defined by the preceding
    two case classes:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一个LSTM单元由其状态和参数定义，如前面两个案例类所定义：
- en: '**LSTM state**: **c** is the cell stat (its memory knowledge) to be used during
    the training and **h** is the output'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LSTM状态**：**c**是单元状态（它的记忆知识），用于训练过程中，**h**是输出'
- en: '**LSTM parameters**: To be optimized by the training algorithm'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LSTM参数**：通过训练算法进行优化'
- en: '**i2hWeight**: Input to hidden weight'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**i2hWeight**：输入到隐藏的权重'
- en: '**i2hBias**: Input to hidden bias'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**i2hBias**：输入到隐藏的偏置'
- en: '**h2hWeight**: Hidden to hidden weight'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**h2hWeight**：隐藏到隐藏的权重'
- en: '**h2hBias**: Hidden to hidden bias'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**h2hBias**：隐藏到隐藏的偏置'
- en: '**i2h**: An NN for input data'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**i2h**：输入数据的神经网络'
- en: '**h2h**: An NN from the previous **h**'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**h2h**：来自前一个**h**的神经网络'
- en: 'In the code, the two fully connected layers have been created, concatenated,
    and transformed to four copies by the following code. Let''s add a hidden layer
    of size `numHidden * 4` (`numHidden` set to 28) that takes as input the `inputdata`:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，两个全连接层已经创建、连接，并通过以下代码转换为四个副本。让我们添加一个大小为`numHidden * 4`（`numHidden`设置为28）的隐藏层，它以`inputdata`作为输入：
- en: '[PRE22]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then we add a hidden layer of size `numHidden * 4` (`numHidden` set to 28)
    that takes as input the previous output of the cell:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们添加一个大小为`numHidden * 4`（`numHidden`设置为28）的隐藏层，它以单元的先前输出作为输入：
- en: '[PRE23]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now let''s concatenate them together:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将它们连接起来：
- en: '[PRE24]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then let''s make four copies of gates before we compute the gates:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们在计算门之前制作四个副本：
- en: '[PRE25]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then we compute the gates:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算各个门：
- en: '[PRE26]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now the activation for the forget gate is represented by the following code:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，遗忘门的激活表示为以下代码：
- en: '[PRE27]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can see this in the following figure:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下图示中看到这一点：
- en: '![](img/11da362b-ef68-4a02-bd87-bf8c8ac8b0c0.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/11da362b-ef68-4a02-bd87-bf8c8ac8b0c0.png)'
- en: 'Figure 13: Forget gate in an LSTM cell'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：LSTM单元中的遗忘门
- en: 'Now, the activation for the in gate and in transform are represented by the
    following code:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，输入门和输入变换的激活表示为以下代码：
- en: '[PRE28]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We can also see this in *Figure 14:*
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在*图14*中看到这一点：
- en: '![](img/358d8386-48d9-498a-92e8-e71974a4cd78.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/358d8386-48d9-498a-92e8-e71974a4cd78.png)'
- en: 'Figure 14: In gate and transform gate in an LSTM cell'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：LSTM单元中的输入门和变换门
- en: 'The next state is defined by the following code:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个状态由以下代码定义：
- en: '[PRE29]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The preceding code can be represented by the following figure too:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码也可以用以下图示表示：
- en: '![](img/ed5bd699-4ed1-40d4-9dab-01afb9246576.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed5bd699-4ed1-40d4-9dab-01afb9246576.png)'
- en: 'Figure 15: Next or transited gate in an LSTM cell'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：LSTM单元中的下一个或转换门
- en: 'Finally, the output gate can be represented by the following code:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，输出门可以用以下代码表示：
- en: '[PRE30]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The preceding code can be represented by the following figure too:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码也可以用以下图示表示：
- en: '![](img/8bb371d3-d4e8-4768-a8c4-4aeecc5494b2.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8bb371d3-d4e8-4768-a8c4-4aeecc5494b2.png)'
- en: 'Figure 16: Output gate in an LSTM cell'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：LSTM单元中的输出门
- en: 'Too much of a mouthful? No worries, here I have provided the full code for
    this method:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 太复杂了？没关系，这里我提供了该方法的完整代码：
- en: '[PRE31]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Step 7 - Setting up an optimizer
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤7 - 设置优化器
- en: 'As suggested by many researchers, the `RMSProp` optimizer helps an LSTM network
    to converge quickly. Therefore, I have decided to use it here too:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 正如许多研究人员所建议的，`RMSProp`优化器帮助LSTM网络快速收敛。因此，我也决定在这里使用它：
- en: '[PRE32]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Additionally, the model parameters to be optimized are its parameters, except
    the training data and the label (weights and biases):'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，待优化的模型参数是其所有参数，除了训练数据和标签（权重和偏置）：
- en: '[PRE33]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Step 8 - Training the LSTM network
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8步 - 训练LSTM网络
- en: 'Now we will start training the LSTM network. However, before getting started,
    let''s try to define some variables to keep track of the training''s performance:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将开始训练LSTM网络。不过，在开始之前，我们先定义一些变量来跟踪训练的表现：
- en: '[PRE34]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Then, we start performing the training steps with `batch_size` iterations at
    each loop:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们开始执行训练步骤，每次循环进行`batch_size`次迭代：
- en: '[PRE35]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Don''t get derailed but take a quick look at *step 6* previously, where we
    have instantiated the LSTM model. Now it''s time to feed the input and labels
    to the RNN:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 不要偏离主题，但快速回顾一下*第6步*，我们在这里实例化了LSTM模型。现在是时候将输入和标签传递给RNN了：
- en: '[PRE36]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then we do forward and backward passes:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们进行前向和后向传播：
- en: '[PRE37]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Additionally, we need to update the parameters using the `RMSProp` optimizer
    that we defined in *step 7*:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们需要使用在*第7步*中定义的`RMSProp`优化器来更新参数：
- en: '[PRE38]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'It would also be great to get metrics such as training errors—that is, loss
    and accuracy over the training data:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 获取如训练误差（即训练数据上的损失和准确度）等指标也会非常有用：
- en: '[PRE39]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In the preceding code segment, `getAccAndLoss()` is a method that computes
    the loss and accuracy and can be seen as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码段中，`getAccAndLoss()`是一个计算损失和准确度的方法，具体实现如下：
- en: '[PRE40]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Additionally, it would be exciting to evaluate only the network at some steps
    for faster training:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了更快的训练，评估网络的某些步骤是很令人兴奋的：
- en: '[PRE41]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Step 9 - Evaluating the model
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9步 - 评估模型
- en: 'Well done! We have finished the training. How about now evaluating the test
    set:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好！我们已经完成了训练。现在如何评估测试集呢：
- en: '[PRE42]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Yahoo! We have managed to achieve 94% accuracy, which is really outstanding.
    In the previous code, `test()` is the method used for evaluating the performance
    of the model. The signature of the model is given in the following code:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！我们成功达到了94%的准确度，真是非常棒。在之前的代码中，`test()`是用来评估模型性能的方法。模型的签名如下所示：
- en: '[PRE44]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'When done, it''s good practice to destroy the model to release resources:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，最好销毁模型以释放资源：
- en: '[PRE45]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We saw earlier that we achieved up to 93% accuracy on the test set. How about
    seeing the previous accuracy and errors in a graph:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到，在测试集上取得了高达93%的准确率。那么，如何通过图形展示之前的准确度和误差呢：
- en: '[PRE46]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![](img/60e98ce8-9c88-4c7d-9682-ec49100b8391.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60e98ce8-9c88-4c7d-9682-ec49100b8391.png)'
- en: 'Figure 17: Training and test losses and accuracies per iteration'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：每次迭代的训练和测试损失及准确度
- en: From the preceding graph, it is clear that with only a few iterations, our LSTM
    converged well and produced very good classification accuracy.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表来看，很明显，经过几次迭代，我们的LSTM模型很好地收敛，并且产生了非常好的分类准确度。
- en: Tuning LSTM hyperparameters and GRU
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整LSTM超参数和GRU
- en: 'Nevertheless, I still believe it is possible to attain about 100% accuracy
    with more LSTM layers. The following are the hyperparameters that I would still
    try to tune to see the accuracy:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我仍然相信，通过增加更多的LSTM层，能够达到接近100%的准确率。以下是我仍然会尝试调整的超参数，以便查看准确度：
- en: '[PRE47]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: There are many other variants of the LSTM cell. One particularly popular variant
    is the **Gated Recurrent Unit** (**GRU**) cell, which is a slightly dramatic variation
    on the LSTM. It also merges the cell state and hidden state and makes some other
    changes. The resulting model is simpler than standard LSTM models and has been
    growing increasingly popular. This cell was proposed by Kyunghyun Cho et al. in
    a 2014 paper that also introduced the encoder-decoder network we mentioned earlier.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM单元有很多其他变种。其中一个特别流行的变种是**门控循环单元**（**GRU**）单元，它是LSTM的稍微变化形式。它还将单元状态和隐藏状态合并，并做了一些其他改动。结果模型比标准的LSTM模型更简单，并且越来越受欢迎。这个单元是Kyunghyun
    Cho等人在2014年的一篇论文中提出的，论文还介绍了我们之前提到的编码器-解码器网络。
- en: 'For this type of LSTM, interested readers should refer to the following publications:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种类型的LSTM，感兴趣的读者可以参考以下文献：
- en: '*Learning Phrase Representations using RNN Encoder-Decoder for Statistical
    Machine Translation*, K. Cho et al. (2014).'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用RNN编码器-解码器进行统计机器翻译的学习短语表示*，K. Cho 等人（2014）。'
- en: 'A 2015 paper by Klaus Greff et al., *LSTM: A Search Space Odyssey*, seems to
    show that all LSTM variants perform roughly the same.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Klaus Greff 等人于 2015 年发表的论文 *LSTM: A Search Space Odyssey*，似乎表明所有 LSTM 变体的表现大致相同。'
- en: 'Technically, a GRU cell is a simplified version of an LSTM cell, where both
    the state vectors are merged into a single vector called **h(t)**. A single gate
    controller controls both the forget gate and the input gate. If the gate controller
    outputs a 1, the input gate is open and the forget gate is closed:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，GRU 单元是 LSTM 单元的简化版，其中两个状态向量合并成一个叫做 **h(t)** 的向量。一个单一的门控制器控制着遗忘门和输入门。如果门控制器输出
    1，输入门打开，遗忘门关闭：
- en: '![](img/30895ce6-726f-4424-b456-f5c2ef15f40f.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30895ce6-726f-4424-b456-f5c2ef15f40f.png)'
- en: 'Figure 18: Internal structure of a GRU cell'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：GRU 单元的内部结构
- en: 'On the other hand, if it outputs a 0, the opposite happens. Whenever a memory
    must be stored, the location where it will be stored is erased first, which is
    actually a frequent variant to the LSTM cell in and of itself. The second simplification
    is that since the full state vector is output at every time step, there is no
    output gate. However, there is a new gate controller introduced that controls
    which part of the previous state will be shown to the main layer. The following
    equations are used to do the GRU computations of a cell''s long-term state, its
    short-term state, and its output at each time step for a single instance:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果输出为 0，则会发生相反的情况。每当需要存储记忆时，首先会清除它将被存储的位置，这实际上是 LSTM 单元的一种常见变体。第二个简化是，由于每个时间步都会输出完整的状态向量，因此没有输出门。然而，引入了一个新的门控制器，用来控制前一个状态的哪个部分会显示给主层。以下方程用于进行
    GRU 单元在每个时间步长的长短期状态计算及输出：
- en: '![](img/e585549b-3e79-453e-a74b-3e1c044ce67c.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e585549b-3e79-453e-a74b-3e1c044ce67c.png)'
- en: The LSTM and GRU cells are one of the main reasons for the success of RNNs in
    recent years, in particular for applications in NLP.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 和 GRU 单元是近年来 RNN 成功的主要原因之一，尤其是在 NLP 应用中。
- en: Summary
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we have seen how to develop an ML project using the RNN implementation,
    and called LSTM for HAR using the smartphones dataset. Our LSTM model has been
    able to classify the type of movement from six categories: walking, walking upstairs,
    walking downstairs, sitting, standing, and lying. In particular, we have achieved
    up to 94% accuracy. Later on, we discussed some possible ways to improve the accuracy
    further using GRU cell.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经学习了如何使用 RNN 实现开发 ML 项目，并使用智能手机数据集进行 HAR 的 LSTM 模型。我们的 LSTM 模型能够从六个类别中分类运动类型：步行、走楼梯、下楼梯、坐着、站着和躺着。特别地，我们达到了
    94% 的准确率。接着，我们讨论了如何通过使用 GRU 单元进一步提高准确性的一些可能方法。
- en: A **convolutional neural network** (**CNN**) is a type of feedforward neural
    network in which the connectivity pattern between its neurons is inspired by the
    animal visual cortex. Over the last few years, CNNs have demonstrated superhuman
    performance in complex visual tasks such as image search services, self-driving
    cars, automatic video classification, voice recognition, and **natural language
    processing** (**NLP**).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNN**）是一种前馈神经网络，其中神经元之间的连接模式受到动物视觉皮层的启发。近年来，CNN 在复杂的视觉任务中表现出超越人类的性能，如图像搜索服务、自动驾驶汽车、自动视频分类、语音识别和
    **自然语言处理**（**NLP**）。'
- en: Considering these, in the next chapter we will see how to develop an end-to-end
    project for handling a multi-label (that is, each entity can belong to multiple
    classes) image classification problem using CNN based on the Scala and Deeplearning4j
    framework on real Yelp image datasets. We will also discuss some theoretical aspects
    of CNNs before getting started. Furthermore, we will discuss how to tune hyperparameters
    for better classification results.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些，在下一章我们将看到如何开发一个端到端项目，使用基于 Scala 和 Deeplearning4j 框架的 CNN 来处理多标签（即每个实体可以属于多个类别）图像分类问题，并且使用真实的
    Yelp 图像数据集。我们还将在开始之前讨论一些 CNN 的理论方面。更进一步地，我们将讨论如何调整超参数，以获得更好的分类结果。
