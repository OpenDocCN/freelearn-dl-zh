- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Applying Whisper in Various Contexts
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在各种环境中应用 Whisper
- en: Welcome to [*Chapter 5*](B21020_05.xhtml#_idTextAnchor142), where we explore
    the remarkable capabilities of OpenAI’s Whisper in transforming spoken language
    into written text. As we navigate various applications, including transcription
    services, voice assistants, chatbots, and accessibility features, you’ll gain
    an in-depth understanding of Whisper’s pivotal role in these domains.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到 [*第 5 章*](B21020_05.xhtml#_idTextAnchor142)，在这里我们探索 OpenAI 的 Whisper 在将口语转化为书面文本方面的卓越能力。在探讨包括转录服务、语音助手、聊天机器人和辅助功能等多个应用时，你将深入了解
    Whisper 在这些领域中的关键作用。
- en: First, we will explore transcription services and examine how Whisper streamlines
    the conversion of audio files, such as meetings and interviews, into text. Its
    accuracy and efficiency reduce the need for manual transcription, making it an
    indispensable tool.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将探讨转录服务，并研究 Whisper 如何简化音频文件（如会议和采访）的文本转换。它的准确性和效率减少了手动转录的需求，使其成为一项不可或缺的工具。
- en: Furthermore, we’ll delve into the integration of Whisper into voice assistants
    and chatbots, enhancing their responsiveness and user interaction. By converting
    spoken commands into text, Whisper elevates these technologies to new levels of
    interactivity.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将深入探讨 Whisper 在语音助手和聊天机器人中的集成，提升它们的响应能力和用户互动。通过将语音命令转换为文本，Whisper 将这些技术提升到新的互动水平。
- en: Regarding accessibility, this chapter highlights Whisper’s contribution to tools
    for those with hearing or speech impairments. Its **voice-to-text** features not
    only offer practical solutions but also enrich user experiences.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 关于辅助功能，本章突出了 Whisper 对听力或语言障碍人士工具的贡献。其 **语音转文本** 功能不仅提供了实用的解决方案，还丰富了用户体验。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Exploring transcription services
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索转录服务
- en: Integrating Whisper into voice assistants and chatbots
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Whisper 集成到语音助手和聊天机器人中
- en: Enhancing accessibility features with Whisper
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Whisper 增强辅助功能
- en: By the end of this chapter, you will have a comprehensive understanding of how
    to apply Whisper effectively in various settings. You’ll learn about the best
    practices for setup and optimization, discover innovative use cases, and appreciate
    ethical considerations in implementing this technology. With this knowledge, you’ll
    be well equipped to leverage Whisper’s full potential to enhance digital experiences
    across different domains.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你将全面了解如何在各种环境中有效应用 Whisper。你将学习最佳的设置和优化实践，发现创新的使用案例，并理解实施这项技术时的伦理考量。有了这些知识，你将能够充分利用
    Whisper 的潜力，在不同领域提升数字体验。
- en: Let’s start by delving into the innovative world of transcription through Whisper,
    where we uncover how this cutting-edge technology is reshaping the way we convert
    spoken language into written text, enhancing efficiency and accuracy across various
    professional and personal settings.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从深入探索通过 Whisper 进行转录的创新世界开始，揭示这项前沿技术如何重塑我们将口语转化为书面文本的方式，提高效率和准确性，适用于各种专业和个人环境。
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To harness the capabilities of OpenAI’s Whisper for advanced applications, this
    chapter leverages Python and Google Colab for ease of use and accessibility. The
    Python environment setup includes the Whisper library for transcription tasks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用 OpenAI 的 Whisper 实现高级应用，本章采用 Python 和 Google Colab，以便于使用和提高可访问性。Python
    环境设置包括用于转录任务的 Whisper 库。
- en: 'Key requirements:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 关键要求：
- en: '**Google Colab notebooks**: The notebooks are set to run our Python code with
    the minimum required memory and capacity. If the **T4 GPU** runtime type is available,
    select it for better performance.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Colab 笔记本**：这些笔记本设置为使用最低要求的内存和容量运行我们的 Python 代码。如果可用，选择 **T4 GPU**
    运行时类型以获得更好的性能。'
- en: '**Python environment**: Each notebook contains directives to load the required
    Python libraries, including Whisper and Gradio.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Python 环境**：每个笔记本都包含加载所需 Python 库的指令，包括 Whisper 和 Gradio。'
- en: '**Hugging Face account**: Some notebooks require a Hugging Face account and
    login API key. The Colab notebooks include information about this topic.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hugging Face 账户**：一些笔记本需要 Hugging Face 账户和登录 API 密钥。Colab 笔记本中包含了关于这一主题的信息。'
- en: '**Microphone and speakers**: Some notebooks implement a Gradio app with voice
    recording and audio playback. A microphone and speakers connected to your computer
    might help you experience the interactive voice features. Another option is to
    open the URL link Gradio provides at runtime on your mobile phone; from there,
    you might be able to use the phone’s microphone to record your voice.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**麦克风和扬声器**：一些笔记本实现了一个带有语音录制和音频播放功能的 Gradio 应用程序。连接到您计算机的麦克风和扬声器可能有助于您体验互动语音功能。另一种选择是通过
    Gradio 在运行时提供的 URL 链接在手机上打开；在该链接中，您可能可以使用手机的麦克风录制您的声音。'
- en: '**GitHub repository access**: All Python code, including examples, is available
    in the chapter’s GitHub repository ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter05](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter05)).
    These Colab notebooks are ready to run, providing a practical and hands-on approach
    to learning.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GitHub 仓库访问**：所有 Python 代码，包括示例，均可在本章的 GitHub 仓库中找到（[https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter05](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter05)）。这些
    Colab 笔记本可以直接运行，为学习提供了实用的动手实践方式。'
- en: By meeting these technical requirements, you will be prepared to explore Whisper
    in different contexts while enjoying the streamlined experience of Google Colab
    and the comprehensive resources available on GitHub.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过满足这些技术要求，您将为在不同环境中探索 Whisper 做好准备，同时享受 Google Colab 提供的流畅体验以及 GitHub 上的全面资源。
- en: Exploring transcription services
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索转录服务
- en: From capturing the nuances of a brainstorming session to documenting pivotal
    interviews, transcription services bridge the gap between the ephemeral nature
    of speech and the permanence of text. Within this exploration, we will unravel
    the intricate dance between Whisper’s advanced technology and ever-expanding transcription
    needs. This section lays the foundational knowledge of how Whisper, with its encoder-decoder
    transformer model, tackles diverse acoustic environments, accents, and dialects
    with remarkable precision. Yet, it doesn’t shy away from discussing current limitations
    and vibrant community efforts to push the boundaries further.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从捕捉头脑风暴会议的微妙差异到记录关键访谈，转录服务架起了语言的短暂性与文字的永久性之间的桥梁。在本节中，我们将揭示 Whisper 的先进技术与日益增长的转录需求之间复杂的互动。这一部分为您奠定了基础知识，展示了
    Whisper 如何利用其编码器-解码器变换器模型，以卓越的精度应对各种音响环境、口音和方言。然而，它也不回避讨论当前的局限性以及充满活力的社区努力，以推动其进一步发展。
- en: We will also transition from the theoretical to the practical. From installing
    dependencies to running the model, it equips you with the knowledge to turn audio
    files into accurate text transcripts efficiently. We will optimize Whisper’s performance,
    ensuring transcriptions are accurate and seamlessly integrated into various applications,
    from subtitling to detailed content analysis.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将从理论过渡到实践。从安装依赖项到运行模型，本章将使您掌握将音频文件转化为准确文本转录的知识，确保 Whisper 的表现得到优化，确保转录内容准确无误，并无缝整合到各种应用程序中，从字幕生成到详细内容分析。
- en: By the end of this section, you’ll have grasped Whisper’s vital role in transcription
    services and be armed with the know-how to harness its capabilities effectively.
    This journey is a pathway to unlocking the full potential of voice within the
    digital landscape, making information accessible, and enhancing communication
    across diverse domains.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本节结束时，您将掌握 Whisper 在转录服务中的重要作用，并具备有效利用其功能的知识。这段旅程是开启数字领域中声音潜力的道路，使信息更易获取，并在各个领域提升沟通效率。
- en: Understanding the role of Whisper in transcription services
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Whisper 在转录服务中的作用
- en: Understanding the role of Whisper in transcription services requires a deep
    dive into its capabilities, limitations, and potential for integration into various
    applications. As we embark on this exploration, we will not only appreciate the
    technical prowess of Whisper but also consider its practical implications in the
    transcription landscape.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 Whisper 在转录服务中的作用，需要深入探索其能力、局限性以及在各类应用中的整合潜力。在这一探索过程中，我们不仅会欣赏 Whisper 的技术实力，还会考虑其在转录领域的实际应用影响。
- en: Whisper’s architecture, an encoder-decoder transformer model, is adept at handling
    a wide range of audio inputs. Whisper ensures that each speech segment is given
    attention by converting audio into a log-Mel spectrogram and processing it in
    30-second chunks. This meticulous approach to audio processing is one of the reasons
    behind Whisper’s high accuracy in transcription.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper的架构是一个编码器-解码器变压器模型，擅长处理各种音频输入。Whisper通过将音频转换为log-Mel频谱图并按30秒的时间段处理，确保每个语音片段都得到关注。这种细致的音频处理方法是Whisper在转录中高准确率的原因之一。
- en: The robustness of Whisper to accents, background noise, and technical language
    is particularly noteworthy. In transcription services, these factors are often
    the bane of accuracy and reliability. Whisper’s resilience in these areas means
    it can provide high-quality transcriptions across diverse acoustic conditions,
    which is invaluable for businesses and individuals requiring precise spoken content
    documentation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper对口音、背景噪声和技术性语言的强大适应性尤为值得注意。在转录服务中，这些因素通常是影响准确性和可靠性的“祸根”。Whisper在这些领域的抗干扰能力意味着它能够在多种声学条件下提供高质量的转录，对于需要精确口语内容文档的企业和个人来说具有不可估量的价值。
- en: While Whisper excels at transcription, it is essential to note its limitations
    in speaker diarization, distinguishing between different speakers in an audio
    file. However, the community around Whisper is actively exploring ways to enhance
    its capabilities, for example, integrating it with other models such as **Pyannote**
    for speaker identification. We will learn more about diarization and Pyannote
    in the following chapters. Additionally, Whisper’s word-level timestamping feature
    is a significant step forward, enabling users to synchronize transcribed text
    with audio, a crucial requirement for applications such as subtitling and detailed
    content analysis.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Whisper在转录方面表现出色，但需要注意的是它在讲话者分离（即区分音频文件中的不同说话者）方面的局限性。然而，Whisper的社区正在积极探索增强其功能的方法，例如将其与其他模型（如**Pyannote**）结合进行说话人识别。在接下来的章节中，我们将进一步了解讲话者分离和Pyannote。此外，Whisper的单词级时间戳功能是一个重要的进步，允许用户将转录文本与音频同步，这对于字幕制作和详细内容分析等应用至关重要。
- en: A brief introduction to Pyannote
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Pyannote简介
- en: Pyannote is an open source toolkit designed for speaker diarization, a process
    crucial in analyzing conversations by identifying when and by whom each utterance
    is spoken. Developed by Hervé Bredin, Pyannote leverages the PyTorch machine learning
    framework to provide trainable, end-to-end neural components. These components
    can be combined and jointly optimized to construct speaker diarization pipelines.
    `pyannote.audio`, one element of this toolkit, comes with pre-trained models and
    pipelines that cover a wide range of domains, including **voice activity** **detection**
    (**VAD**), speaker segmentation, overlapped speech detection, and speaker embedding.
    It achieves state-of-the-art performance in most of these areas.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Pyannote是一个开源工具包，专为讲话者分离设计，这一过程对于通过识别每个发言的时间和发言者来分析对话至关重要。Pyannote由Hervé Bredin开发，利用PyTorch机器学习框架提供可训练的端到端神经组件。这些组件可以组合并联合优化，以构建讲话者分离流水线。`pyannote.audio`是该工具包的一个组成部分，提供了包括**语音活动检测**
    (**VAD**)、讲话者分割、重叠语音检测和讲话者嵌入等多个领域的预训练模型和流水线。在大多数这些领域，它都达到了最先进的性能。
- en: The relationship between Pyannote and OpenAI Whisper in the context of diarization
    is complementary. Pyannote can perform the diarization task, identifying different
    speakers within an audio file, which Whisper can transcribe. This synergy allows
    for creating more detailed and valuable transcriptions that include speaker labels,
    enhancing the analysis of conversations. However, integrating these two systems
    can be complex and may only sometimes yield ideal results, as noted by some users.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在讲话者分离的背景下，Pyannote与OpenAI Whisper的关系是互补的。Pyannote可以执行讲话者分离任务，识别音频文件中的不同说话者，而Whisper可以进行转录。这种协同作用使得创建更详细且有价值的转录成为可能，其中包含了讲话者标签，从而增强了对话分析。然而，整合这两个系统可能比较复杂，且并非总能产生理想的结果，正如一些用户所指出的。
- en: Despite these challenges, combining Pyannote’s diarization capabilities with
    Whisper’s transcription prowess represents a powerful tool for speech analysis,
    especially when accurate speaker identification is required.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些挑战，将Pyannote的讲话者分离能力与Whisper的转录能力相结合，代表了语音分析的强大工具，特别是在需要准确识别讲话者的情况下。
- en: From a business perspective, the cost of transcription services is a critical
    factor. If using OpenAI’s API, Whisper’s competitive pricing at $0.006 per minute
    of audio makes it an attractive option for companies looking to incorporate transcription
    services without incurring excessive costs. Of course, Whisper is available via
    open source as well. This affordability and high accuracy position Whisper as
    a disruptive force in the transcription market.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从商业角度来看，转录服务的成本是一个关键因素。如果使用 OpenAI 的 API，Whisper 每分钟音频的定价为 0.006 美元，使其成为企业在不承担过高成本的情况下引入转录服务的一个有吸引力的选择。当然，Whisper
    也可以通过开源获得。这种价格实惠和高准确性的结合，使 Whisper 成为转录市场中的颠覆性力量。
- en: The Whisper API’s file size limit of 25 MB is a consideration for developers
    integrating the model into applications. While this may pose challenges for longer
    audio files, the community has devised strategies to work around this limitation,
    such as splitting audio files and using compressed formats. The API’s ease of
    use and the potential for real-time transcription further enhance Whisper’s appeal
    as a developer tool.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper API 的文件大小限制为 25 MB，这是开发者在将模型集成到应用程序时需要考虑的因素。尽管这对于较长的音频文件可能带来挑战，但社区已经提出了绕过这一限制的策略，例如拆分音频文件和使用压缩格式。API
    的易用性以及实时转录的潜力，进一步增强了 Whisper 作为开发者工具的吸引力。
- en: OpenAI’s decision to open source Whisper has catalyzed innovation and customization.
    By providing access to the model’s code and weights, OpenAI has empowered a community
    of developers to adapt and extend Whisper’s capabilities. This leads to a modular
    future for AI, where tools such as Whisper serve as foundational building blocks
    for many applications.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 决定开源 Whisper，催生了创新和定制化的进程。通过提供模型代码和权重的访问权限，OpenAI 使得开发者社区能够调整和扩展 Whisper
    的功能。这引领了人工智能的模块化未来，像 Whisper 这样的工具将成为许多应用的基础构件。
- en: 'As we look to the future, the role of Whisper in transcription services is
    set to become even more integral. With the model’s continuous evolution and the
    growth of its surrounding community, we can anticipate advancements in diarization,
    language support, and other areas. The open source nature of Whisper ensures that
    it will remain at the forefront of innovation, driven by a collaborative effort
    to refine and perfect its transcription capabilities. This sets the stage for
    our next topic of discussion: setting up Whisper for transcription tasks, where
    we will delve into the practical steps and considerations for harnessing Whisper’s
    capabilities to meet transcription needs effectively.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，Whisper 在转录服务中的角色将变得更加重要。随着模型的不断演进以及其周围社区的成长，我们可以预见在说话人分离、语言支持以及其他领域的进展。Whisper
    的开源性质确保它将始终处于创新的前沿，由一个协作努力推动，不断完善其转录能力。这为我们接下来的讨论主题奠定了基础：设置 Whisper 用于转录任务，在这里我们将深入探讨如何利用
    Whisper 的能力，满足转录需求的实际步骤和注意事项。
- en: Setting up Whisper for transcription tasks
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 Whisper 用于转录任务
- en: 'Setting up Whisper for transcription tasks involves several steps, including
    installing dependencies, installing Whisper, and running the model. Use the `LOAIW_ch05_1_setting_up_Whisper_for_transcription.ipynb`
    Google Colab notebook ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter05/LOAIW_ch05_1_setting_up_Whisper_for_transcription.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter05/LOAIW_ch05_1_setting_up_Whisper_for_transcription.ipynb))
    from the book’s GitHub repository for more comprehensive hands-on implementation.
    In the notebook, we’ll walk through the end-to-end process of preparing your environment,
    downloading sample audio, and transcribing it with Whisper. The following diagram
    describes the high-level steps:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 Whisper 用于转录任务包括多个步骤，其中包括安装依赖项、安装 Whisper 和运行模型。可以使用书籍 GitHub 仓库中的 `LOAIW_ch05_1_setting_up_Whisper_for_transcription.ipynb`
    Google Colab 笔记本 ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter05/LOAIW_ch05_1_setting_up_Whisper_for_transcription.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter05/LOAIW_ch05_1_setting_up_Whisper_for_transcription.ipynb))
    获取更全面的实践实现。在该笔记本中，我们将逐步讲解从准备环境、下载示例音频到使用 Whisper 进行转录的全过程。下图描述了整体步骤：
- en: '![Figure 5.1 – Setting up Whisper for transcription tasks](img/B21020_05_1.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 设置 Whisper 用于转录任务](img/B21020_05_1.jpg)'
- en: Figure 5.1 – Setting up Whisper for transcription tasks
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 设置 Whisper 用于转录任务
- en: '*Figure 5**.1* describes the step-by-step approach in the notebook, ensuring
    you have a solid foundation in using Whisper, from basic setup to exploring advanced
    transcription techniques. I encourage you to find and run the entire notebook
    from the GitHub repository. Here are the high-level steps with some selected code
    snippets to illustrate:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.1* 描述了笔记本中的逐步方法，确保你在使用 Whisper 时建立了扎实的基础，从基本设置到探索高级转录技术。我鼓励你从 GitHub 仓库中找到并运行整个笔记本。以下是一些高层步骤和部分代码片段，供你参考：'
- en: '**Installing necessary dependencies**: We begin by setting up our environment
    and installing crucial packages:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**安装必要的依赖项**：我们首先设置环境并安装关键包：'
- en: '[PRE0]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Downloading audio samples**: Next, we download various audio samples, both
    in English and Spanish, from a GitHub repository and OpenAI’s **content delivery
    network** or **CDN**. These samples will serve as our testing ground, allowing
    us to explore Whisper’s transcription capabilities across different languages.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**下载音频样本**：接下来，我们从 GitHub 仓库和 OpenAI 的 **内容分发网络**（CDN）下载多种音频样本，包括英语和西班牙语。这些样本将作为我们的测试用例，帮助我们探索
    Whisper 在不同语言中的转录能力。'
- en: '**Verifying compute resources**: We check GPU availability to ensure efficient
    processing. Whisper’s performance significantly benefits from GPU acceleration,
    so we configure our environment to use the GPU if available:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**验证计算资源**：我们检查 GPU 的可用性，以确保高效处理。Whisper 的性能显著受益于 GPU 加速，因此我们配置环境以在 GPU 可用时使用它：'
- en: '[PRE1]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`"medium"` size multilingual model, choosing a specific configuration that
    suits our needs:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`"medium"` 大小的多语言模型，选择适合我们需求的特定配置：'
- en: '[PRE2]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Setting up the Natural Language Toolkit (NLTK) for text processing**: We
    install and set up NLTK to enhance the readability of our transcriptions. NLTK
    helps segment the transcribed text, making it easier to read and understand.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置自然语言工具包 (NLTK) 进行文本处理**：我们安装并配置 NLTK 以增强转录的可读性。NLTK 帮助将转录文本分割，使其更易阅读和理解。'
- en: '`whisper.DecodingOptions()` function in OpenAI’s `Whisper` class is used to
    specify various options that control the behavior of the decoding process when
    transcribing audio. The parameters in the `DecodingOptions` function allow users
    to specify options such as the language for transcription, whether timestamps
    should be included, and whether to use `DecodingOptions` in conjunction with the
    `whisper.decode()` function:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenAI 的 `whisper.DecodingOptions()` 函数用于指定控制解码过程行为的各种选项，当转录音频时使用。`DecodingOptions`
    函数中的参数允许用户指定诸如转录语言、是否包含时间戳，以及是否将 `DecodingOptions` 与 `whisper.decode()` 函数结合使用等选项：
- en: '[PRE3]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this example, the `DecodingOptions` function is set with three options:'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个例子中，`DecodingOptions` 函数设置了三个选项：
- en: '`language=detected_language`: This specifies the language of the transcription.
    Setting the language can improve the transcription accuracy if you know the language
    in advance and want to rely on something other than the model’s automatic language
    detection.'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`language=detected_language`：此选项指定转录的语言。如果你提前知道语言并希望依赖其他方式而非模型的自动语言检测，设置语言可以提高转录准确性。'
- en: '`without_timestamps=True`: When set to `True`, this option indicates that the
    transcription should not include timestamps. If you require timestamps for each
    word or sentence, you will set this to `False`.'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`without_timestamps=True`：当设置为 `True` 时，表示转录不包括时间戳。如果需要每个单词或句子的时间戳，则应将此选项设置为
    `False`。'
- en: '`fp16=(DEVICE == "cuda")`: This option determines whether to use FP16 (16-bit
    floating-point precision) for the decoding. The `(DEVICE == "cuda")` evaluation
    checks if CUDA is available. Earlier in the notebook, we used `DEVICE = "cuda"
    if torch.cuda.is_available() else "cpu"` to set `DEVICE` accordingly. Then, it
    sets `fp16` to `True` if `DEVICE` is `"cuda"`, meaning you plan to run the model
    on a GPU. If `DEVICE` is `"cpu"`, it sets `fp16` to `False`, ensuring compatibility
    and avoiding unnecessary warnings or errors.'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp16=(DEVICE == "cuda")`：此选项决定是否使用 FP16（16 位浮动精度）进行解码。`(DEVICE == "cuda")`
    评估检查 CUDA 是否可用。在笔记本的早期，我们使用 `DEVICE = "cuda" if torch.cuda.is_available() else
    "cpu"` 来设置 `DEVICE`。然后，如果 `DEVICE` 为 `"cuda"`，则将 `fp16` 设置为 `True`，表示计划在 GPU 上运行模型。如果
    `DEVICE` 为 `"cpu"`，则将 `fp16` 设置为 `False`，以确保兼容性并避免不必要的警告或错误。'
- en: These options can be adjusted based on the specific requirements of your transcription
    task. For instance, if you transcribe audio in a different language, you will
    change the language option accordingly. If you need to optimize for performance
    and your hardware supports it, you might enable `fp16` to use half precision.
    FP16 (16-bit floating-point) computation is beneficial on compatible GPUs, as
    it can significantly reduce memory usage and potentially increase computation
    speed without substantially affecting the model’s accuracy. However, not all CPUs
    support FP16 computation, and attempting to use it on a CPU can lead to errors
    or fallbacks to FP32 (single-precision floating-point) computation.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些选项可以根据你的转录任务的具体要求进行调整。例如，如果你转录的是另一种语言的音频，你会相应地更改语言选项。如果你需要优化性能，且你的硬件支持，你可以启用
    `fp16` 来使用半精度。FP16（16位浮动点数）计算在兼容的GPU上非常有利，因为它可以显著减少内存使用，并且可能提高计算速度，而不会对模型的准确性造成实质性影响。然而，并非所有CPU都支持FP16计算，尝试在CPU上使用它可能会导致错误或回退到FP32（单精度浮动点数）计算。
- en: '**Defining a function for streamlined transcription**: We introduce a custom
    function to streamline the transcription process. This function simplifies handling
    multiple files, and we explore how to incorporate translation options within it,
    enhancing its utility:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义一个简化转录的函数**：我们介绍了一个自定义函数来简化转录过程。这个函数简化了处理多个文件的流程，并且我们探索了如何在其中加入翻译选项，从而增强其功能：'
- en: '[PRE4]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`process_file()` function to transcribe non-English audio samples. This demonstrates
    Whisper’s robust support for multiple languages, showcasing its effectiveness
    in a global context:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`process_file()` 函数来转录非英语音频样本。这个示例展示了Whisper在全球范围内对多语言的强大支持，展示了其在全球化环境中的有效性。'
- en: '[PRE5]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`initial_prompt` parameter and adjusting settings such as the temperature.
    We will examine two methods that refine the transcription output using `initial_prompt`,
    especially for audio with ambiguously spelled words or specialized terminology:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`initial_prompt` 参数以及调整设置如温度。我们将研究两种方法，通过使用 `initial_prompt` 来优化转录输出，特别是针对那些拼写模糊或具有专门术语的音频：'
- en: '`initial_prompt` parameter. That approach is helpful when facing a common challenge:
    accurate transcription of uncommon proper nouns, such as product names, company
    names, or individuals. These elements often trip up even the most sophisticated
    transcription tools, leading to misspellings. A simple transcription without `initial_prompt`
    values results in the following:'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initial_prompt` 参数。在面对一个常见挑战时，这种方法非常有用：准确转录不常见的专有名词，如产品名称、公司名称或个人姓名。这些元素常常会让即便是最先进的转录工具也犯错，导致拼写错误。如果没有
    `initial_prompt` 值的简单转录，结果如下：'
- en: '[PRE6]'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '------'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '------'
- en: 'Transcription of file ''product_names.wav'':'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 转录文件 'product_names.wav'：
- en: Welcome to Quirk Quid Quill Inc., where finance meets innovation.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 欢迎来到Quirk Quid Quill Inc.，在这里，金融与创新交汇。
- en: Explore diverse offerings from the P3-Quattro, a unique investment portfolio
    quadrant to the O3-Omni, a platform for intricate derivative trading strategies.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 探索多样化的产品，包括P3-Quattro，一个独特的投资组合象限，和O3-Omni，一个复杂衍生品交易策略平台。
- en: Delve into unconventional bond markets with our B3-BondX and experience non-standard
    equity trading with E3-Equity.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深入探索非常规债券市场，使用我们的B3-BondX，并体验E3-Equity的非标准股票交易。
- en: Surpass your wealth management with W3-WrapZ and anticipate market trends with
    the O2-Outlier, our forward-thinking financial forecasting tool.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 超越财富管理，使用W3-WrapZ，并通过我们的前瞻性金融预测工具O2-Outlier预判市场趋势。
- en: Explore venture capital world with U3-UniFund or move your money with the M3-Mover,
    our sophisticated monetary transfer module.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 探索风险投资世界，使用U3-UniFund，或通过我们的M3-Mover模块转移资金，这是一个精密的货币转移工具。
- en: At Quirk Quid Quill Inc., we turn complex finance into creative solutions.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在Quirk Quid Quill Inc.，我们将复杂的金融问题转化为创造性的解决方案。
- en: 'initial_prompt parameter are significant. Without initial_prompt, Whisper struggled
    with the proper nouns, resulting in misspellings such as “Quirk, Quid, Quill,
    Inc.”, “P3 Quattro”, “O3 Omni”, “B3 Bond X”, “E3 Equity”, “W3 Rap Z”, “O2 Outlier”,
    “U3 Unifund”, and “M3 Mover”. However, after including the correct spellings in
    the initial_prompt parameter, Whisper accurately transcribed these terms as “Quirk
    Quid Quill Inc.”, “P3-Quattro”, “O3-Omni”, “B3-BondX”, “E3-Equity”, “W3-WrapZ”,
    “O2-Outlier”, “U3-UniFund”, and “M3-Mover”. This demonstrates the power of the
    initial_prompt parameter in guiding Whisper to produce more accurate transcriptions,
    especially when dealing with uncommon or tricky terms.*   `initial_prompt` parameter.
    The most effective approach is to craft and provide either an actual or a fictitious
    prompt to steer Whisper using sure spellings, styles, or terminology. To illustrate
    the second method, we’ll pivot to a different audio clip crafted specifically
    for this exercise. The scenario is an unusual barbecue event. Our first step involves
    generating a baseline transcript with Whisper to assess its initial accuracy.
    A simple transcription without `initial_prompt` values results in the following:'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`initial_prompt`参数非常重要。如果没有`initial_prompt`，Whisper会在处理专有名词时遇到困难，导致拼写错误，例如“Quirk,
    Quid, Quill, Inc.”，“P3 Quattro”，“O3 Omni”，“B3 Bond X”，“E3 Equity”，“W3 Rap Z”，“O2
    Outlier”，“U3 Unifund”和“M3 Mover”。然而，在`initial_prompt`参数中包含正确的拼写后，Whisper成功地将这些术语转录为“Quirk
    Quid Quill Inc.”，“P3-Quattro”，“O3-Omni”，“B3-BondX”，“E3-Equity”，“W3-WrapZ”，“O2-Outlier”，“U3-UniFund”和“M3-Mover”。这展示了`initial_prompt`参数的强大作用，能够引导Whisper产生更准确的转录，特别是在处理不常见或复杂术语时。*
    `initial_prompt`参数。最有效的方法是创建并提供一个实际的或虚构的提示，以通过确保拼写、风格或术语来引导Whisper。为了说明第二种方法，我们将转向为本练习专门制作的另一个音频片段。场景是一次不同寻常的烧烤活动。我们的第一步是使用Whisper生成一个基准转录，以评估其初步准确性。没有`initial_prompt`值的简单转录结果如下：'
- en: '[PRE8]'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '------'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '------'
- en: 'Transcription of file ''bbq_plans.wav'':'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文件'bqq_plans.wav'的转录：
- en: Hello, my name is Preston Tuggle.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你好，我叫普雷斯顿·塔格尔。
- en: I'm based in New York City.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我住在纽约市。
- en: This weekend I have really exciting plans with some friends of mine, Aimee and
    Shawn.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个周末我有一些非常令人兴奋的计划，要和我的朋友Aimee和Shawn一起度过。
- en: We're going to a BBQ here in Brooklyn.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们要去布鲁克林的烧烤聚会。
- en: Hopefully, it's actually going to be a little bit of kind of an odd BBQ.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 希望这实际上会是一场有点奇怪的烧烤聚会。
- en: We're going to have doughnuts, omelets.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们要吃甜甜圈，煎蛋卷。
- en: It's kind of like a breakfast, as well as whisky.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它有点像早餐，也像威士忌。
- en: So that should be fun.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所以那应该会很有趣。
- en: initial_prompt and the second output after proving a fictitious prompt, we got
    a more precise output (for example, “Aimee and Shawn” rather than “Amy and Sean”,
    “doughnuts” instead of “donuts”, “BBQ” rather than “barbeque”, and “whisky” instead
    of “whiskey”.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在提供虚构的提示后，`initial_prompt`和第二次输出，我们得到了更精确的结果（例如，“Aimee 和 Shawn”而不是“Amy 和 Sean”，“doughnuts”而不是“donuts”，“BBQ”而不是“barbeque”，“whisky”而不是“whiskey”）。
- en: '[PRE10]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As you run the cells in the notebook, each section builds upon the previous
    ones, gradually introducing more complex features and techniques for using Whisper.
    This structured approach helps set up Whisper for transcription tasks and explores
    strategies for increasing transcription accuracy, catering to a wide range of
    audio content.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在笔记本中运行单元时，每个部分都会在前一个部分的基础上构建，逐步介绍使用Whisper的更复杂功能和技术。这种结构化的方式有助于为转录任务设置Whisper，并探索提高转录准确度的策略，以适应各种音频内容。
- en: Understanding the superpowers and limitations of Whisper’s `initial_prompt`
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 理解Whisper的`initial_prompt`的超能力和局限性
- en: 'The `initial_prompt` parameter in OpenAI’s Whisper is an optional text prompt
    providing context to the model for the first audio window being transcribed. Here
    are the key things to understand about `initial_prompt`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的Whisper中的`initial_prompt`参数是一个可选的文本提示，用于在转录第一个音频窗口时为模型提供上下文。以下是关于`initial_prompt`需要理解的关键内容：
- en: '`initial_prompt` is used to prime the model with relevant context before it
    begins transcribing the audio. This can help improve transcription accuracy, especially
    for specialized vocabularies or desired writing styles.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`initial_prompt`用于在模型开始转录音频之前，为其提供相关的上下文。这有助于提高转录的准确性，特别是在涉及专业词汇或特定写作风格时。'
- en: '`initial_prompt` only affects the first segment of audio being transcribed.
    For longer audio files that get split into multiple segments, its influence may
    diminish after the first 30-90 seconds of audio. For shorter audios, manually
    segmenting or splitting the audio and then applying the `initial_prompt` parameter
    is an option to overcome this limitation. For larger scripts, that segmentation
    could be automated. There is also the option to apply some postprocessing adjustments,
    including passing the entire transcript to an LLM with a more sophisticated prompt.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`initial_prompt` 只影响正在转录的音频的第一个片段。对于较长的音频文件，如果被拆分成多个片段，那么在音频的前30-90秒之后，它的影响可能会减弱。对于较短的音频，手动分割或拆分音频并应用
    `initial_prompt` 参数是一种克服这一限制的选择。对于较大的脚本，分割过程可以自动化。也可以选择进行一些后处理调整，包括将整个转录本传递给一个更复杂提示的
    LLM。'
- en: '`initial_prompt`. The documentation seems to be inconsistent about whether
    the first 224 or last 224 tokens are used, but in either case, anything beyond
    that limit is ignored.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`initial_prompt`。文档中似乎对使用前224个令牌还是后224个令牌存在不一致的描述，但无论哪种情况，超出该限制的部分都会被忽略。'
- en: '`initial_prompt` does not have to be an actual transcript. Fictitious prompts
    can be crafted to steer Whisper using sure spellings, styles, or terminology.
    Techniques such as including spelling guides or generating prompts with GPT-3
    can be effective.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`initial_prompt` 不一定要是一个实际的转录文本。可以创建虚构的提示来引导 Whisper，使用准确的拼写、风格或术语等。包括拼写指南或通过
    GPT-3 生成提示等技术都可以有效地发挥作用。'
- en: '`initial_prompt` parameter differs from the `prompt` parameter, which provides
    the previous transcribed segment context for the current segment, helping maintain
    consistency across a long audio file.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`initial_prompt` 参数不同于 `prompt` 参数，后者为当前片段提供之前转录片段的上下文，有助于在较长的音频文件中保持一致性。'
- en: The `initial_prompt` parameter is a way to frontload relevant context to Whisper
    to improve transcription accuracy. However, its impact is limited to the beginning
    of the audio and subject to a token limit. Thus, it is a useful but bounded tool
    for enhancing Whisper’s performance on niche audio content.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`initial_prompt` 参数是一种将相关上下文提前提供给 Whisper 以提高转录准确性的方法。然而，它的影响仅限于音频的开始部分，并且受限于令牌数量。因此，它是一个有用但有限的工具，用于提升
    Whisper 在特定音频内容上的表现。'
- en: Now, let’s go deeper into transcription techniques to gain a more comprehensive
    understanding of the options available in Whisper. Applying these techniques,
    you’ll be well prepared to tackle various audio-processing tasks, from simple
    transcriptions to more complex, multilingual projects.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解转录技巧，以更全面地理解 Whisper 中可用的选项。应用这些技巧，你将能够为各种音频处理任务做好充分准备，从简单的转录到更复杂的多语言项目。
- en: Transcribing audio files with Whisper efficiently
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Whisper 高效转录音频文件
- en: 'Before delving into the relevant parameters, let’s consider the model size
    selection: tiny, base, small, medium, or large. That choice directly impacts the
    balance between transcription speed and accuracy. For instance, while the medium
    model offers a faster transcription rate, the large model excels in accuracy,
    making it the preferred choice for applications where precision is non-negotiable.
    The model’s accuracy escalates with its size, positioning the large model as the
    pinnacle of precision. The large model is the benchmark for reported accuracies
    in the literature (*Efficient and Accurate Transcription in Mental Health Research
    - A Tutorial on Using Whisper AI for Audio File Transcription* – November 10,
    2023 – [https://osf.io/preprints/osf/9fue8](https://osf.io/preprints/osf/9fue8)),
    underscoring its significance for tasks where accuracy is paramount.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨相关参数之前，先来考虑一下模型大小的选择：tiny、base、small、medium 或 large。这个选择直接影响转录速度和准确性之间的平衡。例如，尽管
    medium 模型提供了更快的转录速度，但 large 模型在准确性方面表现出色，因此在需要精度的应用中，它是首选。随着模型大小的增加，其准确性也随之提升，使得
    large 模型成为精度的巅峰。large 模型是文献中报告准确性的基准（*《心理健康研究中的高效和准确转录——使用 Whisper AI 进行音频文件转录的教程*——2023年11月10日——[https://osf.io/preprints/osf/9fue8](https://osf.io/preprints/osf/9fue8)），这也凸显了它在需要高精度任务中的重要性。
- en: My practical experience has underscored the necessity of selecting the appropriate
    model size and computational resources. Running Whisper, especially its more significant
    variants, efficiently requires GPU acceleration to reduce transcription times
    significantly. For instance, testing has shown that using a GPU can dramatically
    reduce the time it takes to transcribe a minute of audio. Furthermore, it’s essential
    to consider the trade-off between speed and accuracy when choosing the model size.
    For example, while the medium model is twice as fast as the large model, the large
    model offers increased accuracy.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我的实际经验强调了选择适当的模型大小和计算资源的必要性。特别是在运行Whisper时，尤其是其更大的变体，需要GPU加速以显著减少转录时间。例如，测试表明使用GPU可以显著缩短转录一分钟音频所需的时间。此外，在选择模型大小时，考虑速度和准确性之间的权衡是至关重要的。例如，虽然中等模型的速度是大模型的两倍，但大模型提供了更高的准确性。
- en: Selecting key inference parameters for optimized transcription
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择优化转录的关键推理参数
- en: 'Configuring inference parameters and decoding options in OpenAI’s Whisper is
    crucial for achieving accurate transcriptions, as these settings can significantly
    impact the performance and precision of the transcription process. This exploration
    enhances transcription accuracy and optimizes performance, fully leveraging Whisper’s
    capabilities. In my experience, parameters such as `temperature`, `beam_size`,
    and `best_of` emerged as pivotal in fine-tuning Whisper’s transcription capabilities.:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 配置OpenAI的Whisper中的推理参数和解码选项对于实现准确的转录至关重要，因为这些设置可以显著影响转录过程的性能和精度。这种探索增强了转录的准确性并优化了性能，充分发挥了Whisper的能力。根据我的经验，参数如
    `temperature`、`beam_size` 和 `best_of` 在微调Whisper的转录能力中显得至关重要。
- en: The `temperature` parameter controls the level of variability in the generated
    text, which can result in more accurate transcriptions.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temperature` 参数控制生成文本中的变化水平，这可以导致更准确的转录。'
- en: The `beam_size` parameter is critical in decoding, influencing the breadth of
    the search for potential transcriptions. A larger `beam_size` value can improve
    the transcription accuracy by considering a more comprehensive array of possibilities.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beam_size` 参数在解码中至关重要，影响潜在转录的搜索广度。较大的 `beam_size` 值可以通过考虑更全面的可能性来提高转录准确性。'
- en: Similarly, `best_of` allows us to control the diversity of the decoding process,
    selecting the best result from multiple attempts. This can be particularly useful
    in achieving the highest possible accuracy in our transcriptions.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，`best_of` 允许我们控制解码过程的多样性，从多次尝试中选择最佳结果。这在实现转录的最高可能准确性方面特别有用。
- en: Understanding the relationship among the temperature, beam_size, and best_of
    inference parameters
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 `temperature`、`beam_size` 和 `best_of` 推理参数之间的关系
- en: The `beam_size` parameter in the Whisper model refers to the number of beams
    used in beam search during the decoding process. Beam search is a heuristic search
    algorithm that explores a graph by expanding the most promising node in a limited
    set. In the context of Whisper, beam search is used to find the most likely sequence
    of words given the audio input.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 模型中的 `beam_size` 参数指的是在解码过程中使用的束搜索中的束数量。束搜索是一种启发式搜索算法，通过扩展有限集中最有前途的节点来探索图。在Whisper的背景下，束搜索用于在给定音频输入时找到最可能的单词序列。
- en: The `temperature` parameter controls the randomness of the output during sampling.
    A higher temperature produces more random outputs, while a lower temperature makes
    the model’s outputs more deterministic. When the temperature is set to zero, the
    model uses a greedy decoding strategy, always choosing the most likely next word.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`temperature` 参数控制抽样过程中输出的随机性。较高的温度会产生更随机的输出，而较低的温度会使模型的输出更为确定性。当温度设置为零时，模型采用贪婪解码策略，总是选择最可能的下一个词。'
- en: '`beam_size` and `temperature` influence the decoding strategy and the diversity
    of the generated text. A larger `beam_size` value can increase the accuracy of
    the transcription by considering more alternative word sequences, but it also
    requires more computational resources and can slow down the inference process.
    On the other hand, `temperature` affects the variability of the output; a nonzero
    temperature allows for sampling from a distribution of possible following words,
    which can introduce variability and potentially capture more nuances in the speech.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`beam_size` 和 `temperature` 影响解码策略以及生成文本的多样性。较大的 `beam_size` 值可以通过考虑更多的替代单词序列来提高转录的准确性，但它也需要更多的计算资源，并可能导致推理过程变慢。另一方面，`temperature`
    影响输出的可变性；非零温度允许从可能的后续单词分布中进行采样，这可以引入可变性并潜在地捕捉到语音中的更多细微差别。'
- en: In practice, the `beam_size` parameter is used when the temperature is set to
    zero, indicating that beam search should be used. If the temperature is nonzero,
    the `best_of` parameter is used instead to determine the number of candidates
    to sample from. The Whisper model uses a dynamic temperature setting, starting
    with a temperature of `0` and increasing it by `0.2` up to `1.0` when certain
    conditions are met, such as when the average log probability over the generated
    tokens is lower than a threshold or when the generated text has a *gzip* compression
    rate higher than a specific value.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，当温度设置为零时，使用 `beam_size` 参数，表示应该使用束搜索。如果温度非零，则使用 `best_of` 参数来确定要从中采样的候选项数量。Whisper
    模型使用动态温度设置，初始温度为 `0`，在满足特定条件时（例如生成的标记的平均对数概率低于阈值，或生成的文本的 *gzip* 压缩率高于特定值），会将温度提高
    `0.2`，直到达到 `1.0`。
- en: In summary, `beam_size` controls the breadth of the search in beam search decoding,
    and `--temperature` controls the randomness of the output during sampling. They
    are part of the decoding strategy that affects the final transcription or translation
    produced by the Whisper model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，`beam_size` 控制了束搜索解码中的搜索范围，而 `--temperature` 控制了采样过程中输出的随机性。它们是解码策略的一部分，影响最终由
    Whisper 模型生成的转录或翻译结果。
- en: Configuring parameters and decoding options in Whisper is a nuanced process
    that requires a deep understanding of the model and its capabilities. By carefully
    adjusting these settings, users can optimize the accuracy and performance of their
    transcriptions, making Whisper a powerful tool for a wide range of applications.
    As with any AI model, it’s essential to thoroughly test and validate the results
    in the specific context of your use case to ensure they meet your requirements.
    The following section goes even deeper into a hands-on notebook specifically designed
    to showcase the power of runtime parameters during decoding in Whisper.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Whisper 中配置参数和解码选项是一个微妙的过程，需要深入理解模型及其功能。通过仔细调整这些设置，用户可以优化转录的准确性和性能，使 Whisper
    成为一个适用于广泛应用的强大工具。与任何 AI 模型一样，必须在特定使用场景下彻底测试和验证结果，确保它们满足要求。以下部分将更深入地介绍一个专门设计的实践笔记本，旨在展示
    Whisper 解码过程中运行时参数的强大功能。
- en: Applying Whisper’s runtime parameters in practice
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在实践中应用 Whisper 的运行时参数
- en: 'This section will explore the `LOAIW_ch05_2_transcribing_and_translating_with_``Whisper.ipynb`
    Colab notebook ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter05/LOAIW_ch05_2_transcribing_and_translating_with_Whisper.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter05/LOAIW_ch05_2_transcribing_and_translating_with_Whisper.ipynb))
    for more comprehensive hands-on implementation. I encourage you to find the notebook
    in the book’s GitHub repository and run it in Google Colab. The notebook is designed
    to demonstrate the installation and usage of Whisper within a Python environment,
    showcasing its capabilities in handling multilingual ASR and translation tasks.
    Specifically, it leverages the **FLEURS** dataset to illustrate Whisper’s proficiency
    in processing multilingual audio data. **FLEURS** stands for **Few-shot Learning
    Evaluation of Universal Representations of Speech**. It’s a benchmark designed
    to evaluate the performance of universal speech representations in a few-shot
    learning scenario, which refers to the ability of a model to learn or adapt to
    new tasks or languages with a minimal amount of data. This is particularly important
    for languages that do not have large datasets available for training models. The
    following diagram illustrates the high-level structure of the notebook:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将探索 `LOAIW_ch05_2_transcribing_and_translating_with_Whisper.ipynb` Colab 笔记本（[https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter05/LOAIW_ch05_2_transcribing_and_translating_with_Whisper.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter05/LOAIW_ch05_2_transcribing_and_translating_with_Whisper.ipynb)）以进行更全面的实践操作。我鼓励你在本书的
    GitHub 仓库中找到该笔记本，并在 Google Colab 中运行。该笔记本旨在展示如何在 Python 环境中安装和使用 Whisper，展示其处理多语言
    ASR 和翻译任务的能力。具体而言，它利用 **FLEURS** 数据集展示了 Whisper 在处理多语言音频数据方面的高效性。**FLEURS** 代表
    **少量学习评估通用语音表示**，这是一个用于评估通用语音表示在少量学习场景中的性能的基准，少量学习指的是模型能够在仅有少量数据的情况下学习或适应新任务或新语言。这对于那些没有大量数据集可供训练模型的语言尤其重要。以下图示展示了笔记本的高级结构：
- en: '![Figure 5.2 – Transcription and translation with Whisper](img/B21020_05_2.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 使用 Whisper 进行转录和翻译](img/B21020_05_2.jpg)'
- en: Figure 5.2 – Transcription and translation with Whisper
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 使用 Whisper 进行转录和翻译
- en: 'As shown in *Figure 5**.2*, the notebook also incorporates an interactive Gradio
    interface for hands-on experimentation with Whisper’s transcription and translation
    features on selected audio samples. Here are the high-level steps with some selected
    code snippets to illustrate:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 5.2*所示，笔记本还集成了一个交互式 Gradio 界面，供用户进行 Whisper 转录和翻译功能的实地实验，使用选定的音频样本。以下是一些高级步骤和代码片段，展示了相关操作：
- en: '`librosa`, `gradio`, and `kaleido`. These libraries can significantly enhance
    the capabilities and applications of Whisper-based projects. `librosa` can preprocess
    audio files to meet Whisper’s requirements, `gradio` can create interactive demos
    to showcase Whisper’s functionalities, and `kaleido` can generate visualizations
    to complement audio-processing tasks. Together, they prepare the Python environment
    for the tasks ahead, addressing potential compatibility issues and setting up
    the computation device:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`librosa`、`gradio` 和 `kaleido`。这些库能够显著增强基于 Whisper 的项目的功能和应用。`librosa` 可以预处理音频文件以满足
    Whisper 的要求，`gradio` 可以创建交互式演示以展示 Whisper 的功能，而 `kaleido` 则可以生成可视化内容以补充音频处理任务。它们共同为即将进行的任务准备了
    Python 环境，解决了潜在的兼容性问题，并设置了计算设备：'
- en: '[PRE12]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: dataset = Fleurs(lang, subsample_rate=5)
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: dataset = Fleurs(lang, subsample_rate=5)
- en: '[PRE13]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`temperature`, `beam_size`, and `best_of` inference parameters:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`temperature`、`beam_size` 和 `best_of` 推理参数：'
- en: '[PRE14]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Launching an interactive exploration with Gradio**: An interactive Gradio
    interface allows users to select audio samples, adjust inference parameters, and
    view the ASR and translation results alongside the original audio. This section
    aims to provide a real-time experience with changing the inference parameters
    and observing the transcription results.'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**启动与 Gradio 的交互式探索**：一个交互式 Gradio 界面允许用户选择音频样本，调整推理参数，并查看 ASR 和翻译结果，同时显示原始音频。本节旨在提供实时体验，通过更改推理参数并观察转录结果。'
- en: '**Exploring advanced techniques for word-level timestamps**: This section demonstrates
    extracting word-level timestamps from audio transcriptions using Whisper’s cross-attention
    weights. It involves dynamic time warping, attention weight processing, and visualization
    techniques to align words in the transcript with specific times in the audio recording,
    catering to applications such as subtitle generation and detailed audio analysis.'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**探索用于单词级时间戳的高级技术**：本节展示了如何通过 Whisper 的交叉注意力权重，从音频转录中提取单词级时间戳。这涉及到动态时间规整、注意力权重处理和可视化技术，将转录中的单词与音频录音中的特定时间对齐，适用于字幕生成和详细音频分析等应用。'
- en: This Colab notebook is a well-structured guide that introduces Whisper and its
    multilingual capabilities and provides practical, hands-on experience with the
    model’s inference parameters. It covers the entire workflow from data preparation
    to model inference and result visualization, offering valuable insights for anyone
    interested in speech processing and machine learning. This comprehensive approach
    ensures that you can grasp the intricacies of working with one of the most advanced
    ASR and translation models available, paving the way for further exploration and
    application development in speech technology.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 本 Colab 笔记本是一个结构良好的指南，介绍了 Whisper 及其多语言功能，并提供了关于模型推理参数的实践操作经验。它涵盖了从数据准备到模型推理和结果可视化的整个工作流程，为任何对语音处理和机器学习感兴趣的人提供了宝贵的见解。这种全面的方法确保你能掌握使用现有最先进的自动语音识别（ASR）和翻译模型之一的复杂性，为语音技术的进一步探索和应用开发铺平道路。
- en: 'Having established Whisper’s efficiency in transcribing audio files, we now
    focus on the next frontier: integrating this advanced speech recognition technology
    into voice assistants and chatbots. This integration promises to revolutionize
    our interactions with AI, offering seamless and intuitive communication that can
    accurately understand and respond to our spoken requests. Let’s explore how Whisper’s
    capabilities can be harnessed to enhance the user experience in these interactive
    applications.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在确立了 Whisper 在转录音频文件方面的高效性后，我们现在将重点放在下一个前沿：将这一先进的语音识别技术集成到语音助手和聊天机器人中。这一整合有望彻底改变我们与人工智能的互动，提供无缝且直观的沟通，能够准确理解并响应我们的口头请求。让我们探索如何利用
    Whisper 的功能来提升这些交互式应用中的用户体验。
- en: Integrating Whisper into voice assistants and chatbots
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 Whisper 集成到语音助手和聊天机器人中
- en: Incorporating Whisper’s advanced speech recognition capabilities into voice
    assistants and chatbots can significantly uplift the user experience. This involves
    understanding spoken words and interpreting them with higher accuracy and context
    awareness. The goal is to create systems that hear and understand, making interactions
    more natural and human-like.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Whisper 的先进语音识别功能融入语音助手和聊天机器人中，可以显著提升用户体验。这涉及到理解口语，并用更高的准确性和上下文意识进行解释。目标是创造能够听懂和理解的系统，使互动更加自然和类人化。
- en: In this section, we are taking a hands-on approach to learning and understanding
    how Whisper can complement and enhance the existing structures. This integration
    is not about replacing current systems but augmenting them with Whisper’s robust
    capabilities. It involves fine-tuning the interaction between Whisper and the
    assistant or chatbot to ensure seamless communication. This synergy is vital to
    unlocking the full potential of voice technology.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将通过实践的方式学习和理解 Whisper 如何补充和增强现有结构。这一整合并不是要替代现有系统，而是通过 Whisper 强大的功能对其进行增强。它涉及到对
    Whisper 与助手或聊天机器人之间互动的微调，以确保无缝的沟通。这种协同作用对于释放语音技术的全部潜力至关重要。
- en: Optimizing Whisper for efficiency and user experience is critical to this integration.
    Efficiency is not just about speed but also about the accuracy and relevance of
    responses. Whisper’s ability to accurately transcribe and understand diverse accents,
    dialects, and languages is a cornerstone of its utility. Moreover, the user experience
    is greatly enhanced when the technology can handle spontaneous and everyday speech,
    making interactions more engaging and less robotic. Therefore, the focus is on
    creating a harmonious balance between technical proficiency and user-centric design.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 优化Whisper以提高效率和用户体验对这一整合至关重要。效率不仅仅是速度，还包括回应的准确性和相关性。Whisper准确地转录和理解各种口音、方言和语言的能力，是其实用性的基石。此外，当技术能够处理自发的、日常的语言交流时，用户体验会大大增强，使互动更加生动、自然，而不那么机械化。因此，重点是创建技术熟练和以用户为中心的设计之间的和谐平衡。
- en: Whisper’s role in transcription services is multifaceted and significant. Its
    technical sophistication, robustness to challenging audio conditions, and cost-effectiveness
    make it a powerful tool for businesses and developers. So, let’s dive in!
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper在转录服务中的角色是多方面且重要的。其技术的复杂性、对复杂音频条件的鲁棒性以及成本效益，使其成为企业和开发者的重要工具。所以，让我们深入了解一下！
- en: Recognizing the potential of Whisper in voice assistants and chatbots
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 认识到Whisper在语音助手和聊天机器人中的潜力
- en: In our digitally driven era, **intelligent personal assistants** (**IPAs**)
    such as Siri, Google Assistant, and Alexa have become ubiquitous in facilitating
    tasks such as shopping, playing music, and managing schedules. Voice assistants
    and chatbots, integral to digital interactions, are evolving rapidly. While their
    architecture varies depending on use cases and requirements, their potential is
    immense, especially when incorporating technologies such as Whisper.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们这个数字化驱动的时代，**智能个人助手**（**IPAs**）如Siri、Google Assistant和Alexa，已经无处不在，帮助完成购物、播放音乐和管理日程等任务。语音助手和聊天机器人，作为数字互动的重要组成部分，正在迅速发展。尽管它们的架构根据使用场景和需求有所不同，但它们的潜力巨大，尤其是在引入像Whisper这样的技术时。
- en: Chatbots and voice assistants are increasingly becoming integral to our digital
    interactions, providing customer support, virtual assistance, and more. While
    varying based on specific use cases and requirements, their architecture generally
    follows a similar structure.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人和语音助手正日益成为我们数字互动的重要组成部分，提供客户支持、虚拟助手等服务。虽然根据具体的使用场景和需求有所不同，但它们的架构通常遵循类似的结构。
- en: Evolving toward sophistication with chatbots
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向着更复杂的聊天机器人发展
- en: 'Chatbots can be broadly classified into two types: rule-based and AI-based.
    Rule-based chatbots operate on predefined rules and patterns, providing responses
    based on a simple true-false algorithm. AI-based chatbots, on the other hand,
    leverage machine learning and NLP to understand and respond to user queries. A
    typical chatbot architecture consists of several key components:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人可以大致分为两类：基于规则的和基于AI的。基于规则的聊天机器人根据预定义的规则和模式进行操作，通过简单的真/假算法提供回应。而基于AI的聊天机器人则利用机器学习和自然语言处理（NLP）来理解和回应用户查询。一个典型的聊天机器人架构由几个关键组件组成：
- en: '**NLU engine**: This component interprets the user’s input, using machine learning
    and NLP to understand the context and intent of the message.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言理解引擎（NLU）**：这个组件解析用户输入，利用机器学习和NLP理解信息的上下文和意图。'
- en: '**Knowledge base**: This is a repository of information the chatbot uses to
    respond. It can include frequently asked questions, information about a company’s
    products or services, and other relevant data.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识库**：这是聊天机器人用来回应的一个信息库。它可以包括常见问题、关于公司产品或服务的信息以及其他相关数据。'
- en: '**Data storage**: The chatbot stores conversation history and analytics.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据存储**：聊天机器人存储对话历史和分析数据。'
- en: '**Q&A system**: This system answers customers’ frequently asked questions.
    The question is interpreted by the Q&A system, which then replies with appropriate
    responses from the knowledge base.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问答系统**：该系统回答客户的常见问题。问题由问答系统进行解析，随后从知识库中提供合适的回答。'
- en: Bridging gaps in digital communication with voice assistants
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在数字通信中弥合语音助手的空白
- en: 'Voice assistants, such as Amazon Alexa or Google Assistant, have a slightly
    different architecture. The general pipeline for a voice assistant starts with
    a client device microphone recording the user’s raw audio. This audio is then
    processed using a VAD system, which separates the audio into phrases. These phrases
    are transcribed into text and sent to the server for further processing. The architecture
    of a voice assistant is typically split into two main components:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 语音助手，如亚马逊 Alexa 或 Google Assistant，具有稍微不同的架构。语音助手的一般流程从客户端设备的麦克风录制用户的原始音频开始。然后使用语音活动检测（VAD）系统处理这些音频，将其分离成短语。这些短语被转录成文本，并发送到服务器进行进一步处理。语音助手的架构通常分为两个主要组件：
- en: '**Client-server**: The client processes audio information and converts it into
    text phrases. The information is then sent to the server for further processing.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户端-服务器**：客户端处理音频信息并将其转换为文本短语。然后将信息发送到服务器进行进一步处理。'
- en: '**Skills**: These independent applications run on the client’s processed text/audio.
    They process the information and return the results. In the context of voice assistants
    such as Amazon Alexa or Google Assistant, **skills** refers to third-party applications
    that extend the capabilities of the voice assistant platform. Skills are developed
    by third-party creators using platforms such as the Alexa Skills Kit ([https://www.amazon.science/blog/the-scalable-neural-architecture-behind-alexas-ability-to-select-skills](https://www.amazon.science/blog/the-scalable-neural-architecture-behind-alexas-ability-to-select-skills))
    provided by Amazon. They enable voice assistants to perform a wide range of functions
    beyond the built-in features, such as playing games, providing news updates, controlling
    smart home devices, and more. The architecture of voice assistants allows these
    skills to interact with the user’s voice commands and provide a tailored response
    or service.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**技能**：这些独立应用程序在客户端处理的文本/音频上运行。它们处理信息并返回结果。在像亚马逊 Alexa 或 Google Assistant 这样的语音助手的上下文中，**技能**指的是扩展语音助手平台功能的第三方应用程序。这些技能由第三方创作者使用亚马逊提供的
    Alexa 技能套件（[https://www.amazon.science/blog/the-scalable-neural-architecture-behind-alexas-ability-to-select-skills](https://www.amazon.science/blog/the-scalable-neural-architecture-behind-alexas-ability-to-select-skills)）开发。它们使语音助手能够执行广泛的功能，超越内置特性，如玩游戏、提供新闻更新、控制智能家居设备等。语音助手的架构允许这些技能与用户的语音命令互动，并提供量身定制的响应或服务。'
- en: Currently, IPAs lack interoperability, particularly in exchanging learned user
    behaviors. The architecture of IPAs is highly customized to the usability and
    context of business operations and client requirements. This limitation underscores
    the need for standardization in IPA architecture, focusing on voice as the primary
    modality. However, the concept extends beyond voice, encompassing text-based chatbots
    and multimodal interactions. For example, in multimodal scenarios, components
    may include speech recognition, NLP, or even environmental action execution, such
    as controlling industrial machinery.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，智能个人助理（IPA）缺乏互操作性，尤其是在交换用户行为学习方面。IPA 的架构高度定制化，以适应商业操作和客户需求的可用性和上下文。这一局限性突显了IPA架构标准化的必要性，尤其是以语音为主要交互方式。然而，这一概念不仅限于语音，还包括基于文本的聊天机器人和多模态交互。例如，在多模态场景中，组件可能包括语音识别、自然语言处理（NLP）甚至环境动作执行，如控制工业机械。
- en: We anticipate more sophisticated, context-aware chatbots and voice assistants
    as AI and machine learning technologies evolve, particularly with advancements
    such as OpenAI’s Whisper. These advancements promise enhanced user experiences
    and digital interaction possibilities. This evolution is crucial for specialized
    virtual assistants in enterprises and organizations, requiring interoperability
    with general-purpose assistants to avoid redundant implementations. Whisper’s
    potential in this landscape lies in its advanced voice-processing capabilities,
    setting a new standard for IPAs and revolutionizing user interaction with digital
    platforms.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能和机器学习技术的不断发展，特别是像 OpenAI 的 Whisper 这样的进展，我们预期将出现更加复杂、具有上下文感知能力的聊天机器人和语音助手。这些进展有望提升用户体验和数字互动的可能性。这一演变对企业和组织中的专业虚拟助手至关重要，因为它要求与通用助手进行互操作，以避免重复实现。Whisper
    在这个领域的潜力体现在其先进的语音处理能力上，树立了智能个人助理（IPA）的新标准，并革新了用户与数字平台的互动方式。
- en: As we pivot our focus to the next section, it’s essential to understand why
    we are centering our discussion specifically on chatbots, diverging from the realm
    of voice assistants. This strategic decision aligns with OpenAI’s approach to
    developing ChatGPT, a landmark in AI chatbot technology. ChatGPT’s design philosophy
    and implementation offer critical insights into integrating advanced technologies
    such as Whisper into chatbot architectures. The following section explores how
    Whisper can seamlessly incorporate into existing chatbot frameworks, enhancing
    their functionality and intelligence.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将注意力转向下一个部分时，理解为何我们专门讨论聊天机器人，而不涉及语音助手领域，显得尤为重要。这一战略决策与 OpenAI 开发 ChatGPT
    的方法一致，ChatGPT 代表了 AI 聊天机器人技术的一个里程碑。ChatGPT 的设计理念和实现方式为如何将先进技术如 Whisper 集成到聊天机器人架构中提供了关键的见解。以下部分将探讨
    Whisper 如何无缝地融入现有的聊天机器人框架，增强其功能和智能。
- en: Integrating Whisper into chatbot architectures
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 Whisper 集成到聊天机器人架构中
- en: In this section, we embark on a journey to explore the practical application
    of OpenAI’s Whisper in chatbot architectures. A chatbot architecture refers to
    a chatbot system’s basic structure and design. It includes the components and
    processes that enable a chatbot to understand user input, provide accurate responses,
    and deliver a seamless conversational experience. The architecture of a chatbot
    is crucial to its effectiveness and is determined by the specific use case, user
    interactions, integration needs, scalability requirements, available resources,
    and budget constraints.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将踏上探索 OpenAI Whisper 在聊天机器人架构中的实际应用之旅。聊天机器人架构指的是聊天机器人系统的基本结构和设计。它包括使聊天机器人能够理解用户输入、提供准确回应并提供无缝对话体验的组件和流程。聊天机器人的架构对于其有效性至关重要，并由具体的使用案例、用户互动、集成需求、可扩展性要求、可用资源以及预算限制等因素决定。
- en: Whisper’s architecture is designed to convert spoken language into text, a process
    known as transcription. This capability is fundamental to voice-based chatbots,
    which must understand and respond to spoken user input.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 的架构旨在将口语语言转换为文本，这一过程被称为转录。这个能力对于基于语音的聊天机器人至关重要，因为聊天机器人必须理解并响应口头的用户输入。
- en: Selecting the appropriate chatbot architecture for Whisper
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为 Whisper 选择合适的聊天机器人架构
- en: Choosing the chatbot architecture for Whisper involves considering the specific
    use case and requirements. The architecture should be capable of handling the
    tasks the chatbot will perform, the target audience, and the desired functionalities.
    For instance, if the chatbot is intended to answer frequently asked questions,
    the architecture might include a Q&A system that interprets questions and provides
    appropriate responses from a knowledge base.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 Whisper 聊天机器人架构时需要考虑具体的使用案例和需求。架构应能够处理聊天机器人将执行的任务、目标受众和所需功能。例如，如果聊天机器人用于回答常见问题，那么架构可能会包含一个问答系统，用于解读问题并从知识库中提供适当的回应。
- en: Adapting its neural network architecture, the Whisper model can be optimized
    for specific use cases. For example, a chatbot development company might use Whisper
    to build a real-time transcription service. In contrast, a company with intelligent
    assistants and IoT devices might integrate Whisper with a language model to process
    transcribed speech and perform tasks based on user commands.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整其神经网络架构，Whisper 模型可以针对特定的使用场景进行优化。例如，一家聊天机器人开发公司可能会使用 Whisper 来构建实时转录服务。相反，一家拥有智能助手和物联网设备的公司可能会将
    Whisper 与语言模型结合，以处理转录后的语音并根据用户命令执行任务。
- en: Applying Whisper chatbot architecture to use cases in the industry
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 Whisper 聊天机器人架构应用于行业中的使用案例
- en: Whisper’s chatbot architecture can be applied to various use cases across consumer,
    business, and industry contexts. For instance, a chatbot using Whisper can understand
    customer queries through speech and generate detailed, context-aware written or
    spoken responses in customer service. This can enhance the customer experience
    by providing quick, accurate, and personalized responses.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 的聊天机器人架构可以应用于各种消费者、商业和工业领域的使用案例。例如，使用 Whisper 的聊天机器人可以通过语音理解客户查询，并生成详细的、具备上下文意识的书面或口头回应，用于客户服务。这可以通过提供快速、准确且个性化的回应来增强客户体验。
- en: In business, Whisper can automate tasks such as taking notes during meetings,
    transcribing interviews, and converting lectures and podcasts into text for analysis
    and record-keeping. Automating routine tasks and enabling easy access to information
    can boost efficiency and productivity.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在商业领域，Whisper 可以自动化诸如会议记录、访谈转录、以及将讲座和播客转换为文本以便进行分析和记录的任务。自动化日常任务并方便获取信息可以提升效率和生产力。
- en: Whisper can be integrated into intelligent assistants and IoT devices in the
    industry context to enable more natural, efficient, and accurate voice interactions.
    For example, an intelligent assistant could process transcribed speech to perform
    tasks, answer questions, or control smart devices based on user commands.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 可以在工业领域的智能助手和物联网设备中集成，提供更自然、高效和准确的语音交互。例如，智能助手可以处理转录后的语音，执行任务、回答问题或根据用户命令控制智能设备。
- en: Implementing a Whisper-based chatbot involves integrating the Whisper API into
    your application, which can be done using Python. The Whisper API is part of `open`/`open-python`,
    which allows you to access various OpenAI services and models. The implementation
    process also involves defining the use case, choosing the appropriate chatbot
    architecture, and setting up the user interface.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 实现基于 Whisper 的聊天机器人需要将 Whisper API 集成到您的应用程序中，这可以通过 Python 实现。Whisper API 是
    `open`/`open-python` 的一部分，允许您访问各种 OpenAI 服务和模型。实现过程还涉及定义使用场景、选择合适的聊天机器人架构以及设置用户界面。
- en: As a starting point, let’s proceed with understanding a hands-on coding example,
    demonstrating how to build an essential voice assistant using Whisper. The entire
    coding example we will delve into can be found in our GitHub repository in the
    form of the `LOAIW_ch05_3_Whisper_and_Stable_LM_Zephyr_3B_voice_assistant_GPU.ipynb`
    Colab notebook ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter05/LOAIW_ch05_3_Whisper_and_Stable_LM_Zephyr_3B_voice_assistant_GPU.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter05/LOAIW_ch05_3_Whisper_and_Stable_LM_Zephyr_3B_voice_assistant_GPU.ipynb)).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 作为起点，让我们通过一个实际的编码示例来了解如何构建一个基本的语音助手，使用 Whisper 进行演示。我们将深入探讨的完整编码示例可以在我们的 GitHub
    仓库中找到，格式为 `LOAIW_ch05_3_Whisper_and_Stable_LM_Zephyr_3B_voice_assistant_GPU.ipynb`
    Colab 笔记本 ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter05/LOAIW_ch05_3_Whisper_and_Stable_LM_Zephyr_3B_voice_assistant_GPU.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter05/LOAIW_ch05_3_Whisper_and_Stable_LM_Zephyr_3B_voice_assistant_GPU.ipynb))。
- en: 'The following diagram provides a high-level step-by-step illustration of how
    the notebook sets up a simple voice assistant that leverages the capabilities
    of Whisper:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 下图提供了一个高层次的逐步说明，展示了笔记本如何设置一个简单的语音助手，利用 Whisper 的能力：
- en: '![Figure 5.3 – Creating a voice assistant with Whisper](img/B21020_05_3.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 使用 Whisper 创建语音助手](img/B21020_05_3.jpg)'
- en: Figure 5.3 – Creating a voice assistant with Whisper
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 使用 Whisper 创建语音助手
- en: '*Figure 5**.3* illustrates the steps of loading the Whisper model, transcribing
    audio input into text, and generating responses using StableLM Zephyr 3B – GGUF,
    a 3-billion-parameter-quantized GGUFv2 model created after Stability AI’s StableLM
    Zephyr 3B. The model files are compatible with `llama.cpp`. The responses are
    then converted into speech using the **Google Text-to-Speech** (**gTTS**) service,
    providing complete voice-to-voice interaction.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.3* 展示了加载 Whisper 模型、将音频输入转录为文本并使用 StableLM Zephyr 3B – GGUF 生成响应的步骤。StableLM
    Zephyr 3B – GGUF 是一个拥有 30 亿参数的量化 GGUFv2 模型，继 Stability AI 的 StableLM Zephyr 3B
    后开发。该模型文件与 `llama.cpp` 兼容。随后，使用 **Google 文本转语音** (**gTTS**) 服务将响应转换为语音，实现完整的语音对话交互。'
- en: Introducing StableLM Zephyr 3B – GGUF
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍 StableLM Zephyr 3B – GGUF
- en: 'StableLM Zephyr 3B – GGUF is a language model developed by Stability AI. Here
    are some details about it:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: StableLM Zephyr 3B – GGUF 是由 Stability AI 开发的语言模型。以下是一些关于该模型的详细信息：
- en: '**Model description**: StableLM Zephyr 3B is a 3-billion-parameter instruction-tuned
    model inspired by Hugging Face’s Zephyr 7B training pipeline. It was trained on
    a mix of publicly available and synthetic datasets using **direct preference optimization**
    (**DPO**). The evaluation for this model is based on MT Bench and Alpaca Benchmark.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型描述**：StableLM Zephyr 3B 是一个拥有 30 亿参数的指令调优模型，灵感来自 Hugging Face 的 Zephyr
    7B 训练管道。它使用 **直接偏好优化** (**DPO**) 在公开可用和合成数据集的混合上进行了训练。该模型的评估基于 MT Bench 和 Alpaca
    Benchmark。'
- en: '**Purpose and capabilities**: StableLM Zephyr 3B efficiently caters to various
    text generation needs, from simple queries to complex instructional contexts.
    It can be used for multiple tasks, including NLU, text completion, and more.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**目的和能力**：StableLM Zephyr 3B高效地满足各种文本生成需求，从简单查询到复杂的指导性文本。它可用于多种任务，包括自然语言理解（NLU）、文本补全等。'
- en: '`llama.cpp` team at Meta. GGUF stands for “Georgi Gervanov’s unified format,”
    a replacement for GGML, a C library focused on machine learning. GGUF is supported
    by various clients and libraries, including `llama.cpp`, `text-generation-webui`,
    `koboldcpp`, `gpt4all`, and more.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Meta的`llama.cpp`团队。GGUF代表“Georgi Gervanov的统一格式”，是GGML的替代品，GGML是一个专注于机器学习的C语言库。GGUF被各种客户端和库支持，包括`llama.cpp`、`text-generation-webui`、`koboldcpp`、`gpt4all`等。
- en: '**Quantization levels**: The model files come in different quantization levels:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**量化级别**：模型文件有不同的量化级别：'
- en: '`Q5_0`: Legacy; medium, balanced quality.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`Q5_0`：遗留版；中等，平衡质量。'
- en: '`Q5_K_S`: Large, low-quality loss (recommended).'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`Q5_K_S`：大，低质量损失（推荐）。'
- en: '`Q5_K_M`: Large, low-quality loss (recommended).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`Q5_K_M`：大，低质量损失（推荐）。'
- en: '`llama.cpp` from August 27, 2023, onward and with many third-party UIs and
    libraries.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 从2023年8月27日起，`llama.cpp`以及许多第三方UI和库。
- en: This section is not just about understanding the code; it’s about appreciating
    the potential of integrating Whisper into chatbot architectures. It’s about envisioning
    how this technology can revolutionize how we interact with chatbots, making these
    interactions more natural and intuitive. It’s about recognizing the potential
    of voice-enabled chatbots in various applications, from customer service to personal
    assistants and beyond.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 本节不仅仅是理解代码，更重要的是欣赏将Whisper集成到聊天机器人架构中的潜力。它让我们设想，这项技术如何革新我们与聊天机器人互动的方式，使这些互动更加自然和直观。它让我们认识到语音启用的聊天机器人在各种应用中的潜力，从客户服务到个人助手，乃至更广泛的领域。
- en: As we delve into the details of the coding example, remember that our goal is
    to understand the broader implications of the technology. How can the integration
    of Whisper into chatbot architectures enhance our AI solutions? How can it provide
    a competitive edge in the marketplace? These are the questions we should consider
    as we navigate this section.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们深入探讨编码示例的细节时，请记住我们的目标是理解技术的更广泛影响。Whisper集成到聊天机器人架构中如何提升我们的AI解决方案？它如何在市场上提供竞争优势？这些是我们在本节中应考虑的问题。
- en: 'I encourage you to open the Colab notebook and follow along. Here are the high-level
    steps with some selected code snippets to illustrate:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励你打开Colab Notebook并跟着一起操作。这里是一些高层次步骤，并附有部分代码片段供参考：
- en: '`llama-cpp-python`, `whisper`, `gradio`, and `gTTS` for `stablelm-zephyr-3b-GGUF
    stablelm-zephyr-3b.Q5_K_S.gguf` model, we must install and compile the `llama-cpp-python`
    package. To leverage NVIDIA CUDA acceleration, we must first set a `CMAKE_ARGS="-DLLAMA_CUBLAS=on"`
    environmental variable:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于`stablelm-zephyr-3b-GGUF stablelm-zephyr-3b.Q5_K_S.gguf`模型，我们需要安装并编译`llama-cpp-python`包，同时安装`whisper`、`gradio`和`gTTS`。为了利用NVIDIA
    CUDA加速，我们必须先设置`CMAKE_ARGS="-DLLAMA_CUBLAS=on"`环境变量：
- en: '[PRE15]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Initializing Python libraries**: We now import essential libraries and set
    up a logger to record events and outputs during the notebook’s execution:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化Python库**：我们现在导入必要的库，并设置一个日志记录器，用于记录在Notebook执行期间的事件和输出：'
- en: '[PRE16]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`llama.cpp`, configuring it for GPU usage if available. It specifies parameters
    such as the maximum sequence length, the number of CPU threads, and the number
    of layers to offload to the GPU:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`llama.cpp`，如果有GPU可用，则进行配置。它指定了诸如最大序列长度、CPU线程数和要卸载到GPU的层数等参数：'
- en: '[PRE17]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Exploring an inference example**: A simple example demonstrates how to generate
    a response from the StableLM Zephyr 3B model given a text prompt:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**探索推理示例**：一个简单的示例演示了如何根据文本提示从StableLM Zephyr 3B模型生成响应：'
- en: '[PRE18]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Defining supporting functions for the LLM**: Here, we create and test a function
    for interacting with the StableLM model:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义LLM的支持函数**：在这里，我们创建并测试一个与StableLM模型交互的函数：'
- en: '[PRE19]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`transcribe(audio)` function is a crucial component of the voice assistant
    system. It seamlessly integrates Whisper’s transcription capabilities with the
    StableLM Zephyr 3B model and gTTS, enabling the voice assistant to understand
    and respond to user queries in a natural, conversational manner:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`transcribe(audio)` 函数是语音助手系统的关键组件。它无缝集成了Whisper的转录能力与StableLM Zephyr 3B模型和gTTS，能够让语音助手以自然、对话的方式理解并回应用户的查询：'
- en: '[PRE20]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Creating the user interface**: This Python code creates a user interface
    using the Gradio library, allowing users to interact with the voice assistant
    system. The interface consists of a microphone input for capturing audio, two
    text boxes displaying the transcribed text and the generated response, and an
    audio player to back the response as speech:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建用户界面**：这段 Python 代码使用 Gradio 库创建了一个用户界面，允许用户与语音助手系统进行交互。该界面包括一个麦克风输入用于捕捉音频，两个文本框分别显示转录的文本和生成的响应，以及一个音频播放器播放语音响应：'
- en: '[PRE21]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This cell creates a user interface using Gradio. The interface includes a microphone
    input for the user’s voice, a textbox to display the transcribed text, a textbox
    to display the GPT-3 model’s response, and an audio player to play the model’s
    response in audio format:'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个单元使用 Gradio 创建了一个用户界面。界面包括一个麦克风输入用于接收用户的声音，一个文本框显示转录的文本，一个文本框显示 GPT-3 模型的响应，以及一个音频播放器播放模型的音频响应：
- en: '![Figure 5.4 – Whisper voice assistant](img/B21020_05_4.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – Whisper 语音助手](img/B21020_05_4.jpg)'
- en: Figure 5.4 – Whisper voice assistant
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – Whisper 语音助手
- en: The Python code review shows how OpenAI’s Whisper can be integrated into a chatbot
    architecture. We’ve learned how to install and import necessary libraries, set
    up environment variables for OpenAI API authentication, load the Whisper model,
    and create a user interface for interaction. We’ve also seen how to define functions
    for interacting with free models, such as Stability AI’s StableLM Zephyr 3B, Google’s
    gTTS, and transcribing audio input into text using Whisper. This hands-on approach
    has given us a practical understanding of how Whisper can be utilized to build
    a voice assistant, demonstrating its potential to enhance chatbot architectures.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Python 代码审查展示了如何将 OpenAI 的 Whisper 集成到聊天机器人架构中。我们学习了如何安装和导入必要的库，设置 OpenAI API
    身份验证的环境变量，加载 Whisper 模型，并创建用于交互的用户界面。我们还看到如何定义与免费模型交互的函数，例如 Stability AI 的 StableLM
    Zephyr 3B，Google 的 gTTS，以及如何使用 Whisper 将音频输入转录为文本。这种动手实践让我们对如何利用 Whisper 构建语音助手有了实际的理解，展示了它在增强聊天机器人架构中的潜力。
- en: As we move forward, we’ll delve into the next section, *Quantizing Whisper for
    chatbot efficiency and user experience*, where we’ll explore how to fine-tune
    the integration of Whisper into our chatbot to improve its performance and make
    the user experience more seamless and engaging. We’ll look at techniques for optimizing
    the transcription process, handling different languages and accents, and improving
    the responsiveness of our chatbot. So, let’s continue our journey and discover
    how to unlock the full potential of Whisper in creating efficient and user-friendly
    chatbot systems.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将深入探讨 *为聊天机器人效率和用户体验量化 Whisper*，我们将探索如何微调 Whisper 在聊天机器人中的集成，以提升其性能，使用户体验更加流畅和有趣。我们将关注优化转录过程、处理不同语言和口音，以及提升聊天机器人响应速度的技术。所以，让我们继续前行，发现如何释放
    Whisper 在创建高效且用户友好的聊天机器人系统中的全部潜力。
- en: Quantizing Whisper for chatbot efficiency and user experience
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为了提高聊天机器人的效率和用户体验，对 Whisper 进行量化
- en: The quest for efficiency and performance optimization is a constant endeavor.
    One such technique that has gained significant attention is the quantization of
    models, particularly in the context of ASR systems such as OpenAI’s Whisper.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 对效率和性能优化的追求是一个持续的努力。近年来，量化模型技术在 ASR（自动语音识别）系统中，特别是在 OpenAI 的 Whisper 中，得到了广泛关注。
- en: '**Quantization** is a family of techniques that aim to decrease a model’s size
    and prediction latency, primarily by reducing the precision of the model’s weights.
    For instance, this could involve decreasing the precision from 16 to 8 decimal
    points or converting from floating-point to integer representation. This process
    can significantly reduce memory requirements, enabling efficient deployment on
    edge devices and embedded platforms for real-time applications.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**量化**是一系列旨在减少模型大小和预测延迟的技术，主要通过降低模型权重的精度来实现。例如，这可能涉及将精度从 16 位小数点降至 8 位小数点，或将浮点数转换为整数表示。这一过程可以显著减少内存需求，从而实现高效的部署在边缘设备和嵌入式平台上，支持实时应用。'
- en: 'The quantification of Whisper can offer several benefits, particularly in the
    context of chatbots and voice assistants:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 对 Whisper 进行量化可以带来多个好处，特别是在聊天机器人和语音助手的应用中：
- en: '**Performance improvement**: Quantization can significantly speed up the inference
    time of the Whisper model, especially on CPU-based deployments. This is particularly
    beneficial for applications with limited computational resources, such as laptops
    or mobile devices. For instance, applying a simple post-training dynamic quantization
    process included with PyTorch to OpenAI Whisper can provide up to 3x speedups
    for CPU-based deployment.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能提升**：量化可以显著加速Whisper模型的推理时间，特别是在基于CPU的部署中。这对于计算资源有限的应用非常有益，比如笔记本电脑或移动设备。例如，将PyTorch中包含的简单后训练动态量化过程应用于OpenAI
    Whisper，可以为基于CPU的部署提供最多3倍的加速。'
- en: '**Model size reduction**: Quantization can also reduce the model’s size, making
    it more efficient to store and transfer. This is particularly useful for deploying
    models on edge devices with limited storage capacity.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型尺寸缩减**：量化还可以减少模型的大小，使其存储和传输更加高效。这对于在存储容量有限的边缘设备上部署模型尤为有用。'
- en: '**Maintained accuracy**: Anecdotal results show that the accuracy for smaller
    models remains the same, if not slightly higher, after quantization. However,
    accuracy may be reduced somewhat for the largest model.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保持准确性**：根据经验结果，较小模型的准确性在量化后保持不变，甚至略有提高。然而，最大的模型的准确性可能会略微降低。'
- en: However, it’s important to note that the benefits of quantization can vary depending
    on the specific model and the hardware it’s deployed on. As such, it’s essential
    to carefully evaluate the impact of quantization in your particular context. The
    next chapter will explore Whisper quantization in more detail with hands-on coding.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，值得注意的是，量化的好处可能会根据具体的模型和部署的硬件有所不同。因此，在您的特定环境中，必须仔细评估量化的影响。下一章将更详细地探讨Whisper的量化，并结合实际编程示范。
- en: Having explored the integration of Whisper into chatbots and voice assistants,
    let’s now turn our attention to another crucial application area. The following
    section will delve into how Whisper can enhance accessibility features, starting
    with identifying the need for Whisper in accessibility tools and evaluating its
    impact on user experience.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨了Whisper在聊天机器人和语音助手中的应用后，我们现在将注意力转向另一个至关重要的应用领域。接下来的部分将深入探讨Whisper如何增强辅助功能，从识别Whisper在辅助工具中的需求开始，并评估其对用户体验的影响。
- en: Enhancing accessibility features with Whisper
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Whisper增强辅助功能
- en: 'In the previous sections, we explored how Whisper can be utilized for transcription
    services and integrated into voice assistants and chatbots. Now, we turn our attention
    to a different, yet equally important, application of this technology: enhancing
    accessibility features.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们探讨了Whisper如何用于转录服务，并与语音助手和聊天机器人相结合。现在，我们将关注这种技术的另一种同样重要的应用：增强辅助功能。
- en: The first subsection will delve into the current landscape of accessibility
    tools and identify gaps that Whisper can fill. Why is there a need for Whisper
    in this space? What unique capabilities does it bring to the table that can enhance
    the functionality of existing tools? These are the questions we will explore,
    providing a comprehensive understanding of the necessity and potential of Whisper
    in this domain.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 第一小节将深入探讨当前辅助工具的现状，并识别Whisper能够填补的空白。为什么在这个领域需要Whisper？它带来了哪些独特的能力，能够增强现有工具的功能？这些问题是我们将要探讨的内容，旨在全面了解Whisper在该领域的必要性和潜力。
- en: Following this, we will assess Whisper’s tangible impact on the user experience.
    How does the integration of Whisper into accessibility tools affect the end user?
    What improvements can be observed, and what are the implications of these improvements
    for individuals who rely on these tools? This section will provide a detailed
    evaluation, offering insights into the real-world impact of Whisper’s integration.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将评估Whisper对用户体验的实际影响。Whisper在辅助工具中的集成如何影响最终用户？能观察到哪些改进？这些改进对依赖这些工具的个人有哪些意义？本节将提供详细的评估，深入分析Whisper集成的实际影响。
- en: As we embark on this exploration, it’s important to remember that our journey
    is about more than understanding Whisper’s technical aspects. It’s about recognizing
    its transformative potential and how it can enhance the lives of individuals with
    hearing or speech challenges.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们展开这次探索时，重要的是要记住，我们的旅程不仅仅是理解Whisper的技术方面。更重要的是认识到它的变革性潜力，以及它如何改善听力或语言障碍者的生活。
- en: So, are you ready to delve into the world of Whisper and its potential to enhance
    accessibility features? Let’s begin this exciting exploration, and remember –
    the journey of understanding is just as important as the destination.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你准备好深入探索 Whisper 及其在提升可访问性方面的潜力了吗？让我们开始这段令人兴奋的探索之旅，记住——理解的过程与最终的目标同样重要。
- en: Identifying the need for Whisper in accessibility tools
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 识别在辅助工具中需要 Whisper 的地方
- en: The world is becoming more digitally connected, and with this comes the need
    for more inclusive and accessible technologies. Interaction with digital devices
    can be challenging for individuals with hearing or speech impairments. Traditional
    input methods, such as typing or touch, may be more feasible and efficient for
    these users. This is where Whisper comes into play.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 随着世界数字化连接的加深，对更具包容性和可访问性技术的需求也在增加。对于听力或语言障碍的人群，与数字设备的互动可能是一个挑战。传统的输入方式，如打字或触控，可能对这些用户来说更加可行和高效。这正是
    Whisper 发挥作用的地方。
- en: Whisper’s ASR technology can transcribe spoken language into written text, making
    digital content more accessible for those with hearing impairments. It can also
    convert written commands into actions, providing an alternative input method for
    those with speech impairments. By integrating Whisper into accessibility tools,
    we can improve the user experience for these individuals, making digital devices
    more inclusive and user-friendly.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 的 ASR 技术可以将口语转录为书面文本，使听力障碍者能够更轻松地访问数字内容。它还可以将书面命令转化为行动，为语言障碍者提供一种替代输入方式。通过将
    Whisper 集成到辅助工具中，我们可以改善这些用户的体验，使数字设备变得更加包容和易于使用。
- en: Leveraging the unique capabilities of Whisper
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 利用 Whisper 的独特能力
- en: Whisper offers several unique capabilities that can enhance the functionality
    of existing tools. One critical advantage of Whisper is its exceptional accuracy.
    Whisper demonstrated an impressive accuracy rate when tested against various speech
    recognition systems. This high level of accuracy can significantly improve the
    reliability of transcription services, making them more useful for individuals
    with hearing impairments.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 提供了几个独特的功能，可以增强现有工具的功能。Whisper 的一个关键优势是其卓越的准确性。在与多种语音识别系统的测试中，Whisper
    展现了令人印象深刻的准确率。这种高准确度可以显著提高转录服务的可靠性，使其对听力障碍者更加有用。
- en: Whisper is also capable of understanding and transcribing multiple languages.
    This multilingual capability can make digital content more accessible to a broader
    range of users, breaking down language barriers and fostering more efficient and
    inclusive communication.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 还能够理解和转录多种语言。这种多语言能力可以使数字内容对更广泛的用户更具可访问性，打破语言障碍，促进更加高效和包容的沟通。
- en: Another unique feature of Whisper is its open source nature. OpenAI has made
    Whisper available for public use, encouraging developers to integrate it into
    various applications and explore new possibilities. This open source approach
    promotes innovation and allows for continuously improving technology, expanding
    Whisper’s reach and impact.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 的另一个独特特点是其开源性质。OpenAI 已将 Whisper 提供给公众使用，鼓励开发者将其集成到各种应用中，并探索新的可能性。这种开源方法促进了创新，并使技术得以不断改进，扩大了
    Whisper 的影响力和覆盖范围。
- en: Enhancing existing accessibility tools with Whisper
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Whisper 增强现有的辅助工具
- en: Whisper’s capabilities can be leveraged to enhance the functionality of existing
    accessibility tools. For instance, Whisper can be integrated into transcription
    services to provide more accurate and reliable transcriptions. This can improve
    the accessibility of audio content for those with hearing impairments, making
    it easier for them to consume and engage with this content.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 的功能可以用于提升现有辅助工具的功能。例如，Whisper 可以集成到转录服务中，以提供更准确和可靠的转录结果。这将改善听力障碍者对音频内容的可访问性，使他们更容易理解和参与这些内容。
- en: Whisper can also be integrated into voice assistants and chatbots to enhance
    their capabilities. By transcribing spoken commands into written text, Whisper
    can make these tools more interactive and user-friendly, particularly for those
    with speech impairments.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 还可以集成到语音助手和聊天机器人中，提升其功能。通过将口语命令转录为书面文本，Whisper 可以使这些工具更具互动性和用户友好性，特别是对于有语言障碍的用户。
- en: Furthermore, Whisper’s multilingual capability can be used to enhance language
    learning tools. By transcribing and translating spoken language in near real time,
    Whisper can provide immediate feedback to learners, helping them to improve their
    language skills more effectively.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Whisper的多语言能力可以用于增强语言学习工具。通过几乎实时地转录和翻译口语，Whisper可以为学习者提供即时反馈，帮助他们更有效地提高语言技能。
- en: The integration of Whisper into accessibility tools is just the beginning. As
    Whisper continues to evolve, we expect to see even more improvements in user experience.
    For instance, the possibility of Whisper extending its capabilities to more languages
    could lead to a genuinely global transcription tool.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper融入辅助工具的整合仅仅是个开始。随着Whisper的持续发展，我们期待在用户体验方面看到更多的改进。例如，Whisper扩展到更多语言的可能性，可能会使其成为真正全球化的转录工具。
- en: Moreover, integrating Whisper with other AI models could create more powerful
    and versatile systems. For instance, combining Whisper with GPT-3, OpenAI’s language
    prediction model, could lead to systems that understand the spoken language and
    predict and generate human-like text.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，将Whisper与其他AI模型整合，可以创造出更强大、更多样化的系统。例如，将Whisper与OpenAI的语言预测模型GPT-3结合，可以实现理解口语并预测生成类人文本的系统。
- en: Thus, let’s delve deeper into the tangible impact of Whisper on the user experience,
    exploring the improvements it brings and the implications of these enhancements
    for individuals who rely on these tools.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们深入探讨Whisper对用户体验的实际影响，探索它带来的改进以及这些增强对依赖这些工具的个人的影响。
- en: Whisper’s primary function is to convert spoken language into written text,
    a feature that has proven invaluable in enhancing the functionality of accessibility
    tools. For instance, it has been integrated into transcription services, voice
    assistants, and chatbots, making these technologies more interactive and user-friendly.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper的主要功能是将口语转换为书面文本，这一功能在增强辅助工具功能方面被证明是非常宝贵的。例如，它已经被整合到转录服务、语音助手和聊天机器人中，使得这些技术更加互动和用户友好。
- en: One of the most significant impacts of Whisper is its potential to bridge communication
    gaps and make the world more inclusive. It has improved inclusivity in applications
    such as the On Wheels app ([https://dataroots.io/blog/on-wheels](https://dataroots.io/blog/on-wheels)),
    a mobile application redefining accessibility in urban environments for wheelchair
    users, people with reduced mobility, and parents using a stroller or baby carriage.
    It provides a map that displays a wide range of practical information, such as
    the location of accessible restaurants, bars, museums, toilets, shops, parking
    spots, hospitals, pharmacies, and petrol stations. For instance, a user might
    say, “*Add a new accessible restaurant at 123 Main Street. The entrance is 32
    inches wide, and there is a ramp leading to the door. The restroom is also accessible,
    with a doorway width of 36 inches.*” The app, powered by Whisper, would transcribe
    this voice input into text. It would then extract the relevant information, such
    as the restaurant’s address, entrance width, presence of a ramp, and restroom
    accessibility details. This data would be added to the app’s database, making
    it available for other users searching for accessible locations in the area. The
    integration of Whisper AI into the On Wheels app has significantly improved the
    user experience for people with speech or hearing impairments. Whisper has been
    utilized to develop a voice assistant for the app. This voice-powered functionality
    caters to users who face typing or visual impairments, allowing them to participate
    more fully by using voice commands to interact with the app. Using **natural language**,
    the voice assistant enables users to provide information about locations they
    want to add to the app, such as the function of the building, the address, the
    entrance, or the toilet. This has increased inclusivity by allowing users who
    might not be able to use or contribute to the app’s accessibility information
    through traditional means to do so via voice. The app will enable users to personalize
    their experience based on the width of their wheelchair and the height of the
    doorstep they can manage, showing only locations that are easily accessible to
    them. Users can also contribute by measuring their favorite places in the city
    to help others enjoy them in the future.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper的最显著影响之一是它有潜力弥合沟通鸿沟，让世界变得更加包容。它改善了诸如On Wheels应用（[https://dataroots.io/blog/on-wheels](https://dataroots.io/blog/on-wheels)）中的包容性，这是一款为轮椅使用者、行动不便人士和推婴儿车或手推车的父母重新定义城市环境无障碍的移动应用。该应用提供一个地图，显示一系列实用信息，例如无障碍餐厅、酒吧、博物馆、厕所、商店、停车位、医院、药店和加油站的位置。例如，用户可能会说：“*在123主街添加一家新的无障碍餐厅。入口宽度为32英寸，并有一条坡道通向门口。厕所也可通行，门宽为36英寸。*”该应用由Whisper提供支持，将这段语音输入转录为文本，然后提取相关信息，如餐厅地址、入口宽度、是否有坡道以及厕所的无障碍详情。这些数据会被添加到应用的数据库中，使其他搜索该地区无障碍位置的用户可以使用。Whisper
    AI的整合显著改善了语音或听力障碍者的用户体验。Whisper还被用来开发该应用的语音助手功能。这个语音功能特别适合那些有打字或视力障碍的用户，使他们可以通过语音命令与应用互动，更全面地参与其中。通过**自然语言**，语音助手使用户能够提供他们想添加到应用中的地点信息，例如建筑物的功能、地址、入口或厕所。这增加了包容性，让那些可能无法通过传统方式使用或贡献无障碍信息的用户，能够通过语音实现这一目标。该应用还将根据用户的轮椅宽度和可管理的门口高度，个性化其体验，显示仅对他们容易到达的地点。用户也可以通过测量自己喜欢的城市地点来贡献，帮助他人未来也能享受这些地方。
- en: The integration of Whisper into accessibility tools has led to several observable
    improvements in user experience. For instance, Whisper has replaced keyboards,
    allowing users to write with their voice, which can be particularly beneficial
    for individuals with motor impairments.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper技术的整合到辅助工具中，带来了用户体验的几项显著改善。例如，Whisper替代了键盘，允许用户通过语音输入，这对运动障碍者尤其有益。
- en: In education, WhisperPhone ([https://whisperphone.com/](https://whisperphone.com/)),
    a learning tool, uses Whisper to amplify and convey learners’ voices directly
    to their ears, enhancing the auditory feedback loop and assisting learners in
    hearing, producing, and correcting the proper sounds of a language. This tool
    has been particularly beneficial for learners with learning and developmental
    disabilities and those on the autism spectrum.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在教育领域，WhisperPhone（[https://whisperphone.com/](https://whisperphone.com/)）作为一种学习工具，利用
    Whisper 技术将学习者的声音直接传递到耳朵，增强听觉反馈回路，帮助学习者听到、产生和纠正语言的正确发音。这个工具对有学习和发育障碍的学习者以及自闭症谱系的个体尤其有益。
- en: Moreover, Whisper’s robustness and generalizability make integrating existing
    products or services easier, improving their usability. Its high accuracy and
    speed also contribute to a more seamless user experience.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Whisper 的鲁棒性和通用性使得集成现有的产品或服务变得更加容易，从而提高了它们的可用性。它的高准确性和速度也有助于提供更流畅的用户体验。
- en: For instance, the On Wheels app’s voice-powered functionality allows users with
    typing or visual impairments to contribute to the app’s database, enhancing their
    participation and engagement. Similarly, WhisperPhone’s ability to enhance the
    auditory feedback loop can improve language learning outcomes for individuals
    with learning and developmental disabilities.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，On Wheels 应用程序的语音驱动功能使得有打字或视觉障碍的用户能够为应用程序的数据库贡献内容，从而提高他们的参与度和互动性。同样，WhisperPhone
    增强听觉反馈回路的能力也能改善有学习和发育障碍的个体的语言学习成果。
- en: Transitioning from the conceptual to the practical, we now focus on a hands-on
    application that leverages Whisper alongside vision-to-text generative AI models
    and Google’s gTTS service. This next section illustrates how these technologies
    can be integrated to develop an interactive image-to-text application, demonstrating
    Whisper’s versatility and role in advancing accessibility and user engagement.
    Let’s explore the step-by-step process and insights gained from this implementation.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念到实践的过渡，我们现在集中于一个实际应用，利用 Whisper 与视觉转文本生成 AI 模型以及 Google 的 gTTS 服务相结合。接下来的部分展示了如何将这些技术集成，以开发一个互动的图像到文本应用，展示了
    Whisper 的多功能性及其在推动无障碍和用户互动中的作用。让我们一起探索这个实现的逐步过程和收获的见解。
- en: Building an interactive image-to-text application with Whisper
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Whisper 构建互动的图像到文本应用
- en: Transitioning from evaluating Whisper’s impact on user experience in accessibility
    tools, let’s delve into a practical application that combines Whisper, GPT-4 Vision,
    and Google’s gTTS service. This application will take an image and audio input,
    transcribe the audio, describe the image, and then convert the description back
    into speech.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 从评估 Whisper 对无障碍工具中用户体验的影响过渡到实际应用，让我们深入探讨一个结合 Whisper、GPT-4 Vision 和 Google
    的 gTTS 服务的应用。该应用将接收图像和音频输入，转录音频，描述图像，然后将描述转回语音。
- en: 'I encourage you to visit the book’s GitHub repository, find the notebook `LOAIW_ch05_4_Whisper_img2txt_LlaVa_image_assistant.ipynb`
    notebook ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter05/LOAIW_ch05_4_Whisper_img2txt_LlaVa_image_assistant.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter05/LOAIW_ch05_4_Whisper_img2txt_LlaVa_image_assistant.ipynb)),
    and try the application yourself. The following diagram describes how the notebook
    is a practical example of using these models in tandem to process and interpret
    audio and visual data:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励你访问本书的 GitHub 仓库，找到 `LOAIW_ch05_4_Whisper_img2txt_LlaVa_image_assistant.ipynb`
    笔记本（[https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter05/LOAIW_ch05_4_Whisper_img2txt_LlaVa_image_assistant.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter05/LOAIW_ch05_4_Whisper_img2txt_LlaVa_image_assistant.ipynb)），亲自尝试这个应用。下图描述了该笔记本如何作为实际示例，展示如何结合使用这些模型来处理和解释音频与视觉数据：
- en: '![Figure 5.5 – Whisper img2txt LlaVa image assistant](img/B21020_05_5.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – Whisper img2txt LlaVa 图像助手](img/B21020_05_5.jpg)'
- en: Figure 5.5 – Whisper img2txt LlaVa image assistant
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – Whisper img2txt LlaVa 图像助手
- en: '*Figure 5**.5* illustrates the primary goal of the notebook: showcase the capabilities
    of **LlaVa** as a multimodal image-text-to-text model, which is described as an
    *open source version of GPT-4-vision*, and to demonstrate how it can be combined
    with Whisper’s audio processing to build a comprehensive multimodal AI system.
    Here are the high-level steps with some selected code snippets to illustrate:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.5* 说明了本笔记本的主要目标：展示**LlaVa**作为一个多模态图像文本转文本模型的能力，它被描述为*GPT-4-vision的开源版本*，并展示了如何将其与Whisper的音频处理结合，构建一个全面的多模态AI系统。以下是高层步骤，并附有一些选定的代码片段以进行说明：'
- en: '`transformers`, `bitsandbytes`, `accelerate`, `whisper`, `gradio`, and `gTTS`.
    A temporary audio file is also created using `ffmpeg` to facilitate audio processing:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`transformers`、`bitsandbytes`、`accelerate`、`whisper`、`gradio`和`gTTS`。还使用`ffmpeg`创建一个临时音频文件以便音频处理：'
- en: '[PRE22]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '**Configuring quantization**: This section includes code to prepare the quantization
    configuration, which is essential for loading the LlaVa model with 4-bit precision.
    This step is crucial for optimizing the model’s memory and speed performance:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**配置量化**：本节包括准备量化配置的代码，这对于加载具有4位精度的LlaVa模型至关重要。这一步骤对优化模型的内存和速度性能至关重要：'
- en: '[PRE23]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Initializing the LlaVa model**: We log in to the Hugging Face Hub and initialize
    the image-to-text pipeline with the LlaVa model, applying the earlier quantization
    configuration. This pipeline processes images and generates descriptive text:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化LlaVa模型**：我们登录Hugging Face Hub并初始化图像到文本的管道，使用先前的量化配置。此管道处理图像并生成描述性文本：'
- en: '[PRE24]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`PIL` library:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`PIL`库：'
- en: '[PRE25]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**Generating text from images**: This section prompts the LlaVa model to describe
    the loaded image in detail. It uses a specific format for the prompt and processes
    the output to extract and print the generated text:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**从图像生成文本**：本节提示LlaVa模型详细描述加载的图像。它使用特定格式的提示，并处理输出以提取并打印生成的文本：'
- en: '[PRE26]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '**Processing speech to text**: The Whisper model is loaded, and a function
    is defined to transcribe audio input into text. This section also includes code
    to check for GPU availability, which is preferred for running Whisper:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将语音转为文本**：加载Whisper模型，并定义一个函数将音频输入转录为文本。本节还包括检查GPU可用性的代码，这对于运行Whisper是首选的：'
- en: '[PRE27]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: import re
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 导入re
- en: import requests
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 导入requests
- en: from PIL import Image
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从PIL导入Image
- en: 'def img2txt(input_text, input_image):'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '定义img2txt(input_text, input_image):'
- en: '# load the image'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '# 加载图像'
- en: image = Image.open(input_image)
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: image = Image.open(input_image)
- en: '# writehistory(f"Input text: {input_text} - Type: {type(input_text)} - Dir:
    {dir(input_text)}")'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '# 写入历史记录(f"输入文本: {input_text} - 类型: {type(input_text)} - 目录: {dir(input_text)}")'
- en: 'if type(input_text) == tuple:'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '如果type(input_text) == tuple:'
- en: prompt_instructions = """
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: prompt_instructions = """
- en: Describe the image using as much detail as possible, is it a painting, a photograph,
    what colors are predominant, what is the image about?
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用尽可能多的细节描述图像，它是画作、照片吗？有哪些主色调？图像的内容是什么？
- en: '"""'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"""'
- en: 'else:'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '否则:'
- en: prompt_instructions = """
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: prompt_instructions = """
- en: 'Act as an expert in imagery descriptive analysis, using as much detail as possible
    from the image, respond to the following prompt:'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作为影像描述分析专家，使用尽可能多的细节从图像中提取信息，回答以下提示：
- en: '""" + input_text'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '""" + input_text'
- en: 'prompt = "USER: <image>\n" + prompt_instructions + "\nASSISTANT:"'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'prompt = "USER: <image>\n" + prompt_instructions + "\nASSISTANT:"'
- en: 'outputs = pipe(image, prompt=prompt, generate_kwargs={"max_new_tokens": 200})'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'outputs = pipe(image, prompt=prompt, generate_kwargs={"max_new_tokens": 200})'
- en: '# Properly extract the response text'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '# 正确提取响应文本'
- en: 'if outputs is not None and len(outputs[0]["generated_text"]) > 0:'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '如果outputs不为空且len(outputs[0]["generated_text"]) > 0:'
- en: match = re.search(r'ASSISTANT:\s*(.*)', outputs[0]["generated_text"])
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: match = re.search(r'ASSISTANT:\s*(.*)', outputs[0]["generated_text"])
- en: 'if match:'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '如果匹配成功:'
- en: '# Extract the text after "ASSISTANT:"'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '# 提取"ASSISTANT:"后的文本'
- en: reply = match.group(1)
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: reply = match.group(1)
- en: 'else:'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '否则:'
- en: reply = "No response found."
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: reply = "未找到响应。"
- en: 'else:'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '否则:'
- en: reply = "No response generated."
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: reply = "未生成响应。"
- en: return reply
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 返回回复
- en: 'def transcribe(audio):'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '定义转录(audio):'
- en: '# Check if the audio input is None or empty'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '# 检查音频输入是否为None或为空'
- en: 'if audio is None or audio == '''':'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果音频为None或为空：
- en: return ('','',None)  # Return empty strings and None audio file
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '返回 ('''','''',None)  # 返回空字符串和None音频文件'
- en: '# language = ''en'''
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '# 语言 = ''en'''
- en: audio = whisper.load_audio(audio)
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: audio = whisper.load_audio(audio)
- en: audio = whisper.pad_or_trim(audio)
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: audio = whisper.pad_or_trim(audio)
- en: mel = whisper.log_mel_spectrogram(audio).to(model.device)
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: mel = whisper.log_mel_spectrogram(audio).to(model.device)
- en: _, probs = model.detect_language(mel)
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: _, probs = model.detect_language(mel)
- en: options = whisper.DecodingOptions()
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: options = whisper.DecodingOptions()
- en: result = whisper.decode(model, mel, options)
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: result = whisper.decode(model, mel, options)
- en: result_text = result.text
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: result_text = result.text
- en: return result_text
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return result_text
- en: 'def text_to_speech(text, file_path):'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def text_to_speech(text, file_path):'
- en: language = 'en'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: language = 'en'
- en: audioobj = gTTS(text = text,
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: audioobj = gTTS(text = text,
- en: lang = language,
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: lang = language,
- en: slow = False)
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: slow = False)
- en: audioobj.save(file_path)
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: audioobj.save(file_path)
- en: return file_path
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return file_path
- en: '[PRE28]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Running the Gradio interface**: The final section of the notebook sets up
    a Gradio interface that allows users to interact with the system by uploading
    images and providing voice input. The interface processes the inputs using the
    defined functions for image description and audio transcription, providing audio
    and text outputs. See the notebook for the implementation:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**运行Gradio界面**：本笔记本的最后一部分设置了一个Gradio界面，允许用户通过上传图像和提供语音输入与系统进行交互。该界面使用定义的函数处理输入，进行图像描述和音频转录，并提供音频和文本输出。有关实现，请参阅笔记本：'
- en: '![Figure 5.6 – This application demonstrates the power of combining Whisper,
    LlaVa, and gTTS; it provides a practical tool for describing images based on audio
    input, which can be particularly useful for accessibility applications](img/B21020_05_6.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – 本应用演示了结合Whisper、LlaVa和gTTS的强大功能；它提供了一个基于音频输入描述图像的实用工具，对于无障碍应用尤其有用](img/B21020_05_6.jpg)'
- en: Figure 5.6 – This application demonstrates the power of combining Whisper, LlaVa,
    and gTTS; it provides a practical tool for describing images based on audio input,
    which can be particularly useful for accessibility applications
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – 本应用演示了结合Whisper、LlaVa和gTTS的强大功能；它提供了一个基于音频输入描述图像的实用工具，对于无障碍应用尤其有用
- en: Summary
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小结
- en: In this chapter, we embarked on an enlightening journey exploring the expansive
    capabilities of OpenAI’s Whisper. Together, we took a deep dive into how Whisper
    is revolutionizing voice technology, especially in transcription services, voice
    assistants, chatbots, and enhancing accessibility features.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们开始了一段启发性的旅程，探索了OpenAI的Whisper的广泛功能。我们深入了解了Whisper如何革新语音技术，特别是在转录服务、语音助手、聊天机器人以及增强无障碍功能方面的应用。
- en: We began by exploring transcription services, where Whisper excels in converting
    spoken language into written text. Its encoder-decoder Transformer model ensures
    high accuracy, even in challenging acoustic conditions. We also discussed Whisper’s
    limitations, such as speaker diarization, while highlighting the community’s efforts
    to enhance its capabilities.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先探讨了转录服务，在这方面Whisper擅长将口语转化为书面文字。其编码器-解码器Transformer模型确保了即使在复杂的音频条件下也能提供高准确性。我们还讨论了Whisper的局限性，如说话人分离问题，并强调了社区在提升其能力方面的努力。
- en: Next, we delved into setting up Whisper for transcription tasks, providing a
    comprehensive hands-on guide covering installation and configuration steps. The
    chapter emphasized the importance of understanding and adjusting Whisper’s parameters,
    such as `DecodingOptions`, for optimal performance.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们探讨了如何设置Whisper进行转录任务，并提供了一个涵盖安装和配置步骤的全面实践指南。本章强调了理解并调整Whisper参数（如`DecodingOptions`）以实现最佳性能的重要性。
- en: In the voice assistants and chatbots section, we explored how Whisper’s integration
    elevates user experiences. We discussed the architecture of chatbots and voice
    assistants, explaining how Whisper complements their existing structures. The
    focus here was on balancing technical proficiency and user-centric design.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在语音助手和聊天机器人部分，我们探讨了Whisper集成如何提升用户体验。我们讨论了聊天机器人和语音助手的架构，解释了Whisper如何与现有结构相辅相成。本部分的重点在于平衡技术能力和以用户为中心的设计。
- en: Then, we turned our attention to enhancing accessibility features with Whisper.
    We assessed Whisper’s impact on user experience, particularly for individuals
    with hearing or speech challenges. Whisper’s high accuracy, multilingual capabilities,
    and open source nature make it a game-changer in accessibility tools.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将注意力转向了如何利用Whisper增强无障碍功能。我们评估了Whisper对用户体验的影响，特别是对于听力或语言障碍的个体。Whisper的高准确性、多语言能力以及开源特性使其成为无障碍工具的革命性改变者。
- en: Finally, we concluded the chapter with a second hands-on coding example, demonstrating
    the integration of Whisper into a voice assistant. We provided a step-by-step
    guide showcasing the practical application of Whisper in a chatbot architecture.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过第二个实践编码示例来总结本章，演示了Whisper如何集成到语音助手中。我们提供了逐步的指南，展示了Whisper在聊天机器人架构中的实际应用。
- en: As we wrap up this chapter, we look ahead to [*Chapter 6*](B21020_06.xhtml#_idTextAnchor160),
    *Expanding Applications with Whisper*. Here, we’ll go deeper into Whisper’s versatile
    applications across various industries. From transcription services to voice-based
    search, we’ll explore how Whisper’s transformative potential can be harnessed
    in diverse sectors, enhancing professional and consumer experiences. Join us as
    we continue to unravel the endless possibilities with Whisper.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我们展望[*第6章*](B21020_06.xhtml#_idTextAnchor160)，*使用Whisper扩展应用*。在这一章中，我们将深入探讨Whisper在各个行业中的多种应用。从转录服务到基于语音的搜索，我们将探索如何利用Whisper的变革性潜力，提升各个领域的专业和消费者体验。加入我们，一起继续揭开Whisper带来无限可能的面纱。
