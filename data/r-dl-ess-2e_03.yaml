- en: Deep Learning Fundamentals
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习基础
- en: In the previous chapter, we created some machine learning models using neural
    network packages in R. This chapter will look at some of the fundamentals of neural
    networks and deep learning by creating a neural network using basic mathematical
    and matrix operations. This application sample will be useful for explaining some
    key parameters in deep learning algorithms and some of the optimizations that
    allow them to train on large datasets. We will also demonstrate how to evaluate
    different hyper-parameters for models to find the best set. In the previous chapter,
    we briefly looked at the problem of overfitting; this chapter goes into that topic
    in more depth and looks at how you can overcome this problem. It includes an example
    use case using dropout, the most common regularization technique in deep learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用 R 中的神经网络包创建了一些机器学习模型。本章将通过使用基本的数学和矩阵运算创建一个神经网络，来介绍一些神经网络和深度学习的基础知识。这个应用示例将有助于解释深度学习算法中的一些关键参数，以及一些允许它们在大数据集上训练的优化方法。我们还将展示如何评估模型的不同超参数，以找到最佳的设置。在上一章中，我们简要地讨论了过拟合问题；本章将更深入地探讨这个话题，并介绍如何克服这个问题。它包括一个使用
    dropout（深度学习中最常见的正则化技术）的示例用例。
- en: 'This chapter covers the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Building neural networks from scratch in R
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零开始在 R 中构建神经网络
- en: Common parameters in deep learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习中的常见参数
- en: Some key components in deep learning algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习算法中的一些关键组件
- en: Using regularization to overcome overfitting
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正则化克服过拟合
- en: Use case—improving out-of-sample model performance using dropout
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例—使用 dropout 改善样本外模型性能
- en: Building neural networks from scratch in R
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始在 R 中构建神经网络
- en: Although we have already used some neural network algorithms, it's time to dig
    a bit deeper into how they work. This section demonstrates how to code a neural
    network from scratch. It might surprise you to see that the core code for a neural
    network can be written in fewer than 80 lines! The code for this chapter does
    just that using an interactive web application written in R. It should give you
    more of an intuitive understanding of neural networks. First we will look at the
    web application, then we will delve more deeply into the code for the neural network.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经使用了一些神经网络算法，但现在是时候更深入地了解它们是如何工作的了。本节演示了如何从头开始编写神经网络的代码。你可能会惊讶地发现，神经网络的核心代码可以用不到
    80 行代码来编写！本章的代码正是这样做的，它使用 R 编写了一个交互式网络应用程序。这应该能让你更直观地理解神经网络。首先我们将看这个网络应用程序，然后我们将更深入地探讨神经网络的代码。
- en: Neural network web application
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络 Web 应用程序
- en: First, we will look at an R Shiny web application. I encourage you to run the
    application and follow the examples as it will really help you to get a better
    understanding of how neural networks work. In order to run it, you will have to
    open the `Chapter3` project in RStudio.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将看一个 R Shiny 网络应用程序。我鼓励你运行该应用并按照示例操作，这将真正帮助你更好地理解神经网络是如何工作的。为了运行它，你需要在
    RStudio 中打开 `Chapter3` 项目。
- en: '**What is R Shiny**?'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是 R Shiny**？'
- en: R Shiny is an R package from the RStudio company that allows you to create interactive
    web apps using only R code. You can build dashboards and visualizations, and use
    the full functionality of R. You can extend R Shiny apps with CSS, widgets, and
    JavaScript. It is also possible to host your applications online. It is a great
    tool with which to showcase data science applications and I encourage you to look
    into it if you are not already familiar with it. For more information, see [https://shiny.rstudio.com/](https://shiny.rstudio.com/),
    and, for examples of what is possible with R Shiny, see [https://shiny.rstudio.com/gallery/](https://shiny.rstudio.com/gallery/).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: R Shiny 是 RStudio 公司提供的一个 R 包，它允许你仅使用 R 代码创建交互式 Web 应用程序。你可以构建仪表板和可视化，使用 R 的完整功能。你还可以通过
    CSS、组件和 JavaScript 扩展 R Shiny 应用程序。还可以将你的应用程序托管在网上。它是一个展示数据科学应用的好工具，如果你还不熟悉它，我鼓励你了解一下。更多信息请见
    [https://shiny.rstudio.com/](https://shiny.rstudio.com/)，如果你想了解 R Shiny 能做什么，可以查看
    [https://shiny.rstudio.com/gallery/](https://shiny.rstudio.com/gallery/)。
- en: 'Open the `server.R` file in RStudio and click on the Run App button:'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 `server.R` 文件，在 RStudio 中点击 Run App 按钮：
- en: '![](img/fb1268b3-83d8-4a8a-816f-88ead0e115a4.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb1268b3-83d8-4a8a-816f-88ead0e115a4.png)'
- en: 'Figure 3.1: How to run an R Shiny application'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1：如何运行 R Shiny 应用程序
- en: 'When you click on the Run App button, you should get a pop-up screen for your
    web application. The following is a screenshot of the web application after it
    starts up:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你点击 "Run App" 按钮时，应该会弹出一个网页应用的屏幕。以下是应用启动后的截图：
- en: '![](img/bd66fb12-3ed7-40ef-8407-b5b2a8d444d9.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd66fb12-3ed7-40ef-8407-b5b2a8d444d9.png)'
- en: Figure 3.2: R Shiny application on startup
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：R Shiny 应用启动时的界面
- en: 'This web application can be used in the pop-up window or opened in a browser.
    On the left, there is a set of input choices; these are parameters for the neural
    network. These are known as hyper-parameters, in order to distinguish between
    the *parameters* that the model is trying to optimize. From top to bottom, these hyper-parameters
    are:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网页应用可以在弹出窗口中使用，也可以在浏览器中打开。左侧是输入选择的集合，这些是神经网络的参数。它们被称为超参数，以便与模型尝试优化的*参数*区分开来。从上到下，这些超参数包括：
- en: '**Select data**: There are four different datasets that you can use as training
    data.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择数据**：你可以使用四个不同的数据集作为训练数据。'
- en: '**Nodes in hidden layer**: The number of nodes in the hidden layer. The neural
    network has only one hidden layer.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层中的节点数**：隐藏层中节点的数量。神经网络只有一个隐藏层。'
- en: '**# Epochs**: The number of times that the algorithm iterates over the data
    during model-building.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**# 迭代次数**：模型构建过程中算法对数据迭代的次数。'
- en: '**Learning rate**: The learning rate applied during backpropagation. The learning
    rate affects how much the algorithm changes the weights during every epoch.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**：在反向传播过程中应用的学习率。学习率影响算法在每次迭代时权重的变化量。'
- en: '**Activation function**: The activation function applied to the output of each
    node.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**：应用于每个节点输出的激活函数。'
- en: The Run NN Model button trains a model with the selection of input. The Reset
    button restores input choices to the default values.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"Run NN Model" 按钮使用输入选择的内容训练一个模型。"Reset" 按钮将输入选择恢复为默认值。'
- en: There are four different datasets to choose from, each with a different data
    distribution; you can select them from the drop-down box. They have descriptive
    names; for example, the data that is plotted in *Figure 3.2* is called `bulls_eye`.
    These datasets are from another R package that is used to test clustering algorithms.
    The data has two classes of equal size and is composed of various geometric shapes.
    You can explore these datasets using the web application. The only change we make
    to the data is to randomly switches labels for 5% of the data. When you run the
    application, you will notice that there are some red points in the inner circle
    and some blue points in the outer circle. This is done so that our models should
    only achieve a maximum accuracy of 0.95 (95%). This gives us confidence that the
    model is working correctly. If the accuracy is higher than this, the model could
    be overfitting because the function it has learned is too complex. We will discuss
    overfitting again in the next section.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有四个不同的数据集可供选择，每个数据集的分布不同；你可以从下拉框中选择它们。它们有描述性名称；例如，*图 3.2* 中绘制的数据被称为 `bulls_eye`。这些数据集来自另一个用于测试聚类算法的
    R 包。数据包含两个相等大小的类别，并由各种几何形状组成。你可以使用这个网页应用探索这些数据集。我们对数据所做的唯一改变是随机交换 5% 数据的标签。当你运行应用时，你会注意到在内圈有一些红色点，外圈有一些蓝色点。这是为了让我们的模型的最大准确度只能达到
    0.95（95%）。这能让我们确信模型运行正常。如果准确度超过了这个值，模型可能会过拟合，因为它学习到的函数过于复杂。我们将在下一节中再次讨论过拟合问题。
- en: One of the first steps in machine learning should be to establish a benchmark
    score, this is useful for gauging your progress. A benchmark score could be a
    rule of thumb, or a simple machine learning algorithm; it should not be something
    that you spend a lot of time working on. In this application, we use a basic logistic
    regression model as a benchmark. We can see that in the previous screenshot, the
    accuracy for the logistic regression model is only 0.6075, or 60.75% accuracy.
    This is not much over 50%, but recall that logistic regression can only fit a
    straight line and this data cannot be separated using a straight line. A neural
    network should improve on the logistic regression benchmark, so if we get an accuracy
    of less than 0.6075 on this dataset, something is wrong with our model and we
    should review it.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的第一步之一应该是建立一个基准分数，这对于衡量你的进展非常有用。基准分数可以是经验法则，或者是一个简单的机器学习算法；它不应是你花费大量时间精力去研究的内容。在这个应用程序中，我们使用基本的逻辑回归模型作为基准。我们可以看到，在前面的截图中，逻辑回归模型的准确率只有0.6075，或者说60.75%。这仅仅比50%稍高，但请记住，逻辑回归只能拟合一条直线，而这组数据无法用一条直线分割开来。神经网络应该能在逻辑回归的基准上有所改进，因此，如果我们在这个数据集上的准确率低于0.6075，那说明我们的模型存在问题，应该检查。
- en: 'So let''s begin! Click on the Run NN Model button, which runs a neural network
    model on the data using the input choices. After a few seconds, the application
    should change to resemble the following screenshot:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，开始吧！点击“运行神经网络模型”按钮，该按钮使用输入的选择在数据上运行神经网络模型。几秒钟后，应用程序应会更改为以下截图所示的样子：
- en: '![](img/a81d0b71-cb02-4770-8b2c-f98b0a58ef3e.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a81d0b71-cb02-4770-8b2c-f98b0a58ef3e.png)'
- en: 'Figure 3.3: Neural network model execution with default settings'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3：神经网络模型默认设置下的执行
- en: The application takes a few seconds and then it creates a graph of the cost
    function over the **# Epochs** and outputs cost function values as the algorithm
    iterates over the data. The text output also includes the final accuracy for the
    model in the text at the bottom right of the screen. In the diagnostic messages
    in the bottom right, we can see that the cost decreases during training and we
    achieved a final accuracy rate of 0.825\. The cost is what the model is trying
    to minimize – a lower cost means better accuracy. It took some time for the cost
    to start decreasing as the model struggled initially to get the right weights.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序需要几秒钟时间，然后它会创建一个关于**# Epochs**的成本函数图，并输出随着算法对数据进行迭代而得到的成本函数值。文本输出还包括屏幕右下角的最终准确率。在右下角的诊断信息中，我们可以看到，在训练过程中成本下降，我们达到了0.825的最终准确率。成本是模型试图最小化的值——成本越低，准确率越高。由于模型最初很难获得正确的权重，所以成本下降花了一些时间。
- en: In deep learning models, weights and biases should be not initialized with random
    values. If random values are used, this can lead to problems with training, such
    as vanishing or exploding gradients. This is where the weights get too small or
    too large and the model fails to train successfully. Also, if the weights are
    not correctly initialized, the model will take longer to train, as we saw earlier.
    Two of the most popular techniques to initialize weights to avoid these problems
    are the Xavier initialization and the He initialization (named after their inventors).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习模型中，权重和偏差不应使用随机值进行初始化。如果使用随机值，可能会导致训练问题，例如梯度消失或梯度爆炸。这是因为权重变得过小或过大，导致模型无法成功训练。另外，如果权重没有正确初始化，模型训练的时间会更长，正如我们之前所看到的。避免这些问题的两种最流行的初始化技术是Xavier初始化和He初始化（以它们的发明者命名）。
- en: 'We can see in *Figure 3.3* that the cost has not plateaued, the last few values
    show it is still decreasing. This indicates that the model can be improved if
    we train it for longer. Change **# Epochs** to **7000** and click the **Run NN
    Model** button again; the screen will change to resemble the following plot:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图 3.3*中看到，成本尚未趋于平稳，最后几组数值显示它仍在下降。这表明，如果我们训练模型更长时间，它是可以改进的。将**# Epochs**更改为**7000**，然后再次点击**运行神经网络模型**按钮；屏幕将更改为以下图形：
- en: '![](img/3d8c44aa-f4e8-4abc-b034-ad19f339540c.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d8c44aa-f4e8-4abc-b034-ad19f339540c.png)'
- en: Figure 3.4: Neural network model execution with more epochs
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4：神经网络模型执行（更多的训练周期）
- en: 'Now we get an accuracy of 0.95, which is the maximum possible accuracy rate.
    We notice that the cost values have plateaued (that is, are not decreasing further)
    to around 0.21\. This indicates that training the model for longer (that is, more
    epochs) will probably not improve the results, regardless of the current accuracy
    number. If the model is under training and the cost values have plateaued, we
    would need to consider changing the architecture of the model or getting more
    data to improve our accuracy. Let''s look at changing the number of nodes in our
    model. Click the Reset button to change the input values to their defaults, then
    change the number of nodes to 7, and click the **Run NN Model** button. Now the
    screen will change to the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到了0.95的准确率，这是最大可能的准确率。我们注意到成本值已经平稳（即不再进一步下降），大约为0.21。这表明即使训练模型更长时间（即更多的迭代次数），结果也可能不会有所改善，无论当前的准确率如何。如果模型尚未充分训练且成本值已经平稳，我们就需要考虑更改模型的架构，或者获取更多的数据来提高准确率。让我们看看如何更改模型中的节点数量。点击重置按钮，将输入值更改为默认值，然后将节点数量更改为7，点击**运行神经网络模型**按钮。现在屏幕将变为以下内容：
- en: '![](img/a8b24308-5c5f-4579-84a9-d4464c3bfaf3.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a8b24308-5c5f-4579-84a9-d4464c3bfaf3.png)'
- en: Figure 3.5: Neural network model execution with more nodes
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5：更多节点的神经网络模型执行
- en: Our accuracy here is 0.68, but compare this to the earlier examples, when we
    used the same input and only three nodes. We actually get worse performance with
    more nodes! This is because our data has a relatively simple pattern, and a model
    with seven nodes might be too complex and will take longer to train. Adding more
    nodes to a layer will increase training time but does not always improve performance.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的准确率是0.68，但与之前的示例相比，当我们使用相同的输入并且只有三个节点时，结果要差得多！实际上，更多的节点反而使性能更差！这是因为我们的数据模式相对简单，七个节点的模型可能过于复杂，训练时间也更长。向一层中添加更多的节点会增加训练时间，但不一定提高性能。
- en: 'Let''s look at the **Learning rate**. Click the **Reset** button to change
    the input values to their defaults, then change the **Learning rate** to around
    **5**, and click the **Run NN Model** button again to replicate the following
    screen:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看**学习率**。点击**重置**按钮，将输入值更改为默认值，然后将**学习率**更改为大约**5**，然后再次点击**运行神经网络模型**按钮，复现以下屏幕：
- en: '![](img/763dd827-fdfa-48c6-9dcc-42e3465a985a.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/763dd827-fdfa-48c6-9dcc-42e3465a985a.png)'
- en: Figure 3.6: Neural network model execution with larger learning rate
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6：较大学习率的神经网络模型执行
- en: We get 0.95 accuracy again, which is the best possible accuracy. If we compare
    it to the previous examples, we can see that the model *converged* (that is, the
    length of time it took for the cost function to plateau) much quicker, after just
    **500** epochs. We needed fewer epochs, so we can see an inverse relationship
    between learning rates and training epochs. A higher learning rate may mean you
    need fewer epochs. But are bigger learning rates always better? Well, no.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次得到了0.95的准确率，这是最好的准确率。如果我们与之前的示例进行比较，可以看到模型*收敛*得更快（即成本函数平稳的时间），仅用了**500**次迭代。我们需要的迭代次数更少，因此可以看到学习率和训练周期数之间存在反比关系。较高的学习率可能意味着你需要更少的训练周期。但更大的学习率总是更好吗？嗯，不是的。
- en: 'Click the **Reset** button to change the input values to their defaults, then
    change the **Learning rate** to the maximum value (**20**), and click the **Run
    NN Model** button again. When you do, you will get similar output to the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**重置**按钮，将输入值更改为默认值，然后将**学习率**更改为最大值（**20**），然后再次点击**运行神经网络模型**按钮。当你这么做时，你将看到类似于以下的输出：
- en: '![](img/68feee2c-d3dc-44ca-9f99-7cf61160a1c2.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/68feee2c-d3dc-44ca-9f99-7cf61160a1c2.png)'
- en: Figure 3.7: Neural network model execution with too great a learning rate
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7：学习率过大的神经网络模型执行
- en: We get an accuracy rate of 0.83\. What just happened? By selecting a huge learning
    rate, our model failed to converge at all. We can see that the cost function actually
    increases at the start of training, which indicates that the Learning rate is
    too high. Our cost function graph seems to have repeating values, which indicates
    that the gradient-descent algorithm is overshooting the minima at times.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了0.83的准确率。刚刚发生了什么？通过选择一个非常大的学习率，我们的模型根本没有收敛。我们可以看到，成本函数在训练开始时实际上增加了，这表明学习率太高。我们的成本函数图看起来有重复的值，这表明梯度下降算法有时会超过最小值。
- en: 'Finally, we can look at how the choice of activation function affects model
    training. By changing the activation function, you may also need to change the
    **Learning rate**. Click the **Reset** button to change the input values to their
    defaults and select `tanh` for the activation function. When we select `tanh`
    as the activation function and 1.5 as the **Learning rate**, the cost gets `stuck` at
    0.4 from epochs 500-3,500 before suddenly decreasing to 0.2\. This can occur in
    neural networks when they get stuck in local optima. This phenomena can be seen
    in the following plot:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以观察激活函数的选择如何影响模型训练。通过更改激活函数，您可能还需要更改**学习率**。点击**重置**按钮，将输入值恢复为默认值，并选择激活函数为`tanh`。当我们选择`tanh`作为激活函数，且学习率为1.5时，成本在500到3,500个epochs之间卡在0.4，随后突然下降到0.2。这种现象在神经网络中会出现，当网络卡在局部最优解时。这种现象可以通过以下图表看到：
- en: '![](img/205bbcd5-1d22-4e25-a98b-39cfc4ef1baa.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/205bbcd5-1d22-4e25-a98b-39cfc4ef1baa.png)'
- en: Figure 3.8: Neural network model execution with the tanh activation function
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8：使用tanh激活函数的神经网络模型执行
- en: 'In contrast, using relu activation results in the model training faster. The
    following is an example where we only run 1,500 epochs with the relu activation
    to get the maximum possible accuracy of 0.95:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，使用relu激活函数会导致模型训练速度更快。以下是一个示例，我们只运行1,500个epochs，使用relu激活函数即可获得最大准确率0.95：
- en: '![](img/06a0a37f-e52a-4037-a549-b455d75dd459.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06a0a37f-e52a-4037-a549-b455d75dd459.png)'
- en: Figure 3.9: Neural network model execution with the relu activation function
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9：使用relu激活函数的神经网络模型执行
- en: 'I encourage you to experiment with the other datasets. For reference purposes,
    here is the max accuracy I got for each of those datasets. An interesting experiment
    is to see how different activation functions and learning rates work with these
    datasets:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励您尝试其他数据集。作为参考，以下是我在每个数据集上获得的最大准确率。一个有趣的实验是观察不同的激活函数和学习率在这些数据集上的表现：
- en: '**worms (accuracy=0.95)**: 3 nodes, 3,000 epochs, Learning rate = 0.5, activation
    = tanh'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蠕虫（准确率=0.95）**：3个节点，3,000个epochs，学习率=0.5，激活函数=tanh'
- en: '**moon (accuracy=0.95)**: 5 nodes, 5,000 epochs, Learning rate = 5, activation
    = sigmoid'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**月亮（准确率=0.95）**：5个节点，5,000个epochs，学习率=5，激活函数=sigmoid'
- en: '**blocks (accuracy=0.9025)**: 5 nodes, 5,000 epochs, Learning rate = 10, activation
    = sigmoid'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**块（准确率=0.9025）**：5个节点，5,000个epochs，学习率=10，激活函数=sigmoid'
- en: 'In general, you will see the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，您将看到以下结果：
- en: Using more epochs means a longer training time, which may not always be needed.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更多的epochs意味着更长的训练时间，但这并不总是必要的。
- en: 'If the model has not achieved the best accuracy and the cost function has plateaued
    (that is, it is not decreasing by much) toward the end of the training, then running
    it longer (that is, more epochs) or increasing the learning rate is unlikely to
    improve performance. Instead, look at changing the model''s architecture, such
    as by changing the # layers (not an option in this demo), adding more nodes, or
    changing the activation functions.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型尚未达到最佳准确率，并且成本函数在训练的后期已经趋于平稳（即，变化不大），那么延长训练时间（即更多epochs）或增加学习率不太可能改善性能。相反，可以考虑更改模型的架构，例如更改层数（在此演示中无法选择）、增加节点数或更改激活函数。
- en: The learning rate must be selected carefully. If the value selected is too low,
    it will take a long time for the model to train. If the value selected is too
    high, the model will fail to train.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率必须谨慎选择。如果选择的值太低，模型的训练时间将非常长。如果选择的值太高，模型将无法训练。
- en: Neural network code
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络代码
- en: 'While the web application is useful to see the output of the neural network,
    we can also run the code for the neural network to really see how it works. The
    code in `Chapter3/nnet.R` allows us to do just that. This code has the same hyper-parameters
    as in the web application; this file allows you to run the neural network from
    the RStudio IDE. The following is the code that loads the data and sets the initial
    hyper-parameters for the neural network:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Web应用程序在查看神经网络输出时非常有用，但我们也可以运行神经网络代码，真正理解它是如何工作的。`Chapter3/nnet.R`中的代码可以实现这一点。此代码具有与Web应用程序中相同的超参数；该文件允许您从RStudio
    IDE运行神经网络。以下是加载数据并设置神经网络初始超参数的代码：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This code should not be too difficult to understand, it loads a dataset and
    sets some variables. The data is created in the `getData` function from the `Chapter3/nnet_functions.R`
    file. The data is created from functions in the `clustersim` package. The `Chapter3/nnet_functions.R`
    file contains the core functionality of our neural network that we will look at
    here. Once we load our data, the next step is to initialize our weights and biases.
    The `hidden` variable controls the number of nodes in the hidden layer; we set
    it to 3\. We need two sets of weights and biases, one for the hidden layer and
    one for the output layer:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码应该不难理解，它加载了一个数据集并设置了一些变量。数据是通过 `Chapter3/nnet_functions.R` 文件中的 `getData`
    函数创建的。数据来自 `clustersim` 包中的函数。`Chapter3/nnet_functions.R` 文件包含了我们神经网络的核心功能，我们将在这里详细查看。一旦加载了数据，接下来的步骤是初始化权重和偏置。`hidden`
    变量控制隐藏层中的节点数，我们将其设置为 3。我们需要两组权重和偏置，一组用于隐藏层，另一组用于输出层：
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This creates matrices for the `(weights1, bias1)` hidden layer and the `(weights2,
    bias2)` output layer. We need to ensure our matrices have the correct dimensions.
    For example, the `weights1` matrix should have the same number of columns as the
    input layer and the same number of rows as the hidden layer. Now we move on to
    the actual processing loop of the neural network:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为 `(weights1, bias1)` 隐藏层和 `(weights2, bias2)` 输出层创建矩阵。我们需要确保矩阵的维度是正确的。例如，`weights1`
    矩阵的列数应与输入层的列数相同，行数应与隐藏层的行数相同。现在我们继续进行神经网络的实际处理循环：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We first run the forward-propagation function, then calculate a cost. We then
    call a backward-propagation step that calculates our derivatives, `(dweights1,
    dbias1, dweights2, dbias2)`. Then we update the weights and biases, `(weights1, bias1, weights2, bias2)`,
    using our Learning rate, `(lr)`. We run this loop for the number of `epochs (3000)`
    and print out a diagnostic message every 500 `epochs`. This describes how every
    neural network and deep learning model works: first call forward-propagation,
    then calculate costs and derivative values, use those to update the weights through
    back-propagation and repeat.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先运行前向传播函数，然后计算代价。接着，我们调用反向传播步骤来计算我们的导数，`(dweights1, dbias1, dweights2, dbias2)`。然后我们使用学习率
    `(lr)` 来更新权重和偏置 `(weights1, bias1, weights2, bias2)`。我们会运行这个循环指定的 `epochs (3000)`
    次，并且每 500 次 `epochs` 输出一条诊断消息。这描述了每个神经网络和深度学习模型的工作原理：首先调用前向传播，然后计算代价和导数值，利用这些值通过反向传播更新权重并重复执行。
- en: 'Now let''s look at some of the functions in the `nnet_functions.R` file. The
    following is the `forward` propagation function:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下 `nnet_functions.R` 文件中的一些函数。以下是 `forward` 传播函数：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If you looked at the code carefully, you may have noticed that the assignment
    to the `activation1`, `activation2`, `Z1`, and `Z2` variables uses `<<-` rather
    than `<-`. This makes those variables global in scope; we also want to use these
    values during back propagation. Using global variables is generally frowned upon
    and I could have returned a list, but it is acceptable here to use them because
    this application is for learning purposes.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细查看代码，可能已经注意到 `activation1`、`activation2`、`Z1` 和 `Z2` 变量的赋值使用了 `<<-` 而不是
    `<-`。这使得这些变量在全局范围内有效；我们还希望在反向传播时使用这些值。使用全局变量通常不被推荐，我本可以返回一个列表，但在这里使用它们是可以接受的，因为这个应用是为了学习目的。
- en: The two for loops expand the bias vectors into matrices, then repeat the vector
    n times. The interesting code starts with the `Z1` assignment. `Z1` is a matrix
    multiplication, followed by an addition. We call the `activation_function` function
    on that value. We then use that output value and perform a similar operation for
    `Z2`. Finally, we apply a sigmoid activation to our output layer because our problem
    is binary classification.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个 `for` 循环将偏置向量扩展为矩阵，然后将该向量重复 n 次。关键的代码从 `Z1` 赋值开始。`Z1` 是一个矩阵乘法操作，后面跟着一个加法运算。我们对该值调用
    `activation_function` 函数。然后，我们使用该输出值并对 `Z2` 执行类似的操作。最后，我们对输出层应用 sigmoid 激活函数，因为我们的问题是二分类问题。
- en: 'The following is the code for the activation function; the first parameter
    decides which function to use (`sigmoid`, `tanh`, or `relu`). The second parameter
    is the value to be used as input:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是激活函数的代码；第一个参数决定了使用哪种函数（`sigmoid`、`tanh` 或 `relu`）。第二个参数是作为输入的值：
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following is the `cost` function:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 `cost` 函数：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As a reminder, the output of the `cost` function is what we are trying to minimize.
    There are many types of `cost` functions; in this application we are using binary
    cross-entropy. The formula for binary cross-entropy is *-1/m ∑ log(ȳ[i]) * y[i] +
    (log(1 -ȳ[i]) * (1-y[i])*. Our target values (*y[i]*) are always either *1* or
    *0*, so for instances where *y[i] = 1*, this reduces to *∑**log(ȳ[i]**)*. If we
    have two rows where *y[i] = 1* and suppose that our model predicts *1.0* for the
    first row and the *0.0001* for the second row, then the costs for the rows are *log(1)=0*
    and *log(0.0001)=-9.1*, respectively. We can see that the closer to *1* the prediction
    is for these rows, the lower the `cost` value. Similarly, for rows where y[i] =
    0, this reduces to log(1-ȳ[i]), so the closer to 0 the prediction is for these
    rows, the lower the `cost` value.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，`cost`函数的输出是我们试图最小化的目标。`cost`函数有很多种类型；在这个应用中我们使用的是二元交叉熵。二元交叉熵的公式是*-1/m
    ∑ log(ȳ[i]) * y[i] + (log(1 -ȳ[i]) * (1-y[i]))*。我们的目标值（*y[i]*）始终是*1*或*0*，因此当*y[i]
    = 1*时，这就简化为*∑ log(ȳ[i])*。如果我们有两行数据，其中*y[i] = 1*，假设我们的模型对第一行预测*1.0*，对第二行预测*0.0001*，那么这两行的代价分别是*log(1)=0*和*log(0.0001)=-9.1*。我们可以看到，越接近*1*的预测，其`cost`值越低。同样，对于*y[i]
    = 0*的行，这就简化为*log(1-ȳ[i])*，因此对于这些行，越接近0的预测，其`cost`值越低。
- en: If we are trying to maximize accuracy, why don't we just use what during model
    training? Binary cross-entropy is a better `cost` function because our model does
    not just output 0 or 1, but instead outputs continuous values from 0.0 to 1.0\.
    For example, if two input rows had a target value=1 (that is, y=1), and our model
    gave probabilities of 0.51 and 0.99, then binary cross-entropy would give them
    a cost of 0.67 and 0.01, respectively. It assigns a higher cost to the first row
    because the model is unsure about it (the probability is close to 0.5). If instead
    we just looked at accuracy, we might decide that both rows have the same cost
    value because they are classified correctly (assuming we assign class=0 where
    predicted values < 0.5, and class=1 where predicted values >= 0.5).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们试图最大化准确性，为什么我们不在模型训练过程中直接使用它呢？二元交叉熵是一个更好的`cost`函数，因为我们的模型不仅仅输出0或1，而是输出从0.0到1.0之间的连续值。例如，如果两个输入行的目标值为1（即y=1），而我们的模型给出的概率分别为0.51和0.99，那么二元交叉熵分别会给它们的代价为0.67和0.01。它为第一行分配了更高的代价，因为模型对它不确定（概率接近0.5）。如果我们仅仅看准确度，我们可能会认为这两行具有相同的代价值，因为它们都被正确分类了（假设我们为预测值<0.5的分配类=0，预测值>=0.5的分配类=1）。
- en: 'The following is the code for the backward-propagation function:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是反向传播函数的代码：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Backward propagation processes the network in reverse, starting at the last
    hidden layer and finishing at the first hidden layer, that is, in the direction
    of the output layer to the input layer. In our case, we only have one hidden layer,
    so it first calculates the loss from the output layer and calculates `dweight2`
    and `dbias2`. It then calculates the `derivative` of the `activation1` value,
    which was calculated during the forward-propagation step. The `derivative` function
    is similar to the activation function, but instead of calling an activation function,
    it calculates the `derivative` of that function. For example, the `derivative`
    of `sigmoid(x)` is *sigmoid(x) * (1 - sigmoid(x))*. The `derivative` values of
    simple functions can be found in any calculus reference or online:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播按相反方向处理网络，从最后一层隐层开始，直到第一层隐层，也就是说，从输出层到输入层的方向。在我们的情况下，我们只有一层隐层，因此它首先计算来自输出层的损失，并计算`dweight2`和`dbias2`。然后它计算`activation1`值的`derivative`，这个值是在前向传播步骤中计算出来的。`derivative`函数类似于激活函数，但它不是调用激活函数，而是计算该函数的`derivative`。例如，`sigmoid(x)`的`derivative`是*sigmoid(x)*(1
    - sigmoid(x))*。简单函数的`derivative`值可以在任何微积分参考书或在线找到：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'That''s it! A working neural network using basic R code. It can fit complex
    functions and performs better than logistic regression. You might not get all
    the parts at once, that''s OK. The following is a quick recap of the steps:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！一个使用基础R代码工作的神经网络。它可以拟合复杂的函数，并且表现得比逻辑回归更好。你可能不会一次性理解所有的部分，没关系。以下是步骤的简要回顾：
- en: Run a forward-propagation step, which involves multiplying the weights by the
    input for each layer and passing the output to the next layer.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行一次前向传播步骤，这包括对每一层的输入乘以权重，并将输出传递给下一层。
- en: Evaluate the output from the final layer using the `cost` function.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`cost`函数评估最终层的输出。
- en: Based on the error rate, use backpropagation to make small adjustments to the
    weights in the nodes in each layer. The learning rate controls how much of an
    adjustment we make each time.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据错误率，使用反向传播对每一层中节点的权重进行小幅调整。学习率控制每次调整的幅度。
- en: Repeat steps 1-3, maybe thousands of times, until the `cost` function begins
    to plateau, which indicates our model is trained.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 1-3，可能成千上万次，直到 `cost` 函数开始趋于平稳，这表明我们的模型已被训练好。
- en: Back to deep learning
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回到深度学习
- en: 'Many of the concepts in the previous section apply to deep learning because
    deep learning is simply neural networks with two or more hidden layers. To demonstrate
    this, let''s look at the following code in R that loads the `mxnet` deep learning
    library and calls the help command on the function in that library that trains
    a deep learning model. Even though we have not trained any models using this library
    yet, we have already seen many of the parameters in this function:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 前一部分的许多概念适用于深度学习，因为深度学习只是具有两个或更多隐藏层的神经网络。为了演示这一点，让我们看看以下在 R 中加载 `mxnet` 深度学习库并调用该库中训练深度学习模型的函数帮助命令的代码。尽管我们尚未使用此库训练任何模型，但我们已经看到该函数中的许多参数：
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If you get errors saying the `mxnet` package is unavailable, see [Chapter 1](00c01383-1886-46d0-9435-29dfb3e08055.xhtml), *Getting
    Started with Deep Learning*, for installation instructions. However, we are not
    running any `mxnet` code in this chapter, we only want to display the help page
    for a function. So feel free to just continue reading and you can install the
    package later when we use it in the next chapter.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果出现错误提示 `mxnet` 包不可用，请参见 [第 1 章](00c01383-1886-46d0-9435-29dfb3e08055.xhtml)，*深度学习入门*，获取安装说明。然而，我们在本章中并未运行任何
    `mxnet` 代码，我们只想展示某个函数的帮助页面。所以请放心继续阅读，等到下章我们使用该库时再进行安装。
- en: 'This brings up the help page for the `FeedForward` function in the `mxnet`
    library, which is the forward-propagation/model train function. `mxnet` and most
    deep learning libraries do not have a specific *backward-*propagation function,
    they handle this implicitly:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示 `mxnet` 库中 `FeedForward` 函数的帮助页面，这是前向传播/模型训练函数。`mxnet` 和大多数深度学习库没有特定的 *反向*传播函数，它们会隐式地处理这一过程：
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We will see more of this function in subsequent chapters; for now we will just
    look at the parameters.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后续章节中看到更多此函数的内容；目前我们仅关注其参数。
- en: The symbol, X, y, and ctx parameters
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`symbol`、X、y 和 `ctx` 参数'
- en: The symbol parameter defines the deep learning architecture; X and y are the
    input and output data structures. The ctx parameter controls which device (for
    example, CPU/GPU) the model is trained on.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`symbol` 参数定义了深度学习架构；X 和 y 是输入和输出数据结构。`ctx` 参数控制模型在特定设备（例如 CPU/GPU）上进行训练。'
- en: The num.round and begin.round parameters
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`num.round` 和 `begin.round` 参数'
- en: '`num.round` is equivalent to epochs in our code; that is, however many times
    we iterate over the data. `begin.round` is where we resume training the model
    if we paused training previously. If we pause training, we can save the partially-trained
    model, reload it later, and resume training.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`num.round` 相当于我们代码中的 epochs；也就是我们迭代数据的次数。`begin.round` 是我们如果暂停训练时，恢复训练的起始点。如果我们暂停了训练，可以保存部分训练好的模型，稍后重新加载并继续训练。'
- en: The optimizer parameter
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`optimizer` 参数'
- en: 'Our implementation of neural networks used gradient descent. When researchers
    started creating more complicated multilayer neural network models, they found
    that they took an extraordinarily long time to train. This is because the basic
    gradient-descent algorithm with no optimization is not very efficient; it makes
    small steps towards its goal in each epoch regardless of what occurred in previous
    epochs. We can compare it with a guessing game: one person has to guess a number
    in a range and for each guess, they are told to go higher or lower (assuming they
    do not guess the correct number!). The higher/lower instruction is similar to
    the derivative value, it indicates the direction we must travel. Now let''s say
    that the range of possible numbers is 1 to 1,000,000 and the first guess is 1,000\.
    The person is told to go higher, which should they do:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的神经网络实现使用了梯度下降。当研究人员开始创建更复杂的多层神经网络模型时，他们发现训练时间异常长。这是因为没有优化的基本梯度下降算法效率不高；它在每个epoch中朝着目标迈出小步，无论前几个epoch发生了什么。我们可以将其与猜数字游戏进行比较：一个人必须在一个范围内猜一个数字，每次猜测后，他们会被告知是往上还是往下（假设他们没有猜对！）。高/低的指示类似于导数值，它指示了我们必须前进的方向。现在，假设可能的数字范围是1到1,000,000，第一个猜测是1,000。这个人被告知要往上猜，他们应该做什么：
- en: Try 1001.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试1001。
- en: Take the difference between the guess and the max value and divide by 2\. Add
    this value to the previous guess.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算猜测值和最大值的差，然后除以2。将这个值加到之前的猜测值上。
- en: The second option is much better and should mean the person gets to the right
    answer in 20 guesses or fewer. If you have a background in computer science, you
    may recognize this as the binary-search algorithm. The first option, guessing
    1,001, 1,002, ...., 1,000,000, is a terrible choice and will probably fail as
    one party will give up! But this is similar to how gradient descent works. It
    moves incrementally towards the target. If you try increasing the learning rate
    to overcome this problem, you can overshoot the target and the model fails to
    converge.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种选择要好得多，应该意味着这个人在20次猜测内能得到正确答案。如果你有计算机科学背景，你可能会认出这是二分查找算法。第一种选择，猜1,001，1,002，....，1,000,000，是一个糟糕的选择，可能会失败，因为一方会放弃！但这与梯度下降的工作方式相似。它逐步朝着目标前进。如果你尝试增加学习率来解决这个问题，可能会超过目标，导致模型无法收敛。
- en: Researchers came up with some clever optimizations to speed up training. One
    of the first optimizers was called momentum, and it does exactly what its name
    states. It looks at the extent of the derivative and takes bigger *steps* for
    each epoch if the previous steps were all in the same direction. It should mean
    that the model trains much quicker. There are other algorithms that are enhancements
    of these, such as RMS-Prop and Adam. You don't usually need to know how they work,
    just that, when you change the optimizer, you may also have to adjust other hyper-parameters,
    such as the learning rate. In general, look for previous examples done by others
    and copy those hyper-parameters.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员提出了一些聪明的优化方法来加速训练。第一个优化器被称为动量，它正如其名字所示。它查看导数的大小，如果前面的步骤都在同一方向，它会在每个epoch中采取更大的*步伐*。这应该意味着模型训练会更快。还有一些其他算法是这些算法的增强版，比如RMS-Prop和Adam。你通常不需要了解它们的工作原理，只需要知道，当你更换优化器时，可能还需要调整其他超参数，比如学习率。一般来说，可以参考别人做过的例子，复制那些超参数。
- en: We actually used one of these optimizers in an example in the previous chapter.
    In that chapter, we had 2 models with a similar architecture (40 hidden nodes).
    The first model (`digits.m3`) used the `nnet` library and took 40 minutes to train.
    The second model (`digits.m3`) used resilient backpropagation and took 3 minutes
    to train. This shows the benefit of using an optimizer in neural networks and
    deep learning.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们在上一章的示例中使用了其中一种优化器。在那一章中，我们有两个具有相似架构的模型（40个隐藏节点）。第一个模型（`digits.m3`）使用了`nnet`库，训练时间为40分钟。第二个模型（`digits.m3`）使用了弹性反向传播，训练时间为3分钟。这展示了在神经网络和深度学习中使用优化器的好处。
- en: The initializer parameter
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化参数
- en: When we created the initial values for our weights and biases (that is, model
    parameters), we used random numbers, but limited them to the values of -0.005
    to +0.005\. If you go back and review some of the graphs of the `cost` functions,
    you see that it took 2,000 epochs before the `cost` function began to decline.
    This is because the initial values were not in the right range and it took 2,000
    epochs to get to the correct magnitude. Fortunately, we do not have to worry about
    how to set these parameters in the `mxnet` library because this parameter controls
    how the weights and biases are initialized before training.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们为权重和偏置（即模型参数）创建初始值时，我们使用了随机数，但将它们限制在 -0.005 到 +0.005 之间。如果你回顾一下 `cost` 函数的一些图形，你会发现
    `cost` 函数开始下降时需要 2,000 轮训练。这是因为初始值不在正确的范围内，花了 2,000 轮训练才达到正确的幅度。幸运的是，我们不需要担心如何设置这些参数，因为
    `mxnet` 库中的该参数控制了在训练之前如何初始化权重和偏置。
- en: The eval.metric and eval.data parameters
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`eval.metric` 和 `eval.data` 参数'
- en: These two parameters control what data and which metric are used to evaluate the
    model. `eval.metric` is equivalent to the `cost` function we used in our code. `eval.data`
    is used if you want to evaluate the model on a holdout dataset that is not used
    in training.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个参数控制用于评估模型的数据和度量标准。`eval.metric` 相当于我们代码中使用的 `cost` 函数。`eval.data` 用于当你想要在未用于训练的验证数据集上评估模型时。
- en: The epoch.end.callback parameter
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`epoch.end.callback` 参数'
- en: This is a `callback` function that allows you to register another function that
    is called after *n* epochs. Deep learning models take a long time to train, so
    you need some feedback to know they are working correctly! You can write a custom
    `callback` function to do whatever you need, but usually it outputs to the screen
    or log after *n* epochs. This is equivalent to the code in our neural network
    that printed a diagnostic message every *500* epochs. The `callback` function
    can also be used to save the model to disk, for example, if you wanted to save
    the model before it begins to overfit.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 `callback` 函数，允许你注册另一个函数，该函数会在 *n* 轮训练后被调用。深度学习模型的训练时间通常较长，因此你需要一些反馈来确保模型正常工作！你可以编写自定义的
    `callback` 函数来执行所需的操作，但通常它会在 *n* 轮训练后输出到屏幕或日志。这相当于我们在神经网络中每 *500* 轮训练时打印诊断信息的代码。`callback`
    函数还可以用来将模型保存到磁盘，例如，如果你想在模型开始过拟合之前保存模型。
- en: The array.batch.size parameter
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`array.batch.size` 参数'
- en: We only had 400 instances (rows) in our data, which can easily fit into memory.
    However, if your input data has millions of instances, the data needs to be split
    into batches during training in order to fit in the memory of the CPU/GPU. The
    number of instances you train at a time is the batch size. Note, you still iterate
    over all the data for the number of epochs, you just split the data into batches
    during each iteration and run the forward-propagation, backpropagation step over
    each batch for each epoch. For example, if you had 100 instances and selected
    a batch size of *32* with *6* epochs, you would need *4* batches for each epoch
    (*100/32 = 3.125*, so we need *4* batches to process all the data), for a total
    of *24* loops.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据中只有 400 个实例（行），这可以轻松地适应内存。然而，如果你的输入数据有数百万个实例，则在训练过程中需要将数据分批，以便能适应 CPU/GPU
    的内存。你一次训练的实例数量就是批量大小。请注意，你仍然需要对所有数据进行迭代以完成指定轮次，你只是将数据分成多个批次，在每次迭代中将每个批次用于正向传播和反向传播步骤。例如，如果你有
    100 个实例，并选择批量大小为 *32*，训练 *6* 轮，你每轮需要 *4* 个批次（*100/32 = 3.125*，因此我们需要 *4* 个批次来处理所有数据），总共需要
    *24* 次循环。
- en: There is a trade-off in choosing the batch size. If you choose too low a value,
    the model will take a longer time to train because it's running more operations
    and batches will have more variability because of the small size. You cannot choose
    an enormous batch size either, this might cause your model to crash because it
    loads too much data into either the CPU or GPU. In most cases, you either take
    a sensible default that works from another deep learning model, or you set it
    at some value (for example, 1,024) and if your model crashes, then try again with
    a value of half the previous value (512).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 选择批量大小时存在一个权衡。如果你选择的值过小，模型的训练时间会更长，因为它需要进行更多的操作，而且由于批量较小，每个批次的数据会有更多的波动。你也不能选择一个过大的批量大小，这可能会导致模型崩溃，因为它将过多的数据加载到
    CPU 或 GPU 中。在大多数情况下，你要么使用另一个深度学习模型中的合理默认值，要么设置一个值（例如，1,024），如果模型崩溃，再尝试将其值减半（例如，512）。
- en: There is a relationship between **Batch size**, **Learning rates**, and **#
    Epochs** for training. But there are no hard and fast rules in selecting values.
    However, in general, consider changing these values together and do not use an
    extreme value for one of these hyper-parameters. For example, picking a large
    Learning rate should mean fewer epochs, but if your batch size is too small, the
    model may fail to train. The best advice is to look at similar architectures and
    pick a similar set and range of values.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**批次大小**、**学习率**和**# 训练周期**之间存在一定的关系。但在选择这些值时，并没有硬性规定。然而，通常建议一起调整这些值，并且避免使用某一超参数的极端值。例如，选择较大的学习率通常意味着训练周期较少，但如果批次大小过小，模型可能无法训练成功。最好的建议是查看类似的架构，选择一组相似的值和范围。'
- en: 'Now that we can see that deep learning still uses many of the concepts from
    neural networks, we will move on to talk about an important issue that you will
    probably encounter with every deep learning model: overfitting.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到，深度学习仍然使用许多神经网络中的概念，我们将继续讨论一个你在每个深度学习模型中都可能遇到的重要问题：过拟合。
- en: Using regularization to overcome overfitting
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用正则化来克服过拟合
- en: In the previous chapter, we saw the diminishing returns from further training
    iterations on neural networks in terms of their predictive ability on holdout
    or test data (that is, data not used to train the model). This is because complex
    models may memorize some of the noise in the data rather than learning the general
    patterns. These models then perform much worse when predicting new data. There
    are some methods we can apply to make our model generalize, that is, fit the overall
    patterns. These are called **regularization** and aim to reduce testing errors
    so that the model performs well on new data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到神经网络在进一步训练迭代中的回报逐渐递减，尤其是在对持出数据或测试数据（即未用于训练模型的数据）进行预测时。这是因为复杂的模型可能会记住数据中的一些噪音，而不是学习到一般的模式。这些模型在预测新数据时表现会更差。我们可以应用一些方法来让模型具备更强的泛化能力，也就是说，能够拟合整体的模式。这些方法被称为**正则化**，旨在减少测试误差，使得模型在新数据上表现良好。
- en: The most common regularization technique used in deep learning is dropout. However,
    we will also discuss two other regularization techniques that have a basis in
    regression and deep learning. These two regularization techniques are **L1 penalty**,
    which is also known as **Lasso**, and **L2 penalty**, which is also known as **Ridge**.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中最常用的正则化技术是dropout（丢弃法）。然而，我们还将讨论另外两种在回归和深度学习中有应用基础的正则化技术。这两种正则化技术分别是**L1惩罚**，也叫做**Lasso**，和**L2惩罚**，也叫做**Ridge**。
- en: L1 penalty
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L1惩罚
- en: The basic concept of the **L1 penalty**, also known as the **least-absolute
    shrinkage and selection operator** (**Lasso**–Hastie, T., Tibshirani, R., and
    Friedman, J. (2009)), is that a penalty is used to shrink weights toward zero.
    The penalty term uses the sum of the absolute weights, so some weights may get
    shrunken to zero. This means that Lasso can also be used as a type of variable
    selection. The strength of the penalty is controlled by a hyper-parameter, alpha
    (λ), which multiplies the sum of the absolute weights, and it can be a fixed value
    or, as with other hyper-parameters, optimized using cross-validation or some similar
    approach.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**L1惩罚**（也称为**最小绝对收缩与选择算子**，**Lasso**–Hastie, T.，Tibshirani, R. 和 Friedman,
    J.（2009））的基本概念是，通过惩罚项将权重收缩到零。惩罚项使用的是权重的绝对值之和，因此一些权重可能会被收缩为零。这意味着Lasso也可以作为一种变量选择方法。惩罚项的强度由一个超参数alpha（λ）控制，它会乘以绝对权重之和，alpha可以是一个固定值，或者像其他超参数一样，通过交叉验证或类似方法来优化。'
- en: It is easier to describe Lasso if we use an **ordinary least squares** (**OLS**)
    regression model. In regression, a set of coefficients or model weights is estimated
    using the least-squared error criterion, where the weight/coefficient vector,
    *Θ*, is estimated such that it minimizes *∑(y[i] - ȳ[i])* where *ȳ[i]=b+Θx, y[i]*
    is the target value we want to predict and *ȳ[i]* is the predicted value. Lasso
    regression adds an additional penalty term that now tries to minimize ∑(y[i] -
    ȳ[i]) + λ⌊Θ⌋, where ⌊Θ⌋ is the absolute value of *Θ*. Typically, the intercept
    or offset term is excluded from this constraint.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用**普通最小二乘法**（**OLS**）回归模型来描述Lasso，会更加直观。在回归中，使用最小二乘误差准则来估计一组系数或模型权重，其中权重/系数向量*Θ*通过最小化*∑(y[i]
    - ȳ[i])*来估计，其中*ȳ[i]=b+Θx*，*y[i]*是我们要预测的目标值，*ȳ[i]*是预测值。Lasso回归在此基础上增加了一个惩罚项，它尝试最小化∑(y[i]
    - ȳ[i]) + λ⌊Θ⌋，其中⌊Θ⌋是*Θ*的绝对值。通常，截距项或偏置项会被排除在这个约束之外。
- en: There are a number of practical implications for Lasso regression. First, the
    effect of the penalty depends on the size of the weights, and the size of the
    weights depends on the scale of the data. Therefore, data is typically standardized
    to have unit variance first (or at least to make the variance of each variable
    equal). The L1 penalty has a tendency to shrink small weights to zero (for explanations
    as to why this happens, see Hastie, T., Tibshirani, R., and Friedman, J. (2009)).
    If you only consider variables for which the L1 penalty leaves non-zero weights,
    it can essentially function as feature-selection. The tendency for the L1 penalty
    to shrink small coefficients to zero can also be convenient for simplifying the
    interpretation of the model results.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso回归有一些实际的影响。首先，惩罚的效果取决于权重的大小，而权重的大小又取决于数据的尺度。因此，数据通常会先进行标准化，使其方差为单位方差（或者至少使每个变量的方差相等）。L1惩罚倾向于将小的权重缩小到零（有关此现象的解释，见Hastie,
    T.，Tibshirani, R.和Friedman, J.（2009））。如果你只考虑L1惩罚留下非零权重的变量，它实际上可以作为特征选择。L1惩罚将小系数缩小到零的趋势，也有助于简化模型结果的解释。
- en: Applying the L1 penalty to neural networks works exactly the same for neural
    networks as it does for regression. If *X* represents the input, *Y* is the outcome
    or dependent variable, *B* the parameters, and *F* the objective function that
    will be optimized to obtain *B*, that is, we want to minimize *F(B; X, Y)*. The
    L1 penalty modifies the objective function to be F(B; X, Y) + λ⌊Θ⌋, where *Θ*
    represents the weights (typically offsets are ignored). The L1 penalty tends to
    result in a sparse solution (that is, more zero weights) as small and larger weights
    result in equal penalties, so that at each update of the gradient, the weights
    are moved toward zero.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对神经网络应用L1惩罚的方式与回归中的应用完全相同。如果*X*代表输入，*Y*是输出或因变量，*B*是参数，*F*是目标函数，我们希望通过优化目标函数*F(B;
    X, Y)*来获得*B*，即我们要最小化*F(B; X, Y)*。L1惩罚会修改目标函数为F(B; X, Y) + λ⌊Θ⌋，其中*Θ*代表权重（通常忽略偏置项）。L1惩罚倾向于产生稀疏解（即更多的零权重），因为小的和较大的权重会产生相同的惩罚，因此在每次梯度更新时，权重会朝零移动。
- en: We have only considered the case where *λ* is a constant, controlling the degree
    of penalty or regularization. However, it is possible to set different values
    with deep neural networks, where varying degrees of regularization can be applied
    to different layers. One reason for considering such differential regularization
    is that it is sometimes desirable to allow a greater number of parameters (say
    by including more neurons in a particular layer) but then counteract this somewhat
    through stronger regularization. However, this approach can be quite computationally
    demanding if we are allowing the L1 penalty to vary for every layer of a deep
    neural network and using cross-validation to optimize all possible combinations
    of the L1 penalty. Therefore, usually a single value is used across the entire
    model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只考虑了*λ*为常量的情况，这控制着惩罚或正则化的程度。然而，在深度神经网络中，可以设置不同的值，其中可以对不同的层应用不同程度的正则化。考虑这种差异化正则化的一个原因是，有时希望允许更多的参数（比如在某一层增加更多的神经元），然后通过更强的正则化来抵消这种增加。然而，如果我们允许L1惩罚在深度神经网络的每一层都变化，并使用交叉验证来优化L1惩罚的所有可能组合，这种方法可能会非常耗费计算资源。因此，通常在整个模型中使用一个单一的值。
- en: L1 penalty in action
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L1惩罚的应用
- en: 'To see how the L1 penalty works, we can use a simulated linear regression problem.
    The code for the rest of this chapter is in `Chapter3/overfitting.R`. We simulate
    the data, using a correlated set of predictors:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了观察L1惩罚是如何工作的，我们可以使用一个模拟的线性回归问题。本章其余部分的代码在`Chapter3/overfitting.R`中。我们使用一组相关的预测变量来模拟数据：
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we can fit an OLS regression model to the first 100 cases, and then use `lasso`.
    To use `lasso`, we use the `glmnet()` function from the `glmnet` package. This
    function can actually fit the L1 or the L2 (discussed in the next section) penalties,
    and which one occurs is determined by the argument, alpha. When `alpha = 1`, it
    is the L1 penalty (that is, `lasso`), and when `alpha = 0`, it is the L2 penalty
    (that is, ridge regression). Further, because we don''t know which value of `lambda`
    we should pick, we can evaluate a range of options and tune this hyper-parameter
    automatically using cross-validation, which is the `cv.glmnet()` function. We
    can then plot the `lasso` object to see the mean squared error for a variety of
    `lambda` values to allow us to select the correct level of regularization:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以将 OLS 回归模型拟合到前 100 个案例，然后使用 `lasso`。为了使用 `lasso`，我们需要使用 `glmnet` 包中的
    `glmnet()` 函数。这个函数实际上可以拟合 L1 或 L2 惩罚（将在下一节讨论），哪种惩罚取决于参数 alpha。当 `alpha = 1` 时，它是
    L1 惩罚（即 `lasso`），而当 `alpha = 0` 时，它是 L2 惩罚（即岭回归）。此外，由于我们不知道应该选择哪个 `lambda` 值，我们可以评估多个选项，并使用交叉验证自动调整这个超参数，这就是
    `cv.glmnet()` 函数。然后，我们可以绘制 `lasso` 对象，查看不同 `lambda` 值下的均方误差，以便选择合适的正则化水平：
- en: '[PRE11]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](img/d0b8aa13-f250-4111-9d27-c7b13b088466.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0b8aa13-f250-4111-9d27-c7b13b088466.png)'
- en: 'Figure 3.10: Lasso regularization'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10：Lasso 正则化
- en: 'One thing that we can see from the graph is that, when the penalty gets too
    high, the cross-validated model increases. Indeed, `lasso` seems to do well with
    very low lambda values, perhaps indicating `lasso` does not help improve out-of-sample
    performance/generalizability much for this dataset. For the sake of this example,
    we will continue but in actual use, this might give us pause to consider whether `lasso`
    was really helping. Finally, we can compare the coefficients with those from `lasso`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中我们可以看到，当惩罚过高时，交叉验证模型的误差增加。实际上，`lasso` 在非常低的 lambda 值下表现较好，这可能表明对于这个数据集，`lasso`
    对于提升样本外表现/泛化能力并没有太大帮助。为了这个示例，我们将继续，但在实际应用中，这可能会让我们停下来思考 `lasso` 是否真的起到了帮助作用。最后，我们可以将系数与
    `lasso` 的结果进行比较：
- en: '[PRE12]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Notice that the OLS coefficients are noisier and also that, in `lasso`, predictor
    5 is penalized to 0\. Recall from the simulated data that the true coefficients
    are 3, 1, 1, 1, 1, and 0\. The OLS estimates have much too low a value for the
    first predictor and much too high a value for the second, whereas `lasso` has
    more accurate values for each. This demonstrates that `lasso` regression generalizes
    better than OLS regression for this dataset.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 OLS 系数更为嘈杂，而且在 `lasso` 中，第 5 个预测变量被惩罚到 0。回顾一下模拟数据，真实的系数是 3, 1, 1, 1, 1 和
    0。OLS 的估计值对于第一个预测变量过低，对于第二个预测变量过高，而 `lasso` 对每个系数的估计值更为准确。这表明，`lasso` 回归比 OLS
    回归在这个数据集上的泛化能力更强。
- en: L2 penalty
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L2 惩罚
- en: The **L2 penalty**, also known as **ridge regression**, is similar in many ways
    to the L1 penalty, but instead of adding a penalty based on the sum of the absolute
    weights, the penalty is based on the squared weights. This means that larger absolute
    weights are penalized more. In the context of neural networks, this is sometimes
    referred to as weight decay. If you examine the gradient of the regularized objective
    function, there is a penalty such that, at every update, there is a multiplicative
    penalty to the weights. As for the L1 penalty, although they could be included,
    biases or offsets are usually excluded from this.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**L2 惩罚**，也称为 **岭回归**，在许多方面与 L1 惩罚相似，但与基于绝对权重和惩罚的 L1 惩罚不同，L2 惩罚是基于权重的平方。这意味着较大的绝对权重会受到更大的惩罚。在神经网络的背景下，这有时被称为权重衰减。如果你检查正则化目标函数的梯度，你会发现有一个惩罚项，在每次更新时，权重会受到乘法惩罚。至于
    L1 惩罚，尽管可以包括它，但通常会将偏置或偏移量排除在外。'
- en: From the perspective of a linear regression problem, the L2 penalty is a modification
    to the objective function minimized, from *∑(y[i] - ȳ[i]) *to* ∑(y[i] - ȳ[i]) + λΘ²*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 从线性回归问题的角度来看，L2 惩罚是对最小化目标函数的一种修改，修改的形式是从 *∑(y[i] - ȳ[i])* 到 *∑(y[i] - ȳ[i])
    + λΘ²*。
- en: L2 penalty in action
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L2 惩罚的应用
- en: 'To see how the L2 penalty works, we can use the same simulated linear regression
    problem we used for the Ll penalty. To fit a ridge regression model, we use the
    `glmnet()` function from the `glmnet` package. As mentioned previously, this function
    can actually fit the L1 or the L2 penalties, and which one occurs is determined
    by the argument, alpha. When `alpha = 1`, it fits `lasso`, and when `alpha = 0`,
    it fits ridge regression. This time, we choose `alpha = 0`. Again, we evaluate
    a range of lambda options and tune this hyper-parameter automatically using cross-validation.
    This is accomplished by using the `cv.glmnet()` function. We plot the ridge regression
    object to see the error for a variety of lambda values:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了观察L2惩罚项的作用，我们可以使用与L1惩罚项相同的模拟线性回归问题。为了拟合岭回归模型，我们使用来自`glmnet`包的`glmnet()`函数。如前所述，该函数实际上可以拟合L1或L2惩罚项，哪种惩罚项取决于参数alpha。当`alpha
    = 1`时，拟合的是`lasso`，而当`alpha = 0`时，拟合的是岭回归。这次我们选择`alpha = 0`。同样，我们评估一系列lambda选项，并通过交叉验证自动调节该超参数。通过使用`cv.glmnet()`函数来实现这一点。我们绘制岭回归对象，查看不同lambda值下的误差：
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](img/e89c9b9a-ea90-4458-9471-b6a6ea584165.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e89c9b9a-ea90-4458-9471-b6a6ea584165.png)'
- en: 'Figure 3.11: Ridge regularization'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11：岭正则化
- en: Although the shape is different from `lasso` in that the error appears to asymptote
    for higher lambda values, it is still clear that, when the penalty gets too high,
    the cross-validated model error increases. As with `lasso`, the ridge regression
    model seems to do well with very low lambda values, perhaps indicating the L2
    penalty does not improve out-of-sample performance/generalizability by much.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管形状与`lasso`不同，因为在更高的lambda值下误差似乎趋于平稳，但仍然可以明显看出，当惩罚项过高时，交叉验证的模型误差会增加。与`lasso`类似，岭回归模型在非常低的lambda值下似乎表现不错，这可能表明L2惩罚并未显著改善模型的外部样本表现/泛化能力。
- en: 'Finally, we can compare the OLS coefficients with those from `lasso` and the
    ridge regression model:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将OLS系数与`lasso`和岭回归模型的系数进行比较：
- en: '[PRE14]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Although ridge regression does not shrink the coefficient for the fifth predictor
    to exactly 0, it is smaller than in the OLS, and the remaining parameters are
    all slightly shrunken, but quite close to their true values of 3, 1, 1, 1, 1,
    and 0.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管岭回归没有将第五个预测变量的系数压缩到精确为0，但它比OLS中的系数要小，而且其余的参数都稍微被收缩，但仍然接近它们的真实值：3，1，1，1，1和0。
- en: Weight decay (L2 penalty in neural networks)
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重衰减（神经网络中的L2惩罚）
- en: 'We have already unknowingly used regularization in the previous chapter. The
    neural network we trained using the `caret` and `nnet` package used a weight decay
    of `0.10`. We can investigate the use of weight decay by varying it, and tuning
    it using cross-validation:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一章中已经无意间使用了正则化。我们使用`caret`和`nnet`包训练的神经网络使用了`0.10`的权重衰减。我们可以通过变化该衰减值并使用交叉验证进行调优来进一步研究权重衰减的使用：
- en: 'Load the data as before. Then we create a local cluster to run the cross-validation
    in parallel:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如之前一样加载数据。然后我们创建一个本地集群来并行运行交叉验证：
- en: '[PRE15]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Train a neural network on the digit classification, and vary the weight-decay
    penalty at `0` (no penalty) and `0.10`. We also loop through two sets of the number
    of iterations allowed: `100` or `150`. Note that this code is computationally
    intensive and takes some time to run:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数字分类任务上训练一个神经网络，并将权重衰减惩罚分别设置为`0`（无惩罚）和`0.10`。我们还分别使用两组迭代次数：`100`次或`150`次。请注意，这段代码计算量较大，运行需要一些时间：
- en: '[PRE16]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Examining the results, we see that, when we limit to only `100` iterations,
    both the non-­regularized model and regularized model have the same accuracy at
    `0.56`, based on cross-validated results, which is not very good on this data:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查结果时，我们看到，当我们限制迭代次数为`100`时，无论是非正则化模型还是正则化模型，基于交叉验证的结果，其准确率均为`0.56`，对于这些数据来说并不是很理想：
- en: '[PRE17]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Examine the model with `150` iterations to see whether the regularized or non-regularized
    model performs better:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`150`次迭代来检查模型，看看正则化模型和非正则化模型哪个表现更好：
- en: '[PRE18]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Overall, the model with more iterations outperforms the model with fewer iterations,
    regardless of the regularization. However, comparing both models with 150 iterations,
    the regularized model is superior (`accuracy= 0.66`) to the non-regularized model
    (`accuracy= 0.65`), although here the difference is relatively small.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，具有更多迭代次数的模型在表现上优于具有较少迭代次数的模型，无论是否进行正则化。然而，比较这两种均为150次迭代的模型时，正则化模型（`准确率
    = 0.66`）优于非正则化模型（`准确率 = 0.65`），尽管两者之间的差异相对较小。
- en: These results highlight that regularization is often most useful for more complex
    models that have greater flexibility to fit (and overfit) the data. In models
    that are appropriate or overly simplistic for the data, regularization will probably
    decrease performance. When developing a new model architecture, you should avoid
    adding regularization until the model is performing well on the training data. If
    you add regularization beforehand and the model performs poorly on the training
    data, you will not know whether the problem is with the model's architecture or
    because of the regularization. In the next section, we'll discuss ensemble and
    model averaging techniques, the last forms of regularization that are highlighted
    in this book.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果突出了正则化通常对于那些具有更大灵活性、能够拟合（并过拟合）数据的复杂模型最为有用。在那些对于数据来说合适或过于简单的模型中，正则化可能会降低性能。在开发新模型架构时，应该避免在模型尚未在训练数据上表现良好之前加入正则化。如果在提前加入正则化并且模型在训练数据上的表现较差，你将无法判断问题是出在模型架构上，还是正则化的影响。接下来的章节将讨论集成和模型平均技术，这是本书中突出的最后一种正则化形式。
- en: Ensembles and model-averaging
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成和模型平均
- en: Another approach to regularization involves creating multiple models (ensembles)
    and combining them, such as by model-averaging or some other algorithm for combining
    individual model results. There is a rich history of using ensemble techniques
    in machine learning, such as bagging, boosting, and random forest, that use this
    technique. The general idea is that, if you build different models using the training
    data, each model has different errors in the predicted values. Where one model
    predicts too high a value, another may predict too low a value, and when averaged,
    some of the errors cancel out, resulting in a more accurate prediction than would
    have been otherwise obtained.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种正则化方法是创建多个模型（集成），并将它们结合起来，比如通过模型平均或其他某种结合单个模型结果的算法。使用集成技术在机器学习中有着丰富的历史，比如袋装法、提升法和随机森林等方法都采用了这一技术。其基本思想是，如果你使用训练数据构建不同的模型，每个模型在预测值上会有不同的误差。当一个模型预测的值过高时，另一个模型可能预测的值过低，而当将这些模型的结果进行平均时，一些误差会相互抵消，从而得到比单独模型更准确的预测。
- en: 'The key to ensemble methods is that the different models must have some variability
    in their predictions. If the predictions from the different models are highly
    correlated, then using ensemble techniques will not be beneficial. If the predictions
    from the different models have very low correlations, then the average will be
    far more accurate as it gains the strengths of each model. The following code
    gives an example using simulated data. This small example illustrates the point
    with just three models:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法的关键在于，不同的模型必须在其预测结果中有一定的变异性。如果不同模型的预测结果高度相关，那么使用集成技术将不会有益。如果不同模型的预测结果之间相关性非常低，那么平均值将更加准确，因为它能够融合每个模型的优点。以下代码给出了一个使用模拟数据的示例。这个小示例通过仅仅三个模型来说明这一点：
- en: '[PRE19]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can see that the predictive value of each model, at least in the training
    data, varies quite a bit. Evaluating the correlations among fitted values in the
    training data can also help to indicate how much overlap there is among the model
    predictions:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，每个模型的预测值，至少在训练数据上，变化非常大。评估训练数据中拟合值之间的相关性也有助于指出模型预测之间的重叠程度：
- en: '[PRE20]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we generate predicted values for the testing data, the average of the
    predicted values, and again correlate the predictions along with reality in the
    testing data:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为测试数据生成预测值、预测值的平均值，并再次将预测值与测试数据中的实际值进行相关性分析：
- en: '[PRE21]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: From the results, we can see that the average of the three models' predictions
    performs better than any of the models individually. However, this is not always
    the case; one good model may have better predictions than the average predictions. In
    general, it is good to check that the models being averaged perform similarly,
    at least in the training data. The second lesson is that, given models with similar
    performance, it is desirable to have lower correlations between model predictions,
    as this will result in the best performing average.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果可以看出，三种模型预测的平均值表现优于任何单独的模型。然而，这并非总是如此；一个表现好的模型可能会比平均预测值表现得更好。一般来说，最好检查被平均的模型在训练数据上是否表现相似。第二个教训是，给定具有相似表现的模型，最好让模型预测之间的相关性较低，因为这将产生表现最好的平均值。
- en: There are other forms of ensemble methods that are included in other machine
    learning algorithms, for example, bagging and boosting. Bagging is used in random
    forests, where many models are generated, each having different samples of the
    data. The models are deliberately designed to be small, incomplete models. By
    averaging the predictions of lots of undertrained models that use only a portion
    of the data, we should get a more powerful model. An example of boosting includes
    gradient-boosted models (GBMs), which also use multiple models, but this time
    each model focuses on the instances that were incorrectly predicted in the previous
    model. Both random forests and GBMs have proven to be very successful with structured
    data because they reduce variance, that is, avoid overfitting the data.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他形式的集成方法，它们被包含在其他机器学习算法中，例如，bagging 和 boosting。Bagging 被用于随机森林中，其中生成了许多模型，每个模型都有不同的数据样本。这些模型故意被设计成小型、不完全的模型。通过对大量使用数据子集的、训练不足的模型的预测结果进行平均，我们应该能得到一个更强大的模型。boosting
    的一个例子是梯度提升模型（GBMs），它也使用多个模型，但这次每个模型都专注于在前一个模型中被错误预测的实例。随机森林和 GBMs 在结构化数据中已被证明非常成功，因为它们能够减少方差，即避免过拟合数据。
- en: Bagging and model-averaging are not used as frequently in deep neural networks
    because the computational cost of training each model can be quite high, and thus
    repeating the process many times becomes prohibitively expensive in terms of time
    and compute resources. Nevertheless, it is still possible to use model averaging
    in the context of deep neural networks, even if perhaps it is on only a handful
    of models rather than hundreds, as is common in random forests and some other
    approaches.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度神经网络中，bagging 和模型平均使用得不太频繁，因为训练每个模型的计算成本可能相当高，因此重复这一过程在时间和计算资源上变得非常昂贵。尽管如此，在深度神经网络中仍然可以使用模型平均，尽管可能只是在少数几个模型中进行，而不是像随机森林和其他一些方法中那样使用数百个模型。
- en: Use case – improving out-of-sample model performance using dropout
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例 – 使用 dropout 改进样本外模型的表现
- en: Dropout is a novel approach to regularization that is particularly valuable
    for large and complex deep neural networks. For a much more detailed exploration
    of dropout in deep neural networks, see Srivastava, N., Hinton, G., Krizhevsky,
    A., Sutskever, I., and Salakhutdinav, R. (2014). The concept behind dropout is
    actually quite straightforward. During the training of the model, units (for example,
    input and hidden neurons) are probabilistically dropped along with all connections
    to and from them.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 是一种新颖的正则化方法，对于大型复杂的深度神经网络尤其有价值。关于在深度神经网络中使用 dropout 的更详细探讨，请参见 Srivastava,
    N., Hinton, G., Krizhevsky, A., Sutskever, I., 和 Salakhutdinav, R. (2014)。dropout
    背后的概念实际上非常简单。在模型训练过程中，单位（例如，输入和隐藏神经元）会以一定的概率被丢弃，连同它们的所有输入输出连接一同丢弃。
- en: 'For example, the following diagram is an example of what might happen at each
    step of training for a model where hidden neurons and their connections are dropped
    with a probability of 1/3 for each epoch. Once a node is dropped, its connections
    to the next layer are also dropped. In the the following diagram, the grayed-out
    nodes and dashed connections are the ones that were dropped. It is important to
    note that the choice of nodes that are dropped changes for each epoch:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，下面的图示是一个示例，展示了在训练过程中，对于每个训练周期，隐藏神经元及其连接以 1/3 的概率被丢弃时可能发生的情况。一旦一个节点被丢弃，它与下一层的连接也会被丢弃。在下图中，灰色的节点和虚线连接表示被丢弃的部分。需要注意的是，被丢弃的节点选择在每个训练周期都会发生变化：
- en: '![](img/d34f15bc-117f-450b-a667-8d2809c2ae95.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d34f15bc-117f-450b-a667-8d2809c2ae95.jpg)'
- en: 'Figure 3.12: Dropout applied to a layer for different epochs'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12：在不同的训练周期中应用 dropout 到一个层
- en: One way to think about dropout is that it forces models to be more robust to
    perturbations. Although many neurons are included in the full model, during training
    they are not all simultaneously present, and so neurons must operate somewhat
    more independently than they would otherwise. Another way of viewing dropout is
    that, if you have a large model with N weights between hidden neurons, but 50%
    are dropped during training, although all N weights will be used during some stages
    of training, you have effectively halved the total model complexity as the average
    number of weights will be halved. This reduces model complexity, and hence helps
    to prevent the overfitting of the data. Because of this feature, if the proportion
    of dropout is p, Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and
    Salakhutdinov, R. (2014) recommend scaling up the target model complexity by 1/p
    in order to end up with a roughly equally complex model.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 思考 dropout 的一种方式是，它迫使模型对扰动更加鲁棒。虽然完整模型中包含了许多神经元，但在训练过程中并非所有神经元都会同时存在，因此神经元必须比平时更独立地运作。另一种看待
    dropout 的方式是，如果你有一个包含 N 个权重的大型模型，并且在训练过程中有 50% 的权重被丢弃，那么虽然所有 N 个权重在训练的某些阶段都会被使用，但你实际上将总模型复杂度减少了一半，因为平均权重数量将减少一半。这减少了模型的复杂度，因此有助于防止数据的过拟合。由于这一特性，Srivastava,
    N., Hinton, G., Krizhevsky, A., Sutskever, I., 和 Salakhutdinov, R. (2014) 推荐通过
    1/p 来放大目标模型的复杂度，以便最终得到一个大致相同复杂度的模型。
- en: During model testing/scoring, neurons are not usually dropped because it is
    computationally inconvenient. Instead, we can use an approximate average based
    on scaling the weights from a single neural network based on each weight's probability
    of being included (that is, 1/p). This is usually taken care of by the deep learning
    library.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型测试/评分时，通常不会丢弃神经元，因为这在计算上不方便。相反，我们可以使用一种近似平均值，通过根据每个权重被包含的概率（即 1/p）对单个神经网络的权重进行缩放。这通常由深度学习库处理。
- en: In addition to working well, this approximate weight re-scaling is a fairly
    trivial calculation. Thus, the primary computational cost of dropout comes from
    the fact that a model with more neurons and weights must be used because so many
    (a commonly recommended value is around 50% for hidden neurons) are dropped during
    each training update.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 除了效果良好外，这种近似的权重重缩放是一个相对简单的计算。因此，dropout 的主要计算成本来自于必须使用一个包含更多神经元和权重的模型，因为在每次训练更新中，会丢弃很多（通常推荐隐藏神经元的丢弃比例为
    50%）。
- en: Although dropout is easy to implement, a larger model may be required to compensate.
    To speed up training, a higher learning rate can be used so that fewer epochs
    are required. One potential downside of combining these approaches is that, with
    fewer neurons and a faster learning rate, some weights may become quite large.
    Fortunately, it is possible to use dropout along with other forms of regularization,
    such as the L1 or L2 penalty. Taken together, the result is a larger model that
    that can quickly (a faster Learning rate) explore a broader parameter space, but
    is regularized through dropout and a penalty to keep the weights in check.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 dropout 很容易实现，但可能需要一个更大的模型来进行补偿。为了加速训练，可以使用更高的学习率，从而减少所需的训练轮数。结合这些方法的一个潜在缺点是，由于神经元减少和学习率加快，某些权重可能会变得非常大。幸运的是，可以将
    dropout 与其他正则化方法结合使用，例如 L1 或 L2 惩罚。综合来看，结果是一个更大的模型，它能够快速（通过更快的学习率）探索更广泛的参数空间，但通过
    dropout 和惩罚进行正则化，以保持权重在合理范围内。
- en: 'To show the use of dropout in a neural network, we will return to the **Modified
    National Institute of Standards and Technology** (**MNIST**) dataset (which we
    downloaded in [Chapter 2](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml), *Training
    a Prediction Model*) we worked with previously. We will use the `nn.train()` function
    from the `deepnet` package, as it allows for dropout. As in the previous chapter,
    we will run the four models in parallel to reduce the time it takes. Specifically,
    we compare four models, two with and two without dropout regularization and with
    either 40 or 80 hidden neurons. For dropout, we specify the proportion to dropout
    separately for the hidden and visible units. Based on the rule of thumb that about
    50% of hidden units (and 80% of observed units) should be kept, we specify the
    dropout proportions at `0.5` and `0.2`, respectively:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示 dropout 在神经网络中的应用，我们将回到**修改版国家标准与技术研究院**（**MNIST**）数据集（我们在[第二章](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml)，*训练预测模型*中下载过该数据集）进行操作。我们将使用来自`deepnet`包的`nn.train()`函数，因为它支持
    dropout。与前一章一样，我们将并行运行四个模型，以减少所需时间。具体来说，我们比较了四个模型，其中两个使用 dropout 正则化，两个不使用 dropout
    正则化，且每个模型有 40 或 80 个隐藏神经元。对于 dropout，我们分别为隐藏层和可见单元指定不同的 dropout 比例。根据经验法则，大约 50%
    的隐藏单元（和 80% 的观察单元）应被保留，我们分别将 dropout 比例指定为 `0.5` 和 `0.2`：
- en: '[PRE22]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we can loop through the models to obtain predicted values and get the
    overall model performance:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以遍历模型，获取预测值并评估整体模型的性能：
- en: '[PRE23]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'When evaluating the models in the in-sample training data, it seems those without
    regularization perform better those with regularization. Of course, the real test
    comes with the testing or holdout data:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估样本内训练数据时，似乎没有正则化的模型表现优于有正则化的模型。当然，真正的考验来自于测试数据或保留数据：
- en: '[PRE24]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The testing data highlights that the in-sample performance was overly optimistic
    (accuracy = 0.9622 versus accuracy = 0.8520 for the 80-neuron, non-regularized
    model in the training and testing data, respectively). We can see the advantage
    of the regularized models for both the 40- and the 80-neuron models. Although
    both still perform worse in the testing data than they did in the training data,
    they perform on a par with, or better than, the equivalent non-regularized models
    in the testing data. This difference is particularly important for the 80-neuron
    model as the best performing model on the test data is the regularized model.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据突显了样本内性能过于乐观（准确率 = 0.9622 与训练和测试数据中 80 神经元、无正则化模型的准确率 = 0.8520）。我们可以看到，正则化模型在
    40 和 80 神经元模型中都有明显的优势。尽管这两个模型在测试数据中的表现仍然不如训练数据，但它们在测试数据中的表现与同等的无正则化模型相当，甚至更好。这一差异对
    80 神经元模型尤其重要，因为在测试数据中表现最好的模型是正则化模型。
- en: Although these numbers are by no means record-setting, they do show the value
    of using dropout, or regularization more generally, and how one might go about
    trying to tune the model and dropout parameters to improve the ultimate testing
    performance.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些数字远未创造记录，但它们确实展示了使用 dropout 或更广泛的正则化的价值，以及如何调整模型和 dropout 参数以提高最终的测试性能。
- en: Summary
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter began by showing you how to program a neural network from scratch.
    We demonstrated the neural network in a web application created by just using
    R code. We delved into how the neural network actually worked, showing how to
    code forward-propagation, `cost` functions, and backpropagation. Then we looked
    at how the parameters for our neural network apply to modern deep learning libraries
    by looking at the `mx.model.FeedForward.create` function from the `mxnet` deep
    learning library.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 本章一开始向你展示了如何从零开始编程一个神经网络。我们展示了如何仅使用 R 代码创建的 Web 应用程序中的神经网络。我们深入探讨了神经网络的实际工作原理，展示了如何编码前向传播、`cost`
    函数和反向传播。然后，我们查看了神经网络参数如何应用于现代深度学习库，通过查看 `mxnet` 深度学习库中的`mx.model.FeedForward.create`函数。
- en: Then we covered overfitting, demonstrating several approaches to preventing
    overfitting, including common penalties, the Ll penalty and L2 penalty, ensembles
    of simpler models, and dropout, where variables and/or cases are dropped to make
    the model noisy. We examined the role of penalties in regression problems and
    neural networks. In the next chapter, we will move into deep learning and deep
    neural networks, and see how to push the accuracy and performance of our predictive
    models even further.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们讲解了过拟合，展示了几种防止过拟合的方法，包括常见的惩罚项、L1惩罚和L2惩罚、简单模型的集成以及丢弃法（dropout），其中变量和/或样本被丢弃以增加模型的噪声。我们考察了惩罚项在回归问题和神经网络中的作用。在下一章中，我们将进入深度学习和深度神经网络的内容，看看如何进一步提高预测模型的准确性和性能。
