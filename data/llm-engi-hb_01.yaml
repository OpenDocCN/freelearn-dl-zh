- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Understanding the LLM Twin Concept and Architecture
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解LLM Twin的概念和架构
- en: By the end of this book, we will have walked you through the journey of building
    an end-to-end **large language model** (**LLM**) product. We firmly believe that
    the best way to learn about LLMs and production **machine learning** (**ML**)
    is to get your hands dirty and build systems. This book will show you how to build
    an LLM Twin, an AI character that learns to write like a particular person by
    incorporating its style, voice, and personality into an LLM. Using this example,
    we will walk you through the complete ML life cycle, from data gathering to deployment
    and monitoring. Most of the concepts learned while implementing your LLM Twin
    can be applied in other LLM-based or ML applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到这本书的结尾，我们将带你完成构建一个端到端**大型语言模型**（**LLM**）产品的旅程。我们坚信，了解LLMs和生产的**机器学习**（**ML**）的最佳方式是通过亲自动手构建系统。这本书将向你展示如何构建一个LLM
    Twin，这是一个通过将特定人的风格、声音和个性融入LLM来学习写作的AI角色。通过这个例子，我们将带你了解完整的ML生命周期，从数据收集到部署和监控。在实现你的LLM
    Twin过程中学到的多数概念都可以应用于其他基于LLM或ML的应用。
- en: 'When starting to implement a new product, from an engineering point of view,
    there are three planning steps we must go through before we start building. First,
    it is critical to understand the problem we are trying to solve and what we want
    to build. In our case, what exactly is an LLM Twin, and why build it? This step
    is where we must dream and focus on the “Why.” Secondly, to reflect a real-world
    scenario, we will design the first iteration of a product with minimum functionality.
    Here, we must clearly define the core features required to create a working and
    valuable product. The choices are made based on the timeline, resources, and team’s
    knowledge. This is where we bridge the gap between dreaming and focusing on what
    is realistic and eventually answer the following question: “What are we going
    to build?”.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当开始实施新产品时，从工程角度来看，在开始构建之前，我们必须经历三个规划步骤。首先，理解我们试图解决的问题以及我们想要构建的内容至关重要。在我们的案例中，LLM
    Twin究竟是什么，为什么构建它？这一步是我们必须梦想并专注于“为什么”。其次，为了反映现实世界场景，我们将设计一个具有最小功能的产品第一迭代。在这里，我们必须明确定义创建一个有效且有价值的产品所需的核心功能。选择基于时间表、资源和团队的知识。这是我们将梦想与现实之间的差距连接起来，并最终回答以下问题的地方：“我们将构建什么？”。
- en: Finally, we will go through a system design step, laying out the core architecture
    and design choices used to build the LLM system. Note that the first two components
    are primarily product-related, while the last one is technical and focuses on
    the “How.”
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将进行系统设计步骤，概述构建LLM系统所使用的核心架构和设计选择。请注意，前两个组件主要是与产品相关的，而最后一个则是技术性的，专注于“如何”。
- en: 'These three steps are natural in building a real-world product. Even if the
    first two do not require much ML knowledge, it is critical to go through them
    to understand “how” to build the product with a clear vision. In a nutshell, this
    chapter covers the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个步骤在构建现实世界产品时是自然而然的。即使前两个步骤不需要太多的ML知识，但理解“如何”以清晰的愿景构建产品是至关重要的。简而言之，本章涵盖了以下内容：
- en: Understanding the LLM Twin concept
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解LLM Twin的概念
- en: Planning the MVP of the LLM Twin product
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规划LLM Twin产品的最小可行产品（MVP）
- en: Building ML systems with feature/training/inference pipelines
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用特征/训练/推理管道构建ML系统
- en: Designing the system architecture of the LLM Twin
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计LLM Twin的系统架构
- en: By the end of this chapter, you will have a clear picture of what you will learn
    to build throughout the book.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将清楚地了解在整个书中你将学习如何构建的内容。
- en: Understanding the LLM Twin concept
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解LLM Twin的概念
- en: The first step is to have a clear vision of what we want to create and why it’s
    valuable to build it. The concept of an LLM Twin is new. Thus, before diving into
    the technical details, it is essential to understand what it is, what we should
    expect from it, and how it should work. Having a solid intuition of your end goal
    makes it much easier to digest the theory, code, and infrastructure presented
    in this book.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是明确我们想要创造什么，以及为什么构建它是宝贵的。LLM Twin的概念是新的。因此，在深入技术细节之前，理解它是什么，我们应该期望它做什么，以及它应该如何工作是非常重要的。对最终目标的稳固直觉使得消化本书中提出的理论、代码和基础设施变得更加容易。
- en: What is an LLM Twin?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是LLM Twin？
- en: In a few words, an LLM Twin is an AI character that incorporates your writing
    style, voice, and personality into an LLM, which is a complex AI model. It is
    a digital version of yourself *projected* into an LLM. Instead of a generic LLM
    trained on the whole internet, an LLM Twin is fine-tuned on yourself. Naturally,
    as an ML model reflects the data it is trained on, this LLM will incorporate your
    writing style, voice, and personality. We intentionally used the word “projected.”
    As with any other projection, you lose a lot of information along the way. Thus,
    this LLM will not *be you*; it will copy the side of you reflected in the data
    it was trained on.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，LLM双胞胎是一个将你的写作风格、声音和个性融入LLM（一个复杂的AI模型）的AI角色。这是你自己的数字版本，*投射*到LLM中。与在互联网上训练的通用LLM不同，LLM双胞胎是在你自己的基础上微调的。自然地，作为一个ML模型反映了它所训练的数据，这个LLM将融入你的写作风格、声音和个性。我们故意使用了“投射”这个词。就像任何其他投射一样，你会在过程中丢失大量信息。因此，这个LLM不会*成为你*；它将复制在它所训练的数据中反映出的你的那一面。
- en: It is essential to understand that an LLM reflects the data it was trained on.
    If you feed it Shakespeare, it will start writing like him. If you train it on
    Billie Eilish, it will start writing songs in her style. This is also known as
    style transfer. This concept is prevalent in generating images, too. For example,
    let’s say you want to create a cat image using Van Gogh’s style. We will leverage
    the style transfer strategy, but instead of choosing a personality, we will do
    it on our own persona.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这一点至关重要：LLM反映了它所训练的数据。如果你给它喂莎士比亚的作品，它就会开始像他一样写作。如果你用比莉·艾利什的作品训练它，它就会开始以她的风格写歌。这也被称为风格迁移。这个概念在生成图像中也很普遍。例如，假设你想要用梵高的风格创建一张猫的图片。我们将利用风格迁移策略，但不是选择一个个性，而是基于我们自己的个性来做。
- en: To adjust the LLM to a given style and voice along with fine-tuning, we will
    also leverage various advanced **retrieval-augmented generation** (**RAG**) techniques
    to condition the autoregressive process with previous embeddings of ourselves.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调整LLM以适应给定的风格和声音，并在微调的同时，我们还将利用各种高级**检索增强生成**（**RAG**）技术，以我们自己的先前嵌入来条件化自回归过程。
- en: We will explore the details in *Chapter 5* on fine-tuning and *Chapters 4* and
    *9* on RAG, but for now, let’s look at a few examples to intuitively understand
    what we stated previously.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第5章中探讨微调的细节，在第4章和第9章中探讨RAG，但就目前而言，让我们看看一些例子，直观地理解我们之前所说的。
- en: 'Here are some scenarios of what you can fine-tune an LLM on to become your
    twin:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些场景，说明你可以在LLM上进行微调以成为你的双胞胎：
- en: '**LinkedIn posts and X threads**: Specialize the LLM in writing social media
    content.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LinkedIn帖子和X线程**：将LLM专门化，用于撰写社交媒体内容。'
- en: '**Messages with your friends and family**: Adapt the LLM to an unfiltered version
    of yourself.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与朋友和家人交流的消息**：将LLM调整为你未经过滤的自我版本。'
- en: '**Academic papers and articles**: Calibrate the LLM in writing formal and educative
    content.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学术论文和文章**：校准LLM以撰写正式和有教育意义的内容。'
- en: '**Code**: Specialize the LLM in implementing code as you would.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码**：将LLM专门化，使其以你的方式实现代码。'
- en: 'All the preceding scenarios can be reduced to one core strategy: collecting
    your digital data (or some parts of it) and feeding it to an LLM using different
    algorithms. Ultimately, the LLM reflects the voice and style of the collected
    data. Easy, right?'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的前述场景都可以归结为一个核心策略：收集你的数字数据（或其部分）并使用不同的算法将其喂给LLM。最终，LLM反映了收集到的数据的声调和风格。简单，对吧？
- en: Unfortunately, this raises many technical and moral issues. First, on the technical
    side, how can we access this data? Do we have enough digital data to project ourselves
    into an LLM? What kind of data would be valuable? Secondly, on the moral side,
    is it OK to do this in the first place? Do we want to create a copycat of ourselves?
    Will it write using our voice and personality, or just try to replicate it?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这引发了许多技术和道德问题。首先，在技术方面，我们如何访问这些数据？我们是否有足够的数字数据将自己投射到LLM中？哪种数据是有价值的？其次，在道德方面，我们是否应该首先做这件事？我们是否想要创造一个自己的复制品？它将使用我们的声音和个性写作，还是只是试图复制它？
- en: Remember that the role of this section is not to bother with the “What” and
    “How” but with the “Why.” Let’s understand why it makes sense to have your LLM
    Twin, why it can be valuable, and why it is morally correct if we frame the problem
    correctly.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，本节的作用不是纠结于“是什么”和“怎么做”，而是关注“为什么”。让我们了解为什么拥有你的LLM双胞胎是有意义的，为什么它有价值，以及如果我们正确地界定问题，为什么在道德上是正确的。
- en: Why building an LLM Twin matters
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么建立LLM双胞胎很重要
- en: As an engineer (or any other professional career), building a personal brand
    is more valuable than a standard CV. The biggest issue with creating a personal
    brand is that writing content on platforms such as LinkedIn, X, or Medium takes
    a lot of time. Even if you enjoy writing and creating content, you will eventually
    run out of inspiration or time and feel like you need assistance. We don’t want
    to transform this section into a pitch, but we have to understand the scope of
    this product/project clearly.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名工程师（或任何其他职业），建立个人品牌比标准的简历更有价值。创建个人品牌最大的问题是，在LinkedIn、X或Medium等平台上撰写内容需要花费大量时间。即使你喜欢写作和创作内容，你最终也会耗尽灵感或时间，并感觉需要帮助。我们不希望将这一部分变成一个推销，但我们必须清楚地了解这个产品/项目的范围。
- en: We want to build an LLM Twin to write personalized content on LinkedIn, X, Instagram,
    Substack, and Medium (or other blogs) using our style and voice. It will not be
    used in any immoral scenarios, but it will act as your writing co-pilot. Based
    on what we will teach you in this book, you can get creative and adapt it to various
    use cases, but we will focus on the niche of generating social media content and
    articles. Thus, instead of writing the content from scratch, we can feed the skeleton
    of our main idea to the LLM Twin and let it do the grunt work.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望构建一个LLM Twin，使用我们的风格和声音在LinkedIn、X、Instagram、Substack和Medium（或其他博客）上撰写个性化内容。它不会用于任何不道德的场景，但它将作为你的写作共同飞行员。基于本书中我们将教授你的内容，你可以发挥创意并适应各种用例，但我们将专注于生成社交媒体内容和文章的利基市场。因此，我们不必从头开始撰写内容，我们可以将我们主要想法的框架输入LLM
    Twin，让它做苦力工作。
- en: Ultimately, we will have to check whether everything is correct and format it
    to our liking (more on the concrete features in the *Planning the MVP of the LLM
    Twin product* section). Hence, we project ourselves into a content-writing LLM
    Twin that will help us automate our writing process. It will likely fail if we
    try to use this particular LLM in a different scenario, as this is where we will
    specialize the LLM through fine-tuning, prompt engineering, and RAG.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们必须检查一切是否正确，并按照我们的喜好进行格式化（更多关于具体功能的内容请参阅*规划LLM Twin产品的MVP*部分）。因此，我们设想自己成为一位内容写作的LLM
    Twin，这将帮助我们自动化写作过程。如果我们尝试在不同的场景中使用这个特定的LLM，它很可能会失败，因为这就是我们将通过微调、提示工程和RAG来专门化LLM的地方。
- en: 'So, why does building an LLM Twin matter? It helps you do the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么构建LLM Twin很重要呢？它可以帮助你做到以下几点：
- en: Create your brand
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建你的品牌
- en: Automate the writing process
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化写作过程
- en: Brainstorm new creative ideas
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灵感激发新的创意想法
- en: W**hat’s the difference between a co-pilot and an LLM Twin?**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: W**共同飞行员和LLM Twin之间的区别是什么？**
- en: 'A co-pilot and digital twin are two different concepts that work together and
    can be combined into a powerful solution:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 共同飞行员和数字双胞胎是两个不同的概念，它们可以协同工作并组合成一个强大的解决方案：
- en: The co-pilot is an AI assistant or tool that augments human users in various
    programming, writing, or content creation tasks.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共同飞行员是一个AI助手或工具，它增强了人类用户在编程、写作或内容创作任务中的能力。
- en: The twin serves as a 1:1 digital representation of a real-world entity, often
    using AI to bridge the gap between the physical and digital worlds. For instance,
    an LLM Twin is an LLM that learns to mimic your voice, personality, and writing
    style.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双胞胎是现实世界实体的1:1数字表示，通常使用AI来弥合物理世界和数字世界之间的差距。例如，LLM Twin是一个学习模仿你声音、个性和写作风格的LLM。
- en: With these definitions in mind, a writing and content creation AI assistant
    who writes like you is your LLM Twin co-pilot.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些定义的基础上，一个像你一样写作的内容创作AI助手就是你的LLM Twin共同飞行员。
- en: 'Also, it is critical to understand that building an LLM Twin is entirely moral.
    The LLM will be fine-tuned only on our personal digital data. We won’t collect
    and use other people’s data to try to impersonate anyone’s identity. We have a
    clear goal in mind: creating our personalized writing copycat. Everyone will have
    their own LLM Twin with restricted access.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，理解构建LLM Twin完全是道德的至关重要。LLM将仅在我们个人的数字数据上进行微调。我们不会收集和使用他人的数据来试图模仿任何人的身份。我们有一个明确的目标：创建我们的个性化写作复制品。每个人都将拥有自己的LLM
    Twin，并受到限制的访问权限。
- en: Of course, many security concerns are involved, but we won’t go into that here
    as it could be a book in itself.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，涉及许多安全问题，但在这里我们不会深入探讨，因为这可能是一本自己的书。
- en: Why not use ChatGPT (or another similar chatbot)?
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么不使用ChatGPT（或另一个类似的聊天机器人）？
- en: This subsection will refer to using ChatGPT (or another similar chatbot) just
    in the context of generating personalized content.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节将讨论在生成个性化内容的情况下使用 ChatGPT（或类似聊天机器人）。
- en: 'We have already provided the answer. ChatGPT is not *personalized* to your
    writing style and voice. Instead, it is very generic, unarticulated, and wordy.
    Maintaining an original voice is critical for long-term success when building
    your brand. Thus, directly using ChatGPT or Gemini will not yield the most optimal
    results. Even if you are OK with sharing impersonalized content, mindlessly using
    ChatGPT can result in the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提供了答案。ChatGPT 并没有 *个性化* 到你的写作风格和声音。相反，它非常通用、不连贯且冗长。在建立品牌时保持原创声音对于长期成功至关重要。因此，直接使用
    ChatGPT 或 Gemini 并不会产生最佳结果。即使你愿意分享非个性化的内容，盲目使用 ChatGPT 也可能导致以下情况：
- en: '**Misinformation due to hallucination**: Manually checking the results for
    hallucinations or using third-party tools to evaluate your results is a tedious
    and unproductive experience.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**由于幻觉而产生的不实信息**：手动检查结果以查找幻觉或使用第三方工具评估你的结果是一个繁琐且低效的经历。'
- en: '**Tedious manual prompting**: You must manually craft your prompts and inject
    external information, which is a tiresome experience. Also, the generated answers
    will be hard to replicate between multiple sessions as you don’t have complete
    control over your prompts and injected data. You can solve part of this problem
    using an API and a tool such as LangChain, but you need programming experience
    to do so.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**繁琐的手动提示**：你必须手动制作你的提示并注入外部信息，这是一个令人疲惫的经历。此外，由于你无法完全控制你的提示和注入的数据，生成的答案在多个会话之间难以复制。你可以通过使用
    API 和 LangChain 等工具来解决部分问题，但你需要编程经验才能做到这一点。'
- en: From our experience, if you want high-quality content that provides real value,
    you will spend more time debugging the generated text than writing it yourself.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的经验，如果你想要提供真正价值的高质量内容，你将花费更多的时间调试生成的文本，而不是自己编写。
- en: 'The key of the LLM Twin stands in the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: LLM Twin 的关键在于以下方面：
- en: What data we collect
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们收集哪些数据
- en: How we preprocess the data
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何预处理数据
- en: How we feed the data into the LLM
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何将数据输入到 LLM 中
- en: How we chain multiple prompts for the desired results
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何链式多个提示以获得期望的结果
- en: How we evaluate the generated content
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何评估生成的内容
- en: 'The LLM itself is important, but we want to highlight that using ChatGPT’s
    web interface is exceptionally tedious in managing and injecting various data
    sources or evaluating the outputs. The solution is to build an LLM system that
    encapsulates and automates all the following steps (manually replicating them
    each time is not a long-term and feasible solution):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 本身很重要，但我们想强调的是，使用 ChatGPT 的网页界面在管理和注入各种数据源或评估输出方面非常繁琐。解决方案是构建一个 LLM 系统，该系统封装并自动化以下所有步骤（每次手动复制并不是一个长期且可行的解决方案）：
- en: Data collection
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据收集
- en: Data preprocessing
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Data storage, versioning, and retrieval
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据存储、版本控制和检索
- en: LLM fine-tuning
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 微调
- en: RAG
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG
- en: Content generation evaluation
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容生成评估
- en: Note that we never said not to use OpenAI’s GPT API, just that the LLM framework
    we will present is LLM-agnostic. Thus, if it can be manipulated programmatically
    and exposes a fine-tuning interface, it can be integrated into the LLM Twin system
    we will learn to build. The key to most successful ML products is to be data-centric
    and make your architecture model-agnostic. Thus, you can quickly experiment with
    multiple models on your specific data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们从未说过不要使用 OpenAI 的 GPT API，只是我们将要介绍的 LLM 框架是 LLM 无关的。因此，如果它可以被程序化操作并暴露微调接口，它就可以集成到我们将学习构建的
    LLM Twin 系统中。大多数成功的 ML 产品关键在于以数据为中心，并使你的架构模型无关。因此，你可以快速在你的特定数据上对多个模型进行实验。
- en: Planning the MVP of the LLM Twin product
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规划 LLM Twin 产品的 MVP
- en: Now that we understand what an LLM Twin is and why we want to build it, we must
    clearly define the product’s features. In this book, we will focus on the first
    iteration, often labeled the **minimum viable product** (**MVP**), to follow the
    natural cycle of most products. Here, the main objective is to align our ideas
    with realistic and doable business objectives using the available resources to
    produce the product. Even as an engineer, as you grow up in responsibilities,
    you must go through these steps to bridge the gap between the business needs and
    what can be implemented.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了 LLM Twin 是什么以及为什么我们要构建它，我们必须明确定义产品的功能。在这本书中，我们将关注第一代产品，通常被称为**最小可行产品**（**MVP**），以遵循大多数产品的自然周期。在这里，主要目标是使用可用的资源将我们的想法与实际可行的商业目标对齐，以生产产品。即使作为一个工程师，随着你责任的增加，你也必须经历这些步骤，以弥合商业需求与可实现实施之间的差距。
- en: What is an MVP?
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 MVP？
- en: An MVP is a version of a product that includes just enough features to draw
    in early users and test the viability of the product concept in the initial stages
    of development. Usually, the purpose of the MVP is to gather insights from the
    market with minimal effort.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: MVP 是产品的一个版本，它包含足够的功能来吸引早期用户，并在开发的初始阶段测试产品概念的可行性。通常，MVP 的目的是以最小的努力从市场收集见解。
- en: 'An MVP is a powerful strategy because of the following reasons:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: MVP 是一种强大的策略，原因如下：
- en: '**Accelerated time-to-market**: Launch a product quickly to gain early traction'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加速上市时间**：快速推出产品以获得早期吸引力'
- en: '**Idea validation**: Test it with real users before investing in the full development
    of the product'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理念验证**：在投入产品全面开发之前，用真实用户测试它'
- en: '**Market research**: Gain insights into what resonates with the target audience'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**市场研究**：深入了解哪些内容与目标受众产生共鸣'
- en: '**Risk minimization**: Reduces the time and resources needed for a product
    that might not achieve market success'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**风险最小化**：减少可能不会取得市场成功的产品所需的时间和资源'
- en: Sticking to the *V* in MVP is essential, meaning the product must be *viable*.
    The product must provide an end-to-end user journey without half-implemented features,
    even if the product is minimal. It must be a working product with a good user
    experience that people will love and want to keep using to see how it evolves
    to its full potential.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 坚持 MVP 中的 *V* 是至关重要的，这意味着产品必须是**可行的**。产品必须提供一个端到端的用户旅程，没有半成品的功能，即使产品很简陋。它必须是一个具有良好用户体验的运行产品，人们会喜欢并希望继续使用它，以查看它如何发展到其全部潜力。
- en: Defining the LLM Twin MVP
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义 LLM Twin MVP
- en: 'As a thought experiment, let’s assume that instead of building this project
    for this book, we want to make a real product. In that case, what are our resources?
    Well, unfortunately, not many:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种思想实验，让我们假设我们不是为这本书构建这个项目，而是想制作一个真正的产品。在这种情况下，我们的资源是什么？不幸的是，并不多：
- en: We are a team of three people with two ML engineers and one ML researcher
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是一个由三个人组成的团队，其中有两个机器学习工程师和一个机器学习研究员
- en: Our laptops
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的笔记本电脑
- en: Personal funding for computing, such as training LLMs
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于计算的个人资金，例如训练 LLM
- en: Our enthusiasm
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的热情
- en: 'As you can see, we don’t have many resources. Even if this is just a thought
    experiment, it reflects the reality for most start-ups at the beginning of their
    journey. Thus, we must be very strategic in defining our LLM Twin MVP and what
    features we want to pick. Our goal is simple: we want to maximize the product’s
    value relative to the effort and resources poured into it.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，我们资源有限。即使这只是一个思想实验，它也反映了大多数初创公司在创业初期的现实情况。因此，我们必须在定义我们的 LLM Twin MVP 以及我们想要选择的功能时非常策略。我们的目标很简单：我们希望最大化产品的价值，相对于投入的努力和资源。
- en: 'To keep it simple, we will build the features that can do the following for
    the LLM Twin:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持简单，我们将构建以下功能，以供 LLM Twin 使用：
- en: Collect data from your LinkedIn, Medium, Substack, and GitHub profiles
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从你的 LinkedIn、Medium、Substack 和 GitHub 个人资料收集数据
- en: Fine-tune an open-source LLM using the collected data
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用收集到的数据微调开源 LLM
- en: Populate a vector database (DB) using our digital data for RAG
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用我们的数字数据为 RAG 填充向量数据库（DB）
- en: 'Create LinkedIn posts leveraging the following:'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用以下内容创建 LinkedIn 帖子：
- en: User prompts
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户提示
- en: RAG to reuse and reference old content
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG 以重用和引用旧内容
- en: New posts, articles, or papers as additional knowledge to the LLM
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 LLM 提供新帖子、文章或论文作为额外的知识
- en: 'Have a simple web interface to interact with the LLM Twin and be able to do
    the following:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有一个简单的网页界面来与 LLM Twin 交互，并能够执行以下操作：
- en: Configure your social media links and trigger the collection step
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置你的社交媒体链接并触发收集步骤
- en: Send prompts or links to external resources
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发送提示或链接到外部资源
- en: That will be the LLM Twin MVP. Even if it doesn’t sound like much, remember
    that we must make this system cost effective, scalable, and modular.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是LLM Twin MVP。即使它听起来不多，记住我们必须使这个系统具有成本效益、可扩展性和模块化。
- en: Even if we focus only on the core features of the LLM Twin defined in this section,
    we will build the product with the latest LLM research and best software engineering
    and MLOps practices in mind. We aim to show you how to engineer a cost-effective
    and scalable LLM application.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 即使只关注本节中定义的LLM Twin的核心功能，我们也会考虑到最新的LLM研究和最佳软件工程以及MLOps实践来构建产品。我们的目标是向您展示如何构建一个成本效益高且可扩展的LLM应用程序。
- en: Until now, we have examined the LLM Twin from the users’ and businesses’ perspectives.
    The last step is to examine it from an engineering perspective and define a development
    plan to understand how to solve it technically. From now on, the book’s focus
    will be on the implementation of the LLM Twin.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经从用户和企业的角度审视了LLM Twin。最后一步是从工程角度审视它，并制定一个开发计划来了解如何从技术上解决它。从现在起，本书的重点将转向LLM
    Twin的实现。
- en: Building ML systems with feature/training/inference pipelines
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用特征/训练/推理管道构建ML系统
- en: Before diving into the specifics of the LLM Twin architecture, we must understand
    an ML system pattern at the core of the architecture, known as the **feature/training/inference**
    (**FTI**) architecture. This section will present a general overview of the FTI
    pipeline design and how it can structure an ML application.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解LLM Twin架构的具体细节之前，我们必须了解架构核心的一个ML系统模式，称为**特征/训练/推理**（**FTI**）架构。本节将介绍FTI管道设计的一般概述以及它如何结构化ML应用程序。
- en: Let’s see how we can apply the FTI pipelines to the LLM Twin architecture.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何将FTI管道应用于LLM Twin架构。
- en: The problem with building ML systems
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建ML系统的问题
- en: Building production-ready ML systems is much more than just training a model.
    From an engineering point of view, training the model is the most straightforward
    step in most use cases. However, training a model becomes complex when deciding
    on the correct architecture and hyperparameters. That’s not an engineering problem
    but a research problem.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 构建生产就绪的ML系统远不止训练一个模型。从工程角度来看，在大多数用例中，训练模型是最直接的一步。然而，当决定正确的架构和超参数时，训练模型变得复杂。这不仅仅是一个工程问题，而是一个研究问题。
- en: 'At this point, we want to focus on how to design a production-ready architecture.
    Training a model with high accuracy is extremely valuable, but just by training
    it on a static dataset, you are far from deploying it robustly. We have to consider
    how to do the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们想要专注于如何设计一个生产就绪的架构。训练一个高精度的模型非常有价值，但仅仅在静态数据集上训练，你离稳健部署还远。我们必须考虑如何做到以下：
- en: Ingest, clean, and validate fresh data
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摄入、清洗和验证新鲜数据
- en: Training versus inference setups
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练与推理设置
- en: Compute and serve features in the right environment
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在正确环境中计算和提供特征
- en: Serve the model in a cost-effective way
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以成本效益的方式提供服务模型
- en: Version, track, and share the datasets and models
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本、跟踪和共享数据集和模型
- en: Monitor your infrastructure and models
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控你的基础设施和模型
- en: Deploy the model on a scalable infrastructure
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在可扩展的基础设施上部署模型
- en: Automate the deployments and training
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动部署和训练
- en: These are the types of problems an ML or MLOps engineer must consider, while
    the research or data science team is often responsible for training the model.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是机器学习或MLOps工程师必须考虑的问题，而研究或数据科学团队通常负责训练模型。
- en: '![](img/B31105_01_01.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_01_01.png)'
- en: 'Figure 1.1: Common elements from an ML system'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1：ML系统中的常见元素
- en: The preceding figure shows all the components the Google Cloud team suggests
    that a mature ML and MLOps system requires. Along with the ML code, there are
    many moving pieces. The rest of the system comprises configuration, automation,
    data collection, data verification, testing and debugging, resource management,
    model analysis, process and metadata management, serving infrastructure, and monitoring.
    The point is that there are many components we must consider when productionizing
    an ML model.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图示展示了谷歌云团队建议的成熟机器学习和MLOps系统所需的所有组件。除了机器学习代码，还有很多动态部分。系统的其余部分包括配置、自动化、数据收集、数据验证、测试和调试、资源管理、模型分析、流程和元数据管理、服务基础设施和监控。重点是，在生产化机器学习模型时，我们必须考虑许多组件。
- en: 'Thus, the critical question is this: How do we connect all these components
    into a single homogenous system? We must create a boilerplate for clearly designing
    ML systems to answer that question.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，关键问题是这样的：我们如何将这些组件连接成一个单一的统一系统？我们必须为清晰地设计机器学习系统创建一个模板来回答这个问题。
- en: Similar solutions exist for classic software. For example, if you zoom out,
    most software applications can be split between a DB, business logic, and UI layer.
    Every layer can be as complex as needed, but at a high-level overview, the architecture
    of standard software can be boiled down to the previous three components.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于经典软件，也存在类似的解决方案。例如，如果你从宏观角度观察，大多数软件应用程序都可以分为数据库、业务逻辑和UI层。每一层都可以根据需要变得复杂，但从高层次概述来看，标准软件的架构可以简化为前三个组件。
- en: Do we have something similar for ML applications? The first step is to examine
    previous solutions and why they are unsuitable for building scalable ML systems.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对于机器学习应用有类似的东西吗？第一步是检查以前的解决方案以及为什么它们不适合构建可扩展的机器学习系统。
- en: The issue with previous solutions
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 以前解决方案的问题
- en: 'In *Figure 1.2*, you can observe the typical architecture present in most ML
    applications. It is based on a monolithic batch architecture that couples the
    feature creation, model training, and inference into the same component. By taking
    this approach, you quickly solve one critical problem in the ML world: the training-serving
    skew. The training-serving skew happens when the features passed to the model
    are computed differently at training and inference time.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图1.2*中，你可以观察到大多数机器学习应用中典型的架构。它基于单体批量架构，将特征创建、模型训练和推理耦合到同一组件中。通过采取这种方法，你迅速解决了机器学习世界中的一个关键问题：训练-服务偏差。训练-服务偏差发生在传递给模型的特征在训练和推理时计算不同时。
- en: In this architecture, the features are created using the same code. Hence, the
    training-serving skew issue is solved by default. This pattern works fine when
    working with small data. The pipeline runs on a schedule in batch mode, and the
    predictions are consumed by a third-party application such as a dashboard.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个架构中，特征是通过相同的代码创建的。因此，默认情况下解决了训练-服务偏差问题。当处理小数据时，这种模式运行良好。管道以批量模式按计划运行，预测由第三方应用程序（如仪表板）消费。
- en: '![](img/B31105_01_02.png)Figure 1.2: Monolithic batch pipeline architecture'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.2：单体批量管道架构](img/B31105_01_02.png)'
- en: 'Unfortunately, building a monolithic batch system raises many other issues,
    such as the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，构建单体批量系统会引发许多其他问题，例如以下：
- en: Features are not reusable (by your system or others)
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征不可重用（由您的系统或其他人）
- en: If the data increases, you have to refactor the whole code to support PySpark
    or Ray
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据增加，你必须重构整个代码以支持PySpark或Ray
- en: It’s hard to rewrite the prediction module in a more efficient language such
    as C++, Java, or Rust
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新编写预测模块以更高效的编程语言（如C++、Java或Rust）是很困难的
- en: It’s hard to share the work between multiple teams between the features, training,
    and prediction modules
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在特征、训练和预测模块之间共享工作很困难
- en: It’s impossible to switch to streaming technology for real-time training
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于实时训练来说，切换到流式技术是不可能的
- en: In *Figure 1.3*, we can see a similar scenario for a real-time system. This
    use case introduces another issue in addition to what we listed before. To make
    the predictions, we have to transfer the whole state through the client request
    so the features can be computed and passed to the model.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图1.3*中，我们可以看到一个实时系统的类似场景。这个用例除了我们之前列出的之外，还引入了另一个问题。为了做出预测，我们必须通过客户端请求传输整个状态，以便计算特征并将其传递给模型。
- en: Consider the scenario of computing movie recommendations for a user. Instead
    of simply passing the user ID, we must transmit the entire user state, including
    their name, age, gender, movie history, and more. This approach is fraught with
    potential errors, as the client must understand how to access this state, and
    it’s tightly coupled with the model service.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑为用户计算电影推荐场景。我们不仅要传递用户ID，还必须传输整个用户状态，包括他们的姓名、年龄、性别、电影历史等。这种方法充满了潜在的错误，因为客户端必须了解如何访问这个状态，并且它与模型服务紧密耦合。
- en: Another example would be when implementing an LLM with RAG support. The documents
    we add as context along the query represent our external state. If we didn’t store
    the records in a vector DB, we would have to pass them with the user query. To
    do so, the client must know how to query and retrieve the documents, which is
    not feasible. It is an antipattern for the client application to know how to access
    or compute the features. If you don’t understand how RAG works, we will explain
    it in detail in *Chapters 8* and *9*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是在实现具有RAG支持的LLM时。我们添加到查询中的文档作为上下文代表我们的外部状态。如果我们没有在向量数据库中存储记录，我们就必须与用户查询一起传递它们。为了做到这一点，客户端必须知道如何查询和检索文档，这是不可行的。客户端应用程序知道如何访问或计算特征是一种反模式。如果您不了解RAG的工作原理，我们将在*第8章*和*第9章*中详细解释。
- en: '![](img/B31105_01_03.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_01_03.png)'
- en: 'Figure 1.3: Stateless real-time architecture'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：无状态实时架构
- en: In conclusion, our problem is accessing the features to make predictions without
    passing them at the client’s request. For example, based on our first user movie
    recommendation example, how can we predict the recommendations solely based on
    the user’s ID? Remember these questions, as we will answer them shortly.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们的问题是访问特征以进行预测，而不需要根据客户端的请求传递它们。例如，基于我们的第一个用户电影推荐示例，我们如何仅根据用户的ID来预测推荐？请记住这些问题，因为我们将在不久后回答它们。
- en: Ultimately, on the other spectrum, Google Cloud provides a production-ready
    architecture, as shown in *Figure 1.4*. Unfortunately, even if it’s a feasible
    solution, it’s very complex and not intuitive. You will have difficulty understanding
    this if you are not highly experienced in deploying and keeping ML models in production.
    Also, it is not straightforward to understand how to start small and grow the
    system in time.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，在另一端，Google Cloud提供了一个生产就绪的架构，如*图1.4*所示。不幸的是，即使这是一个可行的解决方案，它也非常复杂且不直观。如果您在部署和保持机器学习模型在生产中的经验不是很丰富，您将难以理解这一点。此外，了解如何从小处开始并在一段时间内扩展系统也不是那么简单。
- en: 'The following image is reproduced from work created and shared by Google and
    used according to terms described in the Creative Commons 4.0 Attribution License:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像是从Google创建和共享的工作中复制的，并按照Creative Commons 4.0署名许可协议的条款使用：
- en: '![](img/B31105_01_04.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_01_04.png)'
- en: 'Figure 1.4: ML pipeline automation for CT (source: https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4：CT的机器学习管道自动化（来源：https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning）
- en: But here is where the FTI pipeline architectures kick in. The following section
    will show you how to solve these fundamental issues using an intuitive ML design.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 但在这里，FTI管道架构开始发挥作用。接下来的部分将向您展示如何使用直观的机器学习设计来解决这些基本问题。
- en: The solution – ML pipelines for ML systems
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案——机器学习系统的机器学习管道
- en: The solution is based on creating a clear and straightforward mind map that
    any team or person can follow to compute the features, train the model, and make
    predictions. Based on these three critical steps that any ML system requires,
    the pattern is known as the FTI pipeline. So, how does this differ from what we
    presented before?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是基于创建一个清晰且直接的思维导图，任何团队或个人都可以遵循它来计算特征、训练模型和进行预测。基于任何机器学习系统所需的这三个关键步骤，这种模式被称为FTI管道。那么，这与我们之前展示的内容有何不同？
- en: 'The pattern suggests that any ML system can be boiled down to these three pipelines:
    feature, training, and inference (similar to the DB, business logic, and UI layers
    from classic software). This is powerful, as we can clearly define the scope and
    interface of each pipeline. Also, it’s easier to understand how the three components
    interact. Ultimately, we have just three instead of 20 moving pieces, as suggested
    in *Figure 1.4*, which is much easier to work with and define.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式表明，任何机器学习系统都可以简化为这三个管道：特征、训练和推理（类似于经典软件中的数据库、业务逻辑和UI层）。这很强大，因为我们可以清楚地定义每个管道的范围和接口。此外，理解这三个组件如何交互也更容易。最终，我们只有三个而不是20个移动部件，正如*图1.4*所示，这更容易处理和定义。
- en: As shown in *Figure 1.5*, we have the feature, training, and inference pipelines.
    We will zoom in on each of them and understand their scope and interface.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图1.5*所示，我们有特征、训练和推理管道。我们将逐一深入探讨它们，了解它们的范围和接口。
- en: '![](img/B31105_01_05.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_01_05.png)'
- en: 'Figure 1.5: FTI pipelines architecture'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5：FTI管道架构
- en: Before going into the details, it is essential to understand that each pipeline
    is a different component that can run on a different process or hardware. Thus,
    each pipeline can be written using a different technology, by a different team,
    or scaled differently. The key idea is that the design is very flexible to the
    needs of your team. It acts as a mind map for structuring your architecture.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解之前，重要的是要理解每个管道都是一个不同的组件，可以在不同的进程或硬件上运行。因此，每个管道可以使用不同的技术编写，由不同的团队完成，或以不同的方式进行扩展。关键思想是设计非常灵活，以满足你团队的需求。它充当了构建你架构的思维导图。
- en: The feature pipeline
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征管道
- en: The feature pipeline takes raw data as input, processes it, and outputs the
    features and labels required by the model for training or inference. Instead of
    directly passing them to the model, the features and labels are stored inside
    a feature store. Its responsibility is to store, version, track, and share the
    features. By saving the features in a feature store, we always have a state of
    our features. Thus, we can easily send the features to the training and inference
    pipelines.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 特征管道以原始数据作为输入，对其进行处理，并输出模型训练或推理所需的特征和标签。而不是直接将它们传递给模型，特征和标签存储在特征存储中。其责任是存储、版本化、跟踪和共享特征。通过在特征存储中保存特征，我们始终有特征的状态。因此，我们可以轻松地将特征发送到训练和推理管道。
- en: As the data is versioned, we can always ensure that the training and inference
    time features match. Thus, we avoid the training-serving skew problem.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据有版本号，我们始终可以确保训练和推理时间特征匹配。因此，我们避免了训练-服务偏差问题。
- en: The training pipeline
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练管道
- en: The training pipeline takes the features and labels from the features stored
    as input and outputs a train model or models. The models are stored in a model
    registry. Its role is similar to that of feature stores, but this time, the model
    is the first-class citizen. Thus, the model registry will store, version, track,
    and share the model with the inference pipeline.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 训练管道从作为输入的特征存储中的特征和标签中获取，并输出一个或多个训练模型。这些模型存储在模型注册表中。其作用类似于特征存储，但这次模型是第一类公民。因此，模型注册表将存储、版本化、跟踪并与推理管道共享模型。
- en: Also, most modern model registries support a metadata store that allows you
    to specify essential aspects of how the model was trained. The most important
    are the features, labels, and their version used to train the model. Thus, we
    will always know what data the model was trained on.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，大多数现代模型注册表支持元数据存储，允许你指定模型训练的必要方面。最重要的是特征、标签以及用于训练模型的版本。因此，我们始终知道模型是在什么数据上训练的。
- en: The inference pipeline
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理管道
- en: The inference pipeline takes as input the features and labels from the feature
    store and the trained model from the model registry. With these two, predictions
    can be easily made in either batch or real-time mode.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 推理管道以特征存储中的特征和标签以及模型注册表中的训练模型作为输入。有了这两个，可以轻松地在批量或实时模式下进行预测。
- en: As this is a versatile pattern, it is up to you to decide what you do with your
    predictions. If it’s a batch system, they will probably be stored in a DB. If
    it’s a real-time system, the predictions will be served to the client who requested
    them. Additionally, the features, labels, and models are versioned. We can easily
    upgrade or roll back the deployment of the model. For example, we will always
    know that model v1 uses features F1, F2, and F3, and model v2 uses F2, F3, and
    F4\. Thus, we can quickly change the connections between the model and features.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个通用的模式，如何处理你的预测取决于你。如果是批量系统，它们可能会存储在数据库中。如果是实时系统，预测将提供给请求它们的客户端。此外，特征、标签和模型都有版本号。我们可以轻松升级或回滚模型的部署。例如，我们始终知道模型v1使用特征F1、F2和F3，而模型v2使用F2、F3和F4。因此，我们可以快速更改模型与特征之间的连接。
- en: Benefits of the FTI architecture
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FTI架构的好处
- en: 'To conclude, the most important thing you must remember about the FTI pipelines
    is their interface:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，你必须记住关于FTI管道最重要的东西是它们的接口：
- en: The feature pipeline takes in data and outputs the features and labels saved
    to the feature store.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征管道接收数据并输出保存到特征存储中的特征和标签。
- en: The training pipeline queries the features store for features and labels and
    outputs a model to the model registry.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练管道查询特征存储以获取特征和标签，并将模型输出到模型注册表。
- en: The inference pipeline uses the features from the feature store and the model
    from the model registry to make predictions.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理管道使用特征存储中的特征和模型注册表中的模型进行预测。
- en: It doesn’t matter how complex your ML system gets, these interfaces will remain
    the same.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你的ML系统多么复杂，这些接口都将保持不变。
- en: 'Now that we understand better how the pattern works, we want to highlight the
    main benefits of using this pattern:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们更好地理解了这种模式的工作原理，我们想强调使用这种模式的主要好处：
- en: As you have just three components, it is intuitive to use and easy to understand.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于你只有三个组件，使用起来直观且易于理解。
- en: Each component can be written into its tech stack, so we can quickly adapt them
    to specific needs, such as big or streaming data. Also, it allows us to pick the
    best tools for the job.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个组件都可以写入其技术栈中，这样我们可以快速适应特定的需求，例如大数据或流数据。这也允许我们选择最适合工作的工具。
- en: As there is a transparent interface between the three components, each one can
    be developed by a different team (if necessary), making the development more manageable
    and scalable.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于三个组件之间存在透明的接口，因此每个组件可以由不同的团队（如果需要）开发，这使得开发更加可管理和可扩展。
- en: Every component can be deployed, scaled, and monitored independently.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个组件都可以独立部署、扩展和监控。
- en: The final thing you must understand about the FTI pattern is that the system
    doesn’t have to contain only three pipelines. In most cases, it will include more.
    For example, the feature pipeline can be composed of a service that computes the
    features and one that validates the data. Also, the training pipeline can be composed
    of the training and evaluation components.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 关于FTI模式，你必须理解的最后一点是，系统不必只包含三个管道。在大多数情况下，它将包含更多。例如，特征管道可以由一个计算特征的服务和验证数据的另一个服务组成。同样，训练管道可以由训练和评估组件组成。
- en: The FTI pipelines act as logical layers. Thus, it is perfectly fine for each
    to be complex and contain multiple services. However, what is essential is to
    stick to the same interface on how the FTI pipelines interact with each other
    through the feature store and model registries. By doing so, each FTI component
    can evolve differently, without knowing the details of each other and without
    breaking the system on new changes.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: FTI管道作为逻辑层。因此，每个管道都复杂且包含多个服务是完全可以接受的。然而，最重要的是坚持FTI管道通过特征存储和模型注册表相互交互的相同接口。通过这样做，每个FTI组件可以独立进化，无需了解彼此的细节，也不会因新的变化而破坏系统。
- en: 'To learn more about the FTI pipeline pattern, consider reading *From MLOps
    to ML Systems with Feature/Training/Inference Pipelines* by Jim Dowling, CEO and
    co-founder of Hopsworks: [https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines).
    His article inspired this section.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于FTI管道模式的信息，请考虑阅读Jim Dowling（Hopsworks的首席执行官和联合创始人）所著的《从MLOps到ML系统：使用特征/训练/推理管道》一书：[https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines)。他的文章启发了本节。
- en: Now that we understand the FTI pipeline architecture, the final step of this
    chapter is to see how it can be applied to the LLM Twin use case.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了FTI管道架构，本章的最后一步是看看它如何应用于LLM Twin用例。
- en: Designing the system architecture of the LLM Twin
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计LLM Twin的系统架构
- en: In this section, we will list the concrete technical details of the LLM Twin
    application and understand how we can solve them by designing our LLM system using
    the FTI architecture. However, before diving into the pipelines, we want to highlight
    that we won’t focus on the tooling or the tech stack at this step. We only want
    to define a high-level architecture of the system, which is language-, framework-,
    platform-, and infrastructure-agnostic at this point. We will focus on each component’s
    scope, interface, and interconnectivity. In future chapters, we will cover the
    implementation details and tech stack.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将列出LLM Twin应用的详细技术细节，并了解我们如何通过使用FTI架构设计我们的LLM系统来解决这些问题。然而，在深入管道之前，我们想强调，我们在这个阶段不会关注工具或技术栈。我们只想定义一个系统的高级架构，在这个阶段它是语言无关、框架无关、平台无关和基础设施无关的。我们将关注每个组件的范围、接口和互连性。在未来的章节中，我们将介绍实现细节和技术栈。
- en: Listing the technical details of the LLM Twin architecture
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列出LLM Twin架构的技术细节
- en: 'Until now, we defined what the LLM Twin should support from the user’s point
    of view. Now, let’s clarify the requirements of the ML system from a purely technical
    perspective:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经从用户的角度定义了LLM Twin应该支持的内容。现在，让我们从纯粹的技术角度明确ML系统的要求：
- en: 'On the data side, we have to do the following:'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据方面，我们必须做以下事情：
- en: Collect data from LinkedIn, Medium, Substack, and GitHub completely autonomously
    and on a schedule
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全自主地从LinkedIn、Medium、Substack和GitHub收集数据，并按照计划进行
- en: Standardize the crawled data and store it in a data warehouse
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化爬取的数据并将其存储在数据仓库中
- en: Clean the raw data
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清洗原始数据
- en: Create instruct datasets for fine-tuning an LLM
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建用于微调LLM的指令数据集
- en: Chunk and embed the cleaned data. Store the vectorized data into a vector DB
    for RAG.
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将清洗后的数据分块并嵌入。将向量化的数据存储到向量数据库中，以供RAG使用。
- en: 'For training, we have to do the following:'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于训练，我们必须做以下事情：
- en: Fine-tune LLMs of various sizes (7B, 14B, 30B, or 70B parameters)
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调各种大小（7B、14B、30B或70B参数）的LLM
- en: Fine-tune on instruction datasets of multiple sizes
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个大小的指令数据集上进行微调
- en: Switch between LLM types (for example, between Mistral, Llama, and GPT)
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在LLM类型之间切换（例如，在Mistral、Llama和GPT之间）
- en: Track and compare experiments
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪和比较实验
- en: Test potential production LLM candidates before deploying them
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在部署之前测试潜在的LLM（大型语言模型）生产候选者
- en: Automatically start the training when new instruction datasets are available.
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当有新的指令数据集可用时，自动启动训练。
- en: 'The inference code will have the following properties:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理代码将具有以下特性：
- en: A REST API interface for clients to interact with the LLM Twin
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为客户端与LLM双提供REST API接口
- en: Access to the vector DB in real time for RAG
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时访问向量数据库以进行RAG（阅读-询问-生成）
- en: Inference with LLMs of various sizes
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用各种大小的LLM进行推理
- en: Autoscaling based on user requests
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据用户请求进行自动扩展
- en: Automatically deploy the LLMs that pass the evaluation step.
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动部署通过评估步骤的LLM
- en: 'The system will support the following LLMOps features:'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统将支持以下LLMOps功能：
- en: Instruction dataset versioning, lineage, and reusability
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指令数据集版本控制、血缘关系和可重用性
- en: Model versioning, lineage, and reusability
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型版本控制、血缘关系和可重用性
- en: Experiment tracking
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验跟踪
- en: '**Continuous training**, **continuous integration**, and **continuous delivery**
    (CT/**CI**/**CD**)'
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续训练**、**持续集成**和**持续交付**（CT/**CI**/**CD**）'
- en: Prompt and system monitoring
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示和系统监控
- en: If any technical requirement doesn’t make sense now, bear with us. To avoid
    repetition, we will examine the details in their specific chapter.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任何技术要求现在听起来没有意义，请耐心等待。为了避免重复，我们将在它们各自的章节中详细探讨。
- en: 'The preceding list is quite comprehensive. We could have detailed it even more,
    but at this point, we want to focus on the core functionality. When implementing
    each component, we will look into all the little details. But for now, the fundamental
    question we must ask ourselves is this: How can we apply the FTI pipeline design
    to implement the preceding list of requirements?'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 上述列表相当全面。我们本可以更详细地说明，但在此阶段，我们想专注于核心功能。在实现每个组件时，我们将关注所有细节。但就目前而言，我们必须问自己这样一个基本问题：我们如何将FTI管道设计应用于实现上述要求列表？
- en: How to design the LLM Twin architecture using the FTI pipeline design
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用FTI管道设计来设计LLM双架构
- en: 'We will split the system into four core components. You will ask yourself this:
    “Four? Why not three, as the FTI pipeline design clearly states?” That is a great
    question. Fortunately, the answer is simple. We must also implement the data pipeline
    along the three feature/training/inference pipelines. According to best practices:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将系统分为四个核心组件。你会问自己这个问题：“四个？为什么不按FTI管道设计明确指出的那样是三个？”这是一个很好的问题。幸运的是，答案很简单。我们必须在三个特征/训练/推理管道中实现数据管道。根据最佳实践：
- en: The data engineering team owns the data pipeline
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据工程团队负责数据管道
- en: The ML engineering team owns the FTI pipelines.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习工程团队负责FTI管道。
- en: Given our goal of building an MVP with a small team, we must implement the entire
    application. This includes defining the data collection and FTI pipelines. Tackling
    a problem end to end is often encountered in start-ups that can’t afford dedicated
    teams. Thus, engineers have to wear many hats, depending on the state of the product.
    Nevertheless, in any scenario, knowing how an end-to-end ML system works is valuable
    for better understanding other people’s work.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们的目标是使用小团队构建一个MVP（最小可行产品），我们必须实现整个应用程序。这包括定义数据收集和FTI（特征/训练/推理）管道。从头到尾解决问题在无法承担专用团队的初创公司中很常见。因此，工程师必须根据产品的状态扮演多个角色。尽管如此，在任何情况下，了解端到端机器学习系统的工作原理对于更好地理解他人的工作都是非常有价值的。
- en: '*Figure 1.6* shows the LLM system architecture. The best way to understand
    it is to review the four components individually and explain how they work.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1.6*显示了LLM系统架构。理解它的最好方法是分别审查四个组件，并解释它们是如何工作的。'
- en: '![](img/B31105_01_06.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_01_06.png)'
- en: 'Figure 1.6: LLM Twin high-level architecture'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6：LLM Twin高级架构
- en: Data collection pipeline
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据收集管道
- en: The data collection pipeline involves crawling your personal data from Medium,
    Substack, LinkedIn, and GitHub. As a data pipeline, we will use the **extract,
    load, transform** (**ETL**) pattern to extract data from social media platforms,
    standardize it, and load it into a data warehouse.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集管道涉及从Medium、Substack、LinkedIn和GitHub爬取您的个人数据。作为一个数据管道，我们将使用**提取、加载、转换**（**ETL**）模式从社交媒体平台提取数据，对其进行标准化，并将其加载到数据仓库中。
- en: It is critical to highlight that the data collection pipeline is designed to
    crawl data only from your social media platform. It will not have access to other
    people. As an example for this book, we agreed to make our collected data available
    for learning purposes. Otherwise, using other people’s data without their consent
    is not moral.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 强调这一点至关重要，即数据收集管道仅设计用于从您的社交媒体平台爬取数据。它将无法访问其他人。作为本书的示例，我们同意将我们的收集数据用于学习目的。否则，未经他人同意使用他人的数据是不道德的。
- en: The output of this component will be a NoSQL DB, which will act as our data
    warehouse. As we work with text data, which is naturally unstructured, a NoSQL
    DB fits like a glove.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 此组件的输出将是一个NoSQL数据库，它将充当我们的数据仓库。由于我们处理的是自然无结构的文本数据，因此NoSQL数据库非常适合。
- en: Even though a NoSQL DB, such as MongoDB, is not labeled as a data warehouse,
    from our point of view, it will act as one. Why? Because it stores standardized
    raw data gathered by various ETL pipelines that are ready to be ingested into
    an ML system.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管NoSQL数据库，如MongoDB，没有被标记为数据仓库，但根据我们的观点，它将充当数据仓库。为什么？因为它存储了由各种ETL管道收集的标准化原始数据，这些数据已准备好被摄入到ML系统中。
- en: 'The collected digital data is binned into three categories:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 收集的数字数据被分为三个类别：
- en: Articles (Medium, Substack)
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文章（Medium、Substack）
- en: Posts (LinkedIn)
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帖子（LinkedIn）
- en: Code (GitHub)
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码（GitHub）
- en: We want to abstract away the platform where the data was crawled. For example,
    when feeding an article to the LLM, knowing it came from Medium or Substack is
    not essential. We can keep the source URL as metadata to give references. However,
    from the processing, fine-tuning, and RAG points of view, it is vital to know
    what type of data we ingested, as each category must be processed differently.
    For example, the chunking strategy between a post, article, and piece of code
    will look different.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望抽象出数据被爬取的平台。例如，当向LLM提供文章时，知道它来自Medium或Substack并不是必要的。我们可以保留源URL作为元数据以提供参考。然而，从处理、微调和RAG的角度来看，了解我们摄入的数据类型至关重要，因为每个类别都必须以不同的方式处理。例如，帖子、文章和代码之间的分块策略将看起来不同。
- en: Also, by grouping the data by category, not the source, we can quickly plug
    data from other platforms, such as X into the posts or GitLab into the code collection.
    As a modular system, we must attach an additional ETL in the data collection pipeline,
    and everything else will work without further code modifications.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过按类别而不是来源分组数据，我们可以快速将来自其他平台的数据，如X插入帖子或GitLab插入代码收集。作为一个模块化系统，我们必须在数据收集管道中附加额外的ETL，其他所有操作将无需进一步代码修改。
- en: Feature pipeline
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征管道
- en: The feature pipeline’s role is to take raw articles, posts, and code data points
    from the data warehouse, process them, and load them into the feature store.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 特征管道的作用是从数据仓库中提取原始文章、帖子以及代码数据点，对其进行处理，并将它们加载到特征存储中。
- en: The characteristics of the FTI pattern are already present.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: FTI模式的特点已经存在。
- en: 'Here are some custom properties of the LLM Twin’s feature pipeline:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是LLM Twin特征管道的一些自定义属性：
- en: 'It processes three types of data differently: articles, posts, and code'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它以不同的方式处理三种类型的数据：文章、帖子以及代码
- en: 'It contains three main processing steps necessary for fine-tuning and RAG:
    cleaning, chunking, and embedding'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它包含三个主要处理步骤，这些步骤对于微调和RAG是必要的：清理、分块和嵌入
- en: It creates two snapshots of the digital data, one after cleaning (used for fine-tuning)
    and one after embedding (used for RAG)
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它创建了数字数据的两个快照，一个在清理后（用于微调）和一个在嵌入后（用于RAG）
- en: It uses a logical feature store instead of a specialized feature store
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用逻辑特征存储而不是专用特征存储
- en: Let’s zoom in on the logical feature store part a bit. As with any RAG-based
    system, one of the central pieces of the infrastructure is a vector DB. Instead
    of integrating another DB, more concretely, a specialized feature store, we used
    the vector DB, plus some additional logic to check all the properties of a feature
    store our system needs.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微深入探讨一下逻辑特征存储库的部分。与任何基于RAG的系统一样，基础设施的核心部分之一是一个向量数据库。我们不是集成另一个数据库，更具体地说，是一个专门的特征存储库，而是使用了向量数据库，以及一些额外的逻辑来检查我们系统需要的特征存储库的所有属性。
- en: The vector DB doesn’t offer the concept of a training dataset, but it can be
    used as a NoSQL DB. This means we can access data points using their ID and collection
    name. Thus, we can easily query the vector DB for new data points without any
    vector search logic. Ultimately, we will wrap the retrieved data into a versioned,
    tracked, and shareable artifact—more on artifacts in *Chapter 2*. For now, you
    must know it is an MLOps concept used to wrap data and enrich it with the properties
    listed before.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库不提供训练数据集的概念，但它可以用作NoSQL数据库。这意味着我们可以使用它们的ID和集合名称来访问数据点。因此，我们可以轻松地查询向量数据库以获取新的数据点，而无需任何向量搜索逻辑。最终，我们将检索到的数据封装成一个版本化、可追踪和可共享的工件——关于工件的内容将在*第二章*中详细介绍。现在，你必须知道这是一个MLOps概念，用于封装数据，并使用之前列出的属性来丰富它。
- en: How will the rest of the system access the logical feature store? The training
    pipeline will use the instruct datasets as artifacts, and the inference pipeline
    will query the vector DB for additional context using vector search techniques.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的其余部分将如何访问逻辑特征存储库？训练流程将使用指示数据集作为工件，推理流程将使用向量搜索技术查询向量数据库以获取额外的上下文。
- en: 'For our use case, this is more than enough because of the following reasons:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的用例，这已经足够了，原因如下：
- en: The artifacts work great for offline use cases such as training
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工件非常适合离线用例，如训练
- en: The vector DB is built for online access, which we require for inference.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量数据库是为了在线访问而构建的，这是我们进行推理所必需的。
- en: In future chapters, however, we will explain how the three data categories (articles,
    posts, and code) are cleaned, chunked, and embedded.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的章节中，我们将解释三种数据类别（文章、帖子以及代码）是如何被清理、分块和嵌入的。
- en: To conclude, we take in raw article, post, or code data points, process them,
    and store them in a feature store to make them accessible to the training and
    inference pipelines. Note that trimming all the complexity away and focusing only
    on the interface is a perfect match with the FTI pattern. Beautiful, right?
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们接收原始的文章、帖子或代码数据点，对它们进行处理，并将它们存储在特征存储库中，以便于训练和推理流程的访问。注意，去除所有复杂性并仅关注接口与FTI模式完美匹配。美丽，对吧？
- en: Training pipeline
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练流程
- en: The training pipeline consumes instruct datasets from the feature store, fine-tunes
    an LLM with it, and stores the tuned LLM weights in a model registry. More concretely,
    when a new instruct dataset is available in the logical feature store, we will
    trigger the training pipeline, consume the artifact, and fine-tune the LLM.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 训练流程从特征存储库中消耗指示数据集，用其微调一个LLM，并将微调后的LLM权重存储在模型注册库中。更具体地说，当逻辑特征存储库中有新的指示数据集可用时，我们将触发训练流程，消耗工件，并微调LLM。
- en: In the initial stages, the data science team owns this step. They run multiple
    experiments to find the best model and hyperparameters for the job, either through
    automatic hyperparameter tuning or manually. To compare and pick the best set
    of hyperparameters, we will use an experiment tracker to log everything of value
    and compare it between experiments. Ultimately, they will pick the best hyperparameters
    and fine-tuned LLM and propose it as the LLM production candidate. The proposed
    LLM is then stored in the model registry. After the experimentation phase is over,
    we store and reuse the best hyperparameters found to eliminate the manual restrictions
    of the process. Now, we can completely automate the training process, known as
    continuous training.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始阶段，数据科学团队负责这一步骤。他们运行多个实验以找到最适合该工作的最佳模型和超参数，无论是通过自动超参数调整还是手动调整。为了比较和选择最佳的超参数集，我们将使用实验跟踪器来记录所有有价值的内容，并在实验之间进行比较。最终，他们将选择最佳的超参数和微调后的LLM，并将其作为LLM生产候选者提出。提出的LLM随后将存储在模型注册库中。实验阶段结束后，我们将存储和重用找到的最佳超参数，以消除过程的手动限制。现在，我们可以完全自动化训练过程，这被称为持续训练。
- en: The testing pipeline is triggered for a more detailed analysis than during fine-tuning.
    Before pushing the new model to production, assessing it against a stricter set
    of tests is critical to see that the latest candidate is better than what is currently
    in production. If this step passes, the model is ultimately tagged as accepted
    and deployed to the production inference pipeline. Even in a fully automated ML
    system, it is recommended to have a manual step before accepting a new production
    model. It is like pushing the red button before a significant action with high
    consequences. Thus, at this stage, an expert looks at a report generated by the
    testing component. If everything looks good, it approves the model, and the automation
    can continue.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 测试流水线被触发以进行比微调期间更详细的分析。在将新模型推送到生产环境之前，对其进行更严格的测试评估是至关重要的，以确保最新的候选者比当前生产中的更好。如果这一步通过，该模型最终会被标记为接受并部署到生产推理流水线。即使在完全自动化的ML系统中，也建议在接受新的生产模型之前有一个手动步骤。这就像在具有高后果的重大行动之前按下红色按钮。因此，在这个阶段，专家会查看测试组件生成的报告。如果一切看起来都很好，它会批准该模型，自动化可以继续。
- en: 'The particularities of this component will be on LLM aspects, such as the following:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 本组件的特定之处在于LLM方面，例如以下内容：
- en: How do you implement an LLM agnostic pipeline?
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何实现一个与LLM无关的流水线？
- en: What fine-tuning techniques should you use?
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该使用哪些微调技术？
- en: How do you scale the fine-tuning algorithm on LLMs and datasets of various sizes?
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何将微调算法扩展到LLMs和各种大小的数据集上？
- en: How do you pick an LLM production candidate from multiple experiments?
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何从多个实验中挑选LLM生产候选者？
- en: How do you test the LLM to decide whether to push it to production or not?
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何测试LLM以决定是否将其推送到生产环境？
- en: By the end of this book, you will know how to answer all these questions.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 到这本书的结尾，你将知道如何回答所有这些问题。
- en: One last aspect we want to clarify is **CT**. Our modular design allows us to
    quickly leverage an ML orchestrator to schedule and trigger different system parts.
    For example, we can schedule the data collection pipeline to crawl data every
    week.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后想澄清的一个方面是**CT**。我们的模块化设计使我们能够快速利用ML编排器来调度和触发不同的系统部分。例如，我们可以安排数据收集流水线每周爬取数据。
- en: Then, we can trigger the feature pipeline when new data is available in the
    data warehouse and the training pipeline when new instruction datasets are available.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当数据仓库中有新数据可用时，我们可以触发特征流水线；当有新的指令数据集可用时，我们可以触发训练流水线。
- en: Inference pipeline
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理流水线
- en: The inference pipeline is the last piece of the puzzle. It is connected to the
    model registry and logical feature store. It loads a fine-tuned LLM from the model
    registry, and from the logical feature store, it accesses the vector DB for RAG.
    It takes in client requests through a REST API as queries. It uses the fine-tuned
    LLM and access to the vector DB to carry out RAG and answer the queries.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 推理流水线是最后一部分。它与模型注册表和逻辑特征存储相连。它从模型注册表中加载微调后的LLM，并从逻辑特征存储中访问RAG的向量数据库。它通过REST
    API接收客户端请求作为查询。它使用微调后的LLM和访问向量数据库来执行RAG并回答查询。
- en: All the client queries, enriched prompts using RAG, and generated answers are
    sent to a prompt monitoring system to analyze, debug, and better understand the
    system. Based on specific requirements, the monitoring system can trigger alarms
    to take action either manually or automatically.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 所有客户端查询、使用RAG丰富后的提示以及生成的答案都会发送到提示监控系统进行分析、调试和更好地理解系统。根据具体要求，监控系统可以触发警报，手动或自动采取行动。
- en: 'At the interface level, this component follows exactly the FTI architecture,
    but when zooming in, we can observe unique characteristics of an LLM and RAG system,
    such as the following:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在接口层面，该组件严格遵循FTI架构，但当我们放大查看时，我们可以观察到LLM和RAG系统的独特特征，例如以下内容：
- en: A retrieval client used to do vector searches for RAG
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于为RAG执行向量搜索的检索客户端
- en: Prompt templates used to map user queries and external information to LLM inputs
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于将用户查询和外部信息映射到LLM输入的提示模板
- en: Special tools for prompt monitoring
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专门用于即时监控的工具
- en: Final thoughts on the FTI design and the LLM Twin architecture
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对FTI设计和LLM双架构的最终思考
- en: We don’t have to be highly rigid about the FTI pattern. It is a tool used to
    clarify how to design ML systems. For example, instead of using a dedicated features
    store just because that is how it is done, in our system, it is easier and cheaper
    to use a logical feature store based on a vector DB and artifacts. What was important
    to focus on were the required properties a feature store provides, such as a versioned
    and reusable training dataset.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不必对FTI模式过于严格。它是一个用于阐明如何设计机器学习系统的工具。例如，我们系统中使用基于向量数据库和工件的逻辑特征存储库比使用专门的特性存储库更容易且成本更低。重要的是要关注特性存储库提供的所需属性，例如版本化和可重用的训练数据集。
- en: Ultimately, we will explain the computing requirements of each component briefly.
    The data collection and feature pipeline are mostly CPU-based and do not require
    powerful machines. The training pipeline requires powerful GPU-based machines
    that could load an LLM and fine-tune it. The inference pipeline is somewhere in
    the middle. It still needs a powerful machine but is less compute-intensive than
    the training step. However, it must be tested carefully, as the inference pipeline
    directly interfaces with the user. Thus, we want the latency to be within the
    required parameters for a good user experience. However, using the FTI design
    is not an issue. We can pick the proper computing requirements for each component.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们将简要解释每个组件的计算需求。数据收集和特征管道主要基于CPU，不需要强大的机器。训练管道需要能够加载LLM并进行微调的强大GPU机器。推理管道位于中间位置，它仍然需要强大的机器，但计算密集度低于训练步骤。然而，它必须经过仔细测试，因为推理管道直接与用户接口。因此，我们希望延迟在良好的用户体验所需的参数范围内。然而，使用FTI设计模式没有问题。我们可以为每个组件选择适当的计算需求。
- en: Also, each pipeline will be scaled differently. The data and feature pipelines
    will be scaled horizontally based on the CPU and RAM load. The training pipeline
    will be scaled vertically by adding more GPUs. The inference pipeline will be
    scaled horizontally based on the number of client requests.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，每个管道的扩展方式也将不同。数据和特征管道将根据CPU和RAM负载进行水平扩展。训练管道将通过添加更多GPU进行垂直扩展。推理管道将根据客户端请求的数量进行水平扩展。
- en: To conclude, the presented LLM architecture checks all the technical requirements
    listed at the beginning of the section. It processes the data as requested, and
    the training is modular and can be quickly adapted to different LLMs, datasets,
    or fine-tuning techniques. The inference pipeline supports RAG and is exposed
    as a REST API. On the LLMOps side, the system supports dataset and model versioning,
    lineage, and reusability. The system has a monitoring service, and the whole ML
    architecture is designed with CT/CI/CD in mind.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，所提出的LLM架构满足了该节开头列出的所有技术要求。它按照要求处理数据，训练是模块化的，并且可以快速适应不同的LLM、数据集或微调技术。推理管道支持RAG，并以REST
    API的形式公开。在LLMOps方面，系统支持数据集和模型版本控制、血缘关系和可重用性。系统具有监控服务，整个机器学习架构都是考虑到CT/CI/CD（持续测试/持续集成/持续部署）来设计的。
- en: This concludes the high-level overview of the LLM Twin architecture.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了LLM Twin架构的高级概述。
- en: Summary
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This first chapter was critical to understanding the book’s goal. As a product-oriented
    book that will walk you through building an end-to-end ML system, it was essential
    to understand the concept of an LLM Twin initially. Afterward, we walked you through
    what an MVP is and how to plan our LLM Twin MVP based on our available resources.
    Following this, we translated our concept into a practical technical solution
    with specific requirements. In this context, we introduced the FTI design pattern
    and showcased its real-world application in designing systems that are both modular
    and scalable. Ultimately, we successfully applied the FTI pattern to design the
    architecture of the LLM Twin to fit all our technical requirements.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这第一章对于理解本书的目标至关重要。作为一本以产品为导向的书籍，它将引导你构建一个端到端的机器学习系统，因此首先理解LLM Twin的概念是至关重要的。之后，我们向您介绍了MVP是什么以及如何根据我们可用的资源来规划我们的LLM
    Twin MVP。接着，我们将我们的概念转化为一个具有具体要求的实际技术解决方案。在此背景下，我们介绍了FTI设计模式，并展示了其在设计既模块化又可扩展的系统中的实际应用。最终，我们成功地将FTI模式应用于设计LLM
    Twin的架构，以满足所有技术要求。
- en: Having a clear vision of the big picture is essential when building systems.
    Understanding how a single component will be integrated into the rest of the application
    can be very valuable when working on it. We started with a more abstract presentation
    of the LLM Twin architecture, focusing on each component’s scope, interface, and
    interconnectivity.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建系统时，拥有清晰的总体视图至关重要。理解单个组件如何集成到应用程序的其他部分，在开发过程中可能会非常有价值。我们首先对LLM Twin架构进行了更抽象的介绍，重点关注每个组件的范围、接口和互连性。
- en: The following chapters will explore how to implement and deploy each component.
    On the MLOps side, we will walk you through using a computing platform, orchestrator,
    model registry, artifacts, and other tools and concepts to support all MLOps best
    practices.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节将探讨如何实现和部署每个组件。在MLOps方面，我们将向您介绍如何使用计算平台、编排器、模型注册、工件和其他工具和概念来支持所有MLOps最佳实践。
- en: References
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Dowling, J. (2024a, July 11). *From MLOps to ML Systems with Feature/Training/Inference
    Pipelines*. *Hopsworks*. [https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines
    )
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dowling, J. (2024a, July 11). *从MLOps到ML系统：特征/训练/推理管道*。 *Hopsworks*。 [https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines)
- en: Dowling, J. (2024b, August 5). *Modularity and Composability for AI Systems
    with AI Pipelines and Shared Storage*. *Hopsworks*. [https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage](https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage
    )
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dowling, J. (2024b, August 5). *使用AI管道和共享存储构建AI系统的模块化和可组合性*。 *Hopsworks*。 [https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage](https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage)
- en: Joseph, M. (2024, August 23). *The Taxonomy for Data Transformations in AI Systems*.
    *Hopsworks*. [https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems
    )
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joseph, M. (2024, August 23). *AI系统中数据转换的分类法*。 *Hopsworks*。 [https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems)
- en: '*MLOps: Continuous delivery and automation pipelines in machine learning*.
    (2024, August 28). Google Cloud. [https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning
    )'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MLOps：机器学习中的持续交付和自动化管道*。 (2024, August 28). Google Cloud. [https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)'
- en: 'Qwak. (2024a, June 2). *CI/CD for Machine Learning in 2024: Best Practices
    to build, test, and Deploy* | Infer. *Medium*. [https://medium.com/infer-qwak/ci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2](https://medium.com/infer-qwak/ci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2
    )'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qwak. (2024a, June 2). *2024年机器学习的CI/CD：构建、测试和部署的最佳实践* | Infer. *Medium*。 [https://medium.com/infer-qwak/ci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2](https://medium.com/infer-qwak/ci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2)
- en: Qwak. (2024b, July 23). *5 Best Open Source Tools to build End-to-End MLOPs
    Pipeline* in 2024\. *Medium*. [https://medium.com/infer-qwak/building-an-end-to-end-mlops-pipeline-with-open-source-tools-d8bacbf4184f](https://medium.com/infer-qwak/building-an-end-to-end-mlops-pipeline-with-open-source-tools-d8bacbf4184f)
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qwak. (2024b, July 23). *2024年5大最佳开源工具构建端到端MLOps管道*。 *Medium*。 [https://medium.com/infer-qwak/building-an-end-to-end-mlops-pipeline-with-open-source-tools-d8bacbf4184f](https://medium.com/infer-qwak/building-an-end-to-end-mlops-pipeline-with-open-source-tools-d8bacbf4184f)
- en: 'Salama, K., Kazmierczak, J., & Schut, D. (2021). *Practitioners guide to MLOPs:
    A framework for continuous delivery and automation of machine learning* (1^(st)
    ed.) [PDF]. Google Cloud. [https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf](https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf  )'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salama, K., Kazmierczak, J., & Schut, D. (2021). *MLOps实践指南：机器学习持续交付和自动化的框架*（第1版）[PDF].
    Google Cloud. [https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf](https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf)
- en: Join our book’s Discord space
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llmeng](https://packt.link/llmeng)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llmeng](https://packt.link/llmeng)'
- en: '![](img/QR_Code79969828252392890.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code79969828252392890.png)'
