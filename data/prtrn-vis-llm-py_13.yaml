- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Prompt Engineering
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示工程
- en: 'In this chapter, we’ll dive into a special set of techniques called prompt
    engineering. You’ll learn about this technique at a high level, including how
    it is similar to and different from other learning-based topics covered throughout
    this book. We’ll explore examples across vision and language and dive into key
    terms and success metrics. In particular, this chapter covers all of the tips
    and tricks for improving performance *without updating the model weights*. This
    means we’ll be mimicking the learning process, without necessarily changing any
    of the model parameters. This includes some advanced techniques such as prompt
    and prefix tuning. We will cover the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨一种特殊的技术集合——提示工程。你将从高层次了解这一技术，包括它与本书中其他基于学习的主题的相似性和差异性。我们将探索视觉和语言方面的示例，深入研究关键术语和成功指标。特别地，本章涵盖了所有改进性能的技巧与窍门，*无需更新模型权重*。这意味着我们将模拟学习过程，而不一定改变任何模型参数。这包括一些高级技术，如提示和前缀微调。本章将涵盖以下主题：
- en: Prompt engineering – the art of getting more with less
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程 – 用更少的输入获得更多的艺术
- en: From few- to zero-shot learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从少量学习到零-shot学习
- en: Tips and tricks for text-to-image prompting
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本到图像提示的技巧与窍门
- en: Best practices for image-to-image prompting
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像到图像提示的最佳实践
- en: Prompting large language models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示大型语言模型
- en: Advanced techniques – prompt and prefix tuning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级技术 – 提示和前缀微调
- en: Prompt engineering – the art of getting more with less
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示工程 – 用更少的输入获得更多的艺术
- en: At this point in the book, and in your project, you should have a lot invested
    in your new foundation model. From compute costs to datasets, custom code, and
    research papers you’ve read, you might have spent a solid 50-100 hours or more
    of your own time eking out performance gains. Kudos to you! It’s a great life
    to live.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的这个阶段，以及在你的项目中，你应该已经在你的新基础模型上投入了大量的时间。从计算成本、数据集、定制代码到你阅读过的研究论文，你可能已经花费了50到100小时或更多的时间来提升性能。为你鼓掌！这是一种很棒的生活方式。
- en: After you’ve done this, however, and especially after you’ve learned how to
    build a complete application around your model, it’s time to maximize your model’s
    performance on inference. In the last chapter, we learned about multiple ways
    to optimize your model’s runtime, from compilation to quantization and distillation
    to distribution, and each of these is helpful in speeding up your inference results.
    This entire chapter, however, is dedicated to getting the most accurate response
    you can. Here, I use the word “accurate” heuristically to indicate any type of
    model *quality* or *evaluation* metric. As you learned in the previous chapter
    on evaluation, accuracy itself is a misleading term and frequently not your best
    pick for an evaluation metric. Please see [*Chapter 10*](B18942_10.xhtml#_idTextAnchor152)
    for more details
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在你完成这一切之后，尤其是当你学会如何围绕你的模型构建一个完整的应用时，接下来就是最大化你的模型在推理时的表现了。在上一章中，我们了解了多种优化模型运行时的方法，从编译到量化，再到蒸馏和分发，每一种都对加速推理结果有所帮助。然而，本章的重点是获得你能够得到的最准确的响应。在这里，我使用“准确”一词是为了表明任何类型的模型*质量*或*评估*指标。如你在上一章关于评估中所学，准确率本身是一个具有误导性的术语，通常不是评估指标中最好的选择。更多细节请参见[*第10章*](B18942_10.xhtml#_idTextAnchor152)
- en: Prompt engineering includes a set of techniques related to picking the best
    input to the model for inference. Inference refers to getting a result out of
    your model without updating the weights; think of it like just the forward pass
    without any backpropagation. This is interesting because it’s how you can get
    *predictions* out of your model. When you deploy your model, you’re deploying
    it for inference.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程包括一组与选择最佳输入以进行模型推理相关的技术。推理是指在不更新权重的情况下从模型中获取结果；可以将其理解为没有任何反向传播的前向传递。这很有趣，因为它是你如何从模型中获取*预测*的方式。当你部署你的模型时，你是在进行推理。
- en: Prompt engineering includes a huge swath of techniques. It includes things such
    as *zero- and few-shot learning*, where we send multiple examples to the model
    and ask it to complete the logical sequence. It includes picking the right hyperparameters.
    It includes a lot of guessing and checking, testing your model results and figuring
    out the best techniques for it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程包含了大量的技术。它包括像*零-shot和少量-shot学习*这样的内容，我们向模型提供多个示例，要求它完成逻辑序列。它还包括选择正确的超参数。它包括大量的猜测和检验，测试你的模型结果并找出最适合的技术。
- en: For those of you who are hosting a generative AI model for end consumers outside
    of your direct team, you might even consider standing up a client to handle prompt
    engineering for you. This seems to be somewhat common in model playgrounds, where
    not all the parameters and model invocation are directly exposed. As an app developer,
    you can and should modify the prompts your customer is sending to your models
    to ensure they get the best performance they can. This might include adding extra
    terms to the invocation, updating the hyperparameters, and rephrasing the request.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些为终端消费者托管生成性AI模型的你们，可能会考虑搭建一个客户端来处理提示工程。这在模型演示平台中似乎比较常见，其中并非所有的参数和模型调用都是直接公开的。作为应用开发者，你可以并且应该修改客户发送给你模型的提示，以确保他们获得最佳性能。这可能包括在调用中添加额外的术语、更新超参数以及重新表述请求。
- en: Let’s explore prompt engineering in more detail and its related skill, few-shot
    learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地探讨一下提示工程以及与之相关的技能——少样本学习。
- en: From few- to zero-shot learning
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从少样本到零样本学习
- en: As you’ll remember, a key model we’ve been referring back to is **GPT-3**, **Generative
    Pretrained Transformers**. The paper that gave us the third version of this is
    called *Language models are few shot learners*. *(1)* Why? Because the primary
    goal of the paper was to develop a model capable of performing well without extensive
    fine-tuning. This is an advantage because it means you can use one model to cover
    a much broader array of use cases without needing to develop custom code or curate
    custom datasets. Said another way, the unit economics are much stronger for zero-shot
    learning than they are for fine-tuning. In a fine-tuning world, you need to work
    harder for your base model to solve a use case. This is in contrast to a few-shot
    world, where it’s easier to solve additional use cases from your base model. This
    makes the few-shot model more valuable because the fine-tuning model becomes too
    expensive at scale. While in practice fine-tuning solves problems more robustly
    than few-shot learning, it makes the entire practice of prompt engineering very
    attractive. Let’s look at a few examples in the following screenshot.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所记得的，我们一直在参考的一个关键模型是**GPT-3**，即**生成预训练变换器**。提供第三版的论文叫做*语言模型是少样本学习者*。（1）为什么？因为这篇论文的主要目标是开发一个能够在没有大量微调的情况下表现良好的模型。这是一个优势，因为它意味着你可以使用一个模型来覆盖更多的使用场景，而不需要开发定制代码或整理定制数据集。换句话说，零样本学习的单位经济学比微调更强。在微调的世界中，你需要为基础模型的任务解决方案付出更多努力。这与少样本学习的世界不同，在少样本学习中，基于你的基础模型解决其他任务要容易得多。这使得少样本模型更有价值，因为微调模型在大规模应用时成本太高。虽然在实践中，微调比少样本学习更稳健地解决问题，但它使得提示工程的整个实践变得非常有吸引力。接下来让我们看看以下屏幕截图中的一些示例。
- en: '![Figure 13.1 – Few-shot learning examples from the GPT-3 paper](img/B18942_Figure_13_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图13.1 – 来自GPT-3论文的少样本学习示例](img/B18942_Figure_13_01.jpg)'
- en: Figure 13.1 – Few-shot learning examples from the GPT-3 paper
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1 – 来自GPT-3论文的少样本学习示例
- en: On the left-hand side, we see different options for inputs to the model on inference.
    In the paper, they use the phrase “in-context learning,” referring to the fact
    that in the dataset, there can be samples of a task definition and examples. These
    repeated samples help the model learn both the name and the example of the learning.
    Here, the name of the task, or the task description, is **Translate English to
    French**. Then, we see examples of this, such as **sea otter -> loutre de mer**.
    When you provide the name of the task to GPT-3, along with a few samples, it is
    then able to respond quite well.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，我们看到模型推理的不同输入选项。在论文中，他们使用了“上下文学习”这一短语，指的是在数据集中，可以包含任务定义和示例的样本。这些重复的样本帮助模型学习任务的名称和示例。在这里，任务的名称或任务描述是**将英语翻译成法语**。然后，我们看到这样的示例，例如**海獺
    -> loutre de mer**。当你向GPT-3提供任务名称以及一些样本时，它便能够做出相当不错的响应。
- en: We call this **few-shot learning**. This is because we’re providing a few examples
    to the model, notably more than one and less than a full dataset. I struggle with
    using the word “learn” here, because technically, the model’s weights and parameters
    aren’t being updated. The model isn’t changing at all, so arguably we shouldn’t
    even use the word “learn.” On the other hand, providing these examples as input
    ahead of time clearly improves the performance of the model, so from an output-alone
    perspective perhaps we could use the word “learn.” In any case, this is the standard
    terminology.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称之为**少-shot学习**。这是因为我们为模型提供了少量示例，显然比一个示例多，但又少于完整的数据集。我在这里使用“学习”这个词有些困难，因为从技术上讲，模型的权重和参数并没有更新。模型并没有发生变化，因此可以说我们甚至不应该使用“学习”这个词。另一方面，提前提供这些示例作为输入显著提升了模型的表现，因此从输出的角度来看，或许我们可以使用“学习”这个词。无论如何，这已是标准的术语。
- en: A similar example would then be **zero-shot learning**, where we provide no
    examples to the model of how we expect it to complete its task, and hope it performs
    well. This is ideal for open-domain question-answering, such as ChatGPT. However,
    as many people have discovered, a model that performs well in a zero-shot learning
    scenario can also be shown to perform well in a few-shot or even single-shot example.
    All of these are useful techniques to understand large models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一个类似的例子是**零-shot学习**，在这种情况下，我们不给模型提供任何完成任务的示例，而是希望它能够表现良好。这种方法非常适合开放领域的问答任务，比如ChatGPT。然而，正如许多人发现的那样，一个在零-shot学习场景中表现良好的模型，也可以在少-shot甚至单-shot示例中表现得很好。所有这些技术对于理解大型模型都非常有用。
- en: 'As we saw in *Figure 13**.1*, a natural comparison with this type of learning
    is fine-tuning. In a fine-tuning approach, as we learned in [*Chapter 10*](B18942_10.xhtml#_idTextAnchor152),
    we use the pretrained model as a base and train it again using a larger dataset
    sample. Usually, this dataset sample will be supervised, but it is possible to
    use unsupervised fine-tuning when necessary. In a language scenario, this supervision
    might be classification, question answering, or summarization. In vision, you
    might see new image and text pairs across any number of use cases: fashion, e-commerce,
    image design, marketing, media and entertainment, manufacturing, product design,
    and so on.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*图13.1*中看到的，这种学习方法的自然比较是微调。在微调方法中，正如我们在[*第10章*](B18942_10.xhtml#_idTextAnchor152)中学到的，我们使用预训练的模型作为基础，并使用更大的数据集样本对其进行再训练。通常，这个数据集样本是有监督的，但在必要时也可以使用无监督的微调。在语言场景中，这种监督可能是分类、问答或摘要。在视觉领域，你可能会看到新的图像和文本对，涵盖了各种应用场景：时尚、电商、图像设计、营销、媒体娱乐、制造业、产品设计等等。
- en: 'The most common progression would entail the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的进展通常包括以下几个步骤：
- en: First, try zero-shot learning with your model. Does it work perfectly out of
    the box on every use case and edge scenario? Likely, it does in a few very narrow
    cases but can use some help elsewhere.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，尝试在你的模型上进行零-shot学习。它能在每个用例和边缘场景下完美运行吗？很可能，它在一些非常狭窄的场景下能工作得很好，但在其他场景下可能需要一些帮助。
- en: Next, try single- and few-shot learning.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，尝试单-shot和少-shot学习。
- en: If you give it a few examples of what you are looking for, does it figure it
    out? Can it follow the prompts you provide and get a good response? If all else
    fails, move on to fine-tuning. Go collect a dataset more specific to the use case
    you want to enhance your model in and train it there. Interestingly, fine-tuning
    seems to be much more successful in language-only scenarios. In vision, fine-tuning
    very easily overfits or falls into *catastrophic forgetting*, where the model
    loses its ability to hold onto the images and objects provided in the base dataset.
    You may be better off exploring an image-to-image approach, which follows later.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你提供一些你期望的示例，模型能否理解并给出合适的响应？它是否能按照你提供的提示做出好的反应？如果这些都不行，可以尝试微调。去收集一个更具体的、与你想要增强模型的应用场景相关的数据集并进行训练。有趣的是，微调在仅涉及语言的场景中似乎更加成功。在视觉领域，微调很容易导致模型过拟合或陷入*灾难性遗忘*，即模型失去了保持基础数据集中的图像和物体的能力。你可能更适合探索图像到图像的方式，下面会进一步讲解。
- en: Now, let’s learn a few best practices for prompt engineering across vision and
    language.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习一些关于视觉和语言的最佳提示工程实践。
- en: Text-to-image prompt engineering tips
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本到图像的提示工程技巧
- en: 'As we mentioned earlier in the book, Stable Diffusion is a great model you
    can use to interact with via natural language and produce new images. The beauty,
    fun, and simplicity of Stable Diffusion-based models are that you can be endlessly
    creative in designing your prompt. In this example, I made up a provocative title
    for a work of art. I asked the model to imagine what an image would look like
    if it were created by Ansel Adams, a famous American photographer from the mid-twentieth
    century known for his black-and-white photographs of the natural world. Here was
    the full prompt: “*Closed is open” by Ansel Adams, high resolution, black and
    white, award winning. Guidance (20)*. Let’s take a closer look.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前在书中提到的，Stable Diffusion 是一个很棒的模型，你可以通过自然语言与其互动并生成新图像。Stable Diffusion
    基于模型的美妙、趣味和简单性在于，你可以在设计提示时尽情发挥创意。在这个例子中，我为一件艺术作品编造了一个挑衅性的标题。我要求模型想象一下，如果这幅图像是由安塞尔·亚当斯（Ansel
    Adams）创作的，会是什么样子。安塞尔·亚当斯是 20 世纪中期著名的美国摄影师，以其黑白自然风光摄影而闻名。以下是完整的提示：“*关闭即是开放*，安塞尔·亚当斯，高分辨率，黑白，获奖。指导（20）*”。让我们仔细看看。
- en: '![Figure 13.2 – An image generated by Stable Diffusion](img/B18942_Figure_13_02.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.2 – 由 Stable Diffusion 生成的图像](img/B18942_Figure_13_02.jpg)'
- en: Figure 13.2 – An image generated by Stable Diffusion
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2 – 由 Stable Diffusion 生成的图像
- en: 'In the following list, you’ll find a few helpful tips to improve your Stable
    Diffusion results:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，你会找到一些有助于提升你 Stable Diffusion 结果的提示：
- en: '**Add any of the following words to your prompt**: *Award-winning, high resolution,
    trending on <your favorite site here>, in the style of <your favorite artist here>,
    400 high dpi*, and so on. There are thousands of examples of great photos and
    their corresponding prompts online; a great site is lexica.art. Starting from
    what works is always a great path. If you’re passionate about vision, you can
    easily spend hours of time just pouring through these and finding good examples.
    For a faster route, that same site lets you search for words as a prompt and renders
    the images. It’s a quick way to get started with prompting your model.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在你的提示中添加以下词汇**：*获奖、高清晰度、在<你喜欢的网站>上流行、以<你喜欢的艺术家>的风格、400高dpi*，等等。网上有成千上万的优秀照片和它们的相应提示；一个很好的网站是
    lexica.art。以有效的方法为起点总是一个不错的选择。如果你对视觉充满热情，你可以轻松地花费几个小时只是在浏览这些示例并找到好的案例。为了更快速的方式，同一个网站允许你以关键词作为提示进行搜索并渲染图像。这是开始提示模型的快捷方式。'
- en: '**Add negative prompts**: Stable Diffusion offers a negative prompt option,
    which lets you provide words to the model that it will explicitly not use. Common
    examples of this are hands, human, oversaturated, poorly drawn, and disfigured.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**添加负面提示**：Stable Diffusion 提供了负面提示选项，允许你向模型提供它明确不使用的词语。常见的负面提示包括：手、人类、过度饱和、绘制不佳、畸形等。'
- en: '**Upscaling**: While most prompting with Stable Diffusion results in smaller
    images, such as size 512x512, you can use another technique, called upscaling,
    to render that same image into a much larger, higher quality image, of size 1,024x1,024
    or even more. Upscaling is a great step you can use to get the best quality Stable
    Diffusion models today, both on SageMaker *(2)* and through Hugging Face directly.
    *(3)* We’ll dive into this in a bit more detail in the upcoming section on image-to-image.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像放大**：虽然大多数使用 Stable Diffusion 的提示生成的图像较小，比如 512x512 的大小，但你可以使用另一种技巧，称为图像放大，将同一图像渲染成更大、更高质量的图像，大小可以是
    1,024x1,024 或更大。图像放大是你可以用来获得今天最佳质量 Stable Diffusion 模型的一个重要步骤，无论是在 SageMaker *(2)*
    上，还是通过 Hugging Face 直接使用 *(3)*。我们将在接下来的图像到图像部分进一步探讨这一点。'
- en: '**Precision and detail**: When you provide longer prompts to Stable Diffusion,
    such as including more terms in your prompt and being extremely descriptive about
    the types and styles of objects you’d like it to generate, you actually increase
    your odds of the response being good. Be careful about the words you use in the
    prompt. As we learned earlier in [*Chapter 11*](B18942_11.xhtml#_idTextAnchor167)
    on bias, most large models are trained on the backbone of the internet. With Stable
    Diffusion, for better or for worse, this means you want to use language that is
    common online. This means that punctuation and casing actually aren’t as important,
    and you can be really creative and spontaneous with how you’re describing what
    you want to see.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精准度与细节**：当你向Stable Diffusion提供更长的提示词时，例如在提示中加入更多术语并详细描述你想要它生成的物体类型和风格，实际上会提高响应效果的几率。要小心你在提示中使用的词汇。正如我们在[*第11章*](B18942_11.xhtml#_idTextAnchor167)关于偏见的部分学到的，大多数大型模型都是基于互联网上的数据进行训练的。对于Stable
    Diffusion来说，无论是好是坏，这意味着你要使用网络上常见的语言。这也意味着标点符号和大小写并不是那么重要，你可以非常创造性和自发地描述你想要看到的内容。'
- en: '**Order**: Interestingly, the order of your words matters in prompting Stable
    Diffusion. If you want to make some part of your prompt more impactful, such as
    *dark* or *beautiful*, move that to the front of your prompt. If it’s too strong,
    move it to the back.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顺序**：有趣的是，单词的顺序在给Stable Diffusion下达提示时非常重要。如果你想让提示中的某部分更具冲击力，比如*黑暗*或*美丽*，可以把它放在提示词的前面。如果它太强烈了，可以把它移到后面。'
- en: '**Hyperparameters**: These are also relevant in language-only models, but let’s
    call out a few that are especially relevant to Stable Diffusion.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数**：这些在纯语言模型中也有关系，但我们来特别提一下几个对Stable Diffusion尤为重要的超参数。'
- en: Key hyperparameters for Stable Diffusion prompt engineering
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion提示工程的关键超参数
- en: 1\. `guidance=20` on the second image, the model captures the stark contrast
    and shadow fades that characterized Adams’ work. In addition, we get a new style,
    almost like M. C. Escher, where the tree seems to turn into the floor.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. `guidance=20` 在第二张图像中，模型捕捉到了亚当斯作品中明显的对比和阴影渐变。此外，我们得到了一个新的风格，几乎像M.C. 艾舍尔（M.C.
    Escher），树木似乎变成了地板。
- en: '2\. **Seed**: This refers to an integer you can set to baseline your diffusion
    process. Setting the seed can have a big impact on your model response. Especially
    if my prompt isn’t very good, I like to start with the seed hyperparameter and
    try a few random starts. Seed impacts high-level image attributes such as style,
    size of objects, and coloration. If your prompt is strong, you may not need to
    experiment heavily here, but it’s a good starting point.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **种子**：这指的是一个整数，你可以设置它作为扩散过程的基准。设置种子对模型响应有很大影响。特别是如果我的提示词不太好，我喜欢从种子超参数开始，尝试几次随机启动。种子影响图像的高层属性，如风格、物体大小和颜色。如果你的提示词足够强大，你可能不需要在这里做大量实验，但它是一个不错的起点。
- en: '3\. **Width and height**: These are straightforward; they’re just the pixel
    dimensions of your output image! You can use them to change the scope of your
    result, and hence the type of picture the model generates. If you want a perfectly
    square image, use 512x512\. If you want a portrait orientation, use 512x768\.
    For a landscape orientation, use 768x512\. Remember you can use the upscaling
    process we’ll learn about shortly to increase the resolution on the image, so
    start with smaller dimensions first.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. **宽度和高度**：这些非常直接；它们只是输出图像的像素尺寸！你可以通过它们来改变结果的范围，从而影响模型生成的图片类型。如果你想要一张完全方形的图像，使用512x512。如果你想要竖版方向，使用512x768。如果是横版方向，使用768x512。记住，你可以使用稍后我们会学习的放大过程来提高图像的分辨率，因此最好先从较小的尺寸开始。
- en: 4\. `steps` set to `50`. Increasing this number will also increase the processing
    time. To get great results, personally, I like to scale this against guidance.
    If you plan on using a very high guidance term (~16), such as with a killer prompt,
    then I wouldn’t set inference steps to anything over 50\. This looks like it overfits,
    and the results are just plain bad. However, if your guidance scale is lower,
    closer to 8, then increasing the number of steps can get you a better result.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. `steps` 设置为 `50`。增加这个数字也会增加处理时间。为了得到很好的结果，个人来说，我喜欢根据guidance的值来调整这个数字。如果你计划使用非常高的guidance值（大约16），比如在一个强力提示下，那么我不建议将推理步骤设置为超过50。这看起来会出现过拟合，结果会很差。不过，如果你的guidance值较低，接近8，那么增加步骤数量可能会得到更好的结果。
- en: There are many more hyperparameters to explore for Stable Diffusion and other
    text-to-image diffusion models. For now, let’s explore techniques around image-to-image!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Stable Diffusion 和其他文本到图像的扩散模型，还有许多其他的超参数可以探索。现在，让我们来探讨一下图像到图像的技术！
- en: Image-to-image prompt engineering tips
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像到图像提示工程技巧
- en: A fascinating trend in generative AI, especially when prompting the model, is
    *image-to-image*. This covers a broad array of techniques that let you bring an
    image when you invoke the model. The response will then incorporate your source
    image into the response, letting you more concretely determine what response the
    model will provide. This is incredibly helpful for increasing the resolution of
    the images, adding a mask, or even introducing objects to then seamlessly format
    into the output image in any context.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成性 AI 中，尤其是在提示模型时，*图像到图像* 是一个引人入胜的趋势。这涵盖了一系列广泛的技术，允许你在调用模型时带入一张图像。然后，模型的响应将把你的源图像整合到响应中，让你更具体地确定模型提供的响应。这对于提高图像分辨率、添加遮罩，甚至引入物体以无缝地格式化到输出图像中的任何背景都非常有帮助。
- en: These core capabilities are possible through a technique introduced in early
    2022 *(4)*, called **Stochastic Differential Equations Edit** (**SDEdit**), which
    uses stochastic differential equations to make image synthesis and editing a lot
    easier. While it sounds a bit intimidating, it’s actually very intuitive. It lets
    you add a source image to your pretrained diffusion model and use that base image
    as inspiration. How? Through iteratively adding and removing noise in a variety
    of ways until the final result meets your preferred criteria. SDEdit improved
    on its predecessor, GAN-based methods, by up to 98% on realism and 91% on human
    satisfaction scores.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这些核心能力是通过一种在 2022 年初提出的技术实现的 *(4)*，称为**随机微分方程编辑**（**SDEdit**），它利用随机微分方程使得图像合成和编辑变得更加容易。虽然听起来有点吓人，但实际上它非常直观。它允许你将源图像添加到预训练的扩散模型中，并使用该基础图像作为灵感来源。怎么做呢？通过以多种方式反复添加和去除噪声，直到最终结果符合你的偏好标准。SDEdit
    在真实性上比其前身——基于 GAN 的方法提高了多达 98%，在人类满意度评分上提高了 91%。
- en: Let’s explore the ways we can use this enhanced image-to-image technique while
    prompting with your diffusion models!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索一下，在使用扩散模型进行提示时，如何利用这一增强的图像到图像技术！
- en: Upscaling
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 放大
- en: As mentioned earlier, this is a simple and fast way to increase the resolution
    of your images. When prompting the model, you can enhance a low-resolution image
    along with other parameters to increase quality. You have a built-in option for
    this with SageMaker JumpStart *(5)*, and you also have a full upscaling pipeline
    available through Hugging Face directly.*(6)* This can take another textual prompt
    in addition to the source image.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这是一种简单且快速的提高图像分辨率的方法。在提示模型时，你可以增强低分辨率的图像，并结合其他参数来提高质量。使用 SageMaker JumpStart
    *(5)*，你可以直接使用内置的选项，此外，通过 Hugging Face，你还可以使用完整的放大流程。*(6)* 这还可以接受除了源图像之外的其他文本提示。
- en: Masking
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 遮罩
- en: 'Another interesting technique when prompting diffusion models is **masking**.
    A mask is simply a set of pixels that covers a given area in a photo: mountains,
    cars, humans, dogs, and any other type of object present in the images. How do
    you find a pixel map? These days, honestly, an easy way might be to start with
    Meta’s new **Segment Anything Model** *(***SAM***)*. *(7)* You can upload an image
    and ask the model to generate a pixel map for anything in that image.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 提示扩散模型时，另一个有趣的技术是**遮罩**。遮罩就是一组像素，覆盖在照片中的某个区域：山脉、汽车、人类、狗或图像中的任何其他物体。如何找到像素图呢？现在，老实说，一个简单的方法可能是从
    Meta 的新**任何物体分割模型**（**SAM**）*（***SAM**）* 开始。*(7)* 你可以上传一张图像，并让模型生成该图像中任何物体的像素图。
- en: Once you have a mask, you can send it to a Stable Diffusion image to generate
    a new image inside of the mask. Classic examples of this are changing the styles
    of clothing that people seem to be wearing. You can extract the area of a photo
    with clothing using either SAM or open source CV tools, render the mask, and then
    send the mask to Stable Diffusion. It will generate a new image, combining the
    original with a newly generated twist to fill in the area of the mask.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了一个遮罩，你可以将其发送到 Stable Diffusion 图像中，在遮罩区域生成一张新图像。经典的例子是改变人们所穿衣服的风格。你可以使用
    SAM 或开源的 CV 工具提取照片中衣服的区域，渲染遮罩，然后将遮罩发送给 Stable Diffusion。它将生成一张新图像，将原始图像与新生成的元素结合起来，填充遮罩区域。
- en: For a nice and simple end-to-end example of this, check out one I just found
    on GitHub! *(8)*
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想看一个简单又完整的端到端示例，可以看看我刚在 GitHub 上找到的一个！*(8)*
- en: Prompting for object-to-image with DreamBooth
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用DreamBooth进行物体到图像的提示
- en: Unlike the previous methods we looked at in the previous sections, DreamBooth
    *(9)* does not use the underlying SDEdit method. Instead, it uses a handful of
    input images and runs a type of fine-tuning process, combined with textual guidance,
    to place the source object from all of the input images into the target scene
    generated by the model. The technique uses two loss functions, one to preserve
    the previous class learned by the pretrained model and another to reconstruct
    the new object into the final image.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在前面几节中看到的其他方法不同，DreamBooth *(9)* 并不使用底层的SDEdit方法。相反，它使用一组输入图像，并通过结合文本指导的微调过程，将来自所有输入图像的源物体放置到模型生成的目标场景中。该技术使用了两个损失函数，一个用于保留预训练模型已经学到的先前类别，另一个用于将新物体重建到最终图像中。
- en: This means arguably, it’s not a prompting technique; it’s closer to a fine-tuning
    technique. However, I’m including it here because I find the intention more similar
    to masking than to creating a net-new model, but that is actually the outcome.
    Let’s take a closer look at the DreamBooth loss function.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着可以说，它并不是一种提示技术；它更接近于一种微调技术。然而，我将它包括在这里，因为我觉得它的意图更类似于遮蔽（masking），而不是创建一个全新的模型，但实际上它的结果就是如此。我们来更仔细地看看DreamBooth的损失函数。
- en: '![](img/B18942_Figure_13_03.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18942_Figure_13_03.jpg)'
- en: Figure 13.3 – The DreamBooth loss function
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3 - DreamBooth损失函数
- en: Figure 13.3 - Dreambooth prior-preserving loss function
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3 - DreamBooth保留先前信息的损失函数
- en: DreamBooth is a great open source solution you can use to take any object you
    like and place it onto any background of your choice! Next, let’s learn about
    some techniques you can use to improve your prompts for language models.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: DreamBooth是一个很棒的开源解决方案，你可以用它将你喜欢的任何物体放置到你选择的任何背景中！接下来，我们来学习一些你可以用来改善语言模型提示的技巧。
- en: Prompting large language models
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示大型语言模型
- en: 'I’ve said this before: I am a huge fan and big advocate of Hugging Face. I’ve
    learned a lot about **natural language processing** (**NLP**) from and with them,
    so I’d be remiss if I didn’t call out their book as a great source for prompt
    engineering tips and techniques. *(10)* Most of those practices center around
    picking the right hyperparameters for your model, with each type of model offering
    slightly different results.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前说过：我非常喜欢并大力支持Hugging Face。我从他们那里学到了很多关于**自然语言处理**（**NLP**）的知识，因此如果不提到他们的书作为一个关于提示工程技巧和技术的极佳资源，我就不对得起自己了。*(10)*
    其中大多数做法围绕着为你的模型选择合适的超参数展开，每种类型的模型都会提供稍有不同的结果。
- en: However, I would argue that the rise of ChatGPT has now almost completely thrown
    that out of consideration. In today’s world, the extremely accurate performance
    of OpenAI’s model raises the bar for all NLP developers, pushing us to deliver
    comparable results. For better or worse, there is no going back. Let’s try to
    understand how to prompt our **large language models** (**LLMs**)! We’ll start
    with instruction fine-tuning.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我认为ChatGPT的崛起几乎完全改变了这一点。在今天的世界里，OpenAI模型的极高准确度提高了所有NLP开发者的标准，迫使我们交付相似的结果。无论好坏，都无法回头。让我们尝试理解如何提示我们的**大型语言模型**（**LLMs**）！我们将从指令微调开始。
- en: Instruction fine-tuning
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指令微调
- en: First, it’s helpful to really understand the difference between a model that
    has been *instruction fine-tuned* and one that has not. As we learned in [*Chapter
    10*](B18942_10.xhtml#_idTextAnchor152) on fine-tuning, instruction fine-tuning
    refers to a supervised fine-tuning step that uses instructions provided to the
    model, such as “Tell me the difference between a mimosa and a samosa,” and pairs
    these with an answer, something such as “While a mimosa is an alcoholic drink
    combining champagne with orange juice, a samosa is an Indian pastry filled with
    either vegetables or meat, and commonly with a potato filling.” The model then
    explicitly learns what it means to follow instructions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，理解一个经过*指令微调*的模型和一个未经过微调的模型之间的区别是非常有帮助的。正如我们在[*第10章*](B18942_10.xhtml#_idTextAnchor152)关于微调的内容中所学到的，指令微调是指一种监督式的微调步骤，它使用模型提供的指令，例如“告诉我含羞草和三角饺的区别”，并将这些指令与答案配对，如“含羞草是一种含有香槟和橙汁的酒精饮料，而三角饺则是一种印度酥皮点心，里面填充有蔬菜或肉类，通常还有土豆馅料。”模型随后会明确学习如何遵循指令。
- en: This matters for prompting LLMs because it will completely change your prompting
    style. If you’re working with an LLM that has already been instruction fine-tuned,
    you can jump right into zero-shot performance and immediately have it execute
    tasks for you seamlessly. If not, you will probably need to add some examples
    to your prompt, that is, few-shot learning, to encourage it to respond in the
    way you want it to.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这对提示LLM非常重要，因为它会完全改变你的提示风格。如果你正在使用一个已经经过指令微调的LLM，你可以直接进入零-shot性能，并立即让它无缝地为你执行任务。如果没有，你可能需要在提示中添加一些示例，也就是少量示例学习，以鼓励它按照你希望的方式做出回应。
- en: Once you’ve sorted out this key difference, it’s helpful to also spend some
    time trying out the model of your choice. They have nuances; some of them look
    for different tokens and separators, while others respond well to keywords and
    phrases. You want to get to know and test your LLM, and prompt engineering is
    a great way to do that. Another style to learn is *chain-of-thought prompting*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你弄清楚了这个关键区别，花时间尝试你选择的模型也是有帮助的。它们有细微的差别；有些模型需要不同的符号和分隔符，而其他的则对关键字和短语反应较好。你需要了解并测试你的LLM，而提示工程是一个很好的方法。另一个值得学习的风格是*思维链提示*。
- en: Chain-of-thought prompting
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思维链提示
- en: Even if you are working with a model that performs well in zero-shot cases,
    such as one that has received instruction fine-tuning, as we discussed previously,
    you may still come across use cases where you need to add some examples in the
    prompt to get the desired output. A great example of this is chain-of-thought
    prompting. Chain-of-thought prompting refers to prompting the model *to demonstrate
    how it* *arrives at an answer*. This is extremely valuable in scenarios where
    explainability is critical, such as explaining why an LLM makes stylistic updates
    or classification decisions. Imagine you are using LLMs in legal scenarios, for
    example, and you’d like the LLM to update the language in a legal document. When
    you prompt it as follows, instead of simply providing the answer, the model can
    explain step by step how it came to a given conclusion. This logical clarity helps
    most users have more trust in the system, helping them understand and trust that
    the suggestions made by the model are valid.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你正在使用一个在零-shot情况下表现良好的模型，比如一个已经接受了指令微调的模型，正如我们之前讨论的，你仍然可能会遇到一些需要在提示中添加示例以获得期望输出的使用场景。思维链提示就是一个很好的例子。思维链提示指的是提示模型*展示它是如何*
    *得出答案的*。这在解释性至关重要的场景中极为有价值，例如解释LLM为什么做出风格更新或分类决策。举个例子，假设你正在法律场景中使用LLM，并且你希望LLM更新法律文件中的语言。当你这样提示它时，模型不仅仅是提供答案，而是可以一步步解释它是如何得出某个结论的。这种逻辑清晰性有助于大多数用户对系统更有信任，帮助他们理解并相信模型提出的建议是有效的。
- en: It also in many cases helps with accuracy! This is because most LLMs are inherently
    auto-regressive; they’re very good at predicting which word is most likely to
    come next in a given string. When you prompt them into a chain of thought, you’re
    pushing them to generate thought by thought, keeping them closer to the truth.
    Let’s take a closer look at this visually, using the following graphic from the
    original paper. *(11)*
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 它在很多情况下也有助于提高准确性！这是因为大多数大语言模型（LLMs）本质上是自回归的；它们非常擅长预测在给定字符串中下一个最可能出现的词。当你引导它们进入一连串的思维时，你实际上是在推动它们按一步步的思路生成，从而使它们更接近真实情况。让我们通过以下原始论文中的图示来更直观地理解这一点。*(11)*
- en: '![Figure 13.4 – Chain-of-thought prompting](img/B18942_Figure_13_04.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图13.4 – 思维链提示](img/B18942_Figure_13_04.jpg)'
- en: Figure 13.4 – Chain-of-thought prompting
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4 – 思维链提示
- en: As you can see, on the left-hand side, we’re still doing some few-shot learning,
    but the answer provided in the prompt is simple. It only answers the question,
    full-stop. On the right-hand side, however, we prompt the model by providing an
    answer that *rephrases the question*. Now, the answer starts with re-generating
    a quick summary of the information provided in the question, then taking exactly
    one logical leap to output the correct answer. You can see that the model on the
    left-hand side fails to answer correctly, while the one on the right is correct.
    Actually, the model itself is the same, but the only difference here is the prompt.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在左侧，我们仍然在进行一些少量学习，但提示中的答案很简单。它仅仅回答问题，完结。可是，在右侧，我们通过提供一个*重新表述问题*的答案来提示模型。现在，答案开始于重新生成问题中提供的信息的快速摘要，然后进行一次逻辑跳跃，输出正确答案。你可以看到，左侧的模型没有正确回答，而右侧的模型是正确的。实际上，模型本身是一样的，唯一的区别是提示。
- en: For a model that has been instruction fine-tuned, you can also trigger a chain-of-thought
    performance with a statement such as “walk me through step by step how to...”.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于经过指令微调的模型，你还可以通过一个像“逐步指导我如何...”这样的语句来触发连锁思维表现。
- en: Summarization
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This is possibly the most common LLM scenario I see today: summarizing call
    transcripts, documents, and more. Summarization is now very easy with top LLMs.
    Simply paste as much of your document into the LLM as you can, based on the model’s
    context length, and add *Summarize*: at the bottom of the prompt. Some models
    will vary; you can also add *TL;DR*, *in summary*:, or any similar variant. Will
    they all work perfectly? No way. Will they catch absolutely everything? Absolutely
    not. Will they occasionally hallucinate? Without a doubt. How do we mitigate that?
    Fine-tuning, extensive validation, entity recognition, and auditing.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是我今天看到的最常见的LLM应用场景：总结电话记录、文档等。使用顶级LLM，摘要现在变得非常容易。只需根据模型的上下文长度，将尽可能多的文档粘贴到LLM中，并在提示的底部添加*Summarize*：。一些模型可能有所不同；你也可以添加*TL;DR*、*in
    summary*：或其他类似的变体。它们会完美地工作吗？当然不行。它们会抓住所有内容吗？绝对不可能。它们偶尔会产生幻觉吗？毫无疑问。我们该如何减轻这个问题？微调、大规模验证、实体识别和审计。
- en: Defending against prompt injections and jailbreaking
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 防御提示注入和越狱
- en: One technique to consider in prompting your model is *how sensitive it is to
    jailbreaking*. As we learned in [*Chapter 11*](B18942_11.xhtml#_idTextAnchor167)
    on detecting and mitigating bias, jailbreaking refers to malicious users prompting
    your model to engage in harmful behavior. This might be like asking your model
    to tell a rude joke about certain groups of people, asking it for instructions
    about theft, or asking its opinion about certain politicians or social groups.
    Please anticipate that in every LLM application, at least some of your users will
    try to jailbreak your model to see whether they can trick it into behaving poorly.
    A parallel method is **prompt injection**, where users can maliciously trick your
    model into responding with IP from your dataset, from your prompt set, or anything
    else from your instruction list.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示模型时需要考虑的一种技巧是*它对越狱的敏感程度*。正如我们在[*第11章*](B18942_11.xhtml#_idTextAnchor167)中学到的关于检测和缓解偏见的内容，越狱是指恶意用户诱导模型执行有害行为。这可能就像是要求模型讲一些关于某些群体的粗鲁笑话，要求它提供盗窃的指导，或者询问它对某些政治家或社会群体的看法。在每个LLM应用中，至少会有一些用户尝试越狱，看看是否能诱使模型做出不当行为。一个类似的方法是**提示注入**，用户可以恶意欺骗模型，让它输出来自数据集、提示集或任何其他指令列表中的IP。
- en: How can you defend against this? One way is with supervised fine-tuning. Anthropic
    maintains a large dataset of red-teamed data, available on Hugging Face here.
    *(12)* Please proceed with caution; the words used in this dataset are extremely
    graphic and may be triggering to some readers. Personally, I find it hard to study
    even a few lines of this dataset. As a supervised fine-tuning technique, or even,
    as suggested by Anthropic, as a reinforcement learning with human feedback technique,
    you can fine-tune your model to reject anything that looks malicious or harmful.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如何防御这一点？一种方法是通过有监督的微调。Anthropic维护了一个大型的红队数据集，可以在Hugging Face上找到，链接在这里。*(12)*
    请小心使用；这个数据集中的词汇非常直白，可能会让一些读者感到不适。就个人而言，我甚至觉得很难研究这个数据集中的几行内容。作为一种有监督的微调技术，或者正如Anthropic所建议的，作为一种带有人工反馈的强化学习技术，你可以对模型进行微调，以拒绝任何看起来恶意或有害的内容。
- en: On top of this, you can *add classifiers to your application ingest*. This means
    as your app is taking in new questions from your users, you can easily add extra
    machine learning models to detect any malicious or odd behavior in these questions
    and circumvent the answer. This gives you a lot of control over how your app responds.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，你还可以*为应用程序增加分类器来处理输入数据*。这意味着，当你的应用接收到来自用户的新问题时，你可以轻松地增加额外的机器学习模型来检测这些问题中的恶意或异常行为，并绕过回答。这使你能更好地控制应用的响应方式。
- en: Now that we’ve learned about some basic techniques for prompting LLMs, let’s
    look at a few advanced techniques!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了一些提示LLM的基本技巧，让我们看看一些更高级的技巧！
- en: Advanced techniques – prefix and prompt tuning
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级技巧 – 前缀与提示调优
- en: You might be wondering; isn’t there some sophisticated way to use optimization
    techniques and find the right prompt, without even updating the model parameters?
    The answer is yes, there are many ways of doing this. First, let’s try to understand
    **prefix tuning**.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在想，难道没有一种更精巧的方式，通过优化技术找到正确的提示，而不需要更新模型参数吗？答案是肯定的，确实有许多方法可以实现这一点。首先，让我们试着理解**前缀调优**。
- en: Prefix tuning
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前缀调优
- en: This technique was proposed *(13)* by a pair of Stanford researchers in 2021
    specifically for text generation. The core idea, as you can see in the following
    diagram from their paper, is that instead of producing a net-new model for each
    downstream task, a less resource-intensive option is to create a simple vector
    for each task itself, called the prefix.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这一技术由一对斯坦福研究人员在2021年提出，专门用于文本生成。正如你在他们论文中的下图所看到的，核心思想是，与其为每个下游任务生成一个全新的模型，不如创建一个简单的向量来表示每个任务，这个向量被称为前缀，且该方法消耗的资源较少。
- en: '![Figure 13.5 – Prefix tuning](img/B18942_Figure_13_05.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.5 – 前缀调优](img/B18942_Figure_13_05.jpg)'
- en: Figure 13.5 – Prefix tuning
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5 – 前缀调优
- en: The core idea here is that instead of fine-tuning the entire pretrained transformer
    for each downstream task, let’s try to update just a single vector for that task.
    Then, we don’t need to store all of the model weights; we can just store that
    vector!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的核心思想是，不是为每个下游任务微调整个预训练的变换器，而是尝试只更新该任务的一个单一向量。这样，我们就不需要存储所有的模型权重；我们只需要存储那个向量！
- en: Arguably, this technique is similar to one we briefly touched on in [*Chapter
    10*](B18942_10.xhtml#_idTextAnchor152)*, Fine-Tuning and Evaluating.* This technique
    injects trainable weights into an LLM, letting us learn just the new parameters
    rather than updating the entire model itself. I find prefix tuning interesting
    because we’re not really touching the model architecture at all; we’re just learning
    this basic object right at the start.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，这一技术与我们在[*第10章*](B18942_10.xhtml#_idTextAnchor152)*中简要提到的有些相似，*即微调与评估*。这项技术向LLM注入可训练的权重，让我们只学习新的参数，而不是更新整个模型本身。我觉得前缀调优很有趣，因为我们根本没有触及模型架构；我们只是从一开始就学习这个基本对象。
- en: Why should you learn about this? Because, as the Stanford team shows, this method
    uses only 0.1% of the parameters of the full model yet gives performance comparable
    to fine-tuning the entire model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么你应该了解这个？因为正如斯坦福团队所展示的，这种方法只使用了完整模型的0.1%参数，却能提供与微调整个模型相媲美的性能。
- en: 'How can you get started with prefix tuning? Using the new library from our
    friends at Hugging Face! They’re building an open source library to make all kinds
    of parameter-efficient fine-tuning available here: [https://github.com/huggingface/peft](https://github.com/huggingface/peft).
    Prefix tuning is certainly available.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如何开始前缀调优？使用我们Hugging Face的朋友们提供的新库！他们正在构建一个开源库，让各种参数高效的微调在这里变得可用：[https://github.com/huggingface/peft](https://github.com/huggingface/peft)。前缀调优肯定是可用的。
- en: Fortunately, the example for general PEFT, coming from the inimitable Phill
    Schmid, seems quite accessible here. *(14)* With some specialized data preprocessing
    and custom model configs, you too can add this to your scripts.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，来自无可比拟的Phill Schmid的通用PEFT示例，在这里似乎非常易于访问。*(14)* 通过一些专门的数据预处理和自定义模型配置，你也可以将其添加到你的脚本中。
- en: Now, let’s look at prompt tuning.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看提示调优。
- en: Prompt tuning
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示调优
- en: As we’ve seen, finding the right prompt is quite challenging. Usually, they
    are built on discrete words in human natural language and can require a fair amount
    of manual iteration to trick the model into providing the expected answer. In
    Google’s 2021 ACL paper *(15)* introducing this concept, they proposed ”soft prompts”
    that are learnable through backpropagation. Thankfully, this incorporates the
    signal from any number of labeled examples, simplifying the prefix-tuning approach
    proposed previously.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，找到合适的提示是相当具有挑战性的。通常，这些提示基于人类自然语言中的离散单词，可能需要相当多的手动迭代才能欺骗模型提供预期的答案。在谷歌2021年ACL论文中（*(15)*）引入这一概念时，他们提出了可以通过反向传播学习的“软提示”。幸运的是，这种方法包含了来自任何数量标记示例的信号，从而简化了之前提出的前缀调优方法。
- en: With prompt tuning, we freeze the entire pretrained model but allow an extra
    *k* tunable tokens per each downstream task to be added to the input text. These
    are then considered soft tokens, or signals learned by the model to recognize
    each downstream task. You can see this in the diagram from their paper shown here.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提示调优，我们冻结整个预训练模型，但允许为每个下游任务向输入文本添加额外的*k*可调令牌。这些令牌被视为软令牌，或者是模型学习到的信号，用于识别每个下游任务。你可以在他们论文中的图示中看到这一点。
- en: '![Figure 13.6 – Prompt tuning](img/B18942_Figure_13_06.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.6 – 提示调优](img/B18942_Figure_13_06.jpg)'
- en: Figure 13.6 – Prompt tuning
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.6 – 提示调优
- en: Similar to prefix tuning, using prompt tuning, we still freeze the foundation
    model weights. We are also still adding some new, learnable items to the input
    dataset mixed with a variety of downstream task data samples. The key difference
    is that instead of learning full blocks for the model, we learn new, machine-readable
    tokens. That means the tokens themselves should change after the gradient updating,
    signaling something the model recognizes as basically a trigger for that type
    of downstream task. If you are working on scenarios where parameter-efficient
    fine-tuning isn’t an option, such as where the model is completely obscured to
    you, then prefix or prompt tuning maybe be a good option to explore. Both techniques
    are available in the relevant Hugging Face library, `peft`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于前缀调优，通过提示调优，我们仍然冻结基础模型的权重。我们同样还会向输入数据集中添加一些新的、可学习的项，并与各种下游任务数据样本混合。关键的区别在于，与学习完整模块不同，我们学习新的、机器可读的令牌。这意味着这些令牌本身应在梯度更新后发生变化，标志着模型识别出这基本上是该类型下游任务的触发信号。如果你正在处理参数高效微调不可行的场景，例如模型完全对你隐藏的情况，那么前缀调优或提示调优可能是一个不错的探索选项。这两种技术都可以在相关的Hugging
    Face库中找到，`peft`。
- en: Now, let’s close out the chapter with a quick summary.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过快速总结来结束本章。
- en: Summary
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced the concept of prompt engineering. I’d define
    that as everything that ekes out accuracy gains from your model without updating
    the weights of the model itself. Said another way, this is the art of getting
    more with less. We walked through few-shot learning, where you send a few examples
    of your desired inference results to the model, to zero-shot learning, where you
    hope to get a response from the model without any prior information. Needless
    to say, consumers tend to strongly prefer zero-shot learning. We covered a few
    tips and tricks for prompting text-to-image models, especially how to get good
    performance out of the open source Stable Diffusion. We learned about image-to-image
    prompting, where you can pass images to your diffusion-based models to produce
    a new image using an intersection. We also learned about prompting LLMs, including
    the implications of instruction fine-tuning, chain-of-thought prompting, summarization,
    and defending against prompt injections and jailbreaking. Finally, we introduced
    a few advanced techniques, including prompt and prefix tuning.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了提示工程的概念。我将其定义为在不更新模型权重的情况下，从模型中榨取准确性提升的所有方法。换句话说，这就是以更少的投入获取更多成果的艺术。我们介绍了少样本学习，其中你向模型提供几个期望推理结果的示例，零样本学习则是你希望模型在没有任何先前信息的情况下给出回应。毋庸置疑，消费者通常更喜欢零样本学习。我们讨论了一些针对文本到图像模型的提示技巧，特别是如何从开源的Stable
    Diffusion中获得良好的表现。我们了解了图像到图像的提示方法，通过向基于扩散的模型传递图像，从而利用交集生成新图像。我们还了解了提示大语言模型（LLM），包括指令微调、思维链提示、摘要生成、防止提示注入和越狱的影响。最后，我们介绍了一些高级技巧，包括提示和前缀调优。
- en: Now, let’s get started on [*Chapter 14*](B18942_14.xhtml#_idTextAnchor217),
    which is on MLOps for vision and LLMs!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始阅读 [*第 14 章*](B18942_14.xhtml#_idTextAnchor217)，内容是关于视觉和LLM的MLOps！
- en: References
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Please go through the following content for more information on some topics
    covered in the chapter:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下内容，了解本章节涵盖的一些主题的更多信息：
- en: 'Language Models are Few-Shot Learners: [https://arxiv.org/pdf/2005.14165.pdf](https://arxiv.org/pdf/2005.14165.pdf)'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Language Models are Few-Shot Learners：[https://arxiv.org/pdf/2005.14165.pdf](https://arxiv.org/pdf/2005.14165.pdf)
- en: 'Upscale images with Stable Diffusion in Amazon SageMaker JumpStart: [https://aws.amazon.com/blogs/machine-learning/upscale-images-with-stable-diffusion-in-amazon-sagemaker-jumpstart/](https://aws.amazon.com/blogs/machine-learning/upscale-images-with-stable-diffusion-in-amazon-sagemaker-jumpstart/)'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用稳定扩散在Amazon SageMaker JumpStart中提升图像质量：[https://aws.amazon.com/blogs/machine-learning/upscale-images-with-stable-diffusion-in-amazon-sagemaker-jumpstart/](https://aws.amazon.com/blogs/machine-learning/upscale-images-with-stable-diffusion-in-amazon-sagemaker-jumpstart/)
- en: 'stabilityai/stable-diffusion-x4-upscaler Copied: [https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler)'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: stabilityai/stable-diffusion-x4-upscaler Copied：[https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler)
- en: 'SDEDIT: GUIDED IMAGE SYNTHESIS AND EDITING WITH STOCHASTIC DIFFERENTIAL EQUATIONS:
    [https://arxiv.org/pdf/2108.01073.pdf](https://arxiv.org/pdf/2108.01073.pdf)'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SDEDIT：使用随机微分方程进行引导图像合成和编辑：[https://arxiv.org/pdf/2108.01073.pdf](https://arxiv.org/pdf/2108.01073.pdf)
- en: 'Upscale images with Stable Diffusion in Amazon SageMaker JumpStart: [https://aws.amazon.com/blogs/machine-learning/upscale-images-with-stable-diffusion-in-amazon-sagemaker-jumpstart/](https://aws.amazon.com/blogs/machine-learning/upscale-images-with-stable-diffusion-in-amazon-sagemaker-jumpstart/)'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用稳定扩散在Amazon SageMaker JumpStart中提升图像质量：[https://aws.amazon.com/blogs/machine-learning/upscale-images-with-stable-diffusion-in-amazon-sagemaker-jumpstart/](https://aws.amazon.com/blogs/machine-learning/upscale-images-with-stable-diffusion-in-amazon-sagemaker-jumpstart/)
- en: 'Hugging Face: [https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler
    )'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hugging Face：[https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler
    )
- en: 'Segment Anything: [https://arxiv.org/pdf/2304.02643.pdf](https://arxiv.org/pdf/2304.02643.pdf)'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Segment Anything：[https://arxiv.org/pdf/2304.02643.pdf](https://arxiv.org/pdf/2304.02643.pdf)
- en: 'amrrs/stable-diffusion-prompt-inpainting: [https://github.com/amrrs/stable-diffusion-prompt-inpainting/blob/main/Prompt_based_Image_In_Painting_powered_by_ClipSeg.ipynb](https://github.com/amrrs/stable-diffusion-prompt-inpainting/blob/main/Prompt_based_Image_In_Painting_powered_by_ClipSeg.ipynb)'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: amrrs/stable-diffusion-prompt-inpainting：[https://github.com/amrrs/stable-diffusion-prompt-inpainting/blob/main/Prompt_based_Image_In_Painting_powered_by_ClipSeg.ipynb](https://github.com/amrrs/stable-diffusion-prompt-inpainting/blob/main/Prompt_based_Image_In_Painting_powered_by_ClipSeg.ipynb)
- en: 'DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation:
    [https://arxiv.org/pdf/2208.12242.pdf](https://arxiv.org/pdf/2208.12242.pdf)'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DreamBooth：Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation：[https://arxiv.org/pdf/2208.12242.pdf](https://arxiv.org/pdf/2208.12242.pdf)
- en: 'nlp-with-transformers/website: [https://github.com/nlp-with-transformers/website](https://github.com/nlp-with-transformers/website)'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: nlp-with-transformers/website：[https://github.com/nlp-with-transformers/website](https://github.com/nlp-with-transformers/website)
- en: 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models: [https://arxiv.org/pdf/2201.11903.pdf](https://arxiv.org/pdf/2201.11903.pdf)'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models：[https://arxiv.org/pdf/2201.11903.pdf](https://arxiv.org/pdf/2201.11903.pdf)
- en: 'Hugging Face: [https://huggingface.co/datasets/Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hugging Face：[https://huggingface.co/datasets/Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)
- en: 'Prefix-Tuning: Optimizing Continuous Prompts for Generation: [https://arxiv.org/pdf/2101.00190.pdf](https://arxiv.org/pdf/2101.00190.pdf)'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Prefix-Tuning：优化连续提示以进行生成：[https://arxiv.org/pdf/2101.00190.pdf](https://arxiv.org/pdf/2101.00190.pdf)
- en: 'huggingface/notebooks: [https://github.com/huggingface/notebooks/blob/main/sagemaker/24_train_bloom_peft_lora/scripts/run_clm.py](https://github.com/huggingface/notebooks/blob/main/sagemaker/24_train_bloom_peft_lora/scripts/run_clm.py)'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: huggingface/notebooks：[https://github.com/huggingface/notebooks/blob/main/sagemaker/24_train_bloom_peft_lora/scripts/run_clm.py](https://github.com/huggingface/notebooks/blob/main/sagemaker/24_train_bloom_peft_lora/scripts/run_clm.py)
- en: 'The Power of Scale for Parameter-Efficient Prompt Tuning: [https://aclanthology.org/2021.emnlp-main.243.pdf](https://aclanthology.org/2021.emnlp-main.243.pdf)'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: The Power of Scale for Parameter-Efficient Prompt Tuning：[https://aclanthology.org/2021.emnlp-main.243.pdf](https://aclanthology.org/2021.emnlp-main.243.pdf)
- en: '[https://arxiv.org/pdf/1902.00751.pdhttps://arxiv.org/pdf/1902.00751.pd](https://arxiv.org/pdf/1902.00751.pdhttps://arxiv.org/pdf/1902.00751.pd)'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1902.00751.pdhttps://arxiv.org/pdf/1902.00751.pd](https://arxiv.org/pdf/1902.00751.pdhttps://arxiv.org/pdf/1902.00751.pd)'
