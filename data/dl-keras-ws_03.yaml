- en: 3\. Deep Learning with Keras
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 使用 Keras 的深度学习
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, you will experiment with different neural network architectures.
    You will create Keras sequential models—building single-layer and multi-layer
    models—and evaluate the performance of trained models. Networks of different architectures
    will help you understand overfitting and underfitting. By the end of this chapter,
    you will have explored early stopping that can be used to combat overfitting to
    the training data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将尝试不同的神经网络架构。你将创建 Keras 顺序模型——构建单层和多层模型——并评估训练模型的性能。不同架构的网络将帮助你理解过拟合和欠拟合。到本章结束时，你将探索如何使用**早停**方法来应对对训练数据的过拟合问题。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In the previous chapter, you learned about the mathematics of neural networks,
    including `linear transformations` with `scalars`, `vectors`, `matrices`, and
    `tensors`. Then, you implemented your first neural network using Keras by building
    a logistic regression model to classify users of a website into those who will
    purchase from the website and those who will not.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你了解了神经网络的数学基础，包括与`标量`、`向量`、`矩阵`和`张量`相关的`线性变换`。然后，你使用 Keras 实现了你的第一个神经网络，通过构建一个逻辑回归模型将网站用户分类为会购买和不会购买的人。
- en: In this chapter, you will extend your knowledge of building neural networks
    using Keras. This chapter covers the basics of deep learning and will provide
    you with the necessary foundations so that you can build highly complex neural
    network architectures. We will start by extending the `logistic regression` model
    to a simple single-layer neural network and then proceed to more complicated neural
    networks with multiple hidden layers.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将扩展使用 Keras 构建神经网络的知识。本章介绍了深度学习的基础知识，并为你提供了必要的基础，使你能够构建高度复杂的神经网络架构。我们将从扩展`逻辑回归`模型到一个简单的单层神经网络开始，然后逐步进入具有多个隐藏层的更复杂的神经网络。
- en: In this process, you will learn about the underlying basic concepts of neural
    networks, including forward propagation for making predictions, computing loss,
    backpropagation for computing derivatives of loss with respect to model parameters,
    and, finally, gradient descent for learning about optimal parameters for the model.
    You will also learn about the various choices that are available so that you can
    build and train a neural network in terms of `activation functions`, `loss functions`,
    and `optimizers`.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，你将了解神经网络的基本概念，包括用于做出预测的前向传播、计算损失、用于计算损失对模型参数导数的反向传播，最后是用于学习模型最佳参数的梯度下降。你还将了解如何根据`激活函数`、`损失函数`和`优化器`等不同的选择来构建和训练神经网络。
- en: Furthermore, you will learn how to evaluate your model while understanding issues
    such as **overfitting** and **underfitting**, all while looking at how they can
    impact the performance of your model and how to detect them. You will learn about
    the drawbacks of evaluating a model on the same dataset that's used for training,
    as well as the alternative approach of holding back a part of the available dataset
    for evaluation purposes. Subsequently, you will learn how to compare the model
    error rate on each of these two subsets of the dataset that can be used to detect
    problems such as high bias and high variance in the model. Lastly, you will learn
    about a technique called **early stopping** to reduce overfitting, which is again
    based on comparing the model's error rate to the two subsets of the dataset.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你将学习如何评估模型，同时了解**过拟合**和**欠拟合**等问题，同时分析这些问题如何影响模型的性能以及如何检测它们。你将了解到在相同的数据集上评估模型的缺陷，以及将一部分可用数据集保留用于评估的替代方法。接着，你将学习如何比较模型在这两个数据集子集上的错误率，从而检测出如高偏差和高方差等问题。最后，你将学习一种叫做**早停**的技术，用于减少过拟合，该技术通过将模型的错误率与数据集的两个子集进行比较来实现。
- en: Building Your First Neural Network
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建你的第一个神经网络
- en: In this section, you will learn about the representations and concepts of deep
    learning, such as **forward propagation**—the propagation of data through the
    network, multiplying the input values by the weight of each connection for every
    node, and **backpropagation**—the calculation of the gradient of the loss function
    with respect to the weights in the matrix, and **gradient descent**—the optimization
    algorithm that's used to find the minimum of the loss function.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习深度学习的表示和概念，例如**前向传播**—通过网络传播数据，乘以每个节点每个连接的输入值与权重的乘积，以及**反向传播**—计算损失函数相对于矩阵中权重的梯度，和**梯度下降**—用于寻找损失函数最小值的优化算法。
- en: We will not delve deeply into these concepts as it isn't required for this book.
    However, this coverage will essentially help anyone who wants to apply deep learning
    to a problem.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入探讨这些概念，因为本书并不需要。然而，这部分内容将帮助任何想将深度学习应用于问题的人。
- en: Then, we will move on to implementing neural networks using Keras. Also, we
    will stick to the simplest case, which is a neural network with a single hidden
    layer. You will learn how to define a model in Keras, choose the **hyperparameters**—the
    parameters of the model that are set before training the model—and then train
    your model. At the end of this section, you will have the opportunity to practice
    what you have learned by implementing a neural network in Keras so that you can
    perform classification on a dataset and observe how neural networks outperform
    simpler models such as logistic regression.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将开始使用Keras实现神经网络。此外，我们将坚持最简单的情况，即具有单个隐藏层的神经网络。你将学习如何在Keras中定义模型，选择**超参数**—在训练模型之前设置的模型参数，然后训练你的模型。在本节的最后，你将有机会通过在Keras中实现神经网络进行实践，以便你能够在数据集上执行分类并观察神经网络如何优于逻辑回归等简单模型。
- en: Logistic Regression to a Deep Neural Network
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归到深度神经网络
- en: In *Chapter 1*, *Introduction to Machine Learning with Keras*, you learned about
    the logistic regression model, and then how to implement it as a sequential model
    using Keras in *Chapter 2*, *Machine Learning versus Deep Learning*. Technically
    speaking, logistic regression involves a very simple neural network with only
    one hidden layer and only one node in its hidden layer.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第一章*，*使用Keras的机器学习简介*中，你学习了逻辑回归模型，然后在*第二章*，*机器学习与深度学习*中学习了如何使用Keras将其实现为顺序模型。从技术上讲，逻辑回归涉及一个非常简单的神经网络，只有一个隐藏层，且其隐藏层只有一个节点。
- en: 'An overview of the logistic regression model with two-dimensional input can
    be seen in the following image. What you see in this image is called one **node**
    or **unit** in the deep learning world, which is represented by the green circle.
    As you may have noticed, there are some differences between logistic regression
    terminology and deep learning terminology. In logistic regression, we call the
    parameters of the model **coefficients** and **intercepts**. In deep learning
    models, the parameters are referred to as **weights** (w) and **biases** (b):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 具有二维输入的逻辑回归模型概览可以在以下图片中看到。你在这张图片中看到的被称为一个**节点**或**单元**，在深度学习领域中由绿色圆圈表示。正如你可能已经注意到的那样，逻辑回归术语和深度学习术语之间存在一些差异。在逻辑回归中，我们将模型的参数称为**系数**和**截距**。在深度学习模型中，参数被称为**权重**（w）和**偏置**（b）：
- en: '![Figure 3.1: Overview of the logistic regression model with a two-dimensional
    input'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.1：具有二维输入的逻辑回归模型概览'
- en: '](img/B15777_03_01.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_03_01.jpg)'
- en: 'Figure 3.1: Overview of the logistic regression model with a two-dimensional
    input'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1：具有二维输入的逻辑回归模型概览
- en: At each node/unit, the inputs are multiplied by some weights and then a bias
    term is added to the sum of these weighted inputs. This can be seen in the calculation
    above the node in the preceding image. The `inputs` are `X1` and `X2`, the `weights`
    are `W1` and `W2`, and the `bias` is `b`. Next, a nonlinear function (for example,
    a sigmoid function in the case of a logistic regression model) is applied to the
    sum of the weighted inputs and the bias term is used to compute the final output
    of the node. In the calculation shown in the preceding image, this is `σ`. In
    deep learning, the nonlinear function is called the **activation function** and
    the output of the node is called the **activation** of that node.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个节点/单元处，输入会与一些权重相乘，然后将一个偏置项加到这些加权输入的和上。这可以在前一张图中节点上方的计算中看到。`inputs`是`X1`和`X2`，`weights`是`W1`和`W2`，`bias`是`b`。接下来，非线性函数（例如，在逻辑回归模型中是sigmoid函数）会应用于加权输入和偏置项的和，以计算节点的最终输出。在前一张图中的计算中，这是`σ`。在深度学习中，非线性函数被称为**激活函数**，节点的输出被称为该节点的**激活值**。
- en: 'It is possible to build a single-layer neural network by stacking logistic
    regression nodes/units on top of each other in a layer, as shown in the following
    image. Every value at the input layers, `X1` and `X2`, is passed to all three
    nodes at the hidden layer:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过将逻辑回归节点/单元垂直堆叠在同一层上来构建单层神经网络，如下图所示。每个输入层的值，`X1`和`X2`，都传递给隐藏层的三个节点：
- en: '![Figure 3.2: Overview of a single-layer neural network with a two-dimensional
    input'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.2：具有二维输入的单层神经网络概览'
- en: and a hidden layer of size 3
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 且隐藏层的大小为3
- en: '](img/B15777_03_02.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_03_02.jpg)'
- en: 'Figure 3.2: Overview of a single-layer neural network with a two-dimensional
    input and a hidden layer of size 3'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：具有二维输入和大小为3的隐藏层的单层神经网络概览
- en: 'It is also possible to build multi-layer neural networks by stacking multiple
    layers of processing nodes after one another, as shown in the following image.
    The following image shows a two-layer neural network with two-dimensional input:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以通过将多个处理节点层叠在一起构建多层神经网络，如下图所示。下图展示了一个具有二维输入的双层神经网络：
- en: '![Figure 3.3: Overview of a two-layer neural network with a two-dimensional
    input'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.3：具有二维输入的双层神经网络概览'
- en: '](img/B15777_03_03.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_03_03.jpg)'
- en: 'Figure 3.3: Overview of a two-layer neural network with a two-dimensional input'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：具有二维输入的双层神经网络概览
- en: The preceding two images show the most common way of representing a neural network.
    Every neural network consists of an **input layer**, an **output layer**, and
    one or many **hidden layers**. If there is only one hidden layer, the network
    is called a **shallow neural network**. On the other hand, neural networks with
    many hidden layers are called **deep neural networks**, and the process of training
    them is called **deep learning**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 前两张图展示了神经网络最常见的表示方式。每个神经网络由**输入层**、**输出层**和一个或多个**隐藏层**组成。如果只有一个隐藏层，则该网络被称为**浅层神经网络**。另一方面，具有多个隐藏层的神经网络被称为**深层神经网络**，其训练过程被称为**深度学习**。
- en: '*Figure 3.2* shows a neural network with only one hidden layer, so this would
    be a shallow neural network, whereas the neural network in *Figure 3.3* has two
    hidden layers, so it is a deep neural network. The input layers are generally
    on the left. In the case of *Figure 3.3*, these are features `X1` and `X2`, and
    they are input into the first hidden layer, which has three nodes. The arrows
    represent the weight values that are applied to the input. At the second hidden
    layer, the result of the first hidden layer becomes the input to the second hidden
    layer. The arrows between the first and second hidden layers represent the weights.
    The output is generally the layer on the far right and, in the case of *Figure
    3.3*, is represented by the layer labeled `Y`.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.2*展示了一个只有一个隐藏层的神经网络，因此这是一个浅层神经网络，而*图3.3*中的神经网络有两个隐藏层，因此它是一个深层神经网络。输入层通常位于左侧。在*图3.3*的情况下，这些是特征`X1`和`X2`，它们被输入到第一个隐藏层，该层有三个节点。箭头表示应用于输入的权重值。在第二个隐藏层处，第一个隐藏层的结果成为第二个隐藏层的输入。第一个和第二个隐藏层之间的箭头表示权重。输出通常是最右边的层，在*图3.3*中，用`Y`标记的层表示输出。'
- en: Note
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In some resources, you may see that a network, such as the one shown in the
    preceding image, is referred to as a **four-layer network**. This is because the
    input and output layers are counted along with the hidden layers. However, the
    more common convention is to count only the hidden layers, so the network we mentioned
    previously will be referred to as a two-layer network.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些资源中，您可能会看到一个网络，如前面图示所示，被称为**四层网络**。这是因为输入层和输出层与隐藏层一起计算。然而，更常见的惯例是只计算隐藏层，因此我们之前提到的网络将被称为二层网络。
- en: In a deep learning setting, the number of nodes in the input layer is equal
    to the number of features of the input data, and the number of nodes in the output
    layer is equal to the number of dimensions of the output data. However, you need
    to select the number of nodes in the hidden layers or the size of the hidden layers.
    If you choose a larger size layer, the model becomes more flexible and will be
    able to model more complex functions. This increase in flexibility comes at the
    cost of the need for more training data and more computations to train the model
    on. The parameters that are required to be selected by the developer are called
    `hyperparameters` and include parameters such as the number of layers and the
    number of nodes in each layer. Common hyperparameters to be chosen include the
    number of epochs to train for and the loss function to use.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，输入层的节点数等于输入数据的特征数，输出层的节点数等于输出数据的维度。然而，您需要选择隐藏层的节点数或隐藏层的大小。如果选择较大的层，模型会变得更加灵活，并能够建模更复杂的函数。这种灵活性的增加需要更多的训练数据和更多的计算来训练模型。开发者需要选择的参数称为`超参数`，包括层数和每层节点数等参数。常见的超参数包括训练的轮次和使用的损失函数。
- en: In the next section, we will cover `activation functions` that are applied after
    each hidden layer.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将介绍在每个隐藏层之后应用的`激活函数`。
- en: Activation Functions
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: In addition to the size of the layer, you need to choose an activation function
    for each hidden layer that you add to the model, and also do the same for the
    output layer. We learned about the sigmoid activation function in the logistic
    regression model. However, there are more options for activation functions that
    you can choose from when building a neural network in Keras. For example, the
    sigmoid activation function is a good choice as the activation function on the
    output layer for binary classification tasks since the result of a sigmoid function
    is bounded between 0 and 1\. Some commonly used activation functions for deep
    learning are **sigmoid**/**logistic**, **tanh** (**hyperbolic tangent**), and
    **Rectified Linear Unit** (**ReLU**).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了层的大小，您还需要为每个添加到模型中的隐藏层选择一个激活函数，输出层也需要进行相同的选择。我们在逻辑回归模型中了解过Sigmoid激活函数。然而，在Keras中构建神经网络时，您可以选择更多的激活函数。例如，Sigmoid激活函数是二分类任务中作为输出层激活函数的一个不错选择，因为Sigmoid函数的结果限制在0到1之间。一些常用的深度学习激活函数包括**Sigmoid**/**Logistic**、**tanh**（**双曲正切**）和**Rectified
    Linear Unit**（**ReLU**）。
- en: 'The following image shows a `sigmoid` activation function:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了`sigmoid`激活函数：
- en: '![Figure 3.4: Sigmoid activation function'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.4：Sigmoid激活函数'
- en: '](img/B15777_03_04.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_03_04.jpg)'
- en: 'Figure 3.4: Sigmoid activation function'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：Sigmoid激活函数
- en: 'The following image shows a `tanh` activation function:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了`tanh`激活函数：
- en: '![Figure 3.5: tanh activation function'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.5：tanh激活函数'
- en: '](img/B15777_03_05.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_03_05.jpg)'
- en: 'Figure 3.5: tanh activation function'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5：tanh激活函数
- en: 'The following image shows a `ReLU` activation function:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了`ReLU`激活函数：
- en: '![Figure 3.6: ReLU activation function'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.6：ReLU激活函数'
- en: '](img/B15777_03_06.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_03_06.jpg)'
- en: 'Figure 3.6: ReLU activation function'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6：ReLU激活函数
- en: As you can see in *Figures 3.4* and *3.5*, the output of a sigmoid function
    is always between `0` and `1`, and the output of tanh is always between `-1` and
    `1`. This makes `tanh` a better choice for hidden layers since it keeps the average
    of the outputs in each layer close to zero. In fact, `sigmoid` is only a good
    choice for the `activation function` of the output layer when building a `binary
    classifier` since its output can be interpreted as the probability of a given
    input belonging to one class.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在 *图 3.4* 和 *3.5* 中看到的，sigmoid函数的输出始终在`0`到`1`之间，而tanh的输出始终在`-1`到`1`之间。这使得`tanh`在隐藏层中成为更好的选择，因为它保持了每层输出的平均值接近零。事实上，`sigmoid`仅在构建二元分类器的输出层的激活函数时是一个好选择，因为其输出可以解释为给定输入属于某一类的概率。
- en: Therefore, `tanh` and `ReLU` are the most common choices of activation function
    for hidden layers. It turns out that the learning process is faster when using
    the `ReLU activation function` because it has a fixed derivative (or slope) for
    an input greater than `0`, and a slope of `0` everywhere else.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`tanh`和`ReLU`是隐藏层激活函数的最常见选择。事实证明，在使用`ReLU`激活函数时，学习过程更快，因为对于大于`0`的输入，它具有固定的导数（或斜率），而在其他地方斜率为`0`。
- en: Note
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can read more about all the available choices for activation functions
    in Keras here: [https://keras.io/activations/](https://keras.io/activations/).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里阅读更多关于Keras中所有可用激活函数的选择：[https://keras.io/activations/](https://keras.io/activations/)。
- en: Forward Propagation for Making Predictions
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于进行预测的前向传播
- en: Neural networks make a prediction about the output by performing **forward propagation**.
    Forward propagation entails the computations that are performed on the input in
    every layer of a neural network until the output layer is reached. It is best
    to understand forward propagation through an example.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通过执行**前向传播**来对输出进行预测。前向传播包括在神经网络的每一层上对输入执行的计算，直到达到输出层。通过一个例子来理解前向传播是最好的。
- en: Let's go through forward propagation equations one by one for a two-layer neural
    network (shown in the following image) where the input data is two-dimensional,
    and the output data is a one-dimensional binary class label. The activation functions
    for layer 1 and layer 2 will be tanh, and the activation function in the output
    layer is sigmoid.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一通过一个两层神经网络的前向传播方程进行详细说明（如下图所示），其中输入数据是二维的，输出数据是一维的二进制类标签。第1层和第2层的激活函数将是tanh，输出层的激活函数是sigmoid。
- en: The following image shows the weights and biases for each layer as matrices
    and vectors with proper indexes. For each layer, the number of rows in the weight's
    matrix is equal to the number of nodes in the previous layer, and the number of
    columns is equal to the number of nodes in that layer.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了每一层的权重和偏置作为矩阵和向量，并带有适当的索引。对于每一层，权重矩阵的行数等于上一层的节点数，列数等于该层的节点数。
- en: 'For example, `W1` has two rows and three columns because the input to layer
    1 is the input layer, `X`, which has two columns, and layer 1 has three nodes.
    Likewise, `W2` has three rows and three columns because the input to layer 2 is
    layer 1, which has two nodes, and layer 2 has five nodes. The bias, however, is
    always a vector with a size equal to the number of nodes in that layer. The total
    number of parameters in a deep learning model is equal to the total number of
    elements in all the weights'' matrices and the biases'' vectors:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`W1`有两行三列，因为第一层的输入是输入层`X`，它有两列，第一层有三个节点。同样，`W2`有三行三列，因为第二层的输入是第一层，它有两个节点，第二层有五个节点。然而，偏置始终是一个大小等于该层节点数的向量。深度学习模型中的参数总数等于所有权重矩阵和偏置向量中的元素总数：
- en: '![Figure 3.7: A two-layer neural network'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.7：一个两层神经网络'
- en: '](img/B15777_03_07.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_03_07.jpg)'
- en: 'Figure 3.7: A two-layer neural network'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7：一个两层神经网络
- en: An example of performing all the steps for forward propagation according to
    the neural network outlined in the preceding image is as follows.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面图像中所述的神经网络进行前向传播的所有步骤的示例如下。
- en: '**Steps to perform forward propagation**:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**执行前向传播的步骤**：'
- en: '`X` is the network input to the network in the preceding image, so it is the
    input for the first hidden layer. First, the input matrix, `X`, is the matrix
    multiplied by the weight matrix for layer 1, `W1`, and the bias, `b1`, is added:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`X`是上图中网络的输入，因此它是第一个隐藏层的输入。首先，输入矩阵`X`是矩阵与层1的权重矩阵`W1`相乘，然后加上偏置`b1`：'
- en: '*z1 = X*W1 + b1*'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*z1 = X*W1 + b1*'
- en: 'Next, the layer 1 output is computed by applying an activation function to
    *z1*, which is the output of the previous step:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，层1的输出是通过对*z1*（前一步的输出）应用激活函数计算得出的：
- en: '*a1 = tanh(z1)*'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*a1 = tanh(z1)*'
- en: '`a1` is the output of layer 1 and is called the `activation of layer 1`. The
    output of layer 1 is, in fact, the `input` for layer 2\. Next, the activation
    of layer 1 is the matrix multiplied by the weight matrix for layer 2, `W2`, and
    the bias, `b2`, is added:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`a1`是层1的输出，称为`层1的激活`。层1的输出实际上是层2的`输入`。接下来，层1的激活是矩阵与层2的权重矩阵`W2`相乘，然后加上偏置`b2`：'
- en: '*z2 = a1 * W2 + b2*'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*z2 = a1 * W2 + b2*'
- en: 'The layer 2 output/activation is computed by applying an activation function
    to `z2`:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 层2的输出/激活是通过对`z2`应用激活函数计算得出的：
- en: '*a2 = tanh(z2)*'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*a2 = tanh(z2)*'
- en: 'The output of layer 2 is, in fact, the input for the next layer (the network
    output layer here). Following this, the activation of layer 2 is the matrix multiplied
    by the weight matrix for the output layer, `W3`, and the bias, `b3`, is added:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 层2的输出实际上是下一个层（这里是网络输出层）的输入。接着，层2的激活是矩阵与输出层的权重矩阵`W3`相乘，再加上偏置`b3`：
- en: '*z3 = a2 * W3 + b3*'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*z3 = a2 * W3 + b3*'
- en: 'Finally, the network output, Y, is computed by applying the sigmoid activation
    function to `z3`:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，网络输出Y是通过对`z3`应用sigmoid激活函数计算得出的：
- en: '*Y = sigmoid(z3)*'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*Y = sigmoid(z3)*'
- en: The total number of parameters in this model is equal to the sum of the number
    of elements in `W1`, `W2`, `W3`, `b1`, `b2`, and `b3`. Therefore, the number of
    parameters can be calculated by summing the parameters in each of the parameters
    in weight matrices and biases, which is equal to 6 + 15 + 5 + 3 + 5 + 1 = 35\.
    These are the parameters that need to be learned in the process of deep learning.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的总参数数量等于`W1`、`W2`、`W3`、`b1`、`b2`和`b3`中元素的总和。因此，可以通过对每个参数的权重矩阵和偏置中的参数求和来计算参数数量，结果为6
    + 15 + 5 + 3 + 5 + 1 = 35。这些参数是深度学习过程中需要学习的。
- en: Now that we have learned about the forward propagation step, we have to evaluate
    our model and compare it to the real target values. To achieve that, we will use
    a loss function, which we will cover in the next section. Here, we will learn
    about some common loss functions that we can use for classification and regression
    tasks.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了前向传播步骤，接下来我们需要评估我们的模型并将其与真实的目标值进行比较。为此，我们将使用损失函数，接下来我们将讨论这一部分。在这里，我们将学习一些常见的损失函数，适用于分类和回归任务。
- en: Loss Function
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: When learning the optimal parameters (weights and biases) of a model, we need
    to define a function to measure error. This function is called the **loss function**
    and it provides us with a measure of how different network-predicted outputs are
    from the real outputs in the dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习模型的最优参数（权重和偏置）时，我们需要定义一个函数来衡量误差。这个函数叫做**损失函数**，它为我们提供了一个度量，表示网络预测的输出与数据集中的真实输出之间的差异。
- en: The loss function can be defined in several different ways, depending on the
    problem and the goal. For example, in the case of a classification problem, one
    common way to define loss is to compute the proportion of misclassified inputs
    in the dataset and use that as the probability of the model making an error. On
    the other hand, in the case of a regression problem, the loss function is usually
    defined by computing the distance between the predicted outputs and their corresponding
    real outputs, and then averaging over all the examples in the dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数可以根据问题和目标以不同的方式定义。例如，在分类问题中，定义损失的一种常见方法是计算数据集中误分类输入的比例，并将其作为模型出错的概率。另一方面，在回归问题中，损失函数通常通过计算预测输出和其对应真实输出之间的距离来定义，然后对数据集中的所有示例取平均值。
- en: 'Brief descriptions of some commonly used loss functions that are available
    in Keras are as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Keras中一些常用损失函数的简要描述：
- en: '`(real output – predicted output)^2` for each example in the dataset and then
    returns their average.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(真实输出 – 预测输出)^2` 对数据集中的每个示例进行计算，然后返回它们的平均值。'
- en: '`abs (real output – predicted output)` for each example in the dataset and
    then returns their average.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`abs (实际输出 - 预测输出)` 对数据集中的每个示例进行计算，然后返回它们的平均值。'
- en: '`abs [(real output – predicted output) / real output]` for each example in
    the dataset and then returns their average, multiplied by 100%.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`abs [(实际输出 - 预测输出) / 实际输出]` 对数据集中的每个示例进行计算，然后返回它们的平均值，再乘以 100%。'
- en: '`0` and `1`.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0` 和 `1`。'
- en: '**categorical_crossentropy** is a loss function for multi-class (more than
    two classes) classification problems.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**categorical_crossentropy** 是用于多类别（超过两个类别）分类问题的损失函数。'
- en: Note
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can read more about all the available choices for loss functions in Keras
    here: [https://keras.io/losses/](https://keras.io/losses/).'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在这里阅读更多关于 Keras 中可用的损失函数选项：[https://keras.io/losses/](https://keras.io/losses/)。
- en: During the training process, we keep changing the model parameters until the
    minimum difference between the model-predicted outputs and the real outputs is
    reached. This is called an **optimization process**, and we will learn more about
    how it works in later sections. For neural networks, we use backpropagation to
    compute the derivatives of the loss function with respect to the weights.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们不断地改变模型参数，直到模型预测输出与实际输出之间的最小差异被达到。这被称为**优化过程**，我们将在后面的章节中深入了解它是如何工作的。对于神经网络，我们使用反向传播来计算损失函数关于权重的导数。
- en: Backpropagation for Computing Derivatives of Loss Function
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播用于计算损失函数的导数
- en: '**Backpropagation** is the process of performing the chain rule of calculus
    from the output layer to the input layer of a neural network in order to compute
    the derivatives of the loss function with respect to the model parameters in each
    layer. The derivative of a function is simply the slope of that function. We are
    interested in the slope of the loss function because it provides us with the direction
    in which model parameters need to change in order for the loss value to be minimized.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向传播**是通过对神经网络从输出层到输入层应用微积分中的链式法则，来计算每一层损失函数关于模型参数的导数。函数的导数就是该函数的斜率。我们关心损失函数的斜率，因为它为我们提供了模型参数需要改变的方向，从而最小化损失值。'
- en: 'The chain rule of calculus states that if, for example, `z` is a function of
    `y`, and `y` is a function of `x`, then the derivative of `z` with respect to
    `x` can be reached by multiplying the derivative of `z` with respect to `y` by
    the derivative of `y` with respect to `x`. This can be written as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 微积分中的链式法则表明，如果，例如，`z` 是 `y` 的函数，而 `y` 是 `x` 的函数，那么 `z` 关于 `x` 的导数可以通过将 `z` 关于
    `y` 的导数与 `y` 关于 `x` 的导数相乘来得到。可以将其写作如下：
- en: '*dz/dx = dz/dy * dy/dx*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*dz/dx = dz/dy * dy/dx*'
- en: 'In deep neural networks, the loss function is a function of predicted outputs.
    We can show this through the equation given here:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度神经网络中，损失函数是预测输出的函数。我们可以通过下面给出的方程来展示这一点：
- en: '*loss = L(y_predicted)*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*loss = L(y_predicted)*'
- en: On the other hand, according to forward propagation equations, the output predicted
    by the model is a function of the model parameters—that is, the weights and biases
    in each layer. Therefore, according to the chain rule of calculus, the derivative
    of the loss with respect to the model parameters can be computed by multiplying
    the derivative of the loss with respect to the predicted output by the derivative
    of the predicted output with respect to the model parameters.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，根据前向传播方程，模型预测的输出是模型参数的函数——即每一层的权重和偏置。因此，根据微积分的链式法则，损失函数关于模型参数的导数可以通过将损失函数关于预测输出的导数与预测输出关于模型参数的导数相乘来计算。
- en: In the next section, we will learn how the optimal weight parameters are modified
    when given the derivatives of the loss function with respect to the weights.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习在给定损失函数关于权重的导数时，如何修改最优的权重参数。
- en: Gradient Descent for Learning Parameters
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降法用于学习参数
- en: In this section, we will learn how a deep learning model learns its optimal
    parameters. Our goal is to update the weight parameters so that the loss function
    is minimized. This will be an iterative process in which we continue to update
    the weight parameters so that the loss function is at a minimum. This process
    is called **learning parameters** and it is done through the use of an optimization
    algorithm. One very common optimization algorithm that's used for learning parameters
    in machine learning is **gradient descent**. Let's see how gradient descent works.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习深度学习模型如何学习其最优参数。我们的目标是更新权重参数，以便最小化损失函数。这将是一个迭代过程，我们将不断更新权重参数，直到损失函数达到最小值。这个过程称为**学习参数**，通过使用优化算法来完成。一个在机器学习中用于学习参数的非常常见的优化算法是**梯度下降**。让我们来看看梯度下降是如何工作的。
- en: If we plot the average of loss over all the examples in the dataset for all
    the possible values of the model parameters, it is usually a convex shape (such
    as the one shown in the following plot). In gradient descent, our goal is to find
    the minimum point (`Pt`) on the plot. The algorithm starts by initializing the
    model parameters with some random values (`P1`). Then, it computes the loss and
    the derivatives of the loss with respect to the parameters at that point. As we
    mentioned previously, the derivative of a function is, in fact, the slope of the
    function. After computing the slope at an initial point, we have the direction
    in which we need to update the parameters.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们绘制数据集中所有示例在所有可能的模型参数值下的平均损失，它通常是一个凸形状（如下图所示）。在梯度下降中，我们的目标是找到图中的最小点（`Pt`）。算法首先通过初始化模型参数为一些随机值（`P1`）开始。然后，它计算该点的损失和相对于参数的损失导数。如前所述，函数的导数实际上是该函数的斜率。计算出初始点的斜率后，我们就知道了需要更新参数的方向。
- en: The hyperparameter, called the `P2` in the following plot). As shown in the
    following plot, `P2` is closer to the target point, and if we keep moving in that
    direction, we will eventually get to the target point, `Pt`. The algorithm computes
    the slope of the function again at `P2` and takes another step.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数，称为`P2`（如下图所示）。如图所示，`P2`更接近目标点，如果我们继续朝该方向移动，我们最终会到达目标点`Pt`。算法再次计算`P2`处的函数斜率，并迈出下一步。
- en: 'This process is repeated until the slope is equal to zero and therefore no
    direction for further movement is provided:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程会重复进行，直到斜率为零，因此不再提供进一步移动的方向：
- en: '![Figure 3.8: A schematic view of the gradient descent algorithm finding the
    set'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.8：梯度下降算法寻找最小化损失的参数集的示意图'
- en: of parameters that minimize loss
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化损失的参数集
- en: '](img/B15777_03_08.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_03_08.jpg)'
- en: 'Figure 3.8: A schematic view of the gradient descent algorithm finding the
    set of parameters that minimize loss'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8：梯度下降算法寻找最小化损失的参数集的示意图
- en: 'The pseudocode for the gradient descent algorithm is provided here:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是梯度下降算法的伪代码：
- en: '[PRE0]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To summarize, the following steps are repeated when training a deep neural
    network (after initializing the parameters to some random values):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在训练深度神经网络时（在将参数初始化为一些随机值之后），以下步骤会被反复执行：
- en: Use forward propagation and the current parameters to predict the outputs for
    the entire dataset.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前向传播和当前的参数预测整个数据集的输出。
- en: Use the predicted outputs to compute the loss over all the examples.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预测的输出计算所有示例的损失。
- en: Use backpropagation to compute the derivatives of the loss with respect to the
    weights and biases at each layer.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用反向传播计算每一层权重和偏置相对于损失的导数。
- en: Update the weights and biases using the derivative values and the learning rate.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用导数值和学习率更新权重和偏置。
- en: What we discussed here was the standard gradient descent algorithm, which computes
    the loss and the derivatives using the entire dataset in order to update the parameters.
    There is another version of gradient descent called **stochastic gradient descent**
    (**SGD**), which computes the loss and the derivatives each time using a subset
    or a batch of data examples only; therefore, its learning process is faster than
    standard gradient descent.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里讨论的是标准的梯度下降算法，它使用整个数据集计算损失和导数，以更新参数。还有一种叫做**随机梯度下降**（**SGD**）的梯度下降版本，它每次只使用数据集的一个子集或一个批次来计算损失和导数；因此，它的学习过程比标准的梯度下降更快。
- en: Note
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Another common choice is an optimization algorithm called `SGD` when training
    deep learning models. As we've already learned, `SGD` uses a single hyperparameter
    (called a **learning rate**) to update the parameters. However, Adam improves
    this process by using a learning rate, a weighted average of gradients, and a
    weighted average of squared gradients to update the parameters at each iteration.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的选择是名为 `SGD` 的优化算法，它用于训练深度学习模型。如我们已经学习过，`SGD` 只使用一个超参数（称为**学习率**）来更新参数。然而，Adam
    改进了这一过程，通过使用学习率、加权平均梯度和加权平均平方梯度，在每次迭代时更新参数。
- en: Usually, when building a neural network, you need to choose two hyperparameters
    (called the `batch_size` argument determines the number of data examples to be
    included at each iteration of the optimization algorithm. `batch_size=None` is
    equivalent to the standard version of gradient descent, which uses the entire
    dataset in each iteration. The `epochs` argument determines how many times the
    optimization algorithm passes through the entire training dataset before it stops.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在构建神经网络时，您需要选择两个超参数（其中`batch_size`参数决定了每次优化算法迭代时包含的数据样本数量。`batch_size=None`相当于标准版本的梯度下降方法，它在每次迭代时使用整个数据集。`epochs`参数决定优化算法在停止之前需要遍历整个训练数据集多少次。
- en: For example, imagine we have a dataset of size `n=400`, and we choose `batch_size=5`
    and `epochs=20`. In this case, the optimizer will have `400/5 = 80` iterations
    in one pass through the entire dataset. Since it is supposed to go through the
    entire dataset `20` times, it will have `80 * 20` iterations in total.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个大小为 `n=400` 的数据集，我们选择 `batch_size=5` 和 `epochs=20`。在这种情况下，优化器在遍历整个数据集时将有
    `400/5 = 80` 次迭代。由于它需要遍历整个数据集 `20` 次，因此总共将进行 `80 * 20` 次迭代。
- en: Note
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'When building a model in Keras, you need to choose the type of optimizer to
    be used when training your model. There are some other options other than SGD
    and Adam available in Keras. You can read more about all the possible options
    for optimizers in Keras here: [https://keras.io/optimizers/](https://keras.io/optimizers/).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中构建模型时，您需要选择训练模型时使用的优化器类型。除了 SGD 和 Adam，Keras 还提供了其他一些选择。您可以在此链接中阅读关于
    Keras 中所有可能的优化器选项：[https://keras.io/optimizers/](https://keras.io/optimizers/)。
- en: Note
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: All the activities and exercises in this chapter will be developed in a Jupyter
    notebook. Please download this book's GitHub repository, along with all the prepared
    templates, from [https://packt.live/39pOUMT](https://packt.live/39pOUMT).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有活动和练习将使用 Jupyter notebook 进行开发。请从 [https://packt.live/39pOUMT](https://packt.live/39pOUMT)
    下载本书的 GitHub 仓库以及所有准备好的模板。
- en: 'Exercise 3.01: Neural Network Implementation with Keras'
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 3.01：使用 Keras 实现神经网络
- en: 'In this exercise, you will learn the step-by-step process of implementing a
    neural network using Keras. Our simulated dataset represents various measurements
    of trees, such as height, the number of branches, the girth of the trunk at the
    base, and more, that are found in a forest. Our goal is to classify the records
    into either deciduous or coniferous type trees based on the measurements given.
    First, execute the following code block to load a simulated dataset of `10000`
    records that consist of two classes, representing the two tree species, where
    each data example has `10` feature values:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，您将学习如何使用 Keras 步骤化地实现神经网络。我们模拟的数据集代表了森林中不同树木的各种测量值，如树高、树枝数量、树干基部的周长等。我们的目标是根据给定的测量值将记录分类为落叶树或针叶树两类。首先，执行以下代码块来加载一个包含
    `10000` 条记录的模拟数据集，其中有两个类别，代表两种树种，每条数据包含 `10` 个特征值：
- en: '[PRE1]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Expected output**:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**预期输出**：'
- en: '[PRE2]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Since each data example in this dataset can only belong to one of the two classes,
    this is a binary classification problem. Binary classification problems are very
    important and very common in real-life scenarios. For example, let's assume that
    the examples in this dataset represent the measurement results for `10000` trees
    from a forest. The goal is to build a model using this dataset to predict whether
    the species of each tree that's measured is a deciduous or coniferous species
    of tree. The `10` features for the trees can include predictors such as height,
    number of branches, and girth of the trunk at the base.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该数据集中的每个数据示例只能属于两个类别中的一个，因此这是一个二分类问题。二分类问题在现实生活中非常重要且常见。例如，假设该数据集中的示例代表了`10000`棵树的测量结果。这些树来自一片森林。目标是使用该数据集构建一个模型，以预测每棵被测量的树的物种是落叶树还是针叶树。树木的`10`个特征可能包括身高、树枝数量和基部的树干周长等预测因子。
- en: The output class `0` means that the tree is a coniferous species of tree, while
    the output class `1` means that the tree is a deciduous species of tree.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 输出类别`0`表示树是针叶树物种，而输出类别`1`表示树是落叶树物种。
- en: 'Now, let''s go through the steps for building and training a Keras model to
    perform the classification:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们逐步了解如何构建和训练一个Keras模型以执行分类任务：
- en: 'Set a seed in `numpy` and `tensorflow` and define your model as a Keras sequential
    model. `Sequential` models are, in fact, stacks of layers. After defining the
    model, we can add as many layers to it as desired:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`numpy`和`tensorflow`中设置种子，并将模型定义为Keras的顺序模型。`Sequential`模型实际上是各层的堆叠。定义模型后，我们可以根据需要向其中添加任意数量的层：
- en: '[PRE3]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Add one hidden layer of size `10` with an activation function of type `tanh`
    to your model (remember that the input dimension is equal to `10`). There are
    different types of layers available in Keras. For now, we will use only the simplest
    type of layer, called the `Dense` layer. A Dense layer is equivalent to the `fully
    connected layers` that we have seen in all the examples so far:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向模型中添加一个大小为`10`的隐藏层，激活函数类型为`tanh`（记住输入维度为`10`）。Keras中有多种类型的层。现在，我们只使用最简单的层类型——`Dense`层。Dense层相当于我们之前见过的`全连接层`：
- en: '[PRE4]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Add another hidden layer, this time of size `5` and with an activation function
    of type `tanh`, to your model. Please note that the input dimension argument is
    only provided for the first layer since the input dimension for the next layers
    is known:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向模型中添加另一个隐藏层，这次层的大小为`5`，激活函数为`tanh`。请注意，输入维度参数仅在第一层提供，因为后续层的输入维度是已知的：
- en: '[PRE5]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Add the output layer with the `sigmoid` activation function. Please note that
    the number of units in the output layer is equal to the output dimension:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加带有`sigmoid`激活函数的输出层。请注意，输出层的单元数量等于输出维度：
- en: '[PRE6]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Ensure that the loss function is binary cross-entropy and that the optimizer
    is `SGD` for training the model using the `compile()` method and print out a summary
    of the model to see its architecture:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`compile()`方法确保损失函数为二元交叉熵，并将优化器设置为`SGD`以训练模型，并打印出模型的摘要以查看其架构：
- en: '[PRE7]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following image shows the output of the preceding code:'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下图显示了前面代码的输出：
- en: '![Figure 3.9: A summary of the model that was created'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.9：创建的模型概述'
- en: '](img/B15777_03_09.jpg)'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15777_03_09.jpg)'
- en: 'Figure 3.9: A summary of the model that was created'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.9：创建的模型概述
- en: 'Train your model for `100` epochs and set a `batch_size` equal to 5 and a `validation_split`
    equal to `0.2`, and then set `shuffle` equal to `false` using the `fit()` method.
    Remember that you need to pass the input data, `X`, and its corresponding outputs,
    `y`, to the `fit()` method to train the model. Also, keep in mind that training
    a network may take a long time, depending on the size of the dataset, the size
    of the network, the number of epochs, and the number of CPUs or GPUs available.
    Save the results to a variable named `history`:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型`100`个周期，并将`batch_size`设置为5，`validation_split`设置为`0.2`，然后使用`fit()`方法将`shuffle`设置为`false`。记住，你需要将输入数据`X`及其对应的输出`y`传递给`fit()`方法以训练模型。此外，请记住，训练一个网络可能需要很长时间，这取决于数据集的大小、网络的大小、周期的数量以及可用的CPU或GPU的数量。将结果保存到名为`history`的变量中：
- en: '[PRE8]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `verbose` argument can take any of these three values: `0`, `1`, or `2`.
    By choosing `verbose=0`, no information will be printed during training. `verbose=1`
    will print a full progress bar at every iteration, while `verbose=2` will print
    only the epoch number:'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`verbose` 参数可以取以下三个值：`0`、`1` 或 `2`。选择 `verbose=0` 时，训练过程中不会打印任何信息。`verbose=1`
    会在每次迭代时打印完整的进度条，而 `verbose=2` 只会打印周期号：'
- en: '![Figure 3.10: The loss details of the last 5 epochs out of 400'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.10：最后 5 个周期（共 400 个周期）的损失详情'
- en: '](img/B15777_03_10.jpg)'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15777_03_10.jpg)'
- en: 'Figure 3.10: The loss details of the last 5 epochs out of 400'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.10：最后 5 个周期（共 400 个周期）的损失详情
- en: 'Print the accuracy and loss of the model on the training and validation data
    as a function of the epoch:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印模型在训练和验证数据上的准确率和损失，作为周期的函数：
- en: '[PRE9]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following image shows the output of the preceding code:'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下图像展示了前面代码的输出：
- en: '![Figure 3.11: The model''s accuracy and loss as a function of an epoch during'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.11：模型的准确率和损失作为训练周期的函数'
- en: the training process
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练过程
- en: '](img/B15777_03_11.jpg)'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15777_03_11.jpg)'
- en: 'Figure 3.11: The model''s accuracy and loss as a function of an epoch during
    the training process'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.11：模型的准确率和损失作为训练过程中的周期函数
- en: 'Use your trained model to predict the output class for the first 10 input data
    examples (`X.iloc[0:10,:]`):'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你训练好的模型预测前 10 个输入数据示例的输出类别（`X.iloc[0:10,:]`）：
- en: '[PRE10]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can print the predicted classes using the following code block:'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以使用以下代码块打印预测的类别：
- en: '[PRE11]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Expected output**:'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出**：'
- en: '[PRE12]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, we used the trained model to predict the output for the first 10 tree
    species in the dataset. As you can see, the model predicted that the second, fourth,
    fifth, and tenth trees were predicted as the species of class 1, which is deciduous.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，我们使用训练好的模型预测数据集中前 10 个树种的输出。如你所见，模型预测第二、第四、第五和第十棵树为类别 1 的树种，即**落叶树**。
- en: Note
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2YX3fxX](https://packt.live/2YX3fxX).
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/2YX3fxX](https://packt.live/2YX3fxX)。
- en: You can also run this example online at [https://packt.live/38pztVR](https://packt.live/38pztVR).
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，网址是 [https://packt.live/38pztVR](https://packt.live/38pztVR)。
- en: Please note that you can extend these steps by adding more hidden layers to
    your network. In fact, you can add as many layers as you want to your model before
    adding the output layer. However, the input dimension argument is only provided
    for the first layer since the input dimension for the next layers is known. Now
    that you have learned how to implement a neural network in Keras, you are ready
    to practice with them further by implementing a neural network that can perform
    classification in the following activity.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你可以通过向网络添加更多的隐藏层来扩展这些步骤。实际上，在添加输出层之前，你可以根据需要向模型添加任意数量的层。然而，输入维度参数仅提供给第一层，因为后续层的输入维度是已知的。现在，你已经学会了如何在
    Keras 中实现神经网络，你可以通过在以下活动中实现一个能够执行分类的神经网络来进一步实践。
- en: 'Activity 3.01: Building a Single-Layer Neural Network for Performing Binary
    Classification'
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 3.01：构建单层神经网络进行二分类
- en: In this activity, we will use a Keras sequential model to build a binary classifier.
    The simulated dataset provided represents the testing results of the production
    of aircraft propellers. Our target variable will be the results of the manual
    inspection of the propellers, designated as either "pass" (represented as a value
    of 1) or "fail" (represented as a value of 0).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将使用 Keras 顺序模型构建一个二分类器。提供的模拟数据集表示飞机螺旋桨生产的测试结果。我们的目标变量将是螺旋桨的人工检验结果，标记为“通过”（表示为值
    1）或“失败”（表示为值 0）。
- en: Our goal is to classify the testing results into either "pass" or "fail" classes
    to match the manual inspections. We will use models with different architectures
    and observe the visualization of the different models' performance. This will
    help you gain a better sense of how going from one processing unit to a layer
    of processing units changes the flexibility and performance of the model.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将测试结果分类为“通过”或“失败”类别，以匹配人工检验结果。我们将使用不同架构的模型，并观察不同模型性能的可视化效果。这将帮助你更好地理解从一个处理单元到一层处理单元的变化如何影响模型的灵活性和性能。
- en: 'Assume that this dataset contains two features representing the test results
    of two different tests inspecting the aircraft propellers of over `3000` propellers
    (the two features are normalized to have a mean of zero). The output is the likelihood
    of the propeller passing the test, with 1 representing a pass and zero representing
    a fail. The company would like to rely less on time-consuming, error-prone manual
    inspections of the aircraft propellers and shift resources to developing automated
    tests to assess the propellers faster. Therefore, the goal is to build a model
    that can predict whether an aircraft propeller will pass the manual inspection
    when given the results from the two tests. In this activity, you will first build
    a logistic regression model, then a single-layer neural network with three units,
    and finally a single-layer neural network with six units, to perform the classification.
    Follow these steps to complete this activity:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 假设该数据集包含两个特征，代表检查超过`3000`个螺旋桨的两项不同测试的结果（这两个特征已归一化，使其均值为零）。输出是螺旋桨通过测试的可能性，1表示通过，0表示未通过。公司希望减少对耗时且容易出错的人工检查的依赖，将资源转移到开发自动化测试上，以便更快地评估螺旋桨。因此，目标是建立一个模型，当给定两项测试的结果时，能够预测飞机螺旋桨是否能够通过人工检查。在这个活动中，你将首先建立一个逻辑回归模型，然后建立一个具有三单位的单层神经网络，最后建立一个具有六单位的单层神经网络来执行分类任务。请按照以下步骤完成此活动：
- en: 'Import the required packages:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的包：
- en: '[PRE13]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'You will need to download the `utils.py` file from the GitHub repository and
    save it into your activity folder in order for the utils import statement to work
    correctly. You can find the file here: [https://packt.live/31EumPY](https://packt.live/31EumPY).'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你需要从GitHub仓库下载`utils.py`文件并将其保存在你的活动文件夹中，才能使utils导入语句正常工作。你可以在这里找到该文件：[https://packt.live/31EumPY](https://packt.live/31EumPY)。
- en: 'Set up a seed for a random number generator so that the results will be reproducible:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为随机数生成器设置种子，以便结果是可重复的：
- en: '[PRE14]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The triple-quotes ( `"""` ) shown in the code snippet above are used to denote
    the start and end points of a multi-line code comment. Comments are added into
    code to help explain specific bits of logic.
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码片段中的三引号（`"""`）用于表示多行代码注释的开始和结束。注释被添加到代码中，以帮助解释特定的逻辑部分。
- en: 'Load the dataset using the `read_csv` function from the `pandas` library. Print
    the `X` and `Y` sizes and the number of examples in the training dataset using
    `feats.shape`, `target.shape`, and `feats.shape[0]`:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas`库中的`read_csv`函数加载数据集。使用`feats.shape`、`target.shape`和`feats.shape[0]`打印`X`和`Y`的大小，以及训练数据集中示例的数量：
- en: '[PRE15]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Plot the dataset using the following code:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码绘制数据集：
- en: '[PRE16]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Implement a logistic regression model as a sequential model in Keras. Remember
    that the activation function for binary classification needs to be sigmoid.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Keras中实现一个逻辑回归模型，作为一个顺序模型。记住，二分类的激活函数需要使用`sigmoid`。
- en: Train the model with `optimizer='sgd'`, `loss='binary_crossentropy'`, `batch_size
    = 5`, `epochs = 100`, and `shuffle=False`. Observe the loss values in each iteration
    by using `verbose=1` and `validation_split=0.2`.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`optimizer='sgd'`、`loss='binary_crossentropy'`、`batch_size = 5`、`epochs =
    100`和`shuffle=False`训练模型。通过设置`verbose=1`和`validation_split=0.2`，观察每次迭代的损失值。
- en: 'Plot the decision boundary of the trained model using the following code:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码绘制训练模型的决策边界：
- en: '[PRE17]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Implement a single-layer neural network with three nodes in the hidden layer
    and the `ReLU activation function` for `200` epochs. It is important to remember
    that the activation function for the output layer still needs to be sigmoid since
    it is a binary classification problem. Choosing `ReLU` or having no activation
    function for the output layer will not produce outputs that can be interpreted
    as class labels. Train the model with `verbose=1` and observe the loss in every
    iteration. After the model has been trained, plot the decision boundary and evaluate
    the loss and accuracy on the test dataset.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个具有三节点隐藏层的单层神经网络，并使用`ReLU激活函数`进行`200`个epoch的训练。重要的是要记住，输出层的激活函数仍然需要是sigmoid，因为这是一个二分类问题。如果选择`ReLU`或者没有输出层激活函数，将无法得到可以解释为类别标签的输出。使用`verbose=1`训练模型，并观察每次迭代的损失。在模型训练完成后，绘制决策边界，并在测试数据集上评估损失和准确度。
- en: Repeat *step 8* for the hidden layer of `size 6` and `400` epochs and compare
    the final loss value and the decision boundary plot.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤 8*，使用`大小为6`的隐藏层和`400`个epoch，并比较最终的损失值和决策边界图。
- en: Repeat *steps 8* and *9* using the `tanh` activation function for the hidden
    layer and compare the results with the models with `relu` activation. Which activation
    function do you think is a better choice for this problem?
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤8*和*步骤9*，使用`tanh`激活函数为隐藏层进行训练，并将结果与使用`relu`激活函数的模型进行比较。你认为哪种激活函数更适合这个问题？
- en: Note
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 362.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第362页找到。
- en: In this activity, you observed how stacking multiple processing units in a layer
    can create a much more powerful model than a single processing unit. This is the
    basic reason why neural networks are such powerful models. You also observed that
    increasing the number of units in the layer increases the flexibility of the model,
    meaning a non-linear separating decision boundary can be estimated more precisely.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，你观察到在一个层中堆叠多个处理单元，能够创建一个比单个处理单元强大的模型。这是神经网络如此强大的基本原因。你还观察到，增加层中单元的数量可以提高模型的灵活性，这意味着可以更精确地估计非线性分隔决策边界。
- en: However, a model with more processing units takes longer to learn the patterns,
    requires more epochs to be trained, and can overfit the training data. As such,
    neural networks are computationally expensive models. You also observed that using
    the tanh activation function results in a slower training process in comparison
    to using the `ReLU activation function`.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，拥有更多处理单元的模型需要更长的时间来学习模式，需要更多的训练周期，并且可能会过拟合训练数据。因此，神经网络是计算开销较大的模型。你还观察到，与使用`ReLU激活函数`相比，使用tanh激活函数的训练过程较慢。
- en: In this section, we created various models and trained them on our data. We
    observed that some models performed better than others by evaluating them on the
    data that they were trained on. In the next section, we learn about some alternative
    methods we can use to evaluate our models that provide an unbiased evaluation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们创建了多种模型，并在我们的数据上对其进行了训练。我们通过在训练数据上评估它们，观察到一些模型比其他模型表现更好。在下一节中，我们将了解一些可以用来评估模型的替代方法，这些方法提供了无偏的评估。
- en: Model Evaluation
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估
- en: In this section, we will move on to multi-layer or deep neural networks while
    learning about techniques for assessing the performance of a model. As you may
    have already realized, there are many hyperparameter choices to be made when building
    a deep neural network.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将进入多层或深度神经网络的学习，同时了解评估模型性能的技术。正如你可能已经意识到的，构建深度神经网络时需要做出许多超参数选择。
- en: Some of the challenges of applied deep learning include how to find the right
    values for the number of hidden layers, the number of units in each hidden layer,
    the type of activation function to use for each layer, and the type of optimizer
    and loss function for training the network. Model evaluation is required when
    making these decisions. By performing model evaluation, you can say whether a
    specific deep architecture or a specific set of hyperparameters is working poorly
    or well on a particular dataset, and therefore decide whether to change them or
    not.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 应用深度学习的一些挑战包括如何找到隐藏层数量、每个隐藏层中的单元数量、每层使用的激活函数类型，以及用于训练网络的优化器和损失函数的类型。做出这些决策时需要进行模型评估。通过执行模型评估，你可以判断特定的深度架构或一组特定的超参数在某个数据集上是表现不佳还是表现良好，从而决定是否进行更改。
- en: Furthermore, you will learn about `overfitting` and `underfitting`. These are
    two very important issues that can arise when building and training deep neural
    networks. Understanding the concepts of overfitting and underfitting and whether
    they are happening in practice is essential when it comes to finding the right
    deep neural network for a particular problem and improving its performance as
    much as possible.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你将学习关于`过拟合`和`欠拟合`的知识。这是构建和训练深度神经网络时可能出现的两个非常重要的问题。理解过拟合和欠拟合的概念，并判断它们在实际中是否发生，对于找到适合特定问题的深度神经网络并尽可能提高其性能至关重要。
- en: Evaluating a Trained Model with Keras
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Keras评估已训练的模型
- en: In the previous activity, we plotted the decision boundary of the model by predicting
    the output for every possible value of the input. Such visualization of model
    performance was possible because we were dealing with two-dimensional input data.
    The number of features or measurements in the input space is almost always way
    more than two, and so visualization by 2D plotting is not an option. One way to
    figure out how well a model is doing on a particular dataset is to compute the
    overall loss when predicting outputs for many examples. This can be done by using
    the `evaluate()` method in Keras, which receives a set of inputs (`X`) and their
    corresponding outputs (`y`), and calculates and returns the overall loss of the
    model on the inputs, `X`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的活动中，我们通过预测每个可能输入值的输出，绘制了模型的决策边界。之所以能进行这种模型性能可视化，是因为我们处理的是二维输入数据。输入空间中的特征或度量通常远多于两个，因此不能通过二维绘图进行可视化。评估模型在特定数据集上的表现的一种方法是，在多个示例上计算总体损失。这可以通过在Keras中使用`evaluate()`方法实现，该方法接收一组输入（`X`）及其对应的输出（`y`），然后计算并返回模型在输入数据`X`上的整体损失。
- en: 'For example, let''s consider a case of building a neural network with two hidden
    layers of sizes `8` and `4`, respectively, in order to perform binary or two-class
    classification. The available data points and their corresponding class labels
    are stored in `X`, `y` arrays. We can build and train the mentioned model as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们要构建一个具有两个隐藏层（大小分别为`8`和`4`）的神经网络，以进行二分类任务。可用的数据点及其对应的类别标签存储在`X`、`y`数组中。我们可以按如下方式构建并训练上述模型：
- en: '[PRE18]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, instead of using `model.predict()` to predict the output for a given set
    of inputs, we can evaluate the overall performance of the model by calculating
    the loss on the whole dataset by writing the following:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过计算整个数据集上的损失来评估模型的总体性能，而不是使用`model.predict()`预测给定输入集的输出，方法如下所示：
- en: '[PRE19]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If you include other metrics, such as accuracy, when defining the `compile()`
    method for the model, the `evaluate()` method will return those metrics along
    with the loss when it is called. For example, if we add metrics to the `compile()`
    arguments, as shown in the following code, then calling the `evaluate()` method
    will return the overall loss and the overall accuracy of the trained model on
    the whole dataset:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在定义模型的`compile()`方法时加入了其他度量指标，如准确率，那么调用`evaluate()`方法时将返回这些度量指标和损失。例如，如果我们在`compile()`参数中加入度量指标，如以下代码所示，那么调用`evaluate()`方法时将返回训练模型在整个数据集上的总体损失和总体准确率：
- en: '[PRE20]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can check out all the possible options for the `metrics` argument in Keras
    here: [https://keras.io/metrics/](https://keras.io/metrics/).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在Keras中查看`metrics`参数的所有可能选项，网址：[https://keras.io/metrics/](https://keras.io/metrics/)。
- en: In the next section, we will learn about splitting the dataset into training
    and test datasets. Much like we did in *Chapter 1*, *Introduction to Machine Learning
    with Keras*, training on separate data for evaluation can provide an unbiased
    evaluation of your model's performance.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，我们将学习如何将数据集划分为训练集和测试集。就像在*第1章*《Keras机器学习简介》中所做的那样，使用独立数据进行训练和评估可以提供模型性能的无偏估计。
- en: Splitting Data into Training and Test Sets
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据划分为训练集和测试集
- en: In general, evaluating a model on the same dataset that has been used for training
    the model is a methodological mistake. Since the model has been trained to reduce
    the errors on this dataset, performing an evaluation on it will result in a biased
    estimation of the model performance. In other words, the error rate on the dataset
    that has been used for training is always an underestimation of the error rate
    on new unseen examples.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，在用于训练模型的同一数据集上评估模型是一个方法上的错误。由于模型已针对该数据集进行训练并减少了误差，在该数据集上进行评估会导致模型性能的偏估计。换句话说，在用于训练的数据集上的误差率总是低估了新数据集上的误差率。
- en: On the other hand, when building a machine learning model, the goal is not to
    achieve good performance on the training data only, but to achieve good performance
    on future examples that the model has not seen during training. That is why we
    are interested in evaluating the performance of a model using a dataset that has
    not been used for training the model.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在构建机器学习模型时，目标不仅仅是在训练数据上取得良好表现，更是要在模型未曾见过的未来数据上取得良好表现。这就是为什么我们希望使用未曾用于训练的测试数据集来评估模型性能。
- en: 'One way to achieve this is to split the available dataset into two sets: a
    training set and a test set. The training set is used to train the model, while
    the test set is used for performance evaluation. More precisely, the role of the
    training set is to provide enough examples for the model that it will learn the
    relations and patterns in the data, while the role of the test set is to provide
    us with an unbiased estimation of the model performance on new unseen examples.
    The common practice in machine learning is to perform `70%-30%` or `80%-20%` splitting
    for training-test sets. This is usually the case for relatively small datasets.
    When dealing with a dataset with millions of examples in which the goal is to
    train a large deep neural network, the training-test splitting can be done using
    `98%-2%` or `99%-1%` ratios.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一目标的一种方式是将可用的数据集拆分为两个集合：训练集和测试集。训练集用于训练模型，而测试集用于性能评估。更准确地说，训练集的作用是为模型提供足够的示例，使其能够学习数据中的关系和模式，而测试集的作用是为我们提供对模型在新未见数据上的表现的无偏估计。机器学习中的常见做法是进行
    `70%-30%` 或 `80%-20%` 的训练集与测试集拆分。对于相对较小的数据集，通常是这种情况。当处理一个包含数百万示例的数据集，且目标是训练一个大型深度神经网络时，训练集与测试集的拆分比例可以使用
    `98%-2%` 或 `99%-1%`。
- en: 'The following image shows the division of a dataset into a training set and
    test set. Notice that there is no overlap between the `training` set and `test`
    set:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了数据集被拆分为训练集和测试集。请注意，`训练` 集和 `测试` 集之间没有重叠：
- en: '![Figure 3.12: Illustration of splitting a dataset into training and test sets'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.12：将数据集拆分为训练集和测试集的示意图'
- en: '](img/B15777_03_12.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_03_12.jpg)'
- en: 'Figure 3.12: Illustration of splitting a dataset into training and test sets'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12：将数据集拆分为训练集和测试集的示意图
- en: 'You can easily perform splitting on your dataset using scikit-learn''s `train_test_split`
    function. For example, the following code will perform a `70%-30%` training-test
    split on the dataset:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 scikit-learn 的 `train_test_split` 函数轻松地对数据集进行拆分。例如，以下代码将对数据集进行 `70%-30%`
    的训练集与测试集拆分：
- en: '[PRE21]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `test_size` argument represents the proportion of the dataset to be kept
    in the test set, so it should be between `0` and `1`. By assigning an `int` to
    the `random_state` argument, you can choose the seed to be used to generate the
    random split between the training and test sets.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`test_size` 参数表示要保留在测试集中的数据集比例，因此它应该在 `0` 和 `1` 之间。通过为 `random_state` 参数分配一个整数，您可以选择用于生成训练集和测试集之间随机拆分的种子。'
- en: 'After splitting the data into training and test sets, we can change the code
    from the previous section by providing only the training set as an argument to
    `fit()`:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据拆分为训练集和测试集后，我们可以通过只将训练集作为参数传递给 `fit()` 来更改前一节的代码：
- en: '[PRE22]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, we can compute the model error rate on the training set and the test set separately:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以分别计算模型在训练集和测试集上的误差率：
- en: '[PRE23]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Another way of doing the splitting is by including the `validation_split` argument
    for the `fit()` method in Keras. For example, by only changing the `model.fit(X,
    y)` line in the code from the previous section to `model.fit(X, y, validation_split=0.3)`,
    the model will keep the last 30% of the data examples in a separate test set.
    It will only train the model on the other 70% of the samples, and it will evaluate
    the model on the training set and the test set at the end of each epoch. In doing
    so, it would be possible to observe the changes in the training error rate, as
    well as the test error rate as the training progresses.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种拆分方法是在 Keras 的 `fit()` 方法中包含 `validation_split` 参数。例如，通过仅将前一节代码中的 `model.fit(X,
    y)` 行更改为 `model.fit(X, y, validation_split=0.3)`，模型将把数据集的最后 30% 保留在单独的测试集上。它只会在其余
    70% 的样本上训练模型，并且在每个周期结束时会在训练集和测试集上评估模型。这样，您可以观察到训练误差率和测试误差率在训练过程中是如何变化的。
- en: The reason that we want to have an unbiased evaluation of our model is so that
    we can see where there is room for improvement. Since neural networks have so
    many parameters to learn and can learn complex functions, they can often overfit
    to the training data and learn the noise in the training data, which can prevent
    the model from performing well on new, unseen data. The next section will explore
    these concepts in detail.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望对模型进行无偏评估的原因是，这样我们可以看到哪里有改进的空间。由于神经网络有许多参数需要学习，并且能够学习复杂的函数，它们经常会过拟合训练数据，学习到训练数据中的噪声，这可能导致模型在新的、未见过的数据上表现不佳。下一节将详细探讨这些概念。
- en: Underfitting and Overfitting
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欠拟合与过拟合
- en: In this section, you will learn about two issues you may face when building
    a machine learning model that needs to fit into a dataset. These issues are called
    **overfitting** and **underfitting** and are similar to the concepts of **bias**
    and **variance** for a model.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习构建需要适配数据集的机器学习模型时可能面临的两个问题。这些问题被称为**过拟合**和**欠拟合**，与模型的**偏差**和**方差**概念类似。
- en: In general, if a model is not flexible enough to learn the relations and patterns
    in a dataset, there will be a high training error. We can call such a model a
    model with high bias. On the other hand, if a model is too flexible for a given
    dataset, it will learn the noise in the training data, as well as the relations
    and patterns in the data. Such a system will cause a large increase in the test
    error in comparison to the training error. We mentioned previously that it is
    always the case that the test error is slightly higher than the training error.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，如果一个模型不够灵活，无法学习数据集中的关系和模式，就会出现较高的训练误差。我们可以称这种模型为具有高偏差的模型。另一方面，如果一个模型对给定数据集过于灵活，它不仅会学习训练数据中的噪声，还会学习数据中的关系和模式。这样，系统的测试误差将大大高于训练误差。我们之前提到过，测试误差通常会略高于训练误差。
- en: However, having a large gap between the test error and the training error is
    an indicator of a system with high variance. In data analysis, neither of these
    situations (`high bias` and `high variance`) are desirable. In fact, the aim is
    to find the model with the lowest possible amount of bias and variance at the
    same time.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，测试误差和训练误差之间有较大差距通常是系统具有高方差的一个指示。在数据分析中，这两种情况（`高偏差`和`高方差`）都不是理想的。事实上，目标是找到一个具有最低偏差和方差的模型。
- en: 'For example, let''s consider a dataset that represents the normalized locations
    of the sightings of two species of butterfly, as shown in the following plot.
    The goal is to find a model that can separate these two species of the butterfly
    when given the location of their sighting. Clearly, the separating line between
    the two classes is not linear. Therefore, if we choose a simple model such as
    logistic regression (a neural network with one hidden layer of size one) to perform
    the classification on this dataset, we will get a linear separating line/decision
    boundary between two classes that is unable to capture the true pattern in the
    dataset:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个数据集，表示两种蝴蝶物种的观测位置归一化后的数据，如下图所示。目标是找到一个模型，当给定蝴蝶观测的位置时，可以将这两种蝴蝶物种分开。显然，这两类之间的分隔线不是线性的。因此，如果我们选择一个简单的模型，如逻辑回归（一个具有一个隐藏层且大小为一的神经网络），来对这个数据集进行分类，我们将得到一条线性分隔线/决策边界，而这条线无法捕捉数据集中的真实模式：
- en: '![Figure 3.13: Two-dimensional data points of two different classes'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.13：两类不同数据点的二维分布'
- en: '](img/B15777_03_13.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_03_13.jpg)'
- en: 'Figure 3.13: Two-dimensional data points of two different classes'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13：两类不同数据点的二维分布
- en: 'The following plot illustrates the decision boundary that''s achieved by such
    a model. By evaluating this model, it will be observed that the training error
    rate is high and that the test error rate is slightly higher than the training
    error. Having a high training error rate is indicative of a model with high bias
    while having a slight difference between the training error and test error is
    representative of a low-variance model. This is a clear case of underfitting;
    the model fails to fit the true separating line between the two classes:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了通过这种模型实现的决策边界。通过评估这个模型，我们会观察到训练误差率较高，并且测试误差率略高于训练误差。较高的训练误差率表明该模型具有高偏差，而训练误差和测试误差之间的微小差异则代表了一个低方差模型。这是一个典型的欠拟合案例；该模型未能拟合出两类之间的真实分隔线：
- en: '![Figure 3.14: Underfitting'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.14：欠拟合'
- en: '](img/B15777_03_14.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_03_14.jpg)'
- en: 'Figure 3.14: Underfitting'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.14：欠拟合
- en: 'If we increase the flexibility of the neural network by adding more layers
    to it and increase the number of units in each layer, we can train a better model
    and succeed in capturing the non-linearity in the decision boundary. Such a model
    can be seen in the following plot. This is a model with a low training error rate
    and low-test error rate (again, the test error rate is slightly higher than the
    training error rate). Having a low training error rate and a slight difference
    between the test error rate and the training error rate is indicative of a model
    with low bias and low variance. A model with low bias and low variance represents
    the right amount of fitting for a given dataset:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们通过增加神经网络的层数，并增加每一层的单元数来提高其灵活性，我们可以训练出更好的模型，并成功捕捉决策边界中的非线性。下图展示了这样的模型。这是一个低训练误差率和低测试误差率的模型（同样，测试误差率略高于训练误差率）。训练误差率低且测试误差率与训练误差率之间的差距较小，表明这是一个低偏差和低方差的模型。低偏差和低方差的模型代表了对给定数据集的适当拟合：
- en: '![Figure 3.15: Correct fit'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.15：正确拟合'
- en: '](img/B15777_03_15.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_03_15.jpg)'
- en: 'Figure 3.15: Correct fit'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.15：正确拟合
- en: 'But what will happen if we increase the flexibility of the neural network even
    more? By adding too much flexibility to the model, it will learn not only the
    patterns and relations in the training data but also the noise in them. In other
    words, the model will fit each individual training example as opposed to fitting
    only to the overall trends and relations in them. The following plot shows such
    a system. Evaluating this model will show a very low training error rate and a
    high-test error rate (with a large difference between the training error rate
    and test error rate). This is a model with low bias and high variance, and this
    situation is called overfitting:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果我们进一步增加神经网络的灵活性会发生什么呢？通过向模型添加过多的灵活性，它不仅会学习训练数据中的模式和关系，还会学习其中的噪声。换句话说，模型会对每个单独的训练示例进行拟合，而不是仅对数据中的整体趋势和关系进行拟合。下图展示了这样的系统。评估该模型时，会看到非常低的训练误差率和很高的测试误差率（训练误差率与测试误差率之间存在较大差距）。这是一个低偏差和高方差的模型，这种情况称为过拟合：
- en: '![Figure 3.16: Overfitting'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.16：过拟合'
- en: '](img/B15777_03_16.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_03_16.jpg)'
- en: 'Figure 3.16: Overfitting'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.16：过拟合
- en: Evaluating the model on both the training set and the test set and comparing
    their error rates provide valuable information on whether the current model is
    right for a given dataset. Also, in cases where the current model is not fitting
    the dataset correctly, it is possible to determine whether it is overfitting or
    underfitting to the data and change the model accordingly to find the right model.
    For example, if the model is underfitting, you can make the network larger. On
    the other hand, if the model is overfitting, you can reduce the overfitting by
    making the network smaller or providing more training data to it. There are many
    methods that can be implemented to prevent underfitting or overfitting in practice,
    one of which we will explore in the next section.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练集和测试集上评估模型并比较其误差率，可以提供有关当前模型是否适用于给定数据集的宝贵信息。此外，在当前模型没有正确拟合数据集的情况下，还可以确定它是否过拟合或欠拟合数据，并相应地更改模型以找到正确的模型。例如，如果模型出现欠拟合，您可以增大网络。另一方面，如果模型过拟合，您可以通过缩小网络或提供更多训练数据来减少过拟合。实际中有许多方法可以防止欠拟合或过拟合，其中一种方法将在下一节中探讨。
- en: Early Stopping
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提前停止
- en: Sometimes, the flexibility of a model is right for the dataset but overfitting
    or underfitting is still happening. This is because we are training the model
    for either too many iterations or too few iterations. When using an iterative
    optimizer such as `gradient descent`, the optimizer tries to fit the training
    data better and better in every iteration. Therefore, if we keep updating the
    parameters after the patterns in the data are learned, it will start fitting to
    the individual data examples.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，模型的灵活性可能适用于数据集，但仍然会出现过拟合或欠拟合的情况。这是因为我们训练模型的迭代次数太多或太少。当使用像`梯度下降`这样的迭代优化器时，优化器会在每次迭代中尽可能地拟合训练数据。因此，如果在数据模式已经学到之后继续更新参数，它将开始对个别数据样本进行拟合。
- en: 'By observing the training and test error rates in every iteration, it is possible
    to determine when the network is starting to overfit to the training data and
    stop the training process before this happens. Regions associated with `underfitting`
    and `overfitting` have been labeled on the following plot. The correct number
    of iterations for training the model can be determined from the region at which
    the test error rate has its lowest value. We labeled this region as the right
    fit on the plot and it can be seen that, in this region, both the `training error
    rate` and the `test error rate` are low:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察每次迭代中的训练误差率和测试误差率，可以确定网络何时开始过拟合训练数据，并在这种情况发生之前停止训练过程。下图标记了与`欠拟合`和`过拟合`相关的区域。可以从测试误差率最低值的区域确定训练模型的正确迭代次数。我们将该区域标记为“合适的拟合”区域，可以看到在该区域内，`训练误差率`和`测试误差率`都很低：
- en: '![Figure 3.17: Plot of training error rate and test error rate while training
    a model'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.17：训练模型时训练误差率和测试误差率的图表](img/B15777_03_17.jpg)'
- en: '](img/B15777_03_17.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_03_17.jpg)'
- en: 'Figure 3.17: Plot of training error rate and test error rate while training
    a model'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.17：训练模型时训练误差率和测试误差率的图表
- en: 'You can easily store the values for training loss and test loss in every epoch
    while training with Keras. To do this, you need to provide the test set as the
    `validation_data` argument when defining the `fit()` method for the model and
    store it in a `history` dictionary:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Keras训练时，你可以很容易地在每个epoch存储训练损失和测试损失的值。为此，你需要在定义模型的`fit()`方法时，提供测试集作为`validation_data`参数，并将其存储在一个`history`字典中：
- en: '[PRE24]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You can plot the values stored in `history` later to find the correct number
    of iterations to train your model with:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以稍后绘制存储在`history`中的值，来找到训练模型所需的正确迭代次数：
- en: '[PRE25]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In general, since deep neural networks are highly flexible models, the chance
    of overfitting happening is very high. There is a whole group of techniques, called
    **regularization** techniques, that have been developed to reduce overfitting
    in machine learning models in general, and deep neural networks in particular.
    You will learn more about these techniques in *Chapter 5*, *Improving Model Accuracy*.
    In the next activity, we will put our understanding into practice and attempt
    to find the optimal number of epochs to train for so as to prevent overfitting.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，由于深度神经网络是高度灵活的模型，因此过拟合的可能性很高。为了减少机器学习模型，特别是深度神经网络中的过拟合，已经开发了一整套称为**正则化**的技术。你将在*第五章*，《提高模型准确性》中学到更多关于这些技术的内容。在下一个活动中，我们将把我们的理解付诸实践，尝试找出训练的最佳epoch数量，以避免过拟合。
- en: 'Activity 3.02: Advanced Fibrosis Diagnosis with Neural Networks'
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 3.02：使用神经网络进行晚期纤维化诊断
- en: 'In this activity, you are going to use a real dataset to predict whether a
    patient has advanced fibrosis based on measurements such as age, gender, and BMI.
    The dataset consists of information for 1,385 patients who underwent treatment
    dosages for hepatitis C. For each patient, `28` different attributes are available,
    as well as a class label, which can only take two values: `1`, indicating advanced
    fibrosis, and `0`, indicating no indication of advanced fibrosis. This is a binary/two-class
    classification problem with an input dimension equal to `28`.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，你将使用一个真实的数据集，根据年龄、性别和BMI等测量数据预测患者是否患有晚期纤维化。该数据集包含1,385名接受过丙型肝炎治疗剂量的患者的信息。每个患者有`28`个不同的属性，并且有一个类别标签，只有两个值：`1`，表示晚期纤维化，和`0`，表示没有晚期纤维化的迹象。这是一个二分类问题，输入维度为`28`。
- en: 'In this activity, you will implement different deep neural network architectures
    to perform this classification. Plot the trends in the training error rates and
    test error rates and determine how many epochs the final classifier needs to be
    trained for:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，你将实现不同的深度神经网络架构来执行这个分类任务。绘制训练误差率和测试误差率的趋势图，并确定最终分类器需要训练的轮数：
- en: Note
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset that''s being used in this activity can be found here: [https://packt.live/39pOUMT](https://packt.live/39pOUMT).'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动使用的数据集可以在这里找到：[https://packt.live/39pOUMT](https://packt.live/39pOUMT)。
- en: '![Figure 3.18: Schematic view of the binary classifier for a diabetes diagnosis'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.18：糖尿病诊断的二分类器示意图](img/B15777_03_18.jpg)'
- en: '](img/B15777_03_18.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_03_18.jpg)'
- en: 'Figure 3.18: Schematic view of the binary classifier for a diabetes diagnosis'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.18：糖尿病诊断的二分类器示意图
- en: 'Follow these steps to complete this activity:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤完成这个活动：
- en: 'Import all the necessary dependencies. Load the dataset from the `data` subfolder
    of the `Chapter03` folder from GitHub:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必要的依赖项。从GitHub的`Chapter03`文件夹的`data`子文件夹中加载数据集：
- en: '[PRE26]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Print the number of examples in the dataset, the number of features available,
    and the possible values for the class labels.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印数据集中的示例数量、可用特征的数量以及类标签的可能值。
- en: Scale the data using the `StandardScalar` function from `sklearn.preprocessing`
    and split the dataset into the training set and test set with an `80:20` ratio.
    Then, print the number of examples in each set after splitting.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`sklearn.preprocessing`中的`StandardScalar`函数对数据进行缩放，并按`80:20`比例将数据集拆分为训练集和测试集。然后，在拆分后打印出每个集中的示例数量。
- en: 'Implement a shallow neural network with one hidden layer of size 3 and a `tanh`
    activation function to perform the classification. Compile the model with the
    following values for the hyperparameters: `optimizer = ''sgd'', loss = ''binary_crossentropy'',
    metrics = [''accuracy'']`'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个具有一个隐藏层（大小为3）并使用`tanh`激活函数的浅层神经网络来执行分类任务。使用以下超参数值来编译模型：`optimizer = 'sgd',
    loss = 'binary_crossentropy', metrics = ['accuracy']`
- en: 'Fit the model with the following hyperparameters and store the values for training
    error rate and test error rate during the training process: `batch_size = 20`,
    `epochs = 100`, `validation_split=0.1`, and `shuffle=False`.'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下超参数拟合模型，并在训练过程中存储训练错误率和测试错误率的值：`batch_size = 20`，`epochs = 100`，`validation_split=0.1`，`shuffle=False`。
- en: Plot the training error rate and test error rate for every epoch of training.
    Use the plot to determine at which epoch the network is starting to overfit to
    the dataset. Also, print the values of the best accuracy that were reached on
    the training set and on the test set, as well as the loss and accuracy that were
    evaluated on the test dataset.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制每个训练轮次的训练错误率和测试错误率。使用图表确定网络开始对数据集发生过拟合的轮次。同时，打印出在训练集和测试集上达到的最佳准确度值，以及在测试数据集上评估得到的损失和准确度。
- en: Repeat *steps 4* and *5* for a deep neural network with two hidden layers (the
    first layer of size 4 and the second layer of size 3) and a `'tanh'` activation
    function for both layers in order to perform the classification.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对具有两个隐藏层的深层神经网络（第一个隐藏层大小为4，第二个隐藏层大小为3）和`'tanh'`激活函数（两个层均使用）的*步骤 4*和*步骤 5*进行重复操作，以执行分类任务。
- en: Note
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 374.
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第374页找到。
- en: Please note that both models were able to achieve better `accuracy` on the `training`
    or `validation` set compared to the `test` set, and the `training error rate`
    kept decreasing when it was trained for a significant number of epochs. However,
    the `validation error rate` decreased during training to a certain value, and
    after that, it started increasing, which is indicative of `overfitting` to the
    `training` data. The maximum validation accuracy corresponds to the point on the
    plots where the validation loss is at its lowest and is truly representative of
    how well the model will perform on independent examples later.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，两个模型在`训练`集或`验证`集上的`准确度`都比在`测试`集上要好，并且在经过大量训练轮次后，`训练错误率`持续下降。然而，`验证错误率`在训练过程中下降到某个值后开始上升，这表明模型出现了对`训练`数据的`过拟合`。最大验证准确度对应于图中验证损失最低的点，是真正代表模型在未来独立样本上表现的指标。
- en: It can be seen from the results that the model with one hidden layer is able
    to reach a lower validation and `test error rate` in comparison to the two-layer
    models. From this, we may conclude that this model is the best match for this
    particular problem. The model with one hidden layer shows a large amount of bias,
    indicated by the large gap between the training and validation errors, and both
    were still decreasing, indicating that the model can be trained for more epochs.
    Lastly, it can be determined from the plots that we should stop training around
    the region where the validation error rate starts increasing to prevent the model
    from `overfitting` to the data points.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中可以看出，具有一个隐藏层的模型能够比两个隐藏层的模型达到更低的验证和`测试错误率`。由此我们可以得出结论，该模型最适合该特定问题。具有一个隐藏层的模型表现出较大的偏差，这体现在训练错误率和验证错误率之间的较大差距，且两者仍在下降，这表明该模型可以进行更多轮次的训练。最后，从图表中可以判断，应该在验证错误率开始上升的区域停止训练，以防止模型对数据点发生`过拟合`。
- en: Summary
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you extended your knowledge of deep learning, from understanding
    the common representations and terminology to implementing them in practice through
    exercises and activities. You learned how `forward propagation` in neural networks
    works and how it is used for predicting outputs, how the loss function works as
    a measure of model performance, and how backpropagation is used to compute the
    derivatives of loss functions with respect to model parameters.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你扩展了对深度学习的理解，从理解常见的表示法和术语，到通过练习和活动将其付诸实践。你学习了神经网络中`前向传播`的工作原理以及如何用于预测输出，了解了损失函数如何作为模型性能的衡量标准，并学习了如何利用反向传播计算损失函数相对于模型参数的导数。
- en: You also learned about gradient descent, which uses the gradients that are computed
    by `backpropagation` to gradually update the model parameters. In addition to
    basic theory and concepts, you implemented and trained both shallow and deep neural
    networks with Keras and utilized them to make predictions about the output of
    a given input.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 你还学习了梯度下降法，它利用`反向传播`计算出的梯度来逐步更新模型参数。除了基础的理论和概念，你还使用 Keras 实现并训练了浅层和深层神经网络，并利用它们对给定输入的输出进行预测。
- en: To evaluate your models appropriately, you split a dataset into a training set
    and a test set as an alternative approach to improving network evaluation and
    learned the reasons why evaluating a model on training examples can be misleading.
    This helped further your understanding of overfitting and underfitting that can
    happen when training a model. Finally, you utilized the training error rate and
    test error rate to detect overfitting and underfitting in a network and implemented
    early stopping in order to reduce overfitting in a network.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适当地评估你的模型，你将数据集分割为训练集和测试集，这是一种改进网络评估的替代方法，并学习了为什么仅在训练示例上评估模型可能会产生误导性结果。这有助于进一步理解在训练模型时可能出现的过拟合和欠拟合问题。最后，你利用训练误差率和测试误差率来检测网络中的过拟合和欠拟合，并实现了早停技术以减少网络中的过拟合。
- en: In the next chapter, you will learn about the Keras wrapper with `scikit-learn`
    and how to use it to further improve model evaluation by using resampling methods
    such as cross-validation. By doing this, you will learn how to find the best set
    of hyperparameters for a deep neural network.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，你将学习如何使用`scikit-learn`中的 Keras 封装器，以及如何通过使用如交叉验证等重采样方法进一步改进模型评估。通过这样做，你将学习如何为深度神经网络找到最佳的超参数设置。
