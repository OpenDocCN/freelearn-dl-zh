- en: '<html:html><html:head><html:title>Querying Our Data, Part 1 – Context Retrieval</html:title></html:head>
    <html:body><html:div class="epub-source"><html:h1 id="_idParaDest-133">Querying
    Our Data, Part 1 – Context Retrieval</html:h1> <html:div id="_idContainer071"><html:p>The
    focus of this chapter will be on understanding the querying capabilities of LlamaIndex
    in an RAG workflow. We’ll be covering the overall working of the querying system,
    mostly focusing on the retrieval capabilities of <html:span class="No-Break">the
    framework.</html:span></html:p> <html:p>Here are the main sections that will be
    covered in <html:span class="No-Break">this chapter:</html:span></html:p> <html:ul><html:li>Learning
    about query mechanics – <html:span class="No-Break">an overview</html:span></html:li>
    <html:li>Understanding the <html:span class="No-Break">basic retrievers</html:span></html:li>
    <html:li>Building more advanced <html:span class="No-Break">retrieval mechanisms</html:span></html:li>
    <html:li>Increasing efficiency with <html:span class="No-Break">asynchronous retrieval</html:span></html:li>
    <html:li>Working with metadata filters, tools, <html:span class="No-Break">and
    selectors</html:span></html:li> <html:li>Transforming queries and <html:span class="No-Break">generating
    sub-queries</html:span></html:li> <html:li>Understanding the concepts of dense
    and <html:span class="No-Break">sparse retrieval</html:span></html:li></html:ul>
    <html:a id="_idTextAnchor133"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Technical
    requirements</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-134">Technical requirements</html:h1> <html:div id="_idContainer071"><html:p>For
    this chapter, you will need to install the <html:code class="literal">Rank-BM25</html:code>
    package in your environment. You can find it <html:span class="No-Break">at</html:span>
    <html:a><html:span class="No-Break">https://pypi.org/project/rank-bm25/</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>Two additional integration
    packages are required to run the <html:span class="No-Break">sample code:</html:span></html:p>
    <html:ul><html:li><html:em class="italic">OpenAI Question</html:em> <html:span
    class="No-Break"><html:em class="italic">Generator</html:em></html:span> <html:span
    class="No-Break">:</html:span> <html:a><html:span class="No-Break">https://pypi.org/project/llama-index-question-gen-openai/</html:span></html:a></html:li>
    <html:li><html:em class="italic">BM25</html:em> <html:span class="No-Break"><html:em
    class="italic">Retriever</html:em></html:span> <html:span class="No-Break">:</html:span>
    <html:a><html:span class="No-Break">https://pypi.org/project/llama-index-retrievers-bm25/</html:span></html:a></html:li>
    <html:li>All the code samples for this chapter can be found in the ch6 subfolder
    of this book’s GitHub <html:span class="No-Break">repository:</html:span> <html:a><html:span
    class="No-Break">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:li></html:ul> <html:a id="_idTextAnchor134"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Learning
    about query mechanics – an overview</html:title></html:head> <html:body><html:div
    class="epub-source"><html:h1 id="_idParaDest-135">Learning about query mechanics
    – an overview</html:h1> <html:div id="_idContainer071"><html:p>In this chapter,
    we will finally begin to reap the fruits of our work so far. Document ingestion,
    parsing and segmenting, metadata extraction, and index building were all just
    preparatory steps for what we are about to <html:a id="_idIndexMarker516"></html:a>discuss:
    <html:strong class="bold">querying</html:strong> . At the heart of any RAG workflow
    is the idea of being able to bring relevant context into the prompt we use in
    the LLM query. So far, we have been concerned with constructing and organizing
    this context, but now, it is time to <html:a id="_idIndexMarker517"></html:a>use
    it and extract the best possible answers from our interactions with LLMs. In the
    following sections, we will discuss various techniques that LlamaIndex provides
    us for the query part. As usual, we will start with the simplest <html:a id="_idIndexMarker518"></html:a>query
    methods – called <html:em class="italic">naive</html:em> methods in jargon – and
    then discuss more advanced <html:span class="No-Break">query variants.</html:span></html:p>
    <html:p>First, we need to <html:a id="_idIndexMarker519"></html:a>understand the
    <html:a id="_idIndexMarker520"></html:a>typical steps in the query <html:a id="_idIndexMarker521"></html:a>process:
    <html:strong class="bold">retrieval</html:strong> , <html:strong class="bold">postprocessing</html:strong>
    , and <html:span class="No-Break"><html:strong class="bold">response synthesis</html:strong></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>In <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 3</html:em></html:span></html:a>
    <html:em class="italic">,</html:em> <html:em class="italic">Kickstarting Your
    Journey with LlamaIndex</html:em> , in the <html:em class="italic">Indexes</html:em>
    section, we discussed the simplest way to go through the three steps – using <html:code
    class="literal">QueryEngine</html:code> but built very simply by running <html:code
    class="literal">index.as_query_engine()</html:code> . This is very simple but
    not necessarily always effective as this <html:em class="italic">naive</html:em>
    way of querying an index is just the tip of the iceberg. We will now explore the
    three mechanisms individually and understand how they work and the customizable
    options <html:span class="No-Break">they offer.</html:span></html:p> <html:p>First,
    we’ll focus <html:span class="No-Break">on</html:span> <html:span class="No-Break"><html:strong
    class="bold">retrievers</html:strong></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:a id="_idTextAnchor135"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Understanding
    the basic retrievers</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-136">Understanding the basic retrievers</html:h1> <html:div id="_idContainer071">from
    llama_index.core import SummaryIndex, SimpleDirectoryReader documents = SimpleDirectoryReader("files").load_data()
    summary_index = SummaryIndex.from_documents(documents) retriever = summary_index.as_retriever(
        retriever_mode=''embedding'' ) result = retriever.retrieve("Tell me about
    ancient Rome") print(result[0].text) from llama_index.core import SummaryIndex,
    SimpleDirectoryReader from llama_index.core.retrievers import SummaryIndexEmbeddingRetriever
    documents = SimpleDirectoryReader("files").load_data() summary_index = SummaryIndex.from_documents(documents)
    retriever = SummaryIndexEmbeddingRetriever(     index=summary_index ) result =
    retriever.retrieve("Tell me about ancient Rome") print(result[0].text) VectorStoreIndex.as_retriever()
    SummaryIndex.as_retriever(retriever_mode = ''default'') SummaryIndex.as_retriever(retriever_mode=''embedding'')
    SummaryIndex.as_retriever(retriever_mode=''llm'') DocumentSummaryIndex.as_retriever(retriever_mode=''llm'')
    DocumentSummaryIndex.as_retriever(     retriever_mode=''embedding'' ) TreeIndex.as_retriever(retriever_mode=''select_leaf'').
    TreeIndex.as_retriever(     retriever_mode=''select_leaf_embedding'' ) TreeIndex.as_retriever(retriever_mode=''all_leaf'')
    TreeIndex.as_retriever(retriever_mode=''root'') KeywordTableIndex.as_retriever(retriever_mode=''default'')
    KeywordTableIndex.as_retriever(retriever_mode=''simple'') KeywordTableIndex.as_retriever(retriever_mode=''rake'')
    KnowledgeGraphIndex.as_retriever(retriever_mode=''keyword'') KnowledgeGraphIndex.as_retriever(
        retriever_mode=''embedding'' ) KnowledgeGraphIndex.as_retriever(retriever_mode=''hybrid'')
    import asyncio from llama_index.core import KeywordTableIndex from llama_index.core
    import SimpleDirectoryReader async def retrieve(retriever, query, label):     response
    = await retriever.aretrieve(query)     print(f"{label} retrieved {str(len(response))}
    nodes") async def main():     reader = SimpleDirectoryReader(''files'')     documents
    = reader.load_data()     index = KeywordTableIndex.from_documents(documents)     retriever1
    = index.as_retriever( retriever_mode=''default'' )     retriever2 = index.as_retriever(
            retriever_mode=''simple'' )     query = "Where is the Colosseum?"     await
    asyncio.gather(         retrieve(retriever1, query, ''<llm>''),         retrieve(retriever2,
    query, ''<simple>'')     ) asyncio.run(main()) <html:p><html:strong class="bold">Retrieval
    mechanisms</html:strong> are a central <html:a id="_idIndexMarker522"></html:a>element
    in any RAG system. Although they work in different ways, all <html:a id="_idIndexMarker523"></html:a>types
    of retrievers are based on the same principle: they browse an index and select
    the relevant nodes to build the necessary context. Each index type offers several
    retrieval modes, each providing different features and customization options.
    Regardless of the retriever type, the result that will be returned is in the form
    of a <html:code class="literal">NodeWithScore</html:code> object – a structure
    that combines a node with an associated score. The score can be useful further
    in the RAG flow because it allows us to sort the returned nodes according to their
    relevance. However, keep in mind that while all retrievers return <html:code class="literal">NodeWithScore</html:code>
    , not all of them <html:a id="_idIndexMarker524"></html:a>associate a specific
    <html:span class="No-Break">node score.</html:span></html:p> <html:p>As usual,
    LlamaIndex offers multiple alternatives to accomplish a task, so a retriever can
    be constructed in several <html:a id="_idIndexMarker525"></html:a>ways. The simplest
    path is direct construction from an <html:code class="literal">Index</html:code>
    object. Assuming that we have already dealt with document ingestion, the following
    code builds an index and then builds a retriever based on the structure of <html:span
    class="No-Break">the index:</html:span></html:p> <html:p>In the previous example,
    the generated retriever is of the <html:code class="literal">SummaryIndexRetriever</html:code>
    type. This is the default retriever for <html:span class="No-Break">this index.</html:span></html:p>
    <html:p>The second option is direct instantiation, as shown in the <html:span
    class="No-Break">following example:</html:span></html:p> <html:p>In the next section,
    we’ll go through a list of retrieval options that are available for each index
    type. Next to each retriever type, I’ve specified how it can be instantiated from
    the corresponding index. I warn you <html:a id="_idIndexMarker526"></html:a>now
    that a lot of information has been condensed in the next section. However, it
    is useful information that you can bookmark and come back to later when you start
    <html:a id="_idIndexMarker527"></html:a>building real applications with the <html:span
    class="No-Break">LlamaIndex framework.</html:span></html:p> <html:p>So, here’s
    the list of retrievers for each type <html:span class="No-Break">of index.</html:span></html:p>
    <html:a id="_idTextAnchor136"></html:a><html:h2 id="_idParaDest-137">The VectorStoreIndex
    retrievers</html:h2> <html:p>We have two retriever <html:a id="_idIndexMarker528"></html:a>options
    available for this index. Let’s have a look at how <html:a id="_idIndexMarker529"></html:a>they
    work and how to customize them for different <html:span class="No-Break">use cases.</html:span></html:p>
    <html:h3>VectorIndexRetriever</html:h3> <html:p>The default retriever that’s used
    by <html:code class="literal">VectorStoreIndex</html:code> is <html:code class="literal">VectorIndexRetriever</html:code>
    . It can easily be <html:a id="_idIndexMarker530"></html:a>constructed using the
    <html:span class="No-Break">following command:</html:span></html:p> <html:p>As
    expected, since <html:code class="literal">VectorStoreIndex</html:code> is one
    of the most sophisticated and widely used indexes, this retriever is <html:span
    class="No-Break">also complex.</html:span></html:p> <html:p><html:span class="No-Break"><html:em
    class="italic">Figure 6</html:em></html:span> <html:em class="italic">.1</html:em>
    exemplifies its <html:span class="No-Break">operating mode:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 6.1 – Node retrieval using VectorIndexRetriever</html:p>
    <html:p>This retriever operates by converting queries into vectors and then performing
    <html:em class="italic">similarity-based</html:em> searches in the vector <html:a
    id="_idIndexMarker531"></html:a>space. Several parameters can be customized for
    different <html:span class="No-Break">use cases:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">similarity_top_k</html:code> : This defines the number of <html:em
    class="italic">top (k)</html:em> results returned by the retriever. This determines
    how many of the most similar results are returned for each query. For example,
    if we want a broader search, we can change the default value, which <html:span
    class="No-Break">is</html:span> <html:span class="No-Break"><html:code class="literal">2</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:li> <html:li><html:code class="literal">vector_store_query_mode</html:code>
    : This sets the query mode of the vector store. Different variants of external
    vector stores, such as <html:em class="italic">Pinecone</html:em> ( <html:a>https://www.pinecone.io/</html:a>
    ), <html:em class="italic">OpenSearch</html:em> ( <html:a>https://opensearch.org/</html:a>
    ), and others, support different query modes. This is the mechanism by which we
    can make best use of their <html:span class="No-Break">search capabilities.</html:span></html:li>
    <html:li><html:code class="literal">filters</html:code> : Remember that in <html:span
    class="No-Break"><html:em class="italic">Chapter 3</html:em></html:span> , in
    the <html:em class="italic">Nodes</html:em> section, we saw how to add metadata
    to our nodes? Well, we can use this metadata to narrow down the search scope of
    the retriever. We will see a practical example of this in this chapter, where
    we will use metadata filters to implement a simple system for filtering nodes
    returned by <html:span class="No-Break">an index.</html:span></html:li> <html:li><html:code
    class="literal">alpha</html:code> : This one is useful when using a hybrid search
    mode (a combination of sparse and <html:strong class="bold">dense search</html:strong>
    ). We will <html:a id="_idIndexMarker532"></html:a>discuss the difference between
    sparse and dense search in more detail later in <html:span class="No-Break">this
    chapter.</html:span></html:li> <html:li><html:code class="literal">sparse_top_k</html:code>
    : The number of top <html:a id="_idIndexMarker533"></html:a>results for the <html:strong
    class="bold">sparse search</html:strong> . This is relevant in hybrid search modes.
    The previous mention applies <html:span class="No-Break">here also.</html:span></html:li>
    <html:li><html:code class="literal">doc_ids</html:code> : Similar to metadata
    filters, but slightly coarser, <html:code class="literal">doc_ids</html:code>
    can be used to restrict the search to a specific subset of documents. For example,
    suppose the organization uses a <html:a id="_idIndexMarker534"></html:a>common
    knowledge base that is shared by all departments. At the same time, however, the
    organization has a clear naming convention for documents. If the department’s
    name or code is found in the document name, we could use this parameter to limit
    a user’s query to documents in their <html:span class="No-Break">department only.</html:span></html:li>
    <html:li><html:code class="literal">node_ids</html:code> : This parameter is similar
    to <html:code class="literal">doc_ids</html:code> but refers to node IDs within
    the index. This can give us even more granular control over the information that’s
    returned by <html:span class="No-Break">the retriever.</html:span></html:li> <html:li><html:code
    class="literal">vector_store_kwargs</html:code> : This parameter can pass additional
    arguments that are specific to each vector store so that they can be sent at <html:span
    class="No-Break">query time.</html:span></html:li></html:ul> <html:p>As a secure
    design principle, security should be implemented as early as possible in the life
    cycle of an application. This is also true for an RAG application. For example,
    if we want to better control access to information, we should filter the information
    that’s processed by the application as early as possible. In an RAG flow, which
    means from the moment it is retrieved – if not earlier. There are ways to filter
    the information later in the query engine – for example, in post-processing or
    even in response synthesis – but it is much easier not to introduce risks in the
    first place by introducing information into the flow that is outside the user’s
    security context. There is also a cost issue. Since much of the processing in
    an RAG flow is based on LLM ingestion, the less information we process, the lower
    <html:span class="No-Break">the cost.</html:span></html:p> <html:h3>VectorIndexAutoRetriever</html:h3>
    <html:p>All the parameters we discussed earlier regarding <html:code class="literal">VectorIndexRetriever</html:code>
    are very useful when we know exactly what we are looking for and understand the
    structure of the data very well. Unfortunately, in some situations, we will be
    dealing with complex structures or ambiguities in the <html:span class="No-Break">indexed
    data.</html:span></html:p> <html:p><html:code class="literal">VectorIndexAutoRetriever</html:code>
    is a more advanced form of retriever that can use an LLM to automatically set
    query parameters in a vector store based on a natural language description of
    the content and supporting metadata. This is particularly useful when users are
    unfamiliar with the <html:a id="_idIndexMarker535"></html:a>structure of the data
    or do not know how to formulate an effective query. In these situations, this
    retriever can transform vague or unclear queries into more structured queries
    and better leverage the capabilities of the vector store, thus increasing the
    chances of finding relevant results. Since a detailed discussion of this mechanism
    would take several pages and I am probably digressing too much from the main topic,
    if you want to learn more about how it works, I suggest that you consult the official
    documentation <html:span class="No-Break">at</html:span> <html:a><html:span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/vector_stores/elasticsearch_auto_retriever.html</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:h3>The SummaryIndex retrievers</html:h3>
    <html:p>There are three retriever options available for this <html:a id="_idIndexMarker536"></html:a>index.
    Let’s take <html:span class="No-Break">a look.</html:span></html:p> <html:h3>SummaryIndexRetriever</html:h3>
    <html:p>This retriever can be built using the <html:a id="_idIndexMarker537"></html:a><html:span
    class="No-Break">following command:</html:span></html:p> <html:p>This is the default
    retriever for <html:code class="literal">SummaryIndex</html:code> . As seen in
    <html:span class="No-Break"><html:em class="italic">Figure 6</html:em></html:span>
    <html:em class="italic">.2</html:em> , it has a very simple approach – it returns
    all nodes in the index without applying any filtering <html:span class="No-Break">or
    sorting:</html:span></html:p> <html:p class="IMG---Caption" lang="en-US">Figure
    6.2 – Retrieving nodes using SummaryIndexRetriever</html:p> <html:p>This is useful
    when we want to <html:a id="_idIndexMarker538"></html:a>get a complete view of
    the data in the index, without having to filter or sort the results. No relevance
    score is returned for <html:span class="No-Break">the nodes.</html:span></html:p>
    <html:h3>SummaryIndexEmbeddingRetriever</html:h3> <html:p>We can build this one
    with the <html:a id="_idIndexMarker539"></html:a><html:span class="No-Break">following
    command:</html:span></html:p> <html:p>This retriever relies on embeddings to retrieve
    nodes from <html:code class="literal">SummaryIndex</html:code> . While <html:code
    class="literal">SummaryIndex</html:code> itself stores nodes in plain text, this
    retriever uses an embedding model to convert these plain text nodes into embeddings
    when a query is made. Have a look at <html:span class="No-Break"><html:em class="italic">Figure
    6</html:em></html:span> <html:em class="italic">.3</html:em> to get a better view
    of its <html:span class="No-Break">operating mode:</html:span></html:p> <html:p
    class="IMG---Caption" lang="en-US">Figure 6.3 – Inner workings of SummaryIndexEmbeddingRetriever</html:p>
    <html:p>The embeddings are created dynamically as needed for retrieval, rather
    than being stored persistently with the index. The <html:code class="literal">similarity_top_k</html:code>
    parameter determines the number of nodes <html:a id="_idIndexMarker540"></html:a>to
    return, based on their similarity to the query. This retriever is useful for finding
    the most relevant nodes concerning a given query by using <html:span class="No-Break">similarity
    computation.</html:span></html:p> <html:p>For each selected node, the retriever
    calculates a similarity score – based on embeddings – which is then returned alongside
    the node as <html:code class="literal">NodeWithScore</html:code> . This score
    is a reflection of the extent to which each node corresponds to <html:span class="No-Break">the
    query.</html:span></html:p> <html:h3>SummaryIndexLLMRetriever</html:h3> <html:p>This
    retriever can be built using the <html:a id="_idIndexMarker541"></html:a><html:span
    class="No-Break">following command:</html:span></html:p> <html:p>As its name suggests,
    this retriever uses an LLM to retrieve nodes from <html:code class="literal">SummaryIndex</html:code>
    . It uses a prompt to select the most relevant nodes. Check out <html:span class="No-Break"><html:em
    class="italic">Figure 6</html:em></html:span> <html:em class="italic">.4</html:em>
    for an overview of <html:span class="No-Break">its approach:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 6.4 – SummaryIndexLLMRetriever
    in action</html:p> <html:p>If we wish, we can override the default prompt using
    the <html:code class="literal">choice_select_prompt</html:code> parameter. Queries
    are processed in <html:a id="_idIndexMarker542"></html:a>batches; the size of
    each batch is determined by the <html:code class="literal">choice_batch_size</html:code>
    parameter. Optionally, we can also provide the <html:code class="literal">format_node_batch_fn</html:code>
    and <html:code class="literal">parse_choice_select_answer_fn</html:code> functions
    as parameters. These are used to format the batch of nodes and parse the LLM responses.
    The <html:code class="literal">parse_choice_select_answer_fn</html:code> function
    is also responsible for calculating node-specific relevance scores. The scores
    are determined by parsing the LLM responses. These scores are then associated
    with the corresponding nodes and returned as <html:code class="literal">NodeWithScore</html:code>
    . If we don’t want to use the default LLM, that’s not a problem: the retriever
    accepts <html:code class="literal">service_context</html:code> as a parameter.
    In <html:span class="No-Break">Chapter 3</html:span> , we saw how to customize
    the default LLM <html:span class="No-Break">using</html:span> <html:span class="No-Break"><html:code
    class="literal">ServiceContext</html:code></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p>This type of retriever is useful in complex search systems where LLMs
    can provide contextual and detailed answers <html:span class="No-Break">to queries.</html:span></html:p>
    <html:p>Next, we’ll talk about retrievers <html:span class="No-Break">for</html:span>
    <html:span class="No-Break"><html:code class="literal">DocumentSummaryIndex</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:a id="_idTextAnchor137"></html:a><html:h2
    id="_idParaDest-138">The DocumentSummaryIndex retrievers</html:h2> <html:p>For
    this index, we only have <html:a id="_idIndexMarker543"></html:a>two retrieval
    options. Let’s <html:a id="_idIndexMarker544"></html:a>take <html:span class="No-Break">a
    look.</html:span></html:p> <html:h3>DocumentSummaryIndexLLMRetriever</html:h3>
    <html:p>We can build this with the <html:span class="No-Break">following command:</html:span></html:p>
    <html:p>This retriever uses an <html:a id="_idIndexMarker545"></html:a>LLM to
    select relevant summaries from an index of document summaries. You can get a better
    understanding of how it works by looking at <html:span class="No-Break"><html:em
    class="italic">Figure 6</html:em></html:span> <html:span class="No-Break"><html:em
    class="italic">.5</html:em></html:span> <html:span class="No-Break">:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 6.5 – How DocumentSummaryIndexLLMRetriever
    works</html:p> <html:p>This retriever processes queries in batches, with each
    batch containing a specified number of nodes to send to the LLM for evaluation.
    The <html:code class="literal">choice_batch_size</html:code> parameter can be
    used to specify the size of a batch. The retriever can use a custom prompt provided
    via the <html:code class="literal">choice_select_prompt</html:code> parameter
    to determine the relevance of the abstracts to the query. Results are sorted by
    relevance and returned according to the number specified by <html:code class="literal">choice_top_k</html:code>
    . The <html:code class="literal">format_node_batch_fn</html:code> and <html:code
    class="literal">parse_choice_select_answer_fn</html:code> functions can also be
    specified as parameters. The first function, <html:code class="literal">format_node_batch_fn</html:code>
    , prepares the information from nodes in a format suitable for the LLM. This may
    include combining text from multiple nodes, structuring the information in a particular
    way, or adding contextual elements to help the LLM understand and evaluate the
    content. The second function, <html:code class="literal">parse_choice_select_answer_fn</html:code>
    , can, for example, determine which nodes are most relevant to the query and extract
    relevance scores or other metrics associated with each node. By analyzing the
    LLM response, this function allows the retriever to decide which nodes are most
    relevant to the user’s query. To summarize, <html:code class="literal">DocumentSummaryIndexLLMRetriever</html:code>
    is useful for retrieving useful data from a large number of documents using the
    natural language processing power of LLMs. As a useful side note, it is good to
    know that this retriever also returns the relevance score that is <html:a id="_idIndexMarker546"></html:a>associated
    with each of <html:span class="No-Break">the nodes.</html:span></html:p> <html:p
    class="callout-heading">Additional observation</html:p> <html:p class="callout">During
    my experimentation with this type of retriever, I noticed that the relevance scores
    that are assigned to each node by the LLM were consistently high, often reaching
    the maximum value of 10 (tested using GPT3.5-Turbo). For applications where nuanced
    differentiation between degrees of relevance is crucial, it might be beneficial
    to adjust the prompt or apply post-processing to the LLM’s responses to achieve
    a more balanced and nuanced distribution of relevance scores. This issue also
    underscores the importance of tailoring LLM prompts and response handling to suit
    the specific needs and contexts of different applications. We’ll talk more about
    prompt customization in <html:a><html:span class="No-Break"><html:em class="italic">Chapter
    10</html:em></html:span></html:a> <html:span class="No-Break">.</html:span></html:p>
    <html:h3>DocumentSummaryIndexEmbeddingRetriever</html:h3> <html:p>To build this
    retriever, we can use <html:a id="_idIndexMarker547"></html:a>the <html:span class="No-Break">following
    code:</html:span></html:p> <html:p>This retriever relies on embeddings to retrieve
    summary nodes from the index. <html:span class="No-Break"><html:em class="italic">Figure
    6</html:em></html:span> <html:em class="italic">.6</html:em> exemplifies <html:span
    class="No-Break">its operation:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 6.6 – DocumentSummaryIndexEmbeddingRetriever</html:p> <html:p>It
    computes the <html:a id="_idIndexMarker548"></html:a>embeddings for the query
    and then finds the summaries with the highest similarity to the query. For this
    method to work, the index should have been built with the <html:code class="literal">embed_summaries</html:code>
    parameters set to <html:code class="literal">True</html:code> . The <html:code
    class="literal">similarity_top_k</html:code> parameter specifies the number of
    summary nodes to return based on similarity. The retriever does not return a relevance
    score associated with <html:span class="No-Break">each node.</html:span></html:p>
    <html:p>It is effective for finding the most relevant summaries relative to a
    given query, using similarity calculation techniques based <html:span class="No-Break">on
    embeddings.</html:span></html:p> <html:a id="_idTextAnchor138"></html:a><html:h2
    id="_idParaDest-139">The TreeIndex retrievers</html:h2> <html:p>This is a more
    complex <html:a id="_idIndexMarker549"></html:a>index type that constructs a tree
    graph of nodes, as <html:a id="_idIndexMarker550"></html:a>we saw in <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 5</html:em></html:span></html:a>
    <html:em class="italic">, Indexing with LlamaIndex</html:em> , in the <html:em
    class="italic">Other index types in</html:em> <html:span class="No-Break"><html:em
    class="italic">LlamaIndex</html:em></html:span> <html:span class="No-Break">section.</html:span></html:p>
    <html:p class="callout-heading">Important note</html:p> <html:p class="callout"><html:code
    class="literal">TreeIndex</html:code> , by its very nature, is designed to reflect
    hierarchical relationships within data, making it a great tool for scenarios where
    data is naturally organized in a tree-like structure, such as filesystems, organizational
    charts, or product categories. That being said, the LlamaIndex implementation
    of this structure is a tree of summaries about the data. Regardless of any existing
    structure in the initial document, this index builds a parallel hierarchical structure
    by chunking it down and creating summaries at each level of the tree. Because
    of the recursive nature of <html:code class="literal">TreeSelectLeafRetriever</html:code>
    and <html:code class="literal">TreeSelectLeafEmbeddingRetriever</html:code> ,
    navigating this structure at query time could be more computationally expensive
    than with other types of indexes. This recursive process adds computational overhead,
    especially for deep trees or <html:span class="No-Break">large datasets.</html:span></html:p>
    <html:p>That being said, we have <html:a id="_idIndexMarker551"></html:a>several
    ways to <html:span class="No-Break">query</html:span> <html:span class="No-Break"><html:code
    class="literal">TreeIndex</html:code></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:h3>TreeSelectLeafRetriever</html:h3> <html:p>We can construct this retriever
    <html:span class="No-Break">like this:</html:span></html:p> <html:p>This is also
    the default retriever <html:a id="_idIndexMarker552"></html:a>that’s used by <html:code
    class="literal">TreeIndex</html:code> . Its purpose is to recursively navigate
    the index structure and identify the leaf nodes that are most relevant to the
    query being formulated. This can be seen in <html:span class="No-Break"><html:em
    class="italic">Figure 6</html:em></html:span> <html:span class="No-Break"><html:em
    class="italic">.7</html:em></html:span> <html:span class="No-Break">:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 6.7 – TreeSelectLeafRetriever
    configured with a child_branch_factor argument value of 1</html:p> <html:p>The
    <html:code class="literal">child_branch_factor</html:code> argument specifies
    the number of child nodes to be considered at each level of the tree. Setting
    a higher value can result in a more exhaustive search and increase the chance
    of finding the most relevant nodes. However, this has the disadvantage of increasing
    the computational cost and processing time. If no value is specified, the retriever
    defaults to a value of <html:code class="literal">1</html:code> . Another very
    useful parameter is <html:code class="literal">Verbose</html:code> , which, when
    set to <html:code class="literal">True</html:code> , causes the detailed selection
    process to be displayed. This is a very good way to understand how the retriever
    works or troubleshoot possible execution problems. The nodes that are returned
    by this retriever do not contain an associated <html:a id="_idIndexMarker553"></html:a>relevance
    score. As this retriever uses an LLM for node selection, several parameters can
    be used to customize <html:span class="No-Break">the prompts:</html:span></html:p>
    <html:ul><html:li><html:code class="literal">query_template</html:code> : This
    is a prompt template that we can use to customize queries for <html:span class="No-Break">the
    LLM</html:span></html:li> <html:li><html:code class="literal">text_qa_template</html:code>
    : This is another template that’s used for text-based Q&A queries. It is used
    to get specific answers from <html:span class="No-Break">text nodes</html:span></html:li>
    <html:li><html:code class="literal">refine_template</html:code> : This template
    is used to refine or enhance the initial answers that are obtained from the LLM.
    It can be used to add additional context or <html:span class="No-Break">clarify
    answers</html:span></html:li> <html:li><html:code class="literal">query_template_multiple</html:code>
    : An alternative prompt template that allows queries to be formulated for multiple
    nodes simultaneously. It is useful when using a <html:code class="literal">child_branch_factor</html:code>
    argument that’s higher <html:span class="No-Break">than 1</html:span></html:li></html:ul>
    <html:p>We’ll talk <html:span class="No-Break">about</html:span> <html:span class="No-Break"><html:code
    class="literal">TreeSelectEmbeddingRetriever</html:code></html:span> <html:span
    class="No-Break">.</html:span></html:p> <html:h3>TreeSelectLeafEmbeddingRetriever</html:h3>
    <html:p>This particular kind of retriever can be built using the <html:span class="No-Break">following
    code:</html:span></html:p> <html:p>As its name suggests, this retriever <html:a
    id="_idIndexMarker554"></html:a>navigates the index by using the similarity of
    the embeddings between the query and the node text to select the <html:span class="No-Break">relevant
    nodes.</html:span></html:p> <html:p>This process is recursive, navigating all
    levels of the tree. It works almost identically to <html:code class="literal">TreeSelectLeafRetriever</html:code>
    , with the only difference being that it uses embeddings for <html:span class="No-Break">node
    selection.</html:span></html:p> <html:p>The parameters we discussed earlier are
    also valid here, but there is an additional parameter: <html:code class="literal">embed_model</html:code>
    . This <html:a id="_idIndexMarker555"></html:a>can be used to specify a preferred
    embedding model. As with the previous retriever, the nodes that are returned by
    this retriever do not contain an associated <html:span class="No-Break">relevance
    score.</html:span></html:p> <html:h3>TreeAllLeafRetriever</html:h3> <html:p>Here’s
    the fastest way to <html:a id="_idIndexMarker556"></html:a>construct <html:span
    class="No-Break">this retriever:</html:span></html:p> <html:p>You can find an
    explanatory diagram in <html:span class="No-Break"><html:em class="italic">Figure
    6</html:em></html:span> <html:span class="No-Break"><html:em class="italic">.8</html:em></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 6.8 – Retrieving all nodes by using TreeAllLeafRetriever</html:p>
    <html:p>This retriever is useful for its ability to analyze a large amount of
    data, ensuring that no potentially relevant information is missed in the response
    generation process. In a similar way to <html:code class="literal">SummaryIndexRetriever</html:code>
    , this retriever extracts all nodes from the index and sorts them, regardless
    of their position in the hierarchy. This is akin to a bulk retrieval but without
    it returning any <html:span class="No-Break">relevance score.</html:span></html:p>
    <html:h3>TreeRootRetriever</html:h3> <html:p>We can build this with the <html:span
    class="No-Break">following command:</html:span></html:p> <html:p>Unlike <html:code
    class="literal">TreeAllLeafRetriever</html:code> , this retriever focuses on retrieving
    responses directly from the root nodes of the tree. It <html:a id="_idIndexMarker557"></html:a>assumes
    that the index tree already stores the response. Unlike other methods that might
    parse information down the tree to extract relevant nodes, <html:code class="literal">TreeRootRetriever</html:code>
    relies on the fact that the answer is already at the root level. <html:span class="No-Break"><html:em
    class="italic">Figure 6</html:em></html:span> <html:em class="italic">.9</html:em>
    provides a <html:span class="No-Break">visual explanation:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 6.9 – Retrieving from the root
    of the tree</html:p> <html:p>It is effective in cases where essential information
    is aggregated or synthesized at the top level of the data structure, such as data
    summaries, general conclusions, or answers to frequently asked questions. This
    retriever also does not return relevance scores associated <html:span class="No-Break">with
    nodes.</html:span></html:p> <html:p class="callout-heading">Practical use case</html:p>
    <html:p class="callout">A practical example would be a <html:strong class="bold">clinical
    decision support system</html:strong> ( <html:strong class="bold">CDSS</html:strong>
    ) in the medical field. Imagine such a <html:a id="_idIndexMarker558"></html:a>system
    having a <html:code class="literal">TreeIndex</html:code> retriever in which each
    root node represents a specific medical question and the corresponding answers
    or clinical advice are pre-computed and stored in these root nodes. For example,
    the root nodes may store a pre-computed answer such as <html:em class="italic">Common
    symptoms of COVID-19 include fever, dry cough, tiredness, and so on</html:em>
    . In this scenario, when a doctor or patient interrogates the system with the
    <html:em class="italic">Symptoms of a COVID-19 infection</html:em> query, this
    retriever will look at the appropriate root node and return the pre-computed answer
    without any additional <html:a id="_idIndexMarker559"></html:a>processing or having
    to traverse the tree to <html:span class="No-Break">find information.</html:span></html:p>
    <html:h3>The KeywordTableIndex retrievers</html:h3> <html:p>The retrieval process
    from <html:code class="literal">KeywordTableIndex</html:code> starts by extracting
    the relevant keywords from the query given to the <html:a id="_idIndexMarker560"></html:a>retriever.
    Extraction can be done in several ways, depending on the retriever being used.
    Once the keywords have been extracted, the retriever counts their frequency in
    the different indexed. All retrievers that are available for this index operate
    as described in <html:span class="No-Break"><html:em class="italic">Figure 6</html:em></html:span>
    <html:em class="italic">.10</html:em> . The only difference is the method that’s
    used to extract <html:span class="No-Break">the keywords:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 6.10 – KeywordTableIndex</html:p>
    <html:p>The nodes are sorted by the number of matching keywords, usually in descending
    order of relevance, and returned as a <html:span class="No-Break"><html:code class="literal">NodeWithScore</html:code></html:span>
    <html:span class="No-Break">response.</html:span></html:p> <html:p>It’s worth
    noting that queries against this type of index do not return a relevance score
    associated with <html:span class="No-Break">the nodes.</html:span></html:p> <html:p>Let’s
    have a look at the available retrievers for <html:span class="No-Break">this Index.</html:span></html:p>
    <html:h3>KeywordTableGPTRetriever</html:h3> <html:p>We can build this type of
    retriever <html:a id="_idIndexMarker561"></html:a>with the <html:span class="No-Break">following
    command:</html:span></html:p> <html:p>It uses an LLM query to identify relevant
    keywords in a query and then returns the nodes associated with <html:span class="No-Break">those
    keywords.</html:span></html:p> <html:h3>KeywordTableSimpleRetriever</html:h3>
    <html:p>This retriever can be built <html:span class="No-Break">as follows:</html:span></html:p>
    <html:p>This is a simpler method that does <html:a id="_idIndexMarker562"></html:a>not
    use the LLM and is faster. However, it may be less efficient at identifying complex
    or contextual keywords. It uses a regular expression-based <html:span class="No-Break">keyword
    extractor.</html:span></html:p> <html:h3>KeywordTableRAKERetriever</html:h3> <html:p>To
    define this, we can use the <html:a id="_idIndexMarker563"></html:a><html:span
    class="No-Break">following command:</html:span></html:p> <html:p>Similar to the
    previous retriever, this one uses the <html:em class="italic">RAKE method</html:em>
    to efficiently extract relevant keywords. We discussed the RAKE method in <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 5</html:em></html:span></html:a>
    , <html:em class="italic">Indexing with LlamaIndex</html:em> , i <html:em class="italic">n
    the A simple usage model for</html:em> <html:span class="No-Break"><html:em class="italic">KeywordTableIndex</html:em></html:span>
    <html:span class="No-Break">section.</html:span></html:p> <html:p>There are also
    several common arguments that we can use to set up the retrievers <html:span class="No-Break">of</html:span>
    <html:span class="No-Break"><html:code class="literal">KeywordTableIndex</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">query_keyword_extract_template</html:code> : This is used to change
    the default prompt that’s used to extract keywords from the text of a query. This
    can only be applied to the <html:span class="No-Break">default mode.</html:span></html:li>
    <html:li><html:code class="literal">max_keywords_per_query</html:code> : This
    specifies the maximum number of keywords that can be extracted from a single query.
    This parameter is important to control query complexity and to avoid overloading
    the system with too <html:span class="No-Break">many keywords.</html:span></html:li>
    <html:li><html:code class="literal">num_chunks_per_query</html:code> : This specifies
    the maximum number of chunks that can be retrieved in a query. This parameter
    helps limit the amount of data that can be processed in a single query, optimizing
    system performance <html:span class="No-Break">and efficiency.</html:span></html:li></html:ul>
    <html:p>Next, we’ll talk about how to <html:a id="_idIndexMarker564"></html:a>retrieve
    data from <html:span class="No-Break">knowledge graphs.</html:span></html:p> <html:a
    id="_idTextAnchor139"></html:a><html:h2 id="_idParaDest-140">The KnowledgeGraphIndex
    retrievers</html:h2> <html:p>As discussed in the previous chapter, this <html:a
    id="_idIndexMarker565"></html:a>type of Index constructs a <html:a id="_idIndexMarker566"></html:a>graph
    made up of <html:em class="italic">triplets</html:em> . Each <html:strong class="bold">triplet</html:strong>
    consists of a subject, a predicate, and an object. The <html:strong class="bold">subject</html:strong>
    is the entity or concept about which a statement is being made. The <html:strong
    class="bold">predicate</html:strong> is the relationship or verb that links the
    subject to the object, describing <html:a id="_idIndexMarker567"></html:a>how
    the two are related, and the object is the entity or concept that is linked to
    the subject by the predicate. At the core of this index, there are two retrievers,
    <html:code class="literal">KGTableRetriever</html:code> and <html:code class="literal">KnowledgeGraphRAGRetriever</html:code>
    , both of which extract relevant nodes from a knowledge graph based <html:span
    class="No-Break">on queries.</html:span></html:p> <html:p><html:code class="literal">KGTableRetriever</html:code>
    is the default retriever for <html:code class="literal">KnowledgeGraphIndex</html:code>
    and can be configured in three retrieval modes: using keywords only, using embeddings
    only, or a combination of both – in hybrid mode. All modes operate as described
    in <html:span class="No-Break"><html:em class="italic">Figure 6</html:em></html:span>
    <html:span class="No-Break"><html:em class="italic">.11</html:em></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 6.11 – The inner workings of KGTableRetriever</html:p> <html:p>Let’s
    look at how they work under <html:span class="No-Break">the hood.</html:span></html:p>
    <html:h3>Keyword mode</html:h3> <html:p>The retriever can be built in this <html:a
    id="_idIndexMarker568"></html:a>mode using the <html:span class="No-Break">following
    command:</html:span></html:p> <html:p>When configured in keyword mode, the retriever
    uses keywords extracted from the query to find relevant nodes containing <html:span
    class="No-Break">those keywords.</html:span></html:p> <html:p>Keywords are evaluated
    in case-sensitive mode. This means that on a hypothetical index, a query of the
    form <html:em class="italic">where is the Colosseum?</html:em> will return a correct
    result, while <html:em class="italic">where is the colosseum?</html:em> will return
    <html:span class="No-Break">no nodes.</html:span></html:p> <html:h3>Embedding
    mode</html:h3> <html:p>We can set it to this mode using <html:a id="_idIndexMarker569"></html:a>the
    <html:span class="No-Break">following code:</html:span></html:p> <html:p>In this
    mode, the retriever turns the query into an embedding and the system finds nodes
    in the graph whose vector representation is similar to the embedding of the query,
    even if the same keywords are <html:span class="No-Break">not used.</html:span></html:p>
    <html:h3>Hybrid mode</html:h3> <html:p>This mode can be configured using the <html:span
    class="No-Break">following command:</html:span></html:p> <html:p>In hybrid mode,
    the retriever uses <html:a id="_idIndexMarker570"></html:a>both the keywords extracted
    from the query and the embeddings to find a set of relevant Nodes. It combines
    the results from both the keyword-based and embedding-based retrieval steps and
    removes any duplicated results. This approach combines the precision of keyword-based
    search with the semantic understanding of <html:span class="No-Break">the embeddings.</html:span></html:p>
    <html:p>There are several customizable parameters for this type of retriever.
    For example, <html:code class="literal">query_keyword_extract_template</html:code>
    , <html:code class="literal">refine_template</html:code> , and <html:code class="literal">text_qa_template</html:code>
    can be used to change the default prompt for keyword extraction, the default prompt
    for query refinement, and the default prompt for text queries and answers, respectively.
    Here are some other <html:span class="No-Break">useful parameters:</html:span></html:p>
    <html:ul><html:li><html:code class="literal">max_keywords_per_query</html:code>
    : This limits the number of keywords to avoid overloading the search process.
    The default value <html:span class="No-Break">is</html:span> <html:span class="No-Break"><html:code
    class="literal">10</html:code></html:span> <html:span class="No-Break">.</html:span></html:li>
    <html:li><html:code class="literal">num_chunks_per_query</html:code> : This determines
    how many text fragments can be parsed in a single query. The default is <html:code
    class="literal">10</html:code> and any change must take into account the performance
    impact and limitations of the <html:span class="No-Break">LLM used.</html:span></html:li>
    <html:li><html:code class="literal">include_text</html:code> : The default value
    is <html:code class="literal">True</html:code> . This argument indicates whether
    the text of the source document <html:a id="_idIndexMarker571"></html:a>should
    be used in queries in each relevant triplet. This can enrich the query with additional
    context but inevitably increases the <html:span class="No-Break">computational
    cost.</html:span></html:li> <html:li><html:code class="literal">similarity_top_k</html:code>
    : When the retriever is configured in embedding or hybrid mode, this parameter
    specifies the number of similar embeddings to be considered in the retrieval process.
    The default value <html:span class="No-Break">is</html:span> <html:span class="No-Break"><html:code
    class="literal">2</html:code></html:span> <html:span class="No-Break">.</html:span></html:li>
    <html:li><html:code class="literal">graph_store_query_depth</html:code> : This
    parameter controls how deep into the graph structure to search for relevant information.
    The default value <html:span class="No-Break">is</html:span> <html:span class="No-Break"><html:code
    class="literal">2</html:code></html:span> <html:span class="No-Break">.</html:span></html:li>
    <html:li><html:code class="literal">use_global_node_triplets</html:code> : When
    set to <html:code class="literal">True</html:code> , the retriever will not limit
    itself to keywords extracted directly from the user query; instead, it will search
    for other keywords or entities in the text fragments that have already been identified
    as relevant to the initial keywords. This process helps bring an additional layer
    of knowledge to the query. By exploring the relationships and connections between
    different nodes in the graph, the retriever can access richer and more contextual
    information than would be possible by limiting itself to the original keywords.
    However, this approach is more costly in terms of computing resources and search
    time as it involves analyzing a greater number of nodes and relationships in the
    graph. For this reason, the option is disabled by default – that is, it’s set
    <html:span class="No-Break">to</html:span> <html:span class="No-Break"><html:code
    class="literal">False</html:code></html:span> <html:span class="No-Break">.</html:span></html:li>
    <html:li><html:code class="literal">max_knowledge_sequence</html:code> : This
    parameter provides a balance between the quality and quantity of information presented.
    For example, if a query can theoretically generate 100 sequences of relevant knowledge,
    but <html:code class="literal">max_knowledge_sequence</html:code> is set to 30,
    only the most relevant 30 sequences will be presented as answers. This is also
    the default. Setting a limit ensures that the answer does not become too long
    or difficult to interpret, while still retaining <html:a id="_idIndexMarker572"></html:a>enough
    information to <html:span class="No-Break">be useful</html:span></html:li></html:ul>
    <html:p>Although they return <html:code class="literal">NodeWithScore</html:code>
    objects, the knowledge graph retrievers do not provide any score for the actual
    nodes. Instead, they simply return a default value of <html:code class="literal">1000</html:code>
    for each <html:span class="No-Break">retrieved node.</html:span></html:p> <html:p>If
    the retrievers do not find any nodes in the index based on the configured mode
    and search parameters, they will first try to identify nodes based on the provided
    keywords only. If they do not find any relevant nodes, they will return a single
    placeholder node with the text <html:em class="italic">No relationships found</html:em>
    and a score <html:span class="No-Break">of 1.</html:span></html:p> <html:h3>KnowledgeGraphRAGRetriever</html:h3>
    <html:p>This additional retriever is a bit more <html:a id="_idIndexMarker573"></html:a>special
    in that it operates by identifying key entities within a query and leveraging
    these to navigate the knowledge graph. It utilizes functions and templates for
    entity extraction ( <html:code class="literal">extraction entity_extract_fn</html:code>
    and <html:code class="literal">entity_extract_template</html:code> ) and synonym
    expansion ( <html:code class="literal">synonym_expand_fn</html:code> and <html:code
    class="literal">synonym_expand_template</html:code> ) to enrich the query with
    a broader context of related terms and concepts. The retriever traverses the graph
    to a specified depth – <html:code class="literal">graph_traversal_depth</html:code>
    – based on these entities and their synonyms, constructing a knowledge sequence
    relevant to <html:span class="No-Break">the query.</html:span></html:p> <html:p>This
    retriever can operate in various modes and can be configured by setting <html:code
    class="literal">retriever_mode</html:code> , allowing for flexibility in its approach
    to finding <html:span class="No-Break">relevant nodes.</html:span></html:p> <html:p>Just
    like <html:code class="literal">KGTableRetriever</html:code> , this retriever
    has three operating modes: <html:code class="literal">keyword</html:code> , <html:code
    class="literal">embedding</html:code> , <html:span class="No-Break">and</html:span>
    <html:span class="No-Break"><html:code class="literal">keyword_embedding</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p class="callout-heading">A
    note regarding retrieval modes</html:p> <html:p class="callout">As of January
    2024, in LlamaIndex v0.9.25, only the keyword <html:a id="_idIndexMarker574"></html:a>retrieval
    mode <html:span class="No-Break">was implemented.</html:span></html:p> <html:p>In
    addition, the retriever features the <html:code class="literal">with_nl2graphquery</html:code>
    option, which, when enabled, combines <html:strong class="bold">Natural Language
    to Graph Query</html:strong> ( <html:strong class="bold">NL2GraphQuery</html:strong>
    ) capabilities, enhancing its ability to interpret and <html:a id="_idIndexMarker575"></html:a>respond
    to complex queries. NL2GraphQuery is a process that converts natural language
    queries into graph-based query languages. This is achieved via a combination of
    entity extraction, synonym expansion, and graph <html:a id="_idIndexMarker576"></html:a>traversal
    techniques. This parameter is set to <html:code class="literal">False</html:code>
    <html:span class="No-Break">by default.</html:span></html:p> <html:p>Here are
    some other parameters that we may wish <html:span class="No-Break">to customize:</html:span></html:p>
    <html:ul><html:li><html:code class="literal">max_knowledge_sequence</html:code>
    : Sets a limit on the number of knowledge sequences included in the response,
    balancing detail <html:span class="No-Break">with clarity</html:span></html:li>
    <html:li><html:code class="literal">max_entities</html:code> : Specifies the maximum
    number of entities to extract from the query, defaulting <html:span class="No-Break">to</html:span>
    <html:span class="No-Break"><html:code class="literal">5</html:code></html:span></html:li>
    <html:li><html:code class="literal">max_synonyms</html:code> : Determines the
    maximum number of synonyms to expand for each entity, with a default value <html:span
    class="No-Break">of</html:span> <html:span class="No-Break"><html:code class="literal">5</html:code></html:span></html:li>
    <html:li><html:code class="literal">synonym_expand_policy</html:code> : Controls
    the policy for synonym expansion, either <html:em class="italic">union</html:em>
    or <html:em class="italic">intersection</html:em> , with <html:em class="italic">union</html:em>
    as <html:span class="No-Break">the default</html:span></html:li> <html:li><html:code
    class="literal">entity_extract_policy</html:code> : Sets the policy for entity
    extraction, also either <html:em class="italic">union</html:em> or <html:em class="italic">intersection</html:em>
    , defaulting <html:span class="No-Break">to</html:span> <html:span class="No-Break"><html:em
    class="italic">union</html:em></html:span></html:li> <html:li><html:code class="literal">verbose</html:code>
    : As usual, this is used to enable or disable the printing of debug information,
    aiding in the understanding of the <html:span class="No-Break">Retriever’s operation</html:span></html:li>
    <html:li><html:code class="literal">graph_traversal_depth</html:code> : Determines
    the depth of the traversal within the knowledge graph. By default, this is set
    <html:span class="No-Break">to</html:span> <html:span class="No-Break"><html:code
    class="literal">2</html:code></html:span></html:li></html:ul> <html:p class="callout-heading">A
    quick note</html:p> <html:p class="callout">There’s something important to highlight
    for all retrievers that use LLMs and accept parameters for customization prompts:
    All of these parameters are of the <html:code class="literal">BasePromptTemplate</html:code>
    type. We will talk more about the structure of this class and how to use it in
    <html:a><html:span class="No-Break"><html:em class="italic">Chapter 10</html:em></html:span></html:a>
    , <html:em class="italic">Prompt Engineering Guidelines and</html:em> <html:span
    class="No-Break"><html:em class="italic">Best Practices</html:em></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>With that, we’ve covered
    the differences between each type of retriever. Now, let’s see what they all have
    <html:span class="No-Break">in</html:span> <html:span class="No-Break"><html:a
    id="_idIndexMarker577"></html:a></html:span><html:span class="No-Break">common.</html:span></html:p>
    <html:a id="_idTextAnchor140"></html:a><html:h2 id="_idParaDest-141">Common characteristics
    shared by all retrievers</html:h2> <html:p>All retrievers accept either <html:a
    id="_idIndexMarker578"></html:a>a query directly or a <html:code class="literal">QueryBundle</html:code>
    object as a parameter. <html:code class="literal">QueryBundle</html:code> is a
    universal mechanism that can be used for more advanced use cases, such as searching
    based on embeddings or searching for images and/or text in a <html:span class="No-Break">multimodal
    scenario.</html:span></html:p> <html:p>In addition, all retrievers accept the
    <html:code class="literal">callback_manager</html:code> argument. We will discuss
    this mechanism in more detail in <html:a><html:span class="No-Break"><html:em
    class="italic">Chapter 10</html:em></html:span></html:a> , <html:em class="italic">Prompt
    Engineering Guidelines and</html:em> <html:span class="No-Break"><html:em class="italic">Best
    Practices</html:em></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p>These are the basic building blocks for the retrieval logic of our RAG
    applications. If we want a generic and easy-to-build solution, we can use them
    directly. However, for more complex cases, there are several advanced retrieval
    modules in LlamaIndex that either combine the functionality of the basic retrievers
    or add new features to the mix. We will discuss some of them later in <html:span
    class="No-Break">this chapter.</html:span></html:p> <html:p>As we have seen, some
    retrievers use either embedding models or LLM queries to identify the most relevant
    nodes. However, at their core, all of the retriever types listed here are subclasses
    of <html:code class="literal">BaseRetriever</html:code> . This means that they
    all inherit the main <html:code class="literal">retrieve()</html:code> method,
    as well as <html:code class="literal">aretrieve()</html:code> , for <html:span
    class="No-Break">asynchronous operation.</html:span></html:p> <html:p>We will
    discuss the asynchronous <html:span class="No-Break">operation next.</html:span></html:p>
    <html:a id="_idTextAnchor141"></html:a><html:h2 id="_idParaDest-142">Efficient
    use of retrieval mechanisms – asynchronous operation</html:h2> <html:p>For the
    sake of simplicity, all the code <html:a id="_idIndexMarker579"></html:a>examples
    we have discussed so <html:a id="_idIndexMarker580"></html:a>far have used <html:strong
    class="bold">synchronous methods</html:strong> . Although the synchronous – or
    <html:strong class="bold">serialized</html:strong> – mode of operation is linear,
    easy to understand, and predictable, in modern applications, performance and low
    latency are very important to provide a great <html:span class="No-Break">user
    experience.</html:span></html:p> <html:p>The good news is that LlamaIndex already
    <html:a id="_idIndexMarker581"></html:a>offers – in most cases – <html:strong
    class="bold">asynchronous execution</html:strong> alternatives. Here’s a simple
    example of asynchronous execution for two Retrievers defined <html:span class="No-Break">over</html:span>
    <html:span class="No-Break"><html:code class="literal">KeywordTableIndex</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p>The preceding code
    executes <html:a id="_idIndexMarker582"></html:a>the two retrievals in parallel.
    Of <html:a id="_idIndexMarker583"></html:a>course, being a trivial example with
    a very small dataset, the performance benefits of <html:strong class="bold">asynchronous
    operation</html:strong> will not be significant in <html:span class="No-Break">this
    case.</html:span></html:p> <html:p>However, in the context of a commercial application
    that frequently calls retrievers and operates numerous complex queries over many
    indexed nodes, the benefits will be substantial. Asynchronous operation improves
    performance, uses resources more efficiently, reduces latency, and generally provides
    a more natural user experience by reducing <html:span class="No-Break">waiting
    times.</html:span></html:p> <html:p>Now, it’s time to talk <html:a id="_idIndexMarker584"></html:a>about
    the more advanced <html:span class="No-Break">retrieval methods.</html:span></html:p>
    <html:a id="_idTextAnchor142"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Building
    more advanced retrieval mechanisms</html:title></html:head> <html:body><html:div
    class="epub-source"><html:h1 id="_idParaDest-143">Building more advanced retrieval
    mechanisms</html:h1> <html:div id="_idContainer071">from llama_index.core.vector_stores.types
    import (     FilterOperator, FilterCondition) filters = MetadataFilters(     filters=[
            MetadataFilter(             key="department",             value="Procurement"
            ),         MetadataFilter(             key="security_classification",
                value=<user_clearance_level>,             operator=FilterOperator.LTE
            ),     ],     condition=FilterCondition.AND ) from llama_index.core.selectors.llm_selectors
    import LLMSingleSelector options = [     "option 1: this is good for summarization
    questions",     "option 2: this is useful for precise definitions",     "option
    3: this is useful for comparing concepts", ] selector = LLMSingleSelector.from_defaults()
    decision = selector.select(     options,     query="What''s the definition of
    space?" ).selections[0] print(decision.index+1) print(decision.reason) from llama_index.core.selectors
    import PydanticMultiSelector from llama_index.core.retrievers import RouterRetriever
    from llama_index.core.tools import RetrieverTool from llama_index.core import
    (     VectorStoreIndex, SummaryIndex, SimpleDirectoryReader) documents = SimpleDirectoryReader("files").load_data()
    vector_index = VectorStoreIndex.from_documents([documents[0]]) summary_index =
    SummaryIndex.from_documents([documents[1]]) vector_retriever = vector_index.as_retriever()
    summary_retriever = summary_index.as_retriever() vector_tool = RetrieverTool.from_defaults(
        retriever=vector_retriever,     description="Use this for answering questions
    about Ancient Rome" ) summary_tool = RetrieverTool.from_defaults(     retriever=summary_retriever,
        description="Use this for answering questions about dogs" ) retriever = RouterRetriever(
        selector=PydanticMultiSelector.from_defaults(),     retriever_tools=[         vector_tool,
            summary_tool     ] ) response = retriever.retrieve(     "What can you
    tell me about the Ancient Rome?" ) for r in response:     print(r.text) retriever.retrieve("What
    can you tell me about the Ancient Rome?") retriever.retrieve("Tell me all you
    know about dogs") retriever.retrieve("Tell me about dogs in Ancient Rome") from
    llama_index.core.indices.query.query_transform.base import DecomposeQueryTransform
    decompose = DecomposeQueryTransform() query_bundle = decompose.run(     "Tell
    me about buildings in ancient Rome" ) print(query_bundle.query_str) What were
    some famous buildings in ancient Rome? from llama_index.question_gen.openai import
    OpenAIQuestionGenerator from llama_index.core.tools import RetrieverTool, ToolMetadata
    from llama_index.core import (     VectorStoreIndex, SummaryIndex,     SimpleDirectoryReader,
    QueryBundle) documents = SimpleDirectoryReader("files").load_data() vector_index
    = VectorStoreIndex.from_documents(     [documents[0]] ) summary_index = SummaryIndex.from_documents([documents[1]])
    vector_tool_metadata = ToolMetadata(     name="Vector Tool",     description="Use
    this for answering questions about Ancient Rome" ) summary_tool_metadata = ToolMetadata(
        name="Summary Tool",     description="Use this for answering questions about
    dogs" ) vector_tool = RetrieverTool(     retriever=vector_index.as_retriever(),
        metadata=vector_tool_metadata ) summary_tool = RetrieverTool(     retriever=summary_index.as_retriever(),
        metadata=summary_tool_metadata ) question_generator = OpenAIQuestionGenerator.from_defaults()
    query_bundle = QueryBundle(     query_str="Tell me about dogs and Ancient Rome")
    sub_questions = question_generator.generate(     tools=[vector_tool.metadata,
    summary_tool.metadata],     query=query_bundle ) for sub_question in sub_questions:
        print(f"{sub_question.tool_name}: {sub_question.sub_question}") Summary Tool:
    What are the different breeds of dog? Summary Tool: What was the role of dogs
    in ancient Rome? Vector Tool: What were the most important events in Ancient Rome?
    Vector Tool: What were the most famous buildings in ancient Rome? <html:p>Now
    we understand the basic components offered by LlamaIndex, we can build increasingly
    sophisticated solutions. On one hand, the retrievers we have discussed already
    provide efficient solutions for knowledge base querying and context enhancement
    in an RAG flow. On the other hand, we’ll see that there are many more advanced
    retrieval methods that either use specific techniques or ingeniously combine the
    retrievers <html:span class="No-Break">already discussed.</html:span></html:p>
    <html:a id="_idTextAnchor143"></html:a><html:h2 id="_idParaDest-144">The naive
    retrieval method</html:h2> <html:p>LlamaIndex provides fast query <html:a id="_idIndexMarker585"></html:a>methods
    by default. As we have seen, in just a few lines of code, we can ingest documents,
    create nodes and, for example, build a <html:code class="literal">VectorStoreIndex</html:code>
    retriever, which we can then just as easily query to return the most relevant
    parts using a retriever that uses similarity <html:span class="No-Break">measurement
    techniques.</html:span></html:p> <html:p>The method is very simple and easy to
    implement. However, it is not an ideal method in all situations. More often than
    not, the <html:strong class="bold">naive method</html:strong> , as it is <html:a
    id="_idIndexMarker586"></html:a>usually called, produces <html:a id="_idIndexMarker587"></html:a>mediocre
    rather than <html:strong class="bold">state-of-the-art</html:strong> ( <html:span
    class="No-Break"><html:strong class="bold">SOTA</html:strong></html:span> <html:span
    class="No-Break">) solutions.</html:span></html:p> <html:p class="callout-heading">To
    use an analogy…</html:p> <html:p class="callout">It’s pretty much like using a
    hammer for all kinds of repairs in a house. The hammer is an essential and easy-to-use
    tool, but it is not always the best solution for every problem. Similarly, using
    a simplified method of questioning may be effective for basic situations but will
    not be as effective for more complex situations or specific needs that require
    a greater degree of finesse <html:span class="No-Break">and adaptation.</html:span></html:p>
    <html:p>In these more complex cases, it is necessary to explore more advanced
    and tailored solutions, which may involve adapting the retrieval algorithms or
    combining them in <html:span class="No-Break">different ways.</html:span></html:p>
    <html:p>Also, for large datasets, naive methods can be inefficient, either returning
    too many irrelevant results or missing important information. They can also underperform
    in terms of response time and <html:span class="No-Break">resource consumption.</html:span></html:p>
    <html:p>In addition, in a real-world <html:a id="_idIndexMarker588"></html:a>situation,
    data can vary significantly in terms of quality, structure, and format. Simple
    methods are not always able to manage this diversity and extract <html:span class="No-Break">valuable
    information.</html:span></html:p> <html:p>For example, if the specific information
    we are looking for is scattered in small chunks that are randomly distributed
    throughout the document, the results will be below expectations. In the next few
    sections, we’ll discuss some more advanced retrieval methods that can provide
    much better results in various <html:span class="No-Break">specific situations.</html:span></html:p>
    <html:a id="_idTextAnchor144"></html:a><html:h2 id="_idParaDest-145">Implementing
    metadata filters</html:h2> <html:p>A very simple but also effective <html:a id="_idIndexMarker589"></html:a>retrieval
    mechanism is filtering <html:a id="_idIndexMarker590"></html:a>the retrieved nodes
    by <html:strong class="bold">metadata</html:strong> . We’ll tackle a practical
    problem that’s usually encountered in an organization and for which the retrieval
    functions in LlamaIndex can provide <html:span class="No-Break">a solution.</html:span></html:p>
    <html:p>We will see how to implement a retrieval system that filters the returned
    nodes according to the user’s department. Similar to the concept of polymorphism
    in object-oriented programming, it often happens that the same concept has different
    definitions, depending on the area <html:span class="No-Break">of use.</html:span></html:p>
    <html:p>In our example, the user is looking for the definition of an incident
    in an organizational knowledge base. However, the term <html:em class="italic">incident</html:em>
    may have a different definition for those who deal with information security than
    for those who deal with IT service operations. Let’s have a look at how we can
    implement a form of polymorphism in a <html:span class="No-Break">retrieval mechanism.</html:span></html:p>
    <html:ol><html:li>First, we must take care of the necessary imports and define
    a mapping of users from llama_index.core.vector_stores.types import MetadataFilter,
    MetadataFilters from llama_index.core import VectorStoreIndex from llama_index.core.schema
    import TextNode user_departments = {"Alice": "Security", "Bob": "IT"} <html:span
    class="No-Break">to departments:</html:span></html:li> <html:li>Then, we must
    define two nodes that both store the definition of the concept of incident. The
    difference is in nodes = [     TextNode(         text=(             "An incident
    is an accidental or malicious event that has the potential to cause unwanted effects
    on the security of our IT assets."),         metadata={"department": "Security"},
        ),     TextNode(         text=("An incident is an unexpected interruption
    or             degradation of an IT service."),         metadata={"department":
    "IT"},     ) ] Next, we must define the function that''s responsible for filtering
    and retrieval: def show_report(index, user, query):     user_department = user_departments[user]
        filters = MetadataFilters(         filters=[             MetadataFilter(key="department",
                    value=user_department)         ]     )     retriever = index.as_retriever(filters=filters)
        response = retriever.retrieve(query)     print(f"Response for {user}: {response[0].node.text}")
    <html:a id="_idIndexMarker591"></html:a>the metadata, which specifies the department
    where the <html:span class="No-Break">definition applies:</html:span></html:li>
    <html:li>Now, if we run the same index = VectorStoreIndex(nodes) query = "What
    is an incident?" show_report(index, "Alice", query) show_report(index, "Bob",
    query) Response for Alice: An incident is an accidental or malicious event that
    has the potential to cause unwanted effects on the security of our IT assets.
    Response for Bob: An incident is an unexpected interruption or degradation of
    an IT service. <html:a id="_idIndexMarker592"></html:a>query in the context of
    each user, we will get different answers, depending on the department each user
    <html:span class="No-Break">belongs to:</html:span> <html:p class="list-inset">The
    output will look <html:span class="No-Break">like this:</html:span></html:p></html:li></html:ol>
    <html:p>See how simple that was? The same mechanism can be used, for example,
    to control access to information and define <html:span class="No-Break">security
    rules.</html:span></html:p> <html:p>For example, in a knowledge base system shared
    by several clients on a multi-tenancy model, we can restrict access by <html:span
    class="No-Break">implementing</html:span> <html:span class="No-Break"><html:code
    class="literal">MetadataFilters</html:code></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p>The code you saw earlier only does simple filtering: it restricts the
    search to nodes for which the value of the <html:code class="literal">department</html:code>
    key is equal to the user’s department. But there are also more complex filtering
    variants that use operators based on the <html:code class="literal">FilterOperator</html:code>
    class. Unfortunately, the default vector store in LlamaIndex only supports the
    <html:code class="literal">EQ</html:code> (equal) operator – that is, it can only
    apply filters where the value of a key is equal to a certain parameter. If we
    use a more sophisticated version of vector store (such as Pinecone or <html:a
    id="_idIndexMarker593"></html:a>ChromaDB), we can use the full range of operators
    available in <html:code class="literal">FilterOperator</html:code> , as listed
    in the <html:span class="No-Break">following table:</html:span></html:p> <html:table
    class="No-Table-Style _idGenTablePara-1" id="table001-1"><html:thead><html:tr
    class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span class="No-Break"><html:strong
    class="bold">Symbolic Operator</html:strong></html:span></html:p></html:td> <html:td
    class="No-Table-Style"><html:p><html:span class="No-Break"><html:strong class="bold">Programming
    Equivalent</html:strong></html:span></html:p></html:td> <html:td class="No-Table-Style"><html:p><html:span
    class="No-Break"><html:strong class="bold">Description</html:strong></html:span></html:p></html:td></html:tr></html:thead>
    <html:tbody><html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break">EQ</html:span></html:p></html:td> <html:td class="No-Table-Style"><html:p>==</html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Equal (default)</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break">GT</html:span></html:p></html:td> <html:td class="No-Table-Style"><html:p>></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Greater than</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break">LT</html:span></html:p></html:td> <html:td class="No-Table-Style"><html:p><</html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">Less than</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break">NE</html:span></html:p></html:td> <html:td class="No-Table-Style"><html:p>!=</html:p></html:td>
    <html:td class="No-Table-Style"><html:p>Not <html:span class="No-Break">equal
    to</html:span></html:p></html:td></html:tr> <html:tr class="No-Table-Style"><html:td
    class="No-Table-Style"><html:p><html:span class="No-Break">GTE</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>>=</html:p></html:td> <html:td class="No-Table-Style"><html:p>Greater
    than or <html:span class="No-Break">equal to</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break">LTE</html:span></html:p></html:td> <html:td class="No-Table-Style"><html:p><=</html:p></html:td>
    <html:td class="No-Table-Style"><html:p>Less than or <html:span class="No-Break">equal
    to</html:span></html:p></html:td></html:tr> <html:tr class="No-Table-Style"><html:td
    class="No-Table-Style"><html:p><html:span class="No-Break">IN</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">in</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p><html:span class="No-Break">In array</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break">NIN</html:span></html:p></html:td> <html:td class="No-Table-Style"><html:p><html:span
    class="No-Break">nin</html:span></html:p></html:td> <html:td class="No-Table-Style"><html:p>Not
    <html:span class="No-Break">in array</html:span></html:p></html:td></html:tr></html:tbody></html:table>
    <html:p class="IMG---Caption" lang="en-US">Table 6.1 – A complete list of operators
    available for FilterOperator</html:p> <html:p>Here is an example where we use
    filter operators and filter aggregation conditions to implement more <html:span
    class="No-Break">complex scenarios:</html:span></html:p> <html:p>In the previous
    example, we implemented a very simple access control mechanism based on clearance
    level and security classification. Only nodes that belong to a particular department
    and <html:a id="_idIndexMarker594"></html:a>have a classification level less than
    or equal to the user’s access level will be returned. We’ll talk about another
    <html:span class="No-Break">method next.</html:span></html:p> <html:a id="_idTextAnchor145"></html:a><html:h2
    id="_idParaDest-146">Using selectors for more advanced decision logic</html:h2>
    <html:p>In an advanced user interaction <html:a id="_idIndexMarker595"></html:a>system,
    the user may employ a wide variety of queries. For example, they may ask a very
    specific question, looking for a precise definition. At other times, the user
    may be looking for more general information or may be asking the system to summarize
    or compare <html:span class="No-Break">two documents.</html:span></html:p> <html:p>In
    these complex situations, which retriever should be used? It becomes clear that
    the best implementation is based on the combined strength of many retrieval systems.
    But this implicitly means that the RAG application must have an internal selection
    mechanism to choose the most appropriate retriever according to the query. This
    brings us to the topic of this section: the <html:a id="_idIndexMarker596"></html:a>use
    <html:span class="No-Break">of</html:span> <html:span class="No-Break"><html:strong
    class="bold">selectors</html:strong></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p>In LlamaIndex, they come in five different flavors: <html:code class="literal">LLMSingleSelector</html:code>
    , <html:code class="literal">LLMMultiSelector</html:code> , <html:code class="literal">EmbeddingSingleSelector</html:code>
    , <html:code class="literal">PydanticSingleSelector</html:code> , <html:span class="No-Break">and</html:span>
    <html:span class="No-Break"><html:code class="literal">PydanticMultiSelector</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>The way they work is
    slightly different. As the name suggests, some rely on the decision capabilities
    of an LLM, others select a particular option from a list of options based on a
    similarity calculation, and others use Pydantic objects to return a selection.
    Some return a single option from a list; others may return multiple selections
    from a list of options. In the end, however, their result is more or less the
    same: they help us implement advanced conditional logic in the applications <html:span
    class="No-Break">we develop.</html:span></html:p> <html:p>That is because they
    <html:a id="_idIndexMarker597"></html:a>can evaluate complex conditions and decide
    which logic branch the application should follow – just like an <html:em class="italic">IF</html:em>
    decision block, but able to handle more <html:span class="No-Break">complex scenarios.</html:span></html:p>
    <html:p>The following diagram can help us better understand the role a selector
    plays in the logic of an RAG application. <html:span class="No-Break"><html:em
    class="italic">Figure 6</html:em></html:span> <html:em class="italic">.12</html:em>
    , provides a visual representation of how <html:span class="No-Break"><html:code
    class="literal">LLMSingleSelector</html:code></html:span> <html:span class="No-Break">works:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 6.12 – Visualizing LLMSingleSelector</html:p>
    <html:p>Here is a very simple implementation of a selector that uses an LLM to
    return a single option from a list of <html:span class="No-Break">predefined options:</html:span></html:p>
    <html:p>In the first part of the code, we <html:a id="_idIndexMarker598"></html:a>defined
    the options as a list of strings to be sent to the LLM via the <html:code class="literal">.</html:code>
    <html:span class="No-Break"><html:code class="literal">select()</html:code></html:span>
    <html:span class="No-Break">method:</html:span></html:p> <html:p>The <html:code
    class="literal">.select()</html:code> method takes the defined options and the
    user query as arguments. Under the hood, the selector uses a specially constructed
    prompt to ask the LLM to choose the best option from the list based on <html:span
    class="No-Break">the query.</html:span></html:p> <html:p>As a response, the selector
    returns a <html:code class="literal">SingleSelection</html:code> object containing
    the number of the selected option and a justification for the selection made.
    As you can see, the selector is not something specific to retrievers. We haven’t
    even defined a retriever in <html:span class="No-Break">this example.</html:span></html:p>
    <html:p>This is because I wanted to show that the mechanism is generic and can
    be used for absolutely any conditional logic we want to implement in the application.
    The returned option number could help us to choose from a list of parsers, indexes,
    retrievers, and so on. In this simple version, the selector simply chooses from
    a list of strings defining the available options. However, there is a more advanced
    form of selection that involves the use of the <html:code class="literal">ToolMetadata</html:code>
    class. But to understand this concept, we first need to clarify what a <html:span
    class="No-Break"><html:strong class="bold">tool</html:strong></html:span> <html:span
    class="No-Break">is.</html:span></html:p> <html:a id="_idTextAnchor146"></html:a><html:h2
    id="_idParaDest-147">Understanding tools</html:h2> <html:p>An essential element
    <html:a id="_idIndexMarker599"></html:a>in any <html:strong class="bold">agentic
    functionality</html:strong> , where the application decides which method to use
    depending on the context, is a generic container. It may contain different functionalities
    that can be called by <html:a id="_idIndexMarker600"></html:a>the application
    <html:span class="No-Break">at runtime.</html:span></html:p> <html:p>There is
    a rich collection of tools already developed and available in LlamaHub: <html:a>https://llamahub.ai/?tab=tools</html:a>
    . They can perform various specific functions, from composing and sending emails
    to querying various APIs or interacting with the computer’s filesystem. We will
    talk much more about the use of tools in implementing <html:strong class="bold">agents</html:strong>
    in <html:a><html:span class="No-Break"><html:em class="italic">Chapter 8</html:em></html:span></html:a>
    , <html:em class="italic">Building Chatbots and Agents with LlamaIndex</html:em>
    , where we will build our <html:span class="No-Break">PITS chatbot.</html:span></html:p>
    <html:p>For now, I want to show you how we can encapsulate a retriever in a tool
    container, and then use selectors to implement an adaptive retrieval mechanism.
    We will focus on the <html:code class="literal">RetrieverTool</html:code> class,
    which takes two important arguments: a retriever and a textual description of
    the retriever. Based on <html:a id="_idIndexMarker601"></html:a>the description,
    the selector decides, for example, whether to use one retriever or another for
    a particular query. We define a <html:code class="literal">RouterRetriever</html:code>
    object on top of each retriever we build. This <html:code class="literal">RouterRetriever</html:code>
    is a complex decision mechanism that uses the selector to decide which retriever
    to use depending on the situation. The most important arguments to give it are
    the selector and the options to choose from – in the form of <html:code class="literal">RetrieverTool</html:code>
    objects. Let’s see how we can implement this <html:span class="No-Break">in code:</html:span></html:p>
    <html:p>First, we took the two sample files from the <html:code class="literal">files</html:code>
    subfolder. The first file contains information about ancient Rome and the second
    is a generic text about dogs. Then, we created an index for <html:a id="_idIndexMarker602"></html:a>each
    file and from each index, we created a retriever. Now, we must define <html:span
    class="No-Break">the tools:</html:span></html:p> <html:p>As you can see, we have
    wrapped each retriever into <html:code class="literal">RetrieverTool</html:code>
    and added a clear description for the selector to use. Next, we must <html:span
    class="No-Break">build</html:span> <html:span class="No-Break"><html:code class="literal">RouterRetriever</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p>That’s all we need
    to do. From this point on, every time we query this dynamic retriever, the selector
    will determine which individual retriever to use to return the context. Here’s
    <html:span class="No-Break">an example:</html:span></html:p> <html:p>This will
    use <html:code class="literal">vector_tool</html:code> for retrieval. Now, take
    a look at the <html:span class="No-Break">following code:</html:span></html:p>
    <html:p>This will call <html:code class="literal">summary_tool</html:code> . Because
    we used <html:code class="literal">PydanticMultiSelector</html:code> , we can
    also handle situations where both retrievers should be used, <html:span class="No-Break">like
    so:</html:span></html:p> <html:p>Unlike <html:code class="literal">PydanticSingleSelector</html:code>
    , <html:code class="literal">PydanticMultiSelector</html:code> can simultaneously
    select multiple options from the selector list, covering multiple use cases. Similarly,
    we can also define <html:a id="_idIndexMarker603"></html:a>more complex routers
    at the query engine level by using <html:code class="literal">RouterQueryEngine</html:code>
    . We will discuss this in more detail in <html:a><html:span class="No-Break"><html:em
    class="italic">Chapter 7</html:em></html:span></html:a> <html:span class="No-Break">.</html:span></html:p>
    <html:p>First, we need to cover a few other advanced forms <html:span class="No-Break">of
    retrievers.</html:span></html:p> <html:a id="_idTextAnchor147"></html:a><html:h2
    id="_idParaDest-148">Transforming and rewriting queries</html:h2> <html:p>In the
    previous section, we saw how <html:a id="_idIndexMarker604"></html:a>we can use
    selectors and the router concept to let the application decide which retriever
    <html:span class="No-Break">to use.</html:span></html:p> <html:p>Another very
    powerful tool that our RAG application can use is the <html:code class="literal">QueryTransform</html:code>
    construct. This allows us to rewrite and modify a query before using it to interrogate
    the index, as shown in <html:span class="No-Break"><html:em class="italic">Figure
    6</html:em></html:span> <html:span class="No-Break"><html:em class="italic">.13</html:em></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 6.13 – QueryTransform improving the retrieval process</html:p>
    <html:p>Let’s imagine a scenario where we <html:a id="_idIndexMarker605"></html:a>might
    need the functionality provided <html:span class="No-Break">by</html:span> <html:span
    class="No-Break"><html:code class="literal">QueryTransform</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p class="callout-heading">Practical
    example</html:p> <html:p class="callout"><html:strong class="bold">A chatbot designed
    to provide technical support for complex software</html:strong> : Users often
    describe their problems in vague or non-technical terms. <html:code class="literal">QueryTransform</html:code>
    can interpret these descriptions, break them down into more specific sub-queries,
    or enrich them with technical terms that better match the documentation. For example,
    a query of the form <html:em class="italic">My computer keeps freezing</html:em>
    could be transformed into a more specific query, such as <html:em class="italic">Troubleshooting
    steps for operating</html:em> <html:span class="No-Break"><html:em class="italic">system
    freezes</html:em></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p>There are several variations of <html:code class="literal">QueryTransform</html:code>
    that we can use. Each has its specific role in augmenting the information retrieval
    process. Let’s look at <html:span class="No-Break">each one:</html:span></html:p>
    <html:ul><html:li><html:code class="literal">IdentityQueryTransform</html:code>
    : This is a basic transform that does not modify the query. It returns the query
    as it was received, without any transformation. It’s useful for maintaining default
    or basic behavior where no specific transformations <html:span class="No-Break">are
    required</html:span></html:li> <html:li><html:code class="literal">HyDEQueryTransform</html:code>
    : <html:strong class="bold">Hypothetical Document Embeddings</html:strong> ( <html:strong
    class="bold">HyDE</html:strong> ) transforms the query into a <html:a id="_idIndexMarker606"></html:a>hypothetical
    document generated by an LLM. The idea is to generate hypothetical query answers
    and use them as embedding strings. This can help improve the relevance of the
    results. This method filters out inaccurate details while <html:a id="_idIndexMarker607"></html:a>grounding
    the generated response in the actual content. You can read more about the benefits
    of using this technique here: <html:em class="italic">Gao, Luyu; Ma, Xueguang;
    Lin, Jimmy; Callan, Jamie (2022). “Precise Zero-Shot Dense Retrieval without Relevance
    Labels”. arXiv:2212.10496v1 [</html:em> <html:span class="No-Break"><html:em class="italic">cs.IR].</html:em></html:span>
    <html:span class="No-Break"></html:span><html:a><html:span class="No-Break">https://arxiv.org/abs/2212.10496</html:span></html:a></html:li>
    <html:li><html:code class="literal">DecomposeQueryTransform</html:code> : This
    type of transform decomposes a complex query into a simpler and more focused subquery.
    This can be useful to make queries easier for the index to process and increase
    the chances of finding relevant nodes, especially if the index structure is not
    optimized for complex or <html:span class="No-Break">ambiguous queries</html:span></html:li>
    <html:li><html:code class="literal">ImageOutputQueryTransform</html:code> : This
    method adds instructions for formatting results as images, such as generating
    HTML <html:em class="italic"><img></html:em> tags. It is useful for cases where
    query results are expected to be displayed as images or when the output is just
    an intermediate step in more complex logic and has to be further processed in
    a <html:span class="No-Break">particular format</html:span></html:li> <html:li><html:code
    class="literal">StepDecomposeQueryTransform</html:code> : This is similar to <html:code
    class="literal">DecomposeQueryTransform</html:code> but it adds an extra layer
    by taking previous reasoning or context into account when decomposing the query.
    This can help to continually refine the query based on feedback or previous results,
    thus improving <html:span class="No-Break">retrieval accuracy</html:span></html:li></html:ul>
    <html:p>Each of these transformations improves a system’s ability to process and
    respond to queries in a more efficient way that is better tailored to the user’s
    specific needs or the nature of <html:span class="No-Break">the data.</html:span></html:p>
    <html:p>Let’s have a look at a practical example to better understand how <html:span
    class="No-Break">they work:</html:span></html:p> <html:p>Once we run the code,
    <html:code class="literal">DecomposeQueryTransform</html:code> takes in our original
    – and otherwise very ambiguous – query. It then uses a specially designed prompt
    to generate a more precise query using the LLM. In our example, the output should
    look something <html:span class="No-Break">like this:</html:span></html:p> <html:p>You
    can immediately see <html:a id="_idIndexMarker608"></html:a>that the new query
    is much clearer and greatly increases the chances of the retriever generating
    a correct context from <html:span class="No-Break">the index.</html:span></html:p>
    <html:a id="_idTextAnchor148"></html:a><html:h2 id="_idParaDest-149">Creating
    more specific sub-queries</html:h2> <html:p>Another useful approach to <html:a
    id="_idIndexMarker609"></html:a>augmenting a query is to generate sub-queries.
    Sometimes, an ambiguous or very complex question becomes much clearer when it
    is split into several specific questions. LlamaIndex comes to our rescue this
    time too. <html:code class="literal">OpenAIQuestionGenerator</html:code> is a
    mechanism that’s designed exactly for this operation. Here is the code we used
    as an example earlier when we talked about selectors and routers. This time, we
    will adapt it a bit to demonstrate how <html:span class="No-Break"><html:code
    class="literal">OpenAIQuestionGenerator</html:code></html:span> <html:span class="No-Break">works:</html:span></html:p>
    <html:p>So far, the code is identical to <html:a id="_idIndexMarker610"></html:a>the
    earlier example. We read the two files from the <html:code class="literal">files</html:code>
    subfolder and then create an index for <html:span class="No-Break">each document:</html:span></html:p>
    <html:p>For each index, we define a name and a description in a <html:code class="literal">ToolMetadata</html:code>
    structure. This information will be used by <html:code class="literal">OpenAIQuestionGenerator</html:code>
    to <html:em class="italic">understand</html:em> what role each retriever has and
    what type of questions it might answer. Next, we will define the <html:span class="No-Break">two
    retrievers:</html:span></html:p> <html:p>Now follows the generation of sub-queries.
    First, we initialize an <html:code class="literal">OpenAIQuestionGenerator</html:code>
    object with the default settings. Then, we build a <html:code class="literal">QueryBundle</html:code>
    object that will <html:a id="_idIndexMarker611"></html:a>contain the original
    query received from the user. This <html:code class="literal">QueryBundle</html:code>
    will be sent as an argument to the <html:span class="No-Break">question generator:</html:span></html:p>
    <html:p>As you can see, the subquery generator takes two arguments – a list of
    tools at its disposal, and the original query from which it can build more <html:span
    class="No-Break">specific queries:</html:span></html:p> <html:p>In the end, the
    generated questions might look something <html:span class="No-Break">like this:</html:span></html:p>
    <html:p><html:code class="literal">OpenAIQuestionGenerator</html:code> took the
    initial query and, using the LLM, returned a list of more <html:span class="No-Break">specific
    questions.</html:span></html:p> <html:p>The answer that’s returned in the <html:code
    class="literal">sub_questions</html:code> variable is a list of <html:code class="literal">SubQuestion</html:code>
    items - a simple class with two attributes: <html:code class="literal">tool_name</html:code>
    and <html:code class="literal">sub_question</html:code> . We can now iterate through
    all the items in the list and get the tools and questions we are <html:span class="No-Break">looking
    for.</html:span></html:p> <html:p>In practice, using more specific queries, as
    in the preceding example, is likely to generate more context with the retriever
    and therefore likely to get a better-quality answer <html:span class="No-Break">from</html:span>
    <html:span class="No-Break"><html:code class="literal">QueryEngine</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>As an alternative to
    <html:code class="literal">OpenAIQuestionGenerator</html:code> , it is good to
    know that there is also <html:code class="literal">LLMQuestionGenerator</html:code>
    , which, as its name suggests, allows us to use any LLM. Another difference <html:a
    id="_idIndexMarker612"></html:a>between the two is that <html:code class="literal">LLMQuestionGenerator</html:code>
    uses a special parser to structure the output, unlike <html:code class="literal">OpenAIQuestionGenerator</html:code>
    , which relies on the generation of <html:span class="No-Break">Pydantic objects.</html:span></html:p>
    <html:p>The same collection of question generators also includes <html:code class="literal">GuidanceQuestionGenerator</html:code>
    . This mechanism uses an LLM to create helper questions to guide the query engine.
    It can be extremely useful when you’re dealing with complex queries that need
    to be broken down and processed in a <html:span class="No-Break">particular order.</html:span></html:p>
    <html:p>Once these sub-queries have been generated, they can be used in a specially
    constructed query engine. We will discuss this step in more detail in <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 7</html:em></html:span></html:a>
    , <html:em class="italic">Querying Our Data, Part 2 – Postprocessing and Response
    Synthesis</html:em> , when we talk <html:span class="No-Break">about</html:span>
    <html:span class="No-Break"><html:code class="literal">SubQuestionQueryEngine</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>Next, we’ll talk about
    two important concepts related to <html:span class="No-Break">information retrieval.</html:span></html:p>
    <html:a id="_idTextAnchor149"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Understanding
    the concepts of dense and sparse retrieval</html:title></html:head> <html:body><html:div
    class="epub-source"><html:h1 id="_idParaDest-150">Understanding the concepts of
    dense and sparse retrieval</html:h1> <html:div id="_idContainer071">pip install
    rank-bm25 pip install llama-index-retrievers-bm25 from llama_index.retrievers.bm25
    import BM25Retriever from llama_index.core.node_parser import SentenceSplitter
    from llama_index.core import SimpleDirectoryReader reader = SimpleDirectoryReader(''files'')
    documents = reader.load_data() splitter = SentenceSplitter.from_defaults(     chunk_size=60,
        chunk_overlap=0,     include_metadata=False ) nodes = splitter.get_nodes_from_documents(
        documents ) retriever = BM25Retriever.from_defaults(     nodes=nodes,     similarity_top_k=2
    ) response = retriever.retrieve("Who built the Colosseum? ") for node_with_score
    in response:     print(''Text:''+node_with_score.node.text)     print(''Score:
    ''+str(node_with_score.score)) <html:p>As we have seen, retrieval methods are
    a critical component of RAG systems. They enable the identification and ranking
    of relevant content for queries, which is the first step in generating useful
    answers from an LLM. During your journey into RAG application development, you’re
    <html:a id="_idIndexMarker613"></html:a>likely to encounter two <html:a id="_idIndexMarker614"></html:a>dominant
    retrieval paradigms – <html:strong class="bold">dense retrieval</html:strong>
    and <html:strong class="bold">sparse retrieval</html:strong> . Because it is important
    to understand these concepts, this section will focus on their characteristics,
    trade-offs, and the benefits of <html:span class="No-Break">combining them.</html:span></html:p>
    <html:a id="_idTextAnchor150"></html:a><html:h2 id="_idParaDest-151">Dense retrieval</html:h2>
    <html:p>The dense retrieval method relies on embedding vectors to represent text
    in a continuous, high-dimensional <html:a id="_idIndexMarker615"></html:a>space.
    Using embedding models, texts are <html:strong class="bold">encoded</html:strong>
    into fixed-length numerical vectors that are intended to capture semantic meaning.
    Queries are also encoded so that the similarity between them and the node vectors
    can be measured using geometric operations. In dense retrieval, nodes are embedded
    in vectors and stored in a specialized index such <html:span class="No-Break">as</html:span>
    <html:span class="No-Break"><html:code class="literal">VectorStoreIndex</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>We call them <html:strong
    class="bold">dense</html:strong> because these vectors are typically densely populated
    with non-zero values, representing rich and nuanced semantic information in a
    compact form. During retrieval, incoming queries are dynamically embedded and
    used to retrieve the top-k nodes using similarity search algorithms, such as those
    discussed in <html:a><html:span class="No-Break"><html:em class="italic">Chapter
    5</html:em></html:span></html:a> <html:span class="No-Break">.</html:span></html:p>
    <html:p>This approach has several advantages, particularly in terms of semantic
    understanding, speed, and scalability. Nodes that convey similar meanings tend
    to cluster closer together. Also, the words themselves do not have to match perfectly.
    Synonyms and polysemous words don’t affect precision <html:span class="No-Break">as
    much.</html:span></html:p> <html:p>Specialized indexing solutions, such as those
    provided by a Pinecone vector database ( <html:a>https://www.pinecone.io/product/</html:a>
    ), also allow lightning-fast similarity searches over millions of vectors. Latencies
    range from milliseconds to less than a second and scaling is <html:span class="No-Break">easily
    achieved.</html:span></html:p> <html:p>There are, however, several drawbacks associated
    with <html:a id="_idIndexMarker616"></html:a><html:span class="No-Break">dense
    search:</html:span></html:p> <html:ul><html:li><html:strong class="bold">Computational
    cost</html:strong> : Embedding and indexing large volumes of data can be computationally
    expensive <html:span class="No-Break">and time-consuming.</html:span></html:li>
    <html:li><html:strong class="bold">A trade-off between precision and recall</html:strong>
    : Dense retrieval systems can sometimes favor recall over precision or vice versa,
    depending on how the embedding model is tuned. Finding the right balance between
    retrieving all relevant documents and not retrieving too many irrelevant documents
    can <html:span class="No-Break">be difficult.</html:span></html:li> <html:li><html:strong
    class="bold">Difficulty in dealing with long documents</html:strong> : Dense models
    that generate fixed-length vectors can sometimes struggle with very long content,
    where important information can be diluted or lost in the <html:span class="No-Break">embedding
    process.</html:span></html:li> <html:li><html:strong class="bold">Logical reasoning
    gaps</html:strong> : While these methods are excellent at capturing semantic similarity,
    they typically lack logical reasoning capabilities. This means that they can identify
    documents that are semantically similar to the query but may struggle to understand
    <html:a id="_idIndexMarker617"></html:a>context or logical relationships that
    require reasoning beyond this pattern matching. As a result, they may retrieve
    documents that are <html:a id="_idIndexMarker618"></html:a>superficially related
    to the query but not truly relevant to the user’s intent, especially in cases
    where the query requires an understanding of complex relationships or <html:span
    class="No-Break">nuanced reasoning.</html:span></html:li> <html:li><html:strong
    class="bold">Dependence on model quality</html:strong> : The effectiveness of
    a dense retrieval system is highly dependent on the quality of the underlying
    embedding model. Poorly trained models can result in suboptimal <html:span class="No-Break">retrieval
    performance.</html:span></html:li></html:ul> <html:p>Next, we’ll talk about <html:span
    class="No-Break">sparse retrieval.</html:span></html:p> <html:a id="_idTextAnchor151"></html:a><html:h2
    id="_idParaDest-152">Sparse retrieval</html:h2> <html:p>Sparse retrieval methods
    associate <html:a id="_idIndexMarker619"></html:a>documents with keywords. These
    methods are based on exact keyword matching or overlaps between the query and
    <html:span class="No-Break">the documents.</html:span></html:p> <html:p>The general
    process involves indexing documents by analyzing them for important terms. These
    keywords are then recorded in inverted indexes, which are data structures used
    to quickly retrieve documents containing a <html:span class="No-Break">given keyword.</html:span></html:p>
    <html:p>During the retrieval phase, queries are searched against these inverted
    indexes to find documents that share keywords with the query. Documents are ranked
    based on the number of common terms identified between the query and each indexed
    document. One of the most common techniques used in sparse <html:a id="_idIndexMarker620"></html:a>retrieval
    is the <html:strong class="bold">Term Frequency – Inverse Document Frequency</html:strong>
    ( <html:span class="No-Break"><html:strong class="bold">TF-IDF</html:strong></html:span>
    <html:span class="No-Break">) method.</html:span></html:p> <html:h3>TF-IDF in
    sparse retrieval</html:h3> <html:p>TF-IDF is a numerical statistic that reflects
    how important a word is to each document in a collection of documents. This method
    transforms text into a numerical representation that captures the significance
    of words in documents, taking into account both their frequency in individual
    documents and across the entire collection <html:span class="No-Break">of documents.</html:span></html:p>
    <html:ul><html:li><html:strong class="bold">Term Frequency</html:strong> ( <html:strong
    class="bold">TF</html:strong> ) measures <html:a id="_idIndexMarker621"></html:a>how
    often a term occurs in a <html:a id="_idIndexMarker622"></html:a>document, normalized
    by the total number of terms in the document. It’s calculated by dividing the
    number of times a particular term – that is, a word – appears in a document by
    the total number of words in that document. This indicates the importance of the
    term within the <html:span class="No-Break">specific document.</html:span></html:li>
    <html:li><html:strong class="bold">Inverse Document Frequency</html:strong> (
    <html:strong class="bold">IDF</html:strong> ) assesses the importance of the term
    across the collection. It is calculated by taking the logarithm of the ratio of
    the total number of documents to <html:a id="_idIndexMarker623"></html:a>the number
    of documents containing the term. This helps to downplay the importance of terms
    that occur very frequently in many documents. Common terms such as <html:em class="italic">the</html:em>
    or <html:em class="italic">is</html:em> appear in many documents and are less
    informative, so they have lower IDF scores. Unique terms have higher <html:span
    class="No-Break">IDF scores.</html:span></html:li></html:ul> <html:p>The <html:strong
    class="bold">TF-IDF score</html:strong> , which is obtained by <html:a id="_idIndexMarker624"></html:a>multiplying
    the TF by the IDF, represents the importance of each term in a document, adjusted
    for its commonness across the collection. In sparse retrieval, each document is
    represented as a vector in a high-dimensional space, where each dimension corresponds
    to a unique term and the value is the TF-IDF <html:span class="No-Break">score:</html:span>
    <html:a><html:span class="No-Break">https://en.wikipedia.org/wiki/Tf%E2%80%93idf</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>We call it <html:em
    class="italic">sparse</html:em> because, in this high-dimensional vector space,
    most dimensions (terms) will have a value of zero for any given document, indicating
    that most terms in the collection do not appear in that document. If we were to
    visualize these vectors, this would result in a <html:em class="italic">sparse</html:em>
    representation, with many zeros, as most documents contain only a small subset
    of the total vocabulary of <html:span class="No-Break">the collection.</html:span></html:p>
    <html:p>During retrieval, a query is also converted into its TF-IDF vector representation.
    The relevance of each document to the query is calculated using measures such
    as cosine similarity, and the documents are ranked accordingly. The top-ranked
    documents with the highest similarity scores to the query are then returned <html:span
    class="No-Break">as results.</html:span></html:p> <html:p>Sparse retrieval methods
    such as TF-IDF are particularly effective for tasks where exact term matching
    is important. However, they may not capture the semantic meaning of the text or
    the context in which terms are used, which can be addressed by more advanced retrieval
    techniques such as dense <html:span class="No-Break">retrieval methods.</html:span></html:p>
    <html:p>As you’ve probably guessed, they have several advantages compared to <html:span
    class="No-Break">dense retrieval:</html:span></html:p> <html:ul><html:li><html:strong
    class="bold">Efficient handling of large datasets</html:strong> : Sparse retrieval
    methods, such as TF-IDF, are generally more efficient at handling large datasets.
    The inverted index structure allows fast search and retrieval of documents based
    on keyword matching, making it suitable for large collections <html:span class="No-Break">of
    text</html:span></html:li> <html:li><html:strong class="bold">High precision</html:strong>
    : Sparse methods often <html:a id="_idIndexMarker625"></html:a>provide high accuracy
    in scenarios where the exact matching of terms is critical. They excel at retrieving
    documents that contain specific keywords present in the user’s query, which is
    beneficial in applications where keyword specificity <html:span class="No-Break">is
    essential</html:span></html:li> <html:li><html:strong class="bold">Simplicity
    and interpretability</html:strong> : Sparse retrieval methods are conceptually
    simpler and more interpretable than dense methods. The fact that they rely on
    explicit keyword frequencies makes it easier to understand why certain documents
    are retrieved in response to <html:span class="No-Break">a query</html:span></html:li>
    <html:li><html:strong class="bold">Less resource intensive</html:strong> : Unlike
    dense retrieval, sparse methods do not require complex neural network models to
    generate embeddings. This makes them less resource-intensive in terms of computing
    power and memory requirements. This means they’re easier to deploy <html:span
    class="No-Break">and maintain</html:span></html:li> <html:li><html:strong class="bold">Less
    dependence on model variability</html:strong> : Because sparse retrieval doesn’t
    depend on the nuances of machine learning models to the same extent as dense retrieval,
    it’s generally more robust to variations in model quality. Performance is more
    predictable and consistent across <html:span class="No-Break">different datasets</html:span></html:li></html:ul>
    <html:p>Sparse methods also have their limitations. Some of the most important
    are <html:span class="No-Break">as follows:</html:span></html:p> <html:ul><html:li><html:strong
    class="bold">Lack of semantic understanding</html:strong> : Sparse methods may
    not capture the semantic relationships between words. They may miss documents
    that are contextually relevant but do not share exact keyword matches with <html:span
    class="No-Break">the query.</html:span></html:li> <html:li><html:strong class="bold">Vulnerability
    to synonymy and polysemy</html:strong> : These methods struggle with synonymy
    – different words with similar meanings – and polysemy – words with multiple meanings
    – leading to potential misses or <html:span class="No-Break">irrelevant retrievals.</html:span></html:li>
    <html:li><html:strong class="bold">Failure to capture context and nuance</html:strong>
    : Sparse retrieval does not effectively capture the broader context or nuances
    in language that can be critical to understanding <html:a id="_idIndexMarker626"></html:a>the
    true intent behind <html:span class="No-Break">a query.</html:span></html:li></html:ul>
    <html:a id="_idTextAnchor152"></html:a><html:h2 id="_idParaDest-153">Implementing
    sparse retrieval in LlamaIndex</html:h2> <html:p>At a core level, constructs such
    as <html:code class="literal">KeywordTableIndex</html:code> can already be considered
    a basic form of sparse retrieval. After all, they share most of the principles
    and methods described above. However, there are <html:a id="_idIndexMarker627"></html:a>even
    more advanced <html:a id="_idIndexMarker628"></html:a>sparse retrieval capabilities
    available <html:span class="No-Break">in LlamaIndex.</html:span></html:p> <html:p>A
    perfect example is <html:code class="literal">BM25Retriever</html:code> , which
    implements the <html:strong class="bold">Best Matching 25</html:strong> ( <html:strong
    class="bold">BM25</html:strong> ) <html:span class="No-Break">retrieval algorithm.</html:span></html:p>
    <html:p>BM25, a refinement of the TF-IDF method, is a more sophisticated algorithm
    that’s used for sparse retrieval. Unlike TF-IDF, BM25 takes into account both
    term frequency and document length, providing a more nuanced approach to document
    relevance scoring. With this retriever, nodes are ranked based on their BM25 scores
    relative to the query. The top-k nodes with the highest scores are returned as
    query results, providing users with the most <html:span class="No-Break">relevant
    results.</html:span></html:p> <html:p>Let’s look at an example of how we can <html:span
    class="No-Break">use</html:span> <html:span class="No-Break"><html:code class="literal">BM25Retriever</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>To use this particular
    retriever, you’ll need to install the required Python package and the corresponding
    LlamaIndex integration package by running the <html:span class="No-Break">following
    commands:</html:span></html:p> <html:p>After installing the <html:code class="literal">rank-bm25</html:code>
    package, you can test it with this <html:span class="No-Break">sample code:</html:span></html:p>
    <html:p>We’re using the two initial <html:a id="_idIndexMarker629"></html:a>sample
    files containing data about ancient Rome and different breeds of dogs. In this
    example, I’ve used <html:code class="literal">SentenceSplitter</html:code> , configured
    with a relatively small chunk size. That is because the sample file is small in
    size and I <html:a id="_idIndexMarker630"></html:a>wanted to produce more granular
    nodes structured as sentences to better exemplify the workings of <html:code class="literal">BM25Retriever</html:code>
    . Next, let’s implement <html:span class="No-Break">the retriever:</html:span></html:p>
    <html:p>After chunking the two documents, we use the retriever to apply the BM25
    algorithm and retrieve the two most relevant chunks relative to our query about
    <html:span class="No-Break">the Colosseum.</html:span></html:p> <html:p>You can
    further experiment with <html:a id="_idIndexMarker631"></html:a>this sample and
    try to adjust the <html:code class="literal">similarity_top_k</html:code> parameter,
    the query, or the chunking strategy to better understand <html:a id="_idIndexMarker632"></html:a>how
    this <html:span class="No-Break">retriever works.</html:span></html:p> <html:h3>When
    should we use sparse retrieval instead of dense retrieval?</html:h3> <html:p>Let’s
    consider a practical example <html:a id="_idIndexMarker633"></html:a>of when sparse
    retrieval might give better results than dense retrieval in an <html:span class="No-Break">RAG
    application.</html:span></html:p> <html:p class="callout-heading">A practical
    use case for sparse retrieval</html:p> <html:p class="callout">Suppose we’ve built
    a system for retrieving legal documents. In this scenario, user queries would
    likely include precise legal terms, citations, or specific phrases found in legal
    texts. Let’s assume a user inputs a query such as, “ <html:em class="italic">Article
    45 of the GDPR regarding personal data transfers on the basis of an adequacy decision.</html:em>
    ” This query contains specific phrases, such as “Article 45” and “GDPR,” which
    are likely to be found in relevant legal documents exactly in <html:span class="No-Break">this
    form.</html:span></html:p> <html:p class="callout">Sparse search is likely to
    provide very accurate results for such a query. It will accurately locate documents
    that contain the specific article from the GDPR, reducing noise and irrelevant
    retrievals. Given that legal documents often have a structured format, with different
    sections and articles, sparse retrieval methods can efficiently parse through
    this structured data and retrieve nodes based on direct references found in <html:span
    class="No-Break">the query.</html:span></html:p> <html:p>Because dense retrieval
    methods tend to prioritize general meaning over exact term matching, they may
    produce less accurate results in such a specialized, <html:span class="No-Break">keyword-specific
    query.</html:span></html:p> <html:p>Unless trained specifically on legal texts,
    an embedding model used for dense retrieval might struggle to accurately interpret
    and match the complex legal jargon and specific citation styles used in <html:span
    class="No-Break">legal queries.</html:span></html:p> <html:h3>When would dense
    retrieval be a better choice?</html:h3> <html:p>Here’s another <html:span class="No-Break">practical
    example.</html:span></html:p> <html:p>A typical use case where dense retrieval
    would most likely produce better results would be a customer support chatbot <html:a
    id="_idIndexMarker634"></html:a>designed to understand and respond to a wide range
    of customer queries. Let’s <html:a id="_idIndexMarker635"></html:a>say the chatbot
    is tasked with assisting users with various issues <html:a id="_idIndexMarker636"></html:a>related
    to technical products, such as hardware troubleshooting, software features, usage
    tips, and general inquiries about products <html:span class="No-Break">and services.</html:span></html:p>
    <html:p>A user might ask a question such as “ <html:em class="italic">My laptop
    battery is draining really quickly, even when I’m not using it much. What can
    I do about it?</html:em> ” Because dense search excels at understanding the semantic
    context of queries, in this case, it could understand the broader meaning behind
    phrases such as “battery drains really fast” and relate them to similar problems,
    even if the exact phrase isn’t in the <html:span class="No-Break">knowledge base.</html:span></html:p>
    <html:p>Sparse methods, on the other hand, may not perform well if the query doesn’t
    contain specific keywords that are present in the support documents. In our example,
    the user might describe a problem using different terms to those used in the technical
    manuals <html:span class="No-Break">or FAQs.</html:span></html:p> <html:h3>Can
    we combine the two methods in a single retriever?</html:h3> <html:p>The short
    answer is yes. You’ve probably already guessed that I’m building a case along
    these lines. By combining them, we’d get the best of both worlds in terms of benefits
    and features. A few pages ago, we talked about using selectors and routers to
    implement more complex query behavior in our <html:span class="No-Break">RAG application.</html:span></html:p>
    <html:p>I’ll leave it up to you to adapt the methods I’ve demonstrated there and
    implement a hybrid system that uses both dense and sparse retrieval methods. If
    you feel the need for an additional example, you can have a look at this one,
    which uses the Pinecone vector database to implement hybrid <html:span class="No-Break">search:</html:span>
    <html:a><html:span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/vector_stores/PineconeIndexDemo-Hybrid.html</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:h3>Dealing with empty
    results from the retrieval process</html:h3> <html:p>Sometimes, our retrievers
    may come up empty-handed, without finding any indexed content matching the current
    query. This typically means that there are no relevant nodes in the index for
    that <html:span class="No-Break">particular query.</html:span></html:p> <html:p>In
    such cases, the retriever may return an empty result set, indicating that no matching
    nodes were found. Depending on the type of index used, this situation can arise
    if the query keywords are very specific or rare, and none of the nodes in the
    index contain those exact keywords, or, in the case of embedding-based indexes,
    the similarity search that was performed during the search did not find any matching
    nodes with the current parameters used. To handle this scenario, we can consider
    <html:span class="No-Break">various approaches:</html:span></html:p> <html:ul><html:li><html:strong
    class="bold">Fallback mechanisms</html:strong> : The search system can have fallback
    strategies in place, such as performing a more general search by adjusting the
    retriever’s parameters or suggesting alternative query terms to <html:span class="No-Break">the
    user.</html:span></html:li> <html:li><html:strong class="bold">Query expansion</html:strong>
    : The query can be automatically expanded to include synonyms, related terms,
    or broader concepts to increase the chances of finding <html:span class="No-Break">relevant
    nodes.</html:span></html:li> <html:li><html:strong class="bold">Relevance scoring</html:strong>
    : Even if no exact keyword matches are found, the search system can employ relevance
    <html:a id="_idIndexMarker637"></html:a>scoring algorithms to identify nodes that
    are semantically similar <html:a id="_idIndexMarker638"></html:a>to the query
    or contain <html:span class="No-Break">partial matches.</html:span></html:li></html:ul>
    <html:a id="_idTextAnchor153"></html:a><html:h2 id="_idParaDest-154">Discovering
    other advanced retrieval methods</html:h2> <html:p>In addition to the basic concepts
    just discussed, several other advanced retrieval methods are worth familiarizing
    yourself with. There is a special section in the official documentation where
    these methods are <html:span class="No-Break">explained:</html:span> <html:a><html:span
    class="No-Break">https://docs.llamaindex.ai/en/stable/optimizing/advanced_retrieval/advanced_retrieval.html</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>There, you will learn
    more about special techniques, such as <html:em class="italic">Small-to-Big</html:em>
    <html:em class="italic">retrieval</html:em> , <html:em class="italic">recursive
    retrieval</html:em> , <html:em class="italic">retrieval from embedded tables</html:em>
    , <html:em class="italic">multi-modal retrieval</html:em> , <html:em class="italic">auto-merging
    retrieval</html:em> , <html:span class="No-Break">and others.</html:span></html:p>
    <html:p>A detailed explanation of each retrieval strategy would go far beyond
    what I intend to cover in this book, but that doesn’t mean they aren’t important.
    After all, there is no point in ingesting and indexing the original documents
    if we cannot effectively extract the context we need <html:span class="No-Break">in
    RAG.</html:span></html:p> <html:p class="callout-heading">Practical advice</html:p>
    <html:p class="callout">Always read the latest version of the official documentation
    before starting a major project. Things move so fast, and new methods and techniques
    emerge so quickly, that it is a shame to waste time reinventing the wheel. As
    an anecdote, I can tell you from personal experience that I have spent hours <html:em
    class="italic">inventing</html:em> something very similar to the <html:em class="italic">small-to-big</html:em>
    method, only to discover a few days later that it was already a tested and <html:span
    class="No-Break">documented technique.</html:span></html:p> <html:p>That’s enough
    information <html:a id="_idIndexMarker639"></html:a>for one chapter. We’ll skip
    the PITS coding practice now as we’ll let more <html:a id="_idIndexMarker640"></html:a>information
    accumulate in the next chapter before implementing additional features in our
    personal <html:span class="No-Break">tutoring project.</html:span></html:p> <html:a
    id="_idTextAnchor154"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Summary</html:title></html:head>
    <html:body><html:div class="epub-source"><html:h1 id="_idParaDest-155">Summary</html:h1>
    <html:div id="_idContainer071"><html:p>In this chapter, we explored various querying
    strategies and architectures within LlamaIndex with a deep focus on retrievers.
    Retrievers provide essential capabilities for extracting relevant information
    from indexes to generate useful responses in RAG systems. Throughout this chapter,
    we looked at basic retriever types such as <html:code class="literal">VectorIndexRetriever</html:code>
    and <html:code class="literal">SummaryIndexRetriever</html:code> . We also gained
    an understanding of advanced concepts such as asynchronous retrieval, metadata
    filters, tools, selectors, and query transformations. These allow us to build
    more sophisticated <html:span class="No-Break">retrieval logic.</html:span></html:p>
    <html:p>Additionally, we covered fundamental paradigms such as dense retrieval
    and sparse retrieval and discussed their strengths and weaknesses. Implementations
    in LlamaIndex such as BM25Retriever were <html:span class="No-Break">also introduced.</html:span></html:p>
    <html:p>Overall, this chapter provided an overview of retrieval capabilities in
    LlamaIndex, laying the foundation for building high-performance and contextually-aware
    <html:span class="No-Break">RAG applications.</html:span></html:p> <html:p>We’re
    now equipped with the necessary knowledge to effectively retrieve information
    from indexes. In the next chapter, we’ll build on this knowledge by addressing
    the other important components of a query engine: post-processors and <html:span
    class="No-Break">response synthesizers.</html:span></html:p></html:div></html:div></html:body></html:html>'
  prefs: []
  type: TYPE_NORMAL
