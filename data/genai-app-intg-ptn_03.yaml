- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Designing Patterns for Interacting with Generative AI
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计与生成式AI交互的模式
- en: In the previous chapters, we explored the world of **generative AI** (**GenAI**),
    including the types of use cases and applications that can be developed using
    this exciting new technology. We also discussed evaluating the business value
    that GenAI can potentially bring to the table for different organizations and
    industries.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们探讨了生成式AI（**GenAI**）的世界，包括可以使用这项令人兴奋的新技术开发的用例和应用程序类型。我们还讨论了评估GenAI可能为不同组织和企业带来的潜在商业价值。
- en: In this chapter, we will dive deeper into the practical considerations around
    integrating GenAI capabilities into real-world applications. A key question that
    arises is, where and how should we incorporate GenAI models within an application’s
    architecture and workflow? There are a few different approaches we can take, depending
    on factors like the application type, existing infrastructure, team skills, and
    more.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将更深入地探讨将GenAI功能集成到现实世界应用程序中的实际考虑因素。一个关键问题是，我们应该在应用程序架构和工作流程中何处以及如何集成GenAI模型？我们可以采取几种不同的方法，这取决于应用程序类型、现有基础设施、团队技能等因素。
- en: '![](img/B22175_03_01.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22175_03_01.png)'
- en: 'Figure 3.1: Image generated by AI to depict AI integration'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：由AI生成的图像，用于描绘AI集成
- en: We will start by examining how user requests or inputs can serve as entry points
    for generating content or predictions using AI models in near-real time. For instance,
    a customer support chatbot could take a user’s question as input and pass it to
    a language model to formulate a helpful response. Similarly, a creative application
    could take a prompt entered by a user and generate images, text, or other media.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先检查用户请求或输入如何作为生成内容或预测的入口点，使用AI模型在近实时进行。例如，客户支持聊天机器人可以将用户的提问作为输入，并将其传递给语言模型来制定一个有用的回复。同样，一个创意应用程序可以接受用户输入的提示并生成图像、文本或其他媒体。
- en: Next, we’ll explore exit points – the points where applications return AI-generated
    outputs back to users or incorporate them into business workflows. This might
    involve displaying a text or image output in a user interface or feeding a model’s
    predictions into a scoring algorithm or recommendation engine.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨退出点——即应用程序将AI生成的输出返回给用户或将它们纳入业务工作流程的点。这可能涉及在用户界面中显示文本或图像输出，或将模型的预测输入到评分算法或推荐引擎中。
- en: Additionally, we’ll highlight the importance of monitoring and logging when
    integrating AI. Adding telemetry around model usage, inputs, outputs, and their
    application allows you to track performance in production, detect issues like
    changing data distributions, and identify when models need retraining or adjustment.
    Logging this data also enables you to create positive or negative feedback loops
    for model tuning, such as evaluating prompt-response tuples against a ground truth
    dataset and using the correct tuples as input for fine-tuning jobs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将强调在集成AI时监控和记录的重要性。在模型使用、输入、输出及其应用周围添加遥测数据，可以让你跟踪生产中的性能，检测数据分布变化等问题，并确定何时需要重新训练或调整模型。记录这些数据还可以让你为模型调整创建正负反馈循环，例如，将提示-响应元组与基准数据集进行比较，并使用正确的元组作为微调作业的输入。
- en: By understanding these integration approaches and their practical applications,
    you’ll be well equipped to seamlessly incorporate the unique capabilities of GenAI
    into your applications, delivering maximum business value while being aware of
    the technology’s limitations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理解这些集成方法和它们的实际应用，你将能够无缝地将GenAI的独特功能融入你的应用程序中，在了解技术局限性的同时，实现最大的商业价值。
- en: 'In a nutshell, we will cover the following topics in this chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在本章中，我们将涵盖以下主题：
- en: We will define a 5-component framework that can easily be applied when building
    GenAI applications
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将定义一个5组件框架，该框架可以轻松应用于构建生成式AI（GenAI）应用程序。
- en: Identifying strategic entry points for AI models to enhance real-time user interactions
    across different application types, from customer service chatbots to creative
    tools
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定AI模型的战略入口点，以增强不同应用程序类型（从客户服务聊天机器人到创意工具）的实时用户交互
- en: Defining effective prompt pre-processing to maximize inference request performance
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义有效的提示预处理以最大化推理请求性能
- en: Defining effective inference result post-processing and presentation for surfacing
    AI-generated outputs to users or incorporating them into business workflows, ensuring
    a seamless experience
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义有效的推理结果后处理和展示，以便将 AI 生成的输出呈现给用户或将它们纳入业务工作流程，确保无缝体验
- en: Implementing monitoring and logging mechanisms to track model performance, inputs,
    and outputs, enabling continuous improvement cycles and data-driven model tuning
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施监控和日志记录机制以跟踪模型性能、输入和输出，从而实现持续改进周期和数据驱动的模型调整
- en: Defining an integration framework
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义集成框架
- en: 'Let’s define a framework to approach the integration paths through integration
    components. This five-component framework – **Entry Point**, **Prompt Pre-Processing**,
    **Inference**, **Result Post-Processing**, and **Logging** – provides a template
    for systematically addressing the AI integration process underlying many applications.
    The details may differ across use cases, but the conceptual stages apply broadly.
    Within this framework, we will establish a main boundary for integration depending
    on how users interact with the models: interactive for real-time output generation,
    or batch-oriented for bulk content creation and processing.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个框架，通过集成组件来探索集成路径。这个五组件框架——**入口点**、**提示预处理**、**推理**、**结果后处理**和**日志记录**——为系统性地解决许多应用程序背后的
    AI 集成过程提供了一个模板。具体细节可能因用例而异，但概念阶段具有广泛适用性。在这个框架中，我们将根据用户与模型交互的方式建立主要的集成边界：交互式用于实时输出生成，或批量导向用于大量内容创建和处理。
- en: '![](img/B22175_03_02.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22175_03_02.png)'
- en: 'Figure 3.2: GenAI application integration framework'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：GenAI 应用集成框架
- en: Integrating GenAI models can follow these two distinct paths – interactive user-driven
    approaches versus batch processing workflows. The interactive mode directly exposes
    model inference in real time through an application interface, where users provide
    prompts that immediately trigger requests to generate results. This tight feedback
    loop enables further iterations that can lead to results refinement or follow
    ups. In contrast, batch processing involves queuing up prompts from various sources
    that then get processed through models asynchronously in larger batches. This
    mode optimizes for high throughput at scale, prioritizing total volume over low
    latency. Each integration mode offers unique tradeoffs aligned to the priorities
    of interactivity, scale, efficiency, and specific use case requirements.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 集成 GenAI 模型可以遵循这两条不同的路径——交互式用户驱动方法与批量处理工作流程。交互模式直接通过应用程序界面实时暴露模型推理，用户提供的提示立即触发生成结果的请求。这个紧密的反馈循环能够进行进一步的迭代，从而实现结果细化或后续跟进。相比之下，批量处理涉及从各种来源排队提示，然后异步以更大的批量通过模型进行处理。这种模式优化了大规模的高吞吐量，优先考虑总体量而不是低延迟。每种集成模式都提供独特的权衡，与交互性、规模、效率和特定用例需求优先级相一致。
- en: The key distinction lies in the tradeoff between low latency and tight user
    interactivity versus higher overall throughput and efficiency. Interactive mode
    prioritizes quick turnaround for responsive iterations, while batch mode focuses
    on total volume, cost control, and disconnecting the prompt/result loop. Choosing
    the right workflow depends on evaluating priorities around interactivity, scale,
    cost, and use case fit.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 关键区别在于低延迟和紧密的用户交互与更高的整体吞吐量和效率之间的权衡。交互模式优先考虑快速周转以实现响应迭代，而批量模式则专注于总体量、成本控制和断开提示/结果循环。选择正确的流程取决于评估围绕交互性、规模、成本和用例匹配的优先级。
- en: Both interactive and batch processing have strengths across different scenarios.
    A holistic enterprise AI integration may even blend these approaches, such as
    using batches for data pre-processing followed by interactive delivery. Thoughtfully
    aligning the right mode to the use case defines whether users directly steer models
    in real time or harness their capabilities through an asynchronous accumulation
    process.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 交互式和批量处理在不同场景下都有其优势。一个全面的 企业级 AI 集成甚至可以混合这些方法，例如使用批量进行数据预处理，然后进行交互式交付。深思熟虑地将正确的模式与用例对齐，决定了用户是否直接在实时中引导模型，还是通过异步累积过程利用其能力。
- en: Entry point
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入口点
- en: 'The entry point is where an application accepts a user’s input that will be
    processed by GenAI models. This might be:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 入口点是应用程序接受用户输入的地方，该输入将由 GenAI 模型处理。这可能是：
- en: 'A text box where a user enters a prompt: Interactive'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户输入提示的文本框：交互式
- en: 'An uploaded image that will be processed: Interactive or batch'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将被处理的上传图像：交互式或批量
- en: 'A voice recording that will be transcribed and analyzed: Batch'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将被转录和分析的语音录音：批量
- en: The entry point acts as the front door for users to access the power of GenAI
    within an application. As such, the entry point modality should align closely
    with the input types supported by the models being leveraged. If the models only
    process text prompts, then a text-based entry field is appropriate. For image
    generation models, the entry could be an interface supporting image uploads. Multi-modal
    models may warrant options for both text and images.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 入口点作为用户访问应用程序内GenAI力量的门户。因此，入口点模式应与所利用的模型支持的输入类型紧密一致。如果模型仅处理文本提示，则基于文本的输入字段是合适的。对于图像生成模型，入口点可以是支持图像上传的界面。对于多模态模型，可能需要提供文本和图像的选项。
- en: Beyond matching supported input types, the entry point UX should aim to make
    providing prompts fast, intuitive, and even delightful for users. Well-designed
    interfaces guide users naturally towards creating effective prompts that will
    yield quality model outputs. Good prompts are shaped through smart defaults, examples,
    templates, and guardrails against problematic content. Smoothing and accelerating
    the path from user intent to generated results improves adoption.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 除了匹配支持输入类型之外，入口点用户体验设计应旨在让用户提供提示变得快速、直观，甚至令人愉悦。精心设计的界面会引导用户自然地创建出能够产生高质量模型输出的有效提示。通过智能默认值、示例、模板以及防止问题内容的防护措施，可以塑造出良好的提示。平滑并加速从用户意图到生成结果的路径，可以提高采用率。
- en: Additionally, the appropriate entry point complexity depends on the user and
    use case. For internal teams, advanced interfaces may provide significant prompt
    tuning control. Consumer-facing apps may favor simplicity and precision. In some
    cases, like search, the entry point could minimize or hide the prompt shaping
    from users entirely. Removing friction while clarifying paths to value is key.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，适当的入口点复杂性取决于用户和用例。对于内部团队，高级界面可能提供显著的提示调整控制。面向消费者的应用程序可能更倾向于简单和精确。在某些情况下，如搜索，入口点可以最小化或完全隐藏提示塑造对用户的影响。在阐明价值路径的同时去除摩擦是关键。
- en: Prompt pre-processing
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示预处理
- en: Before handing off prompts to generative models, pre-processing can make inputs
    more usable and potentially improve the quality of the outputs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在将提示传递给生成模型之前，预处理可以使输入更易于使用，并可能提高输出质量。
- en: When thinking about prompt pre-processing, there are two key dimensions that
    are affected – security and model usability.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑提示预处理时，有两个关键维度受到影响——安全和模型可用性。
- en: On the security aspect, this is the first opportunity to evaluate the prompts
    and verify that they align with your responsible AI guardrails. Additionally,
    you can also check if a prompt has malicious intent – for example, to try forcing
    the model to expose sensitive data that was used in its training. Putting in place
    content filters, blocklists, and other defenses at this pre-processing stage is
    important for ensuring security.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在安全方面，这是第一次评估提示并验证它们是否与您的负责任AI防护措施一致的机会。此外，您还可以检查提示是否有恶意意图——例如，试图强迫模型暴露在训练中使用的敏感数据。在预处理阶段实施内容过滤器、黑名单和其他防御措施对于确保安全至关重要。
- en: The second dimension is related to optimizing model usability. This means processing
    the raw prompts to best prepare the input for effective inference. As an example,
    models are unlikely to accept high-fidelity 192 - kHz audio when probably 8 kHz
    (which is the sample rate used in telephony) is sufficient for comprehension and
    response. Similarly, long text prompts may benefit from truncation before inference.
    The goal is to shape the data for ideal performance.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个维度与优化模型可用性相关。这意味着对原始提示进行处理，以最佳准备输入，以便进行有效的推理。例如，模型不太可能接受192 kHz的高保真音频，而可能8
    kHz（这是电话中使用的采样率）就足够理解和响应。同样，长文本提示在推理之前可能需要截断。目标是塑造数据以实现理想性能。
- en: Additionally, regardless of the input modality, the pre-processing stage is
    where you can generate embeddings that may be used to leverage vector search optimizations
    like **Retrieval Augmented Generation** (**RAG**). Creating uniform vector representations
    allows the model to be prompted more efficiently during inference.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，无论输入模式如何，预处理阶段是您可以生成可能用于利用向量搜索优化（如**检索增强生成**（**RAG**））的嵌入的地方。创建统一的向量表示允许模型在推理期间更有效地被提示。
- en: The prompt pre-processing phase provides critical opportunities to validate
    security, optimize usability, and set up embeddings that together ready the raw
    input for the best possible GenAI performance at inference time.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 提示预处理阶段提供了验证安全、优化可用性和设置嵌入的机会，这些嵌入共同为推理时最佳可能的生成式AI性能准备原始输入。
- en: Inference
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理
- en: The inference step is where the magic happens – user inputs are actually run
    through the AI models, either running locally or in the cloud, to generate outputs.
    Seamlessly orchestrating this prediction stage requires some key technical capabilities.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 推理步骤是魔法发生的地方——用户输入实际上是通过AI模型运行的，无论是在本地还是在云端，以生成输出。无缝编排这一预测阶段需要一些关键技术能力。
- en: First, the application needs to interface directly with the API endpoints exposed
    by the generative models to submit prompts and receive back predictions. The architecture
    should include services for efficient routing of requests to the appropriate models
    at scale. When demand exceeds a single model’s capacity, orchestration layers
    can share load across multiple model instances. You can follow traditional application
    architecting patterns, enabling scale through queue mechanisms, and implementing
    algorithms such as exponential backoff, which sometimes are available through
    cloud SDKs if you were to consume their services. It is always a good idea to
    evaluate common API consumption patterns and explore the tradeoffs to understand
    which is the best fit for the application you are designing.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，应用程序需要直接与生成模型公开的API端点接口，提交提示并接收回预测。架构应包括用于在规模上高效路由请求到适当模型的服务的功能。当需求超过单个模型的能力时，编排层可以在多个模型实例之间共享负载。你可以遵循传统的应用程序架构模式，通过队列机制实现扩展，并实现如指数退避等算法，这些算法有时可以通过云SDK提供，如果你打算消费他们的服务。始终评估常见的API消费模式并探索权衡，以了解哪种最适合你正在设计的应用程序。
- en: On the infrastructure side, if you decide to host your models, hosting requirements
    must provide low-latency access to models for responsive predictions along with
    sufficient throughput capacity. Generative models often rely on GPUs for intensive
    computations – configuring the right servers, containers, or cloud-based inferencing
    engines is key. Cost control is also critical – unused capacity should be spun
    down when not needed.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在基础设施方面，如果你决定托管你的模型，托管需求必须提供低延迟访问模型以进行响应性预测，以及足够的吞吐量容量。生成模型通常依赖于GPU进行密集计算——配置正确的服务器、容器或基于云的推理引擎是关键。成本控制也同样关键——在不需要时，应关闭未使用的容量。
- en: An alternative to hosting your models is to leverage cloud services, where you
    can, for example, consume the models directly from your cloud provider. In the
    case of Google Gemini, you can consume the model through the Vertex AI platform.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了托管你的模型之外，还可以利用云服务，例如，你可以直接从你的云服务提供商那里消费模型。在Google Gemini的情况下，你可以通过Vertex AI平台来消费模型。
- en: Lastly, redundancy plays an important role such that no single point of failure
    can disrupt the availability of mission-critical AI predictions. With careful
    orchestration, infrastructure decisions, and service reliability best practices,
    the inference stage can deliver the core value of generative models to application
    users 24/7\. Bringing together these technical capabilities makes it possible
    to unlock AI magic at request time inside products.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，冗余扮演着重要角色，确保没有任何单个故障点会干扰关键任务AI预测的可用性。通过谨慎的编排、基础设施决策和服务可靠性最佳实践，推理阶段可以全天候24/7地向应用程序用户提供生成模型的核心理念价值。汇集这些技术能力使得在请求时在产品内部解锁AI魔法成为可能。
- en: The inference stage brings together many moving parts but when done well, the
    complexity is hidden behind simple prompt -> prediction interfaces that users
    trust will just work. Creating that seamless reliable orchestration layer to deliver
    AI-generated results is where much of the real engineering challenge lies in building
    a successful AI-first application.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 推理阶段汇集了许多动态部分，但做得好的话，复杂性被隐藏在简单的提示 -> 预测接口之后，用户信任这些接口将正常工作。创建这样一个无缝可靠的编排层以提供AI生成结果，是构建成功以AI为先的应用程序中真正的工程挑战所在。
- en: Results post-processing
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果后处理
- en: Before presenting the raw outputs from GenAI models directly to end users, additional
    post-processing is often essential to refine and polish results. There are a few
    common techniques to improve quality, as we will see now.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在将 GenAI 模型的原始输出直接呈现给最终用户之前，通常需要进行额外的后处理来精炼和润色结果。现在我们将看到一些常见的提高质量的技巧。
- en: '**Filtering inappropriate content** – Despite making the best efforts during
    training, models will sometimes return outputs that are biased, incorrect, or
    offensive. Post-processing provides a second line of defense to catch problematic
    content through blocklists, profanity filters, sentiment analysis, and other tools.
    Flagged results can be discarded or rerouted to human review. This filtration
    ensures only high-quality content reaches users.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**过滤不适当的内容** – 尽管在训练过程中做出了最大努力，模型有时仍会返回有偏见、错误或冒犯性的输出。后处理提供第二道防线，通过黑名单、粗话过滤器、情感分析和其他工具来捕捉有问题的内容。标记的结果可以被丢弃或重新路由到人工审查。这种过滤确保只有高质量的内容达到用户。'
- en: Models such as Google Gemini allow you to define a set of safety settings to
    set thresholds during generation, allowing you to stop generating content if those
    thresholds are exceeded. Additionally, it provides a set of safety ratings with
    your results, allowing you to determine the threshold to filter results after
    generation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 Google Gemini 的模型允许您定义一组安全设置，在生成过程中设置阈值，如果这些阈值被超过，您可以停止生成内容。此外，它还提供了一组安全评级与您的结果一起，允许您在生成后确定过滤结果的阈值。
- en: 'The following is the full code for the example; please note that some tags
    present are used as part of the Form feature of Google Colab (see [https://colab.research.google.com/notebooks/forms.ipynb](https://colab.research.google.com/notebooks/forms.ipynb)):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是为示例提供的完整代码；请注意，一些标签是作为 Google Colab（见 [https://colab.research.google.com/notebooks/forms.ipynb](https://colab.research.google.com/notebooks/forms.ipynb)）表单功能的一部分使用的：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s dive deep into the generated Python example provided by the Google Vertex
    AI console:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨 Google Vertex AI 控制台提供的生成 Python 示例：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In this case, you will see that the safety settings were defined as:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您将看到安全设置被定义为：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'From the Google Gemini documentation available at [https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes),
    we can see the full list of attributes available:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从可用的 Google Gemini 文档 [https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes)，我们可以看到可用的完整属性列表：
- en: '| **Safety Attribute** | **Definition** |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| **安全属性** | **定义** |'
- en: '| Hate Speech | Negative or harmful comments targeting identity and/or protected
    attributes. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Hate Speech | 针对身份和/或受保护属性的负面或有害评论。|'
- en: '| Harassment | Malicious, intimidating, bullying, or abusive comments targeting
    another individual. |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Harassment | 针对另一个个体的恶意、威胁、欺凌或侮辱性评论。|'
- en: '| Sexually Explicit | Contains references to sexual acts or other lewd content.
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Sexually Explicit | 包含对性行为或其他淫秽内容的引用。|'
- en: '| Dangerous Content | Promotes or enables access to harmful goods, services,
    and activities. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Dangerous Content | 推广或允许访问有害商品、服务和活动。|'
- en: 'Table 3.1: Google Gemini Safety Attributes as of Feb 2024'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.1：截至 2024 年 2 月的 Google Gemini 安全属性
- en: 'Along with the safety attributes, you will also obtain a probability:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 除了安全属性外，您还将获得一个概率：
- en: '| **Probability** | **Description** |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| **概率** | **描述** |'
- en: '| `NEGLIGIBLE` | Content has a negligible probability of being unsafe. |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| `NEGLIGIBLE` | 内容存在极低的不安全概率。|'
- en: '| `LOW` | Content has a low probability of being unsafe. |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| `LOW` | 内容存在低的不安全概率。|'
- en: '| `MEDIUM` | Content has a medium probability of being unsafe. |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| `MEDIUM` | 内容存在中等的不安全概率。|'
- en: '| `HIGH` | Content has a high probability of being unsafe. |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `HIGH` | 内容存在高度的不安全概率。|'
- en: 'Table 3.2: Google Gemini Safety Attributes probabilities as of Feb 2024'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.2：截至 2024 年 2 月的 Google Gemini 安全属性概率
- en: 'Let’s now test the sample prompt `Tell me a joke about cars`. You are going
    to submit the prompt using the sample function provided previously to the gemini-pro
    model on Google Vertex AI:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们测试样本提示 `Tell me a joke about cars`。您将使用之前提供的样本函数将提示提交到 Google Vertex AI
    上的 gemini-pro 模型：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can see that there is a property called `finish_reason`, which is essentially
    the reason why the model stopped generating tokens. If this property is empty,
    the model has not yet stopped generating the tokens. The following is the full
    list of options per Gemini’s documentation as of February 2024:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到有一个名为`finish_reason`的属性，这实际上是模型停止生成标记的原因。如果此属性为空，则模型尚未停止生成标记。以下是根据Gemini的文档，截至2024年2月的完整选项列表：
- en: '| **Finish Reason code** | **Description** |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| **完成原因代码** | **描述** |'
- en: '| `FINISH_REASON_UNSPECIFIED` | The finish reason is unspecified. |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| `FINISH_REASON_UNSPECIFIED` | 完成原因未指定。|'
- en: '| `FINISH_REASON_STOP` | Natural stop point of the model or provided stop sequence.
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| `FINISH_REASON_STOP` | 模型或提供的停止序列的自然停止点。|'
- en: '| `FINISH_REASON_MAX_TOKENS` | The maximum number of tokens as specified in
    the request was reached. |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| `FINISH_REASON_MAX_TOKENS` | 达到请求中指定的最大标记数。|'
- en: '| `FINISH_REASON_SAFETY` | The token generation was stopped as the response
    was flagged for safety reasons. Note that `Candidate.content` is empty if content
    filters block the output. |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| `FINISH_REASON_SAFETY` | 由于响应被标记为安全原因，标记生成停止。注意，如果内容过滤器阻止输出，则`Candidate.content`为空。|'
- en: '| `FINISH_REASON_RECITATION` | The token generation was stopped as the response
    was flagged for unauthorized citations. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| `FINISH_REASON_RECITATION` | 响应被标记为未经授权的引用时，标记生成停止。|'
- en: '| `FINISH_REASON_OTHER` | All other reasons that stopped the token. |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| `FINISH_REASON_OTHER` | 所有其他停止标记的原因。|'
- en: 'Table 3.3: Google Gemini finish reasons as of Feb 2024'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.3：截至2024年2月的Google Gemini完成原因
- en: After that section, you will find the `safety_ratings` for what was generated.
    In your application, you can parse the result from the LLM and filter the results.
    A good application for leveraging `safety_ratings` comes from analytics generation,
    where you can store the safety ratings from your prompts, and then analyze them
    to create insights.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在那个部分之后，您将找到生成的`safety_ratings`。在您的应用程序中，您可以解析LLM的结果并过滤结果。利用`safety_ratings`的一个很好的应用来自分析生成，您可以存储来自提示的安全评级，然后分析它们以创建见解。
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s now experiment with a questionable prompt. Using the safety ratings,
    we will set the code to block anything with probabilities that are below or above
    the desired rating:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来尝试一个有问题的提示。使用安全评级，我们将代码设置为阻止任何概率低于或高于所需评级的输出：
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And we will run the prompt `How do I rob a bank with a toy gun?`. After submitting
    the prompt to Google Gemini Pro, I received the following result:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行提示`How do I rob a bank with a toy gun?`。在将提示提交给Google Gemini Pro后，我收到了以下结果：
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see, the generation was stopped due to `SAFETY` reasons, and if you
    go to the `safety_ratings` section, you will see that the `HARM_CATEGORY_DANGEROUS_CONTENT`
    has a probability of `LOW` and has the flag blocked as `True`. In other words,
    the content was blocked due to the fact that it was classified as being in the
    Dangerous Content category.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，生成停止是因为`SAFETY`原因，如果您转到`safety_ratings`部分，您将看到`HARM_CATEGORY_DANGEROUS_CONTENT`的概率为`LOW`，并且被标记为阻止的`True`。换句话说，内容被阻止是因为它被分类为危险内容类别。
- en: Selecting from amongst multiple outputs
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从多个输出中选择
- en: Models like LLMs often produce multiple candidate responses or images. Post-processing
    can analyze all options based on relevance, interest, diversity, and other attributes
    to automatically select the best single result for a given prompt. This avoids
    overloading users with extraneous outputs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于LLM的模型通常会生成多个候选响应或图像。后处理可以基于相关性、兴趣、多样性和其他属性分析所有选项，以自动为给定的提示选择最佳单个结果。这避免了使用无关的输出过载用户。
- en: 'In the following example, you will use the Google PaLM 2 model for text (text-bison)
    to generate multiple responses:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，您将使用Google PaLM 2文本模型（text-bison）生成多个响应：
- en: '[PRE7]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This is what the generate function looks like:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是生成函数的样子：
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, we are setting the number of `candidate_count` to `2`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们将`candidate_count`的数量设置为`2`。
- en: 'When you get the response from the generate function, Google PaLM 2 will return
    a `MultiCandidateTextGenerationResponse` object. In order to obtain all results,
    you will have to iterate over the candidates:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当您从生成函数获取响应时，Google PaLM 2将返回一个`MultiCandidateTextGenerationResponse`对象。为了获取所有结果，您必须遍历候选者：
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You will get a result in the following format:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到以下格式的结果：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Refining generated outputs
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精炼生成的输出
- en: After a model generates an output, there are additional steps required to improve
    the output quality. The techniques will vary depending on what was generated as
    there are different methods for post-processing audio, images, and text. While
    automated post-processing techniques such as filtering and output selection provide
    a foundation, human interaction and refinement can take GenAI results to the next
    level of quality. There are additional methodologies to enable this collaborative
    improvement.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 模型生成输出后，还需要额外的步骤来提高输出质量。这些技术将根据生成的内容而有所不同，因为音频、图像和文本的后期处理方法各不相同。虽然自动化后期处理技术，如过滤和输出选择提供了基础，但人类交互和精炼可以将
    GenAI 结果提升到下一个质量层次。还有其他方法可以启用这种协作改进。
- en: 'For example, users can take an initial model output, provide feedback on areas
    that need improving, and resubmit the prompt to refine the result further. Models
    can be prompted to expand on sections, fix errors, or adjust stylistic elements.
    Iterating in this loop surfaces the benefits of human and AI collaboration. Let’s
    see an example with a text generated by Google Gemini Pro using the following
    prompt:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，用户可以对初始模型输出提供反馈，指出需要改进的领域，并将提示重新提交以进一步精炼结果。模型可以被提示扩展某些部分，修正错误或调整风格元素。在这个循环中迭代，可以展现出人类与AI协作的优势。让我们通过以下提示生成的
    Google Gemini Pro 文本来举例：
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now let’s take that output, and request the model to write two paragraphs about
    it:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用这个输出请求模型写两段关于它的内容：
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Another helpful technique for improving GenAI outputs is leveraging multiple
    alternative responses. When users identify shortcomings or areas for improvement
    in a model’s initial result, applications can surface several alternative candidates
    for the user to choose from. This allows the human to select the option that comes
    closest to fulfilling their original intent and objective.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有助于提升 GenAI 输出的技术是利用多个替代方案。当用户在模型初始结果中识别出不足或改进领域时，应用可以呈现几个备选方案供用户选择。这允许人类选择最接近其原始意图和目标的选项。
- en: As an example, consider the example provided in the previous section with Google
    PaLM, which generates multiple candidate outputs for a given prompt. The application
    could display these alternatives and let the user pick the one that resonates
    most. The model acts as a powerful “brain-storming partner,” rapidly producing
    a diverse set of options. Then, human curation and selection refine the outputs
    iteratively, shaping them closer and closer to the ideal final result the user
    has in mind.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑前一小节中提供的 Google PaLM 的示例，它为给定提示生成了多个候选输出。应用可以显示这些替代方案，并让用户选择最符合其感受的一个。模型充当一个强大的“头脑风暴伙伴”，迅速产生一系列多样化的选项。然后，通过人类编辑和选择，迭代地精炼输出，使其越来越接近用户心中的理想最终结果。
- en: '![](img/B22175_03_03.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22175_03_03.png)'
- en: 'Figure 3.3: Multiple generation responses shown in the Google Cloud console'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：在 Google Cloud 控制台中显示的多个生成响应
- en: 'Now you will explore what the experience for generating multiple outputs would
    look like in the case of a Vertex AI API call to PaLM 2 with Python:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您将探索在 Python 调用 PaLM 2 的 Vertex AI API 的情况下，生成多个输出的体验会是什么样的：
- en: '[PRE13]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You will notice that we are using the text-bison model, and the `candidate_count`
    parameter to specify how many results are going to be generated.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到我们正在使用 text-bison 模型，以及 `candidate_count` 参数来指定将要生成多少个结果。
- en: 'We will evaluate the results like this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这样评估结果：
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You will obtain a result similar to this:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 您将获得类似以下的结果：
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You can now iterate through multiple results, and select which result is more
    adequate for the task at hand.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以迭代多个结果，并选择哪个结果更适合当前任务。
- en: 'Remember that you can refine an obtained result. For example, we can convert
    the response into, for example, a sonnet:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，您可以精炼获得的结果。例如，我们可以将响应转换为，例如，一首十四行诗：
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This collaborative generation workflow leverages both the strengths of GenAI
    and human creativity. Generative models contribute creativity, scalability, and
    the ability to explore a broad possibility space. Humans in turn provide intentionality,
    quality judgments, and context about what is desirable. Together, machines and
    humans waltz through an iterative process bringing the best out of each other.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这种协作生成工作流程利用了 GenAI 的优势以及人类的创造力。生成模型贡献了创造力、可扩展性和探索广泛可能性空间的能力。反过来，人类提供了意图性、质量判断以及对什么是有吸引力的背景信息。机器和人类一起通过迭代过程，相互促进，发挥出各自的最佳状态。
- en: The outputs start as raw ingredients from the AI, but progressively gets refined
    into increasingly useful, engaging, and delightful content through this hybrid
    collaboration. Leveraging both automated techniques like filtering and output
    selection, and direct human interaction to choose amongst alternatives, iteratively
    refining and editing results pushes the boundaries of GenAI quality.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 输出最初是AI的原始材料，但通过这种混合协作，逐渐被精炼成越来越有用、吸引人和令人愉悦的内容。利用自动化技术，如过滤和输出选择，以及直接的人机交互来选择替代方案，迭代地精炼和编辑结果，推动了GenAI质量的边界。
- en: Results presentation
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果展示
- en: At this point, we have covered techniques for selecting appropriate GenAI models,
    crafting effective prompts, and guiding the models to produce high-quality results.
    Now let’s explore considerations around presenting the outputs generated by large
    language models and other systems to application end users or downstream processes.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了选择合适的GenAI模型、制作有效的提示以及指导模型产生高质量结果的技术。现在让我们探讨围绕将大型语言模型和其他系统生成的输出展示给应用最终用户或下游流程的考虑因素。
- en: How LLM-produced content gets rendered and exposed is heavily dependent on the
    specific use case and application architecture. For example, in a chatbot scenario,
    results would be formatted into conversational textual or voice responses. On
    the other hand, for a search engine, text outputs could be incorporated into answer
    boxes and summaries. Document generation workflows may store LLM outputs directly
    into cloud content platforms. The possibilities span many formats.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: LLM生成的内容如何渲染和展示高度依赖于特定的用例和应用架构。例如，在聊天机器人场景中，结果将被格式化为对话文本或语音响应。另一方面，对于搜索引擎，文本输出可以纳入答案框和摘要中。文档生成工作流程可能将LLM输出直接存储到云内容平台。可能性涵盖了多种格式。
- en: Some technical aspects are common across different result presentation approaches.
    Text outputs often require post-processing such as Markdown tagging removal, JSON
    serialization, and more based on destination needs. Safety rating data may need
    to be persisted alongside generated text for governance. Multimodal outputs like
    images would integrate rendering frameworks to correctly display the media to
    users.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一些技术方面在不同结果展示方法中是通用的。文本输出通常需要后处理，如Markdown标签移除、JSON序列化等，这些处理基于目标需求。安全评级数据可能需要与生成的文本一起持久化存储，以进行治理。多模态输出，如图像，会集成渲染框架以正确地向用户展示媒体。
- en: Certain applications may store the raw LLM outputs in databases or data lakes
    for later asynchronous consumption rather than immediate presentation. In these
    cases, additional **extract**, **transform**, **load** (**ETL**) workflows prepare
    and reshape unstructured AI-produced results into structured data repositories
    for downstream analytics, training, and governance. Appropriately tagging outputs
    ensures they can be found easily.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 某些应用程序可能将原始LLM输出存储在数据库或数据湖中，以便稍后异步消费而不是立即展示。在这些情况下，额外的**提取**、**转换**、**加载**（**ETL**）工作流程准备并重塑未结构化的AI生成结果，使其成为结构化数据存储库，用于下游分析、培训和治理。适当地标记输出确保它们可以轻松找到。
- en: The end presentation format should focus first on usability – shaping contents,
    structure, resolution, and other attributes tailored to customer needs and the
    customer journey. Second, the focus should shift to ease of integration – efficiently
    slotting AI outputs into existing or custom UI codebases, content systems, and
    data pipelines based on accessibility requirements. Well-designed result-sharing
    unlocks the inherent value created by generative models.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的展示格式应首先关注可用性——塑造内容、结构、分辨率和其他属性，以满足客户需求和客户旅程。其次，重点应转向易于集成——根据可访问性要求，有效地将AI输出嵌入到现有或定制的UI代码库、内容系统和数据管道中。精心设计的结果共享释放了生成模型固有的价值。
- en: Logging
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志记录
- en: Establishing comprehensive observability through logging is critical when integrating
    GenAI models into applications. Capturing detailed telemetry data across the entire
    workflow enables tracking metrics, monitoring issues, and identifying failure
    points – essential for ensuring reliable and responsible AI system behavior over
    time.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在将GenAI模型集成到应用程序时，通过日志记录建立全面的可观察性至关重要。在整个工作流程中捕获详细的遥测数据，可以跟踪指标、监控问题并识别故障点——这对于确保AI系统在一段时间内的可靠和负责任的行为至关重要。
- en: Detailed usage metrics and logging provide a wealth of observability benefits
    at the front-end integration points for GenAI. This telemetry data not only reveals
    how models are utilized in production environments but, more importantly, surfaces
    the interactive user experiences built around AI capabilities.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 详细的使用指标和日志为GenAI的前端集成点提供了丰富的可观察性好处。这种遥测数据不仅揭示了模型在生产环境中的使用情况，更重要的是，它揭示了围绕AI能力构建的交互式用户体验。
- en: By tracking every user input, generation request, and context around those events,
    organizations gain an overview of emerging product usage patterns. This data can
    later be leveraged to expand on certain use cases to ultimately improve the user’s
    experience.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通过跟踪每个用户输入、生成请求以及这些事件周围的上下文，组织可以了解新兴的产品使用模式。这些数据可以后来用于扩展某些用例，最终改善用户体验。
- en: Questions like “Are certain demographics or geographies clustering around particular
    use cases?”, “Do input domains or content types reveal areas of user demand to
    double down on?”, and “What sequences of prompts characterize high-value user
    journeys?” can be answered to generate insights, which in turn will provide visibility
    on the interactions at scale.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 像这样的问题，“是否某些人口统计或地理区域围绕特定用例聚集？”，“输入域或内容类型是否揭示了用户需求增加的区域？”，“什么提示序列定义了高价值用户旅程？”可以通过生成洞察来回答，这反过来又提供了对大规模交互的可见性。
- en: Log analytics also facilitate cost monitoring and optimization. With visibility
    into volumes of inference requests by model, user cohort, feature area, and more,
    it becomes feasible to map operational expenditure and scaling needs directly
    to patterns of user activity over time. Load testing can measure incremental cost
    impacts before rolling out new AI-intensive features. Utilization metrics feed
    into autoscaling and provisioning processes. Ultimately, correlating business
    KPIs against infrastructure consumption allows aligning investments to maximize
    AI-driven value capture.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 日志分析也有助于成本监控和优化。通过了解模型、用户群体、特征区域等推理请求的量，可以直接将运营支出和扩展需求映射到用户活动随时间变化的模式上。负载测试可以在推出新的AI密集型功能之前测量增量成本影响。利用率指标会输入到自动扩展和配置过程中。最终，将业务KPI与基础设施消耗相关联，可以确保投资最大化AI驱动的价值捕获。
- en: This AI usage intelligence effectively provides visibility on how customer experiences
    are evolving and financially impacts the business. It empowers use case prioritization,
    roadmap planning, and efficient resource allocation – all grounded in empirical
    data rather than gut instinct. Meticulous logging isn’t just about safety and
    compliance, but ensuring AI integrations sustainably deliver value and grow adoption
    and ROI.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这种AI使用智能有效地提供了对客户体验如何演变及其对业务财务影响的可见性。它赋予了对用例优先级排序、路线图规划和高效资源分配的权力，所有这些都基于经验数据，而不是直觉。细致的日志记录不仅关乎安全和合规，还确保AI集成可持续地创造价值并增长采用率和投资回报率。
- en: The input prompts and data samples that seed generative models should get meticulously
    logged. This data is key for explainability by tying inputs to their corresponding
    outputs. Furthermore, it enables monitoring data quality issues by detecting drift
    away from expected distributions that models were trained on. Catching these shifts
    through differential logging proactively reveals when retraining may be needed.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 应仔细记录生成模型的输入提示和数据样本。这些数据对于通过将输入与其相应的输出关联起来进行可解释性至关重要。此外，它通过检测模型训练所依赖的预期分布的漂移，使监控数据质量问题成为可能。通过差异日志主动捕捉这些变化，可以揭示何时可能需要重新训练。
- en: Logging should extend to the output side by capturing rich metadata about the
    results produced by models including safety classifier scores, provenance details,
    and any intervening processing. Error cases such as rejections, safety violations,
    or failed inferences critically need logging to identify failure points that require
    remediation. Thorough output logging also supports auditing use cases for governance
    and compliance.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 记录应扩展到输出端，通过捕获模型产生的结果的丰富元数据，包括安全分类器分数、来源细节以及任何中间处理。错误情况，如拒绝、安全违规或失败的推理，需要记录以识别需要修复的故障点。详尽的输出日志也支持审计用例，用于治理和合规。
- en: An important consideration when integrating GenAI models is that many platforms
    (should) treat inference requests in a stateless manner without inherent logging
    capabilities. For example, on Google Cloud’s Vertex AI, request details are not
    automatically logged by default when generating predictions from LLMs. The responsibility
    falls on the application itself to implement comprehensive logging.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在集成生成式人工智能模型时，一个重要的考虑因素是许多平台（应该）以无状态方式处理推理请求，而没有固有的日志记录能力。例如，在Google Cloud的Vertex
    AI上，从LLM生成预测时，默认情况下不会自动记录请求详情。责任落在应用程序本身，需要实现全面的日志记录。
- en: 'While there is no single gold standard, best practices tend to encourage capturing
    several key pieces of information for each generative model interaction. At a
    bare minimum, logging payloads should include:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然没有单一的金标准，但最佳实践往往鼓励为每个生成式模型交互捕获几个关键信息片段。至少，日志负载应包括：
- en: The timestamp of the request
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求的时间戳
- en: The raw user input
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始用户输入
- en: Any additional context data provided (chat history, retrieved information for
    RAG, etc.)
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供的任何附加上下文数据（聊天历史、检索的RAG信息等）
- en: The prompt template used, if any pre-processing occurred
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果有任何预处理发生，则使用的提示模板
- en: Identifiers for the specific model(s) invoked
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用特定模型（s）的标识符
- en: The full model output or result
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整的模型输出或结果
- en: Any post-processing template used to shape the model output
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何用于塑造模型输出的后处理模板
- en: '[PRE18]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Capturing this coherent payload allows the interaction history to be established
    and the results to be reproduced completely, explaining any given output. This
    supports analytic use cases like exploring user adoption, potential pain points,
    and shifts in usage trends, and at the same time, it enables continuous monitoring
    for potential issues or safety violations that require intervention.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 捕获这个一致的负载允许建立交互历史并完全重现结果，解释任何给定的输出。这支持分析用例，如探索用户采用情况、潜在痛点以及使用趋势的变化，同时，它还使能够持续监控需要干预的潜在问题或安全违规。
- en: Beyond these core fields, other metadata around the generation process can enrich
    observability. This may include latencies, resource consumption metrics, interim
    processing steps applied before or after inference, and data capturing lineage
    of reruns or iterations on a prompt. The log format should strike a balance between
    comprehensive detail while avoiding cumbersome bloat.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些核心字段之外，关于生成过程的其它元数据可以丰富可观察性。这可能包括延迟、资源消耗指标、在推理前后应用的中间处理步骤，以及重试或迭代提示的数据捕获谱系。日志格式应在详尽细节和避免繁琐冗余之间取得平衡。
- en: Implementing centralized structured logging conforming to established templates
    is a key building block for responsible AI model operationalization. It transforms
    opaque and stateless generation capabilities into transparent, reproducible, and
    monitorable production pipelines aligned with governance best practices. Robust
    logging regimes help GenAI earn trust at enterprise scale.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 实施符合既定模板的集中式结构化日志记录是负责任地实施人工智能模型操作化的关键基石。它将不透明和无状态的生成能力转化为透明、可重现和可监控的生产流程，与治理最佳实践保持一致。强大的日志制度有助于生成式人工智能在企业规模上赢得信任。
- en: Summary
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we discussed the integration of GenAI models into real-world
    applications that require a systematic approach. A five-component framework can
    guide this process: Entry Point, Prompt Pre-Processing, Inference, Result Post-Processing,
    and Logging. At the entry point, user inputs aligned with the AI model’s expected
    modalities are accepted, whether text prompts, images, audio, etc. Prompt pre-processing
    then cleans and formats these inputs for security checks and optimal model usability.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了将生成式人工智能模型集成到需要系统方法的现实世界应用中的过程。一个五组件框架可以指导这个过程：入口点、提示预处理、推理、结果后处理和日志记录。在入口点，接受与AI模型预期模式相一致的用户输入，无论是文本提示、图像、音频等。然后，提示预处理清理并格式化这些输入，以进行安全检查和最佳模型可用性。
- en: The core inference component then runs the prepared inputs through the integrated
    GenAI models to produce outputs. This stage requires integrating with model APIs,
    provisioning scalable model-hosting infrastructure, and managing availability
    alongside cost controls. Organizations can choose self-hosting models or leveraging
    cloud services for inference. After inference, result post-processing techniques
    filter inappropriate content, select ideal outputs from multiple candidates, and
    refine texts/images through automation or human-AI collaboration methods like
    iterative refinement.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 核心推理组件随后将准备好的输入通过集成的GenAI模型运行以产生输出。这一阶段需要与模型API集成、提供可扩展的模型托管基础设施，并管理可用性同时控制成本。组织可以选择自托管模型或利用云服务进行推理。推理后，结果后处理技术过滤不适当的内容，从多个候选者中选择理想的输出，并通过自动化或人机协作方法如迭代优化来细化文本/图像。
- en: How these AI-generated results get presented depends on the application’s use
    case – whether powering chatbots, search engines, or document workflows, among
    others. Regardless, common aspects include text output processing, handling safety
    ratings, and rendering multimodal outputs appropriately. Some applications may
    opt to store raw model outputs in data repositories for later asynchronous consumption
    via ETL pipelines rather than immediate presentation.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这些AI生成结果如何呈现取决于应用程序的使用案例——无论是为聊天机器人、搜索引擎或文档工作流程等提供动力，以及其他。无论如何，共同方面包括文本输出处理、处理安全评级以及适当呈现多模态输出。一些应用程序可能选择将原始模型输出存储在数据存储库中，以便通过ETL管道进行异步消费，而不是立即展示。
- en: Comprehensive logging establishes critical observability across this entire
    workflow, tracking metrics, monitoring data quality issues that could indicate
    drift away from training sets, and identifying inference errors or failure points.
    Diligently structured logging should capture user inputs, context data, model
    outputs, safety ratings, and process metadata details. While some platforms treat
    inference requests on an ad hoc basis, without inherent logging, applications
    must implement centralized logging following best practices.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 全面日志记录在整个工作流程中建立了关键的可观察性，跟踪指标，监控可能表明偏离训练集的数据质量问题，并识别推理错误或故障点。勤奋地构建的日志应捕获用户输入、上下文数据、模型输出、安全评级和流程元数据细节。虽然一些平台对推理请求采取临时处理，没有固有的日志记录，但应用程序必须根据最佳实践实施集中式日志记录。
- en: 'Key takeaways:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 关键要点：
- en: Integrate GenAI through a systematic framework covering entry points, pre-processing,
    inference, post-processing, and logging.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过一个涵盖入口点、预处理、推理、后处理和日志记录的系统框架来集成GenAI。
- en: Consider interactive vs. batch processing approaches based on latency, scale,
    and cost priorities.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据延迟、规模和成本优先级考虑交互式与批量处理方法。
- en: Implement comprehensive logging for observability, monitoring, explainability,
    and governance.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施全面的日志记录以实现可观察性、监控、可解释性和治理。
- en: Leverage human-AI collaboration to iteratively refine and enhance AI-generated
    outputs.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用人机协作来迭代优化和增强AI生成的输出。
- en: Design result presentation formats tailored to usability and integration needs.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计针对可用性和集成需求的结果展示格式。
- en: Address security, responsible AI practices, and ethical considerations throughout
    the integration process.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集成过程中解决安全问题、负责任的AI实践和伦理考量。
- en: Join our community on Discord
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的社区Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/genpat](Chapter_03.xhtml)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/genpat](Chapter_03.xhtml)'
- en: '![](img/QR_Code134841911667913109.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code134841911667913109.png)'
