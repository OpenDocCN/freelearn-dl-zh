- en: 6\. Monte Carlo Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6. 蒙特卡洛方法
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, you will learn about the various types of Monte Carlo methods,
    including the first visit and every visit techniques. In the case, if the model
    of the environment is not known, you can use Monte Carlo methods to learn the
    environment by generating experience samples or by simulation. This chapter teaches
    you importance sampling and how to apply Monte Carlo methods to solve the frozen
    lake problem. By the end of this chapter, you will be able to identify problems
    where Monte Carlo methods of reinforcement learning can be applied. You will be
    able to solve prediction, estimation, and control problems using Monte Carlo reinforcement
    learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你将学习各种类型的蒙特卡洛方法，包括首次访问和每次访问技术。如果环境的模型未知，你可以通过生成经验样本或通过仿真来使用蒙特卡洛方法学习环境。本章将教授你重要性采样，并教你如何应用蒙特卡洛方法解决冰湖问题。到本章结束时，你将能够识别可以应用蒙特卡洛方法的强化学习问题。你将能够使用蒙特卡洛强化学习解决预测、估计和控制问题。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapter, we learned about dynamic programming. Dynamic programming
    is a way of doing reinforcement learning where the model of the environment is
    known beforehand. Agents in reinforcement learning can learn a policy, value function,
    and/or model. Dynamic programming helps solve a known **Markov Decision Process**
    (**MDP**). The probabilistic distribution for all possible transitions is known
    in an MDP and is required for dynamic programming.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了动态规划。动态规划是一种在已知环境模型的情况下进行强化学习的方法。强化学习中的智能体可以学习策略、价值函数和/或模型。动态规划帮助解决已知的**马尔可夫决策过程**（**MDP**）。在MDP中，所有可能转换的概率分布都是已知的，并且这是动态规划所必需的。
- en: But what happens when the model of the environment is not known? In many real-life
    situations, the model of the environment is not known beforehand. Can the algorithm
    learn the model of the environment? Can the agents in reinforcement learning still
    learn to make good decisions?
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，当环境模型未知时会发生什么呢？在许多现实生活中的情况下，环境模型是事先未知的。那么算法是否能够学习到环境的模型呢？强化学习中的智能体是否仍然能够学会做出正确的决策呢？
- en: Monte Carlo methods are a way of learning when the model of the environment
    is not known and so they are called model-free learning. We can make a model-free
    prediction that estimates the value function of an unknown MDP. We can also use
    model-free control, which optimizes the value functions of an unknown MDP. Monte
    Carlo methods can also handle non-Markovian domains too.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法是在环境模型未知时的一种学习方式，因此它们被称为无模型学习。我们可以进行无模型预测，估计未知MDP的价值函数。我们还可以使用无模型控制，优化未知MDP的价值函数。蒙特卡洛方法也能够处理非马尔可夫领域。
- en: The transition probabilities between one state and another are not known in
    many cases. You need to play around and get a sense of the environment before
    learning how to play the game well. Monte Carlo methods can learn a model of an
    environment from experiencing the environment. Monte Carlo methods take actual
    or stochastically simulated scenarios and get an average of the sample returns.
    By using the sample sequence of states, actions, and rewards from actual or simulated
    interactions with the environment, Monte Carlo methods can learn from experience.
    A well-defined set of rewards is needed for Monte Carlo methods to work. This
    criterion is met only for episodic tasks, where experience is divided into clearly
    defined episodes, and episodes eventually terminate irrespective of the action
    selected. An example application is AlphaGo, which is one of the most complex
    games; the number of possible moves in any state is over 200\. One of the key
    algorithms used to solve it was a tree search based on Monte Carlo.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，状态之间的转换概率是未知的。你需要先进行试探，熟悉环境，然后才能学会如何玩好这个游戏。蒙特卡洛方法可以通过经历环境来学习环境的模型。蒙特卡洛方法通过实际或随机仿真场景来获得样本回报的平均值。通过使用来自与环境实际或模拟交互的状态、动作和回报的样本序列，蒙特卡洛方法可以通过经验学习。当蒙特卡洛方法工作时，需要一个明确的回报集合。这个标准仅在情节任务中满足，其中经验被划分为明确定义的情节，并且无论选择的动作如何，情节最终都会终止。一个应用示例是AlphaGo，它是最复杂的游戏之一；任何状态下可能的动作数量超过200。用来解决它的关键算法之一是基于蒙特卡洛的树搜索。
- en: In this chapter, we will first understand Monte Carlo methods of reinforcement
    learning. We will apply them to the Blackjack environment in OpenAI. We will learn
    about various methods, such as the first visit method and every visit method.
    We will also learn about importance sampling and, later in the chapter, revisit
    the frozen lake problem. In the next section, we will introduce the basic workings
    of Monte Carlo methods.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先了解蒙特卡洛强化学习方法。我们将把它们应用到OpenAI的二十一点环境中。我们将学习各种方法，如首次访问法和每次访问法。我们还将学习重要性采样，并在本章后面重新审视冻结湖问题。在接下来的部分中，我们将介绍蒙特卡洛方法的基本原理。
- en: The Workings of Monte Carlo Methods
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法的原理
- en: Monte Carlo methods solve reinforcement problems by averaging the sample returns
    for each state-action pair. Monte Carlo methods work only for episodic tasks.
    This means the experience is split into various episodes and all episodes finally
    terminate. Only after the episode is complete are the value functions recalculated.
    Monte Carlo methods can be incrementally optimized episode by episode but not
    step by step.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法通过对每个状态-动作对的样本回报进行平均，来解决强化学习问题。蒙特卡洛方法仅适用于情节任务。这意味着经验被分成多个情节，所有情节最终都会结束。只有在情节结束后，价值函数才会被重新计算。蒙特卡洛方法可以逐集优化，但不能逐步优化。
- en: Let's take the example of a game like Go. This game has millions of states;
    it is going to be difficult to learn all of those millions of states and their
    transition probabilities beforehand. The other approach would be to play the game
    of Go repeatedly and assign a positive reward for winning and a negative reward
    for losing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以围棋为例。围棋有数百万种状态；在事先学习所有这些状态及其转移概率将会很困难。另一种方法是反复进行围棋游戏，并为胜利分配正奖励，为失败分配负奖励。
- en: As we don't have information about the policy of the model, we need to use experience
    samples to learn. This technique is also a sample-based model. We call this direct
    sampling of episodes in Monte Carlo.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不了解模型的策略，需要使用经验样本来学习。这种技术也是一种基于样本的模型。我们称之为蒙特卡洛中的情节直接采样。
- en: Monte Carlo is model-free. As no knowledge of MDP is required, the model is
    inferred from the samples. You can perform model-free prediction or model-free
    estimation. We can perform an evaluation, also called a prediction, on a policy.
    We can also evaluate and improve a policy, which is often called control or optimization.
    Monte Carlo reinforcement learning can learn only from episodes that terminate.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛是无模型的。由于不需要了解MDP（马尔科夫决策过程），模型是从样本中推断出来的。你可以执行无模型的预测或无模型的估计。我们可以对一个策略进行评估，也称为预测。我们还可以评估并改进一个策略，这通常被称为控制或优化。蒙特卡洛强化学习只能从终止的情节中学习。
- en: For example, if you have a game of chess, played by a set of rules or policies,
    that would be playing several episodes according to those rules or policies and
    evaluating the success rate of the policy. If we are playing a game according
    to a policy and modifying the policy based on the game, then it would be a policy
    improvement, optimization, or control.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你玩的是一盘棋，按照一套规则或策略进行游戏，那么你就是根据这些规则或策略进行多个情节，并评估策略的成功率。如果我们根据某个策略进行游戏，并根据游戏的结果调整该策略，那就属于策略改进、优化或控制。
- en: Understanding Monte Carlo with Blackjack
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过二十一点理解蒙特卡洛方法
- en: 'Blackjack is a simple card game that is quite popular in casinos. It is a great
    game, as it is simple to simulate and take samples, and lends itself to Monte
    Carlo methods. Blackjack is also available as part of the OpenAI framework. Players
    and the dealer are dealt two cards each. The dealer shows one card face up and
    lays the other card face down. The players and the dealer have a choice of whether
    to be dealt additional cards or not:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 二十一点是一种简单的卡牌游戏，在赌场中非常流行。这是一款非常棒的游戏，因为它简单易模拟并且容易进行采样，适合蒙特卡洛方法。二十一点也可以作为OpenAI框架的一部分。玩家和庄家各发两张牌。庄家亮出一张牌，另一张牌面朝下。玩家和庄家可以选择是否要继续发牌：
- en: '**The aim of the game**: To obtain cards whose sum is close to or equal to
    21 but not greater than 21.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**游戏的目标**：获得一副卡牌，其点数之和接近或等于21，但不超过21。'
- en: '**Players**: There are two players, called the player and the dealer.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**玩家**：有两个玩家，分别称为玩家和庄家。'
- en: '**The start of the game**: The player is dealt with two cards. The dealer is
    also dealt with two cards, and the rest of the cards are pooled into a stack.
    One of the dealer''s cards is shown to the player.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**游戏开始**：玩家被发两张牌，庄家也被发两张牌，剩余的牌堆放在一边。庄家的其中一张牌展示给玩家。'
- en: '**Possible actions** – **stick or hit**: "Stick" is to stop asking for more
    cards. "Hit" is to ask for more cards. The player will choose "Hit" if the sum
    of their cards is less than 17\. If the sum of their cards is greater than or
    equal to 17, the player will stick. This threshold of 17 to decide whether to
    hit or stick can be changed if needed in various versions of Blackjack. In this
    chapter, we will consistently keep the threshold at 17 to decide whether to hit
    or stick.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可能的行动** – **停牌或要牌**："停牌"是指停止要求更多的牌。"要牌"是指要求更多的牌。如果玩家手牌的总和小于 17，玩家将选择"要牌"。如果手牌总和大于或等于
    17，玩家将选择停牌。是否要牌或停牌的阈值为 17，可以根据需要在不同版本的二十一点中进行调整。在本章中，我们将始终保持这个 17 的阈值，决定是否要牌或停牌。'
- en: '**Rewards**: +1 for a win, -1 for a loss, and 0 for a draw.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励**：赢得一局为 +1，输掉一局为 -1，平局为 0。'
- en: '**Strategy**: The player has to decide whether to stick or hit by looking at
    the dealer''s cards. The ace can be considered to be 1 or 11, based on the value
    of the other cards.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略**：玩家需要根据庄家的手牌决定是否停牌或要牌。根据其他牌的点数，王牌可以被视为 1 或 11。'
- en: 'We will explain the game of Blackjack in the following table. The table has
    the following columns:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下表中解释二十一点游戏。该表包含以下列：
- en: '**Game**: The game number and the sub-state of the game: i, ii, or iii'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**游戏**：游戏编号和游戏的子状态：i、ii 或 iii'
- en: '**Player Cards**: The cards the player has; for example, K♣, 8♦ means the player
    has the King of clubs and the eight of diamonds.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**玩家手牌**：玩家拥有的牌；例如，K♣, 8♦ 表示玩家有一张梅花国王和一张方块八。'
- en: '**Dealer Cards**: The cards the dealer gets. For example, 8♠, Xx means the
    dealer has the eight of spades and a hidden card.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**庄家手牌**：庄家获得的牌。例如，8♠, Xx 表示庄家有一张黑桃八和一张隐藏牌。'
- en: '**Action**: This is the action the player decides to choose.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行动**：这是玩家决定选择的行动。'
- en: '**Result**: The result of the game based on the player''s actions and the cards
    the dealer gets.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结果**：根据玩家的行动和庄家手牌的情况，游戏的结果。'
- en: '**Sum of Player Cards**: The sum of the player''s two cards. Please note that
    the King (K), Queen (Q), and Jack (J) face cards are scored as 10.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**玩家手牌总和**：玩家两张牌的总和。请注意，国王（K）、皇后（Q）和杰克（J）面牌的点数为 10。'
- en: '**Comments**: An explanation of why a particular action was taken or a result
    was declared.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评论**：解释为什么采取了某个特定行动或宣布了某个结果。'
- en: 'In game 1, the player decided to stick as the sum of the cards was 18\. "Stick"
    means the player will no longer receive cards. Now the dealer shows the hidden
    card. It is a draw as both the dealer''s and player''s cards sum 18\. In game
    2, the player''s cards sum 15, which is less than 17\. The player hits and gets
    another card, which takes the sum to 17\. The player then sticks, which means
    the player will no longer receive cards. The dealer shows the cards and as the
    sum of the cards is less than 17, gets another card. With the dealer''s new card,
    the sum is 25, which is greater than 21\. The game aims to get close to or equal
    to 21 without the score becoming greater than 21\. The dealer loses and the player
    wins the second game. The following figure presents a summary of this game:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏 1 中，玩家选择了停牌，因为手牌总和为 18。 "停牌"意味着玩家将不再接收牌。现在庄家展示了隐藏牌。由于庄家和玩家的手牌总和都是 18，结果为平局。在游戏
    2 中，玩家的手牌总和为 15，小于 17。玩家要牌并获得另一张牌，总和变为 17。然后玩家停牌，不再接收牌。庄家展示了手牌，由于手牌总和小于 17，庄家要牌。庄家得到新的一张牌，总和为
    25，超过了 21。游戏的目标是尽量接近或等于 21，而不超过 21。庄家失败，玩家赢得了第二局。以下图展示了此游戏的总结：
- en: '![Figure 6.1: Explanation of a Blackjack game'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.1：二十一点游戏的解释'
- en: '](img/B16182_06_01.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_06_01.jpg)'
- en: 'Figure 6.1: Explanation of a Blackjack game'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1：二十一点游戏的解释
- en: Next, we will be implementing the game of Blackjack using the OpenAI framework.
    This will serve as a foundation for the simulation and application of Monte Carlo
    methods.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 OpenAI 框架实现一款二十一点游戏。这将作为蒙特卡洛方法的模拟和应用的基础。
- en: 'Exercise 6.01: Implementing Monte Carlo in Blackjack'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 6.01：在二十一点中实现蒙特卡洛方法
- en: We will learn how to use the OpenAI framework for Blackjack, and get to know
    about observation space, action space, and generating an episode. The goal of
    this exercise is to implement Monte Carlo techniques in the game of Blackjack.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习如何使用OpenAI框架来玩二十一点，并了解观察空间、动作空间和生成回合。此练习的目标是在二十一点游戏中实现蒙特卡罗技术。
- en: 'Perform the following steps to complete the exercise:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤完成练习：
- en: 'Import the necessary libraries:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库：
- en: '[PRE0]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`gym` is the OpenAI framework, `numpy` is the framework for data processing,
    and `defaultdict` is for dictionary support.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`gym`是OpenAI框架，`numpy`是数据处理框架，`defaultdict`用于字典支持。'
- en: 'We start the `Blackjack` environment with `gym.make()` and assign it to `env`:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`gym.make()`启动`Blackjack`环境，并将其分配给`env`：
- en: '[PRE1]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Find the number of observation spaces and action spaces:'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 找出观察空间和动作空间的数量：
- en: '[PRE2]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You will get the following output:'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将得到以下输出：
- en: '[PRE3]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The number of observation spaces is the number of states. The number of action
    spaces is the number of actions possible in each state. The output shows as discrete,
    as the observation and action space in a Blackjack game is not continuous. For
    example, there are other games in OpenAI, such as balancing a CartPole and pendulum
    where the observation and action spaces are continuous.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 观察空间的数量是状态的数量。动作空间的数量是每个状态下可能的动作数。输出结果显示为离散型，因为二十一点游戏中的观察和动作空间不是连续的。例如，OpenAI中还有其他游戏，如平衡杆和摆钟，这些游戏的观察和动作空间是连续的。
- en: 'Write a function to play the game. If the sum of the player''s cards is more
    than or equal to 17, stick (don''t choose more cards); otherwise, hit (choose
    more cards), as shown in the following code:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个函数来玩游戏。如果玩家的卡片总和大于或等于17，则停牌（不再抽卡）；否则，抽牌（选择更多卡片），如以下代码所示：
- en: '[PRE4]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we are initializing the episode, choosing the initial state, and assigning
    it to `player_score`, `dealer_score,` and `usable_ace`.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们初始化回合，选择初始状态，并将其分配给`player_score`、`dealer_score`和`usable_ace`。
- en: 'Add a dictionary, `action_text`, that has a key-value mapping for two action
    integers to action text. Here''s the code to convert the integer value of the
    action into text format:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个字典`action_text`，它将两个动作整数映射到相应的动作文本。以下是将动作的整数值转换为文本格式的代码：
- en: '[PRE5]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Play the game in batches of 100 and calculate `state`, `reward`, and `action`:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以每100个回合的批次玩游戏，并计算`state`、`reward`和`action`：
- en: '[PRE6]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You will get the following output:'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将得到以下输出：
- en: '![Figure 6.2: The output is the episode of the Blackjack game in progress'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.2：输出的是正在进行的二十一点游戏的回合'
- en: '](img/B16182_06_02.jpg)'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_06_02.jpg)'
- en: 'Figure 6.2: The output is the episode of the Blackjack game in progress'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：输出的是正在进行的二十一点游戏的回合
- en: Note
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The Monte Carlo technique is based on generating random samples. As such, two
    executions of the same code will not match in values. So, you might have a similar
    output but not the same for all the exercises and activities.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡罗技术基于生成随机样本。因此，同一段代码的两次执行结果值可能不同。所以，你可能会得到类似的输出，但并不完全相同，适用于所有练习和活动。
- en: In the code, `done` has the value of `True` or `False`. If `done` is `True`,
    the game stops, we note the value of the rewards and print the game result. In
    the output, we simulated the game of Blackjack using the Monte Carlo method and
    noted the various actions, states, and game completion. We were also able to simulate
    the rewards when the game ends.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，`done`的值为`True`或`False`。如果`done`为`True`，游戏结束，我们记录奖励值并打印游戏结果。在输出中，我们使用蒙特卡罗方法模拟了二十一点游戏，并记录了不同的动作、状态和游戏完成情况。我们还模拟了游戏结束时的奖励。
- en: Note
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2XZssYh](https://packt.live/2XZssYh).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定章节的源代码，请参考[https://packt.live/2XZssYh](https://packt.live/2XZssYh)。
- en: You can also run this example online at [https://packt.live/2Ys0cMJ](https://packt.live/2Ys0cMJ).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/2Ys0cMJ](https://packt.live/2Ys0cMJ)在线运行这个示例。
- en: Next, we will describe the different types of Monte Carlo methods, namely, the
    first visit and every visit method, which will be used to estimate the value function.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将描述两种不同的蒙特卡罗方法，即首次访问法和每次访问法，这些方法将用于估计值函数。
- en: Types of Monte Carlo Methods
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡罗方法的类型
- en: We have implemented the game of Blackjack using Monte Carlo. Typically, a trajectory
    of Monte Carlo is a sequence of state, action, and reward. In several episodes,
    it is possible that the state repeats. For example, the trajectory could be S0,
    S1, S2, S0, S3\. How do we handle the calculation of the reward function when
    we have multiple visits to the states?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用蒙特卡洛实现了黑杰克游戏。通常，蒙特卡洛轨迹是一个状态、动作和奖励的序列。在多个回合中，可能会出现状态重复。例如，轨迹可能是S0，S1，S2，S0，S3。我们如何在状态多次访问时处理奖励函数的计算呢？
- en: Broadly, this highlights that there are two types of Monte Carlo methods – first
    visit and every visit. We will understand the implications of both methods.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 从广义上讲，这突出了两种蒙特卡洛方法——首次访问和每次访问。我们将理解这两种方法的含义。
- en: As stated previously, in Monte Carlo methods, we approximate the value function
    by averaging the rewards. In the first visit Monte Carlo method, only the first
    visit to a state in an episode is included to calculate the average reward. For
    example, in a given game of traversing a maze, you could make several visits to
    the sample place. In the first visit Monte Carlo method, only the first visit
    is used for the calculation of the reward. When the agent revisits the same state
    in the episode, the reward is not included for the calculation of the average
    reward.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在蒙特卡洛方法中，我们通过平均奖励来逼近值函数。在首次访问蒙特卡洛方法中，只有在一个回合中首次访问某个状态时才会被用来计算平均奖励。例如，在某个迷宫游戏中，你可能会多次访问同一个地方。使用首次访问蒙特卡洛方法时，只有首次访问时的奖励才会被用于计算奖励。当智能体在回合中重新访问相同的状态时，奖励不会被纳入计算平均奖励中。
- en: In every visit Monte Carlo, every time the agent visits the same state, the
    rewards are included in the calculation of the average return. For example, let's
    use the same game of maze. Every time the agent comes to the same point in the
    maze, we include the rewards earned in that state for the calculation of the reward
    function.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次访问蒙特卡洛中，每次智能体访问相同的状态时，奖励都会被纳入计算平均回报。例如，使用相同的迷宫游戏。每次智能体到达迷宫中的相同位置时，我们都会将该状态下获得的奖励纳入奖励函数的计算。
- en: Both first visit and every visit converge to the same value function. For a
    smaller number of episodes, the choice between the first visit and every visit
    is based on the particular game and the rules of the game.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 首次访问和每次访问都会收敛到相同的值函数。对于较少的回合，首次访问和每次访问之间的选择取决于具体的游戏和游戏规则。
- en: Let's understand the pseudocode for first visit Monte Carlo prediction.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过理解首次访问蒙特卡洛预测的伪代码来深入了解。
- en: First Visit Monte Carlo Prediction for Estimating the Value Function
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 首次访问蒙特卡洛预测用于估算值函数
- en: 'In the pseudocode for first visit Monte Carlo prediction for estimating the
    value function, the key is to calculate the value function *V(s)*. Gamma is the
    discount factor. The discount factor is used to reward future rewards less than
    immediate rewards:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在用于估算值函数的首次访问蒙特卡洛预测的伪代码中，关键是计算值函数*V(s)*。Gamma是折扣因子。折扣因子用于将未来的奖励减少到低于即时奖励：
- en: '![Figure 6.3: Pseudocode for first visit Monte Carlo prediction'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.3：首次访问蒙特卡洛预测的伪代码'
- en: '](img/B16182_06_03.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_06_03.jpg)'
- en: 'Figure 6.3: Pseudocode for first visit Monte Carlo prediction'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：首次访问蒙特卡洛预测的伪代码
- en: 'What we have done in the first visit is to generate an episode, calculate the
    result value, and append the result to the rewards. We then calculate the average
    returns. In the upcoming exercise, we will apply the first visit Monte Carlo prediction
    to estimate the value function by following the steps detailed in the pseudocode.
    The key block of code for the first visit algorithm is navigating the states only
    through the first visit:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在首次访问中，我们所做的就是生成一个回合，计算结果值，并将结果附加到奖励中。然后我们计算平均回报。在接下来的练习中，我们将通过遵循伪代码中的步骤应用首次访问蒙特卡洛预测来估算值函数。首次访问算法的关键代码块是仅通过首次访问来遍历状态：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Consider the `states` that have not been visited. We increase the count for
    the number of `states` by `1`, calculate the value function with the incremental
    method, and return the value function. This is implemented as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑那些尚未访问过的`states`。我们通过`1`增加`states`的数量，使用增量方法计算值函数，并返回值函数。实现方式如下：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let's understand it better through the next exercise.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过下一个练习更好地理解这一点。
- en: 'Exercise 6.02: First Visit Monte Carlo Prediction for Estimating the Value
    Function in Blackjack'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习6.02：使用首次访问蒙特卡洛预测估算黑杰克中的值函数
- en: This exercise aims to understand how to apply first visit Monte Carlo prediction
    to estimate the value function in the game of Blackjack. We will apply the steps
    outlined in the pseudocode step by step.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 本次练习旨在理解如何应用首次访问蒙特卡洛预测来估计黑杰克游戏中的价值函数。我们将按照伪代码中概述的步骤一步步进行。
- en: 'Perform the following steps to complete the exercise:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以完成练习：
- en: 'Import the necessary libraries:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库：
- en: '[PRE9]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`gym` is the OpenAI framework, `numpy` is the framework for data processing,
    and `defaultdict` is for dictionary support.'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`gym`是OpenAI的框架，`numpy`是数据处理框架，`defaultdict`用于字典支持。'
- en: 'Select the environment as `Blackjack` in OpenAI:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在OpenAI中选择环境为`Blackjack`：
- en: '[PRE10]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Write the `policy_blackjack_game` function, which takes the state as input
    and returns the action `0` or `1` based on `player_score`:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写`policy_blackjack_game`函数，该函数接受状态作为输入，并根据`player_score`返回`0`或`1`的动作：
- en: '[PRE11]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the function, if the player score is greater than or equal to `17`, it does
    not take more cards. But if `player_score` is less than 17, it takes additional
    cards.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在该函数中，如果玩家分数大于或等于`17`，则不再抽取更多牌。但如果`player_score`小于17，则抽取更多牌。
- en: 'Write a function to generate a Blackjack episode. Initialize `episode`, `states`,
    `actions`, and `rewards`:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个生成黑杰克回合的函数。初始化`episode`、`states`、`actions`和`rewards`：
- en: '[PRE12]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Reset the environment and set the `state` value to `player_score`, `dealer_score`,
    and `usable_ace`:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境，并将`state`的值设置为`player_score`、`dealer_score`和`usable_ace`：
- en: '[PRE13]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Write a function that generates the action from the state. We then step through
    the action and find `next_state` and `reward`:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个函数从状态中生成动作。然后我们执行该动作，找到`next_state`和`reward`：
- en: '[PRE14]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Create a list of `episode`, `state`, `action,` and `reward` by appending them
    to the existing list:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`episode`、`state`、`action`和`reward`的列表，将它们附加到现有列表中：
- en: '[PRE15]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If the episode is complete (`done` is true), we `break` the loop. If not, we
    update `state` to `next_state` and repeat the loop:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果这一集已完成（`done` 为 true），我们就`break`跳出循环。如果没有，我们更新`state`为`next_state`并重复循环：
- en: '[PRE16]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We return `episodes`, `states`, `actions`, and `rewards` from the function:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从函数中返回`episodes`、`states`、`actions`和`rewards`：
- en: '[PRE17]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Write the function for calculating the value function for Blackjack. The first
    step is to initialize the value of `total_rewards`, `num_states`, and `value_function`:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个计算黑杰克价值函数的函数。第一步是初始化`total_rewards`、`num_states`和`value_function`的值：
- en: '[PRE18]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Generate an `episode`, and for an `episode`, we find the total `rewards` for
    all the `states` in reverse order in the `episode`:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个`episode`，对于每个`episode`，我们按逆序查找所有`states`的总`rewards`：
- en: '[PRE19]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Consider the `states` that have not been visited. We increase the count for
    the number of `states` by `1` and calculate the value function using the incremental
    method, and return the value function:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑未访问过的`states`。我们将`states`的计数增加`1`，并使用增量方法计算价值函数，然后返回价值函数：
- en: '[PRE20]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, execute first visit prediction 10,000 times:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，执行首次访问预测10,000次：
- en: '[PRE21]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You will get the following output:'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下输出：
- en: '![Figure 6.4: First visit value function'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.4：首次访问价值函数'
- en: '](img/B16182_06_04.jpg)'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_06_04.jpg)'
- en: 'Figure 6.4: First visit value function'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4：首次访问价值函数
- en: 'The value function for the first visit is printed. For all the states, a combination
    of `player_score`, `dealer_score,` and `usable_space` has a value function value
    from the first visit evaluation. Take the example output of `(16, 3, False): -0.625`.
    This means that the value function for the state with player score `16`, dealer
    score `3`, and a reusable ace as `False` is `-0.625`. The number of episodes and
    batches are configurable.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '首次访问的价值函数被打印出来。对于所有的状态，`player_score`、`dealer_score`和`usable_space`的组合都有一个来自首次访问评估的价值函数值。以`(16,
    3, False): -0.625`为例。这意味着玩家分数为`16`、庄家分数为`3`、可用的A牌为`False`的状态的价值函数为`-0.625`。集数和批次数是可配置的。'
- en: Note
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/37zbza1](https://packt.live/37zbza1).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/37zbza1](https://packt.live/37zbza1)。
- en: You can also run this example online at [https://packt.live/2AYnhyH](https://packt.live/2AYnhyH).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在在线运行这个例子：[https://packt.live/2AYnhyH](https://packt.live/2AYnhyH)。
- en: We have covered the first visit Monte Carlo in this section. In the next section,
    we will understand every visit Monte Carlo prediction for estimating the value
    function.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们已经覆盖了首次访问蒙特卡洛方法。下一节我们将理解每次访问蒙特卡洛预测以估计价值函数。
- en: Every Visit Monte Carlo Prediction for Estimating the Value Function
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 每次访问蒙特卡洛预测用于估计价值函数
- en: 'In every visit Monte Carlo prediction, every visit to the state is used for
    the reward calculation. We have a gamma factor that is the discount factor, which
    enables us to discount the rewards in the far future relative to rewards in the
    immediate future:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次访问蒙特卡洛预测中，每次访问状态都用于奖励计算。我们有一个 gamma 因子作为折扣因子，用于相对于近期奖励对未来奖励进行折扣：
- en: '![Figure 6.5: Pseudocode for every visit Monte Carlo prediction'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.5：每次访问蒙特卡洛预测的伪代码'
- en: '](img/B16182_06_05.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_06_05.jpg)'
- en: 'Figure 6.5: Pseudocode for every visit Monte Carlo prediction'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5：每次访问蒙特卡洛预测的伪代码
- en: The difference is primarily visiting every step instead of just the first to
    calculate the rewards. The code remains similar to the first visit exercise, except
    for the Blackjack prediction function where the rewards are calculated.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的区别在于每次访问每一步，而不仅仅是第一次，来计算奖励。代码与第一次访问的练习类似，唯一不同的是在 Blackjack 预测函数中计算奖励。
- en: 'The following line in the first visit implementation checks if the current
    state has not been traversed before. This line is no longer in every visit algorithm:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下这一行在第一次访问实现中检查当前状态是否之前未被遍历。这个检查在每次访问算法中不再需要：
- en: '[PRE22]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The code for the calculation of the value function is as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 计算值函数的代码如下：
- en: '[PRE23]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In this exercise, we will use every visit Monte Carlo method to estimate the
    value function.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用每次访问蒙特卡洛方法来估计值函数。
- en: 'Exercise 6.03: Every Visit Monte Carlo Prediction for Estimating the Value
    Function'
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 6.03：用于估计值函数的每次访问蒙特卡洛预测
- en: 'This exercise aims to understand how to apply every visit Monte Carlo prediction
    to estimate the value function. We will apply the steps outlined in the pseudocode
    step by step. Perform the following steps to complete the exercise:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习旨在帮助理解如何应用每次访问蒙特卡洛预测来估计值函数。我们将一步一步地应用伪代码中概述的步骤。执行以下步骤以完成练习：
- en: 'Import the necessary libraries:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库：
- en: '[PRE24]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Select the environment as `Blackjack` in OpenAI:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 OpenAI 中选择环境为 `Blackjack`：
- en: '[PRE25]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Write the `policy_blackjack_game` function that takes the state as input and
    returns the action `0` or `1` based on `player_score`:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写 `policy_blackjack_game` 函数，接受状态作为输入，并根据 `player_score` 返回 `action` 为 `0`
    或 `1`：
- en: '[PRE26]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In the function, if the player score is greater than or equal to `17`, it does
    not take more cards. But if `player_score` is less than `17`, it takes additional
    cards.
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在该函数中，如果玩家的分数大于或等于`17`，则不再抽取牌。但如果`player_score`小于`17`，则会继续抽取牌。
- en: 'Write a function to generate a Blackjack episode. Initialize `episode`, `states`,
    `actions`, and `rewards`:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个生成 Blackjack 回合的函数。初始化 `episode`、`states`、`actions` 和 `rewards`：
- en: '[PRE27]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We reset the environment and set the value of `state` to `player_score`, `dealer_score`,
    and `usable_ace`, as shown in the following code:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们重置环境，并将 `state` 的值设置为 `player_score`、`dealer_score` 和 `usable_ace`，如以下代码所示：
- en: '[PRE28]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Write a function that generates `action` from `state`. We then step through
    `action` and find `next_state` and `reward`:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个函数，通过 `state` 生成 `action`，然后通过 `action` 步骤找到 `next_state` 和 `reward`：
- en: '[PRE29]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Create a list of `episode`, `state`, `action`, and `reward` by appending them
    to the existing list:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将 `episode`、`state`、`action` 和 `reward` 添加到现有列表中，创建一个列表：
- en: '[PRE30]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'If the episode is complete (`done` is true), we `break` the loop. If not, we
    update `state` to `next_state` and repeat the loop:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果回合完成（`done` 为真），我们就 `break` 循环。如果没有完成，我们更新 `state` 为 `next_state` 并重复循环：
- en: '[PRE31]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We return `episodes`, `states`, `actions`, and `rewards` from the function:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从函数中返回 `episodes`、`states`、`actions` 和 `rewards`：
- en: '[PRE32]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Write the function for calculating the value function for Blackjack. The first
    step is to initialize the values of `total_rewards`, `num_states`, and `value_function`:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写用于计算 Blackjack 值函数的函数。第一步是初始化 `total_rewards`、`num_states` 和 `value_function`
    的值：
- en: '[PRE33]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Generate an `episode` and for the `episode`, we find the total `rewards` for
    all the `states` in reverse order in the `episode`:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个 `episode`，对于该 `episode`，我们在 `episode` 中逆序找到所有 `states` 的总 `rewards`：
- en: '[PRE34]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Consider every `state` visited. We increase the count for the number of `states`
    by `1` and calculate the value function with the incremental method and return
    the value function:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑每个访问的 `state`。我们将 `states` 的计数增加 `1`，并通过增量方法计算值函数，然后返回该值函数：
- en: '[PRE35]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, execute every visit prediction 10,000 times:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，执行每次访问预测 10,000 次：
- en: '[PRE36]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You will get the following output:'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将得到以下输出：
- en: '![Figure 6.6: Every visit value function'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.6：每次访问值函数'
- en: '](img/B16182_06_06.jpg)'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_06_06.jpg)'
- en: 'Figure 6.6: Every visit value function'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6：每次访问值函数
- en: The value function for every visit is printed. For all the states, a combination
    of `player_score`, `dealer_score,` and `usable_space` has a value function value
    from every visit evaluation. We can increase the number of episodes and run this
    again too. As the number of episodes is made larger and larger, the first visit
    and every visit functions will converge.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 每次访问的价值函数都会被打印出来。对于所有状态，`player_score`、`dealer_score`和`usable_space`的组合都有来自每次访问评估的价值函数值。我们还可以增加训练回合数，并再次运行此操作。随着回合数的增大，首次访问和每次访问的函数将逐渐收敛。
- en: Note
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2C0wAP4](https://packt.live/2C0wAP4).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2C0wAP4](https://packt.live/2C0wAP4)。
- en: You can also run this example online at [https://packt.live/2zqXsH3](https://packt.live/2zqXsH3).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在[https://packt.live/2zqXsH3](https://packt.live/2zqXsH3)在线运行此示例。
- en: In the next section, we will talk about a key concept of Monte Carlo reinforcement
    learning, which is the need to balance exploration and exploitation. This is also
    the basis of the greedy epsilon technique of the Monte Carlo method. Balancing
    exploration and exploitation helps us to improve the policy function.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论蒙特卡洛强化学习的一个关键概念，即探索与利用的平衡需求。这也是蒙特卡洛方法的贪婪ε策略的基础。平衡探索和利用有助于我们改进策略函数。
- en: Exploration versus Exploitation Trade-Off
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索与利用的权衡
- en: Learning happens by exploring new things and exploiting or applying what has
    been learned before. The right combination of these is the essence of any learning.
    Similarly, in the context of reinforcement learning, we have exploration and exploitation.
    **Exploration** is trying out different actions, while **exploitation** is following
    an action that is known to have a good reward.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 学习是通过探索新事物以及利用或应用之前学到的知识来进行的。这两者的正确结合是任何学习的核心。同样，在强化学习的背景下，我们也有探索和利用。**探索**是尝试不同的动作，而**利用**则是采取已知能带来良好奖励的动作。
- en: Reinforcement learning has to balance between exploration and exploitation.
    Every agent can learn only from the experience of trying an action. Exploration
    helps try new actions that might enable the agent to make better decisions in
    the future. Exploitation is choosing actions that yield good rewards based on
    experience. The agent needs to trade off gaining rewards by exploitation by experimenting
    in exploration. If an agent exploits more, the agent might miss learning about
    other policies with even greater rewards. If the agent explores more, the agent
    might miss the opportunity to exploit a known path and lose out on rewards.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习必须在探索和利用之间取得平衡。每个智能体只能通过尝试某个动作的经验来学习。探索有助于尝试新的动作，这可能使智能体在未来做出更好的决策。利用是基于经验选择那些能带来良好奖励的动作。智能体需要在通过探索实验来获取奖励和通过利用已知路径来获得奖励之间做出权衡。如果智能体更多地进行利用，可能会错过学习其他更有回报的策略的机会。如果智能体更多地进行探索，可能会错失利用已知路径并失去奖励的机会。
- en: For example, think of a student who is trying to maximize their grades in college.
    The student can either "explore" by taking courses with new subjects or "exploit"
    by taking courses in their favorite subjects. If the student indexes towards "exploitation,"
    the student might miss out on both getting good grades in new unexplored courses
    and the overall learning. If the student explores too many diverse subjects by
    taking courses in them, this might impact their grades and might make the learning
    too broad.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象一个学生正在努力在大学中最大化自己的成绩。这个学生可以通过选修新学科的课程来“探索”，或者通过选修自己喜欢的课程来“利用”。如果学生倾向于“利用”，他可能会错过在新学科课程中获得好成绩和整体学习的机会。如果学生通过选修太多不同的学科课程来进行探索，这可能会影响他的成绩，并且可能让学习变得过于宽泛。
- en: Similarly, if you choose to read books, you could exploit by reading books belonging
    to the same genre or author or explore by reading books across different genres
    and authors. Similarly, while driving from one place to another, you could exploit
    by following the same known route based on past experience or explore by taking
    different routes. In the next section, we will get an understanding of the techniques
    of on-policy and off-policy learning. We will then get an understanding of a key
    factor called importance sampling, for off-policy learning.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果你选择阅读书籍，你可以通过阅读同一类型或同一作者的书籍来进行“开发”或通过跨越不同类型和作者的书籍来进行“探索”。类似地，当你从一个地方开车到另一个地方时，你可以通过基于过去经验沿用相同的已知路线来进行“开发”或通过选择不同的路线来进行“探索”。在下一部分中，我们将了解“在政策学习”和“脱政策学习”的技术。然后，我们将了解一个名为重要性采样的关键因素，它对于脱政策学习非常重要。
- en: Exploration and exploitation are techniques used in reinforcement learning.
    In off-policy learning, you can have an exploitation technique as the target policy
    and an exploration technique as the behavior policy. We could have a greedy policy
    as the exploitation technique and a random policy as the exploration technique.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 探索与开发是强化学习中常用的技术。在脱政策学习中，你可以将开发技术作为目标策略，而将探索技术作为行为策略。我们可以把贪婪策略作为开发技术，把随机策略作为探索技术。
- en: Importance Sampling
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重要性采样
- en: 'Monte Carlo methods can be on-policy or off-policy. In **on-policy** learning,
    we learn from the agent experience of the following policy. In **off-policy**
    learning, we learn how to estimate a target policy from the experience of following
    a different behavioral policy. Importance sampling is a key technique for off-policy
    learning. The following figure compares on-policy and off-policy learning:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法可以是“在政策”或“脱政策”的。在**在政策**学习中，我们从代理遵循的策略经验中进行学习。在**脱政策**学习中，我们学习如何从遵循不同行为策略的经验中估计目标策略。重要性采样是脱政策学习的关键技术。下图展示了在政策与脱政策学习的对比：
- en: '![Figure 6.7: On-Policy versus Off-Policy comparison'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.7：在政策与脱政策的比较'
- en: '](img/B16182_06_07.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_06_07.jpg)'
- en: 'Figure 6.7: On-Policy versus Off-Policy comparison'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7：在政策与脱政策的比较
- en: You might think that on-policy learning is learning while playing, while off-policy
    learning is learning by watching someone else play. You could improve your cricket
    game by playing cricket yourself. This will help you learn from your mistakes
    and best actions. That would be on-policy learning. You could also learn by watching
    others play the game of cricket and learning from their mistakes and best actions.
    That would be off-policy learning.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为，在政策学习是在玩耍时学习，而脱政策学习是在观看别人玩耍时学习。你可以通过自己玩板球来提高你的板球水平。这有助于你从自己的错误和最佳行动中学习。这就是在政策学习。你也可以通过观察别人玩板球来学习，并从他们的错误和最佳行动中学习。这就是脱政策学习。
- en: Human beings typically do both on-policy and off-policy learning. For example,
    cycling is primarily on-policy learning. We learn to cycle by learning to balance
    ourselves while cycling. Dancing is a kind of off-policy learning; you watch someone
    else dance and learn the dance steps.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 人类通常会同时进行在政策和脱政策学习。例如，骑自行车主要是属于在政策学习。我们通过学习在骑车时保持平衡来学习骑车。跳舞则是一种脱政策学习；你通过观察别人跳舞来学习舞步。
- en: On-policy methods are simple compared to off-policy methods. Off-policy methods
    are more powerful due to the "transfer learning" effect. In off-policy methods,
    you are learning from a different policy, the convergence is slower, and the variance
    is higher.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 与脱政策方法相比，在政策方法较为简单。脱政策方法更强大，因为它具有“迁移学习”的效果。在脱政策方法中，你是从不同的策略中学习，收敛速度较慢，方差较大。
- en: The advantage of off-policy learning is that the behavior policy can be very
    exploratory in nature, while the target policy can be deterministic and greedily
    optimize the rewards.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 脱政策学习的优点在于，行为策略可以非常具有探索性，而目标策略可以是确定性的，并贪婪地优化奖励。
- en: Off-policy reinforcement methods are based on a concept called importance sampling.
    This methodology helps estimate values under one policy probability distribution
    given samples from another policy probability distribution. Let's understand Monte
    Carlo off-policy evaluation by detailing the pseudocode. We'll then apply it to
    the Blackjack game in the OpenAI framework.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 脱政策强化方法基于一个名为重要性采样的概念。该方法帮助在一个政策概率分布下估计值，前提是你拥有来自另一个政策概率分布的样本。让我们通过详细的伪代码理解蒙特卡洛脱政策评估。接着我们将在OpenAI框架中将其应用到21点游戏。
- en: The Pseudocode for Monte Carlo Off-Policy Evaluation
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蒙特卡洛脱策略评估的伪代码
- en: What we see in the following figure is that we are estimating `Q(s,a)` by learning
    from behavior policy `b`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下图中看到的是，我们正在通过从行为策略`b`中学习来估计`Q(s,a)`。
- en: '![Figure 6.8: Pseudocode for Monte Carlo off-policy evaluation'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.8：蒙特卡洛脱策略评估的伪代码'
- en: '](img/B16182_06_08.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_06_08.jpg)'
- en: 'Figure 6.8: Pseudocode for Monte Carlo off-policy evaluation'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8：蒙特卡洛脱策略评估的伪代码
- en: The target policy is a greedy policy; hence we choose the action with the maximum
    rewards by using `argmax Q(s,a)`. Gamma is the discount factor that allows us
    to discount rewards in the distant future compared to immediate rewards in the
    future. The cumulative value function `C(s,a)` is calculated by incrementing it
    with weight `W`. Gamma is used to discount the rewards.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 目标策略是贪婪策略；因此，我们通过使用`argmax Q(s,a)`选择具有最大回报的动作。Gamma是折扣因子，它使我们能够将远期奖励与未来即时奖励进行折扣。累积价值函数`C(s,a)`通过加权`W`来计算。Gamma用于折扣奖励。
- en: 'The essence of off-policy Monte Carlo is the loop through every episode:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 脱策略蒙特卡洛的核心是遍历每个回合：
- en: '[PRE37]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Let's understand the implementation of the off-policy method of Monte Carlo
    by using importance sampling. This exercise will help us learn how to set the
    target policy and the behavior policy, and learn the target policy from the behavior
    policy.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用重要性采样来理解蒙特卡洛脱策略方法的实现。这个练习将帮助我们学习如何设置目标策略和行为策略，并从行为策略中学习目标策略。
- en: 'Exercise 6.04: Importance Sampling with Monte Carlo'
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习6.04：使用蒙特卡洛进行重要性采样
- en: This exercise will aim to do off-policy learning by using a Monte Carlo method.
    We have chosen a greedy target policy. We also have a behavior policy, which is
    any soft, non-greedy policy. By learning from the behavior policy, we will estimate
    the value function of the target policy. We will apply this technique of importance
    sampling to the Blackjack game environment. We will apply the steps outlined in
    the pseudocode step by step.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习的目标是通过使用蒙特卡洛方法进行脱策略学习。我们选择了一个贪婪的目标策略。我们也有一个行为策略，即任何软性、非贪婪策略。通过从行为策略中学习，我们将估计目标策略的价值函数。我们将把这种重要性采样技术应用到Blackjack游戏环境中。我们将按步骤执行伪代码中概述的步骤。
- en: 'Perform the following steps to complete the exercise:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以完成该练习：
- en: 'Import the necessary libraries:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库：
- en: '[PRE38]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Select the environment as `Blackjack` in OpenAI by using `gym.make`:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`gym.make`选择OpenAI中的`Blackjack`环境：
- en: '[PRE39]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Create two policy functions. One of them is a random policy. The random policy
    chooses a random action, which is a list of size n with 1/n probability where
    n is the number of actions:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个策略函数。一个是随机策略。随机策略选择一个随机动作，它是一个大小为n的列表，每个动作有1/n的概率，其中n是动作的数量：
- en: '[PRE40]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Write a function to create a greedy policy:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个函数来创建贪婪策略：
- en: '[PRE41]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The greedy policy chooses an action that maximizes the rewards. We first identify
    the `best_possible_action`, that is, the maximum value of `Q` across states. We
    then assign a value to the `Action` corresponding to the `best_possible_action`.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 贪婪策略选择一个最大化奖励的动作。我们首先识别`best_possible_action`，即`Q`在所有状态中的最大值。然后，我们将值分配给对应于`best_possible_action`的`Action`。
- en: 'Define a function for Blackjack importance sampling that takes `env`, `num_episodes`,
    `behaviour_policy`, and `discount_factor` as arguments:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个用于Blackjack重要性采样的函数，该函数以`env`、`num_episodes`、`behaviour_policy`和`discount_factor`作为参数：
- en: '[PRE42]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We initialize the value of `Q` and `C`, and set the target policy as a greedy
    policy.
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们初始化`Q`和`C`的值，并将目标策略设为贪婪策略。
- en: 'We loop for the number of episodes, initialize the episodes list, and state
    the initial set by doing `env.reset()`:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们按回合数循环，初始化episode列表，并通过`env.reset()`声明初始状态集：
- en: '[PRE43]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'For a batch of 100, apply the behavior policy on a state to calculate the probability:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于100个批次，在某个状态下应用行为策略来计算概率：
- en: '[PRE44]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We pick a random action from that list. The step is taken with the random action,
    returning `next_state` and `reward`. The episode list is appended with the `state`,
    `action`, and `reward`.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们从列表中随机选择一个动作。用随机动作执行一步，返回`next_state`和`reward`。将`state`、`action`和`reward`附加到episode列表中。
- en: 'If the `episode` is completed, we break the loop and assign `next_state` to
    `state`:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`episode`完成，我们跳出循环并将`next_state`赋值给`state`：
- en: '[PRE45]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Initialize `G`, the results as `0` and `W`, and the weight as `1`:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化`G`，结果为`0`，并将`W`和权重设为`1`：
- en: '[PRE46]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Perform the steps detailed in the pseudocode using a `for` loop, as shown in
    the following code:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`for`循环执行伪代码中详细描述的步骤，如下代码所示：
- en: '[PRE47]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Return `Q` and `target_policy`:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回 `Q` 和 `target_policy`：
- en: '[PRE48]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Create a random policy:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个随机策略：
- en: '[PRE49]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The random policy is used as a behavior policy. We pass the behavior policy
    and using the importance sampling method, get the `Q` value function or the target
    policy.
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机策略作为行为策略使用。我们传入行为策略，并使用重要性采样方法，获得 `Q` 值函数或目标策略。
- en: 'Iterate through the items in `Q` and then find the action that has the maximum
    value. This is then stored as the value function for the corresponding state:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历 `Q` 中的项，然后找到具有最大值的动作。然后将其作为相应状态的值函数存储：
- en: '[PRE50]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'You will get the following output:'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将得到如下输出：
- en: '![Figure 6.9: Output of off-policy Monte Carlo evaluation'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.9：离策略蒙特卡罗评估输出'
- en: '](img/B16182_06_09.jpg)'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_06_09.jpg)'
- en: 'Figure 6.9: Output of off-policy Monte Carlo evaluation'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9：离策略蒙特卡罗评估输出
- en: The off-policy evaluation has calculated and returned the value function for
    every state-action pair. In this exercise, we have applied the concept of importance
    sampling using a behavior policy and applied the learning to a target policy.
    The output is provided for every combination state and action pair. It has helped
    us understand off-policy learning. We had two policies – a behavior policy and
    a target policy. We learned the target policy by following the behavior policy.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 离策略评估已经计算并返回了每个状态-动作对的值函数。在这个练习中，我们使用行为策略应用了重要性采样的概念，并将学习应用于目标策略。输出为每个状态-动作对的组合提供了结果。这帮助我们理解了离策略学习。我们有两个策略——行为策略和目标策略。我们通过遵循行为策略学习目标策略。
- en: Note
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3hpOOKa](https://packt.live/3hpOOKa).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此部分的源代码，请参考 [https://packt.live/3hpOOKa](https://packt.live/3hpOOKa)。
- en: You can also run this example online at [https://packt.live/2B1GQGa](https://packt.live/2B1GQGa).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行此示例：[https://packt.live/2B1GQGa](https://packt.live/2B1GQGa)。
- en: In the next section, we will learn how to solve the frozen lake problem, available
    in the OpenAI framework, using Monte Carlo techniques.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将学习如何使用蒙特卡罗技术解决 OpenAI 框架中的冰冻湖问题。
- en: Solving Frozen Lake Using Monte Carlo
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用蒙特卡罗解决冰冻湖问题
- en: Frozen Lake is another simple game found in the OpenAI framework. This is a
    classic game where you can do sampling and simulations for Monte Carlo reinforcement
    learning. We have already described and used the Frozen Lake environment in *Chapter
    05*, *Dynamic Programming*. Here we shall quickly revise the basics of the game
    so that we can solve it using Monte Carlo methods in the upcoming activity.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 冰冻湖是 OpenAI 框架中另一个简单的游戏。这是一个经典游戏，你可以用蒙特卡罗强化学习进行采样和模拟。我们已经在 *第 05 章*，*动态规划* 中描述并使用了冰冻湖环境。在这里，我们将快速复习游戏的基础知识，以便在接下来的活动中使用蒙特卡罗方法解决它。
- en: 'We have a 4x4 grid of cells, which is the entire frozen lake. It contains 16
    cells (a 4x4 grid). The cells are marked as `S` – Start, `F` – Frozen, `H` – Hole,
    and `G` – Goal. The player needs to move from the Start cell, `S`, to the Goal
    cell, along with the Frozen areas (`F` cells), without falling into Holes (`H`
    cells). The following figure visually presents the aforementioned information:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个 4x4 的网格，这就是整个冰冻湖。它包含 16 个格子（一个 4x4 的网格）。这些格子标记为 `S` – 起始点，`F` – 冰冻区域，`H`
    – 坑洞，`G` – 目标。玩家需要从起始格子 `S` 移动到目标格子 `G`，并且穿过冰冻区域（`F` 格子），避免掉进坑洞（`H` 格子）。下图直观地展示了上述信息：
- en: '![Figure 6.10: The Frozen Lake game'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.10：冰冻湖游戏'
- en: '](img/B16182_06_10.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_06_10.jpg)'
- en: 'Figure 6.10: The Frozen Lake game'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10：冰冻湖游戏
- en: 'Here are some basic details of the game:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是游戏的一些基本信息：
- en: '`S`) to the Goal (cell `G`).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`S`）到达目标（`G` 格子）。'
- en: '**States** = 16'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态** = 16'
- en: '**Actions** = 4'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作** = 4'
- en: '**Total state-action pairs** = 64'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总状态-动作对** = 64'
- en: '`F`) without falling into the Holes in the lake (cells `H`). Reaching the Goal
    (cell `G`) or falling into any Hole (cells `H`) ends the game.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`F`）避免掉进湖中的坑洞（`H` 格子）。到达目标（`G` 格子）或掉进任何坑洞（`H` 格子）都会结束游戏。'
- en: '**Actions**: The actions that can be performed in any cell are left, down,
    right, and up.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作**：在任意一个格子中可以执行的动作有：左、下、右、上。'
- en: '**Players**: It is a single-player game.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**玩家**：这是一个单人游戏。'
- en: '`F`), +1 for reaching the Goal (cell `G`), and 0 for falling into a Hole (cells
    `H`).'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`F`），到达目标（`G` 格子）得 +1，掉进坑洞（`H` 格子）得 0。'
- en: '**Configuration**: You can configure the frozen lake to be slippery or non-slippery.
    If the frozen lake is slippery, then the intended action and the actual action
    can vary, so if someone wants to move left, they might end up moving right or
    down or up. If the frozen lake is non-slippery, the intended action and the actual
    action are always aligned. The grid has 16 possible cells where the agent can
    be at any point in time. The agent can take 4 possible actions in each of these
    cells. So, there are 64 possibilities in the game, whose likelihood is updated
    based on the learning. In the next activity, we will learn more about the Frozen
    Lake game, and understand the various steps and actions.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**配置**：你可以配置冰湖是滑的还是不滑的。如果冰湖是滑的，那么预期的动作和实际动作可能会有所不同，因此如果有人想向左移动，他们可能最终会向右、向下或向上移动。如果冰湖是非滑的，预期的动作和实际动作始终对齐。该网格有
    16 个可能的单元，代理可以在任何时刻处于其中一个单元。代理可以在每个单元中执行 4 种可能的动作。因此，游戏中有 64 种可能性，这些可能性会根据学习过程不断更新。在下一次活动中，我们将深入了解
    Frozen Lake 游戏，并了解其中的各种步骤和动作。'
- en: 'Activity 6.01: Exploring the Frozen Lake Problem – the Reward Function'
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 6.01：探索 Frozen Lake 问题 – 奖励函数
- en: Frozen Lake is a game in OpenAI Gym that's helpful to apply learning and reinforcement
    techniques. In this activity, we will solve the Frozen Lake problem and determine
    the various states and actions using Monte Carlo methods. We will track the success
    rate through batches of episodes.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Frozen Lake 是 OpenAI Gym 中的一款游戏，有助于应用学习和强化学习技术。在本次活动中，我们将解决 Frozen Lake 问题，并通过蒙特卡罗方法确定各种状态和动作。我们将通过一批批的回合来跟踪成功率。
- en: 'Perform the following steps to complete the activity:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以完成活动：
- en: 'We import the necessary libraries: `gym` for the OpenAI Gym framework, `numpy`,
    and `defaultdict` is required to process dictionaries.'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入必要的库：`gym` 用于 OpenAI Gym 框架，`numpy`，以及处理字典所需的 `defaultdict`。
- en: The next step is to select the environment as `FrozenLake`. `is_slippery` is
    set to `False`. The environment is reset with the line `env.reset()` and rendered
    with the line `env.render()`.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是选择环境为 `FrozenLake`，并将 `is_slippery` 设置为 `False`。通过 `env.reset()` 重置环境，并通过
    `env.render()` 渲染环境。
- en: The number of possible values in the observation space is printed with `print(env.observation_space)`.
    Similarly, the number of action values is printed with the `print(env.action_space)`
    command.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察空间中可能的值的数量通过 `print(env.observation_space)` 打印输出。同样，动作空间中的值的数量通过 `print(env.action_space)`
    命令打印输出。
- en: The next step is to define a function to generate a frozen lake `episode`. We
    initialize the episodes and the environment.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是定义一个函数来生成一个冰湖 `episode`。我们初始化回合和环境。
- en: We simulate various episodes by using a Monte Carlo method. We then navigate
    step by step and store `episode` and return `reward`. The action is obtained with
    `env.action_space.sample()`. `next_state`, `action`, and `reward` are obtained
    by calling the `env_step(action)` function. They are then appended to an episode.
    The episode is now a list of states, actions, and rewards.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过使用蒙特卡罗方法模拟不同的回合。然后我们逐步导航，存储 `episode` 并返回 `reward`。通过 `env.action_space.sample()`
    获取动作。`next_state`、`action` 和 `reward` 通过调用 `env_step(action)` 函数获得。然后将它们附加到回合中。现在，回合变成了一个包含状态、动作和奖励的列表。
- en: The key is now to calculate the success rate, which is the likelihood of success
    for a batch of episodes. The way we do this is by calculating the total number
    of attempts in a batch of episodes. We calculate how many of them successfully
    reached the goal. The ratio of the agent successfully reaching the goal to the
    number of attempts made by the agent is the success ratio. First, we initialize
    the total rewards.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关键是计算成功率，即一批回合的成功概率。我们的方法是计算一批回合中的总尝试次数。我们计算其中有多少次成功到达目标。代理成功到达目标的次数与代理尝试次数的比率即为成功率。首先，我们初始化总奖励。
- en: We generate `episode` and `reward` for every iteration and calculate the total
    `reward`.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为每次迭代生成 `episode` 和 `reward`，并计算总 `reward`。
- en: The success ratio is calculated by dividing `total_reward` by `100` and is printed.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 成功率是通过将 `total_reward` 除以 `100` 来计算的，并打印输出。
- en: The frozen lake prediction is calculated using the `frozen_lake_prediction`
    function. The final output will demonstrate the default success ratio of the game
    without any reinforcement learning when the game is played randomly.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冰湖预测通过 `frozen_lake_prediction` 函数计算得出。最终输出将展示游戏的默认成功率，即在没有任何强化学习的情况下，游戏随机进行时的成功率。
- en: 'You will get the following output:'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下输出：
- en: '![Figure 6.11: Output of Frozen Lake without learning'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.11：没有学习的冰湖输出](img/B16182_06_11.jpg)'
- en: '](img/B16182_06_11.jpg)'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_06_11.jpg)'
- en: 'Figure 6.11: Output of Frozen Lake without learning'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11：没有学习的冰湖输出
- en: Note
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: The solution to this activity can be found on page 719.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第 719 页找到。
- en: In the next section, we detail how we can enable improvement by balancing exploration
    and exploitation, by using the epsilon soft policy and greedy policy. This ensures
    that we balance exploration and exploitation.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将详细介绍如何通过平衡探索和利用，使用 epsilon软策略和贪婪策略来实现改进。这可以确保我们平衡探索和利用。
- en: The Pseudocode for Every Visit Monte Carlo Control for Epsilon Soft
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 每次访问蒙特卡洛控制伪代码（用于 epsilon软）
- en: 'We have previously implemented every visit Monte Carlo algorithm for estimating
    the value function. In this section, we will briefly describe every visit Monte
    Carlo control for epsilon soft so that we can use this in our final activity of
    this chapter. The following figure shows the pseudo-code for every visit for Epsilon
    soft by balancing exploration and exploitation:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经实现了每次访问蒙特卡洛算法来估算价值函数。在本节中，我们将简要描述用于 epsilon软的每次访问蒙特卡洛控制，以便我们可以在本章的最终活动中使用它。下图展示了通过平衡探索和利用，针对
    epsilon软的每次访问伪代码：
- en: '![Figure 6.12: Pseudocode for Monte Carlo every visit for epsilon soft'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.12：蒙特卡洛每次访问的伪代码（用于 epsilon软）](img/B16182_06_12.jpg)'
- en: '](img/B16182_06_12.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_06_12.jpg)'
- en: 'Figure 6.12: Pseudocode for Monte Carlo every visit for epsilon soft'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12：蒙特卡洛每次访问的伪代码（用于 epsilon软）
- en: 'The following code picks a random action with epsilon probability and picks
    an action that has a maximum `Q(s,a)` with 1-epsilon probability. So, we can choose
    between exploration with epsilon probability and exploitation with 1-epsilon probability:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码以 epsilon 概率选择一个随机动作，并以 1-epsilon 概率选择一个具有最大 `Q(s,a)` 的动作。因此，我们可以在以 epsilon
    概率进行探索和以 1-epsilon 概率进行利用之间做出选择：
- en: '[PRE51]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: In the next activity, we will evaluate and improve the policy for Frozen Lake
    by implementing the Monte Carlo control every visit for the epsilon soft method.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一项活动中，我们将通过实现蒙特卡洛控制每次访问的 epsilon软方法来评估和改进冰湖问题的策略。
- en: Activity 6.02 Solving Frozen Lake Using Monte Carlo Control Every Visit Epsilon
    Soft
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 6.02 使用蒙特卡洛控制每次访问解决冰湖问题（epsilon软）
- en: The activity aims to evaluate and improve the policy for the Frozen Lake problem
    by using every visit epsilon soft method.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的目标是通过使用每次访问 epsilon软方法来评估和改进冰湖问题的策略。
- en: 'You can launch the Frozen Lake game by importing `gym` and executing `gym.make()`:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过导入 `gym` 并执行 `gym.make()` 来启动冰湖游戏：
- en: '[PRE52]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Perform the following step to complete the activity:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以完成活动：
- en: Import the necessary libraries.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库。
- en: Select the environment as `FrozenLake`. `is_slippery` is set to `False`.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择环境为 `FrozenLake`。`is_slippery` 设置为 `False`。
- en: Initialize the `Q` value and `num_state_action` to zeros.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `Q` 值和 `num_state_action` 初始化为零。
- en: Set the value of `num_episodes` to `100000` and create `rewardsList`. Set epsilon
    to `0.30`.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `num_episodes` 的值设置为 `100000`，并创建 `rewardsList`。将 epsilon 设置为 `0.30`。
- en: Run the loop till `num_episodes`. Initialize the environment, `results_List`,
    and `result_sum` to zero. Also, reset the environment.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环运行直到 `num_episodes`。初始化环境、`results_List` 和 `result_sum` 为零。同时重置环境。
- en: We need to now have both exploration and exploitation. Exploration will be a
    random policy with epsilon probability and exploitation will be a greedy policy
    with 1-epsilon. We start a `while` loop and check whether we need to pick a random
    value with probability epsilon or a greedy policy with a probability of 1-epsilon.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要同时进行探索和利用。探索将是一个具有 epsilon 概率的随机策略，利用将是一个具有 1-epsilon 概率的贪婪策略。我们开始一个`while`循环，并检查是否需要以
    epsilon 的概率选择一个随机值，或者以 1-epsilon 的概率选择一个贪婪策略。
- en: Step through the `action` and get `new_state` and `reward`.
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逐步执行 `action` 并获得 `new_state` 和 `reward`。
- en: The result list is appended, with the state and action pair. `result_sum` is
    incremented by the value of the result.
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果列表将被附加，包括状态和动作对。`result_sum` 会根据结果的值递增。
- en: '`new_state` is assigned to `state` and `result_sum` is appended to `rewardsList`.'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `new_state` 赋值给 `state`，并将 `result_sum` 添加到 `rewardsList` 中。
- en: Calculate `Q[s,a]` using the incremental method, as `Q[s,a] + (result_sum –
    Q[s,a]) / N(s,a)`.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用增量方法计算 `Q[s,a]`，公式为 `Q[s,a] + (result_sum – Q[s,a]) / N(s,a)`。
- en: Print the value of the success rate in batches of `1000`.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印每 1000 次的成功率值。
- en: Print the final success rate.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印最终的成功率。
- en: 'You will get the following output initially:'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您将最初获得以下输出：
- en: '![Figure 6.13: Initial output of the Frozen Lake success rate'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.13：Frozen Lake成功率的初始输出'
- en: '](img/B16182_06_13.jpg)'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_06_13.jpg)'
- en: 'Figure 6.13: Initial output of the Frozen Lake success rate'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13：Frozen Lake成功率的初始输出
- en: 'You will get the following output finally:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 最终你将得到以下输出：
- en: '![Figure 6.14: Final output of the Frozen Lake success rate'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.14：Frozen Lake成功率的最终输出'
- en: '](img/B16182_06_14.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_06_14.jpg)'
- en: 'Figure 6.14: Final output of the Frozen Lake success rate'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14：Frozen Lake成功率的最终输出
- en: Note
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 722.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第722页找到。
- en: Summary
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Monte Carlo methods learn from experience in the form of sample episodes. Without
    having a model of the environment, by interacting with the environment, the agent
    can learn a policy. In several cases of simulation or sampling, an episode is
    feasible. We learned about the first visit and every visit evaluation. Also, we
    learned about the balance between exploration and exploitation. This is achieved
    by having an epsilon soft policy. We then learned about on-policy and off-policy
    learnings, and how importance sampling plays a key role in off-policy methods.
    We learned about the Monte Carlo methods by applying them to Blackjack and the
    Frozen Lake environment available in the OpenAI framework.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法通过样本情节的形式从经验中学习。在没有环境模型的情况下，智能体通过与环境互动，可以学习到一个策略。在多次仿真或抽样的情况下，情节是可行的。我们了解了首次访问和每次访问评估的方法。同时，我们也学习了探索与利用之间的平衡。这是通过采用一个ε软策略来实现的。接着，我们了解了基于策略学习和非基于策略学习，并且学习了重要性抽样在非基于策略方法中的关键作用。我们通过将蒙特卡洛方法应用于《黑杰克》游戏和OpenAI框架中的Frozen
    Lake环境来学习这些方法。
- en: In the next chapter, we will learn about temporal learning and its applications.
    Temporal learning combines the best of dynamic programming and the Monte Carlo
    methods. It can work where the model is not known, like the Monte Carlo methods,
    but can provide incremental learning instead of waiting for the episode to end.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习时间学习及其应用。时间学习结合了动态规划和蒙特卡洛方法的优点。它可以在模型未知的情况下工作，像蒙特卡洛方法一样，但可以提供增量学习，而不是等待情节结束。
