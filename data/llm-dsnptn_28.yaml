- en: '28'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '28'
- en: Advanced RAG
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级RAG
- en: In [*Chapter 26*](B31249_26.xhtml#_idTextAnchor366), we covered the basics of
    the RAG pattern, a simple process where a user’s query triggers a search in an
    external knowledge base. The information that’s retrieved is then directly appended
    to the query, and this augmented prompt is passed to the LLM to generate a response,
    allowing it to access external data without complex processing.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第26章*](B31249_26.xhtml#_idTextAnchor366)中，我们介绍了RAG模式的基础，这是一个简单的流程，其中用户的查询触发对外部知识库的搜索。检索到的信息随后直接附加到查询中，并将这个增强的提示传递给LLM以生成响应，允许它在不进行复杂处理的情况下访问外部数据。
- en: Now, in this chapter, we’ll move beyond these basic RAG methods and explore
    more sophisticated techniques designed to significantly enhance LLM performance
    across a wide range of tasks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在本章中，我们将超越这些基本的RAG方法，并探索更多旨在显著提高LLM在各种任务上性能的复杂技术。
- en: By the end of this chapter, you’ll be equipped with the knowledge to implement
    these advanced RAG strategies, enabling your LLM applications to achieve greater
    accuracy and efficiency.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将具备实施这些高级RAG策略的知识，使你的LLM应用能够实现更高的准确性和效率。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Multi-step and iterative retrieval techniques for LLMs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM的多步和迭代检索技术
- en: Adaptive retrieval based on context and task in LLMs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在LLM中基于上下文和任务的自适应检索
- en: Meta-learning for improved retrieval in LLMs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过元学习改进LLM中的检索
- en: Combining RAG with other LLM prompting techniques
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将RAG与其他LLM提示技术相结合
- en: Handling ambiguity and uncertainty in LLM-based RAG
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理基于LLM的RAG中的模糊性和不确定性
- en: Scaling RAG to very large knowledge bases
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将RAG扩展到非常大的知识库
- en: Future directions in RAG research for LLMs
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM的RAG研究未来方向
- en: Multi-step and iterative retrieval techniques for LLMs
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM的多步和迭代检索技术
- en: Using multi-step and iterative retrieval techniques for LLMs is a dynamic, recursive
    approach to information gathering where the model progressively refines its search
    strategy. The code provided in this section illustrates a multi-step RAG framework
    that expands context iteratively, retrieves additional documents, and generates
    responses through multiple steps, allowing for increasingly comprehensive and
    nuanced information retrieval by dynamically adjusting queries and integrating
    retrieved knowledge.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LLM的多步和迭代检索技术是一种动态、递归的信息收集方法，其中模型逐步优化其搜索策略。本节提供的代码演示了一个多步RAG框架，该框架通过迭代扩展上下文，检索额外的文档，并通过多个步骤生成响应，通过动态调整查询和整合检索到的知识，实现越来越全面和细致的信息检索。
- en: 'Here are some of its key characteristics:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 它的一些关键特性包括：
- en: Iterative context expansion
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代上下文扩展
- en: Multiple retrieval steps (configurable up to `max_steps`)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多步检索步骤（可配置至`max_steps`）
- en: Dynamic query refinement
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态查询优化
- en: Contextual document retrieval
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文文档检索
- en: Adaptive response generation
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自适应响应生成
- en: 'Multi-step and iterative retrieval techniques for LLMs, with their dynamic
    and recursive approaches, benefit use cases that require the following aspects:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的多步和迭代检索技术，其动态和递归方法，对以下方面的用例有益：
- en: '**Complex question-answering**: When questions require information to be synthesized
    from multiple sources or involve intricate logical reasoning, iterative retrieval
    allows the LLM to gather the necessary context progressively. Examples include
    legal document analysis, scientific research, and in-depth financial analysis.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂问答**：当问题需要从多个来源综合信息或涉及复杂的逻辑推理时，迭代检索允许LLM逐步收集必要的环境。例如，包括法律文件分析、科学研究以及深入的财务分析。'
- en: '**Knowledge-intensive conversations**: In conversational AI scenarios where
    the dialogue involves exploring a topic in depth, iterative RAG enables the LLM
    to maintain context and refine its understanding over multiple turns. This is
    valuable for educational chatbots, technical support, and interactive tutorials.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识密集型对话**：在涉及深入探讨主题的对话式AI场景中，迭代RAG允许LLM在多个回合中保持上下文并逐步优化其理解。这对于教育聊天机器人、技术支持和交互式教程非常有价值。'
- en: '**Research and exploration**: For tasks such as literature reviews, market
    research, or investigative journalism, the ability to dynamically refine queries
    and explore related information is crucial. Iterative retrieval allows the LLM
    to act as a research assistant, uncovering connections and insights that would
    be difficult to find with a single query.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**研究和探索**：对于文献综述、市场研究或调查性新闻等任务，动态细化查询和探索相关信息的能力至关重要。迭代检索允许LLM充当研究助手，揭示难以通过单次查询找到的关联和见解。'
- en: '**Technical documentation and troubleshooting**: When dealing with complex
    technical issues, iterative RAG can help the LLM navigate extensive documentation,
    progressively narrowing down the search to pinpoint relevant information. This
    improves the efficiency of troubleshooting and technical support.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**技术文档和故障排除**：在处理复杂技术问题时，迭代RAG可以帮助LLM导航广泛的文档，逐步缩小搜索范围以定位相关信息。这提高了故障排除和技术支持效率。'
- en: '**Dynamic information gathering**: This includes any situation where the information
    that is needed isn’t able to be gathered in a single pass. For example, if a user
    wants to find out all the news articles related to a specific court case and then
    wants to know what people are saying about those news articles on social media,
    multiple steps of information gathering would be required.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态信息收集**：这包括任何需要通过单次遍历无法收集到所需信息的情况。例如，如果用户想要查找与特定法庭案件相关的所有新闻文章，然后又想了解社交媒体上人们对这些新闻文章的看法，就需要进行多个步骤的信息收集。'
- en: '**Dealing with ambiguous queries**: When a user’s query is ambiguous, the LLM
    can ask clarifying questions and then use the user’s response to refine the search.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理模糊查询**：当用户的查询模糊时，LLM可以提出澄清问题，然后使用用户的响应来细化搜索。'
- en: In essence, any use case that demands a deep, nuanced understanding of information,
    and where a single retrieval step is insufficient, stands to gain significantly
    from multi-step and iterative RAG.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，任何需要深入、细致理解信息，并且单次检索步骤不足的应用场景，都能从多步和迭代RAG中获得显著收益。
- en: 'Let’s take a look at the following code example:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看以下代码示例：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In this pseudocode example, the `MultiStepRAG` class implements multistep retrieval
    through three critical methods:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个伪代码示例中，`MultiStepRAG`类通过三个关键方法实现了多步检索：
- en: '`retrieve_and_generate()`: This method iteratively expands context by retrieving
    documents, generating responses, and dynamically updating the search context across
    multiple steps. It manages the retrieval process, limiting iterations to a configurable
    maximum.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieve_and_generate()`: 此方法通过检索文档、生成响应和动态更新多个步骤中的搜索上下文来迭代地扩展上下文。它管理检索过程，将迭代限制在可配置的最大值内。'
- en: '`is_response_complete()`: This method evaluates response quality by detecting
    whether the generated answer addresses the query sufficiently, typically checking
    for indicators of incomplete information.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_response_complete()`: 此方法通过检测生成的答案是否充分回答了查询，通常检查不完整信息的指标，来评估响应质量。'
- en: '`generate_follow_up_query()`: This method creates refined follow-up queries
    by using the language model to generate new questions based on the original query
    and current response, enabling intelligent context exploration.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generate_follow_up_query()`: 此方法通过使用语言模型根据原始查询和当前响应生成新问题，创建精细的后续查询，从而实现智能上下文探索。'
- en: This implementation allows for progressive information gathering, where each
    retrieval step dynamically refines the context and generates more comprehensive
    responses by recursively expanding the knowledge base.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现允许逐步收集信息，其中每个检索步骤都动态地细化上下文，并通过递归扩展知识库来生成更全面的响应。
- en: Adaptive retrieval based on context and task in LLMs
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于上下文和任务的自适应检索在LLM中
- en: Adaptive retrieval is a sophisticated approach to information retrieval that
    dynamically adjusts strategies based on specific task requirements.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应检索是一种复杂的信息检索方法，它根据特定的任务需求动态调整策略。
- en: 'The following code demonstrates this concept through an implementation that
    tailors retrieval and generation processes across different task types:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码通过针对不同任务类型定制检索和生成过程的实现来演示这一概念：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding code introduces an `AdaptiveRAG` class that uses an `Enum` value
    called `TaskType` to define distinct retrieval strategies for different scenarios:
    factual question-answering, summarization, and analysis. Each task type receives
    customized treatment in terms of document retrieval volume and prompt formatting.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码引入了一个名为`AdaptiveRAG`的类，它使用一个名为`TaskType`的`Enum`值来定义不同场景下的不同检索策略：事实性问题回答、摘要和分析。每种任务类型在文档检索量和提示格式方面都接受定制处理。
- en: 'In the `retrieve_and_generate()` method, the system dynamically configures
    retrieval parameters:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在`retrieve_and_generate()`方法中，系统动态配置检索参数：
- en: '`Factual QA`: This retrieves three documents with a direct question-answer
    format'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Factual QA`：此操作检索三个具有直接问答格式的文档'
- en: '`Summarization`: This retrieves ten documents with a summary-focused template'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Summarization`：此操作检索十个以摘要为重点的文档'
- en: '`Analysis`: This retrieves five documents with an analytical prompt structure'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Analysis`：此操作检索五个具有分析提示结构的文档'
- en: The method retrieves relevant documents, constructs a context, generates a task-specific
    prompt, and produces a response tailored to the specific task type. This approach
    allows for more nuanced and contextually appropriate information retrieval and
    generation across different knowledge exploration scenarios.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法检索相关文档，构建上下文，生成特定任务的提示，并产生针对特定任务类型的定制响应。这种方法允许在不同知识探索场景中进行更细致和上下文相关的信息检索和生成。
- en: This example usage demonstrates flexibility by generating responses for factual
    queries, summaries, and analytical tasks using the same adaptive framework.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例用法通过使用相同的自适应框架生成事实查询、摘要和分析任务的响应，展示了其灵活性。
- en: Meta-learning for improved retrieval in LLMs
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于改进LLM检索的元学习
- en: Meta-learning in retrieval systems is a dynamic approach where the model learns
    to improve its retrieval strategy by analyzing past performance and relevance
    feedback. In this implementation, meta-learning focuses on selecting and ranking
    documents adaptively based on learned relevance patterns.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习在检索系统中是一种动态方法，模型通过分析过去的表现和相关性反馈来学习改进其检索策略。在本实现中，元学习侧重于根据学习到的相关性模式自适应地选择和排序文档。
- en: Let’s implement a simple meta-learning approach for RAG.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为RAG实现一个简单的元学习方法。
- en: 'The following code demonstrates meta-learning by retrieving documents about
    dark matter theories and simulating relevance feedback to train the model, showcasing
    how the system can improve its information retrieval capabilities iteratively:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码通过检索关于暗物质理论的文档并模拟相关性反馈来训练模型，展示了如何迭代地提高系统的信息检索能力：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The key meta-learning components in the preceding code include the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码中的关键元学习组件包括以下内容：
- en: '`predict_relevance()` method estimates the probability of document usefulness'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict_relevance()`方法估计文档的有用性概率'
- en: Dynamically adjusts document selection based on learned features
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据学习到的特征动态调整文档选择
- en: '`compute_features()` method generates document representation features*   Currently,
    it uses randomly generated values as placeholder features for demonstration or
    testing purposes*   In practice, it would include semantic similarity, keyword
    matching, and more.*   **Adaptive** **learning mechanism**:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compute_features()`方法生成文档表示特征*   目前，它使用随机生成的值作为演示或测试目的的占位符特征*   在实践中，它将包括语义相似度、关键词匹配等更多内容.*   **自适应**
    **学习机制**：'
- en: Accumulates training data from relevance feedback
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从相关性反馈中积累训练数据
- en: Retrains the meta-model when sufficient data is collected (100 samples)
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当收集到足够的数据时（100个样本）重新训练元模型
- en: 'Clears the training data after model updates to prevent overfitting*   **Retrieval**
    **strategy modification**:'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型更新后清除训练数据以防止过拟合*   **检索** **策略修改**：
- en: Initially uses the top 10 retrieved documents
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始使用检索到的前10个文档
- en: After meta-model training, it selects the top three documents based on learned
    relevance scores
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在元模型训练后，根据学习到的相关性分数选择前三个文档
- en: Continuously refines the document selection process
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续优化文档选择过程
- en: The code implements a `MetaLearningRAG` class that dynamically enhances retrieval
    performance using machine learning techniques. The core innovation lies in its
    ability to learn from relevance feedback and adjust document selection strategies.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码实现了一个`MetaLearningRAG`类，它使用机器学习技术动态增强检索性能。其核心创新在于其能够从相关性反馈中学习并调整文档选择策略。
- en: 'Let’s look at the key methods:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看关键的方法：
- en: '`retrieve_and_generate()`: Selects the top documents using a trained meta-model'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieve_and_generate()`: 使用训练好的元模型选择顶级文档'
- en: '`predict_relevance()`: Estimates document relevance probabilities'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict_relevance()`: 估计文档的相关性概率'
- en: '`compute_features()`: Generates feature representations for documents'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compute_features()`: 为文档生成特征表示'
- en: '`update_meta_model()`: Periodically retrains the model based on relevance feedback'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`update_meta_model()`: 根据相关性反馈定期重新训练模型'
- en: The implementation uses logistic regression to predict document relevance, progressively
    refining retrieval by learning from user interactions. When sufficient training
    data has been accumulated, the meta-model is retrained, allowing the system to
    adapt its document selection strategy based on historical performance and feedback.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 实现使用逻辑回归来预测文档的相关性，通过学习用户交互逐步优化检索。当积累足够的训练数据后，元模型被重新训练，使系统能够根据历史性能和反馈调整其文档选择策略。
- en: In the context of meta-learning for retrieval systems, *relevance* refers to
    the contextual usefulness and information value of the documents that were retrieved
    for a specific query.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索系统的元学习背景下，*相关性*指的是为特定查询检索到的文档的上下文有用性和信息价值。
- en: 'Let’s look at the key *relevance* aspects shown in the preceding code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看前面代码中显示的关键 *相关性* 方面：
- en: '**Relevance scoring**:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相关性评分**：'
- en: Predicts the probability of the document being useful
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测文档的有用性概率
- en: Uses machine learning to learn relevance patterns
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习学习相关性模式
- en: Allows dynamic document ranking
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许动态文档排名
- en: '`1` = relevant, `0` = not relevant)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1` = 相关，`0` = 不相关）'
- en: Enables the system to learn from user-provided quality signals
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使系统能够从用户提供的质量信号中学习
- en: Improves future document selection
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改善未来的文档选择
- en: '**Feature-based relevance**:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于特征的相关性**：'
- en: Computes document features representing potential usefulness
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算表示潜在有用性的文档特征
- en: The preceding code uses random features
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之前的代码使用随机特征
- en: Captures semantic and contextual relationships
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕获语义和上下文关系
- en: The core goal is to create an adaptive retrieval system that learns to select
    increasingly precise and valuable documents through iterative feedback and machine
    learning techniques.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 核心目标是创建一个自适应的检索系统，通过迭代反馈和机器学习技术学习选择越来越精确和有价值的文档。
- en: Combining RAG with other LLM prompting techniques
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将RAG与其他LLM提示技术结合
- en: 'We can enhance RAG by combining it with other prompting techniques, such as
    CoT (see [*Chapter 20*](B31249_20.xhtml#_idTextAnchor305)) or few-shot learning.
    Here’s an example that combines RAG with CoT:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过结合其他提示技术，如CoT（见[*第20章*](B31249_20.xhtml#_idTextAnchor305)）或少样本学习来增强RAG。以下是一个将RAG与CoT结合的示例：
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `RAGWithCoT` class implements a RAG approach enhanced with CoT reasoning.
    By retrieving relevant documents and constructing a prompt that encourages step-by-step
    problem solving, the method transforms standard query response generation into
    a more structured, analytical process.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`RAGWithCoT`类实现了增强CoT推理的RAG方法。通过检索相关文档并构建一个鼓励逐步解决问题的提示，该方法将标准的查询响应生成转变为一个更结构化、分析性的过程。'
- en: The implementation guides the language model through an explicit reasoning framework,
    breaking complex queries into logical steps. This approach prompts the model to
    demonstrate intermediate reasoning, creating a more transparent and potentially
    more accurate response generation process.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 实现引导语言模型通过一个明确的推理框架，将复杂查询分解为逻辑步骤。这种方法促使模型展示中间推理，创建一个更透明且可能更准确的响应生成过程。
- en: The method combines contextual document retrieval with a carefully designed
    prompt template that explicitly structures the model’s reasoning. By requiring
    the model to outline its thinking process before presenting a final answer, the
    implementation seeks to improve the depth and quality of generated responses.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法将上下文文档检索与精心设计的提示模板相结合，该模板明确地结构化了模型的推理。通过要求模型在呈现最终答案之前概述其思考过程，实现寻求提高生成响应的深度和质量。
- en: 'As we explore advanced RAG techniques, the next critical challenge emerges:
    handling ambiguity and uncertainty in language-model-based information retrieval.
    The following section will delve into sophisticated strategies for managing complex,
    nuanced, and potentially conflicting information sources, highlighting approaches
    that enable more robust and reliable knowledge extraction and generation.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们探索高级RAG技术，下一个关键挑战出现：处理基于语言模型的信息检索中的模糊性和不确定性。下一节将深入探讨管理复杂、细微且可能存在冲突的信息源的复杂策略，突出能够实现更稳健和可靠的知识提取和生成的途径。
- en: Handling ambiguity and uncertainty in LLM-based RAG
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理基于LLM的RAG中的模糊性和不确定性
- en: Ambiguity and uncertainty directly compromise the accuracy and reliability of
    generated responses. Ambiguous queries, for instance, can trigger the process
    of retrieving irrelevant or conflicting information, leading the LLM to produce
    incoherent or incorrect outputs. Consider the query, “What about apples?” This
    could refer to Apple Inc., the fruit, or specific apple varieties. A naive RAG
    system might pull data from all contexts, resulting in a confused response.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊性和不确定性直接损害了生成响应的准确性和可靠性。例如，模糊的查询可能会触发检索无关或冲突信息的过程，导致LLM产生不连贯或错误的输出。考虑查询“关于苹果怎么样？”这可能指的是苹果公司、水果或特定的苹果品种。一个简单的RAG系统可能会从所有上下文中提取数据，导致混乱的响应。
- en: Furthermore, uncertainty in retrieved information – due to conflicting or outdated
    data in the knowledge base – exacerbates the problem. Without mechanisms to assess
    data reliability, the LLM may propagate inaccuracies. LLMs themselves operate
    on probabilities, adding another layer of uncertainty. For example, when addressing
    a niche topic, an LLM might generate a “best guess” that, without proper uncertainty
    estimation, could be presented as fact. Combining multiple pieces of uncertain
    information further compounds this issue, potentially leading to misleading and
    unreliable responses, ultimately undermining user trust and limiting the practical
    applications of RAG systems.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于知识库中存在冲突或过时的数据，检索到的信息中的不确定性加剧了问题。如果没有评估数据可靠性的机制，LLM可能会传播不准确性。LLMs自身基于概率运作，增加了另一层不确定性。例如，在处理一个细分主题时，LLM可能会生成一个“最佳猜测”，如果没有适当的概率估计，可能会被当作事实呈现。结合多个不确定信息进一步加剧了这个问题，可能导致误导和不可靠的反应，最终损害用户信任并限制RAG系统的实际应用。
- en: 'To handle ambiguity and uncertainty, we can implement a system that generates
    multiple hypotheses and ranks them based on confidence:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理模糊性和不确定性，我们可以实施一个生成多个假设并按置信度对它们进行排名的系统：
- en: '[PRE4]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding code implements an `UncertaintyAwareRAG` class that intelligently
    handles ambiguous queries by generating multiple possible answers with confidence
    scores. It works by initializing with a retriever component (for fetching relevant
    documents), a generator (language model), and a parameter for the number of hypotheses
    to generate. When `retrieve_and_generate` is called with a query, it retrieves
    relevant documents and combines them into a context, then constructs a specialized
    prompt asking for multiple possible answers with confidence scores. The generator
    produces multiple hypotheses using the `num_return_sequences` parameter, each
    including a confidence score. These hypotheses are parsed using the `parse_hypothesis`
    method, which extracts both the answer text and its confidence score from a standardized
    format of `"Answer (Confidence: X%): ..."`. The results are then sorted by confidence
    score and returned as a dictionary that maps answers to their confidence values.
    This approach is particularly valuable for questions that may not have a single
    definitive answer (such as future predictions or complex scenarios) as it explicitly
    acknowledges uncertainty and provides multiple plausible responses with their
    associated confidence levels, allowing users to make more informed decisions based
    on the range of possibilities and their relative likelihoods.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '上述代码实现了一个`UncertaintyAwareRAG`类，该类通过生成具有置信度分数的多个可能答案来智能地处理模糊查询。它通过初始化检索组件（用于获取相关文档）、生成器（语言模型）和生成假设数量的参数来工作。当使用查询调用`retrieve_and_generate`时，它检索相关文档并将它们组合成一个上下文，然后构建一个专门的提示，要求提供具有置信度分数的多个可能答案。生成器使用`num_return_sequences`参数生成多个假设，每个假设都包括一个置信度分数。这些假设使用`parse_hypothesis`方法进行解析，该方法从标准格式`"Answer
    (Confidence: X%): ..."`中提取答案文本及其置信度分数。然后，结果按置信度分数排序，并返回一个将答案映射到其置信度值的字典。这种方法对于可能没有单一确定答案的问题（如未来预测或复杂场景）特别有价值，因为它明确承认了不确定性，并提供了具有相关置信水平的多条可能的响应，使用户能够根据可能性的范围及其相对可能性做出更明智的决定。'
- en: After implementing uncertainty handling in our RAG system, the next crucial
    challenge is dealing with massive document collections. As knowledge bases grow
    to millions or even billions of documents, traditional retrieval methods become
    impractical, requiring more sophisticated approaches. Let’s explore how we can
    scale RAG so that it can handle very large knowledge bases efficiently through
    hierarchical indexing.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的RAG系统中实现不确定性处理之后，下一个关键挑战是处理大量的文档集合。随着知识库增长到数百万甚至数十亿文档，传统的检索方法变得不切实际，需要更复杂的方法。让我们探讨如何通过分层索引来扩展RAG，使其能够高效地处理非常大的知识库。
- en: Scaling RAG to very large knowledge bases
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将RAG扩展到非常大的知识库
- en: We can scale RAG using a hierarchical system. A hierarchical RAG system is an
    advanced architecture that organizes document retrieval in a tree-like structure
    with multiple levels. Instead of searching through all documents linearly, it
    first clusters similar documents together and creates a hierarchy of these clusters.
    When a query comes in, the system identifies the most relevant cluster(s) at the
    top level, drills down to find the most relevant sub-clusters, and finally retrieves
    the most similar documents from within those targeted sub-clusters. Think of it
    like a library where books are first organized by broad categories (science, history,
    fiction), then by sub-categories (physics, biology, chemistry), and finally by
    specific topics – this makes finding a particular book much faster than searching
    through every single book.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用分层系统来扩展RAG。一个分层RAG系统是一种高级架构，它以树状结构组织文档检索，具有多个层级。它不是线性地搜索所有文档，而是首先将相似的文档聚类在一起，并创建这些聚类的层次结构。当查询到来时，系统识别出最相关的顶级聚类（们），然后深入挖掘以找到最相关的子聚类，并最终从这些目标子聚类中检索出最相似的文档。想象一下，就像一个图书馆，书籍首先按广泛的类别（科学、历史、小说）组织，然后按子类别（物理学、生物学、化学）组织，最后按具体主题组织——这使得找到特定书籍比搜索每一本书要快得多。
- en: The hierarchical approach to RAG offers significant advantages because it dramatically
    improves both the efficiency and scalability of document retrieval while maintaining
    high accuracy. By organizing documents into clusters and sub-clusters, the system
    can quickly narrow down the search space from potentially millions of documents
    to a much smaller, relevant subset, which not only speeds up retrieval but also
    reduces computational resources and memory requirements. This makes it possible
    to handle massive document collections that would be impractical with traditional
    flat retrieval approaches. The hierarchical structure also enables better parallelization
    of search operations and can even improve result quality by considering document
    relationships within the hierarchy.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: RAG的分层方法提供了显著的优势，因为它显著提高了文档检索的效率和可扩展性，同时保持了高精度。通过将文档组织成簇和子簇，系统可以快速将搜索空间从可能数百万份文档缩小到一个更小、更相关的子集，这不仅加快了检索速度，还减少了计算资源和内存需求。这使得处理大规模文档集合成为可能，这在传统的平面检索方法中是不切实际的。分层结构还使得搜索操作能够更好地并行化，并且通过考虑层次结构内的文档关系，甚至可以提高结果质量。
- en: 'The following code snippet defines a class for hierarchical RAG, leveraging
    Facebook’s AI Similarity Search (Faiss) library for efficient similarity search
    and generation capabilities:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段定义了一个用于分层RAG的类，利用Facebook的AI相似性搜索（Faiss）库进行高效的相似性搜索和生成能力：
- en: '[PRE5]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The preceding code implements a `HierarchicalRAG` class that creates an efficient
    retrieval system using `1000`) – it uses FAISS’s `IVFFlat` index, which is a hierarchical
    index that first clusters the vectors and then performs an exact search within
    relevant clusters, where the quantizer (`IndexFlatL2`) is used to assign vectors
    to clusters during training. The `retrieve` method takes a query and returns *k*
    similar documents by first computing the query’s embedding and then searching
    the hierarchical index. The `compute_embedding` method is a placeholder that would
    typically implement actual embedding computation. The `retrieve_and_generate`
    method ties everything together by retrieving relevant documents, concatenating
    them into a context, creating a prompt that combines the context and query, and
    then using the language model to generate a response. The example usage shows
    how to initialize the system with 1 million documents (using random embeddings
    for demonstration purposes) and perform a query about quantum computing. First,
    the `IVFFlat` index groups similar documents together during training (`index.train()`)
    and then uses these clusters to speed up search operations by only searching in
    the most relevant clusters instead of the entire dataset, making it much more
    efficient than a brute-force approach when dealing with large document collections.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码实现了一个`HierarchicalRAG`类，它使用`1000`创建了一个高效的检索系统——它使用FAISS的`IVFFlat`索引，这是一个先对向量进行聚类然后在这些相关簇内进行精确搜索的分层索引，其中量化器（`IndexFlatL2`）在训练期间用于将向量分配到簇中。`retrieve`方法接收一个查询并返回*k*个相似的文档，首先计算查询的嵌入，然后搜索分层索引。`compute_embedding`方法是一个占位符，通常用于实现实际的嵌入计算。`retrieve_and_generate`方法通过检索相关文档，将它们连接成一个上下文，创建一个结合上下文和查询的提示，然后使用语言模型生成响应。示例用法展示了如何使用1百万份文档（出于演示目的使用随机嵌入）初始化系统，并执行关于量子计算的查询。首先，`IVFFlat`索引在训练期间将相似的文档分组在一起（`index.train()`），然后使用这些簇通过仅在最相关的簇中搜索来加速搜索操作，而不是在整个数据集中搜索，这使得在处理大型文档集合时比蛮力方法更加高效。
- en: Now that we’ve explored how to scale RAG systems to handle massive knowledge
    bases through hierarchical indexing, let’s look ahead to some exciting future
    directions in RAG research for LLMs.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了如何通过分层索引扩展RAG系统以处理大规模知识库，让我们展望一下LLMs在RAG研究中的令人兴奋的未来方向。
- en: Future directions in RAG research for LLMs
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs在RAG研究中的未来方向
- en: 'As RAG continues to evolve, several promising research directions have begun
    to emerge:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 随着RAG的不断发展，几个有希望的研究方向开始出现：
- en: '**Multi-modal RAG**: Incorporating image, audio, and video data in retrieval
    and generation'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多模态RAG**：在检索和生成中结合图像、音频和视频数据'
- en: '**Temporal RAG**: Handling time-sensitive information and updates'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间敏感RAG**：处理时间敏感信息和更新'
- en: '**Personalized RAG**: Adapting retrieval and generation to individual user
    preferences and knowledge'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个性化RAG**：根据个人用户偏好和知识调整检索和生成'
- en: '**Explainable RAG**: Providing transparency in the retrieval and generation
    process'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释的RAG**：在检索和生成过程中提供透明度'
- en: '**Continual learning in RAG**: Updating knowledge bases and retrieval mechanisms
    in real time'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RAG中的持续学习**：实时更新知识库和检索机制'
- en: 'Here’s a conceptual implementation of a multi-modal RAG system:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个多模态RAG系统的概念实现：
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s understand how this code implements a multi-modal RAG system that combines
    both text and image processing capabilities.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解这段代码是如何实现一个结合文本和图像处理能力的多模态RAG系统的。
- en: 'The `MultiModalRAG` class represents an advanced RAG system that can process
    both textual and visual information simultaneously to provide more comprehensive
    responses. It’s initialized with three key components: a text retriever (for handling
    textual documents), an image retriever (for processing visual content), and a
    generator (language model for response generation), along with an image transformer
    that standardizes images to a consistent size (`224` x `224`). The core method,
    `retrieve_and_generate`, takes both a text query and an optional image query,
    first retrieving relevant text documents using the text retriever. Then, if an
    image is provided, it processes it through the image transformer and retrieves
    relevant images using the image retriever. These retrieved images are then converted
    into textual descriptions using the `describe_images` method (which, in a real
    implementation, would use an image captioning model). All this information is
    combined into a structured prompt that includes both text and image context, allowing
    the generator to create responses that incorporate both textual and visual information.
    This multi-modal approach is particularly powerful for queries that benefit from
    visual contexts, such as explaining scientific processes, describing physical
    objects, or analyzing visual patterns. This is demonstrated in the preceding example,
    where it’s used to explain photosynthesis with both textual information and a
    plant image.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`MultiModalRAG`类代表了一种高级RAG系统，能够同时处理文本和视觉信息，以提供更全面的响应。它由三个关键组件初始化：一个文本检索器（用于处理文本文档）、一个图像检索器（用于处理视觉内容）和一个生成器（用于响应生成的语言模型），以及一个图像转换器，该转换器将图像标准化为一致的大小（`224`
    x `224`）。核心方法`retrieve_and_generate`接受一个文本查询和一个可选的图像查询，首先使用文本检索器检索相关文本文档。然后，如果提供了图像，它通过图像转换器处理该图像，并使用图像检索器检索相关图像。这些检索到的图像随后通过`describe_images`方法（在实际实现中会使用图像标题模型）转换为文本描述。所有这些信息结合成一个结构化的提示，包括文本和图像上下文，使生成器能够创建结合文本和视觉信息的响应。这种多模态方法对于受益于视觉上下文的查询特别强大，例如解释科学过程、描述物理对象或分析视觉模式。这在先前的示例中得到了演示，其中它被用来结合文本信息和植物图像来解释光合作用。'
- en: 'The preceding code represents an important step forward in RAG systems by doing
    the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码代表了RAG系统向前迈出的重要一步，通过以下方式实现：
- en: Breaking down the traditional text-only barrier
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打破传统的纯文本障碍
- en: Enabling richer, more contextual responses
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现更丰富、更具上下文的相关响应
- en: Creating a flexible framework that could be extended to other modalities
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个灵活的框架，可以扩展到其他模态
- en: Demonstrating how different types of information can be unified in a single
    system
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示如何将不同类型的信息统一在一个系统中
- en: Summary
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter elevated RAG from a basic data retrieval method to a dynamic framework
    for building truly adaptive LLM-powered systems. It explored techniques such as
    iterative and adaptive retrieval, meta-learning, and synergistic prompting, transforming
    RAG into a context-aware problem solver capable of complex analysis and nuanced
    understanding, mirroring expert-level research. Addressing ambiguity, uncertainty,
    and scalability isn’t just about overcoming hurdles, but about building trust
    and enabling real-world deployment.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将RAG从一种基本的数据检索方法提升为一个构建真正自适应的LLM（大型语言模型）系统的动态框架。它探讨了诸如迭代和自适应检索、元学习和协同提示等技术，将RAG转变为一个具有复杂分析和细微理解能力的上下文感知问题解决者，与专家级研究相呼应。解决歧义、不确定性和可扩展性问题并不仅仅是克服障碍，更是建立信任并实现现实世界部署。
- en: In the next chapter, we’ll explore various evaluation techniques for RAG systems.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨RAG系统的各种评估技术。
