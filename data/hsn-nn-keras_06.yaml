- en: Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: In the last chapter, we saw how to perform several signal-processing tasks while
    leveraging the predictive power of feedforward neural networks. This foundational
    architecture allowed us to introduce many of the basic features that comprise
    the learning mechanisms of **Artificial Neural Networks** (**ANNs**).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到如何利用前馈神经网络的预测能力执行多个信号处理任务。这一基础架构使我们能够引入构成**人工神经网络**（**ANNs**）学习机制的许多基本特性。
- en: In this chapter, we dive deeper to explore another type of ANN, namely the **Convolutional
    Neural Network** (**CNN**), famous for its adeptness at visual tasks such as image
    recognition, object detection, and semantic segmentation, to name a few. Indeed,
    the inspiration for these particular architectures also refers back to our own
    biology. Soon, we will go over the experiments and discoveries of the human race
    that led to the inspiration for these complex systems that perform so well. The
    latest iterations of this idea can be traced back to the ImageNet classification
    challenge, where AlexNet was able to outperform the state-of-the-art computer
    vision systems of the time at image classification tasks on supermassive datasets.
    However, the idea behind CNNs, as we will soon see, was a product of multidisciplinary
    scientific research, backed by millions of years of trail runs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将更深入地探索另一种类型的人工神经网络，即**卷积神经网络**（**CNN**），它因在图像识别、目标检测和语义分割等视觉任务中的高效表现而闻名。事实上，这些特定架构的灵感也回溯到我们自己的生物学。很快，我们将回顾人类的实验和发现，这些实验和发现促成了这些复杂系统的灵感，这些系统在视觉任务上表现优异。这个概念的最新版本可以追溯到ImageNet分类挑战赛，在这项挑战中，AlexNet能够在超大数据集上的图像分类任务中超越当时最先进的计算机视觉系统。然而，正如我们很快将看到的，CNN的思想是跨学科科学研究的产物，背后有着数百万年的试验积累。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将涵盖以下主题：
- en: Why CNNs?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么是CNN？
- en: The birth of vision
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉的诞生
- en: Understanding biological vision
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解生物学中的视觉
- en: The birth of the modern CNN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代卷积神经网络的诞生
- en: Designing a CNN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计卷积神经网络（CNN）
- en: Dense versus convolutional layer
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稠密层与卷积层
- en: The convolution operation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积操作
- en: Preserving the spatial structure of an image
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持图像的空间结构
- en: Feature extraction using filters
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用滤波器进行特征提取
- en: Why CNNs?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么是CNN？
- en: CNNs are very similar to ordinary neural networks. As we have seen in the previous
    chapter, neural networks are made up of neurons that have learnable weights and
    biases. Each neuron still computes the weighted sum of its inputs using dot products,
    adds a bias term, and passes it through a nonlinear equation. The network will
    show just one differentiable score function that  will be, from raw images at
    one end to the class scores at other end.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络与普通神经网络非常相似。正如我们在上一章中看到的，神经网络由具有可学习权重和偏差的神经元组成。每个神经元仍然使用点积计算其输入的加权和，添加一个偏置项，然后通过非线性方程传递。网络将展示一个可微分的评分函数，这个函数将从一端的原始图像到另一端的分类分数。
- en: And they will also have a loss function such as the softmax, or SVM on the last
    layer. Moreover, all the techniques that we learned ti develop neural networks
    will be applicable.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 它们也会有像softmax或SVM这样的损失函数在最后一层。此外，我们所学的开发神经网络的所有技术都将适用。
- en: But then what's different with ConvNets you may ask. So the main point to note
    is that the ConvNet architecture explicitly assumes that the inputs that are received
    are all images, this assumption actually helps us to encode other properties of
    the architecture itself. Doing so permits the network to be more efficient from
    an implementation perspective, vastly reducing the number of parameters required
    in the network. We call a network *convolutional* because of the convolutional
    layers it has, in addition to other types of layers. Soon, we will explore how
    these special layers, along with a few other mathematical operations, can help
    computers visually comprehend the world around us.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但你可能会问，卷积神经网络（ConvNets）有什么不同？所以需要注意的主要一点是，卷积网络架构明确假设接收到的输入都是图像，这一假设实际上帮助我们编码架构自身的其他属性。这样做可以使网络在实现上更高效，大大减少所需参数的数量。我们称一个网络为*卷积*网络，是因为它有卷积层，除此之外还有其他类型的层。很快，我们将探索这些特殊的层以及其他一些数学运算，如何帮助计算机更好地理解我们周围的世界。
- en: Hence, this specific architecture of neural networks excels at a variety of
    visual-processing tasks, ranging from object detection, face recognition, video
    classification, semantic segmentation, image captioning, human pose estimation,
    and many, many more. These networks allow an array of computer vision tasks to
    be performed effectively, some critical for the advancement of our species (such
    as medical diagnoses), and others bordering on entertainment (superimposing a
    certain artistic style on a given image). Before we dive deep into its conception
    and contemporary implementation, it is quite useful to understand the broader
    scope of what we are trying to replicate, by taking a quick tour of how vision,
    something so complex, yet so innate to us humans, actually came about.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这种特定的神经网络架构在各种视觉处理任务中表现出色，涵盖了物体检测、面部识别、视频分类、语义分割、图像描述、人类姿态估计等多个领域。这些网络使得一系列计算机视觉任务能够高效执行，其中一些任务对人类进步至关重要（如医学诊断），而另一些则接近娱乐领域（如将特定艺术风格叠加到图像上）。在我们深入探讨其构思和当代应用之前，了解我们试图复制的更广泛领域是非常有用的，可以通过快速了解视觉这一既复杂又与我们人类息息相关的事物是如何诞生的。
- en: The birth of vision
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉的诞生
- en: The following is an epic tale, an epic tale that took place nearly 540 million
    years ago.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是一个史诗般的故事，一个发生在大约5.4亿年前的史诗般的故事。
- en: Around this time, on the pale blue cosmic dot that would later become known
    as Earth, life was quite tranquil and hassle-free. Back then, almost all of our
    ancestors were water dwellers, who would just float about in the serenity of the
    oceans, munching on sources of food only if they were to float by them. Yes, this
    was quite different than the predatory, stressful, and stimulating world of today.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个时期，在后来被称为地球的淡蓝色宇宙点上，生命是相当宁静且无忧无虑的。那时，几乎所有的祖先都是水生生物，他们会在海洋的宁静中漂浮，只有当食物漂浮在他们面前时，才会吃上一口。是的，这与今天的掠食性、压力重重且充满刺激的世界是截然不同的。
- en: Suddenly, something quite curious occurred. In a comparatively short period
    of time that followed, there was an explosion in the number and variety of animal
    species present on our planet. In only a span of about 20 million years that came
    after, the kind of creatures you could find on our watery earth drastically changed.
    They changed from the occasional single-celled organisms you would encounter,
    organized in loosely connected colonies, to complex multi-cellular organisms,
    popping up in every creek and corner.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 突然，一件相当奇特的事情发生了。在随后的相对较短时间内，地球上动物物种的数量和种类发生了爆炸性增长。在接下来的大约2000万年里，你会发现地球上的生物发生了剧烈变化。它们从偶尔会遇到的单细胞生物，组成松散的群体，变成了复杂的多细胞生物，遍布地球的每个小溪和角落。
- en: Biologists remained baffled for a very long time, debating what caused this *big
    bang* of evolutionary acceleration. What we had actually discovered was the birth
    of the biological visual system. Studying the fossil records of the organisms
    from that time, zoologists were able to present decisive proof, connecting this
    explosion of species with the first appearance of photo-receptive cells. These
    cells allowed organisms to sense and respond to light, triggering an evolutionary
    arms race that eventually led to the sophisticated mammalian visual cortex that
    you are likely using right now to interpret this piece of text. Indeed, the gift
    of sight made life much more dynamic and proactive, as now organisms were able
    to sense and respond to their environments.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 生物学家曾长期困惑，争论是什么导致了这一*大爆炸*式的进化加速。我们真正发现的是生物视觉系统的诞生。通过研究当时生物的化石记录，动物学家能够提供确凿的证据，将这种物种爆发与首次出现的光感受细胞联系起来。这些细胞使得生物能够感知并对光作出反应，触发了一场进化的军备竞赛，最终导致了复杂的哺乳动物视觉皮层的形成，而你现在可能正是依赖这个视觉皮层来解读这段文字的。的确，视觉的恩赐使得生命变得更加动态和主动，因为生物现在能够感知并应对周围的环境。
- en: Today, vision is one of the main sensory systems in nearly all organisms, intelligent
    or not. In fact, we humans use almost half of our neuronal capacity for visual
    processing, making it the biggest sensory system we employ to orient ourselves,
    recognize people and objects, and go about our daily lives. As it turns out, vision
    is a very important component of cognitive systems, biological or otherwise. Hence,
    it is quite reasonable to go about examining the development and implementation
    of visual systems created by nature. After all, there's no point reinventing the
    wheel.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，视觉是几乎所有生物中主要的感官系统，无论其是否具有智能。事实上，我们人类几乎将一半的神经元容量用于视觉处理，这使得视觉成为我们用来定位自己、识别他人和物体、并进行日常活动的最大感官系统。事实证明，视觉是认知系统的一个非常重要的组成部分，无论是生物系统还是其他系统。因此，研究自然界创造的视觉系统的发展与实现是完全合理的。毕竟，重复造轮子没有意义。
- en: Understanding biological vision
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解生物学视觉
- en: 'Our next insight into biological visual systems comes from a series of experiments
    conducted by scientists from Harvard University, back in the late 1950s. Nobel
    laureates David Hubel and Torstein Wiesel showed the world the inner workings
    of the mammalian visual cortex, by mapping the action of receptor cells along
    with the visual pathway of a cat, from the retina to the visual cortex. These
    scientists used electrophysiology to understand exactly how our sensory organs
    intake, process, and interpret electromagnetic radiation, to generate the reality
    we see around us. This allowed them to better appreciate the flow of stimuli and
    related responses that occur at the level of individual neurons:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对生物视觉系统的下一项见解来源于20世纪50年代末，哈佛大学的科学家们进行的一系列实验。诺贝尔奖得主大卫·胡贝尔和托尔斯坦·维塞尔通过映射猫的视觉神经元，从视网膜到视觉皮层，向世界展示了哺乳动物视觉皮层的内部工作原理。这些科学家利用电生理学方法，了解我们的感官器官如何摄取、处理和解释电磁辐射，从而生成我们周围所见的现实。这使他们能够更好地理解在单个神经元水平上，刺激和相关反应的流动：
- en: '![](img/a057c3ee-63ef-4454-b8e4-1acbc968f2a9.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a057c3ee-63ef-4454-b8e4-1acbc968f2a9.png)'
- en: 'The following screenshot describes how cells respond to light:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图描述了细胞如何响应光：
- en: '![](img/55e51fb4-6c25-4605-aec1-3b30a1ba6fba.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55e51fb4-6c25-4605-aec1-3b30a1ba6fba.png)'
- en: Thanks to their experiments in the field of neuroscience, we are able to share
    with you several key elements of their research that directly affected the scientific
    understanding of visual signal processing, leading to a cascade of academic contributions
    that brings us to this date. These elements enlighten the mechanism of visual
    information processing leveraged by our own brain, inspiring the design of the
    CNN, a cornerstone of modern visual intelligence systems.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过他们在神经科学领域的实验，我们能够与您分享他们研究中的几个关键元素，这些元素直接影响了科学界对视觉信号处理的理解，并引发了一系列学术贡献，最终促成了今天的成就。这些元素启示了我们大脑在视觉信息处理中的机制，激发了卷积神经网络（CNN）的设计，而这成为现代视觉智能系统的基石。
- en: Conceptualizing spatial invariance
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概念化空间不变性
- en: The first of these notions comes from the concept of **spatial invariance**.
    The researchers noticed that the cat's neural activations to particular patterns
    would be consistent, regardless of the exact location of the patterns on the screen.
    Intuitively, the same set of neurons were noted to fire for a given pattern (that
    is, a line segment), even if the pattern appeared at the top or the bottom of
    the screen. This showed that the neurons' activations were spatially invariant,
    meaning that their activations were not dependent on the spatial location of the
    given patterns.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中的第一个概念来自于**空间不变性**。研究人员注意到，猫对特定图案的神经反应是一致的，无论这些图案在屏幕上的具体位置如何。从直觉上看，研究人员发现相同的一组神经元会对给定的图案（即一条线段）做出反应，即使该图案出现在屏幕的顶部或底部。这表明这些神经元的激活具有空间不变性，意味着它们的激活与给定图案的空间位置无关。
- en: Defining receptive fields of neurons
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义神经元的接受域
- en: Secondly, they also noted that neurons were *in charge* of responding to specific
    regions of a given input. They named this property of a neuron as its **receptive
    field**. In other words, certain neurons only responded to certain regions of
    a given input, whereas others responded to different regions of the same input.
    The receptive field of a neuron simply denotes the span of input to which a neuron
    is likely to respond.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，他们还注意到神经元*负责*响应给定输入的特定区域。他们将这种神经元的特性称为**感受野**。换句话说，某些神经元只对给定输入的特定区域做出响应，而其他神经元则对同一输入的不同区域作出反应。神经元的感受野简单来说就是指神经元可能响应的输入范围。
- en: Implementing a hierarchy of neurons
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现神经元层次结构
- en: 'Finally, the researchers were able to demonstrate that a hierarchy of neurons
    exists in the visual cortex. They showed that lower-level cells are tasked to
    detect simple visual patterns, such as line segments. The output from these neurons
    is used in subsequent layers of neurons to construct more and more complex patterns,
    forming the objects and people we see and interact with. Indeed, modern neuroscience
    confirms that the structure of the visual cortex is hierarchically organized to
    perform increasingly complex inferences using the output of previous layers, as
    illustrated:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，研究人员成功证明了视觉皮层中存在神经元的层次结构。他们展示了低级别的细胞负责检测简单的视觉模式，如线段。这些神经元的输出被后续神经元层用来构建越来越复杂的模式，最终形成我们看到并与之互动的物体和人物。实际上，现代神经科学证实，视觉皮层的结构是按层次组织的，通过使用前一层的输出，执行越来越复杂的推理，正如下图所示：
- en: '![](img/fb3d45e9-1025-4e42-a472-06eef7410b21.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb3d45e9-1025-4e42-a472-06eef7410b21.png)'
- en: The preceding diagram means that recognizing a friend involves detecting the
    line segments making up their face (**V1**), using those segments to build shapes
    and edges (**V2**), using those shapes and edges to form complex shapes such as
    eyes and noses (**V3**), and then leveraging previous knowledge to infer who out
    of your friends does this current bundle of eyes and nose resemble the most (**IT-posterior**).
    Based upon that reasoning, even higher-level activations pertaining to notions
    of that friend's personality, attractiveness, and so on, may emerge (**IT-anterior**).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图示意味着，识别一个朋友涉及检测组成他们面部的线段（**V1**），利用这些线段构建形状和边缘（**V2**），使用这些形状和边缘形成复杂的形状，如眼睛和鼻子（**V3**），然后运用先前的知识推断出这组眼睛和鼻子最像你哪位朋友（**IT-后部**）。基于这种推理，甚至可能出现更高层次的激活，涉及到关于该朋友的个性、吸引力等概念（**IT-前部**）。
- en: The birth of the modern CNN
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现代卷积神经网络的诞生
- en: 'It wasn''t until the 1980s that Heubel and Wiesel''s findings were repurposed
    in the field of computer science. The *Neurocognitron* (Fukushima, 1980: [https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf](https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf))
    leveraged the concept of simple and complex cells by sandwiching layers of one
    after the other. This ancestor of the modern neural network used the aforementioned
    alternating layers to sequentially include modifiable parameters (or simple cells),
    while using pooling layers (or complex cells) to make the network invariant to
    minor altercations from the simple cells. While intuitive, this architecture was
    still not powerful enough to capture the intricate complexities present in visual
    signals.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '直到1980年代，Heubel 和 Wiesel 的发现才在计算机科学领域得到了重新利用。*神经认知层次网*（Fukushima, 1980: [https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf](https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf)）通过将简单细胞和复杂细胞的概念交替堆叠在不同的层中。这个现代神经网络的前身利用前述的交替层，顺序地包含可修改的参数（或简单细胞），并通过池化层（或复杂细胞）使得网络对简单细胞的小变化具有不变性。尽管这一架构具有直观性，但它仍然不足以捕捉视觉信号中复杂的细节。'
- en: One of the major breakthroughs followed in 1998, when famed AI researchers,
    Yan Lecun and Yoshua Bengio, were able to train a CNN, leveraging gradient-based
    weight updates, to perform document recognition. This network did an excellent
    job of recognizing digits of zip codes. Similar networks were quickly adopted
    by organizations such as the US postal service, to automate the tedious task of
    sorting mail (nope, not the electronic kind). While the results were impressive
    enough for commercial interest in narrow segments, these networks were still not
    able to handle more challenging and complex data, such as faces, cars, and other
    real-world objects. However, the collective work of these researchers and many
    others led to the modern incarnation of the larger and deeper CNNs, first appearing
    at the ImageNet classification challenge. These networks have now come to dominate
    the realm of computer vision, making their appearance in today's most sophisticated
    artificial visual intelligence systems. These networks are now used for tasks
    such as medical image diagnosis, detecting celestial objects in outer space, and
    making computers play old-school Atari games, to name a few.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个重要的突破发生在1998年，当时著名的AI研究人员**颜麟恩**和**约书亚·本吉奥**成功训练了一个卷积神经网络（CNN），利用基于梯度的权重更新，来执行文档识别任务。这个网络在识别邮政编码的数字方面表现得非常出色。类似的网络很快被美国邮政服务等组织采用，用于自动化处理繁琐的邮件分类工作（不是电子邮件哦）。尽管这些成果足够引起商业领域的兴趣，并且在狭窄的领域内产生了显著影响，但这些网络仍然无法处理更具挑战性和复杂的数据，比如面孔、汽车和其他真实世界的物体。然而，这些研究人员和许多其他人的集体努力，促成了现代较大且更深的卷积神经网络的出现，这些网络首次出现在ImageNet分类挑战赛中。如今，这些网络已经在计算机视觉领域占据主导地位，并广泛应用于当今最先进的人工视觉智能系统中。这些网络现在被用于诸如医学影像诊断、探测外太空天体、以及让计算机玩老式Atari游戏等任务，举几个例子。
- en: Designing a CNN
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计卷积神经网络（CNN）
- en: Now, armed with the intuition of biological vision, we understand how neurons
    must be organized hierarchically, to detect simple patterns and use these to progressively
    build more complex patterns corresponding to real-world objects. We also know
    that we must implement a mechanism for spatial invariance to allow neurons to
    deal with similar inputs occurring at different spatial locations of a given image.
    Finally, we are aware that implementing a receptive field for each neuron is useful
    to achieve a topographical mapping of neurons to spatial locations in the real
    world, so that nearby neurons may represent nearby regions in the field of vision.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，基于对生物视觉的直觉，我们理解了神经元必须层次化组织，以便检测简单的模式，并利用这些模式逐步构建更复杂的模式，以对应真实世界中的物体。我们还知道，必须实现一个空间不变性机制，允许神经元处理在给定图像的不同空间位置上出现的相似输入。最后，我们意识到，为每个神经元实现一个感受野，对于实现神经元与现实世界中空间位置的拓扑映射非常有用，这样附近的神经元就可以表示视觉场中的相邻区域。
- en: Dense versus convolutional layer
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稠密层与卷积层
- en: You will recall from the previous chapter that we used a feedforward neural
    network, composed of fully connected dense layers of neurons, to perform the task
    of handwritten digit recognition. While constructing our network, we were forced
    to flatten our input pixels of 28 x 28 into a vector of 784 pixels, for each image.
    Doing so caused us to lose any spatially relevant information that our network
    could leverage for classifying the digits it was shown. We simply showed it a
    784-dimensional vector for each image in our dataset and expected it to recognize
    digits thereafter. While this approach was sufficient to attain a considerable
    accuracy in classifying simple handwritten digits from the nice and clean MNIST
    dataset, it quickly becomes impractical in more complex data that we may want
    to deal with, involving a multitude of local patterns of different spatial orientations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你会记得在上一章中，我们使用了一个前馈神经网络，它由全连接的稠密神经元层组成，用于执行手写数字识别任务。在构建我们的网络时，我们不得不将每个图像的28
    x 28的输入像素展平为一个784个像素的向量。这样做导致我们失去了网络可以利用的任何空间相关信息，来帮助分类它所看到的数字。我们仅仅将每个图像展平成一个784维的向量，并期待它能够识别出数字。虽然这种方法足以在MNIST数据集中进行手写数字分类，并取得了相当不错的准确度，但在面对更复杂的数据时，这种方法很快就变得不实用了，尤其是那些涉及多种不同空间方向局部模式的数据。
- en: We would ideally like to preserve spatial information and reuse neurons for
    the detection of similar patterns occurring in different spatial regions. This
    would allow our convolutional networks to be more efficient. Through reusing its
    neurons to recognize specific patterns in our data irrespective of their location,
    CNNs are advantageous to use in visual tasks. A densely connected network, on
    the other hand, would be forced to learn a pattern again, if it were to appear
    in another location of the image. Given the natural spatial hierarchies that exist
    in visual data, using convolution layers is an ideal way to detect minute local
    patterns and use them to progressively build more complex ones.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们理想的目标是保留空间信息，并重用神经元来检测不同空间区域中出现的相似模式。这将使我们的卷积网络更加高效。通过重用神经元来识别数据中具体模式，而不论它们的位置，CNN在视觉任务中具有优势。另一方面，如果密集连接的网络遇到相同的模式出现在图像的另一个位置，它将被迫再次学习该模式。考虑到视觉数据中存在的自然空间层次结构，使用卷积层是一种理想的方式，能够检测微小的局部模式，并逐步构建出更复杂的模式。
- en: 'The following screenshot illustrates the hierarchical nature of visual data:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了视觉数据的层次性：
- en: '![](img/896eb13d-29a1-4df1-baa0-657440c08e48.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/896eb13d-29a1-4df1-baa0-657440c08e48.png)'
- en: Another problem we mentioned with dense layers is to do with their weakness
    in capturing local patterns from data. The dense layer is well known for capturing
    global patterns involving all pixels in images. As the findings of Hubel and Wiesel
    suggest, however, we want to limit the receptive field of our neurons to local
    patterns present in the data and use those patterns to progressively form more
    complex ones. This would allow our network to deal with very different forms of
    visual data, each with different types of local patterns enclosed. To overcome
    these problems, among others, the core component of CNN was developed, known as
    the **convolution operation**.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到的另一个密集层的问题是，它在捕捉数据中的局部模式方面存在弱点。密集层因能够捕捉涉及图像中所有像素的全局模式而广为人知。然而，正如Hubel和Wiesel的研究所示，我们希望将神经元的感受野限制为数据中存在的局部模式，并利用这些模式逐步形成更复杂的模式。这将使我们的网络能够处理非常不同类型的视觉数据，每种数据都有不同类型的局部模式。为了解决这些问题等，卷积神经网络的核心组件——**卷积操作**被开发出来。
- en: The convolution operation
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积操作
- en: 'The word *convolvere* comes from Latin and translates to *convolve* or *roll
    together*. From a mathematical perspective, you may define a convolution as the
    calculus-based integral denoting the amount by which two given functions overlap,
    as one of the two is slid across the other. In other words, performing the convolution
    operation on two functions (*f* and *g*) will produce a third function that expresses
    how the shape of one is modified by the other. The term *convolution* refers to
    both the result function and the computation process, with roots in the mathematical
    subfield of signal processing, as we can here in this diagram:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 词语*convolvere*来自拉丁语，意思是*卷积*或*一起滚动*。从数学角度来看，你可以将卷积定义为基于微积分的积分，表示当一个函数在另一个函数上滑动时，它们重叠的程度。换句话说，对两个函数（*f*和*g*）进行卷积操作将产生第三个函数，表示一个函数的形状如何被另一个函数所改变。术语*卷积*既指结果函数，也指计算过程，其根源在于信号处理的数学分支，如我们在此图中所见：
- en: '![](img/98d0d59a-e7b7-454c-9bba-6fe684609c99.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98d0d59a-e7b7-454c-9bba-6fe684609c99.png)'
- en: So, how can we leverage this operation to our advantage?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何利用这个操作来获得优势呢？
- en: Preserving the spatial structure of an image
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保留图像的空间结构
- en: 'Firstly, we will use the inherent spatial structure of an image by simply feeding
    it as an *n*-dimensional tensor into our neural network. This means that the network
    will receive each pixel in its original matrix position, and not reduced to a
    position inside a single vector, as we did before. In the case of the MNIST example,
    a convolutional network would receive as input a 28 x 28 x 1 tensor, representing
    each image. Here, 28 x 28 represents a two-dimensional grid upon which the pixels
    are arranged to form the handwritten digits, whereas `1` at the end denotes the
    color channels of the image (that is, the number of pixel values per pixel, in
    a given image). As we know, a colored dataset may have image dimensions such as
    28 x 28 x 3, where `3` denotes the different red, green, and blue values that
    form the color of an individual pixel. Since we only deal with single grayscale
    values per pixel in the MNIST dataset, the color channel is denoted as 1\. Hence,
    it is important to understand that an image enters our network as a three-dimensional
    tensor, preserving the spatial structure of our image data:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将通过简单地将图像作为*n*维张量输入到神经网络中，来利用图像固有的空间结构。这意味着网络将接收每个像素的原始矩阵位置，而不是像之前那样将其缩减为单一向量中的一个位置。以MNIST示例为例，卷积神经网络将接收一个28
    x 28 x 1的张量作为输入，表示每一张图像。这里，28 x 28表示一个二维网格，像素在其上排列形成手写数字，而`1`表示图像的颜色通道（即每个像素的像素值数量）。如我们所知，彩色数据集的图像尺寸可能是28
    x 28 x 3，其中`3`表示构成单个像素颜色的红、绿、蓝三种值。由于我们在MNIST数据集中每个像素只处理单一的灰度值，颜色通道表示为1。因此，理解图像作为三维张量进入网络并保留图像数据的空间结构是非常重要的：
- en: '![](img/e0c88e93-5c70-4ca0-a02c-cc73dbbce452.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0c88e93-5c70-4ca0-a02c-cc73dbbce452.png)'
- en: Receptive field
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接受域
- en: Now we can leverage the properties of additional spatial information, inherent
    to the image, in the architecture of our network. In other words, we are now ready
    to perform a convolution operation. This simply means that we will use a smaller
    segment of space and slide it over our input image as a filter to detect local
    patterns. The intuition here is to connect patches of our input data space to
    corresponding neurons in the hidden layers. Doing so allows the neuron to only
    observe a local area of the image at each convolution, thereby limiting its receptive
    field. Limiting a neuron's receptive field is useful for two main reasons. Since
    we reason that nearby pixels are more likely to be related to each other in an
    image, limiting the receptive field of neurons in our network makes these neurons
    better able to distinguish local variances between pixels in a given image. Moreover,
    this practice also allows us to drastically reduce the number of learnable parameters
    (or weights) in our network. In this manner, we use a **filter**, which is essentially
    a matrix of weights and apply it iteratively over the input space starting from
    the top-left corner of an image. We move a stride to the right at each convolution,
    by centering our filter on top of each pixel, in our input space. Once having
    convolved over the image segment corresponding to the top rows of pixels, we repeat
    the operation from the left side of the image once again, this time, for the rows
    underneath. This is how we slide the filter across our entire input image, extracting
    local features at each convolution by computing the dot products of our input
    area and the so-called **filters**.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在网络架构中利用图像固有的附加空间信息。换句话说，我们现在已经准备好执行卷积操作。这意味着我们将使用一个较小的空间片段，并将其滑动到输入图像上，作为滤波器来检测局部模式。直观的理解是将输入数据空间的某些区域与隐藏层中对应的神经元连接起来。这样做可以让神经元每次卷积时仅观察图像的局部区域，从而限制其接受域。限制神经元的接受域有两个主要的好处。首先，我们认为图像中相邻的像素更有可能相互关联，因此在网络中限制神经元的接受域使得这些神经元更能够区分图像中像素之间的局部差异。此外，这种做法还允许我们大幅减少网络中可学习的参数（或权重）数量。通过这种方式，我们使用一个**滤波器**，本质上是一个权重矩阵，并在输入空间上从图像的左上角开始迭代应用它。每次卷积时，我们会将滤波器集中在输入空间中每个像素的上方，并向右移动步幅。完成图像顶部像素行的卷积后，我们会从图像的左侧再次执行相同的操作，这次是针对下面的行。通过这种方式，我们将滤波器滑过整个输入图像，在每次卷积时通过计算输入区域和所谓的**滤波器**的点积来提取局部特征。
- en: 'The diagram here depicts the initial step of a convolution operation. Progressively,
    the three-dimensional blue rectangle shown here will be slid across segments (in
    red) of the entire image, giving the convolution operation its name. The rectangle
    itself, is known as a filter, or a **convolutional kernel**:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的图示描绘了卷积操作的初始步骤。随着操作的进行，这个三维的蓝色矩形将会在整个图像的片段（用红色标示）上滑动，从而使得该操作得名为卷积。矩形本身被称为滤波器，或**卷积核**：
- en: '![](img/89364f65-c2de-43ec-ba69-99f8f3a67982.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89364f65-c2de-43ec-ba69-99f8f3a67982.png)'
- en: Feature extraction using filters
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用滤波器进行特征提取
- en: Each filter can be essentially thought of as an arrangement of neurons, similar
    in spirit to the ones we encountered in [Chapter 3](46e25614-bb5a-4cca-ac3e-b6dfbe29eea5.xhtml), *Signal
    Processing - Data Analysis with Neural Networks*. Here, the filter's neurons are
    initialized with random weights and progressively updated using the backpropagation
    algorithm during training. The filter itself is in charge of detecting a particular
    type of pattern, ranging from line segments to curves and more complex shapes.
    As a filter passes over an area of the input image, the filter weights are multiplied
    (element-wise) with the pixel values at that location, generating a single output
    vector. Then, we simply reduce this vector to a scalar value using a summation
    operation, exactly as we did for our feedforward neural network previously. These
    scalar values are generated at each new position the filter is moved to, spanning
    the entire input image. The values are stored in something referred to as the
    **activation map** (also known as a **feature map** or a **response map**) for
    a given filter. The activation map itself is conventionally smaller in dimension
    than the input image, and it embodies a new representation of the input image,
    while accentuating certain patterns within.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 每个滤波器本质上可以看作是一个神经元的排列，类似于我们在[第3章](46e25614-bb5a-4cca-ac3e-b6dfbe29eea5.xhtml)《信号处理
    - 使用神经网络进行数据分析》中遇到的那种。在这里，滤波器的神经元用随机权重初始化，并在训练过程中通过反向传播算法逐步更新。滤波器本身负责检测某种特定类型的模式，从线段到曲线以及更复杂的形状。当滤波器在输入图像的某一位置滑动时，滤波器的权重与该位置的像素值进行元素级相乘，从而生成一个单一的输出向量。然后，我们通过求和操作将该向量简化为一个标量值，正如我们之前在前馈神经网络中所做的那样。这些标量值在滤波器移动到每一个新位置时生成，遍历整个输入图像。它们被存储在一个被称为**激活图**（也称为**特征图**或**响应图**）的东西中，针对给定的滤波器。激活图本身的尺寸通常小于输入图像，并且它体现了输入图像的新表示，同时突出了其中的某些模式。
- en: 'The weights of the filters themselves can be thought of as the learnable parameters
    of a convolutional layer and are updated to capture patterns relevant to the task
    at hand, while the network trains:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 滤波器本身的权重可以被看作是卷积层的可学习参数，并在网络训练过程中不断更新，以捕捉与当前任务相关的模式：
- en: '![](img/3fd980af-7e27-4b63-9338-0dbbad1f50ef.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3fd980af-7e27-4b63-9338-0dbbad1f50ef.png)'
- en: Backpropagation of errors in CNNs
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络中的误差反向传播
- en: As our network trains to find ideal filter weights, we hope that the activation
    map of a given filter is able to capture the most informative visual patterns
    that may exist in our data. Essentially, these activation maps are generated using
    matrix-wise multiplication. These activation maps are fed into the subsequent
    layer as inputs, and thus information propagates forward, up until the final layers
    of our model, which perform the classification. At this point, our `loss` function
    assesses the difference in the network's predictions versus the actual output,
    and backpropagates the prediction errors to adjust the networks weights, for each
    layer.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的网络训练以找到理想的滤波器权重时，我们希望某个滤波器的激活图能够捕捉到数据中可能存在的最有信息量的视觉模式。本质上，这些激活图是通过矩阵乘法生成的。这些激活图作为输入被送入后续层，因此信息会向前传播，直到模型的最终层，最终执行分类。在此时，我们的`loss`函数评估网络预测与实际输出之间的差异，并将预测误差反向传播以调整每一层的网络权重。
- en: 'This is basically how a ConvNet is trained, on a very high level. The convolution
    operation consists of iteratively computing the dot product of a transposed filter
    matrix (holding the weights of our convolutional filter) with the corresponding
    input space of pixels to extract generalizable features from the training examples.
    These feature maps (or activation maps) are then first fed into pooling layers
    to reduce their dimensionality, and subsequently fed into fully connected layers
    to determine which combination of filters best represent a given output class.
    As the model weights are updated during a backward pass, new activation maps are
    generated at the next forward pass, which ideally encode more representative features
    of our data. Here, we present a brief visual summary of the ConvNet architecture:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上就是如何训练卷积神经网络（ConvNet）的高层次过程。卷积操作包括通过转置的滤波器矩阵（持有卷积滤波器的权重）与对应输入像素空间的点积运算，来从训练样本中提取可泛化的特征。然后，这些特征图（或激活图）首先被送入池化层以降低维度，随后再送入全连接层以确定哪种滤波器组合最能代表给定的输出类别。在反向传播过程中，随着模型权重的更新，在下一次前向传播时会生成新的激活图，这些激活图理想地编码了我们数据中更具代表性的特征。在这里，我们简要展示了ConvNet架构的视觉总结：
- en: '![](img/b288bdad-fdfc-45d1-a7a5-be63bb8a9a9f.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b288bdad-fdfc-45d1-a7a5-be63bb8a9a9f.png)'
- en: 'It follows the following steps:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 它遵循以下步骤：
- en: Learns features in input image through convolution
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过卷积学习输入图像中的特征
- en: Introduce non-linearity through activation function(real-world data is non-linear)
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过激活函数引入非线性（现实世界数据是非线性的）
- en: Reduce dimensionality and preserve spatial invariance with pooling
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过池化减少维度并保持空间不变性
- en: Using multiple filters
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多个滤波器
- en: Since filters are pattern-specific (that is, each filter excels at picking up
    a certain type of pattern), we require more than just one filter to pick up all
    the different types of patterns that may exist in our image. This means that we
    may use multiple filters for a given convolutional layer in our network, allowing
    us to extract multiple distinct local features for a given region of input space.
    These local features, stored in the activation map of that specific filter, may
    be passed on to subsequent layers to build more complex patterns. Progressive
    convolutional layers will use different filters to convolve over the input activation
    map from the previous layer, again extracting and transforming the input map into
    a three-dimensional tensor output corresponding to the activation maps for each
    filter used. The new activation maps will again be three-dimensional tensors and
    can be similarly passed on the subsequent layers. Recall that when an image enters
    our network, it does so as a three-dimensional tensor with (width, height, and
    depth) corresponding to the input image dimensions (where depth is denoted by
    the pixel channels). In our output tensor, on the other hand, the depth axis denotes
    the number of filters used in the previous convolutional layer, where each filter
    produces its own activation map. In essence, this is how data propagates forward
    in a CNN, entering as an image and exiting as a three-dimensional activation map,
    being progressively transformed by various filters as the data propagates through
    deeper layers. The exact nature of transformations will become clearer soon, when
    we will build a ConvNet. We'll discuss theory a little bit more, and then we will
    be ready to proceed.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于滤波器是特定模式的（即每个滤波器擅长捕捉某种类型的模式），我们需要不止一个滤波器来捕捉图像中可能存在的所有不同类型的模式。这意味着我们可能会在网络中的某一卷积层使用多个滤波器，从而允许我们为给定输入空间的区域提取多个不同的局部特征。这些局部特征存储在特定滤波器的激活图中，并可以传递到后续层以构建更复杂的模式。渐进的卷积层将使用不同的滤波器对前一层的输入激活图进行卷积，再次提取并将输入图转化为一个三维张量输出，代表每个使用的滤波器的激活图。新的激活图将再次是三维张量，并可以类似地传递到后续层。回想一下，当图像进入我们的网络时，它作为一个三维张量进入，具有（宽度、高度和深度）对应输入图像的维度（其中深度由像素通道表示）。而在我们的输出张量中，深度轴表示前一卷积层中使用的滤波器数量，每个滤波器产生自己的激活图。从本质上讲，这就是数据如何在卷积神经网络（CNN）中向前传播的过程，它以图像的形式进入，最终以三维激活图的形式输出，并在数据通过更深层时被各个滤波器逐步转化。转换的具体性质将在稍后更清晰地展示，当我们构建卷积神经网络时。我们将再多讨论一点理论，然后我们就准备好继续了。
- en: Stride of a convolution
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积的步幅
- en: 'The following diagram depicts the convolution operation that you have now familiarized
    yourself with, in two dimensions (for simplicity). It shows a 4 x 4 filter (red
    box) being slid across a larger image (14 x 14), taking a step of two pixels at
    a time:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了你现在已经熟悉的卷积操作，在二维中（为了简化）。它展示了一个4x4的滤波器（红框）滑过一个更大的图像（14x14），每次移动两个像素：
- en: '![](img/45490089-e87f-47df-8725-450ecaefce34.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/45490089-e87f-47df-8725-450ecaefce34.png)'
- en: The number of pixels by which a filter shifts at each iteration is known as
    its **stride**. At each stride, a dot product is computed using the pixel matrix
    from the corresponding input region, as well as the filter weights. These dot
    products are stored as a scalar value in the activation map for that filter (shown
    as a 6 x 6 square as follows). Hence, the activation map denotes a reduced representation
    of the layer inputs, and essentially is a matrix composed of the summed-up dot
    products we get by convolving our filter over segments of the input data. On a
    higher level, the activation map represents the activation of neurons for specific
    patterns that the respective filter is detecting. Longer strides of these filters
    will lead to less detailed sampling of the corresponding input regions, whereas
    shorter strides will allow individual pixels to be sampled more often, permitting
    a higher definition activation map.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 滤波器在每次迭代时移动的像素数称为**步幅（stride）**。在每次步幅操作中，使用对应输入区域的像素矩阵和滤波器权重计算一个点积。这个点积作为标量值存储在该滤波器的激活图中（如下所示的6x6方形图）。因此，激活图表示了该层输入的简化表示，本质上是一个矩阵，由我们在对输入数据的各个片段进行卷积时得到的点积之和组成。从更高的层次来看，激活图表示了神经元对特定模式的激活，该模式由相应的滤波器检测到。较长的步幅会导致对应输入区域的采样更少，而较短的步幅则允许更多的像素被采样，从而产生更高分辨率的激活图。
- en: What are features?
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是特征？
- en: While the overall mechanism of collecting features with different types of filters
    may be clear, you may well be wondering what a feature actually looks like and
    how we go about extracting them from a filter.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用不同类型的滤波器收集特征的整体机制可能很清楚，但你可能会想知道一个特征到底是什么样子，我们如何从滤波器中提取它们。
- en: 'Let''s consider a simple example to clarify our understanding. Suppose you
    wished to detect the letter X from a bunch of grayscale images of letters. How
    would a CNN go about doing this? Well, let''s first consider the image of an X.
    As shown as follows, we can think of the pixels with positive values form the
    lines of an X, whereas pixels with a negative value simply represent blank space
    in our image, as shown in this diagram:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的例子来澄清我们的理解。假设你想从一堆灰度字母图像中检测字母X。那么，卷积神经网络（CNN）是如何进行这项工作的呢？首先，让我们考虑一个X的图像。如下面所示，我们可以认为具有正值的像素形成了X的线条，而负值的像素则表示图像中的空白区域，如下图所示：
- en: '![](img/63093442-c5bf-41be-87b7-e09edd13968c.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/63093442-c5bf-41be-87b7-e09edd13968c.png)'
- en: 'But then comes the problem of positional variance: What if the X appears slightly
    rotated or distorted in any other way? In the real world, the letter X comes in
    many sizes, shapes, and so on. How can we break up the image of an X using smaller
    filters that capture its underlying patterns? Well, here''s one way:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 但接下来是位置变化的问题：如果X稍微旋转或以其他方式扭曲了怎么办？在现实世界中，字母X有许多不同的大小、形状等等。我们如何使用较小的滤波器来分解X的图像，从而捕捉到它的基本模式呢？这里有一种方法：
- en: '![](img/ba63b741-18d9-40a1-ae8c-6d7bd9d98f13.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba63b741-18d9-40a1-ae8c-6d7bd9d98f13.png)'
- en: As you may have noticed, we can actually break down the image into smaller segments,
    where each segment (denoted by the green, orange, and purple boxes) represents
    recurring patterns within the image. In our case, we are able to use two diagonally
    oriented filters and one intersecting filter and slide each of them across our
    image to pick up on the line segments that form an X. Essentially, you may think
    of each filter as a pattern detector of sorts. As these different filters convolve
    over our input image, we are left with activation maps that start to resemble
    long horizontal lines and crosses, which the network learns to combine to recognize
    the letter X.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经注意到的那样，我们实际上可以将图像分解成更小的片段，其中每个片段（由绿色、橙色和紫色框表示）代表图像中的重复模式。在我们的例子中，我们能够使用两个对角线方向的滤波器和一个交叉滤波器，并将它们逐个滑过图像，以捕捉形成X的线段。本质上，你可以将每个滤波器视为某种模式检测器。当这些不同的滤波器对输入图像进行卷积时，我们得到的激活图开始像长的横线和交叉线一样，这些图像组合后帮助网络识别字母X。
- en: Visualizing feature extraction with filters
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用滤波器可视化特征提取
- en: 'Let''s consider another example, to solidify our understanding of how filters
    detect patterns. Consider this depiction of the number 7, taken from the MNIST
    dataset. We use this 28 x 28 pixelated image to show how filters actually pick
    up on different patterns:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑另一个例子，以巩固我们对滤波器如何检测模式的理解。考虑这张来自MNIST数据集的数字7的图像。我们使用这张28 x 28像素的图像来展示滤波器如何实际捕捉到不同的模式：
- en: '![](img/e3666371-2cdb-4ae5-975f-f3e73b6e5a56.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3666371-2cdb-4ae5-975f-f3e73b6e5a56.png)'
- en: 'Intuitively, we notice that this 7 is composed of two horizontal lines, as
    well as a slanted vertical line. We essentially need to initialize our filters
    with values that can pick up on these separate patterns. Next, we observe some
    3 x 3 filter matrices that a ConvNet would typically learn for the task at hand:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，我们注意到这个数字7由两条水平线和一条倾斜的垂直线组成。我们基本上需要用可以捕捉到这些不同模式的值来初始化我们的滤波器。接下来，我们观察一些3
    x 3的滤波器矩阵，这是卷积神经网络通常会为当前任务学习到的：
- en: '![](img/08cb92c2-c8cb-466c-a60f-edfc86715ef0.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08cb92c2-c8cb-466c-a60f-edfc86715ef0.png)'
- en: 'While not very intuitive to visualize, these filters are actually sophisticated
    edge detectors. To see how they work, let''s picture each 0 in our filter weights
    as the color grey, whereas each value of 1 takes the color white, leaving -1 with
    the color black. As these filters convolve over an input image, element-wise multiplications
    are performed using the filter values and the pixels underneath. The product of
    this operation is yet another matrix, known as an **activation map**, representing
    particular features picked up by their respective filters, for a given input image.
    Now let''s see the effects of executing a convolution operation over the input
    image of a 7, using each of these four filters, to exactly understand what kind
    of patterns they individually pick up on. The following illustration plots out
    the activation map for each filter in the convolutional layer, once it is shown
    the image of a 7:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管不太直观，但这些滤波器实际上是复杂的边缘检测器。为了理解它们如何工作，我们可以将滤波器权重中的每个0视为灰色，而每个1则为白色，-1则为黑色。当这些滤波器在输入图像上进行卷积时，会对滤波器值和下面的像素进行逐元素相乘。这个操作的结果是另一个矩阵，称为**激活图**，表示给定输入图像中由各自滤波器捕捉到的特定特征。现在，让我们看看在对数字7的输入图像执行卷积操作时，使用这四个滤波器分别捕捉到什么样的模式。以下插图展示了卷积层中每个滤波器的激活图，展示了它们在看到数字7的图像后的效果：
- en: '![](img/53e3fdc9-124c-4381-8cf4-cb75e93adafc.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/53e3fdc9-124c-4381-8cf4-cb75e93adafc.png)'
- en: We observe that each filter is able to pick up on a specific pattern in the
    input image by simply computing the dot product summation of pixel values and
    filter weights at each spatial location. The patterns picked up can be denoted
    by the white regions in the aforementioned activation maps. We see that the first
    two filters adeptly pick up on upper and lower horizontal lines in the image of
    the 7, respectively. We also notice how the latter two filters pick up on the
    inner and outer vertical lines that form the body of the digit 7, respectively.
    While these are still examples of fairly simple pattern detections, progressive
    layers in ConvNets tend to be able to pick up on much more informative structures,
    by differentiating colors and shapes, which are essentially patterns of numbers,
    to our network.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，每个滤波器能够通过简单地计算输入图像中每个空间位置的像素值与滤波器权重的点积和，捕捉到图像中的特定模式。所捕捉到的模式可以通过前述的激活图中的白色区域表示。我们看到前两个滤波器分别巧妙地捕捉到了数字7图像中的上下水平线。我们还注意到后两个滤波器分别捕捉到了构成数字7主体的内外垂直线。虽然这些仍然是相当简单的模式检测示例，但卷积神经网络中的渐进层通常能够捕捉到更加丰富的结构，通过区分颜色和形状，这些本质上是数字模式，对我们的网络来说。
- en: Looking at complex filters
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查看复杂的滤波器
- en: 'The following image shows the top nine activation maps per grid, associated
    with specific inputs, for the second layer of a ConvNet. On the left, you can
    think of the mini-grids as activations of individual neurons, for given inputs.
    The corresponding colored grids on the right relate to the inputs these neurons
    were shown. What we are visualizing here is the kind of input that *maximizes*
    the activation of these neurons. We notice that already some pretty-clear circle
    detector neurons are visible (grid 2, 2), being activated for inputs such as the
    top of lamp shades and animal eyes:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了在 ConvNet 第二层中，按网格展示的每个网格对应的九个激活图，分别与特定输入相关联。在左侧，你可以将小网格看作是个别神经元的激活，针对给定输入。右侧的彩色网格则与这些神经元显示的输入相关。我们在这里可视化的是那些能够*最大化*这些神经元激活的输入。我们注意到，已经可以看到一些非常明显的圆形检测神经元（网格
    2, 2），这些神经元会对例如灯罩顶部和动物眼睛等输入激活：
- en: '![](img/2bcd19b0-5d97-436d-8491-fffc1f3d477e.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2bcd19b0-5d97-436d-8491-fffc1f3d477e.png)'
- en: 'Similarly, we notice some square-like pattern detectors (grid 4, 4) that seem
    to activate for images containing door and window frames. As we progressively
    visualize activation maps for deeper layers in CNNs, we observe even more complex
    geometric patterns being picked up, representing faces of dogs (grid 1, 1), bird
    legs (grid 2, 4), and so on:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们注意到一些类似正方形的模式检测器（网格 4, 4），似乎会对包含门窗框架的图像激活。当我们逐步可视化 CNN 更深层次的激活图时，我们会看到更加复杂的几何图案被捕捉到，代表了狗的面部（网格
    1, 1）、鸟的腿（网格 2, 4）等等：
- en: '![](img/99378b16-e96a-4e0f-8614-46d48a6d3f34.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99378b16-e96a-4e0f-8614-46d48a6d3f34.png)'
- en: Summarizing the convolution operation
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结卷积操作
- en: All that we are doing here is applying a set of weights (that is, a filter)
    to local input spaces for feature extraction. We do this iteratively, moving our
    filter across the input space in fixed steps, known as a **stride**. Moreover,
    the use of different filters allows us to capture different patterns from a given
    input. Finally, since the filters convolve over the entire image, we are able
    to spatially share parameters for a given filter. This allows us to use the *same*
    filter to detect similar patterns in different locations of the image, relating
    to the concept of spatial invariance discussed earlier. However, these activation
    maps that a convolutional layer outputs are essentially abstract high-dimensional
    representations. We need to implement a mechanism to reduce these representations
    into more manageable dimensions, before we go ahead and perform classification.
    This brings us to the **pooling layers**.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里所做的就是将一组权重（即一个滤波器）应用于局部输入空间进行特征提取。我们是通过迭代的方式进行的，在固定的步长（称为**步幅**）中将滤波器移动到输入空间。此外，使用不同的滤波器使我们能够从给定输入中捕获不同的模式。最后，由于滤波器会对整个图像进行卷积操作，我们可以对给定的滤波器实现空间共享参数。这使得我们能够使用*相同的*滤波器在图像的不同位置检测到相似的模式，这与之前讨论的空间不变性概念相关。然而，卷积层输出的这些激活图本质上是抽象的高维表示。在进行分类之前，我们需要实现一种机制，将这些表示减少到更易处理的维度。这引出了**池化层**的概念。
- en: Understanding pooling layers
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解池化层
- en: 'A final consideration when using convolutional layers is to do with the idea
    of stacking simple cells to detect local patterns and complex cells to downsample
    representations, as we saw earlier with the cat-brain experiments, and the neocognitron.
    The convolutional filters we saw behave like simple cells by focusing on specific
    locations on the input and training neurons to fire, given some stimuli from the
    local regions of our input image. Complex cells, on the other hand, are required
    to be less specific to the location of the stimuli. This is where the pooling
    layer comes in. This technique of pooling intends to reduce the output of CNN
    layers to more manageable representations. Pooling layers are periodically added
    between convolutional layers to spatially downsample the outputs of our convolutional
    layer. All this does is progressively reduce the size of the convolutional layer
    outputs, thereby leading to more efficient representations, shown as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用卷积层时的最后一个考虑因素涉及堆叠简单单元以检测局部模式和堆叠复杂单元以下采样表示的概念，正如我们在猫脑实验和neocognitron中看到的那样。我们看到的卷积滤波器表现得像简单单元，通过集中关注输入的特定位置并训练神经元在接受到来自输入图像局部区域的某些刺激时激活。而复杂单元则需要对刺激的位置不那么具体。这时，池化层就发挥了作用。池化技术旨在将卷积神经网络层的输出减少到更易管理的表示。池化层定期添加到卷积层之间，按空间下采样卷积层的输出。这样做的效果就是逐渐减少卷积层输出的大小，从而得到更高效的表示，具体如图所示：
- en: '![](img/9e3eb8a8-0a3a-4565-b004-318a4fba670b.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e3eb8a8-0a3a-4565-b004-318a4fba670b.png)'
- en: As you can see, the depth of volume is preserved because we have pooled the
    input volume of size 224 x 224 x 64 with a filter of size 2 and a stride of 2\.
    And this gives an output volume of 112 x 112 x 64.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，深度体积被保留，因为我们使用2大小的滤波器和步幅为2对大小为224 x 224 x 64的输入体积进行了池化。因此，输出体积为112 x 112
    x 64。
- en: Types of pooling operations
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化操作类型
- en: 'This similarly reduces the number of learnable parameters required in our network
    for the same task and prevents our network from overfitting. Finally, pooling
    is performed for every activation map generated by a given layer (that is, every
    slice of depth of the input tensor) and resizes its input spatially using its
    own filters. It is very common to see pooling layers with 2 x 2 filters along
    with a stride of 2, performed on every depth slice. There are many ways to perform
    this sort of downsampling. Most commonly, this is achieved through a **max pooling**
    operation, which simply means that we preserve the pixel with the highest value
    out of all the pixels in the input region under our pooling filter. The following
    diagram illustrates this max pooling operation on a given slice of our input tensor:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这同样减少了我们网络中所需的可学习参数数量，以执行相同的任务，并防止网络过拟合。最后，对于由给定层生成的每个激活图（即输入张量的每个深度切片），都会执行池化，并使用其自身的滤波器在空间上调整输入大小。常见的做法是在每个深度切片上执行池化层，使用2
    x 2的滤波器和步幅为2。执行这种下采样有多种方式。最常见的方法是通过**最大池化**操作来实现，这意味着我们保留池化滤波器下输入区域中像素值最高的像素。下图演示了在输入张量的给定切片上执行最大池化操作：
- en: '![](img/dc8b9aa0-1ece-47cb-9762-fbeef39a3276.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc8b9aa0-1ece-47cb-9762-fbeef39a3276.png)'
- en: The pooling layer downsamples the activation volume spatially, independently
    in each depth slice of the input volume. The most common downsampling operation
    is max, giving rise to max pooling, here shown with a stride of 2\. That is, each
    max is taken over four numbers (the little 2 x 2 square).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层按空间独立地对输入体积的每个深度切片进行下采样。最常见的下采样操作是最大池化，这里显示的是步幅为2的最大池化。也就是说，每个最大值是从四个数字中取的（即2
    x 2的小方格）。
- en: You can also downsample by taking the average value of the 2 x 2 squares as
    shown. This operation is known as **average pooling**. As we will see throughout
    later chapters, Keras comes with quite a few pooling layers that each perform
    different types of downsampling operations on the outputs from the previous layer.
    The choice will largely depend on your specific use case. In the case of image
    classification, a two-dimensional or three-dimensional max pooling layer is most
    commonly used. The dimension of the layer simply refers to the type of input it
    accepts. For example, two-dimensional layers are used to downsample when processing
    grayscale images, and three-dimensional layers are used in the case of colored
    images. You can refer to the well-maintained documentation to study it yourself.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过取 2 x 2 方格的平均值来进行降采样，如图所示。这个操作被称为**平均池化**。正如我们在后续章节中将看到的，Keras 提供了许多池化层，每个池化层执行不同类型的降采样操作，作用于上一层的输出。选择哪种池化层将主要取决于你的具体使用案例。在图像分类任务中，最常用的是二维或三维的最大池化层。池化层的维度指的是它接受的输入类型。例如，处理灰度图像时会使用二维池化层，而在处理彩色图像时，则会使用三维池化层。你可以参考维护良好的文档，自己深入学习。
- en: Implementing CNNs in Keras
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Keras 中实现 CNN
- en: Having achieved a high-level understanding of the key components of a CNN, we
    may now proceed with actually implementing one ourselves. This will allow us to
    become familiar with the key architectural considerations when building convolutional
    networks and get an overview of the implementational details that make these networks
    perform so well. Soon, we will implement the convolutional layer in Keras, and
    explore downsampling techniques such as pooling layers to see how we can leverage
    a combination of convolutional, pooling, and densely connected layers for various
    image classification tasks.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在对卷积神经网络（CNN）的关键组件有了较为深入的理解后，我们现在可以开始实际实现一个 CNN。这样，我们可以熟悉构建卷积网络时需要考虑的关键架构因素，并概览实现细节，了解这些细节是如何使得网络表现如此优秀的。很快，我们将会在
    Keras 中实现卷积层，并探索如池化层等降采样技术，看看我们如何通过卷积、池化和全连接层的组合来处理各种图像分类任务。
- en: For this example, we will adopt a simple use case. Let's say we wanted our CNN
    to detect human emotion, in the form of a smile or a frown. This is a simple binary
    classification task. How do we proceed? Well, firstly, we will need a labeled
    dataset of humans smiling and frowning. While there are many ways to go about
    doing this, we select the **Happy House Dataset** for this purpose. This dataset
    comes with about 750 images of people, each either smiling or frowning, stored
    in the `h5py` files. To follow along, all you need to do is to download the dataset
    hosted on the Kaggle website, which you can access freely through this link: [https://www.kaggle.com/iarunava/happy-house-dataset](https://www.kaggle.com/iarunava/happy-house-dataset)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将采用一个简单的用例。假设我们希望我们的 CNN 能够检测人类的情感，例如微笑或皱眉。这是一个简单的二分类任务。我们该如何进行呢？首先，我们需要一个标注了微笑和皱眉的人的数据集。虽然有很多方式可以实现这一目标，但我们选择了**Happy
    House 数据集**。该数据集包含约 750 张人类的图像，每张图片中的人物要么微笑，要么皱眉，存储在 `h5py` 文件中。要跟着学习，你只需要下载存放在
    Kaggle 网站上的数据集，你可以通过这个链接免费访问：[https://www.kaggle.com/iarunava/happy-house-dataset](https://www.kaggle.com/iarunava/happy-house-dataset)
- en: Probing our data
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查我们的数据
- en: 'Let''s start by loading in and probing our dataset, to get an idea of what
    we are dealing with. We make a simple function that reads our `h5py` files, extracts
    the training and test data, and places it into the standard NumPy arrays, shown
    as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先加载并检查一下数据集，了解一下我们正在处理的内容。我们编写了一个简单的函数，读取我们的 `h5py` 文件，提取训练数据和测试数据，并将其放入标准的
    NumPy 数组中，代码如下：
- en: '[PRE0]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Verifying the data shape
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证数据形状
- en: 'Next, we will print out the shape of our training and test data. In the following
    code block, we notice that we are dealing with colored images of 64 x 64 pixels.
    We have 600 of these in our training set and `150` in the test set. We can also
    have a look at what an image actually looks like, as we did in previous examples
    using Matplotlib:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将打印出训练数据和测试数据的形状。在以下代码块中，我们可以看到我们处理的是 64 x 64 像素的彩色图像。我们的训练集中有 600 张这样的图像，测试集则有
    `150` 张。我们还可以查看其中一张图像的实际样子，正如我们在之前的例子中使用 Matplotlib 所做的那样：
- en: '[PRE1]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '—et voila! Ladies and gentlemen, we have a frowny face:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ——瞧！女士们，先生们，我们看到了一个皱眉的脸：
- en: '![](img/1ab7a4ab-dbf3-4581-8103-06808ab90da1.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1ab7a4ab-dbf3-4581-8103-06808ab90da1.png)'
- en: Normalizing our data
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规范化我们的数据
- en: 'Now, we will prepare our images by rescaling pixel values between 0 and 1\.
    We also transpose our label matrices, as we want them to be oriented as (600,
    1), and not (1, 600), referring to our training labels, as shown previously in
    the training labels. Finally, we print out the shape of our features and labels
    for both the training and the test set:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过将像素值重新缩放到0到1之间来准备我们的图像。我们还转置了标签矩阵，因为我们希望它们的方向是（600，1），而不是（1，600），这与我们之前在训练标签中提到的标签一致。最后，我们打印出训练集和测试集的特征和标签的形状：
- en: '[PRE2]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we convert our NumPy arrays to floating-point arithmetic values, which
    our network prefers:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将NumPy数组转换为浮点数值，这是我们网络偏好的格式：
- en: '[PRE3]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Making some imports
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入一些必要的库
- en: 'Finally, we get to import the new layers that we will be employing for our
    emotion classification task. In the bottom section of the block of the previous
    code, we import a two-dimensional convolutional layer. The dimension of the convolutional
    layer is a property specific to the task you wish to perform. Since we are dealing
    with images, a two-dimensional convolutional layer is the best choice. If we were
    dealing with time-series sensor data (such as biomedical data—for example, EEGs
    or financial data such as the stock market), then a one-dimensional convolutional
    layer would be a more appropriate choice. Similarly, we would use three-dimensional
    convolutional layers if videos were the input data:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们导入将在情感分类任务中使用的新层。在前一段代码的底部部分，我们导入了一个二维卷积层。卷积层的维度是特定于你想执行的任务的属性。因为我们处理的是图像，所以二维卷积层是最佳选择。如果我们处理的是时间序列传感器数据（例如生物医学数据——如EEG或股市等财务数据），那么一维卷积层将是更合适的选择。类似地，如果输入数据是视频，那么我们会使用三维卷积层：
- en: '[PRE4]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Similarly, we also imported a two-dimensional max pooling layer, along with
    a batch-wise normalizer. Batch normalization simply allows us to deal with the
    changing values of layer outputs, as data propagates through our network.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们还导入了一个二维最大池化层，以及一个按批次进行归一化的层。批量归一化使我们能够处理网络传播过程中层输出的变化值。
- en: The problem of ‘internal covariate shift’ is indeed a well noted phenomena in
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: “内部协变量偏移”问题确实是一个广为人知的现象。
- en: CNNs as well as other ANN architectures, and refers to the change in the input’s
    statistical
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 以及其他ANN架构，指的是输入统计量的变化
- en: distribution after a few training iterations, slowing down the convergence of
    our model to ideal weights. This problem can be avoided by simply normalizing
    our data in mini-batches, using a mean and variance reference. While we encourage
    you to further research the problem of internal covariate shift and the mathematics
    behind batch normalization, for now it suffices to know that this helps us train
    our network faster and allows higher learning rates, while making our network
    weights easier to initialize
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过几次训练迭代后，分布会发生变化，导致我们模型的收敛速度减慢，无法达到理想的权重。这一问题可以通过简单地在小批量中对数据进行归一化，使用均值和方差作为参考来避免。虽然我们鼓励你进一步研究“内部协变量偏移”问题以及批量归一化背后的数学原理，但现在了解它有助于我们更快地训练网络，并允许更高的学习率，同时使网络权重更容易初始化，这就足够了。
- en: Convolutional layer
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层
- en: 'Two main architectural considerations are associated with the convolutional
    layer in Keras. The first is to do with the number of filters to employ in the
    given layer, whereas the second denotes the size of the filters themselves. So,
    let''s see how this is implemented by initializing a blank sequential model and
    adding our first convolutional layer to it:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，卷积层涉及两个主要的架构考虑因素。第一个与在给定层中使用的滤波器数量有关，第二个与滤波器本身的大小有关。那么，让我们看看如何通过初始化一个空的顺序模型并向其中添加第一个卷积层来实现这一点：
- en: '[PRE5]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Defining the number and size of the filters
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义滤波器的数量和大小
- en: As we saw previously, we define the layer by embodying it with 16 filters, each
    with a height and width of 5 x 5\. In actuality, the proper dimensions of our
    filters are 5 x 5 x 3\. However, the depth of all the filters spans the full depth
    of a given input tensor, and hence never needs to be specified. Since this is
    the first layer, receiving as input a tensor representation of our training image,
    the depth of our filters would be 3, for each of the red, green, and blue values
    per pixel covered.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所见，我们通过赋予层16个过滤器，每个过滤器的高度和宽度为5 x 5，来定义这一层。实际上，我们过滤器的正确维度应该是5 x 5 x 3。然而，所有过滤器的深度覆盖了给定输入张量的整个深度，因此无需明确指定。由于这是第一层，它接收的是我们训练图像的张量表示，因此我们过滤器的深度为3，表示每个像素的红色、绿色和蓝色值。
- en: Padding input tensors
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 填充输入张量
- en: Thinking about the convolution operation intuitively, it becomes apparent that
    as we slide our filter across an input tensor, what ends up happening is that
    our filter passes over borders and edges less frequently than it does over other
    parts of the input tensor. This is simply because each pixel that is not located
    at the edge of an input may be resampled by the filter multiple times, as it strides
    across an image. This leaves our output representations with uneven samplings
    of borders and edges from our input, referred to as **the border effect**.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观的角度思考卷积操作，显而易见，当我们将过滤器滑过输入张量时，最终发生的情况是，过滤器通过边缘和边界的频率低于它通过输入张量其他部分的频率。这仅仅是因为每个不位于输入边缘的像素，随着过滤器在图像中滑动，可能会被过滤器多次重新采样。这使得输出表示在输入的边界和边缘部分具有不均匀的采样，称为**边界效应**。
- en: 'We can avoid this by simply padding our input tensor with zeros, shown as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过简单地用零填充输入张量来避免这个问题，如下所示：
- en: '![](img/c1ad6a02-499a-4aaf-8654-16637e6cc1bb.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c1ad6a02-499a-4aaf-8654-16637e6cc1bb.png)'
- en: 'In this manner, the pixels that would normally appear at the edge of our input
    tensor now appear later on, allowing an even sampling to be performed on all pixels
    within the input. We specify in our first convolutional layer that we want to
    preserve the input length and width of our inputs, so as to ensure the output
    tensor has the same spatial dimensions with respect to its length and width. This
    is done by defining the padding parameter of the layer as `same`. The depth of
    a convolutional layer output, as previously stated, is denoted by the number of
    filters we choose to use. In our case, this would be 16, denoting the 16 activation
    maps produced as each of our 16 filters convolve over the input space. Finally,
    we define the input shape as the dimensions of any single input image. For us,
    this corresponds to the 64 x 64 x 3 coloured pixels that we have in our dataset,
    shown again as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，通常位于输入张量边缘的像素将被后续处理，允许在输入的所有像素上执行均匀采样。我们在第一个卷积层中指定希望保持输入的长度和宽度，以确保输出张量在其长度和宽度上具有相同的空间维度。这是通过将该层的填充参数定义为`same`来实现的。卷积层输出的深度，如前所述，由我们选择使用的过滤器数量表示。在我们的案例中，这将是16，表示16个激活图是在每个过滤器卷积输入空间时产生的。最后，我们定义输入形状为任何单一输入图像的维度。对于我们来说，这对应于数据集中我们拥有的64
    x 64 x 3彩色像素，具体如下所示：
- en: '[PRE6]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Max Pooling layer
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大池化层
- en: The activation maps from our first convolutional layer are normalized and fed
    into the max pooling layer below. Similar to the convolution operation, pooling
    is applied one input region at a time. For the case of max pooling, we simply
    take the largest value in our grid of pixels, which represents the strongest correlating
    pixels to each feature, and combine these max values to form a lower dimensional
    representation of the input image. In this manner, we preserve more important
    values and discard the remaining values in a given grid of the respective activation
    map.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第一个卷积层的激活图经过归一化，并输入到下面的最大池化层。与卷积操作类似，池化操作是逐个输入区域应用的。对于最大池化操作，我们仅在像素网格中取最大的值，该值代表与每个特征最相关的像素，并将这些最大值组合形成输入图像的低维表示。通过这种方式，我们保留了更重要的值，并丢弃了给定激活图网格中的其余值。
- en: 'This downsampling operation does naturally cause a certain degree of information
    loss, yet drastically reduces the amount of storage space required in networks,
    giving it a considerable efficiency boost:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这种下采样操作自然会导致一定程度的信息丢失，但大大减少了网络所需的存储空间，从而显著提高了效率：
- en: '[PRE7]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Leveraging a fully connected layer for classification
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用全连接层进行分类
- en: 'Then, we simply add a few more layers of convolution, batch normalization,
    and dropouts, progressively building our network until we reach the final layers.
    Just like in the MNIST example, we will leverage densely connected layers to implement
    the classification mechanism in our network. Before we can do this, we must flatten
    our input from the previous layer (16 x 16 x 32) to a 1D vector of dimension (8,192).
    We do this because dense layer-based classifiers prefer to receive 1D vectors,
    unlike the output from our previous layer. We proceed by adding two densely connected
    layers, the first one with 128 neurons (an arbitrary choice) and the second one
    with just one neuron, since we are dealing with a binary classification problem.
    If everything goes according to plan, this one neuron will be supported by its
    cabinet of neurons from the previous layers and learn to fire when it sees a certain
    output class (for example, smiling faces) and abstain from this when presented
    with images from the other class (for example, frowning faces). Note that we again
    use a sigmoid activation function for our last layer, which computes the class
    probability per given input image:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们简单地添加一些卷积层、批量归一化层和dropout层，逐步构建我们的网络，直到达到最后的几层。就像在MNIST示例中一样，我们将利用密集连接层来实现我们网络中的分类机制。在此之前，我们必须将来自上一层的输入（16
    x 16 x 32）展平成一个维度为（8,192）的1D向量。我们这么做是因为基于密集层的分类器倾向于接收1D向量，而不像我们上一层的输出那样是多维的。接下来，我们添加了两层密集连接层，第一层有128个神经元（这是一个任意选择），第二层只有一个神经元，因为我们处理的是一个二分类问题。如果一切按计划进行，这一个神经元将由前几层的神经元支撑，并学会在看到某个特定的输出类别时激活（例如，笑脸），而在遇到其他类别的图像时（例如，愁眉苦脸的脸）则不会激活。请注意，我们再次在最后一层使用sigmoid激活函数来计算每个输入图像的类别概率：
- en: '[PRE8]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Summarizing our model
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结我们的模型
- en: Let's visualize our model to better understand what we just built. You will
    notice that the number of activation maps (denoted by the depth of subsequent
    layer outputs) progressively increases throughout the network. On the other hand,
    the length and width of the activation maps tend to decrease, from (64 x 64) to
    (16 x 16), by the time the dropout layer is reached. These two patterns are conventional
    in most, if not all, modern iterations of CNNs.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化我们的模型，以便更好地理解我们刚刚构建的内容。你会注意到，激活图的数量（由后续层输出的深度表示）在整个网络中逐渐增加。另一方面，激活图的长度和宽度则趋向于减小，从（64
    x 64）到（16 x 16），直到达到dropout层。这两种模式在大多数现代卷积神经网络（CNN）中是常见的，甚至可以说是标准的。
- en: 'The reason behind the variance in input and output dimensions between layers
    can depend on how you have chosen to address *the border effects* we discussed
    earlier, or what *stride* you have implemented for the filters in your convolutional
    layer. Smaller strides will lead to higher dimensions, whereas larger strides
    will lead to lower dimensions. This is simply to do with the number of locations
    you are computing dot products at, while storing the result in an activation map.
    Larger filter strides (or steps) will reach the end of an image earlier, having
    computed fewer dot product values over the same input space of a smaller filter
    stride. The strides of a convolutional layer may be set by defining the strides
    parameter, with an integer or tuple/list of single integers for the given layer.
    The integer value(s) will refer to the length of the stride at each convolution
    operation:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和输出维度之间的变化可能与我们之前讨论的*边界效应*的处理方式有关，或者与在卷积层中为过滤器实现的*步幅*有关。较小的步幅会导致更高的维度，而较大的步幅则会导致更低的维度。这与计算点积的位置数量有关，同时将结果存储在激活图中。较大的过滤器步幅（或步长）会更早到达图像的末尾，在同一输入空间中计算较少的点积值。卷积层的步幅可以通过定义步幅参数来设置，参数可以是一个整数或整数的元组/列表，表示给定层的步幅长度：
- en: '[PRE9]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Following will be the summary:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是总结：
- en: '![](img/219aa4f8-8c91-4f49-8679-6a85e4769a3b.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/219aa4f8-8c91-4f49-8679-6a85e4769a3b.png)'
- en: As we noted earlier, you will notice that both the convolutional and the max
    pooling layers generate a three-dimensional tensor with dimensions corresponding
    to the output height, output width, and output depth. The depth of the layer outputs
    are essentially activation maps for each filter initialized. We implemented our
    first convolutional layer with 16 filters, and the second layer with 32 filters,
    hence the respective layers will produce that many numbers of activation maps,
    as seen previously.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所述，您会注意到卷积层和最大池化层都会生成一个三维张量，其维度对应于输出的高度、宽度和深度。层输出的深度本质上是每个初始化的滤波器的激活图。我们实现了第一个卷积层，使用了16个滤波器，第二层使用了32个滤波器，因此每一层将生成相应数量的激活图，正如前面所见。
- en: Compiling the model
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译模型
- en: 'At the moment, we have covered all the key architectural decisions involved
    in designing a ConvNet and are now ready to compile the network we have been building.
    We choose the `adam` optimizer, and the `binary_crossentropy` loss function, as
    we previously did for the binary sentiment analysis task from the previous chapter.
    Similarly, we also import the `EarlyStopping` callback to monitor our loss on
    the validation set, to get an idea of how well our model is doing on unseen data,
    at each epoch:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们已经涵盖了设计卷积神经网络（ConvNet）过程中所有关键的架构决策，现在可以编译我们一直在构建的网络了。我们选择了`adam`优化器和`binary_crossentropy`损失函数，就像在上一章的二元情感分析任务中做的那样。类似地，我们还导入了`EarlyStopping`回调函数，用来监控验证集上的损失，以便了解模型在每个训练周期（epoch）中在未见数据上的表现：
- en: '![](img/8d2424db-d215-464b-8371-1754e63cba37.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d2424db-d215-464b-8371-1754e63cba37.png)'
- en: Checking model accuracy
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查模型的准确度
- en: 'As we saw previously, we achieved a test accuracy of 88% at the last epoch
    of our training session. Let''s have a look at what this really means, by interpreting
    the precision and recall scores of our classifier:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所见，在训练的最后一个周期，我们达到了88%的测试准确度。让我们看看这到底意味着什么，通过解释分类器的精确度和召回率得分：
- en: '![](img/bc8a4fd2-d20e-4f13-91a9-525cdde053fb.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc8a4fd2-d20e-4f13-91a9-525cdde053fb.png)'
- en: As we noticed previously, the ratio of correctly predicted positive observations
    to the total number of positive observations in our test set (otherwise known
    as the **precision score**) is pretty high at 0.98\. The recall score is a bit
    lower and denotes the number of correctly predicted results divided by the number
    of results that should have been returned. Finally, the F-measure simply combines
    both the precision and recall scores as a harmonic mean.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前注意到的，测试集中正确预测的正类观察值与正类观察值总数的比率（即**精确度**）相当高，达到了0.98。召回率略低，表示正确预测的结果与应返回的结果数之间的比率。最后，F-值是精确度和召回率的调和平均数。
- en: To supplement our understanding, we plot out a confusion matrix of our classifier
    on the test set, as shown as follows. This is essentially an error matrix that
    lets us visualize how our model performed. The *x* axis denotes the predicted
    classes of our classifier, whereas the *y* axis denotes the actual classes of
    our test examples. As we see, our classifier falsely detects about 17 images where
    it thinks that the person is smiling, whereas they are in fact frowning (also
    known as a **false positive**).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了补充我们的理解，我们绘制了分类器在测试集上的混淆矩阵，如下所示。这本质上是一个错误矩阵，它让我们可视化模型的表现。*x*轴表示分类器的预测类别，而*y*轴表示测试样本的实际类别。正如我们所看到的，分类器错误地检测出约17张图片，认为这些人是在微笑，而实际上他们是在皱眉（也称为**假阳性**）。
- en: 'On the other hand, our classifier only makes one mistake in classifying a smiling
    face as a frowning face (also known as a **false negative**). Thinking of false
    positives and negatives helps us evaluate the utility of our classifier in real-world
    scenarios and lets us perform a cost benefit analysis of deploying such a system.
    In our case, this would not be necessary, given the subject matter of our classification
    task; however, other scenarios (such as skin cancer detection, for example) will
    require careful consideration and evaluation before using such learning systems:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们的分类器在将微笑的面孔错误分类为皱眉的面孔时只犯了一个错误（也称为**假阴性**）。考虑假阳性和假阴性有助于我们评估分类器在实际场景中的实用性，并进行部署此类系统的成本效益分析。在我们的分类任务中，这种分析可能不太必要；然而，在其他场景（例如皮肤癌检测）中，这种分析和评估就显得尤为重要。
- en: '![](img/20d88cab-a52e-4927-9bab-8febf0d5501a.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20d88cab-a52e-4927-9bab-8febf0d5501a.png)'
- en: 'Whenever you are satisfied with your models'' accuracy, you may save a model
    shown as follows. Note that this not only constitutes best practice, as it gives
    you well-documented records of your previous attempts, steps taken, and results
    achieved, but is also useful if you want to further probe the model, by peering
    into its intermediate layers to see what it has actually learned, as we will momentarily
    do:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 每当你对模型的准确度感到满意时，你可以保存一个模型，如下所示。请注意，这不仅是最佳实践，因为它为你提供了关于之前尝试、采取的步骤和取得的结果的良好文档记录，而且如果你想进一步探讨模型，通过查看它的中间层，看看它实际学到了什么，这也非常有用，正如我们马上要做的那样：
- en: '![](img/e06e4e2d-c845-4935-8774-122927e3916e.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e06e4e2d-c845-4935-8774-122927e3916e.png)'
- en: The problem with detecting smiles
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测微笑的问题
- en: We must note at this point, that the problem of external validity (that is,
    the generalizability of our model) persists with a dataset like the smile detector.
    Given the restricted manner in which data has been collected, it would be unreasonable
    to expect our CNN to generalize well on other data. Firstly, the network is trained
    with low resolution input images. Moreover, it has only seen images of one smiling
    or frowning person in the same location each time. Feeding this network an image
    of, say, the managerial board of FIFA will not cause it to detect smiles however
    large and present they may be. We would need to readapt our approach. One way
    can be through applying the same transformations to the input image as done for
    the training data, by segmenting and resizing the input image per face. A better
    approach would be to gather more varied data and augment the training set by rotating
    and contorting the training examples, as we will see in later chapters. The key
    here is to include people smiling at different poses and orientations, with different
    lighting conditions, in your dataset, to truly capture all useful visual representations
    of a smile. If it is too expensive to collect more data, generating synthetic
    images (using the Keras image generator) can also be a viable option. The quality
    of your data can drastically improve the performance of your network. For now,
    we will explore some techniques to inform ourselves of the inner workings of a
    CNN.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须指出的是，外部效度的问题（即模型的可推广性）在像微笑检测器这样的数据集上仍然存在。鉴于数据收集的方式比较有限，期望我们的卷积神经网络（CNN）能在其他数据上很好地推广是没有道理的。首先，网络是用低分辨率的输入图像训练的。此外，它每次只见到的是同一个地方的一位笑脸或皱眉的人。给这个网络输入一张比如说国际足联管理委员会的照片，无论照片中笑容多么明显，它都不会检测出微笑。我们需要重新调整方法。一种方法是通过对输入图像应用与训练数据相同的转换方式，例如按面部分割和调整大小。更好的方法是收集更多样化的数据，并通过旋转和扭曲训练样本来增强训练集，正如我们在后面的章节中将看到的那样。这里的关键是，在数据集中包括不同姿势、不同方向、不同光照条件下微笑的人，以便真正捕捉到微笑的所有有用视觉表现。如果收集更多数据成本太高，生成合成图像（使用Keras图像生成器）也是一个可行的选择。数据的质量可以大幅提高网络的性能。现在，我们将探索一些技术，了解CNN的内部工作原理。
- en: Inside the black box
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 黑箱内部
- en: It is one thing to train a **smile detector** from a narrow dataset of similar
    images. Not only can you directly verify your predictions, but each false prediction
    doesn't exactly cost you a fortune. Now, if you were using a similar system to
    monitor behavioral responses of psychiatric patients (as a hypothetical example),
    you would probably want to ensure a high accuracy by ensuring that your model
    really understands what a smile means and is not picking up some irrelevant pattern
    in your training set. In the context of high-risk industries such as healthcare
    or energy, any misunderstandings can have disastrous consequences ranging from
    the loss of lives to the loss of resources. Hence, we want to be able to ensure
    that our deployed model has indeed picked up on truly predictive trends in the
    data and has not memorized some random features with no out-of-set predictivity.
    This has happened throughout the history of the use of neural networks. In the
    following section, we have selected a few tales from neural network folklore to
    illustrate these dilemmas.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个**微笑检测器**，仅仅使用一个狭窄的相似图像数据集，这是一回事。你不仅可以直接验证预测结果，而且每次错误预测也不会让你付出巨大的代价。现在，如果你使用类似的系统来监控精神病患者的行为反应（作为一个假设的例子），你可能会希望通过确保模型真正理解“微笑”的含义，而不是在训练集中识别一些无关的模式来确保高准确度。在医疗或能源等高风险行业中，任何误解都可能带来灾难性后果，从失去生命到资源浪费不等。因此，我们希望能够确保我们部署的模型确实捕捉到了数据中真正具有预测性的趋势，而不是记住了某些随机的特征，这些特征在训练集之外无法进行有效预测。这种情况在神经网络使用的历史中屡屡发生。在接下来的部分，我们从神经网络的民间故事中挑选了一些例子，以说明这些困境。
- en: Neural network fails
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络失败
- en: 'Once upon a time, the US Army had gotten the idea of using neural networks
    for automated detection of camouflaged enemy tanks. Researchers were commissioned
    to design and train a neural network to detect camouflaged tank images, from aerial
    photography of enemy locations. The researchers simply fine-tuned the model weights
    to reflect the correct output labels for each training example, and then tested
    the model on their secluded test examples. Luckily (or so it appeared), their
    network was able to classify all test images adequately, confirming to the researchers
    that their task had ended. Yet, soon enough, the researchers heard back from angry
    Pentagon officials claiming that the network they had handed over did no better
    than random chance at classifying camo-tanks. Confused, the researchers probed
    their training data and compared it to what the Pentagon had tested the network
    with. They discovered that the photos of camouflaged tanks used for training the
    network were all taken on cloudy days, whereas the negatives (photos without camo-tanks)
    were all taken on sunny days. The consequence of this was that their network had
    only learned to distinguish the weather (through brightness of the pixels) rather
    than approaching the intended classification task at hand:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 曾几何时，美国陆军想到了利用神经网络来自动检测伪装的敌方坦克。研究人员被委托设计并训练一个神经网络，用于从敌方位置的航拍照片中检测伪装的坦克图像。研究人员仅仅是微调了模型的权重，使其能够为每个训练样本反映正确的输出标签，然后在他们隔离的测试样本上测试该模型。幸运的是（或者说看起来是这样），他们的网络能够充分分类所有的测试图像，这让研究人员确信他们的任务已经完成。然而，很快，他们收到愤怒的五角大楼官员的反馈，声称他们交付的网络在分类伪装坦克方面的表现不过是随机的。困惑的研究人员检查了他们的训练数据，并将其与五角大楼用于测试网络的数据进行了对比。他们发现，训练网络时使用的伪装坦克照片都是在阴天拍摄的，而负样本（没有伪装坦克的照片）则都是在晴天拍摄的。结果是，他们的网络只学会了区分天气（通过像素的亮度），而没有真正完成原本的分类任务：
- en: '![](img/b4494810-16c1-4784-9bed-19f1652ea8bc.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4494810-16c1-4784-9bed-19f1652ea8bc.png)'
- en: It is quite often that neural networks, after a deluge of training iterations,
    achieve super-human accuracy on their training sets. For instance, one researcher
    observed such a phenomenon when they attempted to train a network to classify
    different types of land and sea mammals. Having achieved a great performance,
    the researchers tried to dig in further in order to decode any classification
    rules we humans might still ignore for this task. It turned out that a large part
    of what their sophisticated network had learned to do was with the presence or
    absence of blue pixels in an image, which naturally do not occur often on pictures
    with land mammals.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 很常见的是，神经网络在经历大量训练迭代后，能够在训练集上达到超越人类的准确度。例如，一位研究人员在尝试训练一个网络来分类不同类型的陆地和海洋哺乳动物时，观察到了这种现象。在取得良好表现后，研究人员尝试进一步深入研究，以解码人类可能忽视的任何分类规则。结果发现，他们的复杂网络所学到的一个重要部分是图像中蓝色像素的存在或缺失，这在陆地哺乳动物的图片中自然不会经常出现。
- en: The last in our short selection of neural network failure tales is the case
    of the self-driving car that self-drove off a bridge. Confused automation engineers
    attempted to probe the trained network to understand what went wrong. To their
    surprise, they discovered something very curious. Instead of detecting the road
    on the street to follow, the network was, for some reason, relying on the continuous
    patches of green grass separating the road from the sidewalk for its orientation.
    When encountering the bridge, this patch of green grass disappeared, causing the
    network to behave in the seemingly unpredictable way it did.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们短小的神经网络失败故事中，最后一个案例是自动驾驶汽车自行开车驶下桥的事件。困惑的自动化工程师试图探查已训练的网络，想弄清楚出了什么问题。令他们惊讶的是，他们发现了一件非常奇怪的事。网络并没有检测到街道上的道路来进行导航，而是出于某种原因，依赖于分隔道路与人行道的连续绿色草地来确定其方向。当遇到桥梁时，这片绿色草地消失了，导致网络表现出似乎不可预测的行为。
- en: Visualizing ConvNet learnings
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化卷积神经网络的学习
- en: These stories motivate our need to ensure that our models do not overfit on
    random noise, but actually capture representatively predictive features. We know
    how predictive inaccuracies can be introduced through careless data considerations,
    the inherent nature of your task, or inherent randomness in modeling. The conventionally
    popularized narrative on neural networks often includes terms such as **black
    box** to describe its learning mechanism. While understanding what individual
    neurons have learned may not be intuitive for all sorts of neural nets, this is
    hardly true for CNNs. Interestingly, ConvNets allow us to, literally, visualize
    their learned features. As we saw earlier, we can visualize neural activations
    for given input images. But we can do even more. In fact, there are a multitude
    of methods that have been developed in recent years to probe a CNN, to better
    understand what it has learned. While we do not have the time to cover all of
    these, we will be able to cover the most practically useful ones.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这些故事激发了我们确保模型不会过拟合随机噪声，而是能够捕捉到具有代表性的预测特征的需求。我们知道，不小心的数据处理、任务本身的固有特性或建模中的内在随机性都可能引入预测不准确性。神经网络中广泛传播的叙事通常会用到**黑箱**等术语来描述其学习机制。虽然理解个别神经元所学内容对各种神经网络来说可能并不直观，但这对卷积神经网络（CNN）来说并非如此。令人有趣的是，卷积神经网络允许我们直观地可视化其学习到的特征。正如我们之前看到的，我们可以可视化给定输入图像的神经激活。但我们可以做得更多。事实上，近年来已经开发了多种方法来探查CNN，更好地理解它所学到的东西。虽然我们没有时间涵盖所有这些方法，但我们将能够介绍一些最具实用性的。
- en: Visualizing neural activations of intermediate layers
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化中间层的神经激活
- en: Firstly, we can visualize how progressive layers in our CNN transform the inputs
    they are fed, by looking at their activation maps. Recall that these are simply
    the reduced representations of the inputs that the network propagates through
    its architecture as it sees data. Visualizing the intermediate layers (convolutional
    or pooling) gives us an idea of the activation of neurons in our network at each
    stage as the input is broken down by the various learned filters. Since each two-dimensional
    activation map stores features extracted by a given filter, we must visualize
    these maps as two-dimensional images, where each image corresponds to a learned
    feature. This method is often referred to as visualizing intermediate activations.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以通过查看它们的激活图来可视化 CNN 中逐层如何转换它们接收到的输入。回忆一下，这些激活图只是网络在处理数据时传播通过其架构的输入的简化表示。可视化中间层（卷积层或池化层）能让我们了解网络在每个阶段的神经元激活情况，因为输入会被各种学习到的滤波器逐步拆解。由于每个二维激活图存储了由给定滤波器提取的特征，因此我们必须将这些图像作为二维图像来进行可视化，其中每张图像对应于一个学习到的特征。这个方法通常被称为可视化中间激活。
- en: To be able to extract the learned features of our network, we will have to make
    some minor architectural tweaks to our model. This brings us to Keras's functional
    API. Recall that, previously, we were defining a sequential model using Keras's
    sequential API, which essentially let us sequentially stack layers of neurons
    to perform our classification tasks. These models ingested input tensors of images
    or word representations and spat out class probabilities assigned to each input.
    Now we will use the functional API, which allows building multi-output models,
    directed acyclic graphs, and even models with shared layers. We will use this
    API to peer into the depths of our convolutional network.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够提取我们网络学习到的特征，我们需要对模型做一些小的架构调整。这就引出了 Keras 的功能性 API。回忆一下，之前我们使用 Keras 的顺序
    API 定义了一个顺序模型，基本上让我们可以顺序堆叠神经元层来执行分类任务。这些模型接收图像或单词表示的输入张量，并输出为每个输入分配的类别概率。现在，我们将使用功能性
    API，它允许构建多输出模型、有向无环图，甚至是共享层的模型。我们将使用这个 API 来深入探索我们的卷积网络。
- en: Predictions on an input image
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对输入图像的预测
- en: 'First and foremost, we prepare an image (although you may use several) to be
    ingested by our multi-output model, so we are able to see the intermediate layer
    activations that occur as this image propagates through our new model. We take
    a random image from our test set for this purpose and prepare it as a four-dimensional
    tensor (with batch size 1, since we are only feeding our network a single image):'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们准备一张图像（尽管你可以使用几张图像）作为输入，让它通过我们的多输出模型，以便我们可以看到该图像在通过新模型时产生的中间层激活。为了这个目的，我们从测试集中随机选取一张图像，并将其准备为一个四维张量（批量大小为
    1，因为我们只输入一张图像）：
- en: '![](img/ffc11494-ab65-4fe9-951f-de179a99ec72.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ffc11494-ab65-4fe9-951f-de179a99ec72.png)'
- en: Next, we initialize a multi-output model to make a prediction on our input image.
    The purpose of this is to capture the intermediate activations, per each layer
    of our network, so we can visually plot out the activation maps generated by different
    filters. This helps us understand which features our model has actually learned.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们初始化一个多输出模型，对输入图像进行预测。这样做的目的是捕获每一层网络的中间激活，以便我们能够直观地绘制出不同滤波器生成的激活图。这有助于我们理解我们的模型实际学习到了哪些特征。
- en: Introducing Keras's functional API
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Keras 的功能性 API
- en: How exactly will we do this? Well we start by importing the `Model` class from
    the functional API. This lets us define a new model. The key difference in our
    new model is that this one is capable of giving us back multiple outputs, pertaining
    to the outputs of intermediate layers. This is achieved by using the layer outputs
    from a trained CNN (such as our smile detector) and feed it into this new multi-output
    model. Essentially, our multi-output model will take an input image and return
    filter-wise activations for each of the eight layers in our smile detector model
    that we previously trained.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到底如何做到这一点呢？我们从导入 `Model` 类开始，该类来自功能性 API。这让我们可以定义一个新的模型。我们新模型的关键区别在于，它能够返回多个输出，涉及到中间层的输出。我们通过使用一个经过训练的
    CNN（比如我们的笑脸检测器）中的层输出，将其输入到这个新的多输出模型中，从而实现这一点。实际上，我们的多输出模型会接受一个输入图像，并返回我们之前训练的笑脸检测器模型中每个八个层的滤波器激活值。
- en: 'You can also limit the number of layers to visualize through the list slicing
    notation used on `model.layers`, shown as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过在`model.layers`上使用列表切片符号，限制可视化的层数，如下所示：
- en: '![](img/2bd743e5-6b33-4288-989d-9e3410362c5e.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2bd743e5-6b33-4288-989d-9e3410362c5e.png)'
- en: 'The last line of the preceding code defines the activations variable, by making
    our multi-output model perform inference on the input image we fed it. This operation
    returns the multiple outputs corresponding to each layer of our CNN, now stored
    as a set of NumPy arrays:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的最后一行定义了`activations`变量，通过让我们的多输出模型对输入图像进行推理操作。此操作返回了与CNN每一层对应的多个输出，这些输出现在存储为一组NumPy数组：
- en: '![](img/3a828fd7-0c27-498c-a448-95966af6bd45.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a828fd7-0c27-498c-a448-95966af6bd45.png)'
- en: As you see, the activations variable stores a list of `8` NumPy *n*-dimensional
    arrays. Each of these `8` arrays represents a tensor output of a particular layer
    in our smile detector CNN. Each layer output represents the activations from the
    multiple filters used. Hence, we observe multiple activation maps per layer. These
    activation maps are essentially two-dimensional tensors that encode different
    features from the input image.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，`activations`变量存储了一个包含`8`个NumPy *n*维数组的列表。每一个`8`个数组都表示我们微笑检测器CNN中特定层的张量输出。每个层的输出代表了多个过滤器的激活。因此，我们在每层上观察到多个激活图。这些激活图本质上是二维张量，编码了输入图像的不同特征。
- en: Verifying the number of channels per layer
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证每层的通道数
- en: 'We saw that each layer has a depth that denoted the number of activation maps.
    These are also referred to as channels, where each channel contains an activation
    map, with a height and width of (*n* x *n*). Our first layer, for example, has
    16 different maps of size 64 x 64\. Similarly, the fourth layer has 16 activation
    maps of size 32 x 32\. The eighth layer has 32 activation maps, each of size 16
    x 16\. Each of these activation maps was generated by a specific filter from its
    respective layer, and are passed forward to subsequent layers to encode higher-level
    features. This will concur with our smile detector model''s architectural build,
    which we can always verify, as shown here:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到每层都有一个深度，表示激活图的数量。这些也被称为通道，每个通道包含一个激活图，其高度和宽度为(*n* x *n*)。例如，我们的第一层有16个不同的激活图，大小为64
    x 64。类似地，第四层有16个大小为32 x 32的激活图。第八层有32个激活图，每个大小为16 x 16。这些激活图是由各自层中的特定过滤器生成的，并被传递到后续层，以编码更高级的特征。这与我们微笑检测器模型的架构相符，我们总是可以验证，如下所示：
- en: '![](img/ef30f0c5-299c-4aa7-a50a-0ae05e5ea016.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef30f0c5-299c-4aa7-a50a-0ae05e5ea016.png)'
- en: Visualizing activation maps
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化激活图
- en: 'Now the fun part! We will plot out the activation maps for different filters
    in a given layer. Let''s start with the first layer. We can plot each of the 16
    activation maps out, as shown here:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是有趣的部分！我们将绘制给定层中不同过滤器的激活图。我们从第一层开始。我们可以绘制每个16个激活图，如下所示：
- en: 'While we will not show all 16 of these activation maps, here are a few interesting
    ones we found:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不会展示所有16个激活图，但以下是我们发现的一些有趣的激活图：
- en: '![](img/3096988d-4caa-4871-9e88-e462458fd77d.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3096988d-4caa-4871-9e88-e462458fd77d.png)'
- en: As we can clearly see, each filter has captured distinct features from the input
    image, relating to the horizontal and vertical edges of the face, as well as the
    background of the image. As you visualize deeper activation maps of deeper layers,
    you will note that the activations become increasingly abstract in nature and
    less interpretable to the human eye. These activations are said to be encoding
    higher-level notions pertaining to a face's location and the eyes and ears within.
    You will also notice that the more and more activation maps remain blank the deeper
    you probe into the network. This means that fewer filters were activated in the
    deeper layers because the input image did not have the patterns corresponding
    to the ones encoded by the filters. This is quite common, as we would expect activation
    patterns to increasingly relate to the class of the image shown as it propagates
    through deeper layers of the network.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们可以清楚看到的，每个滤波器从输入图像中捕捉到了不同的特征，涉及到面部的水平和垂直边缘，以及图像的背景。当你可视化更深层的激活图时，你会注意到，激活变得越来越抽象，且不再容易被人眼解读。这些激活被认为是在编码与面部位置以及眼睛和耳朵相关的高级概念。你还会注意到，随着你深入探查网络，越来越多的激活图保持空白。这意味着在更深的层中激活的滤波器较少，因为输入图像没有包含与滤波器编码的模式相对应的特征。这是很常见的现象，因为我们可以预期随着网络层次的加深，激活模式会越来越与图像的类别相关。
- en: Understanding saliency
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解显著性
- en: We saw earlier that the intermediate layers of our ConvNet seemed to encode
    some pretty clear detectors of face edges. It is harder to distinguish, however,
    whether our network understands what a smile actually is. You will notice in our
    smiling faces dataset that all pictures have been taken on the same background
    at the same approximate angle from the camera. Moreover, you will notice that
    the individuals in our dataset tend to smile as they lift their head up high and
    clear, yet mostly tilt their head downward while frowning. That's a lot of opportunity
    for our network to overfit on some irrelevant pattern. Hence, how do we actually
    know that our network understands that a smile has more to do with the movement
    of a person’s lips than it has to do with the angle at which someone’s face is
    tilted? As we saw in our neural network fails, it can happen quite often that
    the network picks up on irrelevant patterns. In this part of our experiments,
    we will visualize the **saliency maps** for the given network inputs.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到，ConvNet 的中间层似乎能够编码一些非常明显的面部边缘检测器。然而，更难区分的是，我们的网络是否真正理解微笑是什么。你会注意到，在我们的微笑面孔数据集中，所有图片都在相同的背景下拍摄，且大致从相同的角度拍摄。此外，你还会注意到，数据集中的人们通常在抬起头并清晰地微笑时才会笑，而在皱眉时大多是低头的。这为我们的网络提供了很多过拟合无关模式的机会。那么，我们怎么知道网络理解微笑更多是与一个人嘴唇的动作有关，而不是与面部倾斜的角度有关呢？正如我们在神经网络失败案例中看到的那样，网络经常会捕捉到无关的模式。在我们实验的这一部分，我们将可视化给定网络输入的**显著性图**。
- en: 'First introduced in a paper by the visual geometry group at Oxford University,
    the idea behind saliency maps is to simply compute the gradient of a desired output
    category with respect to changes in the input image. Put differently, we are trying
    to determine how a small change in the pixel values of our image affects our network’s
    beliefs on what it is seeing for a given image:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 显著性图的概念最早由牛津大学视觉几何小组在一篇论文中提出，其背后的思想是通过计算所需输出类别相对于输入图像变化的梯度。换句话说，我们试图确定图像中像素值的微小变化如何影响网络对给定图像的理解：
- en: '![](img/14ff46b8-8287-417c-b26b-7b84338678f4.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/14ff46b8-8287-417c-b26b-7b84338678f4.png)'
- en: 'Intuitively, let''s suppose that we have trained a convolutional network on
    images of various animals: giraffes, leopards, dogs, cats, and many more. Then,
    to test what it has learned, we show it the image of a leopard and ask it, *Where
    do you think the leopard is in this image?* Technically speaking, we are ranking
    the pixels of our input image, based on the influence each of them has on the
    **class probability score** that our network spits out for this image. Then, we
    can simply visualize the pixels that had the greatest influence in classifying
    a given image, as these would be the pixels where positive changes lead to increasing
    our network''s class probability score, or confidence, that the given image pertains
    to a certain class.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，假设我们已经用不同动物的图像训练了一个卷积神经网络：长颈鹿、豹子、狗、猫等等。然后，为了测试它学到了什么，我们给它展示一张豹子的图片，并问它，*你认为这张图片中的豹子在哪里？*
    从技术上讲，我们是在对输入图像中的像素进行排序，基于每个像素对网络输出的**类别概率分数**的影响。接着，我们可以简单地可视化对图像分类产生最大影响的像素，这些像素就是那些正向改变会导致增加网络对该图像属于某一类别的概率得分或置信度的像素。
- en: Visualizing saliency maps with ResNet50
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ResNet50可视化显著性图
- en: 'To keep things interesting, we will conclude our smile detector experiments
    and actually use a pre-trained, very deep CNN to demonstrate our leopard example.
    We also use the Keras `vis`, which is a great higher-level toolkit to visualize
    and debug CNNs built on Keras. You can install this package using the `pip` package
    manager:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持趣味性，我们将结束我们的微笑检测实验，并实际使用一个预训练的、非常深的CNN来展示我们的豹子示例。我们还使用Keras `vis`，它是一个非常好的高级工具包，用于可视化和调试基于Keras构建的CNN。你可以使用`pip`包管理器安装这个工具包：
- en: '![](img/b8b36506-43aa-459e-883f-c5573d6c10de.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b8b36506-43aa-459e-883f-c5573d6c10de.png)'
- en: Here, we import the ResNet50 CNN architecture with pretrained weights for the
    ImageNet dataset. We encourage you to explore other models stored in Keras as
    well, accessible through `keras.applications`. We also switch out the Softmax activation
    for the linear activation function in the last layer of this network using `utils.apply_modifications`,
    which rebuilds the network graph to help us visualize the saliency of maps better.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们导入了带有预训练权重的ResNet50 CNN架构，用于ImageNet数据集。我们鼓励你也探索Keras中存储的其他模型，可以通过`keras.applications`访问。我们还将网络最后一层的Softmax激活函数替换为线性激活函数，使用`utils.apply_modifications`，该函数重建了网络图，以帮助我们更好地可视化显著性图。
- en: ResNet50 was first introduced as the ILSVRC competition and won first place
    in 2015. It does very well at avoiding the accuracy degradation problem associated
    with very deep neural networks. It was trained on about a thousand output classes
    from the ImageNet dataset. It is considered a high-performing, state-of-the-art
    CNN architecture, made available for free by its creators. While it uses some
    interesting mechanics, known as **residual blocks**, we will refrain from commenting
    further on its architecture till later chapters. For now, let's see how we can
    use the pretrained weights of this model to visualize the saliency maps of a few
    leopard pictures.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet50首次出现在ILSVRC竞赛中，并在2015年获得了第一名。它在避免与非常深的神经网络相关的精度下降问题方面表现得非常好。该模型是基于ImageNet数据集中的大约千个输出类别进行训练的。它被认为是一种高性能的、最先进的CNN架构，由其创建者免费提供。虽然它使用了一些有趣的机制，被称为**残差模块**，但我们将在后面的章节中才进一步讨论其架构。现在，让我们来看一下如何使用该模型的预训练权重来可视化一些豹子图片的显著性图。
- en: Loading pictures from a local directory
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从本地目录加载图片
- en: 'If you would like to follow along, simply google some nice leopard pictures
    and store them in a local directory. You can use the image loader from the `utils`
    module in Keras `vis` to resize your images to the target size that the ResNet50
    model accepts (that is, images of 224 x 224 pixels):'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想跟着一起做，只需在Google上搜索一些漂亮的豹子图片，并将它们存储在本地目录中。你可以使用Keras `vis`模块中的图像加载器来调整图片的大小，以符合ResNet50模型所接受的目标尺寸（即224
    x 224像素的图片）：
- en: '![](img/8c5243eb-cb45-4ffc-9411-dc00ce627250.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c5243eb-cb45-4ffc-9411-dc00ce627250.png)'
- en: 'Since we wish to make the experiment considerably arduous for our network,
    we purposefully selected pictures of camouflaged leopards to see how well this
    network does at detecting some of nature''s most intricate attempts to hide these
    predatory creatures from the sight of prey, such as ourselves:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望让实验对网络来说相当具有挑战性，因此特意选择了伪装的豹子图片，以观察该网络在检测大自然中最复杂的伪装尝试时表现如何——这些伪装试图将这些掠食性动物从猎物（比如我们自己）的视线中隐藏起来：
- en: '![](img/5b333364-2b72-4d3b-8452-67c0986b289d.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b333364-2b72-4d3b-8452-67c0986b289d.png)'
- en: Using Keras's visualization module
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras的可视化模块
- en: 'Even our biological neural networks implemented throughout our visual cortex
    seem to have some difficulty finding the leopard in each image, at first glance.
    Let''s see how well its artificial counterpart does at this the task. In the following
    segment of code, we import the saliency visualizer object from the `keras-vis`
    module, as well as a utils tool that lets us search for layers by name. Note that
    this module does not come with the standard Keras install. However, it can be
    easily installed using the `pip` package manager on Python. You can even execute
    the install through your Jupyter environment:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是我们整个视觉皮层中实施的生物神经网络，乍一看似乎在每张图像中找到豹子有些困难。让我们看看其人工对应物在这项任务中表现如何。在以下代码段中，我们从`keras-vis`模块导入显著性可视化器对象，以及一个工具，让我们能够按名称搜索层。请注意，这个模块不随标准的Keras安装一起提供。但是，可以通过Python的`pip`包管理器轻松安装它。您甚至可以通过Jupyter环境执行安装：
- en: '[PRE10]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Searching through layers
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 搜索通过层
- en: 'Next, we perform a utility search to define our last densely connected layer
    in the model. We want this layer as it outputs the class probability scores per
    output category, which we need to be able to visualize the saliency on the input
    image. The names of the layer can be found in the summary of the model (`model.summary()`).
    We will pass four specific arguments to the `visualize_salency()` function:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们执行一个实用搜索来定义模型中的最后一个密集连接层。我们需要这个层，因为它输出每个输出类别的类概率分数，我们需要能够可视化输入图像上的显著性。层的名称可以在模型的摘要中找到（`model.summary()`）。我们将向`visualize_salency()`函数传递四个特定的参数：
- en: '![](img/bcf27d3e-44ec-4237-bd72-d6bdac2afdd0.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bcf27d3e-44ec-4237-bd72-d6bdac2afdd0.png)'
- en: 'This will return the gradients of our output with respect to our input, which
    intuitively inform us what pixels have the largest effect on our model''s prediction.
    The gradient variable stores six 224 x 224 images (corresponding to the input
    size for the ResNet50 architecture), one for each of the six input images of leopards.
    As we noted, these images are generated by the `visualize_salency` function, which
    takes four arguments as input:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回我们的输出相对于输入的梯度，直观地告诉我们哪些像素对我们模型的预测有最大影响。梯度变量存储了六个224 x 224的图像（对应于ResNet50架构的输入尺寸），每个图像代表六个输入豹子图像。正如我们注意到的，这些图像是由`visualize_salency`函数生成的，该函数接受四个参数作为输入：
- en: A seed input image to perform prediction on (`seed_input`)
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于执行预测的种子输入图像（`seed_input`）
- en: A Keras CNN model (`model`)
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个Keras CNN模型（`model`）
- en: An identifier for the model's output layer (`layer_idx`)
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型输出层的标识符（`layer_idx`）
- en: The index of the output class we want to visualize (`filter_indices`)
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想要可视化的输出类的索引（`filter_indices`）
- en: The index reference we use here (288) refers to the index of the label *leopard* on
    the ImageNet dataset. Recall that earlier we imported pretrained layer weights
    for the currently initialized model. These weights were achieved by training the
    ResNet50 model on the ImageNet dataset. If you're curious about the different
    output classes, you can find them along with their respective indices, here: [https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 此处使用的索引参考（288）是指ImageNet数据集上标签*leopard*的索引。回想一下，我们之前导入了当前初始化模型的预训练层权重。这些权重是通过在ImageNet数据集上训练ResNet50模型获得的。如果您对不同的输出类别感兴趣，您可以在这里找到它们以及它们各自的索引：[https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)。
- en: 'Visualizing the saliency maps for the first three images, we can actually see
    that the network is paying attention to the locations where we find the leopard
    in the image—perfect! This is indeed what we want to see, as it denotes that our
    network really understands (roughly) where the leopard is located in our image,
    despite our best attempts at showing it noisy images of camouflaged leopards:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化前三张图像的显著性图，我们实际上可以看到网络正在关注我们在图像中找到豹子的位置。太棒了！这确实是我们想要看到的，因为它表明我们的网络确实（大致）理解豹子在我们的图像中的位置，尽管我们尽最大努力展示了伪装豹子的嘈杂图像：
- en: '![](img/bad43907-8885-42ab-bbd1-998bdea3b0ca.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bad43907-8885-42ab-bbd1-998bdea3b0ca.png)'
- en: Exercise
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Probe all the layers in the network. What do you notice?
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探查网络中的所有层。您注意到了什么？
- en: Gradient weighted class activation mapping
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度加权类激活映射
- en: Another nifty gradient-based method is the **gradient weighted class activation
    map** (**Grad-CAM**). This is useful specifically if you have input images with
    entities belonging to several output classes and you want to visualize which areas
    in the input picture your network associates most with a specific output class.
    This technique leverages the class-specific gradient information flowing into
    the final convolutional layer of a CNN to produce a coarse localization map of
    the important regions in the image. In other words, we feed our network an input
    image and take the output activation map of a convolution layer by weighing every
    channel of the output (that is, the activation maps) by the gradient of the output
    class with respect to the channel. This allows us to better utilize the spatial
    information corresponding to what our network pays most attention to, represented
    in the last convolutional layer of the network. We can take these gradient weighted
    activation maps and overlay them on top of the input image to get an idea of which
    parts of our input the network associates highly with a given output class (that
    is, leopard).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个巧妙的基于梯度的方法是**梯度加权类别激活图**（**Grad-CAM**）。如果你的输入图像包含属于多个输出类别的实体，并且你想要可视化网络与特定输出类别最相关的输入图像区域，这个方法特别有用。该技术利用流入CNN最终卷积层的类别特定梯度信息，生成图像中重要区域的粗略定位图。换句话说，我们将输入图像喂给网络，并通过按输出类别相对于通道的梯度加权来获取卷积层的输出激活图（即激活图）。这使我们能够更好地利用与网络最关注的空间信息，这些信息在网络的最后一个卷积层中表现出来。我们可以将这些梯度加权激活图叠加到输入图像上，从而了解网络将输入图像的哪些部分与特定输出类别（即豹子）高度关联。
- en: Visualizing class activations with Keras-vis
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras-vis可视化类别激活
- en: For this purpose, we use the `visualize_cam` function, which essentially generates
    a Grad-CAM that maximizes the layer activations for a given input, for a specified
    output class.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们使用`visualize_cam`函数，它本质上生成一个Grad-CAM图，最大化指定输入的层激活，以便生成一个特定输出类别的激活图。
- en: 'The `visualize_cam` function takes the same four arguments we saw earlier,
    plus an additional one. We pass it the arguments corresponding to a Keras model,
    a **seed input** image, a **filter index** corresponding to our output class (ImageNet
    index for leopard), as well as two model layers. One of these layers remains the
    fully connected dense output player, whereas the other layer refers to the final
    convolutional layer in the ResNet50 model. The method essentially leverages these
    two reference points to generate the gradient weighted class activation maps,
    as shown:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`visualize_cam`函数接受与之前相同的四个参数，并增加了一个额外的参数。我们传递给它与Keras模型相对应的参数，一个**种子输入**图像，一个**滤波器索引**对应我们的输出类别（豹子的ImageNet索引），以及两个模型层。其中一个层保持为完全连接的密集输出层，另一个层指的是ResNet50模型中的最终卷积层。该方法本质上利用这两个参考点生成梯度加权类别激活图，如下所示：'
- en: '![](img/d831d8b4-2ef4-45c1-be17-9da168645641.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d831d8b4-2ef4-45c1-be17-9da168645641.png)'
- en: 'As we see, the network correctly identifies the leopards in both images. Moreover,
    we notice that the network relies on the leopard''s black-dotted patterns for
    identification of its class. It stands to reason that the network uses this pattern
    to identify a leopard, as this is pretty distinctive of its class. We can see
    the network''s attention through the heatmap, which focuses mostly on the clear
    dotted regions of the leopard''s body and not necessarily the leopard''s face,
    as we might do ourselves if confronted with one. Perhaps millions of years of
    biological evolution have adapted the layer weights in the fusiform gyrus region
    of our brain to distinctively pick up on faces, as this was a pattern of consequence
    for our survival:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，网络正确识别了两张图片中的豹子。此外，我们注意到，网络依赖于豹子身上的黑色斑点图案来识别其种类。可以推测，网络使用这个图案来识别豹子，因为它是这个物种的显著特征。我们可以通过热力图看到网络的注意力，它主要集中在豹子身体上清晰的斑点区域，而不一定是豹子的脸部，就像我们自己在遇到豹子时可能会做的那样。或许数百万年的生物进化已将我们大脑枕状回区域的层权重调整为特别能识别面孔，因为面孔识别对我们的生存至关重要：
- en: '**Paper on Grad-CAM**: [https://arxiv.org/pdf/1610.02391.pdf](https://arxiv.org/pdf/1610.02391.pdf)'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Grad-CAM论文**：[https://arxiv.org/pdf/1610.02391.pdf](https://arxiv.org/pdf/1610.02391.pdf)'
- en: Using the pretrained model for prediction
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预训练模型进行预测
- en: 'By the way, you may actually run an inference on a given image using the ResNet50
    architecture on pretrained ImageNet weights, as we have initialized here. You
    can do this by first preprocessing the desired image on which you want to run
    inference into the appropriate four-dimensional tensor format, as shown here.
    The same of course applies for any dataset of images you may have, as long as
    they are resized to the appropriate format:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，您实际上可以使用预训练的ImageNet权重在给定图像上运行推理，就像我们在这里初始化的ResNet50架构一样。您可以通过首先将所需图像预处理为适当的四维张量格式，来在其上运行推理。对于您可能拥有的任何图像数据集，只要它们调整到适当的格式，当然也可以这么做：
- en: '![](img/48cf2cc8-abb6-4b70-a71a-2e3035c84f80.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/48cf2cc8-abb6-4b70-a71a-2e3035c84f80.png)'
- en: 'The preceding code reshapes one of our leopard images into a 4D tensor by expanding
    its dimension along the 0 axis, then feeds the tensor to our initialized ResNet50
    model to get a class probability prediction. We then proceed to decode the prediction
    class into a human-readable output. For fun, we also defined the `labels` variable,
    which includes all the possible labels our network predicted for this image, in
    descending order of probability. Let''s see which other labels our network attributes
    to our input image:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码通过沿着0轴扩展图像的维度，将我们的豹子图像重新塑造成一个4D张量，然后将该张量输入到初始化的ResNet50模型中，以获得类别概率预测。接着，我们对预测类别进行解码，生成可读的输出。为了好玩，我们还定义了`labels`变量，包含了网络为该图像预测的所有可能标签，按概率从高到低排序。让我们看看网络还为输入图像归类了哪些标签：
- en: '![](img/05b3700c-fb5e-4470-889c-cb4136f98d50.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05b3700c-fb5e-4470-889c-cb4136f98d50.png)'
- en: Visualizing maximal activations per output class
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化每个输出类别的最大激活
- en: 'In the final method, we simply visualize the overall activations associated
    with a particular output class, without explicitly passing our model an input
    image. This method can be very intuitive, while being quite aesthetically pleasing.
    For the purpose of our last experiment, we import yet another pretrained model,
    **the VGG16 network**. This network is another deep architecture based on the
    model that won the ImageNet classification challenge in 2014\. Similar to our
    last example, we switch out the Softmax activation of our last layer with a linear
    one:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后的方法中，我们简单地可视化了与特定输出类别相关的总体激活，而没有显式地将输入图像传递给模型。这个方法既直观又美观。为了进行最后的实验，我们导入了另一个预训练的模型，**VGG16网络**。这个网络是一个深度架构，基于2014年ImageNet分类挑战赛的冠军模型。与我们的上一个例子类似，我们将最后一层的Softmax激活替换为线性激活：
- en: '![](img/eefcd9de-8c03-48b9-89f6-4a65d52ac5ef.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eefcd9de-8c03-48b9-89f6-4a65d52ac5ef.png)'
- en: 'Then, we simply import the activation visualizer object from the visualization
    module implemented in `keras-vis`. We plot out the overall activations for the
    leopard class, by passing the `visualize_activation` function our model, the output
    layer, and the index corresponding to our output class, leopard. As we see here,
    the network has actually captured the general shape of leopards at different orientations
    and locations in the image. Some appear zoomed-in, others are far less distinct,
    yet cat-like ears and the dotted-black pattern are quite distinguishable throughout
    the image—neat, right? Let''s have a look at the following screenshot:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从`keras-vis`的可视化模块中导入激活可视化对象。通过将`visualize_activation`函数传递给我们的模型、输出层以及与豹子类别对应的索引，我们绘制了豹子类别的总体激活。如我们所见，网络实际上捕捉到了图像中不同方向和位置的豹子的整体形状。有些看起来被放大了，其他的则不太明显，但猫耳朵和斑点黑色图案在整个图像中都很容易辨认——是不是很酷？让我们看一下下面的截图：
- en: '![](img/2a32de60-9ffb-4348-92f2-68bc963acdde.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a32de60-9ffb-4348-92f2-68bc963acdde.png)'
- en: Converging a model
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收敛模型
- en: 'Next, you can make the model converge on this output class to visualize what
    the model thinks is a leopard (or another output class) after many iterations
    of convergence. You can define how long you want your model to converge through
    the `max_iter` argument, as shown here:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您可以让模型在该输出类别上收敛，以可视化模型在多次收敛迭代后认为的豹子（或其他输出类别）的特征。您可以通过`max_iter`参数定义模型收敛的次数，如下所示：
- en: '![](img/3f916cf1-b7e6-4352-95e8-7ab7078977a3.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3f916cf1-b7e6-4352-95e8-7ab7078977a3.png)'
- en: Using multiple filter indices to hallucinate
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多个过滤器索引进行幻觉生成
- en: 'You can also play around by passing the `filter_indices` parameter different
    indices corresponding to different output classes from the ImageNet dataset. You
    could also pass it a list of two integers, corresponding to two different output
    classes. This basically lets your neural network *imagine* visual combinations
    of two separate output classes by simultaneously visualizing the activations pertaining
    to both output classes. These can at times turn out to be very interesting, so
    let both your imaginations run wild! It is noteworthy that Google''s DeepDream
    leverages similar concepts, showing how overexcited activation maps can be superimposed
    over input images to generate artistic patterns and images. The intricacy of these
    patterns is at times remarkable and awe-inspiring:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过传递不同的`filter_indices`参数来尝试，选择与ImageNet数据集中的不同输出类别对应的索引。你还可以传递一个由两个整数构成的列表，分别对应两个不同的输出类别。这基本上让你的神经网络*想象*两种不同输出类别的视觉组合，同时可视化与这两个输出类别相关的激活情况。有时这些组合会变得非常有趣，因此，尽情释放你的想象力吧！值得注意的是，谷歌的DeepDream也运用了类似的概念，展示了如何通过叠加过度激活的激活图层在输入图像上生成艺术图案。这些图案的复杂性有时令人惊叹、充满敬畏：
- en: '![](img/8dc35402-607f-4839-bd40-119114172faf.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8dc35402-607f-4839-bd40-119114172faf.png)'
- en: Picture of the author of this book, taken in front of the haunted mansion in
    Disneyland, Paris. The image has been processed using the open source DeepDream
    generator, which we encourage you to play around with, not just to marvel at its
    beauty. It can also generate quite handy gifts for artistic relatives, around
    the holiday season.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 本书作者的照片，拍摄地点为巴黎迪士尼乐园的鬼屋前。该图像已通过开源的DeepDream生成器进行处理，我们鼓励你也来尝试，不仅仅是为了欣赏其美丽，它还可以在节假日期间为艺术天赋的亲戚们生成一些非常实用的礼物。
- en: Problems with CNNs
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）的问题
- en: 'Many may claim that the hierarchically nested pattern recognition technique
    leveraged by CNNs very much resembles the functioning of our own visual cortex.
    This may be true at a certain level. However, the visual cortex implements a much
    more complex architecture and makes it run efficiently on about 10 watts of energy.
    Our visual cortex also does not easily get fooled by images where face-like features
    appear, (although this phenomenon occurs often enough to have secured its proper
    term in modern neuroscience. **Pareidolia** is a term associated with the human
    mind interpreting signals in a manner to generate higher-level concepts, where
    none actually exists. Scientists have shown how this phenomenon is related to
    the earlier activation of neurons located in the fusiform gyrus area of the visual
    cortex, responsible for several visual recognition and classification tasks. In
    cases of pareidolia, these neurons *jump the gun,* as it were, and cause us to
    detect faces or hear sounds, even when this is really not the case. The famous
    picture of the Martian surface illustrates this point, as most of us can clearly
    make out the outlines and features of a face, whereas all the picture actually
    contains is a pile of red dust:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人可能会声称，CNN所采用的层次嵌套模式识别技术在某种程度上与我们自身视觉皮层的运作非常相似。这在某种程度上是有道理的。然而，视觉皮层实现的是一种更加复杂的架构，并且能够在大约10瓦的能量下高效运行。我们的视觉皮层也不会轻易被含有面部特征的图像所欺骗（尽管这种现象足够常见，已经在现代神经科学中得到了正式的命名）。**错觉**是一个与人类大脑解读信号相关的术语，它指的是大脑在没有实际存在的情况下，生成更高层次的概念。科学家们已经表明，这一现象与位于视觉皮层枕状回区域的神经元的早期激活有关，这些神经元负责进行多个视觉识别和分类任务。在错觉的情况下，这些神经元可以说是*过早激活*，导致我们检测到面部特征或听到声音，即便实际上并不存在。这一现象可以通过火星表面的著名图片来说明，在这张图片中，大多数人都能清楚地辨认出面部轮廓和特征，尽管这张图片实际上只是堆积的红色尘土：
- en: '![](img/8345fe93-9fd7-497e-9c8f-5ea27130aa43.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8345fe93-9fd7-497e-9c8f-5ea27130aa43.png)'
- en: Image Courtesy NASA/JPL-Caltech
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：NASA/JPL-Caltech
- en: Neural network pareidolia
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络错觉
- en: 'This problem is naturally not unique to our biological brains. In fact, despite
    the excellent functioning of CNNs for many visual tasks, this problem of neural
    network pareidolia is one that computer vision researchers are always trying to
    solve. As we noted, CNNs learn to classify images through learning an assortment
    of filters that pick up useful features, capable of breaking down the input image
    in a probabilistic manner. However, the features learned by these filters do not
    represent all the information present in a given image. The orientation of these
    features, with respect to one another, matters just as much! The presence of two
    eyes, lips, and a nose does not inherently constitute the essence of a face. Rather,
    it''s the spatial arrangement of these elements within an image that makes the
    face in question:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题显然并非我们生物大脑独有的。事实上，尽管卷积神经网络（CNN）在许多视觉任务中表现优秀，神经网络幻觉（pareidolia）问题一直是计算机视觉研究人员不断努力解决的问题。正如我们所提到的，CNN通过学习一系列过滤器来分类图像，这些过滤器能提取有用的特征，并能以概率的方式对输入图像进行分解。然而，这些过滤器学到的特征并不代表给定图像中的所有信息。这些特征之间的相对方向同样至关重要！两只眼睛、嘴唇和鼻子的存在并不能固有地构成一个面部特征。实际上，正是这些元素在图像中的空间排列决定了我们所说的面部：
- en: '![](img/e7bb30e2-2ccf-4512-aec5-32d92c82758e.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7bb30e2-2ccf-4512-aec5-32d92c82758e.png)'
- en: Summary
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, firstly, we used convolutional layers that are capable of
    decomposing a given visual input space into hierarchically nested probabilistic
    activations of convolution filters that subsequently connect to dense neurons
    that perform classification. The filters in these convolutional layers learn weights
    corresponding to useful representations that may be queried in a probabilistic
    manner to map the set of input features present in a dataset to the respective
    output classes. Furthermore, we saw how we can dive deep into our convolution
    network to understand what it has learned. We saw four specific ways to do this:
    intermediate activation-based, saliency-based, gradient weighted class activations,
    and activation maximization visualizations. Each gives a unique intuition into
    which patterns are picked up by the different layers of our network. We visualized
    these patterns for given images, as well as for entire output classes, to intuitively
    understand which elements of our network pays attention while performing inference.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，首先，我们使用了卷积层，这些卷积层能够将给定的视觉输入空间分解为逐层嵌套的卷积过滤器的概率激活，并随后连接到执行分类的稠密神经元。这些卷积层中的过滤器学习与有用表示相对应的权重，这些表示可以以概率的方式进行查询，从而将数据集中存在的输入特征集映射到各自的输出类别。此外，我们还看到如何深入理解我们的卷积网络所学到的内容。我们看到了四种具体的方法：基于中间激活的、基于显著性的、梯度加权类激活的，以及激活最大化可视化。每种方法都能提供不同层次的网络所捕捉到的模式的独特直觉。我们可视化了这些模式，不仅对于给定的图像，也对于整个输出类别，以直观地理解在推理时我们的网络关注的元素。
- en: Finally, although we reviewed a lot of neuroscience-based inspirations that
    led to the development of the CNN architecture, modern CNNs in no way compete
    with the intricate mechanisms implemented throughout the mammalian visual cortex.
    In fact, many of the structural design of the layers in the visual cortex do not
    even remotely resemble what we have designed here so far. For instance, the layers
    of the visual cortex are themselves structured into subsequent cortical columns,
    containing neurons with supposedly non-overlapping receptive fields, the purpose
    of which is unbeknown to modern neuroscience. Even our retina performs a deluge
    of sensory preprocessing through the use of rod cells (receptive to low-intensity
    light), cone cells (receptive to high-intensity light), and ipRGC cells (receptive
    to time-dependent stimuli) before sending the visual signals in the form of electrical
    impulses to the lateral geniculate nucleus at the base of the thalamus, otherwise
    known as the relay center for visual signals. It is from here that the signals
    begin their journey, propagating back and forth through the six densely interconnected
    (and not convolutional) layers of the visual cortex, as we go about our lives.
    In essence, human vision is quite sequential and dynamic, much different than
    the artificial implementation of it. In summation, while we are far from endowing
    visual intelligence to machines in the manner biology has done to us, CNNs represent
    the pinnacle of modern achievements in computer vision, making it a venerably
    adaptable architecture for countless machine vision tasks.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，尽管我们回顾了许多基于神经科学的灵感，这些灵感促成了 CNN 架构的发展，但现代的 CNN 在任何方面都无法与哺乳动物视觉皮层中实施的复杂机制相竞争。事实上，视觉皮层层次结构的许多设计甚至与我们目前所设计的完全不同。例如，视觉皮层的各层本身就被结构化为后续的皮层柱，包含着神经元，这些神经元的感受野据说是彼此不重叠的，其目的至今仍不为现代神经科学所知。即使我们的视网膜也通过使用棒状细胞（对低强度光敏感）、锥状细胞（对高强度光敏感）和
    ipRGC 细胞（对时间相关刺激敏感）在将视觉信号以电信号形式发送至丘脑基底的外侧膝状核之前，进行了一系列的感官预处理。外侧膝状核被称为视觉信号的中继中心。从这里，信号开始其旅程，往返于视觉皮层六层密集相互连接的层次结构中（而不是卷积层），伴随我们的一生。本质上，人类的视觉是高度顺序化和动态的，远远不同于人工实现的视觉。总结来说，尽管我们距离像生物学那样赋予机器视觉智能还有很长的路要走，但
    CNN 代表了计算机视觉领域的现代成就的巅峰，使其成为无数机器视觉任务中非常适应的架构。
- en: Here, we conclude our chapter on the exploration of CNNs. We will revisit more
    complex architectures in later chapters and experiment with data augmentation
    techniques and more complex computer vision tasks. In the next chapter, we will
    explore another neural network architecture known as RNN, which is especially
    useful for capturing and modeling sequential information such as time variant
    data, common in many fields ranging from industrial engineering to natural language
    dialogue generation.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，我们结束了关于 CNN 探索的章节。我们将在后续章节中回顾更复杂的架构，并实验数据增强技术和更复杂的计算机视觉任务。在下一章节中，我们将探索另一种神经网络架构——RNN，它特别适用于捕捉和建模序列信息，如时间变化数据，这在许多领域中都很常见，从工业工程到自然语言对话生成。
