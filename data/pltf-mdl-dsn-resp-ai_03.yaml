- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Regulations and Policies Surrounding Trustworthy AI
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于可信AI的法规和政策
- en: This chapter provides an outline of the regulations and laws that have been
    passed in various countries that relate to the adoption of ethical AI practices
    in organizations and companies developing large-scale AI solutions. The primary
    objective of this chapter is to enable you to understand the main principles of
    Responsible AI so that you are aware of the legal implications if you fail to
    abide by the regulations. You will also be made aware of the different levels
    of risk associated with AI applications and what it means to accept or ban AI
    systems on the basis of their potential risk. Furthermore, you will learn about
    the commonly followed and enforced initiatives, actions, and guidelines that remove
    bias against different minority groups in populations. In addition, this chapter
    also studies the problems and roadblocks faced by governments when designing fair
    ML solutions and provides recommended actions for building large-scale trustworthy
    AI solutions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章概述了各国在组织和公司开发大规模人工智能解决方案时，通过的与采纳伦理AI实践相关的法规和法律。本章的主要目标是帮助您理解负责任AI的基本原则，以便您了解如果未遵守法规，可能面临的法律后果。您还将了解到与AI应用相关的不同风险级别，以及基于潜在风险接受或禁止AI系统的含义。此外，您将了解常见的倡议、行动和指导方针，这些措施致力于消除对不同少数群体的偏见。除此之外，本章还研究了政府在设计公平机器学习解决方案时所面临的问题和障碍，并提供了构建大规模可信AI解决方案的推荐措施。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Regulations and enforcements passed by individual nations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各国通过的法规和执行措施
- en: Special regulations for children and minority groups
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对儿童和少数群体的特别法规
- en: Next steps for trustworthy AI
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可信AI的下一步
- en: Regulations and enforcements under different authorities
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同权威机构下的法规和执行措施
- en: Regulating AI is still a nascent area. Many countries and organizations have
    come up with proposals for how to regulate and enforce laws and rules related
    to AI adoption in various industries. In this section, we will discuss the regulations
    put forward by different authorities to enforce the unbiased implementation of
    AI systems.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对AI的监管仍然是一个初步领域。许多国家和组织提出了关于如何在不同行业中监管和执行与AI采纳相关的法律和规则的建议。在本节中，我们将讨论不同权威机构提出的旨在强制执行AI系统无偏实施的法规。
- en: Regulations in the European Union
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欧盟的法规
- en: 'On October 20, 2020, the European Parliament of the **European Union** (**EU**)
    adopted three resolutions primarily aimed at the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 2020年10月20日，**欧盟**（**EU**）欧洲议会通过了三项决议，主要目的是：
- en: Developing a structured foundational system for the ethical aspects of **artificial
    intelligence** (**AI**), robotics, automation, and other transformational changes
    that impact the everyday lives of ordinary people
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为伦理方面的**人工智能**（**AI**）、机器人技术、自动化以及其他影响普通人日常生活的变革性变化，开发一个结构化的基础性系统
- en: Formulating an accountable civil authority that will judge the aforementioned
    impact and decide on punitive action
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制定一个负责任的民间机构来评判上述影响并决定惩罚措施
- en: Strategizing a technique to respond to the challenges posed by AI systems regarding
    intellectual property rights
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制定应对AI系统在知识产权方面所带来的挑战的策略
- en: 'The first resolution gave the EU the opportunity to highlight the essence of
    a human-centric and human-created AI approach. The second resolution prompted
    the EU to take the initiative to address the risks created by AI-based systems
    by introducing proportionate and flexible rules for different types of risks.
    These risks were classified with the following labels: **unacceptable risk**,
    **high risk**, **limited risk**, and **minimal risk**. Furthermore, there were
    different liability rules according to the severity of the risk.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项决议为欧盟提供了突出人本中心和人类创造的AI方法精髓的机会。第二项决议促使欧盟主动通过引入适用于不同类型风险的比例性和灵活规则，来应对AI系统带来的风险。这些风险被分类为以下标签：**不可接受的风险**、**高风险**、**有限风险**和**最小风险**。此外，根据风险的严重性，也有不同的责任规则。
- en: However, the last resolution, which *focuses on the intellectual property rights
    at stake in the development of AI technologies* ([https://ai-regulation.com/news-eps-resolutions-on-ethical-framework-civil-liability-and-intellectual-property-rights-for-ai/](https://ai-regulation.com/news-eps-resolutions-on-ethical-framework-civil-liability-and-intellectual-property-rights-for-ai/)),
    still remains unaddressed.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最后一项决议，*专注于人工智能技术发展中涉及的知识产权问题*（[https://ai-regulation.com/news-eps-resolutions-on-ethical-framework-civil-liability-and-intellectual-property-rights-for-ai/](https://ai-regulation.com/news-eps-resolutions-on-ethical-framework-civil-liability-and-intellectual-property-rights-for-ai/))，仍然未得到解决。
- en: 'These AI rules set forth in the draft regulation are harmonized (meaning that
    they apply throughout the EU) and will not have a direct effect on countries outside
    the EU, but they will have some extra-territorial impact. For instance, compliance
    with these rules by all countries making AI services available in the EU is mandatory,
    just as with the **General Data Protection Regulation** (**GDPR**). The EU’s AI
    regulation has an extra-territorial effect on the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 草案中的这些人工智能规则是统一的（意味着它们适用于整个欧盟），并不会直接影响欧盟以外的国家，但它们会产生一定的跨境影响。例如，所有在欧盟提供人工智能服务的国家都必须遵守这些规则，正如**通用数据保护条例**（**GDPR**）一样。欧盟的人工智能法规对以下方面具有跨境效应：
- en: '**Providers** releasing AI-enabled products, solutions, and services to the
    EU market (irrespective of where the services originate from and who the providers
    of the services are)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向欧盟市场推出人工智能产品、解决方案和服务的提供商**（无论服务来源于何处，且无论服务提供者是谁）'
- en: '**Customers** of AI-enabled systems who use AI systems within the EU'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在欧盟内使用人工智能系统的消费者**'
- en: '**Providers and consumers** of AI systems geographically positioned outside
    the EU who are consuming services that reside within the EU'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位于欧盟以外地区的人工智能系统提供商和消费者**，这些提供商和消费者使用位于欧盟内的服务'
- en: 'The EU has placed special emphasis on the governance of AI systems. It has
    also stressed the importance of regulating the algorithm-driven systems used by
    consumers. This can address issues related to privacy, especially where biometric
    recognition systems are concerned. In essence, the objectives of the EU regulations
    are as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟对人工智能系统的治理给予了特别的重视。它还强调了规范消费者使用的算法驱动系统的重要性。这可以解决与隐私相关的问题，尤其是在生物识别系统方面。实质上，欧盟法规的目标如下：
- en: Make sure that AI systems in the EU market are safe and respect existing laws
    on fundamental rights and wider EU values
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保欧盟市场上的人工智能系统安全，并遵守现有的基本权利和更广泛的欧盟价值观
- en: Make sure the law is clear to help investment and innovation in AI
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保法律明确，有助于人工智能的投资与创新
- en: Make sure the laws that protect people’s rights and safety are enforced for
    AI systems
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保执行保护人们权利和安全的法律，以适用于人工智能系统
- en: Make sure that there is a single market for safe and trustworthy AI applications
    so that people can trust them
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保存在一个安全可信的人工智能应用单一市场，使人们能够信任这些应用
- en: These regulations provide a new legal framework for AI in the EU. This framework
    establishes rules regarding the development, market placement, and use of AI systems
    that follow a proportionate, risk-based approach with specific restrictions to
    protect humans from harm. The rules aim for an open, connected community that
    can benefit from the rewards offered by AI technology while protecting citizens’
    safety.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些法规为欧盟的人工智能提供了一个新的法律框架。该框架建立了关于人工智能系统开发、市场投放和使用的规则，采用了比例原则和基于风险的方式，并设有具体的限制，以保护人类免受伤害。这些规则旨在创建一个开放、连接的社区，既能从人工智能技术带来的回报中受益，又能保护公民的安全。
- en: Propositions/acts passed by other countries
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他国家通过的提案/法案
- en: Similar to the EU, other attempts to regulate AI have been made. The regulations
    passed/proposed by the US, Australia, and the **Institute of Electrical and Electronics
    Engineers** (**IEEE**) are noteworthy in this regard.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与欧盟类似，其他国家也已尝试对人工智能进行监管。美国、澳大利亚和**电气与电子工程师学会**（**IEEE**）通过/提出的法规在这方面值得关注。
- en: AI regulation acts in the US
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 美国的人工智能法规
- en: 'In the consumer finance context, to eliminate bias from AI algorithms, two
    main acts have been passed by the US government that have received attention:
    the **Equal Credit Opportunity Act** (**ECOA**) and the **Fair Housing Act** ([https://www.brookings.edu/research/an-ai-fair-lending-policy-agenda-for-the-federal-financial-regulators/](https://www.brookings.edu/research/an-ai-fair-lending-policy-agenda-for-the-federal-financial-regulators/)).
    The ECOA laid down rules that prohibit creditors from issuing, sanctioning, or
    approving credit transactions from lenders who discriminate against race, color,
    religion, national origin, sex, marital status, and age. Even allowing discrimination
    against individuals who may have received income from public or government authorities
    or have legal employment rights is treated as a violation of the act. Likewise,
    the Fair Housing Act has issued prohibitory orders against any discrimination
    when properties are being sold or rented or when assigning mortgages. Both of
    these acts aim to ban **disparate treatment** and **disparate impact**, where
    bias emerges intentionally or unintentionally.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在消费者金融领域，为了消除 AI 算法中的偏见，美国政府通过了两个受到关注的主要法案：**平等信用机会法**（**ECOA**）和**公平住房法**（[https://www.brookings.edu/research/an-ai-fair-lending-policy-agenda-for-the-federal-financial-regulators/](https://www.brookings.edu/research/an-ai-fair-lending-policy-agenda-for-the-federal-financial-regulators/)）。ECOA
    规定，禁止贷方发布、批准或许可歧视种族、肤色、宗教、国籍、性别、婚姻状况和年龄的信贷交易。即使是允许歧视那些可能接受来自公共或政府部门收入的人，或有合法就业权的人，也会被视为违反该法案。同样，公平住房法针对销售、出租房产或分配按揭时的任何歧视行为发布了禁止令。这两项法案旨在禁止**差别对待**和**差别影响**，即偏见可能有意或无意地出现。
- en: '![Figure 3.1 – Graphs from the Zillow Housing Aspirations Report, showing housing
    discrimination observed in young adults and black communities](img/B18681_03_001.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – 来自 Zillow 住房愿望报告的图表，展示了在年轻人和黑人社区中观察到的住房歧视](img/B18681_03_001.jpg)'
- en: Figure 3.1 – Graphs from the Zillow Housing Aspirations Report, showing housing
    discrimination observed in young adults and black communities
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – 来自 Zillow 住房愿望报告的图表，展示了在年轻人和黑人社区中观察到的住房歧视
- en: As demonstrated in *Figure 3**.1*, this act seeks to eliminate discrimination
    against black, Hispanic, Asian, and other minority communities when searching
    for housing, where white people are likely to be favored by housing authorities
    and owners as potential renters over equally qualified minority groups.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 3.1*所示，该法案旨在消除在寻找住房时对黑人、西班牙裔、亚洲裔和其他少数民族社区的歧视，在这种情况下，住房管理部门和房东更可能偏袒白人，选择他们作为潜在租户，而不是具有同等资格的少数群体。
- en: The **Federal Trade Commission** (**FTC**) has put forward a proposition for
    truth, fairness, and equity in any AI-based service that an organization wants
    to promote and sell to its customers. The FTC memo made it clear that the FTC’s
    authority is to be used under Section 5 of the FTC act. This, along with the implementation
    of the **Fair Credit Reporting Act** (**FCRA**) and the ECOA, should curb the
    application of biased algorithms.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**联邦贸易委员会**（**FTC**）提出了一个关于真相、公平和公正的提议，旨在确保任何组织希望推广并销售给客户的基于 AI 的服务都符合这些标准。FTC
    备忘录明确表示，FTC 的权限将根据《FTC 法案》第五条行使。结合**公平信用报告法**（**FCRA**）和 ECOA 的实施，这应该有助于遏制偏见算法的应用。'
- en: FTC chair Rebecca Slaughter has been a prominent voice for economic justice,
    raising concerns on issues related to algorithm-based bias. The FTC has further
    warned that companies could suffer severe punitive action, even prosecution, under
    the ECOA or the FCRA for biased and unfair predictions from AI systems. The most
    important guideline by the FTC is that organizations must stay transparent and
    accountable for the algorithms they develop and put into practice.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: FTC 主席丽贝卡·斯劳特一直是经济公正的突出声音，提出了与基于算法的偏见相关的担忧。FTC 进一步警告，企业可能会因 AI 系统的偏见和不公平预测而面临
    ECOA 或 FCRA 下的严厉惩罚，甚至起诉。FTC 最重要的指导方针是，组织必须对其开发和投入实践的算法保持透明和负责。
- en: The need for an unbiased system and a defined risk management framework has
    been felt by other bodies, such as the US **Department of Commerce** (**DoC**).
    There was new momentum in the area of trustworthy AI with the passing of the National
    Defense Authorization Act in 2021\. This led US Congress to have the **National
    Institute of Standards and Technology** (**NIST**) devise “*a voluntary risk management
    framework for trustworthy AI systems*.” This initiative triggered the development
    of the **AI Risk Management Framework** (**AI RMF**), drafting best practices
    for organizations to control risks emerging from AI systems. It also helped them
    to select the right trade-off between fairness and accuracy, privacy and accuracy,
    and privacy and fairness. The DoC has also established the **National Artificial
    Intelligence Advisory Committee** (**NAIAC**) as part of the National AI Initiative
    Act of 2020\. The primary objective of this committee is to study the current
    state of AI in the US, evaluate the competency and state of the science around
    AI, and accordingly, offer recommendations to increase opportunities for historically
    underrepresented populations.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 美国**商务部**（**DoC**）等其他机构也感受到了对无偏见系统和明确风险管理框架的需求。2021年《国防授权法案》的通过为可信赖的人工智能领域注入了新的动力。这促使美国国会要求**国家标准与技术研究院**（**NIST**）制定“*可信赖AI系统的自愿风险管理框架*”。这一举措催生了**AI风险管理框架**（**AI
    RMF**），为组织制定了管理AI系统带来的风险的最佳实践。同时，它帮助组织在公平性与准确性、隐私与准确性、隐私与公平性之间做出合适的权衡。商务部还根据2020年《国家人工智能倡议法》成立了**国家人工智能咨询委员会**（**NAIAC**）。该委员会的主要任务是研究美国人工智能的现状，评估人工智能领域的能力与科学水平，并相应地提出建议，以增加历史上被忽视群体的机会。
- en: '![Figure 3.2 – Healthcare discrimination observed in black adults](img/B18681_03_002.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图3.2 – 在黑人成年人中观察到的医疗歧视](img/B18681_03_002.jpg)'
- en: Figure 3.2 – Healthcare discrimination observed in black adults
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 – 在黑人成年人中观察到的医疗歧视
- en: Time and again, limiting societal benefits only to privileged groups has had
    a negative impact on society. As illustrated in *Figure 3**.2*, restricting opportunities
    for minority populations has not only impeded extending social facilities to black
    adults but also resulted in denying them adequate healthcare facilities and the
    right to receive equal treatment facilities. The NAIAC has become more vigilant
    in curbing such discrimination to promote equality and justice. In the absence
    of action to promote fairness and equity, our society will continue to create
    biased real-world datasets that generate biased ML models. The absence of equity
    and justice in society leads to more discrimination against minority groups, which
    limits their opportunities for quality education, healthcare, career advancement,
    and more. The previous figure (taken from [https://www.kff.org/report-section/kff-the-undefeated-survey-on-race-and-health-main-findings/](https://www.kff.org/report-section/kff-the-undefeated-survey-on-race-and-health-main-findings/))
    shows the percentage by which, in the US, the black community falls short of having
    proper access to quality services as compared to the white community.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一再地，将社会福利仅限于特权群体对社会产生了负面影响。如*图3.2*所示，限制少数群体的机会不仅阻碍了为黑人成年人提供社会设施，还导致了剥夺他们获得充分医疗设施和享受平等待遇的权利。NAIAC在打击这种歧视方面变得更加警觉，以促进平等和公正。如果不采取行动促进公平与公正，我们的社会将继续创造出偏见的数据集，从而生成偏见的机器学习模型。社会缺乏公平和公正会导致对少数群体的更多歧视，限制他们获得优质教育、医疗、职业晋升等机会。前述图表（摘自[https://www.kff.org/report-section/kff-the-undefeated-survey-on-race-and-health-main-findings/](https://www.kff.org/report-section/kff-the-undefeated-survey-on-race-and-health-main-findings/)）展示了在美国，黑人社区在获得优质服务方面比白人社区的差距百分比。
- en: To foster trade and technology between the EU and the US, the EU-US **Trade
    and Technology Council** (**TTC**) released an **Inaugural Joint Statement** to
    aid in the development of new transformational systems that use AI and promote
    universal human rights. Without this kind of action, biased systems will foster
    a loss of respect and egalitarianism for the common man.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了促进欧盟与美国之间的贸易与技术合作，欧盟-美国**贸易与技术委员会**（**TTC**）发布了**首次联合声明**，以促进利用人工智能并推动普遍人权的变革性系统的开发。如果没有此类行动，偏见的系统将滋生对普通人的尊重和公正的丧失。
- en: Other trustworthy AI initiatives have been advocated by the **United Nations
    Educational, Scientific and Cultural Organization** (**UNESCO**), the **Organization
    for Economic Co-operation and Development** (**OECD**), and the Council of Europe.
    The OECD has been the greatest supporter of the EU government in analyzing and
    measuring the socioeconomic impacts of AI technologies and applications. In addition,
    it actively engages with policy-makers and regulators to increase opportunities
    for underrepresented sectors, classify AI systems, and evaluate risk, fairness,
    transparency, safety, and accountability to converge AI practices consistently
    across borders.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其他可信赖的人工智能倡议已由**联合国教育、科学及文化组织**（**UNESCO**）、**经济合作与发展组织**（**OECD**）以及**欧洲委员会**提倡。OECD一直是欧盟政府在分析和衡量人工智能技术及应用的社会经济影响方面最重要的支持者。此外，OECD积极与政策制定者和监管者合作，增加对弱势群体的机会，对人工智能系统进行分类，并评估风险、公平性、透明度、安全性和问责制，以便在跨国界的人工智能实践中保持一致性。
- en: 'The White House **Office of Science and Technology Policy** (**OSTP**), established
    by Congress in 1976, promulgated 10 principles for consideration in the regulatory
    and non-regulatory approaches to the advancement and application of AI:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**白宫科学技术政策办公室**（**OSTP**）由国会于1976年设立，发布了10项原则，以供在监管和非监管方法中考虑，以推进和应用人工智能：'
- en: Use best practices to build systems and frameworks that help to bolster public
    confidence, faith, and belief.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用最佳实践构建系统和框架，帮助增强公众的信任、信心和信仰。
- en: Ensure that the right education and tools are in place with regard to AI standards
    and technology so that people are more likely to participate and exchange their
    views on the real-world experience of large-scale, productionized AI systems.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保人工智能标准和技术方面的教育和工具到位，使人们更有可能参与并交换关于大规模、生产化人工智能系统的实际经验。
- en: Incorporate the best levels of scientific honesty, morality, and righteousness
    with regard to any kind of input, such as data that impacts AI model predictions.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在任何形式的输入（如影响人工智能模型预测的数据）中，融入最高水平的科学诚实、道德和正义。
- en: Develop transparent risk assessment and risk management methodologies across
    business units in a collaborative manner in order to learn from each other’s ethical
    issues and take proactive action.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在商业单位之间以协作方式开发透明的风险评估和风险管理方法，以便相互学习伦理问题并采取积极行动。
- en: Predict the costs associated with the positives and negatives of the practical
    use of AI by production systems.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测生产系统实际使用人工智能的正负影响相关的成本。
- en: Analyze performance metrics and adopt dynamic learning methodologies to learn
    about and make changes to system behavior and data.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析绩效指标，采用动态学习方法，了解并调整系统行为和数据。
- en: Evaluate and formulate the standards needed to promote fairness by removing
    any kind of bias and discrimination from AI systems.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估并制定促进公平所需的标准，通过消除任何形式的偏见和歧视来优化人工智能系统。
- en: Use transparency tools to gain public trust and guarantee the design of safe
    and secure systems
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用透明度工具赢得公众信任，并确保设计安全可靠的系统。
- en: Establish regular checkpoints to ensure the confidentiality, integrity, and
    availability of unbiased AI data that can be used to build safe and fair systems
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立定期检查点，确保能够用于构建安全、公平系统的无偏见人工智能数据的机密性、完整性和可用性。
- en: Support collaboration, partnership, and inter-departmental teamwork to ensure
    the consistency and predictability of AI policies
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持协作、伙伴关系和跨部门团队合作，确保人工智能政策的一致性和可预测性。
- en: The US has been very proactive in investigating loopholes in current AI solutions
    and has come up with an initial proposal to curb discrimination. The **Blueprint
    for an AI Bill of Rights**, published by the White House in October 2022, is a
    stepping-stone toward protecting individuals and communities from threats caused
    by AI-driven technologies.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 美国在调查当前人工智能解决方案中的漏洞方面非常积极，并提出了初步提案以遏制歧视。**《人工智能权利法案蓝图》**由白宫于2022年10月发布，是保护个人和社区免受人工智能驱动技术威胁的一个重要步骤。
- en: President Biden wanted to remove inequity from all decision-making processes
    by ensuring the incorporation of fairness principles that respect the civil rights
    of Americans and grant equality of opportunity and radical justice in the US.
    With his support, the OSTP has stated five principles that respect civil rights,
    civil liberties, and privacy. It will defend freedom of speech and voting and
    forbid discriminatory practices. By protecting the public’s private data across
    public and private sectors, the guiding principles seek to provide a structured
    framework that can be applied to automated systems. In an attempt to provide equality
    of opportunity in the areas of education, housing, credit, employment, healthcare,
    financial services, safety, social services, non-deceptive information about goods
    and services, and government benefits, the five basic principles have been laid
    out as follows.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 拜登总统希望通过确保在所有决策过程中纳入尊重美国公民权利的公平原则，消除不平等现象，从而在美国实现机会平等和根本的公正。在他的支持下，OSTP提出了五项尊重公民权利、民权和隐私的原则。它将捍卫言论自由和投票权，并禁止歧视性做法。通过在公共和私营部门保护公众的私人数据，这些指导原则旨在提供一个可应用于自动化系统的结构化框架。为了在教育、住房、信贷、就业、医疗保健、金融服务、安全、社会服务、商品和服务的非欺骗性信息以及政府福利等领域提供机会平等，已制定以下五项基本原则。
- en: Safe and effective systems
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安全有效的系统
- en: The objective of this principle is to protect the public from unsafe or ineffective
    systems. There is to be an audit process to validate the risks assessed by domain
    experts and stakeholders from different communities. In addition, this principle
    lays down the best practices for risk mitigation and 24-hour monitoring for AI-based
    systems. By following rigorous, domain-specific standards, automated systems should
    be able to prevent unforeseen dangers and ensure public safety.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本原则的目标是保护公众免受不安全或无效系统的影响。需要有审计流程，以验证由领域专家和不同社区的利益相关者评估的风险。此外，本原则还规定了AI系统的风险缓解最佳实践以及24小时监控。通过遵循严格的、领域特定的标准，自动化系统应能够预防不可预见的危险并确保公共安全。
- en: Algorithmic discrimination protections
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 算法歧视保护
- en: The objective of this principle is to proactively prevent discrimination by
    algorithms. This will not only prevent unjustified differential treatment relating
    to differences in race, sex (including pregnancy, medical conditions, intersex
    status, and sexual orientation), gender identity, religion, age, national origin,
    disability, veteran status, genetic information, and other demographic and social
    statuses, but also ensure continuous measures to provide equitable treatment to
    everyone.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本原则的目标是积极防止算法歧视。这不仅可以防止与种族、性别（包括怀孕、健康状况、双性人身份和性取向）、性别认同、宗教、年龄、国籍、残疾、退伍军人身份、基因信息及其他人口统计和社会状态的差异相关的无理差别待遇，还可以确保采取持续的措施为每个人提供公平对待。
- en: Data privacy
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据隐私
- en: The objective of this principle is to ensure adequate built-in protection and
    safety standards during data collection, use, access, transfer, and deletion processes
    to avoid any violation of data privacy. This principle governs seeking appropriate
    permission (by designers and developers) from customers to fully respect users’
    privacy choices.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本原则的目标是确保在数据收集、使用、访问、传输和删除过程中建立足够的保护措施和安全标准，以避免任何数据隐私侵犯。该原则要求设计师和开发者在进行数据处理前，征得客户的适当许可，以充分尊重用户的隐私选择。
- en: Notice and explanation
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通知和解释
- en: The objective of this principle is to provide transparent, authentic, and up-to-date
    documentation of the overall system functionality, with adequate descriptions
    of the individual system components. In addition, such systems should notify the
    public of any change in system functionality after calibrating the level of risk
    caused by the changes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 本原则的目标是提供透明、真实和最新的系统功能文档，并对各个系统组件进行充分描述。此外，系统应在调整变更带来的风险等级后，通知公众系统功能的任何变化。
- en: Human alternatives and fallback
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人工替代方案和备用方案
- en: The objective of this principle is to ensure quick fallback to human alternatives
    when automated systems fail. In such cases, public safety is of paramount importance,
    so a fallback can guarantee equitable, accessible, and effective functionality
    without any harmful effects.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 本原则的目标是确保在自动化系统失败时能够快速切换到人工替代方案。在这种情况下，公共安全至关重要，因此备用方案可以确保公平、可及和有效的功能，而不会产生任何有害影响。
- en: AI regulation acts in India
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 印度的AI监管法案
- en: The Responsible AI Proposal given by India’s **National Association of Software
    and Service Companies (NASSCOM)** has introduced a concept of licensing (called
    **Responsible AI Licenses (RAIL)**) in order to protect developers’ AI source
    code against malicious use. Furthermore, the licenses provide a way to restrict
    the misuse of the code for inequity, bias, or societal harm. In addition to licensing,
    NASSCOM has stressed the communication of risks both internally and externally
    through dashboards, where different stakeholders (such as senior leadership, legal
    experts, data scientists, and DevOps) can collaborate on Responsible AI metrics
    and regulatory compliance. Transparency through visualization tools, dashboards,
    internal audits, and proactive communication to users about privacy risks is the
    top priority for NASSCOM to curb the ethical and societal risks of AI.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 印度**国家软件与服务公司联合会（NASSCOM）**提出的《负责任的人工智能提案》引入了一种许可概念（称为**负责任的人工智能许可证（RAIL）**），旨在保护开发者的AI源代码免受恶意使用。此外，这些许可证提供了一种方式，限制代码在不公平、偏见或对社会造成危害的情况下被误用。除了许可外，NASSCOM还强调通过仪表板在内部和外部沟通风险，在不同利益相关者（如高级领导、法律专家、数据科学家和DevOps）之间协作关于负责任AI的指标和法规遵从。通过可视化工具、仪表板、内部审计和主动向用户沟通隐私风险的透明度，是NASSCOM遏制AI伦理和社会风险的首要任务。
- en: 'NASSCOM has also highlighted the following aspects of dataset verification
    and validation in the creation of unbiased models:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: NASSCOM还强调了在创建无偏模型时数据集验证和确认的以下方面：
- en: Identify the principal elements, their importance, and the learning parameters
    in the model that govern the model’s outcomes.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定模型中主要元素、其重要性以及支配模型结果的学习参数。
- en: Assess the ability of the model to predict results correctly for all demographics
    through a fair, representative dataset, without skew in any of the features.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过公平且具有代表性的数据集评估模型在所有人口群体中正确预测结果的能力，确保任何特征都不会出现偏差。
- en: Evaluate the target variable and any links with present data features that can
    have a positive or negative impact.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估目标变量及其与当前数据特征的任何关联，这些关联可能会产生积极或消极的影响。
- en: Ensure legal compliance. Collect data with permission and organize timely reviews/audits
    as mentioned in GDPR. This includes having an agreement for processing data that
    states the possible uses of the data that is collected.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保法律合规。按GDPR中所述收集经授权的数据，并组织及时的审查/审计。这包括签订数据处理协议，声明收集的数据的可能用途。
- en: Ensure bare-minimum data collection to avoid the unauthorized use of data by
    third parties, thereby minimizing privacy attacks.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保收集的最低限度数据，以避免第三方未经授权使用数据，从而最大限度减少隐私攻击。
- en: Adhere to special guidelines while collecting unique data. With regard to regulations
    such as the EU’s GDPR, unique or special datasets containing data such as political
    beliefs, sexual orientation, and religious beliefs should not result in unintended
    use without prior consent.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在收集独特数据时遵循特别的指导原则。对于像欧盟的GDPR这样的法规，包含政治信仰、性取向和宗教信仰等数据的独特或特殊数据集，未经事先同意不应导致数据的非预期使用。
- en: Prevent the unauthorized use of data by tracking the data management life cycle
    and maintaining up-to-date information.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过跟踪数据管理生命周期并保持最新信息，防止数据被未经授权使用。
- en: 'NASSCOM has come up with best practices for data collection, training research
    personnel, making evaluation guidelines for error estimation, and the peer review
    of datasets. It has emphasized the importance of mitigating the harm caused by
    discrimination during the preprocessing, in-processing, and postprocessing stages
    of the algorithmic pipeline. Local, global, attribute-based, and causal explanations
    (discussed in [*Chapter 9*](B18681_09.xhtml#_idTextAnchor198), *Model Explainability*)
    have been recommended to assess fairness and transparency. Recommendations put
    forward by EU regulations have been adopted by NASSCOM. NASSCOM also provides
    best practices for deploying production-grade applications, including the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: NASSCOM提出了数据收集、研究人员培训、误差估算评估指南以及数据集同行评审的最佳实践。它强调了在算法处理过程的预处理、处理中和后处理阶段，减少歧视带来的危害的重要性。推荐使用本地、全球、基于属性和因果的解释（在[*第9章*](B18681_09.xhtml#_idTextAnchor198)《模型可解释性》一节中讨论），以评估公平性和透明度。NASSCOM采纳了欧盟法规提出的建议。NASSCOM还提供了部署生产级应用程序的最佳实践，包括以下内容：
- en: Use counterfactual analysis to understand the impact of a feature that may be
    surpassed or perturbed for a specific data point, due to the dominance of other
    features. This helps to analyze how features affect prediction results, such as
    a loan application being rejected by a model that would have accepted the application
    if the applicant’s income was 10,000 USD higher.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用反事实分析来理解某一特征因其他特征的主导作用而可能被超越或扰动对特定数据点的影响。这有助于分析特征如何影响预测结果，例如一个贷款申请被模型拒绝，而如果申请者的收入高出10,000美元，模型原本会接受该申请。
- en: Use a robust and secure deployment workflow to handle peak load and bursts without
    causing any downtime for production systems. A well-managed security pipeline
    should sustain an AI system in extreme environments.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用强大且安全的部署工作流程来处理高负荷和突发情况，确保生产系统不出现任何停机。一个良好管理的安全管道应能在极端环境下维持AI系统的稳定。
- en: Use **Continuous Integration and Continuous Deployment** (**CI/CD**) pipelines
    to monitor and retrain models based on data drift. The objective is to establish
    feedback channels and put an escalation chain in place that ensures trustworthy
    and scalable deployment in practice.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**持续集成与持续部署**（**CI/CD**）管道来监控和重新训练模型，以应对数据漂移。目标是建立反馈渠道，并设立升级链，确保在实际中能够实现值得信赖和可扩展的部署。
- en: AI regulation acts in Australia
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 澳大利亚的AI监管法案
- en: 'Australia has also come up with principles and practices for ethical AI regulation,
    not only to build public trust in AI-enabled products and organizations but also
    to drive customer loyalty to AI-enabled services. Under Australian regulations,
    AI outcomes should be geared toward the following objectives:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 澳大利亚还提出了有关伦理AI监管的原则和实践，不仅为了建立公众对AI驱动产品和组织的信任，还为了推动客户对AI驱动服务的忠诚。在澳大利亚的监管框架下，AI的结果应当朝以下目标发展：
- en: '**Human, societal, and environmental well-being**: AI-powered decision systems
    and architectures should act for the benefit of individuals, society, and the
    environment. They should empower human beings, providing them with enough data
    to make informed decisions. Furthermore, overall societal and environmental well-being
    cannot be achieved by solely depending on machines. Hence, there should be support
    for proper review mechanisms, such as human-in-the-loop, human-on-the-loop, and
    human-in-command approaches. These defined processes involve the optimization
    of the entire ML process, with active feedback from human beings. Using human
    intervention methods, tasks such as annotation, active learning, transfer learning,
    and step-by-step optimization become easier as machines and humans work together
    effectively and collaboratively.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人类、社会和环境福祉**：AI驱动的决策系统和架构应当为了个人、社会和环境的利益而运作。它们应该赋能人类，提供足够的数据以做出明智的决策。此外，单单依赖机器无法实现整体的社会和环境福祉。因此，应该支持适当的审查机制，如人类参与、人工监督和人类主导的方法。这些明确的流程涉及整个机器学习过程的优化，并且有来自人类的积极反馈。通过使用人类干预方法，任务如注释、主动学习、迁移学习和逐步优化变得更加容易，因为机器与人类能够有效且协作地共同工作。'
- en: '**Human-centered values**: AI predictions should respect human rights, morals,
    diversity, and the autonomy of individuals. They should benefit all human beings,
    including future generations, and other living beings. Design choices and parameters
    should have sustainable and environmentally friendly techniques in place to carefully
    evaluate and certify that AI systems are considering all potential harmful effects.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**以人为本的价值观**：AI的预测应尊重人权、道德、多样性和个人的自主性。它们应当造福全体人类，包括未来的世代和其他生命体。设计选择和参数应当采用可持续和环保的技术，仔细评估并认证AI系统是否考虑了所有可能的有害影响。'
- en: '**Fairness**: AI-/ML-based systems should be designed to be inclusive and accessible
    for individuals with disabilities. The accessibility options should not unfairly
    discriminate against individuals, communities, or groups. This is a key objective
    to prevent unfair and biased systems that could lead to several negative effects,
    from disrespecting and disregarding vulnerable groups to aggravating prejudice
    and discrimination. As is evident from *Figure 3**.3* (sourced from [https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/](https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/)),
    four major face recognition algorithms demonstrated poor performance on darker-skinned
    women, with error rates shooting higher than 34% than for lighter-skinned men.
    The objective of introducing fairness into AI/ML algorithms is not only to remove
    bias in the recognition of skin tones or gender but also to foster diversity by
    ensuring accessibility for all, irrespective of attributes.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公平性**：基于AI/ML的系统应设计得包容性强，便于残障人士访问。无论是残障人士、社区还是群体，系统的可访问性选项都不应存在不公平的歧视。这是防止不公平和偏见系统的关键目标，否则可能会导致多个负面影响，从无视和忽视弱势群体到加剧偏见和歧视。如*图3.3*所示（来源：[https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/](https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/)），四种主要的面部识别算法在较深肤色女性上表现较差，错误率比较浅肤色男性高出34%以上。将公平性引入AI/ML算法的目标，不仅是消除对肤色或性别的偏见，还包括通过确保所有人都能获得访问权限，推动多样性的发展。'
- en: '![Figure 3.3 – Lower accuracy for darker female and male populations from face
    recognition technology applications](img/B18681_03_003.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图3.3 – 面部识别技术在较深肤色女性和男性群体中的准确度较低](img/B18681_03_003.jpg)'
- en: Figure 3.3 – Lower accuracy for darker female and male populations from face
    recognition technology applications
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – 面部识别技术在较深肤色女性和男性群体中的准确度较低
- en: '**Privacy protection and security**: AI systems should have strong security
    policies built in to protect any individual’s privacy rights. There should be
    a strong emphasis on all kinds of data protection, from data ingestion to building
    ML models, to ensure the security of data. In addition to privacy and data protection,
    ample data governance mechanisms must also be ensured. Lineage and data governance
    have big roles to play as far as quality, integrity, and legitimate access to
    data are concerned.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私保护和安全**：AI系统应具备强大的安全政策，以保护任何个人的隐私权利。应特别强调各种数据保护，从数据摄取到构建机器学习模型，以确保数据的安全。除了隐私和数据保护外，还必须确保充足的数据治理机制。在数据的质量、完整性和合法访问方面，数据血统和数据治理起着至关重要的作用。'
- en: '**Reliability and safety**: AI systems should not deviate from their stated
    goals and should reliably operate in accordance with their intended purpose and
    with sufficient resilience and security. They need to be safe, accurate, reliable,
    and reproducible. In addition, they should be designed with a fallback plan so
    that if they break the **Service-Level Agreement** (**SLA**) or exhibit a failure
    in their operational routine, they can be restored to a previous state (based
    on defined checkpoints). Such restore activities should also be reliable and safe
    to reduce and prevent unintentional harm.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可靠性和安全性**：AI系统不应偏离其既定目标，应可靠地按照预定目的和充分的韧性与安全性运行。它们需要安全、准确、可靠且可复现。此外，它们应设计有应急计划，以便在违反**服务水平协议**（**SLA**）或在操作过程中出现故障时，能够恢复到先前状态（基于定义的检查点）。此类恢复活动还应可靠、安全，以减少和防止无意的伤害。'
- en: '**Transparency and explainability**: AI systems should be transparent, responsible,
    and easily understood by people. It should be easy for people to recognize the
    cause of the outcomes of any model. Transparent data and AI models should be traceable
    so it is possible to deduce present and past outcomes, and causes should be explained
    to concerned stakeholders in simple language. This is one of the major requirements,
    not only for businesses but also for individuals, to make people aware of the
    system’s capabilities and limitations.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明性和可解释性**：AI系统应具有透明性、责任性，并且易于人们理解。人们应该能够轻松识别任何模型输出结果的原因。透明的数据和AI模型应具有可追溯性，从而可以推断出当前和过去的结果，并且原因应以简单的语言向相关利益方进行解释。这是企业和个人的主要需求之一，旨在让人们了解系统的能力和局限性。'
- en: '**Contestability**: When the outcome of a predictive ML model/AI system significantly
    influences society by impacting an individual, community, group, or environment,
    there should be a timely auditing process. The auditing process provides a mechanism
    to analyze model results in terms of fairness. People should even be allowed to
    challenge the use or outcomes of an AI system.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可争议性**：当预测型ML模型/AI系统的结果显著影响社会，进而影响个人、社区、群体或环境时，应当有及时的审计过程。审计过程提供了一种机制，能够从公平性角度分析模型结果。人们甚至应当有权挑战AI系统的使用或结果。'
- en: '**Accountability**: The people responsible for the different phases of the
    ML model should be identifiable and accountable for the outcomes of the AI systems,
    and human oversight of such AI systems should be provided. The auditability and
    assessment of algorithms, data, and design processes play a key role therein,
    especially in critical applications.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问责制**：负责不同阶段的ML模型的人员应当是可识别的，并对AI系统的结果负责，同时应当为这些AI系统提供人工监督。算法、数据和设计过程的审计性与评估在其中起着关键作用，特别是在关键应用中。'
- en: The same ethical principles (including privacy and fairness) were put forward
    by NITI Aayog in India in 2021\. Their proposal focuses on the safety and reliability
    of AI systems through continuous monitoring.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的伦理原则（包括隐私和公平）于2021年由印度的NITI Aayog提出。该提案重点关注通过持续监控确保AI系统的安全性和可靠性。
- en: IEEE AI regulation guidelines
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IEEE AI监管指南
- en: 'The IEEE’s *Ethically Aligned Design: A Vision for Prioritizing Human Well-being
    with Autonomous and Intelligent Systems* document, first released in 2019, lays
    out eight key aspects related to ethical issues of AI and mitigation strategies.
    These areas of focus, as depicted in *Figure 3**.4,* include principles for creating
    a strong foundation for Ethically Aligned Design frameworks and platforms:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: IEEE的*《伦理对齐设计：以优先考虑人类福祉为目标的自主和智能系统愿景》*文件首次发布于2019年，列出了与AI伦理问题和缓解策略相关的八个关键方面。这些关注点，如*图
    3.4*所示，包含了为构建伦理对齐设计框架和平台奠定坚实基础的原则：
- en: Sustainable development
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可持续发展
- en: Personal data rights and agency over digital identity
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个人数据权利与数字身份代理
- en: Legal frameworks for accountability
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 责任追究的法律框架
- en: Policies for education and awareness
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教育和意识政策
- en: 'The system serves as a guideline to increase human trust in data-driven AI
    systems:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统作为增加人们对数据驱动的AI系统信任的指南：
- en: '![Figure 3.4 – The  fundamental pillars of Responsible AI](img/B18681_03_004.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – 负责任的AI的基本支柱](img/B18681_03_004.jpg)'
- en: Figure 3.4 – The fundamental pillars of Responsible AI
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 负责任的AI的基本支柱
- en: We have learned about the fundamental building blocks of an ethical system.
    Now let’s discuss how collaboration among different organizations can enable us
    to leverage the power of AI/ML to address the needs of entire population segments.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了伦理系统的基本构建块。现在让我们讨论不同组织之间如何合作，帮助我们利用AI/ML的力量来应对整个群体的需求。
- en: Special regulations for children and minority groups
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 针对儿童和少数群体的特殊规定
- en: AI-based predictive analytics and profiling have demonstrated many limitations
    pertaining to minority groups’ opportunities and development. To promote social
    welfare facilities (such as improving facilities used by children), there has
    been extensive statistical analysis of studies related to different cases in the
    past, where data has been sourced from different databases, including public welfare
    benefits data, medical records, and judicial information. Studies and detailed
    investigations have revealed that there have been wide variations/inconsistencies
    in model input data, the data recorded is not systematic, and validation criteria
    have been applied inconsistently.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 基于AI的预测分析和画像展示了与少数群体机会和发展的许多局限性。为了促进社会福利设施（例如改善儿童使用的设施），对过去不同案例的研究进行了广泛的统计分析，数据来源包括公共福利数据、医疗记录和司法信息。研究和详细调查表明，模型输入数据存在广泛的变化/不一致，记录的数据不系统，验证标准的应用也不一致。
- en: Along with the draft regulation (from the EU Commission) and regulatory proposals
    set forth by individual countries, regulatory bodies have come forward to translate
    the proposals for the digital space. The respective bodies have been instrumental
    in proposing a new regulatory framework for machinery products and transformational
    digital techniques being applied in the fields of robotics, IoT, IoMT, blockchain,
    VR, military, autonomous vehicles, and others. The key goal is to enable trust
    between AI providers and users by addressing the risks posed by such systems.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 随着欧盟委员会的草案法规和各国提出的监管建议，监管机构已开始着手将这些建议转化为数字空间的规范。相关机构在提出新的监管框架方面发挥了重要作用，涉及的领域包括机器人学、物联网、物联网医疗、区块链、虚拟现实、军事、自动驾驶车辆等技术的应用。关键目标是通过应对这些系统带来的风险，建立人工智能供应商和用户之间的信任。
- en: '**Chatbots** are one such practical example of real-world AI systems. In some
    cases, they have led to increased risk for children with developmental disabilities
    because chatbots don’t recognize the children’s disabilities, and hence do not
    recognize appeals for help. Neither can a chatbot provide adequate advice to children.
    One instance was recorded in 2018 and was shared with the public by the BBC ([https://www.bbc.com/news/technology-46507900](https://www.bbc.com/news/technology-46507900)).
    The report demonstrates how two mental health chatbots failed to capture children’s
    reports of sexual abuse. Even though the chatbot had been designed with children
    in mind and was considered safe for children, its confused response posed additional
    challenges to young users.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**聊天机器人**是现实世界中人工智能系统的一个实际例子。在某些情况下，它们给患有发育障碍的儿童带来了更大的风险，因为聊天机器人无法识别儿童的障碍，因此无法识别求助的请求。聊天机器人也无法为儿童提供足够的建议。2018年有一个实例被记录下来，并由BBC向公众分享（[https://www.bbc.com/news/technology-46507900](https://www.bbc.com/news/technology-46507900)）。该报告展示了两个心理健康聊天机器人未能捕捉到儿童报告的性虐待案件。尽管该聊天机器人是专为儿童设计的，并被认为对儿童是安全的，但它的混乱反应给年轻用户带来了额外的挑战。'
- en: This is not the only problem with chatbots. The privacy threats from chatbots
    include spoofing (impersonating someone else), tampering with data, data theft,
    and vulnerability to cyberattacks. Security and fairness are primary considerations
    of AI ethics, and when chatbots are found to enforce bias by responding to a reply
    based on the best-matching keywords, it has raised concern among ethicists. Ideally,
    chatbots should not respond with biased responses when the input does not match
    exactly with the words already in its store, and instead, it should learn and
    update its dictionary with the input. Hence, it has become the top priority among
    AI ethicists to educate social organizations as well as producers and companies
    about the side effects of AI solutions. Furthermore, child rights advocates have
    raised questions about data retention policies in chatbots and issues related
    to parental consent, because some chatbots rely on stored voice recordings to
    continuously learn and respond. Historical data has been found to reinforce systemic
    bias and introduce discrimination among children. Hence, it is essential for experts
    to validate individual profiles and stop individuals from being impacted by bias
    from AI system proxies.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是聊天机器人面临的唯一问题。聊天机器人带来的隐私威胁包括伪装（冒充他人）、篡改数据、数据盗窃以及易受网络攻击。安全性和公平性是人工智能伦理的主要考虑因素，当聊天机器人根据最佳匹配的关键词回应时，表现出偏见，这引发了伦理学家的关注。理想情况下，当输入与其库中的词语完全不匹配时，聊天机器人不应给出偏见的回答，而应通过学习并更新字典来应对输入。因此，教育社会组织以及生产商和公司了解人工智能解决方案的副作用，已成为人工智能伦理学家的首要任务。此外，儿童权益倡导者对聊天机器人中的数据保留政策和与父母同意相关的问题提出了质疑，因为一些聊天机器人依赖存储的语音记录来不断学习和回应。历史数据已被发现强化了系统性偏见，并在儿童中引入了歧视。因此，专家们必须验证个体档案，并阻止个体受到人工智能系统代理的偏见影响。
- en: Privacy advocates have become increasingly concerned with data privacy and have
    cautiously taken steps to warn about government mass surveillance activities,
    law enforcement, and examination activities, as well as other inquiry-based tools.
    Private information about individuals cannot be collected without consent, as
    this information can be utilized to identify, segment, investigate, and suppress
    communities.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私倡导者对数据隐私越来越关注，并谨慎地采取措施，警告政府的大规模监视活动、执法活动、审查活动以及其他基于调查的工具。在没有同意的情况下，不能收集个人的私人信息，因为这些信息可能被用来识别、分割、调查和压制社区。
- en: Lack of knowledge of the collection process of biometric data, the people involved
    in the collection process, the storage methodology, and the application of techniques
    risks the trust that people have in AI. Some instances demonstrate the availability
    of limited data for minority groups, and these instances have raised questions
    related to fairness. Success rates for facial recognition detection software have
    been questioned by AI ethicists, especially for groups such as children and women
    of color. Biased training datasets have been known to promote social bias and
    lead to discrimination or further disadvantages for minority communities.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏对生物识别数据收集过程、参与收集过程的人员、存储方法及技术应用的了解，可能会破坏人们对AI的信任。一些案例展示了少数群体数据的有限性，这些案例引发了与公平性相关的问题。AI伦理学家对面部识别软件的成功率提出质疑，尤其是对于儿童和有色女性等群体。已知偏见训练数据集会促进社会偏见，导致对少数群体的歧视或进一步的不利影响。
- en: Promoting equality for minority groups
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 促进少数群体的平等。
- en: 'It is essential to stop bias from flowing into AI systems. Bias prevents a
    system from supporting alternative developmental trajectories for minority groups
    (even children). Trustworthy, equal-opportunity-oriented AI-enabled systems should
    be free of stereotypes and enable possibilities for every child, including girls
    and LGBT children. Some actions centered around the development and inclusion
    of children, ethnic minorities, and LGBT people would be the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 停止偏见流入AI系统至关重要。偏见会妨碍系统支持少数群体（包括儿童）发展的不同路径。值得信赖、平等机会导向的AI系统应避免刻板印象，为每个孩子（包括女孩和LGBT儿童）提供可能性。一些围绕儿童、少数民族和LGBT人群的行动包括以下内容：
- en: Prioritize fairness and non-discrimination among all minorities who are vulnerable,
    such as children, the black community, and LGBT people.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先考虑所有弱势群体的公平与非歧视，包括儿童、黑人群体和LGBT人群。
- en: Safeguard children’s data and privacy rights to ensure their safety.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保障儿童数据和隐私权，以确保他们的安全。
- en: Increase transparency, explainability, and accountability for children so that
    the moral rights and dignity of children are preserved.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高儿童数据的透明度、可解释性和问责性，以确保儿童的道德权利和尊严得以保护。
- en: Empower governments and businesses with knowledge of AI and children’s rights.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赋能政府和企业，提升他们对AI和儿童权利的知识。
- en: Prepare minority groups by educating them so that they are aware of the harmful
    impacts of biased AI solutions when deployed at scale.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过教育准备少数群体，使他们意识到在大规模部署时，偏见AI解决方案可能带来的有害影响。
- en: Enable an AI-driven ecosystem to support and nurture individual talent to fulfill
    its full potential. This necessitates the elimination of any prejudicial bias
    against children, or certain groups of children, that directly or indirectly leads
    to biased treatment.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持并培育个体才能的AI驱动生态系统，以实现其最大潜力。这需要消除任何针对儿童或特定群体儿童的偏见，这些偏见直接或间接导致了偏见对待。
- en: AI must be for everyone, including all children; also, privacy must be ensured
    in an AI world.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: AI必须面向所有人，包括所有儿童；同时，在AI世界中必须确保隐私得到保护。
- en: Educational initiatives
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 教育倡议
- en: The initiatives that have been organized to educate institutions, educational
    agencies, and data professionals are the first step toward making people aware
    of the existence of AI systems and their risk levels ([https://www.orrick.com/en/Insights/2021/07/AI-Tips-10-Steps-to-Future-Proof-Your-Artificial-Intelligence-Regulatory-Strategy](https://www.orrick.com/en/Insights/2021/07/AI-Tips-10-Steps-to-Future-Proof-Your-Artificial-Intelligence-Regulatory-Strategy)).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 已组织的教育机构、教育机构和数据专业人员的倡议是让人们意识到AI系统及其风险水平的第一步（[https://www.orrick.com/en/Insights/2021/07/AI-Tips-10-Steps-to-Future-Proof-Your-Artificial-Intelligence-Regulatory-Strategy](https://www.orrick.com/en/Insights/2021/07/AI-Tips-10-Steps-to-Future-Proof-Your-Artificial-Intelligence-Regulatory-Strategy)）。
- en: One of the primary purposes of such educational initiatives is to educate AI
    practitioners on how to identify high, medium, and low risks and take appropriate
    action to evaluate an AI model’s predictive competence.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这类教育项目的主要目的是教育人工智能从业者如何识别高、中、低风险，并采取适当的行动来评估人工智能模型的预测能力。
- en: Identifying and evaluating the risk levels of systems
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 识别和评估系统的风险等级
- en: 'High-risk AI systems should be banned for public use, and their usage can be
    limited for research and future improvement. AI-enabled systems that should be
    banned include those that do the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 高风险人工智能系统应禁止用于公共领域，其使用可限于研究和未来的改进。这些应当被禁止的人工智能系统包括：
- en: Manipulate human behavior, opinions, or decisions through their design (such
    as their user interfaces), architectural frameworks, and AI models, which can
    lead to detrimental decisions against individuals.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过其设计（如用户界面）、架构框架和人工智能模型操控人类行为、观点或决策，这可能导致对个体的不利决策。
- en: Make predictions about an individual or a group based on protected attributes
    (such as gender and ethnicity).
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于受保护属性（如性别和种族）对个人或群体进行预测。
- en: Execute indiscriminate surveillance on all individuals without any differentiation
    with respect to protected attributes. General surveillance can lead to the monitoring
    of, spying on, or tracking of individuals, without receiving user consent.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对所有个人进行不加区分的监控，不考虑受保护属性。一般监控可能导致对个人的监视、间谍行为或跟踪，而无需获得用户同意。
- en: 'In addition to putting best practices in place, we should be able to identify
    high-risk systems. Along with general public education, we need to educate experienced
    data professionals who are building large-scale ML systems. Only then will we
    be able to prevent the default prediction of, say, a “captain” as a man. To do
    that, we need to remove probabilistic bias, as illustrated in the following figure
    for a word embedding model ([https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html](https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html)):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 除了实施最佳实践外，我们还应能够识别高风险系统。除了普及公众教育外，我们还需要教育经验丰富的数据专业人士，他们正在构建大规模的机器学习系统。只有这样，我们才能防止默认预测“船长”是男性。例如，要做到这一点，我们需要消除概率偏见，如下图所示，这是针对词嵌入模型的一个示例（[https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html](https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html)）：
- en: '![Figure 3.5 – Higher probability of a captain being predicted as male](img/B18681_03_005.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – 更高的概率预测“船长”为男性](img/B18681_03_005.jpg)'
- en: Figure 3.5 – Higher probability of a captain being predicted as male
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 更高的概率预测“船长”为男性
- en: As you can see, the model predicts that the pronoun “he” is more likely to be
    appropriate for this scenario.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，模型预测“他”这个代词在此情境下更可能是合适的。
- en: Understanding high-risk systems
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解高风险系统
- en: 'Systems classified as high-risk enable the government and authorities to take
    measures such as adding security techniques and conducting audits to identify
    the risks. AI-enabled systems that are identified as high-risk are those that
    do the following:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 被分类为高风险的系统使政府和相关部门能够采取措施，如增加安全技术和进行审计，以识别风险。被认定为高风险的人工智能系统包括以下几种：
- en: Perform remote biometric identification of people in publicly accessible spaces.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在公众可访问的空间内进行远程生物识别。
- en: Function as safety components in the operation of essential public infrastructure
    networks.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为关键公共基础设施网络运作中的安全组件。
- en: Prioritize the dispatching of emergency services, such as firefighters and medical
    aid.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先调度紧急服务，如消防员和医疗援助。
- en: Determine merit access and assign individuals to educational and vocational
    training institutions through admissions tests and other merit assessments.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过入学考试和其他成绩评估，决定 Merit 访问权限并将个人分配到教育和职业培训机构。
- en: Evaluate performance related to recruitment, promotion, rewards, and the termination
    of any kind of contractual relationship.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估与招聘、晋升、奖励及任何类型合同关系终止相关的绩效。
- en: 'Evaluate the creditworthiness of people to certify their eligibility for benefits,
    grants, and services. This principle aims to avoid discrepancies in the creditworthiness
    of individuals by reducing, for example, the likelihood of higher mortgage rates
    being assigned to black and Hispanic people, as shown in *Figure 3**.6*:'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估个人的信用worthiness，以认证其资格获得福利、资助和服务。此原则旨在通过减少例如向黑人和西班牙裔人群分配更高抵押贷款利率的可能性，避免个人信用worthiness的不一致，正如*图
    3.6*所示。
- en: '![Figure 3.6 – A biased system resulting in some minorities paying higher mortgage
    amounts](img/B18681_03_006.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – 一个偏见系统导致某些少数群体支付更高的抵押贷款金额](img/B18681_03_006.jpg)'
- en: Figure 3.6 – A biased system resulting in some minorities paying higher mortgage
    amounts
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 一个偏见系统导致某些少数群体支付更高的抵押贷款金额
- en: Access individual risks and use predictions to be proactive in governing the
    risk. The central objective is to determine the authenticity of the information
    provided by a person. Transparent, reliable systems should have proper validation
    schemes incorporated to prevent, investigate, detect, or prosecute a criminal
    offense.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估个人风险并利用预测积极管理这些风险。核心目标是确定个人提供信息的真实性。透明且可靠的系统应当包含适当的验证机制，以防止、调查、侦测或起诉犯罪行为。
- en: Function as prediction systems for crimes or events of social unrest to evaluate
    and assign resources for monitoring and surveillance as part of a detailed criminal
    investigation.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为犯罪或社会动荡事件的预测系统，评估并分配监控和监视资源，作为详细刑事调查的一部分。
- en: Examine and enable the processing of asylum and visa applications and associated
    complaints to determine the eligibility of individuals to enter specific countries/territories,
    such as the EU.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审查并启用处理庇护和签证申请及相关投诉，以确定个人进入特定国家/地区的资格，例如欧盟。
- en: Aid legal systems, such as judges at court, except for ancillary tasks.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协助法律系统，如法庭上的法官，但不包括辅助任务。
- en: Access safety metrics before deploying drones in a warzone. One current example
    of this, at the time of writing, is in the Ukraine-Russia war, where Russian forces
    have tested new “swarm” drones. It comes under the umbrella of military ethics
    to evaluate the accuracy of these automated weapons, which are capable of tracking
    and shooting down enemy aircraft.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在战区部署无人机之前，访问安全指标。当前的一个例子是乌克兰-俄罗斯战争，俄罗斯部队已经测试了新的“蜂群”无人机。这属于军事伦理的范畴，评估这些自动化武器的准确性，这些武器能够跟踪并击落敌方飞行器。
- en: Such high-risk systems mandate that the training and testing datasets used in
    building the ML models work using authentic and unbiased representations. This
    is to allow sufficient transparency to understand their outputs and necessitates
    the inclusion of proper human-interface tools. In addition, these systems require
    consistent performance and system monitoring throughout the life cycle, along
    with ensuring high accuracy, robustness, and security. Non-high-risk systems should
    also be subjected to national testing and piloting schemes.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这类高风险系统要求用于构建机器学习模型的训练和测试数据集必须使用真实且公正的表示。这是为了确保足够的透明度，以便理解其输出，并且需要包括适当的人机交互工具。此外，这些系统要求在整个生命周期内保持一致的性能和系统监控，并确保高精度、鲁棒性和安全性。非高风险系统也应接受国家级的测试和试点方案。
- en: 'EU regulations have stressed the importance of ethical risk assessment procedures
    for AI systems where misuse can be foreseen and risks arise due to limited knowledge
    of application, control failures, and hazards associated with robotics applications.
    Risk assessment frameworks should apply the same acceptable risk standards to
    AI-based robotic applications as they do to tasks performed by a human. Standards
    set forth by EU regulations encourage public and stakeholder participation in
    the development of robots; some of the key design considerations for AI models
    and robots are as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟法规强调了对人工智能系统进行伦理风险评估程序的重要性，尤其是当预见到滥用行为并且由于应用知识有限、控制失效以及与机器人应用相关的风险而产生风险时。风险评估框架应当将与人类执行任务相同的可接受风险标准应用于基于人工智能的机器人应用。欧盟法规设定的标准鼓励公众和利益相关者参与机器人开发；人工智能模型和机器人设计中的一些关键考虑因素如下：
- en: Forbid the design of AI systems and robots that would be responsible for societal
    harm. This ensures that AI equipment invented and manufactured by humans does
    not kill humans. Robots should be safe and designed to serve the purpose for which
    they are intended, without having any hidden motivations.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 禁止设计会对社会造成伤害的 AI 系统和机器人。这样可以确保人类发明和制造的 AI 设备不会伤害人类。机器人应该是安全的，并且设计用来服务于其预定目的，且不带有任何隐藏动机。
- en: Hold responsible the designers or manufacturers of AI-enabled robots.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要追究具备 AI 能力的机器人的设计师或制造商的责任。
- en: Incorporate privacy in every design that deals with data, AI, ML, and models
    as long as the system is available to the public.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个涉及数据、AI、ML 和模型的设计中都要融入隐私保护，前提是该系统对公众开放。
- en: Allow the safe and ethical use of robots, so as to prohibit their unethical
    behavior through detailed accounting.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许机器人在安全且伦理的前提下使用，通过详细的核算禁止其不道德行为。
- en: Run educative training programs for AI robot designers, individual and organizational
    users, and industry and government bodies, as well as civil society groups.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 AI 机器人设计师、个体和组织用户、行业与政府部门以及民间社会团体开展教育培训项目。
- en: Autonomous robots should be categorized, based on their behavior, as either
    **illegal**, **immoral**, **permissible**, or **supererogatory**, allowing them
    to be withdrawn from service according to regulatory guidelines.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自主机器人应根据其行为进行分类，分为**非法**、**不道德**、**允许的**或**过度的**，并根据监管指南将其撤离服务。
- en: International AI initiatives and cooperative actions
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 国际 AI 举措与合作行动
- en: International initiatives are undertaken to provide harmony and synchronization
    to educate people across different nations. The primary aim is to establish a
    universal standard to raise awareness of AI ethics and support research and technology
    collaboration. This enables organizations across different countries to join hands
    and contribute to establishing synergy in the initiatives.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 国际举措旨在为不同国家的人们提供和谐与同步的教育。其主要目标是建立一个普遍的标准，提高对 AI 伦理的意识，并支持研究与技术合作。这使得不同国家的组织能够携手合作，推动举措中的协同效应。
- en: 'Some of the international strategies on AI aim to resolve the differences between
    humans due to protected attributes. In an attempt to provide a consolidated framework
    for governments across nations, they strongly uphold a “*common vision for the
    Future of AI. G7 Common Vision is one of those, where the leaders of the G7 (Canada,
    France, Germany, Italy, Japan, the United Kingdom and the United States) met in
    2018 and committed to 12 principles for AI*” ([https://www.europarl.europa.eu/RegData/etudes/STUD/2020/634452/EPRS_STU(2020)634452_EN.pdf](https://www.europarl.europa.eu/RegData/etudes/STUD/2020/634452/EPRS_STU(2020)634452_EN.pdf))
    by the following means:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一些关于 AI 的国际策略旨在解决因保护属性而导致的人类差异。为了为各国政府提供一个统一的框架，它们坚决支持“*共同的 AI 未来愿景*。G7 共同愿景就是其中之一，G7（加拿大、法国、德国、意大利、日本、英国和美国）的领导人在
    2018 年会面，承诺遵循 12 条 AI 原则*” ([https://www.europarl.europa.eu/RegData/etudes/STUD/2020/634452/EPRS_STU(2020)634452_EN.pdf](https://www.europarl.europa.eu/RegData/etudes/STUD/2020/634452/EPRS_STU(2020)634452_EN.pdf))，并通过以下方式实现：
- en: Support, lead, and educate human-centric AI through commercial adoption in a
    systematic way that allows technology to be implemented in an ethical and trustworthy
    manner.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过商业化应用，支持、领导并教育以人为本的 AI，采用系统化方式，确保技术以伦理且可信的方式实施。
- en: Allocate funds to encourage investment in R&D so that, along with the efforts
    of researchers, architects, and data professionals, it generates public interest
    in new technologies.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分配资金以鼓励投资于研发，以便在研究人员、建筑师和数据专家的努力下，激发公众对新技术的兴趣。
- en: Educate, train, and reskill individuals for the workforce to help them adapt
    to emerging innovative technologies.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 教育、培训并提升个人的技能，以帮助他们适应新兴的创新技术。
- en: Support campaigns and research studies to investigate the sources of bias for
    underrepresented groups, such that sufficient data for women and marginalized
    individuals is taken into consideration before building models.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 支持开展活动和研究，调查欠代表群体的偏见来源，确保在建立模型之前，充分考虑到女性和边缘化群体的数据。
- en: The WHO’s ethics and governance team for AI ([https://news.un.org/en/story/2021/06/1094902](https://news.un.org/en/story/2021/06/1094902))
    for health has reported that AI in healthcare is already being used among rich
    nations to aid the speed and accuracy of diagnosis and screening for diseases.
    The use of AI in clinical care and improved health research and drug development
    should involve mandatory safety and privacy mechanisms that respect the confidentiality
    of patient data. Furthermore, the WHO stresses the need to develop systems that
    comply with regulations, have robust quality control measures, and provide accurate
    predictions for all individuals irrespective of age, gender, ethnicity, and features
    protected by human rights codes.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 世界卫生组织（WHO）关于健康AI的伦理与治理团队（[https://news.un.org/en/story/2021/06/1094902](https://news.un.org/en/story/2021/06/1094902)）报告称，富裕国家已经在医疗保健中使用AI来加速和提高疾病的诊断和筛查的准确性。在临床护理中使用AI和改善健康研究和药物开发应包括强制性的安全和隐私机制，尊重患者数据的保密性。此外，WHO强调需要开发符合法规、具有健全质量控制措施并为所有人提供准确预测的系统，无论年龄、性别、种族或人权法典保护的其他特征如何。
- en: The UN General Assembly resolution UN A/RES/74/299 and the AI for Road Safety
    initiative have aligned to highlight the role of innovative automotive and digital
    technologies in reducing the number of global deaths and injuries from road traffic
    accidents by more than 50% by 2030 ([https://news.un.org/en/story/2021/10/1102522](https://news.un.org/en/story/2021/10/1102522)).
    The objective is to concentrate on road safety data, regulatory frameworks, and
    the design of safer vehicles and road infrastructure to yield a much easier post-crash
    response. The International Telecommunication Union has also been working with
    AI for Road Safety to support continuous monitoring and automated driving safety
    data protocols that are aligned with ethical and legal bodies.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 联合国大会第74/299号决议和AI道路安全倡议已经对创新的汽车和数字技术在2030年之前减少全球道路交通事故死亡和伤害人数超过50%的角色进行了对齐（[https://news.un.org/en/story/2021/10/1102522](https://news.un.org/en/story/2021/10/1102522)）。其目标是集中在道路安全数据、监管框架以及设计更安全的车辆和道路基础设施，以实现更容易的事故后响应。国际电信联盟也与AI道路安全合作，支持与伦理和法律机构一致的持续监控和自动驾驶安全数据协议。
- en: We even see ads promoted by Facebook (such as one in 2019) that are biased against
    gender, race, or religion. Facebook has been active “*in promoting job advertisements
    for roles in nursing or secretarial work [to women], whereas job ads for janitors
    and taxi drivers had been mostly shown to men*” ([https://research.aimultiple.com/ai-bias/](https://research.aimultiple.com/ai-bias/)).
    These examples demonstrate how important it is for organizations to educate strategic,
    data, and architectural teams to abide by laws and follow the right practices
    to build and release ethical systems.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们看到Facebook推广的广告（例如2019年的一则广告），也存在对性别、种族或宗教有偏见的情况。Facebook积极推广“*向女性推广护士或秘书工作的工作广告，而把清洁工和出租车司机的工作广告主要展示给男性*”（[https://research.aimultiple.com/ai-bias/](https://research.aimultiple.com/ai-bias/)）。这些例子展示了教育组织战略、数据和架构团队遵守法律并遵循正确实践以建立和发布道德系统的重要性。
- en: Implications of law enforcement
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执法的法律影响
- en: The **European** **Commission for the Efficiency of Justice (CEPEJ)** of the
    Council of Europe has been playing a leading role in setting ethical principles
    pertaining to the use of AI in judicial systems to guide policy-makers, legislators,
    and justice professionals ([https://www.coe.int/en/web/cepej/cepej-european-ethical-charter-on-the-use-of-artificial-intelligence-ai-in-judicial-systems-and-their-environment](https://www.coe.int/en/web/cepej/cepej-european-ethical-charter-on-the-use-of-artificial-intelligence-ai-in-judicial-systems-and-their-environment)).
    The aim is to apply AI responsibly to improve efficiency and quality in a manner
    that complies with the fundamental rights guaranteed in the **European Convention
    on Human Rights (ECHR)** and the Council of Europe’s Convention on the Protection
    of Personal Data.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 欧洲理事会的**欧洲司法效率委员会（CEPEJ）**在为司法系统中AI使用设置伦理原则方面发挥着领导作用，以指导政策制定者、立法者和司法专业人员（[https://www.coe.int/en/web/cepej/cepej-european-ethical-charter-on-the-use-of-artificial-intelligence-ai-in-judicial-systems-and-their-environment](https://www.coe.int/en/web/cepej/cepej-european-ethical-charter-on-the-use-of-artificial-intelligence-ai-in-judicial-systems-and-their-environment)）。其目标是以符合**欧洲人权公约（ECHR）**和欧洲理事会个人数据保护公约所保障的基本权利为前提，负责任地应用AI，以改善效率和质量。
- en: With regard to privacy standards put forward by the **European Data Protection
    Board (EDPB),** a violation of GDPR can result in a fine of 30 million euros or
    6% of the organization's global annual turnover (whichever is higher). The same
    penalty is expected in the case of a breach of an unacceptable-risk AI system
    or infringement of the data governance provisions for high-risk AI systems. The
    maximum fines can increase based on the degree of infringement, such as releasing
    a prohibited AI application on the market. Other kinds of infringements related
    to issuing incorrect information would result in fines of up to 10 million euros,
    or 2% of the global annual turnover. The fines may be as high as 20 million euros,
    or 4% of the total worldwide annual turnover, for acts of non-compliance.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 关于**欧洲数据保护委员会（EDPB）**提出的隐私标准，违反GDPR可能导致高达3000万欧元的罚款或占组织全球年营业额6%的罚款（以较高者为准）。如果违反了不可接受风险的AI系统或高风险AI系统的数据治理条款，也会面临同样的处罚。根据侵权程度，罚款最高可能会增加，比如将禁止的AI应用推向市场。其他种类的违法行为，如发布不正确信息，将导致最高1000万欧元的罚款，或占全球年营业额2%的罚款。对于不遵守规定的行为，罚款可能高达2000万欧元，或占全球年营业额4%的罚款。
- en: One of the foremost examples where regulations and ethical AI laws can be applied
    is in the usage of unethical AI tools such as the biased recruiting tool that
    was revealed by Amazon’s ML specialists. Amazon realized in 2015 that the AI-based
    tool they had in place to aid in their hiring process was not rating candidates
    in a gender-neutral fashion for software developer and other technical positions.
    The tool eventually turned out to make biased decisions against women. On further
    investigation, the organization found out that ML models were trained to vet applicants
    that were solely based on the observed patterns that were submitted in the company
    database over a 10-year period. As the tool penalized resumes containing words
    such as “women’s,” the organization decided to scrap this hiring tool and disband
    the team working on the project.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 一个最典型的可以应用法规和道德AI法律的例子，是不道德AI工具的使用，例如亚马逊的机器学习专家揭露的偏见招聘工具。亚马逊在2015年意识到，他们用来帮助招聘过程的基于AI的工具，在软件开发人员和其他技术职位的候选人评估中，并没有以性别中立的方式进行评分。最终，这个工具对女性做出了有偏见的决策。经过进一步调查，组织发现机器学习模型的训练是基于公司数据库中提交的10年期间的观察模式，来筛选申请人。由于该工具惩罚了包含“女性”一词的简历，组织决定废弃这个招聘工具，并解散了负责该项目的团队。
- en: In these sections, we have learned about the AI regulations formulated by the
    EU and the acts and initiatives being undertaken by the US, Australia, and other
    international bodies. Now, let’s look at how organizations, companies, and AI
    researchers should build trustworthy AI systems.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些部分中，我们已经了解了欧盟制定的AI监管法规，以及美国、澳大利亚和其他国际机构正在采取的行动和倡议。现在，让我们来看看组织、公司和AI研究人员应该如何构建可信赖的AI系统。
- en: Next steps for trustworthy AI
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可信赖AI的下一步
- en: At the time of writing this book, more than 50 countries have adopted some form
    of AI regulation policy ([https://oecd.ai/en/dashboards](https://oecd.ai/en/dashboards)).
    Still, like AI itself, forming a robust policy around AI is a difficult task,
    made more difficult because of the evolving nature of AI itself. However, there
    are some steps put forward by AI researchers and policy-makers that can help provide
    trustworthy AI to consumers.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时，已有超过50个国家采取了某种形式的AI监管政策（[https://oecd.ai/en/dashboards](https://oecd.ai/en/dashboards)）。然而，像AI本身一样，围绕AI制定一项稳健的政策是一项艰巨的任务，这个任务因AI本身的不断发展而变得更加困难。然而，AI研究人员和政策制定者提出了一些步骤，可以帮助向消费者提供可信赖的AI。
- en: One of the main steps for companies is crafting organization-wide policies and
    procedures to create a compliance-by-design program. Such programs help to promote
    the transparency and explainability of systems and support innovation at the same
    time. Furthermore, they should establish a regular audit-and-review process to
    analyze usage regularly and document all such processes. A design-and-review process
    should be established. This would entail having a process built to respond to
    any questions from regulators seeking additional information. In addition to the
    principles stated in the preceding sections formulated by the EU or the regulatory
    bodies of respective countries, companies also need to engage in PR/external communication
    to raise awareness of their products and connect potential customers and partners
    to revenue-generating channels.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 企业的主要步骤之一是制定全公司范围的政策和程序，以创建合规设计方案。这样的方案有助于促进系统的透明度和可解释性，同时支持创新。此外，还应建立定期的审计和复审流程，定期分析使用情况并记录所有相关过程。应建立设计和审查流程，确保能够回应监管机构提出的额外信息要求。除了欧盟或各国监管机构提出的前述原则外，企业还需要通过公关/外部沟通提高产品意识，并将潜在客户和合作伙伴连接到收入渠道。
- en: 'Organizations should continuously seek to improve the fairness, integrity,
    privacy, and accuracy of any AI system. Some of the recommended best practices
    laid down by Google include the following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 组织应持续寻求改善任何 AI 系统的公平性、完整性、隐私性和准确性。Google 提出的部分推荐最佳实践包括：
- en: Equip all AI-enabled systems with a human-centered design approach so that they
    focus on user experience and account for the diversity of users and use cases.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让所有具备 AI 能力的系统采用以人为本的设计方法，以便它们专注于用户体验，并考虑到用户和使用场景的多样性。
- en: Identify and monitor multiple governance criteria to evaluate training and set
    up proactive measures based on monitoring results. This helps to determine the
    usefulness of model metrics to the goals of the AI system.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别并监控多个治理标准以评估训练，并根据监控结果设置主动措施。这有助于判断模型指标对 AI 系统目标的相关性。
- en: Examine raw data to assess the evaluation metrics, including accuracy, and judge
    the predictive capabilities of the system.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查原始数据以评估评估指标，包括准确性，并判断系统的预测能力。
- en: Ascertain the limits of the dataset and model, propagate these limitations across
    departments, and follow the recommended practices to remove bias and ensure fairness
    in terms of the input data.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定数据集和模型的限制，将这些限制传达至各部门，并遵循推荐的做法去除偏见，确保输入数据的公平性。
- en: Undertake rigorous, diverse, and regular integration and unit testing of AI
    systems.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对 AI 系统进行严格、多样化和定期的集成测试和单元测试。
- en: Put in place regular monitoring and update techniques for all AI systems after
    deployment to consider real-world performance and user feedback.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在部署后，针对所有 AI 系统制定定期监控和更新技术，以考虑实际表现和用户反馈。
- en: 'The increasing adaptation of AI and solutions for minorities, including children,
    has led ethicists and data professionals to realize the need for unbiased AI systems.
    The goal is to uphold children’s collective right to protection and increase the
    participation of children from all backgrounds. This holds good for any AI-based
    systems with which children interact, irrespective of whether or not the system
    was designed for children. Some of the principles that organizations should infuse
    in their AI system development value proposal and technical know-how are as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: AI 和少数群体（包括儿童）解决方案的日益普及使得伦理学家和数据专业人士认识到需要去偏见的 AI 系统。目标是维护儿童的集体保护权，并增加各背景儿童的参与。这适用于任何儿童互动的
    AI 系统，无论该系统是否专门为儿童设计。组织在 AI 系统开发的价值提案和技术方案中应注入的部分原则如下：
- en: Include specially designed KPIs and metrics that can be integrated into AI-enabled
    systems to monitor and track children’s well-being. As children are influenced
    by AI systems during their interactions with those systems, the design of such
    systems should pay special attention to well-being frameworks and metrics. Furthermore,
    such systems should be tested on children and success parameters should be closely
    linked to children’s well-being and development.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含特别设计的关键绩效指标（KPIs）和度量标准，可以集成到启用人工智能的系统中，以监控和跟踪儿童的福祉。由于儿童在与人工智能系统互动时会受到其影响，因此这些系统的设计应特别关注福祉框架和度量标准。此外，这些系统应在儿童身上进行测试，成功的参数应与儿童的福祉和发展紧密相关。
- en: Formulate AI policies and strategies by evaluating how AI systems can benefit
    children. This will help to ascertain the overall benefits received by children,
    as existing policies and strategies already evaluate the risks associated when
    children interact with such systems.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制定人工智能政策和战略，评估人工智能系统如何惠及儿童。这将有助于确定儿童所获得的整体益处，因为现有的政策和战略已经评估了儿童与此类系统互动时所涉及的风险。
- en: Adopt design styles that protect children’s rights by placing the child at the
    center of ethical AI policy and system design, development, and deployment. Protecting
    children’s rights helps children’s development and well-being by enforcing privacy
    by design, safety by design, and inclusion by design.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用保护儿童权利的设计风格，将儿童置于伦理人工智能政策和系统设计、开发与部署的核心。通过设计保障隐私、安全和包容性来保护儿童权利，有助于促进儿童的成长和福祉。
- en: Design AI systems such that children’s development opportunities and rights
    to health, education, clean air, water, and safety are ensured. For example, AI-based
    educational and recommendation systems developed for children should not display
    advertisements that are not beneficial to children’s education and intellectual
    growth. AI-based systems should increase environmental sustainability and not
    impact the environment negatively. Any negative impact, such as an increase in
    carbon footprint, would seriously impact the well-being of children and their
    ability to live on a sustainable and healthy planet. The training, deployment,
    and computational infrastructure of AI systems should be tracked using carbon
    emission metrics to combat climate change, devise mitigation strategies, and promote
    better ML modeling.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计人工智能系统，确保儿童的健康、教育、清洁空气、水和安全等发展机会和权利。例如，专为儿童开发的基于人工智能的教育和推荐系统，不应显示不利于儿童教育和智力发展的广告。基于人工智能的系统应增加环境可持续性，而不是对环境产生负面影响。任何负面影响，例如碳足迹的增加，将严重影响儿童的福祉及其在可持续和健康的星球上生活的能力。应通过碳排放度量标准跟踪人工智能系统的训练、部署和计算基础设施，以应对气候变化，制定缓解策略，并推动更好的机器学习建模。
- en: Promote the explainability and transparency of AI systems by addressing children’s
    needs, such as the use of age-appropriate language to describe AI. This would
    add more meaning to AI systems’ explainability and make them transparent. Furthermore,
    the AI application should empower child users according to legal and policy frameworks,
    principles, and regulations. There should also be a space for redressal.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过满足儿童需求，推动人工智能系统的可解释性和透明度，例如使用适龄的语言来描述人工智能。这将为人工智能系统的可解释性增添更多意义，使其更加透明。此外，人工智能应用应根据法律、政策框架、原则和规定赋能儿童用户，并应为救济提供空间。
- en: Develop systems to facilitate necessary skills among children and teachers to
    prepare children for present and future developments in AI. Furthermore, funding
    and incentives for child-centered AI policies and strategies should aim for infrastructure
    development and bridge the digital divide in a way that supports the equitable
    sharing of the benefits of AI. The end goal is to build a solid foundation of
    child-centered AI that includes **protection (do no harm)**, **provision (do good)**,
    and **participation (include** **all children)**.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发系统，帮助儿童和教师培养必要的技能，以便为儿童准备应对人工智能的当前和未来发展。此外，针对儿童中心化的人工智能政策和战略的资助和激励应侧重于基础设施建设，并通过支持人工智能益处的公平共享来弥合数字鸿沟。最终目标是建立一个以儿童为中心的人工智能的坚实基础，其中包括**保护（不造成伤害）**、**提供（做出积极贡献）**和**参与（包括所有儿童）**。
- en: 'Despite the identification of the next steps in successfully building next-generation
    ethical AI systems, some direct challenges to government adoption of AI have been
    cited by the World Economic Forum ([https://www.weforum.org/agenda/2019/08/artificial-intelligence-government-public-sector/](https://www.weforum.org/agenda/2019/08/artificial-intelligence-government-public-sector/)).
    The main roadblocks that were identified include the following:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经确定了成功构建下一代伦理 AI 系统的下一步措施，但世界经济论坛指出了一些直接影响政府采用 AI 的挑战（[https://www.weforum.org/agenda/2019/08/artificial-intelligence-government-public-sector/](https://www.weforum.org/agenda/2019/08/artificial-intelligence-government-public-sector/))。其中识别出的主要障碍包括：
- en: '**Effective use of data**: There’s a lack of understanding of the value of
    data and it is rarely deployed in a scalable infrastructure. There is also a lack
    of implementation of data governance processes.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据的有效使用**：缺乏对数据价值的理解，且数据很少在可扩展的基础设施中部署。同时，数据治理流程的实施也存在不足。'
- en: '**Data and AI skills**: Governments, and other government-funded organizations,
    face a shortage of funds to hire expert data professionals that many big private
    companies can afford. This gap in attracting talent affects the quality of the
    AI solutions produced.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据和 AI 技能**：政府及其他政府资助的组织面临资金短缺，无法雇佣许多大公司能够负担得起的专家数据专业人员。这种吸引人才的差距影响了生产的 AI
    解决方案的质量。'
- en: '**AI ecosystem**: Companies operating in the AI market experience frequent
    shifts in the problem they are solving and how customers see the solutions. Start-ups
    pioneering AI solutions also experience a dearth of talent to scale ML models
    ethically for large projects.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI 生态系统**：在 AI 市场中运营的公司经历了频繁的变化，包括他们所解决的问题和客户对解决方案的看法。开创 AI 解决方案的初创企业也面临着人才短缺的问题，难以为大规模项目伦理地扩展机器学习模型。'
- en: '**Legacy culture**: Governments and government-funded organizations have difficulty
    in adopting transformative technology, mostly because employees are not educated
    on ethical principles and are reluctant to take risks.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗留文化**：政府及政府资助的组织在采用变革性技术方面遇到困难，主要原因是员工没有接受伦理原则的教育，并且不愿冒险。'
- en: '**Procurement mechanisms**: ML-based solutions and systems are often treated
    as intellectual property with terms and conditions, which may make it difficult
    for governments to customize existing solutions within the stipulated time.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采购机制**：基于机器学习的解决方案和系统通常被视为知识产权，具有条款和条件，这可能使政府在规定的时间内定制现有解决方案变得困难。'
- en: Existing challenges and gaps
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 现有的挑战和空白
- en: The frameworks proposed by different ethical associations and bodies have addressed
    the moral and ethical dilemmas identified and presented previously, but there
    are still a few gaps to address.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 不同伦理协会和机构提出的框架已解决了之前识别和呈现的道德和伦理困境，但仍有一些空白需要填补。
- en: "![Figure 3.7 – How human psychology plays a role in determining profession,\
    \ exhibiting bias toward\uFEFF women on the left and bias toward men on the right](img/B18681_03_007.jpg)"
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.7 – 人类心理在决定职业中的作用，左侧偏向女性，右侧偏向男性](img/B18681_03_007.jpg)'
- en: Figure 3.7 – How human psychology plays a role in determining profession, exhibiting
    bias toward women on the left and bias toward men on the right
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – 人类心理在决定职业中的作用，左侧偏向女性，右侧偏向男性
- en: Along with the OECD, the EU has stressed the importance of societal and environmental
    well-being (including sustainability and environmental friendliness) and formulated
    ethics guidelines on the principle of prevention of harm. However, it does not
    provide examples of how to achieve this.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 与经济合作与发展组织（OECD）一道，欧盟强调了社会和环境福祉（包括可持续性和环保）的重要性，并根据防止伤害的原则制定了伦理指南。然而，它没有提供如何实现这一目标的示例。
- en: Ethics bodies have also looked at the impacts on human psychology of using AI
    and how people interact with AI. This is demonstrated in *Figure 3**.7*, which
    shows how our existing social structure tends to affect how we perceive different
    professions. Hence, it is evident that the EU needs to reposition social relationships
    within the context of ethics so that the data available in our society (demonstrating
    biased decisions concerning different protected attributes, as shown in the figure)
    does not end up training ML systems, consequently leading to circular bias. The
    *x* axis here represents how different professions tend to be associated with
    men, women, or both.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 伦理委员会也关注使用AI对人类心理的影响，以及人们与AI的互动方式。这一点在*图 3.7*中得到了体现，图中展示了现有的社会结构如何影响我们对不同职业的看法。因此，很明显，欧盟需要在伦理的背景下重新定位社会关系，以确保社会中可用的数据（展示了关于不同受保护属性的偏见决策，如图所示）不会最终训练机器学习系统，从而导致循环偏见。此处的*x*轴表示不同职业如何倾向于与男性、女性或两者相关联。
- en: The EU draft on Responsible AI emphasizes the need for constant monitoring to
    evaluate AI interaction with humans and sets forth that in the event of such interactions,
    the system should be well tested against all types of social interactions through
    creative simulations. Such simulations can create different scenarios involving
    human and robot interactions. Even this specification is not sufficient to provide
    safety and protection in the long run, as human-robot relationships are complex
    and have an impact on the human psyche. Such complex interactions need to be considered
    in the creation of next-generation robotic autonomous AI systems.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟关于负责任AI的草案强调了持续监控的必要性，以评估AI与人类的互动，并规定在此类互动发生时，系统应通过创造性模拟进行全面测试，模拟所有类型的社会互动。这些模拟可以创建涉及人类与机器人互动的不同场景。即使这一规范也不足以在长期内提供安全性和保护，因为人类与机器人关系复杂并对人类心理产生影响。这些复杂的互动需要在下一代机器人自动化AI系统的创建中加以考虑。
- en: 'As *Figure 3**.8* illustrates, there is still fear among pedestrians about
    self-driving autonomous vehicles. One of the primary elements of future autonomous
    vehicle systems is using real test data to increase the percentage of pedestrians
    who feel safe with self-driving autonomous cars. In addition, accountability is
    also an important factor, in case of any harm caused by these vehicles. Hence,
    quantifying safety and protection remains one of the main tasks when defining
    trustworthy AI systems in the future:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 3.8*所示，行人对于自动驾驶汽车仍然存在恐惧感。未来自动驾驶汽车系统的主要元素之一是利用真实的测试数据，增加那些感到自动驾驶汽车安全的行人的比例。此外，责任问题也是一个重要因素，以防这些车辆造成任何伤害。因此，量化安全性和保护性仍然是未来定义可信AI系统时的主要任务之一：
- en: '![Figure 3.8 – Safety concerns among pedestrians about self-driving cars](img/B18681_03_008.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.8 – 行人对自动驾驶汽车的安全担忧](img/B18681_03_008.jpg)'
- en: Figure 3.8 – Safety concerns among pedestrians about self-driving cars
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 – 行人对自动驾驶汽车的安全担忧
- en: Proposed solutions and improvement areas
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提出的解决方案和改进领域
- en: 'The art of making an AI solution public comes with several challenges and questions.
    To address the questions, experiments were conducted by the French Council of
    State and other countries of the EU, as well as the US, to implement four different
    types of regulations. These regulations, described here, attempt to develop and
    certify authentic ethical AI solutions:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 公布AI解决方案的艺术面临着多个挑战和问题。为了回应这些问题，法国国务委员会及其他欧盟国家和美国开展了实验，实施了四种不同类型的法规。这里描述的这些法规试图开发并认证真实的伦理AI解决方案：
- en: Algorithmic Accountability Act-driven regulation
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由算法问责法案驱动的法规
- en: Regulations proposed by government regulators, including the **Food and Drug
    Administration** (**FDA**) (for the healthcare industry), the **National Highway
    Traffic Safety Administration** (**NHTSA**) (transportation), and the **Federal
    Trade Commission** (**FTC**) (retail)
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 政府监管机构提出的法规，包括**食品和药物管理局**(**FDA**)(针对医疗行业)、**国家公路交通安全管理局**(**NHTSA**)(交通运输)和**联邦贸易委员会**(**FTC**)(零售)
- en: Regulations enabled by considering labor laws and civil rights laws
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过考虑劳动法和民权法启用的法规
- en: Regulations driven by the California Consumer Privacy Act
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由加利福尼亚消费者隐私法案驱动的法规
- en: A survey has shown that after applying experimental regulations to AI solutions,
    the adoption of AI in business processes decreases by 16% due to a significant
    increase in the cost of implementing the AI strategy. It has been observed that,
    often, managers reduce the funds allocated for employee training to develop a
    wider AI strategy that can sustain AI regulations. The need at present is to set
    standards for certification, testing, auditing, and technology that validate model
    performance metrics in all use cases.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 一项调查显示，在对人工智能解决方案应用实验性监管后，企业流程中人工智能的采用率下降了16%，因为实施人工智能战略的成本显著增加。观察到，通常情况下，管理者减少了用于员工培训的资金，以发展一个更广泛的人工智能战略，从而支撑人工智能监管。目前的需求是制定认证、测试、审计和技术标准，验证所有使用案例中的模型性能指标。
- en: Organizations have come up with regulatory sandboxes that allow private sectors
    to run tests to evaluate how systems perform with real individuals and local regulatory
    constraints. As illustrated in the following figure, these sandboxes involve a
    few rounds of internal evaluation and stress testing with **Explainable AI** (**XAI**)
    to audit and correct the systems with appropriate feedback.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 组织们已经提出了监管沙盒，允许私营部门进行测试，以评估系统在现实个体和地方性监管约束下的表现。如以下图所示，这些沙盒包括几轮内部评估和压力测试，使用**可解释人工智能**（**XAI**）对系统进行审计和修正，并给予适当的反馈。
- en: 'The figure illustrates the different components of a regulatory sandbox. This
    kind of sandbox allows us to determine the following:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了监管沙盒的不同组成部分。这种沙盒使我们能够确定以下内容：
- en: The eligibility criteria for a dataset to be considered, such as how diverse
    the dataset is in terms of attributes. The procedure of application is to be included
    in the regulatory sandbox testing.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集被考虑的资格标准，例如数据集在属性方面的多样性。申请程序应纳入监管沙盒测试。
- en: Actual participation of the individuals in the regulatory testing process.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个人在监管测试过程中的实际参与。
- en: The exiting condition for the sandbox, depending on the models being evaluated
    for risk.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沙盒的退出条件，取决于评估的风险模型。
- en: The rights and obligations of participants.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参与者的权利和义务。
- en: 'An XAI assessment framework helps us to do the following:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: XAI 评估框架帮助我们实现以下目标：
- en: Understand and explain the internal characteristics to validate interpretability.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解并解释内部特征，以验证可解释性。
- en: Evaluate the effect of the external environment on the system.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估外部环境对系统的影响。
- en: Assess the effect of the system on the external environment in safety-critical
    scenarios.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估系统在安全关键场景中对外部环境的影响。
- en: Ensure the application of better governance, policy, and regulatory theories.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保更好的治理、政策和监管理论的应用。
- en: If a system poses life-threatening risks to health and safety and fundamental
    rights, it should be stopped right away to prevent any further damage during its
    development and testing efforts. These experimental regulations and regulatory
    sandboxes help with evidence-based lawmaking. The core structure comprises pre-established
    facts, assumptions, and a legislative framework, which are the building blocks
    of an authentic, mature, and scalable AI system that abides by regulatory compliance.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个系统对健康、安全和基本权利构成生命威胁风险，应立即停止，以防在其开发和测试过程中造成进一步的损害。这些实验性监管和监管沙盒有助于基于证据的立法。其核心结构包括预设的事实、假设和立法框架，这些是遵循监管合规性的真实、成熟和可扩展人工智能系统的基础。
- en: The following figure shows an AI system in a sandbox to evaluate the effectiveness
    of the overall AI regulatory framework for economic, social, and political impact.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了一个人工智能系统在沙盒中，用以评估整体人工智能监管框架对经济、社会和政治影响的有效性。
- en: '![Figure 3.9 – AI sandbox for risk assessment and testing for regulations and
    policies](img/B18681_03_009.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.9 – 风险评估和监管政策测试的人工智能沙盒](img/B18681_03_009.jpg)'
- en: Figure 3.9 – AI sandbox for risk assessment and testing for regulations and
    policies
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 – 风险评估和监管政策测试的人工智能沙盒
- en: Let’s wrap up this chapter.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结本章内容。
- en: Summary
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we have learned about the regulations and laws put into practice
    by different ethical and governing bodies from varying nations. We now understand
    their basic principles and how they can aid in nurturing the growth of equitable
    systems, particularly those that fall under the high-risk category, such as models
    to do with justice, autonomous cars, healthcare, and systems that are expected
    to not only protect basic human rights but also promote the development of sustainable
    activities on earth. In addition, we stressed the importance of generating awareness
    about any possible misuse of AI by concentrating on overall well-being, effectiveness,
    transparency, accountability, competence, privacy, and fairness. We now understand
    the potential loss, or the damage done, in the case of a violation or failure
    and why it is important to abide by existing regulations or laws, as well as how
    this loss has a negative impact on society. This chapter also provided us with
    an insight into the gaps in existing regulations and laws and what further refinement
    is needed. These gaps leave us with a responsibility to be mindful of the design
    of such systems so that they serve as demonstrative examples for future generations’
    systems.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了不同国家的伦理和治理机构所实施的法规与法律。我们现在理解了这些法规的基本原则，以及它们如何帮助培育公平系统的发展，特别是那些属于高风险类别的系统，例如与司法、自动驾驶汽车、医疗保健相关的模型，以及那些不仅旨在保护基本人权，还旨在促进地球可持续活动发展的系统。此外，我们强调了通过专注于整体福祉、有效性、透明度、问责制、能力、隐私和公平性，增强对人工智能可能被滥用的意识的重要性。我们现在理解了在发生违规或失败时可能造成的损失或损害，以及遵守现有法规或法律的重要性，为什么这种损失对社会产生负面影响。本章还让我们深入了解了现有法规和法律中的空白，以及需要进一步完善的地方。这些空白使我们有责任在设计这些系统时保持警觉，确保它们成为未来一代系统的示范性案例。
- en: In order to do that, let’s study how to build and evaluate large-scale big data
    and ML model pipelines in the next chapter.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，让我们在下一章中学习如何构建和评估大规模大数据和机器学习模型管道。
- en: Further reading
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Key provisions of the Draft AI* *Regulation*: [https://www.allenovery.com/en-gb/global/news-and-insights/publications/key-provisions-of-the-draft-ai-regulation](https://www.allenovery.com/en-gb/global/news-and-insights/publications/key-provisions-of-the-draft-ai-regulation)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*草案人工智能法规的关键条款*：[https://www.allenovery.com/en-gb/global/news-and-insights/publications/key-provisions-of-the-draft-ai-regulation](https://www.allenovery.com/en-gb/global/news-and-insights/publications/key-provisions-of-the-draft-ai-regulation)'
- en: '*Policy guidance on AI for* *children*: [https://www.unicef.org/globalinsight/media/2356/file/UNICEF-Global-Insight-policy-guidance-AI-children-2.0-2021.pdf](https://www.unicef.org/globalinsight/media/2356/file/UNICEF-Global-Insight-policy-guidance-AI-children-2.0-2021.pdf)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关于儿童的人工智能政策指导*：[https://www.unicef.org/globalinsight/media/2356/file/UNICEF-Global-Insight-policy-guidance-AI-children-2.0-2021.pdf](https://www.unicef.org/globalinsight/media/2356/file/UNICEF-Global-Insight-policy-guidance-AI-children-2.0-2021.pdf)'
- en: '*AI Policy and National* *Strategies*: [https://aiindex.stanford.edu/wp-content/uploads/2021/03/2021-AI-Index-Report-_Chapter-7.pdf](https://aiindex.stanford.edu/wp-content/uploads/2021/03/2021-AI-Index-Report-_Chapter-7.pdf)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*人工智能政策与国家* *战略*：[https://aiindex.stanford.edu/wp-content/uploads/2021/03/2021-AI-Index-Report-_Chapter-7.pdf](https://aiindex.stanford.edu/wp-content/uploads/2021/03/2021-AI-Index-Report-_Chapter-7.pdf)'
- en: '*European draft Regulation on artificial intelligence: Key questions* *answered*:
    [https://www.ey.com/en_ch/law/european-draft-regulation-on-artificial-intelligence-key-questions-answered](https://www.ey.com/en_ch/law/european-draft-regulation-on-artificial-intelligence-key-questions-answered)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*欧洲人工智能草案法规：关键问题* *解答*：[https://www.ey.com/en_ch/law/european-draft-regulation-on-artificial-intelligence-key-questions-answered](https://www.ey.com/en_ch/law/european-draft-regulation-on-artificial-intelligence-key-questions-answered)'
- en: '*Amazon scraps secret AI recruiting tool that showed bias against* *women*:
    [https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*亚马逊放弃了对* *女性有偏见的秘密人工智能招聘工具*：[https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)'
- en: '*AI, Machine Learning & Big Data Laws and Regulations 2022 |* *Australia*:
    [https://www.globallegalinsights.com/practice-areas/ai-machine-learning-and-big-data-laws-and-regulations/australia](https://www.globallegalinsights.com/practice-areas/ai-machine-learning-and-big-data-laws-and-regulations/australia)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*2022年AI、机器学习与大数据法律与法规 |* *澳大利亚*：[https://www.globallegalinsights.com/practice-areas/ai-machine-learning-and-big-data-laws-and-regulations/australia](https://www.globallegalinsights.com/practice-areas/ai-machine-learning-and-big-data-laws-and-regulations/australia)'
- en: '*U.S. Artificial Intelligence Regulation Takes* *Shape*: [https://www.jdsupra.com/legalnews/u-s-artificial-intelligence-regulation-1161759/](https://www.jdsupra.com/legalnews/u-s-artificial-intelligence-regulation-1161759/)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*美国人工智能法规逐渐成型*：[https://www.jdsupra.com/legalnews/u-s-artificial-intelligence-regulation-1161759/](https://www.jdsupra.com/legalnews/u-s-artificial-intelligence-regulation-1161759/)'
- en: '*Draft Eu Regulation On Ai And Its Impact On* *Healthcare*: [https://www.kantify.com/insights/draft-eu-regulation-on-ai-and-its-impact-on-healthcare](https://www.kantify.com/insights/draft-eu-regulation-on-ai-and-its-impact-on-healthcare)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*欧盟关于AI的草案法规及其对* *医疗保健的影响*： [https://www.kantify.com/insights/draft-eu-regulation-on-ai-and-its-impact-on-healthcare](https://www.kantify.com/insights/draft-eu-regulation-on-ai-and-its-impact-on-healthcare)'
- en: '*FTC warns it could crack down on biased* *AI*: [https://www.theverge.com/2021/4/20/22393873/ftc-ai-machine-learning-race-gender-bias-legal-violation](https://www.theverge.com/2021/4/20/22393873/ftc-ai-machine-learning-race-gender-bias-legal-violation)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*FTC警告可能会打击偏见* *AI*： [https://www.theverge.com/2021/4/20/22393873/ftc-ai-machine-learning-race-gender-bias-legal-violation](https://www.theverge.com/2021/4/20/22393873/ftc-ai-machine-learning-race-gender-bias-legal-violation)'
- en: 'Ranchordas, S. (2021). *Experimental Regulations for AI: Sandboxes for Morals
    and Mores*. University of Groningen Faculty of Law Research Paper, (7). [https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3839744](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3839744),
    [https://www.nomos-elibrary.de/10.5771/2747-5174-2021-1-86.pdf](https://www.nomos-elibrary.de/10.5771/2747-5174-2021-1-86.pdf)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ranchordas, S. (2021). *AI的实验性法规：道德和风俗的沙箱*。格罗宁根大学法学院研究论文，（7）。[https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3839744](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3839744)，[https://www.nomos-elibrary.de/10.5771/2747-5174-2021-1-86.pdf](https://www.nomos-elibrary.de/10.5771/2747-5174-2021-1-86.pdf)
- en: '*An AI fair lending policy agenda for the federal financial* *regulators*:
    [https://www.brookings.edu/research/an-ai-fair-lending-policy-agenda-for-the-federal-financial-regulators/](https://www.brookings.edu/research/an-ai-fair-lending-policy-agenda-for-the-federal-financial-regulators/)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*联邦金融监管机构的AI公平贷款政策议程*: [https://www.brookings.edu/research/an-ai-fair-lending-policy-agenda-for-the-federal-financial-regulators/](https://www.brookings.edu/research/an-ai-fair-lending-policy-agenda-for-the-federal-financial-regulators/)'
- en: '*Blueprint For An Ai Bill Of* *Rights*: [https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf](https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AI权利法案蓝图*：[https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf](https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf)'
- en: '*Responsible Ai Architect’s* *Guide*:[https://indiaai.gov.in/responsible-ai/pdf/architect-guide.pdf](https://indiaai.gov.in/responsible-ai/pdf/architect-guide.pdf)'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*负责任的AI架构师* *指南*：[https://indiaai.gov.in/responsible-ai/pdf/architect-guide.pdf](https://indiaai.gov.in/responsible-ai/pdf/architect-guide.pdf)'
- en: 'Part 2: Building Blocks and Patterns for a Next-Generation AI Ecosystem'
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：下一代AI生态系统的构建模块与模式
- en: This part of the book provides a comprehensive exploration of big data systems
    and AI/ML workflows, emphasizing privacy management, model design pipelines, and
    life cycle management. It covers various stages of the machine learning pipeline,
    model evaluation, and handling uncertainty, and addresses common challenges. Additionally,
    it discusses advanced topics such as hyperparameter tuning, MLOps practices, and
    AutoML. By offering theoretical discussions and practical guidance, this part
    equips you with the knowledge and tools to navigate the complex landscape of deploying
    AI models on top of big data systems while ensuring robust, efficient, and privacy-preserving
    solutions.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分深入探讨大数据系统和AI/ML工作流，重点关注隐私管理、模型设计管道和生命周期管理。涵盖机器学习管道的各个阶段、模型评估和不确定性处理，并解决常见的挑战。此外，还讨论了高级主题，如超参数调整、MLOps实践和AutoML。通过提供理论讨论和实践指导，本部分为您提供了在大数据系统上部署AI模型时确保稳健、高效且保护隐私的解决方案所需的知识和工具。
- en: 'This part is made up of the following chapters:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分由以下章节组成：
- en: '[*Chapter 4*](B18681_04.xhtml#_idTextAnchor093), *Privacy Management in Big
    Data and Model Design Pipelines*'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第4章*](B18681_04.xhtml#_idTextAnchor093), *大数据中的隐私管理与模型设计流水线*'
- en: '[*Chapter 5*](B18681_05.xhtml#_idTextAnchor110), *ML Pipeline, Model Evaluation,
    and Handling Uncertainty*'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第5章*](B18681_05.xhtml#_idTextAnchor110), *机器学习流水线、模型评估与不确定性处理*'
- en: '[*Chapter 6*](B18681_06.xhtml#_idTextAnchor126), *Hyperparameter Tuning, MLOps,
    and AutoML*'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第6章*](B18681_06.xhtml#_idTextAnchor126), *超参数调优、MLOps与AutoML*'
