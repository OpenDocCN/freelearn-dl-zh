- en: Building Your Personal Assistant
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建你的个人助手
- en: 'In this chapter, we will focus our full attention on the practical side of
    recurrent neural networks when building a conversational chatbot. Using your most
    recent knowledge on sequence models, you will create an end-to-end model that
    aims to yield meaningful results. You will make use of a high-level TensorFlow-based
    library, called TensorLayer. This library makes it easier to create simple prototypes
    of complicated systems such as that of a chatbot. The main topics that will be
    covered are the following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将将全部注意力集中在构建对话型聊天机器人时递归神经网络的实际应用上。通过利用你最新掌握的序列模型知识，你将创建一个端到端的模型，旨在得到有意义的结果。你将使用一个基于TensorFlow的高级库，名为TensorLayer。这个库可以让你更轻松地创建像聊天机器人这样的复杂系统的简单原型。我们将涵盖以下主要话题：
- en: '**What are we building?**:This is a more detailed introduction to the exact
    problem and its solution'
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我们在构建什么？**：这是对具体问题及其解决方案的更详细介绍。'
- en: '**Preparing the data**: As always, any deep learning model requires this step,
    so it is crucial to mention it here'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准备数据**：和往常一样，任何深度学习模型都需要这一步，因此在这里提及它至关重要。'
- en: '**Creating the chatbot network**: You will learn how to use TensorLayer to
    build the graph for the sequence-to-sequence model used for the chatbot'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建聊天机器人网络**：你将学习如何使用TensorLayer构建聊天机器人所需的序列到序列模型图。'
- en: '**Training the chatbot**: This step combines the data and the network graph
    in order to find the best possible combination of weights and biases'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练聊天机器人**：这一步将数据和网络图结合起来，以找到最合适的权重和偏置的组合。'
- en: '**Building a conversation**: This last step uses the already trained model,
    together with sample sentences, to produce a meaningful conversation'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建对话**：最后一步使用已经训练好的模型，并结合示例句子，生成有意义的对话。'
- en: What are we building?
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们在构建什么？
- en: The focus of this chapter is to walk you through building a simple conversational
    chatbot that is able to give answers to a set of different questions. Recently,
    chatbots have become more and more popular, and we can see them in numerous practical
    applications.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重点是带你一步步构建一个能够回答不同问题的简单对话型聊天机器人。近年来，聊天机器人越来越受欢迎，我们可以在许多实际应用中看到它们。
- en: 'Some areas where you can see the use of this software include the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下一些领域看到此软件的应用：
- en: '**Communication between clients and businesses**, where the chatbot assists
    users in finding what they need, or provides support if something does not work
    properly. For example, Facebook offers a really handy way of implementing a chatbot
    for your business'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户与企业之间的沟通**，其中聊天机器人帮助用户找到他们需要的东西，或者提供支持如果某些东西没有正常工作。例如，Facebook提供了一种非常方便的方式来为你的企业实现聊天机器人。'
- en: '**The personal assistant behind voice control systems such as Amazon Alexa,
    Apple Siri, and more**: You have a full end-to-end human-like conversation where
    you can set reminders, order products, and more'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音控制系统背后的个人助手，如Amazon Alexa、Apple Siri等**：你将体验一个完整的端到端类人对话，可以设置提醒、订购产品等。'
- en: Our simple example will present a slightly augmented version of the TensorLayer
    chatbot code example ([https://github.com/tensorlayer/seq2seq-chatbot](https://github.com/tensorlayer/seq2seq-chatbot)).
    We will be using a dataset formed of pre-collected tweets and will utilize the
    sequence-to-sequence model. Recall from previous chapters that this kind of model
    uses two recurrent neural networks, where the first one is an encoder and the
    second one a decoder. Later, we will give more detail on how this architecture
    is used for building the chatbot.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的简单示例将展示一个稍微扩展版的TensorLayer聊天机器人代码示例（[https://github.com/tensorlayer/seq2seq-chatbot](https://github.com/tensorlayer/seq2seq-chatbot)）。我们将使用由预收集推文组成的数据集，并将利用序列到序列模型。回顾之前的章节，这种模型使用两个递归神经网络，第一个是编码器，第二个是解码器。稍后我们将详细介绍如何使用这种架构来构建聊天机器人。
- en: Preparing the data
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: In this section, we will focus on how our data (tweets, in this case) is transformed
    to fit the model's requirements. We will first see how, using the files in the `data/`
    folder from the GitHub repo for this task, the model can help us extract the needed
    tweets. Then, we will look at how, with the help of a simple set of functions,
    we can split and transform the data to achieve the needed results.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将专注于如何将我们的数据（在此案例中为推文）转换以满足模型的要求。我们将首先看到，如何使用来自GitHub任务仓库中的`data/`文件夹中的文件，模型可以帮助我们提取所需的推文。然后，我们将看看如何通过一组简单的函数，将数据拆分并转换为所需的结果。
- en: 'An important file to examine is `data.py`, inside the `data/twitter` folder.
    It transforms plain text into a numeric format so it is easy for us to train the
    network. We won''t go deep into the implementation, since you can examine it by
    yourself. After running the code, we produce three important files:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的文件是`data.py`，位于`data/twitter`文件夹内。它将纯文本转换为数字格式，以便我们轻松训练网络。我们不会深入探讨其实现，因为你可以自己查看。在运行代码后，我们会生成三个重要的文件：
- en: '`idx_q.npy`: This is an array of arrays containing index representation of
    all the words in different sentences forming the chatbot questions'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`idx_q.npy`：这是一个包含所有单词在不同句子中索引表示的数组，构成了聊天机器人问题的内容。'
- en: '`idx_a.npy`: This is an array of arrays containing index representation of
    all the words in different sentences forming the chatbot answers'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`idx_a.npy`：这是一个包含所有单词在不同句子中索引表示的数组，构成了聊天机器人回答的内容。'
- en: '`metadata.pkl`: This contains both the *index to word* (`idx2w`) and *word
    to index* (`w2idx`) dictionaries used for this dataset'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metadata.pkl`：它包含了用于此数据集的*索引到单词*（`idx2w`）和*单词到索引*（`w2idx`）字典。'
- en: Now, let's focus on the actual usage of this data. You can review it in the
    first 20 lines of `ch5_task.py` from the GitHub repository for this chapter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们聚焦于这个数据的实际应用。你可以在本章GitHub仓库中的`ch5_task.py`文件的前20行查看它的使用。
- en: 'First, we import several Python libraries that will be used throughout the
    whole program:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入几个将在整个程序中使用的Python库：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here is a breakdown of these libraries, accompanied with descriptions:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这些库的详细说明，附带描述：
- en: '`time`: This is used for keeping track of how long our operations take. You
    will see its usage in the following section, where we train the network'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time`：这是用于跟踪操作所花时间的变量。你将在接下来的部分看到它的使用，在那里我们将训练网络。'
- en: '`tensorflow`: This is used only for a handful of operations (initializing variables,
    optimizing the network using adam optimizer, and initializing the TensorFlow session: `tf.Session()`)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow`：它仅用于执行少数操作（初始化变量、使用Adam优化器优化网络，以及初始化TensorFlow会话：`tf.Session()`）。'
- en: '`tensorlayer`: As you already know, TensorLayer ([https://tensorlayer.readthedocs.io/en/stable/](https://tensorlayer.readthedocs.io/en/stable/))
    is a deep learning library on top of TensorFlow. It offers a wide range of methods
    and classes that make it easy for any developer to simply build solutions for
    complicated tasks. This library will help us construct and train our sequence-to-sequence
    model easily'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorlayer`：正如你已经知道的，TensorLayer（[https://tensorlayer.readthedocs.io/en/stable/](https://tensorlayer.readthedocs.io/en/stable/)）是一个基于TensorFlow的深度学习库。它提供了广泛的方法和类，使得任何开发者都能轻松构建复杂任务的解决方案。这个库将帮助我们轻松构建和训练我们的序列到序列模型。'
- en: '`shuffle`: We use this to shuffle all arrays, which represent different sentences,
    inside `trainX` and `trainY`. You will see how we obtain `trainX` and `trainY`
    in the following section'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shuffle`：我们用它来打乱`trainX`和`trainY`中表示不同句子的所有数组。你将在接下来的部分看到我们如何获取`trainX`和`trainY`。'
- en: '`EmbeddingInputlayer`: A TensorLayer class that represents the input layer
    of a sequence-to-sequence model. As you know, every `Seq2Seq` model has two input
    layers, the encoder and the decoder'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EmbeddingInputlayer`：一个TensorLayer类，表示序列到序列模型的输入层。正如你所知道的，每个`Seq2Seq`模型都有两个输入层，编码器和解码器。'
- en: '`Seq2Seq`: A TensorLayer class that builds a sequence-to-sequence model similar
    to the one in the following diagram:'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Seq2Seq`：一个TensorLayer类，用于构建类似于下图所示的序列到序列模型：'
- en: '![](img/3c97d0f9-8a48-4ceb-951a-953bc79f5f8e.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c97d0f9-8a48-4ceb-951a-953bc79f5f8e.png)'
- en: '`DenseLayer`: A TensorLayer representation of a fully connected (dense) layer.
    There are different types of layers that perform different transformations and
    are used in specific scenarios. For example, we have already used a recurrent
    layer, which is used for time series data. There is also a convolutional layer
    used for images, and so on. You can learn more about them in this video ([https://www.youtube.com/watch?v=FK77zZxaBoI](https://www.youtube.com/watch?v=FK77zZxaBoI))'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DenseLayer`：TensorLayer表示的全连接（密集）层。有多种类型的层执行不同的转换，且在特定场景中使用。例如，我们已经使用过递归层，它用于时间序列数据。还有用于图像的卷积层等等。你可以通过这个视频了解更多内容（[https://www.youtube.com/watch?v=FK77zZxaBoI](https://www.youtube.com/watch?v=FK77zZxaBoI)）。'
- en: '`retrieve_seq_length_op2`: A TensorLayer function used for calculating the
    sequence length, excluding any paddings of zeros. We will use this function for
    both the encoding and decoding sequences'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieve_seq_length_op2`：一个TensorLayer函数，用于计算序列长度，排除任何零填充部分。我们将会在编码和解码序列中使用这个函数。'
- en: 'After importing the libraries, we need to access the data as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 导入库后，我们需要按如下方式访问数据：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: First, we load the `metadata`, `idx_q`, and `idx_a` from the `data/twitter/`
    folder found in the GitHub repository. Second, we use the `split_dataset` method
    to separate the encoder (`idx_q`) and decoder (`idx_a`) data into training (70%),
    testing (15%), and validation (15%).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从GitHub仓库中的`data/twitter/`文件夹加载`metadata`、`idx_q`和`idx_a`。其次，我们使用`split_dataset`方法将编码器（`idx_q`）和解码器（`idx_a`）数据分为训练集（70%）、测试集（15%）和验证集（15%）。
- en: Finally, we convert `trainX, trainY, testX, testY, validX, validY` into Python
    lists, and then remove the padding (zero elements) from the end of every list
    using a TensorLayer function, `tlayer.prepro.remove_pad_sequences()`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将`trainX, trainY, testX, testY, validX, validY`转换为Python列表，然后使用TensorLayer函数`tlayer.prepro.remove_pad_sequences()`从每个列表的末尾去除填充（零元素）。
- en: Combining the preceding operations leads to well-defined training, testing,
    and validation data. You will see how we make use of them during training and
    prediction later in this chapter.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 将前面的操作结合起来，就得到了明确的训练、测试和验证数据。你将在本章后面看到我们如何在训练和预测中使用这些数据。
- en: Creating the chatbot network
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建聊天机器人网络
- en: This section is one of the most important, so you need to make sure you understand
    it quite well in order to grasp the full concept of our application. We will be
    introducing the network graph that will be used for training and prediction.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本节是最重要的部分之一，因此你需要确保自己能够充分理解它，以便掌握我们应用程序的整体概念。我们将介绍将用于训练和预测的网络图。
- en: 'But first, let''s define the hyperparameters of the model. These are predefined
    constants that play a significant role in determining how well the model performs.
    As you will learn in the next chapter, our main task is to tweak the hyperparameters''
    values until we''re satisfied with the model''s prediction. In this case, an initial
    set of hyperparameters is selected. Of course, for better performance, one needs
    to do some optimization on them. This chapter won''t focus on this part but I
    highly recommend doing it using techniques from the last chapter of this book
    ([Chapter 6](85ffa70b-f86d-4432-9c53-9f3e2ab0e007.xhtml), *Improving Your RNN
    Performance*). The current hyperparameter selection is as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，让我们定义模型的超参数。这些是预定义的常量，对于确定模型的表现非常重要。正如你将在下一章中学到的那样，我们的主要任务是调整超参数的值，直到我们对模型的预测满意为止。在这种情况下，选择了一组初始的超参数。当然，为了更好的性能，必须对它们进行一些优化。本章不会专注于这一部分，但我强烈建议你使用本书最后一章（[第六章](85ffa70b-f86d-4432-9c53-9f3e2ab0e007.xhtml)，*提升你的RNN性能*）中的技术来优化这些参数。当前的超参数选择如下：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here is a brief explanation of these hyperparameters:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这些超参数的简要解释：
- en: '`batch_size`: This determines how many elements each batch should have. Normally,
    training is done on batches where data is separated into subarrays, each with
    the size of `batch_size`'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`：此项决定每个批次应包含多少元素。通常，训练是在批次上进行的，其中数据被分割成子数组，每个子数组的大小为`batch_size`。'
- en: '`embedding_dimension`: This determines the size of the word embedding vector.
    A single word from the input is encoded into a vector with the size of `embedding_dimension`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedding_dimension`：此项决定词嵌入向量的大小。输入中的一个单词会被编码成大小为`embedding_dimension`的向量。'
- en: '`learning_rate`: Its value determines how fast a network learns. It is typically
    a really small value (`0.001, 0.0001`). If the loss function does not decrease
    during training, it is good practice to reduce the learning rate'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：其值决定了网络学习的速度。通常这是一个非常小的值（`0.001, 0.0001`）。如果在训练过程中损失函数没有下降，通常的做法是减小学习率。'
- en: '`number_epochs`: This determines the number of training iterations (epochs).
    In the beginning of each iteration, we shuffle the data and, since an epoch is too
    big to feed to the computer at once, we divide it into several smaller batches.
    Then we train the network using these batches. After every iteration, we shuffle
    again and run the second epoch. This operation is done for the number of epochs
    we have set.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`number_epochs`：此项决定训练迭代次数（epoch）。在每次迭代开始时，我们会打乱数据，并且由于一个epoch的大小太大，无法一次性输入计算机，我们将其分成多个较小的批次。然后，我们使用这些批次来训练网络。在每次迭代之后，我们会再次打乱数据并执行第二个epoch。这个操作会根据我们设置的epoch数量进行。'
- en: 'After determining the set of hyperparameters, the time comes for additional
    values that help us in building our model:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定了超参数集后，接下来需要为我们构建模型提供额外的值：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s examine each line, one by one:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行检查每一项：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We use `xseq_len` and `yseq_len` to store the length of the encoder's and decoder's
    input sequence. Then, we make sure both values are equal, otherwise, the program
    will break.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`xseq_len`和`yseq_len`来存储编码器和解码器输入序列的长度。然后，我们确保这两个值相等，否则程序将会中断。
- en: '`n_step = int(xseq_len/batch_size)`: with this, we store the number of steps
    that our training is about to perform. This value is only used when printing the
    state of training and we will see its usage later in the chapter.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_step = int(xseq_len/batch_size)`：通过这个，我们存储训练即将执行的步数。这个值仅用于打印训练状态，稍后我们会在本章中看到它的用法。'
- en: 'We use `w2idx` and `idx2w` to store the word dictionary in both formats (the
    word as the dictionary key, and the ID as the dictionary key). These dictionaries
    are used when predicting the chatbot responses:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`w2idx`和`idx2w`以两种格式存储词典（词作为字典键，ID作为字典键）。这些字典在预测聊天机器人响应时使用：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We make `start_id = xvocab_size` and `end_id = xvocab_size + 1` to assure uniqueness
    of these two indices. They are used for indicating the start and end of a single
    sentence:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置`start_id = xvocab_size`和`end_id = xvocab_size + 1`以确保这两个索引的唯一性。它们用于表示单个句子的开始和结束：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, we extend these dictionaries to include starting and ending elements.
    An example set of our data is the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们扩展这些字典，包含起始和结束元素。我们数据的一个示例集如下：
- en: '`encode_seqs` (the input encoder sentence): `[''how'', ''are'', ''you'', ''<PAD_ID>'']`'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encode_seqs`（输入编码器句子）：`[''how'', ''are'', ''you'', ''<PAD_ID>'']`'
- en: '`decode_seqs` (the input decoder sentence): `[''<START_ID>'', ''I'', ''am'',
    ''fine'', ''<PAD_ID>'']`'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decode_seqs`（输入解码器句子）：`[''<START_ID>'', ''I'', ''am'', ''fine'', ''<PAD_ID>'']`'
- en: '`target_seqs` (the predicted decoder sentence): `[''I'', ''am'', ''fine'',
    ''<END_ID>'', ''<PAD_ID>'']`'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_seqs`（预测的解码器句子）：`[''I'', ''am'', ''fine'', ''<END_ID>'', ''<PAD_ID>'']`'
- en: '`target_mask` (a mask applied at each sequence): `[1, 1, 1, 1, 0]`. This is
    an array the same size as `target_seqs`, but has `0` at the places where padding
    is applied, and `1` everywhere else. You can learn more about masking in recurrent
    neural networks by reading this great Quora answer ([https://www.quora.com/What-is-masking-in-a-recurrent-neural-network-RNN](https://www.quora.com/What-is-masking-in-a-recurrent-neural-network-RNN))'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_mask`（每个序列应用的掩码）：`[1, 1, 1, 1, 0]`。这是一个与`target_seqs`大小相同的数组，但在应用填充的地方为`0`，其他地方为`1`。你可以通过阅读这篇很棒的Quora回答来了解更多关于循环神经网络中掩码的知识（[https://www.quora.com/What-is-masking-in-a-recurrent-neural-network-RNN](https://www.quora.com/What-is-masking-in-a-recurrent-neural-network-RNN)）。'
- en: 'The next step is to define our model structure. We start by introducing the
    model''s placeholders:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是定义我们的模型结构。我们从引入模型的占位符开始：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As you can see, this is the same set of variables shown previously. Each one
    has a `batch_size` dimension and `tf.int64` type. Then, we calculate the model
    output as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这是之前展示的相同变量集。每个变量都有一个`batch_size`维度和`tf.int64`类型。然后，我们按如下方式计算模型输出：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The purpose of the preceding line is to find the network's output using the
    input encoder and decoder sequences. We will define and explain the `model` method
    in the following section.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码行的目的是通过输入编码器和解码器序列来找到网络的输出。我们将在接下来的部分定义并解释`model`方法。
- en: 'Finally, we define the loss function and optimizer:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义损失函数和优化器：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see, the loss function is a cross entropy with applied mask to make
    sure each input sequence has the same length. The `logits` (predicted outputs)
    are taken from the preceding model output and are accessed using `net_out.outputs`.
    The `target_seqs` are the expected results for every input.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，损失函数是应用了掩码的交叉熵，以确保每个输入序列的长度相同。`logits`（预测输出）来自前面的模型输出，并通过`net_out.outputs`访问。`target_seqs`是每个输入的预期结果。
- en: The model's optimizer is `AdamOptimizer` and is defined using the built-in function
    from TensorFlow, `tf.train.AdamOptimizer`. As usual, we pass the `learning_rate`
    to decide the rate of the `loss` function minimization.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的优化器是`AdamOptimizer`，并通过TensorFlow内置函数`tf.train.AdamOptimizer`来定义。像往常一样，我们传递`learning_rate`来决定`loss`函数最小化的速率。
- en: 'The last step is defining and explaining the `model` function:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的步骤是定义并解释`model`函数：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'TensorLayer makes it as simple as possible to build the sequence-to-sequence
    model. It uses four main components:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: TensorLayer尽可能简化了构建序列到序列模型的过程。它使用了四个主要组件：
- en: '`net_encode`: An encoder network using the `EmbeddingInputlayer` class.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`net_encode`：使用`EmbeddingInputlayer`类的编码器网络。'
- en: '`net_decode`: A decoder network using the `EmbeddingInputlayer` class.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`net_decode`：使用`EmbeddingInputlayer`类的解码器网络。'
- en: '`net_rnn`: A sequence-to-sequence model that combines the two aforementioned
    networks. It is implemented using the `Seq2Seq` class.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`net_rnn`: 一个将两个上述网络组合起来的序列到序列模型。它使用`Seq2Seq` 类实现。'
- en: '`net_out`: The final, fully connected (dense) layer producing the end result.
    This layer is built on top of the sequence-to-sequence network.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`net_out`: 最终的全连接（密集）层，产生最终结果。该层建立在序列到序列网络之上。'
- en: '`net_encode` and `net_decode` are similarly initialized using `EmbeddingInputlayer`
    ([https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.EmbeddingInputlayer](https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.EmbeddingInputlayer)).
    Three important parameters are used: `inputs`, `vocabulary_size`, and `embedding_size`.
    The `inputs` are `encode_seqs` or `decode_seqs`, which we defined in the preceding
    section. In both cases, `vocabulary_size` is equal to `xvocab_size`, and the `embedding_size`
    is equal to `embedding_dimension`. This embedding layer transforms the input vector
    into one of the `embedding_dimension` size.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`EmbeddingInputlayer` ([https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.EmbeddingInputlayer](https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.EmbeddingInputlayer))
    类初始化`net_encode` 和 `net_decode`。使用了三个重要的参数：`inputs`、`vocabulary_size` 和 `embedding_size`。`inputs`
    是我们在前面部分中定义的 `encode_seqs` 或 `decode_seqs`。在两种情况下，`vocabulary_size` 等于 `xvocab_size`，`embedding_size`
    等于 `embedding_dimension`。这个嵌入层将输入向量转换为指定 `embedding_dimension` 大小之一。
- en: '`net_rnn` combines both the encoder and decoder layers into a full sequence-to-sequence
    model. The parameters are the following:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`net_rnn` 将编码器和解码器层结合成一个完整的序列到序列模型。以下是其参数：'
- en: '`cell_fn`: The RNN cell used throughout the whole network. In our case, this
    is `BasicLSTMCell`.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cell_fn`: 整个网络中使用的RNN单元。在我们的情况下，这是`BasicLSTMCell`。'
- en: '`n_hidden`: The number of hidden units in each of the two layers.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_hidden`: 每个网络层中隐藏单元的数量。'
- en: '`initializer`: The distribution used for defining the parameters (weights,
    biases).'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer`: 用于定义参数（权重、偏置）的分布。'
- en: '`encode_sequence_length`: This specifies the length of the encoder input sequence.
    It uses the `retrieve_seq_length_op2` ([https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.retrieve_seq_length_op2](https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.retrieve_seq_length_op2))
    method on the `encode_seqs`.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encode_sequence_length`: 指定编码器输入序列的长度。它使用`retrieve_seq_length_op2` ([https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.retrieve_seq_length_op2](https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.retrieve_seq_length_op2))
    方法来处理`encode_seqs`。'
- en: '`decode_sequence_length`: This specifies the length of the decoder input sequence.
    It uses the `retrive_seq_length_op2` method on the `decode_seqs`.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decode_sequence_length`: 指定解码器输入序列的长度。它使用`retrieve_seq_length_op2` 方法处理`decode_seqs`。'
- en: '`initial_state_encode`: If `None`, the initial state of the encoder networks is
    zero state and can be set automatically by the placeholder or another RNN.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initial_state_encode`: 如果为`None`，编码器网络的初始状态为零状态，并可以由占位符或另一个循环神经网络自动设置。'
- en: '`n_layer`: The number of RNN layers stacked together in each of the two networks
    (encoder and decoder).'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_layer`: 每个两个网络（编码器和解码器）堆叠的RNN层的数量。'
- en: '`return_seq_2d`: If the value is `True`, return `2D Tensor [n_example, 2 *
    n_hidden]`, for stacking DenseLayer after it.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_seq_2d`: 如果为 `True`，返回 `2D Tensor [n_example, 2 * n_hidden]`，以便在其后堆叠
    `DenseLayer`。'
- en: In the end, we use a fully connected (dense) layer `net_out` to calculate the
    final output of the network. It uses the `Seq2Seq` network as a previous layer,
    the vocabulary size (`xvocab_size`) as the number of units, and `tf.identity`
    as the activation function. It is normally used for explicit transport of a tensor
    between devices (for example, from a GPU to a CPU). In our case, we use it to
    build dummy nodes that copy the values from the previous layer.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用一个全连接（密集）层`net_out`来计算网络的最终输出。它将`Seq2Seq` 网络作为前一层，词汇表大小（`xvocab_size`）作为单元数，并使用`tf.identity`
    作为激活函数。通常用于在设备之间显式传输张量的场景（例如，从GPU到CPU）。在我们的情况下，我们使用它来构建复制前一层数值的虚拟节点。
- en: One last thing to point out is the use of the `reuse` parameter and the `vs.reuse_variables()`
    method call. During training, we are not reusing the model's parameters (weights
    and biases), so `reuse = False`, but when predicting the chatbot response, we
    make use of the pre-trained parameters, and so have `reuse = True.` The method
    call triggers a reuse for the next set of calculations.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后需要指出的是`reuse`参数和`vs.reuse_variables()`方法调用的使用。在训练过程中，我们并没有重用模型的参数（权重和偏置），因此`reuse
    = False`，但是在预测聊天机器人响应时，我们会利用预训练的参数，因此将`reuse = True`。该方法调用会触发下一组计算的重用。
- en: 'And with this, we have finished defining the model. There are only two parts
    left from now: training and predicting.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 到这里，我们已经完成了模型的定义。从现在开始，剩下的只有两个部分：训练和预测。
- en: Training the chatbot
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练聊天机器人
- en: Once we have defined the model graph, we want to train it using our input data.
    Then, we will have a well-tuned set of parameters that can be used for accurate
    predictions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了模型图，我们就希望使用输入数据来训练它。然后，我们将拥有一组经过良好调整的参数，可以用于准确的预测。
- en: 'First, we specify the TensorFlow''s Session object that encapsulates the environment
    in which Operation (summation, subtraction, and so on) objects are executed and
    Tensor (placeholders, variables, and so on) objects are evaluated:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们指定TensorFlow的Session对象，它封装了执行操作（加法、减法等）对象和评估张量（占位符、变量等）对象的环境：
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: A good explanation of the `config` parameter can be found at [https://stackoverflow.com/questions/44873273/what-do-the-options-in-configproto-like-allow-soft-placement-and-log-device-plac](https://stackoverflow.com/questions/44873273/what-do-the-options-in-configproto-like-allow-soft-placement-and-log-device-plac).
    In summary, once we specify `allow_soft_placement`, the operations will be executed
    on the CPU only if there is no GPU registered. If this value is false, we are
    not allowed to execute any operation on a GPU.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`config`参数的一个很好的解释可以在[https://stackoverflow.com/questions/44873273/what-do-the-options-in-configproto-like-allow-soft-placement-and-log-device-plac](https://stackoverflow.com/questions/44873273/what-do-the-options-in-configproto-like-allow-soft-placement-and-log-device-plac)找到。总结来说，一旦我们指定了`allow_soft_placement`，只有在没有注册GPU的情况下，操作才会在CPU上执行。如果该值为false，则不允许在GPU上执行任何操作。
- en: Only after running the second line (`sess.run(tf.global_variables_initializer())`)
    will all variables actually hold their values. Initially, they only store a persistent
    Tensor.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在运行第二行代码（`sess.run(tf.global_variables_initializer())`）后，所有的变量才会实际持有它们的值。最初，它们只存储一个持久的张量。
- en: 'Now, we will train the network using the `train()` function, defined as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用`train()`函数来训练网络，函数定义如下：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Let's explain what the preceding code does line by line.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行解释前面的代码做了什么。
- en: The implementation has two nested loops, where the outer one decides how many
    times the training should go through the whole set of data. This is often done
    using epochs, and aims to strengthen the model accuracy. It is rarely the case
    that weights and biases have learned enough from a certain example when it is
    propagated just once. This is the reason why we should go over every example multiple
    times—in our case, this will be 1,000 times (the number of epochs).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 该实现有两个嵌套循环，其中外层循环决定训练应该遍历整个数据集多少次。通常使用epoch来完成此任务，目的是加强模型的准确性。因为权重和偏置在仅仅一次传播中，往往无法从某个示例中学到足够的内容。所以，我们应该多次遍历每个示例——在我们这个例子中是1,000次（epoch的数量）。
- en: After we enter an epoch iteration, we shuffle the data using the `shuffle` function
    in `sklearn`, which prepares it for entering the inner loop. Then, we use `tl.iterate.minibatches`
    to split the data into sub-arrays each with the `batch_size` size. Each iteration
    inside the inner loop trains the network using the current batch of data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入一个epoch迭代后，我们使用`sklearn`中的`shuffle`函数打乱数据，为进入内部循环做准备。然后，我们使用`tl.iterate.minibatches`将数据分割成子数组，每个子数组的大小为`batch_size`。内部循环中的每一次迭代都会使用当前批次的数据训练网络。
- en: Before calculating the optimizer, we do some small modification on X (encoder
    batch data) and Y (decoder batch data). As you remember, the model has an encoder
    input (`encoder_seqs`), a decoder input (`decoder_seqs`), and a target output
    (`target_seqs`) incorporated into two RNNs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算优化器之前，我们对X（编码器批数据）和Y（解码器批数据）做一些小的修改。如你所记得，模型有一个编码器输入（`encoder_seqs`）、一个解码器输入（`decoder_seqs`）和一个目标输出（`target_seqs`），并且它们被结合进两个RNN中。
- en: The first recurrent neural network is the encoder, which accepts `encoder_seqs`
    as an input. In the preceding code block, this is marked with *X*. We only need
    to add padding to this sequence before applying it to the network. Padding is
    the operation of adding zeros to the end of a sequence in order for it to match
    a fixed length, determined by the longest sequence in the training set. This network
    produces a single vector which is then used in the second RNN.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个递归神经网络是编码器，它接受`encoder_seqs`作为输入。在上面的代码块中，它标记为*X*。我们只需要在将此序列应用到网络之前给它添加填充。填充是将零添加到序列的末尾，使其与固定长度匹配，这个长度是由训练集中最长的序列决定的。这个网络会生成一个向量，之后它会被用到第二个RNN中。
- en: The second recurrent neural network accepts the encoded vector from the first
    RNN and a decoder input sequence (`decoder_seqs`), and returns a predicted result.
    During training, we compare the predicted result to a target sequence (`target_seqs`),
    which happens to be the exact same sequence.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个递归神经网络接收来自第一个RNN的编码向量和解码器输入序列（`decoder_seqs`），并返回预测结果。在训练过程中，我们将预测结果与目标序列（`target_seqs`）进行比较，而目标序列恰好与预测结果相同。
- en: 'Let''s clarify the preceding statement. Say you have the sentence *Hello, how
    are you?* as an input, and its response, *I am fine.*, as the output. The first
    sentence goes into the encoder network. The second sentence is the expected output
    of the second decoder network. We need to compare this expected output with the
    actual output that our decoder produces. We get the first word **I** and try to
    predict the following word **am**, then we get **am** and try to predict **fine**,
    and so on. In the beginning, our prediction will be way off, but with time, the
    weights and biases should be adjusted to produce accurate results. The following
    diagram can accompany the explanation:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们澄清一下前面的陈述。假设你有一个输入句子*Hello, how are you?*，它的回应是*I am fine.*。第一个句子进入编码器网络。第二个句子是第二个解码器网络的预期输出。我们需要将这个预期输出与解码器实际产生的输出进行比较。我们先得到第一个单词**I**，并尝试预测下一个单词**am**，然后得到**am**并尝试预测**fine**，依此类推。刚开始时，我们的预测可能会偏差很大，但随着时间推移，权重和偏差应该会调整得更加准确。下面的图表可以辅助说明：
- en: '![](img/6a698f1d-cc35-4910-9cdd-ec42863c98b2.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a698f1d-cc35-4910-9cdd-ec42863c98b2.png)'
- en: As you can see, we need to add a starting symbol to `decoder_seqs` and an ending
    symbol to `target_seqs`. This is what `_decode_seqs = tl.prepro.sequences_add_start_id(Y,
    start_id=start_id, remove_last=False)` and `_target_seqs = tl.prepro.sequences_add_end_id(Y,
    end_id=end_id)` do, where `start_id = xvocab_size` and `end_id = xvocab_size+1`.
    Finally, we add padding to both sequences, equalizing the lengths.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们需要在`decoder_seqs`中添加一个起始符号，在`target_seqs`中添加一个结束符号。这正是`_decode_seqs =
    tl.prepro.sequences_add_start_id(Y, start_id=start_id, remove_last=False)`和`_target_seqs
    = tl.prepro.sequences_add_end_id(Y, end_id=end_id)`所做的事情，其中`start_id = xvocab_size`和`end_id
    = xvocab_size+1`。最后，我们给两个序列都添加填充，使它们长度相等。
- en: Just before the actual training, we extract `_target_mask` from `_target_seqs`.
    Recall from earlier that if `_target_seqs = ["I", "am", "fine", "<END_ID>", "<PAD_ID>"]`,
    then `_target_mask = [1, 1, 1, 1, 0]`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际训练之前，我们从`_target_seqs`中提取`_target_mask`。回想一下，如果`_target_seqs = ["I", "am",
    "fine", "<END_ID>", "<PAD_ID>"]`，那么`_target_mask = [1, 1, 1, 1, 0]`。
- en: In the end, we use the sequence arrays defined previously to train our network.
    It might take some time, so we have added a printing statement every 200 iterations.
    I would recommend leaving your computer running overnight for this training so
    you extract the maximum potential from your data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们使用之前定义的序列数组来训练我们的网络。这可能需要一些时间，因此我们在每200次迭代后添加了打印语句。我建议你在训练时让计算机整夜运行，这样你就能从数据中提取最大潜力。
- en: The next step is to use our model in predicting an actual output. Let's see
    how well it can handle this task.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用我们的模型预测实际输出。让我们看看它能多好地完成这个任务。
- en: Building a conversation
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建对话
- en: This step is really similar to the training one. The first difference is that
    we don't make any evaluation of our predictions, but instead use the input to
    generate the results. The second difference is that we use the already trained
    set of variables to yield this result. You will see how it is done later in this
    chapter.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步实际上与训练步骤非常相似。第一个区别是我们不会对预测结果进行评估，而是使用输入生成结果。第二个区别是我们使用已训练好的变量集来生成此结果。你将看到本章后面如何实现这一点。
- en: 'To make things clearer, we first initialize a new sequence-to-sequence model.
    Its purpose is to use the already trained weights and biases and make predictions
    based on different sets of inputs. We only have an encoder and decoder sequence,
    where the encoder one is an input sentence and the decoder sequence is fed one
    word at a time. We define the new model as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地说明，我们首先初始化一个新的序列到序列的模型。其目的是使用已经训练好的权重和偏置，并根据不同的输入集进行预测。我们只有一个编码器和解码器序列，其中编码器序列是输入句子，而解码器序列则是一次输入一个单词。我们定义新模型如下：
- en: '[PRE13]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see, it follows exactly the same pattern as the training architecture,
    with the difference that our sequence matrices are of shape `1`, instead of `batch_size`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，它遵循与训练架构完全相同的模式，唯一的区别是我们的序列矩阵形状为 `1`，而不是 `batch_size`。
- en: An important thing to note is that when calculating the network's results, we
    must **reuse** the same parameters used during training. This step is essential
    because it makes sure our prediction is a result of the recent training we have
    done.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，在计算网络结果时，我们必须**重用**训练过程中使用的相同参数。这一步至关重要，因为它确保了我们的预测是最近训练结果的产物。
- en: Finally, we calculate the final output, `y`, using the softmax function. This
    is usually done at the final layer to make sure that our vector values sum up
    to 1, and is a necessary step during classification.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用 softmax 函数计算最终输出`y`。这通常在最后一层完成，以确保我们的向量值加起来为 1，并且是分类过程中的必要步骤。
- en: 'After defining our new model, the time comes for the actual prediction. We
    follow this pattern:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义好我们的新模型后，接下来就是进行实际预测的时刻。我们遵循以下模式：
- en: Generate an initial sentence that will start the conversation.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个初始句子，作为对话的开端。
- en: Convert the sentence into a list of word indices using the `word2idx` dictionary.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `word2idx` 字典将句子转换为单词索引列表。
- en: Decide how many replies back and forth we want the conversation to have (in
    our case, this would be five).
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定我们希望对话有多少次来回交互（在我们的案例中，这将是五次）。
- en: Calculate the final state of the encoder by feeding the `net_rnn` (as defined
    previously) with the initial sentence.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将初始句子传入 `net_rnn`（如前所定义）来计算编码器的最终状态。
- en: Finally, we iteratively predict the next word using the previously predicted
    word and the network. At the first time step, we use `start_id`, as defined previously,
    as the first word from the decoder.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用之前预测的单词和网络迭代地预测下一个单词。在第一次迭代时，我们使用先前定义的 `start_id` 作为解码器的第一个单词。
- en: 'These steps are executed in the following code snippet:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤在以下代码片段中执行：
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'An interesting thing to note is how `# 2\. decode, feed start_id, get first
    word` and `# 3\. decode, feed state iteratively` perform exactly the same action,
    but step #2 is a special case, focused on predicting only the first word. Step
    #3 uses this first word to iteratively predict all the others.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '一个有趣的地方在于，`# 2\. decode, feed start_id, get first word` 和 `# 3\. decode, feed
    state iteratively` 执行的动作完全相同，但步骤 #2 是一个特殊的情况，专注于仅预测第一个单词。步骤 #3 使用第一个单词，迭代地预测后续所有单词。'
- en: '`tl.nlp.sample_top(o[0], top_k=3)` might also be confusing to you. This line
    samples an index from the probability array o[0], where you consider only three
    candidates. The same functionality goes for `w_id = tl.nlp.sample_top(o[0], top_k
    = 2)`. You can learn more on the TensorLayer documentation ([https://tensorlayer.readthedocs.io/en/stable/modules/nlp.html#sampling-functions](https://tensorlayer.readthedocs.io/en/stable/modules/nlp.html#sampling-functions)).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`tl.nlp.sample_top(o[0], top_k=3)` 可能也会让你感到困惑。这一行从概率数组 o[0] 中采样一个索引，考虑的候选项只有三个。同样的功能也适用于
    `w_id = tl.nlp.sample_top(o[0], top_k = 2)`。你可以在 TensorLayer 文档中了解更多内容（[https://tensorlayer.readthedocs.io/en/stable/modules/nlp.html#sampling-functions](https://tensorlayer.readthedocs.io/en/stable/modules/nlp.html#sampling-functions)）。'
- en: Finally, we print the formed sentence of 30 words (we cap the number of words
    per sentence). If you trained the network long enough, you should see some decent
    results. If they don't satisfy you, then extensive work is needed. You will learn
    more about this in the upcoming [Chapter 6](85ffa70b-f86d-4432-9c53-9f3e2ab0e007.xhtml),
    *Improving Your RNN Performance*.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们打印出由 30 个单词组成的句子（我们限制了每个句子的单词数）。如果你训练了足够长的时间，你应该能看到一些不错的结果。如果结果不满意，那就需要进行大量的工作。你将在接下来的[第六章](85ffa70b-f86d-4432-9c53-9f3e2ab0e007.xhtml)中了解更多内容，*提升你的
    RNN 性能*。
- en: Summary
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter reveals a full implementation of a chatbot system that manages
    to construct a short conversation. The prototype shows, in detail, each stage
    of building the intelligent chatbot. This includes collecting data, training the
    network, and making predictions (generating conversation).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本章展示了一个完整的聊天机器人系统实现，该系统能够构建一个简短的对话。原型详细展示了构建智能聊天机器人的每个阶段，包括收集数据、训练网络和进行预测（生成对话）。
- en: For the network's architecture, we use the powerful encoder-decoder sequence-to-sequence
    model that utilizes two recurrent neural networks, while connecting them using
    an encoder vector. For the actual implementation, we make use of a deep learning
    library built on top of TensorFlow, called TensorLayer. It simplifies most of
    the work by introducing simple one-line implementations of standard models such
    as *sequence-to sequence*. In addition, this library is useful for preprocessing
    your data before using it for training.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于网络的架构，我们使用强大的编码器-解码器序列到序列模型，该模型利用两个递归神经网络，并通过编码器向量连接它们。在实际实现中，我们使用一个建立在TensorFlow之上的深度学习库——TensorLayer。通过引入简单的一行代码实现标准模型，如*序列到序列*，它简化了大部分工作。此外，该库在训练之前对数据进行预处理时也非常有用。
- en: The next chapter shifts focus to, probably, the most important part of building
    a recurrent neural network (and any deep learning model), which is how to improve
    your performance and actually make your program return satisfying results. As
    you have already seen, building a neural network follows a similar pattern in
    most basic/medium examples. The hard part is to make sure the implementations
    are actually useful and produce meaningful results. This will be the focus of
    our next chapter—I hope you enjoy it.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将重点讨论构建递归神经网络（以及任何深度学习模型）过程中，可能是最重要的部分——如何提升性能并使程序返回令人满意的结果。正如你已经看到的，在大多数基础/中等难度的示例中，构建神经网络遵循类似的模式。困难的部分是确保实现确实有效并产生有意义的结果。这将是我们下一章的重点——希望你能喜欢。
- en: External links
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部链接
- en: TensorLayer chatbot code example: [https://github.com/tensorlayer/seq2seq-chatbot](https://github.com/tensorlayer/seq2seq-chatbot)
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorLayer聊天机器人代码示例：[https://github.com/tensorlayer/seq2seq-chatbot](https://github.com/tensorlayer/seq2seq-chatbot)
- en: TensorLayer library: [https://tensorlayer.readthedocs.io/en/stable/](https://tensorlayer.readthedocs.io/en/stable/)
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorLayer库：[https://tensorlayer.readthedocs.io/en/stable/](https://tensorlayer.readthedocs.io/en/stable/)
- en: Layers in neural network:[ https://www.youtube.com/watch?v=FK77zZxaBoI](https://www.youtube.com/watch?v=FK77zZxaBoI)
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络中的层：[https://www.youtube.com/watch?v=FK77zZxaBoI](https://www.youtube.com/watch?v=FK77zZxaBoI)
- en: What is masking in a recurrent neural network (RNN)?: [https://www.quora.com/What-is-masking-in-a-recurrent-neural-network-RNN](https://www.quora.com/What-is-masking-in-a-recurrent-neural-network-RNN)
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归神经网络（RNN）中的掩码是什么？：[https://www.quora.com/What-is-masking-in-a-recurrent-neural-network-RNN](https://www.quora.com/What-is-masking-in-a-recurrent-neural-network-RNN)
- en: TensorLayer's embeddingInputlayer class: [https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.EmbeddingInputlayer](https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.EmbeddingInputlayer)
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorLayer的embeddingInputlayer类：[https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.EmbeddingInputlayer](https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.EmbeddingInputlayer)
- en: TensorLayer's `retrieve_seq_length_op2` method: [https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.retrieve_seq_length_op2](https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.retrieve_seq_length_op2)
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorLayer的`retrieve_seq_length_op2`方法：[https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.retrieve_seq_length_op2](https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.retrieve_seq_length_op2)
- en: TensorFlow session's config parameter: [https://stackoverflow.com/questions/44873273/what-do-the-options-in-configproto-like-allow-soft-placement-and-log-device-plac](https://stackoverflow.com/questions/44873273/what-do-the-options-in-configproto-like-allow-soft-placement-and-log-device-plac)
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow会话的config参数：[https://stackoverflow.com/questions/44873273/what-do-the-options-in-configproto-like-allow-soft-placement-and-log-device-plac](https://stackoverflow.com/questions/44873273/what-do-the-options-in-configproto-like-allow-soft-placement-and-log-device-plac)
- en: TensorLayer Sampling Functions: [https://tensorlayer.readthedocs.io/en/stable/modules/nlp.html#sampling-functions](https://tensorlayer.readthedocs.io/en/stable/modules/nlp.html#sampling-functions)
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorLayer采样函数：[https://tensorlayer.readthedocs.io/en/stable/modules/nlp.html#sampling-functions](https://tensorlayer.readthedocs.io/en/stable/modules/nlp.html#sampling-functions)
