- en: Introduction to Deep Learning in Java
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Java中的深度学习简介
- en: Let's discuss various deep learning libraries so as to pick the best for the
    purpose at hand. This is a context-dependent decision and will vary according
    to the situation. In this chapter, we will start with a brief introduction to
    deep learning and explore how DL4J is a good choice for solving deep learning
    puzzles. We will also discuss how to set up DL4J in your workspace.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下各种深度学习库，以便选择最适合当前任务的库。这是一个依赖于具体情境的决策，会根据情况有所不同。在本章中，我们将首先简要介绍深度学习，并探讨为什么DL4J是解决深度学习问题的一个好选择。我们还将讨论如何在工作空间中设置DL4J。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Deep learning intuition
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的直觉
- en: Determining the right network type to solve deep learning problems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定解决深度学习问题的正确网络类型
- en: Determining the right activation function
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定正确的激活函数
- en: Combating overfitting problems
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克服过拟合问题
- en: Determining the right batch size and learning rates
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定正确的批处理大小和学习率
- en: Configuring Maven for DL4J
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置Maven以支持DL4J
- en: Configuring DL4J for a GPU-accelerated environment
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置DL4J以支持GPU加速环境
- en: Troubleshooting installation issues
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决安装问题
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You''ll need the following to get the most out of this cookbook:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要充分利用这本食谱，你需要以下内容：
- en: Java SE 7, or higher, installed
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装Java SE 7或更高版本
- en: Basic core Java knowledge
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本的Java核心知识
- en: DL4J basics
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL4J基础
- en: Maven basics
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maven基础
- en: Basic data analytical skills
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本的数据分析技能
- en: Deep learning/machine learning basics
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习/机器学习基础
- en: OS command basics (Linux/Windows)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统命令基础（Linux/Windows）
- en: IntelliJ IDEA IDE (this is a very easy and hassle-free way of managing code;
    however, you're free to try another IDE, such as Eclipse)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IntelliJ IDEA IDE（这是管理代码的非常简便和无烦恼的方法；不过，你也可以尝试其他IDE，如Eclipse）
- en: Spring Boot basics (to integrate DL4J with Spring Boot for use with web applications)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spring Boot基础（将DL4J与Spring Boot集成，用于Web应用）
- en: We use DL4J version 1.0.0-beta3 throughout this book except for [Chapter 7](ade34354-ad22-4d3e-b45e-ce112947df49.xhtml),
    *Constructing an LSTM Neural Network for Sequence Classification*, where we used
    the current latest version, 1.0.0-beta4, to avoid bugs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们使用的是DL4J版本1.0.0-beta3，除了[第七章](ade34354-ad22-4d3e-b45e-ce112947df49.xhtml)《构建LSTM神经网络进行序列分类》外，在那里我们使用了最新版本1.0.0-beta4，以避免出现BUG。
- en: Deep learning intuition
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的直觉
- en: 'If you''re a newbie to deep learning, you may be wondering how exactly it is differs
    from machine learning; or is it the same? Deep learning is a subset of the larger
    domain of machine learning. Let''s think about this in the context of an automobile image classification
    problem:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是深度学习的新手，可能会好奇它到底与机器学习有什么不同；还是一样的？深度学习是机器学习这个大领域的一个子集。让我们以汽车图像分类问题为例来思考这个问题：
- en: '![](img/aff78d29-b0a5-4b31-984b-c329227ed7da.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aff78d29-b0a5-4b31-984b-c329227ed7da.png)'
- en: 'As you can see in the preceding diagram, we need to perform feature extraction
    ourselves as legacy machine learning algorithms cannot do that on their own. They
    might be super-efficient with accurate results, but they cannot learn signals
    from data. In fact, they don''t learn on their own and still rely on human effort:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前面的图示中看到的，我们需要自行执行特征提取，因为传统的机器学习算法无法自行完成这些工作。它们可能在准确性上非常高效，但无法从数据中学习信号。事实上，它们并不会自己学习，仍然依赖于人类的努力：
- en: '![](img/78a06015-1bc7-42a6-bf4c-df580e370f0a.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78a06015-1bc7-42a6-bf4c-df580e370f0a.png)'
- en: On the other hand, deep learning algorithms learn to perform tasks on their
    own. Neural networks under the hood are based on the concept of deep learning
    and it trains on their own to optimize the results. However, the final decision
    process is hidden and cannot be tracked. The intent of deep learning is to imitate
    the functioning of a human brain.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，深度学习算法通过自我学习来执行任务。神经网络的工作原理基于深度学习的概念，它们通过自身训练来优化结果。然而，最终的决策过程是隐藏的，无法追踪。深度学习的目标是模仿人类大脑的工作方式。
- en: Backpropagation
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: 'The backbone of a neural network is the backpropagation algorithm. Refer to
    the sample neural network structure shown as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的核心是反向传播算法。请参阅下图所示的示例神经网络结构：
- en: '![](img/fc9dffda-d154-47bc-852c-25b7fca57cb9.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc9dffda-d154-47bc-852c-25b7fca57cb9.png)'
- en: For any neural network, data flows from the input layer to the output layer
    during the forward pass. Each circle in the diagram represents a neuron. Every
    layer has a number of neurons present. Our data will pass through the neurons
    across layers. The input needs to be in a numerical format to support computational
    operations in neurons. Each neuron in the neural network is assigned a weight
    (matrix) and an activation function. Using the input data, weight matrix, and
    an activation function, a probabilistic value is generated at each neuron. The
    error (that is, a deviation from the actual value) is calculated at the output
    layer using a loss function. We utilize the loss score during the backward pass
    (that is, from the output layer to the input layer ) by reassigning weights to
    the neurons to reduce the loss score. During this stage, some output layer neurons
    will be assigned with high weights and vice versa depending upon the loss score
    results. This process will continue backward as far as the input layer by updating
    the weights of neurons. In a nutshell, we are tracking the rate of change of loss
    with respect to the change in weights across all neurons. This entire cycle (a
    forward and backward pass) is called an epoch. We perform multiple epochs during
    a training session. A neural network will tend to optimize the results after every
    training epoch.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何神经网络，在前向传播过程中，数据从输入层流向输出层。图中的每个圆圈代表一个神经元。每一层都有若干个神经元。我们的数据将在各层之间的神经元中流动。输入需要是数值格式，以支持神经元中的计算操作。每个神经元都分配有一个权重（矩阵）和一个激活函数。通过输入数据、权重矩阵和激活函数，生成每个神经元的概率值。通过损失函数，在输出层计算误差（即与实际值的偏差）。我们在反向传播过程中利用损失值（即从输出层到输入层），通过重新分配权重给神经元来减少损失值。在这个阶段，一些输出层的神经元会分配较高的权重，反之亦然，这取决于损失值的结果。这个过程将一直向后推进，直到输入层，通过更新神经元的权重。在简言之，我们正在追踪损失相对于神经元权重变化的变化率。整个过程（前向传播和反向传播）被称为一个周期（epoch）。在训练过程中，我们会进行多个周期。神经网络会在每个训练周期后优化结果。
- en: Multilayer Perceptron (MLP)
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器（MLP）
- en: 'An MLP is a standard feed-forward neural network with at least three layers:
    an input layer, a hidden layer, and an output layer. Hidden layers come after
    the input layer in the structure. Deep neural networks have two or more hidden
    layers in the structure, while an MLP has only one.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器（MLP）是一个标准的前馈神经网络，至少有三层：输入层、隐藏层和输出层。隐藏层在结构中位于输入层之后。深度神经网络在结构中有两层或更多的隐藏层，而MLP只有一层。
- en: Convolutional Neural Network (CNN)
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）
- en: CNNs are generally used for image classification problems, but can also be exposed
    in **Natural Language Processing** (**NLP**), in conjunction with word vectors,
    because of their proven results. Unlike a regular neural network, a CNN will have
    additional layers such as convolutional layers and subsampling layers. Convolutional
    layers take input data (such as images) and apply convolution operations on top
    of them. You can think of it as applying a function to the input. Convolutional
    layers act as filters that pass a feature of interest to the upcoming subsampling
    layer. A feature of interest can be anything (for example, a fur, shade and so
    on in the case of an image) that can be used to identify the image. In the subsampling
    layer, the input from convolutional layers is further smoothed. So, we end up
    with a much smaller image resolution and reduced color contrast, preserving only
    the important information. The input is then passed on to fully connected layers.
    Fully connected layers resemble regular feed-forward neural networks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络通常用于图像分类问题，但由于其良好的效果，也可以应用于**自然语言处理**（**NLP**），与词向量结合使用。与普通神经网络不同，CNN会有额外的层，如卷积层和子采样层。卷积层接受输入数据（如图像），并在其上执行卷积操作。你可以把它理解为对输入应用一个函数。卷积层充当过滤器，将感兴趣的特征传递给下一个子采样层。感兴趣的特征可以是任何东西（例如，在图像的情况下，可以是毛发、阴影等），这些特征可以用来识别图像。在子采样层，来自卷积层的输入会被进一步平滑处理。因此，我们最终得到的是一个分辨率较小、色彩对比度较低的图像，保留了重要的信息。然后，输入会传递给全连接层。全连接层类似于常规的前馈神经网络。
- en: Recurrent Neural Network (RNN)
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）
- en: An RNN is a neural network that can process sequential data. In a regular feed-forward
    neural network, the current input is considered for neurons in the next layer.
    On the other hand, an RNN can accept previously received inputs as well. It can
    also use memory to memorize previous inputs. So, it is capable of preserving long-term
    dependencies throughout the training session. RNN is a popular choice for NLP
    tasks such as speech recognition. In practice, a slightly variant structure called
    **Long Short-Term Memory** (**LSTM**) is used as a better alternative to RNN.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: RNN是一个能够处理序列数据的神经网络。在常规的前馈神经网络中，当前的输入仅对下一个层的神经元有影响。相反，RNN不仅可以接受当前的输入，还能接受先前的输入。它还可以利用内存来记忆之前的输入。因此，它能够在整个训练过程中保持长期依赖关系。RNN在自然语言处理任务中，特别是在语音识别中非常流行。在实践中，稍作变化的结构**长短期记忆网络**（**LSTM**）常常作为RNN的更好替代方案。
- en: Why is DL4J important for deep learning?
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么DL4J对深度学习如此重要？
- en: 'The following points will help you understand why DL4J is important for deep
    learning:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下几点将帮助你理解为什么DL4J在深度学习中如此重要：
- en: DL4J provides commercial support. It is the first commercial-grade, open source,
    deep learning library in Java.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL4J提供商业支持。它是第一个商用级别的、开源的、Java中的深度学习库。
- en: Writing training code is simple and precise. DL4J supports Plug and Play mode,
    which means switching between hardware (CPU to GPU) is just a matter of changing
    the Maven dependencies and no modifications are needed on the code.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写训练代码简单而精确。DL4J支持即插即用模式，这意味着在硬件之间切换（从CPU到GPU）只需修改Maven依赖项，无需更改代码。
- en: DL4J uses ND4J as its backend. ND4J is a computation library that can run twice
    as fast as NumPy (a computation library in Python) in large matrix operations.
    DL4J exhibits faster training times in GPU environments compared to other Python
    counterparts.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL4J使用ND4J作为其后端。ND4J是一个计算库，在大规模矩阵运算中，其速度比NumPy（Python中的计算库）快两倍。与其他Python库相比，DL4J在GPU环境下展现出更快的训练速度。
- en: DL4J supports training on a cluster of machines that are running in CPU/GPU
    using Apache Spark. DL4J brings in automated parallelism in distributed training.
    This means that DL4J bypasses the need for extra libraries by setting up worker
    nodes and connections.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL4J支持在使用Apache Spark的集群机器上进行训练，无论是CPU还是GPU。DL4J引入了分布式训练中的自动并行化。这意味着DL4J通过设置工作节点和连接，绕过了额外库的需求。
- en: DL4J is a good production-oriented deep learning library. As a JVM-based library,
    DL4J applications can be easily integrated/deployed with existing corporate applications
    that are running in Java/Scala.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL4J是一个非常适合生产的深度学习库。作为一个基于JVM的库，DL4J应用可以轻松与现有的运行在Java/Scala中的企业应用集成或部署。
- en: Determining the right network type to solve deep learning problems
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定解决深度学习问题的正确网络类型
- en: It is crucial to identify the right neural network type to solve a business
    problem efficiently. A standard neural network can be a best fit for most use
    cases and can produce approximate results. However, in some scenarios, the core
    neural network architecture needs to be changed in order to accommodate the features
    (input) and to produce the desired results. In the following recipe, we will walk
    through key steps to decide the best network architecture for a deep learning
    problem with the help of known use cases.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 识别正确的神经网络类型对于高效解决业务问题至关重要。标准的神经网络对于大多数用例而言是最佳选择，并能产生近似的结果。然而，在某些场景下，核心神经网络架构需要进行修改，以便适应特征（输入）并产生所需的结果。在下面的实例中，我们将通过已知用例的帮助，逐步介绍如何为深度学习问题选择最佳的网络架构。
- en: How to do it...
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: Determine the problem type.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定问题类型。
- en: Determine the type of data engaged in the system.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定系统中所涉及的数据类型。
- en: How it works...
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'To solve use cases effectively, we need to use the right neural network architecture
    by determining the problem type. The following are globally some use cases and
    respective problem types to consider for step 1:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地解决用例，我们需要通过确定问题类型来使用正确的神经网络架构。以下是一些全球性的用例及其相应的 problem 类型，供第一步参考：
- en: '**Fraud detection problems**:We want to differentiate between legitimate and
    suspicious transactions so as to separate unusual activities from the entire activity
    list. The intent is to reduce false-positive (that is, incorrectly tagging legitimate
    transactions as fraud) cases. Hence, this is an anomaly detection problem.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欺诈检测问题**：我们希望区分合法和可疑交易，以便从整个交易列表中分离出异常活动。目标是减少假阳性（即错误地将合法交易标记为欺诈）案例。因此，这是一个异常检测问题。'
- en: '**Prediction problems**:Prediction problems can be classification or regression
    problems. For labeled classified data, we can have discrete labels. We need to
    model data against those discrete labels. On the other hand, regression models
    don''t have discrete labels.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测问题**：预测问题可以是分类问题或回归问题。对于标记的分类数据，我们可以有离散标签。我们需要针对这些离散标签对数据进行建模。另一方面，回归模型没有离散标签。'
- en: '**Recommendation problems**:You would need to build a recommender system (a
    recommendation engine) to recommend products or content to customers. Recommendation
    engines can also be applied to an agent performing tasks such as gaming, autonomous
    driving, robotic movements, and more. Recommendation engines implement reinforcement learning
    and can be enhanced further by introducing deep learning into it.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐问题**：你需要构建一个推荐系统（推荐引擎）来向客户推荐产品或内容。推荐引擎还可以应用于执行任务的代理，例如游戏、自动驾驶、机器人运动等。推荐引擎实施强化学习，并且通过引入深度学习可以进一步增强。'
- en: 'We also need to know the type of data that is consumed by the neural network.
    Here are some use cases and respective data types for step 2:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要知道神经网络所消耗的数据类型。以下是一些使用案例及其相应的数据类型，适用于步骤2：
- en: '**Fraud detection problems**:Transactions usually happen over a number of time
    steps. So, we need to continuously collect transaction data over time. This is
    an example of time series data. Each time sequence represents a new transaction
    sequence. These time sequences can be regular or irregular. For instance, if you
    have credit card transaction data to analyze, then you have labeled data. You
    can also have unlabeled data in the case of user metadata from production logs. We
    can have supervised/unsupervised datasets for fraud detection analysis, for example.
    Take a look at the following CSV supervised dataset:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欺诈检测问题**：交易通常发生在若干时间步骤中。因此，我们需要持续收集交易数据。这是一个时间序列数据的例子。每个时间序列代表一组新的交易记录。这些时间序列可以是规则的或不规则的。例如，如果你有信用卡交易数据需要分析，那么你拥有的是有标签的数据。如果是来自生产日志的用户元数据，则可能是无标签数据。我们可以有用于欺诈检测分析的有监督/无监督数据集，例如。看看以下的CSV有监督数据集：'
- en: '![](img/d882cc72-71ce-454a-9289-1cb7b3261bbf.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d882cc72-71ce-454a-9289-1cb7b3261bbf.png)'
- en: In the preceding screenshot, features such as `amount`, `oldBalanceOrg`, and
    so on make sense and each record has a label indicating whether the particular
    observation is fraudulent or not.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，像`amount`、`oldBalanceOrg`等特征是有意义的，每个记录都有一个标签，指示该观察是否为欺诈。
- en: 'On the other hand, an unsupervised dataset will not give you any clue about
    input features. It doesn''t have any labels either, as shown in the following
    CSV data:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，无监督数据集不会为你提供任何关于输入特征的线索。它也没有任何标签，如下所示的CSV数据所示：
- en: '![](img/be8a3dd1-e5e9-4241-ab1c-f036d08f883a.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be8a3dd1-e5e9-4241-ab1c-f036d08f883a.png)'
- en: As you can see, the feature labels (top row) follow a numbered naming convention
    without any clue as to its significance for fraud detection outcomes. We can also
    have time series data where transactions are logged over a series of time steps.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，特征标签（顶部行）遵循一个编号命名规范，但没有任何关于其对欺诈检测结果重要性的提示。我们还可以拥有时间序列数据，其中交易记录会在一系列时间步骤中被记录。
- en: '**Prediction problems**:Historical data collected from organizations can be
    used to train neural networks. These are usually simple file types such as a CSV/text
    files. Data can be obtained as records. For a stock market prediction problem,
    the data type would be a time series. A dog breed prediction problem requires
    feeding in dog images for network training. Stock price prediction is an example
    of a regression problem. Stock price datasets usually are time series data where
    stock prices are measured over a series as follows:'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测问题**：从组织收集的历史数据可以用来训练神经网络。这些数据通常是简单的文件类型，如CSV/text文件。数据可以作为记录获得。对于股市预测问题，数据类型将是时间序列。狗品种预测问题需要提供狗的图片来训练网络。股票价格预测是回归问题的一个例子。股票价格数据集通常是时间序列数据，其中股价在一系列时间步骤中被记录，如下所示：'
- en: '![](img/0061cfc1-2139-4c7a-bbd5-612b220c2b28.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0061cfc1-2139-4c7a-bbd5-612b220c2b28.png)'
- en: 'In most stock price datasets, there are multiple files. Each one of them represents
    a company stock market. And each file will have stock prices recorded over a series
    of time steps, as shown here:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数股票价格数据集中，包含多个文件。每个文件代表一个公司股市。每个文件都记录了股价在一系列时间步骤中的变化，如此处所示：
- en: '![](img/f553df10-3d7d-4339-9a60-14d4afe47f67.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f553df10-3d7d-4339-9a60-14d4afe47f67.png)'
- en: '**Recommendation problems**:For a product recommendation system, explicit data
    might be customer reviews posted on a website and implicit data might be the customer
    activity history, such as product search or purchase history. We will use unlabeled data
    to feed the neural network. Recommender systems can also solve games or learn
    a job that requires skills. Agents (trained to perform tasks during reinforcement learning)
    can take real-time data in the form of image frames or any text data (unsupervised)
    to learn what actions to make depending on their states.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐问题**：对于产品推荐系统，显性数据可能是网站上发布的客户评价，而隐性数据可能是客户活动历史，例如产品搜索或购买历史。我们将使用未标注的数据来馈送神经网络。推荐系统还可以解决游戏问题或学习需要技能的工作。代理（在强化学习过程中训练以执行任务）可以通过图像帧或任何文本数据（无监督）实时获取数据，根据其状态学习该采取何种行动。'
- en: There's more...
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The following are possible deep learning solutions to the problem types previously
    discussed:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是可能的深度学习解决方案，用于之前讨论的不同问题类型：
- en: '**Fraud detection problems**: The optimal solution varies according to the
    data. We previously mentioned two data sources. One was credit card transactions
    and the other was user metadata based on their login/logoff activities. In the
    first case, we have labeled data and have a transaction sequence to analyze.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欺诈检测问题**：最优解决方案根据数据的不同而有所变化。我们之前提到了两种数据源。一种是信用卡交易，另一种是基于用户登录/注销活动的元数据。在第一种情况下，我们有标注数据，并且有交易序列需要分析。'
- en: Recurrent networks may be best suited to sequencing data. You can add LSTM ([https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/layers/recurrent/LSTM.html](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/layers/recurrent/LSTM.html)) recurrent
    layers, and DL4J has an implementation for that. For the second case, we have
    unlabeled data and the best choice would be a variational ([https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/layers/variational/VariationalAutoencoder.html](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/layers/variational/VariationalAutoencoder.html)) autoencoder
    to compress unlabeled data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络可能最适合处理序列数据。你可以添加LSTM（[https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/layers/recurrent/LSTM.html](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/layers/recurrent/LSTM.html)）循环层，DL4J也有相应的实现。对于第二种情况，我们有未标注的数据，最佳选择是变分自编码器（[https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/layers/variational/VariationalAutoencoder.html](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/layers/variational/VariationalAutoencoder.html)）来压缩未标注数据。
- en: '**Prediction problems**: For classification problems that use CSV records,
    a feed-forward neural network will do. For time series data, the best choice would
    be recurrent networks because of the nature of sequential data. For image classification
    problems, you would need a CNN ([https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/ConvolutionLayer.Builder.html)](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/ConvolutionLayer.Builder.html).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测问题**：对于使用CSV记录的分类问题，前馈神经网络足够了。对于时间序列数据，最佳选择是循环神经网络，因为数据具有序列性。对于图像分类问题，你需要使用CNN（[https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/ConvolutionLayer.Builder.html)](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/ConvolutionLayer.Builder.html)。'
- en: '**Recommendation problems**: We can employ **Reinforcement Learning** (**RL**)
    to solve recommendation problems. RL is very often used for such use cases and
    might be a better option. RL4J was specifically developed for this purpose. We
    will introduce RL4J in [Chapter 9](9d49be5b-7d29-47f8-848d-b1c5c1b742e9.xhtml),
    *Using RL4J for Reinforcement Learning*, as it would be an advanced topic at this
    point. We can also go for simpler options such as feed-forward networks RNNs)
    with a different approach. We can feed an unlabeled data sequence to recurrent
    or convolutional layers as per the data type (image/text/video). Once the recommended
    content/product is classified, you can apply further logic to pull random products
    from the list based on customer preferences.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐问题**：我们可以使用**强化学习**（**RL**）来解决推荐问题。强化学习通常用于此类应用场景，并且可能是更好的选择。RL4J是专为此目的开发的。我们将在[第9章](9d49be5b-7d29-47f8-848d-b1c5c1b742e9.xhtml)中介绍RL4J，*使用RL4J进行强化学习*，因为此时它将是一个较为高级的话题。我们还可以选择更简单的选项，如前馈神经网络（RNN）并采用不同的方法。我们可以将未标注的数据序列馈送到循环神经网络或卷积层，具体根据数据类型（图像/文本/视频）。一旦推荐的内容/产品被分类，你可以应用进一步的逻辑，从列表中根据客户偏好随机拉取产品。'
- en: In order to choose the right network type, you need to understand the type of
    data and the problem it tries to solve. The most basic neural network that you
    could construct is a feed-forward network or a multilayer perceptron. You can
    create multilayer network architectures using `NeuralNetConfiguration` in DL4J.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择合适的网络类型，你需要了解数据的类型以及它试图解决的问题。你可以构建的最基本的神经网络是前馈网络或多层感知器。你可以在DL4J中使用`NeuralNetConfiguration`来创建多层网络架构。
- en: 'Refer to the following sample neural network configuration in DL4J:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅以下在DL4J中的神经网络配置示例：
- en: '[PRE0]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We specify activation functions for every layer in a neural network, and `nIn()`
    and `nOut()` represent the number of connections in/out of the layer of neurons.
    The purpose of the `dropOut()` function is to deal with network performance optimization.
    We mentioned it in [Chapter 3](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml), *Building
    Deep Neural Networks for Binary Classification*. Essentially, we are ignoring
    some neurons at random to avoid blindly memorizing patterns during training. Activation
    functions will be discussed in the *Determining the right activation function *recipe
    in this chapter. Other attributes control how weights are distributed between
    neurons and how to deal with errors calculated across each epoch.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为神经网络中的每一层指定激活函数，`nIn()`和`nOut()`表示神经元层的输入/输出连接数。`dropOut()`函数的目的是优化网络性能。我们在[第3章](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml)中提到过这一点，*构建用于二分类的深度神经网络*。本质上，我们通过随机忽略一些神经元来避免在训练过程中盲目记忆模式。激活函数将在本章的*确定合适的激活函数*部分讨论。其他属性则控制神经元之间权重的分配以及如何处理每个周期计算出的误差。
- en: 'Let''s focus on a specific decision-making process: choosing the right network
    type. Sometimes, it is better to use a custom architecture to yield better results.
    For example, you can perform sentence classification using word vectors combined
    with a CNN. DL4J offers the `ComputationGraph` ([https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/graph/ComputationGraph.html](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/graph/ComputationGraph.html))
    implementation to accommodate CNN architecture.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们专注于一个具体的决策过程：选择合适的网络类型。有时，使用自定义架构会得到更好的结果。例如，你可以使用词向量结合CNN来执行句子分类。DL4J提供了`ComputationGraph`([https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/graph/ComputationGraph.html](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/graph/ComputationGraph.html))实现，以适应CNN架构。
- en: '`ComputationGraph` allows an arbitrary (custom) neural network architecture.
    Here is how it is defined in DL4J:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`ComputationGraph`允许任意（自定义）神经网络架构。以下是它在DL4J中的定义：'
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Implementing a CNN is just like constructing network layers for a feed-forward
    network:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 实现CNN就像为前馈网络构建网络层一样：
- en: '[PRE2]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A CNN has `ConvolutionalLayer` and `SubsamplingLayer` apart from `DenseLayer`
    and `OutputLayer`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一个CNN除了`DenseLayer`和`OutputLayer`外，还包括`ConvolutionalLayer`和`SubsamplingLayer`。
- en: Determining the right activation function
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定合适的激活函数
- en: The purpose of an activation function is to introduce non-linearity into a neural
    network. Non-linearity helps a neural network to learn more complex patterns.
    We will discuss some important activation functions, and their respective DL4J
    implementations.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的目的是引入非线性到神经网络中。非线性有助于神经网络学习更复杂的模式。我们将讨论一些重要的激活函数及其相应的DL4J实现。
- en: 'The following are the activation functions that we will consider:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们将考虑的激活函数：
- en: Tanh
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanh
- en: Sigmoid
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid
- en: ReLU (short for **Rectified Linear Unit**)
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU（**修正线性单元**的简称）
- en: Leaky ReLU
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leaky ReLU
- en: Softmax
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Softmax
- en: In this recipe, we will walk through the key steps to decide the right activation
    functions for a neural network.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将介绍决定神经网络激活函数的关键步骤。
- en: How to do it...
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: '**Choose an activation function according to the network layers**: We need
    to know the activation functions to be used for the input/hidden layers and output
    layers. Use ReLU for input/hidden layers preferably.'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**根据网络层选择激活函数**：我们需要知道输入/隐藏层和输出层使用的激活函数。最好为输入/隐藏层使用ReLU。'
- en: '**Choose the right activation function to handle data impurities**: Inspect
    the data that you feed to the neural network. Do you have inputs with a majority
    of negative values observing dead neurons? Choose the appropriate activation functions
    accordingly. Use Leaky ReLU if dead neurons are observed during training.'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择合适的激活函数来处理数据杂质**：检查馈送给神经网络的数据。你是否有大部分为负值的输入，导致死神经元出现？根据情况选择合适的激活函数。如果在训练中观察到死神经元，请使用Leaky
    ReLU。'
- en: '**Choose the right activation function to handle overfitting**: Observe the
    evaluation metrics and their variation for each training period. Understand gradient
    behavior and how well your model performs on new unseen data.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择合适的激活函数以应对过拟合**：观察每个训练周期的评估指标及其变化。理解梯度行为以及模型在新未见数据上的表现。'
- en: '**Choose the right activation function as per the expected output of your use
    case**: Examine the desired outcome of your network as a first step. For example,
    the SOFTMAX function can be used when you need to measure the probability of the
    occurrence of the output class. It is used in the output layer. For any input/hidden
    layers, ReLU is what you need for most cases. If you''re not sure about what to
    use, just start experimenting with ReLU; if that doesn''t improve your expectations,
    then try other activation functions.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**根据预期输出选择合适的激活函数**：首先检查网络的期望结果。例如，当你需要衡量输出类别发生的概率时，可以使用SOFTMAX函数。它通常用于输出层。对于任何输入/隐藏层，大多数情况下你需要使用ReLU。如果不确定该使用哪种激活函数，可以先尝试使用ReLU；如果它没有改善你的期望，再尝试其他激活函数。'
- en: How it works...
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: For step 1, ReLU is most commonly used because of its non-linear behavior. The
    output layer activation function depends on the expected output behavior. Step
    4 targets this too.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第1步，ReLU因其非线性行为而被广泛使用。输出层的激活函数取决于期望的输出行为。第4步也针对这一点。
- en: For step 2, Leaky ReLU is an improved version of ReLU and is used to avoid the
    zero gradient problem. However, you might observe a performance drop. We use Leaky
    ReLU if dead neurons are observed during training. Dead neurons are referred to
    as neurons with a zero gradient for all possible inputs, which makes them useless
    for training.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第2步，Leaky ReLU是ReLU的改进版本，用于避免零梯度问题。然而，可能会观察到性能下降。如果在训练过程中观察到死神经元，我们会使用Leaky
    ReLU。死神经元指的是对于所有可能的输入，其梯度为零的神经元，这使得它们在训练中没有任何作用。
- en: For step 3, the tanh and sigmoid activation functions are similar and are used
    in feed-forward networks. If you use these activation functions, then make sure
    you add regularization to network layers to avoid the vanishing gradient problem.
    These are generally used for classifier problems.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第3步，tanh和sigmoid激活函数是相似的，通常用于前馈网络。如果你使用这些激活函数，请确保对网络层进行正则化，以避免梯度消失问题。这些激活函数通常用于分类问题。
- en: There's more...
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The ReLU activation function is non-linear, hence, the backpropagation of errors
    can easily be performed. Backpropagation is the backbone of neural networks. This
    is the learning algorithm that computes gradient descent with respect to weights
    across neurons. The following are ReLU variations currently supported in DL4J:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU激活函数是非线性的，因此，误差的反向传播可以轻松进行。反向传播是神经网络的核心算法。这是一个学习算法，通过计算神经元权重的梯度下降来更新神经网络。以下是目前在DL4J中支持的ReLU变体：
- en: '`ReLU`: The standard ReLU activation function:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReLU`: 标准的ReLU激活函数：'
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`ReLU6`: ReLU activation, which is capped at 6, where 6 is an arbitrary choice:'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReLU6`: ReLU激活函数，其输出最大值为6，6是一个任意选择：'
- en: '[PRE4]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`RReLU`: The randomized ReLU activation function:'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RReLU`: 随机化的ReLU激活函数：'
- en: '[PRE5]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`ThresholdedReLU`: Threshold ReLU:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ThresholdedReLU`: 阈值ReLU：'
- en: '[PRE6]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: There are a few more implementations, such as **SeLU** (short for the **Scaled
    Exponential Linear Unit**), which is similar to the ReLU activation function but
    has a slope for negative values.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他实现，比如**SeLU**（**缩放指数线性单元**的缩写），它与ReLU激活函数类似，但对于负值有一个斜率。
- en: Combating overfitting problems
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应对过拟合问题
- en: As we know, overfitting is a major challenge that machine learning developers
    face. It becomes a big challenge when the neural network architecture is complex
    and training data is huge. While mentioning overfitting, we're not ignoring the
    chances of underfitting at all. We will keep overfitting and underfitting in the
    same category. Let's discuss how we can combat overfitting problems.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，过拟合是机器学习开发者面临的主要挑战。当神经网络架构复杂且训练数据庞大时，过拟合问题尤为严重。提到过拟合时，我们并没有忽视欠拟合的可能性。我们将过拟合和欠拟合放在同一个类别中讨论。让我们讨论一下如何应对过拟合问题。
- en: 'The following are possible reasons for overfitting, including but not limited
    to:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 可能导致过拟合的原因包括但不限于：
- en: Too many feature variables compared to the number of data records
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征变量的数量相对于数据记录的数量过多。
- en: A complex neural network model
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个复杂的神经网络模型
- en: Self-evidently, overfitting reduces the generalization power of the network
    and the network will fit noise instead of a signal when this happens. In this
    recipe, we will walk through key steps to prevent overfitting problems.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 显而易见，过拟合会降低网络的泛化能力，当发生过拟合时，网络将会拟合噪声而不是信号。在这个方案中，我们将介绍预防过拟合问题的关键步骤。
- en: How to do it...
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Use `KFoldIterator` to perform k-fold cross-validation-based resampling:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`KFoldIterator`进行基于k折交叉验证的重采样：
- en: '[PRE7]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Construct a simpler neural network architecture.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个更简单的神经网络架构。
- en: Use enough train data to train the neural network.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用足够的训练数据来训练神经网络。
- en: How it works...
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In step 1, **`k`** is the arbitrary number of choice and `dataSet` is the dataset
    object that represents your training data. We perform k-fold cross-validation
    to optimize the model evaluation process.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤1中，**`k`**是任意选择的数字，而`dataSet`是代表训练数据的数据集对象。我们执行k折交叉验证来优化模型评估过程。
- en: Complex neural network architectures can cause the network to tend to memorize
    patterns. Hence, your neural network will have a hard time generalizing unseen
    data. For example, it's better and more efficient to have a few hidden layers
    rather than hundreds of hidden layers. That's the relevance of step 2.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂的神经网络架构可能导致网络倾向于记忆模式。因此，神经网络将很难对未见过的数据进行泛化。例如，拥有少量的隐藏层要比拥有数百个隐藏层更好、更高效。这就是步骤2的相关性。
- en: 'Fairly large training data will encourage the network to learn better and a
    batch-wise evaluation of test data will increase the generalization power of the
    network. That''s the relevance of step 3. Although there are multiple types of
    data iterator and various ways to introduce batch size in an iterator in DL4J,
    the following is a more conventional definition for `RecordReaderDataSetIterator`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 相对较大的训练数据将鼓励网络更好地学习，而按批次评估测试数据将增加网络的泛化能力。这就是步骤3的相关性。尽管在DL4J中有多种类型的数据迭代器和不同方式引入批量大小，但以下是`RecordReaderDataSetIterator`的更常规定义：
- en: '[PRE8]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: There's more...
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多……
- en: When you perform k-fold cross-validation, data is divided into *k* number of
    subsets. For every subset, we perform evaluation by keeping one of the subsets
    for testing and the remaining *k-1* subsets for training. We will repeat this *k*
    number of times. Effectively, we use the entire data for training with no data
    loss, as opposed to wasting some of the data on testing.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当你执行k折交叉验证时，数据被划分为*k*个子集。对于每个子集，我们通过将其中一个子集用于测试，剩余的*k-1*个子集用于训练来进行评估。我们将重复执行这一过程*k*次。实际上，我们使用所有数据进行训练，而不会丢失任何数据，这与浪费部分数据进行测试是不同的。
- en: Underfitting is handled here. However, note that we perform the evaluation *k*
    number of times only.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这里处理了欠拟合问题。然而，请注意，我们仅执行*k*次评估。
- en: When you perform batch training, the entire dataset is divided as per the batch
    size. If your dataset has 1,000 records and the batch size is 8, then you have
    125 training batches.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当你进行批量训练时，整个数据集会根据批量大小进行划分。如果你的数据集有1,000条记录，而批量大小是8，那么你将有125个训练批次。
- en: You need to note the training-to-testing ratio as well. According to that ratio,
    every batch will be divided into a training set and testing set. Then the evaluation
    will be performed accordingly. For 8-fold cross-validation, you evaluate the model
    8 times, but for a batch size of 8, you perform 125 model evaluations.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要注意训练与测试的比例。根据这个比例，每个批次将被划分为训练集和测试集。然后，评估将根据此进行。对于8折交叉验证，你评估模型8次，但对于批量大小为8，你将进行125次模型评估。
- en: Note the rigorous mode of evaluation here, which will help to improve the generalization
    power while increasing the chances of underfitting.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里严格的评估模式，这将有助于提高泛化能力，同时增加欠拟合的几率。
- en: Determining the right batch size and learning rates
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定正确的批量大小和学习率
- en: Although there is no specific batch size or learning rate that works for all
    models, we can find the best values for them by experimenting with multiple training
    instances. The primary step is to experiment with a set of batch size values and
    learning rates with the model. Observe the efficiency of the model by evaluating
    additional parameters such as `Precision`, `Recall`, and `F1 Score`. Test scores
    alone don't confirm the model's performance. Also, parameters such as `Precision`, `Recall`,
    and `F1 Score` vary according to the use case. You need to analyze your problem
    statement to get an idea about this. In this recipe, we will walk through key
    steps to determine the right batch size and learning rates.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管没有适用于所有模型的特定批量大小或学习率，我们可以通过尝试多个训练实例来找到它们的最佳值。首要步骤是通过模型尝试一组批量大小值和学习率。通过评估额外的参数如`Precision`、`Recall`和`F1
    Score`来观察模型的效率。仅仅测试分数并不能确认模型的性能。同时，诸如`Precision`、`Recall`和`F1 Score`这些参数根据使用情况会有所不同。您需要分析您的问题陈述以了解这一点。在这个示例中，我们将介绍确定正确批量大小和学习率的关键步骤。
- en: How to do it...
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Run the training instance multiple times and track the evaluation metrics.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多次运行训练实例并跟踪评估指标。
- en: Run experiments by increasing the learning rate and track the results.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过增加学习率运行实验并跟踪结果。
- en: How it works...
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理如下...
- en: Consider the following experiments to illustrate step 1.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下实验以说明第1步。
- en: 'The following training was performed on 10,000 records with a batch size of
    8 and a learning rate of 0.008:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用批量大小为8和学习率为0.008对10,000条记录进行了以下训练：
- en: '![](img/8669f24e-f834-47fa-9a11-2b6e4e8e85fa.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8669f24e-f834-47fa-9a11-2b6e4e8e85fa.png)'
- en: 'The following is the evaluation performed on the same dataset for a batch size
    of 50 and a learning rate of 0.008:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对相同数据集进行了批量大小为50和学习率为0.008的评估：
- en: '![](img/4dc5d658-38f8-4eda-9402-b2531fbcbeab.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4dc5d658-38f8-4eda-9402-b2531fbcbeab.png)'
- en: 'To perform step 2, we increased the learning rate to 0.6, to observe the results.
    Note that a learning rate beyond a certain limit will not help efficiency in any
    way. Our job is to find that limit:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行第2步，我们将学习率增加到0.6，以观察结果。请注意，超过一定限制的学习率将不会以任何方式提高效率。我们的任务是找到这个限制：
- en: '![](img/4b100a4e-a906-4e13-a870-639b4447828f.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b100a4e-a906-4e13-a870-639b4447828f.png)'
- en: You can observe that `Accuracy` is reduced to 82.40% and `F1 Score` is reduced
    to 20.7%. This indicates that `F1 Score` might be the evaluation parameter to
    be accounted for in this model. This is not true for all models, and we reach
    this conclusion after experimenting with a couple of batch sizes and learning
    rates. In a nutshell, you have to repeat the same process for your model's training
    and choose arbitrary values that yield the best results.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以观察到`Accuracy`降低到82.40%，`F1 Score`降低到20.7%。这表明`F1 Score`可能是该模型中需要考虑的评估参数。这并不适用于所有模型，我们在尝试了几个批量大小和学习率后得出这一结论。简而言之，您必须重复相同的训练过程，并选择能产生最佳结果的任意值。
- en: There's more...
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: When we increase the batch size, the number of iterations will eventually reduce,
    hence the number of evaluations will also be reduced. This can overfit the data
    for a large batch size. A batch size of 1 is as useless as a batch size based
    on an entire dataset. So, you need to experiment with values starting from a safe
    arbitrary point.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们增加批量大小时，迭代次数最终会减少，因此评估次数也会减少。这可能会导致对于大批量大小的数据过拟合。批量大小为1与基于整个数据集的批量大小一样无效。因此，您需要从一个安全的任意点开始尝试不同的值。
- en: A very small learning rate will lead to a very small convergence rate to the
    target. This can also impact the training time. If the learning rate is very large,
    this will cause divergent behavior in the model. We need to increase the learning
    rate until we observe the evaluation metrics getting better. There is an implementation
    of a cyclic learning rate in the fast.ai and Keras libraries; however, a cyclic
    learning rate is not implemented in DL4J.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 非常小的学习率将导致收敛速度非常缓慢，这也会影响训练时间。如果学习率非常大，这将导致模型的发散行为。我们需要增加学习率，直到观察到评估指标变得更好。fast.ai和Keras库中有循环学习率的实现；然而，在DL4J中并没有实现循环学习率。
- en: Configuring Maven for DL4J
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置 Maven 以用于 DL4J。
- en: We need to add DL4J/ND4J Maven dependencies to leverage DL4J capabilities. ND4J
    is a scientific computation library dedicated to DL4J. It is necessary to mention
    the ND4J backend dependency in your `pom.xml` file. In this recipe, we will add
    a CPU-specific Maven configuration in `pom.xml`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要添加 DL4J/ND4J Maven 依赖项以利用 DL4J 的功能。ND4J 是专为 DL4J 设计的科学计算库。必须在你的 `pom.xml`
    文件中提及 ND4J 后端依赖项。在本示例中，我们将在 `pom.xml` 中添加一个特定于 CPU 的 Maven 配置。
- en: Getting ready
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作。
- en: 'Let''s discuss the required Maven dependencies. We assume you have already
    done the following:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论所需的 Maven 依赖项。我们假设您已经完成了以下工作：
- en: JDK 1.7, or higher, is installed and the `PATH` variable is set.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已安装 JDK 1.7 或更高版本，并设置了 `PATH` 变量。
- en: Maven is installed and the `PATH` variable is set.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maven 已安装并且 `PATH` 变量已设置。
- en: A 64-bit JVM is required to run DL4J.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 DL4J 需要 64 位 JVM。
- en: 'Set the `PATH` variable for JDK and Maven:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 JDK 和 Maven 的 `PATH` 变量：
- en: '**On Linux**: Use the `export` command to add Maven and JDK to the `PATH` variable:'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在 Linux 上**：使用 `export` 命令将 Maven 和 JDK 添加到 `PATH` 变量中：'
- en: '[PRE9]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Replace the version number as per the installation.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 根据安装更换版本号。
- en: '**On Windows**: Set System Environment variables from system Properties:'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在 Windows 上**：从系统属性设置系统环境变量：'
- en: '[PRE10]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Replace the JDK version number as per the installation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 根据安装更换 JDK 版本号。
- en: How to do it...
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到……
- en: 'Add the DL4J core dependency:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加 DL4J 核心依赖项：
- en: '[PRE11]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Add the ND4J native dependency:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加 ND4J 本地依赖项：
- en: '[PRE12]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Add the DataVec dependency to perform ETL (short for **Extract, Transform and
    Load**) operations:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加 DataVec 依赖项以执行 ETL（**提取、转换和加载**）操作：
- en: '[PRE13]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Enable logging for debugging:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用日志以进行调试：
- en: '[PRE14]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that 1.0.0-beta 3 is the current DL4J release version at the time of writing
    this book, and is the official version used in this cookbook. Also, note that
    DL4J relies on an ND4J backend for hardware-specific implementations.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在编写本书时，1.0.0-beta 3 是当前 DL4J 的发布版本，并且是本食谱中使用的官方版本。此外，请注意 DL4J 依赖于一个 ND4J
    后端用于硬件特定的实现。
- en: How it works...
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理……
- en: After adding DL4J core dependency and ND4J dependencies, as mentioned in step
    1 and step 2, we are able to create neural networks. In step 2, the ND4J maven
    configuration is mentioned as a necessary backend dependency for Deeplearnign4j.
    ND4J is the scientific computation library for Deeplearning4j.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加了 DL4J 核心依赖项和 ND4J 依赖项后，正如步骤 1 和步骤 2 中所述，我们能够创建神经网络。在步骤 2 中，ND4J Maven 配置被提及为
    Deeplearnign4j 必要的后端依赖项。ND4J 是 Deeplearning4j 的科学计算库。
- en: ND4J is a scientific computing library written for Java, just like NumPy is
    for Python.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ND4J 是为 Java 编写的科学计算库，就像 NumPy 是为 Python 编写的一样。
- en: 'Step 3 is very crucial for the ETL process: that is, data extraction, transformation,
    and loading. So, we definitely need this as well in order to train the neural
    network using data.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 3 对于 ETL 过程非常关键：即数据提取、转换和加载。因此，我们在使用数据训练神经网络时肯定需要它。
- en: Step 4 is optional but recommended, since logging will reducee the effort involved
    in debugging.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 4 是可选的，但建议进行，因为记录日志将减少调试所需的工作量。
- en: Configuring DL4J for a GPU-accelerated environment
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置 DL4J 以用于 GPU 加速环境。
- en: For GPU-powered hardware, DL4J comes with a different API implementation. This
    is to ensure the GPU hardware is utilized effectively without wasting hardware
    resources. Resource optimization is a major concern for expensive GPU-powered
    applications in production. In this recipe, we will add a GPU-specific Maven configuration
    to `pom.xml`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GPU 驱动硬件，DL4J 提供了不同的 API 实现。这是为了确保有效利用 GPU 硬件，而不浪费硬件资源。资源优化是生产中昂贵的 GPU 应用程序的主要关注点。在本示例中，我们将向
    `pom.xml` 中添加一个特定于 GPU 的 Maven 配置。
- en: Getting ready
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作。
- en: 'You will need the following in order to complete this recipe:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 您将需要以下内容才能完成此食谱：
- en: JDK version 1.7, or higher, installed and added to the `PATH` variable
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JDK 版本为 1.7 或更高，并已安装并添加到 `PATH` 变量中。
- en: Maven installed and added to the `PATH` variable
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maven 已安装并添加到 `PATH` 变量。
- en: NVIDIA-compatible hardware
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 兼容 NVIDIA 硬件。
- en: CUDA v9.2+ installed and configured
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA v9.2+ 已安装并配置。
- en: '**cuDNN** (short for **CUDA Deep Neural Network**) installed and configured'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cuDNN**（CUDA 深度神经网络）已安装并配置。'
- en: How to do it...
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到……
- en: Download and install CUDA v9.2+ from the NVIDIA developer website URL: [https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads).
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 NVIDIA 开发者网站 URL：[https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads)
    下载并安装 CUDA v9.2+。
- en: 'Configure the CUDA dependencies. For Linux, go to a Terminal and edit the `.bashrc`
    file. Run the following commands and make sure you replace username and the CUDA
    version number as per your downloaded version:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置CUDA依赖项。对于Linux，打开终端并编辑`.bashrc`文件。运行以下命令，并确保根据你的下载版本替换用户名和CUDA版本号：
- en: '[PRE15]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Add the `lib64` directory to `PATH` for older DL4J versions.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于旧版本的DL4J，需将`lib64`目录添加到`PATH`中。
- en: Run the `nvcc --version` command to verify the CUDA installation.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`nvcc --version`命令以验证CUDA的安装。
- en: 'Add Maven dependencies for the ND4J CUDA backend:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加ND4J CUDA后端的Maven依赖项：
- en: '[PRE16]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Add the DL4J CUDA Maven dependency:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加DL4J CUDA Maven依赖项：
- en: '[PRE17]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Add cuDNN dependencies to use bundled CUDA and cuDNN:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加cuDNN依赖项，以使用捆绑的CUDA和cuDNN：
- en: '[PRE18]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works...
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We configured NVIDIA CUDA using steps 1 to 4\. For more detailed OS-specific
    instructions, refer to the official NVIDIA CUDA website at [https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经通过步骤1到4配置了NVIDIA CUDA。有关更详细的操作系统特定说明，请参考官方的NVIDIA CUDA网站：[https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads)。
- en: 'Depending on your OS, installation instructions will be displayed on the website.
    DL4J version 1.0.0-beta 3 currently supports CUDA installation versions 9.0, 9.2,
    and 10.0\. For instance, if you need to install CUDA v10.0 for Ubuntu 16.04, you
    should navigate the CUDA website as shown here:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的操作系统，网站上将显示安装说明。DL4J版本1.0.0-beta 3目前支持CUDA安装版本9.0、9.2和10.0。例如，如果你需要为Ubuntu
    16.04安装CUDA v10.0，你应按照以下步骤访问CUDA网站：
- en: '![](img/9472152e-f538-4507-8961-48716ebdd996.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9472152e-f538-4507-8961-48716ebdd996.png)'
- en: Note that step 3 is not applicable to newer versions of DL4J. For of 1.0.0-beta
    and later versions, the necessary CUDA libraries are bundled with DL4J. However,
    this is not applicable for step 7.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，步骤3不适用于DL4J的较新版本。对于1.0.0-beta及更高版本，所需的CUDA库已经与DL4J捆绑在一起。然而，这不适用于步骤7。
- en: Additionally, before proceeding with steps 5 and 6, make sure that there are
    no redundant dependencies (such as CPU-specific dependencies) present in `pom.xml`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在继续执行步骤5和6之前，请确保`pom.xml`中没有冗余的依赖项（如CPU专用依赖项）。
- en: DL4J supports CUDA, but performance can be further accelerated by adding a cuDNN
    library. cuDNN does not show up as a bundled package in DL4J. Hence, make sure
    you download and install NVIDIA cuDNN from the NVIDIA developer website. Once
    cuDNN is installed and configured, we can follow step 7 to add support for cuDNN
    in the DL4J application.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: DL4J支持CUDA，但通过添加cuDNN库可以进一步加速性能。cuDNN不会作为捆绑包出现在DL4J中。因此，请确保从NVIDIA开发者网站下载并安装NVIDIA
    cuDNN。安装并配置完cuDNN后，我们可以按照步骤7在DL4J应用程序中添加对cuDNN的支持。
- en: There's more...
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'For multi-GPU systems, you can consume all GPU resources by placing the following
    code in the main method of your application:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多GPU系统，你可以通过在应用程序的主方法中加入以下代码来消耗所有GPU资源：
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This is a temporary workaround for initializing the ND4J backend in the case
    of multi-GPU hardware. In this way, we will not be limited to only a few GPU resources
    if more are available.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个临时的解决方法，用于在多GPU硬件的情况下初始化ND4J后端。通过这种方式，如果有更多的GPU可用，我们将不会只限于少量的GPU资源。
- en: Troubleshooting installation issues
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 排查安装问题
- en: Though the DL4J setup doesn't seem complex, installation issues can still happen
    because of different OSes or applications installed on the system, and so on. CUDA
    installation issues are not within the scope of this book. Maven build issues
    that are due to unresolved dependencies can have multiple causes. If you are working
    for an organization with its own internal repositories and proxies, then you need
    to make relevant changes in the `pom.xml` file. These issues are also outside
    the scope of this book. In this recipe, we will walk through the steps to mitigate
    common installation issues with DL4J.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然DL4J的设置看起来不复杂，但由于操作系统或系统中安装的应用程序等因素，仍然可能会遇到安装问题。CUDA安装问题不在本书的范围内。由于未解析的依赖关系引起的Maven构建问题可能有多种原因。如果你在一个有自己内部仓库和代理的组织中工作，那么你需要在`pom.xml`文件中进行相关更改。这些问题也不在本书的范围内。在本篇教程中，我们将逐步介绍如何解决DL4J常见的安装问题。
- en: Getting ready
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'The following checks are mandatory before we proceed:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，必须进行以下检查：
- en: Verify Java and Maven are installed and the `PATH` variables are configured.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证Java和Maven是否已安装，并且`PATH`变量已配置。
- en: Verify the CUDA and cuDNN installations.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证CUDA和cuDNN的安装。
- en: Verify that the Maven build is successful and the dependencies are downloaded
    at `~/.m2/repository`.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证 Maven 构建是否成功，并且依赖项是否已下载到 `~/.m2/repository`。
- en: How to do it...
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Enable logging levels to yield more information on errors:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用日志级别以提供更多关于错误的信息：
- en: '[PRE20]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Verify the JDK/Maven installation and configuration.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证 JDK/Maven 的安装和配置。
- en: Check whether all the required dependencies are added in the `pom.xml` file.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查是否所有必要的依赖项都已添加到 `pom.xml` 文件中。
- en: 'Remove the contents of the Maven local repository and rebuild Maven to mitigate `NoClassDefFoundError` in
    DL4J. For Linux, this is as follows:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除 Maven 本地仓库的内容并重新构建 Maven，以减轻 DL4J 中的 `NoClassDefFoundError`。在 Linux 中，操作如下：
- en: '[PRE21]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Mitigate `ClassNotFoundException` in DL4J. You can try this if step 4 didn't
    help to resolve the issue. DL4J/ND4J/DataVec should have the same version. For
    CUDA-related error stacks, check the installation as well.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少在 DL4J 中出现 `ClassNotFoundException`。如果第 4 步没有帮助解决问题，您可以尝试此方法。DL4J/ND4J/DataVec
    应该使用相同的版本。对于与 CUDA 相关的错误堆栈，请检查安装是否正常。
- en: If adding the proper DL4J CUDAversion doesn't fix this, then check your cuDNN
    installation.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如果添加适当的 DL4J CUDA 版本仍然无法解决问题，请检查您的 cuDNN 安装。
- en: How it works...
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: To mitigate exceptions such as `ClassNotFoundException`, the primary task is
    to verify we installed the JDK properly (step 2) and whether the environment variables
    we set up point to the right place. Step 3 is also important as the missing dependencies
    result in the same error.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少如 `ClassNotFoundException` 之类的异常，主要任务是验证我们是否正确安装了 JDK（第 2 步），以及我们设置的环境变量是否指向正确的位置。第
    3 步也很重要，因为缺失的依赖项会导致相同的错误。
- en: 'In step 4, we are removing redundant dependencies that are present in the local
    repository and are attempting a fresh Maven build. Here is a sample for `NoClassDefFoundError`
    while trying to run a DL4J application:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 4 步中，我们删除本地仓库中冗余的依赖项，并尝试重新构建 Maven。以下是尝试运行 DL4J 应用程序时出现 `NoClassDefFoundError`
    的示例：
- en: '[PRE22]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: One possible reason for `NoClassDefFoundError` could be the absence of required
    dependencies in the Maven local repository. So, we remove the repository contents
    and rebuild Maven to download the dependencies again. If any dependencies were
    not downloaded previously due to an interruption, it should happen now.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`NoClassDefFoundError` 的一个可能原因是 Maven 本地仓库中缺少必要的依赖项。因此，我们删除仓库内容并重新构建 Maven
    以重新下载依赖项。如果由于中断导致某些依赖项未被下载，现在应该会重新下载。'
- en: 'Here is an example of `ClassNotFoundException` during DL4J training:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个在 DL4J 训练期间遇到 `ClassNotFoundException` 的示例：
- en: '![](img/128dd62a-9167-4658-9060-b6a46a826555.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/128dd62a-9167-4658-9060-b6a46a826555.png)'
- en: Again, this suggests version issues or redundant dependencies.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提醒，这可能是版本问题或冗余依赖导致的。
- en: There's more...
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'In addition to the common runtime issues that were discussed previously, Windows
    users may face cuDNN-specific errors while training a CNN. The actual root cause
    could be different and is tagged under `UnsatisfiedLinkError`:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面讨论的常见运行时问题，Windows 用户在训练 CNN 时可能会遇到 cuDNN 特定的错误。实际的根本原因可能不同，通常标记为 `UnsatisfiedLinkError`：
- en: '[PRE23]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Perform the following steps to fix the issue:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来修复该问题：
- en: Download the latest dependency walker here: [https://github.com/lucasg/Dependencies/](https://github.com/lucasg/Dependencies/).
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此下载最新的依赖关系检查器：[https://github.com/lucasg/Dependencies/](https://github.com/lucasg/Dependencies/)。
- en: 'Add the following code to your DL4J `main()` method:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下代码添加到您的 DL4J `main()` 方法中：
- en: '[PRE24]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Replace `<module>` with the name of the JavaCPP preset module that is experiencing
    the problem; for example, `cudnn`. For newer DL4J versions, the necessary CUDA
    libraries are bundled with DL4J. Hence, you should not face this issue.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `<module>` 替换为遇到问题的 JavaCPP 预设模块的名称；例如，`cudnn`。对于较新的 DL4J 版本，必要的 CUDA 库已经与
    DL4J 打包。因此，您不应该再遇到此问题。
- en: If you feel like you might have found a bug or functional error with DL4J, then
    feel free to create an issue tracker at [https://github.com/eclipse/deeplearning4j](https://github.com/eclipse/deeplearning4j).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您觉得可能发现了 DL4J 的 bug 或功能错误，欢迎在 [https://github.com/eclipse/deeplearning4j](https://github.com/eclipse/deeplearning4j)
    上创建问题跟踪。
- en: You're also welcome to initiate a discussion with the Deeplearning4j community
    here: [https://gitter.im/deeplearning4j/deeplearning4j](https://gitter.im/deeplearning4j/deeplearning4j).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在这里与 Deeplearning4j 社区进行讨论：[https://gitter.im/deeplearning4j/deeplearning4j](https://gitter.im/deeplearning4j/deeplearning4j)。
