- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Convolutional Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: 'In *Chapter 1*, *Neural Network Foundations with TF*, we discussed dense networks,
    in which each layer is fully connected to the adjacent layers. We looked at one
    application of those dense networks in classifying the MNIST handwritten characters
    dataset. In that context, each pixel in the input image has been assigned to a
    neuron for a total of 784 (28 x 28 pixels) input neurons. However, this strategy
    does not leverage the spatial structure and relationships between each image.
    In particular, this piece of code is a dense network that transforms the bitmap
    representing each written digit into a flat vector where the local spatial structure
    is removed. Removing the spatial structure is a problem because important information
    is lost:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第1章*，*使用TF的神经网络基础*中，我们讨论了密集网络，其中每一层都与相邻的层完全连接。我们探讨了这些密集网络在分类MNIST手写字符数据集中的应用。在那个情境中，输入图像中的每个像素都分配给一个神经元，总共有784个输入神经元（28
    x 28像素）。然而，这种策略并没有利用图像之间的空间结构和关系。特别是，这段代码是一个密集网络，它将表示每个手写数字的位图转换为一个平坦的向量，移除了局部空间结构。移除空间结构是一个问题，因为重要的信息会丢失：
- en: '[PRE0]'
  id: totrans-3
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Convolutional neural networks leverage spatial information, and they are therefore
    very well-suited for classifying images. These nets use an ad hoc architecture
    inspired by biological data taken from physiological experiments performed on
    the visual cortex. Biological studies show that our vision is based on multiple
    cortex levels, each one recognizing more and more structured information. First,
    we see single pixels, then from that, we recognize simple geometric forms and
    then more and more sophisticated elements such as objects, faces, human bodies,
    animals, and so on.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络利用空间信息，因此它们非常适合用于图像分类。这些网络采用了一种灵活的架构，灵感来自于生物学数据，这些数据来自于在视觉皮层上进行的生理实验。生物学研究表明，我们的视觉是基于多个皮层层次的，每一层都识别更复杂的信息。首先，我们看到的是单个像素，然后从这些像素中我们能识别出简单的几何形状，接着是更复杂的元素，如物体、面孔、人类身体、动物等。
- en: Convolutional neural networks are a fascinating subject. Over a short period
    of time, they have shown themselves to be a disruptive technology, breaking performance
    records in multiple domains from text, to video, to speech, going well beyond
    the initial image processing domain where they were originally conceived. In this
    chapter, we will introduce the idea of convolutional neural networks (also known
    as CNNs, DCNNs, and ConvNets), a particular type of neural network that has large
    importance for deep learning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络是一个令人着迷的课题。在短短的时间内，它们已经展示出了颠覆性的技术突破，打破了多个领域的性能记录，从文本到视频再到语音，远远超出了它们最初用于图像处理的领域。在本章中，我们将介绍卷积神经网络（也称为CNN、DCNN和ConvNets）的概念，这是一种对深度学习具有重要意义的神经网络类型。
- en: 'This chapter covers the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Deep convolutional neural networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度卷积神经网络
- en: An example of a deep convolutional neural network
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度卷积神经网络的示例
- en: Recognizing CIFAR-10 images with deep learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习识别 CIFAR-10 图像
- en: Very deep convolutional networks for large-scale image recognition
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于大规模图像识别的非常深的卷积网络
- en: Deep Inception V3 networks for transfer learning
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于迁移学习的深度Inception V3网络
- en: Other CNN architectures
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他CNN架构
- en: Style transfer
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风格迁移
- en: All the code files for this chapter can be found at [https://packt.link/dltfchp3](https://packt.link/dltfchp3).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码文件可以在[https://packt.link/dltfchp3](https://packt.link/dltfchp3)找到。
- en: Let’s begin with deep convolutional neural networks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从深度卷积神经网络开始。
- en: Deep convolutional neural networks
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度卷积神经网络
- en: A **Deep Convolutional Neural Network** (**DCNN**) consists of many neural network
    layers. Two different types of layers, convolutional and pooling (i.e., subsampling),
    are typically alternated. The depth of each filter increases from left to right
    in the network. The last stage is typically made of one or more fully connected
    layers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度卷积神经网络**（**DCNN**）由多个神经网络层组成。通常，卷积层和池化层（即下采样）交替排列。每个滤波器的深度从网络的左到右逐渐增加。最后一阶段通常由一个或多个全连接层组成。'
- en: '![Typical_cnn.png](img/B18331_03_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![Typical_cnn.png](img/B18331_03_01.png)'
- en: 'Figure 3.1: An example of a DCNN'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：DCNN的示例
- en: 'There are three key underlying concepts for ConvNets: local receptive fields,
    shared weights, and pooling. Let’s review them together.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（ConvNets）有三个关键的基本概念：局部感受野、共享权重和池化。让我们一起回顾它们。
- en: Local receptive fields
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局部感受野
- en: If we want to preserve the spatial information of an image or other form of
    data, then it is convenient to represent each image with a matrix of pixels. Given
    this, a simple way to encode the local structure is to connect a submatrix of
    adjacent input neurons into one single hidden neuron belonging to the next layer.
    That single hidden neuron represents one local receptive field. Note that this
    operation is named convolution, and this is where the name for this type of network
    is derived. You can think about convolution as the treatment of a matrix by another
    matrix, referred to as a kernel.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望保留图像或其他形式数据的空间信息，则将每个图像用像素矩阵表示是很方便的。在这种情况下，编码局部结构的简单方法是将相邻输入神经元的子矩阵连接成下一层的单个隐藏神经元。那个单个隐藏神经元代表一个局部感受野。请注意，这个操作被称为卷积，这也是这种类型网络的名称来源。您可以将卷积理解为一个矩阵对另一个矩阵的处理，后者被称为核。
- en: Of course, we can encode more information by having overlapping submatrices.
    For instance, let’s suppose that the size of every single submatrix is 5 x 5 and
    that those submatrices are used with MNIST images of 28 x 28 pixels. Then we will
    be able to generate 24 x 24 local receptive field neurons in the hidden layer.
    In fact, it is possible to slide the submatrices by only 23 positions before touching
    the borders of the images. In TensorFlow, the number of pixels along one edge
    of the kernel, or submatrix, is the kernel size, and the stride length is the
    number of pixels by which the kernel is moved at each step in the convolution.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们可以通过具有重叠子矩阵来编码更多信息。例如，假设每个单个子矩阵的大小为 5 x 5，并且这些子矩阵与 28 x 28 像素的 MNIST 图像一起使用。然后我们将能够在隐藏层生成
    24 x 24 的局部感受野神经元。实际上，在触及图像边界之前，可以仅将子矩阵滑动 23 个位置。在 TensorFlow 中，沿核的一个边的像素数是核大小，而步幅长度是卷积中每步移动核的像素数。
- en: Let’s define the feature map from one layer to another. Of course, we can have
    multiple feature maps that learn independently from each hidden layer. For example,
    we can start with 28 x 28 input neurons for processing MNIST images, and then
    define *k* feature maps of size 24 x 24 neurons each (again with shape of 5 x
    5) in the next hidden layer.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义从一层到另一层的特征图。当然，我们可以有多个特征图，它们可以独立地从每个隐藏层学习。例如，我们可以从处理 MNIST 图像的 28 x 28
    输入神经元开始，然后在下一个隐藏层中定义大小为 24 x 24 的 *k* 个特征图（形状再次为 5 x 5）。
- en: Shared weights and bias
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共享权重和偏置
- en: Let’s suppose that we want to move away from the pixel representation in a raw
    image, by gaining the ability to detect the same feature independently from the
    location where it is placed in the input image. A simple approach is to use the
    same set of weights and biases for all the neurons in the hidden layers. In this
    way, each layer will learn a set of position-independent latent features derived
    from the image, bearing in mind that a layer consists of a set of kernels in parallel,
    and each kernel only learns one feature.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们希望摆脱原始图像中的像素表示，通过获取能够在输入图像中的任何位置独立检测相同特征的能力。一个简单的方法是在所有隐藏层的神经元中使用相同的权重和偏置。这样一来，每一层将学习从图像中导出的一组位置无关的潜在特征，需要记住，每一层由并行的一组核组成，每个核只学习一个特征。
- en: A mathematical example
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个数学示例
- en: 'One simple way to understand convolution is to think about a sliding window
    function applied to a matrix. In the following example, given the input matrix
    **I** and the kernel **K**, we get the convolved output. The 3 x 3 kernel **K**
    (sometimes called the filter or feature detector) is multiplied elementwise with
    the input matrix to get one cell in the output matrix. All the other cells are
    obtained by sliding the window over **I**:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 理解卷积的一个简单方法是将其视为应用于矩阵的滑动窗口函数。在以下示例中，给定输入矩阵 **I** 和核 **K**，我们得到卷积输出。3 x 3 的核
    **K**（有时称为过滤器或特征检测器）与输入矩阵逐元素相乘，以获得输出矩阵中的一个单元。通过在 **I** 上滑动窗口获得其他所有单元：
- en: '| J'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '| J'
- en: '&#124; 1 &#124; 1 &#124; 1 &#124; 0 &#124; 0 &#124;'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1 &#124; 1 &#124; 1 &#124; 0 &#124; 0 &#124;'
- en: '&#124; 0 &#124; 1 &#124; 1 &#124; 1 &#124; 0 &#124;'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0 &#124; 1 &#124; 1 &#124; 1 &#124; 0 &#124;'
- en: '&#124; 0 &#124; 0 &#124; 1 &#124; 1 &#124; 1 &#124;'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0 &#124; 0 &#124; 1 &#124; 1 &#124; 1 &#124;'
- en: '&#124; 0 &#124; 0 &#124; 1 &#124; 1 &#124; 0 &#124;'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0 &#124; 0 &#124; 1 &#124; 1 &#124; 0 &#124;'
- en: '&#124; 0 &#124; 1 &#124; 1 &#124; 0 &#124; 0 &#124;'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0 &#124; 1 &#124; 1 &#124; 0 &#124; 0 &#124;'
- en: '| K'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '| K'
- en: '&#124; 1 &#124; 0 &#124; 1 &#124;'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1 &#124; 0 &#124; 1 &#124;'
- en: '&#124; 0 &#124; 1 &#124; 0 &#124;'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0 &#124; 1 &#124; 0 &#124;'
- en: '&#124; 1 &#124; 0 &#124; 1 &#124;'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1 &#124; 0 &#124; 1 &#124;'
- en: '| Convolved'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '| Convolved'
- en: '&#124; 4 &#124; 3 &#124; 4 &#124;'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 4 &#124; 3 &#124; 4 &#124;'
- en: '&#124; 2 &#124; 4 &#124; 3 &#124;'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2 &#124; 4 &#124; 3 &#124;'
- en: '&#124; 2 &#124; 3 &#124; 4 &#124;'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2 &#124; 3 &#124; 4 &#124;'
- en: '|'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: In this example, we decided to stop the sliding window as soon as we touch the
    borders of **I** (so the output is 3 x 3). Alternatively, we could have chosen
    to pad the input with zeros (so that the output would have been 5 x 5). This decision
    relates to the padding choice adopted. Note that kernel depth is equal to input
    depth (channel).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们决定一旦滑动窗口碰到 **I** 的边界就停止滑动（因此输出是 3x3）。或者，我们也可以选择用零填充输入（这样输出就是 5x5）。这个决策与采用的填充选择有关。请注意，卷积核深度等于输入深度（通道）。
- en: Another choice is about how far along we slide our sliding windows with each
    step. This is called the stride and it can be one or more. A larger stride generates
    fewer applications of the kernel and a smaller output size, while a smaller stride
    generates more output and retains more information.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是我们每次滑动窗口时滑动的距离，这被称为步幅（stride），可以是 1 或更大。较大的步幅会产生较少的核应用并且输出尺寸较小，而较小的步幅会产生更多的输出并保留更多信息。
- en: The size of the filter, the stride, and the type of padding are hyperparameters
    that can be fine-tuned during the training of the network.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 滤波器的大小、步幅和填充类型是超参数，可以在训练过程中进行微调。
- en: ConvNets in TensorFlow
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow 中的 ConvNets
- en: 'In TensorFlow, if we want to add a convolutional layer with 32 parallel features
    and a filter size of 3x3, we write:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，如果我们想添加一个具有 32 个并行特征和 3x3 滤波器的卷积层，我们写：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This means that we are applying a 3x3 convolution on 28x28 images with 1 input
    channel (or input filters) resulting in 32 output channels (or output filters).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们对 28x28 的图像应用 3x3 卷积，输入通道为 1（或输入滤波器），输出通道为 32（或输出滤波器）。
- en: 'An example of convolution is provided in *Figure 3.2*:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积的示例见 *图 3.2*：
- en: '![Screen Shot 2016-12-04 at 8.10.51 PM.png](img/B18331_03_02.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![Screen Shot 2016-12-04 at 8.10.51 PM.png](img/B18331_03_02.png)'
- en: 'Figure.3.2: An example of convolution'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：卷积示例
- en: Pooling layers
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 池化层
- en: Let’s suppose that we want to summarize the output of a feature map. Again,
    we can use the spatial contiguity of the output produced from a single feature
    map and aggregate the values of a sub-matrix into one single output value synthetically
    describing the “meaning” associated with that physical region.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们希望总结特征图的输出。我们可以再次利用单个特征图产生的输出的空间连续性，将子矩阵的值聚合为一个单一的输出值，从而合成地描述该物理区域相关的“含义”。
- en: Max pooling
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最大池化
- en: 'One easy and common choice is the so-called max pooling operator, which simply
    outputs the maximum activation as observed in the region. In Keras, if we want
    to define a max pooling layer of size 2 x 2, we write:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单且常见的选择是所谓的最大池化运算符，它仅输出在该区域内观察到的最大激活值。在 Keras 中，如果我们想定义一个 2x2 的最大池化层，我们写：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'An example of the max-pooling operation is given in *Figure 3.3*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化操作的示例见 *图 3.3*：
- en: '![Screen Shot 2016-12-04 at 7.49.01 PM.png](img/B18331_03_03.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![Screen Shot 2016-12-04 at 7.49.01 PM.png](img/B18331_03_03.png)'
- en: 'Figure 3.3: An example of max pooling'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3：最大池化示例
- en: Average pooling
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平均池化
- en: Another choice is average pooling, which simply aggregates a region into the
    average values of the activations observed in that region.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是平均池化，它简单地将一个区域聚合为该区域观察到的激活值的平均值。
- en: Note that Keras implements a large number of pooling layers, and a complete
    list is available online (see [https://keras.io/layers/pooling/](https://keras.io/layers/pooling/)).
    In short, all the pooling operations are nothing more than a summary operation
    on a given region.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Keras 实现了大量的池化层，完整的池化层列表可以在线查看（见[https://keras.io/layers/pooling/](https://keras.io/layers/pooling/)）。简而言之，所有池化操作不过是对给定区域的汇总操作。
- en: ConvNets summary
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ConvNets 总结
- en: So far, we have described the basic concepts of ConvNets. CNNs apply convolution
    and pooling operations in one dimension for audio and text data along the time
    dimension, in two dimensions for images along the (height x width) dimensions,
    and in three dimensions for videos along the (height x width x time) dimensions.
    For images, sliding the filter over an input volume produces a map that provides
    the responses of the filter for each spatial position.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经描述了卷积神经网络（ConvNets）的基本概念。CNN 在一维上对音频和文本数据沿时间维度应用卷积和池化操作，在二维上对图像沿（高度
    x 宽度）维度应用，在三维上对视频沿（高度 x 宽度 x 时间）维度应用。对于图像，将滤波器滑动到输入体积上会产生一个映射，提供每个空间位置上滤波器的响应。
- en: In other words, a ConvNet has multiple filters stacked together that learn to
    recognize specific visual features independently from the location in the image
    itself. Those visual features are simple in the initial layers of the network
    and become more and more sophisticated deeper in the network. Training of a CNN
    requires the identification of the right values for each filter so that an input,
    when passed through multiple layers, activates certain neurons of the last layer
    so that it will predict the correct values.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，ConvNet有多个滤波器堆叠在一起，能够独立于图像中的位置学习识别特定的视觉特征。这些视觉特征在网络的初始层是简单的，而在更深层的网络中变得越来越复杂。CNN的训练需要识别每个滤波器的正确值，这样当输入通过多个层时，会激活最后一层的某些神经元，从而预测正确的值。
- en: 'An example of DCNN: LeNet'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DCNN的一个例子：LeNet
- en: Yann LeCun, who won the Turing Award, proposed [1] a family of ConvNets named
    LeNet, trained for recognizing MNIST handwritten characters with robustness to
    simple geometric transformations and distortion. The core idea of LeNet is to
    have lower layers alternating convolution operations with max-pooling operations.
    The convolution operations are based on carefully chosen local receptive fields
    with shared weights for multiple feature maps. Then, higher levels are fully connected
    based on a traditional MLP with hidden layers and softmax as the output layer.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 获得图灵奖的Yann LeCun提出了[1]一类名为LeNet的卷积神经网络（ConvNets），该网络用于识别MNIST手写字符，并具有对简单几何变换和畸变的鲁棒性。LeNet的核心思想是让较低层交替进行卷积操作和最大池化操作。卷积操作基于精心选择的局部感受野，并为多个特征图共享权重。然后，高层通过传统的多层感知器（MLP）进行全连接，隐藏层使用softmax作为输出层。
- en: LeNet code in TF
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LeNet代码示例（TF）
- en: 'To define a LeNet in code, we use a convolutional 2D module (note that `tf.keras.layers.Conv2D`
    is an alias of `tf.keras.layers.Convolution2D`, so the two can be used in an interchangeable
    way – see [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 要在代码中定义LeNet，我们使用一个2D卷积模块（请注意，`tf.keras.layers.Conv2D`是`tf.keras.layers.Convolution2D`的别名，因此这两者可以互换使用——详见[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)）：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'where the first parameter is the number of output filters in the convolution
    and the next tuple is the extension of each filter. An interesting optional parameter
    is padding. There are two options: `padding=''valid''` means that the convolution
    is only computed where the input and the filter fully overlap and therefore the
    output is smaller than the input, while `padding=''same''` means that we have
    an output that is the `same` size as the input, for which the area around the
    input is padded with zeros.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，第一个参数是卷积中输出滤波器的数量，接下来的元组是每个滤波器的扩展。一个有趣的可选参数是padding。有两个选项：`padding='valid'`表示卷积只在输入和滤波器完全重叠的地方计算，因此输出会小于输入，而`padding='same'`表示我们得到的输出与输入的大小`相同`，并且输入周围的区域会用零填充。
- en: 'In addition, we use a `MaxPooling2D` module:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还使用了`MaxPooling2D`模块：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: where `pool_size=(2, 2)` is a tuple of 2 integers representing the factors by
    which the image is vertically and horizontally downscaled. So (2, 2) will halve
    the image in each dimension, and `strides=(2, 2)` is the stride used for processing.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，`pool_size=(2, 2)`是一个包含2个整数的元组，表示图像在垂直和水平方向下缩小的因子。所以(2, 2)将在每个维度上将图像缩小一半，`strides=(2,
    2)`是用于处理的步幅。
- en: 'Now, let us review the code. First, we import a number of modules:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下代码。首先，我们导入一些模块：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then we define the LeNet network:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义LeNet网络：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We have a first convolutional stage with ReLU activations followed by max pooling.
    Our network will learn 20 convolutional filters, each one of which has a size
    of 5x5\. The output dimension is the same as the input shape, so it will be 28
    x 28\. Note that since `Convolutional2D` is the first stage of our pipeline, we
    are also required to define its `input_shape`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个第一个卷积阶段，使用ReLU激活函数，接着是最大池化。我们的网络将学习20个卷积滤波器，每个滤波器的大小为5x5。输出维度与输入形状相同，因此将是28
    x 28。请注意，由于`Convolutional2D`是我们管道的第一个阶段，我们还需要定义其`input_shape`。
- en: 'The max pooling operation implements a sliding window which slides over the
    layer and takes the maximum of each region with a step of two pixels both vertically
    and horizontally:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化操作实现了一个滑动窗口，窗口在层上滑动，并在每个区域中取最大值，步长为2个像素，垂直和水平方向都适用：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then there is a second convolutional stage with ReLU activations, followed
    again by a max pooling layer. In this case, we increase the number of convolutional
    filters learned to 50 from the previous 20\. Increasing the number of filters
    in deeper layers is a common technique in deep learning:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 接着是第二个卷积阶段，使用ReLU激活函数，之后再次是最大池化层。在这种情况下，我们将学习到的卷积滤波器数量从之前的20增加到50。增加深层滤波器数量是深度学习中的常见技术：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then we have a pretty standard flattening and a dense network of 500 neurons,
    followed by a softmax classifier with 10 classes:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们进行一个标准的展平操作，接着是一个包含500个神经元的全连接网络，再接一个具有10类的softmax分类器：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Congratulations, you have just defined your first deep convolutional learning
    network! Let’s see how it looks visually:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，你刚刚定义了你的第一个深度卷积学习网络！让我们看看它的视觉效果：
- en: '![Screen Shot 2016-12-04 at 8.51.07 PM.png](img/B18331_03_04.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![2016-12-04 8.51.07 PM的屏幕截图.png](img/B18331_03_04.png)'
- en: 'Figure 3.4: Visualization of LeNet'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4：LeNet 可视化
- en: 'Now we need some additional code for training the network, but this is very
    similar to what we described in *Chapter 1*, *Neural Network Foundations with
    TF*. This time we also show the code for printing the loss:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要一些额外的代码来训练网络，但这与我们在*第一章*《使用TF的神经网络基础》中描述的非常相似。这次我们还展示了打印损失的代码：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now let’s run the code. As you can see in *Figure 3.5*, the time had a significant
    increase, and each iteration in our deep net now takes ~28 seconds against ~1-2
    seconds for the network defined in *Chapter 1*, *Neural Network Foundations with
    TF*. However, the accuracy reached a new peak at 99.991% on training, 99.91% on
    validation, and 99.15% on test!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们运行代码。如*图3.5*所示，训练时间显著增加，每次迭代在我们的深度网络中现在需要约28秒，而在*第一章*《使用TF的神经网络基础》中定义的网络则只需约1-2秒。然而，准确率在训练集上达到了99.991%的新高，验证集上为99.91%，测试集上为99.15%！
- en: '![Chart, line chart  Description automatically generated](img/B18331_03_05.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图 说明自动生成](img/B18331_03_05.png)'
- en: 'Figure 3.5: LeNet accuracy'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5：LeNet 准确率
- en: 'Let’s see the execution of a full run for 20 epochs:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看完整运行20个周期的执行情况：
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s plot the model accuracy and the model loss, and we understand that we
    can train in only 10 iterations to achieve a similar accuracy of 99.1%:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制模型的准确率和模型损失图，并了解到，我们只需训练10次迭代，就能达到类似的99.1%准确率：
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let us see some of the MNIST images just to understand how good the number
    99.1% is! For instance, there are many ways in which humans write a 9, one of
    them being in *Figure 3.6*. The same goes for 3, 7, 4, and 5, and number 1 in
    this figure is so difficult to recognize that even a human would likely have trouble:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些MNIST图像，以便理解99.1%准确率的实际效果！例如，人的写9的方式有很多种，其中一种在*图3.6*中展示。对于3、7、4和5也一样，图中的1号字符难度极高，即使是人类也可能难以识别：
- en: '![Screen Shot 2016-12-04 at 8.19.34 PM.png](img/B18331_03_06.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![2016-12-04 8.19.34 PM的屏幕截图.png](img/B18331_03_06.png)'
- en: 'Figure 3.6: An example of MNIST handwritten characters'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6：MNIST 手写字符示例
- en: 'We can summarize all the progress made so far with our different models in
    the following graph. Our simple net started with an accuracy of 90.71%, meaning
    that about 9 handwritten characters out of 100 are not correctly recognized. Then,
    we gained 8% with the deep learning architecture, reaching an accuracy of 99.2%,
    which means that less than one handwritten character out of one hundred is incorrectly
    recognized, as shown in *Figure 3.7*:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用以下图表总结目前不同模型的所有进展。我们的简单网络起始准确率为90.71%，这意味着每100个手写字符中大约有9个无法正确识别。然后，使用深度学习架构我们提高了8%的准确率，达到了99.2%，这意味着每100个手写字符中少于1个被误识别，正如*图3.7*所示：
- en: '![Chart, line chart, scatter chart  Description automatically generated](img/B18331_03_07.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图，散点图 说明自动生成](img/B18331_03_07.png)'
- en: 'Figure 3.7: Accuracy for different models and optimizers'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7：不同模型和优化器的准确率
- en: Understanding the power of deep learning
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解深度学习的威力
- en: 'Another test we can run for a better understanding of the power of deep learning
    and ConvNets is to reduce the size of the training set and observe the resulting
    decay in performance. One way to do this is to split the training set of 50,000
    examples into two different sets:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们可以运行的测试是为了更好地理解深度学习和卷积网络的威力，我们可以减少训练集的大小并观察性能的下降。一个方法是将50,000个训练样本分成两组：
- en: 'The proper training set used for training our model will progressively reduce
    in size: 5,900, 3,000, 1,800, 600, and 300 examples.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练我们模型的正确训练集将逐步减少大小：5,900、3,000、1,800、600 和 300 个示例。
- en: The validation set used to estimate how well our model has been trained will
    consist of the remaining examples. Our test set is always fixed, and it consists
    of 10,000 examples.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于估计我们的模型训练效果的验证集将由剩余的示例组成。我们的测试集始终是固定的，包含 10,000 个示例。
- en: With this setup, we compare the previously defined deep learning ConvNet against
    the first example neural network defined in *Chapter 1*, *Neural Network Foundations
    with TF*. As we can see in the following graph, our deep network always outperforms
    the simple network when there is more data available. With 5,900 training examples,
    the deep learning net had an accuracy of 97.23% against an accuracy of 94% for
    the simple net.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在此设置下，我们将前面定义的深度学习卷积神经网络与*第 1 章*中定义的第一个示例神经网络进行比较，*神经网络基础与 TF*。正如我们在下面的图表中所看到的，当有更多数据可用时，我们的深度网络总是优于简单网络。在有
    5,900 个训练示例时，深度学习网络的准确率为 97.23%，而简单网络的准确率为 94%。
- en: 'In general, deep networks require more training data available to fully express
    their power, as shown in *Figure 3.8*:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，深度网络需要更多的训练数据才能充分展现其能力，如*图 3.8*所示：
- en: '![Chart, line chart  Description automatically generated](img/B18331_03_08.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, line chart  Description automatically generated](img/B18331_03_08.png)'
- en: 'Figure 3.8: Accuracy for different amounts of data'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8：不同数据量下的准确率
- en: A list of state-of-the-art results (for example, the highest performance available)
    for MNIST is available online (see [http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.xhtml](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.xhtml)).
    As of March 2019, the best result has an error rate of 0.21% [2].
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 MNIST 的最新结果（例如，当前可用的最高性能）可以在线查阅（参见 [http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.xhtml](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.xhtml)）。截至
    2019 年 3 月，最佳结果的错误率为 0.21% [2]。
- en: Recognizing CIFAR-10 images with deep learning
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习识别 CIFAR-10 图像
- en: 'The CIFAR-10 dataset contains 60,000 color images of 32 x 32 pixels in three
    channels, divided into 10 classes. Each class contains 6,000 images. The training
    set contains 50,000 images, while the test set provides 10,000 images. This image
    taken from the CIFAR repository (see [https://www.cs.toronto.edu/~kriz/cifar.xhtml](https://www.cs.toronto.edu/~kriz/cifar.xhtml))
    shows a few random examples from the 10 classes:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10 数据集包含 60,000 张 32 x 32 像素的彩色图像，分为三个通道，并划分为 10 个类别。每个类别包含 6,000 张图像。训练集包含
    50,000 张图像，而测试集提供 10,000 张图像。以下图片来自 CIFAR 数据库（参见 [https://www.cs.toronto.edu/~kriz/cifar.xhtml](https://www.cs.toronto.edu/~kriz/cifar.xhtml)），展示了来自
    10 个类别的一些随机示例：
- en: '![A picture containing text  Description automatically generated](img/B18331_03_09.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text  Description automatically generated](img/B18331_03_09.png)'
- en: 'Figure 3.9: An example of CIFAR-10 images'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9：CIFAR-10 图像示例
- en: 'The images in this section are from *Learning Multiple Layers of Features from
    Tiny Images*, Alex Krizhevsky, 2009: [https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf).
    They are part of the CIFAR-10 dataset (toronto.edu): [https://www.cs.toronto.edu/~kriz/cifar.xhtml](https://www.cs.toronto.edu/~kriz/cifar.xhtml).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的图像来自 *Learning Multiple Layers of Features from Tiny Images*，Alex Krizhevsky，2009：[https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)。它们是
    CIFAR-10 数据集的一部分（toronto.edu）：[https://www.cs.toronto.edu/~kriz/cifar.xhtml](https://www.cs.toronto.edu/~kriz/cifar.xhtml)。
- en: The goal is to recognize previously unseen images and assign them to one of
    the ten classes. Let us define a suitable deep net.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是识别之前未见过的图像，并将其分配到十个类别中的一个。让我们定义一个合适的深度网络。
- en: 'First of all, we import a number of useful modules and define a few constants
    and load the dataset (the full code including the load operations is available
    online):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入一些有用的模块，定义几个常量并加载数据集（包括加载操作的完整代码可在线获取）：
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Our net will learn 32 convolutional filters, each of which with a 3 x 3 size.
    The output dimension is the same as the input shape, so it will be 32 x 32 and
    the activation function used is a ReLU function, which is a simple way of introducing
    non-linearity. After that, we have a `MaxPooling` operation with a pool size of
    2 x 2 and dropout at 25%:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的网络将学习 32 个卷积滤波器，每个滤波器的大小为 3 x 3。输出维度与输入形状相同，因此为 32 x 32，所使用的激活函数是 ReLU 函数，这是引入非线性的一种简单方法。之后，我们有一个
    `MaxPooling` 操作，池大小为 2 x 2，并且丢弃率为 25%：
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The next stage in the deep pipeline is a dense network with 512 units and ReLU
    activation followed by dropout at 50% and by a softmax layer with 10 classes as
    output, one for each category:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 深度管道中的下一个阶段是一个包含512个单元的密集网络，并使用ReLU激活，随后是50%的dropout，最后是一个带有10个类别输出的softmax层，每个类别对应一个类别：
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After defining the network, we can train the model. In this case, we split
    the data and compute a validation set in addition to the training and testing
    sets. The training is used to build our models, the validation is used to select
    the best-performing approach, while the test set is used to check the performance
    of our best models on fresh unseen data:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了网络之后，我们可以训练模型。在这种情况下，我们将数据进行拆分，并计算一个验证集，除了训练集和测试集外。训练集用于构建我们的模型，验证集用于选择表现最佳的方式，而测试集用于检查我们最佳模型在新数据上的表现：
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s run the code. Our network reaches a test accuracy of 66.8% with 20 iterations.
    We also print the accuracy and loss plot and dump the network with `model.summary()`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行代码。我们的网络在20次迭代后达到了66.8%的测试准确率。我们还打印了准确率和损失图，并使用`model.summary()`输出了网络的总结：
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '*Figure 3.10* shows the accuracy and loss plot:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.10* 显示了准确率和损失图：'
- en: '![Chart, line chart  Description automatically generated](img/B18331_03_10.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  自动生成描述](img/B18331_03_10.png)'
- en: 'Figure 3.10: Accuracy and loss for the defined network'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10：定义网络的准确率和损失
- en: We have seen how to improve accuracy and how the loss changes for CIFAR-10 datasets.
    The next section is about improving the current results.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何提高准确率，以及CIFAR-10数据集的损失如何变化。下一节将讨论如何改善当前的结果。
- en: Improving the CIFAR-10 performance with a deeper network
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过更深的网络提高CIFAR-10性能
- en: 'One way to improve the performance is to define a deeper network with multiple
    convolutional operations. In the following example, we have a sequence of modules:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 提高性能的一种方法是定义一个更深的网络，使用多个卷积操作。在以下示例中，我们有一系列模块：
- en: '1st module: (CONV+CONV+MaxPool+DropOut)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 第1个模块：（CONV+CONV+MaxPool+DropOut）
- en: '2nd module: (CONV+CONV+MaxPool+DropOut)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 第2个模块：（CONV+CONV+MaxPool+DropOut）
- en: '3rd module: (CONV+CONV+MaxPool+DropOut)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 第3个模块：（CONV+CONV+MaxPool+DropOut）
- en: 'These are followed by a standard dense output layer. All the activation functions
    used are ReLU functions. There is a new layer that we also discussed in *Chapter
    1*, *Neural Network Foundations with TF*, `BatchNormalization()`, used to introduce
    a form of regularization between modules:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 随后是一个标准的密集输出层。所有使用的激活函数都是ReLU函数。还有一个新层，我们在*第1章*中也讨论过，*神经网络基础与TF*，`BatchNormalization()`，用于在模块之间引入一种正则化形式：
- en: '[PRE18]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Congratulations! You have defined a deeper network. Let us run the code for
    40 iterations reaching an accuracy of 82%! Let’s add the remaining part of the
    code for the sake of completeness. The first part is to load and normalize the
    data:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经定义了一个更深的网络。让我们运行代码40次，达到82%的准确率！为了完整性，我们加上剩余的代码部分。第一部分是加载和标准化数据：
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then we need to have a part to train the network:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要有一部分代码来训练网络：
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: So, we have an improvement of 15.14% with respect to the previous simpler deeper
    network.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，相较于先前的简单深度网络，我们提高了15.14%的性能。
- en: Improving the CIFAR-10 performance with data augmentation
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过数据增强提高CIFAR-10性能
- en: 'Another way to improve the performance is to generate more images for our training.
    The idea here is that we can take the standard CIFAR training set and augment
    this set with multiple types of transformation, including rotation, rescaling,
    horizontal or vertical flip, zooming, channel shift, and many more. Let’s see
    the code applied on the same network defined in the previous section:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 提高性能的另一种方法是为我们的训练生成更多的图像。这里的想法是，我们可以从标准的CIFAR训练集开始，通过多种转换类型来增强这个集合，包括旋转、重新缩放、水平或垂直翻转、缩放、通道偏移等。让我们看看在上一节中定义的相同网络上应用的代码：
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`rotation_range` is a value in degrees (0-180) for randomly rotating pictures;
    `width_shift` and `height_shift` are ranges for randomly translating pictures
    vertically or horizontally; `zoom_range` is for randomly zooming pictures; `horizontal_flip`
    is for randomly flipping half of the images horizontally; `fill_mode` is the strategy
    used for filling in new pixels that can appear after a rotation or a shift.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`rotation_range`是一个度数值（0-180），用于随机旋转图片；`width_shift`和`height_shift`是随机平移图片的垂直或水平方向的范围；`zoom_range`用于随机缩放图片；`horizontal_flip`用于随机水平翻转一半图像；`fill_mode`是用于填充旋转或平移后可能出现的新像素的策略。'
- en: 'After augmentation we have generated many more training images starting from
    the standard CIFAR-10 set, as shown in *Figure 3.11*:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 通过数据增强，我们从标准CIFAR-10数据集生成了更多的训练图像，如*图3.11*所示：
- en: '![A screenshot of a video game  Description automatically generated with medium
    confidence](img/B18331_03_11.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![视频游戏截图，描述自动生成，信心较中等](img/B18331_03_11.png)'
- en: 'Figure.3.11: An example of image augmentation'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11：图像增强的一个示例
- en: 'Now we can apply this intuition directly for training. Using the same ConvNet
    defined before, we simply generate more augmented images, and then we train. For
    efficiency, the generator runs in parallel to the model. This allows an image
    augmentation on a CPU while training in parallel on a GPU. Here is the code:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以直接应用这个直觉来进行训练。使用之前定义的相同ConvNet，我们只需生成更多的增强图像，然后进行训练。为了提高效率，生成器与模型并行运行。这使得图像增强可以在CPU上进行，同时在GPU上并行训练。以下是代码：
- en: '[PRE22]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Each iteration is now more expensive because we have more training data. Therefore,
    let’s run for 50 iterations only. We see that by doing this we reach an accuracy
    of 85.91%:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代现在变得更昂贵，因为我们拥有更多的训练数据。因此，我们只进行50次迭代。我们可以看到，通过这样做，我们达到了85.91%的准确率：
- en: '[PRE23]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The results obtained during our experiments are summarized in the following
    figure:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实验中获得的结果总结在下图中：
- en: '![Chart](img/B18331_03_12.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/B18331_03_12.png)'
- en: 'Figure 3.12: Accuracy on CIFAR-10 with different networks. On the x-axis, we
    have the increasing number of iterations'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12：不同网络在CIFAR-10上的准确率。在x轴上，我们有递增的迭代次数
- en: A list of state-of-the-art results for CIFAR-10 is available online (see [http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.xhtml](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.xhtml)).
    As of April 2019, the best result has an accuracy of 96.53% [3].
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10的最新结果列表可以在网上找到（请见[http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.xhtml](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.xhtml)）。截至2019年4月，最佳结果的准确率为96.53%
    [3]。
- en: Predicting with CIFAR-10
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用CIFAR-10进行预测
- en: 'Let’s suppose that we want to use the deep learning model we just trained for
    CIFAR-10 for a bulk evaluation of images. Since we saved the model and the weights,
    we do not need to train each time:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们希望使用刚刚训练的CIFAR-10深度学习模型来批量评估图像。由于我们已经保存了模型和权重，因此每次不需要重新训练：
- en: '[PRE24]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note that we use SciPy’s `imread` to load the images and then resize them to
    32 × 32 pixels. The resulting image tensor has dimensions of (32, 32, 3). However,
    we want the color dimension to be first instead of last, so we take the transpose.
    After that, the list of image tensors is combined into a single tensor and normalized
    to be between 0 and 1.0.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用SciPy的`imread`来加载图像，然后将其调整为32 × 32像素。生成的图像张量的维度为（32, 32, 3）。然而，我们希望颜色维度位于第一位而非最后，因此我们对其进行了转置。之后，图像张量的列表被合并为一个单一张量，并归一化至0到1.0之间。
- en: 'Now let us get the prediction for a ![cat-standing.jpg](img/B18331_03_Cat.png)and
    for a ![dog.jpg](img/B18331_03_Dog.png). We get categories 3 (cat) and 5 (dog)
    as output as expected. We successfully created a ConvNet to classify CIFAR-10
    images. Next, we will look at VGG16: a breakthrough in deep learning.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们分别获取一只 ![猫站立.jpg](img/B18331_03_Cat.png) 和一只 ![狗.jpg](img/B18331_03_Dog.png)
    的预测。我们得到的类别是3（猫）和5（狗），如预期一样。我们成功地创建了一个卷积神经网络（ConvNet）来分类CIFAR-10图像。接下来，我们将研究VGG16：深度学习的突破。
- en: Very deep convolutional networks for large-scale image recognition
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于大规模图像识别的非常深的卷积神经网络
- en: 'In 2014, an interesting contribution to image recognition was presented in
    the paper *Very Deep Convolutional Networks for Large-Scale Image Recognition*,
    K. Simonyan and A. Zisserman [4]. The paper showed that a *significant improvement
    on the prior-art configurations can be achieved by pushing the depth to 16-19
    weight layers*. One model in the paper denoted as D or VGG16 had 16 deep layers.
    An implementation in Java Caffe (see [http://caffe.berkeleyvision.org/](http://caffe.berkeleyvision.org/))
    was used for training the model on the ImageNet ILSVRC-2012 (see [http://image-net.org/challenges/LSVRC/2012/](http://image-net.org/challenges/LSVRC/2012/))
    dataset, which includes images of 1,000 classes, and is split into three sets:
    training (1.3M images), validation (50K images), and testing (100K images). Each
    image is (224 x 224) on 3 channels. The model achieves 7.5% top-5 error (the error
    of the top 5 results) on ILSVRC-2012-val and 7.4% top-5 error on ILSVRC-2012-test.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年，一篇名为*Very Deep Convolutional Networks for Large-Scale Image Recognition*的论文提出了对图像识别的有趣贡献，由K.
    Simonyan和A. Zisserman [4]提出。论文表明，通过将深度推到16-19层，可以显著改进之前的配置。论文中的一个模型标记为D或VGG16，有16层深度。Java
    Caffe（参见[http://caffe.berkeleyvision.org/](http://caffe.berkeleyvision.org/)）用于在ImageNet
    ILSVRC-2012上训练模型（参见[http://image-net.org/challenges/LSVRC/2012/](http://image-net.org/challenges/LSVRC/2012/)），包括1000个类别的图像，分为三组：训练（130万张图像）、验证（50K张图像）和测试（100K张图像）。每张图像为（224
    x 224），3通道。该模型在ILSVRC-2012-val上达到7.5%的top-5错误率（前5个结果的错误率），在ILSVRC-2012-test上达到7.4%的top-5错误率。
- en: 'According to the ImageNet site:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 根据ImageNet网站：
- en: The goal of this competition is to estimate the content of photographs for the
    purpose of retrieval and automatic annotation using a subset of the large hand-labeled
    ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories)
    as training. Test images will be presented with no initial annotation -- no segmentation
    or labels -- and algorithms will have to produce labelings specifying what objects
    are present in the images.
  id: totrans-170
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本竞赛的目标是利用大量手工标记的ImageNet数据集子集（包含1000万张标记图像，涵盖10000多个物体类别）进行训练，用于检索和自动注释照片内容。测试图像将不带任何初始注释
    —— 没有分割或标签 —— 算法必须生成标签，指定图像中存在的对象。
- en: 'The weights learned by the model implemented in Caffe have been directly converted
    ([https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3](https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3))
    in `tf.Keras` and can be used by preloading them into the `tf.Keras` model, which
    is implemented below, as described in the paper:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在Caffe中实现的权重已经直接转换为（[https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3](https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3)）`tf.Keras`，可以通过预加载到下面实现的`tf.Keras`模型中使用，正如论文所述：
- en: '[PRE25]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We have implemented a VGG16 network. Note that we could also have used `tf.keras.applications.vgg16`.
    to get the model and its weights directly. Here, I wanted to show how VGG16 works
    internally. Next, we are going to utilize it.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经实现了一个VGG16网络。请注意，我们也可以直接使用`tf.keras.applications.vgg16`获取模型及其权重。在这里，我想展示VGG16是如何内部工作的。接下来，我们将利用它。
- en: Recognizing cats with a VGG16 network
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过VGG16网络识别猫
- en: Now let us test the image of a ![cat.jpg](img/B18331_03_Cat_2.png).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们测试一个![cat.jpg](img/B18331_03_Cat_2.png)的图像。
- en: 'Note that we are going to use predefined weights:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将使用预定义的权重：
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'When the code is executed, the class `285` is returned, which corresponds (see
    [https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a))
    to “Egyptian cat”:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当代码执行时，返回类别`285`，对应于“埃及猫”（参见[https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)）：
- en: '[PRE27]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Impressive, isn’t it? Our VGG16 network can successfully recognize images of
    cats! An important step for deep learning. It is only seven years since the paper
    in [4], but that was a game-changing moment.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 令人印象深刻，不是吗？我们的VGG16网络可以成功识别猫的图像！这是深度学习的重要一步。距离[4]中的论文仅过去七年，但那是一个改变游戏规则的时刻。
- en: Utilizing the tf.Keras built-in VGG16 net module
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用内置的tf.Keras VGG16网络模块
- en: '`tf.Keras` applications are pre-built and pretrained deep learning models.
    The weights are downloaded automatically when instantiating a model and stored
    at `~/.keras/models/`. Using built-in code is very easy:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Keras`应用程序是预构建和预训练的深度学习模型。在实例化模型时，权重将自动下载并存储在`~/.keras/models/`中。使用内置代码非常简单：'
- en: '[PRE28]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, let us consider a train, ![steam-locomotive.jpg](img/B18331_03_Train.png).
    If we run the code, we get `820` as a result, which is the ImageNet code for “steam
    locomotive.” Equally important, all the other classes have very weak support,
    as shown in *Figure 3.13*:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑一辆火车，![steam-locomotive.jpg](img/B18331_03_Train.png)。如果我们运行代码，结果是 `820`，这是“蒸汽机车”在
    ImageNet 中的代码。同样重要的是，所有其他类别的支持度非常弱，如*图 3.13*所示：
- en: '![Chart, histogram  Description automatically generated](img/B18331_03_13.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, histogram  Description automatically generated](img/B18331_03_13.png)'
- en: 'Figure 3.13: A steam train is the most likely outcome'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13：蒸汽火车是最可能的结果
- en: To conclude this section, note that VGG16 is only one of the modules that are
    pre-built in `tf.Keras`. A full list of pretrained models is available online
    (see [https://www.tensorflow.org/api_docs/python/tf/keras/applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications)).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 结束本节时，请注意 VGG16 只是 `tf.Keras` 中预构建模块之一。预训练模型的完整列表可在线获取（见 [https://www.tensorflow.org/api_docs/python/tf/keras/applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications)）。
- en: Recycling pre-built deep learning models for extracting features
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回收预构建的深度学习模型以提取特征
- en: One very simple idea is to use VGG16, and more generally DCNN, for feature extraction.
    This code implements the idea by extracting features from a specific layer.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常简单的思路是使用 VGG16，通常是 DCNN，用于特征提取。此代码通过从特定层提取特征来实现这一思路。
- en: 'Note that we need to switch to the functional API since the sequential model
    only accepts layers:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们需要切换到函数式 API，因为顺序模型仅接受层：
- en: '[PRE29]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You might wonder why we want to extract the features from an intermediate layer
    in a DCNN. The reasoning is that as the network learns to classify images into
    categories, each layer learns to identify the features that are necessary to perform
    the final classification. Lower layers identify lower-order features such as color
    and edges, and higher layers compose these lower-order features into higher-order
    features such as shapes or objects. Hence, the intermediate layer has the capability
    to extract important features from an image, and these features are more likely
    to help in different kinds of classification.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么我们要从 DCNN 的中间层提取特征。其理由是，当网络学习将图像分类到不同类别时，每一层都会学习识别执行最终分类所需的特征。较低层识别较低阶的特征，如颜色和边缘，而较高层则将这些低阶特征组合成更高阶的特征，如形状或物体。因此，中间层有能力从图像中提取重要特征，这些特征更有可能帮助不同种类的分类。
- en: This has multiple advantages. First, we can rely on publicly available large-scale
    training and transfer this learning to novel domains. Second, we can save time
    on expensive training. Third, we can provide reasonable solutions even when we
    don’t have a large number of training examples for our domain. We also get a good
    starting network shape for the task at hand, instead of guessing it.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这有多个优点。首先，我们可以依赖公开的大规模训练，并将这种学习转移到新的领域。其次，我们可以节省在昂贵训练上的时间。第三，即使我们没有大量的领域训练样本，我们也能提供合理的解决方案。我们还可以为当前任务获得一个良好的初始网络结构，而不是盲目猜测。
- en: With this, we will conclude the overview of VGG16 CNNs, the last deep learning
    model defined in this chapter.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们将结束对 VGG16 CNN 的概述，这是本章定义的最后一个深度学习模型。
- en: Deep Inception V3 for transfer learning
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于迁移学习的深度 Inception V3
- en: Transfer learning is a very powerful deep learning technique that has applications
    in a number of different domains. The idea behind transfer learning is very simple
    and can be explained with an analogy. Suppose you want to learn a new language,
    say Spanish. Then it could be useful to start from what you already know in a
    different language, say English.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是一种非常强大的深度学习技术，应用于多个不同领域。迁移学习的基本思想非常简单，可以通过一个类比来解释。假设你想学一门新语言，比如西班牙语。那么，从你已经掌握的其他语言（比如英语）开始可能会很有帮助。
- en: Following this line of thinking, computer vision researchers now commonly use
    pretrained CNNs to generate representations for novel tasks [1], where the dataset
    may not be large enough to train an entire CNN from scratch. Another common tactic
    is to take the pretrained ImageNet network and then fine-tune the entire network
    to the novel task. For instance, we can take a network trained to recognize 10
    categories of music and fine-tune it to recognize 20 categories of movies.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这个思路，计算机视觉研究人员现在通常使用预训练的 CNN 来为新任务生成表示[1]，其中数据集可能不足以从头训练一个完整的 CNN。另一种常见策略是拿到预训练的
    ImageNet 网络，然后微调整个网络以适应新的任务。例如，我们可以拿一个训练来识别 10 类音乐的网络，并微调它来识别 20 类电影。
- en: 'Inception V3 is a very deep ConvNet developed by Google [2]. `tf.Keras` implements
    the full network, as described in *Figure 3.14*, and it comes pretrained on ImageNet.
    The default input size for this model is 299x299 on three channels:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Inception V3 是一个由 Google 开发的非常深的卷积神经网络[2]。`tf.Keras`实现了完整的网络，如*图 3.14*所示，并且它已经在
    ImageNet 上进行了预训练。该模型的默认输入尺寸是 299x299，且有三个通道：
- en: '![](img/B18331_03_14.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_03_14.png)'
- en: 'Figure 3.14: The Inception V3 deep learning model'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.14：Inception V3 深度学习模型
- en: 'This skeleton example is inspired by a scheme available online (see [https://keras.io/applications/](https://keras.io/applications/)).
    Let’s suppose we have a training dataset *D* in a different domain from ImageNet.
    *D* has 1,024 features in input and 200 categories in output. Let’s look at a
    code fragment:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这个骨架示例灵感来源于一个在线可用的方案（见[https://keras.io/applications/](https://keras.io/applications/)）。假设我们有一个来自不同领域的训练数据集
    *D*，与 ImageNet 不同。*D* 的输入有 1,024 个特征，输出有 200 个类别。让我们看一个代码片段：
- en: '[PRE30]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We use a trained Inception V3 model: we do not include the fully connected
    layer – the dense layer with 1,024 inputs – because we want to fine-tune on *D*.
    The preceding code fragment will download the pretrained weights on our behalf:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个经过训练的 Inception V3 模型：我们不包括全连接层——具有 1,024 个输入的稠密层——因为我们希望在 *D* 上进行微调。上述代码片段将会为我们下载预训练的权重：
- en: '[PRE31]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'So, if you look at the last four layers (where `include_top=True`), you see
    these shapes:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你查看最后四层（`include_top=True`时），你会看到这些形状：
- en: '[PRE32]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'When `include_top=False`, you are removing the last three layers and exposing
    the `mixed_10` layer. The `GlobalAveragePooling2D` layer converts `(None, 8, 8,
    2048)` to `(None, 2048)`, where each element in the `(None, 2048)` tensor is the
    average value for each corresponding `(8,8)` subtensor in the `(None, 8, 8, 2048)`
    tensor. `None` means an unspecified dimension, which is useful if you define a
    placeholder:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 当`include_top=False`时，你移除了最后三层并暴露了`mixed_10`层。`GlobalAveragePooling2D`层将`(None,
    8, 8, 2048)`转换为`(None, 2048)`，其中`(None, 2048)`张量中的每个元素是对应的`(8,8)`子张量在`(None, 8,
    8, 2048)`张量中的平均值。`None`表示未指定的维度，这对于定义占位符非常有用：
- en: '[PRE33]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'All the convolutional levels are pretrained, so we freeze them during the training
    of the full model:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 所有卷积层都是预训练的，因此我们在训练完整模型时冻结这些层：
- en: '[PRE34]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The model is then compiled and trained for a few epochs so that the top layers
    are trained. For the sake of simplicity, here we are omitting the training code
    itself:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，模型会被编译并训练几个周期，以便训练顶层。为了简化，这里我们省略了训练代码本身：
- en: '[PRE35]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Then, we freeze the top inception layers and fine-tune the other inception
    layers. In this example, we decide to freeze the first 172 layers (this is a tunable
    hyperparameter):'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们冻结顶层的 Inception 层，并微调其他 Inception 层。在这个示例中，我们决定冻结前 172 层（这是一个可调的超参数）：
- en: '[PRE36]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The model is then recompiled for fine-tuning optimization:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，模型会重新编译以进行微调优化：
- en: '[PRE37]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now we have a new deep network that reuses a standard Inception V3 network,
    but it is trained on a new domain *D* via transfer learning. Of course, there
    are many fine-tuning parameters for achieving good accuracy. However, we are now
    re-using a very large pretrained network as a starting point via transfer learning.
    In doing so, we can save the need for training on our machines by reusing what
    is already available in `tf.Keras`.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个新的深度网络，它重用了一个标准的 Inception V3 网络，但通过迁移学习训练于新领域 *D*。当然，针对实现良好精度有许多微调参数。然而，我们现在通过迁移学习重新使用了一个非常大的预训练网络作为起点。这样做的好处是我们可以避免在我们的机器上进行训练，而是重用`tf.Keras`中已经可用的内容。
- en: Other CNN architectures
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他 CNN 架构
- en: In this section, we will discuss many other different CNN architectures, including
    AlexNet, residual networks, highwayNets, DenseNets, and Xception.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论许多其他不同的 CNN 架构，包括 AlexNet、残差网络、highwayNets、DenseNets 和 Xception。
- en: AlexNet
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AlexNet
- en: One of the first convolutional networks was AlexNet [4], which consisted of
    only eight layers; the first five were convolutional ones with max-pooling layers,
    and the last three were fully connected. AlexNet [4] is an article cited more
    than 35,000 times, which started the deep learning revolution (for computer vision).
    Then, networks started to become deeper and deeper. Recently, a new idea has been
    proposed.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 最早的卷积网络之一是 AlexNet [4]，它只有八层；前五层是卷积层和最大池化层，最后三层是全连接层。AlexNet [4] 是一篇被引用超过 35,000
    次的文章，它开启了深度学习（尤其是计算机视觉）的革命。此后，网络变得越来越深。最近，提出了一种新思路。
- en: Residual networks
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 残差网络
- en: Residual networks are based on the interesting idea of allowing earlier layers
    to be fed directly into deeper layers. These are the so-called skip connections
    (or fast-forward connections). The key idea is to minimize the risk of vanishing
    or exploding gradients for deep networks (see *Chapter 8*, *Autoencoders*).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 残差网络基于一个有趣的思想，即允许早期层的输出直接输入到更深的层中。这就是所谓的跳跃连接（或快进连接）。其关键思想是最小化深度网络中梯度消失或爆炸的风险（见
    *第 8 章*，*自编码器*）。
- en: 'The building block of a ResNet is called a “residual block” or “identity block,”
    which includes both forward and fast-forward connections. In this example (*Figure
    3.15*), the output of an earlier layer is added to the output of a later layer
    before being sent into a ReLU activation function:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet 的构建块称为“残差块”或“恒等块”，包括前向连接和快进连接。在这个示例中（*图 3.15*），早期层的输出与后期层的输出相加，然后传递到
    ReLU 激活函数中：
- en: '![Diagram  Description automatically generated](img/B18331_03_15.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图示描述](img/B18331_03_15.png)'
- en: 'Figure 3.15: An example of image segmentation'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.15：图像分割示例
- en: HighwayNets and DenseNets
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HighwayNets 和 DenseNets
- en: 'An additional weight matrix may be used to learn the skip weights and these
    models are frequently denoted as HighwayNets. Instead, models with several parallel
    skips are known as DenseNets [5]. It has been noticed that the human brain might
    have similar patterns to residual networks since the cortical layer VI neurons
    get input from layer I, skipping intermediary layers. In addition, residual networks
    can be faster to train than traditional CNNs since there are fewer layers to propagate
    through during each iteration (deeper layers get input sooner due to the skip
    connection). *Figure 3.16* shows an example of a DenseNet (based on [http://arxiv.org/abs/1608.06993](http://arxiv.org/abs/1608.06993)):'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会使用额外的权重矩阵来学习跳跃权重，这些模型通常被称为 HighwayNets。相反，具有多个并行跳跃的模型被称为 DenseNets [5]。有研究指出，人类大脑可能有类似于残差网络的结构，因为大脑皮层
    VI 层的神经元从 I 层获取输入，跳过了中间层。此外，残差网络比传统的 CNN 更快训练，因为每次迭代时传播的层数较少（由于跳跃连接，深层输入更早到达）。*图
    3.16* 显示了一个 DenseNet 的示例（基于 [http://arxiv.org/abs/1608.06993](http://arxiv.org/abs/1608.06993)）：
- en: '![](img/B18331_03_16.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_03_16.png)'
- en: 'Figure 3.16: An example of a DenseNet'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.16：DenseNet 示例
- en: Xception
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Xception
- en: 'Xception networks use two basic blocks: a depthwise convolution and a pointwise
    convolution. A depthwise convolution is the channel-wise n x n spatial convolution.
    Suppose an image has three channels, then we have three convolutions of n x n.
    A pointwise convolution is a 1 x 1 convolution. In Xception, an “extreme” version
    of an Inception module, we first use a 1 x 1 convolution to map cross-channel
    correlations, and then separately map the spatial correlations of every output
    channel as shown in *Figure 3.17* (from [https://arxiv.org/pdf/1610.02357.pdf](https://arxiv.org/pdf/1610.02357.pdf)):'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Xception 网络使用两个基本模块：深度卷积和点卷积。深度卷积是通道级的 n x n 空间卷积。假设一张图像有三个通道，那么我们就有三个 n x n
    的卷积。点卷积是 1 x 1 卷积。在 Xception 中，作为 Inception 模块的“极端”版本，首先使用 1 x 1 卷积来映射跨通道的相关性，然后分别映射每个输出通道的空间相关性，如
    *图 3.17* 所示（来自 [https://arxiv.org/pdf/1610.02357.pdf](https://arxiv.org/pdf/1610.02357.pdf)）：
- en: '![Diagram  Description automatically generated](img/B18331_03_17.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图示描述](img/B18331_03_17.png)'
- en: 'Figure 3.17: An example of an extreme form of an Inception module'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.17：Inception 模块的极端形式示例
- en: '**Xception** (**eXtreme Inception**) is a deep convolutional neural network
    architecture inspired by Inception, where Inception modules have been replaced
    with depthwise separable convolutions. Xception uses multiple skip-connections
    in a similar way to ResNet. The final architecture is rather complex as illustrated
    in *Figure 3.18* (from [https://arxiv.org/pdf/1610.02357.pdf](https://arxiv.org/pdf/1610.02357.pdf)).
    Data first goes through the entry flow, then through the middle flow, which is
    repeated eight times, and finally through the exit flow:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**Xception**（**极限 Inception**）是一种深度卷积神经网络架构，灵感来自 Inception，其中 Inception 模块已被深度可分离卷积所替代。Xception
    使用了多个跳跃连接，方式类似于 ResNet。最终的架构相当复杂，如*图 3.18*所示（来自 [https://arxiv.org/pdf/1610.02357.pdf](https://arxiv.org/pdf/1610.02357.pdf)）。数据首先通过入口流，然后通过中间流，中间流重复八次，最后通过出口流：'
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B18331_03_18.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面 描述自动生成，信心较高](img/B18331_03_18.png)'
- en: 'Figure 3.18: The full Xception architecture'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.18：完整的 Xception 架构
- en: 'Residual networks, HyperNets, DenseNets, Inception, and Xceptions are all available
    as pretrained nets in both `tf.Keras.application` and `tf.Hub`. The Keras website
    has a nice summary of the performance achieved on the ImageNet dataset and the
    depth of each network. The summary is available at [https://keras.io/applications/](https://keras.io/applications/):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 残差网络、HyperNets、DenseNets、Inception 和 Xception 都可以作为预训练网络，在 `tf.Keras.application`
    和 `tf.Hub` 中使用。Keras 网站上有一个很好的总结，展示了在 ImageNet 数据集上的表现以及每个网络的深度。总结可以在 [https://keras.io/applications/](https://keras.io/applications/)
    上找到：
- en: '![Table  Description automatically generated](img/B18331_03_19.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![表格 描述自动生成](img/B18331_03_19.png)'
- en: 'Figure 3.19: Different CNNs and accuracy on top-1 and top-5 results'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.19：不同的 CNN 和 top-1 及 top-5 准确率结果
- en: The top-1 and top-5 accuracy refers to a model’s performance on the ImageNet
    validation dataset.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: top-1 和 top-5 准确率指的是模型在 ImageNet 验证数据集上的表现。
- en: In this section, we have discussed many CNN architectures. The next section
    is about style transfer, a deep learning technique used for training neural networks
    to create art.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们讨论了许多 CNN 架构。下一节将介绍风格迁移，这是一种用于训练神经网络创造艺术的深度学习技术。
- en: Style transfer
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 风格迁移
- en: 'Style transfer is a funny neural network application that provides many insights
    into the power of neural networks. So what exactly is it? Imagine that you observe
    a painting done by a famous artist. In principle, you are observing two elements:
    the painting itself (say the face of a woman, or a landscape) and something more
    intrinsic, the “style” of the artist. What is the style? That is more difficult
    to define, but humans know that Picasso had his own style, Matisse had his own
    style, and each artist has his/her own style. Now, imagine taking a famous painting
    of Matisse, giving it to a neural network, and letting the neural network repaint
    it in Picasso’s style. Or, imagine taking your own photo, giving it to a neural
    network, and having your photo painted in Matisse’s or Picasso’s style, or in
    the style of any other artist that you like. That’s what style transfer does.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 风格迁移是一个有趣的神经网络应用，提供了许多关于神经网络强大能力的见解。那么它到底是什么呢？假设你看到一幅著名艺术家的画作。从原则上讲，你观察到的是两个元素：画作本身（例如一位女性的肖像，或者一幅风景画）以及更内在的东西——艺术家的“风格”。风格是什么？这很难定义，但人们知道毕加索有他的风格，马蒂斯有他的风格，每个艺术家都有他/她的独特风格。现在，假设你拿到了一幅马蒂斯的名画，交给神经网络，让神经网络以毕加索的风格重新绘制它。或者，假设你拿到一张自己的照片，交给神经网络，让神经网络将你的照片以马蒂斯或毕加索的风格，或者以任何你喜欢的艺术家的风格来绘制。这就是风格迁移的作用。
- en: 'For instance, go to [https://deepart.io/](https://deepart.io/) and see a cool
    demo as shown in the image below, where deepart has been applied by taking the
    “Van Gogh” style as observed in the Sunflowers painting (this is a public domain
    image: “Sonnenblumen. Arles, 1888 Öl auf Leinwand, 92,5 x 73 cm Vincent van Gogh”
    [https://commons.wikimedia.org/wiki/Vincent_van_Gogh#/media/File:Vincent_Van_Gogh_0010.jpg](https://commons.wikimedia.org/wiki/Vincent_van_Gogh#/media/File:Vincent_Van_Gogh_0010.jpg))
    and applying it to a picture of my daughter Aurora:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，访问 [https://deepart.io/](https://deepart.io/) 查看一个酷炫的演示，如下图所示，其中 deepart
    应用了“梵高”风格，灵感来自《向日葵》画作（这是一张公有领域图像：“Sonnenblumen. Arles, 1888 油画，92.5 x 73 cm Vincent
    van Gogh” [https://commons.wikimedia.org/wiki/Vincent_van_Gogh#/media/File:Vincent_Van_Gogh_0010.jpg](https://commons.wikimedia.org/wiki/Vincent_van_Gogh#/media/File:Vincent_Van_Gogh_0010.jpg)），并将其应用于我女儿
    Aurora 的照片：
- en: '![A person smiling next to a painting  Description automatically generated
    with low confidence](img/B18331_03_20.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![一位微笑的人站在一幅画旁，描述自动生成，置信度较低](img/B18331_03_20.png)'
- en: 'Figure 3.20: An example of deepart'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.20：Deepart 示例
- en: 'Now, how can we define more formally the process of style transfer? Well, style
    transfer is the task of producing an artificial image *x* that shares the content
    of a source content image *p* and the style of a source style image *a*. So, intuitively
    we need two distance functions: one distance function measures how different the
    content of two images is, *L*[content], while the other distance function measures
    how different the style of two images is, *L*[style]. Then, the transfer style
    can be seen as an optimization problem where we try to minimize these two metrics.
    As in *A Neural Algorithm of Artistic Style* by Leon A. Gatys, Alexander S. Ecker,
    and Matthias Bethge ([https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576)),
    we use a pretrained network to achieve style transfer. In particular, we can feed
    a VGG19 (or any suitable pretrained network) to extract features that represent
    images in an efficient way. Now we are going to define two functions used for
    training the network: the content distance and the style distance.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何更正式地定义风格迁移的过程呢？实际上，风格迁移是生成一张人工图像 *x* 的任务，这张图像具有源内容图像 *p* 的内容和源风格图像 *a*
    的风格。所以，从直觉上讲，我们需要两个距离函数：一个距离函数衡量两张图像内容的差异，*L*[content]，另一个距离函数衡量两张图像风格的差异，*L*[style]。然后，风格迁移可以看作是一个优化问题，在这个问题中，我们尝试最小化这两个度量值。正如
    Leon A. Gatys、Alexander S. Ecker 和 Matthias Bethge 在 *A Neural Algorithm of Artistic
    Style* 中所述（[https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576)），我们使用预训练的网络来实现风格迁移。特别是，我们可以输入一个
    VGG19（或任何合适的预训练网络）来提取有效表示图像的特征。现在我们将定义两个用于训练网络的函数：内容距离和风格距离。
- en: Content distance
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内容距离
- en: 'Given two images, *p* content image and *x* input image, we define the content
    distance as the distance in the feature space defined by a layer *l* for a VGG19
    network receiving the two images as an input. In other words, the two images are
    represented by the features extracted by a pretrained VGG19\. These features project
    the images into a feature “content” space where the “content” distance can be
    conveniently computed as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两张图像，*p* 内容图像和 *x* 输入图像，我们定义内容距离为通过 VGG19 网络的一个层 *l* 在特征空间中的距离，该网络接收两张图像作为输入。换句话说，这两张图像通过预训练的
    VGG19 提取的特征来表示。这些特征将图像投射到一个特征“内容”空间，在这个空间中，可以方便地计算“内容”距离，如下所示：
- en: '![](img/B18331_03_001.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_03_001.png)'
- en: 'To generate nice images, we need to ensure that the content of the generated
    image is similar to (i.e. has a small distance from) that of the input image.
    The distance is therefore minimized with standard backpropagation. The code is
    simple:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成漂亮的图像，我们需要确保生成图像的内容与输入图像的内容相似（即，距离较小）。因此，通过标准的反向传播算法来最小化该距离。代码非常简单：
- en: '[PRE38]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Style distance
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 风格距离
- en: As discussed, the features in the higher layers of VGG19 are used as content
    representations. You can think about these features as filter responses. To represent
    the style we use in a gram matrix *G* (defined as the matrix *v*^T*v* for a vector
    *v*), we consider ![](img/B18331_03_002.png) as the inner matrix for map *i* and
    map *j* at layer *l* of the VGG19\. It is possible to show that the Gram matrix
    represents the correlation matrix between different filter responses.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，VGG19 更高层次的特征用于作为内容表示。你可以把这些特征看作是滤波器的响应。为了表示风格，我们使用 Gram 矩阵 *G*（定义为向量 *v*
    的矩阵 *v*^T *v*），我们考虑 ![](img/B18331_03_002.png) 作为 VGG19 网络第 *l* 层中映射 *i* 和映射 *j*
    的内积矩阵。可以证明，Gram 矩阵表示不同滤波器响应之间的相关性矩阵。
- en: 'The contribution of each layer to the total style loss is defined as:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层对总风格损失的贡献定义为：
- en: '![](img/B18331_03_003.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_03_003.png)'
- en: 'where ![](img/B18331_03_002.png) is the Gram matrix for input image *x* and
    ![](img/B18331_03_005.png) is the gram matrix for the style image a, and *N*[l]
    is the number of feature maps, each of size ![](img/B18331_03_006.png). The Gram
    matrix can project the images into a space where the style is taken into account.
    In addition, the feature correlations from multiple VGG19 layers are used because
    we want to consider multi-scale information and a more robust style representation.
    The total style loss across levels is the weighted sum:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，![](img/B18331_03_002.png)是输入图像*x*的Gram矩阵，![](img/B18331_03_005.png)是风格图像a的Gram矩阵，*N*[l]是特征图的数量，每个特征图的大小为![](img/B18331_03_006.png)。Gram矩阵可以将图像投影到一个空间中，在该空间内，风格得到了考虑。此外，还使用了来自多个VGG19层的特征相关性，因为我们希望考虑多尺度信息和更强大的风格表示。跨层的总风格损失是加权和：
- en: '| ![](img/B18331_03_007.png) | ![](img/B18331_03_008.png) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/B18331_03_007.png) | ![](img/B18331_03_008.png) |'
- en: 'The key idea is therefore to perform gradient descent on the content image
    to make its style similar to the style image. The code is simple:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，关键思想是对内容图像执行梯度下降，使其风格与风格图像相似。代码很简单：
- en: '[PRE39]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In short, the concepts behind style transfer are simple: first, we use VGG19
    as a feature extractor and then we define two suitable function distances, one
    for style and the other one for contents, which are appropriately minimized. If
    you want to try this out for yourself, then TensorFlow tutorials are available
    online. A tutorial is available at [https://colab.research.google.com/github/tensorflow/models/blob/master/research/nst_blogpost/4_Neural_Style_Transfer_with_Eager_Execution.ipynb](https://colab.research.google.com/github/tensorflow/models/blob/master/research/nst_blogpost/4_Neural_Style_Transfer_with_Eager_Execution.ipynb).
    If you are interested in a demo of this technique, you can go to the deepart.io
    free site where they do style transfer.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，风格迁移背后的概念很简单：首先，我们使用VGG19作为特征提取器，然后定义两个合适的函数距离，一个用于风格，另一个用于内容，并进行适当的最小化。如果你想亲自尝试，可以在线找到TensorFlow教程。教程可以在[https://colab.research.google.com/github/tensorflow/models/blob/master/research/nst_blogpost/4_Neural_Style_Transfer_with_Eager_Execution.ipynb](https://colab.research.google.com/github/tensorflow/models/blob/master/research/nst_blogpost/4_Neural_Style_Transfer_with_Eager_Execution.ipynb)找到。如果你对这个技术的演示感兴趣，可以去deepart.io的免费站点，他们提供风格迁移服务。
- en: Summary
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have learned how to use deep learning ConvNets to recognize
    MNIST handwritten characters with high accuracy. We used the CIFAR-10 dataset
    to build a deep learning classifier with 10 categories, and the ImageNet dataset
    to build an accurate classifier with 1,000 categories. In addition, we investigated
    how to use large deep learning networks such as VGG16 and very deep networks such
    as Inception V3\. We concluded with a discussion on transfer learning.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用深度学习卷积神经网络（ConvNets）来高精度地识别MNIST手写字符。我们使用CIFAR-10数据集构建了一个包含10个类别的深度学习分类器，并使用ImageNet数据集构建了一个准确的包含1000个类别的分类器。此外，我们还探讨了如何使用大型深度学习网络，如VGG16，以及非常深的网络，如Inception
    V3。最后，我们讨论了迁移学习的应用。
- en: In the next chapter, we’ll see how to work with word embeddings and why these
    techniques are important for deep learning.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何使用词嵌入，并探讨这些技术为何对深度学习至关重要。
- en: References
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: LeCun, Y. and Bengio, Y. (1995). *Convolutional networks for images, speech,
    and time series*. The Handbook of Brain Theory Neural Networks, vol. 3361\.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LeCun, Y. 和 Bengio, Y. (1995). *用于图像、语音和时间序列的卷积神经网络*。大脑理论与神经网络手册，第3361卷。
- en: Wan. L, Zeiler M., Zhang S., Cun, Y. L., and Fergus R. (2014). *Regularization
    of neural networks using dropconnect*. *Proc. 30th Int. Conf. Mach. Learn*., pp.
    1058–1066.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wan. L, Zeiler M., Zhang S., Cun, Y. L., 和 Fergus R. (2014). *使用dropconnect的神经网络正则化*。*第30届国际机器学习会议论文集*，第1058-1066页。
- en: 'Graham B. (2014). *Fractional Max-Pooling*. arXiv Prepr. arXiv: 1412.6071.'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Graham B. (2014). *分数最大池化*。arXiv预印本，arXiv: 1412.6071。'
- en: Simonyan K. and Zisserman A. (Sep. 2014). *Very Deep Convolutional Networks
    for Large-Scale Image Recognition*. arXiv ePrints.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Simonyan K. 和 Zisserman A. (2014年9月). *用于大规模图像识别的非常深的卷积神经网络*。arXiv电子打印。
- en: Join our book’s Discord space
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区，与志同道合的人一起学习，和超过2000名成员一起进步：[https://packt.link/keras](https://packt.link/keras)
- en: '![](img/QR_Code1831217224278819687.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1831217224278819687.png)'
