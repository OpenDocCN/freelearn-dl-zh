- en: Chapter 10. Predicting Times Sequences with Advanced RNN
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章：使用高级RNN预测时间序列
- en: This chapter covers advanced techniques for recurrent neural networks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了递归神经网络的高级技术。
- en: The techniques seen in [Chapter 2](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 2. Classifying Handwritten Digits with a Feedforward Network"), *Classifying
    Handwritten Digits with a Feedforward Network*, for feedforward networks, such
    as going deeper with more layers, or adding a dropout layer, have been more challenging
    for recurrent networks and require some new design principles.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b "第2章：使用前馈网络分类手写数字")中看到的技术，*使用前馈网络分类手写数字*，对于前馈网络，例如通过增加更多层次或添加Dropout层等，已经成为递归网络面临的挑战，并且需要一些新的设计原则。
- en: Since adding new layers increases the vanishing/exploding gradient issue, a
    new technique based on identity connections as for [Chapter 7](part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 7. Classifying Images with Residual Networks"), *Classifying Images with
    Residual Networks* has proved to provide state-of-the-art results.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于增加新层会加剧消失/爆炸梯度问题，一种基于身份连接的新技术，如在[第7章](part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b
    "第7章：使用残差网络分类图像")中所述，*使用残差网络分类图像*，已证明能够提供最先进的结果。
- en: 'The topics covered are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 涵盖的主题包括：
- en: Variational RNN
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变分RNN
- en: Stacked RNN
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠RNN
- en: Deep Transition RNN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度过渡RNN
- en: Highway connections and their application to RNN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高速公路连接及其在RNN中的应用
- en: Dropout for RNN
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN的Dropout
- en: The application of dropout inside neural networks has long been a subject of
    research, since the naïve application of dropout to the recurrent connection introduced
    lots more instability and difficulties to training the RNN.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout在神经网络中的应用一直是研究的主题，因为简单地将Dropout应用于递归连接会引入更多的不稳定性和训练RNN的困难。
- en: 'A solution has been discovered, derived from the variational **Bayes Network**
    theory. The resulting idea is very simple and consists of preserving the same
    dropout mask for the whole sequence on which the RNN is training, as shown in
    the following picture, and generating a new dropout mask at each new sequence:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案已经被发现，它源自变分**贝叶斯网络**理论。最终的思想非常简单， consiste of 保持相同的Dropout掩码用于整个RNN训练序列，如下图所示，并在每个新序列上生成新的Dropout掩码：
- en: '![Dropout for RNN](img/00179.jpeg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![RNN的Dropout](img/00179.jpeg)'
- en: Such a technique is called **variational RNN.** For the connections that have
    the same arrows in the preceding figure, we'll keep the noise mask constant for
    the all sequence.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术被称为**变分RNN**。对于前图中具有相同箭头的连接，我们将为整个序列保持噪声掩码不变。
- en: 'For that purpose, we''ll introduce the symbolic variables `_is_training` and
    `_noise_x` to add a random (variational) noise (dropout) to input, output, and
    recurrent connection during training:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将引入符号变量`_is_training`和`_noise_x`，在训练过程中为输入、输出和递归连接添加随机（变分）噪声（Dropout）：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Deep approaches for RNN
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN的深度方法
- en: 'The core principle of deep learning to improve the representative power of
    a network is to add more layers. For RNN, two approaches to increase the number
    of layers are possible:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的核心原理是通过增加更多层次来提升网络的表示能力。对于RNN，增加层数有两种可能的方式：
- en: The first one is known as **stacking** or **stacked recurrent network**, where
    the output of the hidden layer of a first recurrent net is used as input to a
    second recurrent net, and so on, with as many recurrent networks on top of each
    other:![Deep approaches for RNN](img/00180.jpeg)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个方法被称为**堆叠**或**堆叠递归网络**，其中第一个递归网络的隐藏层输出作为第二个递归网络的输入，依此类推，多个递归网络层叠在一起：![RNN的深度方法](img/00180.jpeg)
- en: 'For a depth *d* and *T* time steps, the maximum number of connections between
    input and output is *d + T – 1*:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度*d*和*T*时间步长，输入与输出之间的最大连接数为*d + T – 1*：
- en: The second approach is the **deep transition network**, consisting of adding
    more layers to the recurrent connection:![Deep approaches for RNN](img/00181.jpeg)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二种方法是**深度过渡网络**，它通过向递归连接中添加更多层次来实现：![RNN的深度方法](img/00181.jpeg)
- en: Figure 2
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图2
- en: In this case, the maximum number of connections between input and output is
    *d x T*, which has been proved to be a lot more powerful.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，输入与输出之间的最大连接数为*d x T*，已被证明更加强大。
- en: Both approaches provide better results.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 两种方法都能提供更好的结果。
- en: However, in the second approach, as the number of layers increases by a factor,
    the training becomes much more complicated and unstable since the signal fades
    or explodes a lot faster. We'll address this problem later by tackling the principle
    of recurrent highway connections.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在第二种方法中，随着层数的增加，训练变得更加复杂和不稳定，因为信号会更快地消失或爆炸。我们将在稍后通过处理递归高速公路连接的原理来解决这个问题。
- en: 'First, as usual, sequences of words, represented as an array of index values
    in the vocabulary, and of dimension (`batch_size, num_steps`), are embedded into
    an input tensor of dimension (`num_steps, batch_size, hidden_size`):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，像往常一样，将作为词汇索引值数组的单词序列，维度为（`batch_size, num_steps`），嵌入到维度为（`num_steps, batch_size,
    hidden_size`）的输入张量中：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The symbolic input variable `_lr` enables the decrease of the learning rate
    during training:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 符号输入变量`_lr`使得在训练过程中可以减少学习率：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let's begin with the first approach, the stacked recurrent networks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从第一种方法开始，即堆叠递归网络。
- en: Stacked recurrent networks
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠递归网络
- en: 'To stack recurrent networks, we connect the hidden layer of the following recurrent
    network, to the input of the preceding recurrent network:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要堆叠递归网络，我们将下一个递归网络的隐藏层连接到前一个递归网络的输入：
- en: '![Stacked recurrent networks](img/00182.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![堆叠递归网络](img/00182.jpeg)'
- en: When the number of layers is one, our implementation is a recurrent network
    as in the previous chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当层数为一时，我们的实现就是前一章中的递归网络。
- en: 'First we implement dropout in our simple RNN model:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在简单的RNN模型中实现了dropout：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We do the same in our LSTM model:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在LSTM模型中做同样的事情：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Running our stacked networks:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 运行我们的堆叠网络：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We get 15,203,150 parameters for the RNN, with 326 **words per seconds** (WPS)
    on a CPU and 4,806 WPS on a GPU.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RNN，我们得到了15,203,150个参数，在CPU上的速度为326 **每秒字数**（WPS），在GPU上的速度为4,806 WPS。
- en: For LSTM, the number of parameters is 35,882,600 with a speed of 1,445 WPS on
    a GPU.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LSTM，参数数量为35,882,600，在GPU上的速度为1,445 WPS。
- en: 'The stacked RNN do not converge, as we might have imagined: the vanishing/exploding
    gradient issue is increased with depth.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠RNN没有收敛，正如我们预想的那样：随着深度增加，梯度消失/爆炸问题加剧。
- en: LSTM, designed to reduce such as an issue, do converge a lot better when stacked,
    than as a single layer.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM，旨在减少此类问题，在堆叠时的收敛效果远好于单层网络。
- en: Deep transition recurrent network
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度转移递归网络
- en: Contrary to stacked recurrent network, a deep transition recurrent network consists
    of increasing the depth of the network along the time direction, by adding more
    layers or *micro-timesteps* inside the recurrent connection.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与堆叠递归网络相反，深度转移递归网络通过在递归连接中增加更多的层次或*微时间步*，来沿时间方向增加网络的深度。
- en: 'To illustrate this, let us come back to the definition of a transition/recurrent
    connection in a recurrent network: it takes as input the previous state ![Deep
    transition recurrent network](img/00183.jpeg) and the input data ![Deep transition
    recurrent network](img/00184.jpeg) at time step *t*, to predict its new state
    ![Deep transition recurrent network](img/00185.jpeg).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，让我们回到递归网络中转移/递归连接的定义：它以前一状态![深度转移递归网络](img/00183.jpeg)和时间步*t*时的输入数据![深度转移递归网络](img/00184.jpeg)为输入，预测其新状态![深度转移递归网络](img/00185.jpeg)。
- en: 'In a deep transition recurrent network (figure 2), the recurrent transition
    is developed with more than one layer, up to a recurrency depth *L*: the initial
    state is set to the output of the last transition:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度转移递归网络（图2）中，递归转移通过多个层次开发，直到递归深度*L*：初始状态被设置为最后一个转移的输出：
- en: '![Deep transition recurrent network](img/00186.jpeg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![深度转移递归网络](img/00186.jpeg)'
- en: 'Furthermore, inside the transition, multiple states or steps are computed:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在转移中，计算多个状态或步骤：
- en: '![Deep transition recurrent network](img/00187.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![深度转移递归网络](img/00187.jpeg)'
- en: 'The final state is the output of the transition:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最终状态是转移的输出：
- en: '![Deep transition recurrent network](img/00188.jpeg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![深度转移递归网络](img/00188.jpeg)'
- en: Highway networks design principle
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高速公路网络设计原理
- en: Adding more layers in the transition connections increases the vanishing or
    exploding gradient issue during backpropagation in long term dependency.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在转移连接中增加更多层会在长时间依赖中增加梯度消失或爆炸问题，特别是在反向传播过程中。
- en: In the [Chapter 4](part0051_split_000.html#1GKCM1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 4. Generating Text with a Recurrent Neural Net"), *Generating Text with
    a Recurrent Neural Net*, LSTM and GRU networks have been introduced as solutions
    to address this issue. Second order optimization techniques also help overcome
    this problem.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第4章](part0051_split_000.html#1GKCM1-ccdadb29edc54339afcb9bdf9350ba6b "第4章。使用递归神经网络生成文本")，*使用递归神经网络生成文本*
    中，已经介绍了 LSTM 和 GRU 网络作为解决方案来应对这个问题。二阶优化技术也有助于克服这个问题。
- en: A more general principle, based on **identity connections**, to improve the
    training in deep networks [Chapter 7](part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 7. Classifying Images with Residual Networks"), *Classifying Images with
    Residual Networks*, can also be applied to deep transition networks.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更一般的原理，基于 **恒等连接**，用于改善深度网络的训练，[第7章](part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b
    "第7章。使用残差网络分类图像")，*使用残差网络分类图像*，也可以应用于深度过渡网络。
- en: 'Here is the principle in theory:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是理论上的原理：
- en: 'Given an input *x* to a hidden layer *H* with weigh ![Highway networks design
    principle](img/00189.jpeg):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个输入 *x* 到隐藏层 *H*，并带有权重 ![高速公路网络设计原理](img/00189.jpeg)：
- en: '![Highway networks design principle](img/00190.jpeg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![高速公路网络设计原理](img/00190.jpeg)'
- en: 'A highway networks design consists of adding the original input information
    (with an identity layer) to the output of a layer or a group of layers, as a shortcut:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个高速公路网络设计包括将原始输入信息（通过恒等层）添加到一层或一组层的输出，作为快捷通道：
- en: '*y = x*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = x*'
- en: 'Two mixing gates, the *transform gate* ![Highway networks design principle](img/00191.jpeg)
    and the *carry gate*, ![Highway networks design principle](img/00192.jpeg) learn
    to modulate the influence of the transformation in the hidden layer, and the amount
    of original information to allow to pass through:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 两个混合门，*变换门* ![高速公路网络设计原理](img/00191.jpeg) 和 *传递门*，![高速公路网络设计原理](img/00192.jpeg)
    学会调节隐藏层中变换的影响，以及允许通过的原始信息量：
- en: '![Highway networks design principle](img/00193.jpeg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![高速公路网络设计原理](img/00193.jpeg)'
- en: 'Usually, to reduce the total number of parameters in order to get faster-to-train
    networks, the carry gate is taken as the complementary to 1 for the transform
    gate:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，为了减少总参数量以便加速训练网络，`carry` 门被设置为 `transform` 门的互补：
- en: '![Highway networks design principle](img/00194.jpeg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![高速公路网络设计原理](img/00194.jpeg)'
- en: Recurrent Highway Networks
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归高速公路网络
- en: 'So, let''s apply the highway network design to deep transition recurrent networks,
    which leads to the definition of **Recurrent Highway Networks** (**RHN**), and
    predict the output ![Recurrent Highway Networks](img/00185.jpeg) given ![Recurrent
    Highway Networks](img/00183.jpeg) the input of the transition:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们将高速公路网络设计应用于深度过渡递归网络，从而定义 **递归高速公路网络**（**RHN**），并根据给定的过渡输入预测输出 ![递归高速公路网络](img/00185.jpeg)
    和 ![递归高速公路网络](img/00183.jpeg)：
- en: '![Recurrent Highway Networks](img/00186.jpeg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![循环高速公路网络](img/00186.jpeg)'
- en: 'The transition is built with multiple steps of highway connections:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 过渡是通过多个步骤的高速公路连接构建的：
- en: '![Recurrent Highway Networks](img/00195.jpeg)![Recurrent Highway Networks](img/00188.jpeg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![递归高速公路网络](img/00195.jpeg)![递归高速公路网络](img/00188.jpeg)'
- en: 'Here the transform gate is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的变换门如下：
- en: '![Recurrent Highway Networks](img/00196.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![递归高速公路网络](img/00196.jpeg)'
- en: 'And, to reduce the number of weights, the carry gate is taken as the complementary
    to the transform gate:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少权重数量，`carry` 门被作为 `transform` 门的互补：
- en: '![Recurrent Highway Networks](img/00194.jpeg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![递归高速公路网络](img/00194.jpeg)'
- en: 'For faster computation on a GPU, it is better to compute the linear transformation
    on inputs over different time steps ![Recurrent Highway Networks](img/00197.jpeg)
    and ![Recurrent Highway Networks](img/00198.jpeg) in a single big matrix multiplication,
    all-steps input matrices ![Recurrent Highway Networks](img/00199.jpeg) and ![Recurrent
    Highway Networks](img/00200.jpeg) at once, since the GPU will use a better parallelization,
    and provide these inputs to the recurrency:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 GPU 上更快地计算，最好将不同时间步长的输入上的线性变换通过单次大矩阵乘法计算，即一次性计算所有时间步长的输入矩阵 ![递归高速公路网络](img/00199.jpeg)
    和 ![递归高速公路网络](img/00200.jpeg)，因为 GPU 会更好地并行化这些操作，并将这些输入提供给递归：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'With a deep transition between each step:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 每个步骤之间有深度过渡：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The recurrent hidden state of the RHN is sticky (the last hidden state of one
    batch is carried over to the next batch, to be used as an initial hidden state).
    These states are kept in a shared variable.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: RHN的递归隐藏状态是粘性的（一个批次的最后一个隐藏状态会传递到下一个批次，作为初始隐藏状态使用）。这些状态被保存在一个共享变量中。
- en: 'Let''s run the mode:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行模式：
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The number of parameters of the stacked RHN is *84,172,000*, its speed *420*
    wps on the GPU.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠的RHN的参数数量为*84,172,000*，其在GPU上的速度为*420* wps。
- en: This model is the new state-of-the-art model for recurrent neural network accuracy
    on texts.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是当前在文本上递归神经网络准确度的最新最先进模型。
- en: Further reading
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: 'You can refer to the following topics for more insights:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下主题以获取更多见解：
- en: '*Hi**ghway Networks* at: [https://arxiv.org/abs/1505.00387](https://arxiv.org/abs/1505.00387)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*高速公路网络*： [https://arxiv.org/abs/1505.00387](https://arxiv.org/abs/1505.00387)'
- en: '*Depth-Gated LSTM* at: [https://arxiv.org/abs/1508.03790](https://arxiv.org/abs/1508.03790)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度门控LSTM*： [https://arxiv.org/abs/1508.03790](https://arxiv.org/abs/1508.03790)'
- en: '*Learning Longer Memory in Recurrent N**eural Networks* at: [https://arxiv.org/abs/1412.7753](https://arxiv.org/abs/1412.7753)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*学习递归神经网络中的长期记忆*： [https://arxiv.org/abs/1412.7753](https://arxiv.org/abs/1412.7753)'
- en: '*Grid Long Short-Term Memory*, Nal Kalchbrenner, Ivo Danihelka, Alex Graves'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*网格长短期记忆*，Nal Kalchbrenner, Ivo Danihelka, Alex Graves'
- en: Zilly, J, Srivastava, R, Koutnik, J, Schmidhuber, J., *Recurrent Highway Networks*,
    2016
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zilly, J, Srivastava, R, Koutnik, J, Schmidhuber, J., *递归高速公路网络*，2016
- en: Gal, Y, *A Theoretically Grounded Application of Dropout in Recurrent Neural
    Networks*, 2015.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gal, Y, *递归神经网络中丢弃法的理论基础应用*，2015。
- en: Zaremba, W, Sutskever, I, Vinyals, O, *Recurrent neural network regularization*,
    2014.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zaremba, W, Sutskever, I, Vinyals, O, *递归神经网络正则化*，2014。
- en: Press, O, Wolf, L, *Using the Output Embedding to Improve Language Models*,
    2016.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Press, O, Wolf, L, *利用输出嵌入改进语言模型*，2016。
- en: 'Gated Feedback Recurrent Neural Networks: Junyoung Chung, Caglar Gulcehre,
    Kyunghyun Cho, Yoshua Bengio 2015'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 门控反馈递归神经网络：Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio 2015
- en: 'A Clockwork RNN: Jan Koutník, Klaus Greff, Faustino Gomez, Jürgen Schmidhuber
    2014'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时钟工作递归神经网络：Jan Koutník, Klaus Greff, Faustino Gomez, Jürgen Schmidhuber 2014
- en: Summary
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: A classic dropout method to improve network robustness may be applied to recurrent
    network sequence-wise or batch-wise to avoid instability and destruction of the
    recurrent transition. For example, when applied on word inputs/outputs, it is
    equivalent to removing the same words from the sentence, replacing them with a
    blank value.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一种经典的丢弃法可用于提高网络的鲁棒性，避免递归转换的不稳定性和破坏，且可以在递归网络的序列或批次中应用。例如，当应用于单词输入/输出时，相当于从句子中去除相同的单词，将其替换为空值。
- en: The principle of stacking layers in deep learning to improve accuracy applies
    to recurrent networks that can be stacked in the depth direction without burden.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中堆叠层的原则，通过在深度方向堆叠递归网络而不产生负担，能够提高准确性。
- en: Applying the same principle in the transition of the recurrent nets increases
    the vanishing/exploding issue, but is offset by the invention of the highway networks
    with identity connections.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 将相同的原则应用于递归网络的转换中，会增加消失/爆炸问题，但通过引入具有身份连接的高速公路网络来抵消这一问题。
- en: Advanced techniques for recurrent neural nets give state-of-the-art results
    in sequence prediction.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络的高级技术在序列预测中给出了最先进的结果。
