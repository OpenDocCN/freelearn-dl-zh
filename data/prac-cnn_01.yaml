- en: Deep Neural Networks – Overview
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络 - 概述
- en: In the past few years, we have seen remarkable progress in the field of AI (deep
    learning). Today, deep learning is the cornerstone of many advanced technological applications,
    from self-driving cars to generating art and music. Scientists aim to help computers
    to not only understand speech but also speak in natural languages. Deep learning
    is a kind of machine learning method that is based on learning data representation
    as opposed to task-specific algorithms. Deep learning enables the computer to
    build complex concepts from simpler and smaller concepts. For example, a deep
    learning system recognizes the image of a person by combining lower label edges
    and corners and combines them into parts of the body in a hierarchical way. The
    day is not so far away when deep learning will be extended to applications that
    enable machines to think on their own.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年中，我们在人工智能（深度学习）领域取得了显著进展。今天，深度学习是许多先进技术应用的基石，从自动驾驶汽车到艺术和音乐创作。科学家们旨在帮助计算机不仅能理解语音，还能用自然语言进行对话。深度学习是一种基于数据表示学习的方法，而不是任务特定的算法。深度学习使计算机能够从更简单、更小的概念构建复杂的概念。例如，深度学习系统通过将较低层次的边缘和角点组合起来，并以分层方式将它们组合成身体部位，从而识别一个人的图像。那一天已经不远了，深度学习将扩展到能够让机器独立思考的应用中。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Building blocks of a neural network
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的构建模块
- en: Introduction to TensorFlow
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow简介
- en: Introduction to Keras
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras简介
- en: Backpropagation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播
- en: Building blocks of a neural network
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的构建模块
- en: A neural network is made up of many artificial neurons. Is it a representation
    of the brain or is it a mathematical representation of some knowledge? Here, we
    will simply try to understand how a neural network is used in practice. A **convolutional
    neural network** (**CNN**) is a very special kind of multi-layer neural network.
    CNN is designed to recognize visual patterns directly from images with minimal
    processing. A graphical representation of this network is produced in the following
    image. The field of neural networks was originally inspired by the goal of modeling
    biological neural systems, but since then it has branched in different directions
    and has become a matter of engineering and attaining good results in machine learning
    tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络由许多人工神经元组成。它是大脑的表现形式，还是某些知识的数学表示呢？在这里，我们将简单地尝试理解神经网络在实践中的应用。**卷积神经网络**（**CNN**）是一种非常特殊的多层神经网络。CNN旨在直接从图像中识别视觉模式，且处理过程最小。该网络的图示如下所示。神经网络领域最初受到建模生物神经系统目标的启发，但此后它已朝着不同方向发展，并成为机器学习任务中工程学和获得良好结果的关键。
- en: 'An artificial neuron is a function that takes an input and produces an output.
    The number of neurons that are used depends on the task at hand. It could be as
    low as two or as many as several thousands. There are numerous ways of connecting
    artificial neurons together to create a CNN. One such topology that is commonly
    used is known as a **feed-forward network**:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经元是一个接受输入并产生输出的函数。所使用的神经元数量取决于当前任务的需求。它可能少至两个神经元，也可能多达几千个神经元。连接人工神经元以创建卷积神经网络（CNN）的方式有很多种。常用的拓扑结构之一是**前馈网络**：
- en: '![](img/76320048-d832-4c03-b7ad-bdbdb1b4bbd8.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76320048-d832-4c03-b7ad-bdbdb1b4bbd8.png)'
- en: 'Each neuron receives inputs from other neurons. The effect of each input line
    on the neuron is controlled by the weight. The weight can be positive or negative.
    The entire neural network learns to perform useful computations for recognizing
    objects by understanding the language. Now, we can connect those neurons into
    a network known as a feed-forward network. This means that the neurons in each
    layer feed their output forward to the next layer until we get a final output.
    This can be written as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元从其他神经元接收输入。每条输入线对神经元的影响由权重控制。权重可以是正数也可以是负数。整个神经网络通过理解语言来学习执行有用的计算以识别物体。现在，我们可以将这些神经元连接成一个被称为前馈网络的网络。这意味着每一层的神经元将它们的输出传递到下一层，直到得到最终输出。可以写成如下形式：
- en: '![](img/c628f996-d705-4a2b-ba50-c98351f1d30c.png)![](img/3adae030-df62-40b2-ac2a-e99b0f921708.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c628f996-d705-4a2b-ba50-c98351f1d30c.png)![](img/3adae030-df62-40b2-ac2a-e99b0f921708.png)'
- en: 'The preceding forward-propagating neuron can be implemented as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 上述前向传播的神经元可以通过以下方式实现：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Introduction to TensorFlow
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow简介
- en: 'TensorFlow is based on graph-based computation. Consider the following math
    expression, for example:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 基于图计算。以以下数学表达式为例：
- en: '*c=(a+b)*, *d = b + 5*,'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*c=(a+b)*, *d = b + 5*,'
- en: '*e = c * d *'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*e = c * d *'
- en: 'In TensorFlow, this is represented as a computational graph, as shown here.
    This is powerful because computations are done in parallel:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，这表示为计算图，如下所示。之所以强大，是因为计算可以并行执行：
- en: '![](img/2d33fa46-e26b-4c5c-b29a-b3318b25b39f.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d33fa46-e26b-4c5c-b29a-b3318b25b39f.png)'
- en: Installing TensorFlow
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 TensorFlow
- en: 'There are two easy ways to install TensorFlow:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种简单的方法可以安装 TensorFlow：
- en: Using a virtual environment (recommended and described here)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用虚拟环境（推荐并在此处描述）
- en: With a Docker image
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Docker 镜像
- en: For macOS X/Linux variants
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对于 macOS X/Linux 变体
- en: 'The following code snippet creates a Python virtual environment and installs
    TensorFlow in that environment. You should have Anaconda installed before you
    run this code:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段创建一个 Python 虚拟环境，并在该环境中安装 TensorFlow。运行此代码之前，您应先安装 Anaconda：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Please check out the latest updates on the official TensorFlow page, [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看官方 TensorFlow 页面上的最新更新，[https://www.tensorflow.org/install/](https://www.tensorflow.org/install/)。
- en: 'Try running the following code in your Python console to validate your installation.
    The console should print `Hello World!` if TensorFlow is installed and working:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试在 Python 控制台中运行以下代码以验证您的安装。如果 TensorFlow 已安装并正常工作，控制台应打印 `Hello World!`：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: TensorFlow basics
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 基础
- en: 'In TensorFlow, data isn''t stored as integers, floats, strings, or other primitives.
    These values are encapsulated in an object called a **tensor**. It consists of
    a set of primitive values shaped into an array of any number of dimensions. The
    number of dimensions in a tensor is called its **rank**.In the preceding example,
    `hello_constant` is a constant string tensor with rank zero. A few more examples
    of constant tensors are as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，数据不是以整数、浮点数、字符串或其他基本类型存储的。这些值被封装在一个名为 **张量** 的对象中。它由一组基本值构成，这些值被塑形为任意维度的数组。张量的维度数量称为其
    **秩**。在前面的示例中，`hello_constant` 是一个秩为零的常量字符串张量。以下是一些常量张量的示例：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'TensorFlow''s core program is based on the idea of a computational graph. A
    computational graph is a directed graph consisting of the following two parts:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的核心程序基于计算图的概念。计算图是由以下两部分组成的有向图：
- en: Building a computational graph
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建计算图
- en: Running a computational graph
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行计算图
- en: 'A computational graph executes within a **session**. A TensorFlow session is
    a runtime environment for the computational graph. It allocates the CPU or GPU
    and maintains the state of the TensorFlow runtime. The following code creates
    a session instance named `sess` using `tf.Session`. Then the `sess.run()` function
    evaluates the tensor and returns the results stored in the `output` variable.
    It finally prints as `Hello World!`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图在 **会话** 中执行。TensorFlow 会话是计算图的运行时环境。它分配 CPU 或 GPU，并保持 TensorFlow 运行时的状态。以下代码使用
    `tf.Session` 创建一个名为 `sess` 的会话实例。然后，`sess.run()` 函数计算张量并返回存储在 `output` 变量中的结果。最后输出
    `Hello World!`：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Using TensorBoard, we can visualize the graph. To run TensorBoard, use the
    following command:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorBoard，我们可以可视化计算图。要运行 TensorBoard，请使用以下命令：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s create a piece of simple addition code as follows. Create a constant
    integer `x` with value `5`, set the value of a new variable `y` after adding `5`
    to it, and print it:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一段简单的加法代码，如下所示。创建一个常量整数 `x`，其值为 `5`，然后将 `5` 加到一个新变量 `y` 中并打印结果：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The difference is that `variable_y` isn''t given the current value of `x +
    5` as it should in Python code. Instead, it is an equation; that means, when `variable_y`
    is computed, take the value of `x` at that point in time and add `5` to it. The
    computation of the value of `variable_y` is never actually performed in the preceding
    code. This piece of code actually belongs to the computational graph building
    section of a typical TensorFlow program. After running this, you''ll get something
    like `<tensorflow.python.ops.variables.Variable object at 0x7f074bfd9ef0>` and
    not the actual value of `variable_y` as `10`. To fix this, we have to execute
    the code section of the computational graph, which looks like this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 区别在于，`variable_y`并没有像Python代码中那样给出`x + 5`的当前值。它实际上是一个方程式；也就是说，当计算`variable_y`时，会取`x`在那个时间点的值，并加上`5`。在前面的代码中，`variable_y`的值并没有实际计算出来。这段代码实际上属于典型TensorFlow程序中的计算图构建部分。运行这段代码后，你会得到类似`<tensorflow.python.ops.variables.Variable
    object at 0x7f074bfd9ef0>`的内容，而不是`variable_y`实际的值`10`。为了解决这个问题，我们必须执行计算图的代码部分，代码如下：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is the execution of some basic math functions, such as addition, subtraction,
    multiplication, and division with tensors. For more math functions, please refer
    to the documentation:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些基本数学函数的执行示例，如加法、减法、乘法和除法。更多数学函数，请参考文档：
- en: For TensorFlow math functions, go to [https://www.tensorflow.org/versions/r0.12/api_docs/python/math_ops/basic_math_functions](https://www.tensorflow.org/versions/r0.12/api_docs/python/math_ops/basic_math_functions).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 访问TensorFlow数学函数，请查看[https://www.tensorflow.org/versions/r0.12/api_docs/python/math_ops/basic_math_functions](https://www.tensorflow.org/versions/r0.12/api_docs/python/math_ops/basic_math_functions)。
- en: Basic math with TensorFlow
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow进行基本的数学运算
- en: 'The `tf.add()` function takes two numbers, two tensors, or one of each, and
    it returns their sum as a tensor:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.add()`函数接受两个数字、两个张量或每种一个，并返回它们的和作为一个张量：'
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here''s an example with subtraction and multiplication:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个包含减法和乘法的示例：
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: What if we want to use a non-constant? How to feed an input dataset to TensorFlow?
    For this, TensorFlow provides an API, `tf.placeholder()`, and uses `feed_dict`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想使用非常量值该怎么办？如何将输入数据集传递给TensorFlow？为此，TensorFlow提供了一个API，`tf.placeholder()`，并使用`feed_dict`。
- en: 'A `placeholder` is a variable that data is assigned to later in the `tf.session.run()`
    function. With the help of this, our operations can be created and we can build
    our computational graph without needing the data. Afterwards, this data is fed
    into the graph through these placeholders with the help of the `feed_dict` parameter
    in `tf.session.run()` to set the `placeholder` tensor. In the following example,
    the tensor `x` is set to the string `Hello World` before the session runs:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`placeholder`是一个变量，数据稍后会在`tf.session.run()`函数中分配给它。通过这个方法，我们可以在不需要数据的情况下创建操作，并构建计算图。之后，这些数据通过`feed_dict`参数被传递到计算图中，从而设置`placeholder`张量。在以下示例中，在会话运行之前，张量`x`被设置为字符串`Hello
    World`：'
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'It''s also possible to set more than one tensor using `feed_dict`, as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用`feed_dict`设置多个张量，如下所示：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Placeholders can also allow storage of arrays with the help of multiple dimensions.
    Please see the following example:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 占位符还可以在多维的帮助下存储数组。请参见以下示例：
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will throw an error as `ValueError: invalid literal for...` in cases where
    the data passed to the `feed_dict` parameter doesn''t match the tensor type and
    can''t be cast into the tensor type.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '当传递给`feed_dict`参数的数据类型与张量类型不匹配且无法转换时，将抛出一个错误，例如`ValueError: invalid literal
    for...`。'
- en: 'The `tf.truncated_normal()` function returns a tensor with random values from
    a normal distribution. This is mostly used for weight initialization in a network:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.truncated_normal()`函数返回一个从正态分布中生成的随机值的张量。它主要用于网络中的权重初始化：'
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Softmax in TensorFlow
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow中的softmax
- en: 'The softmax function converts its inputs, known as **logit** or **logit scores**,
    to be between 0 and 1, and also normalizes the outputs so that they all sum up
    to 1\. In other words, the softmax function turns your logits into probabilities. Mathematically, the softmax
    function is defined as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: softmax函数将其输入值（称为**logit**或**logit分数**）转换为0到1之间的值，并将输出值标准化，使其总和为1。换句话说，softmax函数将logits转换为概率。数学上，softmax函数的定义如下：
- en: '![](img/f397aff3-9485-43c6-ae4f-7a33d9374b1e.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f397aff3-9485-43c6-ae4f-7a33d9374b1e.png)'
- en: 'In TensorFlow, the softmax function is implemented. It takes logits and returns
    softmax activations that have the same type and shape as input logits, as shown
    in the following image:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，softmax 函数已经实现。它接受 logits 并返回与输入 logits 类型和形状相同的 softmax 激活，如下图所示：
- en: '![](img/53e7b660-22b9-4eec-b435-623d7458a621.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/53e7b660-22b9-4eec-b435-623d7458a621.png)'
- en: 'The following code is used to implement this:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于实现这一点：
- en: '[PRE14]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The way we represent labels mathematically is often called **one-hot encoding**.
    Each label is represented by a vector that has 1.0 for the correct label and 0.0
    for everything else. This works well for most problem cases. However, when the
    problem has millions of labels, one-hot encoding is not efficient, since most
    of the vector elements are zeros. We measure the similarity distance between two
    probability vectors, known as **cross-entropy** and denoted by **D**.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在数学上表示标签的方式通常称为 **独热编码**。每个标签由一个向量表示，正确标签的值为 1.0，其它标签的值为 0.0。这种方式在大多数问题中都非常有效。然而，当问题中有数百万个标签时，独热编码就不太高效，因为大部分向量元素都是零。我们通过测量两个概率向量之间的相似度来衡量其距离，这个度量被称为
    **交叉熵**，用 **D** 表示。
- en: Cross-entropy is not symmetric. That means: *D(S,L) != D(L,S)*
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵是非对称的。这意味着：*D(S,L) != D(L,S)*
- en: 'In machine learning, we define what it means for a model to be bad usually
    by a mathematical function. This function is called **loss**, **cost**, or **objective**
    function. One very common function used to determine the loss of a model is called
    the **cross-entropy loss**. This concept came from information theory (for more
    on this, please refer to Visual Information Theory at [https://colah.github.io/posts/2015-09-Visual-Information/](https://colah.github.io/posts/2015-09-Visual-Information/)).
    Intuitively, the loss will be high if the model does a poor job of classifying
    on the training data, and it will be low otherwise, as shown here:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们通常通过一个数学函数来定义模型的“坏”与“好”。这个函数叫做 **损失**、**代价** 或 **目标** 函数。用来衡量模型损失的一个非常常见的函数是
    **交叉熵损失**。这一概念来源于信息论（关于这一点的更多内容，请参考[https://colah.github.io/posts/2015-09-Visual-Information/](https://colah.github.io/posts/2015-09-Visual-Information/)）。直观地，如果模型在训练数据上的分类效果差，损失就会很高；反之，如果分类效果好，损失会很低，如下所示：
- en: '![](img/e1236125-87f5-4a42-8af8-84cdcd6920d1.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e1236125-87f5-4a42-8af8-84cdcd6920d1.png)'
- en: Cross-entropy loss function
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失函数
- en: 'In TensorFlow, we can write a cross-entropy function using `tf.reduce_sum()`;
    it takes an array of numbers and returns its sum as a tensor (see the following
    code block):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，我们可以使用 `tf.reduce_sum()` 来编写交叉熵函数；它接受一个数字数组并返回其和作为一个张量（见下方代码块）：
- en: '[PRE15]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'But in practice, while computing the softmax function, intermediate terms may
    be very large due to the exponentials. So, dividing large numbers can be numerically
    unstable. We should use TensorFlow''s provided softmax and cross-entropy loss
    API. The following code snippet manually calculates cross-entropy loss and also
    prints the same using the TensorFlow API:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 但在实际操作中，在计算 softmax 函数时，由于指数的存在，某些中间项可能会非常大。因此，除法操作可能在数值上不稳定。我们应该使用 TensorFlow
    提供的 softmax 和交叉熵损失 API。以下代码片段手动计算交叉熵损失，并使用 TensorFlow API 打印出相同的结果：
- en: '[PRE16]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Introduction to the MNIST dataset
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST 数据集简介
- en: Here we use **MNIST** (**Modified National Institute of Standards and Technology**),
    which consists of images of handwritten numbers and their labels. Since its release
    in 1999, this classic dataset is used for benchmarking classification algorithms.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们使用 **MNIST**（**修改后的国家标准与技术研究院数据集**），它包含手写数字的图像及其标签。自1999年发布以来，这个经典数据集被用来作为分类算法的基准。
- en: The data files `train.csv` and `test.csv` consist of hand-drawn digits, from
    0 through 9 in the form of gray-scale images. A digital image is a mathematical
    function of the form *f(x,y)=pixel* value. The images are two dimensional.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 数据文件 `train.csv` 和 `test.csv` 包含从 0 到 9 的手绘数字，以灰度图像的形式展示。数字图像是一个数学函数，形式为 *f(x,y)=像素*
    值。图像是二维的。
- en: We can perform any mathematical function on the image. By computing the gradient
    on the image, we can measure how fast pixel values are changing and the direction
    in which they are changing. For image recognition, we convert the image into grayscale
    for simplicity and have one color channel. **RGB** representation of an image
    consists of three color channels, **RED**, **BLUE**, and **GREEN**. In the RGB
    color scheme, an image is a stack of three images RED, BLUE, and GREEN. In a grayscale
    color scheme, color is not important. Color images are computationally harder
    to analyze because they take more space in memory. Intensity, which is a measure
    of the lightness and darkness of an image, is very useful for recognizing objects.
    In some applications, for example, detecting lane lines in a self-driving car
    application, color is important because it has to distinguish yellow lanes and
    white lanes. A grayscale image does not provide enough information to distinguish
    between white and yellow lane lines.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对图像执行任何数学函数。通过计算图像的梯度，我们可以测量像素值变化的速度和变化的方向。为了简化图像识别，我们将图像转换为灰度图像，并且只有一个颜色通道。**RGB**图像由三个颜色通道组成，**红色**、**蓝色**和**绿色**。在RGB颜色模式中，一幅图像是由三张图像（红色、蓝色和绿色）叠加而成的。在灰度颜色模式中，颜色不重要。彩色图像在计算上更难分析，因为它们在内存中占用更多空间。强度是图像明暗的度量，对于识别物体非常有用。在某些应用中，例如自动驾驶汽车应用中的车道线检测，颜色非常重要，因为需要区分黄色车道和白色车道。灰度图像无法提供足够的信息来区分白色和黄色车道线。
- en: Any grayscale image is interpreted by the computer as a matrix with one entry
    for each image pixel. Each image is 28 x 28 pixels in height and width, to give
    a sum of 784 pixels. Each pixel has a single pixel-value associated with it. This
    value indicates the lightness or darkness of that particular pixel. This pixel-value
    is an integer ranging from 0 to 255, where a value of zero means darkest and 255
    is the whitest, and a gray pixel is between 0 and 255.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 任何灰度图像都会被计算机解读为一个矩阵，每个图像像素对应一个条目。每幅图像的高度和宽度为28 x 28像素，总共有784个像素。每个像素都有一个单独的像素值与之相关联。这个像素值表示该像素的明暗程度。该像素值是一个从0到255的整数，其中零表示最暗，255表示最亮，灰色像素的值在0和255之间。
- en: The simplest artificial neural network
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最简单的人工神经网络
- en: 'The following image represents a simple two-layer neural network:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像表示一个简单的两层神经网络：
- en: '![](img/246151fb-7893-448d-b9bb-7a87b387a24b.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/246151fb-7893-448d-b9bb-7a87b387a24b.png)'
- en: Simple two-layer neural net
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的两层神经网络
- en: The first layer is the **input layer** and the last layer is the **output layer**.
    The middle layer is the **hidden layer**. If there is more than one hidden layer,
    then such a network is a deep neural network.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层是**输入层**，最后一层是**输出层**。中间层是**隐藏层**。如果有多个隐藏层，那么这样的网络就是深度神经网络。
- en: 'The input and output of each neuron in the hidden layer is connected to each
    neuron in the next layer. There can be any number of neurons in each layer depending
    on the problem. Let us consider an example. The simple example which you may already
    know is the popular hand written digit recognition that detects a number, say
    5\. This network will accept an image of 5 and will output 1 or 0\. A 1 is to
    indicate the image in fact is a 5 and 0 otherwise. Once the network is created,
    it has to be trained. We can initialize with random weights and then feed input
    samples known as the **training dataset**. For each input sample, we check the
    output, compute the error rate and then adjust the weights so that whenever it
    sees 5 it outputs 1 and for everything else it outputs a zero. This type of training
    is called **supervised learning** and the method of adjusting the weights is called
    **backpropagation**. When constructing artificial neural network models, one of
    the primary considerations is how to choose activation functions for hidden and
    output layers. The three most commonly used activation functions are the sigmoid
    function, hyperbolic tangent function, and **Rectified Linear Unit** (**ReLU**).
    The beauty of the sigmoid function is that its derivative is evaluated at* z* and
    is simply *z* multiplied by 1-minus *z*. That means:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 每个隐藏层中神经元的输入和输出与下一层中的每个神经元相连。每层的神经元数量可以根据问题的不同而有所不同。我们来看一个例子。一个简单的例子是你可能已经知道的流行手写数字识别，它用于检测一个数字，比如5。该网络会接受一个数字5的图像，并输出1或0。1表示该图像确实是一个5，0则表示不是。网络一旦创建，就需要进行训练。我们可以先用随机权重初始化，然后输入已知的样本数据集（**training
    dataset**）。对于每个输入样本，我们检查输出，计算误差率，然后调整权重，使得每当它看到5时输出1，而对其他任何数字输出0。这种训练方法叫做**监督学习**，而调整权重的方法则叫做**反向传播**。在构建人工神经网络模型时，主要的考虑因素之一是如何选择隐藏层和输出层的激活函数。三种最常用的激活函数是sigmoid函数、双曲正切函数和**修正线性单元**（**ReLU**）。sigmoid函数的优点在于它的导数可以在*z*上计算，其计算公式为*z*乘以1减去*z*。这意味着：
- en: '* dy/dx =σ(x)(1−σ(x))*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '* dy/dx =σ(x)(1−σ(x)) *'
- en: 'This helps us to efficiently calculate gradients used in neural networks in
    a convenient manner. If the feed-forward activations of the logistic function
    for a given layer is kept in memory, the gradients for that particular layer can
    be evaluated with the help of simple multiplication and subtraction rather than
    implementing and re-evaluating the sigmoid function, since it requires extra exponentiation. The
    following image shows us the ReLU activation function, which is zero when *x <
    0* and then linear with slope 1 when *x > 0*:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于我们以一种便捷的方式有效地计算神经网络中使用的梯度。如果某一层的逻辑函数的前馈激活值保存在内存中，那么该层的梯度可以通过简单的乘法和减法进行计算，而无需实现和重新计算sigmoid函数，因为这需要额外的指数运算。下图展示了ReLU激活函数，当*x
    < 0*时其值为零，而当*x > 0*时其值呈线性关系，斜率为1：
- en: '![](img/0b97ce94-3500-421b-ab0e-ac41ef330b0f.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b97ce94-3500-421b-ab0e-ac41ef330b0f.jpeg)'
- en: 'The ReLU is a nonlinear function that computes the function *f(x)=max(0, x)*.
    That means a ReLU function is 0 for negative inputs and *x* for all inputs *x
    >0*. This means that the activation is thresholded at zero (see the preceding
    image on the left). TensorFlow implements the ReLU function in `tf.nn.relu()`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU是一个非线性函数，其计算公式为*f(x)=max(0, x)*。这意味着对于负输入，ReLU的值为0，而对于所有*x >0*的输入，其值为*x*。这意味着激活函数的阈值设定在零（见前面左侧的图像）。TensorFlow通过`tf.nn.relu()`实现ReLU函数：
- en: '![](img/1eede7d4-c92f-446e-81a1-37361897b254.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1eede7d4-c92f-446e-81a1-37361897b254.png)'
- en: Backpropagation, an abbreviation for "backward propagation of errors", is a
    common method of training artificial neural networks used in conjunction with
    an optimization method such as gradient descent. The method calculates the gradient
    of a loss function with respect to all the weights in the network. The optimization
    method is fed with the gradient and uses it to get the weights updated to reduce
    the loss function.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播（"backward propagation of errors"的缩写）是训练人工神经网络的一种常用方法，通常与优化方法如梯度下降法结合使用。该方法计算损失函数相对于网络中所有权重的梯度。优化方法会获取梯度并利用它更新权重，以减少损失函数。
- en: Building a single-layer neural network with TensorFlow
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow构建单层神经网络
- en: 'Let us build a single-layer neural net with TensorFlow step by step. In this
    example, we''ll be using the MNIST dataset. This dataset is a set of 28 x 28 pixel
    grayscale images of hand written digits. This dataset consists of 55,000 training
    data, 10,000 test data, and 5,000 validation data. Every MNIST data point has
    two parts: an image of a handwritten digit and a corresponding label. The following
    code block loads data. `one_hot=True` means that the labels are one-hot encoded
    vectors instead of actual digits of the label. For example, if the label is `2`,
    you will see [0,0,1,0,0,0,0,0,0,0]. This allows us to directly use it in the output
    layer of the network:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地用 TensorFlow 构建一个单层神经网络。在这个例子中，我们将使用 MNIST 数据集。这个数据集包含28 x 28 像素的手写数字灰度图像。数据集包括
    55,000 条训练数据，10,000 条测试数据和 5,000 条验证数据。每个 MNIST 数据点包含两部分：一个手写数字的图像和一个对应的标签。以下代码块加载数据。`one_hot=True`
    表示标签是 one-hot 编码的向量，而不是标签的实际数字。例如，如果标签是 `2`，你将看到 [0,0,1,0,0,0,0,0,0,0]。这使得我们可以直接在网络的输出层中使用它：
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Setting up placeholders and variables is done as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 设置占位符和变量的方法如下：
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s set up the optimizer in TensorFlow:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 TensorFlow 中设置优化器：
- en: '[PRE19]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Before we begin training, let''s set up the variable initialization operation
    and an operation to measure the accuracy of our predictions, as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练之前，让我们设置变量初始化操作和一个用于衡量预测准确度的操作，如下所示：
- en: '[PRE20]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we can begin training the model, as shown in the following code snippet:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始训练模型，代码如下所示：
- en: '[PRE21]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Keras deep learning library overview
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras 深度学习库概述
- en: Keras is a high-level deep neural networks API in Python that runs on top of
    TensorFlow, CNTK, or Theano.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 是一个 Python 中的高级深度神经网络 API，运行在 TensorFlow、CNTK 或 Theano 之上。
- en: Here are some core concepts you need to know for working with Keras. TensorFlow
    is a deep learning library for numerical computation and machine intelligence.
    It is open source and uses data flow graphs for numerical computation. Mathematical
    operations are represented by nodes and multidimensional data arrays; that is,
    tensors are represented by graph edges. This framework is extremely technical
    and hence it is probably difficult for data analysts. Keras makes deep neural
    network coding simple. It also runs seamlessly on CPU and GPU machines.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些你需要了解的 Keras 核心概念。TensorFlow 是一个用于数值计算和机器智能的深度学习库。它是开源的，并使用数据流图进行数值计算。数学操作由节点表示，多维数据数组（即张量）由图的边表示。这个框架极其技术性，因此对于数据分析师来说可能比较困难。Keras
    使得深度神经网络编码变得简单，同时在 CPU 和 GPU 机器上运行也非常顺畅。
- en: A **model** is thecore data structure of Keras. The sequential model, which
    consists of a linear stack of layers, is the simplest type of model. It provides
    common functions, such as `fit()`, `evaluate()`, and `compile()`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型** 是 Keras 的核心数据结构。顺序模型是最简单的模型类型，它由一系列按顺序堆叠的层组成。它提供了常见的功能，如 `fit()`、`evaluate()`
    和 `compile()`。'
- en: 'You can create a sequential model with the help of the following lines of code:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下代码行创建一个顺序模型：
- en: '[PRE22]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Layers in the Keras model
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras 模型中的层
- en: 'A Keras layer is just like a neural network layer. There are fully connected
    layers, max pool layers, and activation layers. A layer can be added to the model
    using the model''s `add()` function. For example, a simple model can be represented
    by the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 层就像神经网络层一样。有全连接层、最大池化层和激活层。可以使用模型的 `add()` 函数向模型中添加层。例如，一个简单的模型可以表示为以下内容：
- en: '[PRE23]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Keras will automatically infer the shape of all layers after the first layer.
    This means you only have to set the input dimensions for the first layer. The
    first layer from the preceding code snippet, `model.add(Flatten(input_shape=(32,
    32, 3)))`, sets the input dimension to (32, 32, 3) and the output dimension to
    (3072=32 x 32 x 3). The second layer takes in the output of the first layer and
    sets the output dimensions to (100). This chain of passing the output to the next
    layer continues until the last layer, which is the output of the model.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 将自动推断第一层之后所有层的形状。这意味着你只需要为第一层设置输入维度。前面代码片段中的第一层 `model.add(Flatten(input_shape=(32,
    32, 3)))` 将输入维度设置为 (32, 32, 3)，输出维度设置为 (3072=32 x 32 x 3)。第二层接收第一层的输出，并将输出维度设置为
    (100)。这种将输出传递给下一层的链式过程一直持续到最后一层，即模型的输出。
- en: Handwritten number recognition with Keras and MNIST
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 和 MNIST 进行手写数字识别
- en: 'A typical neural network for a digit recognizer may have 784 input pixels connected
    to 1,000 neurons in the hidden layer, which in turn connects to 10 output targets
    — one for each digit. Each layer is fully connected to the layer above. A graphical
    representation of this network is shown as follows, where `x` are the inputs,
    `h` are the hidden neurons, and `y` are the output class variables:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的数字识别神经网络可能有784个输入像素，连接到1000个隐藏层神经元，隐藏层再连接到10个输出目标——每个数字对应一个输出。每一层都与上一层完全连接。下面是这个网络的图形表示，其中`x`是输入，`h`是隐藏层神经元，`y`是输出类别变量：
- en: '![](img/b24d6265-5067-4fd6-abfc-ccbacfa121a8.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b24d6265-5067-4fd6-abfc-ccbacfa121a8.png)'
- en: In this notebook, we will build a neural network that will recognize handwritten
    numbers from 0-9.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个笔记本中，我们将构建一个神经网络来识别从0到9的手写数字。
- en: The type of neural network that we are building is used in a number of real-world
    applications, such as recognizing phone numbers and sorting postal mail by address.
    To build this network, we will use the **MNIST** dataset.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在构建的神经网络类型在许多现实世界应用中得到了应用，例如识别电话号码和按地址排序邮政邮件。为了构建这个网络，我们将使用**MNIST**数据集。
- en: 'We will begin as shown in the following code by importing all the required
    modules, after which the data will be loaded, and then finally building the network:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照以下代码开始，导入所有必需的模块，然后加载数据，最后构建网络：
- en: '[PRE24]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Retrieving training and test data
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取训练数据和测试数据
- en: The MNIST dataset already comprises both training and test data. There are 60,000
    data points of training data and 10,000 points of test data. If you do not have
    the data file locally at the `'~/.keras/datasets/' +` path, it can be downloaded
    at this location.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集已经包含了训练数据和测试数据。训练数据有60,000个数据点，测试数据有10,000个数据点。如果您在`'~/.keras/datasets/'
    +`路径下没有数据文件，可以通过这个位置下载。
- en: 'Each MNIST data point has:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 每个MNIST数据点都有：
- en: An image of a handwritten digit
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一张手写数字的图像
- en: A corresponding label that is a number from 0-9 to help identify the image
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个对应的标签，是一个从0到9的数字，用于帮助识别图像
- en: The images will be called, and will be the input to our neural network, **X**;
    their corresponding labels are **y**.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图像将被调用，并作为我们神经网络的输入，**X**；其对应的标签是**y**。
- en: We want our labels as one-hot vectors. One-hot vectors are vectors of many zeros
    and one. It's easiest to see this in an example. The number 0 is represented as
    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], and 4 is represented as [0, 0, 0, 0, 1, 0, 0,
    0, 0, 0] as a one-hot vector.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望标签是**独热向量**。独热向量是由许多零和一个组成的向量。通过一个例子来看最为直观。数字0表示为[1, 0, 0, 0, 0, 0, 0, 0,
    0, 0]，数字4表示为[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]，这是一个独热向量。
- en: Flattened data
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扁平化数据
- en: We will use flattened data in this example, or a representation of MNIST images
    in one dimension rather than two can also be used. Thus, each 28 x 28 pixels number
    image will be represented as a 784 pixel 1 dimensional array.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用扁平化数据，或者也可以使用MNIST图像的一维表示，而非二维。这样，每个28 x 28像素的数字图像将被表示为一个784像素的一维数组。
- en: 'By flattening the data, information about the 2D structure of the image is
    thrown; however, our data is simplified. With the help of this, all our training
    data can be contained in one array of shape (60,000, 784), wherein the first dimension
    represents the number of training images and the second depicts the number of
    pixels in each image. This kind of data is easy to analyze using a simple neural
    network, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 通过扁平化数据，图像的二维结构信息会丢失；然而，我们的数据得到了简化。借助这一点，所有的训练数据都可以包含在一个形状为（60,000，784）的数组中，其中第一维表示训练图像的数量，第二维表示每张图像中的像素数。这样结构的数据可以用简单的神经网络进行分析，如下所示：
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Visualizing the training data
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化训练数据
- en: 'The following function will help you visualize the MNIST data. By passing in
    the index of a training example, the `show_digit` function will display that training
    image along with its corresponding label in the title:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数将帮助您可视化MNIST数据。通过传入训练样本的索引，`show_digit`函数将显示该训练图像及其对应的标签：
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Building the network
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建网络
- en: 'For this example, you''ll define the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，您将定义以下内容：
- en: The input layer, which you should expect for each piece of MNIST data, as it
    tells the network the number of inputs
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层，您应该预期每一条 MNIST 数据都包含此层，因为它告诉网络输入的数量
- en: Hidden layers, as they recognize patterns in data and also connect the input
    layer to the output layer
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层，它们识别数据中的模式，并将输入层与输出层连接起来
- en: 'The output layer, as it defines how the network learns and gives a label as
    the output for a given image, as follows:'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层，因为它定义了网络如何学习，并为给定图像提供标签输出，如下所示：
- en: '[PRE27]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Training the network
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练网络
- en: 'Now that we''ve constructed the network, we feed it with data and train it,
    as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了网络，我们将数据输入并进行训练，如下所示：
- en: '[PRE28]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Testing
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试
- en: After you're satisfied with the training output and accuracy, you can run the
    network on the **test dataset** to measure its performance!
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当你对训练输出和准确率感到满意时，你可以在**测试数据集**上运行网络，以衡量其性能！
- en: Keep in mind to perform this only after you've completed the training and are
    satisfied with the results.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，只有在完成训练并对结果感到满意后，再执行此操作。
- en: 'A good result will obtain an accuracy **higher than 95%**. Some simple models
    have been known to achieve even up to 99.7% accuracy! We can test the model, as
    shown here:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的结果会获得**高于95%的**准确率。已知一些简单的模型甚至可以达到99.7%的准确率！我们可以测试模型，如下所示：
- en: '[PRE29]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Understanding backpropagation
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解反向传播
- en: In this section, we will understand an intuition about backpropagation. This
    is a way of computing gradients using the chain rule. Understanding this process
    and its subtleties is critical for you to be able to understand and effectively
    develop, design, and debug neural networks.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将理解反向传播的直觉。这是一种使用链式法则计算梯度的方法。理解这个过程及其细节对于你能够理解并有效地开发、设计和调试神经网络至关重要。
- en: 'In general, given a function *f(x)*, where *x* is a vector of inputs, we want
    to compute the gradient of *f* at *x* denoted by *∇(f(x))*. This is because in
    the case of neural networks, the function *f* is basically a loss function (*L*)
    and the input *x* is the combination of weights and training data. The symbol*∇* 
    is pronounced as **nabla**:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，给定一个函数*f(x)*，其中*x*是输入的向量，我们想要计算*f*在*x*处的梯度，记作*∇(f(x))*。这是因为在神经网络的情况下，函数*f*基本上是一个损失函数(*L*)，而输入*x*是权重和训练数据的组合。符号*∇*的发音为**nabla**：
- en: '*(xi, yi ) i = 1......N*'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*(xi, yi ) i = 1......N*'
- en: Why do we take the gradient on weight parameters?
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们要对权重参数计算梯度？
- en: 'It is given that the training data is usually fixed and the parameters are
    variables that we have control over. We usually compute the gradient of the parameters
    so that we can use it for parameter updates. The gradient *∇f* is the vector of
    partial derivatives, that is:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 给定训练数据通常是固定的，而参数是我们可以控制的变量。我们通常计算参数的梯度，以便可以使用它来更新参数。梯度*∇f*是部分导数的向量，也就是说：
- en: '*∇f = [ df/dx, df/dy] = [y,x]*'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*∇f = [ df/dx, df/dy] = [y,x]*'
- en: 'In a nutshell, backpropagation will consist of:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，反向传播将包括：
- en: Doing a feed-forward operation
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行前向传播操作
- en: Comparing the output of the model with the desired output
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型的输出与期望的输出进行比较
- en: Calculating the error
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算误差
- en: Running the feedforward operation backwards (backpropagation) to spread the
    error to each of the weights
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行反向传播操作（反向传播），将误差传播到每个权重上
- en: Using this to update the weights, and get a better model
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这个来更新权重，得到更好的模型
- en: Continuing this until we have a model that is good
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续进行，直到我们得到一个良好的模型
- en: We will be building a neural network that recognizes digits from 0 to 9\. This
    kind of network application is used for sorting postal mail by zip code, recognizing
    phone numbers and house numbers from images, extracting package quantities from
    image of the package and so on.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个识别从0到9的数字的神经网络。这类网络应用被用于按邮政编码排序邮件、从图像中识别电话号码和门牌号、从包裹图像中提取包裹数量等等。
- en: In most cases, backpropagation is implemented in a framework, such as TensorFlow.
    However, it is not always true that by simply adding an arbitrary number of hidden
    layers, backpropagation will magically work on the dataset. The fact is if the
    weight initialization is sloppy, these non linearity functions can saturate and
    stop learning. That means training loss will be flat and refuse to go down. This
    is known as the **vanishing gradient problem**.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，反向传播是在框架中实现的，例如TensorFlow。然而，仅仅通过添加任意数量的隐藏层，反向传播并不总是会神奇地作用于数据集。事实上，如果权重初始化不当，这些非线性函数可能会饱和并停止学习。也就是说，训练损失会变得平稳并拒绝下降。这就是**梯度消失问题**。
- en: 'If your weight matrix *W* is initialized too large, the output of the matrix
    multiply too could probably have a very large range, which in turn will make all
    the outputs in the vector* z* almost binary: either 1 or 0\. However, if this
    is the case, then, *z*(1-z)*, which is the local gradient of the sigmoid non-linearity,
    will become *zero *(vanish) in both cases,which willmake the gradient for both *x* and*W* also zero.
    The rest of the backward pass will also come out all zero from this point onward
    on account of the multiplication in the chain rule.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的权重矩阵*W*初始化得过大，那么矩阵乘法的输出可能也会有非常大的范围，这会导致向量*z*中的所有输出几乎都是二进制的：要么是1，要么是0。然而，如果是这种情况，那么*z*(1-z)*，即sigmoid非线性函数的局部梯度，将会在两种情况下都变为*零*（消失），这将导致*x*和*W*的梯度也为零。由于链式法则中的乘法，接下来的反向传播从这一点开始也会输出全零。
- en: 'Another nonlinear activation function is ReLU, which thresholds neurons at
    zero shown as follows. The forward and backward pass for a fully connected layer
    that uses ReLU would at the core include:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非线性激活函数是ReLU，它将神经元的值阈值化为零，如下所示。使用ReLU的全连接层的前向和反向传播核心包括：
- en: '[PRE30]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If you observe this for a while, you'll see that should a neuron get clamped
    to zero in the forward pass (that is, *z = 0*, it doesn't fire), then its weights
    will get a zero gradient. This can lead to what is called the **dead ReLU** problem.
    This means if a ReLU neuron is unfortunately initialized in such a way that it
    never fires, or if a neuron's weights ever get knocked off with a large update
    during training into this regime, in such cases this neuron will remain permanently
    dead. It is similar to permanent, irrecoverable brain damage. Sometimes, you can
    even forward the entire training set through a trained network and finally realize
    that a large fraction (about 40%) of your neurons were zero the entire time.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你观察一段时间，你会发现，如果一个神经元在前向传播中被限制为零（即*z = 0*，它不激活），那么它的权重将获得一个零梯度。这可能会导致所谓的**死ReLU**问题。这意味着，如果一个ReLU神经元初始化时恰好从未激活，或者在训练过程中其权重因大的更新而进入这种状态，那么这个神经元将永远处于“死”状态。就像永久性的、不可恢复的脑损伤一样。有时，你甚至可以将整个训练集通过一个已训练的网络，最终意识到有很大一部分（大约40%）的神经元一直是零。
- en: 'In calculus, the chain rule is used for computing the derivative of the composition of
    two or more functions. That is, if we have two functions as *f *and *g*, then
    the chain rule represents the derivative of their composition *f ∘ g.* The function
    that maps *x* to *f(g(x))*) in terms of the derivatives of *f* and *g* and the product
    of functions is expressed as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在微积分中，链式法则用于计算两个或多个函数的复合函数的导数。也就是说，如果我们有两个函数*f*和*g*，那么链式法则表示它们复合函数*f ∘ g*的导数。该函数将*x*映射到*f(g(x))*，其导数可以通过*f*和*g*的导数及函数的乘积表示如下：
- en: '![](img/1505eff0-acc2-414b-a387-b63617368eb9.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1505eff0-acc2-414b-a387-b63617368eb9.png)'
- en: 'There is a more explicit way to represent this in terms of the variable. Let *F = f ∘ g*,
    or equivalently, *F(x) = f(g(x))* for all *x*. Then one can also write:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种更明确的方式可以通过变量来表示这一点。设*F = f ∘ g*，或者等价地，*F(x) = f(g(x)) *对所有的*x*都成立。然后也可以写作：
- en: '*F''(x)=f''(g(x))g''(x).*'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '*F''(x) = f''(g(x))g''(x)。*'
- en: 'The chain rule can be written with the help of Leibniz''s notation in the following
    way. If a variable *z* is dependent on a variable* y*, which in turn is dependent
    on a variable *x* (such that *y* and *z* are dependent variables), then *z* depends
    on *x* as well via the intermediate *y*. The chain rule then states:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则可以借助莱布尼茨符号以以下方式写出。如果变量*z*依赖于变量*y*，而*y*又依赖于变量*x*（如此*y*和*z*为依赖变量），那么*z*也通过中介变量*y*依赖于*x*。链式法则则表示：
- en: '![](img/681874b8-f2ac-41f4-9fba-f70007ccd003.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/681874b8-f2ac-41f4-9fba-f70007ccd003.png)'
- en: '*z = 1/(1 + np.exp(-np.dot(W, x)))* # forward pass'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*z = 1/(1 + np.exp(-np.dot(W, x)))* # 前向传播'
- en: '*dx = np.dot(W.T, z*(1-z))* # backward pass: local gradient for *x*'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*dx = np.dot(W.T, z*(1-z))* # 反向传播：*x*的局部梯度'
- en: '*dW = np.outer(z*(1-z), x)* # backward pass: local gradient for *W*'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*dW = np.outer(z*(1-z), x)* # 反向传播：*W*的局部梯度'
- en: 'The forward pass on the left in the following figure calculates *z* as a function
    *f(x,y)* using the input variables *x* and *y*. The right side of the figures
    represents the backward pass. Receiving *dL/dz*, the gradient of the loss function
    with respect to *z*, the gradients of *x* and *y* on the loss function can be
    calculated by applying the chain rule, as shown in the following figure:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 下图左侧的前向传播通过输入变量 *x* 和 *y*，计算函数 *f(x,y)* 得到 *z*。图右侧表示反向传播。接收 *dL/dz*（即损失函数关于
    *z* 的梯度），通过链式法则计算出损失函数对 *x* 和 *y* 的梯度，如下图所示：
- en: '![](img/d1b049ab-5e05-49b1-9229-c01f1cb4e926.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1b049ab-5e05-49b1-9229-c01f1cb4e926.png)'
- en: Summary
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we laid the foundation of neural networks and walked through
    the simplest artificial neural network. We learned how to build a single layer
    neural network using TensorFlow.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们奠定了神经网络的基础，并通过最简单的人工神经网络进行了讲解。我们学习了如何使用 TensorFlow 构建单层神经网络。
- en: We studied the differences in the layers in the Keras model and demonstrated
    the famous handwritten number recognition with Keras and MNIST.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了 Keras 模型中各层的差异，并使用 Keras 和 MNIST 演示了著名的手写数字识别。
- en: Finally, we understood what backpropagation is and used the MNIST dataset to
    build our network and train and test our data.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们理解了什么是反向传播，并使用 MNIST 数据集构建我们的网络，进行数据的训练和测试。
- en: In the next chapter, we will introduce you to CNNs.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将介绍卷积神经网络（CNN）。
