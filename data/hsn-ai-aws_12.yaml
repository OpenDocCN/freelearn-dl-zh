- en: Discovering Topics in Text Collection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在文本集合中发现主题
- en: One of the most useful ways to understand text is through topics. The process
    of learning, recognizing, and extracting these topics is called topic modeling.
    Understanding broad topics in text has several applications. It can be used in
    the legal industry to surface themes from contracts. (Rather than manually reviewing
    mountains of contracts for certain provisions, through unsupervised learning,
    themes or topics can surface). Furthermore, it can be used in the retail industry
    to identify broad trends in social media conversations. These broad trends can
    then be used for product innovation—to introduce new merchandise into online and
    physical stores, to inform others of product assortment, and so on.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 理解文本的一个最有用的方法是通过主题。学习、识别和提取这些主题的过程叫做主题建模。理解文本中的广泛主题有许多应用。它可以在法律行业中用于从合同中提取主题。（与其人工审核大量合同中的某些条款，不如通过无监督学习自动提取主题或模式）。此外，它还可以在零售行业中用于识别社交媒体对话中的广泛趋势。这些广泛的趋势可以用于产品创新——如在线上和实体店推出新商品、告知其他人商品种类等。
- en: In this chapter, we are going to learn how to synthesize topics from long-form
    text (text that's longer than 140 characters). We will review the techniques of
    topic modeling and understand how the **Neural Topic Model** (**NTM**) works.
    We will then look at training and deploying NTM in SageMaker.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何从长文本中合成主题（即文本长度超过140个字符）。我们将回顾主题建模的技术，并理解**神经主题模型**（**NTM**）的工作原理。接着，我们将探讨在SageMaker中训练和部署NTM的方法。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Reviewing topic modeling techniques
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾主题建模技术
- en: Understanding how the Neural Topic Model works
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解神经主题模型的工作原理
- en: Training NTM in SageMaker
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在SageMaker中训练NTM
- en: Deploying NTM and running inference
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署NTM并运行推理
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To illustrate the concepts in this chapter, we will use the **Bag of Words**
    ([https://archive.ics.uci.edu/ml/datasets/bag+of+words](https://archive.ics.uci.edu/ml/datasets/bag+of+words))
    dataset from the **UCI Machine Learning Repository** ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)).
    The dataset contains information on Enron emails, such as email IDs, word IDs,
    and their count, which is the number of times a particular word appeared in a
    given email.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明本章的概念，我们将使用**词袋模型**（[https://archive.ics.uci.edu/ml/datasets/bag+of+words](https://archive.ics.uci.edu/ml/datasets/bag+of+words)）数据集，该数据集来自**UCI机器学习库**（[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)）。该数据集包含有关Enron电子邮件的信息，如电子邮件ID、单词ID及其出现次数，即某个特定单词在给定电子邮件中出现的次数。
- en: 'In the GitHub repository ([https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services/tree/master/Ch9_NTM](https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services/tree/master/Ch9_NTM))
    associated with this chapter, you should find the following files:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在与本章相关的GitHub仓库中([https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services/tree/master/Ch9_NTM](https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services/tree/master/Ch9_NTM))，你应该能够找到以下文件：
- en: '`docword.enron.txt.gz` ([https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services/blob/master/Ch9_NTM/data/docword.enron.txt.gz](https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services/blob/master/Ch9_NTM/data/docword.enron.txt.gz)[):](https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services/blob/master/Ch9_NTM/data/docword.enron.txt.gz)
    Contains Email ID and Word ID'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docword.enron.txt.gz` ([https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services/blob/master/Ch9_NTM/data/docword.enron.txt.gz](https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services/blob/master/Ch9_NTM/data/docword.enron.txt.gz)[)：]
    包含电子邮件ID和单词ID'
- en: '`vocab.enron.txt` ([https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services/tree/master/Ch9_NTM/data/vocab.enron.txt](https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services/tree/master/Ch9_NTM/data/vocab.enron.txt)):
    Contains the actual words that are part of the dataset'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab.enron.txt` ([https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services/tree/master/Ch9_NTM/data/vocab.enron.txt](https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services/tree/master/Ch9_NTM/data/vocab.enron.txt))：包含数据集中的实际单词'
- en: Let's begin by looking at topic modeling techniques.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先从回顾主题建模技术开始。
- en: Reviewing topic modeling techniques
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾主题建模技术
- en: In this section, we look at several linear and non-linear learning techniques
    when it comes to topic modeling. Linear techniques include Latent Semantic Analysis
    (two approaches - Singular Vector Decomposition and Non-negative Matrix Factorization),
    probabilistic Latent Semantic Analysis, and Latent Dirichlet Allocation. On the
    other hand, non-linear techniques include LDA2Vec and the Neural Variational Document
    Model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将讨论几种线性和非线性学习技术，涉及主题建模。线性技术包括潜在语义分析（两种方法——奇异向量分解和非负矩阵分解）、概率潜在语义分析和潜在狄利克雷分配。另一方面，非线性技术包括LDA2Vec和神经变分文档模型。
- en: 'In the case of **Latent Semantic Analysis** (**LSA**), topics are discovered
    by approximating documents into a smaller number of topic vectors. A collection
    of documents is represented by document-word matrix:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在**潜在语义分析**（**LSA**）的情况下，主题是通过将文档近似为较少数量的主题向量来发现的。一组文档通过文档-词矩阵表示：
- en: In its simplest form, the document word matrix consists of raw counts, which
    is the frequency with which a given word occurs in a given document. Since this
    approach doesn't account for the significance of each word in the document, we
    replace raw counts with the **tf-idf** (**term frequency-inverse document frequency**)
    score.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在其最简单的形式中，文档词矩阵由原始计数组成，即给定词在给定文档中出现的频率。由于这种方法没有考虑每个词在文档中的重要性，我们用**tf-idf**（**词频-逆文档频率**）分数替代原始计数。
- en: Through tf-idf, words that occur frequently within the document in question,
    but less frequently across all the other documents, will have a higher weight.
    Given that the matrix of documents and words is sparse and noisy, dimensionality
    must be reduced to obtain meaningful relationships between documents and words
    via topics.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过tf-idf，出现在特定文档中频繁出现但在其他文档中出现较少的词将具有较高的权重。由于文档-词矩阵是稀疏且嘈杂的，必须通过降维来获取文档和词之间通过主题形成的有意义关系。
- en: 'Reducing dimensionality can be done through truncated **SVD** (**Singular Value
    Decomposition**), where the document-word matrix is broken down into three different
    matrices, that is, document topic (*U*), word-topic (*V*), and singular values
    matrix (*S*), where singular values represent the strength of the topics, as shown
    in the following diagram:'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维可以通过截断**SVD**（**奇异值分解**）完成，其中文档-词矩阵被分解为三个不同的矩阵，即文档主题（*U*）、词-主题（*V*）和奇异值矩阵（*S*），其中奇异值表示主题的强度，如下图所示：
- en: '![](img/dfd740bf-c367-4f34-9ef5-59d21c622be1.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfd740bf-c367-4f34-9ef5-59d21c622be1.png)'
- en: 'This decomposition is unique. To represent documents and words in a lower-dimensional
    space, only *T* largest singular values are chosen (a subset of the matrix, as
    shown in the preceding diagram), and only the first *T* columns of *U* and *V*
    are retained. *T* is a hyperparameter and can be adjusted to reflect the number
    of topics we want to find. In linear algebra, any *m x n* matrix *A* can be decomposed
    as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分解是唯一的。为了在低维空间中表示文档和词，只选择*T*个最大的奇异值（如前图所示的矩阵子集），并且只保留*U*和*V*的前*T*列。*T*是一个超参数，可以调整以反映我们想要找到的主题数量。在线性代数中，任何*m
    x n*矩阵*A*都可以按如下方式分解：
- en: '![](img/8c95a260-1eb4-4cbb-9766-b5592440c1fc.png), where *U* is called the
    left singular vector, *V* is called the right singular vector, and *S* is called
    the **singular value matrix**.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/8c95a260-1eb4-4cbb-9766-b5592440c1fc.png)，其中*U*称为左奇异向量，*V*称为右奇异向量，*S*称为**奇异值矩阵**。'
- en: For information on how to compute singular values, as well as left and right
    singular vectors for a given matrix, refer to [https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/](https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/)
    [intuitive explanation—reconstruct matrix from SVD](https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何计算奇异值以及给定矩阵的左奇异向量和右奇异向量的信息，请参考[https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/](https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/)
    [直观解释——从SVD重构矩阵](https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/)。
- en: Therefore, we get, ![](img/f445a99d-9c18-4e3a-a1a8-90dd11889672.png).
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，我们得到了，![](img/f445a99d-9c18-4e3a-a1a8-90dd11889672.png)。
- en: 'Besides SVD, you can also conduct matrix factorization through (**non-negative
    matrix factorization** (**NMF**). NMF belongs to linear algebra algorithms and
    is used to identify a latent structure in the data. Two non-negative matrices
    are used to approximate the document-term matrix, as shown in the following diagram
    (terms and words are used interchangeably):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 除了SVD外，你还可以通过(**非负矩阵分解** (**NMF**))进行矩阵分解。NMF属于线性代数算法，用于识别数据中的潜在结构。两个非负矩阵用于近似文档-词项矩阵，如下图所示（术语和单词可以互换使用）：
- en: '![](img/702fe954-7207-45af-996d-391aa2a76ef1.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/702fe954-7207-45af-996d-391aa2a76ef1.png)'
- en: 'Let''s compare and contrast the different linear techniques of LSA and look
    at a variant of LSA that offers more flexibility:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较并对比LSA的不同线性技术，并查看一种提供更多灵活性的LSA变体：
- en: The difference between NMF and SVD is that with SVD, we can end up with negative
    component (left and/or right) matrices, which is not natural for interpreting
    textual representation. NMF, on the other hand, generates non-negative representations
    for performing LSA.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NMF与SVD的区别在于，使用SVD时，我们可能会得到负的组件（左侧和/或右侧）矩阵，这在解释文本表示时并不自然。而NMF则生成非负的表示，用于执行LSA。
- en: The drawback of LSA, in general, is that it has fewer interpretable topics and
    a less efficient representation. Additionally, it is a linear model and cannot
    be used to model non-linear dependencies. The number of latent topics is limited
    by the rank of the matrix.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSA的缺点通常是它有较少可解释的主题和效率较低的表示。此外，它是一个线性模型，不能用于建模非线性依赖关系。潜在主题的数量受矩阵秩的限制。
- en: '**Probabilistic LSA** (**pLSA**): The whole idea of pLSA is to find a probabilistic
    model of latent topics that can generate documents and words we can observe. Therefore,
    the joint probability, that is, the probability of finding a combination of documents
    and words, [![](img/a1fa04af-15cf-4695-a6e0-41c3bf7bc6a1.png)], can be written
    as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**概率LSA** (**pLSA**)：pLSA的整体思想是找到一个潜在主题的概率模型，该模型能够生成我们可以观察到的文档和单词。因此，联合概率，也就是文档和单词组合的概率，[![](img/a1fa04af-15cf-4695-a6e0-41c3bf7bc6a1.png)]，可以写成如下：'
- en: '![](img/2216cf84-ecdd-418c-9d69-3229a7fe5c1e.png) = ![](img/e76f0379-2351-46f4-83ae-782618ed3405.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2216cf84-ecdd-418c-9d69-3229a7fe5c1e.png) = ![](img/e76f0379-2351-46f4-83ae-782618ed3405.png)'
- en: Here, *D*=Document, *W*=Word, and *Z*=Topic.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*D* = 文档，*W* = 单词，*Z* = 主题。
- en: 'Let''s look at how pLSA works and an example of when it is not adequate:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看pLSA是如何工作的，并举例说明它在某些情况下并不充分：
- en: We can see how pLSA is similar to LSA in that *P(Z)* corresponds to a singular
    value matrix, *P(D|Z)* corresponds to a left singular vector, and *P(W|Z)* corresponds
    to a right singular vector from SVD.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以看到，pLSA与LSA的相似之处在于，*P(Z)*对应于一个奇异值矩阵，*P(D|Z)*对应于一个左奇异向量，而*P(W|Z)*则对应于SVD中的一个右奇异向量。
- en: The number one disadvantage with this approach is that we cannot readily generalize
    it for new documents. LDA addresses this issue.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法的最大缺点是，我们无法轻易地将其推广到新文档。LDA解决了这个问题。
- en: '**Latent Dirichlet Allocation** (**LDA**): While LSA and pLSA are used for
    semantic analysis or information retrieval, LDA is used for topic mining. In simple
    terms, you uncover topics based on the word frequency across a collection of documents:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配** (**LDA**)：虽然LSA和pLSA用于语义分析或信息检索，但LDA用于主题挖掘。简单来说，你基于文档集合中的词频来揭示主题：'
- en: For a collection of documents, you designate the number of topics you want to
    uncover. This number can be adjusted depending on the performance of LDA on unseen
    documents.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于一组文档，你指定你想要揭示的主题数量。这个数字可以根据LDA在未见文档上的表现进行调整。
- en: Then, you tokenize the documents, remove any stop words, retain words that appear
    a certain number of times across the corpus, and conduct stemming.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，你对文档进行标记化，移除停用词，保留在语料库中出现一定次数的单词，并进行词干化处理。
- en: To begin with, for each word, you assign a random topic. You then compute the
    topic mixture by document, that is, the number of times each topic appears in
    the document. You also compute the word mixture by topic across the corpus, that
    is, the number of times each word appears in the topic.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，对于每个单词，你分配一个随机主题。然后你通过文档计算主题混合，即每个主题在文档中出现的次数。你还要计算主题中每个单词在语料库中的混合，即每个单词在该主题中出现的次数。
- en: 'Iteration/Pass 1: For each word, you then reassign a topic after navigating
    through the entire corpus. The topic is reassigned based on other topic assignments
    by document. Let''s say a document is represented by the following topic mixture:
    topic 1 - 40%, topic 2 - 20%, and topic 3 - 40% and the first word in the document
    is assigned to topic 2\. The word in question appears across these topics (the
    entire corpus) in the following manner: topic 1 - 52%, topic 2 - 42%, and topic
    3 - 6%. In the document, we reassign the word from topic 2 to topic 1 because
    the word represents topic 1 (40%*52%) more than topic 2 (20%*42%). This process
    is repeated for all the documents. By the end of pass 1, you will have covered
    each of the words in the corpus.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代/遍历 1：对于每个单词，在遍历整个语料库后，你将重新分配一个主题。主题的重新分配是根据每个文档的其他主题分配来进行的。假设一个文档的主题分布为：主题
    1 - 40%，主题 2 - 20%，主题 3 - 40%，并且文档中的第一个单词被分配到主题 2。该单词在这些主题中（即整个语料库）出现的频率如下：主题
    1 - 52%，主题 2 - 42%，主题 3 - 6%。在文档中，我们将该单词从主题 2 重新分配到主题 1，因为该单词代表主题 1（40%*52%）的概率高于主题
    2（20%*42%）。这个过程会在所有文档中重复进行。经过遍历 1 后，你将覆盖语料库中的每个单词。
- en: We go through several passes or iterations across the entire corpus for each
    word until no further reassignment is necessary.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对整个语料库进行几轮遍历或迭代，直到不再需要重新分配。
- en: In the end, we have a few designated topics, with each topic represented by
    keywords.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终，我们会得到一些指定的主题，每个主题由关键词表示。
- en: Up until now, we have looked at linear techniques for topic modeling. Now, it's
    time to turn our attention to non-linearlearning. Neural network models for topic
    modeling are much more flexible, allowing new capabilities to be added (for example,
    creating contextual words for an input/target word).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已查看了用于主题建模的线性技术。现在，是时候将注意力转向非线性学习了。神经网络模型用于主题建模，具有更大的灵活性，允许增加新的功能（例如，为输入/目标单词创建上下文单词）。
- en: '**Lda2vec** is a superset of word2vec and LDA models. It is a variation of
    the skip-gram word2vec model. Lda2Vec can be used for a variety of applications,
    such as predicting contextual words given a word (known as a pivot or target word),
    including learning topic vectors for topic modeling.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**Lda2vec** 是 word2vec 和 LDA 模型的超集。它是 skip-gram word2vec 模型的一种变体。Lda2vec 可以用于多种应用，如预测给定单词的上下文单词（称为枢轴或目标单词），包括学习用于主题建模的主题向量。'
- en: Lda2vec is similar to **Neural Variational Document Models** (**NVDM**) in terms
    of producing topic embeddings or vectors. However, NVDM adopts a much cleaner
    and flexible approach to topic modeling by creating document vectors using neural
    networks, where a word-to-word relationship is completely disregarded.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Lda2vec 类似于 **神经变分文档模型**（**NVDM**），用于生成主题嵌入或向量。然而，NVDM 采用了更为简洁和灵活的主题建模方法，通过神经网络创建文档向量，而完全忽视了单词之间的关系。
- en: '**NVDM** (**Neural Variational Document Model**) is a flexible generative document
    modeling process where we learn about multiple representations of documents through
    topics (hence the word *variational—*meaning multiple—in NVDM):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**NVDM**（**神经变分文档模型**）是一种灵活的生成文档建模过程，我们通过主题学习多个文档表示（因此，NVDM 中的 *变分*——意味着多个——一词）：'
- en: 'NVDM is based on the **Variational Autoencoder** (**VAE**) framework, which
    uses one neural network to encode (that is, an encoder) a collection of documents
    and a second one to decode (that is, a decoder) the compressed representation
    of documents. The goal of this process is to look at the best way to approximate
    information in a corpus. The autoencoder is optimized by minimizing two types
    of losses:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVDM 基于 **变分自编码器**（**VAE**）框架，该框架使用一个神经网络对文档集合进行编码（即编码器），另一个神经网络对文档的压缩表示进行解码（即解码器）。这个过程的目标是寻找在语料库中近似信息的最佳方式。自编码器通过最小化两种类型的损失进行优化：
- en: '**Loss with Decoding** (the **reconstruction error**): Reconstruct the original
    document from topic embeddings.'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码损失**（**重构误差**）：通过主题嵌入重构原始文档。'
- en: '**Loss with Encoding** (the **Kullback Leibler or KL Divergence**): Build a
    stochastic representation of the input document or topic embeddings. KL divergence
    measures information that''s lost when encoding a bag-of-words representation
    of documents.'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码损失**（**Kullback-Leibler 或 KL 散度**）：构建输入文档或主题嵌入的随机表示。KL 散度衡量在编码文档的词袋表示时丢失的信息。'
- en: Now, let's do a delve into the Neural Topic Model, an implementation of NVDM
    from AWS. Although AWS offers a readily consumable API, AWS Comprehend, to discover
    topics, the NTM algorithm provides the fine-grained control and flexibility to
    uncover topics from long-form text.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解神经主题模型（NTM），这是 AWS 实现的 NVDM。尽管 AWS 提供了一个现成可用的 API——AWS Comprehend，用于发现主题，但
    NTM 算法提供了细粒度的控制和灵活性，可以从长文本中挖掘主题。
- en: Understanding how the Neural Topic Model works
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解神经主题模型的工作原理
- en: 'The **Neural Topic Model** (**NTM**), as we described previously, is a generative
    document model that produces multiple representations of a document. It generates
    two outputs:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，**神经主题模型**（**NTM**）是一个生成型文档模型，可以生成文档的多种表示。它生成两个输出：
- en: The topic mixture for a document
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档的主题混合
- en: A list of keywords that explain a topic, for all the topics across an entire
    corpus
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组解释主题的关键词，涵盖整个语料库中的所有主题
- en: 'NTM is based on the **Variational Autoencoder** architecture. The following
    illustration shows how NTM works:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: NTM 基于 **变分自编码器**（Variational Autoencoder）架构。下图展示了 NTM 的工作原理：
- en: '![](img/3a82f3ee-d762-4fda-a384-1336257c2599.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a82f3ee-d762-4fda-a384-1336257c2599.png)'
- en: 'Let''s explain this diagram, bit by bit:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步解释这个图示：
- en: There are two components—an encoder and a decoder. In the encoder, we have a
    **Multiple Layer Perceptron** (**MLP**) network that takes a bag-of-words representation
    of documents and creates two vectors, a vector of means ![](img/120cf1e9-9997-4dae-a4b6-f345d489d798.png)
    and a vector of standard deviation ![](img/77c0a009-13af-4981-a6f0-45d3ccc5162a.png).
    Intuitively, the mean vector controls where encoding the input should be centered,
    while the standard deviation controls the area around the center. Because the
    sample that was generated from this area is going to vary each time it's generated,
    the decoder will learn to reconstruct different latent encodings of the input.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NTM 由两个组件组成——编码器和解码器。在编码器中，我们有一个 **多层感知器**（**MLP**）网络，它接收文档的词袋表示，并创建两个向量，一个是均值向量
    ![](img/120cf1e9-9997-4dae-a4b6-f345d489d798.png)，另一个是标准差向量 ![](img/77c0a009-13af-4981-a6f0-45d3ccc5162a.png)。直观地讲，均值向量控制编码输入的中心位置，而标准差控制中心周围的区域。由于从该区域生成的样本每次都会有所不同，解码器将学习如何重构输入的不同潜在编码。
- en: 'MLP is a class of feedforward **artificial neuron networks** (**ANNs**). It
    consists of at least three layers of nodes: input, output, and a hidden layer.
    Except for the input node, each node is a neuron that uses a nonlinear activation
    function.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: MLP 是一种前馈 **人工神经网络**（**ANNs**）类别。它由至少三层节点组成：输入层、输出层和隐藏层。除了输入节点外，每个节点都是一个使用非线性激活函数的神经元。
- en: The second component is a decoder, which reconstructs a document by independently
    generating words. The output layer of the network is a Softmax layer that defines
    the probability of each word by topic by reconstructing the topic words matrix.
    Each column in the matrix represents a topic, while each row represents a word.
    The matrix values, for a given column, represent the probability of the distribution
    of words for the topic.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个组件是解码器，它通过独立生成单词来重构文档。网络的输出层是一个 Softmax 层，通过重构主题词矩阵来定义每个单词按主题的概率。矩阵中的每一列表示一个主题，每一行表示一个单词。矩阵中某一列的值表示该主题下单词分布的概率。
- en: The Softmax decoder uses multinomial logistic regression, where we account for
    the conditional probability of different topics. The transformation is, effectively,
    a normalized exponential function that's used to highlight the largest values
    and suppress values that are significantly below the max value.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 解码器使用多项式逻辑回归，我们考虑不同主题的条件概率。这个变换实际上是一个标准化的指数函数，用于突出最大值并抑制远低于最大值的值。
- en: NTM is optimized by reducing reconstruction errors and KL divergence, as well
    as the learning weights and biases of the network. Therefore, NTM is a flexible
    neural network model for topic mining and producing interpretable topics. It is
    now time to look at training NTM in SageMaker.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: NTM 通过减少重构误差和 KL 散度，以及调整网络的学习权重和偏差来进行优化。因此，NTM 是一个灵活的神经网络模型，适用于主题挖掘和生成可解释的主题。现在，是时候在
    SageMaker 中查看如何训练 NTM 了。
- en: Training NTM in SageMaker
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 SageMaker 中训练 NTM
- en: In this section, we will train NTM on Enron emails to produce topics. These
    emails were exchanged between Enron, an American energy company that ceased its
    operations in 2007 due to financial losses, and other parties that did business
    with the company.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将使用NTM（主题模型）对Enron邮件进行训练，以产生话题。这些邮件是在Enron（一家因财务损失于2007年停止运营的美国能源公司）与其他与其有业务往来的各方之间交换的。
- en: The dataset contains 39,861 emails and 28,101 unique words. We will work with
    a subset of these emails – 3,986 emails and 17,524 unique words. Additionally,
    we will create a text file, `vocab.txt`, so that the NTM model can report the
    word distribution of a topic.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含39,861封邮件和28,101个独特的单词。我们将使用这些邮件的一个子集——3,986封邮件和17,524个独特单词。另外，我们将创建一个文本文件`vocab.txt`，以便NTM模型可以报告某个话题的单词分布。
- en: 'Before we get started, make sure that both `docword.enron.txt.gz` and `vocab.enron.txt`
    have been uploaded to a folder called `data` on the local SageMaker compute instance.
    Follow these steps:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，确保`docword.enron.txt.gz`和`vocab.enron.txt`文件已经上传到本地SageMaker计算实例中的一个名为`data`的文件夹。请按照以下步骤操作：
- en: 'Create a bag-of-words representation of emails, as follows:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个邮件的词袋表示，步骤如下：
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding code, we used the `pivot_table()` function in the pandas library
    to pivot emails so that email IDs are indexes and word IDs are columns. The values
    in the pivot table represent word counts. The pivot table contains 3,986 email
    IDs and 17,524 word IDs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用了pandas库中的`pivot_table()`函数来对邮件进行透视，使得邮件ID成为索引，单词ID成为列。透视表中的值表示单词的词频。该透视表包含3,986个邮件ID和17,524个单词ID。
- en: Now, let's multiply the word counts by the **Inverse Document Frequency** (**IDF**)
    factor. Our assumption is that words that occur frequently in an email and less
    frequently in other emails are important in discovering topics, while words that
    occur frequently in all the emails may not be important for discovering topics.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将词频乘以**逆文档频率**（**IDF**）因子。我们的假设是，出现在一封邮件中并且在其他邮件中出现频率较低的单词，对于发现话题很重要，而那些在所有邮件中频繁出现的单词可能对发现话题不重要。
- en: 'IDF is calculated as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: IDF的计算公式如下：
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: IDF is given by ![](img/b6ba616f-fdc7-4ae9-b30a-10a0749c6c11.png), where N is
    the number of emails in the dataset and ![](img/1a526ca1-bdd2-4916-9589-7a96e72a248c.png)
    is the number of documents containing the word *i*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: IDF公式如下所示：![](img/b6ba616f-fdc7-4ae9-b30a-10a0749c6c11.png)，其中N是数据集中邮件的数量，![](img/1a526ca1-bdd2-4916-9589-7a96e72a248c.png)是包含单词*i*的文档数量。
- en: At the end of this step, a new DataFrame representing the pivoted emails with
    tf-idf values is created.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步结束时，将创建一个新的DataFrame，表示透视后的邮件及其tf-idf值。
- en: 'Now, we will create a compressed sparse row matrix from the bag of words representation
    of emails, as follows:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将从邮件的词袋表示中创建一个压缩稀疏行矩阵，步骤如下：
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code, we used the `csr_matrix()` function from the `scipy.sparse`
    module to produce an efficient representation of the emails matrix. With the compressed
    sparse row matrix, you are able to run operations on only non-zero values, in
    addition to using less RAM for computation. The compressed sparse row matrix uses
    the row pointer to point to the row number and the column index to identify the
    column in the row, as well as the values for the given row pointer and column
    index.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用了`scipy.sparse`模块中的`csr_matrix()`函数来高效表示邮件矩阵。使用压缩稀疏行矩阵，您可以仅对非零值进行操作，并且计算时占用更少的内存。压缩稀疏行矩阵使用行指针指向行号，列索引标识该行中的列，以及给定行指针和列索引的值。
- en: 'Split the dataset into training, validation, and test sets as follows:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照如下方式将数据集分成训练集、验证集和测试集：
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We use 80% of the emails for training, 10% for validation, and the remaining
    10% for testing.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将80%的邮件用于训练，10%用于验证，剩余的10%用于测试。
- en: 'Convert the emails from a compressed sparse row matrix into RecordIO wrapped
    Protobuf format, as follows:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将邮件从压缩稀疏行矩阵转换为RecordIO封装的Protobuf格式，步骤如下：
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Protobuf format**, also known as **Protocol Buffers**, is a protocol from
    Google that''s used to serialize or encode structured data. Although JSON, XML,
    and Protobuf can be used interchangeably, Protobuf is much more enhanced and supports
    more data types than other formats. RecordIO is a file format that stores serialized
    data on disk. Its purpose is to store data as a sequence of records for faster
    reading. Under the hood, RecordIO uses Protobuf to serialize structured data.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**Protobuf格式**，也称为**协议缓冲区**，是来自Google的一种协议，用于序列化或编码结构化数据。虽然JSON、XML和Protobuf可以互换使用，但Protobuf比其他格式更为增强，并支持更多的数据类型。RecordIO是一种文件格式，用于在磁盘上存储序列化数据。其目的是将数据存储为一系列记录，以便更快地读取。在底层，RecordIO使用Protobuf来序列化结构化数据。'
- en: For distributed training, we take a training dataset and divide it into portions
    for distributed training. Please see the source code attached to this chapter
    for additional details. The `write_spmatrix_to_sparse_tensor()` function from
    `sagemaker.amazon.common` is used to convert each of these portions from the sparse
    row matrix format into the sparse tensor format. The function takes a sparse row
    matrix as input, along with a binary stream that RecordIO records will be written
    to. We then reset the stream position to the beginning of the stream by calling
    the `seek()` method—this is essential for reading data from the beginning of the
    file.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分布式训练，我们将训练数据集分成若干部分用于分布式训练。更多细节请参考本章附带的源代码。`sagemaker.amazon.common`中的`write_spmatrix_to_sparse_tensor()`函数用于将每个部分从稀疏行矩阵格式转换为稀疏张量格式。该函数以稀疏行矩阵作为输入，同时指定一个二进制流，将RecordIO记录写入其中。然后，我们通过调用`seek()`方法将流的位置重置到流的开始位置——这对于从文件开头读取数据至关重要。
- en: 'Upload the train and validation datasets to an S3 bucket, as follows:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练集和验证集上传到S3存储桶，方法如下：
- en: '[PRE5]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We provide the filename to the binary stream and specify the name of the S3
    bucket, which is where the datasets will be stored for training. We call the `upload_fileobj()`
    method of the S3 object to upload the binary data to a designated location.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将文件名提供给二进制流，并指定S3存储桶的名称，该存储桶用于存储训练数据集。我们调用S3对象的`upload_fileobj()`方法，将二进制数据上传到指定位置。
- en: 'Now, we initialize SageMaker''s `Estimator` object to prepare for training,
    as follows:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们初始化SageMaker的`Estimator`对象，为训练做准备，方法如下：
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The estimator object, `ntm_estmtr`, is created by passing the Docker registry
    path of the NTM image, the SageMaker execution role, the number and type of training
    instances, and the location of the output. Since the number of compute instances
    we are launching is two, we will be conducting distributed training. In distribution
    training, data is partitioned and training is conducted in parallel on several
    chunks of data.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 估算器对象`ntm_estmtr`是通过传入NTM镜像的Docker注册表路径、SageMaker执行角色、训练实例的数量和类型以及输出位置来创建的。由于我们启动的计算实例数量为两个，因此我们将进行分布式训练。在分布式训练中，数据被划分，并在多个数据块上并行进行训练。
- en: 'Now, let''s define the hyperparameters of the NTM algorithm, as follows:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义NTM算法的超参数，方法如下：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s take a look at the hyperparameters we specified in the preceding code:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看在前面的代码中指定的超参数：
- en: '`feature_dim`: This represents the size of the feature vector. It is set to
    the vocabulary size, which is 17,524 words.'
  id: totrans-94
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_dim`：表示特征向量的大小。它设置为词汇表的大小，即17,524个单词。'
- en: '`num_topics`: This represents the number of topics to extract. We chose three
    topics here, but this can be adjusted based on the model''s performance on the
    test set.'
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_topics`：表示要提取的主题数量。我们在这里选择了三个主题，但可以根据模型在测试集上的表现进行调整。'
- en: '`mini_batch_size`: This represents the number of training examples to process
    before updating the weights. We specify 30 training examples here.'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mini_batch_size`：表示在更新权重之前要处理的训练样本数量。我们在这里指定了30个训练样本。'
- en: '`epochs`: This represents the number of backward and forward passes that are
    made.'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs`：表示进行的前向和反向传递的次数。'
- en: '`num_patience_epochs`: This represents the maximum number of bad epochs (epochs
    where the loss does not improve) that are executed before stopping.'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_patience_epochs`：表示在停止之前执行的最大无效epoch数量（即损失没有改善的epoch）。'
- en: '`optimizer`: This represents the algorithm that''s used to optimize network
    weights. We are using the Adadelta optimization algorithm. The Adaptive Delta
    gradient is an enhanced version of **Adagrad** (**Adaptive Gradient**), where
    the learning rate decreases based on a rolling window of gradient updates versus
    all past gradient updates.'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer`：这表示用于优化网络权重的算法。我们使用的是Adadelta优化算法。自适应Delta梯度是**Adagrad**（**自适应梯度**）的增强版本，其中学习率根据梯度更新的滚动窗口与所有过去的梯度更新进行比较而减小。'
- en: '`tolerance`: This represents the threshold for change in the loss function
    – the training stops early if the change in the loss within the last designated
    number of patience epochs falls below this threshold.'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tolerance`：这表示损失函数变化的阈值——如果在最后指定的耐心周期数内损失变化低于此阈值，则训练会提前停止。'
- en: Upload the text file containing the vocabulary or words of the dataset to the
    auxiliary path/channel. This is the channel that's used to provide additional
    information to SageMaker algorithms during training.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将包含词汇或数据集单词的文本文件上传到辅助路径/频道。这是用于在训练过程中向SageMaker算法提供附加信息的通道。
- en: 'Fit the NTM algorithm to the training and validation sets, as follows:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将NTM算法拟合到训练集和验证集，如下所示：
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For training, we call the `fit()` method of the `ntm_estmtr` object by passing
    initialized `S3_input` objects from the `sagemaker.session` module. The `s3_train`,
    `s3_val`, and `s3_aux` objects provide the location of the train, validation,
    and auxiliary datasets, as well as their file format and distribution type.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于训练，我们通过传递初始化的`S3_input`对象（来自`sagemaker.session`模块）来调用`ntm_estmtr`对象的`fit()`方法。`s3_train`、`s3_val`和`s3_aux`对象提供训练、验证和辅助数据集的位置，以及它们的文件格式和分布类型。
- en: 'Now, let''s walk through the results from distributed training:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看分布式训练的结果：
- en: 'Review the training output from the first ML compute instance, as follows:'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看第一个机器学习计算实例的训练输出，如下所示：
- en: '![](img/fc9e2360-1268-4fa7-a057-9b52da5487e8.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc9e2360-1268-4fa7-a057-9b52da5487e8.png)'
- en: Remember that there's a total of 3,188 training examples. Because we launched
    two compute instances for training, on the first instance, we trained on 2,126
    examples.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，总共有3,188个训练示例。由于我们为训练启动了两个计算实例，在第一个实例上，我们使用了2,126个示例进行训练。
- en: 'Review the results from the second training instance, as follows:'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看第二个训练实例的结果，如下所示：
- en: '![](img/3df3c45b-74ad-4994-9628-cbfeab7a06d7.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3df3c45b-74ad-4994-9628-cbfeab7a06d7.png)'
- en: On the second compute instance, we trained on the remaining 1,062 examples.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个计算实例上，我们使用剩余的1,062个示例进行训练。
- en: We will report the model's performance on the validation dataset next. For the
    metrics of the training dataset, please refer to the source code attached to this
    chapter.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们将报告模型在验证数据集上的表现。关于训练数据集的度量标准，请参阅本章附带的源代码。
- en: 'Now, let''s walk through the validation results of the trained model. The model
    is evaluated against 390 data points that are part of the validation dataset.
    Specifically, we will look at the following metrics:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看训练模型的验证结果。该模型在390个数据点上进行评估，这些数据点属于验证数据集。具体来说，我们将关注以下度量标准：
- en: '**Word Embedding Topic Coherence Metric** (**WETC**): This measures the semantic
    similarity of the top words in each topic. A good quality model will have top
    words that are located close to each other in a lower-dimensional space. To locate
    words in a lower-dimensional space, pre-trained word embeddings from GloVe (Global
    Vectors) are used.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词嵌入主题一致性度量**（**WETC**）：这衡量每个主题中顶部单词的语义相似性。一个高质量的模型将在低维空间中将顶部单词聚集在一起。为了在低维空间中定位单词，使用了来自GloVe（全球词向量）的预训练词嵌入。'
- en: '**Topic Uniqueness** (**TU**): This measures the uniqueness of the topics that
    are generated. The measure is inversely proportional to the number of times a
    word appears across all the topics. For example, if a word appears in only one
    topic, then the uniqueness of the topic is high (that is, 1). However, if a word
    appears across, say, five topics, then the uniqueness measure is .2 (1 divided
    by 5). To calculate the topic uniqueness across all the topics, we average the
    TU measures across all topics.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题唯一性**（**TU**）：这衡量生成主题的唯一性。该度量与一个单词在所有主题中出现的次数成反比。例如，如果一个单词只出现在一个主题中，则该主题的唯一性很高（即1）。然而，如果一个单词出现在五个主题中，则唯一性度量为0.2（1除以5）。为了计算所有主题的主题唯一性，我们将所有主题的TU度量取平均值。'
- en: Perplexity (logppx) is a statistical measure of how well a probability model
    predicts a sample (validation dataset). After training, the perplexity of the
    trained model is computed on the validation dataset (performance of the trained
    model on the validation dataset). The lower the perplexity, the better, since
    this maximizes the accuracy of the validation dataset.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 困惑度（logppx）是衡量概率模型预测样本（验证数据集）效果的统计指标。训练后，计算训练模型在验证数据集上的困惑度（训练模型在验证数据集上的表现）。困惑度越低越好，因为这最大化了验证数据集的准确性。
- en: Total Loss (total) is a combination of the Kullback-Leibler Divergence loss
    and Reconstruction loss.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总损失（total）是Kullback-Leibler散度损失和重建损失的组合。
- en: 'Remember that the Neural Topic Model optimizes across several epochs by minimizing
    loss in the following ways:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，神经主题模型通过最小化损失，在多个轮次中进行优化，方法如下：
- en: '**Kullback-Leibler Divergence loss** (**kld**): Builds a stochastic representation
    of emails (topic embeddings) that involve relative entropy, a measure of how one
    probability distribution is different from a second, that is, a proxy probability
    distribution.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kullback-Leibler散度损失**（**kld**）：构建了一个电子邮件（主题嵌入）的随机表示，涉及相对熵，这是一种度量概率分布之间差异的方法，也就是代理概率分布。'
- en: '**Reconstruction loss** (**recons**): Reconstructs original emails from topic
    embeddings.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重建损失**（**recons**）：从主题嵌入中重建原始电子邮件。'
- en: 'The following screenshot shows the validation results and lists all the loss
    types that were defined:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了验证结果，并列出了所有定义的损失类型：
- en: '![](img/e2ab8adc-f6c8-4e68-bf33-b733553f22ba.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2ab8adc-f6c8-4e68-bf33-b733553f22ba.png)'
- en: The **total** Loss is **8.47**, where 0.19 is defined as the **kld** Loss and
    **8.28** is defined as the **recons** loss.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总**损失为**8.47**，其中0.19定义为**kld**损失，**8.28**定义为**重建**损失。'
- en: From the preceding screenshot, we can see that, across the three topics, **Word
    Embedding Topic Coherence** (**WETC**) is **.26**, **Topic Uniqueness** (**TU**)
    is **0.73**, and **Preplexity** (**logppx**) is **8.47** (same as the **t****otal**
    Loss).
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从前面的截图中，我们可以看到，在三个主题中，**词嵌入主题一致性**（**WETC**）为**.26**，**主题唯一性**（**TU**）为**0.73**，**困惑度**（**logppx**）为**8.47**（与**总**损失相同）。
- en: The three topics and the words that define each of them are highlighted in the
    by rectangular boxes.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过矩形框突出显示了三个主题及其定义的单词。
- en: Now, it's time to deploy the trained NTM model as an endpoint.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，到了将训练好的NTM模型部署为终端节点的时候了。
- en: Deploying the trained NTM model and running the inference
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署训练好的NTM模型并运行推理
- en: 'In this section, we will deploy the NTM model, run the inference, and interpret
    the results. Let''s get started:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将部署NTM模型，运行推理，并解释结果。让我们开始吧：
- en: 'First, we deploy the trained NTM model as an endpoint, as follows:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将训练好的NTM模型部署为终端节点，如下所示：
- en: '[PRE9]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the preceding code, we call the `deploy()` method of the SageMaker Estimator
    object, `ntm_estmtr`, to create an endpoint. We pass the number and type of instances
    required to deploy the model. The NTM Docker image is used to create the endpoint.
    SageMaker takes a few minutes to deploy the model. The following screenshot shows
    the endpoint that was provisioned:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们调用SageMaker Estimator对象`ntm_estmtr`的`deploy()`方法来创建终端节点。我们传递了部署模型所需的实例数量和类型。NTM
    Docker镜像用于创建终端节点。SageMaker需要几分钟时间来部署模型。以下截图显示了已配置的终端节点：
- en: '![](img/3876a0a5-ef0b-4a0c-aa02-089323a66f49.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3876a0a5-ef0b-4a0c-aa02-089323a66f49.png)'
- en: You can see the endpoint you've created by navigating to the SageMaker service,
    going to the left navigation pane, looking under the Inference section, and clicking
    on **Endpoints**.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过导航到SageMaker服务，进入左侧导航窗格，在推理部分下找到并点击**终端节点**，查看你创建的端点。
- en: 'Designate the request and response content types of test data, as follows:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定测试数据的请求和响应内容类型，如下所示：
- en: '[PRE10]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the preceding code, the `deploy()` method of `ntm_estmtr` returns a `RealTimePredictor`
    object (from the `sagemaker.predictor` module). We assign the input content type
    of the test data and deserializer (the content type of the response) to `ntm_predctr`,
    the `RealTimePredictor` object we created.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`ntm_estmtr`的`deploy()`方法返回一个`RealTimePredictor`对象（来自`sagemaker.predictor`模块）。我们将测试数据的输入内容类型和反序列化器（响应的内容类型）分配给我们创建的`RealTimePredictor`对象`ntm_predctr`。
- en: 'Now, we prepare the test dataset for inference, as follows:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备测试数据集以进行推理，如下所示：
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding code, we convert the test data format from a compressed sparse
    row matrix into a dense array using the numpy Python library.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用numpy Python库将测试数据格式从压缩稀疏行矩阵转换为稠密数组。
- en: 'Then, we invoke the `predict()` method of `ntm_predctr` to run the inference,
    as follows:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们调用`ntm_predctr`的`predict()`方法进行推理，如下所示：
- en: '[PRE12]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding code, we passed the first five emails from the test dataset
    for inference. Then, we navigated through the prediction results to create a multi-dimensional
    array of topic weights, where rows represent emails and columns represent topics.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们传递了测试数据集中前五封邮件进行推理。然后，我们浏览预测结果，创建了一个多维的主题权重数组，其中行表示邮件，列表示主题。
- en: 'Now, we interpret the results, as follows:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们解释结果，如下所示：
- en: '[PRE13]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the preceding code, we transpose `topic_wts_res`, a multi-dimensional array,
    to create a dataframe, `df_tpcwts`, so that each row represents a topic. We then
    plot the topics, as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们转置了`topic_wts_res`，一个多维数组，创建了一个数据框`df_tpcwts`，使得每一行表示一个主题。然后我们绘制主题，如下所示：
- en: '[PRE14]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'On the x-axis, we plot the topic numbers, and on the y-axis, we plot the percentage
    of emails represented by each topic, as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在x轴上，我们绘制了主题编号，在y轴上，我们绘制了每个主题所代表的邮件百分比，如下所示：
- en: '![](img/c8a27d91-acd7-458c-a100-fd716e0eb889.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8a27d91-acd7-458c-a100-fd716e0eb889.png)'
- en: It is evident from the preceding graph that Topic 0 represents less than 10%
    of all five emails. However, Topics 1 and 2 are dominant to varying degrees in
    the emails—around 70% of the 4^(th) email is represented by Topic 1, while around
    60% of the 5th email is represented by Topic 2.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中可以明显看出，主题0在所有五封邮件中的占比不到10%。然而，主题1和主题2在邮件中具有不同程度的主导地位——大约70%的第4封邮件由主题1表示，而大约60%的第5封邮件由主题2表示。
- en: 'Now, let''s look at the word cloud for each topic. It is important to understand
    the word mixture by each topic so that we know the words that predominantly describe
    a particular topic. Let''s get started:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下每个主题的词云。了解每个主题的词汇混合非常重要，这样我们就能知道哪些词语在描述特定主题时占主导地位。让我们开始吧：
- en: 'Download the trained model, as follows:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载已训练的模型，如下所示：
- en: '[PRE15]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding code, we downloaded a trained NTM model from the path specified
    by the `model_path` variable (the location was specified when at the time of creating
    the Estimator, `ntm_estmtr`).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们从由`model_path`变量指定的路径下载了一个已训练的NTM模型（该路径在创建估算器时指定，`ntm_estmtr`）。
- en: 'Now, we obtain the topic-word matrix from the trained model, as follows:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们从已训练的模型中获取主题-词矩阵，如下所示：
- en: '[PRE16]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the preceding code, we extracted the NTM model, `downloaded_model.tar.gz`,
    to load the learned parameters, `params`. Remember that the size of the output
    layer of the model is the same as that of the number of words (vocabulary) in
    the dataset. We then create a multi-dimensional mxnet array, *W*, to load word
    weights by topic. The shape of W is 17,524 x 3, where 17,524 rows represent words
    and 3 columns represent topics.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们提取了NTM模型`downloaded_model.tar.gz`，加载了学习到的参数`params`。请记住，模型输出层的大小与数据集中单词（词汇）的数量相同。然后，我们创建了一个多维的mxnet数组*W*，按主题加载单词权重。W的形状为17,524
    x 3，其中17,524行表示单词，3列表示主题。
- en: 'For each topic, run the softmax function on the word weights, as follows:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个主题，运行softmax函数计算单词权重，如下所示：
- en: '[PRE17]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the preceding code, we run the softmax function on the word weights for each
    topic to bring their values to between 0 and 1\. The sum of probabilities of words
    for each topic should add up to 1\. Remember that the softmax layer, which is
    the output layer of the NTM network, highlights the largest values and mutes the
    values away from the max value.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们对每个主题的单词权重运行softmax函数，将其值映射到0和1之间。每个主题中单词的概率和应当等于1。请记住，softmax层是NTM网络的输出层，它强调最大的值，并抑制离最大值较远的值。
- en: 'Plot the word cloud by topic, as follows:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按主题绘制词云，如下所示：
- en: '![](img/3e7bc101-f452-4311-8412-1ea7054b38eb.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e7bc101-f452-4311-8412-1ea7054b38eb.png)'
- en: As we can see, **Topic0** is defined predominantly by the words *resource* and
    *pending*, while **Topic1** is primarily defined by the words *instruction*, *andor*,
    and *notification*.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，**Topic0**主要由单词*resource*和*pending*定义，而**Topic1**主要由单词*instruction*、*andor*和*notification*定义。
- en: 'Based on the top-ranking words in each of the topics, we can determine the
    topic that''s being discussed in the emails:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 基于每个主题中排名靠前的单词，我们可以确定邮件中讨论的主题：
- en: '**Topic0** (**access to Enron IT apps**): Resource pending request create acceptance
    admin local type permanent nahoutrdhoustonpwrcommonelectric nahoutrdhoustonpwrcommonpower2region
    approval kobra click application date directory read risk tail.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Topic0**（**访问Enron IT应用**）：资源待请求创建接受管理员本地类型永久nahoutrdhoustonpwrcommonelectric
    nahoutrdhoustonpwrcommonpower2region审批kobra点击应用日期目录读取风险尾部。'
- en: '**Topic1** (**energy trading**): Andor instruction notification reserves buy
    responsible sources prohibited order securities downgraded based intelligence
    strong corp web solicitation privacy clicking coverage.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Topic1**（**能源交易**）：Andor指令通知储备购买责任来源禁止订单证券基于情报降级强大公司网站征询隐私点击覆盖。'
- en: '**Topic2** (**includes a combination of access to IT apps and energy trading**):
    Request resource pending create approval type application date tail directory
    acceptance admin flip permanent counterparty head click swap kobra risk.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Topic2**（**包括对IT应用和能源交易的访问**）：请求资源待创建审批类型应用日期尾目录接受管理员翻转永久对手方头部点击交换kobra风险。'
- en: In this section, we have learned how to interpret results from topic modeling.
    Now, let's summarize all of the concepts we've learned about in this chapter.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何解读主题建模的结果。现在，让我们总结本章中我们所学到的所有概念。
- en: Summary
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we've reviewed topic modeling techniques, including linear
    and non-linear learning methods. We explained how the NTM from SageMaker works
    by discussing its architecture and inner workings. We also looked at distributed
    training of the NTM model, where the dataset is divided into chunks for parallel
    training. Finally, we deployed a trained NTM model as an endpoint and ran the
    inference, interpreting topics from Enron emails. It is essential to synthesize
    information and themes from large volumes of unstructured data for any data scientist.
    NTM from SageMaker provides a flexible approach to doing this.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了主题建模技术，包括线性和非线性学习方法。我们通过讨论其架构和内部机制，解释了SageMaker中的NTM如何工作。我们还探讨了NTM模型的分布式训练，其中数据集被分成多个块进行并行训练。最后，我们将训练好的NTM模型部署为端点并运行推理，解读了Enron邮件中的主题。对于任何数据科学家来说，从大量非结构化数据中提取信息和主题至关重要。SageMaker中的NTM提供了一种灵活的方法来实现这一点。
- en: In the next chapter, we will cover the classification of images using SageMaker.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讲解如何使用SageMaker进行图像分类。
- en: Further reading
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: For references to topic modeling techniques—LDA, then please go to [http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 关于主题建模技术——LDA的参考文献，请访问[http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/)。
- en: 'For an intuitive explanation of VAE, check out the following links:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想直观地理解VAE，可以查看以下链接：
- en: '[https://jaan.io/what-is-variational-autoencoder-vae-tutorial/](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://jaan.io/what-is-variational-autoencoder-vae-tutorial/](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)'
- en: '[https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)'
- en: For references on neural variational inference for text processing, please go
    to [https://arxiv.org/pdf/1511.06038.pdf](https://arxiv.org/pdf/1511.06038.pdf).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 关于神经变分推断在文本处理中的参考文献，请访问[https://arxiv.org/pdf/1511.06038.pdf](https://arxiv.org/pdf/1511.06038.pdf)。
