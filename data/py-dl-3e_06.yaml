- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Natural Language Processing and Recurrent Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理和循环神经网络
- en: This chapter will introduce two different topics that nevertheless complement
    each other – **natural language processing** (**NLP**) and **recurrent neural
    networks** (**RNNs**). NLP teaches computers to process and analyze natural language
    text to perform tasks such as machine translation, sentiment analysis, and text
    generation. Unlike images in computer vision, natural text represents a different
    type of data, where the order (or sequence) of the elements matters. Thankfully,
    RNNs are suitable for processing sequential data, such as text or time series.
    They help us deal with sequences of variable length by defining a recurrence relation
    over these sequences (hence the name). This makes NLP and RNNs natural allies.
    In fact, RNNs can be applied to any problem since it has been proven that they
    are Turing-complete – theoretically, they can simulate any program that a regular
    computer would not be able to compute.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍两个不同但互补的主题——**自然语言处理**（**NLP**）和**循环神经网络**（**RNNs**）。NLP教会计算机处理和分析自然语言文本，以执行诸如机器翻译、情感分析和文本生成等任务。与计算机视觉中的图像不同，自然文本代表了一种不同类型的数据，其中元素的顺序（或序列）非常重要。幸运的是，RNNs非常适合处理顺序数据，如文本或时间序列。通过在这些序列上定义递归关系（因此得名），它们帮助我们处理可变长度的序列。这使得NLP和RNNs成为天然的盟友。事实上，RNNs可以应用于任何问题，因为已经证明它们是图灵完备的——从理论上讲，它们可以模拟任何常规计算机无法计算的程序。
- en: However, it is not only good news, and we’ll have to start with a disclaimer.
    Although RNNs have great theoretical properties, we now know that there are practical
    limitations to their use. These limitations have been mostly surpassed by a more
    recent **neural network** (**NN**) architecture called **transformer**, which
    we’ll discuss in [*Chapter 7*](B19627_07.xhtml#_idTextAnchor202). In theory, the
    transformer has more limitations compared to RNNs. But as sometimes happens, it
    works better in practice. Nevertheless, I believe that this chapter will be beneficial
    to you. On one hand, RNNs have elegant architecture and still represent one of
    the major NN classes; on the other hand, the progression of knowledge presented
    in this and the next three chapters will closely match the real-world progression
    of research on these topics. So, you’ll be able to apply the concepts you’ll learn
    here in the next few chapters as well. This chapter will also allow you to fully
    appreciate the advantages of the newer models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不全是好消息，我们需要从一个免责声明开始。尽管RNNs具有很好的理论特性，但我们现在知道它们在实际应用中有一定的局限性。这些局限性大多已经被一种更新的**神经网络**（**NN**）架构——**transformer**克服，我们将在[*第7章*](B19627_07.xhtml#_idTextAnchor202)中讨论它。从理论上讲，transformer相比RNNs有更多的限制。但有时候，实践证明它表现得更好。尽管如此，我相信本章对你仍然是有益的。一方面，RNNs具有优雅的架构，仍然代表着神经网络中的重要一类；另一方面，本章和接下来的三章所呈现的知识进展，将与这些主题在实际研究中的进展紧密相符。因此，你将在接下来的几章中也能应用这里学到的概念。本章还将帮助你充分理解新模型的优势。
- en: 'This chapter will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Natural language processing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: Introducing RNNs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍RNNs
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We’ll implement the example in this chapter using Python, PyTorch, and the
    TorchText package ([https://github.com/pytorch/text](https://github.com/pytorch/text)).
    If you don’t have an environment set up with these tools, fret not – the example
    is available as a Jupyter Notebook on Google Colab. You can find the code examples
    in this book’s GitHub repository: [https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter06](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter06).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中使用Python、PyTorch和TorchText包（[https://github.com/pytorch/text](https://github.com/pytorch/text)）来实现示例。如果你没有配置这些工具的环境，不必担心——该示例可以在Google
    Colab上的Jupyter Notebook中运行。你可以在本书的GitHub仓库中找到代码示例：[https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter06](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter06)。
- en: Natural language processing
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: 'NLP is a subfield of machine learning that allows computers to interpret, manipulate,
    and comprehend human language. This definition sounds a little dry, so, to provide
    a little clarity, let’s start with a non-exhaustive list of the types of tasks
    that fall under the NLP umbrella:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: NLP是机器学习的一个子领域，使计算机能够解释、操作和理解人类语言。这个定义听起来有点枯燥，因此，为了提供一些清晰度，让我们从一个非详尽的任务列表开始，看看都有哪些任务属于NLP的范畴：
- en: '**Text classification**: This assigns a single label to the entire input text.
    For example, **sentiment analysis** can determine whether a product review is
    positive or negative.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本分类**：这会为整个输入文本分配一个标签。例如，**情感分析**可以判断一篇产品评论是积极的还是消极的。'
- en: '**Token classification**: This assigns a label for each token of the input
    text. A token is a building block (or a unit) of text. Words can be tokens. A
    popular token classification task is **named entity recognition**, which assigns
    each token to a list of predefined classes such as place, company, or person.
    **Part-of-speech** (**POS**) tagging assigns each word to a particular part of
    speech, such as a noun, verb, or adjective.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记分类**：这为每个输入文本的标记分配一个标签。标记是文本的构建块（或单位）。单词可以是标记。一个流行的标记分类任务是**命名实体识别**，它为每个标记分配一个预定义类别列表，如地点、公司或人物。**词性**（**POS**）标注为每个单词分配一个特定的词性，如名词、动词或形容词。'
- en: '**Text generation**: This uses the input text to generate new text with arbitrary
    length. Text generation tasks include machine translation, question answering,
    and text summarization (creating a shorter version of the original text while
    preserving its essence).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本生成**：这是利用输入文本生成具有任意长度的新文本。文本生成任务包括机器翻译、问答和文本摘要（在保留原文精髓的同时创建简短版本）。'
- en: 'Solving NLP problems is not trivial. To understand why, let’s go back to computer
    vision ([*Chapter 4*](B19627_04.xhtml#_idTextAnchor107)), where the input images
    are represented as 2D tensors of pixel intensities with the following properties:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 解决自然语言处理（NLP）问题并非易事。为了理解其原因，我们先回顾一下计算机视觉（[*第4章*](B19627_04.xhtml#_idTextAnchor107)），其中输入的图像以像素强度的二维张量表示，具有以下特点：
- en: The image is composed of pixels and doesn’t have any other explicitly defined
    structure
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像由像素构成，并且没有其他显式定义的结构
- en: The pixels form implicit hierarchical structures of larger objects, based on
    their proximity to each other
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像素基于彼此的接近度，形成了隐式的更大物体的层次结构
- en: There is only one type of pixel, which is defined only by its scalar intensity
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有一种类型的像素，其仅由标量强度来定义
- en: Thanks to its homogenous structure, we can feed the (almost) raw image to a
    **convolutional neural network** (**CNN**) and let it do its magic with relatively
    little data pre-processing.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其同质化的结构，我们可以将（几乎）原始的图像输入到**卷积神经网络**（**CNN**）中，让它以相对较少的数据预处理做出处理。
- en: 'Now, let’s return to text data, which has the following properties:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到文本数据，它具有以下特点：
- en: There are different types of characters with different semantical meanings,
    such as letters, digits, and punctuation marks. In addition, we might encounter
    previously unknown symbols.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有不同类型的字符，具有不同的语义意义，如字母、数字和标点符号。此外，我们还可能遇到以前未见过的符号。
- en: The natural text has an explicit hierarchy in the form of characters, words,
    sentences, and paragraphs. We also have quotes, titles, and a hierarchy of headings.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然文本有着显式的层次结构，包括字符、单词、句子和段落。我们还有引号、标题和层次结构的标题。
- en: Some parts of the text may be related to distant parts of the sequence, rather
    than their immediate context. For example, a fictional story can introduce a person
    by their name but later refer to them only as *he* or *she*. These references
    can be separated by long text sequences, yet, we still have to be able to find
    this relation.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本的某些部分可能与序列中较远的部分有关，而不是它们的直接上下文。例如，一篇虚构故事可能会先介绍一个人名，但随后只用*他*或*她*来提及。这些指代可能被长篇文本序列分隔开，但我们仍然需要能够找到这种关系。
- en: The complexity of natural text requires several pre-processing steps before
    the actual NN model comes into play. The first step is **normalization**, which
    involves operations such as removing extra whitespace and converting all letters
    into lowercase. The next steps are not as straightforward, so we’ll dedicate the
    next two sections to them.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 自然文本的复杂性要求在实际神经网络模型发挥作用之前，进行几步预处理。第一步是**归一化**，包括去除多余的空白字符和将所有字母转换为小写。接下来的步骤并不像前面那样简单，因此我们将专门用接下来的两节来讨论这些步骤。
- en: Tokenization
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词
- en: One intuitive way to approach an NLP task is to split the corpus into words,
    which will represent the basic input units of our model. However, using words
    as input is not set in stone and we can use other elements, such as individual
    characters, phrases, or even whole sentences. The generic term for these units
    is **tokens**. A token refers to a text corpus in the same way as a pixel refers
    to an image. The process of splitting the corpus into tokens is called **tokenization**
    (what a surprise!). The entity
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一种直观的处理自然语言处理任务的方法是将语料库拆分为单词，这些单词将代表我们模型的基本输入单元。然而，使用单词作为输入并不是固定不变的，我们还可以使用其他元素，比如单个字符、短语，甚至整个句子。这些单元的通用术语是**标记**。标记指代文本语料库的方式，就像像素指代图像一样。将语料库拆分成标记的过程被称为**标记化**（真是意外！）。实体
- en: (for example, an algorithm) that performs this tokenization is called a **tokenizer**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: （例如，执行这种标记化的算法）称为**标记器**。
- en: Note
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The tokenizers we’ll discuss in this section are universal in the sense that
    they can work with different NLP ML algorithms. Therefore, the pre-processing
    algorithms in this section are commonly used with transformer models, which we’ll
    introduce in [*Chapter 7*](B19627_07.xhtml#_idTextAnchor202).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节中讨论的标记器是通用的，意味着它们可以与不同的自然语言处理机器学习算法配合使用。因此，本节中的预处理算法通常用于变换器模型，我们将在[*第7章*](B19627_07.xhtml#_idTextAnchor202)中介绍这些模型。
- en: 'With that, let’s discuss the types of tokenizers:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论几种标记器的类型：
- en: '**Word-based**: Each word represents a unique token. This is the most intuitive
    type of tokenization, but it has serious drawbacks. For example, the words *don’t*
    and *do not* will be represented by different tokens, but they mean the same thing.
    Another example is the words *car* and *cars* or *ready* and *readily*, which
    will be represented by different tokens, whereas a single token would be more
    appropriate. Because natural language is so diverse, there are many corner cases
    like these. The issue isn’t just that semantically similar words will have unrelated
    tokens, but also the large number of unique tokens that come out of this. This
    will make the model computationally inefficient. It will also produce many tokens
    with a small number of occurrences, which will prove challenging for the model
    to learn. Finally, we might encounter unknown words in a new text corpus.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于词**：每个单词代表一个独特的标记。这是最直观的标记化方式，但也有严重的缺点。例如，单词*don’t*和*do not*将被表示为不同的标记，但它们的含义是相同的。另一个例子是单词*car*和*cars*，或*ready*和*readily*，它们会被表示为不同的标记，而一个单一的标记会更合适。由于自然语言如此多样，像这样的特殊情况非常多。问题不仅仅在于语义相似的单词会有不同的标记，还在于由此产生的大量唯一标记。这会导致模型计算效率低下。它还会产生许多出现次数较少的标记，这对模型的学习来说是一个挑战。最后，我们可能会遇到在新文本语料库中无法识别的单词。'
- en: '**Character-based**: Each character (letter, digit, punctuation, and so on)
    in the text is a unique token. In this way, we have fewer tokens, as the total
    number of characters is limited and finite. Since we know all the characters in
    advance, we cannot encounter unknown symbols.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于字符**：文本中的每个字符（字母、数字、标点符号等）都是一个独特的标记。通过这种方式，我们可以减少标记数量，因为字符的总数是有限的并且是有限的。由于我们事先知道所有的字符，因此不会遇到未知的符号。'
- en: However, this tokenization is less intuitive than the word-based model because
    a context composed of characters is less meaningful than a context based on words.
    While the number of unique tokens is relatively small, the total number of tokens
    in the corpus will be very large (equal to the total number of characters).
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，与基于词的模型相比，这种标记化方法不太直观，因为由字符组成的上下文比基于词的上下文意义较小。虽然唯一标记的数量相对较少，但语料库中的标记总数将非常庞大（等于字符总数）。
- en: '**Subword tokenization**: This is a two-step process that starts by splitting
    the corpus into words. The most obvious way to split the text is on whitespace.
    In addition, we can also split it on whitespace *and punctuation marks*. In NLP
    parlance, this step is known as **pre-tokenization**'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**子词标记化**：这是一个两步过程，首先将语料库分割成单词。分割文本最明显的方式是通过空格。此外，我们还可以通过空格*和标点符号*来分割文本。在自然语言处理术语中，这一步骤被称为**预标记化**。'
- en: (the prefix implies that tokenization will follow). Then, it preserves the frequently
    used words and decomposes the rare words into meaningful subwords, which are more
    frequent. For example, we can decompose the word *tokenization* into the core
    word *token* and the suffix *ization*, each with its own token. Then, when we
    encounter the word *carbonization*, we can decompose it into *carbon* and *ization*.
    In this way, we’ll have two occurrences of *ization* instead of a single occurrence
    of *tokenization* and *carbonization*. Subword tokenization also makes it possible
    to decompose unknown words into known tokens.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: （前缀意味着接下来会进行标记化）。然后，它保留常用词，并将稀有词拆解为更频繁的有意义子词。例如，我们可以将单词*tokenization*分解为核心词*token*和后缀*ization*，每个部分都有自己的标记。然后，当我们遇到*carbonization*这个词时，我们可以将其分解为*carbon*和*ization*。这样，我们会得到两个*ization*的实例，而不是一个*tokenization*和一个*carbonization*。子词标记化还使得可以将未知词分解为已知标记。
- en: Special service tokens
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 特殊服务标记。
- en: 'For the concept of tokenization to work, it introduces some service tokens.
    These include the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使标记化的概念起作用，它引入了一些服务性标记。以下是一些服务性标记：
- en: '**UNK**: Replaces unknown tokens in the corpus (think of rare words such as
    alphanumeric designations)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**UNK**：替换语料库中的未知标记（可以理解为稀有词汇，如字母数字标识符）。'
- en: '**EOS**: An end-of-sentence (or sequence) token'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EOS**：句子（或序列）结束标记。'
- en: '**BOS**: A beginning-of-sentence (or sequence) token'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BOS**：句子（或序列）开始标记。'
- en: '**SEP**: This separates two semantically different text sequences, such as
    question and answer'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SEP**：用来分隔两个语义上不同的文本序列，例如问题和答案。'
- en: '**PAD**: This is a padding token that is appended to an existing sequence so
    that it can reach some predefined length and fit in a fixed-length mini-batch.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PAD**：这是一个填充标记，它会附加到现有序列中，以便它可以达到某个预定义长度并适应固定长度的小批次。'
- en: For example, we can tokenize the sentence *I bought a product called FD543C*
    into *BOS I bought a product called UNK EOS PAD PAD* to fit a fixed input with
    a length of 10.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以将句子*I bought a product called FD543C*标记化为*BOS I bought a product called
    UNK EOS PAD PAD*，以适应长度为10的固定输入。
- en: Subword tokenization is the most popular type of tokenization because it combines
    the best features of character-based (smaller vocabulary size) and word-based
    (meaningful context) tokenization. In the next few sections, we’ll discuss some
    of the most popular subword tokenizers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 子词标记化是最流行的标记化方式，因为它结合了基于字符（较小的词汇量）和基于词语（有意义的上下文）标记化的最佳特性。在接下来的几个部分中，我们将讨论一些最流行的子词标记器。
- en: Byte-Pair Encoding and WordPiece
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 字节对编码和WordPiece。
- en: '**Byte-Pair Encoding** (**BPE**, Neural Machine Translation of Rare Words with
    Subword Units, [https://arxiv.org/abs/1508.07909](https://arxiv.org/abs/1508.07909))
    is a popular subword tokenization algorithm. As with other such tokenizers, it
    begins with pre-tokenization, which splits the corpus into words.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**字节对编码**（**BPE**，使用子词单元进行稀有词的神经机器翻译，[https://arxiv.org/abs/1508.07909](https://arxiv.org/abs/1508.07909)）是一种流行的子词标记化算法。与其他此类标记器一样，它从预标记化开始，将语料库拆分为单词。'
- en: 'Using this dataset as a starting point, BPE works in the following way:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以这个数据集为起点，BPE的工作方式如下：
- en: Start with an initial **base** (or **seed**) **vocabulary**, which consists
    of the individual characters of all words in the text corpus. Therefore, each
    word is a sequence of single-character tokens.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从初始的**基础**（或**种子**）**词汇**开始，该词汇由文本语料库中所有单词的单个字符组成。因此，每个单词都是一系列单字符标记。
- en: 'Repeat the following until the size of the token vocabulary reaches a certain
    maximum threshold:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复以下步骤，直到标记词汇的大小达到某个最大阈值：
- en: Find the pair of tokens (initially, these are single characters) that occur
    together most frequently and merge them into a new composite token.
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出最常一起出现的一对标记（最初这些是单个字符），并将它们合并成一个新的复合标记。
- en: Extend the existing token vocabulary with the new composite token.
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新的复合标记扩展现有的标记词汇。
- en: Update the tokenized text corpus with the new token structure.
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新的标记结构更新标记化的文本语料库。
- en: 'To understand BPE, let’s assume that our corpus consists of the following (imaginary)
    words: `{dab: 5, adab: 4, aab: 7, bub: 9, bun: 2}`. The digit following each word
    indicates the number of occurrences of that word in the text. And here is the
    same corpus, but split into tokens (that is, characters): `{(d, a, b): 5, (a,
    d, a, b): 4, (a, a, b): 7, (b, u, b): 9, (b, u, c): 2}`. Based on this, we can
    build our initial token vocabulary with occurrence counts for each token: `{b:
    36, a: 27, u: 11, d: 9, c: 2}`. The following list illustrates the first four
    merge operations:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '为了理解BPE，让我们假设我们的语料库包含以下（虚构的）单词：`{dab: 5, adab: 4, aab: 7, bub: 9, bun: 2}`。每个单词后面的数字表示该单词在文本中出现的次数。以下是相同的语料库，但已经按符号（即字符）拆分：`{(d,
    a, b): 5, (a, d, a, b): 4, (a, a, b): 7, (b, u, b): 9, (b, u, c): 2}`。基于此，我们可以构建我们的初始符号词汇表，每个符号的出现次数为：`{b:
    36, a: 27, u: 11, d: 9, c: 2}`。以下列表展示了前四次合并操作：'
- en: 'The most common pair of tokens is `(a, b)`, which occurs `freq((a, b)) = 5
    + 4 + 7 = 16` times. Therefore, we merge them, and the corpus becomes `{(d,` `):
    5, (a, d,` `): 4, (a,` `): 7, (b, u, b): 9, (b, u, c): 2}`. The new token vocabulary
    is `{b: 20,` `: 16, a: 11, u: 11, d: 9,` `c: 2}`.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '最常见的符号对是`(a, b)`，其出现次数为`freq((a, b)) = 5 + 4 + 7 = 16`次。因此，我们将它们合并，语料库变为`{(d,`
    `): 5, (a, d,` `): 4, (a,` `): 7, (b, u, b): 9, (b, u, c): 2}`。新的符号词汇表是`{b: 20,`
    `: 16, a: 11, u: 11, d: 9,` `c: 2}`。'
- en: 'The new most common token pair is `(b, u)` with `freq((b, u)) = 9 + 2 = 11`
    occurrences. Again, we proceed to combine them in a new token: `{(d, ab): 5, (a,
    d, ab): 4, (a, ab): 7, (``, b): 9, (``, c): 2}`. The updated token vocabulary
    is `{ab: 16, a: 11,` `: 11, b: 9, d: 9,` `c: 2}`.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '新的最常见的符号对是`(b, u)`，其`freq((b, u)) = 9 + 2 = 11`次出现。接着，我们将它们合并为一个新的符号：`{(d,
    ab): 5, (a, d, ab): 4, (a, ab): 7, (``, b): 9, (``, c): 2}`。更新后的符号词汇表是`{ab: 16,
    a: 11,` `: 11, b: 9, d: 9,` `c: 2}`。'
- en: 'The next token pair is `(d, ab)` and it occurs `freq((d, ab)) = 5 + 4 = 9`
    times. After combining them, the tokenized corpus becomes `{(``): 5, (a,` `):
    4, (a, ab): 7, (bu, b): 9, (bu, c): 2}`. The new token vocabulary is `{a: 11,
    bu: 11, b: 9,` `: 9, ab: 7,` `c: 2}`.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '下一个符号对是`(d, ab)`，其出现次数为`freq((d, ab)) = 5 + 4 = 9`次。合并后，符号化的语料库变为`{(``): 5,
    (a,` `): 4, (a, ab): 7, (bu, b): 9, (bu, c): 2}`。新的符号词汇表是`{a: 11, bu: 11, b: 9,`
    `: 9, ab: 7,` `c: 2}`。'
- en: 'The new pair of tokens is `(bu, b)` with nine occurrences. After merging them,
    the corpus becomes `{(dab): 5, (a, dab): 4, (a, ab): 7, (``): 9, (bu, c): 2}`,'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '新的符号对是`(bu, b)`，其出现次数为9次。将它们合并后，语料库变为`{(dab): 5, (a, dab): 4, (a, ab): 7, (``):
    9, (bu, c): 2}`，'
- en: 'and the token vocabulary becomes `{a: 11,` `: 9,` `: 9, ab: 7, bu: 2,` `c:
    2}`.'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '而符号词汇表变为`{a: 11,` `: 9,` `: 9, ab: 7, bu: 2,` `c: 2}`。'
- en: BPE stores all token-merge rules and their order and not just the final token
    vocabulary. During model inference, it applies the rules in the same order on
    the new unknown text to tokenize it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: BPE会存储所有符号合并规则及其顺序，而不仅仅是最终的符号词汇表。在模型推理过程中，它会按照相同的顺序将规则应用于新的未知文本，以对其进行符号化。
- en: End-of-word tokens
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 词尾符号
- en: The original BPE implementation appends a special end-of-word token, `<w/>`,
    at the end of each word – for example, the word `aab` becomes `aab<w/>`. Other
    implementations can place the special token at the beginning of the word, instead
    of the end. This makes it possible for the algorithm to distinguish between, say,
    the token `ab`, as presented in the word `ca<w/>`, and the same token in `a``<w/>`.
    In this way, the algorithm can restore the original corpus from the tokenized
    one (**de-tokenization**), which wouldn’t be possible otherwise. In this section,
    we have omitted the end-of-word token for clarity.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 原始BPE实现会在每个单词的末尾添加一个特殊的词尾符号`<w/>`，例如，单词`aab`变为`aab<w/>`。其他实现可以将该特殊符号放在单词的开头，而不是末尾。这使得算法能够区分，例如，单词`ca<w/>`中的符号`ab`，与`a``<w/>`中的相同符号。因此，算法可以从符号化后的语料库恢复出原始语料库（**去符号化**），否则是无法做到的。本节中，为了简洁起见，我们省略了词尾符号。
- en: Let’s recall that our base vocabulary includes all characters of the text corpus.
    If these are Unicode characters (which is the usual case), we could end up with
    a vocabulary of up to 150,000 tokens. And this is before we even start the token-merge
    process. One trick to solve this issue is with the help of **byte-level BPE**.
    Each Unicode character can be encoded with multiple (up to 4) bytes. Byte-level
    BPE initially splits the corpus into a sequence of bytes, instead of full-fledged
    Unicode characters. If a character is encoded with *n* bytes, the tokenizer will
    treat it as a sequence of *n* one-byte tokens. In this way, the size of the base
    vocabulary will always be 256 (the maximum unique values that we can store in
    a byte). In addition, byte-level BPE guarantees that we won’t encounter unknown
    tokens.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下，我们的基础词汇表包括文本语料库中的所有字符。如果这些是 Unicode 字符（这是通常的情况），我们最终可能会得到一个最多包含 150,000
    个词汇的词汇表。而且这还只是我们开始词汇合并过程之前的情况。解决这个问题的一个技巧是借助 **字节级 BPE**。每个 Unicode 字符可以使用多个（最多
    4 个）字节进行编码。字节级 BPE 最初将语料库拆分为字节序列，而不是完整的 Unicode 字符。如果一个字符使用 *n* 个字节编码，分词器将把它当作
    *n* 个单字节词汇进行处理。通过这种方式，基础词汇表的大小将始终为 256（字节中可以存储的最大唯一值）。此外，字节级 BPE 保证我们不会遇到未知的词汇。
- en: '**WordPiece** ([https://arxiv.org/abs/1609.08144](https://arxiv.org/abs/1609.08144))
    is another subword tokenization algorithm. It is similar to BPE but with one main
    difference. Like BPE, it starts with a base vocabulary of individual characters
    and then proceeds to merge them into new composite tokens. However, it defines
    the merge order based on a score, computed with the following formula (unlike
    BPE, which uses frequent co-occurrence):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**WordPiece** ([https://arxiv.org/abs/1609.08144](https://arxiv.org/abs/1609.08144))
    是另一种子词分词算法。它与 BPE 相似，但有一个主要区别。像 BPE 一样，它从单个字符的基础词汇表开始，然后将它们合并成新的复合词汇。然而，它根据一个得分来定义合并顺序，得分通过以下公式计算（与使用频繁共现的
    BPE 不同）：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mfenced
    open="(" close=")"><mfenced open="(" close=")"><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>1</mn></msub><mo>,</mo><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>2</mn></msub></mrow></mfenced></mfenced><mo>=</mo><mfrac><mrow><mi>f</mi><mi>r</mi><mi>e</mi><mi>q</mi><mfenced
    open="(" close=")"><mfenced open="(" close=")"><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>1</mn></msub><mo>,</mo><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>2</mn></msub></mrow></mfenced></mfenced></mrow><mrow><mi>f</mi><mi>r</mi><mi>e</mi><mi>q</mi><mfenced
    open="(" close=")"><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>1</mn></msub></mrow></mfenced><mo>×</mo><mi>f</mi><mi>r</mi><mi>e</mi><mi>q</mi><mfenced
    open="(" close=")"><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>2</mn></msub></mrow></mfenced></mrow></mfrac></mrow></mrow></math>](img/397.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mfenced
    open="(" close=")"><mfenced open="(" close=")"><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>1</mn></msub><mo>,</mo><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>2</mn></msub></mrow></mfenced></mfenced><mo>=</mo><mfrac><mrow><mi>f</mi><mi>r</mi><mi>e</mi><mi>q</mi><mfenced
    open="(" close=")"><mfenced open="(" close=")"><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>1</mn></msub><mo>,</mo><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>2</mn></msub></mrow></mfenced></mfenced></mrow><mrow><mi>f</mi><mi>r</mi><mi>e</mi><mi>q</mi><mfenced
    open="(" close=")"><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>1</mn></msub></mrow></mfenced><mo>×</mo><mi>f</mi><mi>r</mi><mi>e</mi><mi>q</mi><mfenced
    open="(" close=")"><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>2</mn></msub></mrow></mfenced></mrow></mfrac></mrow></mrow></math>](img/397.png)'
- en: In this way, the algorithm prioritizes the merging of pairs where the individual
    tokens are less frequent in the corpus. Let’s compare this approach with BPE,
    which merges tokens based only on the potential gains of the new token. In contrast,
    WordPiece balances the gain (the nominator in the formula) with the potential
    loss of the existing tokens (the denominator). This makes sense because the new
    token will exist instead of the old pair of tokens, rather than alongside them.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，算法优先合并那些在语料库中出现频率较低的词对。让我们将这种方法与 BPE 进行比较，BPE 仅根据新词汇的潜在增益来合并词汇。相比之下，WordPiece
    在增益（公式中的分子）和现有词汇的潜在损失（分母）之间进行平衡。这是有道理的，因为新词汇将取代旧的词对，而不是与它们并存。
- en: In-word tokens
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 内部词汇
- en: 'WordPiece adds a special *##* prefix to all tokens inside a word, except for
    the first. For example, it will tokenize the word *aab* as `[a, ##a, ##b]`. The
    token merge removes the *##* between the tokens. So, when we merge *##a* and *##b*,
    *aab* becomes `[``a, ##ab]`.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 'WordPiece 为单词中的所有标记添加一个特殊的 *##* 前缀，除了第一个。例如，它会将单词 *aab* 标记为 `[a, ##a, ##b]`。标记合并会去掉标记之间的
    *##*。因此，当我们合并 *##a* 和 *##b* 时，*aab* 会变成 `[``a, ##ab]`。'
- en: 'Unlike BPE, WordPiece only stores the final token vocabulary. When it tokenizes
    a new word, it finds the longest matching subword in the vocabulary and splits
    the word on it. For example, let’s assume that we want to split the word *abcd*
    with a token vocabulary of `[a, ##b, ##c, ##d, ab, ##cd, ##bcd]`. Following the
    new rule, WordPiece will first select the longest subword, *bcd*, and it will
    tokenize *abcd* as `[``a, ##bcd]`.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '与 BPE 不同，WordPiece 只存储最终的标记词汇。当它对新词进行标记时，它会在词汇中找到最长的匹配子词，并在此处分割单词。例如，假设我们想用标记词汇
    `[a, ##b, ##c, ##d, ab, ##cd, ##bcd]` 来分割单词 *abcd*。根据新规则，WordPiece 会首先选择最长的子词
    *bcd*，然后将 *abcd* 标记为 `[``a, ##bcd]`。'
- en: BPE and WordPiece are greedy algorithms – they will always merge tokens deterministically,
    based on frequency criteria. However, encoding the same text sequence with different
    tokens might be possible. This could act as regularization for a potential NLP
    algorithm. Next, we’ll introduce a tokenization technique that takes advantage
    of this.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: BPE 和 WordPiece 都是贪心算法——它们总是根据频率标准，确定性地合并标记。然而，使用不同的标记对相同的文本序列进行编码是可能的。这可能作为潜在
    NLP 算法的正则化方法。接下来，我们将介绍一种利用这一点的标记化技术。
- en: Unigram
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Unigram
- en: 'Unlike BPE and WordPiece, the **Unigram** (*Subword Regularization: Improving
    Neural Network Translation Models with Multiple Subword Candidates*, [https://arxiv.org/abs/1804.10959](https://arxiv.org/abs/1804.10959))
    algorithm starts with a large base vocabulary and progressively tries to reduce
    it. The initial base vocabulary is a union of all unique characters and the most
    common substrings of the corpus. One way to find the most common substrings is
    with BPE. The algorithm assumes that each token, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/113.png),
    occurs independently (hence the Unigram name). Because of this assumption, the
    probability of a token, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/399.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/400.png),
    is just the number of its occurrences divided by the total size of the rest of
    the corpus. Then, the probability of a sequence of tokens with length *M*, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>X</mml:mtext><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/401.png),
    is as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 与 BPE 和 WordPiece 不同，**Unigram**（*子词正则化：通过多个子词候选改进神经网络翻译模型*，[https://arxiv.org/abs/1804.10959](https://arxiv.org/abs/1804.10959)）算法从一个大词汇表开始，并逐步尝试将其缩减。初始词汇表是所有独特字符和语料库中最常见子串的并集。找到最常见子串的一种方法是使用
    BPE。该算法假设每个标记，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/113.png)，是独立发生的（因此得名
    Unigram）。基于这一假设，一个标记，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/399.png)，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/400.png)，的概率就是它出现的次数除以语料库其他部分的总大小。然后，长度为
    *M* 的标记序列，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>X</mml:mtext><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/401.png)，的概率如下：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><mi>P</mi><mfenced
    open="(" close=")"><msub><mi>x</mi><mi>i</mi></msub></mfenced></mrow></mrow><mo>,</mo><mo>∀</mo><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><mi>V</mi><mo>,</mo><mrow><munderover><mo>∑</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><mi>V</mi></mrow><mrow
    /></munderover><mrow><mi>P</mi><mfenced open="(" close=")"><msub><mi>x</mi><mi>i</mi></msub></mfenced></mrow></mrow><mo>=</mo><mn>1</mn></mrow></mrow></math>](img/402.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><mi>P</mi><mfenced
    open="(" close=")"><msub><mi>x</mi><mi>i</mi></msub></mfenced></mrow></mrow><mo>,</mo><mo>∀</mo><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><mi>V</mi><mo>,</mo><mrow><munderover><mo>∑</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><mi>V</mi></mrow><mrow
    /></munderover><mrow><mi>P</mi><mfenced open="(" close=")"><msub><mi>x</mi><mi>i</mi></msub></mfenced></mrow></mrow><mo>=</mo><mn>1</mn></mrow></mrow></math>](img/402.png)'
- en: Here, *V* is the full token vocabulary.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*V* 是完整的标记词汇表。
- en: Say that we have the same token sequence, *X*, and multiple token segmentation
    candidates, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/403.png),
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有相同的令牌序列，*X*，并且有多个令牌分割候选项，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/403.png)，
- en: 'for that sequence. The most probable segmentation candidate, *x**, for *X*
    is as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于该序列。最可能的分割候选项，*x**，对于*X*如下：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msup><mi>x</mi><mi
    mathvariant="normal">*</mi></msup><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><mi>x</mi><mi>P</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>,</mo><mi>x</mi><mo>∈</mo><mi>S</mi><mfenced
    open="(" close=")"><mi>X</mi></mfenced></mrow></mrow></math>](img/404.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msup><mi>x</mi><mi
    mathvariant="normal">*</mi></msup><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><mi>x</mi><mi>P</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>,</mo><mi>x</mi><mo>∈</mo><mi>S</mi><mfenced
    open="(" close=")"><mi>X</mi></mfenced></mrow></mrow></math>](img/404.png)'
- en: 'Let’s clarify this with an example. We’ll assume that our corpus consists of
    some (imaginary) words, `{dab: 5, aab: 7, bun: 4}`, where the digits indicate
    the number of occurrences of that word in the text. Our initial token vocabulary
    is a union of all unique characters and all possible substrings (the numbers indicate
    frequency): `{a: 19, b: 16, ab: 12, aa: 7, da: 5, d: 5, bu: 4, un: 4}`. The sum
    of all token frequencies is 19 + 16 + 12 + 7 + 5 + 5 + 4 + 4 = 72\. Then, the
    independent probability for each token is ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mn>72</mml:mn></mml:math>](img/405.png)
    – for example, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>a</mi><mo>)</mo><mo>=</mo><mn>19</mn><mo>/</mo><mn>72</mn><mo>=</mo><mn>0.264</mn></mrow></mrow></mrow></math>](img/406.png),'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们通过一个例子来澄清这一点。我们假设我们的语料库包含一些（假想的）单词，`{dab: 5, aab: 7, bun: 4}`，其中数字表示该单词在文本中的出现次数。我们的初始令牌词汇是所有唯一字符和所有可能子字符串的并集（数字表示频率）：`{a:
    19, b: 16, ab: 12, aa: 7, da: 5, d: 5, bu: 4, un: 4}`。所有令牌频率的总和为 19 + 16 + 12
    + 7 + 5 + 5 + 4 + 4 = 72。然后，每个令牌的独立概率为 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mn>72</mml:mn></mml:math>](img/405.png)
    – 例如，![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>a</mi><mo>)</mo><mo>=</mo><mn>19</mn><mo>/</mo><mn>72</mn><mo>=</mo><mn>0.264</mn></mrow></mrow></mrow></math>](img/406.png)，'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>a</mi><mi>b</mi><mo>)</mo><mo>=</mo><mn>12</mn><mo>/</mo><mn>72</mn><mo>=</mo><mn>0.167</mn></mrow></mrow></mrow></math>](img/407.png),
    and so on.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>a</mi><mi>b</mi><mo>)</mo><mo>=</mo><mn>12</mn><mo>/</mo><mn>72</mn><mo>=</mo><mn>0.167</mn></mrow></mrow></mrow></math>](img/407.png)，等等。'
- en: Our extended vocabulary presents us with the possibility to tokenize each sequence
    (we’ll focus on words for simplicity) in multiple ways. For example, we can represent
    *dab* as either `{d, a, b}`, `{da, b}`, or `{d, ab}`. Here, the probabilities
    for each candidate are P({d, a, b}) = P(d) * P(a) * P(b) = 0.07 * 0.264 * 0.222
    = 0.0041; ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mo>{</mo><mi>d</mi><mi>a</mi><mo>,</mo><mi>b</mi><mo>}</mo><mo>)</mo><mo>=</mo><mn>0.07</mn><mi
    mathvariant="normal">*</mi><mn>0.222</mn><mo>=</mo><mn>0.015</mn></mrow></mrow></mrow></math>](img/408.png);
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mo>{</mo><mi>d</mi><mo>,</mo><mi>a</mi><mi>b</mi><mo>}</mo><mo>)</mo><mo>=</mo><mn>0.07</mn><mi
    mathvariant="normal">*</mi><mn>0.167</mn><mo>=</mo><mn>0.012</mn></mrow></mrow></mrow></math>](img/409.png).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们扩展的词汇表使我们能够以多种方式对每个序列（为了简化起见，我们将重点放在单词上）进行分词。例如，我们可以将*dab*表示为`{d, a, b}`、`{da,
    b}`或`{d, ab}`。在这里，每个候选项的概率为P({d, a, b}) = P(d) * P(a) * P(b) = 0.07 * 0.264 *
    0.222 = 0.0041；![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mo>{</mo><mi>d</mi><mi>a</mi><mo>,</mo><mi>b</mi><mo>}</mo><mo>)</mo><mo>=</mo><mn>0.07</mn><mi
    mathvariant="normal">*</mi><mn>0.222</mn><mo>=</mo><mn>0.015</mn></mrow></mrow></mrow></math>](img/408.png)；![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mo>{</mo><mi>d</mi><mo>,</mo><mi>a</mi><mi>b</mi><mo>}</mo><mo>)</mo><mo>=</mo><mn>0.07</mn><mi
    mathvariant="normal">*</mi><mn>0.167</mn><mo>=</mo><mn>0.012</mn></mrow></mrow></mrow></math>](img/409.png)。
- en: The candidate with the highest probability is *x** = `{da, b}`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 概率最高的候选项是*x** = `{da, b}`。
- en: 'With that, here’s how Unigram tokenization works step by step:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，以下是单元字(token)分词法的逐步实现过程：
- en: Start with the initial large base vocabulary, *V*.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从初始的大型基础词汇表*V*开始。
- en: 'Repeat the following steps until the size of |*V*| reaches some minimum threshold
    value:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复以下步骤，直到|*V*|的大小达到某个最小阈值：
- en: Find the *l*-best tokenization candidates, *x**, for all words in the corpus
    with the help of the **Viterbi** algorithm ([https://en.wikipedia.org/wiki/Viterbi_algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm)
    – using this algorithm is necessary because this is a computationally intensive
    task). Taking *l* candidates, instead of one, makes it possible to sample different
    token sequences over the same text. You can think of this as a data augmentation
    technique over the input data, which provides additional regularization to the
    NLP algorithm. Once we have a tokenized corpus in this way, we can estimate the
    probabilities, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:math>](img/410.png),
    for all tokens of the current token vocabulary, *V*, with the help of an **expectation-minimization**
    algorithm ([https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)).
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**维特比**算法([https://en.wikipedia.org/wiki/Viterbi_algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm))，找到语料库中所有单词的*l*最佳分词候选项*x**。使用此算法是必要的，因为这是一项计算密集型任务。选择*l*个候选项，而不是一个，使得可以在相同文本上采样不同的词元序列。你可以将这看作是对输入数据的一种数据增强技术，它为NLP算法提供了额外的正则化。一旦我们以这种方式得到了一个分词后的语料库，就可以利用**期望最大化**算法([https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm))估计当前词汇表*V*中所有词元的概率![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:math>](img/410.png)。
- en: For each token, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/113.png),
    compute a special loss function, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/412.png),
    which determines how the likelihood of the corpus is reduced if we remove ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/115.png)
    from the token vocabulary.
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个标记，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/113.png)，计算一个特殊的损失函数，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/412.png)，它确定如果我们从标记词汇中移除![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/115.png)，语料库的概率如何减少。
- en: Sort the tokens by their ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/414.png)
    and preserve only the top *n* % of the tokens (for example, *n = 80*). Always
    preserve the individual characters to avoid unknown tokens.
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照它们的![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/414.png)排序，并只保留前*n*%的标记（例如，*n
    = 80*）。始终保留个别字符，以避免未知标记。
- en: This concludes our introduction to tokenization. Some of these techniques were
    developed alongside the transformer architecture and we’ll make the most use of
    them in the following chapters. But for now, let’s focus on another fundamental
    technique in the NLP pipeline.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对分词的介绍。这些技术中的一些是在Transformer架构出现时发展起来的，我们将在接下来的章节中充分利用它们。但现在，让我们集中讨论NLP管道中的另一项基础技术。
- en: Introducing word embeddings
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入词嵌入
- en: Now that we’ve learned how to tokenize the text corpus, we can proceed to the
    next step in the NLP data processing pipeline. For the sake of clarity, we’ll
    assume that we’ve tokenized the corpus into words, rather than subwords or characters
    (in this section, *word* and *token* are interchangeable).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何对文本语料库进行分词，我们可以继续NLP数据处理管道中的下一步。为了简便起见，我们假设我们已将语料库分割成单词，而不是子词或字符（在本节中，*单词*和*标记*是可以互换的）。
- en: 'One way to feed the words of the sequence as input to the NLP algorithm is
    with one-hot encoding. Our input vector will have the same size as the number
    of tokens in the vocabulary and each token will have a unique one-hot encoded
    representation. However, this approach has a few drawbacks, as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 将序列中的词作为输入传递给NLP算法的一种方法是使用独热编码。我们的输入向量的大小将与词汇中标记的数量相同，每个标记将具有唯一的独热编码表示。然而，这种方法有一些缺点，如下所示：
- en: '**Sparse inputs**: The one-hot representation consists of mostly zeros and
    a single value. If our NLP algorithm is an NN (and it is), this type of input
    will activate only a small portion of its weights per word. Because of this, we’ll
    need a large training set to include a sufficient number of training samples of
    each word of the vocabulary.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀疏输入**：独热编码表示大多数值为零，只有一个非零值。如果我们的NLP算法是神经网络（而且确实如此），这种类型的输入每个词只会激活其权重的一小部分。因此，我们需要一个大规模的训练集，以包含每个词汇中足够数量的训练样本。'
- en: '**Computational intensity**: The large size of the vocabulary will result in
    large input tensors, which require large NNs and more computational resources.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算强度**：词汇的庞大规模将导致输入张量很大，这需要更大的神经网络和更多的计算资源。'
- en: '**Impracticality**: Every time we add a new word to the vocabulary, we’ll increase
    its size. However, the size of the one-hot encoded input will also increase. Therefore,
    we’ll have to change the structure of our NN to accommodate the new size and we’ll
    perform additional training.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不切实际**：每次我们向词汇表中添加一个新单词时，词汇表的大小会增加。然而，独热编码的输入大小也会增加。因此，我们必须改变神经网络的结构以适应新的大小，并且需要进行额外的训练。'
- en: '**Lack of context**: Words such as *dog* and *wolf* are semantically similar,
    but the one-hot representation lacks a way to convey this similarity.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏上下文**：像*dog*和*wolf*这样的单词在语义上是相似的，但独热编码表示无法传达这种相似性。'
- en: 'In this section, we’ll try to solve these issues with the help of a lower-dimensional
    distributed representation of the words, known as **word embeddings** (*A Neural
    Probabilistic Language Model*, [http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)).
    The distributed representation is created by learning an embedding function that
    transforms the one-hot encoded words into a lower-dimensional space of word embeddings,
    as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过低维分布式表示法来解决这些问题，这种表示被称为**词嵌入**（*神经概率语言模型*，[http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)）。分布式表示是通过学习一个嵌入函数来创建的，该函数将独热编码的单词转化为低维的词嵌入空间，具体如下：
- en: '![Figure 6.1 – Words -> one-hot encoding -> word embedding vectors](img/B19627_06_1.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 词汇 -> 独热编码 -> 词嵌入向量](img/B19627_06_1.jpg)'
- en: Figure 6.1 – Words -> one-hot encoding -> word embedding vectors
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 词汇 -> 独热编码 -> 词嵌入向量
- en: Words from the vocabulary with size *V* are transformed into one-hot encoding
    vectors of size *V*. Then, an **embedding function** transforms this *V*-dimensional
    space into a distributed representation (vector) of a **fixed** size, *D* (here,
    *D=4*). This vector serves as input to the NLP algorithm. We can see that the
    fixed and smaller vector size solves the issues of sparsity, computational intensity,
    and impracticality we just described. Next, we’ll see how it solves the context
    issue.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从词汇表中，大小为*V*的单词被转化为大小为*V*的独热编码向量。然后，**嵌入函数**将这个*V*维空间转化为一个**固定**大小的分布式表示（向量），*D*（这里，*D=4*）。这个向量作为输入传递给NLP算法。我们可以看到，固定且较小的向量大小解决了我们刚才提到的稀疏性、计算强度和不切实际的问题。接下来，我们将看到它是如何解决上下文问题的。
- en: The embedding function learns semantic information about the words. It maps
    each word in the vocabulary to a continuous-valued vector representation – that
    is, the word embedding. Each word corresponds to a point in this embedding space,
    and different dimensions correspond to the grammatical or semantic properties
    of these words. The concept of embedding space is similar to the latent space
    representation, which we first discussed in the context of diffusion models in
    [*Chapter 5*](B19627_05.xhtml#_idTextAnchor146).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入函数学习关于单词的语义信息。它将词汇表中的每个单词映射到一个连续值向量表示——即词嵌入。每个单词在这个嵌入空间中对应一个点，不同的维度对应这些单词的语法或语义属性。嵌入空间的概念类似于潜在空间表示，我们在[*第
    5 章*](B19627_05.xhtml#_idTextAnchor146)中首次讨论了这一点，涉及到扩散模型。
- en: The goal is to ensure that the words close to each other in the embedding space
    have similar meanings. By *close to each other*, we mean a high value of the dot
    product (similarity) of their embedding vectors. In this way, the information
    that some words are semantically similar can be exploited by the ML algorithm.
    For example, it might learn that *fox* and *cat* are semantically related and
    that both *the quick brown fox* and *the quick brown cat* are valid phrases. A
    sequence of words can then be replaced with a sequence of embedding vectors that
    capture the characteristics of these words. We can use this sequence as a base
    for various NLP tasks. For example, a classifier trying to classify the sentiment
    of an article might be trained on previously learned word embeddings, instead
    of one-hot encoding vectors. In this way, the semantic information of the words
    becomes readily available for the sentiment classifier.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是确保在嵌入空间中彼此接近的词语具有相似的含义。这里所说的*接近*是指它们的嵌入向量的点积（相似度）值较高。通过这种方式，某些词语在语义上相似的信息可以被机器学习算法利用。例如，它可能会学到*fox*和*cat*在语义上是相关的，并且*the
    quick brown fox*和*the quick brown cat*都是有效的短语。然后，一个词语序列可以被一组嵌入向量所替代，这些向量捕捉了这些词语的特征。我们可以将这个序列作为各种自然语言处理（NLP）任务的基础。例如，试图对文章情感进行分类的分类器，可能会基于之前学到的词嵌入进行训练，而不是使用独热编码向量。通过这种方式，词语的语义信息可以轻松地为情感分类器所用。
- en: The mapping between one-hot representation and embedding vectors
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 独热表示与嵌入向量之间的映射
- en: Let’s assume that we have already computed the embedding vectors of each token.
    One way to implement the mapping between the one-hot representation and the actual
    embedding vector is with the help of a *V×D*-shaped matrix, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/415.png).
    We can think of the matrix rows as a lookup table, where each row represents one
    word embedding vector. It works thanks to the one-hot encoded input word, which
    is a vector of all zeros, except for the index of the word itself. Because of
    this, the input word, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/416.png),
    will only activate its unique row (vector) of weights, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/417.png),
    in ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/418.png).
    So, for each input sample (word), only the word’s embedding vector will participate.
    We can also think of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/415.png)
    as a weight matrix of a **fully connected** (**FC**) NN layer. In this way, we
    can embed the embeddings (get it?) as the first NN layer – that is, the NN takes
    the one-hot encoded token as input and the embedding layer transforms it into
    a vector. Then, the rest of the NN uses the embedding vector instead of the one-hot
    representation. This is a standard implementation across all deep learning libraries.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经计算出了每个词元的嵌入向量。一种实现一热编码表示与实际嵌入向量之间映射的方法是借助一个*V×D*形状的矩阵，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/415.png)。我们可以把矩阵的行看作查找表，其中每一行代表一个词的嵌入向量。这个过程之所以可行，是因为输入的词是经过一热编码的，这个向量中除了对应词的索引位置是1外，其它位置全为0。因此，输入的词，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/416.png)，将仅激活其对应的唯一行（向量）权重，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/417.png)，位于![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/418.png)中。因此，对于每一个输入样本（词），只有该词的嵌入向量会参与计算。我们还可以把![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/415.png)看作一个**全连接**（**FC**）神经网络层的权重矩阵。通过这种方式，我们可以将嵌入（明白了吗？）作为神经网络的第一层
    —— 即，神经网络将一热编码的词元作为输入，嵌入层将其转换为一个向量。然后，神经网络的其余部分使用嵌入向量而不是一热编码表示。这是所有深度学习库中常见的标准实现。
- en: The concept of word embeddings was first introduced more than 20 years ago but
    remains one of the central paradigms in NLP today. **Large language models** (**LLMs**),
    such as ChatGPT, use improved versions of word embeddings, which we’ll discuss
    in [*Chapter 7*](B19627_07.xhtml#_idTextAnchor202).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入的概念最早是在20多年前提出的，但至今仍是自然语言处理领域的核心范式之一。**大型语言模型**（**LLMs**），例如ChatGPT，使用的是改进版的词嵌入，我们将在[*第7章*](B19627_07.xhtml#_idTextAnchor202)中讨论。
- en: Now that we are familiar with embedding vectors, we’ll continue with the algorithm
    to obtain and compute them.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了嵌入向量，我们将继续进行获取和计算嵌入向量的算法。
- en: Word2Vec
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Word2Vec
- en: 'A lot of research has gone into creating better word embedding models, in particular
    by omitting to learn the probability function over sequences of words. One of
    the most popular ways to do this is with **Word2Vec** ([http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf),
    https://arxiv.org/abs/1301.3781, and https://arxiv.org/abs/1310.4546). It creates
    embedding vectors based on the context (surrounding words) of the word in focus.
    More specifically, the context is the *n* preceding and the *n* following words
    of the focus word. The following figure shows the context window as it slides
    across the text, surrounding different focus words:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 很多研究都致力于创建更好的词嵌入模型，特别是通过省略对单词序列的概率函数学习来实现。其中一种最流行的方法是**Word2Vec** ([http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf),
    https://arxiv.org/abs/1301.3781, 和 https://arxiv.org/abs/1310.4546)。它基于目标词的上下文（周围单词）创建嵌入向量。更具体地说，上下文是目标词前后的*n*个单词。下图展示了上下文窗口在文本中滑动，围绕不同的目标词：
- en: "![Figure 6.2 – A Word2Vec sliding context window with n=2\\. The same type\
    \ of context window applies to both CBOW and \uFEFFskip-gram](img/B19627_06_2.jpg)"
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – 一个 Word2Vec 滑动上下文窗口，n=2。相同类型的上下文窗口适用于 CBOW 和 skip-gram](img/B19627_06_2.jpg)'
- en: Figure 6.2 – A Word2Vec sliding context window with n=2\. The same type of context
    window applies to both CBOW and skip-gram
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 一个 Word2Vec 滑动上下文窗口，n=2。相同类型的上下文窗口适用于 CBOW 和 skip-gram
- en: 'Word2vec comes in two flavors: **Continuous Bag of Words** (**CBOW**) and **skip-gram**.
    We’ll start with CBOW and then we’ll continue with skip-gram.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 有两种版本：**连续词袋模型** (**CBOW**) 和 **skip-gram**。我们将从 CBOW 开始，然后继续讨论 skip-gram。
- en: CBOW
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CBOW
- en: 'CBOW predicts the most likely word given its context (surrounding words). For
    example, given the sequence *the quick _____ fox jumps*, the model will predict
    *brown*. It takes all words within the context window with equal weights and doesn’t
    consider their order (hence the *bag* in the name). We can train the model with
    the help of the following simple NN with input, hidden, and output layers:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW 根据上下文（周围单词）预测最可能的词。例如，给定序列 *the quick _____ fox jumps*，模型将预测 *brown*。它对上下文窗口内的所有单词赋予相等的权重，并且不考虑它们的顺序（因此名字中有“*bag*”）。我们可以借助以下简单的神经网络进行训练，该网络包含输入层、隐藏层和输出层：
- en: '![Figure 6.3 – A CBOW model NN](img/B19627_06_3.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – 一个 CBOW 模型神经网络](img/B19627_06_3.jpg)'
- en: Figure 6.3 – A CBOW model NN
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – 一个 CBOW 模型神经网络
- en: 'Here’s how the model works:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是模型的工作方式：
- en: The input is the one-hot-encoded word representation (its length is equal to
    the vocabulary size, *V*).
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入是一个独热编码的单词表示（其长度等于词汇表大小，*V*）。
- en: The embedding vectors are represented by the *input-to-hidden* matrix, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/420.png).
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入向量由*输入到隐藏*矩阵表示，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/420.png)。
- en: The embedding vectors of all context words are averaged to produce the output
    of the hidden network layer (there is no activation function).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有上下文单词的嵌入向量被平均以产生隐藏网络层的输出（没有激活函数）。
- en: The hidden activations serve as input to the output **Softmax** layer of size
    *V* (with the hidden-to-output weight matrix, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>×</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/421.png)),
    which predicts the most likely word to be found in the context (proximity) of
    the input words. The index with the highest activation represents the one-hot-encoded
    related word.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层激活值作为输入传递给**Softmax**输出层，大小为*V*（与隐藏到输出的权重矩阵，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>×</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/421.png))，用于预测最可能出现在输入词汇上下文（邻近）中的词汇。具有最高激活值的索引表示最相关的单词，采用独热编码表示。
- en: We’ll train the NN with gradient descent and backpropagation. The training set
    consists of (context and label) one-hot encoded pairs of words that appear close
    to each other in the text. For example, if part of the text is `[the, quick, brown,
    fox, jumps]` and *n=2*, the training tuples will include `([quick, brown], the),
    ([the, brown, fox], quick)`, `([the, quick, fox jumps], brown)`, and so on. Since
    we are only interested in the embeddings, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/422.png),
    we’ll discard the output NN weights, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup></mml:math>](img/423.png),
    when the training is finished.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用梯度下降和反向传播训练神经网络。训练集包含的是（上下文和标签）一对一的独热编码单词对，这些单词在文本中彼此接近。例如，如果文本的一部分是 `[the,
    quick, brown, fox, jumps]` 且 *n=2*，训练元组将包括 `([quick, brown], the)`，`([the, brown,
    fox], quick)`，`([the, quick, fox jumps], brown)` 等等。由于我们只关心词嵌入，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/422.png)，我们将在训练完成后丢弃输出神经网络的权重，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup></mml:math>](img/423.png)。
- en: CBOW will tell us which word is most likely to appear in a given context. This
    could be a problem for rare words. For example, given the context *The weather
    today is really _____*, the model will predict the word *beautiful* rather than
    *fabulous* (hey, it’s just an example). CBOW is several times faster to train
    than the skip-gram and achieves slightly better accuracy for frequent words.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW 会告诉我们在给定上下文中最可能出现的单词。这对于稀有词可能是一个问题。例如，给定上下文 *今天的天气真是_____*, 模型会预测单词 *beautiful*
    而不是 *fabulous*（嘿，这只是个例子）。CBOW 的训练速度是 skip-gram 的几倍，而且对于常见单词的准确度稍好。
- en: Skip-gram
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Skip-gram
- en: 'The skip-gram model can predict the context of a given input word (the opposite
    of CBOW). For example, the word *brown* will predict the words *The quick fox
    jumps*. Unlike CBOW, the input is a single one-hot encoded word vector. But how
    do we represent the context words in the output? Instead of trying to predict
    the whole context (all surrounding words) simultaneously, skip-gram transforms
    the context into multiple training pairs, such as `(fox, the)`, `(fox, quick)`,
    `(fox, brown)`, and `(fox, jumps)`. Once again, we can train the model with a
    simple single-layer NN:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Skip-gram 模型可以预测给定输入单词的上下文（与 CBOW 相反）。例如，单词 *brown* 会预测单词 *The quick fox jumps*。与
    CBOW 不同，输入是单一的独热编码单词向量。但如何在输出中表示上下文单词呢？Skip-gram 不试图同时预测整个上下文（所有周围单词），而是将上下文转化为多个训练对，例如
    `(fox, the)`，`(fox, quick)`，`(fox, brown)` 和 `(fox, jumps)`。再次强调，我们可以用一个简单的单层神经网络训练该模型：
- en: '![Figure 6.4 – A skip-gram model NN](img/B19627_06_4.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 6.4 – A skip-gram model NN](img/B19627_06_4.jpg)'
- en: Figure 6.4 – A skip-gram model NN
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 一个 Skip-gram 模型神经网络
- en: As with CBOW, the output is a softmax, which represents the one-hot-encoded
    most probable context word. The input-to-hidden weights, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/424.png),
    represent the word embeddings lookup table, and the hidden-to-output weights,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup></mml:math>](img/425.png),
    are only relevant during training. The hidden layer doesn’t have an activation
    function (that is, it uses linear activation).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 与CBOW一样，输出是一个softmax，表示独热编码的最可能上下文单词。输入到隐藏层的权重，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/424.png)，表示词嵌入查找表，隐藏到输出的权重，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup></mml:math>](img/425.png)，仅在训练过程中相关。隐藏层没有激活函数（即，它使用线性激活）。
- en: 'We’ll train the model with backpropagation (no surprises here). Given a sequence
    of words, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math>](img/426.png)![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math>](img/427.png),
    the objective of the skip-gram model is to maximize the average log probability,
    where *n* is the window size:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用反向传播训练模型（这里没有惊讶的地方）。给定一系列单词，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math>](img/426.png)![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math>](img/427.png)，skip-gram模型的目标是最大化平均对数概率，其中*n*是窗口大小：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>](img/428.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>](img/428.png)'
- en: 'The model defines the probability, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/429.png),
    as the following softmax formula:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型定义了概率，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/429.png)，如以下softmax公式所示：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mi>O</mi></msub><mo>|</mo><msub><mi>w</mi><mi>I</mi></msub></mrow></mfenced><mo>=</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    open="(" close=")"><mrow><msubsup><mi mathvariant="bold">v</mi><msub><mi>w</mi><mi>O</mi></msub><mrow><mo>′</mo><mi
    mathvariant="normal">⊤</mi></mrow></msubsup><msub><mi mathvariant="bold">v</mi><msub><mi>w</mi><mi>I</mi></msub></msub></mrow></mfenced></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>w</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></msubsup><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    open="(" close=")"><mrow><msubsup><mi mathvariant="bold">v</mi><mi>w</mi><mrow><mo>′</mo><mi
    mathvariant="normal">⊤</mi></mrow></msubsup><msub><mi mathvariant="bold">v</mi><msub><mi>w</mi><mi>I</mi></msub></msub></mrow></mfenced></mrow></mrow></mfrac></mrow></mrow></math>](img/430.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mi>O</mi></msub><mo>|</mo><msub><mi>w</mi><mi>I</mi></msub></mrow></mfenced><mo>=</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    open="(" close=")"><mrow><msubsup><mi mathvariant="bold">v</mi><msub><mi>w</mi><mi>O</mi></msub><mrow><mo>′</mo><mi
    mathvariant="normal">⊤</mi></mrow></msubsup><msub><mi mathvariant="bold">v</mi><msub><mi>w</mi><mi>I</mi></msub></msub></mrow></mfenced></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>w</mi><mo>=</mo><mn>1</mn></mml:math>](img/430.png)'
- en: In this example, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math>](img/431.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:math>](img/432.png)
    are the input and output words, and **v** and **v**’ are the corresponding word
    vectors in the input and output weight matrices, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/420.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>×</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/434.png),
    respectively (we keep the original notation of the paper). Since the NN doesn’t
    have a hidden activation function, its output value for one input/output word
    pair is simply the multiplication of the input word vector, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/435.png),
    and the output word vector, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/436.png)
    (hence the transpose operation).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math>](img/431.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:math>](img/432.png)
    是输入和输出单词，而**v**和**v**' 是输入和输出权重矩阵中的相应词向量，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/420.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>×</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/434.png)，分别表示（我们保留了论文中的原始符号）。由于神经网络没有隐藏激活函数，其对于一对输入/输出单词的输出值仅仅是输入词向量![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/435.png)
    和输出词向量 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/436.png)（因此需要进行转置操作）。
- en: The authors of the Word2Vec paper note that word representations cannot represent
    idiomatic phrases that are not compositions of individual words. For example,
    *New York Times* is a newspaper, and not just a natural combination of the meanings
    of *New*, *York*, and *Times*. To overcome this, the model can be extended to
    include whole phrases. However, this significantly increases the vocabulary size.
    And, as we can see from the preceding formula, the softmax denominator needs to
    compute the output vectors for all words of the vocabulary. Additionally, every
    weight of the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>×</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/434.png)
    matrix is updated on every training step, which slows the training.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 论文的作者指出，词表示无法表示那些不是由单个词组成的习语。例如，*New York Times* 是一家报纸，而不仅仅是 *New*、*York*
    和 *Times* 各自含义的自然组合。为了解决这个问题，模型可以扩展到包括完整的短语。然而，这会显著增加词汇表的大小。而且，正如我们从前面的公式中看到的，softmax
    的分母需要计算所有词汇的输出向量。此外，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>×</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/434.png)
    矩阵的每个权重都在每一步训练时被更新，这也减慢了训练速度。
- en: To solve this, we can replace the softmax operation with the so-called `(fox,
    brown)`), as well as *k* additional negative pairs (for example, `(fox, puzzle)`),
    where *k* is usually in the range of [5,20]. Instead of predicting the word that
    best matches the input word (softmax), we’ll simply predict whether the current
    pair of words is true or not. In effect, we convert the multinomial classification
    problem (classified as one of many classes) into a binary logistic regression
    (or binary classification) problem. By learning the distinction between positive
    and negative pairs, the classifier will eventually learn the word vectors in the
    same way, as with multinomial classification. In Word2Vec, the words for the negative
    pairs are drawn from a special distribution, which draws less frequent words more
    often, compared to more frequent ones.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以用所谓的 `(fox, brown)` 替代 softmax 操作，并添加 *k* 个额外的负样本对（例如，`(fox, puzzle)`），其中
    *k* 通常在 [5,20] 的范围内。我们不再预测最符合输入词的单词（softmax），而是直接预测当前的词对是否为真实的。实际上，我们将多项分类问题（从多个类别中选择一个）转化为二元逻辑回归（或二分类）问题。通过学习正负词对的区别，分类器最终会以与多项分类相同的方式学习词向量。在
    Word2Vec 中，负样本词是从一个特殊分布中抽取的，该分布比频繁的词更常抽取不常见的词。
- en: Some of the most frequent words to occur carry less information value compared
    to the rare words. Examples of such words are the definite and indefinite articles
    *a*, *an*, and *the*. The model will benefit more from observing the pairs *London*
    and *city* compared to *the* and *city* because almost all words co-occur frequently
    with *the*. The opposite is also true – the vector representations of frequent
    words do not change significantly after training on many examples. To counter
    the imbalance between the rare and frequent words, the authors of the paper propose
    a subsampling approach, where each word, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/114.png),
    of the training set is discarded with some probability, computed by the heuristic
    formula where
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 与稀有词相比，一些最常出现的词携带的信息量较少。此类词的例子包括定冠词和不定冠词 *a*、*an* 和 *the*。与 *the* 和 *city* 相比，模型从观察
    *London* 和 *city* 的搭配中获益更多，因为几乎所有的词都与 *the* 经常同时出现。反之亦然——在大量例子上训练后，频繁词的向量表示不会发生显著变化。为了应对稀有词和频繁词之间的不平衡，论文的作者提出了一种子采样方法，其中训练集中的每个词，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/114.png)，会以某个概率被丢弃，这个概率通过启发式公式计算得出。
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/439.png)
    is the frequency of word ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/15.png)
    and *t* is a threshold (usually around ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math>](img/441.png)):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/439.png)
    是单词 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>](img/15.png)
    的频率，*t*是一个阈值（通常约为 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math>](img/441.png)）：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><msub><mi>w</mi><mi>i</mi></msub></mfenced><mo>=</mo><mn>1</mn><mo>−</mo><msqrt><mfrac><mi>t</mi><mrow><mi>f</mi><mfenced
    open="(" close=")"><msub><mi>w</mi><mi>i</mi></msub></mfenced></mrow></mfrac></msqrt></mrow></mrow></math>](img/442.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><msub><mi>w</mi><mi>i</mi></msub></mfenced><mo>=</mo><mn>1</mn><mo>−</mo><msqrt><mfrac><mi>t</mi><mrow><mi>f</mi><mfenced
    open="(" close=")"><msub><mi>w</mi><mi>i</mi></msub></mfenced></mrow></mfrac></msqrt></mrow></mrow></math>](img/442.png)'
- en: It aggressively subsamples words with a frequency greater than *t* but also
    preserves the ranking of the frequencies.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 它会积极地对频率大于*t*的单词进行子采样，同时保持频率的排名。
- en: We can say that, in general, skip-gram performs better on rare words than CBOW,
    but it takes longer to train.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说，一般而言，跳字模型（skip-gram）在稀有词上的表现比CBOW更好，但训练时间更长。
- en: Now that we’ve learned about embedding vectors, let’s learn how to visualize
    them.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了嵌入向量，让我们学习如何可视化它们。
- en: Visualizing embedding vectors
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化嵌入向量
- en: 'A successful word embedding function will map semantically similar words to
    vectors with high dot product similarity in the embedding space. To illustrate
    this, we’ll implement the following steps:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一个成功的词嵌入函数将语义相似的单词映射到嵌入空间中具有高点积相似度的向量。为了说明这一点，我们将实现以下步骤：
- en: Train a Word2Vec skip-gram model on the `text8` dataset, which consists of the
    first 100,000,000 bytes of plain text from Wikipedia ([http://mattmahoney.net/dc/textdata.html](http://mattmahoney.net/dc/textdata.html)).
    Each embedding vector is 100-dimensional, which is the default value for this
    type of model.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`text8`数据集上训练Word2Vec跳字模型，该数据集包含来自维基百科的前1亿字节的纯文本（[http://mattmahoney.net/dc/textdata.html](http://mattmahoney.net/dc/textdata.html)）。每个嵌入向量是100维的，这是该类型模型的默认值。
- en: Select a list of *seed* words. In this case, the words are *mother*, *car*,
    *tree*, *science*, *building, elephant*, and *green*.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个*种子*词列表。在此案例中，词语包括*mother*、*car*、*tree*、*science*、*building*、*elephant*和*green*。
- en: Compute the dot-product similarity between the Word2Vec embedding vector of
    each seed word and the embedding vectors of all other words in the vocabulary.
    Then, select a cluster of the top-*k* (in our case, *k=5*) similar words (based
    on their dot-product similarity) for each seed word.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个种子词的Word2Vec嵌入向量与词汇表中所有其他单词嵌入向量之间的点积相似度。然后，为每个种子词选择一组前*k*（在我们的例子中，*k=5*）个最相似的单词（基于它们的点积相似度）。
- en: 'Visualize the similarity between the seed embeddings and the embeddings of
    their respective clusters of similar words in a 2D plot. Since the embeddings
    are 100-dimensional, we’ll use the t-SNE ([https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding))
    dimensionality-reduction algorithm. It maps each high-dimensional embedding vector
    on a two- or three-dimensional point in a way where similar objects are modeled
    on nearby points and dissimilar objects are modeled on distant points with a high
    probability. We can see the result in the following scatterplot:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在二维图中可视化种子嵌入与其相似词汇聚类嵌入之间的相似性。由于嵌入是100维的，我们将使用t-SNE（[https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)）降维算法。它将每个高维嵌入向量映射到二维或三维点，方法是将相似的对象建模为邻近点，而将不相似的对象建模为距离较远的点，且这种建模方式具有较高的概率。我们可以在下面的散点图中看到结果：
- en: '![Figure 6.5 – t-SNE visualization of the seed words and their clusters of
    the most similar words](img/B19627_06_5.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5 – t-SNE 可视化种子词及其最相似词汇的聚类](img/B19627_06_5.jpg)'
- en: Figure 6.5 – t-SNE visualization of the seed words and their clusters of the
    most similar words
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – t-SNE 可视化种子词及其最相似词汇的聚类
- en: This graph proves that the obtained word vectors contain relevant information
    for the words.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 该图证明了所获得的词向量包含了与单词相关的信息。
- en: Word2Vec (and similar models) create **static** (or **context independent**)
    **embeddings**. Each word has a single embedding vector, based on all occurrences
    (that is, all contexts) of that word in the text corpus. This imposes some limitations.
    For example, *bank* has a different meaning in different contexts, such as *river
    bank*, *savings bank*, and *bank holiday*. Despite this, it is represented with
    a single embedding. In addition, the static embedding doesn’t take into account
    the word order in the context. For example, the expressions *I like apples, but
    I don’t like oranges* and *I like oranges, but I don’t like apples* have opposite
    meanings, but Word2Vec interprets them as the same. We can solve these problems
    with the so-called **dynamic** (**context dependent**) **embeddings**, which we’ll
    discuss in [*Chapter 7*](B19627_07.xhtml#_idTextAnchor202).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec（和类似的模型）创建**静态**（或**上下文无关**）**嵌入**。每个单词都有一个单一的嵌入向量，基于该单词在文本语料库中的所有出现（即所有上下文）。这带来了一些局限性。例如，*bank*在不同的上下文中有不同的含义，如*river
    bank*（河岸）、*savings bank*（储蓄银行）和*bank holiday*（银行假日）。尽管如此，它还是通过单一的嵌入进行表示。此外，静态嵌入没有考虑上下文中的单词顺序。例如，表达式*I
    like apples, but I don’t like oranges*（我喜欢苹果，但我不喜欢橙子）和*I like oranges, but I don’t
    like apples*（我喜欢橙子，但我不喜欢苹果）具有相反的含义，但Word2Vec将它们视为相同的句子。我们可以通过所谓的**动态**（**上下文相关**）**嵌入**来解决这些问题，后者将在[*第7章*](B19627_07.xhtml#_idTextAnchor202)中讨论。
- en: So far, we’ve focused on single words (or tokens). Next, we’ll expand our scope
    to text sequences.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直专注于单个单词（或标记）。接下来，我们将扩展我们的研究范围，探索文本序列。
- en: Language modeling
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型
- en: A word-based **language model** (**LM**) defines a probability distribution
    over sequences of **tokens**. For this section, we’ll assume that the tokens are
    words. Given a sequence of words of length *m* (for example, a sentence), an LM
    assigns a probability, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/443.png),
    that the full sequence of words could exist. One application of these probabilities
    is a generative model to create new text – a word-based LM can compute the likelihood
    of the next word, given an existing sequence of words that precede it. Once we
    have this new word, we can append it to the existing sequence and predict yet
    another new word, and so on. In this way, we can generate new text sequences with
    arbitrary length. For example, given the sequence *the quick brown*, the LM might
    predict *fox* as the next most likely word. Then, the sequence becomes *the quick
    brown fox*, and we task the LM to predict the new most likely word based on the
    updated sequence. A model whose output depends on its previous values, as well
    as its stochastic (that is, with some randomness) output (new value), is called
    an **autoregressive model**.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 基于词的**语言模型**（**LM**）定义了一个词汇序列的概率分布。对于本节内容，我们假设这些词元是单词。给定一个长度为*m*（例如，一个句子）的单词序列，语言模型会为该序列分配一个概率，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/443.png)，表示这个完整的单词序列可能存在。这些概率的一种应用是生成模型，用于创建新文本——基于词的语言模型可以计算出下一个单词的可能性，前提是已知前面的单词序列。一旦我们得到了这个新单词，就可以将它添加到现有序列中，接着预测下一个新单词，依此类推。通过这种方式，我们可以生成任意长度的新文本序列。例如，给定序列*the
    quick brown*，语言模型可能会预测*fox*作为下一个最可能的单词。然后，序列变成*the quick brown fox*，我们再次让语言模型基于更新后的序列预测新的最可能单词。输出依赖于先前值以及其随机性（即带有一定随机性）的输出（新值）的模型，被称为**自回归模型**。
- en: Next, we’ll focus on the properties of the word sequence, rather than the model.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将重点关注词序列的属性，而不是模型本身。
- en: Note
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Even the most advanced LLMs, such as ChatGPT, are autoregressive models – they
    just predict the next word, one word at a time.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是最先进的LLM，例如ChatGPT，也是自回归模型——它们每次只预测下一个单词。
- en: Understanding N-grams
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解N-gram模型
- en: 'The inference of the probability of a long sequence, say ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math>](img/444.png),
    is typically infeasible. To understand why, let’s note that we can calculate the
    joint probability of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/445.png)
    with the chain rule of joint probability ([*Chapter 2*](B19627_02.xhtml#_idTextAnchor047)):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 推断长序列的概率，例如![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/444.png)，通常是不可行的。为了理解原因，我们可以注意到，利用联合概率链式法则可以计算出![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/445.png)。
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>…</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/446.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>…</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/446.png)  '
- en: 'The probability of the later words given the earlier words would be especially
    difficult to estimate from the data. That’s why this joint probability is typically
    approximated by an independence assumption that the *i*-th word is only dependent
    on the *n-1* previous words. We’ll only model the joint probabilities of combinations
    of *n* sequential words, called *n*-grams. For example, in the phrase *the quick
    brown fox*, we have the following *n*-grams:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '给定前面的单词，后面的单词的概率尤其难以从数据中估计。这就是为什么这种联合概率通常通过独立性假设来近似，假设第 *i* 个单词仅依赖于前面 *n-1*
    个单词。我们只会对 *n* 个连续单词的组合进行联合概率建模，这些组合称为 *n*-grams。例如，在短语 *the quick brown fox* 中，我们有以下
    *n*-grams：  '
- en: '**1-gram** (**unigram**): *the*, *quick*, *brown*, and *fox* (this is where
    Unigram tokenization takes its name)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1-gram** (**unigram**)：*the*、*quick*、*brown* 和 *fox*（这就是 unigram 分词法的来源）'
- en: '**2-gram** (**bigram**): *the quick*, *quick brown*, and *brown fox*'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2-gram** (**bigram**)：*the quick*、*quick brown* 和 *brown fox*  '
- en: '**3-gram** (**trigram**): *the quick brown* and *quick* *brown fox*'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3-gram** (**trigram**)：*the quick brown* 和 *quick* *brown fox*  '
- en: '**4-gram**: *the quick* *brown fox*'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**4-gram**：*the quick* *brown fox*  '
- en: Note
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '注意  '
- en: The term *n*-grams can refer to other types of sequences of length *n*, such
    as *n* characters.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*-gram 术语可以指其他长度为 *n* 的序列类型，例如 *n* 个字符。  '
- en: 'The inference of the joint distribution is approximated with the help of *n*-gram
    models that split the joint distribution into multiple independent parts. If we
    have a large corpus of text, we can find all the *n*-grams up until a certain
    *n* (typically 2 to 4) and count the occurrence of each *n*-gram in that corpus.
    From these counts, we can estimate the probabilities of the last word of each
    *n*-gram, given the previous *n-1* words:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 联合分布的推断通过 *n*-gram 模型来逼近，该模型将联合分布分割成多个独立部分。如果我们有大量的文本语料库，可以找到所有 *n*-gram，直到某个
    *n*（通常为 2 到 4），并统计每个 *n*-gram 在该语料库中的出现次数。通过这些计数，我们可以估算每个 *n*-gram 最后一个单词的概率，前提是给定前
    *n-1* 个单词：
- en: 'Unigram: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mtext>total number of words in the corpus</mml:mtext></mml:mrow></mml:mfrac></mml:math>](img/447.png)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Unigram: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mtext>语料库中单词的总数</mml:mtext></mml:mrow></mml:mfrac></mml:math>](img/447.png)'
- en: 'Bigram: ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfenced><mo>=</mo><mstyle
    scriptlevel="+1"><mfrac><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>w</mi><mi>i</mi></msub></mrow></mfenced></mrow><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mfenced
    open="(" close=")"><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mfenced></mrow></mfrac></mstyle></mrow></mrow></math>](img/448.png)'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bigram: ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfenced><mo>=</mo><mstyle
    scriptlevel="+1"><mfrac><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>w</mi><mi>i</mi></msub></mrow></mfenced></mrow><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mfenced
    open="(" close=")"><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mfenced></mrow></mfrac></mstyle></mrow></mrow></math>](img/448.png)'
- en: '*n*-gram: ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mrow><mi>n</mi><mo>+</mo><mi>i</mi></mrow></msub><mo>|</mo><msub><mi>w</mi><mi>n</mi></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>w</mi><mrow><mi>n</mi><mo>+</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfenced><mo>=</mo><mstyle
    scriptlevel="+1"><mfrac><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mi>n</mi></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>w</mi><mrow><mi>n</mi><mo>+</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>w</mi><mrow><mi>n</mi><mo>+</mo><mi>i</mi></mrow></msub></mrow></mfenced></mrow><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mi>n</mi></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>w</mi><mrow><mi>n</mi><mo>+</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfenced></mrow></mfrac></mstyle></mrow></mrow></math>](img/449.png)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n*-gram: ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mrow><mi>n</mi><mo>+</mo><mi>i</mi></mrow></msub><mo>|</mo><msub><mi>w</mi><mi>n</mi></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>w</mi><mrow><mi>n</mi><mo>+</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfenced><mo>=</mo><mstyle
    scriptlevel="+1"><mfrac><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mi>n</mi></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>w</mi><mrow><mi>n</mi><mo>+</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>w</mi><mrow><mi>n</mi><mo>+</mo><mi>i</mi></mrow></msub></mrow></mfenced></mrow><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mi>n</mi></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>w</mi><mrow><mi>n</mi><mo>+</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfenced></mrow></mfrac></mstyle></mrow></mrow></math>](img/449.png)'
- en: The independent assumption that the *i*-th word is only dependent on the previous
    *n-1* words can now be used to approximate the joint distribution.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 假设独立性，即第 *i* 个单词只依赖于前 *n-1* 个单词，现在可以用来逼近联合分布。
- en: 'For example, we can approximate the joint distribution for a unigram with the
    following formula:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以通过以下公式近似单元语法的联合分布：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>…</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/450.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>…</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/450.png)'
- en: 'For a trigram, we can approximate the joint distribution with the following
    formula:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 对于三元组，我们可以通过以下公式近似联合分布：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>…</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/451.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>…</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/451.png)'
- en: We can see that, based on the vocabulary size, the number of *n*-grams grows
    exponentially with *n*. For example, if a small vocabulary contains 100 words,
    then the number of possible 5-grams would be ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mn>100</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>10,000,000,000</mml:mn></mml:math>](img/452.png)
    different 5-grams. In comparison, the entire works of Shakespeare contain around
    30,000 different words, illustrating the infeasibility of using *n*-grams with
    a large *n*. Not only is there the issue of storing all the probabilities, but
    we would also need a very large text corpus to create decent *n*-gram probability
    estimations for larger values of *n*.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，基于词汇表的大小，*n*-gram的数量随着*n*的增加呈指数增长。例如，如果一个小型词汇表包含100个词，那么可能的5-gram数量将是
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mn>100</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>10,000,000,000</mml:mn></mml:math>](img/452.png)
    种不同的5-gram。相比之下，莎士比亚的全部作品包含大约30,000个不同的单词，这说明使用大*n*的*n*-gram是不可行的。不仅存在存储所有概率的问题，而且我们还需要一个非常大的文本语料库，才能为更大的*n*值创建合理的*n*-gram概率估计。
- en: The curse of dimensionality
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 高维诅咒
- en: When the number of possible input variables (words) increases, the number of
    different combinations of these input values increases exponentially. This problem
    is known as the curse of dimensionality. It arises when the learning algorithm
    needs at least one example per relevant combination of values, which is the case
    in *n*-gram modeling. The larger our *n*, the better we can approximate the original
    distribution and the more data we would need to make good estimations of the *n*-gram
    probabilities.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 当可能的输入变量（单词）数量增加时，这些输入值的不同组合数量会呈指数增长。这个问题被称为维度灾难。当学习算法需要每个相关值组合至少一个例子时，就会出现这种情况，这正是*n*-gram建模中所面临的问题。我们的*n*越大，就能越好地逼近原始分布，但我们需要更多的数据才能对*n*-gram概率进行良好的估计。
- en: But fret not, as the *n*-gram LM gives us some important clues on how to proceed.
    Its theoretical formulation is sound, but the curse of dimensionality makes it
    unfeasible. In addition, the *n*-gram model reinforces the importance of the word
    context, just as with Word2Vec. In the next few sections, we’ll learn how to simulate
    an *n*-gram model probability distribution with the help of NNs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 但不用担心，因为*n*-gram语言模型给了我们一些重要线索，帮助我们继续前进。它的理论公式是可靠的，但维度灾难使其不可行。此外，*n*-gram模型强调了单词上下文的重要性，就像Word2Vec一样。在接下来的几节中，我们将学习如何借助神经网络模拟*n*-gram模型的概率分布。
- en: Introducing RNNs
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍RNN
- en: An RNN is a type of NN that can process sequential data with variable length.
    Examples of such data include text sequences or the price of a stock at various
    moments in time. By using the word *sequential*, we imply that the sequence elements
    are related to each other and their order matters. For example, if we take a book
    and randomly shuffle all the words in it, the text will lose its meaning, even
    though we’ll still know the individual words.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: RNN是一种可以处理具有可变长度的顺序数据的神经网络。此类数据的例子包括文本序列或某股票在不同时间点的价格。通过使用*顺序*一词，我们意味着序列元素彼此相关，且它们的顺序很重要。例如，如果我们把一本书中的所有单词随机打乱，文本将失去意义，尽管我们仍然能够知道每个单独的单词。
- en: 'RNNs get their name because they apply the same function over a sequence recurrently.
    We can define an RNN as a recurrence relation:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: RNN得名于它对序列应用相同函数的递归方式。我们可以将RNN定义为递归关系：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/453.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/453.png)'
- en: 'Here, *f* is a differentiable function, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png)
    is a vector of values called internal RNN state (at step *t*), and ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png)
    is the network input at step *t*. Unlike regular NNs, where the state only depends
    on the current input (and RNN weights), here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png)
    is a function of both the current input, as well as the previous state, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/457.png).
    You can think of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/458.png)
    as the RNN’s summary of all previous inputs. The recurrence relation defines how
    the state evolves step by step over the sequence via a feedback loop over previous
    states, as illustrated in the following diagram:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*f* 是一个可微分的函数，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png)
    是称为内部 RNN 状态的值向量（在步骤 *t* 处），而 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png)
    是步骤 *t* 处的网络输入。与常规的神经网络不同，常规神经网络的状态只依赖于当前的输入（和 RNN 权重），而在这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png)
    是当前输入以及先前状态 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/457.png)
    的函数。你可以把 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/458.png)
    看作是 RNN 对所有先前输入的总结。递归关系定义了状态如何在序列中一步一步地演变，通过对先前状态的反馈循环，如下图所示：
- en: '![Figure 6.6 – An unfolded RNN](img/B19627_06_6.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 6.6 – An unfolded RNN](img/B19627_06_6.jpg)'
- en: Figure 6.6 – An unfolded RNN
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – 展开的 RNN
- en: On the left, we have a visual illustration of the RNN recurrence relation. On
    the right, we have the RNN states recurrently unfolded over the sequence *t-1*,
    *t*, *t+1*.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧展示了 RNN 递归关系的可视化示意图，右侧展示了 RNN 状态在序列 *t-1*、*t*、*t+1* 上的递归展开。
- en: 'The RNN has three sets of parameters (or weights), shared between all steps:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 有三组参数（或权重），这些参数在所有步骤之间共享：
- en: '**U**: Transforms the input, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png),
    into the state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**U**：将输入，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png)，转换为状态，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png)'
- en: '**W**: Transforms the previous state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/457.png),
    into the current state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**W**：将前一个状态，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/457.png)，转化为当前状态，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png)'
- en: '**V**: Maps the newly computed internal state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png),
    to the output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/464.png)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**V**：将新计算出的内部状态，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png)，映射到输出，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/464.png)'
- en: '**U**, **V**, and **W** apply linear transformation over their respective inputs.
    The most basic case of such a transformation is the familiar FC operation we know
    and love (therefore, **U**, **V**, and **W** are weight matrices). We can now
    define the internal state and the RNN output as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**U**、**V** 和 **W** 对各自的输入应用线性变换。最基本的这种变换就是我们熟知并喜爱的全连接（FC）操作（因此，**U**、**V**
    和 **W** 是权重矩阵）。我们现在可以定义内部状态和 RNN 输出如下：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi
    mathvariant="bold">W</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/465.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi
    mathvariant="bold">W</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/465.png)'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi
    mathvariant="bold">V</mml:mi></mml:math>](img/466.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi
    mathvariant="bold">V</mml:mi></mml:math>](img/466.png)'
- en: Here, *f* is the non-linear activation function (such as tanh, sigmoid, or ReLU).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*f* 是非线性激活函数（如 tanh、sigmoid 或 ReLU）。
- en: For example, in a word-level LM, the input, *x*, will be a sequence of word
    embedding vectors (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/467.png)).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个基于单词的语言模型（LM）中，输入 *x* 将是一个词嵌入向量的序列 (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/467.png))。
- en: The state, *s*, will be a sequence of state vectors (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/468.png)).
    Finally, the output, *y*, will be a sequence of probability vectors (![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/469.png))
    of the next words in the sequence.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 状态 *s* 将是一个状态向量的序列 (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/468.png))。最后，输出
    *y* 将是下一个单词序列的概率向量 (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/469.png))。
- en: Note that in an RNN, each state is dependent on all previous computations via
    this recurrence relation. An important implication of this is that RNNs have memory
    over time because the states, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png),
    contain information based on the previous steps. In theory, RNNs can remember
    information for an arbitrarily long period, but in practice, they are limited
    to looking back only a few steps. We will address this issue in more detail in
    the *Vanishing and exploding* *gradients* section.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在一个递归神经网络（RNN）中，每个状态依赖于通过递归关系的所有先前计算。这个重要的含义是，RNN 能够随着时间记忆，因为状态 ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png)
    包含基于之前步骤的信息。从理论上讲，RNN 可以记住信息很长一段时间，但实际上它们只能回溯几步。我们将在 *消失和爆炸* *梯度* 部分详细讨论这个问题。
- en: 'The RNN we described is somewhat equivalent to a single-layer regular NN (with
    an additional recurrence relation). But as with regular NNs, we can stack multiple
    RNNs to form a **stacked RNN**. The cell state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/471.png),
    of an RNN cell at level *l* at time *t* will take the output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math>](img/472.png),
    of the RNN cell from level *l-1* and the previous cell state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/473.png),
    of the cell at the same level *l* as the input:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述的 RNN 在某种程度上等同于一个单层常规神经网络（NN），但带有额外的递归关系。和常规神经网络一样，我们可以堆叠多个 RNN 来形成 **堆叠
    RNN**。在时间 *t* 时，RNN 单元在第 *l* 层的单元状态 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/471.png)
    将接收来自第 *l-1* 层的 RNN 单元的输出 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math>](img/472.png)
    以及该层相同层级 *l* 的先前单元状态 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/473.png)。
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math>](img/474.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math>](img/474.png)'
- en: 'In the following diagram, we can see an unfolded, stacked RNN:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到展开的堆叠 RNN：
- en: '![Figure 6.7 – Stacked RNN](img/B19627_06_7.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – 堆叠 RNN](img/B19627_06_7.jpg)'
- en: Figure 6.7 – Stacked RNN
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – 堆叠 RNN
- en: 'Because RNNs are not limited to processing fixed-size inputs, they expand the
    possibilities of what we can compute with NNs. We can identify several types of
    tasks, based on the relationship between the input and output sizes:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 RNN 不仅限于处理固定大小的输入，它们扩展了我们可以通过神经网络（NNs）计算的可能性。根据输入和输出大小之间的关系，我们可以识别几种类型的任务：
- en: '**One-to-one**: Non-sequential processing, such as feedforward NNs and CNNs.
    There isn’t much difference between a feedforward NN and applying an RNN to a
    single time step. An example of one-to-one processing is image classification.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一对一**：非顺序处理，例如前馈神经网络（NNs）和卷积神经网络（CNNs）。前馈神经网络和将 RNN 应用于单个时间步之间没有太大区别。一对一处理的一个例子是图像分类。'
- en: '**One-to-many**: This generates a sequence based on a single input – for example,
    caption generation from an image (*Show and Tell: A Neural Image Caption* *Generator*,
    [https://arxiv.org/abs/1411.4555](https://arxiv.org/abs/1411.4555)).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一对多**：基于单一输入生成一个序列——例如，从图像生成标题（*展示与讲解：神经图像标题生成器*， [https://arxiv.org/abs/1411.4555](https://arxiv.org/abs/1411.4555)）。'
- en: '**Many-to-one**: This outputs a single result based on a sequence – for example,
    sentiment classification of text.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对一**：根据一个序列输出一个结果——例如，文本的情感分类。'
- en: '**Many-to-many indirect**: A sequence is encoded into a state vector, after
    which this state vector is decoded into a new sequence – for example, language
    translation (*Learning Phrase Representations using RNN Encoder-Decoder for Statistical
    Machine Translation*, [https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078)
    and *Sequence to Sequence Learning with Neural* *Networks*, [http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对多间接**：一个序列被编码成一个状态向量，之后该状态向量被解码成一个新的序列——例如，语言翻译（*使用 RNN 编码器-解码器学习短语表示用于统计机器翻译*，
    [https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078) 和 *序列到序列学习与神经网络*，
    [http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)）。'
- en: '**Many-to-many direct**: Outputs a result for each input step – for example,
    frame phoneme labeling in speech recognition.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对多直接**：为每个输入步骤输出一个结果——例如，语音识别中的帧音素标注。'
- en: Note
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The many-to-many models are often referred to as **sequence-to-sequence** (**seq2seq**)
    models.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 多对多模型通常被称为 **序列到序列** (**seq2seq**) 模型。
- en: 'The following is a graphical representation of the preceding input-output combinations:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述输入输出组合的图示表示：
- en: '![Figure 6.8 – RNN input-output combinations, inspired by http://karpathy.github.io/2015/05/21/rnn-effectiveness/](img/B19627_06_8.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8 – RNN 输入输出组合，灵感来源于 http://karpathy.github.io/2015/05/21/rnn-effectiveness/](img/B19627_06_8.jpg)'
- en: Figure 6.8 – RNN input-output combinations, inspired by [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 – RNN 输入输出组合，灵感来源于 [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- en: Now that we’ve introduced RNNs, let’s improve our knowledge by implementing
    a simple RNN example.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经介绍了 RNN，现在让我们通过实现一个简单的 RNN 示例来加深对其的理解。
- en: RNN implementation and training
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN 实现与训练
- en: 'In the preceding section, we briefly discussed what RNNs are and what problems
    they can solve. Let’s dive into the details of an RNN and how to train it with
    a very simple toy example: counting ones in a sequence.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们简要讨论了 RNN 是什么以及它们可以解决哪些问题。接下来，让我们深入了解 RNN 的细节，并通过一个非常简单的玩具示例进行训练：计算序列中的
    1 的数量。
- en: We’ll teach a basic RNN how to count the number of ones in the input and then
    output the result at the end of the sequence. This is an example of a many-to-one
    relationship, which we defined in the previous section.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将教一个基本的 RNN 如何计算输入中 1 的数量，并在序列结束时输出结果。这是一个多对一关系的例子，正如我们在上一节中定义的那样。
- en: 'We’ll implement this example with Python (no DL libraries) and numpy. An example
    of the input and output is as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Python（不使用深度学习库）和 numpy 实现这个例子。以下是输入和输出的示例：
- en: '[PRE0]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The RNN we’ll use is illustrated in the following diagram:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的 RNN 如下图所示：
- en: '![Figure 6.9 – Basic RNN for counting ones in the input](img/B19627_06_9.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9 – 用于计算输入中 1 的基本 RNN](img/B19627_06_9.jpg)'
- en: Figure 6.9 – Basic RNN for counting ones in the input
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – 用于计算输入中 1 的基本 RNN
- en: Note
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Since ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/475.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/476.png),
    *U*, *W*, and *y* are scalar values (**x** remains a vector), we won’t use the
    matrix notation (bold capital letters) in the RNN implementation and training
    section and its subsections. We’ll use italic notation instead. In the code sections,
    we’ll denote them as variables. However, note that the generic versions of these
    formulas use matrix and vector parameters.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/475.png)，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/476.png)，*U*，*W*
    和 *y* 都是标量值（**x** 仍然是一个向量），所以在 RNN 的实现和训练部分及其子部分中，我们不会使用矩阵表示法（粗体大写字母）。我们将使用斜体表示法。在代码部分，我们将它们表示为变量。然而，值得注意的是，这些公式的通用版本使用的是矩阵和向量参数。
- en: 'The RNN will have only two parameters: an input weight, *U*, and a recurrence
    weight, *W*. The output weight, *V*, is set to 1 so that we just read out the
    last state as the output, *y*.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 只会有两个参数：一个输入权重 *U* 和一个递归权重 *W*。输出权重 *V* 设置为 1，这样我们只需将最后的状态作为输出 *y*。
- en: 'First, let’s add some code so that our example can be executed. We’ll import
    numpy and define our training set – inputs, **x**, and labels, *y*. **x** is two-dimensional
    since the first dimension represents the sample in the mini-batch. *y* is a single
    numerical value (it still has a batch dimension). For the sake of simplicity,
    we’ll use a mini-batch with a single sample:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们添加一些代码，以便我们的例子可以执行。我们将导入 numpy 并定义我们的训练集——输入 **x** 和标签 *y*。**x** 是二维的，因为第一维代表了小批量中的样本。*y*
    是一个单一的数值（它仍然有一个批量维度）。为了简单起见，我们将使用一个只有单一样本的小批量：
- en: '[PRE1]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The recurrence relation defined by this RNN is ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>W</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>U</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/477.png).
    Note that this is a linear model since we don’t apply a non-linear function in
    this formula. We can implement a recurrence relationship in the following way:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 由该 RNN 定义的递归关系是 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>W</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>U</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/477.png)。请注意，这是一个线性模型，因为我们没有在这个公式中应用非线性函数。我们可以通过以下方式实现递归关系：
- en: '[PRE2]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The states, `s_t`, and the weights, `W` and `U`, are single scalar values. `x_t`
    represents a single element of the input sequence (in our case, one or zero).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 状态 `s_t` 和权重 `W` 和 `U` 是单一的标量值。`x_t` 表示输入序列中的单个元素（在我们的例子中，是 0 或 1）。
- en: Note
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'One solution to this task is to just get the sum of the elements of the input
    sequence. If we set `U=1`, then whenever input is received, we will get its full
    value. If we set `W=1`, then the value we would accumulate would never decay.
    So, for this example, we would get the desired output: 3\. Nevertheless, let’s
    use this simple example to explain the training and implementation of the RNN.
    This will be interesting, as we will see in the rest of this section.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个任务的一个方法是直接获取输入序列中元素的和。如果我们设置 `U=1`，那么每当输入被接收时，我们将得到它的完整值。如果我们设置 `W=1`，那么我们所累积的值将不会衰减。因此，对于这个例子，我们将得到期望的输出：3。然而，让我们用这个简单的例子来解释
    RNN 的训练和实现。接下来的部分将会很有趣，我们将看到这些内容。
- en: 'We can think of an RNN as a special type of regular NN by unfolding it through
    time for a certain number of time steps (as illustrated in the preceding diagram).
    This regular NN has as many hidden layers as the size of the elements of the input
    sequence. In other words, one hidden layer represents one step through time. The
    only difference is that each layer has multiple inputs: the previous state, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/478.png),
    and the current input, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/479.png).
    The parameters, *U* and *W*, are shared between all of the hidden layers.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将RNN视为一种特殊类型的常规神经网络，通过时间展开它，进行一定数量的时间步（如前面的图所示）。这个常规神经网络的隐藏层数量等于输入序列元素的大小。换句话说，一个隐藏层代表时间中的一步。唯一的区别是每一层有多个输入：前一个状态，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/478.png)，和当前输入，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/479.png)。参数*U*和*W*在所有隐藏层之间共享。
- en: 'The forward pass unfolds the RNN along the sequence and builds a stack of states
    for each step. In the following code block, we can see an implementation of the
    forward pass, which returns the activation, *s*, for each recurrent step and each
    sample in the batch:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播沿序列展开RNN，并为每个步骤构建状态堆栈。在下面的代码块中，我们可以看到前向传播的实现，它返回每个递归步骤和批次中每个样本的激活值*s*：
- en: '[PRE3]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have the RNN forward pass, let’s look at how to train our unfolded
    RNN.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了RNN的前向传播，我们来看一下如何训练展开的RNN。
- en: Backpropagation through time
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间反向传播
- en: '**Backpropagation through time** (**BPTT**) is the typical algorithm we use
    to train RNNs (*Backpropagation Through Time: What It Does and How to Do It*,
    [http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf](http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf)).
    As its name suggests, it’s an adaptation of the backpropagation algorithm we discussed
    in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047).'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**时间反向传播**（**BPTT**）是我们用来训练RNN的典型算法（*时间反向传播：它的作用与如何实现*，[http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf](http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf)）。顾名思义，它是我们在[*第二章*](B19627_02.xhtml#_idTextAnchor047)中讨论的反向传播算法的一个改进版。'
- en: Let’s assume that we’ll use the **mean squared error** (**MSE**) cost function.
    Now that we also have our forward step implementation, we can define how the gradient
    is propagated backward. Since the unfolded RNN is equivalent to a regular feedforward
    NN, we can use the backpropagation chain rule we introduced in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们将使用**均方误差**（**MSE**）损失函数。现在我们也有了前向传播步骤的实现，我们可以定义梯度如何向后传播。由于展开的RNN等同于一个常规的前馈神经网络，我们可以使用在[*第二章*](B19627_02.xhtml#_idTextAnchor047)中介绍的反向传播链式法则。
- en: Because the weights, *W* and *U*, are shared across the layers, we’ll accumulate
    the error derivatives for each recurrent step, and in the end, we’ll update the
    weights with the accumulated value.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 因为权重*W*和*U*在各层之间是共享的，所以我们将在每个递归步骤中累积误差导数，最后用累积的值来更新权重。
- en: 'First, we need to get the gradient of the output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/480.png),
    concerning the loss function, *J*, *∂J/∂s*. Once we have it, we’ll propagate it
    backward through the stack of activities we built during the forward step. This
    backward pass pops activities off of the stack to accumulate their error derivatives
    at each time step. The recurrence relation that propagates this gradient through
    the RNN can be written as follows (chain rule):'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要获取输出的梯度，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/480.png)，关于损失函数
    *J*，*∂J/∂s*。一旦我们获得了它，我们将通过在前向步骤中构建的活动堆栈向后传播。这个反向传播过程从堆栈中弹出活动，在每个时间步积累它们的误差导数。通过
    RNN 传播这个梯度的递归关系可以写成如下（链式法则）：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>W</mml:mi></mml:math>](img/481.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>W</mml:mi></mml:math>](img/481.png)'
- en: 'The gradients of the weights, *U* and *W*, are accumulated as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 权重 *U* 和 *W* 的梯度将如下方式积累：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>U</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/482.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>U</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/482.png)'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/483.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/483.png)'
- en: 'Armed with this knowledge, let’s implement the backward pass:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 掌握了这些知识后，让我们实现反向传播：
- en: 'Accumulate the gradients for `U` and `W` in `gU` and `gW`, respectively:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `U` 和 `W` 的梯度分别累积到 `gU` 和 `gW` 中：
- en: '[PRE4]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Use gradient descent to optimize our RNN. Compute the gradients (using MSE)
    with the help of the backward function and use them to update the weights value:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降法优化我们的 RNN。通过反向传播函数计算梯度（使用 MSE），并用这些梯度来更新权重值：
- en: '[PRE5]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Run the training for 150 epochs:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 150 个训练周期：
- en: '[PRE6]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, display the loss function and the gradients for each weight over the
    epochs. We’ll do this with the help of the `plot_training` function, which is
    not implemented here but is available in the full example on GitHub. `plot_training`
    produces the following graph:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，显示每个权重在训练过程中的损失函数和梯度。我们将通过 `plot_training` 函数来实现，虽然这个函数在此处未实现，但可以在 GitHub
    上的完整示例中找到。`plot_training` 会生成如下图表：
- en: '![Figure 6.10 – The RNN loss – uninterrupted line  – loss value; dashed lines
    – the weight gradients during training](img/B19627_06_10.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.10 – RNN 损失 – 实线 – 损失值；虚线 – 训练过程中权重的梯度](img/B19627_06_10.jpg)'
- en: Figure 6.10 – The RNN loss – uninterrupted line – loss value; dashed lines –
    the weight gradients during training
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 – RNN 损失 – 实线 – 损失值；虚线 – 训练过程中权重的梯度
- en: Now that we’ve learned about backpropagation through time, let’s discuss how
    the familiar vanishing and exploding gradient problems affect it.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了时间反向传播，让我们来讨论熟悉的梯度消失和梯度爆炸问题是如何影响它的。
- en: Vanishing and exploding gradients
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度消失和梯度爆炸
- en: 'The preceding example has an issue. To illustrate it, let’s run the training
    process with a longer sequence:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例有一个问题。为了说明这个问题，让我们用更长的序列运行训练过程：
- en: '[PRE7]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output is as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE8]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The reason for these warnings is that the final parameters, `U` and `W`, end
    up as `plot_training` function to produce the following result:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 出现这些警告的原因是最终的参数 `U` 和 `W` 会通过 `plot_training` 函数生成以下结果：
- en: '![Figure 6.11 – Parameters and loss function during an exploding gradients
    scenario](img/B19627_06_11.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.11 – 在梯度爆炸情景下的参数和损失函数](img/B19627_06_11.jpg)'
- en: Figure 6.11 – Parameters and loss function during an exploding gradients scenario
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 – 在梯度爆炸情景下的参数和损失函数
- en: 'In the initial epochs, the gradients slowly increase, similar to the way they
    increased for the shorter sequence. However, when they get to epoch 23 (the exact
    epoch is unimportant, though), the gradient becomes so large that it goes out
    of the range of the float variable and becomes NaN (as illustrated by the jump
    in the plot). This problem is known as **exploding gradients**. We can stumble
    upon exploding gradients in a regular feedforward NN, but it is especially pronounced
    in RNNs. To understand why, let’s recall the recurrent gradient propagation chain
    rule for the two consecutive sequence steps we defined in the *Backpropagation
    through* *time* section:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在初期的训练阶段，梯度会缓慢增加，类似于它们在较短序列中增加的方式。然而，当达到第23个epoch时（虽然确切的epoch并不重要），梯度变得非常大，以至于超出了浮点变量的表示范围，变成了NaN（如图中的跳跃所示）。这个问题被称为**梯度爆炸**。我们可以在常规的前馈神经网络中遇到梯度爆炸问题，但在RNN中尤为显著。为了理解原因，回顾一下我们在*时间反向传播*部分定义的两个连续序列步骤的递归梯度传播链规则：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>W</mml:mi></mml:math>](img/481.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>W</mml:mi></mml:math>](img/481.png)'
- en: 'Depending on the sequence’s length, an unfolded RNN can be much deeper compared
    to a regular NN. At the same time, the weights, *W*, of an RNN are shared across
    all of the steps. Therefore, we can generalize this formula to compute the gradient
    between two non-consecutive steps of the sequence. Because *W* is shared, the
    equation forms a geometric progression:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 根据序列的长度，展开的RNN相比常规的神经网络可以更深。同时，RNN的权重*W*在所有步骤中是共享的。因此，我们可以将这个公式推广，用来计算序列中两个非连续步骤之间的梯度。由于*W*是共享的，方程形成了一个几何级数：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>…</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math>](img/485.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>…</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math>](img/485.png)'
- en: "In our simple linear RNN, the gradient grows exponentially if *|W|>1* (exploding\
    \ gradient), where *W* is a single scalar weight – for example, 50 time steps\
    \ over *W=1.5* is ![<math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mrow><msup><mi>W</mi><mrow><mo>(</mo><mn>50</mn><mo>)</mo></mrow></msup><mo>≈</mo><mn>637</mn><mi\
    \ mathvariant=\"normal\">\uFEFF</mi><mo>,</mo><mn>621</mn><mi mathvariant=\"normal\"\
    >\uFEFF</mi><mo>,</mo><mn>500</mn></mrow></mrow></math>](img/486.png). The gradient\
    \ shrinks exponentially if *|W|<1* (vanishing gradient), for example, 10 time\
    \ steps over *W=0.6* is ![<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"\
    \ xmlns:m=\"http://schemas.openxmlformats.org/officeDocument/2006/math\"><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>10</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.00097</mml:mn></mml:math>](img/487.png).\
    \ If the weight parameter, *W*, is a matrix instead of a scalar, this exploding\
    \ or vanishing gradient is related to the largest eigenvalue, *ρ*, of *W* (also\
    \ known as a spectral radius). It is sufficient for *ρ<1* for the gradients to\
    \ vanish, and it is necessary for *ρ>1* for them to explode."
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: "在我们简单的线性RNN中，如果*|W|>1*（梯度爆炸），梯度会呈指数增长，其中*W*是一个标量权重——例如，50个时间步长下，当*W=1.5*时，结果是\
    \ ![<math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mrow><msup><mi>W</mi><mrow><mo>(</mo><mn>50</mn><mo>)</mo></mrow></msup><mo>≈</mo><mn>637</mn><mi\
    \ mathvariant=\"normal\">\uFEFF</mi><mo>,</mo><mn>621</mn><mi mathvariant=\"normal\"\
    >\uFEFF</mi><mo>,</mo><mn>500</mn></mrow></mrow></math>](img/486.png)。如果*|W|<1*（梯度消失），梯度会呈指数衰减，例如，10个时间步长下，当*W=0.6*时，结果是\
    \ ![<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:m=\"http://schemas.openxmlformats.org/officeDocument/2006/math\"\
    ><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>10</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.00097</mml:mn></mml:math>](img/487.png)。如果权重参数*W*是矩阵而不是标量，那么这种梯度爆炸或梯度消失现象与*W*的最大特征值*ρ*（也称为谱半径）有关。当*ρ<1*时，梯度消失；当*ρ>1*时，梯度爆炸。"
- en: 'The vanishing gradients problem, which we first mentioned in [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079),
    has another more subtle effect in RNNs: the gradient decays exponentially over
    the number of steps to a point where it becomes extremely small in the earlier
    states. In effect, they are overshadowed by the larger gradients from more recent
    time steps, and the RNN’s ability to retain the history of these earlier states
    vanishes. This problem is harder to detect because the training will still work,
    and the NN will produce valid outputs (unlike with exploding gradients). It just
    won’t be able to learn long-term dependencies.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第*3章*](B19627_03.xhtml#_idTextAnchor079)中首先提到的梯度消失问题，在RNN中还有另一个更微妙的影响：梯度随着步数的增加呈指数衰减，直到在较早的状态下变得极其小。实际上，它们被来自较晚时间步长的更大梯度所掩盖，导致RNN无法保留这些早期状态的历史。这个问题更难以察觉，因为训练仍然会继续进行，且神经网络仍会生成有效的输出（与梯度爆炸不同）。只是它无法学习长期依赖。
- en: With that, we are familiar with some of the problems surrounding RNNs. This
    knowledge will serve us well because, in the next section, we’ll discuss how to
    solve these problems with the help of a special type of RNN cell.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些内容，我们已经熟悉了RNN的一些问题。这些知识对我们接下来的讨论非常有帮助，因为在下一节中，我们将讨论如何借助一种特殊的RNN单元来解决这些问题。
- en: Long-short term memory
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 长短时记忆
- en: 'Hochreiter and Schmidhuber studied the problems of vanishing and exploding
    gradients extensively and came up with a solution called **long short-term memory**
    (**LSTM** – [https://www.bioinf.jku.at/publications/older/2604.pdf](https://www.bioinf.jku.at/publications/older/2604.pdf)
    and *Learning to Forget: Continual Prediction with LSTM*, https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.5709&rep=rep1&type=pdf).
    LSTMs can handle long-term dependencies due to a specially crafted memory cell.
    They work so well that most of the current accomplishments in training RNNs on
    a variety of problems are due to the use of LSTMs. In this section, we’ll explore
    how this memory cell works and how it solves the vanishing gradients issue.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hochreiter和Schmidhuber广泛研究了梯度消失和梯度爆炸的问题，并提出了一种解决方案，称为**长短时记忆网络**（**LSTM** –
    [https://www.bioinf.jku.at/publications/older/2604.pdf](https://www.bioinf.jku.at/publications/older/2604.pdf)
    和 *Learning to Forget: Continual Prediction with LSTM*, https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.5709&rep=rep1&type=pdf）。LSTM由于特别设计的记忆单元，可以处理长期依赖问题。它们表现得如此出色，以至于目前大多数RNN在解决各种问题时的成功都归功于LSTM的使用。在本节中，我们将探索这个记忆单元是如何工作的，以及它如何解决梯度消失的问题。'
- en: 'The following is a diagram of an LSTM cell:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个LSTM单元的示意图：
- en: '![Figure 6.12 – LSTM cell (top); unfolded LSTM cell (bottom). Inspired by http://colah.github.io/posts/2015-08-Understanding-LSTMs/](img/B19627_06_12.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 6.12 – LSTM细胞（顶部）；展开的LSTM细胞（底部）。灵感来源于 http://colah.github.io/posts/2015-08-Understanding-LSTMs/](img/B19627_06_12.jpg)'
- en: Figure 6.12 – LSTM cell (top); unfolded LSTM cell (bottom). Inspired by [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 6.12 – LSTM细胞（顶部）；展开的LSTM细胞（底部）。灵感来源于 [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- en: 'The key idea of LSTM is the cell state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/488.png)
    (in addition to the hidden RNN state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/489.png)),
    where the information can only be explicitly written in or removed so that the
    state stays constant if there is no outside interference. The cell state can only
    be modified by specific gates, which are a way to let information pass through.
    A typical LSTM is composed of three gates: a **forget gate**, an **input gate**,
    and an **output gate**. The cell state, input, and output are all vectors so that
    the LSTM can hold a combination of different information blocks at each time step.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM的关键思想是细胞状态，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/488.png)（除了隐藏的RNN状态，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/489.png)），在没有外部干扰的情况下，信息只能显式写入或移除以保持状态恒定。细胞状态只能通过特定的门来修改，这些门是信息传递的一种方式。典型的LSTM由三个门组成：**遗忘门**，**输入门**和**输出门**。细胞状态、输入和输出都是向量，以便LSTM可以在每个时间步长保持不同信息块的组合。
- en: LSTM notations
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM符号表示
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/490.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/491.png),
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/492.png)
    are the LSTM’s input, cell memory state, and output (or hidden state) vectors
    in moment *t*. ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/493.png)
    is the candidate cell state vector (more on that later). The input, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/494.png),
    and the previous cell output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/495.png),
    are connected to each gate and the candidate cell vector with sets of FC weights,
    **W** and **U**, respectively. ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/496.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/497.png),
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/498.png)
    are the forget, input, and output gates of the LSTM cell (the gates use vector
    notation as well).'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/490.png)，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/491.png)，以及![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/492.png)是LSTM在时刻*t*的输入、细胞记忆状态和输出（或隐藏状态）向量。![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/493.png)是候选细胞状态向量（稍后会详细介绍）。输入![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/494.png)和前一时刻的细胞输出![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/495.png)通过一组全连接（FC）权重**W**和**U**分别与每个门和候选细胞向量相连接。![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/496.png)，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/497.png)，以及![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/498.png)是LSTM细胞的遗忘门、输入门和输出门（这些门也使用向量表示）。'
- en: 'The gates are composed of FC layers, sigmoid activations, and element-wise
    multiplication (denoted with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>⨀</mml:mo></mml:math>](img/499.png)).
    Because the sigmoid only outputs values between 0 and 1, the multiplication can
    only reduce the value running through the gate. Let’s discuss them in order:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这些门由全连接（FC）层、sigmoid 激活函数和逐元素相乘（表示为 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>⨀</mml:mo></mml:math>](img/499.png)）组成。由于
    sigmoid 函数的输出仅限于 0 到 1 之间，乘法操作只能减少通过门的值。我们按顺序讨论它们：
- en: '**Forget gate**, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/500.png):
    It decides whether we want to erase parts of the existing cell state or not. It
    bases its decision on the weighted vector sum of the output of the previous cell,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/501.png),
    and the current input, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png):'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗忘门**，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/500.png)：它决定我们是否要擦除现有单元状态的部分内容。它根据前一单元输出的加权向量和当前输入来做出决定，前一单元的输出是
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/501.png)，当前输入是
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png)：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/503.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/503.png)'
- en: 'From the preceding formula, we can see that the forget gate applies element-wise
    sigmoid activations to each element of the previous state vector, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/504.png):
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/505.png)
    (note the circle-dot notation). Since the operation is elementwise, the values
    of this vector are squashed in the [0, 1] range. An output of 0 erases a specific
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/506.png)
    cell block completely and an output of 1 allows the information in that cell block
    to pass through. In this way, the LSTM can get rid of irrelevant information in
    its cell state vector.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 从前述公式可以看出，遗忘门对先前状态向量的每个元素应用逐元素sigmoid激活，[<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/504.png)：![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/505.png)（注意圆点符号）。由于操作是逐元素的，因此该向量的值被压缩到[0，1]范围内。输出为0意味着特定的![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/506.png)单元块被完全清除，而输出为1则允许该单元块中的信息通过。通过这种方式，LSTM能够清除其单元状态向量中的无关信息。
- en: '**Input gate**, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/507.png):
    It decides what new information is going to be added to the memory cell in a multi-step
    process. The first step determines whether any information is going to be added.
    As in the forget gate, its decision is based on ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/508.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png):
    it outputs 0 or 1 through the sigmoid function for each cell of the candidate
    state vector. An output of 0 means that no information is added to that cell block’s
    memory. As a result, the LSTM can store specific pieces of information in its
    cell state vector:'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入门**，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/507.png)：它通过多步骤过程决定将哪些新信息添加到记忆单元中。第一步是决定是否添加任何信息。与遗忘门类似，它的决策是基于![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/508.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png)：它通过sigmoid函数为候选状态向量的每个单元输出0或1。0的输出意味着没有信息被添加到该单元的记忆中。因此，LSTM可以在其单元状态向量中存储特定的片段信息：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/510.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/510.png)'
- en: 'In the next step of the input gate sequence, we compute the new candidate cell
    state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/511.png).
    It is based on the previous output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/512.png),
    and the current input, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png),
    and is transformed via a tanh function:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入门序列的下一步中，我们计算新的候选细胞状态，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/511.png)。它基于先前的输出，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/512.png)，以及当前的输入，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png)，并通过一个tanh函数进行转换：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/514.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/514.png)'
- en: 'Then, we combine ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/515.png)
    with the sigmoid outputs of the input gate via element-wise multiplication: ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/516.png).'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/515.png)与输入门的Sigmoid输出通过逐元素乘法结合起来：![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/516.png)。
- en: 'To recap, the forget and input gates decide what information to forget and
    include from the previous and candidate cell states, respectively. The final version
    of the new cell state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/517.png),
    is just an element-wise sum between these two components:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，遗忘门和输入门分别决定了要从先前的和候选单元状态中遗忘和包含哪些信息。最终版本的新单元状态，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/517.png)，只是这两个组成部分的逐元素和：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊕</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/518.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊕</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/518.png)'
- en: '**Output gate**, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/519.png):
    It decides what the total cell output is going to be. It takes ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/520.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png)
    as inputs. It outputs a value in the (0, 1) range (via the sigmoid function) for
    each block of the cell’s memory. Like before, 0 means that the block doesn’t output
    any information and 1 means that the block can pass through as a cell’s output.
    Therefore, the LSTM can output specific blocks of information from its cell state
    vector:'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出门**，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/519.png)：它决定了单元格的总输出是什么。它将![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/520.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png)
    作为输入。它为每个单元格记忆块输出一个 (0, 1) 范围内的值（通过 sigmoid 函数）。和之前一样，0 表示该块不输出任何信息，1 表示该块可以作为单元格的输出传递。因此，LSTM
    可以从其单元格状态向量中输出特定的信息块：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/522.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/522.png)'
- en: 'Finally, the LSTM cell’s output is transferred by a tanh function:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，LSTM 单元的输出通过 tanh 函数进行传递：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:mi
    mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/523.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:mi
    mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/523.png)'
- en: Because all these formulas are derivable, we can chain LSTM cells together,
    just like when we chain simple RNN states together and train the network via backpropagation
    through time.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这些公式都是可推导的，我们可以将 LSTM 单元串联起来，就像我们将简单的 RNN 状态串联在一起，并通过时间反向传播来训练网络一样。
- en: But how does the LSTM protect us from vanishing gradients? Let’s start with
    the forward phase. Notice that the cell state is copied identically from step
    to step if the forget gate is 1 and the input gate is
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 LSTM 是如何防止梯度消失的呢？我们从前向传播阶段开始。注意，如果遗忘门为 1，且输入门为
- en: '0: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊕</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊕</mml:mo><mml:mn>0</mml:mn><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/524.png).
    Only the forget gate can completely erase the cell’s memory. As a result, the
    memory can remain unchanged over a long period. Also, note that the input is a
    tanh activation that’s been added to the current cell’s memory. This means that
    the cell’s memory doesn’t blow up and is quite stable.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '0: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊕</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊕</mml:mo><mml:mn>0</mml:mn><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/524.png)。只有遗忘门可以完全清除单元的记忆。因此，记忆可以在长时间内保持不变。此外，注意输入是一个
    tanh 激活函数，它已被加入到当前单元的记忆中。这意味着单元的记忆不会爆炸，并且相当稳定。'
- en: 'Let’s use an example to demonstrate how an LSTM cell is unfolded. For the sake
    of simplicity, we’ll assume that it has one-dimensional (single scalar value)
    input, state, and output vectors. Because the values are scalar, we won’t use
    vector notation for the rest of this example:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来演示 LSTM 单元如何展开。为了简化起见，我们假设它具有一维（单标量值）输入、状态和输出向量。由于值是标量，我们将在此示例的其余部分中不使用向量符号：
- en: '![Figure 6.13 – Unrolling an LSTM through time](img/B19627_06_13.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.13 – 随时间展开 LSTM](img/B19627_06_13.jpg)'
- en: Figure 6.13 – Unrolling an LSTM through time
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13 – 随时间展开 LSTM
- en: 'The process is as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 过程如下：
- en: First, we have a value of 3 as a candidate state. The input gate is set to ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math>](img/525.png)
    and the forget gate is set to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/526.png).
    This means that the previous state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>N</mml:mi></mml:math>](img/527.png),
    is erased and replaced with the new state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>⨀</mml:mo><mml:mi>N</mml:mi><mml:mo>⊕</mml:mo><mml:mn>1</mml:mn><mml:mo>⨀</mml:mo><mml:mn>3</mml:mn><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:math>](img/528.png).
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们有一个值为3的候选状态。输入门设置为 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math>](img/525.png)，而忘记门设置为
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/526.png)。这意味着先前的状态
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>N</mml:mi></mml:math>](img/527.png)
    被抹去，并被新的状态 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>⨀</mml:mo><mml:mi>N</mml:mi><mml:mo>⊕</mml:mo><mml:mn>1</mml:mn><mml:mo>⨀</mml:mo><mml:mn>3</mml:mn><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:math>](img/528.png)
    替换。
- en: 'For the next two time steps, the forget gate is set to 1, while the input gate
    is set to 0\. By doing this, all the information is kept throughout these steps
    and no new information is added because the input gate is set to 0: ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>⨀</mml:mo><mml:mn>3</mml:mn><mml:mo>⊕</mml:mo><mml:mn>0</mml:mn><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:math>](img/529.png).'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于接下来的两个时间步骤，忘记门设置为1，而输入门设置为0。这样，在这些步骤中，所有信息都被保留，没有新信息被添加，因为输入门被设置为0：![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>⨀</mml:mo><mml:mn>3</mml:mn><mml:mo>⊕</mml:mo><mml:mn>0</mml:mn><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:math>](img/529.png)。
- en: Finally, the output gate is set to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math>](img/530.png)
    and 3 is output and remains unchanged. We have successfully demonstrated how the
    internal state is stored across multiple steps.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，输出门设置为 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math>](img/530.png)，3被输出并保持不变。我们已成功展示了如何在多个步骤中存储内部状态。
- en: Next, let’s focus on the backward phase. The cell state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/531.png),
    can mitigate the vanishing/exploding gradients as well with the help of the forget
    gate, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/532.png).
    Like the regular RNN, we can use the chain rule to compute the partial derivative,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/533.png),
    for two consecutive steps. Following the formula ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊕</mml:mo><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/534.png)
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们聚焦于反向阶段。细胞状态，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/531.png)，可以通过遗忘门的帮助，减轻消失/爆炸梯度的问题，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/532.png)。像常规的RNN一样，我们可以利用链式法则计算偏导数，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/533.png)，对于两个连续的步骤。根据公式
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊕</mml:mo><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/534.png)
- en: 'and without going into details, its partial derivative is as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 不展开细节，其偏导数如下：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>≈</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/535.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>≈</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/535.png)'
- en: 'We can also generalize this to non-consecutive steps:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将其推广到非连续的步骤：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>…</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>≈</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/536.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>…</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>≈</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/536.png)'
- en: If the forget gate values are close to 1, gradient information can pass back
    through the network states almost unchanged. This is because ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/532.png)
    uses sigmoid activation and information flow is still subject to the vanishing
    gradient that’s specific to sigmoid activations. But unlike the gradients in the
    regular RNN, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/532.png)
    has a different value at each time step. Therefore, this is not a geometric progression,
    and the vanishing gradient effect is less pronounced.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 如果遗忘门的值接近 1，则梯度信息几乎不变地通过网络状态反向传播。这是因为 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/532.png)
    使用 sigmoid 激活函数，信息流仍然受到 sigmoid 激活特有的消失梯度的影响。但是，与普通 RNN 中的梯度不同，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/532.png)
    在每个时间步的值是不同的。因此，这不是几何级数，消失梯度效应较不明显。
- en: Next, we’ll introduce a new type of lightweight RNN cell that still preserves
    the properties of LSTM.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍一种新的轻量级 RNN 单元，它仍然保留 LSTM 的特性。
- en: Gated recurrent units
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 门控循环单元
- en: 'A **gated recurrent unit** (**GRU**) is a type of recurrent block that was
    introduced in 2014 (*Learning Phrase Representations using RNN Encoder-Decoder
    for Statistical Machine Translation*, [https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078)
    and *Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling*,
    https://arxiv.org/abs/1412.3555) as an improvement over LSTM. A GRU unit usually
    has similar or better performance than an LSTM, but it does so with fewer parameters
    and operations:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '**门控循环单元**（**GRU**）是一种循环模块，首次在 2014 年提出（*使用 RNN 编码器-解码器进行统计机器翻译的学习短语表示*，[https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078)
    和 *门控递归神经网络在序列建模中的实证评估*，https://arxiv.org/abs/1412.3555），作为对 LSTM 的改进。GRU 单元通常具有与
    LSTM 相似或更好的性能，但它的参数和操作更少：'
- en: '![Figure 6.14 – A GRU cell diagram](img/B19627_06_14.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.14 – 一个 GRU 单元示意图](img/B19627_06_14.jpg)'
- en: Figure 6.14 – A GRU cell diagram
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14 – 一个 GRU 单元示意图
- en: 'Similar to the classic RNN, a GRU cell has a single hidden state, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/539.png).
    You can think of it as a combination of the hidden and cell states of an LSTM.
    The GRU cell has two gates:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于经典的 RNN，GRU 单元有一个单一的隐藏状态，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/539.png)。你可以将其看作是
    LSTM 的隐藏状态和细胞状态的结合。GRU 单元有两个门：
- en: '**Update gate**, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/540.png):
    Combines the input and forget LSTM gates. It decides what information to discard
    and what new information to include in its place based on the network input, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png),
    and the previous hidden state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/542.png).
    By combining the two gates, we can ensure that the cell will forget information,
    but only when we are going to include new information in its place:'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新门**，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/540.png)：结合输入和遗忘
    LSTM 门。根据网络输入 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png)
    和先前的隐藏状态 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/542.png)，决定丢弃哪些信息并确定新信息的包含方式。通过结合这两个门，我们可以确保细胞只会在有新信息需要包含时才丢弃信息：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/543.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/543.png)'
- en: '**Reset gate**, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/544.png):
    Uses the previous hidden state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/545.png),
    and the network input, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/357.png),
    to decide how much of the previous state to pass through:'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重置门**，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/544.png)：使用先前的隐藏状态，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/545.png)，和网络输入，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/357.png)，来决定保留多少先前的状态：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/547.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/547.png)'
- en: 'Next, we have the candidate state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/548.png):'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们得到候选状态，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/548.png)：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi
    mathvariant="bold">U</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/549.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi
    mathvariant="bold">U</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/549.png)'
- en: 'Finally, the GRU output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/550.png),
    at time *t* is an element-wise sum between the previous output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/551.png),
    and the candidate output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/548.png):'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，GRU输出，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:msub></mml:math>](img/550.png)，在时刻
    *t* 是前一个输出，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/551.png)，以及候选输出，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/548.png):'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊕</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/553.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊕</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/553.png)'
- en: Since the update gate allows us to both forget and store data, it is directly
    applied to the previous output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/551.png),
    and applied over the candidate output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/548.png).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 由于更新门允许我们同时忘记和存储数据，因此它直接应用于之前的输出，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/551.png)，并应用于候选输出，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/548.png)。
- en: 'We’ll conclude our introduction to RNNs by returning to the disclaimer at the
    start of this chapter – the practical limitations of RNNs. We can solve one of
    them – the vanishing and exploding gradients – with the help of LSTM or GRU cells.
    However, there are two others:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的最后部分回到之前的免责声明——RNN 的实际限制。我们可以通过 LSTM 或 GRU 单元来解决其中的一个限制——消失梯度和梯度爆炸问题。但还有两个问题：
- en: The RNN’s internal state is updated after each element of the sequence – a new
    element requires all preceding elements to be processed in advance. Therefore,
    the RNN sequence processing cannot be parallelized and RNNs cannot take advantage
    of the GPU parallelization capabilities.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN 的内部状态在每个序列元素之后都会更新——每个新元素都需要先处理所有前面的元素。因此，RNN 的序列处理无法并行化，也无法利用 GPU 的并行计算能力。
- en: The information of all preceding sequence elements is summarized in a single
    hidden cell state. The RNN doesn’t have direct access to the historical sequence
    elements and has to rely on the cell state instead. In practice, this means that
    an RNN (even LSTM or GRU) can meaningfully process sequences with a maximum length
    of around 100 elements.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有前序序列元素的信息都被总结在一个单一的隐藏状态单元中。RNN 没有直接访问历史序列元素的能力，而是必须依赖单元状态来处理。实际上，这意味着即使是 LSTM
    或 GRU，RNN 也只能有效处理最大长度约为 100 个元素的序列。
- en: As we’ll see in the next chapter, the transformer architecture successfully
    solves both of these limitations. But for now, let’s see how to use LSTMs in practice.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在下一章看到的，transformer 架构成功地解决了这两种限制。但现在，让我们先看看如何在实践中使用 LSTM。
- en: Implementing text classification
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现文本分类
- en: In this section, we’ll use LSTM to implement a sentiment analysis example over
    the Large Movie Review Dataset (**IMDb**, [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)),
    which consists of 25,000 training and 25,000 testing reviews of popular movies.
    Each review has a binary label that indicates whether it is positive or negative.
    This type of problem is an example of a **many-to-one** relationship, which we
    defined in the *Recurrent neural networks (**RNNs)* section.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将使用 LSTM 实现一个情感分析示例，数据集是 Large Movie Review Dataset (**IMDb**，[http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/))，该数据集包含
    25,000 条训练评论和 25,000 条测试评论，每条评论都有一个二进制标签，表示其是正面还是负面。这个问题是一个 **多对一** 的关系类型，正如我们在
    *循环神经网络 (**RNNs)* 部分定义的那样。
- en: 'The sentiment analysis model is displayed in the following diagram:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析模型显示在下图中：
- en: '![Figure 6.15 – Sentiment analysis with word embeddings and LSTM](img/B19627_06_15.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.15 – 使用词嵌入和 LSTM 的情感分析](img/B19627_06_15.jpg)'
- en: Figure 6.15 – Sentiment analysis with word embeddings and LSTM
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15 – 使用词嵌入和 LSTM 的情感分析
- en: 'Let’s describe the model components (these are valid for any text classification
    algorithm):'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述一下模型组件（这些适用于任何文本分类算法）：
- en: Each word of the sequence is replaced with its embedding vector. These embeddings
    can be produced with word2vec.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 序列中的每个单词都被它的嵌入向量替代。这些嵌入可以通过 word2vec 生成。
- en: The word embedding is fed as input to the LSTM cell.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词嵌入作为输入传递给 LSTM 单元。
- en: The cell output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/556.png),
    serves as input to an FC layer with two output units and softmax. The softmax
    output represents the probability of the review being positive (1) or negative
    (0).
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单元格输出， ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/556.png)，作为输入传递给具有两个输出单元和
    softmax 的 FC 层。softmax 输出表示评论是正面（1）还是负面（0）的概率。
- en: The network can be produced with Word2Vec.
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络可以通过 Word2Vec 生成。
- en: The output for the final element of the sequence is taken as a result of the
    whole sequence.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 序列的最后一个元素的输出被作为整个序列的结果。
- en: 'To implement this example, we’ll use PyTorch and the TorchText package. It
    consists of data processing utilities and popular datasets for natural language.
    We’ll only include the interesting portions of the code, but the full example
    is available in this book’s GitHub repo. With that, let’s start:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这个例子，我们将使用 PyTorch 和 TorchText 包。它包含数据处理工具和流行的自然语言数据集。我们只会包括代码中的有趣部分，但完整示例可以在本书的
    GitHub 仓库中找到。有了这些，我们开始吧：
- en: 'Define the device (by default, this is GPU with a fallback on CPU):'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义设备（默认情况下，这是 GPU，并有 CPU 后备）：
- en: '[PRE9]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Start the training and testing dataset pipeline. First, define the `basic_english`
    tokenizer, which splits the text on spaces (that is, word tokenization):'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动训练和测试数据集管道。首先，定义 `basic_english` 分词器，它通过空格分割文本（即词汇分词）：
- en: '[PRE10]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, use `tokenizer` to build the token `vocabulary`:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用 `tokenizer` 来构建 token `vocabulary`：
- en: '[PRE11]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, `IMDB(split='train')` provides an iterator of all movie reviews in the
    training set (each review is represented as a string). The `yield_tokens(IMDB(split='train'))`
    generator iterates over all samples and splits them into words. The result serves
    as input to `build_vocab_from_iterator`, which iterates over the tokenized samples
    and builds the token `vocabulary`. Note that the vocabulary only includes training
    samples. Therefore, any token that exists in the test set (but not the training
    one) will be replaced with the default unknown `<``unk>` token.
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，`IMDB(split='train')` 提供了一个迭代器，用于访问训练集中的所有电影评论（每条评论都表示为一个字符串）。`yield_tokens(IMDB(split='train'))`
    生成器遍历所有样本并将它们分割成单词。结果作为输入传递给 `build_vocab_from_iterator`，该函数遍历已 token 化的样本并构建
    token `vocabulary`。请注意，词汇表仅包括训练样本。因此，任何出现在测试集（但不在训练集中的）中的 token，都将被替换为默认的未知 `<unk>`
    token。
- en: 'Next, define the `collate_batch` function, which takes a `batch` of tokenized
    samples with varying lengths, and concatenates them in a single long sequence
    of tokens:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义 `collate_batch` 函数，该函数接收一个包含不同长度的 token 化样本的 `batch`，并将它们拼接成一个长的 token
    序列：
- en: '[PRE12]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, the `samples` list aggregates all tokenized `_sample` instances of `batch`.
    In the end, they are concatenated into a single list. The `offsets` list contains
    the offset from the start of each concatenated sample. This information makes
    it possible to reverse-split the long `samples` sequence into separate items again.
    The purpose of the function is to create a compressed `batch` representation.
    This is necessary because of the varying length of each sample. The alternative
    would be to pad all samples to match the length of the longest one so that they
    can fit in the batch tensor. Fortunately, PyTorch provides us with the `offsets`
    optimization to avoid this. Once we feed the compressed batch to the RNN, it will
    automatically reverse it back into separate samples.
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，`samples` 列表聚合了 `batch` 所有分词后的 `_sample` 实例。最终，它们会被拼接成一个单一的列表。`offsets`
    列表包含每个拼接样本的起始偏移量。这些信息使得可以将长的 `samples` 序列逆向拆分回单独的项目。该函数的目的是创建一个压缩的 `batch` 表示。这是必需的，因为每个样本的长度不同。另一种做法是将所有样本填充到与最长样本相同的长度，以便它们能够适配批量张量。幸运的是，PyTorch
    提供了 `offsets` 优化来避免这一点。一旦我们将压缩批次传入 RNN，它会自动将其逆转回单独的样本。
- en: 'Then, we define the LSTM model:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义了 LSTM 模型：
- en: '[PRE13]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The model implements the scheme we introduced at the start of this section.
    As its name suggests, the `embedding` property (an instance of `EmbeddingBag`)
    maps the token (in our case, word) index to its embedding vector. We can see that
    the constructor takes the vocabulary size (`num_embeddings`) and the embedding
    vector size (`embedding_dim`). In theory, we could initialize `EmbeddingBag` with
    pre-computed Word2Vec embedding vectors. But in our case, we’ll simply use random
    initialization and let the model learn them as part of the training. `embedding`
    also takes care of the compressed batch representation (hence the `offsets` parameter
    in the `forward` method). The embedding’s output serves as input to the `rnn`
    LSTM cell, which, in turn, feeds the output `fc` layer.
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该模型实现了我们在本节开始时介绍的方案。顾名思义，`embedding` 属性（一个 `EmbeddingBag` 实例）将 token（在我们这里是单词）索引映射到其嵌入向量。我们可以看到构造函数接受词汇表大小（`num_embeddings`）和嵌入向量的维度（`embedding_dim`）。理论上，我们可以用预先计算好的
    Word2Vec 嵌入向量初始化 `EmbeddingBag`。但在我们的例子中，我们将使用随机初始化，并让模型在训练过程中学习这些向量。`embedding`
    还处理了压缩批量表示（因此在 `forward` 方法中有 `offsets` 参数）。嵌入的输出作为输入传入 `rnn` LSTM 单元，进而将输出传递给
    `fc` 层。
- en: Define the `train_model(model, cost_function, optimizer, data_loader)` and `test_model(model,
    cost_function, data_loader)` functions. These are almost the same functions that
    we first defined in [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079), so we won’t
    include them here. However, they have been adapted to the compressed batch representation
    and the additional `offsets` parameter.
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `train_model(model, cost_function, optimizer, data_loader)` 和 `test_model(model,
    cost_function, data_loader)` 函数。这些函数几乎与我们在[*第 3 章*](B19627_03.xhtml#_idTextAnchor079)中首次定义的相同，因此我们不会在这里再次列出它们。不过，它们已经适配了压缩批量表示和额外的
    `offsets` 参数。
- en: 'Proceed with the experiment. Instantiate the LSTM model, the cross-entropy
    cost function, and the Adam optimizer:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续实验。实例化 LSTM 模型、交叉熵损失函数和 Adam 优化器：
- en: '[PRE14]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Define `train_dataloader`, `test_dataloader`, and their respective datasets
    (use a mini-batch size of 64):'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `train_dataloader`、`test_dataloader` 及其各自的数据集（使用 64 的小批量大小）：
- en: '[PRE15]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Run the training for 5 epochs:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练 5 个周期：
- en: '[PRE16]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The model achieves a test accuracy in the realm of 87%.
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在测试集上的准确率达到了 87%。
- en: This concludes our small practical example of LSTM text classification. Coincidentally,
    it also concludes this chapter.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们关于 LSTM 文本分类的简单实用示例。巧合的是，这也标志着本章的结束。
- en: Summary
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced two complementary topics – NLP and RNNs. We discussed
    the tokenization technique and the most popular tokenization algorithms – BPE,
    WordPiece, and Unigram. Then, we introduced the concept of word embedding vectors
    and the Word2Vec algorithm to produce them. We also discussed the *n*-gram LM,
    which provided us with a smooth transition to the topic of RNNs. There, we implemented
    a basic RNN example and introduced two of the most advanced RNN architectures
    – LSTM and GRU. Finally, we implemented a sentiment analysis model.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了两个互补的主题——NLP 和 RNN。我们讨论了分词技术以及最流行的分词算法——BPE、WordPiece 和 Unigram。接着，我们介绍了词嵌入向量的概念以及生成它们的
    Word2Vec 算法。我们还讨论了 *n* -gram 语言模型，这为我们平滑地过渡到 RNN 的话题。在那里，我们实现了一个基本的 RNN 示例，并介绍了两种最先进的
    RNN 架构——LSTM 和 GRU。最后，我们实现了一个情感分析模型。
- en: In the next chapter, we’ll supercharge our NLP potential by introducing the
    attention mechanism and transformers.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将通过引入注意力机制和变压器来超级增强我们的NLP潜力。
