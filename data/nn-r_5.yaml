- en: Training and Visualizing a Neural Network in R
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在R中训练和可视化神经网络
- en: As seen in [Chapters 1](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4),
    *Neural Network and Artificial Intelligence Concepts*, and [Chapter 2](part0056.html#1LCVG0-263fb608a19f4bb5955f37a7741ba5c4),
    *Learning Process in Neural Networks*, training a neural network model forms the
    basis for building a neural network.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[第1章](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4)《神经网络与人工智能概念》、[第2章](part0056.html#1LCVG0-263fb608a19f4bb5955f37a7741ba5c4)《神经网络中的学习过程》中所见，训练神经网络模型为构建神经网络奠定了基础。
- en: Feed-forward and backpropagation are the techniques used to determine the weights
    and biases of the model. The weights can never be zero but the biases can be zero.
    To start with, the weights are initialized a random number, and by gradient descent,
    the errors are minimized; we get a set of best possible weights and biases for
    the model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈和反向传播是用于确定模型权重和偏置的技术。权重不能为零，但偏置可以为零。首先，权重会被初始化为一个随机数，通过梯度下降法，误差最小化；我们获得一组最佳的权重和偏置值。
- en: 'Once the model is trained using any of the R functions, we can pass on the
    independent variables to predict the target or unknown variable. In this chapter,
    we will use a publicly available dataset to train, test, and visualize a neural
    network model. The following items will be covered:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型使用任何R函数训练完成，我们可以将独立变量传递给模型，以预测目标或未知变量。在本章中，我们将使用一个公开可用的数据集来训练、测试并可视化一个神经网络模型。以下内容将被涵盖：
- en: Training, testing, and evaluating a dataset using NN model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络模型训练、测试和评估数据集
- en: Visualizing the NN model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化神经网络模型
- en: Early stopping
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提前停止
- en: Avoiding overfitting
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免过拟合
- en: Generalization of NN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的泛化
- en: Scaling of NN parameters
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络参数的缩放
- en: Ensemble models
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成模型
- en: By the end of the chapter, we will understand how to train, test, and evaluate
    a dataset using NN model. We will learn how to visualize the NN model in R environment.
    We will cover the concepts like early stopping, avoiding overfitting, generalization
    of NN, and scaling of NN parameters.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，我们将理解如何使用神经网络模型训练、测试和评估数据集。我们将学习如何在R环境中可视化神经网络模型。我们将讨论如提前停止、避免过拟合、神经网络泛化和神经网络参数缩放等概念。
- en: Data fitting with neural network
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络进行数据拟合
- en: 'Data fitting is the process of building a curve or a mathematical function
    that has the best match with a set of previously collected points. The curve fitting
    can relate to both interpolations, where exact data points are required, and smoothing,
    where a flat function is built that approximates the data. The approximate curves
    obtained from the data fitting can be used to help display data, to predict the
    values of a function where no data is available, and to summarize the relationship
    between two or more variables. In the following figure is shown a linear interpolation
    of collected data:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据拟合是构建一条与一组先前收集的数据点最匹配的曲线或数学函数的过程。曲线拟合可以涉及插值，即要求精确的数据点，也可以是平滑拟合，即构建一个平坦的函数来近似数据。通过数据拟合获得的近似曲线可以帮助展示数据、预测没有数据的函数值，并总结两个或多个变量之间的关系。下图展示了收集数据的线性插值：
- en: '![](img/00097.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00097.jpeg)'
- en: Data fitting is the process of training a neural network on a set of inputs
    in order to produce an associated set of target outputs. Once the neural network
    has fit the data, it forms a generalization of the input-output relationship and
    can be used to generate outputs for inputs it was not trained on.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据拟合是将一组输入数据训练神经网络，以产生相关的目标输出数据的过程。一旦神经网络完成数据拟合，它就形成了输入输出关系的泛化，并可以用于生成未曾训练过的输入数据的输出。
- en: The fuel consumption of vehicles has always been studied by the major manufacturers
    of the entire planet. In an era characterized by oil refueling problems and even
    greater air pollution problems, fuel consumption by vehicles has become a key
    factor. In this example, we will build a neural network with the purpose of predicting
    the fuel consumption of the vehicles according to certain characteristics.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 车辆的燃油消耗一直是全球主要制造商研究的课题。在一个以石油补给问题和更严重的空气污染问题为特征的时代，车辆的燃油消耗已成为一个关键因素。在本例中，我们将构建一个神经网络，目的是根据某些特征预测车辆的燃油消耗。
- en: 'To do this, use the `Auto` dataset contained in the `ISLR` package that we
    have already used in an example in [Chapter 3](part0081.html#2D7TI0-263fb608a19f4bb5955f37a7741ba5c4),
    *Deep Learning Using Multilayer Neural Networks*. The `Auto` dataset contain gas
    mileage, horsepower, and other information for 392 vehicles. It is a data frame
    with 392 observations on the following nine variables:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，使用我们在[第3章](part0081.html#2D7TI0-263fb608a19f4bb5955f37a7741ba5c4)的示例中已经使用过的
    `ISLR` 包中的 `Auto` 数据集，*使用多层神经网络进行深度学习*。`Auto` 数据集包含392辆车的油耗、马力和其他信息。它是一个数据框，包含392个观测值，涉及以下九个变量：
- en: '`mpg`: Miles per gallon'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mpg`：每加仑英里数'
- en: '`cylinders`: Number of cylinders between 4 and 8'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cylinders`：气缸数量（4到8个之间）'
- en: '`displacement`: Engine displacement (cubic inches)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`displacement`：发动机排量（立方英寸）'
- en: '`horsepower`: Engine horsepower'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`horsepower`：发动机马力'
- en: '`weight`: Vehicle weight (lbs)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight`：车辆重量（磅）'
- en: '`acceleration`: Time to accelerate from 0 to 60 mph (sec)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`acceleration`：从0加速到60英里每小时的时间（秒）'
- en: '`year`: Model year (modulo 100)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`year`：型号年份（模100）'
- en: '`origin`: Origin of car (American, European, Japanese)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`origin`：汽车的来源（美国、欧洲、日本）'
- en: '`name`: Vehicle name'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`：车辆名称'
- en: 'The following is the code that we will use in this example:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们将在本示例中使用的代码：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As usual, we will analyze the code line-by-line, by explaining in detail all
    the features applied to capture the results.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正如以往一样，我们将逐行分析代码，详细解释应用于捕捉结果的所有特性。
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first two lines of the initial code are used to load the libraries needed
    to run the analysis.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 初始代码的前两行用于加载运行分析所需的库。
- en: Remember, to install a library that is not present in the initial distribution
    of R, you must use the `install.package` function. This is the main function to
    install packages. It takes a vector of names and a destination library, downloads
    the packages from the repositories and installs them. This function should be
    used only once and not every time you run the code.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，要安装在R的初始发行版中没有的库，必须使用`install.package`函数。这是安装包的主要函数。它接受一个名称向量和一个目标库，从仓库中下载包并进行安装。这个函数只需要使用一次，而不是每次运行代码时都调用。
- en: The `neuralnet` library is used to train neural networks using backpropagation,
    **resilient backpropagation** (**RPROP**) with or without weight backtracking,
    or the modified **globally convergent version** (**GRPROP**). The function allows
    flexible settings through custom-choice of error and activation function. Furthermore,
    the calculation of generalized weights is implemented.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`neuralnet` 库用于通过反向传播、**弹性反向传播** (**RPROP**)（带或不带权重回溯）或修改后的**全局收敛版本** (**GRPROP**)训练神经网络。该函数允许通过自定义选择误差和激活函数来灵活设置。此外，还实现了广义权重的计算。'
- en: The `ISLR` library contains a set of datasets freely usable for our examples.
    This is a series of data collected during major studies conducted by research
    centers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`ISLR` 库包含了一组可以自由使用的数据集，供我们在示例中使用。这是一系列在研究中心进行的重要研究中收集的数据。'
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This command loads the `Auto` dataset, which, as we anticipated, is contained
    in the `ISLR` library, and saves it in a given dataframe. Use the `View` function
    to view a compact display of the structure of an arbitrary R object. The following
    screenshot shows some of the data contained in the `Auto` dataset:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令加载`Auto`数据集，正如我们预期的那样，它包含在`ISLR`库中，并将其保存到给定的数据框中。使用`View`函数查看任意R对象的结构的紧凑显示。以下截图显示了`Auto`数据集中的一些数据：
- en: '![](img/00098.jpeg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00098.jpeg)'
- en: 'As you can see, the database consists of 392 rows and 9 columns. The rows represent
    392 commercial vehicles from 1970 to 1982\. The columns represent the 9 characteristics
    collected for each car, in order: `mpg`, `cylinders`, `displacement`, `horsepower`,
    `weight`, `acceleration`, `year`, `origin`, and `name`.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，该数据库由392行和9列组成。行表示从1970到1982年期间的392辆商用车。列则表示为每辆车收集的9个特征，依次为：`mpg`、`cylinders`、`displacement`、`horsepower`、`weight`、`acceleration`、`year`、`origin`和`name`。
- en: Exploratory analysis
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性分析
- en: Before starting with data analysis through the building and training of a neural
    network, we conduct an exploratory analysis to understand how data is distributed
    and extract preliminary knowledge.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始通过构建和训练神经网络进行数据分析之前，我们进行探索性分析，以了解数据如何分布，并提取初步知识。
- en: 'We can begin our explorative analysis by tracing a plot of predictors versus
    target. We recall in this respect that in our analysis, the predictors are the
    following variables: `cylinders`, `displacement`, `horsepower`, `weight`, `acceleration`,
    `year`, `origin`, and `name`. The target is the `mpg` variable that contains measurements
    of the miles per gallon of 392 sample cars.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过绘制预测因子与目标变量之间的图表来开始我们的探索性分析。在这方面，我们回顾一下我们的分析中的预测因子变量包括：`cylinders`、`displacement`、`horsepower`、`weight`、`acceleration`、`year`、`origin`和`name`。目标变量是`mpg`，它包含392辆样本车的每加仑英里数数据。
- en: 'Suppose we want to examine the weight and mileage of cars from three different
    origins, as shown in the next graph, using the following code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要检查来自三个不同地区的汽车的重量和油耗，如下图所示，并使用以下代码：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To plot the chart, we used the `plot()` function, specifying what to point
    on the *x* axis (`weight`), what to point on the *y* axis (`mpg`), and finally,
    based on which variable to group the data (`origin`), as shown in the following
    graph:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绘制图表，我们使用了`plot()`函数，指定* x *轴（`weight`）、* y *轴（`mpg`），并最终基于哪个变量对数据进行分组（`origin`），如下面的图表所示：
- en: '![](img/00099.jpeg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00099.jpeg)'
- en: 'Remember the number in the `origin` column correspond at the following zone:
    1= America, 2=Europe, and 3=Japan). From the analysis of the previous graph, we
    can find that fuel consumption increases with weight gain. Let''s remember that
    the target measures the miles per gallon, so how many miles are going with a gallon
    of fuel. It follows that the greater the value of mpg (miles per gallon), the
    lower the fuel consumption.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`origin`列中的数字对应以下地区：1=美国，2=欧洲，3=日本）。通过对前述图表的分析，我们发现油耗随着重量的增加而增加。我们要记住，目标变量衡量的是每加仑油行驶的英里数，即多少英里可以行驶一加仑的油。因此，mpg值（每加仑英里数）越大，油耗越低。
- en: Another consideration that comes from plot analysis is that cars produced in
    America are heavier. In fact, in the right part of the chart (which corresponds
    to higher values of weight), there are only cars produced in that area.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个来自图表分析的观察是，美国生产的汽车较重。事实上，在图表的右侧（对应较高的重量值）只出现了该地区生产的汽车。
- en: Finally, if we focus our analysis on the left of the graph, in the upper part
    that corresponds to the lowest fuel consumption, we find in most cases Japanese
    and European cars. In conclusion, we can note that cars that have the lowest fuel
    consumption are Japanese.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果我们将分析集中在图表的左侧，即对应最低油耗的上部，我们会发现大多数情况下是日本和欧洲的汽车。总之，我们可以得出结论，油耗最低的汽车是日本制造的。
- en: Now, let's see the other graphs, that is, what we get if we plot the remaining
    numeric predictors (`cylinders`, `displacement`, `horsepower`, and `acceleration`)
    versus target (`mpg`).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看其他的图表，即如果我们将剩余的数值型预测因子（`cylinders`、`displacement`、`horsepower`和`acceleration`）与目标（`mpg`）进行比较，我们得到什么结果。
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: For space reasons, we decided to place the four charts in one. R makes it easy
    to combine multiple plots into one general graph, using the `par()` function.
    Using the par( ) function, we can include the option mfrow=c(nrows, ncols) to
    create a matrix of nrows x ncols plots that are filled in by row. For example
    the option mfrow=c(3,2) creates a matrix plot with 3 rows and 2 columns. In addition,
    the option mfcol=c(nrows, ncols) fills in the matrix by columns.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 出于空间考虑，我们决定将四个图表合并为一个图表。R语言使得将多个图表组合成一个总图变得简单，使用`par()`函数即可。通过使用`par()`函数，我们可以选择mfrow=c(nrows,
    ncols)来创建一个nrows x ncols的图表矩阵，按行填充。例如，mfrow=c(3,2)选项创建一个3行2列的矩阵图表。此外，mfcol=c(nrows,
    ncols)选项则是按列填充矩阵。
- en: 'In the following figure are shown 4 plot arranged in a matrix of 2 rows and
    two columns:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中显示了4个图表，排列成2行2列的矩阵：
- en: '![](img/00100.jpeg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00100.jpeg)'
- en: From the analysis of the previous figure, we find confirmation of what has already
    been mentioned earlier. We can note that cars with higher horsepower have higher
    fuel consumption. The same thing we can say about the engine displacement; also
    in this case, vehicles with higher displacement have higher fuel consumption.
    Again, cars with higher horsepower and displacement values are produced in America.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 从前图的分析中，我们确认了之前提到的内容。我们可以注意到，马力较大的汽车油耗更高。同样的结论也适用于发动机排量；在这种情况下，排量较大的汽车油耗更高。再次强调，马力和排量较大的汽车多为美国生产。
- en: Conversely, cars with higher acceleration values have lower fuel consumption.
    This fact is due to the lesser weight that such cars have. Usually, heavy cars
    are slower in acceleration.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，加速值较高的汽车通常有更低的油耗。这是因为这类车的重量较轻。通常来说，重型汽车加速较慢。
- en: Neural network model
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络模型
- en: In [Chapter 2](part0056.html#1LCVG0-263fb608a19f4bb5955f37a7741ba5c4), *Learning
    Process in Neural Networks*, we scaled the data before building the network. On
    that occasion, we pointed out that it is good practice to normalize the data before
    training a neural network. With normalization, data units are eliminated, allowing
    you to easily compare data from different locations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](part0056.html#1LCVG0-263fb608a19f4bb5955f37a7741ba5c4)，*神经网络中的学习过程*中，我们在构建网络之前进行了数据缩放。当时我们提到，训练神经网络之前对数据进行标准化是一个良好的实践。通过标准化，可以消除数据的单位，使得来自不同位置的数据能够轻松进行比较。
- en: It is not always necessary to normalize numeric data. However, it has been shown
    that when numeric values are normalized, neural network formation is often more
    efficient and leads to better prediction. In fact, if numeric data are not normalized
    and the sizes of two predictors are very distant, a change in the value of a neural
    network weight has much more relative influence on higher value.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有情况下都需要对数值数据进行标准化。然而，研究表明，当数值数据经过标准化时，神经网络的构建通常更高效，并且能够获得更好的预测效果。事实上，如果数值数据没有标准化，而两个预测变量的规模差异非常大，那么神经网络权重的变化对较大值的影响将更为显著。
- en: 'There are several standardization techniques; in [Chapter 2](part0056.html#1LCVG0-263fb608a19f4bb5955f37a7741ba5c4),
    *Learning Process in Neural Networks*, we adopted min-max standardization. In
    this case, we will adopt *Z*-scores normalization. This technique consists of
    subtracting the mean of the column to each value in a column, and then dividing
    the result for the standard deviation of the column. The formula to achieve this
    is the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种标准化技术；在[第2章](part0056.html#1LCVG0-263fb608a19f4bb5955f37a7741ba5c4)，*神经网络中的学习过程*中，我们采用了最小-最大标准化。在本章中，我们将采用
    *Z* 分数标准化。该技术包括从每一列的每个值中减去该列的均值，然后将结果除以该列的标准差。实现这一点的公式如下：
- en: '![](img/00101.jpeg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00101.jpeg)'
- en: In summary, the *Z* score (also called standard score) represents the number
    of standard deviations with which the value of an observation point or data is
    greater than the mean value of what is observed or measured. Values above the
    mean have positive *Z*-scores, while values below the mean have negative *Z*-scores.
    The *Z*-score is a quantity without dimension, obtained by subtracting the population
    mean from a single rough score and then dividing the difference for the standard
    deviation of the population.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，*Z* 分数（也称为标准分数）表示观察值或数据相对于所观察或测量的平均值的标准差个数。大于均值的值有正的 *Z* 分数，而小于均值的值有负的
    *Z* 分数。*Z* 分数是一个无单位的量，通过从单个粗略分数中减去总体均值，然后将差值除以总体的标准差得到。
- en: Before applying the method chosen for normalization, you must calculate the
    mean and standard deviation values of each database column. To do this, we use
    the `apply` function. This function returns a vector or an array or a list of
    values obtained by applying a function to margins of an array or matrix. Let's
    understand the meaning of the arguments used.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用选择的标准化方法之前，你必须计算每个数据库列的均值和标准差。为此，我们使用 `apply` 函数。该函数返回一个向量、数组或列表，通过将函数应用于数组或矩阵的维度来获取值。让我们来理解所使用的参数的含义。
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The first line allows us to calculate the mean of each variable going to the
    second line, allowing us to calculate the standard deviation of each variable.
    Let''s see how we used the function `apply()`. The first argument of the `apply`
    function specifies the dataset to apply the function to, in our case, the dataset
    named data. In particular, we have only considered the first six numeric variables;
    the other ones we will use for other purposes. The second argument must contain
    a vector giving the subscripts which the function will be applied over. In our
    case, one indicates rows and two indicates columns. The third argument must contain
    the function to be applied; in our case, the `mean()` function in the first row
    and the `sd()` function in the second row. The results are shown as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行代码可以让我们计算每个变量的均值，然后进入第二行，计算每个变量的标准差。让我们看看如何使用`apply()`函数。`apply`函数的第一个参数指定了数据集，表示应用该函数的目标数据集，在我们的案例中是名为data的数据集。具体来说，我们只考虑了前六个数值型变量，其他变量将用于其他目的。第二个参数必须包含一个向量，给出应用函数时的下标。在我们的案例中，1表示行，2表示列。第三个参数必须包含要应用的函数；在我们的案例中，第一行是`mean()`函数，第二行是`sd()`函数。结果如下所示：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To normalize the data, we use the `scale()` function, which is a generic function
    whose default method centers and/or scales the columns of a numeric matrix:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了规范化数据，我们使用`scale()`函数，这是一种通用函数，其默认方法会对数字矩阵的列进行居中和/或缩放：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s take a look at the data transformed by normalization:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下经过规范化处理的数据：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The results are as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s now split the data for the training and the test:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来将数据分割为训练集和测试集：
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the first line of the code just suggested, the dataset is split into 70:30,
    with the intention of using 70 percent of the data at our disposal to train the
    network and the remaining 30 percent to test the network. In the second and third
    lines, the data of the dataframe named data is subdivided into two new dataframes,
    called `train_data` and `test_data`. Now we have to build the function to be submitted
    to the network:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在刚才建议的第一行代码中，数据集被按70:30的比例拆分，目的是使用70%的数据训练网络，剩下的30%用于测试网络。在第二行和第三行中，名为data的数据框被细分为两个新的数据框，分别叫做`train_data`和`test_data`。现在，我们需要构建提交给网络的函数：
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the first line, we recover all the variable names in the `data_scaled` dataframe,
    using the `names()` function. In the second line, we build formula that we will
    use to train the network. What does this formula represent?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一行，我们通过使用`names()`函数来恢复`data_scaled`数据框中的所有变量名。在第二行，我们构建了一个公式，用于训练神经网络。这个公式代表了什么？
- en: 'The models fitted by the `neuralnet()` function are specified in a compact
    symbolic form. The ~ operator is basic in the formation of such models. An expression
    of the form *y* ~ model is interpreted as a specification that the response *y*
    is modelled by a predictor specified symbolically by model. Such a model consists
    of a series of terms separated by + operators. The terms themselves consist of
    variable and factor names separated by : operators. Such a term is interpreted
    as the interaction of all the variables and factors appearing in the term. Let''s
    look at the formula we set:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`neuralnet()`函数拟合的模型以紧凑的符号形式表示。~运算符是构建这类模型的基础。像*y* ~ model 这样的表达式表示响应*y*是通过符号表示的预测变量model进行建模的。这样的模型由一系列通过+运算符分隔的项组成。每一项由变量和因子名通过:运算符分隔。这样的项表示出现在该项中的所有变量和因子的交互作用。让我们来看一下我们设置的公式：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now we can build and train the network.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以构建并训练神经网络了。
- en: 'In [Chapter 3](part0081.html#2D7TI0-263fb608a19f4bb5955f37a7741ba5c4), *Deep
    Learning Using Multilayer Neural Networks*, we said that to choose the optimal
    number of neurons, we need to know that:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](part0081.html#2D7TI0-263fb608a19f4bb5955f37a7741ba5c4)，*使用多层神经网络进行深度学习*中，我们提到，为了选择最佳的神经元数量，我们需要知道：
- en: Small number of neurons will lead to high error for your system, as the predictive
    factors might be too complex for a small number of neurons to capture
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元数量过少会导致系统误差较大，因为预测因子可能对较少的神经元来说过于复杂，无法捕捉。
- en: Large number of neurons will overfit your training data and not generalize well
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元数量过多会导致过拟合训练数据，且无法很好地泛化。
- en: The number of neurons in each hidden layer should be somewhere between the size
    of the input and the output layer, potentially the mean
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个隐藏层中的神经元数量应该介于输入层和输出层的大小之间，可能是它们的均值。
- en: The number of neurons in each hidden layer shouldn't exceed twice the number
    of input neurons, as you are probably grossly overfit at this point
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个隐藏层中的神经元数量不应超过输入神经元数量的两倍，因为此时你可能已经严重过拟合。
- en: In this case, we have five input variables (`cylinders`, `displacement`, `horsepower`,
    `weight`, and `acceleration`) and one variable output (`mpg`). We choose to set
    three neurons in the hidden layer.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在此情况下，我们有五个输入变量（`cylinders`、`displacement`、`horsepower`、`weight` 和 `acceleration`），以及一个输出变量（`mpg`）。我们选择在隐藏层设置三个神经元。
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The hidden argument accepts a vector with the number of neurons for each hidden
    layer, while the argument `linear.output` is used to specify whether we want to
    do regression (`linear.output=TRUE`) or classification (`linear.output=FALSE`).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: hidden 参数接受一个包含每个隐藏层神经元数量的向量，而 `linear.output` 参数用于指定我们是进行回归（`linear.output=TRUE`）还是分类（`linear.output=FALSE`）。
- en: 'The algorithm used in `neuralnet()`, by default, is based on the resilient
    backpropagation without weight backtracking and additionally modifies one learning
    rate, either the learning rate associated with the smallest absolute gradient
    (`sag`) or the smallest learning rate (`slr`) itself. The `neuralnet()` function
    returns an object of class `nn`. An object of class `nn` is a list containing
    at most the components shown in the following table:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`neuralnet()` 中使用的算法默认基于弹性反向传播算法，不包括权重回溯，并且额外修改了一个学习率，即与最小绝对梯度（`sag`）相关的学习率或最小学习率（`slr`）本身。`neuralnet()`
    函数返回一个 `nn` 类的对象。`nn` 类的对象是一个列表，最多包含以下表格中显示的组件：'
- en: '| **Components** | **Description** |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| **组件** | **描述** |'
- en: '| `call` | The matched call. |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| `call` | 匹配的调用。 |'
- en: '| `response` | Extracted from the `data` argument. |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| `response` | 从 `data` 参数中提取。 |'
- en: '| `covariate` | The variables extracted from the data argument. |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| `covariate` | 从数据参数中提取的变量。 |'
- en: '| `model.list` | A list containing the covariates and the `response` variables
    extracted from the `formula` argument. |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| `model.list` | 一个列表，包含从 `formula` 参数中提取的协变量和 `response` 变量。 |'
- en: '| `err.fct` | The error function. |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| `err.fct` | 错误函数。 |'
- en: '| `act.fct` | The activation function. |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `act.fct` | 激活函数。 |'
- en: '| `data` | The data argument. |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `data` | 数据参数。 |'
- en: '| `net.result` | A list containing the overall result of the neural network
    for every repetition. |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| `net.result` | 一个列表，包含每次重复的神经网络整体结果。 |'
- en: '| `weights` | A list containing the fitted weights of the neural network for
    every repetition. |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| `weights` | 一个列表，包含每次重复的神经网络拟合权重。 |'
- en: '| `generalized.weights` | A list containing the generalized weights of the
    neural network for every repetition. |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| `generalized.weights` | 一个列表，包含每次重复神经网络的广义权重。 |'
- en: '| `result.matrix` | A matrix containing the reached threshold, needed steps,
    error, AIC and BIC (if computed), and weights for every repetition. Each column
    represents one repetition. |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| `result.matrix` | 一个矩阵，包含每次重复所达到的阈值、所需步骤、误差、AIC 和 BIC（如果已计算），以及权重。每一列表示一次重复。
    |'
- en: '| `startweights` | A list containing the startweights of the neural network
    for every repetition. |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| `startweights` | 一个列表，包含每次重复神经网络的起始权重。 |'
- en: 'To produce result summaries of the results of the model, we use the `summary()`
    function:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成模型结果的摘要，我们使用 `summary()` 函数：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'For each component of the neural network model are displayed three features:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于神经网络模型的每个组件，显示了三个特征：
- en: '**Length**: This is component length, that is how many elements of this type
    are contained in it'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长度**：这是组件的长度，即其中包含多少个此类型的元素。'
- en: '**Class**: This contains specific indication on the component class'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别**：此项包含组件类别的具体说明。'
- en: '**Mode**: This is the type of component (numeric, list, function, logical,
    and so on)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式**：这是组件的类型（数值、列表、函数、逻辑等）。'
- en: To plot the graphical representation of the model with the weights on each connection,
    we can use the `plot()` function. The `plot()` function is a generic function
    for the representation of objects in R. Generic function means that it is suitable
    for different types of objects, from variables to tables to complex function outputs,
    producing different results. Applied to a nominal variable, it will produce a
    bar graph. Applied to a cardinal variable, it will produce a scatterplot. Applied
    to the same variable, but tabulated, that is, to its frequency distribution, it
    will produce a histogram. Finally, applied to two variables, a nominal and a cardinal,
    it will produce a boxplot.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 要绘制带有每个连接权重的模型的图形表示，我们可以使用`plot()`函数。`plot()`函数是R中用于表示对象的通用函数。通用函数意味着它适用于不同类型的对象，从变量到表格，再到复杂的函数输出，产生不同的结果。应用于名义变量时，它将生成条形图；应用于序数变量时，它将生成散点图；如果是相同的变量，但经过制表，即频率分布，它将生成直方图。最后，应用于两个变量，一个名义变量和一个序数变量，它将生成箱线图。
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The neural network plot is shown in the following graph:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络图显示在下图中：
- en: '![](img/00102.gif)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00102.gif)'
- en: In the previous graph, the black lines (these lines start from input nodes)
    show the connections between each layer and the weights on each connection, while
    the blue lines (these lines start from bias nodes which are distinguished by number
    1) show the bias term added in each step. The bias can be thought of as the intercept
    of a linear model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，黑色线条（这些线条从输入节点开始）显示了每一层之间的连接以及每个连接上的权重，而蓝色线条（这些线条从偏置节点开始，偏置节点通过数字1来区分）则显示了在每一步中添加的偏置项。可以把偏置看作是线性模型的截距。
- en: Though over time we have understood a lot about the mechanics that are the basis
    of the neural networks, in many respects, the model we have built and trained
    remains a black box. The fitting, weights, and model are not clear enough. We
    can be satisfied that the training algorithm is convergent and then the model
    is ready to be used.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管随着时间的推移我们已经了解了许多神经网络基础的机制，但在许多方面，我们构建和训练的模型仍然是一个“黑箱”。拟合、权重和模型本身并不够清晰。我们可以满意地认为训练算法是收敛的，然后模型就可以开始使用了。
- en: 'We can print on video, the weights and biases:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在视频中打印权重和偏置：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As can be seen, these are the same values that we can read in the network plot.
    For example, `cylinders.to.1layhid1 = 0.291091600669` is the weight for the connection
    between the input cylinders and the first node of the hidden layer.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，这些值与我们在网络图中看到的值相同。例如，`cylinders.to.1layhid1 = 0.291091600669`是输入“气缸数”与隐层第一个节点之间连接的权重。
- en: Now we can use the network to make predictions. For this, we had set aside 30
    percent of the data in the `test_data` dataframe. It is time to use it.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用网络进行预测。为此，我们已将`test_data`数据框中的30%的数据预留出来。现在是使用它的时候了。
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In our case, we applied the function to the `test_data` dataset, using only
    the columns from `2` to `6`, representing the input variables of the network.
    To evaluate the network performance, we can use the **Mean Squared Error** (**MSE**)
    as a measure of how far away our predictions are from the real data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们将这个函数应用于`test_data`数据集，仅使用`2`到`6`列，代表网络的输入变量。为了评估网络的性能，我们可以使用**均方误差**（**MSE**）作为衡量我们的预测与实际数据之间差距的标准。
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here `test_data$mpg` is the actual data and `predict_net_test$net.result` is
    the predicted data for the target of the analysis. Following is the result:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这里`test_data$mpg`是实际数据，`predict_net_test$net.result`是分析目标的预测数据。以下是结果：
- en: '[PRE20]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'It looks like a good result, but what do we compare it with? To get an idea
    of the accuracy of the network prediction, we can build a linear regression model:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来结果不错，但我们应该与什么进行比较呢？为了了解网络预测的准确性，我们可以构建一个线性回归模型：
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We build a linear regression model using the `lm` function. This function is
    used to fit linear models. It can be used to perform regression, single stratum
    analysis of variance, and analysis of covariance. To produce a summary of the
    results of model fitting obtained, we have used the `summary()` function, which
    returns the following results:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`lm`函数构建了一个线性回归模型。这个函数用于拟合线性模型。它可以用于执行回归、单层方差分析和协方差分析。为了生成模型拟合结果的摘要，我们使用了`summary()`函数，返回以下结果：
- en: '[PRE22]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now we make the prediction with the linear regression model using the data
    contained in the `test_data` dataframe:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用`test_data`数据框中的数据来进行线性回归模型的预测：
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we calculate the MSE for the regression model:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算回归模型的均方误差（MSE）：
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Following is the result:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是结果：
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: From the comparison between the two models (neural network model versus linear
    regression model), once again the neural network wins (0.26 versus 0.31).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 从两种模型（神经网络模型与线性回归模型）之间的比较中可以看出，神经网络再次获胜（0.26与0.31）。
- en: 'We now perform a visual comparison by drawing on a graph the actual value versus
    the predicted value, first for neural network and then for linear regression model:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们通过绘制图形进行视觉对比，先是神经网络的实际值与预测值，然后是线性回归模型的：
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The comparison between the performance of the neural network model (to the
    left) and the linear regression model (to the right) on the test set is plotted
    in the following graph:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络模型（左侧）与线性回归模型（右侧）在测试集上的表现对比如下图所示：
- en: '![](img/00103.gif)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00103.gif)'
- en: As we can see, the predictions by the neural network are more concentrated around
    the line than those by the linear regression model, even if you do not note a
    big difference.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，神经网络的预测值比线性回归模型的更集中在直线周围，尽管你可能没有注意到很大差异。
- en: Classifing breast cancer with a neural network
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络分类乳腺癌
- en: The breast is made up of a set of glands and adipose tissue and is placed between
    the skin and the chest wall. In fact, it is not a single gland, but a set of glandular
    structures, called lobules, joined together to form a lobe. In a breast, there
    are 15 to 20 lobes. The milk reaches the nipple from the lobules through small
    tubes called milk ducts.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 乳房由一组腺体和脂肪组织组成，位于皮肤和胸壁之间。实际上，它不是一个单一的腺体，而是一组叫做小叶的腺体结构组合而成的一个叶。在每个乳房中，有15到20个小叶。乳汁通过小管道（称为乳管）从小叶流向乳头。
- en: Breast cancer is a potentially serious disease if it is not detected and treated
    for a long time. It is caused by uncontrolled multiplication of some cells in
    the mammary gland that are transformed into malignant cells. This means that they
    have the ability to detach themselves from the tissue that has generated them
    to invade the surrounding tissues and eventually the other organs of the body.
    In theory, cancers can be formed from all types of breast tissues, but the most
    common ones are from glandular cells or from those forming the walls of the ducts.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 乳腺癌是一个潜在的严重疾病，如果长时间没有被发现并治疗。它是由乳腺中一些细胞的失控增殖所引起，这些细胞转化为恶性细胞。这意味着它们有能力从生成它们的组织中脱离，侵入周围组织，最终扩散到身体的其他器官。理论上，癌症可以从所有类型的乳腺组织中形成，但最常见的类型是来自腺体细胞或形成导管壁的细胞。
- en: The objective of this example is to identify each of a number of benign or malignant
    classes. To do this, we will use the data contained in the dataset named `BreastCancer`
    (Wisconsin Breast Cancer Database) contained in the `mlbench` package. This data
    has been taken from the UCI Repository Of Machine Learning Databases at DNA samples
    arrive periodically as Dr. Wolberg reports his clinical cases. The database therefore
    reflects this chronological grouping of the data. This grouping information appears
    immediately, having been removed from the data itself. Each variable, except for
    the first, was converted into 11 primitive numerical attributes with values ranging
    from 0 through 10\. There are 16 missing values.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 本例的目标是识别一系列良性或恶性类别中的每一个。为此，我们将使用名为`BreastCancer`的数据集（威斯康星乳腺癌数据库），该数据集包含在`mlbench`包中。这些数据来源于UCI机器学习数据库，在DNA样本定期到达时，沃尔伯格博士会报告他的临床案例。因此，数据库反映了这些数据的时间顺序分组。这个分组信息已经被从数据本身中移除。每个变量（除了第一个）被转换成了11个基本的数值属性，取值范围从0到10。有16个缺失值。
- en: 'The dataframes contain 699 observations on 11 variables—1 being a character
    variable, 9 being ordered or nominal, and 1 target class:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框包含699个观察值，涉及11个变量——其中1个是字符变量，9个是有序或名义变量，1个是目标类别：
- en: '`Id`: Sample code number'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Id`: 样本编码号'
- en: '`Cl.thickness`: Clump thickness'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Cl.thickness`: 聚集厚度'
- en: '`Cell.size`: Uniformity of cell size'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Cell.size`: 细胞大小均匀性'
- en: '`Cell.shape`: Uniformity of cell shape'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Cell.shape`: 细胞形态均匀性'
- en: '`Marg.adhesion`: Marginal adhesion'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Marg.adhesion`: 边缘粘附'
- en: '`Epith.c.size`: Single epithelial cell size'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Epith.c.size`: 单一上皮细胞大小'
- en: '`Bare.nuclei`: Bare nuclei'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Bare.nuclei`: 光核'
- en: '`Bl.cromatin`: Bland chromatin'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Bl.cromatin`: 平淡染色质'
- en: '`Normal.nucleoli`: Normal nucleoli'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Normal.nucleoli`: 正常核仁'
- en: '`Mitoses`: Mitoses'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Mitoses`: 有丝分裂'
- en: '`Class`: Class'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Class`: 类别'
- en: 'As said previously, the objective of this example is to identify each of a
    number of benign or malignant classes. The following is the code that we will
    use in this example:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，此示例的目标是识别一系列良性或恶性类别中的每一个。以下是我们将在此示例中使用的代码：
- en: '[PRE27]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We begin analyzing the code line-by-line, by explaining in detail all the features
    applied to capture the results.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始逐行分析代码，详细解释捕获结果所应用的所有特性。
- en: '[PRE28]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The first two lines of the initial code are used to load the libraries needed
    to run the analysis.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 初始代码的前两行用于加载运行分析所需的库。
- en: Remember, to install a library that is not present in the initial distribution
    of R, you must use the `install.package` function. This is the main function to
    install packages. It takes a vector of names and a destination library, downloads
    the packages from the repositories and installs them. This function should be
    used only once and not every time you run the code.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，要安装 R 的初始分布中没有的库，您必须使用 `install.package` 函数。这是安装包的主要函数。它接受名称向量和目标库，从存储库下载包并安装它们。此函数应仅使用一次，而不是每次运行代码时都使用。
- en: The `mlbench` library contains a collection of artificial and real-world machine
    learning benchmark problems, including, for example, several datasets from the
    UCI Repository.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlbench` 库包含一系列人工和真实世界的机器学习基准问题，包括来自 UCI 仓库的几个数据集。'
- en: 'The `neuralnet` library is used to train neural networks using backpropagation,
    RPROP with or without weight backtracking, or the modified GRPROP. The function
    allows flexible settings through custom-choice of error and activation function.
    Furthermore, the calculation of generalized weights is implemented. A brief description
    of the nnet package, extracted from the official documentation, is shown in the
    following table:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`neuralnet` 库用于使用反向传播、带或不带权重回溯的 RPROP，或修改后的 GRPROP 训练神经网络。该函数通过自定义选择误差和激活函数实现灵活设置。此外，实现了广义权重的计算。从官方文档中提取的
    nnet 包简要描述显示在以下表格中：'
- en: '| `neuralnet`: Training of neural networks |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| `neuralnet`：神经网络的训练 |'
- en: '| **Description**: |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| **描述**： |'
- en: '| Training of neural networks using backpropagation, resilient backpropagation
    with (Riedmiller, 1994), or without weight backtracking (Riedmiller and Braun,
    1993), or the modified globally convergent version by Anastasiadis et al. (2005).
    The package allows flexible settings through custom-choice of error and activation
    function. |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 使用反向传播、弹性反向传播（Riedmiller, 1994），或不带权重回溯（Riedmiller and Braun, 1993），或由Anastasiadis等人修改的全局收敛版本（2005）训练神经网络。该包允许通过自定义选择误差和激活函数进行灵活设置。
    |'
- en: '| **Details**: |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| **细节**： |'
- en: '| Package: `neuralnet` Type: Package'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '| 包：`neuralnet` 类型：包'
- en: 'Version: 1.33'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 版本：1.33
- en: 'Date: 2016-08-05'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2016-08-05
- en: 'License: GPL-2 |'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 许可证：GPL-2 |
- en: '| **Author(s)**: |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| **作者**： |'
- en: '| Stefan Fritsch Frauke Guenther'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '| Stefan Fritsch Frauke Guenther'
- en: Marc Suling
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Marc Suling
- en: Sebastian M. Mueller |
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Sebastian M. Mueller |
- en: 'Returning to the code, at this point we have to load the data to be analyzed:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 回到代码，此时我们需要加载要分析的数据：
- en: '[PRE29]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: With this command, we upload the data set named `BreastCancer`, as mentioned,
    in the `mlbench` library.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此命令，我们上传名为 `BreastCancer` 的数据集，如前所述，在 `mlbench` 库中。
- en: Exploratory analysis
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性分析
- en: Before starting with data analysis through the build and training of a neural
    network, we conduct an exploratory analysis to understand how the data is distributed
    and extract preliminary knowledge.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过构建和训练神经网络进行数据分析之前，我们进行探索性分析，以了解数据如何分布并提取初步知识。
- en: '[PRE30]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: With this command, we will see a brief summary using the `summary()` function.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `summary()` 函数，我们将看到一个简要的摘要。
- en: Remember, the `summary()` function is a generic function used to produce result
    summaries of the results of various model fitting functions. The function invokes
    particular methods which depend on the class of the first argument.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`summary()` 函数是用于生成各种模型拟合函数结果摘要的通用函数。该函数调用依赖于第一个参数的类别的特定方法。
- en: 'In this case, the function was applied to a dataframe and the results are shown
    in the following screenshot:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，该函数被应用于数据框架，并且结果显示在以下截图中：
- en: '![](img/00104.jpeg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00104.jpeg)'
- en: The `summary()` function returns a set of statistics for each variable. In particular,
    it is useful to highlight the result provided for the `class` variable that contains
    the diagnosis of the cancer mass. In this case, 458 cases of benign `class` and
    241 cases of `malignant` class were detected. Another feature to highlight is
    the Bare.nuclei variable. For this variable, 16 cases of missing value were detected.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`summary()` 函数返回每个变量的统计信息。特别地，它对于突出显示 `class` 变量的结果非常有用，该变量包含肿瘤的诊断信息。在此案例中，检测到
    458 个良性 `class` 和 241 个恶性 `class`。另一个值得注意的特征是 Bare.nuclei 变量。对于该变量，检测到 16 个缺失值。'
- en: A missing value is one whose value is unknown. Missing values are represented
    in R by the `NA` symbol. `NA` is a special value whose properties are different
    from other values. `NA` is one of the very few reserved words in R; you cannot
    give anything this name. `NA` can arise when you read in an Excel spreadsheet
    with empty cells, for example. You will also see `NA` when you try certain operations
    that are illegal or don't make sense. Missing values do not necessarily arise
    from an error; often in real life, there is a lack of detection.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值是指其值未知的值。缺失值在 R 中由 `NA` 符号表示。`NA` 是一个特殊的值，其属性不同于其他值。`NA` 是 R 中少数几个保留字之一；你不能将其他任何事物命名为
    `NA`。例如，当你读取包含空单元格的 Excel 表格时，`NA` 可能会出现。当你尝试进行某些非法或无意义的操作时，也会看到 `NA`。缺失值不一定是错误的结果；在现实生活中，缺失值通常是由于未检测到某些数据。
- en: 'A question arises spontaneously: do we have to worry about the presence of
    missing value? Unfortunately, yes, and this is due to the fact that almost every
    operation performed on an `NA` produces an `NA`. Then the presence of missing
    values in our dataset can cause errors in the calculations we will make later.
    This is why we are forced to remove the missing values.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 一个问题自然而然地出现了：我们需要担心缺失值的存在吗？不幸的是，答案是肯定的，这主要是因为几乎在对 `NA` 进行的每个操作中，都会产生一个 `NA`。因此，数据集中缺失值的存在可能会导致我们后续计算中出现错误。这就是为什么我们必须删除缺失值的原因。
- en: 'To remove missing values, we must first identify them. The `is.na()` function
    finds missing values for us; this function returns a logical vector of the same
    length as its argument, with *T* for missing values and *F* for non-missing. It''s
    fairly common to want to know the index of the missing values, and the `which()`
    function helps do this for us. To find all the rows in a dataframe with at least
    one `NA`, try this:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 要删除缺失值，我们必须先识别它们。`is.na()` 函数可以帮助我们找到缺失值；该函数返回一个与其参数长度相同的逻辑向量，对于缺失值为*T*，对于非缺失值为*F*。通常我们希望知道缺失值的索引，`which()`
    函数可以帮助我们实现这一点。要找到数据框中所有包含至少一个 `NA` 的行，可以尝试以下方法：
- en: '[PRE31]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The `lapply()` function applies the function to each column and returns a list
    whose *i*-th element is a vector containing the indices of the elements which
    have missing values in column *i*. The `unlist()` function turns that list into
    a vector and `unique()` gets rid of the duplicates.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`lapply()` 函数对每一列应用指定的函数，并返回一个列表，其中的第 *i* 个元素是包含第 *i* 列缺失值元素索引的向量。`unlist()`
    函数将该列表转换为向量，而 `unique()` 函数则去除重复项。'
- en: 'Now we have the number of lines where the missing value (`NA`) appears, as
    we can see next:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了缺失值（`NA`）出现的行数，正如我们接下来看到的：
- en: '[PRE32]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now we know there are missing values in our database and we know where they
    are. We just have to remove those lines from the original dataset. To do this,
    we can use the following functions:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道数据库中有缺失值，并且知道它们的位置。接下来，我们只需将这些行从原始数据集中删除。为此，我们可以使用以下函数：
- en: '`na.omit`: Drops out any rows with missing values anywhere in them and forgets
    them forever'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`na.omit`：删除所有包含缺失值的行，并将其永久遗忘。'
- en: '`na.exclude`: Drops out rows with missing values, but keeps track of where
    they were, so that when you make predictions, for example, you end up with a vector
    whose length is that of the original response'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`na.exclude`：删除包含缺失值的行，但会记录这些行的位置，以便在进行预测时，得到一个与原始响应长度相同的向量。'
- en: 'We will use the first option, so as to eliminate them forever:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用第一种方法，以便将缺失值永久删除：
- en: '[PRE33]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'To confirm the removal of the rows where the missing values appeared, apply
    the `summary()` function again:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认已经删除了缺失值所在的行，再次应用 `summary()` 函数：
- en: '[PRE34]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The results are shown in the following screenshot:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在以下截图中：
- en: '![](img/00105.jpeg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00105.jpeg)'
- en: As you can see now, there is no missing value anymore.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，现在已经没有缺失值了。
- en: Now, let's go into our exploratory analysis. The first thing we can do is to
    plot the boxplots of the variables. A first idea is already made by looking at
    the results of the `summary()` function. Naturally, we will limit ourselves to
    numeric variables only.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入探索性分析。我们可以做的第一件事是绘制变量的箱线图。通过查看`summary()`函数的结果，已经初步了解了一些情况。自然地，我们将只关注数值型变量。
- en: '[PRE35]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In the following graph, the boxplots of the numeric variables (from 2° to 10°)
    contained in the cleaned dataset (`data_cleaned`) are shown:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，展示了清洗过的数据集（`data_cleaned`）中数值变量（从第2个到第10个）的箱线图：
- en: '![](img/00106.jpeg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00106.jpeg)'
- en: From the analysis of the previous graph, we can note that several variables
    have outliers, with the variable `Mitoses` being the one that has the largest
    number.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 从前一个图形的分析中，我们可以注意到几个变量有异常值，其中`Mitoses`变量的异常值数量最多。
- en: Outlier values are numerically different from the rest of the collected data.
    Statistics derived from samples containing outliers can be misleading.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值在数值上与其余数据显著不同。包含异常值的样本数据所得出的统计结果可能会产生误导。
- en: To better identify the presence of outlier, we can plot histograms of the variables
    in the database. A histogram is an accurate graphical representation of the distribution
    of numerical data. It is an estimate of the probability distribution of a continuous
    variable. To construct a histogram, the first step is to specify the range of
    values (that is, divide the entire range of values into a series of intervals),
    and then count how many values fall into each interval. The bins are usually specified
    as consecutive, non-overlapping intervals of a variable. The bins must be adjacent,
    and are often of equal size. With histogram, we can see where the middle is in
    your data distribution, how close the data lies around this middle, and where
    possible outliers are to be found.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地识别异常值，我们可以绘制数据库中各变量的直方图。直方图是数值数据分布的准确图形表示。它是连续变量概率分布的估计。构建直方图的第一步是指定值的范围（即将整个值域划分为一系列区间），然后统计每个区间内有多少值。直方图的区间通常是连续的、互不重叠的变量区间。区间必须是相邻的，且通常大小相等。通过直方图，我们可以看到数据分布的中心在哪里，数据如何围绕这个中心分布，以及可能存在的异常值位置。
- en: 'In R environment, we can simply make a histogram by using the `hist()` function,
    which computes a histogram of the given data values. We must put the name of the
    dataset in between the parentheses of this function. To plot many graphs in the
    same window, we will use the `par()` function, already used in the previous examples:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在R环境中，我们可以简单地使用`hist()`函数绘制直方图，该函数计算给定数据值的直方图。我们必须将数据集的名称放在该函数的括号内。为了在同一窗口绘制多个图形，我们将使用在前面的示例中已经使用过的`par()`函数：
- en: '[PRE36]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Since the function `hist()` requires a vector as argument, we have transformed
    the values contained in the dataset columns into numeric vectors using the `as.numeric(`)
    function. This function creates or coerces objects of type `numeric`. In the following
    graphs are shown the histograms of the numeric variables (from 2° to 10°) contained
    in the cleaned dataset (`data_cleaned`):'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`hist()`函数需要一个向量作为参数，因此我们使用`as.numeric()`函数将数据集列中的值转换为数值型向量。该函数创建或强制转换为`numeric`类型的对象。在接下来的图形中，展示了清洗过的数据集（`data_cleaned`）中数值变量（从第2个到第10个）的直方图：
- en: '![](img/00107.gif)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00107.gif)'
- en: From the analysis of the histograms, it is possible to note that some variables
    have outliers.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 从直方图的分析中，可以注意到一些变量存在异常值。
- en: Neural network model
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络模型
- en: As we did in the previous example, before building and training the network,
    we have to run the standardization of data. In this case, we will adopt min-max
    standardization.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前的示例一样，在构建和训练网络之前，我们必须先进行数据标准化。在本例中，我们将采用最小-最大标准化方法。
- en: Remember, it is good practice to normalize the data before training a neural
    network. With normalization, data units are eliminated, allowing you to easily
    compare data from different locations.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在训练神经网络之前，进行数据标准化是良好的实践。通过标准化，可以消除数据的单位，使来自不同位置的数据可以轻松进行比较。
- en: Before we begin, make a further check by using the `str()` function. This function
    provides a compact display of the internal structure of an object, a diagnostic
    function, and an alternative to the `summary()` function. Ideally, only one line
    for each basic structure is displayed. It is especially well suited to compactly
    display the (abbreviated) contents of (possibly nested) lists. The idea is to
    give reasonable output for any R object.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，使用`str()`函数做一个进一步的检查。这个函数提供了对象内部结构的紧凑显示，是一种诊断功能，并且是`summary()`函数的替代方法。理想情况下，每个基本结构仅显示一行。它特别适合紧凑地显示（可能嵌套的）列表的（简化）内容。其目的是为任何R对象提供合理的输出。
- en: '[PRE37]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The results are shown in the following screenshot:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在以下截图中：
- en: '![](img/00108.jpeg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00108.jpeg)'
- en: As it is possible to note, the variables are present as a factor. We need to
    make a transformation for our calculations.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 正如可以注意到的，变量作为因子存在。我们需要对其进行转换以便于计算。
- en: '[PRE38]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We first identified the variables of the factor type and then we transformed
    them into numeric type. We can now standardize.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先识别了因子类型的变量，然后将它们转换为数值类型。现在我们可以进行标准化。
- en: 'For this example, we will use the min-max method (usually called feature scaling)
    to get all the scaled data in the range *[0, 1]*. The formula to achieve this
    is the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将使用最小-最大方法（通常称为特征缩放）将所有数据缩放到* [0, 1] * 范围内。实现此目的的公式如下：
- en: '![](img/00109.jpeg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00109.jpeg)'
- en: Before applying the method chosen for normalization, you must calculate the
    minimum and maximum values of each database column. To do this, we use the `apply()`
    function. This function returns a vector or an array or a list of values obtained
    by applying a function to margins of an array or matrix. Let's understand the
    meaning of the arguments used.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用选择的标准化方法之前，您必须计算每个数据库列的最小值和最大值。为此，我们使用`apply()`函数。此函数返回一个向量、数组或值的列表，通过将函数应用于数组或矩阵的边界来获得这些值。让我们来理解所使用参数的含义。
- en: '[PRE39]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The first argument of the apply function specifies the dataset to apply the
    function to, in our case, the dataset named `data`. The second argument must contain
    a vector giving the subscripts which the function will be applied over. In our
    case, one indicates rows and two indicates columns. The third argument must contain
    the function to be applied; in our case, the max function. What we will do next
    is to calculate the minimums for each column:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`apply`函数的第一个参数指定了要应用函数的数据集，在我们的案例中是名为`data`的数据集。第二个参数必须包含一个向量，指定函数将应用于的子脚标。在我们的案例中，1表示行，2表示列。第三个参数必须包含要应用的函数；在我们的案例中是`max`函数。接下来，我们将计算每一列的最小值：'
- en: '[PRE40]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Finally, to normalize the data, we use the `scale()` function, which is a generic
    function whose default method centers and/or scales the columns of a numeric matrix,
    as shown in the following code:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了对数据进行标准化，我们使用`scale()`函数，这是一个通用函数，其默认方法会对数值矩阵的列进行居中和/或缩放，代码如下所示：
- en: '[PRE41]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'To confirm the standardization of data, let''s see the first 20 lines of the
    new matrix we created. To do this, we will use the `View()` function:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认数据的标准化，我们来看一下我们创建的新矩阵的前20行。为此，我们将使用`View()`函数：
- en: '![](img/00110.jpeg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00110.jpeg)'
- en: 'As you can see now, the data is between zero and one. At this point, we reconstruct
    the dataset, adding our target (that is the `class` variable), which represents
    the diagnosis of the cancer (`benign` or `malignant`). This topic requires our
    attention: as we have seen before, this variable (`class`) is categorical. Particularly
    in the data frame is present as a factor, so that we can properly use int the
    network we must necessarily transform it. Our target is a dichotomous variable
    (only two values: `benign` and `malignant`), so it can easily be transformed into
    two dummy variables.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，现在数据在零和一之间。此时，我们重新构建数据集，加入我们的目标变量（即`class`变量），它表示癌症的诊断结果（`良性`或`恶性`）。这个话题需要我们的关注：正如我们之前所看到的，这个变量（`class`）是类别型的，特别是在数据框中它作为因子存在，因此为了正确地在网络中使用它，我们必须对其进行转换。我们的目标是一个二分类变量（只有两个值：`良性`和`恶性`），因此它可以轻松地转换为两个哑变量。
- en: A dummy variable is one that takes the value `0` or `1` to indicate the absence
    or presence of some categorical effect that may be expected to shift the outcome.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 哑变量是取值为`0`或`1`的变量，用来指示某些类别效应的缺失或存在，这些效应可能会影响结果。
- en: What we will do is create two new variables (`Cancerbenign` and `Cancermalignant`),
    starting with the `Class` variable representing our target. The `Cancerbenign`
    variable will contain values of one at each occurrence of the `benign` value present
    in the `Class` variable, and values of zero in other cases. In contrast, the `Cancermalignant`
    variable will contain values of one at each occurrence of the `malignant` value
    present in the `Class` variable and values of zero in other cases.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将做的是创建两个新的变量（`Cancerbenign`和`Cancermalignant`），从表示目标的`Class`变量开始。`Cancerbenign`变量将在`Class`变量中每次出现`benign`值时赋值为1，而在其他情况下赋值为0。相反，`Cancermalignant`变量将在`Class`变量中每次出现`malignant`值时赋值为1，而在其他情况下赋值为0。
- en: '[PRE42]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'To get the two new dummy variables, we used the `model.matrix()` function.
    This function creates a model matrix by expanding factors to a set of dummy variables
    (depending on the contrasts), and expanding interactions similarly. Finally, we
    add the new variables to the dataset:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得两个新的虚拟变量，我们使用了`model.matrix()`函数。该函数通过扩展因子为一组虚拟变量（根据对比方式），并类似地扩展交互作用，来创建一个模型矩阵。最后，我们将新变量添加到数据集中：
- en: '[PRE43]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The time has come to train the network.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 训练网络的时刻已经到来。
- en: The network training phase
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络训练阶段
- en: 'The artificial neural networks are composed of simple elements operating in
    parallel. Connections between network elements are fundamental as they decide
    network functions. These connections affect the result through its weight, which
    is regulated in the neural network training phase. In the following diagram is
    shown a comparison between serial and parallel processing:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络由并行运行的简单元素组成。网络元素之间的连接至关重要，因为它们决定了网络的功能。这些连接通过其权重影响结果，而权重是在神经网络训练阶段调节的。下图展示了串行和并行处理的比较：
- en: '![](img/00111.jpeg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00111.jpeg)'
- en: 'Then, in the training phase, the network is regulated by changing the connection
    weights, so that a particular input will lead to a specific destination. For example,
    the network can be adjusted by comparing the output (what we calculate practically)
    and the target (what we want to get), until the network output matches the target.
    To get sufficiently reliable results, many input/target pairs are needed to form
    a network. In the following diagram is shown a simple flow chart of the training
    phase:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在训练阶段，通过更改连接权重来调节网络，使得特定的输入能引导到特定的输出。例如，通过比较输出（我们实际计算的结果）和目标（我们希望得到的结果），直到网络输出与目标匹配，从而调整网络。为了获得足够可靠的结果，需要许多输入/目标对来形成网络。下图展示了训练阶段的简单流程图：
- en: '![](img/00112.jpeg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00112.jpeg)'
- en: The way these weights are adjusted is defined by the particular algorithm we
    adopt. After highlighting the importance of the algorithm in network training,
    much interest must be placed in the preparation of the data to be provided to
    the network.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这些权重调整的方式由我们采用的具体算法决定。在强调算法在网络训练中的重要性后，必须特别关注提供给网络的数据准备。
- en: In the network training, the weights and bias must be tuned to optimize the
    network performance. It represents the most important phase of the whole process,
    as the better the network is, the better the generalization will be able to operate
    with new data, unknown to it. At this stage, part of the collected data is taken
    randomly (usually 70 percent of the available cases).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络训练过程中，必须调整权重和偏置，以优化网络的性能。这是整个过程中的最重要阶段，因为网络越好，通用化能力在处理新的、未知的数据时表现越好。在这个阶段，随机抽取一部分收集到的数据（通常是70%的可用数据）。
- en: 'After the neural network training, we can use the network, in that phase a
    part of the collected data taken randomly (usually 30 perecnt of the available
    cases) is passed to the network to test it. Then the neural network object can
    be saved and used as many times as you want with any new data. In the following
    figure is shown how an original dataset has been divided:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络训练后，我们可以使用该网络，在这一阶段，随机抽取一部分收集到的数据（通常是30%的可用数据）传递给网络进行测试。然后，神经网络对象可以保存，并且在任何新数据上多次使用。下图展示了原始数据集是如何被划分的：
- en: '![](img/00113.jpeg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00113.jpeg)'
- en: 'This subdivision of data in code looks like this:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码中的数据划分如下：
- en: '[PRE44]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'In the first line of the code just suggested, the dataset is split into 70:30,
    with the intention of using 70 percent of the data at our disposal to train the
    network and the remaining 30 percent to test the network. In the second and third
    lines, the data of the dataframe named `data` is subdivided into two new dataframes,
    called `train_data` and `test_data`. Now we have to build the function to be submitted
    to the network:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在刚刚提到的代码的第一行中，数据集被分割为70:30，目的是使用70%的数据来训练网络，剩余的30%用于测试网络。在第二行和第三行中，名为`data`的数据框被细分为两个新的数据框，分别称为`train_data`和`test_data`。现在我们需要构建要提交给网络的函数：
- en: '[PRE45]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: In the first line, we recover the names of the first nine variables in the `data_scaled`
    dataframe, using the `names()` function. In the second line, we build formula
    that we will use to train the network. What does this formula represent?
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一行中，我们使用`names()`函数恢复`data_scaled`数据框中前九个变量的名称。在第二行中，我们构建了将用于训练网络的公式。这个公式代表了什么？
- en: 'The models fitted by the `neuralnet()` function are specified in a compact
    symbolic form. The ~ operator is basic in the formation of such models. An expression
    of the form *y* ~ model is interpreted as a specification that the response *y*
    is modelled by a predictor specified symbolically by model. Such a model consists
    of a series of terms separated by + operators. The terms themselves consist of
    variable and factor names separated by : operators. Such a term is interpreted
    as the interaction of all the variables and factors appearing in the term. Let''s
    look at the formula we set:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`neuralnet()`函数拟合的模型以紧凑的符号形式表示。~ 运算符在这种模型的构建中是基础。形式为 *y* ~ model 的表达式被解释为响应
    *y* 是通过符号上指定的预测因子 model 建模的。这样的模型由一系列通过 + 运算符分隔的项组成。各项本身由变量和因子名称组成，这些名称通过 : 运算符分隔。这样的项被解释为项中所有变量和因子的交互。让我们来看一下我们设置的公式：'
- en: '[PRE46]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We now have everything we need, we can create and train the network. We recall
    the advice we gave in the previous example for the correct choice of number of
    neurons in the hidden layer. We have eight input variables (`Cl.thickness`, `Cell.size`,
    `Cell.shape`, `Marg.adhesion`, `Epith.c.size`, `Bare.nuclei`, `Bl.cromatin`, `Normal.nucleoli`,
    and `Mitoses`) and one variable output (`Cancer`). Then we choose to set five
    neurons in the hidden layer:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经具备了所需的所有内容，可以创建并训练网络。我们回顾一下在前一个示例中关于隐藏层神经元数目选择的建议。我们有八个输入变量（`Cl.thickness`、`Cell.size`、`Cell.shape`、`Marg.adhesion`、`Epith.c.size`、`Bare.nuclei`、`Bl.cromatin`、`Normal.nucleoli`和`Mitoses`）和一个输出变量（`Cancer`）。然后我们选择在隐藏层设置五个神经元：
- en: '[PRE47]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The `hidden` argument accepts a vector with the number of neurons for each hidden
    layer, while the argument `linear.output` is used to specify whether we want to
    do regression (`linear.output=TRUE`) or classification (`linear.output=FALSE`
    (our case)).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '`hidden`参数接受一个向量，指定每个隐藏层的神经元数，而`linear.output`参数用于指定我们是做回归（`linear.output=TRUE`）还是分类（`linear.output=FALSE`，在我们的例子中是分类）。'
- en: The algorithm used in `neuralnet()`, by default, is based on the resilient backpropagation
    without weight backtracking, and additionally modifies one learning rate, either
    the learning rate associated with the smallest absolute gradient (`sag`) or the
    smallest learning rate (`slr`) itself.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`neuralnet()`中使用的算法默认基于弹性反向传播，且不进行权重回溯，另外还会修改一个学习率，学习率可能是与最小绝对梯度相关的学习率（`sag`），或者是最小学习率（`slr`）本身。'
- en: 'To plot the graphical representation of the model with the weights on each
    connection, we can use the `plot()` function, already widely explained in the
    previous section:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绘制带有每个连接权重的模型图形表示，我们可以使用`plot()`函数，前一部分已经对此进行了广泛的解释：
- en: '[PRE48]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The neural network plot is shown in the following graph:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的图示如下所示：
- en: '![](img/00114.jpeg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00114.jpeg)'
- en: In the previous graph, the black lines (these lines start from input nodes)
    show the connections between each layer and the weights on each connection, while
    the blue lines (these lines start from bias nodes which are distinguished by number
    one) show the bias term added in each step. The bias can be thought of as the
    intercept of a linear model.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的图中，黑色的线（这些线从输入节点开始）显示了各层之间的连接以及每个连接上的权重，而蓝色的线（这些线从偏置节点开始，偏置节点由数字一区分）显示了每一步中添加的偏置项。可以把偏置看作线性模型的截距。
- en: Testing the network
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试网络
- en: We finally have the network trained and ready for use. Now, we can use it to
    make our predictions. Remember, we've set aside 30 percent of available data and
    then use them to test the network. It's time to use it.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于训练好了网络并准备好使用它了。现在，我们可以使用它来进行预测。记住，我们已经将30%的可用数据保留出来，用它来测试网络。是时候使用它了。
- en: '[PRE49]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'To predict data, we have used the compute function, which computes the outputs
    of all neurons for specific arbitrary covariate vectors, given a trained neural
    network. Let''s look at the results by printing the first ten lines:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预测数据，我们使用了计算函数，它在给定训练好的神经网络的情况下，计算所有神经元对于特定任意协变量向量的输出。让我们通过打印前十行来查看结果：
- en: '[PRE50]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: As we can see, these are real numbers with several decimals. In order to compare
    them with the data contained in the dataset, we have to round them to the nearest
    integer. To do this, we will use the `round()` function that rounds the values
    in its first argument to the specified number of decimal places (default zero).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这些是带有若干小数的实数。为了与数据集中包含的数据进行比较，我们必须将它们四舍五入到最接近的整数。为此，我们将使用`round()`函数，它将第一个参数中的值四舍五入到指定的小数位数（默认为零）。
- en: '[PRE51]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We now rebuild the starting variable. We no longer need the two dummy variables;
    they have done their job well, but now we no longer need them.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们重新构建初始变量。我们不再需要那两个虚拟变量；它们已经完成了它们的任务，但现在我们不再需要它们了。
- en: '[PRE52]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Now, we can build the confusion matrix to check the performance of our classifier.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以构建混淆矩阵来检查分类器的性能。
- en: '[PRE53]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The confusion matrix is shown as follows:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵如下所示：
- en: '[PRE54]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Although in a simple way, the matrix tells us that we only made eight errors.
    For more information about the confusion matrix, we can use the `CrossTable()`
    function contained in the `gmodels` package. As always, before loading the book,
    you need to install it.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管简单地讲，矩阵告诉我们我们只犯了8个错误。有关混淆矩阵的更多信息，我们可以使用`gmodels`包中的`CrossTable()`函数。和往常一样，在加载该包之前，你需要先安装它。
- en: '[PRE55]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The confusion matrix obtained by using the `CrossTable()` function is shown
    in the following screenshot:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`CrossTable()`函数得到的混淆矩阵如下所示：
- en: '![](img/00115.jpeg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00115.jpeg)'
- en: The cells falling on the main diagonal contain counts of examples where the
    classifier correctly categorized the examples. In the top-left cell, labeled `TN`,
    are the true negative results. These 132 of 205 values indicate cases where the
    cancer was `benign`, and the algorithm correctly identified it as such. The bottom-right
    cell, labeled `TP`, indicates the true positive results, where the classifier
    and the clinically determined label agree that the mass is `malignant`. A total
    of 65 of 205 predictions were true positives.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 主对角线上的单元格包含分类器正确分类的样本计数。在左上角的单元格中，标记为`TN`，表示真正负例。205个值中的132个表示癌症为`良性`，且算法正确识别为良性。右下角的单元格，标记为`TP`，表示真正正例，分类器与临床确定的标签一致，认为肿块是`恶性`的。在205个预测中，共有65个是真正正例。
- en: The cells falling on the other diagonal contain counts of examples where the
    classifier incorrectly categorized the examples. The three examples in the lower-left
    `FN` cell are false negative results; in this case, the predicted value was `benign`
    but the cancer was actually `malignant`. Errors in this direction could be extremely
    costly, as they might lead a patient to believe that she is cancer-free, when
    in reality the disease may continue to spread. The cell labeled `FP` would contain
    the false positive results, if there were any. These values occur when the model
    classifies a cancer as malignant when in reality it was benign. Although such
    errors are less dangerous than a false negative result, they should also be avoided
    as they could lead to additional financial burden on the health care system, or
    additional stress for the patient, as additional tests or treatment may have to
    be provided.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 另一对角线上的单元格包含分类器错误分类的样本计数。左下角的`FN`单元格中的三个例子是伪阴性结果；在这种情况下，预测值为`良性`，但实际上癌症是`恶性`的。这类错误可能非常昂贵，因为它可能会导致患者误以为自己没有癌症，而实际上疾病可能继续扩散。标记为`FP`的单元格将包含伪阳性结果（如果有的话）。这些值出现于模型将癌症错误分类为恶性，而实际上它是良性的时候。虽然这种错误比伪阴性错误危险性较小，但也应避免，因为它可能导致医疗系统的额外财务负担，或者给患者带来额外的压力，因为可能需要额外的检测或治疗。
- en: Early stopping in neural network training
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络训练中的早停
- en: The epoch is a measure of each round trip from the forward propagation training
    and backpropagation update of weights and biases. The round trip of training has
    to stop once we have convergence (minimal error terms) or after a preset number
    of iterations.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 训练周期（epoch）是每次从正向传播训练到反向传播更新权重和偏置的过程的度量。训练的往返过程必须在收敛（最小误差项）或预设的迭代次数后停止。
- en: 'Early stopping is a technique used to deal with overfitting of the model (more
    on overfitting in the next few pages). The training set is separated into two
    parts: one of them is to be used for training, while the other one is meant for
    validation purposes. We had separated our `IRIS` dataset into two parts: one 75
    percent and another 25 percent.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 提前停止是一种用于处理模型过拟合的技术（在接下来的几页中会详细讨论过拟合）。训练集被分为两部分：一部分用于训练，另一部分用于验证。我们将`IRIS`数据集分成了两部分：一部分占
    75%，另一部分占 25%。
- en: With the training data, we compute the gradient and update the network weights
    and biases. The second set of data, the testing or validation data, is used to
    validate the model overfitting. If the error during validation increases for a
    specified number of iterations (`nnet.abstol`/`reltol`), the training is stopped
    and the weights and biases at that point are used by the model. This method is
    called *early stopping.*
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练数据，我们计算梯度并更新网络权重和偏置。第二组数据，即测试或验证数据，用于验证模型的过拟合。如果验证过程中的误差在指定次数的迭代后增加（`nnet.abstol`/`reltol`），则停止训练，并且此时的权重和偏置将被模型使用。这种方法称为*提前停止*。
- en: An early stopping neural network ensemble generalization error is comparable
    with an individual neural network of optimal architecture that is trained by a
    traditional algorithm. The individual neural network needs a complex and perfect
    tuning to attain this generalization without early stopping.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 使用提前停止的神经网络集成泛化误差与通过传统算法训练的最优架构的单一神经网络相当。单个神经网络需要复杂且完美的调优才能在没有提前停止的情况下实现这种泛化。
- en: Avoiding overfitting in the model
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 避免模型过拟合
- en: The fitting of the training data causes the model to determine the weights and
    biases along with the activation function values. When the algorithm does too
    well in some training dataset, it is said to be too much aligned to that particular
    dataset. This leads to high variance in the output values when the test data is
    very different from the training data. This high estimate variance is called **overfitting**.
    The predictions are affected due to the training data provided.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据的拟合使得模型确定权重、偏置以及激活函数值。当算法在某些训练数据集上表现过好时，就说它与该特定数据集过于契合。这会导致当测试数据与训练数据差异较大时，输出值的方差很高。这个高估计方差被称为**过拟合**。预测会受到训练数据影响。
- en: 'There are many possible ways to handle overfitting in neural networks. The
    first is regularization, similar to regression. There are two kinds of regularizations:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 处理神经网络中过拟合的方式有很多种。第一种是正则化，类似于回归。正则化有两种类型：
- en: L1 or lasso regularization
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1 或 Lasso 正则化
- en: L2 or ridge regularization
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2 或岭正则化
- en: Max norm constraints
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大范数约束
- en: Dropouts in neural networks
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络中的丢弃法
- en: Regularization introduces a cost term to impact the activation function. It
    tries to change most of the coefficients by bringing in more features with the
    objective function. Hence, it tries to push the coefficients for many variables
    to zero and reduce the cost term.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化引入了一个成本项来影响激活函数。它通过引入更多特征到目标函数中来尝试改变大部分系数。因此，它试图将许多变量的系数推向零，并减少成本项。
- en: '**Lasso or L1 regularization or L1 penalty**: This has a penalty term, which
    uses the sum of absolute weights, so that the weights are optimized to reduce
    overfitting. **Least Absolute Shrinkage And Selection Operator** (**LASSO**) introduces
    the penalty weight to shrink the network weights towards zero.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Lasso 或 L1 正则化或 L1 惩罚**：这有一个惩罚项，使用权重的绝对值之和，使得权重得到优化以减少过拟合。**最小绝对收缩与选择算子**（**LASSO**）引入了惩罚权重，将网络权重压缩到接近零。'
- en: '**L2 penalty or ridge regression**: This is similar to L1, but the penalty
    is based on squared weights instead of the sum of absolute weights. Larger weights
    get more penalty.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L2 惩罚或岭回归**：这与 L1 类似，但惩罚是基于权重的平方，而不是绝对权重之和。较大的权重会受到更多的惩罚。'
- en: For both cases, only weights are considered for optimization, and biases (or
    offsets or intercepts) are excluded from the exercise.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两种情况，只考虑权重进行优化，偏置（或偏移量或截距）被排除在外。
- en: '**Max norm constraints**: This is another regularization technique, whereby
    we enforce an absolute upper bound on the magnitude of the incoming weight vector
    for every neuron and the projected gradient descent cannot modify the weights
    due to the constraint. Here, the parameter vector cannot grow out of control (even
    if the learning rates are too high) because the updates to the weights are always
    bounded.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大范数约束**：这是一种正则化技术，通过对每个神经元的输入权重向量施加绝对上限，使得投影梯度下降无法修改权重。这意味着参数向量不能失控（即使学习率过高），因为权重的更新始终受到限制。'
- en: '**Dropout**: This is another overfitting prevention technique. While training,
    dropout is implemented by keeping a neuron active with some probability *p* (a
    hyperparameter) or setting it to zero otherwise. This means that some neurons
    may not be present during training and hence dropout. The network is unaffected
    and becomes more accurate even in the absence of certain information. This prevents
    the network from becoming too dependent on any one (or any small combination)
    of the neurons. The process of dropout is explained in the following diagram.
    The red (or dark) neurons are the ones dropped out, and the neural network model
    survives without these neurons and offers less overfitting and greater accuracy:'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout**：这是一种防止过拟合的技术。在训练过程中，dropout 通过以某个概率 *p*（一个超参数）保持神经元激活，否则将其设置为零。这意味着在训练过程中某些神经元可能不被激活，因此被丢弃。即便某些信息缺失，网络仍不受影响，并且变得更加准确。这防止了网络过度依赖任何单一神经元或任何小组合的神经元。以下图示说明了
    dropout 过程。红色（或深色）神经元为被丢弃的神经元，而神经网络模型在没有这些神经元的情况下依然能够生存，表现出较少的过拟合和更高的准确性：'
- en: '![](img/00116.gif)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00116.gif)'
- en: Generalization of neural networks
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的泛化
- en: Generalization is aimed at fitting the training data. It is an extension of
    the training we have done on the neural networks model. It seeks to minimize the
    sum of squared errors of the model on the training data (such as using ordinary
    least squares) and reduce the complexity of the model.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化的目标是使模型能够拟合训练数据。这是我们在神经网络模型上进行训练的扩展。其目标是最小化模型在训练数据上的平方误差和（例如使用普通最小二乘法），并减少模型的复杂度。
- en: 'The methods of generalization are listed here:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是列出的泛化方法：
- en: Early stopping of training
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提前停止训练
- en: Retraining neural networks with different training data
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同训练数据重新训练神经网络
- en: Using random sampling, stratified sampling, or any good mix of target data
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机抽样、分层抽样或任何有效的目标数据组合
- en: Training multiple neural networks and averaging out their outputs
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练多个神经网络并对它们的输出进行平均
- en: Scaling of data in neural network models
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络模型中的数据缩放
- en: Data scaling or normalization is a process of making model data in a standard
    format so that the training is improved, accurate, and faster. The method of scaling
    data in neural networks is similar to data normalization in any machine learning
    problem.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 数据缩放或归一化是将模型数据转换为标准格式的过程，从而提高训练的效果、准确性和速度。神经网络中数据缩放的方法类似于任何机器学习问题中的数据归一化方法。
- en: 'Some simple methods of data normalization are listed here:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 一些简单的数据归一化方法列举如下：
- en: '**Z-score normalization**: As anticipated in previous sections, the arithmetic
    mean and standard deviation of the given data are calculated first. The standardized
    score or *Z*-score is then calculated as follows:'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Z-score 归一化**：如前所述，首先计算给定数据的算术平均值和标准差。然后按以下方式计算标准化得分或*Z*得分：'
- en: '![](img/00117.jpeg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00117.jpeg)'
- en: Here, *X* is the value of the data element, μ is the mean, and σ is the standard
    deviation. The *Z*-score or standard score indicates how many standard deviations
    the data element is from the mean. Since mean and standard deviation are sensitive
    to outliers, this standardization is sensitive to outliers.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*X* 是数据元素的值，μ 是均值，σ 是标准差。*Z* 得分或标准得分表示数据元素距离均值的标准差数目。由于均值和标准差对异常值敏感，因此此标准化方法对异常值也很敏感。
- en: '**Min-max normalization**: This calculates the following for each data element:'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小-最大归一化**：此方法为每个数据元素计算以下内容：'
- en: '![](img/00118.jpeg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00118.jpeg)'
- en: Here, *x*[*i*] is the data element, *min(x)* is the minimum of all data values,
    and *max(x)* is the maximum of all data values. This method transforms all the
    scores into a common range of [0, 1]. However, it suffers from outlier sensitivity.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x*[*i*] 是数据元素，*min(x)* 是所有数据值中的最小值，*max(x)* 是所有数据值中的最大值。此方法将所有分数转换到一个共同的范围
    [0, 1]。然而，它对异常值比较敏感。
- en: '**Median and MAD**: The median and median absolute deviation (MAD) normalization
    calculates the normalized data value using the following formula:'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中位数和MAD**：中位数和中位数绝对偏差（MAD）归一化通过以下公式计算归一化数据值：'
- en: '![](img/00119.jpeg)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00119.jpeg)'
- en: Here, *x[i]* represents each data value. This method is insensitive to outliers
    and the points in the extreme tails of the distribution, and therefore it is robust.
    However, this technique does not retain the input distribution and does not transform
    the scores into a common numerical range.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x[i]*表示每个数据值。这种方法对离群值和分布极端尾部的点不敏感，因此它是健壮的。然而，这种技术不会保留输入分布，也不会将分数转换为通用的数值范围。
- en: Ensemble predictions using neural networks
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络的集成预测
- en: Another approach to regularization involves combining neural network models
    and averaging out the results. The resultant model is the most accurate one.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种正则化方法是结合神经网络模型并对结果进行平均。最终的模型是最准确的。
- en: A neural network ensemble is a set of neural network models taking a decision
    by averaging the results of individual models. Ensemble technique is a simple
    way to improve generalization, especially when caused by noisy data or a small
    dataset. We train multiple neural networks and average their outputs.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络集成是通过平均各个模型的结果来做决策的一组神经网络模型。集成技术是一种简单的提高泛化能力的方法，特别是在噪声数据或小数据集导致的情况下。我们训练多个神经网络，并平均它们的输出。
- en: As an example, we take 20 neural networks for the same learning problem, we
    adjust the various parameters in the training processing, and then the mean squared
    errors are compared with the mean squared errors of their average.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，我们采用20个神经网络解决相同的学习问题，我们调整训练过程中的各种参数，然后将均方误差与它们平均的均方误差进行比较。
- en: 'The following are the steps followed:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是遵循的步骤：
- en: The dataset is loaded and divided into a train and test set. The percentage
    split can be varied for different neural net models.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集已加载并分为训练集和测试集。不同神经网络模型可以使用不同的百分比划分。
- en: Multiple models are created with the different training sets and by adjusting
    the parameters in the `nnet()` function.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调整`nnet()`函数中的参数，使用不同的训练集创建多个模型。
- en: All the models are trained and errors in each model are tabulated.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有模型都已训练完毕，并将每个模型中的错误列出。
- en: The average error is found for each row in test data and the mean square error
    is calculated for each model.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对测试数据中的每一行计算平均误差，并为每个模型计算均方误差。
- en: The mean square error is compared with the mean square error of the average.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将均方误差与均方误差的平均值进行比较。
- en: The best model is chosen from the comparison and is used further for prediction.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最佳模型是通过比较选择的，并进一步用于预测。
- en: This method allows us to play with the data and the function parameters to arrive
    at the optimal setting of the model. We can choose any number of models in the
    ensemble and do parallel processing of the models using R.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法允许我们通过调整数据和函数参数来得到模型的最佳设置。我们可以选择集成中的任意数量的模型，并使用R进行模型的并行处理。
- en: Overfitting is highly reduced and the best parameters of the model are arrived
    at here.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 高度减少过拟合，并在此处得出模型的最佳参数。
- en: Summary
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the training and visualization of a simple neural
    network using R. Here, we can change the number of neurons, the number of hidden
    layers, the activation functions, and so on, to determine the training of the
    model.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了使用R训练和可视化一个简单的神经网络。在这里，我们可以改变神经元的数量、隐藏层的数量、激活函数等，以确定模型的训练方式。
- en: While dealing with a regression problem, the last layer is a single unit, which
    will give continuous values. For a classification problem, there are n terminal
    units, each representing the class of output with its probability. The breast
    cancer example had two output neurons to represent the two classes of values that
    are output from the neural network.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理回归问题时，最后一层是一个单元，它将给出连续值；对于分类问题，有n个终端单元，每个单元表示输出类别及其概率。乳腺癌示例有两个输出神经元，表示神经网络输出的两类值。
- en: We have learned how to train, test, and evaluate a dataset using NN model. We
    have also learned how to visualize the NN model in R environment. We have covered
    the concepts like early stopping, avoiding overfitting, generalization of NN,
    and scaling of NN parameters.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学会了如何使用神经网络模型训练、测试和评估数据集。我们还学会了如何在R环境中可视化神经网络模型。我们涵盖了早停、避免过拟合、神经网络的泛化以及神经网络参数的缩放等概念。
