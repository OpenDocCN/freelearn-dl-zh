- en: 11\. Policy-Based Methods for Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11\. 基于策略的强化学习方法
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we will implement different policy-based methods of **Reinforcement
    Learning** (**RL**), such as policy gradients, **Deep Deterministic Policy Gradients**
    (**DDPGs**), **Trust Region Policy Optimization** (**TRPO**), and **Proximal Policy**
    **Optimization** (**PPO**). You will be introduced to the math behind some of
    the algorithms and you'll also learn how to code policies for RL agents within
    the OpenAI Gym environment. By the end of this chapter, you will not only have
    a base-level understanding of policy-based RL methods but you'll also be able
    to create complete working prototypes using the previously mentioned policy-based
    RL methods.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将实现不同的基于策略的强化学习方法（**RL**），如策略梯度法、**深度确定性策略梯度**（**DDPGs**）、**信任区域策略优化**（**TRPO**）和**近端策略优化**（**PPO**）。你将了解一些算法背后的数学原理，还将学习如何在OpenAI
    Gym环境中为RL智能体编写策略代码。在本章结束时，你不仅将对基于策略的强化学习方法有一个基础的理解，而且还将能够使用前面提到的基于策略的RL方法创建完整的工作原型。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The focus of this chapter is policy-based methods for RL. However, before diving
    into a formal introduction to policy-based methods for RL, let's spend some time
    understanding the motivation behind them. Let's go back a few hundred years when
    the globe was still mostly undiscovered and maps were incomplete. Brave sailors
    at that time sailed the great oceans with only indomitable courage and unyielding
    curiosity on their side. But they weren't completely blind in the vastness of
    the oceans. They looked up to the night sky for direction. The stars and planets
    in the night sky guided them to their destination. The night sky is viewed differently
    at different times of the year from different parts of the globe. This information,
    along with highly accurate maps of the night sky, guided these brave explorers
    to their destinations and sometimes to unknown, uncharted lands.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重点是基于策略的强化学习方法（RL）。然而，在正式介绍基于策略的强化学习方法之前，我们先花些时间理解它们背后的动机。让我们回到几百年前，那时地球大部分地方还未被探索，地图也不完整。那个时候，勇敢的水手们凭借坚定的勇气和不屈的好奇心航行在广阔的海洋上。但是，他们在辽阔的海洋中并非完全盲目。他们仰望夜空寻找方向。夜空中的星星和行星引导着他们走向目的地。不同时间和地点看到的夜空是不同的。正是这些信息，加上精确的夜空地图，指引着这些勇敢的探险家们到达目的地，有时甚至是未知的、未标记的土地。
- en: Now, you might question what this story has to do with RL at all. A map of the
    night sky wasn't always available to those sailors. They were created by globetrotters,
    sailors, skywatchers, and astronomers over centuries. Sailors actually voyaged
    blindly at one time. They looked at the stars during night time, and every time
    they took a turn, they marked their position relative to the position of the stars
    in the night sky. Upon reaching their destination, they evaluated each turn they
    took and worked out which was more effective during their voyage. Every other
    ship that sailed to the same destination could do the same. With time, they had
    a good assessment of the turns that are the most effective for reaching a certain
    destination with respect to a ship's position in the sea, as assessed by looking
    at the position of the stars in the night sky. You can think of this as computing
    the value function where you know the immediate best move. But once sailors had
    a complete map of the night sky, they could simply derive a policy that would
    lead them to their destination.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能会问这个故事与强化学习有什么关系。那些水手们并非总是能够获得夜空的地图。这些地图是由环球旅行者、水手、天文爱好者和天文学家们经过数百年创造的。水手们实际上曾经一度在盲目中航行。他们在夜间观察星星，每次转弯时，他们都会标记自己相对于星星的位置。当到达目的地时，他们会评估每个转弯，并找出哪些转弯在航行过程中更为有效。每一艘驶向相同目的地的船只也可以做同样的事情。随着时间的推移，他们对哪些转弯在相对于船只在海上的位置，结合夜空中星星的位置，能更有效地到达目的地有了较为清晰的评估。你可以把它看作是在计算价值函数，通过这种方式，你能知道最优的即时动作。但一旦水手们拥有了完整的夜空地图，他们就可以简单地推导出一套策略，带领他们到达目的地。
- en: You can consider the sea and the night sky as the environment and the sailors
    as agents within it. Over the span of a few centuries, our agents (sailors) built
    a model of their environment and so were able to come up with a value function
    (calculating a ship's relative position) that would lead them to the immediate
    best possible step (immediate navigational step) and also helped them build the
    optimal policy (a complete navigation route).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将大海和夜空视为环境，而将水手视为其中的智能体。在几百年的时间里，我们的智能体（水手）建立了对环境的模型，从而能够得出一个价值函数（计算船只相对位置），进而引导他们采取最佳的即时行动步骤（即时航行步骤），并帮助他们建立了最佳策略（完整的航行路线）。
- en: In the last chapter, you learned about **Deep Recurrent Q Networks** (**DRQNs**)
    and their advantage over simple deep Q networks. You also modeled a DRQN network
    for playing the very popular Atari video game *Breakout*. In this chapter, you'll
    learn about policy-based approaches to RL.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了**深度递归Q网络**（**DRQN**）及其相较于简单深度Q网络的优势。你还为非常流行的雅达利电子游戏*Breakout*建模了一个DRQN网络。在本章中，你将学习基于策略的强化学习方法。
- en: We will also learn about the policy gradient, which will help you learn about
    the model in real time. We will then learn about a policy gradient technique called
    DDPG to understand the continuous action space. Here, we will also learn how to
    code the Lunar Lander simulation to understand DDPGs using classes such as the
    `OUActionNoise` class, the `ReplayBuffer` class, the `ActorNetwork` class, and
    the `CriticNetwork` class. We will learn about these classes in detail later in
    this chapter. Finally, we will learn how we can improve the policy gradient technique
    by using the TRPO, PPO, and **Advantage Actor-Critic** (**A2C**) techniques. These
    techniques will help us reduce the operating cost of training the model and so
    will improve the policy-gradient technique.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将学习策略梯度，它将帮助你实时学习模型。接着，我们将了解一种名为DDPG的策略梯度技术，以便理解连续动作空间。在这里，我们还将学习如何编写月球着陆模拟（Lunar
    Lander）代码，使用`OUActionNoise`类、`ReplayBuffer`类、`ActorNetwork`类和`CriticNetwork`类等类来理解DDPG。我们将在本章后面详细了解这些类。最后，我们将学习如何通过使用TRPO、PPO和**优势演员评论家**（**A2C**）技术来改进策略梯度方法。这些技术将帮助我们减少训练模型的运行成本，从而改进策略梯度技术。
- en: Let's begin by learning about some basic concepts, such as value-based RL, model-based
    RL, actor-critic, and action space, in the following sub-sections.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从以下小节开始，学习一些基本概念，如基于值的强化学习（RL）、基于模型的强化学习（RL）、演员-评论家方法、动作空间等。
- en: Introduction to Value-Based and Model-Based RL
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 值基和模型基强化学习简介
- en: While it is useful to have a good model of the environment to be able to predict
    whether a particular move is better with regard to other possible moves, you still
    need to assess all the possible moves from every possible state in order to come
    up with an optimal policy. This is a non-trivial problem and is also computationally
    expensive if, say, our environment is a simulation and our agent is **Artificial
    Intelligence** (**AI**). This approach of model-based learning, when applied within
    a simulation, can look like the following scenario.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然拥有一个良好的环境模型有助于预测某个特定动作相对于其他可能动作是否更好，但你仍然需要评估每一个可能状态下的所有可能动作，以便制定出最佳策略。这是一个非平凡的问题，如果我们的环境是一个仿真，且智能体是**人工智能**（**AI**），那么计算开销也非常大。将基于模型的学习方法应用到仿真中时，可以呈现如下情景。
- en: Consider the game of *Pong* (*Figure 11.1*). (*Pong*—released in 1972—was one
    of the first arcade video games manufactured by Atari.) Now, let's see how the
    model-based learning approach could be beneficial for an optimal policy playing
    *Pong* and what could be its drawbacks. So, suppose our agent has learned how
    to play *Pong* by looking at the game environment—that is, by looking at the black
    and white pixels of each frame. We can then ask our agent to predict the next
    possible state given a certain frame of black and white pixels from the game environment.
    But if there is any background noise in the environment (for example, a random,
    unrelated video playing in the background), our agent would also take that into
    consideration.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以*乒乓*（*图11.1*）为例。（*乒乓*——发布于1972年——是雅达利公司制造的第一批街机电子游戏之一。）现在，让我们看看基于模型的学习方法如何有助于为*乒乓*制定最佳策略，并探讨其可能的缺点。那么，假设我们的智能体通过观察游戏环境（即，观察每一帧的黑白像素）学会了如何玩*乒乓*。接下来，我们可以要求智能体根据游戏环境中的某一帧黑白像素来预测下一个可能的状态。但如果环境中有任何背景噪音（例如，背景中播放着一个随机的、无关的视频），我们的智能体也会将其考虑在内。
- en: 'Now, in most cases, those background noises would not help us in our planning—that
    is, determining an optimal policy—but would still eat up our computational resources.
    Following is a screenshot of *Pong* game:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在大多数情况下，这些背景噪声对我们的规划没有帮助——也就是说，确定最优策略——但仍然会消耗我们的计算资源。以下是*Pong*游戏的截图：
- en: '![Figure 11.1: The Atari Pong game'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.1：雅达利 Pong 游戏'
- en: '](img/B16182_11_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_11_01.jpg)'
- en: 'Figure 11.1: The Atari Pong game'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1：雅达利 Pong 游戏
- en: A value-based approach is better than the model-based approach because while
    performing the transition from one state to another, a value-based approach would
    only care about the value of the action in terms of the cumulative reward that
    we are predicting for each action. It would deem any background noise as mostly
    irrelevant. A value-based approach is well-suited for deriving an optimal policy.
    Imagine you have learned an action-value function—a Q function. Then, you can
    simply look at the highest values in each state and that gives you the optimal
    policy. However, value-based functions could still be inefficient. Let me try
    to explain why with an example. In order to travel from Europe to North America,
    or from South Africa to the southern coasts of India, the optimal policy for our
    explorer ship might just be to go straight. However, the ship might encounter
    icebergs, small islands, or ocean currents that might set it off course temporarily.
    It might still be the optimal policy for the ship to head straight, but the value
    function might change arbitrarily. So, a value-based method, in this case, would
    try to approximate all the arbitrary values, while a policy can be blind and,
    therefore, be more efficient in terms of computational cost. So, in many cases,
    it might be less efficient to compute an optimal policy based on the value function.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 基于值的方法优于基于模型的方法，因为在执行从一个状态到另一个状态的转换时，基于值的方法只关心每个动作的价值，基于我们为每个动作预测的累积奖励。它会认为任何背景噪声大多是无关紧要的。基于值的方法非常适合推导最优策略。假设你已经学会了一个动作-值函数——一个
    Q 函数。那么，你可以简单地查看每个状态下的最高值，这样就能得出最优策略。然而，基于值的函数仍然可能效率低下。让我用一个例子来解释为什么。在从欧洲到北美，或者从南非到印度南部海岸的旅行中，我们的探索船的最优策略可能只是直接前进。然而，船可能会遇到冰山、小岛或洋流，这些可能会暂时偏离航道。它仍然可能是船只前进的最优策略，但值函数可能会任意变化。所以，在这种情况下，基于值的方法会尝试逼近所有这些任意值，而基于策略的方法可以是盲目的，因此在计算成本上可能更高效。因此，在很多情况下，基于值的函数计算最优策略可能效率较低。
- en: Introduction to Actor-Critic Model
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演员-评论员模型简介
- en: So, we have briefly explained the trade-offs between the value-based and model-based
    approaches. Now, can we somehow take the best of both the worlds and create a
    hybrid of them? The actor-critic model will help us to do that. If we draw a Venn
    diagram (*Figure 11.2*), we will see that the actor-critic model lies at the intersection
    of the value-based and policy-based RL approaches. They can basically learn a
    value function as well as a policy. We will discuss actor-critic model further
    in the following sections.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们简要解释了基于值方法和基于模型方法之间的权衡。现在，我们能否以某种方式结合这两者的优点，创建一个它们的混合模型呢？演员-评论员模型将帮助我们实现这一点。如果我们画出一个维恩图（*图
    11.2*），我们会发现演员-评论员模型位于基于值和基于策略的强化学习方法的交集处。它们基本上可以同时学习值函数和策略。我们将在接下来的章节中进一步讨论演员-评论员模型。
- en: '![Figure 11.2: The relation between different RL approaches'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.2：不同强化学习方法之间的关系'
- en: '](img/B16182_11_02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_11_02.jpg)'
- en: 'Figure 11.2: The relation between different RL approaches'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2：不同强化学习方法之间的关系
- en: 'In practice, most of the time, we try to learn a policy based on the values
    yielded by the value function, but we actually learn the policy and the values
    simultaneously. To end this introduction to actor-critic, let me share a quote
    by Bertrand Russell. Russell, in his book *The Problems of Philosophy*, said:
    "*We can know the general proposition without inferring it from instances, although
    some instances are usually necessary to make clear to us what the general proposition
    means.*" Treat that as food for thought. The code on how to implement the actor-critic
    model is shown later in this chapter. Next, we will learn about action spaces,
    the basics of which we already covered in *Chapter 1*, *Introduction to Reinforcement
    Learning*.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，大多数时候，我们尝试基于价值函数所产生的值来学习策略，但实际上我们是同时学习策略和值的。为了结束这部分关于演员-评论家方法的介绍，我想分享一句
    Bertrand Russell 的名言。Russell 在他的书《哲学问题》中说：“*我们可以不通过实例推导一般命题，而仅凭一般命题的意义就能理解它，尽管一些实例通常是必需的，用以帮助我们弄清楚一般命题的含义。*”
    这句话值得我们思考。关于如何实现演员-评论家模型的代码将在本章后面介绍。接下来，我们将学习动作空间的内容，我们已经在*第 1 章*《强化学习导论》中涉及了它的基本概念。
- en: 'In the previous chapters, we already covered the basic definition and the types
    of action spaces. Here, we will quickly revise the concept of action spaces. Action
    spaces define the properties of the game environment. Let''s look at the following
    diagram to understand the types:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们已经介绍了动作空间的基本定义和类型。在这里，我们将快速回顾一下动作空间的概念。动作空间定义了游戏环境的特性。让我们看一下以下图示来理解这些类型：
- en: '![Figure 11.3: Action spaces'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.3：动作空间'
- en: '](img/B16182_11_03.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_11_03.jpg)'
- en: 'Figure 11.3: Action spaces'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3：动作空间
- en: There are two types of action spaces—discrete and continuous. Discrete action
    spaces allow discrete inputs—for example, the buttons on a gamepad. These discrete
    actions can move in either the left or right direction, going either up or down,
    moving forward or backward, and so on.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 动作空间有两种类型——离散和连续。离散动作空间允许离散的输入——例如，游戏手柄上的按钮。这些离散动作可以向左或向右移动，向上或向下移动，向前或向后移动，等等。
- en: On the other hand, continuous action spaces allow continuous inputs—for example,
    inputs from a steering wheel or a joystick. In the following section, we will
    learn how to apply a policy gradient to a continuous action space.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，连续动作空间允许连续输入——例如，方向盘或摇杆的输入。在接下来的章节中，我们将学习如何将策略梯度应用于连续动作空间。
- en: Policy Gradients
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度
- en: 'Now that we have established the motivation behind favoring policy-based methods
    over value-based ones with the navigation example in the previous section, let''s
    begin our formal introduction to policy gradients. Unlike Q-learning, which uses
    a storage buffer to store past experiences, policy-gradient methods learn in real
    time (that is, they learn from the most recent experience or action). A policy
    gradient''s learning is driven by whatever the agent encounters in the environment.
    After each gradient update, the experience is discarded and the policy moves on.
    Let''s look at a pictorial representation of what we have just learned:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经通过上一节中的导航示例阐明了偏好基于策略的方法而非基于价值的方法的动机，那么让我们正式介绍策略梯度。与使用存储缓冲区存储过去经验的 Q 学习不同，策略梯度方法是实时学习的（即它们从最新的经验或动作中学习）。策略梯度的学习是由智能体在环境中遇到的任何情况驱动的。每次梯度更新后，经验都会被丢弃，策略继续前进。让我们看一下我们刚才学到的内容的图示表示：
- en: '![Figure 11.4: The policy gradient method explained pictorially'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.4：策略梯度方法的图示解释'
- en: '](img/B16182_11_04.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_11_04.jpg)'
- en: 'Figure 11.4: The policy gradient method explained pictorially'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4：策略梯度方法的图示解释
- en: 'One thing that should immediately catch our attention is that the policy gradient
    method is, in general, less sample-efficient than Q-learning because the experiences
    are discarded after each gradient update. The mathematical representation of the
    gradient estimator is given as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一个立即引起我们注意的事情是，策略梯度方法通常比 Q 学习效率低，因为每次梯度更新后，经验都会被丢弃。策略梯度估计器的数学表示如下：
- en: '![Figure 11.5: Mathematical representation of policy gradient estimator'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.5：策略梯度估计器的数学表示'
- en: '](img/B16182_11_05.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_11_05.jpg)'
- en: 'Figure 11.5: Mathematical representation of policy gradient estimator'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5：策略梯度估计器的数学表示
- en: In this equation, ![a](img/B16182_11_05a.png) is the stochastic policy and ![b](img/B16182_11_05b.png)
    is our advantage estimation function at time ![c](img/B16182_11_05c.png) —the
    estimate of the relative value of the selected action. The expectation,![d](img/B16182_11_05d.png)
    , indicates the average over a finite batch of samples in our algorithm, where
    we perform sampling and optimization, alternatively. Here, ![e](img/B16182_11_05e.png)
    is the gradient estimator. The ![f](img/B16182_11_05f.png) and ![g](img/B16182_11_05g.png)
    variables define the action and state at the time interval, ![h](img/B16182_11_05h.png).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，![a](img/B16182_11_05a.png) 是随机策略，![b](img/B16182_11_05b.png) 是我们在时间点
    ![c](img/B16182_11_05c.png) 上的优势估计函数——即所选动作的相对价值估计。期望值，![d](img/B16182_11_05d.png)，表示我们算法中有限样本批次的平均值，在其中我们交替进行采样和优化。这里，![e](img/B16182_11_05e.png)
    是梯度估计器。![f](img/B16182_11_05f.png) 和 ![g](img/B16182_11_05g.png) 变量定义了在时间间隔 ![h](img/B16182_11_05h.png)
    时的动作和状态。
- en: 'Finally, the policy gradient loss is defined as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，策略梯度损失定义如下：
- en: '![Figure 11.6: Policy gradient loss defined'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.6：策略梯度损失定义'
- en: '](img/B16182_11_06.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_11_06.jpg)'
- en: 'Figure 11.6: Policy gradient loss defined'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6：策略梯度损失定义
- en: In order to calculate the advantage function, ![a](img/B16182_11_06a.png) ,
    we need the **discounted reward** and the **baseline estimate**. The discounted
    reward is also known as the **return**, which is the weighted sum of all the rewards
    our agent got during the current episode. It is called the discounted reward as
    there is a discount factor associated with it that prioritizes the immediate rewards
    over the long-term ones. ![b](img/B16182_11_06b.png) is basically the difference
    between the discounted reward and the baseline estimate.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算优势函数，![a](img/B16182_11_06a.png)，我们需要 **折扣奖励** 和 **基准估计**。折扣奖励也称为 **回报**，它是我们智能体在当前回合中获得的所有奖励的加权和。之所以称为折扣奖励，是因为它关联了一个折扣因子，优先考虑即时奖励而非长期奖励。![b](img/B16182_11_06b.png)
    本质上是折扣奖励和基准估计之间的差值。
- en: Note that if you still have any problems with wrapping your head around the
    concept, then it's not a big problem. Just try to grasp the overall idea and you'll
    be able to grasp the full concept of it eventually. Having said that, let me also
    introduce you to a stripped-down version of the vanilla policy gradient algorithm.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果你在理解这个概念时仍然有问题，那也不是大问题。只需尝试抓住整体概念，最终你会理解完整的概念。话虽如此，我还将向你介绍一个简化版的基础策略梯度算法。
- en: 'We start by initializing the policy parameter, ![c](img/B16182_11_06c.png),
    and the baseline, ![d](img/B16182_11_06d.png):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先初始化策略参数，![c](img/B16182_11_06c.png)，以及基准值，![d](img/B16182_11_06d.png)：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: One suggestion would be to go through the algorithm multiple times, along with
    the initial explanation, in order to properly understand the concept of policy
    gradients. But again, an overall understanding of things should be your first
    priority.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一个建议是反复浏览算法，并配合初步解释，来更好地理解策略梯度的概念。但再说一次，首先掌握整体的理解才是最重要的。
- en: 'Before implementing the practical elements, please install OpenAI Gym and the
    Box2D environment (which includes environments such as Lunar Lander) using PyPI.
    To carry out the installation, type the following commands into Terminal/Command Prompt:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现实际元素之前，请通过 PyPI 安装 OpenAI Gym 和 Box2D 环境（包括如 Lunar Lander 等环境）。要进行安装，请在终端/命令提示符中输入以下命令：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, let's implement an exercise using the policy gradient method.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用策略梯度方法来实现一个练习。
- en: 'Exercise 11.01: Landing a Spacecraft on the Lunar Surface Using Policy Gradients
    and the Actor-Critic Method'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 11.01：使用策略梯度和演员-评论员方法将航天器着陆在月球表面
- en: 'In this exercise, we will work on a toy problem (OpenAI Lunar Lander) and help
    land the Lunar Lander inside the OpenAI Gym Lunar Lander environment using vanilla
    policy gradients and actor-critic. The following are the steps to implement this exercise:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将处理一个玩具问题（OpenAI Lunar Lander），并使用基础策略梯度和演员-评论员方法帮助将月球着陆器着陆到 OpenAI
    Gym Lunar Lander 环境中。以下是实现此练习的步骤：
- en: 'Open a new Jupyter Notebook, import all the necessary libraries (`gym`, `torch`,
    and `numpy`):'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter Notebook，导入所有必要的库（`gym`，`torch` 和 `numpy`）：
- en: '[PRE2]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Define the `ActorCritic` class:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `ActorCritic` 类：
- en: '[PRE3]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So, during the initialization of the `ActorCritic` class in the preceding code,
    we are creating our action and value networks. We are also creating blank arrays
    for storing the log probabilities, state values, and rewards.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，在前面代码中初始化 `ActorCritic` 类时，我们正在创建我们的动作和价值网络。我们还创建了空数组来存储对数概率、状态值和奖励。
- en: 'Next, create a function to pass our state through the layers and name it `forward`:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个函数，将我们的状态通过各个层并命名为 `forward`：
- en: '[PRE4]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we are taking the state and passing it through the value layer after a
    ReLU transformation to get the state value. Similarly, we are passing the state
    through the action layer, followed by a softmax function, to get the action probabilities.
    Then, we are transforming the probabilities to discrete values for the purpose
    of sampling. Finally, we are adding our log probabilities and state values to
    their respective arrays and returning an action item.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们将状态传递通过值层，经过ReLU转换后得到状态值。类似地，我们将状态通过动作层，然后使用softmax函数得到动作概率。接着，我们将概率转换为离散值以供采样。最后，我们将对数概率和状态值添加到各自的数组中并返回一个动作项。
- en: 'Create the `computeLoss` function to calculate a discounted reward first. This
    will help give greater priority to the immediate reward. Then, we will calculate
    the loss as described in the policy gradient loss equation:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `computeLoss` 函数，首先计算折扣奖励。这有助于优先考虑即时奖励。然后，我们将按照策略梯度损失方程来计算损失：
- en: '[PRE5]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, create a `clear` method to clear the arrays that store the log probabilities,
    state values, and rewards after each episode:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个 `clear` 方法，用于在每回合后清除存储对数概率、状态值和奖励的数组：
- en: '[PRE6]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, let''s start with the main code, which will help us to call the classes
    that we defined previously in the exercise. We start by assigning a random seed:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们开始编写主代码，这将帮助我们调用之前在练习中定义的类。我们首先分配一个随机种子：
- en: '[PRE7]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we need to set up our environment and initialize our policy:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们需要设置我们的环境并初始化我们的策略：
- en: '[PRE8]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we iterate for at least `10000` iterations for proper convergence.
    In each iteration, we sample an action and get the state and reward for that action.
    Then, we update our policy based on that action and clear our observations:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们迭代至少`10000`次以确保适当收敛。在每次迭代中，我们采样一个动作并获取该动作的状态和奖励。然后，我们基于该动作更新我们的策略，并清除我们的观察数据：
- en: '[PRE9]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, when you run the code, you''ll see the running reward for each episode.
    The following is the reward for the first 20 episodes out of the total 10,000 episodes:'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，当你运行代码时，你将看到每个回合的运行奖励。以下是前20个回合的奖励，总代码为10,000回合：
- en: '[PRE10]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The output for only the first 20 episodes is shown here for ease of presentation.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了方便展示，这里仅展示了前20个回合的输出。
- en: To access the source code for this specific section, please refer to [https://packt.live/3hDibst](https://packt.live/3hDibst).
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参阅 [https://packt.live/3hDibst](https://packt.live/3hDibst)。
- en: This section does not currently have an online interactive example and will
    need to be run locally.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本部分目前没有在线互动示例，需要在本地运行。
- en: 'This output indicates that our agent, the Lunar Lander, has started taking
    actions. The negative reward indicates that in the beginning, the agent is not
    smart enough to take the right actions and so it takes random actions, for which
    it is rewarded negatively. A negative reward is a penalty. With time, the agent
    will start getting positive rewards as it starts learning. Soon, you''ll see the
    game window popping up on your screen showing the real-time progress of your Lunar
    Lander, as in the following screenshot:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出表示我们的智能体——月球着陆器，已经开始采取行动。负奖励表明一开始，智能体不够聪明，无法采取正确的行动，因此它采取了随机行动，并因此受到了负面奖励。负奖励是惩罚。随着时间的推移，智能体将开始获得正面奖励，因为它开始学习。很快，你会看到游戏窗口弹出，展示月球着陆器的实时进展，如下图所示：
- en: '![Figure 11.7: The real-time progress of the Lunar Lander'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.7：月球着陆器的实时进展'
- en: '](img/B16182_11_07.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_11_07.jpg)'
- en: 'Figure 11.7: The real-time progress of the Lunar Lander'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7：月球着陆器的实时进展
- en: In the next section, we will look into DDPGs, which extend the idea of policy
    gradients.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将研究DDPG，它扩展了策略梯度的思想。
- en: Deep Deterministic Policy Gradients
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度确定性策略梯度
- en: In this section, we will apply the DDPG technique to understand the continuous
    action space. Moreover, we will learn how to code a moon lander simulation to
    understand DDPGs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将应用DDPG技术以理解连续动作空间。此外，我们还将学习如何编写月球着陆模拟程序来理解DDPG。
- en: Note
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We suggest that you type all the code given in this section into your Jupyter
    notebook as we will be using it later, in *Exercise 11.02*, *Creating a Learning Agent*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议你将本节中给出的所有代码输入到你的Jupyter笔记本中，因为我们将在后续的*练习11.02*中使用它，*创建学习智能体*。
- en: 'We are going to use the OpenAI Gym Lunar Lander environment for continuous
    action spaces here. Let''s start by importing the essentials:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里使用OpenAI Gym的Lunar Lander环境来处理连续动作空间。让我们首先导入必要的内容：
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now, we will learn how to define some classes, such as the `OUActionNoise` class,
    the `ReplayBuffer` class, the `ActorNetwork` class, and the `CriticNetwork` class,
    which will help us to implement the DDGP technique. At the end of this section,
    you'll have the complete code base that applies the DDPG within our OpenAI Gym
    game environment.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将学习如何定义一些类，如`OUActionNoise`类、`ReplayBuffer`类、`ActorNetwork`类和`CriticNetwork`类，这些将帮助我们实现DDPG技术。在本节结束时，你将获得一个完整的代码库，可以在我们的OpenAI
    Gym游戏环境中应用DDPG。
- en: Ornstein-Uhlenbeck Noise
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ornstein-Uhlenbeck噪声
- en: 'First, we will define a class that will provide us with something known as
    Ornstein-Uhlenbeck noise. This Ornstein–Uhlenbeck process, in physics, is used
    to model the velocity of a Brownian particle under the influence of friction.
    Brownian motion, as you may already know, is the random motion of particles when
    suspended in a fluid (liquid or gas) resulting from their collisions with other
    particles in the same fluid. Ornstein–Uhlenbeck noise gives you a type of noise
    that is temporally correlated and is centered on a mean of 0\. Since the agent
    has zero knowledge of the model, it becomes difficult to train it. Here, Ornstein–Uhlenbeck
    noise can be used as a sample to generate that knowledge. Let''s look at the code
    implementation of this class:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义一个类，提供一种被称为Ornstein-Uhlenbeck噪声的东西。物理学中的Ornstein–Uhlenbeck过程用于模拟在摩擦力作用下布朗运动粒子的速度。如你所知，布朗运动是指悬浮在液体或气体中的粒子，由于与同一流体中其他粒子的碰撞而产生的随机运动。Ornstein–Uhlenbeck噪声提供的是一种具有时间相关性的噪声，并且其均值为0。由于智能体对模型的了解为零，因此训练它变得很困难。在这种情况下，Ornstein–Uhlenbeck噪声可以用作生成这种知识的样本。让我们来看一下这个类的代码实现：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding code, we defined three different functions—that is, `_init_()`,
    `_call()_`, and `reset()`. In the next section, we will learn how to implement
    the `ReplayBuffer` class to store the agent's past learnings.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们定义了三个不同的函数——即`_init_()`、`_call()_`和`reset()`。在接下来的部分，我们将学习如何实现`ReplayBuffer`类来存储智能体的过去学习记录。
- en: The ReplayBuffer Class
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReplayBuffer类
- en: 'Replay buffer is a concept we have borrowed from Q-learning. This buffer is
    basically a space to store all of our agent''s past learnings, which will help
    us to train the model better. We will initialize the class by defining the memory
    size for our state, action, and rewards, respectively. So, the initialization
    would look something like this:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Replay buffer是我们从Q-learning中借用的一个概念。这个缓冲区本质上是一个存储所有智能体过去学习记录的空间，它将帮助我们更好地训练模型。我们将通过定义状态、动作和奖励的记忆大小来初始化该类。所以，初始化看起来会像这样：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we need to define the `store_transition` method. This method takes the
    state, action, reward, and new state as arguments and stores the transitions from
    one state to another. There''s also a `done` flag to indicate the terminal state
    of our agent. Note that the index here is just a counter that we initialized previously,
    and it starts from `0` when its value is equal to the maximum memory size:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义`store_transition`方法。这个方法接受状态、动作、奖励和新状态作为参数，并存储从一个状态到另一个状态的过渡。这里还有一个`done`标志，用于指示智能体的终止状态。请注意，这里的索引只是一个计数器，我们之前已初始化，它从`0`开始，当其值等于最大内存大小时：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, we need the `sample_buffer` method, which will be used to randomly
    sample the buffer:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要`sample_buffer`方法，它将用于随机抽样缓冲区：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'So, the entire class, at a glance, looks like this:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，整个类一目了然，应该是这样的：
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In this section, we learned how to store the agent's past learnings to train
    the model better. Next, we will learn in more detail about the actor-critic model,
    which we briefly explained in this chapter's introduction.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们学习了如何存储智能体的过去学习记录以更好地训练模型。接下来，我们将更详细地学习我们在本章简介中简要解释过的actor-critic模型。
- en: The Actor-Critic Model
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Actor-Critic模型
- en: Next, in the DDPG technique, we will define the actor and critic networks. Now,
    we have already introduced actor-critic, but we haven't talked much about it.
    Take the actor as the current policy and the critic as the value. You may conceptualize
    the actor-critic model as a guided policy. We will define our actor-critic model
    using fully connected neural networks.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在DDPG技术中，我们将定义演员和评论家网络。现在，我们已经介绍了演员-评论家模型，但我们还没有详细讨论过它。可以将演员视为当前的策略，评论家则是价值函数。你可以将演员-评论家模型概念化为一个引导策略。我们将使用全连接神经网络来定义我们的演员-评论家模型。
- en: The `CriticNetwork` class starts with an initialization. First, we will explain
    the parameters. The `Linear` layer and we will initialize it using our input and
    output dimensions. Next, is the initialization of the weights and biases of our
    fully connected layer. This initialization restricts the values of the weights
    and biases to a very narrow band of the parameter space when we sample between
    the `-f1` to `f1` range, as seen in the following code. This helps our network
    to better converge. Our initial layer is followed by a batch normalization, which
    again helps to better converge our network. We will repeat the same process with
    our second fully connected layer. The `CriticNetwork` class will also get an action
    value. Finally, the output is a single scalar value, which we will initialize
    next with a constant initialization.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`CriticNetwork`类从初始化开始。首先，我们将解释参数。`Linear`层，我们将使用输入和输出维度对其进行初始化。接下来是全连接层的权重和偏置的初始化。此初始化将权重和偏置的值限制在参数空间的一个非常窄的范围内，我们在`-f1`到`f1`之间采样，如下代码所示。这有助于我们的网络更好地收敛。我们的初始层后面跟着一个批量归一化层，这同样有助于更好地收敛网络。我们将对第二个全连接层重复相同的过程。`CriticNetwork`类还会获取一个动作值。最后，输出是一个标量值，我们接下来将其初始化为常数初始化。'
- en: 'We will optimize our `CriticNetwork` class using the `Adam` optimizer with
    a learning rate beta:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用学习率为 beta 的`Adam`优化器来优化我们的`CriticNetwork`类：
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we have to write the `forward` function for our network. This takes a
    state and an action as input. We get the state-action value from this method.
    So, our state goes through the first fully connected layer, followed by the batch
    normalization and the ReLU activation. The activation is passed through the second
    fully connected layer, followed by another batch normalization, and before the
    final activation, we take into account the action value. Notice that we are adding
    the state and action values together to form the state-action value. The state-action
    value is then passed through the final layer and there we have our output:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须为我们的网络编写`forward`函数。该函数接受一个状态和一个动作作为输入。我们通过这个方法获得状态-动作值。因此，我们的状态经过第一个全连接层，接着是批量归一化和ReLU激活函数。激活值通过第二个全连接层，然后是另一个批量归一化层，在最终激活之前，我们考虑动作值。请注意，我们将状态和动作值相加，形成状态-动作值。然后，状态-动作值通过最后一层，最终得到我们的输出：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'So, finally, the `CriticNetwork` class would look like this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，最终，`CriticNetwork`类的结构如下所示：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we will define `ActorNetwork`. This would be mostly similar to the `CriticNetwork`
    class but with some minor yet important changes. Let''s code it first and then
    we will explain it:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义`ActorNetwork`。它与`CriticNetwork`类大致相同，但有一些细微而重要的变化。让我们先编写代码，然后再解释：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you can see, this is similar to our `CriticNetwork` class. The main difference
    here is that we don't have an action value here and that we have written the `forward`
    function in a slightly different way. Notice that the final output from the `forward`
    function is a `tanh` function, which will bind our output between `0` and `1`.
    This is necessary for the environment we are going to play around with. Let's
    implement an exercise that will help us to create a learning agent.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这与我们的`CriticNetwork`类类似。这里的主要区别是，我们没有动作值，并且我们以稍微不同的方式编写了`forward`函数。请注意，`forward`函数的最终输出是一个`tanh`函数，它将我们的输出限制在`0`和`1`之间。这对于我们将要处理的环境是必要的。让我们实现一个练习，帮助我们创建一个学习代理。
- en: 'Exercise 11.02: Creating a Learning Agent'
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 11.02：创建一个学习代理
- en: In this exercise, we will write our `Agent` class. We are already familiar with
    the concept of a learning agent, so let's see how we can implement one. This exercise
    will conclude the DDPG example that we have been building. Please make sure that
    you have run all the example code in this section before starting the exercise.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将编写我们的`Agent`类。我们已经熟悉学习智能体的概念，那么让我们看看如何实现一个。这个练习将完成我们一直在构建的DDPG示例。在开始练习之前，请确保已经运行了本节中的所有示例代码。
- en: Note
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We have assumed you have typed the code presented in the preceding section into
    a new notebook. Specifically, we have assumed you already have the code for importing
    the necessary libraries and creating the `OUActionNoise`, `ReplayBuffer`, `CriticNetwork`,
    and `ActorNetwork` classes in your notebook. This exercise begins by creating
    the `Agent` class.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设你已经将前一节中呈现的代码输入到新的笔记本中。具体来说，我们假设你已经在笔记本中编写了导入必要库的代码，并创建了`OUActionNoise`、`ReplayBuffer`、`CriticNetwork`和`ActorNetwork`类。本练习开始时会创建`Agent`类。
- en: For convenience, the complete code for this exercise, including the code in
    the example, can be found at [https://packt.live/37Jwhnq](https://packt.live/37Jwhnq).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，本练习的完整代码，包括示例中的代码，可以在[https://packt.live/37Jwhnq](https://packt.live/37Jwhnq)找到。
- en: 'The following are the steps to implement this exercise:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是实现此练习的步骤：
- en: 'Let''s start by using the `__init__` method and passing the alpha and the beta,
    which are the learning rates for our actor and critic networks, respectively.
    Then, pass the input dimensions and a parameter called `tau`, which we will explain
    in a bit. Then, we want to pass the environment, which is our continuous action
    space, gamma, which is the agent''s discount factor, which we talked about earlier.
    Then, the number of actions, the maximum size of the memory, the size of the two
    layers, and the batch size are passed. Then, initialize our actor and critic.
    Finally, we will introduce our noise and the `update_params` function:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先使用`__init__`方法，传入alpha和beta，它们分别是我们演员和评论家网络的学习率。然后，传入输入维度和一个名为`tau`的参数，我们稍后会解释它。接着，我们希望传入环境，它是我们的连续动作空间，gamma是智能体的折扣因子，之前我们已经讨论过。然后，传入动作数量、记忆的最大大小、两层的大小和批处理大小。接着，初始化我们的演员和评论家。最后，我们将引入噪声和`update_params`函数：
- en: '[PRE21]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `update_params` function updates our parameters, but there's a catch. We
    basically have a moving target. This means we are using the same network to calculate
    the action and the value of the action simultaneously as we are updating the estimate
    in every episode. Because we are using the same parameters for both, it may lead
    to divergence. To tackle that, we use the target network, which learns the value
    and the action combinations, and the other network is used to learn the policy.
    We will periodically update the target network's parameters with the parameters
    of the evaluation network.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`update_params`函数更新我们的参数，但有一个关键问题。我们基本上有一个动态目标。这意味着我们使用相同的网络来同时计算动作和动作的值，同时在每个回合中更新估计值。由于我们为两者使用相同的参数，这可能会导致发散。为了解决这个问题，我们使用目标网络，它学习值和动作的组合，另一个网络则用于学习策略。我们将定期用评估网络的参数更新目标网络的参数。'
- en: 'Next, we have the `select_action` method. Here, we take the observation from
    our actor and pass it through the feed-forward network. `mu_prime` here is basically
    the noise we add to the network. It is also called exploration noise. Finally,
    we call `actor.train()` and return the `numpy` value for `mu_prime`:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们有`select_action`方法。在这里，我们从演员（actor）获取观察结果，并将其通过前馈网络传递。这里的`mu_prime`基本上是我们添加到网络中的噪声，也称为探索噪声。最后，我们调用`actor.train()`并返回`mu_prime`的`numpy`值：
- en: '[PRE22]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next comes our `remember` function, which is self-explanatory. This takes the
    `state`, `action`, `reward`, `new_state`, and `done` flags in order to store them
    in memory:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是我们的`remember`函数，它不言自明。这个函数接收`state`、`action`、`reward`、`new_state`和`done`标志，以便将它们存储到记忆中：
- en: '[PRE23]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we will define the `learn` function:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义`learn`函数：
- en: '[PRE24]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Here, we first check whether we have enough samples in our memory buffer for
    learning. So, if our memory counter is less than the batch size—meaning we do
    not have the batch size number of samples in our memory buffer—we simply return
    the value. Otherwise, we sample from our memory buffer the `state`, `action`,
    `reward`, `new_state`, and `done` flags. Once sampled, we must convert all of
    these flags into tensors for implementation. Then, we need to calculate the target
    actions, followed by the calculation of the new critic value using the target
    action states and the new state. Next, we calculate the critic value, which is
    the value we met for the states and actions in the current replay buffer. After
    that, we calculate the targets. Note that the part where we multiply `gamma` with
    the new critic value becomes `0` when the `done` flag is `0`. This basically means
    that when the episode is over, we only take into account the reward from the current
    state. The target is then converted into a tensor and reshaped for implementation
    purposes. Now, we can calculate and backpropagate our loss for the critic. Then,
    we do the same for our actor network. Finally, we update the parameters for our
    target actor and target critic network.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们首先检查我们是否在内存缓冲区中有足够的样本用于学习。因此，如果我们的内存计数器小于批次大小——意味着我们在内存缓冲区中没有足够的样本——我们将直接返回该值。否则，我们将从内存缓冲区中抽取`state`（状态）、`action`（动作）、`reward`（奖励）、`new_state`（新状态）和`done`（结束）标志。一旦采样，我们必须将所有这些标志转换为张量以进行实现。接下来，我们需要计算目标动作，然后使用目标动作状态和新状态计算新的批评者值。然后，我们计算批评者值，这是我们在当前回放缓冲区中遇到的状态和动作的值。之后，我们计算目标。请注意，在我们将`gamma`与新的批评者值相乘时，如果`done`标志为`0`，结果会变为`0`。这基本上意味着当回合结束时，我们只考虑当前状态的奖励。然后，目标被转换为张量并重塑，以便实现目的。现在，我们可以计算并反向传播我们的批评者损失。接下来，我们对执行者网络做同样的事。最后，我们更新目标执行者和目标批评者网络的参数。
- en: 'Next, define the `update_params` function:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义`update_params`函数：
- en: '[PRE25]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here, the `update_params` function takes a `tau` value, which basically allows
    us to update the target network in very small steps. The value for `tau` is typically
    very small, much smaller than `1`. One thing to note is that we start with `tau`
    equal to `1` but later, the value is reduced to a much smaller number. What the
    function does is that it first gets all the names of the parameters for the critic,
    actor, target critic, and target actor. It then updates those parameters with
    the target critic and target actor. Now, we can create the main part of our Python code.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，`update_params`函数接受一个`tau`值，它基本上允许我们以非常小的步骤更新目标网络。`tau`的值通常非常小，远小于`1`。需要注意的是，我们从`tau`等于`1`开始，但后来将其值减少到一个更小的数字。该函数的作用是首先获取批评者、执行者、目标批评者和目标执行者的所有参数名称。然后，它使用目标批评者和目标执行者更新这些参数。现在，我们可以创建Python代码的主要部分。
- en: 'If you have created the `Agent` class properly, then, along with the preceding
    example code, you''ll be able to initialize our learning agent with the following
    bit of code:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你已经正确创建了`Agent`类，那么，结合前面的示例代码，你可以使用以下代码初始化我们的学习智能体：
- en: '[PRE26]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'For the output, you''ll see the reward for each episode. Here is the output
    for the first 10 episodes:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于输出，你将看到每一回合的奖励。以下是前10回合的输出：
- en: '[PRE27]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: What you see in the preceding output is that the reward oscillates between negative
    and positive. That is because until now, our agent was sampling random actions
    from all the actions it could take.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你在前面的输出中看到的奖励在负数和正数之间波动。这是因为到目前为止，我们的智能体一直在从它可以采取的所有动作中随机选择。
- en: Note
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/37Jwhnq](https://packt.live/37Jwhnq).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问这一部分的源代码，请参考[https://packt.live/37Jwhnq](https://packt.live/37Jwhnq)。
- en: This section does not currently have an online interactive example and will
    need to be run locally.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 本节目前没有在线互动示例，需要在本地运行。
- en: 'In the next activity, we will make the agent remember its past learnings and
    learn from them. Here''s how the game environment will look:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的活动中，我们将让智能体记住它过去的学习内容，并从中学习。以下是游戏环境的样子：
- en: '![Figure 11.8: The output window showing the Lunar Lander hovering'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.8：显示Lunar Lander在游戏环境中悬停的输出窗口'
- en: in the game environment
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏环境中
- en: '](img/B16182_11_08.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_11_08.jpg)'
- en: 'Figure 11.8: The output window showing the Lunar Lander hovering in the game
    environment'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8：显示Lunar Lander在游戏环境中悬停的输出窗口
- en: However, you'll find that the Lander doesn't attempt to land, and rather, it
    hovers over the lunar surface in our game environment. That's because we haven't
    enabled the agent to learn yet. We will do that in the following activity.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你会发现 Lander 并没有尝试着陆，而是悬停在我们的游戏环境中的月球表面。这是因为我们还没有让代理学习。我们将在接下来的活动中做到这一点。
- en: In the next activity, we will create an agent that will help to learn a model
    using DDPG.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个活动中，我们将创建一个代理，帮助学习一个使用 DDPG 的模型。
- en: 'Activity 11.01: Creating an Agent That Learns a Model Using DDPG'
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 11.01：创建一个通过 DDPG 学习模型的代理
- en: In this activity, we will implement what we have learned in this section and
    create an agent that learns through DDPG.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将实现本节所学，并创建一个通过 DDPG 学习的代理。
- en: Note
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We have created a Python file for the actual DDPG implementation to be imported
    as a module using `from ddpg import *`. The module and the code of the activity
    can be downloaded from GitHub at [https://packt.live/2YksdXX](https://packt.live/2YksdXX).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为实际的 DDPG 实现创建了一个 Python 文件，可以通过 `from ddpg import *` 作为模块导入。该模块和活动代码可以从
    GitHub 下载，网址是 [https://packt.live/2YksdXX](https://packt.live/2YksdXX)。
- en: 'The following are the steps to perform for this activity:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是执行此活动的步骤：
- en: Import the necessary libraries (`os`, `gym`, and `ddpg`).
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库（`os`、`gym` 和 `ddpg`）。
- en: First, we create our Gym environment (`LunarLanderContinuous-v2`), as we did
    previously.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们像之前一样创建我们的 Gym 环境（`LunarLanderContinuous-v2`）。
- en: Initialize the agent with some sensible hyperparameters, as in *Exercise 11.02*,
    *Creating a Learning Agent*.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一些合理的超参数初始化代理，参考 *练习 11.02*，*创建学习代理*。
- en: Set up a random seed so that our experiments are reproducible.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置随机种子，以便我们的实验可重复。
- en: Create a blank array to story the scores; you can name it `history`. Iterate
    for at least `1000` episodes and in each episode, set a running score variable
    to `0` and the `done` flag to `False`, then reset the environment. Then, when
    the `done` flag is not `True`, carry out the following step.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个空数组来存储分数；你可以将其命名为 `history`。至少迭代 `1000` 次，每次迭代时，将运行分数变量设置为 `0`，并将 `done`
    标志设置为 `False`，然后重置环境。然后，当 `done` 标志不为 `True` 时，执行以下步骤。
- en: Select an action from the observations and get the new `state`, `reward`, and
    `done` flags. Save the `observation`, `action`, `reward`, `state_new`, and `done`
    flags. Call the `learn` function of the agent and add the current reward to the
    running score. Set the new state as the observation and finally, when the `done`
    flag is `True`, append `score` to `history`.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从观察中选择一个动作，并获取新的 `state`、`reward` 和 `done` 标志。保存 `observation`、`action`、`reward`、`state_new`
    和 `done` 标志。调用代理的 `learn` 函数，并将当前奖励添加到运行分数中。将新的状态设置为观察，最后，当 `done` 标志为 `True`
    时，将 `score` 附加到 `history`。
- en: Note
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To observe the rewards, we can simply add a `print` statement. The rewards will
    be similar to those in the previous exercise.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要观察奖励，我们只需添加一个 `print` 语句。奖励值将类似于前一个练习中的奖励。
- en: 'The following is the expected simulation output:'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是预期的仿真输出：
- en: '![Figure 11.9: Screenshots from the environment after 1,000 rounds of training'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 11.9：训练 1000 轮后的环境截图'
- en: '](img/B16182_11_09.jpg)'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_11_09.jpg)'
- en: 'Figure 11.9: Screenshots from the environment after 1,000 rounds of training'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9：训练 1000 轮后的环境截图
- en: Note
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 766.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第 766 页找到。
- en: In the next section, we will see how we can improve the policy gradient approach
    that we just implemented.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何改善刚刚实现的策略梯度方法。
- en: Improving Policy Gradients
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改善策略梯度
- en: In this section, we will learn the various approaches that will help us improve
    the policy gradient approach that we learned about in the previous section. We
    will learn about techniques such as TRPO and PPO.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习一些有助于改善我们在前一节中学习的策略梯度方法的各种方法。我们将学习诸如 TRPO 和 PPO 等技术。
- en: We will also learn about the A2C technique in brief. Let's understand the TRPO
    optimization technique in the next section.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将简要了解 A2C 技术。让我们在下一节中理解 TRPO 优化技术。
- en: Trust Region Policy Optimization
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信任域策略优化
- en: In most cases, RL is very sensitive to the initialization of weights. Take,
    for instance, the learning rate. If our learning rate is too high, then it may
    so happen that our policy update takes our policy network to a region of the parameter
    space where the next batch of data it collects is gathered against a very poor
    policy. This might cause our network to never recover again. Now, we will talk
    about newer methods that try to get rid of this problem. But before we do that,
    let's have a quick recap of what we have already covered.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，强化学习对权重初始化非常敏感。例如，学习率。如果我们的学习率太高，那么可能发生的情况是，我们的策略更新将我们的策略网络推向参数空间的一个区域，在该区域中，下一个批次收集到的数据会遇到一个非常差的策略。这可能导致我们的网络再也无法恢复。现在，我们将讨论一些新方法，这些方法试图消除这个问题。但在此之前，让我们快速回顾一下我们已经涵盖的内容。
- en: In the *Policy Gradients* section, we defined the estimator of the advantage
    function, ![b](img/B16182_11_9a.png) , as the difference between the discounted
    reward and the baseline estimate. Intuitively, the advantage estimator quantifies
    how good the action taken by our agent in a certain state was compared to what
    would typically happen in that state. One problem with the advantage function
    is that if we simply keep on updating our weights based on one batch of samples
    using gradient descent, then our parameter updates might stray far from the range
    where the data was sampled from. That could lead to an inaccurate estimate of
    the advantage function. In short, if we keep running gradient descent on a single
    batch of experiences, we might corrupt our policy.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *策略梯度* 部分，我们定义了优势函数的估计器，![b](img/B16182_11_9a.png)，作为折扣奖励与基线估计之间的差异。从直观上讲，优势估计器量化了在某一状态下，代理所采取的行动相较于该状态下通常发生的情况有多好。优势函数的一个问题是，如果我们仅仅根据一批样本使用梯度下降不断更新权重，那么我们的参数更新可能会偏离数据采样的范围。这可能导致优势函数的估计不准确。简而言之，如果我们持续在一批经验上运行梯度下降，我们可能会破坏我们的策略。
- en: One way to make sure this problem doesn't occur is to ensure that the updated
    policy doesn't differ too much from the old policy. This is basically the main
    crux of TRPO.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 确保这一问题不发生的一种方法是确保更新后的策略与旧策略差异不大。这基本上是 TRPO 的核心要点。
- en: 'We already understand how the gradient estimator works for vanilla policy gradients:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经理解了普通策略梯度的梯度估计器是如何工作的：
- en: '![Figure 11.10: The vanilla policy gradient method'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.10：普通策略梯度方法'
- en: '](img/B16182_11_10.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_11_10.jpg)'
- en: 'Figure 11.10: The vanilla policy gradient method'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10：普通策略梯度方法
- en: 'Here''s how it looks for TRPO:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 TRPO 的效果：
- en: '![Figure 11.11: Mathematical representation of TRPO'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.11：TRPO 的数学表示'
- en: '](img/B16182_11_11.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_11_11.jpg)'
- en: 'Figure 11.11: Mathematical representation of TRPO'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.11：TRPO 的数学表示
- en: The only change here is that the log operator in the preceding equation has
    been replaced by a division by ![b](img/B16182_11_11a.png). This is known as the
    TRPO objective and optimizing it yields the same result as the vanilla policy
    gradients. In order to ensure that the new and updated policy doesn't differ much
    from the old policy, TRPO introduces a constraint known as the KL constraint.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这里唯一的变化是，前面公式中的对数运算符被除以 ![b](img/B16182_11_11a.png) 代替。这就是所谓的 TRPO 目标，优化它将得到与普通策略梯度相同的结果。为了确保新更新的策略与旧策略差异不大，TRPO
    引入了一个叫做 KL 约束的限制。
- en: This constraint, in simple words, makes sure that our new policy doesn't stray
    too far from the old one. Note that the actual TRPO strategy, however, proposes
    a penalty instead of a constraint.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 用简单的话来说，这个约束确保我们的新策略不会偏离旧策略太远。需要注意的是，实际的 TRPO 策略提出的是一个惩罚，而不是一个约束。
- en: Proximal Policy Optimization
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 近端策略优化（PPO）
- en: It might seem like everything is fine and good for TRPO, but the introduction
    of the KL constraint introduces an additional operating cost to our policy. To
    address that problem, and to basically solve the problems with vanilla policy
    gradients once and for all, the researchers at OpenAI have introduced PPO, which
    we will look into now.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 TRPO，看起来一切都很好，但引入 KL 约束会为我们的策略增加额外的操作成本。为了解决这个问题，并基本上一次性解决普通策略梯度的问题，OpenAI
    的研究人员引入了 PPO，我们现在来探讨这个方法。
- en: 'The main motivations behind PPO are as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: PPO 背后的主要动机如下：
- en: Ease of implementation
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现的便捷性
- en: Ease of parameter tuning
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数调优的便捷性
- en: Efficient sampling
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效的采样
- en: One thing to note is that the PPO method doesn't use a replay buffer to store
    past experiences and learns straight from whatever the agent encounters in the
    environment. This is also known as an `online` method of learning while the former—using
    a replay buffer to store past experiences—is known as an `offline` method of learning.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，PPO方法不使用重放缓冲区来存储过去的经验，而是直接从代理在环境中遇到的情况中进行学习。这也被称为`在线`学习方法，而前者——使用重放缓冲区存储过去的经验——被称为`离线`学习方法。
- en: 'The authors of PPO define a probability ratio, ![a](img/B16182_11_11b.png),
    which is basically the probability ratio between the new and the old policy. So,
    we have the following:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: PPO的作者定义了一个概率比率，![a](img/B16182_11_11b.png)，它基本上是新旧策略之间的概率比率。因此，我们得到以下内容：
- en: '![Figure 11.12: The probability ratio between the old and new policy'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.12：旧策略和新策略之间的概率比率'
- en: '](img/B16182_11_12.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_11_12.jpg)'
- en: 'Figure 11.12: The probability ratio between the old and new policy'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.12：旧策略和新策略之间的概率比率
- en: 'When provided with a sampled batch of actions and states, this ratio would
    be greater than `1` if the action is more likely now than in the old policy. Otherwise,
    it would remain between `0` and `1`. Now, the final objective of PPO when written
    down looks like this:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 当提供一批采样的动作和状态时，如果该动作在当前策略下比在旧策略下更有可能，那么这个比率将大于`1`。否则，它将保持在`0`和`1`之间。现在，PPO的最终目标写下来是这样的：
- en: '![Figure 11.13: The final objective of PPO'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.13：PPO的最终目标'
- en: '](img/B16182_11_13.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_11_13.jpg)'
- en: 'Figure 11.13: The final objective of PPO'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.13：PPO的最终目标
- en: Let's explain. Like the vanilla policy gradient, PPO tries to optimize the expectation
    and so we compute this expectation operator over batches of trajectories. Now,
    this is a minimum value of the modified policy gradient objective that we saw
    in TRPO and the second part is a clipped version of it. The clipping operation
    keeps the policy gradient objective between ![a](img/B16182_11_13a.png) and ![b](img/B16182_11_13b.png).
    ![c](img/B16182_11_13c.png) here is a hyperparameter, often equal to `0.2`.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解释一下。像传统的策略梯度一样，PPO试图优化期望，因此我们对一批轨迹计算期望算子。现在，这是我们在TRPO中看到的修改后的策略梯度目标的最小值，第二部分是它的截断版本。截断操作保持策略梯度目标在![a](img/B16182_11_13a.png)和![b](img/B16182_11_13b.png)之间。这里的![c](img/B16182_11_13c.png)是一个超参数，通常等于`0.2`。
- en: 'Although the function looks simple at a glance, its ingenuity is remarkable.
    ![d](img/B16182_11_13d.png) can be both negative and positive, suggesting a negative
    advantage estimation and a positive advantage estimation, respectively. This behavior
    of the advantage estimator determines how the `min` operator works. Here''s an
    illustration of the `clip` parameter from the actual PPO paper:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个函数乍一看很简单，但它的巧妙之处非常显著。![d](img/B16182_11_13d.png)可以是负数也可以是正数，分别表示负优势估计和正优势估计。这种优势估计器的行为决定了`min`操作符的工作方式。以下是实际PPO论文中`clip`参数的插图：
- en: '![Figure 11.14: The clip parameter illustration'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.14：clip参数插图'
- en: '](img/B16182_11_14.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_11_14.jpg)'
- en: 'Figure 11.14: The clip parameter illustration'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14：clip参数插图
- en: The plot on the left is where the advantage is positive. This means our actions
    yielded results that were better than expected. On the other plot on the right
    are the cases where our actions yielded less than the expected return.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的图表示优势为正。这意味着我们的动作产生的结果好于预期。右侧的图表示我们的动作产生的结果低于预期回报。
- en: Notice how on the plot on the left the loss flattens out when `r` is too high.
    This might occur when the current action is much more plausible under the current
    policy than the old one. In this case, the objective function is clipped here,
    thereby ensuring the gradient update doesn't go beyond a certain limit.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在左侧的图中，当`r`过高时，损失趋于平坦。这可能发生在当前策略下的动作比旧策略下的动作更有可能时。在这种情况下，目标函数会在这里被截断，从而确保梯度更新不会超过某个限制。
- en: On the other hand, when the objective function is negative, the loss flattens
    when `r` is approaching `0`. This relates to actions that are more unlikely under
    the current policy than the old one. It may be clear by now how the clipping operation
    keeps the updates to the network parameters within a desirable range. It would
    be a better approach to learn about the PPO technique while implementing it. So,
    let's start with an exercise to reduce the operating cost of our policy using
    PPO.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，当目标函数为负时，当 `r` 接近 `0` 时，损失会趋于平缓。这与在当前策略下比旧策略更不可能执行的动作相关。现在你可能已经能理解截断操作如何将网络参数的更新保持在一个理想的范围内。通过实现
    PPO 技术来学习它会是一个更好的方法。所以，让我们从一个练习开始，使用 PPO 来降低我们策略的操作成本。
- en: 'Exercise 11.03: Improving the Lunar Lander Example Using PPO'
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 11.03：使用 PPO 改进月球着陆器示例
- en: 'In this exercise, we''ll implement the Lunar Lander example using PPO. We will
    follow almost the same structure as before and you''ll be able to follow through
    this exercise easily if you have gone through the previous exercises and examples:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用 PPO 实现月球着陆器示例。我们将遵循几乎与之前相同的结构，如果你已经完成了之前的练习和示例，你将能够轻松跟进这个练习：
- en: 'Open a new Jupyter Notebook and import the necessary libraries (`gym`, `torch`,
    and `numpy`):'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter Notebook，并导入必要的库（`gym`、`torch` 和 `numpy`）：
- en: '[PRE28]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Set our device as we did in the DDPG example:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照在 DDPG 示例中的做法设置我们的设备：
- en: '[PRE29]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we will create the `ReplayBuffer` class. Here, we will create arrays
    to store the actions, states, log probabilities, reward, and terminal states:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建 `ReplayBuffer` 类。在这里，我们将创建数组来存储动作、状态、对数概率、奖励和终止状态：
- en: '[PRE30]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, we will define our `ActorCritic` class. We will define our `action` and
    `value` layers first:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将定义我们的 `ActorCritic` 类。我们将首先定义 `action` 和 `value` 层：
- en: '[PRE31]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, we will define methods to sample from the action space and evaluate the
    log probabilities of the actions, state value, and entropy of the distribution:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将定义方法，从动作空间中采样并评估动作的对数概率、状态值和分布的熵：
- en: '[PRE32]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, the `ActorCritic` class looks like this:'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，`ActorCritic` 类看起来像这样：
- en: '[PRE33]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, we will define our `Agent` class using the `__init__()` and `update()`
    functions. First let''s define `__init__()` function:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用 `__init__()` 和 `update()` 函数定义我们的 `Agent` 类。首先让我们定义 `__init__()` 函数：
- en: '[PRE34]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now let''s define the `update` function:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们定义 `update` 函数：
- en: '[PRE35]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, normalize the rewards and convert them to tensors:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，规范化奖励并将其转换为张量：
- en: '[PRE36]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, find the probability ratio, find the loss and propagate our loss backwards:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，找到概率比，找到损失并向后传播我们的损失：
- en: '[PRE37]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Update the old policy with the new weights:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新权重更新旧策略：
- en: '[PRE38]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: So, here in *steps 6-10* of this exercise we are defining an agent by starting
    with the initialization of our policy, the optimizer, and the old policy. Then,
    in the `update` function, we are at first taking the Monte Carlo estimate of the
    state rewards. After normalizing the rewards, we are converting them into tensors.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所以，在本练习的*步骤 6-10*中，我们通过初始化我们的策略、优化器和旧策略来定义一个智能体。然后，在 `update` 函数中，我们首先采用状态奖励的蒙特卡洛估计。奖励规范化后，我们将其转换为张量。
- en: Then, we are carrying out the policy optimization for `K_epochs`. Here, we have
    to find the probability ratio, ![a](img/B16182_11_14a.png), which is the probability
    ratio between the new and the old policy, as described previously.
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们进行 `K_epochs` 次策略优化。在这里，我们需要找到概率比，![a](img/B16182_11_14a.png)，即新策略和旧策略之间的概率比，如前所述。
- en: After that, we are finding the loss, ![b](img/B16182_11_14b.png), and propagating
    our loss backward. Finally, we are updating our old policy with the new weights.
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 之后，我们找到损失，![b](img/B16182_11_14b.png)，并向后传播我们的损失。最后，我们使用新权重更新旧策略。
- en: 'Now, we can run the simulation as we did in the previous exercise and save
    the policy for future use:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以像在上一个练习中一样运行模拟，并保存策略以供将来使用：
- en: '[PRE39]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The following is the first 10 lines of the output:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出的前 10 行：
- en: '[PRE40]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note that we are saving our policy at certain intervals. This is useful if you
    want to load the policy at a later stage and simply run the simulation from there.
    The simulation output of this exercise would be the same as in *Figure 11.9*,
    only the operating cost is reduced here.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们会在一定间隔保存我们的策略。如果你想稍后加载该策略并从那里运行模拟，这会非常有用。此练习的模拟输出将与*图 11.9*相同，唯一不同的是这里操作成本已被降低。
- en: Here, if you look at the difference between the rewards, the points given in
    each consequent episode are much less as we have used the PPO technique. That
    means the learning is not going haywire as it was in *Exercise 11.01*, *Landing
    a Spacecraft on the Lunar Surface Using Policy Gradients and the Actor-Critic
    Method*, where the difference between the rewards was higher.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，如果你观察奖励之间的差异，由于我们使用了PPO技术，每个连续回合中给予的积分要小得多。这意味着学习并没有像在*练习 11.01*、*使用策略梯度和演员-评论员方法将航天器降落在月球表面*中那样失控，因为在那个例子中，奖励之间的差异更大。
- en: Note
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: To access the source code for this specific section, please refer to [https://packt.live/2zM1Z6Z](https://packt.live/2zM1Z6Z).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本节的源代码，请参考[https://packt.live/2zM1Z6Z](https://packt.live/2zM1Z6Z)。
- en: This section does not currently have an online interactive example and will
    need to be run locally.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 本节目前没有在线互动示例，需要在本地运行。
- en: We have almost covered all the important topics relating to policy-based RL.
    So, now we will talk about the last topic, which is the A2C method.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎涵盖了与基于策略的强化学习相关的所有重要主题。所以，接下来我们将讨论最后一个话题——A2C方法。
- en: The Advantage Actor-Critic Method
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优势演员-评论员方法
- en: We have already learned about the actor-critic method and the reason for using
    it in the introduction, and we have also seen it used in our coding examples.
    But, a quick recap—actor-critic methods lie at the intersection of the value-based
    and policy-based methods, where we simultaneously update our policy and our value,
    which acts as a judge quantifying how good our policy actually is.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在介绍中学习了演员-评论员方法及其使用原因，并且在我们的编码示例中也见过它的应用。简单回顾一下——演员-评论员方法位于基于价值和基于策略方法的交集处，我们同时更新我们的策略和我们的价值，后者充当着衡量我们策略实际效果的评判标准。
- en: 'Next, we will learn how A2C works:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习A2C是如何工作的：
- en: We start by initializing the policy parameter, ![b](img/B16182_11_14c.png),
    with random weights.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先通过随机权重初始化策略参数，![b](img/B16182_11_14c.png)。
- en: Next, we play *N* number of steps with the current policy, ![a](img/B16182_11_14d.png)
    , and store the state, action, reward, and transitions.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用当前策略进行*N*步的操作，![a](img/B16182_11_14d.png)，并存储状态、动作、奖励和转移。
- en: We set our reward to `0` if we reach the final episode of the state; otherwise,
    we set the reward to the value of the current state.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们到达状态的最终回合，我们将奖励设置为`0`；否则，我们将奖励设置为当前状态的值。
- en: Then, we calculate the discounted reward, policy loss, and value loss by looping
    backward from the final episode.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们通过从最后一个回合反向循环，计算折扣奖励、策略损失和价值损失。
- en: Finally, we apply **Stochastic Gradient Descent** (**SGD**) using the mean policy
    and value loss for each batch.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们应用**随机梯度下降**（**SGD**），使用每个批次的平均策略和值损失。
- en: Repeat the steps from *step 2* onward until it reaches convergence.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从*步骤 2*开始，重复执行直到达到收敛。
- en: Note
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注
- en: We have only briefly introduced A2C here. A detailed description and implementation
    of this method is beyond the scope of this book.
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在这里仅简要介绍了A2C。该方法的详细描述和实现超出了本书的范围。
- en: The first coding example (*Exercise 11.01*, *Landing a Spacecraft on the Lunar
    Surface Using Policy Gradients and the Actor-Critic Method*) that we covered follows
    the basic A2C method. However, there's another technique, called the **Asynchronous
    Advantage Actor-Critic** (**A3C**) method. Remember, our policy gradient methods
    work online. That is, we only train on the data obtained using the current policy
    and we do not keep track of past experiences. However, to keep our data independent
    and identically distributed, we need a large buffer of transitions. The solution
    provided by A3C is to run multiple training environments in parallel to acquire
    large amounts of training data. With multiprocessing in Python, this actually
    becomes very fast in practice.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的第一个编码示例（*练习 11.01*、*使用策略梯度和演员-评论员方法将航天器降落在月球表面*）遵循了基本的A2C方法。然而，还有另一种技术，叫做**异步优势演员-评论员**（**A3C**）方法。记住，我们的策略梯度方法是在线工作的。也就是说，我们只在使用当前策略获得的数据上进行训练，并且不跟踪过去的经验。然而，为了保持我们的数据是独立同分布的，我们需要一个大的转移缓冲区。A3C提供的解决方案是并行运行多个训练环境，以获取大量训练数据。借助Python中的多进程，这实际上在实践中非常快速。
- en: 'In the next activity, we will write code to run the Lunar Lander simulation
    that we learned about in *Exercise 11.03*, *Improving the Lunar Lander Example
    Using PPO*. We will also render the environment to see the Lunar Lander. To do
    that, we will have to import the PIL library. The code to render the image is
    as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个活动中，我们将编写代码来运行我们在 *练习 11.03* 中学习的月球着陆器仿真，*使用 PPO 改进月球着陆器示例*。我们还将渲染环境以查看月球着陆器。为此，我们需要导入
    PIL 库。渲染图像的代码如下：
- en: '[PRE41]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Let's begin with the implementation of our final activity.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最后一个活动的实现开始。
- en: 'Activity 11.02: Loading the Saved Policy to Run the Lunar Lander Simulation'
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 11.02：加载已保存的策略以运行月球着陆器仿真
- en: In this activity, we will combine multiple aspects of RL that we have explained
    in previous sections. We will use what we learned in *Exercise 11.03*, *Improving
    the Lunar Lander Example Using PPO*, to write simple code to load the saved policy.
    This activity combines all the essential components of building a working RL prototype—in
    our case, the Lunar Lander simulation.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将结合之前章节中解释的 RL 多个方面。我们将利用在 *练习 11.03* 中学到的知识，*使用 PPO 改进月球着陆器示例*，编写简单代码来加载已保存的策略。这个活动结合了构建工作
    RL 原型的所有基本组件——在我们的案例中，就是月球着陆器仿真。
- en: 'The steps to take are as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 需要执行的步骤如下：
- en: Open Jupyter and in a new notebook, import the essential Python libraries, including
    the PIL library to save the image.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 Jupyter，并在新笔记本中导入必要的 Python 库，包括 PIL 库来保存图像。
- en: Set your device using the device parameter.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `device` 参数设置你的设备。
- en: Define the `ReplayBuffer`, `ActorCritic`, and `Agent` classes. We already defined
    these in the previous exercise.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `ReplayBuffer`、`ActorCritic` 和 `Agent` 类。我们已经在前面的练习中定义过这些类。
- en: Create the Lunar Lander environment. Initialize the random seed.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建月球着陆器环境。初始化随机种子。
- en: Create the memory buffer and initialize the agent with hyperparameters, as in
    the previous exercise.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建内存缓冲区并初始化代理与超参数，就像在前一个练习中一样。
- en: Load the saved policy as an old policy.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将已保存的策略加载为旧策略。
- en: Finally, loop through your desired number of episodes. In every iteration, start
    by initializing the episode reward as `0`. Do not forget to reset the state. Run
    another loop, specifying the `max` timestamp. Get the `state`, `reward`, and `done`
    flags for each action taken and add the reward to the episode reward.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，循环遍历你希望的回合数。在每次迭代开始时，将回合奖励初始化为 `0`。不要忘记重置状态。再执行一个循环，指定 `max` 时间戳。获取每个动作所采取的
    `state`、`reward` 和 `done` 标志，并将奖励加入回合奖励中。
- en: Render the environment to see how your Lunar Lander is doing.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 渲染环境以查看你的月球着陆器运行情况。
- en: 'The following is the expected output:'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是预期的输出：
- en: '[PRE42]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The following screenshot shows the simulation output of some of the stages:'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下屏幕截图展示了某些阶段的仿真输出：
- en: '![Figure 11.15: The environment showing the simulation of the Lunar Lander'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 11.15：展示月球着陆器仿真环境](img/B16182_11_15.jpg)'
- en: '](img/B16182_11_15.jpg)'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_11_15.jpg)'
- en: 'Figure 11.15: The environment showing the simulation of the Lunar Lander'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.15：展示月球着陆器仿真环境
- en: The complete simulation output can be found in the form of images at [https://packt.live/3ehPaAj](https://packt.live/3ehPaAj).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的仿真输出可以在 [https://packt.live/3ehPaAj](https://packt.live/3ehPaAj) 以图像形式找到。
- en: Note
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 769\.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第 769 页找到。
- en: Summary
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about policy-based methods, principally the drawbacks
    to value-based methods such as Q-learning, which motivate the use of policy gradients.
    We discussed the purposes of policy-based methods of RL, along with the trade-offs
    of other RL approaches.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了基于策略的方法，主要是值方法（如 Q 学习）的缺点，这些缺点促使了策略梯度的使用。我们讨论了 RL 中基于策略的方法的目的，以及其他
    RL 方法的权衡。
- en: You learned about the policy gradients that help a model to learn in a real-time
    environment. Next, we learned how to implement the DDPG using the actor-critic
    model, the `ReplayBuffer` class, and Ornstein–Uhlenbeck noise to understand the
    continuous action space. We also learned how you can improve policy gradients
    by using techniques such as TRPO and PPO. Finally, we talked in brief about the
    A2C method, which is an advanced version of the actor-critic model.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解了帮助模型在实时环境中学习的策略梯度。接下来，我们学习了如何使用演员-评论员模型、`ReplayBuffer` 类以及奥恩斯坦-乌伦贝克噪声实现DDPG，以理解连续动作空间。我们还学习了如何通过使用
    TRPO 和 PPO 等技术来改进策略梯度。最后，我们简要讨论了 A2C 方法，它是演员-评论员模型的一个高级版本。
- en: Also, in this chapter, we played around with the Lunar Lander environment in
    OpenAI Gym—for both continuous and discrete action spaces—and coded the multiple
    policy-based RL approaches that we discussed.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在本章中，我们还在 OpenAI Gym 中的月球着陆器环境中进行了一些实验——包括连续和离散动作空间——并编写了我们讨论过的多种基于策略的强化学习方法。
- en: In the next chapter, we will learn about a gradient-free method to optimize
    neural networks and RL-based algorithms. We will then discuss the limitations
    of gradient-based methods. The chapter presents an alternative optimization solution
    to gradient methods through genetic algorithms as they ensure global optimum convergence.
    We will also learn about the hybrid neural networks that use genetic algorithms
    to solve complex problems.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习一种无梯度的优化方法，用于优化神经网络和基于强化学习的算法。随后，我们将讨论基于梯度的方法的局限性。本章通过遗传算法提出了一种替代梯度方法的优化方案，因为它们能够确保全局最优收敛。我们还将学习使用遗传算法解决复杂问题的混合神经网络。
