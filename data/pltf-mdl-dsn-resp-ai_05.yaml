- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: ML Pipeline, Model Evaluation, and Handling Uncertainty
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ML 流水线、模型评估与不确定性处理
- en: This chapter starts with the introduction of the AI/ML workflow. The chapter
    then delves into different ML algorithms used for classification, regression,
    generation, and reinforcement learning. The chapter also discusses issues related
    to the reliability and trustworthiness of these algorithms. We start with an introduction
    to the various components of an ML pipeline and explain the need for continuous
    training. The chapter then briefly explores the important AI/ML algorithms for
    the tasks of classification, regression, and clustering. Further, we discuss approaches
    for identifying bias in learning algorithms and causes of uncertainty in model
    prediction.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章从介绍 AI/ML 工作流开始。随后，本章深入探讨用于分类、回归、生成和强化学习的不同 ML 算法。还讨论了与这些算法的可靠性和可信度相关的问题。我们首先介绍
    ML 流水线的各个组件，并解释了持续训练的必要性。接着，本章简要探讨了分类、回归和聚类任务中重要的 AI/ML 算法。进一步地，我们讨论了识别学习算法中的偏差和模型预测不确定性的原因。
- en: 'In this chapter, these topics will be covered in the following sections:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将通过以下几个部分来覆盖这些主题：
- en: Understanding different components of ML pipelines
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 ML 流水线的不同组件
- en: ML tasks and algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML 任务和算法
- en: Causes of uncertainty in model prediction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型预测中的不确定性原因
- en: Uncertainty in classification algorithms
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类算法中的不确定性
- en: Uncertainty in regression algorithms
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归算法中的不确定性
- en: Further, we will write Python code to quantify both aleatoric and epistemic
    uncertainty for regression tasks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步，我们将编写 Python 代码来量化回归任务中的 aleatoric 和 epistemic 不确定性。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter requires you to have Python 3.8, along with some necessary Python
    packages:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求您使用 Python 3.8，并安装一些必要的 Python 包：
- en: '`TensorFlow 2.7.0`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorFlow 2.7.0`'
- en: '`NumPy`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NumPy`'
- en: '`matplotlib`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib`'
- en: 'Keras-Uncertainty – to install it, use the following:'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras-Uncertainty – 安装方法如下：
- en: '[PRE0]'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Understanding different components of ML pipelines
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解 ML 流水线的不同组件
- en: As any data scientist knows, ML is only as good as the data it’s trained on.
    In the real world, data is messy and complicated. Building a successful ML system,
    therefore, requires more than just building algorithms. It also requires a vast
    and complex infrastructure to support it. This includes everything from collecting
    and cleansing data to deploying and monitoring models. The problem is further
    complicated by the fact that changing anything in the system can have ripple effects
    throughout. A minor tweak in hyperparameters, for example, can require changes
    to the way data is collected and processed. As a result, building a successful
    ML system is an immensely complex undertaking.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如任何数据科学家所知道的，ML 的效果取决于它所训练的数据。在现实世界中，数据是混乱和复杂的。因此，构建一个成功的 ML 系统不仅仅是构建算法。它还需要一个庞大且复杂的基础设施来支持。这包括从收集和清洗数据到部署和监控模型的各个方面。问题更加复杂的是，系统中的任何变动都可能引发连锁反应。例如，超参数的轻微调整可能需要改变数据的收集和处理方式。因此，构建一个成功的
    ML 系统是一项极为复杂的任务。
- en: '![Figure 5.1 – ML workflow](img/Figure_5.1_B18681.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – ML 工作流](img/Figure_5.1_B18681.jpg)'
- en: Figure 5.1 – ML workflow
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – ML 工作流
- en: 'We can divide the ML workflow into five main components:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 ML 工作流分为五个主要组件：
- en: '**Data Extraction**: This is the phase where data needed for training the model
    is curated. It is the process of identifying relevant information from a larger
    dataset and converting it into a format that can be used for further analysis.
    This can be done manually, but it is more commonly done using automated algorithms.
    Data extraction can be used to extract contact information from a list of emails,
    to identify trends in stock prices, or to find new potential customers. The data
    can be in different forms (tabular data, image files, text, and so on). The data
    is decided on by the business needs of the specific use case. For example, if
    your use case is AI for retail, you will have data such as product pictures and
    tags. The process of data extraction typically begins with pre-processing, which
    involves identifying the relevant information from the dataset. Once the relevant
    information has been identified, it is then converted into a format that can be
    used by the ML algorithm. It allows the algorithm to focus on the most relevant
    information and ignore irrelevant data. This can improve the accuracy of the ML
    algorithm and help to speed up the training process.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据提取**：这是为训练模型收集所需数据的阶段。它是从一个更大的数据集中识别相关信息并将其转换为可用于进一步分析的格式的过程。这可以手动完成，但通常使用自动化算法来完成。数据提取可以用来从电子邮件列表中提取联系信息，识别股票价格趋势，或发现潜在的新客户。数据可以以不同形式存在（表格数据、图像文件、文本等）。数据的选择由具体用例的业务需求决定。例如，如果你的用例是零售领域的人工智能，你将拥有诸如产品图片和标签等数据。数据提取的过程通常从预处理开始，预处理涉及从数据集中识别相关信息。一旦相关信息被识别出来，它就会被转换为机器学习算法可以使用的格式。这使得算法能够专注于最相关的信息并忽略无关数据，从而提高机器学习算法的准确性，并帮助加快训练过程。'
- en: '**Data Engineering**: In this phase, the data is analyzed, cleaned, explored,
    and pre-processed to be fed to the model. The phase may involve combining one
    or more features and/or dimensionality reduction of the data for computational
    efficiency. The goal of data engineering is to create a dataset that is both accurate
    and representative of the real world in a reproducible way.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据工程**：在这一阶段，数据会被分析、清洗、探索，并预处理后传入模型。该阶段可能涉及将一个或多个特征合并和/或对数据进行降维处理，以提高计算效率。数据工程的目标是创建一个既准确又能代表现实世界的、可重复的数据集。'
- en: '**Model Training**: This is an iterative phase, where model experiments are
    performed. It involves selecting the best model, performing hyperparameter tuning,
    and validating the trained model using desired metrics. Finding the right set
    of hyperparameters is an art and, in earlier days of ML, only some experts were
    able to do it effectively. As a result, many used to call ML “alchemy.” However,
    today, with the availability of robust and reliable optimizers and AutoML features,
    we can generate reliable and reproducible results. It is worth mentioning, though,
    that this step is time- and compute-intensive.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型训练**：这是一个迭代过程，其中进行模型实验。它包括选择最佳模型、进行超参数调优，并使用所需的指标验证训练过的模型。找到一组合适的超参数是一门艺术，在早期的机器学习发展中，只有一些专家能够有效地做到这一点。因此，许多人曾称机器学习为“炼金术”。然而，今天，借助强大可靠的优化器和
    AutoML 功能，我们能够生成可靠且可重复的结果。值得一提的是，这一步骤是时间密集型和计算密集型的。'
- en: '**Model Deployment**: The final model is deployed to production. It is important
    to monitor the model’s performance and for any degradation in the performance
    to be reported and acted upon. The deployed model needs to be monitored for both
    *data drift* and *concept drift*. We will talk about these concepts in detail
    in [*Chapter 6*](B18681_06.xhtml#_idTextAnchor126).'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型部署**：最终模型部署到生产环境。监控模型的性能非常重要，一旦性能下降，应报告并采取相应的措施。已部署的模型需要监控 *数据漂移* 和 *概念漂移*。我们将在
    [*第6章*](B18681_06.xhtml#_idTextAnchor126) 中详细讨论这些概念。'
- en: '**User Interface**: Once the model is deployed, it is ready for consumption.
    It will take user input and perform predictions, which can be shared via the visual
    dashboards.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户界面**：一旦模型部署完成，它就可以被使用。它将接收用户输入并进行预测，结果可以通过可视化仪表板分享出去。'
- en: Depending upon the level of expertise and need, different components of the
    ML pipeline can be done manually, automated as MLOps (for details, refer to [*Chapter
    6*](B18681_06.xhtml#_idTextAnchor126)), or fully automated as CI/CD processes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 根据专业水平和需求，机器学习管道的不同组件可以手动完成、作为 MLOps 自动化（详细信息请参考 [*第6章*](B18681_06.xhtml#_idTextAnchor126)），或完全自动化为
    CI/CD 流程。
- en: ML tasks and algorithms
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习任务和算法
- en: 'In general, ML tasks can be categorized into four categories: classification,
    regression, clustering, and dimensionality reduction (*Figure 5**.2*).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，机器学习任务可以分为四类：分类、回归、聚类和降维（*图 5.2*）。
- en: '![Figure 5.2 – Different ML tasks and popular algorithms](img/Figure_5.2_B18681.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 不同的机器学习任务和常见算法](img/Figure_5.2_B18681.jpg)'
- en: Figure 5.2 – Different ML tasks and popular algorithms
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 不同的机器学习任务和常见算法
- en: 'For each of them, there exist many standard algorithms. For the sake of completeness,
    we will list ML tasks and some common algorithms for them:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个任务，都存在许多标准算法。为了完整性，我们将列出机器学习任务及其常见算法：
- en: '**Classification**: When data needs to be divided into different categories
    or classes, we use classification algorithms. This is a supervised learning problem
    where the aim is to predict the discrete class label of new examples, based on
    training data. It is one of the most commonly used techniques in ML, and has a
    wide range of applications, from identifying spam emails to facial recognition.
    The basic idea behind classification is to build a model that can learn the relationship
    between the input data and the class labels. This model can then be used to make
    predictions on new data points. There are a variety of different algorithms that
    can be used for classification, each with its own strengths and weaknesses. The
    choice of algorithm will largely depend on the nature of the data and the required
    accuracy of the predictions.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：当数据需要被分为不同的类别或类时，我们使用分类算法。这是一个监督学习问题，目标是根据训练数据预测新样本的离散类别标签。分类是机器学习中最常用的技术之一，应用广泛，从识别垃圾邮件到人脸识别。分类的基本思想是构建一个模型，能够学习输入数据与类别标签之间的关系。然后，这个模型可以用于对新数据点进行预测。对于分类任务，有多种不同的算法可以选择，每种算法都有其优缺点。算法的选择主要取决于数据的性质和预测精度的要求。'
- en: '**Regression**: In ML, regression is a technique used to predict continuous
    values, such as prices or weights. It assumes that there is a linear relationship
    between the predictor variables and the response variable. In other words, as
    the predictor variables increase, the response variable will also increase (or
    decrease) by a constant amount. Regression models can be used to identify which
    predictors are most important for determining the value of the response variable.
    They can also be used to estimate uncertainty in the predictions. Despite its
    simplicity, regression is a powerful tool that can be used to solve many real-world
    problems. Regression, like classification, is a supervised learning problem.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：在机器学习中，回归是一种用于预测连续值的技术，比如价格或重量。它假设预测变量和响应变量之间存在线性关系。换句话说，随着预测变量的增加，响应变量也会按固定量增加（或减少）。回归模型可用于识别哪些预测因子对确定响应变量的值最为重要，也可用于估计预测中的不确定性。尽管回归方法简单，但它是一个强大的工具，能够解决许多实际问题。回归问题与分类问题一样，是一种监督学习问题。'
- en: '**Clustering**: Clustering is a ML technique that groups similar data points
    together. It is used to segment customers, classify documents, and identify patterns
    in data. There are a variety of clustering algorithms, but they all operate on
    the same principle: they partition data into groups, called clusters, by finding
    similarities between data points. Clustering is an unsupervised learning technique,
    which means that it does not require labeled data. This makes it particularly
    well-suited for exploratory data analysis. Clustering can be used to find structure
    in data, generate hypotheses about how the data is organized, and identify outliers.
    By grouping together similar data points, clustering can simplify complex datasets
    and make them easier to work with. Clustering is an essential tool for ML and
    data science. It can be used alone or in combination with other techniques to
    solve a variety of problems. Many clustering algorithms, such as k-means, are
    based on an unsupervised learning paradigm.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：聚类是机器学习中的一种技术，它将相似的数据点分组在一起。它被用于细分客户、分类文档和识别数据中的模式。有许多不同的聚类算法，但它们都遵循相同的原理：通过找到数据点之间的相似性，将数据划分为多个组，称为簇。聚类是一种无监督学习技术，这意味着它不需要标记数据。这使得聚类特别适合用于探索性数据分析。聚类可以用来在数据中寻找结构，生成关于数据如何组织的假设，并识别异常值。通过将相似的数据点聚集在一起，聚类可以简化复杂的数据集，使其更易于处理。聚类是机器学习和数据科学中的一项基本工具。它可以单独使用，也可以与其他技术结合使用来解决各种问题。许多聚类算法，如
    K 均值，基于无监督学习范式。'
- en: '**Dimensionality Reduction**: In ML, dimensionality reduction is the process
    of reducing the number of features in a dataset. This can be done for a variety
    of reasons, such as reducing the training time or improving the model’s performance.
    There are a number of different methods for dimensionality reduction, and the
    most appropriate method will depend on the dataset and the desired results. Some
    common methods include **Principal Component Analysis** (**PCA**) and **Linear
    Discriminant Analysis** (**LDA**). In general, dimensionality reduction can be
    a useful tool for improving ML models. However, it is important to carefully select
    the right method for the specific dataset and desired results. Otherwise, it can
    harm the model’s performance.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：在机器学习中，降维是减少数据集中特征数量的过程。这样做有多种原因，例如减少训练时间或提高模型的性能。降维有许多不同的方法，最合适的方法将取决于数据集和所期望的结果。一些常见的方法包括**主成分分析**（**PCA**）和**线性判别分析**（**LDA**）。一般来说，降维可以作为提高机器学习模型的有用工具。然而，重要的是要为特定数据集和预期结果仔细选择合适的方法。否则，它可能会损害模型的性能。'
- en: The following table lists some of the commonly used algorithms and deep learning
    models used for each task.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格列出了常用于每个任务的一些算法和深度学习模型。
- en: '| **Classification** | **Regression** | **Clustering** | **Dimensionality Reduction**
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| **分类** | **回归** | **聚类** | **降维** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Logistic Regression
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Support Vector Machines
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Decision Tree Classifier
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树分类器
- en: Random Forest Classifier
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林分类器
- en: XGBOOST Classifier
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBOOST 分类器
- en: Naïve Bayes Classifier
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器
- en: Multilayered Perceptron
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多层感知机
- en: Convolutional Neural network
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: '|'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Linear Regression
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归
- en: Ridge Regression
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 岭回归
- en: Decision Tree Regression
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树回归
- en: Random Forest Regressor
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林回归器
- en: Support Vector Machine Regressor
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机回归器
- en: Neural Network Regressor
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络回归器
- en: Lasso Regressor
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lasso 回归器
- en: '|'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: K-means
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K均值
- en: Density-based Spatial Clustering
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于密度的空间聚类
- en: Gaussian Mixture Model
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯混合模型
- en: Affinity Propagation Clustering
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亲和传播聚类
- en: Balance Iterative Reducing and Clustering Hierarchies
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平衡迭代缩减和聚类层次
- en: Mean Shift Clustering
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值漂移聚类
- en: Agglomerative Hierarchy Clustering
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合层次聚类
- en: '**Ordering Points to Identify the Clustering Structure** (**OPTICS**)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**排序点以识别聚类结构**（**OPTICS**）'
- en: '|'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Principal Component Analysis
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析
- en: T-distributed Stochastic Neighbor Embedding
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T分布随机邻居嵌入
- en: Singular Value Decomposition
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奇异值分解
- en: Linear Discriminant Analysis
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性判别分析
- en: Factor Analysis
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因子分析
- en: Multidimensional scaling
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多维尺度分析
- en: Random Forests
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Table 5.1 – A table of ML algorithms
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.1 – ML 算法表
- en: The algorithms listed in *Table 5.1* use either supervised learning or unsupervised
    learning paradigms. However, there is a third paradigm of ML, **Reinforcement
    Learning** (**RL**). It has been explored to train artificial agents to play games,
    for self-driving cars, and in AutoML to find the best models and hyperparameters
    for different tasks.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.1* 中列出的算法采用了监督学习或无监督学习范式。然而，机器学习还有第三种范式——**强化学习**（**RL**）。它已经被探索用于训练人工智能代理进行游戏、用于自动驾驶汽车，以及在
    AutoML 中找到适合不同任务的最佳模型和超参数。'
- en: Additionally, in recent years, researchers have developed AI algorithms that
    can be used to generate data (for example, **generative adversarial networks**,
    **variational autoencoders**, **diffusion models**, and so on). These algorithms
    have been explored to generate text, images, art, and much more.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，近年来，研究人员已经开发出了可以生成数据的AI算法（例如，**生成对抗网络**、**变分自编码器**、**扩散模型**等）。这些算法已经被探索用于生成文本、图像、艺术作品等。
- en: Each of these algorithms has its strengths and weaknesses, so choosing the optimal
    algorithm for a task often requires you to iterate through different algorithms
    and choose the one giving the best performance in terms of the evaluation metrics.
    However, to build a responsible AI solution, it is important that models are also
    evaluated against bias and fairness, and that the end user is made aware of the
    uncertainty in the predictions of the model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 每种算法都有其优缺点，因此选择最优算法来完成任务通常需要通过不同算法进行反复迭代，并选择在评估指标上表现最好的算法。然而，为了构建一个负责任的AI解决方案，重要的是要对模型进行偏差和公平性评估，并且让最终用户意识到模型预测的不确定性。
- en: In the next section, we will talk about uncertainty and how to estimate uncertainty
    in model prediction.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论不确定性以及如何估计模型预测中的不确定性。
- en: Uncertainty in ML
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的不确定性
- en: ML is inseparably connected with uncertainty. After all, machines learn from
    data that is itself uncertain. This data might be noisy, incomplete, or just plain
    wrong. As a result, any conclusions that the machine draws from this data are
    going to be subject to some degree of uncertainty. And thus, we need to be aware
    of the inherent uncertainty in any results that we obtain and account for it appropriately.
    Only then can we hope to make the best use of ML in our decision-making processes.
    To make this point, here is a prediction using `EfficientNet`, one of the top-scoring
    networks on the `ImageNet` dataset (Tan and Le, 2019).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习与不确定性是密不可分的。毕竟，机器是从本身就具有不确定性的数据中学习的。这些数据可能是嘈杂的、不完整的，甚至完全错误的。因此，机器从这些数据中得出的任何结论都将受到某种程度的不确定性影响。因此，我们需要意识到在任何结果中固有的不确定性，并适当考虑它。只有这样，我们才能在决策过程中最大限度地利用机器学习。为了说明这一点，这里给出一个使用
    `EfficientNet` 的预测，它是 `ImageNet` 数据集中得分最高的网络之一（Tan 和 Le，2019）。
- en: '`EfficientNet` receives the following image:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`EfficientNet` 接收以下图像：'
- en: '![Figure 5.3 – Input to EfficientNetB0 from the ImageNet dataset](img/Figure_5.3_B18681.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 来自 ImageNet 数据集的 EfficientNetB0 输入](img/Figure_5.3_B18681.jpg)'
- en: Figure 5.3 – Input to EfficientNetB0 from the ImageNet dataset
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 来自 ImageNet 数据集的 EfficientNetB0 输入
- en: 'It returns the following output:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回以下输出：
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, on being presented with a photo of wooden logs, the model predicts
    it as `'stone_wall'` with 81% confidence.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在展示一张木头原木的照片时，模型将其预测为 `'stone_wall'`，置信度为81%。
- en: 'Before we delve further into uncertainty in ML, it would be nice to remember
    how we humans deal with uncertainty. If you are asked a question – let’s say “*What
    is the NASDAQ index today?*” – if you have been following the stock market, you
    will give either its value or probably say it is bullish or bearish, as the case
    may be. But if you are not into the stock market, you will simply say “*I don’t
    know*." ML algorithms, however, will give a prediction irrespective of the input.
    So, if you have an ML algorithm trained to classify flowers, it will tell you
    what flower *you* are, even though human identification is not its domain. It
    would be wonderful if our ML models could also say “*Sorry, I do not know – it
    is not in my domain!*” Consider, for example, the self-driving car: it should
    be able to tell the driver “*Hey, I am not sure if what I am seeing is a pedestrian
    or not. Can you take control?*” This becomes even more crucial when we are using
    deep learning models for crucial decision making, such as providing a loan, providing
    medical facilities, and so on.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨机器学习中的不确定性之前，回顾一下人类如何应对不确定性会非常有帮助。如果你被问到一个问题——比如“*今天的纳斯达克指数是多少？*”——如果你关注股市，你可能会给出其数值，或者可能会说它是看涨的还是看跌的，具体情况而定。但如果你不关心股市，你可能会简单地说“*我不知道*。”然而，机器学习算法无论输入是什么，都会给出一个预测。因此，如果你有一个训练过的机器学习算法来分类花卉，它会告诉你你是何种花卉，即使人类的识别并不是它的领域。如果我们的机器学习模型也能说“*抱歉，我不知道——它不在我的领域内!*”那该多好。例如，考虑自动驾驶汽车：它应该能够告诉司机“*嘿，我不确定我看到的是否是行人。你能接管吗？*”当我们使用深度学习模型进行重要决策时，比如提供贷款、提供医疗服务等等，这一点变得尤为重要。
- en: One way to deal with this problem is to quantify uncertainty in the prediction
    so that when making a prediction a model will tell us how confident it is about
    its prediction or how reliable its predictions are. In this section, we will explore
    some of the ways the uncertainty of a model can be quantitatively described. But
    first, let us see what the different types of uncertainty are and what causes
    them.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是量化预测中的不确定性，这样当模型做出预测时，它会告诉我们对预测的信心有多大，或者它的预测有多可靠。在本节中，我们将探讨一些不确定性如何定量描述的方法。但首先，让我们看看不确定性的不同类型以及它们的成因。
- en: Types of uncertainty
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不确定性的类型
- en: In ML, we often talk about two different types of uncertainty. The first type
    is **aleatoric uncertainty**, which refers to the inherent noise in data. This
    might be, for example, the fact that two people who are measuring the same thing
    will get slightly different results. Another type of uncertainty is **epistemic
    uncertainty**, which refers to our lack of knowledge about the model itself. For
    instance, if we’re trying to predict the price of a stock, there will always be
    some uncertainty due to the fact that we can’t know everything about the stock
    market. Uncertainty is an important part of ML, and understanding the different
    types can help us build better models.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们通常讨论两种不同类型的不确定性。第一种是**随机不确定性**，指的是数据中的固有噪声。例如，两个测量同一事物的人可能会得到略有不同的结果。另一种不确定性是**认知不确定性**，指的是我们对模型本身缺乏知识。例如，如果我们试图预测股票价格，总会存在一些不确定性，因为我们无法了解股市的所有信息。不确定性是机器学习中的一个重要部分，理解不同类型的不确定性可以帮助我们构建更好的模型。
- en: Let’s delve deep into the types of uncertainty.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨不确定性的类型。
- en: Aleatoric uncertainty
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机不确定性
- en: Aleatoric uncertainty represents the uncertainty that exists because of the
    random nature of natural processes. It is inherent uncertainty, present due to
    probabilistic variability. For example, when tossing a coin, there will always
    be a certain degree of uncertainty in predicting whether the next toss will be
    heads or tails. We can never eliminate this uncertainty, no matter how much data
    we have. To give another example, if we’re trying to predict the weather, there
    will always be some uncertainty due to the chaotic nature of the atmosphere. There
    is no way to remove this uncertainty. In essence, every time you repeat the experiment,
    the results will have certain variations.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 随机不确定性代表了由于自然过程的随机性而存在的不确定性。这是固有的不确定性，由于概率变异性而存在。例如，在抛硬币时，总会存在一定程度的不确定性，无法预测下次抛掷是否会是正面或反面。无论我们有多少数据，都无法消除这种不确定性。再举个例子，如果我们试图预测天气，由于大气的混沌特性，总会存在一些不确定性。我们无法消除这种不确定性。本质上，每次重复实验时，结果都会有一定的变异。
- en: Epistemic uncertainty
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 认知不确定性
- en: Epistemic uncertainty comes from the lack of knowledge or a lack of information.
    There can be various reasons for a lack of knowledge, for example, inadequate
    understanding of underlying processes, incomplete knowledge of the phenomena,
    and so on. This type of uncertainty can be reduced.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 认识论不确定性源于知识缺乏或信息不足。知识缺乏的原因可能有很多，例如对基础过程理解不充分、对现象的知识不完整等。这种类型的不确定性是可以减少的。
- en: For example, you can get more data, conduct more experiments, and so on. You
    can also try to reduce the uncertainty by using a more powerful model or training
    the model for longer.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以获取更多数据，进行更多实验，等等。你也可以尝试通过使用更强大的模型或更长时间的训练来减少不确定性。
- en: Predictive uncertainty
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测不确定性
- en: 'In ML, we are mostly concerned with the uncertainty that is propagated in a
    prediction, also called predictive uncertainty. On the basis of input data, predictive
    uncertainty is of three types:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习（ML）中，我们主要关注预测中的不确定性，也叫预测不确定性。根据输入数据，预测不确定性有三种类型：
- en: '**In-domain uncertainty**: The cause of this uncertainty is unequal training
    data distribution, for example, having more samples of class A as compared to
    class B. This uncertainty can be reduced by improving the data quality.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域内不确定性**：这种不确定性的原因是训练数据分布不均，例如，类别A的样本比类别B更多。这种不确定性可以通过提高数据质量来减少。'
- en: '**Domain-shift uncertainty**: This uncertainty arises when the training data
    distribution varies significantly from the real-world situation. For example,
    a model trained to recognize faces does not work well when the faces are occluded,
    the person is wearing glasses, a cap, or a face mask, and so on. It is possible
    to improve the model by including data-occluded samples in training.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域转移不确定性**：当训练数据分布与实际情况发生显著变化时，会产生这种不确定性。例如，当一个模型被训练用来识别面部时，当面部被遮挡，或人戴眼镜、帽子或口罩时，模型的表现就会变差。通过在训练中加入遮挡样本，可以改善模型的性能。'
- en: '**Out-of-domain (OOD) uncertainty**: As the name suggests, this is uncertainty
    that arises when the input is drawn from a completely unknown subspace. For example,
    if we train a model on dog photos and present it with a human, our ML algorithm
    is unable to tell that the input is outside of the domain it has learned.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域外（OOD）不确定性**：顾名思义，当输入来自一个完全未知的子空间时，就会产生这种不确定性。例如，如果我们训练一个识别狗照片的模型，并将其输入一张人类的照片，机器学习算法无法判断该输入是否超出了其已学习的领域。'
- en: Causes of uncertainty
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不确定性的原因
- en: 'There can be various causes of uncertainty (for more details, you should refer
    to the publication by Zio and Pedroni, 2012 – see the *Further* *reading* section):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性有多种原因（有关更多细节，请参阅Zio和Pedroni 2012年的出版物——详见*进一步阅读*部分）：
- en: '**Lack of knowledge**: In a supervised learning task, this can be translated
    as a lack of data that can encapsulate the relationship between input and output.
    This can be quantitative – for example, we may not know the probability distribution
    of features – or qualitative, where the features and their distribution are known
    but the features to describe the problem are unknown. By gaining more data, working
    closely with experts in the domain can reduce this cause of uncertainty.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识缺乏**：在监督学习任务中，这可以被理解为缺乏能够 encapsulate 输入和输出之间关系的数据。这种缺乏可能是定量的——例如，我们可能不知道特征的概率分布——也可能是定性的，特征及其分布已知，但描述问题的特征却未知。通过获取更多数据，并与领域专家密切合作，可以减少这种不确定性的来源。'
- en: '**An abundance of data**: While today there is no dearth of data that one can
    save and access, the problem is finding features that are relevant to the use
    case. Rigorous feature engineering and data exploration can help in selecting
    relevant features.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据过剩**：尽管如今我们可以存储和访问大量数据，但问题在于如何找到与使用案例相关的特征。严格的特征工程和数据探索有助于选择相关的特征。'
- en: '**The conflicting nature of data**: When we build a ML model, the input features
    and the output variables are supposed to have some form of a cause-and-effect
    relationship. However, despite expert knowledge, we can never be sure that the
    correlations we are seeing in the data are also causation. An interesting example
    of funny correlations can be found on this site: [https://www.fastcompany.com/3030529/hilarious-graphs-prove-that-correlation-isnt-causation](https://www.fastcompany.com/3030529/hilarious-graphs-prove-that-correlation-isnt-causation).'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据的冲突性**：当我们构建一个机器学习模型时，输入特征和输出变量应该具有某种因果关系。然而，尽管有专家知识，我们永远无法确定数据中看到的相关性是否也是因果关系。一个有趣的关于幽默相关性的例子可以在这个网站上找到：[https://www.fastcompany.com/3030529/hilarious-graphs-prove-that-correlation-isnt-causation](https://www.fastcompany.com/3030529/hilarious-graphs-prove-that-correlation-isnt-causation)。'
- en: '**Measurement errors**: The measurement of physical quantities via sensors
    always has some level of uncertainty due to the nature of the measuring device
    itself.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测量误差**：通过传感器测量物理量时，由于测量设备本身的性质，总是会有一定程度的不确定性。'
- en: '**Linguistic ambiguity**: It is often the case that words have different meanings
    in different contexts. Despite great advancements in natural language processing
    and new transformer architecture (Vaswani, 2017) that can converse almost like
    a human being ([https://blog.google/technology/ai/lamda/](https://blog.google/technology/ai/lamda/)),
    we still do not have systems that can understand the complexities of human language.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言歧义**：在不同的语境下，词语常常有不同的含义。尽管自然语言处理和新型变换器架构（Vaswani, 2017）取得了巨大进展，能够几乎像人类一样对话（[https://blog.google/technology/ai/lamda/](https://blog.google/technology/ai/lamda/)），我们仍然没有能够理解人类语言复杂性的系统。'
- en: '**The subjectivity of analyst opinions**: Finally, there is the human factor.
    There can be uncertainty due to the subjective interpretation of the data analyst.
    The same piece of data may result in different interpretations depending on the
    cultural background and competence of the data analyst.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分析师意见的主观性**：最后，还有一个人类因素。由于数据分析师的主观解读，可能会存在不确定性。同一组数据可能会根据数据分析师的文化背景和能力，导致不同的解释。'
- en: As we can see, while there are ways to reduce some of the causes of uncertainty,
    it is not possible to completely eliminate them. Therefore, in the rest of this
    part of the chapter, we will find ways to quantify and ways to reduce uncertainty
    in different ML tasks.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，虽然有一些方法可以减少不确定性的原因，但完全消除它们是不可能的。因此，在本章的接下来的部分，我们将找到量化和减少不同机器学习任务中不确定性的方法。
- en: Quantifying uncertainty
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化不确定性
- en: We can divide the deep neural network uncertainty quantification methods available
    in the literature into four types. They are based on the number (single or multiple)
    and the nature (deterministic or stochastic) of the deep neural networks used.
    *Figure 5**.4* shows the different types of uncertainty quantification methods
    in the literature.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将文献中现有的深度神经网络不确定性量化方法分为四种类型。这些方法基于所使用的深度神经网络的数量（单一或多重）和性质（确定性或随机性）。*图 5.4*
    显示了文献中不同类型的不确定性量化方法。
- en: '![Figure 5.4 – Different uncertainty quantification methods](img/Figure_5.4_B18681.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 不同的不确定性量化方法](img/Figure_5.4_B18681.jpg)'
- en: Figure 5.4 – Different uncertainty quantification methods
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 不同的不确定性量化方法
- en: 'Let us understand each of these methods:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解这些方法：
- en: '**Single-Network Deterministic Methods**: Conventional neural networks have
    their weights fixed after training and therefore, for the same input, each prediction
    results in the same result. In single-network deterministic methods, one works
    with conventional neural networks, but to add the uncertainty score to the model,
    one can modify the network by changing the architecture or loss function (**internal
    methods**). Alternatively, there are approaches (**external methods**) where the
    model and training are not changed. Instead, uncertainty is measured using additional
    components, for example, training two neural networks, one for the actual prediction
    task and a second one for the prediction of uncertainty in the first network’s
    prediction.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单网络确定性方法**：传统神经网络在训练后固定权重，因此对于相同的输入，每次预测结果都是相同的。在单网络确定性方法中，通常使用传统神经网络，但为了向模型添加不确定性分数，可以通过改变网络架构或损失函数来修改网络（**内部方法**）。另外，还有一些方法（**外部方法**），即模型和训练不发生变化，而是通过额外的组件来测量不确定性。例如，训练两个神经网络，一个用于实际的预测任务，另一个用于预测第一个网络预测中的不确定性。'
- en: '**Bayesian Methods**: These methods make use of the Bayesian approach. In **variational
    inference methods**, the general approach is to approximate the posterior distribution
    by optimizing over a family of tractable distributions. This is done by minimizing
    the KL divergence. The **sampling methods** are based on Markov chain Monte Carlo
    approaches. **Laplace approximation methods** work by approximating the log-posterior
    distribution and, based on it, deriving a normal distribution over the weight
    of the network.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贝叶斯方法**：这些方法使用贝叶斯方法。在**变分推理方法**中，通常的做法是通过在一系列可处理的分布上优化，来近似后验分布。这是通过最小化KL散度来实现的。**采样方法**基于马尔科夫链蒙特卡洛（MCMC）方法。**拉普拉斯近似方法**通过近似对数后验分布，并基于此推导出网络权重的正态分布。'
- en: '**Ensemble Methods**: These make use of more than one conventional deterministic
    neural network. Predictions from several models are combined into one prediction.
    Since the method depends on a variety of ensembles, the variety can be introduced
    by using random initialization and data shuffling. Using methods such as bagging
    and boosting to vary the distribution of training data and data augmentation are
    some of the common ways to have multiple models.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成方法**：这些方法使用多个传统的确定性神经网络。多个模型的预测结果会合并成一个预测结果。由于该方法依赖于多种集成，变化可以通过使用随机初始化和数据打乱来引入。使用如袋装法（bagging）和提升法（boosting）等方法来改变训练数据的分布，以及数据增强，是常见的使用多个模型的方法。'
- en: '**Test-Time Augmentation Methods**: Here, the idea is to create multiple test
    data from the original test data using the technique of data augmentation. Augmented
    test data allows the exploration of different views and enables the capturing
    of uncertainty.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试时增强方法**：这里的想法是通过数据增强技术从原始测试数据中创建多个测试数据。增强后的测试数据可以探索不同的视角，并能够捕捉不确定性。'
- en: Out of the four, single-network deterministic methods are very straightforward,
    easy to implement, and computationally less expensive. A lot of interest in recent
    years has been generated in Bayesian methods, and with the availability of frameworks
    such as TensorFlow Probability, it has become easier to experiment with them.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这四种方法中，单网络确定性方法非常直接，易于实现，并且计算开销较小。近年来，贝叶斯方法引起了大量关注，随着像 TensorFlow Probability
    这样的框架的出现，实验这些方法变得更加容易。
- en: Uncertainty in regression tasks
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归任务中的不确定性
- en: Let us consider a sensor whose output can be modeled by a linear process, defined
    by the relationship *y =* *x*.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们考虑一个传感器，其输出可以通过线性过程建模，定义为 *y =* *x*。
- en: 'Let us say we sample data for *x* lying in the range [-2.5,2.5]. Now, there
    would always be some noise introduced because of the inherent physical processes
    of the sensor (for example, white noise). Additionally, the sensor may have limitations
    such as temperature or pressure requirements. The following graph shows data from
    our sensor:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们从范围[-2.5,2.5]内对 *x* 进行采样。由于传感器固有的物理过程（例如，白噪声），总会引入一些噪声。此外，传感器可能具有如温度或压力等限制要求。下图展示了我们传感器的数据：
- en: '![Figure 5.5 – Different kinds of uncertainty in data](img/Figure_5.5_B18681.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – 数据中的不同类型的不确定性](img/Figure_5.5_B18681.jpg)'
- en: Figure 5.5 – Different kinds of uncertainty in data
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 数据中的不同类型的不确定性
- en: We can see that in the lower-left corner, due to some malfunction, there is
    high aleatoric uncertainty. Also, there are large gaps where there is no observed
    training data, which can cause high epistemic uncertainty in that range of inputs.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在左下角，由于某些故障，存在较高的随机不确定性。同时，也有一些较大的空白区域，那里没有观察到训练数据，这可能导致该输入范围内的较高的知识性不确定性。
- en: Before we talk about measuring these uncertainties, let us build a model and
    train it on the training data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论如何衡量这些不确定性之前，让我们先建立一个模型并在训练数据上进行训练。
- en: 'We use TensorFlow Dense layers to build the model. We start with an input layer
    and then add the hidden layers using the `for` loop, along with the `Dropout`
    layer after each hidden layer, finally followed by a `Dense` output layer:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 TensorFlow 的 Dense 层来构建模型。我们从输入层开始，然后使用 `for` 循环添加隐藏层，每个隐藏层后跟一个 `Dropout`
    层，最后跟一个 `Dense` 输出层：
- en: '[PRE2]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'An important point to note here is that the Dropout layers have the `training`
    parameter set to the `True` value. What this means is that dropout will be applied
    even during inference, and as a result, we will get a variation in predictions.
    The model summary is as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的一个重要点是，Dropout 层的`training`参数被设置为`True`。这意味着即使在推理过程中也会应用 Dropout，因此，我们将会得到预测结果的变化。模型摘要如下：
- en: '[PRE3]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We use the `rmsprop` optimizer, and use the mean square error as the loss function:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `rmsprop` 优化器，并使用均方误差作为损失函数：
- en: '[PRE4]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And we train it using the training data plotted (red dots) in *Figure 5**.5*:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用在*图 5.5*中绘制的训练数据（红点）对其进行训练：
- en: '[PRE5]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following graph (*Figure 5**.6*) shows the evolution of loss as the training
    progresses. We can see that the model learned quite early, and after roughly 25
    epochs there is no significant decrease in the loss function:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表（*图 5.6*）展示了随着训练的进行，损失的变化情况。我们可以看到，模型很早就学会了，并且在大约 25 个周期后，损失函数没有显著下降：
- en: "![Figure 5.6 – Training loss as the model learn\uFEFF](img/Figure_5.6_B18681.jpg)"
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – 模型学习过程中的训练损失](img/Figure_5.6_B18681.jpg)'
- en: Figure 5.6 – Training loss as the model learn
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – 模型学习过程中的训练损失
- en: 'Let us now see how this model performs on test data. We take input as varying
    in the range `[-``10, 10]`:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看这个模型在测试数据上的表现。我们将输入范围设置为`[-10, 10]`：
- en: '[PRE6]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let us plot the data:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制数据：
- en: '[PRE7]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following plot shows the performance on the test dataset:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了模型在测试数据集上的表现：
- en: '![Figure 5.7 – Model performance on test dataset](img/Figure_5.7_B18681.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7 – 模型在测试数据集上的表现](img/Figure_5.7_B18681.jpg)'
- en: Figure 5.7 – Model performance on test dataset
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – 模型在测试数据集上的表现
- en: We can see that on the test data, the R2 score is 0.98\. For the most part,
    this is a great model. However, as we said earlier, there are regions of high
    epistemic and high aleatory uncertainty in the data.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在测试数据上，R2 分数为 0.98。大部分情况下，这是一个很棒的模型。然而，正如我们之前所说，数据中确实存在知识性和随机性不确定性较高的区域。
- en: 'Let us first explore epistemic uncertainty. We evaluate our model on the test
    dataset about 500 times, meaning that we predict 500 times using our trained model
    on the same test data. This is equivalent to simulating a Gaussian process. Each
    time, we obtain a range of output values for each input scalar from test data.
    As a result, we can calculate the standard deviation of the posterior distribution
    and display it as a measure of epistemic uncertainty:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先探讨知识性不确定性。我们对测试数据集进行大约 500 次评估，这意味着我们使用训练好的模型对相同的测试数据进行 500 次预测。这相当于模拟一个高斯过程。每次，我们都会得到测试数据中每个输入标量的输出值范围。因此，我们可以计算后验分布的标准差，并将其作为知识性不确定性的度量：
- en: '[PRE8]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*Figure 5**.8* shows the epistemic uncertainty for our model:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.8*展示了我们模型的知识性不确定性：'
- en: '![Figure 5.8 – Epistemic uncertainty on test dataset](img/Figure_5.8_B18681.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8 – 测试数据集上的知识性不确定性](img/Figure_5.8_B18681.jpg)'
- en: Figure 5.8 – Epistemic uncertainty on test dataset
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – 测试数据集上的知识性不确定性
- en: We can see from the figure that epistemic uncertainty is high in the regions
    where we do not have sufficient training data. We would like to mention here that,
    normally, to access epistemic uncertainty one has to build a Bayesian approximation
    model so that the variations in weight can be captured. One can do it using `VariableLayer`,available
    in TensorFlow Probability. However, here, we made use of the fact that dropout
    layers can act as a Bayesian approximation (Gal and Ghahramani, 2017).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看出，认知不确定性在我们没有足够训练数据的区域较高。在这里我们想提到，通常，要访问认知不确定性，必须构建一个贝叶斯近似模型，以便捕获权重的变化。可以使用
    TensorFlow Probability 中的 `VariableLayer` 实现这一点。然而，在这里，我们利用了 Dropout 层可以作为贝叶斯近似（Gal
    和 Ghahramani，2017）的事实。
- en: Epistemic uncertainty is a property of the model. Aleatoric uncertainty, on
    the other hand, is a property of data. There are two types of aleatoric uncertainty.
    One is `y`, but also its variance. The same can be achieved using the `DistributionLambda`
    layer of TensorFlow Probability.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 认知不确定性是模型的属性。而随机不确定性则是数据的属性。随机不确定性有两种类型。一种是`y`，还有它的方差。使用 TensorFlow Probability
    的`DistributionLambda`层也可以实现相同的效果。
- en: 'We use the same data as before, and define the heteroscedastic aleatoric loss
    function (Kendal and Gal, 2017):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与之前相同的数据，并定义异方差随机损失函数（Kendal 和 Gal，2017）：
- en: '![](img/Formula_05_004.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_004.jpg)'
- en: Here, *N* is the number of data samples, *y*i is the ground truth, and ŷi is
    the predicted output. Variance in input is σ2\. Now, if you see this function,
    the first term is a scaled version of the mean square error and ensures that the
    model predicts close to the ground truth. However, if its predictions are not
    accurate, the variance term (uncertainty) in the denominator increases to reduce
    the contribution of the first term. The second term ensures that the model does
    not go on increasing the uncertainty.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*N* 是数据样本的数量，*y*i 是真实值，ŷi 是预测输出。输入的方差是 σ²。现在，如果你查看这个函数，第一个项是均方误差的缩放版本，确保模型预测接近真实值。然而，如果预测不准确，分母中的方差项（不确定性）会增大，从而减少第一个项的贡献。第二个项确保模型不会不断增加不确定性。
- en: 'Let us see how regressive uncertainty works:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看回归不确定性是如何工作的：
- en: 'The following Python code implements the preceding loss function:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下 Python 代码实现了前述的损失函数：
- en: '[PRE9]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we build the model. Earlier, we had only one neuron in the output layer.
    Now, we will have two neurons, one corresponding to predicted output `y`, and
    the other corresponding to the variance:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们构建模型。之前，输出层只有一个神经元。现在，我们将有两个神经元，一个对应预测输出`y`，另一个对应方差：
- en: '[PRE10]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here is the model summary:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型摘要：
- en: '[PRE11]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, to train the model, we need to reshape the data to adjust for the change
    in the model:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，为了训练模型，我们需要重新调整数据的形状，以适应模型的变化：
- en: '[PRE12]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let us now define the optimizer. We keep it the same as in the first case,
    `rmsprop`, and change the loss to our customized loss:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们定义优化器。我们将它保持为与第一次相同的`rmsprop`，并将损失函数更改为我们定制的损失函数：
- en: '[PRE13]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And finally, train it:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，进行训练：
- en: '[PRE14]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let us now plot the aleatoric uncertainty:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们绘制随机不确定性：
- en: '[PRE15]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The plot for aleatoric uncertainty is as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 随机不确定性的图如下所示：
- en: '![Figure 5.9 – Aleatoric uncertainty on the test dataset](img/Figure_5.9_B18681.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – 测试数据集上的随机不确定性](img/Figure_5.9_B18681.jpg)'
- en: Figure 5.9 – Aleatoric uncertainty on the test dataset
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 测试数据集上的随机不确定性
- en: From the figure, we can see that at around **-2.5**, the aleatoric uncertainty
    is high, since this was the place in our original training data where there was
    more noise in the data itself. This type of uncertainty can be problematic for
    applications where the AI may need to make decisions that can have life-threatening
    consequences, for example, in the case of a self-driving car. Therefore, there
    is a need to build models that can not only predict but also score epistemic and
    aleatory uncertainties while making predictions.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看出，在 **-2.5** 附近，随机不确定性较高，因为这是我们原始训练数据中数据本身噪声较多的地方。这种类型的不确定性在一些需要做出可能关系到生死的决策的应用中可能会带来问题，例如自动驾驶汽车的情况。因此，需要构建能够不仅做出预测，而且在预测时还能评估认知不确定性和随机不确定性（不确定性评分）的模型。
- en: Now that we have seen how to model uncertainty in a regression task, let us
    move on to quantifying uncertainty in a classification task.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经看到如何在回归任务中建模不确定性，现在让我们转向在分类任务中量化不确定性。
- en: Uncertainty in classification tasks
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类任务中的不确定性
- en: In classification tasks, the standard procedure is to use **SoftMax activation**
    in the final layer. This SoftMax activation, by default, already has a measure
    of confidence (*Figure 5**.10*). However, SoftMax is not very reliable. Consider,
    for example, a model trained to classify horses and zebras, and it sees a dog;
    it would not say 50% horse and 50% zebra, instead of assuming it is a black dog.
    It might think it’s more like a horse, and classify it as a horse with 60% probability.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类任务中，标准做法是在最后一层使用**SoftMax 激活**函数。默认情况下，这个 SoftMax 激活已经包含了一定的置信度测量（*图 5.10*）。然而，SoftMax
    并不是非常可靠。例如，考虑一个训练用来分类马和斑马的模型，当它看到一只狗时，它不会说 50% 是马，50% 是斑马，而是会假设它是一只黑色的狗。它可能会认为它更像一匹马，并以
    60% 的概率将其分类为马。
- en: '![Figure 5.10 – Image from MNIST data and its SoftMax confidence](img/Figure_5.10_B18681.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10 – 来自 MNIST 数据的图像及其 SoftMax 置信度](img/Figure_5.10_B18681.jpg)'
- en: Figure 5.10 – Image from MNIST data and its SoftMax confidence
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – 来自 MNIST 数据的图像及其 SoftMax 置信度
- en: 'Mathematically, classification tasks use maximal class probability to determine
    the class:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，分类任务使用最大类概率来确定类别：
- en: '![](img/Formula_05_008.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_008.jpg)'
- en: Here, *K* is the total number of classes, and pk is the SoftMax value for class
    *k* for that prediction. Some researchers have also tried to use entropy ![](img/Formula_05_010.png)
    of the SoftMax prediction. The maximal probability represents a direct representation
    of certainty, while entropy describes the average level of information in a random
    variable.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*K* 是类别的总数，pk 是该预测中类别 *k* 的 SoftMax 值。一些研究人员也尝试使用 SoftMax 预测的熵 ![](img/Formula_05_010.png)。最大概率直接表示了确定性，而熵则描述了随机变量中的平均信息量。
- en: Despite being simple and straightforward to use, these approaches are unreliable,
    especially when we are dealing with a critical decision AI model such as medical
    diagnosis.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些方法简单且直接使用，但它们并不可靠，尤其是当我们面对像医学诊断这样的关键决策 AI 模型时。
- en: '![Figure 5.11 – Deep ensemble architecture](img/Figure_5.11_B18681.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.11 – 深度集成架构](img/Figure_5.11_B18681.jpg)'
- en: Figure 5.11 – Deep ensemble architecture
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – 深度集成架构
- en: 'One of the ways to quantify uncertainty in classification tasks is by using
    ensemble methods. Lakshminarayanan et al., show in their paper that using ensemble
    methods is better than other approaches when the model is presented with out-of-domain
    data. They trained an ensemble of deep neural networks (*Figure 5**.11*), using
    the entire MNIST training dataset. The predictions from the trained models were
    combined as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 量化分类任务中的不确定性的一种方法是使用集成方法。Lakshminarayanan 等人在他们的论文中表明，当模型面对域外数据时，使用集成方法比其他方法更有效。他们训练了一个深度神经网络的集成（*图
    5.11*），并使用了整个 MNIST 训练数据集。经过训练的模型的预测结果被如下方式结合：
- en: '![](img/Formula_05_011.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_011.jpg)'
- en: 'Here, *M* is the number of ensembled models. In the paper, *M=5*, *y* is the
    prediction for input *x*, and the θm instances are the network parameters. As
    you can see, for classification, this corresponds to averaging the prediction
    probabilities. Additionally, they also generated adversarial examples and performed
    adversarial training for each network of the ensemble. *Figure 5**.12*, shows
    the results for the MNIST and NotMNIST datasets:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*M* 是集成模型的数量。在论文中，*M=5*，*y* 是输入 *x* 的预测结果，θm 实例是网络参数。如你所见，对于分类任务，这对应于平均预测概率。此外，他们还生成了对抗性样本，并对集成中的每个网络进行了对抗训练。*图
    5.12* 显示了 MNIST 和 NotMNIST 数据集的结果：
- en: '![ Figure 5.12 – Entropy values for the MNIST dataset (blue) and the NotMNIST
    dataset (red)](img/Figure_5.12_B18681.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![ 图 5.12 – MNIST 数据集（蓝色）和 NotMNIST 数据集（红色）的熵值](img/Figure_5.12_B18681.jpg)'
- en: Figure 5.12 – Entropy values for the MNIST dataset (blue) and the NotMNIST dataset
    (red)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 – MNIST 数据集（蓝色）和 NotMNIST 数据集（红色）的熵值
- en: As you can see in the figure, the ensembles work better when presented with
    the NotMNIST dataset, since the entropy value for NotMNIST spreads over a bigger
    range and the peak is reduced. They also compared their approach with the Bayesian
    method (adding Monte Carlo dropout). The figure shows that ensembles give better
    results. Thus, they showed that deep ensembles generate a low confidence output
    for out-of-distribution data. In general, for dataset shift and out-of-domain
    distributions, deep ensembles give consistently better performance when compared
    to other methods.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 正如图中所示，集成方法在面对NotMNIST数据集时表现更好，因为NotMNIST的熵值分布范围更广，且峰值降低了。他们还将他们的方法与贝叶斯方法（加入蒙特卡洛丢弃）进行了比较。图中显示集成方法给出了更好的结果。因此，他们表明，深度集成方法为分布外数据生成了低置信度的输出。一般来说，对于数据集偏移和跨领域分布，深度集成方法相比其他方法表现出一致的更好性能。
- en: Tools for benchmarking and quantifying uncertainty
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于基准测试和量化不确定性的工具
- en: A lot of work has been done to quantify uncertainty and create benchmarks for
    uncertainty and robustness. In this section, we will cover some of the prominent
    ones. Please remember that the standards are still in the nascent stage, and as
    a result, many of these tools and GitHub repos may have certain limitations.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有很多工作致力于量化不确定性并为不确定性和鲁棒性创建基准。在本节中，我们将介绍一些突出的工作。请记住，这些标准仍处于初期阶段，因此，许多工具和GitHub仓库可能存在一定的局限性。
- en: The Uncertainty Baselines library
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不确定性基准库
- en: Developed by researchers from the Google Brain research team, the University
    of Oxford, the University of Cambridge, Harvard University, and the University
    of Texas, the **Uncertainty Baselines library** contains a set of baselines that
    you can use to compare the performance of different deep learning methods. The
    baselines are implemented using high-quality methods, and they are available for
    a variety of tasks. You can use these baselines to get started with your own experiments.
    The complete work is accessible via the GitHub repo at [https://github.com/google/uncertainty-baselines](https://github.com/google/uncertainty-baselines).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 由Google Brain研究团队、牛津大学、剑桥大学、哈佛大学和德克萨斯大学的研究人员开发的**不确定性基准库**包含一组基准，您可以使用这些基准来比较不同深度学习方法的性能。这些基准是使用高质量方法实现的，并且适用于多种任务。您可以使用这些基准来开始自己的实验。完整的工作可以通过GitHub仓库访问，网址为[https://github.com/google/uncertainty-baselines](https://github.com/google/uncertainty-baselines)。
- en: At present, there is no stable version. However, users can install it and experiment
    directly via Git. The library aims to provide users with a modular, framework-agnostic,
    hardware-agnostic tool.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 目前没有稳定版本。然而，用户可以通过Git直接安装并进行实验。该库旨在为用户提供一个模块化、框架无关、硬件无关的工具。
- en: Keras-Uncertainty
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras-Uncertainty
- en: '`make_moon` method from scikit-learn to create synthetic classification data.
    We trained five models and plotted the respective uncertainty using the Keras-Uncertainty
    library.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn中的`make_moon`方法创建合成分类数据。我们训练了五个模型，并使用Keras-Uncertainty库绘制了各自的不确定性。
- en: '![Figure 5.13 – Uncertainty for binary classification problem](img/Figure_5.13_B18681.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图5.13 – 二分类问题的不确定性](img/Figure_5.13_B18681.jpg)'
- en: Figure 5.13 – Uncertainty for binary classification problem
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 – 二分类问题的不确定性
- en: Robustness metrics
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 鲁棒性指标
- en: 'Developed by Google Research, the Robustness Metrics library provides functions
    and methods that can help in the evaluation of classification models. It defines
    three sets of metrics:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 由Google Research开发的鲁棒性指标库提供了可以帮助评估分类模型的函数和方法。它定义了三组指标：
- en: '**Out-of-distribution generalization**: This measure assesses the model’s ability
    to accurately classify objects that share similarities but may have varying perspectives.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布外泛化**：该指标评估模型准确分类具有相似性但可能具有不同视角的物体的能力。'
- en: '**Stability**: This measure evaluates the stability of predictions made by
    the model when there are changes in the input, and how effectively the model performs
    with natural disturbances.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稳定性**：该指标评估模型在输入发生变化时预测的稳定性，以及模型在自然干扰下的表现效果。'
- en: '**Uncertainty**: This measure gauges the proximity of the probabilities estimated
    by a model to the actual probabilities.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不确定性**：该指标衡量模型估计的概率与实际概率之间的接近度。'
- en: The Robustness Metrics library can be used even for non-vision models. Essentially,
    if you have a classification task, you can try this library. It depends on TensorFlow
    and TensorFlow Probability, therefore, before you use it you should install them
    on your system.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒性度量库甚至可以用于非视觉模型。实际上，如果你有分类任务，可以尝试使用这个库。它依赖于TensorFlow和TensorFlow Probability，因此在使用之前，您需要先在系统上安装它们。
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter started by introducing the different components of ML pipelines.
    Then we discussed various ML tasks and different algorithms that are used for
    those tasks.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先介绍了机器学习管道的不同组成部分。接着我们讨论了各种机器学习任务以及为这些任务使用的不同算法。
- en: The main theme of the chapter was uncertainty in deep learning. This uncertainty
    can be due to the data or the model – or both. We discussed the two types of uncertainty,
    aleatory and epistemic. The chapter also discussed various methods that have been
    used in the literature to quantify uncertainty. It also delved deep into the causes
    of uncertainty. Next, we implemented algorithms to quantify uncertainty in regression
    tasks. Finally, the chapter discussed the methods used to quantify uncertainty
    in classification tasks. There is a need to develop baselines and benchmarks for
    uncertainty and robustness in deep learning. A lot of work is taking place in
    this area; however, uncertainty and robustness are still critical research problems
    in AI and ML.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要主题是深度学习中的不确定性。这种不确定性可能来自数据、模型，或者两者兼而有之。我们讨论了两种不确定性类型：随机不确定性和认知不确定性。本章还讨论了文献中用来量化不确定性的各种方法，并深入探讨了不确定性的原因。接着，我们实现了用于回归任务的量化不确定性的算法。最后，本章讨论了用于量化分类任务不确定性的方法。在深度学习中，需要开发不确定性和鲁棒性的基准和基线。尽管该领域正在进行大量研究，但不确定性和鲁棒性仍然是人工智能和机器学习中的关键研究问题。
- en: In the next chapter, we will delve into the concept of hyperparameters, and
    how to use AutoML and MLOps to streamline your ML workflow.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨超参数的概念，以及如何使用AutoML和MLOps来简化机器学习工作流。
- en: References
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Efficientnet: Rethinking model scaling for convolutional neural networks*.
    In *International conference on ML,* Tan, M., & Le, Q. (2019, May). (pp. 6105-6114).
    PMLR. [http://proceedings.mlr.press/v97/tan19a/tan19a.pdf](http://proceedings.mlr.press/v97/tan19a/tan19a.pdf))'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*EfficientNet: 重新思考卷积神经网络的模型缩放*。在*国际机器学习会议*上，Tan, M., & Le, Q.（2019年5月）。（第6105-6114页）。PMLR.
    [http://proceedings.mlr.press/v97/tan19a/tan19a.pdf](http://proceedings.mlr.press/v97/tan19a/tan19a.pdf)'
- en: '*Uncertainty characterization in risk analysis for decision-making practice*,
    Zio, E., & Pedroni, N. (2012). FonCSI. [https://www.researchgate.net/publication/266391086_Uncertainty_characterization_in_risk_analysis_for_decision-making_practice](https://www.researchgate.net/publication/266391086_Uncertainty_characterization_in_risk_analysis_for_decision-making_practice)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*决策分析中的不确定性表征*，Zio, E., & Pedroni, N.（2012年）。FonCSI. [https://www.researchgate.net/publication/266391086_Uncertainty_characterization_in_risk_analysis_for_decision-making_practice](https://www.researchgate.net/publication/266391086_Uncertainty_characterization_in_risk_analysis_for_decision-making_practice)'
- en: '*Attention is all you need. Advances in neural information processing systems*,
    Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    ... & Polosukhin, I. (2017). 30\. [https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Attention is all you need. 神经信息处理系统的进展*，Vaswani, A., Shazeer, N., Parmar,
    N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I.（2017年）。30\. [https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)'
- en: '*Dropout as a Bayesian approximation: Representing model uncertainty in deep
    learning*. In *International conference on ML,* Gal, Y., & Ghahramani, Z. (2016,
    June). (pp. 1050-1059). PMLR. [http://proceedings.mlr.press/v48/gal16.pdf](http://proceedings.mlr.press/v48/gal16.pdf)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dropout作为贝叶斯近似：在深度学习中表示模型的不确定性*。在*国际机器学习会议*上，Gal, Y., & Ghahramani, Z.（2016年6月）。（第1050-1059页）。PMLR.
    [http://proceedings.mlr.press/v48/gal16.pdf](http://proceedings.mlr.press/v48/gal16.pdf)'
- en: '*What uncertainties do we need in Bayesian deep learning for computer vision?*
    In *Advances in neural information processing systems*, Kendall, A., & Gal, Y.
    (2017). 30\. [https://arxiv.org/pdf/1703.04977.pdf](https://arxiv.org/pdf/1703.04977.pdf)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们在贝叶斯深度学习中需要哪些不确定性用于计算机视觉？* 在*神经信息处理系统的进展*中，Kendall, A., & Gal, Y.（2017年）。30\.
    [https://arxiv.org/pdf/1703.04977.pdf](https://arxiv.org/pdf/1703.04977.pdf)'
- en: '*A survey of uncertainty in deep neural networks,* Gawlikowski, J., Tassi,
    C. R. N., Ali, M., Lee, J., Humt, M., Feng, J., ... & Zhu, X. X. (2021). arXiv
    preprint arXiv:2107.03342\. [https://arxiv.org/pdf/2107.03342.pdf](https://arxiv.org/pdf/2107.03342.pdf)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度神经网络中的不确定性调查*，Gawlikowski, J., Tassi, C. R. N., Ali, M., Lee, J., Humt,
    M., Feng, J., ... & Zhu, X. X. (2021)。arXiv预印本arXiv:2107.03342\。 [https://arxiv.org/pdf/2107.03342.pdf](https://arxiv.org/pdf/2107.03342.pdf)'
- en: '*A review of uncertainty quantification in deep learning: Techniques, applications
    and challenges,* Abdar, M., Pourpanah, F., Hussain, S., Rezazadegan, D., Liu,
    L., Ghavamzadeh, M., ... & Nahavandi, S. (2021). In *Information Fusion*, 76,
    243-297\. [https://arxiv.org/pdf/2011.06225.pdf](https://arxiv.org/pdf/2011.06225.pdf)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度学习中的不确定性量化综述：技术、应用与挑战*，Abdar, M., Pourpanah, F., Hussain, S., Rezazadegan,
    D., Liu, L., Ghavamzadeh, M., ... & Nahavandi, S. (2021)。发表于 *信息融合*，76，243-297页。[https://arxiv.org/pdf/2011.06225.pdf](https://arxiv.org/pdf/2011.06225.pdf)'
- en: '*Simple and scalable predictive uncertainty estimation using deep ensembles*.
    In *Advances in neural information processing systems*, Lakshminarayanan, B.,
    Pritzel, A., & Blundell, C. (2017). 30\. [https://arxiv.org/pdf/1612.01474.pdf](https://arxiv.org/pdf/1612.01474.pdf)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用深度集成方法进行简单且可扩展的预测不确定性估计*。发表于 *神经信息处理系统进展*，Lakshminarayanan, B., Pritzel,
    A., & Blundell, C. (2017)。第30页。[https://arxiv.org/pdf/1612.01474.pdf](https://arxiv.org/pdf/1612.01474.pdf)'
- en: '*Robustness Metrics*, J. Djolonga, F. Hubis, M. Minderer, Z. Nado, J. Nixon,
    R. Romijnders, D. Tran, and M. Lucic. 2020\. [https://github.com/google-research/robustness_metrics](https://github.com/google-research/robustness_metrics)'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*鲁棒性指标*，J. Djolonga, F. Hubis, M. Minderer, Z. Nado, J. Nixon, R. Romijnders,
    D. Tran, 和 M. Lucic. 2020年。[https://github.com/google-research/robustness_metrics](https://github.com/google-research/robustness_metrics)'
- en: '*Uncertainty Baselines: Benchmarks for uncertainty & robustness in deep learning*,
    Nado, Z., Band, N., Collier, M., Djolonga, J., Dusenberry, M. W., Farquhar, S.,
    ... & Tran, D. (2021). arXiv preprint arXiv:2106.04015\. [https://arxiv.org/pdf/2106.04015](https://arxiv.org/pdf/2106.04015)'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不确定性基准：深度学习中不确定性与鲁棒性的基准测试*，Nado, Z., Band, N., Collier, M., Djolonga, J.,
    Dusenberry, M. W., Farquhar, S., ... & Tran, D. (2021)。arXiv预印本arXiv:2106.04015\。[https://arxiv.org/pdf/2106.04015](https://arxiv.org/pdf/2106.04015)'
- en: '*On robustness and transferability of convolutional neural networks,* Djolonga,
    J., Yung, J., Tschannen, M., Romijnders, R., Beyer, L., Kolesnikov, A., ... &
    Lucic, M. (2021). In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition* (pp. 16458-16468). [https://arxiv.org/pdf/2007.08558.pdf](https://arxiv.org/pdf/2007.08558.pdf)'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*卷积神经网络的鲁棒性与可转移性*，Djolonga, J., Yung, J., Tschannen, M., Romijnders, R., Beyer,
    L., Kolesnikov, A., ... & Lucic, M. (2021)。发表于 *IEEE/CVF计算机视觉与模式识别会议论文集*（第16458-16468页）。[https://arxiv.org/pdf/2007.08558.pdf](https://arxiv.org/pdf/2007.08558.pdf)'
