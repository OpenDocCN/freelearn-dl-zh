- en: Multi-Armed Bandit Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多臂老虎机模型
- en: The multi-armed bandit problem is a classic reinforcement learning problem that
    exemplifies the exploration versus exploitation dilemma. When we have a limited
    set of resources to base our choices on, it becomes essential to adopt a method
    to establish which of the alternative competing choices allow us to maximize the
    expected profit. The name **multi**-**armed bandit** derives from the example
    of a gambler struggling with a row of slot machines who must decide whether to
    continue with the current machine or try a different machine.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂老虎机问题是一个经典的强化学习问题，体现了探索与利用的困境。当我们只能依赖有限的资源来做出选择时，采用一种方法来确定哪些竞争选择可以最大化预期利润变得至关重要。**多**臂**老虎机**这个名字源于一个赌徒在一排老虎机中挣扎，他必须决定是继续使用当前的机器，还是尝试不同的机器。
- en: In this chapter, we will get an overview of the basic concepts of the multi-armed
    bandit model, discover the different techniques that are available to help resolve
    this problem, and discover the meaning of the action-value implementation. Then,
    we will learn how to address this problem using a contextual approach, how to
    implement asynchronous actor-critic agents, and how to implement a multi-armed
    bandit problem in R.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将概述多臂老虎机模型的基本概念，探索可用于解决此问题的不同技术，并深入了解行动-价值实现的含义。接着，我们将学习如何使用情境方法来解决这个问题，如何实现异步演员-评论家代理，以及如何在R中实现多臂老虎机问题。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Multi-armed bandit model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多臂老虎机模型
- en: Multi-armed bandit applications
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多臂老虎机应用
- en: Action-value implementation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行动-价值实现
- en: Understanding problem solution techniques
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解问题解决技术
- en: Implementing the contextual approach
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现情境方法
- en: Understanding asynchronous actor-critic agents
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解异步演员-评论家代理
- en: Online advertising using the MAB model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MAB模型进行在线广告
- en: Multi-armed bandit model
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多臂老虎机模型
- en: A common problem in learning theory is to identify the best option among a set
    of options, without knowing a priori what benefits each one can give, while minimizing
    the cost of doing so.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 学习理论中的一个常见问题是，在不知道每个选项能提供何种收益的情况下，如何在一组选项中识别最佳选项，同时最小化选择的成本。
- en: The **multi-armed bandit** (**MAB**) problem takes its name from a known problem
    faced in decision theory. A gambler must choose which slot machine to play among
    the many he has in front of him. After playing, he will have a certain degree
    of knowledge about the rewards that are distributed by some machines, but he will
    not know anything about the others, so he will be forced to choose between machines
    that are partly known and machines that are not known.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**多臂老虎机**（**MAB**）问题得名于决策理论中遇到的一个著名问题。一个赌徒必须从他面前的多台老虎机中选择一台进行游戏。游戏后，他将获得关于某些机器奖励的某种程度的知识，但他对其他机器一无所知，因此他将被迫在部分已知的机器和完全未知的机器之间做出选择。'
- en: This problem is ideal for modeling the compromises between the exploitation
    of known opportunities and the exploration of unknown opportunities, as well as
    to test strategies in the presence of a high degree of ignorance and uncertainty.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题非常适合模拟已知机会的利用与未知机会的探索之间的妥协，也可以在高度无知和不确定的情况下测试策略。
- en: In more technical terms, each slot machine is modeled as a probability distribution,
    with an average value and one standard deviation. These two parameters can vary
    over time, either dependently or independently, for example, to model the evolution
    over time or the dynamism of the competitive scenario, or in response to choices
    that are made by one or more players to model competition or influence on the
    context.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术术语上讲，每台老虎机都被建模为一个概率分布，具有一个平均值和一个标准差。这两个参数可以随时间变化，可能是相互依赖的，也可能是独立变化的，例如，模拟随时间的演变或竞争场景的动态，或者响应一个或多个玩家的选择，以模拟竞争或对情境的影响。
- en: The distribution of probabilities is obviously not known to the players but
    can be learned over time as values ​​are obtained from each slot machine.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 概率分布显然对玩家来说是未知的，但随着每台老虎机奖励的获取，玩家可以逐渐学习这些分布。
- en: MAB problems were introduced by H. Robbins to model decision-making under uncertainty
    when the environment is unknown. These problems were treated in the following
    paper: *Some Aspects of the Sequential Design of Experiments*, Robbins, H. (1952), Bulletin
    of the American Mathematical Society, 55, 527–535.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: MAB 问题是由 H. Robbins 提出的，用来模拟在环境未知的情况下的决策制定。这些问题在以下论文中有所讨论：*实验设计的某些方面*，Robbins,
    H. (1952)，《美国数学学会公报》，55，527–535。
- en: At each time step, only one of the k levers is played and a stochastic reward
    is observed, where each play of a lever k generates independent and identically
    distributed samples resulting from some unknown distribution. The goal is to maximize
    the sum of the rewards that are obtained during all the time steps for a defined
    temporary interval.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步长中，只有一个杠杆被操作，并且观察到一个随机奖励，其中每次操作杠杆 k 会生成独立同分布的样本，这些样本来自某个未知的分布。目标是最大化在定义的时间区间内获得的所有奖励之和。
- en: Each algorithm that specifies which leverage should be played, given the past
    history of the rewards already obtained, represents our policy. The metric that's
    usually used to measure the performance of the latter is called regret, which
    is a measure that indicates how much less gain we get, in anticipation, following
    the chosen policy, than to follow the optimal one, in which the average of the
    distributions is known a priori of earnings.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每个指定应当选择哪个杠杆的算法，基于已获得的奖励历史，代表了我们的策略。通常用来衡量后者表现的度量是遗憾，它是衡量我们所选策略与最佳策略（已知收益的分布平均值）相比，预期损失的程度。
- en: Since these distributions are unknown to our policy, it is necessary to learn
    them through multiple plays, but at the same time, we also want to maximize the
    reward by choosing to play with machines that have already been rated as good.
    These two conflicting objectives – exploration of the unknown and exploitation
    of what is known – exemplify a fundamental trade-off that's present in a wide
    class of machine learning problems.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些分布对于我们的策略来说是未知的，因此需要通过多次操作来学习它们，但同时，我们也希望通过选择已经被评定为好的机器来最大化奖励。这两个冲突的目标——探索未知和利用已知——是广泛的机器学习问题中存在的一个基本权衡。
- en: Mathematical model
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数学模型
- en: A K-armed bandit problem is defined by the random variables *X[i, n]* for 1
    ≤ i ≤ K and n ≥ 1 (n is the number of plays), where i is the index that identifies
    a slot machine lever. By lowering the lever i, the rewards X[i, 1], X[i, 2], ·
    · · are obtained, which are independent and identically distributed according
    to an unknown probability law with an unknown expectation µi (expected value).
    Also, the rewards between different levers maintain independence, that is, X[i,
    s] and X[j, t] are independent (and generally not identically distributed) for
    each 1 ≤ i <j ≤ K and for every s, t ≥ 1\. This problem is formally equivalent
    to a one-state Markov decision-making process.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: K 臂赌博机问题由随机变量 *X[i, n]* 定义，其中 1 ≤ i ≤ K 和 n ≥ 1（n 是游戏次数），其中 i 是识别老虎机杠杆的索引。通过操作杠杆
    i，可以获得奖励 X[i, 1], X[i, 2], · · ·，这些奖励是独立且同分布的，遵循一个未知的概率分布，并具有未知的期望 µi（期望值）。此外，不同杠杆之间的奖励保持独立，即，对于每一个
    1 ≤ i < j ≤ K 和所有的 s, t ≥ 1，X[i, s] 和 X[j, t] 是独立的（且通常不是同分布的）。这个问题在形式上等价于一个单状态的马尔可夫决策过程。
- en: An allocation strategy is an algorithm that chooses the next lever to be lowered
    based on the sequence of previous bets and the rewards obtained. As we mentioned
    previously, the concept of regret is used to measure the performance of the model,
    which measures the accumulated loss of gains that are obtained by following the
    chosen policy.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 分配策略是一种算法，它根据之前投注的序列和获得的奖励来选择下一个要操作的杠杆。正如我们之前提到的，遗憾的概念被用来衡量模型的表现，衡量的是跟随所选策略获得的收益与最佳策略之间的累计损失。
- en: 'Let *T[i] (n)* be the number of times the lever *i* is played by the strategy
    in the first *n* plays. Here, the regret of the strategy after *n* plays is defined
    by the following formula:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 设 *T[i] (n)* 为策略在前 *n* 次游戏中操作杠杆 *i* 的次数。这里，策略在 *n* 次游戏后的遗憾定义如下公式：
- en: '![](img/2ec52c5a-667a-4124-aa7c-b4737119a8b7.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ec52c5a-667a-4124-aa7c-b4737119a8b7.png)'
- en: 'Here, we have the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有以下内容：
- en: '*µ* = max[i=1;:::k] µ[i]*'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*µ* = max[i=1;:::k] µ[i]*'
- en: '*Ε(T[k](T))* is the expectation about the number of times the policy will play
    machine k'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ε(T[k](T))* 是关于策略将在多少次中选择机器 k 的期望值。'
- en: The goal is to minimize this regret or, equivalently, to maximize the sum of
    the rewards obtained after n plays. In fact, we can interpret regret as the difference
    between the maximum possible gain (having thrown the best lever n times, by definition
    the one that returns µ* as a reward) and the actual gain.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是最小化这种遗憾，或者等价地，最大化n次操作后获得的奖励总和。事实上，我们可以将遗憾解释为最大可能收益与实际收益之间的差异（最大可能收益是通过定义中每次拉动最佳杆获得的µ*奖励）。
- en: In the next section, you will see practical examples of applications that can
    be addressed with this technology.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，你将看到可以通过这项技术解决的实际应用示例。
- en: Multi-armed bandit applications
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多臂老虎机应用
- en: So far, we have seen how to tackle a MAB problem from a mathematical point of
    view. What are the real applications that can be modeled in this way? We'll look
    at some examples in the following subsections.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经从数学角度看到了如何解决MAB问题。那么，哪些实际应用可以用这种方式建模呢？我们将在以下子章节中探讨一些示例。
- en: Online advertising
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线广告
- en: In online advertising, a MAB solution uses machine learning algorithms to dynamically
    allocate advertisements to pages of websites that are performing well, while avoiding
    ads that show lower performance. In theory, MABs should produce faster results
    since there is no need to wait for a single winning variant.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在在线广告中，MAB解决方案使用机器学习算法动态地将广告分配给表现良好的网站页面，同时避免显示表现较差的广告。从理论上讲，MAB应当能更快地得出结果，因为不需要等待单一获胜的变体。
- en: News allocation system
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新闻分配系统
- en: 'Another application of MAB concerns news sites: when a new user accesses the
    site, they must choose a news item from a series of articles and the site receives
    the reward every time the user clicks on the article. Since the site''s goal is
    to maximize the revenue, it wants to show the items that are most likely to get
    a click. Naturally, the choice depends on the user''s characteristics. The problem
    is that we do not know the probability that an article is clicked, which is the
    parameter we want to learn about. We can clearly see that the exploration and
    exploitation dilemma presented in the preceding scenario can be modeled as a MAB
    problem.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: MAB的另一个应用涉及新闻网站：当一个新用户访问网站时，他们必须从一系列文章中选择一个新闻项目，每次用户点击文章时，网站都会收到奖励。由于网站的目标是最大化收入，它希望展示最有可能被点击的内容。显然，这个选择依赖于用户的特征。问题在于，我们不知道一篇文章被点击的概率，这是我们希望学习的参数。我们可以清楚地看到，前述情境中展示的探索与开发的困境可以被建模为MAB问题。
- en: Health care
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 健康护理
- en: Various treatments are available in this area. The manager needs to decide which
    treatment to use while minimizing the patient's losses. The treatments are experimental
    and imply that the capacity of the treatment must be learned by performing it
    on the patients. The aforementioned problem can be modeled as a MAB problem where
    treatments such as arms and treatment efficiency must be learned. The manager
    can explore their arms to learn about their success rate (efficiency) or choose
    to exploit the arm with the best success rate so far.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一领域有多种治疗方法可供选择。管理者需要在最小化患者损失的同时决定使用哪种治疗方法。这些治疗方法是实验性的，意味着治疗的效果需要通过对患者的实施来学习。上述问题可以被建模为一个MAB问题，其中治疗方法如同“臂”一样，治疗效率也需要被学习。管理者可以通过探索不同的治疗方法来了解它们的成功率（效率），或者选择利用至今为止成功率最高的治疗方法。
- en: Staff recruitment
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人员招聘
- en: In the selection of new personnel, the application of this methodology represents
    a powerful tool that's available to employers or requesting services to complete
    activities in a timely and economical manner. The employer's goal is to maximize
    the number of tasks completed. The workers that are available act as weapons in
    this case since they have qualities that are not known to the employer. So, this
    problem can be posed as a MAB problem, in which the employer can explore their
    arms to learn their qualities, or choose to exploit the best arm that's been identified
    so far.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在新员工选拔中，应用这种方法论代表了一种强大的工具，雇主或服务需求方可以利用它按时且经济地完成任务。雇主的目标是最大化完成的任务数量。可用的员工在这种情况下充当“武器”，因为他们的特质对雇主来说是未知的。因此，这个问题可以被看作是一个MAB问题，雇主可以通过探索员工的特质来了解他们，或者选择利用迄今为止已识别的最佳员工。
- en: Selection of a financial portfolio
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 财务投资组合的选择
- en: 'The selection of an optimal portfolio is a typical decision problem, and as
    such, its solution consists of the following elements: the identification of a
    set of alternatives, using selection criteria to sort through the different possibilities,
    and the solution of the problem. In order to optimize a financial portfolio, we
    start by measuring the yield and risk of the products available. The risk-return
    variables can be considered two sides of the same coin since a certain level of
    risk will correspond to a given return.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最优投资组合的选择是一个典型的决策问题，因此它的解决方案包括以下要素：识别一组备选方案，使用选择标准对不同的可能性进行排序，以及问题的解决。为了优化金融投资组合，我们首先要衡量可用产品的收益和风险。风险与回报变量可以看作是同一枚硬币的两面，因为一定程度的风险将对应于一定的回报。
- en: The return can be defined as the sum of the results that are produced by the
    investment in relation to the capital employed, while the concept of risk can
    be translated into the degree of variability of returns associated with a given
    financial instrument. This problem can be modeled as a MAB problem with financial
    products such as arms and product performance as a result.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 收益可以定义为与所使用的资本相关的投资结果的总和，而风险的概念可以通过与特定金融工具相关的回报波动度来表示。这个问题可以被建模为一个多臂老虎机（MAB）问题，其中金融产品作为“臂”，产品表现作为结果。
- en: In the next section, we will learn how to estimate the action-value function
    in order to implement the MAB algorithm.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何估计动作价值函数，以实现多臂老虎机算法。
- en: Action-value implementation
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动作价值实现
- en: A general solution to the problem of reinforcement learning is to estimate a
    value function using the learning process. This function must be able to evaluate,
    through the sum of the rewards, the convenience or otherwise of a specific policy.
    To start, we will define the state-value function.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习问题的一般解决方案是通过学习过程估计一个价值函数。这个函数必须能够通过奖励的总和来评估一个特定策略的便利性或其他方面。首先，我们将定义状态价值函数。
- en: State-value function
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 状态价值函数
- en: A value function represents how good a state is for an agent. It is equal to
    the total reward that's expected for an agent from the status s. The value function
    depends on the policy that the agent selects the actions to be performed on.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 价值函数表示一个状态对智能体来说有多好。它等于从状态s开始，智能体所期望的总奖励。价值函数依赖于智能体选择要执行的动作的策略。
- en: 'A policy π associates the probability π (s, a) to the pair (s, a), thus returning
    the probability that the action a is executed in the state s. Based on this, we
    can define a value function Vπ (s) as the expected value of the total reinforcement
    R[t] following the policy π starting from the state s:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 策略π将概率π(s, a)与状态-动作对(s, a)关联，从而返回在状态s下执行动作a的概率。基于此，我们可以定义一个价值函数Vπ(s)，它是根据策略π从状态s开始的总奖励R[t]的期望值：
- en: '![](img/fadd9841-a3b9-4187-b6cc-f72c5f23dc87.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fadd9841-a3b9-4187-b6cc-f72c5f23dc87.png)'
- en: 'Here, the sequence of rt is generated following the π policy starting from
    the s state. In other words, the examples of the training pattern must guide the
    learning process toward the evaluation of the optimal π * policy, as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，序列rt是在状态s下按照策略π生成的。换句话说，训练模式的示例必须指导学习过程朝向评估最优策略π*，如下所示：
- en: '![](img/6ccd9d16-54bb-4191-a50e-6cfcb2f71ea2.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ccd9d16-54bb-4191-a50e-6cfcb2f71ea2.png)'
- en: 'Assuming δ (s, a) the function that determines the new state generated by the
    pair (s, a), we can perform a lookahead search to choose the best action starting
    from the s state since we can express the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 假设δ(s, a)是由状态-动作对(s, a)生成的新状态的函数，我们可以执行前瞻搜索，从状态s开始选择最佳动作，因为我们可以表达以下内容：
- en: '![](img/3a4e33aa-cecc-470e-9dd2-6eefbbc57fbc.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a4e33aa-cecc-470e-9dd2-6eefbbc57fbc.png)'
- en: 'Here, r (s, a) represents the reward that''s obtained from executing an action
    a in the state s. This solution is only acceptable if the functions are known,
    as shown in the following equation:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，r(s, a)表示在状态s下执行动作a所获得的奖励。只有在已知这些函数的情况下，这个解才是可接受的，如下方方程所示：
- en: '![](img/37dd3b9c-c5cb-42b3-862d-46671bd3d51e.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37dd3b9c-c5cb-42b3-862d-46671bd3d51e.png)'
- en: '![](img/3d4d0efc-3f05-4ee5-ae31-a01c95de43af.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d4d0efc-3f05-4ee5-ae31-a01c95de43af.png)'
- en: However, this scenario isn't always respected.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种情况并不总是被遵守。
- en: Now that we understand the role that's played by the state-value function, we
    can move on to the definition of the action-value function.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了状态价值函数的作用，我们可以继续定义动作价值函数。
- en: Action-value function
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动作价值函数
- en: 'When this scenario doesn''t happen, it is necessary to define a new function
    similar to Vπ ∗:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当这种情况没有发生时，需要定义一个类似于Vπ ∗的新函数：
- en: '![](img/ea4da394-538a-44c2-8b43-6ec90a2de576.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea4da394-538a-44c2-8b43-6ec90a2de576.png)'
- en: 'If the agent is able to estimate the function Q, it is possible to choose the
    optimal action s, even without knowing the function δ:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果智能体能够估计函数Q，那么即使不知道函数δ，也可以选择最优动作s：
- en: '![](img/d2fe1391-eca0-4b32-9e17-3b6737e94f11.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d2fe1391-eca0-4b32-9e17-3b6737e94f11.png)'
- en: The function Q is usually referred to as an action-value function. Following
    a policy π, the action-value-function returns the expected reward for using action
    a in a certain state s.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 函数Q通常被称为动作值函数。根据策略π，动作值函数返回在特定状态s下使用动作a的期望奖励。
- en: The main difference between the state-value and the action-value functions is
    that the Q value allows you – at least in the first phase – to take a different
    action than the one that was envisaged by the policy. This is because Q reasons
    in terms of total reward, so in a specific state, it can also return a reward
    lower than that paid by another action. The state-value function contains the
    value of reaching a certain state, while the action-value-function contains the
    value for choosing an action in a state. How is the best action chosen? Let's
    take a look.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 状态值函数和动作值函数之间的主要区别在于，Q值允许你——至少在第一阶段——采取与策略预期不同的动作。这是因为Q是根据总奖励来推理的，因此在特定状态下，它也可能返回比其他动作获得的奖励更低的奖励。状态值函数包含到达某一状态的价值，而动作值函数包含在某一状态下选择一个动作的价值。如何选择最佳动作呢？让我们一探究竟。
- en: Choosing an action
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择一个动作
- en: 'Suppose we select n-actions: a = a[1]… .a[n]. Each of these actions has its
    own value of the action-value function. Its estimated value at t-t[h] pitch (play)
    is Q[t] (a[k]). Recall that the true value of an action is the average reward
    received when that action is chosen. A natural way to estimate this value is to
    calculate the average of the rewards that was actually received when the action
    was chosen. In other words, if at the t-th game the action a was chosen k times
    before t, obtaining the rewards r[1], r[2], ..., r[ka], then its value is estimated
    to be as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们选择n个动作：a = a[1]… .a[n]。每个动作都有其自身的动作值函数值。在t-t[h]时刻（回合）其估计值为Q[t] (a[k])。回想一下，动作的真实值是当选择该动作时获得的平均奖励。估计这个值的自然方法是计算实际选择该动作时获得的奖励的平均值。换句话说，如果在第t回合选择了动作a，并且在t之前选择了k次，获得了奖励r[1],
    r[2], ..., r[ka]，那么它的估计值为：
- en: '![](img/8e395896-9a16-48b1-9d9f-89d358fd694c.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e395896-9a16-48b1-9d9f-89d358fd694c.png)'
- en: 'Here, we have the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有如下内容：
- en: For k[a] = 0, we set Q[t] (a[k]) to a default value, Q[0] (a[k]) = 0 (no estimate
    available).
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于k[a] = 0，我们将Q[t] (a[k])设置为默认值，Q[0] (a[k]) = 0（没有可用的估计）。
- en: For k[a] → ∞, Q[t] (a[k]) → Q ^* (a[k]) (for the law of large numbers).
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于k[a] → ∞，Q[t] (a[k]) → Q ^* (a[k])（根据大数法则）。
- en: In all of these cases, the action-value function is calculated as an average
    (sample-average method).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些情况下，动作值函数是作为平均值（样本平均法）计算的。
- en: Now that we've explored the basic concepts of this technology, we will move
    on and explore the possible solutions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了这项技术的基本概念，接下来我们将继续探索可能的解决方案。
- en: Understanding problem solution techniques
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解问题解决技巧
- en: 'So far, we have formalized the problem and we have seen what tools are available
    to make the choice of actions that give us the least regret. Now, we can formulate
    selection methods. We will look at the following problem-solving techniques in
    the following subsections:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将问题形式化，并且我们已经看到有哪些工具可以帮助我们选择那些能带来最小遗憾的动作。现在，我们可以制定选择方法。我们将在以下小节中探讨以下问题解决技巧：
- en: Greedy methods
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贪心方法
- en: Upper confidence bound
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上置信界
- en: Greedy methods
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贪心方法
- en: 'This problem, which is much more complex than it may seem, can be tackled by
    using a very naive strategy, though it''s not very effective:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题，比看起来更复杂，可以通过使用一个非常简单的策略来解决，尽管它并不非常有效：
- en: Initially, each of the levers is played.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最初，每个拉杆都会被试验。
- en: The lever is played that has returned, on average, the highest reward.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择返回平均最高奖励的拉杆。
- en: 'In this algorithm, we can observe the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个算法中，我们可以观察到以下内容：
- en: We allow the agent to have memory
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们允许智能体拥有记忆
- en: We store the value associated with different actions
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们存储与不同动作相关的值
- en: We choose the action that gave the greatest reward
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们选择给出最大奖励的动作
- en: 'The best action is called the **greedy action** and the algorithm based on
    this is called the **greedy** method. The following steps are performed:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳动作称为**贪婪动作**，基于这一动作的算法称为**贪婪**方法。执行以下步骤：
- en: 'At time step t, estimate a value for each action, as follows:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在时间步t，按照如下方式估计每个动作的值：
- en: '![](img/86e88567-c9b8-4e0b-8b8c-777cea11e2b1.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86e88567-c9b8-4e0b-8b8c-777cea11e2b1.png)'
- en: 'Select the action with the maximum value:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择具有最大值的动作：
- en: '![](img/7aff49af-1728-4099-8810-0c4a40e4f02b.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7aff49af-1728-4099-8810-0c4a40e4f02b.png)'
- en: In greedy methods, no alternative solutions are explored through this method.
    Why do we have to choose an action that doesn't look the best? This is because
    we explore different solutions since the reward is not deterministic. This implies
    that we could achieve more with other actions because what matters is not the
    instant reward but the sum of the rewards that's obtained. In the greedy solution,
    the algorithm is completely based on exploitation. To improve the performance
    of the model, it is necessary to introduce exploration. Let's see how.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在贪婪方法中，没有通过该方法探索其他替代解决方案。为什么我们必须选择一个看起来不是最好的动作？这是因为我们需要探索不同的解决方案，因为奖励并不是确定性的。这意味着我们可能通过其他动作获得更多的奖励，因为重要的不是瞬时的奖励，而是通过这些动作累计的奖励。在贪婪解法中，算法完全基于利用。为了提升模型的表现，必须引入探索。接下来我们看看怎么做。
- en: ε-greedy methods
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ε-贪婪方法
- en: Here, it is necessary to maintain a predisposition to explore different actions.
    Once again, we find ourselves dealing with a problem based on the exploration-exploitation
    dilemma we addressed in detail in [Chapter 2](aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml), *Building
    Blocks of Reinforcement Learning*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，需要保持一种倾向于探索不同动作的心态。我们再次面对一个基于探索与利用困境的问题，正如我们在[第2章](aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml)《强化学习的构建模块》中详细讨论过的那样，*探索与利用困境*。
- en: 'Ideally, the agent must explore all the possible actions for each state, finding
    the one that is actually most rewarded for exploiting it in achieving its goal.
    Thus, decision-making involves a fundamental choice:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，智能体必须探索每个状态下所有可能的动作，找出最能通过利用该动作来达成目标的动作。因此，决策过程涉及一个基本的选择：
- en: '**Exploitation**: Making the best decision given the current information'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用**：根据当前信息做出最佳决策'
- en: '**Exploration**: Collecting more information'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索**：收集更多信息'
- en: In this process, the best long-term strategy can lead to considerable sacrifices
    in the short term. Therefore, it is necessary to gather enough information to
    make the best decisions.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，最佳的长期策略可能会在短期内带来相当大的牺牲。因此，需要收集足够的信息来做出最佳决策。
- en: 'I suppose that, with probability ε, a different action is chosen. This action
    is chosen with a uniform probability between the n possible actions available.
    The following steps are performed:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设以概率ε，选择一个不同的动作。这个动作是以均匀概率从n个可用动作中选择的。执行以下步骤：
- en: 'At time step t, a value for each action is estimated, as follows:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在时间步t，按照如下方式估计每个动作的值：
- en: '![](img/723adfc6-c091-4d4d-a1fb-0802a8bfddc0.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/723adfc6-c091-4d4d-a1fb-0802a8bfddc0.png)'
- en: 'With probability 1- 𝜀, the action with the maximum value is selected, as follows:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以概率1-𝜖，选择最大值的动作，如下所示：
- en: '![](img/3d5ca4b6-5d36-44e8-b0a1-3ab2de6d6134.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d5ca4b6-5d36-44e8-b0a1-3ab2de6d6134.png)'
- en: With probability 𝜀, an action from all the actions with equal probability is
    selected randomly.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以概率𝜖，从所有动作中随机选择一个，且每个动作的概率相同。
- en: The parameter value that's used the most is 𝜀 =0.1, but this can vary depending
    on the context. In this approach, we introduce an element of exploration that
    improves performance. However, if two actions have a very small difference between
    their Q values, this algorithm will also choose the action that has a higher probability
    than the others.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的参数值是𝜖 = 0.1，但这可以根据具体情况有所不同。在这种方法中，我们引入了探索的元素，以改善性能。然而，如果两个动作的Q值差异非常小，这个算法也会选择一个概率比其他动作更高的动作。
- en: ε-greedy methods with a progressive decrease
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ε-贪婪方法与渐进递减
- en: The exploration that was introduced with the adoption of 𝜀 offers us the opportunity
    to experiment with options that, so far, are unknown. Nevertheless, the random
    component of the strategy means that actions that have already been taken that
    have received poor results can be explored again. Such inefficient exploration
    can be avoided by progressively reducing the random exploration component.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 引入𝜀进行探索使我们有机会尝试那些至今未知的选项。然而，策略中的随机成分意味着那些已经采取并且得到较差结果的行动仍然可能被重新探索。通过逐渐减少随机探索成分，可以避免这种低效的探索。
- en: In other words, by reducing the parameter ε over time, we could explore even
    less since we have gained confidence in the action, which has a strong potential
    of optimal value. This strategy offers a highly exploratory behavior in the beginning
    and a highly exploitative behavior in the end. Let's learn how to carry out exploitation
    and exploration together.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，通过随着时间的推移减少参数𝜖，我们可以减少探索，因为我们已经对具有强大潜力的最优值的行动产生了信心。该策略在开始时提供了高度的探索行为，而在最后则表现出高度的利用行为。让我们一起来学习如何同时进行利用与探索。
- en: Upper confidence bound
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上置信界
- en: At the beginning of the game, we don't know which the best arm is. Therefore,
    we cannot characterize any arm. Thus, the UCB algorithm states that all arms have
    the same observed average value. So, a confidence limit for each arm will be created
    and an arm will be selected at random.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏开始时，我们并不知道哪个臂是最好的。因此，我们无法对任何一个臂进行描述。因此，UCB算法假设所有臂的观察平均值都是相同的。于是，将为每个臂创建一个置信区间，并随机选择一个臂。
- en: In this context, each arm will either give a reward or won't. If the arm that's
    selected returns a mistake, the average that will be observed by the arm will
    decrease, as well as the confidence limit. If the arm that's selected returns
    a reward, the observed average and the confidence limit will increase. By taking
    advantage of the best, we are decreasing the confidence limit. Adding more and
    more rounds, the likelihood that the other arms are doing well also increases.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，每个臂要么给出奖励，要么不给。如果选中的臂返回错误，臂的观察平均值以及置信区间都会下降。如果选中的臂返回奖励，观察到的平均值和置信区间都会增加。通过利用最佳臂，我们降低了置信区间。随着更多回合的进行，其他臂表现良好的可能性也在增加。
- en: 'To implement this strategy, follow these steps:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实施这一策略，请按照以下步骤操作：
- en: 'At each round, two variables are computed:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每一轮中，都会计算两个变量：
- en: '*R[i](n)*: The sum of the rewards obtained by the lever *i* after *n* plays'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R[i](n)*：杠杆 *i* 在进行 *n* 次操作后获得的奖励总和'
- en: '*T**[i]**(n)*: The number of times the lever *i* is played by the strategy
    in the first *n* plays'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*T**[i]**(n)*：策略在前 *n* 次操作中选择杠杆 *i* 的次数'
- en: 'We calculate the average rewards obtained by the lever *i* after *n* plays
    using the following formula:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用以下公式计算在进行 *n* 次操作后杠杆 *i* 的平均奖励：
- en: '![](img/ec54ea62-c453-4a2c-b253-41aab5679c82.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec54ea62-c453-4a2c-b253-41aab5679c82.png)'
- en: 'We calculate the confidence interval after *n* plays using the following formula:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用以下公式计算在进行 *n* 次操作后的置信区间：
- en: '![](img/044dd621-9e6e-4245-8f96-f97a5124f599.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/044dd621-9e6e-4245-8f96-f97a5124f599.png)'
- en: 'We select the lever *i* that returns the maximum UCB as follows:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们按照以下方式选择返回最大UCB的杠杆 *i*：
- en: '![](img/6edfa64f-85e2-449f-a9a5-09f9aa3311b0.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6edfa64f-85e2-449f-a9a5-09f9aa3311b0.png)'
- en: Both of these algorithms keep track of how much they know of any available arm
    and pay no attention except to how much reward they have obtained from the arms.
    On the contrary, the algorithms that we've analyzed so far have under-explored
    the options whose initial experiences have not returned significant rewards, even
    if they do not have enough data to be sure of those arms.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种算法都记录它们对任何可用臂的了解程度，只关注它们从各个臂获得的奖励。与此相反，我们迄今为止分析的算法，对于那些初始经验没有返回显著奖励的选项，进行了不足的探索，即使它们没有足够的数据来确定这些臂。
- en: In the next section, we will introduce the concept of state, which represents
    a description of the environment that the agent can use to perform targeted actions.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍“状态”的概念，它代表了智能体可以用来执行有针对性行动的环境描述。
- en: Implementing the contextual approach
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现情境方法
- en: So far, in addressing the problem of the MAB, we have generated an action but
    we have not exploited any information on the state of the environment (context).
    The range of actions that are available to the agent consists of pulling one or
    more arms of the bandit. In this way, a reward of +1 or -1 is received.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在解决多臂老虎机（MAB）问题时，我们已经生成了一个动作，但我们并没有利用环境（上下文）的任何状态信息。代理可用的动作范围包括拉动一个或多个老虎机的臂。通过这种方式，可以获得+1或-1的奖励。
- en: The problem is considered to be solved if the agent chooses the arm that increasingly
    returns a positive reward. In this case, we can design an agent that completely
    ignores the state of the environment since, in effect, there is always only one
    immutable state.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果代理选择的臂不断返回正奖励，那么问题就被认为已解决。在这种情况下，我们可以设计一个完全忽视环境状态的代理，因为实际上，始终只有一个不可变的状态。
- en: In the contextual bandit, the concept of state is introduced, which represents
    a description of the environment that the agent can use to carry out targeted
    actions. This model extends the original one by linking the decision to the state
    of the environment.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在上下文老虎机中，引入了“状态”概念，它表示环境的描述，代理可以利用该描述执行有针对性的动作。该模型通过将决策与环境状态相联系，扩展了原始模型。
- en: 'The following diagram shows diagrams of the two models:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了这两个模型的示意图：
- en: '![](img/e65917dc-f7e7-4900-aa21-89c343e8bbec.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e65917dc-f7e7-4900-aa21-89c343e8bbec.png)'
- en: The original problem is substantially modified in the sense that instead of
    a single bandit, which we've considered so far, in the new formalization of the
    problem, there are more bandits. The state of the environment tells us what bandit
    we're dealing with and the agent's goal is to learn the best action for any available
    bandit. Since each bandit will have different probabilities of reward for each
    arm, our agent will have to learn to condition their action on the state of the
    environment. Unless they do, they will not get the maximum possible reward over
    time.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 原始问题在本质上发生了变化，因为在问题的新形式化中，不再只有一个老虎机（我们至今考虑的情况），而是有多个老虎机。环境的状态告诉我们正在处理哪个老虎机，代理的目标是学习针对任何可用老虎机的最佳动作。由于每个老虎机对每个臂的奖励概率不同，我们的代理需要学习根据环境状态来调整其行动。除非他们这样做，否则无法随着时间推移获得最大的奖励。
- en: With the **contextual bandit** model, you not only optimize the decision based
    on the previous observations but also personalize the decisions for each situation.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**上下文老虎机**模型，你不仅基于先前的观察优化决策，还可以根据每种情况个性化决策。
- en: 'From the preceding model, we can observe the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的模型中，我们可以观察到以下几点：
- en: The algorithm observes a context.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法观察一个上下文。
- en: The algorithm makes a decision by choosing an action from a series of alternative
    actions.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法通过从一系列备选动作中选择一个动作来做出决策。
- en: We can observe the result of this decision, which returns a reward.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以观察到该决策的结果，它返回一个奖励。
- en: The goal is to maximize the average reward.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标是最大化平均奖励。
- en: 'An example of applying this algorithm to the real world is the problem of selecting
    advertisements to be displayed on a website to optimize the clickthrough rate.
    The context is information about the user: where it comes from, information about
    the device that was used, pages of the site that were previously visited, geolocation,
    and so on. An action corresponds to the choice of which ad to display. One result
    is whether the user has clicked on a banner or not. A reward is binary: 0 if there
    is no click, 1 if there is a click. Now, let''s learn how to alternate the evaluation
    of the policy with the improvement of the policy.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 将该算法应用于现实世界的一个例子是选择要在网站上展示的广告，以优化点击率的问题。上下文是关于用户的信息：用户来自哪里、使用的设备信息、之前访问过的网站页面、地理位置等等。一个动作对应于选择展示哪条广告。一个结果是用户是否点击了广告横幅。奖励是二元的：如果没有点击，则奖励为0；如果点击，则奖励为1。现在，让我们学习如何交替评估策略与改进策略。
- en: Understanding asynchronous actor-critic agents
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解异步演员-评论员代理
- en: Actor–critic methods implement a generalized policy iteration, alternating between
    a policy evaluation and a policy improvement step. There are two closely related
    processes of actor improvement that aim at improving the current policy and critic
    evaluation by evaluating the current policy. If the process that's defined by
    the critic has the bootstrap, then the variance is reduced. By doing this, the
    learning of the algorithm becomes more stable with respect to the methods of the
    policy gradient.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Actor-Critic 方法实现了一个广义的策略迭代，交替进行策略评估和策略改进步骤。Actor 改进的两个紧密相关的过程旨在改善当前策略，而 Critic
    评估则是通过评估当前策略来实现的。如果 Critic 所定义的过程具有引导作用，那么方差就会减少。通过这样做，相比于策略梯度方法，算法的学习会变得更加稳定。
- en: These methods have the characteristic of separating the memory structure to
    make the policy independent of the value function. The policy block is known as
    an actor because it chooses actions, while the estimated value function is called
    the critic in the sense that it criticizes the actions that are performed by the
    policy that is being followed. From this, we understand that learning is an on-policy
    type – in fact, the critic learns and criticizes the work of politics.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法的特点是分离内存结构，使得策略与价值函数相互独立。策略模块被称为 Actor，因为它选择行动，而估计的价值函数则被称为 Critic，从这个角度看，Critic
    批评的是策略执行的动作。从这一点我们可以理解，学习是基于策略类型的——事实上，Critic 学习并批评当前策略的工作。
- en: 'We have already introduced the actor-critic model, so now, we will explain
    the term asynchronous. This is very simple and has effective intuitions, such
    as the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了 Actor-Critic 模型，现在我们将解释“异步”这一术语。这非常简单，并且具有有效的直觉，例子如下：
- en: The model has several agents exploring the environment at the same time (each
    agent has a copy of the entire environment).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型有多个智能体同时探索环境（每个智能体都有整个环境的副本）。
- en: The model gives different starting policies so that the agents are not related.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型给出不同的初始策略，使得各个智能体彼此独立。
- en: In the model, the global status is updated with the contributions of each agent
    and the process restarts.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在该模型中，全局状态会随着每个智能体的贡献而更新，整个过程会重新开始。
- en: In 2016, the Google DeepMind group proposed an algorithm named **asynchronous
    advantage actor-critic** (**A3C**). The algorithm proved to be faster and simpler
    than most existing algorithms.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年，Google DeepMind 小组提出了一种名为**异步优势 Actor-Critic**（**A3C**）的算法。该算法比大多数现有算法更快、更简洁。
- en: In this algorithm, multiple instances of agents are treated, which have been
    initialized differently in their separate environments. Each agent who begins
    to act and learn gathers their own experiences. These experiences are then used
    to update the global neural network shared by all agents. This network affects
    all the agent actions and each new experience of each agent improves the overall
    network faster. Because there are multiple instances of this agent, the training
    will be much faster and more effective.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在此算法中，处理了多个不同初始化的智能体实例，它们处于各自的环境中。每个开始行动并学习的智能体都会积累自己的经验。然后，这些经验将用于更新所有智能体共享的全局神经网络。该网络会影响所有智能体的行动，每个智能体的新经验会加速整体网络的更新。由于有多个实例的智能体，训练将变得更加快速和高效。
- en: In the next section, we will apply the concepts we've learned about so far by
    addressing a practical case.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将通过解决一个实际案例，应用迄今为止学到的概念。
- en: Online advertising using the MAB model
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 MAB 模型的在线广告
- en: Online advertising falls into the new media category and takes advantage of
    the web's ability to reach a significant number of people. Advertising plays a
    decisive role for companies, which can easily reach a wide audience with lower
    costs than traditional means. One of the main advantages of internet advertising
    is the traceability of results or the effect it has on the public. This happens
    thanks to the ad servers that, in the case of banners, measure the number of views
    and the effective number of users, clicks.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在线广告属于新媒体范畴，并利用了互联网能够接触到大量人群的优势。广告对公司至关重要，因为公司可以以比传统方式更低的成本，轻松接触到广泛的受众。互联网广告的主要优势之一是结果的可追踪性，或者说它对公众的影响。这得益于广告服务器，在横幅广告的情况下，它们可以衡量展示次数、实际用户数量和点击次数。
- en: 'In online advertising, we can distinguish between the following two types of
    macros:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在在线广告中，我们可以区分以下两种宏类型：
- en: Contextual
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文
- en: Behavioral advertising
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行为广告
- en: In contextual advertising, Google is the typical case where you can place ads
    according to the words on the page or the type of site or topic that characterizes
    the website. In behavioral advertising, we select the target using the information
    we've collected regarding the behavior of each user on the web and on the app
    (pages visited, searches made) in order to identify their interests and needs,
    and then submit advertisements in line with them.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在上下文广告中，谷歌是典型的案例，您可以根据页面上的词语或网站的类型或主题放置广告。在行为广告中，我们通过收集关于每个用户在网络和应用程序上的行为（访问的页面、进行的搜索）来选择目标，以识别他们的兴趣和需求，然后根据这些信息投放相关广告。
- en: 'In both cases, it is clear the reference to the context needs to interact with
    the environment. It''s also clear that we won''t know a priori how the user will
    behave before an advertisement. These problems can be addressed through a model
    based on MAB, as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，明显的是，引用上下文需要与环境交互。也显然，在展示广告之前，我们无法先知用户的行为。这些问题可以通过基于MAB的模型来解决，如下所示：
- en: The context is represented by the characteristics of visitors and web pages.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文由访问者和网页的特征表示。
- en: Arms are represented by the types of ads that are available.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手臂通过可用的广告类型来表示。
- en: An action is equivalent to the type of ad to be shown.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个动作等同于将要展示的广告类型。
- en: The rewards are returned by the visitor's behavior. By clicking on the ad shown,
    you receive a reward of 1, while by not clicking on the ad, you receive a reward
    of 0.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励由访问者的行为返回。通过点击展示的广告，您会收到奖励1，而不点击广告则会收到奖励0。
- en: To make this discussion as understandable as possible, we will limit the number
    of ads we want to evaluate to three and aim to find which strategy offers the
    maximum total click rate after a certain number of impressions. So, let's learn
    how to tackle this problem by adopting the contextual approach.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个讨论尽可能易于理解，我们将限制我们希望评估的广告数量为三个，并旨在找出在一定数量的展示后，哪个策略能提供最大的总点击率。所以，让我们通过采用上下文方法来学习如何解决这个问题。
- en: Implementing the contextual package
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现上下文包
- en: In the *Contextual approach* section, we said that, in the contextual bandit,
    the concept of state is introduced, which represents a description of the environment
    that the agent can use to carry out targeted actions. This model extends the original
    one by linking the decision to the state of the environment. In this section,
    we will see some examples of applications of the armed bandit problem addressed
    with the contextual approach. To do this, we will use the contextual package.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在*上下文方法*部分，我们提到，在上下文赌博机中，引入了状态的概念，表示代理可以用来执行有针对性行动的环境描述。该模型通过将决策与环境的状态联系起来，扩展了原始模型。在本节中，我们将看到一些使用上下文方法解决武装赌博机问题的应用示例。为此，我们将使用`contextual`包。
- en: This package facilitates the simulation and evaluation of context-free and contextual
    MAB policies or algorithms to ease the implementation, evaluation, and dissemination
    of existing and new bandit algorithms and policies.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 该包简化了无上下文和上下文MAB策略或算法的模拟和评估，便于现有和新算法及策略的实现、评估和传播。
- en: 'A brief description of the `diagram` package, which has been extracted from
    the official documentation, is shown in the following table:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`diagram`包的简要描述已从官方文档中提取，并显示在下表中：'
- en: '| Version | 0.9.8.2 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 版本 | 0.9.8.2 |'
- en: '| Date | 2019-07-08 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 日期 | 2019-07-08 |'
- en: '| Maintainer | Robin van Emden |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 维护者 | Robin van Emden |'
- en: '| License | GPL-3 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 许可证 | GPL-3 |'
- en: '| Authors | Robin van Emden, Maurits Kaptein |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 作者 | Robin van Emden, Maurits Kaptein |'
- en: 'The `contextual` package was presented by the authors in the following paper:
    van Emden, R. and Kaptein, M., 2018\. *Contextual: Evaluating Contextual Multi-Armed
    Bandit Problems in R*. arXiv preprint arXiv:1811.01926.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`contextual`包由作者在以下论文中提出：van Emden, R. 和 Kaptein, M., 2018\. *Contextual: Evaluating
    Contextual Multi-Armed Bandit Problems in R*. arXiv预印本 arXiv:1811.01926。'
- en: 'In this package, the MAB problems are addressed under the following assumptions:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在此包中，MAB问题是在以下假设下处理的：
- en: The bandit is a set of arms where each arm is defined by some reward function
    mapping dimensional context vector returning a reward for every time step until
    the horizon.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赌博机是一组手臂，每个手臂由某些奖励函数定义，这些奖励函数将维度上下文向量映射到每个时间步长的奖励，直到预定的时间范围。
- en: The function of politics is the maximization of the cumulative reward. This
    function is carried out by selecting one of the currently available bandit arms
    in the sequence.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 政策的功能是最大化累积奖励。这个功能是通过选择当前可用的赌博机臂之一来实现的。
- en: During the learning process, the policy observes the current state of the environment,
    which is represented by the vectors of the context characteristics. Afterward,
    the policy selects one of the available actions using an arm selection strategy.
    As a result, it receives a reward. This procedure allows the policy to update
    the selection of strategic arms. This procedure is then repeated *T* times.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在学习过程中，策略观察环境的当前状态，状态由上下文特征向量表示。之后，策略使用臂选择策略选择一个可用的动作。作为结果，它会获得一个奖励。这个过程允许策略更新策略臂的选择。此过程随后会重复*T*次。
- en: 'To represent what has been said in the algorithm, we can say that, for each
    round, a policy does the following:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示算法中所述的内容，我们可以说，对于每一轮，策略执行以下操作：
- en: Observes the current context feature vectors
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察当前上下文特征向量
- en: Selects an action
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个动作
- en: Receives a reward
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得奖励
- en: Updates the arm-selection strategy parameters
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新臂选择策略参数
- en: The goal of the policy is to optimize its cumulative reward.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 策略的目标是优化其累积奖励。
- en: Then, we apply the functions contained in the contextual package to a practical
    case.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将应用上下文包中包含的函数到实际案例中。
- en: Online advertising context-free policies
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线广告无上下文策略
- en: 'The first simulation we will perform will not consider the context. We will
    simply evaluate how many clicks each advertisement receives after a certain number
    of impressions. First, we need to set the initial settings. As anticipated, we
    will only consider three announcements that correspond in the MAB formulation
    to three arms of the bandit, each with a different probability of generating a
    click. Let''s get started:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进行的第一次模拟将不考虑上下文。我们将简单地评估在一定次数展示后，每个广告收到的点击量。首先，我们需要设置初始设置。正如预期的那样，我们只考虑三个公告，它们在MAB模型中对应于三条臂，每条臂都有不同的点击概率。让我们开始吧：
- en: 'We will use the following code to perform the analysis:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用以下代码进行分析：
- en: '[PRE0]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will analyze this code line by line. The first line loads the library:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将逐行分析这段代码。第一行加载了库：
- en: '[PRE1]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This library contains many functions that allow for the simulation and evaluation
    of context-free and contextual MAB policies or algorithms.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 该库包含许多函数，允许模拟和评估无上下文和有上下文的MAB策略或算法。
- en: 'Now, let''s fix some necessary parameters to set the problem as MAB:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们固定一些必要的参数，以将问题设定为MAB：
- en: '[PRE2]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The horizon is the number of rounds that must be played. Let''s set the number
    of simulations:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 视野是必须玩的轮次数。让我们设置模拟的次数：
- en: '[PRE3]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The number of simulations indicates how many times to repeat the simulation.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟次数表示重复模拟的次数。
- en: 'Let''s move on and set the probability that a user clicks on an advertisement:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续并设置用户点击广告的概率：
- en: '[PRE4]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A vector has been defined and contains the probabilities that each ad is clicked.
    In MAB terms, they represent the probabilities associated with the three arms. At
    this point, the initial parameters are fixed, so we can move on to defining the
    objects.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 已经定义了一个向量，包含每个广告被点击的概率。在MAB术语中，它们表示与三条臂相关联的概率。此时，初始参数已经固定，因此我们可以继续定义对象。
- en: 'The first will be the bandit:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个将是赌博机：
- en: '[PRE5]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, we used the `BasicBernoulliBandit()` function. This function simulates
    k Bernoulli arms, where each arm issues a reward of one with uniform probability
    p, otherwise a reward of zero. In a bandit scenario, this can be used to simulate
    a hit or miss event, such as if a user clicks on a headline, ad, or recommended
    product. Only one argument is expected (weights): it is a numeric vector that
    represents the probability of reward values for each of the bandit''s k arms.
    The `new()` method generates and instantiates a new `BasicBernoulliBandit` instance.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`BasicBernoulliBandit()`函数。这个函数模拟k个伯努利臂，其中每个臂以均匀概率p发放奖励1，否则发放奖励0。在一个赌博机场景中，这可以用来模拟一个命中或未命中的事件，比如用户是否点击了一个标题、广告或推荐的产品。只需要一个参数（权重）：它是一个数值向量，表示每个赌博机臂的奖励概率。`new()`方法生成并实例化一个新的`BasicBernoulliBandit`实例。
- en: 'Let''s move on to defining the policy:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来定义策略：
- en: '[PRE6]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, we used the `EpsilonFirstPolicy()` function to implement a naive policy
    where a pure exploration phase is followed by a pure exploitation phase. Exploration
    happens within the first N time steps that are defined. During this time, at each
    time step t, `EpsilonFirstPolicy` selects an arm at random. Exploitation happens
    in the following steps, where we select the best arm up until N for either the
    remaining N trials or horizon T. Here, we used the `new()` method, which generates
    a new `EpsilonFirstPolicy` object. Let''s move on:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们使用了 `EpsilonFirstPolicy()` 函数来实现一个简单的策略，其中纯探索阶段后跟纯利用阶段。在前 N 个时间步内进行探索。在此期间，每个时间步
    t，`EpsilonFirstPolicy` 会随机选择一个 arm。接下来的步骤中进行利用，我们选择最佳的 arm，直到 N 为止，或者在剩余的 N 次试验或时间步长
    T 内进行利用。这里，我们使用了 `new()` 方法来生成一个新的 `EpsilonFirstPolicy` 对象。接下来继续：
- en: '[PRE7]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The agent class is responsible for running one bandit/policy pair. The following
    arguments are available:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: agent 类负责运行一个 bandit/policy 配对。以下是可用的参数：
- en: '`policy`: A policy instance.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`policy`：一个策略实例。'
- en: '`bandit`: A bandit instance.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bandit`：一个 bandit 实例。'
- en: '`name character`: Sets the name of the Agent. If NULL (default), the agent
    generates a name based on its policy instance''s name.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name character`：设置 agent 的名称。如果为 NULL（默认值），则 agent 根据其策略实例的名称生成一个名称。'
- en: '`sparse numeric`: Artificially reduces the data size by setting a sparsity
    level for the current bandit and policy pair.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sparse numeric`：通过为当前的 bandit 和 policy 配对设置稀疏度水平，人工降低数据大小。'
- en: 'In our case, only two arguments are passed: `policy` and `bandit`. Let''s move
    on to running the simulation:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，只有两个参数被传递：`policy` 和 `bandit`。接下来我们来运行模拟：
- en: '[PRE8]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This is the entry point of any simulation. The `Simulator` class encloses one
    or more agents, creates an agent clone for each to be repeated simulation, runs
    the agents, and saves the log of all agent interactions in a history object. The
    following arguments are passed:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这是任何模拟的入口点。`Simulator` 类封装了一个或多个 agent，针对每个 agent 创建一个克隆进行重复模拟，运行这些 agent，并将所有
    agent 交互的日志保存在历史对象中。以下参数将被传递：
- en: '`agent`: An agent instance or a list of agent instances'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`agent`：一个 agent 实例或一个 agent 实例列表'
- en: '`horizon`: The number of pulls or time steps to run each agent, where t = 1,
    . . . , T (integer value)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`horizon`：运行每个 agent 的拉取次数或时间步数，其中 t = 1, . . . , T（整数值）'
- en: '`simulations`: How many times to repeat each agent''s simulation over t = 1,
    . . . , T, with a new seed on each repeat (integer value)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`simulations`：每个 agent 在 t = 1, . . . , T 的模拟重复次数，每次重复时使用新的随机种子（整数值）'
- en: '`do_parallel`: If running simulator processes in parallel (logical value)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_parallel`：是否并行运行模拟器进程（逻辑值）'
- en: 'For a detailed list of all the topics that are covered by the R6 class simulator,
    please refer to the official documentation: [https://CRAN.R-project.org/package=contextual](https://CRAN.R-project.org/package=contextual).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 R6 类模拟器覆盖的所有主题的详细列表，请参阅官方文档：[https://CRAN.R-project.org/package=contextual](https://CRAN.R-project.org/package=contextual)。
- en: 'Here, we used the `new()` method to generate a new simulator object. It''s
    time to run the simulation:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们使用了 `new()` 方法来生成一个新的模拟器对象。现在是时候运行模拟了：
- en: '[PRE9]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `run()` method simply runs a simulator instance. At this point, we have
    all the history of the simulation recorded in the history variable. Now, we can
    use this data to draw graphs. First, we will analyze the average reward:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`run()` 方法只是运行一个模拟器实例。在此时，我们已经将所有模拟历史记录保存在历史变量中。现在，我们可以利用这些数据绘制图表。首先，我们将分析平均奖励：'
- en: '[PRE10]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `plot()` function generates plots from the history data. The following
    plot types are available:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`plot()` 函数根据历史数据生成图表。以下是可用的图表类型：'
- en: '`cumulative`: Plots the cumulative regret or reward over time. If regret=TRUE
    is passed, a cumulative regret is returned; if regret=FALSE is passed, a cumulative
    reward is returned.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cumulative`：绘制累计遗憾或奖励随时间变化的曲线。如果传入regret=TRUE，则返回累计遗憾；如果传入regret=FALSE，则返回累计奖励。'
- en: '`average`: Plots the average regret or reward. If regret=TRUE is passed, a
    cumulative regret is returned; if regret=FALSE is passed, a cumulative reward
    is returned.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`average`：绘制平均遗憾或奖励的曲线。如果传入regret=TRUE，则返回累计遗憾；如果传入regret=FALSE，则返回累计奖励。'
- en: '`arms`: Plots the percentage of simulations per time step where each arm was
    chosen over time. If multiple agents have been run, it only plots the first agent.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`arms`：绘制每个时间步中每个 arm 被选择的模拟百分比。如果运行了多个 agent，则仅绘制第一个 agent。'
- en: 'The following plot is printed:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的图表将被打印：
- en: '![](img/24635d77-26f1-40f5-acbe-d84ea5160ff6.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24635d77-26f1-40f5-acbe-d84ea5160ff6.png)'
- en: 'Note that, after the first phase of exploration in which all the arms are tested
    in equal measure, we pass this to the exploitation phase, in which the arms that
    return the greatest rewards are preferred. The passage between the two phases
    can be seen by the net increase in the average reward. Let''s move on the cumulative
    plot:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在第一次探索阶段中，所有的臂都被平等测试后，我们进入了利用阶段，在该阶段中，返回最高奖励的臂会被优先选择。两个阶段之间的过渡可以通过平均奖励的净增加看到。接下来，让我们查看累积图：
- en: '[PRE11]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following plot is returned:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表被返回：
- en: '![](img/a7bc6048-9dcf-420a-85f0-305197e410d5.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7bc6048-9dcf-420a-85f0-305197e410d5.png)'
- en: 'The transition between the exploration and exploitation phases in this graph
    is even more evident. After the first 100 steps in which only the exploration
    phase has been used, the cumulative regret starts increasing with a logarithmic
    profile. Finally, we will plot the arms type:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图中探索阶段和利用阶段的过渡更为明显。在前 100 步中，只有探索阶段被使用，之后累积遗憾开始呈现对数型增长。最后，我们将绘制臂的类型：
- en: '[PRE12]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This type of plot returns the percentage of simulations per time step each
    arm was chosen over time. The following plot is printed:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的图表返回了每个时间步骤中每个臂被选择的模拟次数百分比。以下图表被打印出来：
- en: '![](img/0cf25153-6a72-484d-917b-fe1127469258.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0cf25153-6a72-484d-917b-fe1127469258.png)'
- en: From the preceding graph, we can see that the most chosen arm is the number
    3\. In fact, even in this case, after the first exploration phase in which the
    three arms are chosen with comparable percentages, the choice of arm that's passed
    to the exploitation phase falls exclusively on number 3.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，我们可以看到最常选择的臂是 3号臂。事实上，即使在这种情况下，在首次探索阶段中，三个臂的选择比例相近，但进入利用阶段后，选择的臂就完全集中在了
    3号臂上。
- en: Online advertising ε-greedy-based policies
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于 ε-greedy 策略的在线广告
- en: 'Now, we will address the same problem by adopting an ε-greedy policy. As we
    mentioned previously, with this approach, we introduce an element of exploration
    that improves performance. The following code shows the analysis process when
    using the ε-greedy approach:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过采用 ε-greedy 策略来解决同样的问题。正如我们之前提到的，使用这种方法，我们引入了探索元素，从而提高了性能。以下代码展示了使用
    ε-greedy 方法时的分析过程：
- en: '[PRE13]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We will analyze this code line by line. The first line loads the library:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐行分析这段代码。第一行加载了库：
- en: '[PRE14]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, let''s fix some necessary parameters to set the problem as MAB:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们设置一些必要的参数，以将问题设为 MAB：
- en: '[PRE15]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The horizon is the number of rounds that must be played. Let''s set the number
    of simulations:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: horizon 是必须进行的回合数。让我们设置模拟次数：
- en: '[PRE16]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let''s move on and set the probability that a user clicks on an advertisement:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置用户点击广告的概率：
- en: '[PRE17]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The same probability vector that we used in the previous example has been adopted.
    Let''s move on and define the objects:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了与前一个示例相同的概率向量。接下来，我们定义对象：
- en: '[PRE18]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To define the bandit, we used the `BasicBernoulliBandit()` function. This function simulates
    k Bernoulli arms, where each arm issues a reward of one with a uniform probability
    p, and otherwise a reward of zero. Let''s move on to defining the policy:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定义赌博机，我们使用了 `BasicBernoulliBandit()` 函数。这个函数模拟了 k 个伯努利臂，其中每个臂以均匀概率 p 发出奖励
    1，否则发出奖励 0。接下来，我们来定义策略：
- en: '[PRE19]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here, we used the `EpsilonGreedyPolicy()` function. This function chooses an
    arm at random with a probability epsilon in the exploration phase; otherwise,
    it greedily chooses the arm with the highest estimated reward in the exploitation
    phase. Only the epsilon argument is passed, indicating the probability that the
    arms are selected at random. The `new()` method is used to generate a new `EpsilonGreedyPolicy`
    object. Finally, we will create an agent object, as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们使用了 `EpsilonGreedyPolicy()` 函数。该函数在探索阶段以概率 epsilon 随机选择一个臂；否则，它会在利用阶段贪婪地选择回报最高的臂。只有
    epsilon 参数被传入，表示随机选择臂的概率。`new()` 方法用于生成一个新的 `EpsilonGreedyPolicy` 对象。最后，我们将创建一个代理对象，如下所示：
- en: '[PRE20]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The agent class is responsible for running one Bandit/Policy pair. Now, we
    can run the simulation:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 代理类负责运行一个 Bandit/Policy 配对。现在，我们可以运行模拟：
- en: '[PRE21]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following results are printed:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果被打印出来：
- en: '[PRE22]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, we can run the simulation:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以运行模拟：
- en: '[PRE23]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The history of the simulation that we carried out is recorded in the history
    variable. We can use this data to draw graphs. First, the regret average is plotted:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行的模拟历史记录保存在 history 变量中。我们可以使用这些数据来绘制图表。首先，绘制了遗憾平均值图：
- en: '[PRE24]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following plot is returned:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表被返回：
- en: '![](img/16345297-ced0-4d21-9e49-785df4a4b960.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16345297-ced0-4d21-9e49-785df4a4b960.png)'
- en: 'Then, we plot the cumulative regret:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们绘制累计遗憾图：
- en: '[PRE25]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following plot is returned:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下图表：
- en: '![](img/7d982342-a28b-48ef-998e-b4d89fb3be17.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d982342-a28b-48ef-998e-b4d89fb3be17.png)'
- en: 'Finally, the arm choice in percent is plotted:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，绘制了臂选择的百分比图：
- en: '[PRE26]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following plot is returned:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下图表：
- en: '![](img/22cd39d2-2341-4381-967e-3b0d35639a76.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22cd39d2-2341-4381-967e-3b0d35639a76.png)'
- en: In the preceding three graphs, a feature is highlighted (**Arm choice %**).
    The reduction of the regret over time is progressive and does not undergo a discontinuity
    like it did in the previous simulation (context-free policies-based example).
    This is due to the fact that the algorithm simultaneously carries out the exploration
    and exploitation phase. On one hand, it uniformly explores one of the advertisements
    randomly ε of the time, while on the other hand, it exploits the ad with the best
    current click rate 1-ε of the time.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面三个图中，突出了一个特征（**Arm choice %**）。遗憾随时间的减少是逐渐的，并不像之前的仿真（基于无上下文策略的例子）那样出现不连续。这是因为算法同时进行了探索和利用阶段。一方面，它在
    ε 的时间里随机均匀地探索某个广告，而另一方面，它在 1-ε 的时间里利用点击率最高的广告。
- en: Online advertising context-based policies
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线广告基于上下文的策略
- en: 'What happens if visitors are divided into two categories, male and female,
    or young and adult? We can introduce a reference to the context in the models
    we''ve analyzed so far. Recall that, in the contextual bandit, the concept of
    state was introduced, which represents a description of the environment that the
    agent can use to carry out targeted actions. This model extends the original one
    by linking the decision to the state of the environment. The following code performs
    an analysis using the context approach:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将访问者分为男性和女性，或者年轻人和成年人两类，会发生什么情况？我们可以在迄今为止分析的模型中引入对上下文的引用。回想一下，在上下文 Bandit
    模型中，引入了状态的概念，状态代表了代理可以用来执行有针对性行动的环境描述。该模型通过将决策与环境状态关联，扩展了原始模型。以下代码使用上下文方法进行分析：
- en: '[PRE27]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We will analyze this code line by line. The first line loads the library:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐行分析这段代码。第一行加载了库：
- en: '[PRE28]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, let''s fix some of the necessary parameters to set the problem as MAB:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们设置一些必要的参数，将问题设置为 MAB：
- en: '[PRE29]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The horizon is the number of rounds that must be played. Let''s set the number
    of simulations:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 横向轴表示必须进行的轮次数。我们来设置仿真次数：
- en: '[PRE30]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let''s move on and set the probability that a user clicks on an advertisement.
    In this case, the reference to a binary context is introduced. Two possible probability
    distributions are defined, both of which correspond to two user profiles:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 继续设置用户点击广告的概率。在这种情况下，引入了对二元上下文的引用。定义了两种可能的概率分布，它们分别对应两种用户画像：
- en: '[PRE31]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The first profile has a probability vector equal to (0.1, 0.3, 0.7), while
    the second has a probability vector equal to (0.8, 0.4, 0.1). Let''s move on and
    define the objects:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个画像的概率向量为 (0.1, 0.3, 0.7)，而第二个画像的概率向量为 (0.8, 0.4, 0.1)。接下来，让我们继续定义对象：
- en: '[PRE32]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'To define the bandit, we used the `ContextualBinaryBandit()` function. This
    function simulates a contextual Bernoulli MAB problem, where at least one context
    feature is active at a time. In this case, the weights argument contains a d x
    k numeric matrix with probabilities of reward for d contextual features per k arms. Let''s
    move on and define the policy:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定义 Bandit，我们使用了 `ContextualBinaryBandit()` 函数。该函数模拟了一个上下文 Bernoulli MAB 问题，其中至少有一个上下文特征在任何时刻是活动的。在这种情况下，`weights`
    参数包含一个 d x k 的数值矩阵，其中包含了每个 k 个臂的 d 个上下文特征的奖励概率。接下来，让我们继续定义策略：
- en: '[PRE33]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here, we used the `EpsilonGreedyPolicy()` function. Finally, we will create
    an agent object, as follows:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了 `EpsilonGreedyPolicy()` 函数。最后，我们将创建一个代理对象，如下所示：
- en: '[PRE34]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The agent class is responsible for running one Bandit/Policy pair. Now, we
    can run the simulation:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 代理类负责运行一个 Bandit/Policy 配对。现在，我们可以运行仿真：
- en: '[PRE35]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The following results are printed:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 输出以下结果：
- en: '[PRE36]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, we can store the simulation:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以存储仿真结果：
- en: '[PRE37]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The history of the simulation is recorded in the history variable. Now, we
    can use this data to draw graphs. In this case, only the arm choice in percent
    is plotted:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 仿真的历史记录保存在 history 变量中。现在，我们可以使用这些数据绘制图表。在此情况下，仅绘制了臂选择的百分比：
- en: '[PRE38]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The following plot is returned:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下图表：
- en: '![](img/d992f12f-5ab6-4a6c-b505-3067ad9155b7.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d992f12f-5ab6-4a6c-b505-3067ad9155b7.png)'
- en: From this analysis, it is clear that the most chosen arm in percentage is number
    1, then number 3, and finally number 2.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个分析中可以看出，最常选择的臂是第1号臂，然后是第3号臂，最后是第2号臂。
- en: Comparison between solution techniques
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决技术比较
- en: 'Several policies are available for the solution of the MAB problem. In the* Problem
    solution techniques* section, we analyzed some of them. The contextual package proposes
    some policies. The following is a list of those available policies:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 解决MAB问题有几种策略。在*问题解决技术*部分，我们分析了一些策略。上下文包提出了一些策略。以下是可用策略的列表：
- en: '`ContextualEpochGreedyPolicy`'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`上下文时期贪心策略`'
- en: '`ContextualEpsilonGreedyPolicy`'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`上下文ε-贪心策略`'
- en: '`ContextualLogitBTSPolicy`'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`上下文LogitBTS策略`'
- en: '`` `ContextualTSProbitPolicy` ``'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`` `上下文TSProbit策略` ``'
- en: '`EpsilonFirstPolicy`'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ε-贪心策略`'
- en: '`EpsilonGreedyPolicy`'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ε-贪心策略`'
- en: '`Exp3Policy`'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Exp3策略`'
- en: '`FixedPolicy`'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`固定策略`'
- en: '`GittinsBrezziLaiPolicy`'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GittinsBrezziLai策略`'
- en: '`GradientPolicy`'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`梯度策略`'
- en: '`LifPolicy`'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Lif策略`'
- en: '`LinUCBDisjointOptimizedPolicy`'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LinUCB不相交优化策略`'
- en: '`LinUCBDisjointPolicy`'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LinUCB不相交策略`'
- en: '`LinUCBGeneralPolicy`'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LinUCB通用策略`'
- en: '`LinUCBHybridOptimizedPolicy`'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LinUCB混合优化策略`'
- en: '`LinUCBHybridPolicy`'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LinUCB混合策略`'
- en: '`OraclePolicy`'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Oracle策略`'
- en: '`RandomPolicy`'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`随机策略`'
- en: '`SoftmaxPolicy`'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Softmax策略`'
- en: '`ThompsonSamplingPolicy`'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`汤普森采样策略`'
- en: '`UCB1Policy`'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UCB1策略`'
- en: '`UCB2Policy`'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UCB2策略`'
- en: 'For a detailed description of these policies, please refer to the official
    documentation of the package, which is available at the following URL: [https://CRAN.R-project.org/package=contextual](https://cran.r-project.org/package=contextual).'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 有关这些策略的详细描述，请参阅该包的官方文档，文档可通过以下网址访问：[https://CRAN.R-project.org/package=contextual](https://cran.r-project.org/package=contextual)。
- en: 'Some of these have already been adopted in the examples we''ve looked at in
    this chapter. To analyze the characteristics of some of these, we can make a comparison
    based on the results we''ve obtained. Here is the full code:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些策略已经在我们本章查看的示例中采用过。为了分析这些策略的特点，我们可以基于已经获得的结果进行比较。以下是完整的代码：
- en: '[PRE39]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We will analyze this code line by line. The first line loads the library:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐行分析这段代码。第一行加载了库：
- en: '[PRE40]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, let''s fix some of the necessary parameters to set the problem as MAB:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们设置一些必要的参数，将问题设置为MAB：
- en: '[PRE41]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let''s move on and set the probability that a user clicks on an advertisement:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续设置用户点击广告的概率：
- en: '[PRE42]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let''s move on and define the bandit object:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们继续定义赌博机对象：
- en: '[PRE43]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Here, we used the `BasicBernoulliBandit()` function. Now, we will create a
    list of agent objects, as follows:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们使用了`BasicBernoulliBandit()`函数。现在，我们将创建一个代理对象的列表，如下所示：
- en: '[PRE44]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Six agents were defined with different policies, as follows:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了六个具有不同策略的代理，如下所示：
- en: '`OraclePolicy`: This policy knows the reward probabilities at all times, and
    will always play the optimal arm. It is often used as a baseline to compare other
    policies.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Oracle策略`：此策略始终知道奖励概率，并将始终选择最优臂。它通常作为基准，用于与其他策略进行比较。'
- en: '`UCB1Policy`: This policy constructs an optimistic estimate in the form of
    an Upper Confidence Bound to create an estimate of the expected payoff of each
    action and picks the action with the highest estimate. If the guess is wrong,
    the optimistic guess quickly decreases until another action has a higher estimate.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UCB1策略`：此策略构建了一个乐观的估计，以上置信区间的形式创建每个动作的预期回报估计，并选择具有最高估计值的动作。如果估计错误，乐观估计会迅速下降，直到另一个动作的估计值更高。'
- en: '`ThompsonSamplingPolicy`: The procedure that''s followed by this policy exploits
    the memory of the average rewards of the weapons. To do this, use a beta-binomial
    model with alpha and beta parameters, sample the values for each arm from the
    previous step, and select the arm with the highest value. When an arm is pulled
    and a Bernoulli reward is observed, it modifies the prior based on the reward.
    This procedure is repeated for the next arm pull.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`汤普森采样策略`：此策略的过程利用了武器的平均奖励记忆。为此，使用带有alpha和beta参数的beta-二项式模型，从上一步中为每个臂采样值，并选择具有最高值的臂。当拉动一个臂并观察到一个伯努利奖励时，它根据奖励修改先验。此过程会为下一次拉臂重复。'
- en: '`EpsilonGreedyPolicy`: The procedure that''s followed by this policy foresees
    the random choice of an arm with epsilon probability to explore the environment.
    Otherwise, an arm is chosen greedily, with the highest estimated reward. By doing
    this, the agent exploits the information that was acquired in the previous steps.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EpsilonGreedyPolicy`：该策略遵循的程序是以 epsilon 概率随机选择一个臂来探索环境。否则，它会贪婪地选择估计奖励最高的臂。通过这种方式，代理可以利用前一步获得的信息。'
- en: '`SoftmaxPolicy`: This policy selects an arm based on the probability from the
    Boltmann distribution. It makes use of a temperature parameter, tau, which specifies
    how many arms we can explore. When tau is high, all the arms are explored equally,
    and, when tau is low, the arms offering higher rewards will be chosen.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SoftmaxPolicy`：该策略基于 Boltmann 分布中的概率选择一个臂。它使用了一个温度参数 tau，指定我们可以探索多少个臂。当 tau
    较高时，所有臂的探索机会相同；而当 tau 较低时，系统会选择那些提供更高奖励的臂。'
- en: '`Exp3Policy`: The procedure that''s followed by this policy uses a probability
    distribution, which is a mixture of a uniform distribution. It also uses a distribution
    that assigns each action an exponential mass of probability in the cumulative
    reward that was estimated for that action.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Exp3Policy`：该策略遵循的程序使用概率分布，它是均匀分布的混合物。它还使用一种分布，将每个动作的概率质量分配在该动作的累积奖励的指数值上。'
- en: 'Now, we can run the simulation and store its history:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以运行仿真并存储其历史记录：
- en: '[PRE45]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Finally, we will plot the cumulative regrets of the agents that we simulated:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将绘制我们模拟的代理的累积悔恨值：
- en: '[PRE46]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The following plot is returned:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下图表：
- en: '![](img/dd38e685-12b0-4d9c-be60-e1c1a832ae86.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd38e685-12b0-4d9c-be60-e1c1a832ae86.png)'
- en: As we anticipated, the OraclePolicy is the one that presents the lowest values
    in absolute of the regret, which means it succeeds in making better use of the
    arm that supplies better results.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们预期的那样，OraclePolicy 在绝对悔恨值上表现最低，这意味着它成功地更好地利用了提供更好结果的臂。
- en: Summary
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have learned about the basic concepts of the multi-armed
    bandit model. This model is based on the dilemma of exploration and exploitation.
    In the case of limited resources, which is what we base our choices on, it is
    essential to know which competitive alternatives allow us to maximize the expected
    profit. The name derives from the example of a player struggling with a row of
    slot machines, who must decide whether to continue with the current machine or
    try another machine. A mathematical model of the problem was described. Then,
    we discovered the meaning of the action-value implementation and how it differs
    from the value function. The state-value-function contains the value of reaching
    a certain state, while the action-value-function contains the value for choosing
    an action in a state.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了多臂强盗模型的基本概念。该模型基于探索与开发的困境。在资源有限的情况下，基于我们做出选择的标准，了解哪些竞争性替代方案可以最大化预期利润至关重要。该名称源于一个例子，描述了一位玩家在面对一排老虎机时的困境，他必须决定是否继续使用当前机器还是尝试另一台机器。我们描述了该问题的数学模型。然后，我们了解了行动价值实现的含义，并探讨了它与价值函数的区别。状态值函数包含到达某个状态的值，而行动值函数包含在某个状态下选择一个行动的值。
- en: Several problem solution techniques were analyzed and the contextual approach
    was addressed. We also looked at a list of practical applications of the MAB problem.
    Finally, online advertising was treated using several MAB models.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 分析了几种问题求解技术，并讨论了上下文方法。我们还查看了 MAB 问题的一些实际应用列表。最后，使用几个 MAB 模型处理了在线广告问题。
- en: In the next chapter, we will learn about the basic concepts of optimization
    techniques. We will learn how to decompose a problem into subproblems and how
    to implement the various optimization techniques. Then, we will understand the
    difference between recursion and memoization and discover how to use the dynamic
    programming approach to make the most convenient choices.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习优化技术的基本概念。我们将学习如何将一个问题分解为子问题，并如何实现各种优化技术。然后，我们将理解递归与记忆化的区别，并发现如何使用动态规划方法做出最合适的选择。
