- en: Multi-Armed Bandit Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šè‡‚è€è™æœºæ¨¡å‹
- en: The multi-armed bandit problem is a classic reinforcement learning problem that
    exemplifies the exploration versus exploitation dilemma. When we have a limited
    set of resources to base our choices on, it becomes essential to adopt a method
    to establish which of the alternative competing choices allow us to maximize the
    expected profit. The name **multi**-**armed bandit** derives from the example
    of a gambler struggling with a row of slot machines who must decide whether to
    continue with the current machine or try a different machine.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šè‡‚è€è™æœºé—®é¢˜æ˜¯ä¸€ä¸ªç»å…¸çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œä½“ç°äº†æ¢ç´¢ä¸åˆ©ç”¨çš„å›°å¢ƒã€‚å½“æˆ‘ä»¬åªèƒ½ä¾èµ–æœ‰é™çš„èµ„æºæ¥åšå‡ºé€‰æ‹©æ—¶ï¼Œé‡‡ç”¨ä¸€ç§æ–¹æ³•æ¥ç¡®å®šå“ªäº›ç«äº‰é€‰æ‹©å¯ä»¥æœ€å¤§åŒ–é¢„æœŸåˆ©æ¶¦å˜å¾—è‡³å…³é‡è¦ã€‚**å¤š**è‡‚**è€è™æœº**è¿™ä¸ªåå­—æºäºä¸€ä¸ªèµŒå¾’åœ¨ä¸€æ’è€è™æœºä¸­æŒ£æ‰ï¼Œä»–å¿…é¡»å†³å®šæ˜¯ç»§ç»­ä½¿ç”¨å½“å‰çš„æœºå™¨ï¼Œè¿˜æ˜¯å°è¯•ä¸åŒçš„æœºå™¨ã€‚
- en: In this chapter, we will get an overview of the basic concepts of the multi-armed
    bandit model, discover the different techniques that are available to help resolve
    this problem, and discover the meaning of the action-value implementation. Then,
    we will learn how to address this problem using a contextual approach, how to
    implement asynchronous actor-critic agents, and how to implement a multi-armed
    bandit problem in R.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å°†æ¦‚è¿°å¤šè‡‚è€è™æœºæ¨¡å‹çš„åŸºæœ¬æ¦‚å¿µï¼Œæ¢ç´¢å¯ç”¨äºè§£å†³æ­¤é—®é¢˜çš„ä¸åŒæŠ€æœ¯ï¼Œå¹¶æ·±å…¥äº†è§£è¡ŒåŠ¨-ä»·å€¼å®ç°çš„å«ä¹‰ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨æƒ…å¢ƒæ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¦‚ä½•å®ç°å¼‚æ­¥æ¼”å‘˜-è¯„è®ºå®¶ä»£ç†ï¼Œä»¥åŠå¦‚ä½•åœ¨Rä¸­å®ç°å¤šè‡‚è€è™æœºé—®é¢˜ã€‚
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å°†æ¶µç›–ä»¥ä¸‹ä¸»é¢˜ï¼š
- en: Multi-armed bandit model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤šè‡‚è€è™æœºæ¨¡å‹
- en: Multi-armed bandit applications
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤šè‡‚è€è™æœºåº”ç”¨
- en: Action-value implementation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¡ŒåŠ¨-ä»·å€¼å®ç°
- en: Understanding problem solution techniques
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç†è§£é—®é¢˜è§£å†³æŠ€æœ¯
- en: Implementing the contextual approach
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ç°æƒ…å¢ƒæ–¹æ³•
- en: Understanding asynchronous actor-critic agents
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç†è§£å¼‚æ­¥æ¼”å‘˜-è¯„è®ºå®¶ä»£ç†
- en: Online advertising using the MAB model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨MABæ¨¡å‹è¿›è¡Œåœ¨çº¿å¹¿å‘Š
- en: Multi-armed bandit model
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šè‡‚è€è™æœºæ¨¡å‹
- en: A common problem in learning theory is to identify the best option among a set
    of options, without knowing a priori what benefits each one can give, while minimizing
    the cost of doing so.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç†è®ºä¸­çš„ä¸€ä¸ªå¸¸è§é—®é¢˜æ˜¯ï¼Œåœ¨ä¸çŸ¥é“æ¯ä¸ªé€‰é¡¹èƒ½æä¾›ä½•ç§æ”¶ç›Šçš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•åœ¨ä¸€ç»„é€‰é¡¹ä¸­è¯†åˆ«æœ€ä½³é€‰é¡¹ï¼ŒåŒæ—¶æœ€å°åŒ–é€‰æ‹©çš„æˆæœ¬ã€‚
- en: The **multi-armed bandit** (**MAB**) problem takes its name from a known problem
    faced in decision theory. A gambler must choose which slot machine to play among
    the many he has in front of him. After playing, he will have a certain degree
    of knowledge about the rewards that are distributed by some machines, but he will
    not know anything about the others, so he will be forced to choose between machines
    that are partly known and machines that are not known.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¤šè‡‚è€è™æœº**ï¼ˆ**MAB**ï¼‰é—®é¢˜å¾—åäºå†³ç­–ç†è®ºä¸­é‡åˆ°çš„ä¸€ä¸ªè‘—åé—®é¢˜ã€‚ä¸€ä¸ªèµŒå¾’å¿…é¡»ä»ä»–é¢å‰çš„å¤šå°è€è™æœºä¸­é€‰æ‹©ä¸€å°è¿›è¡Œæ¸¸æˆã€‚æ¸¸æˆåï¼Œä»–å°†è·å¾—å…³äºæŸäº›æœºå™¨å¥–åŠ±çš„æŸç§ç¨‹åº¦çš„çŸ¥è¯†ï¼Œä½†ä»–å¯¹å…¶ä»–æœºå™¨ä¸€æ— æ‰€çŸ¥ï¼Œå› æ­¤ä»–å°†è¢«è¿«åœ¨éƒ¨åˆ†å·²çŸ¥çš„æœºå™¨å’Œå®Œå…¨æœªçŸ¥çš„æœºå™¨ä¹‹é—´åšå‡ºé€‰æ‹©ã€‚'
- en: This problem is ideal for modeling the compromises between the exploitation
    of known opportunities and the exploration of unknown opportunities, as well as
    to test strategies in the presence of a high degree of ignorance and uncertainty.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé—®é¢˜éå¸¸é€‚åˆæ¨¡æ‹Ÿå·²çŸ¥æœºä¼šçš„åˆ©ç”¨ä¸æœªçŸ¥æœºä¼šçš„æ¢ç´¢ä¹‹é—´çš„å¦¥åï¼Œä¹Ÿå¯ä»¥åœ¨é«˜åº¦æ— çŸ¥å’Œä¸ç¡®å®šçš„æƒ…å†µä¸‹æµ‹è¯•ç­–ç•¥ã€‚
- en: In more technical terms, each slot machine is modeled as a probability distribution,
    with an average value and one standard deviation. These two parameters can vary
    over time, either dependently or independently, for example, to model the evolution
    over time or the dynamism of the competitive scenario, or in response to choices
    that are made by one or more players to model competition or influence on the
    context.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æŠ€æœ¯æœ¯è¯­ä¸Šè®²ï¼Œæ¯å°è€è™æœºéƒ½è¢«å»ºæ¨¡ä¸ºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œå…·æœ‰ä¸€ä¸ªå¹³å‡å€¼å’Œä¸€ä¸ªæ ‡å‡†å·®ã€‚è¿™ä¸¤ä¸ªå‚æ•°å¯ä»¥éšæ—¶é—´å˜åŒ–ï¼Œå¯èƒ½æ˜¯ç›¸äº’ä¾èµ–çš„ï¼Œä¹Ÿå¯èƒ½æ˜¯ç‹¬ç«‹å˜åŒ–çš„ï¼Œä¾‹å¦‚ï¼Œæ¨¡æ‹Ÿéšæ—¶é—´çš„æ¼”å˜æˆ–ç«äº‰åœºæ™¯çš„åŠ¨æ€ï¼Œæˆ–è€…å“åº”ä¸€ä¸ªæˆ–å¤šä¸ªç©å®¶çš„é€‰æ‹©ï¼Œä»¥æ¨¡æ‹Ÿç«äº‰æˆ–å¯¹æƒ…å¢ƒçš„å½±å“ã€‚
- en: The distribution of probabilities is obviously not known to the players but
    can be learned over time as values â€‹â€‹are obtained from each slot machine.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ¦‚ç‡åˆ†å¸ƒæ˜¾ç„¶å¯¹ç©å®¶æ¥è¯´æ˜¯æœªçŸ¥çš„ï¼Œä½†éšç€æ¯å°è€è™æœºå¥–åŠ±çš„è·å–ï¼Œç©å®¶å¯ä»¥é€æ¸å­¦ä¹ è¿™äº›åˆ†å¸ƒã€‚
- en: MAB problems were introduced by H. Robbins to model decision-making under uncertainty
    when the environment is unknown. These problems were treated in the following
    paper:Â *Some Aspects of the Sequential Design of Experiments*, Robbins, H. (1952),Â Bulletin
    of the American Mathematical Society, 55, 527â€“535.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: MAB é—®é¢˜æ˜¯ç”± H. Robbins æå‡ºçš„ï¼Œç”¨æ¥æ¨¡æ‹Ÿåœ¨ç¯å¢ƒæœªçŸ¥çš„æƒ…å†µä¸‹çš„å†³ç­–åˆ¶å®šã€‚è¿™äº›é—®é¢˜åœ¨ä»¥ä¸‹è®ºæ–‡ä¸­æœ‰æ‰€è®¨è®ºï¼š*å®éªŒè®¾è®¡çš„æŸäº›æ–¹é¢*ï¼ŒRobbins,
    H. (1952)ï¼Œã€Šç¾å›½æ•°å­¦å­¦ä¼šå…¬æŠ¥ã€‹ï¼Œ55ï¼Œ527â€“535ã€‚
- en: At each time step, only one of the k levers is played and a stochastic reward
    is observed, where each play of a lever k generates independent and identically
    distributed samples resulting from some unknown distribution. The goal is to maximize
    the sum of the rewards that are obtained during all the time steps for a defined
    temporary interval.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ªæ—¶é—´æ­¥é•¿ä¸­ï¼Œåªæœ‰ä¸€ä¸ªæ æ†è¢«æ“ä½œï¼Œå¹¶ä¸”è§‚å¯Ÿåˆ°ä¸€ä¸ªéšæœºå¥–åŠ±ï¼Œå…¶ä¸­æ¯æ¬¡æ“ä½œæ æ† k ä¼šç”Ÿæˆç‹¬ç«‹åŒåˆ†å¸ƒçš„æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬æ¥è‡ªæŸä¸ªæœªçŸ¥çš„åˆ†å¸ƒã€‚ç›®æ ‡æ˜¯æœ€å¤§åŒ–åœ¨å®šä¹‰çš„æ—¶é—´åŒºé—´å†…è·å¾—çš„æ‰€æœ‰å¥–åŠ±ä¹‹å’Œã€‚
- en: Each algorithm that specifies which leverage should be played, given the past
    history of the rewards already obtained, represents our policy. The metric that's
    usually used to measure the performance of the latter is called regret, which
    is a measure that indicates how much less gain we get, in anticipation, following
    the chosen policy, than to follow the optimal one, in which the average of the
    distributions is known a priori of earnings.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæŒ‡å®šåº”å½“é€‰æ‹©å“ªä¸ªæ æ†çš„ç®—æ³•ï¼ŒåŸºäºå·²è·å¾—çš„å¥–åŠ±å†å²ï¼Œä»£è¡¨äº†æˆ‘ä»¬çš„ç­–ç•¥ã€‚é€šå¸¸ç”¨æ¥è¡¡é‡åè€…è¡¨ç°çš„åº¦é‡æ˜¯é—æ†¾ï¼Œå®ƒæ˜¯è¡¡é‡æˆ‘ä»¬æ‰€é€‰ç­–ç•¥ä¸æœ€ä½³ç­–ç•¥ï¼ˆå·²çŸ¥æ”¶ç›Šçš„åˆ†å¸ƒå¹³å‡å€¼ï¼‰ç›¸æ¯”ï¼Œé¢„æœŸæŸå¤±çš„ç¨‹åº¦ã€‚
- en: Since these distributions are unknown to our policy, it is necessary to learn
    them through multiple plays, but at the same time, we also want to maximize the
    reward by choosing to play with machines that have already been rated as good.
    These two conflicting objectivesÂ â€“ exploration of the unknown and exploitation
    of what is knownÂ â€“ exemplify a fundamental trade-off that's present in a wide
    class of machine learning problems.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¿™äº›åˆ†å¸ƒå¯¹äºæˆ‘ä»¬çš„ç­–ç•¥æ¥è¯´æ˜¯æœªçŸ¥çš„ï¼Œå› æ­¤éœ€è¦é€šè¿‡å¤šæ¬¡æ“ä½œæ¥å­¦ä¹ å®ƒä»¬ï¼Œä½†åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå¸Œæœ›é€šè¿‡é€‰æ‹©å·²ç»è¢«è¯„å®šä¸ºå¥½çš„æœºå™¨æ¥æœ€å¤§åŒ–å¥–åŠ±ã€‚è¿™ä¸¤ä¸ªå†²çªçš„ç›®æ ‡â€”â€”æ¢ç´¢æœªçŸ¥å’Œåˆ©ç”¨å·²çŸ¥â€”â€”æ˜¯å¹¿æ³›çš„æœºå™¨å­¦ä¹ é—®é¢˜ä¸­å­˜åœ¨çš„ä¸€ä¸ªåŸºæœ¬æƒè¡¡ã€‚
- en: Mathematical model
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°å­¦æ¨¡å‹
- en: A K-armed bandit problem is defined by the random variables *X[i, n]* for 1
    â‰¤ i â‰¤ K and n â‰¥ 1 (n is the number of plays), where i is the index that identifies
    a slot machine lever. By lowering the lever i, the rewards X[i, 1], X[i, 2], Â·
    Â· Â· are obtained, which are independent and identically distributed according
    to an unknown probability law with an unknown expectation Âµi (expected value).
    Also, the rewards between different levers maintain independence, that is, X[i,
    s] and X[j, t] are independent (and generally not identically distributed) for
    each 1 â‰¤ i <j â‰¤ K and for every s, t â‰¥ 1\. This problem is formally equivalent
    to a one-state Markov decision-making process.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: K è‡‚èµŒåšæœºé—®é¢˜ç”±éšæœºå˜é‡ *X[i, n]* å®šä¹‰ï¼Œå…¶ä¸­ 1 â‰¤ i â‰¤ K å’Œ n â‰¥ 1ï¼ˆn æ˜¯æ¸¸æˆæ¬¡æ•°ï¼‰ï¼Œå…¶ä¸­ i æ˜¯è¯†åˆ«è€è™æœºæ æ†çš„ç´¢å¼•ã€‚é€šè¿‡æ“ä½œæ æ†
    iï¼Œå¯ä»¥è·å¾—å¥–åŠ± X[i, 1], X[i, 2], Â· Â· Â·ï¼Œè¿™äº›å¥–åŠ±æ˜¯ç‹¬ç«‹ä¸”åŒåˆ†å¸ƒçš„ï¼Œéµå¾ªä¸€ä¸ªæœªçŸ¥çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶å…·æœ‰æœªçŸ¥çš„æœŸæœ› Âµiï¼ˆæœŸæœ›å€¼ï¼‰ã€‚æ­¤å¤–ï¼Œä¸åŒæ æ†ä¹‹é—´çš„å¥–åŠ±ä¿æŒç‹¬ç«‹ï¼Œå³ï¼Œå¯¹äºæ¯ä¸€ä¸ª
    1 â‰¤ i < j â‰¤ K å’Œæ‰€æœ‰çš„ s, t â‰¥ 1ï¼ŒX[i, s] å’Œ X[j, t] æ˜¯ç‹¬ç«‹çš„ï¼ˆä¸”é€šå¸¸ä¸æ˜¯åŒåˆ†å¸ƒçš„ï¼‰ã€‚è¿™ä¸ªé—®é¢˜åœ¨å½¢å¼ä¸Šç­‰ä»·äºä¸€ä¸ªå•çŠ¶æ€çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ã€‚
- en: An allocation strategy is an algorithm that chooses the next lever to be lowered
    based on the sequence of previous bets and the rewards obtained. As we mentioned
    previously, the concept of regret is used to measure the performance of the model,
    which measures the accumulated loss of gains that are obtained by following the
    chosen policy.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†é…ç­–ç•¥æ˜¯ä¸€ç§ç®—æ³•ï¼Œå®ƒæ ¹æ®ä¹‹å‰æŠ•æ³¨çš„åºåˆ—å’Œè·å¾—çš„å¥–åŠ±æ¥é€‰æ‹©ä¸‹ä¸€ä¸ªè¦æ“ä½œçš„æ æ†ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ï¼Œé—æ†¾çš„æ¦‚å¿µè¢«ç”¨æ¥è¡¡é‡æ¨¡å‹çš„è¡¨ç°ï¼Œè¡¡é‡çš„æ˜¯è·Ÿéšæ‰€é€‰ç­–ç•¥è·å¾—çš„æ”¶ç›Šä¸æœ€ä½³ç­–ç•¥ä¹‹é—´çš„ç´¯è®¡æŸå¤±ã€‚
- en: 'Let *T[i] (n)* be the number of times the lever *i* is played by the strategy
    in the first *n* plays. Here, the regret of the strategy after *n* plays is defined
    by the following formula:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ *T[i] (n)* ä¸ºç­–ç•¥åœ¨å‰ *n* æ¬¡æ¸¸æˆä¸­æ“ä½œæ æ† *i* çš„æ¬¡æ•°ã€‚è¿™é‡Œï¼Œç­–ç•¥åœ¨ *n* æ¬¡æ¸¸æˆåçš„é—æ†¾å®šä¹‰å¦‚ä¸‹å…¬å¼ï¼š
- en: '![](img/2ec52c5a-667a-4124-aa7c-b4737119a8b7.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ec52c5a-667a-4124-aa7c-b4737119a8b7.png)'
- en: 'Here, we have the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰ä»¥ä¸‹å†…å®¹ï¼š
- en: '*Âµ* = max[i=1;:::k] Âµ[i]*'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Âµ* = max[i=1;:::k] Âµ[i]*'
- en: '*Î•(T[k](T))* is the expectation about the number of times the policy will play
    machine k'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Î•(T[k](T))* æ˜¯å…³äºç­–ç•¥å°†åœ¨å¤šå°‘æ¬¡ä¸­é€‰æ‹©æœºå™¨ k çš„æœŸæœ›å€¼ã€‚'
- en: The goal is to minimize this regret or, equivalently, to maximize the sum of
    the rewards obtained after n plays. In fact, we can interpret regret as the difference
    between the maximum possible gain (having thrown the best lever n times, by definition
    the one that returns Âµ* as a reward) and the actual gain.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ˜¯æœ€å°åŒ–è¿™ç§é—æ†¾ï¼Œæˆ–è€…ç­‰ä»·åœ°ï¼Œæœ€å¤§åŒ–næ¬¡æ“ä½œåè·å¾—çš„å¥–åŠ±æ€»å’Œã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å°†é—æ†¾è§£é‡Šä¸ºæœ€å¤§å¯èƒ½æ”¶ç›Šä¸å®é™…æ”¶ç›Šä¹‹é—´çš„å·®å¼‚ï¼ˆæœ€å¤§å¯èƒ½æ”¶ç›Šæ˜¯é€šè¿‡å®šä¹‰ä¸­æ¯æ¬¡æ‹‰åŠ¨æœ€ä½³æ†è·å¾—çš„Âµ*å¥–åŠ±ï¼‰ã€‚
- en: In the next section, you will see practical examples of applications that can
    be addressed with this technology.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€éƒ¨åˆ†ï¼Œä½ å°†çœ‹åˆ°å¯ä»¥é€šè¿‡è¿™é¡¹æŠ€æœ¯è§£å†³çš„å®é™…åº”ç”¨ç¤ºä¾‹ã€‚
- en: Multi-armed bandit applications
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šè‡‚è€è™æœºåº”ç”¨
- en: So far, we have seen how to tackle a MAB problem from a mathematical point of
    view. What are the real applications that can be modeled in this way? We'll look
    at some examples in the following subsections.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»ä»æ•°å­¦è§’åº¦çœ‹åˆ°äº†å¦‚ä½•è§£å†³MABé—®é¢˜ã€‚é‚£ä¹ˆï¼Œå“ªäº›å®é™…åº”ç”¨å¯ä»¥ç”¨è¿™ç§æ–¹å¼å»ºæ¨¡å‘¢ï¼Ÿæˆ‘ä»¬å°†åœ¨ä»¥ä¸‹å­ç« èŠ‚ä¸­æ¢è®¨ä¸€äº›ç¤ºä¾‹ã€‚
- en: Online advertising
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨çº¿å¹¿å‘Š
- en: In online advertising, a MAB solution uses machine learning algorithms to dynamically
    allocate advertisements to pages of websites that are performing well, while avoiding
    ads that show lower performance. In theory, MABs should produce faster results
    since there is no need to wait for a single winning variant.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åœ¨çº¿å¹¿å‘Šä¸­ï¼ŒMABè§£å†³æ–¹æ¡ˆä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•åŠ¨æ€åœ°å°†å¹¿å‘Šåˆ†é…ç»™è¡¨ç°è‰¯å¥½çš„ç½‘ç«™é¡µé¢ï¼ŒåŒæ—¶é¿å…æ˜¾ç¤ºè¡¨ç°è¾ƒå·®çš„å¹¿å‘Šã€‚ä»ç†è®ºä¸Šè®²ï¼ŒMABåº”å½“èƒ½æ›´å¿«åœ°å¾—å‡ºç»“æœï¼Œå› ä¸ºä¸éœ€è¦ç­‰å¾…å•ä¸€è·èƒœçš„å˜ä½“ã€‚
- en: News allocation system
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–°é—»åˆ†é…ç³»ç»Ÿ
- en: 'Another application of MAB concerns news sites: when a new user accesses the
    site, they must choose a news item from a series of articles and the site receives
    the reward every time the user clicks on the article. Since the site''s goal is
    to maximize the revenue, it wants to show the items that are most likely to get
    a click. Naturally, the choice depends on the user''s characteristics. The problem
    is that we do not know the probability that an article is clicked, which is the
    parameter we want to learn about. WeÂ can clearly see that the exploration and
    exploitation dilemma presented in the preceding scenarioÂ can be modeled as a MAB
    problem.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: MABçš„å¦ä¸€ä¸ªåº”ç”¨æ¶‰åŠæ–°é—»ç½‘ç«™ï¼šå½“ä¸€ä¸ªæ–°ç”¨æˆ·è®¿é—®ç½‘ç«™æ—¶ï¼Œä»–ä»¬å¿…é¡»ä»ä¸€ç³»åˆ—æ–‡ç« ä¸­é€‰æ‹©ä¸€ä¸ªæ–°é—»é¡¹ç›®ï¼Œæ¯æ¬¡ç”¨æˆ·ç‚¹å‡»æ–‡ç« æ—¶ï¼Œç½‘ç«™éƒ½ä¼šæ”¶åˆ°å¥–åŠ±ã€‚ç”±äºç½‘ç«™çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–æ”¶å…¥ï¼Œå®ƒå¸Œæœ›å±•ç¤ºæœ€æœ‰å¯èƒ½è¢«ç‚¹å‡»çš„å†…å®¹ã€‚æ˜¾ç„¶ï¼Œè¿™ä¸ªé€‰æ‹©ä¾èµ–äºç”¨æˆ·çš„ç‰¹å¾ã€‚é—®é¢˜åœ¨äºï¼Œæˆ‘ä»¬ä¸çŸ¥é“ä¸€ç¯‡æ–‡ç« è¢«ç‚¹å‡»çš„æ¦‚ç‡ï¼Œè¿™æ˜¯æˆ‘ä»¬å¸Œæœ›å­¦ä¹ çš„å‚æ•°ã€‚æˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°ï¼Œå‰è¿°æƒ…å¢ƒä¸­å±•ç¤ºçš„æ¢ç´¢ä¸å¼€å‘çš„å›°å¢ƒå¯ä»¥è¢«å»ºæ¨¡ä¸ºMABé—®é¢˜ã€‚
- en: Health care
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¥åº·æŠ¤ç†
- en: Various treatments are available in this area. The manager needs to decide which
    treatment to use while minimizing the patient's losses. The treatments are experimental
    and imply that the capacity of the treatment must be learned by performing it
    on the patients. The aforementioned problem can be modeled as a MAB problem where
    treatments such as arms and treatment efficiency must be learned. The manager
    can explore their arms to learn about their success rate (efficiency) or choose
    to exploit the arm with the best success rate so far.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€é¢†åŸŸæœ‰å¤šç§æ²»ç–—æ–¹æ³•å¯ä¾›é€‰æ‹©ã€‚ç®¡ç†è€…éœ€è¦åœ¨æœ€å°åŒ–æ‚£è€…æŸå¤±çš„åŒæ—¶å†³å®šä½¿ç”¨å“ªç§æ²»ç–—æ–¹æ³•ã€‚è¿™äº›æ²»ç–—æ–¹æ³•æ˜¯å®éªŒæ€§çš„ï¼Œæ„å‘³ç€æ²»ç–—çš„æ•ˆæœéœ€è¦é€šè¿‡å¯¹æ‚£è€…çš„å®æ–½æ¥å­¦ä¹ ã€‚ä¸Šè¿°é—®é¢˜å¯ä»¥è¢«å»ºæ¨¡ä¸ºä¸€ä¸ªMABé—®é¢˜ï¼Œå…¶ä¸­æ²»ç–—æ–¹æ³•å¦‚åŒâ€œè‡‚â€ä¸€æ ·ï¼Œæ²»ç–—æ•ˆç‡ä¹Ÿéœ€è¦è¢«å­¦ä¹ ã€‚ç®¡ç†è€…å¯ä»¥é€šè¿‡æ¢ç´¢ä¸åŒçš„æ²»ç–—æ–¹æ³•æ¥äº†è§£å®ƒä»¬çš„æˆåŠŸç‡ï¼ˆæ•ˆç‡ï¼‰ï¼Œæˆ–è€…é€‰æ‹©åˆ©ç”¨è‡³ä»Šä¸ºæ­¢æˆåŠŸç‡æœ€é«˜çš„æ²»ç–—æ–¹æ³•ã€‚
- en: Staff recruitment
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: äººå‘˜æ‹›è˜
- en: In the selection of new personnel, the application of this methodology represents
    a powerful tool that's available to employers or requesting services to complete
    activities in a timely and economical manner. The employer's goal is to maximize
    the number of tasks completed. The workers that are available act as weapons in
    this case since they have qualities that are not known to the employer. So, this
    problem can be posed as a MAB problem, in which the employer can explore their
    arms to learn their qualities, or choose to exploit the best arm that's been identified
    so far.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ–°å‘˜å·¥é€‰æ‹”ä¸­ï¼Œåº”ç”¨è¿™ç§æ–¹æ³•è®ºä»£è¡¨äº†ä¸€ç§å¼ºå¤§çš„å·¥å…·ï¼Œé›‡ä¸»æˆ–æœåŠ¡éœ€æ±‚æ–¹å¯ä»¥åˆ©ç”¨å®ƒæŒ‰æ—¶ä¸”ç»æµåœ°å®Œæˆä»»åŠ¡ã€‚é›‡ä¸»çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–å®Œæˆçš„ä»»åŠ¡æ•°é‡ã€‚å¯ç”¨çš„å‘˜å·¥åœ¨è¿™ç§æƒ…å†µä¸‹å……å½“â€œæ­¦å™¨â€ï¼Œå› ä¸ºä»–ä»¬çš„ç‰¹è´¨å¯¹é›‡ä¸»æ¥è¯´æ˜¯æœªçŸ¥çš„ã€‚å› æ­¤ï¼Œè¿™ä¸ªé—®é¢˜å¯ä»¥è¢«çœ‹ä½œæ˜¯ä¸€ä¸ªMABé—®é¢˜ï¼Œé›‡ä¸»å¯ä»¥é€šè¿‡æ¢ç´¢å‘˜å·¥çš„ç‰¹è´¨æ¥äº†è§£ä»–ä»¬ï¼Œæˆ–è€…é€‰æ‹©åˆ©ç”¨è¿„ä»Šä¸ºæ­¢å·²è¯†åˆ«çš„æœ€ä½³å‘˜å·¥ã€‚
- en: Selection of a financial portfolio
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è´¢åŠ¡æŠ•èµ„ç»„åˆçš„é€‰æ‹©
- en: 'The selection of an optimal portfolio is a typical decision problem, and as
    such, its solution consists of the following elements: the identification of a
    set of alternatives, using selection criteria to sort through the different possibilities,
    and the solution of the problem. In order to optimize a financial portfolio, we
    start by measuring the yield and risk of the products available. The risk-return
    variables can be considered two sides of the same coin since a certain level of
    risk will correspond to a given return.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ä¼˜æŠ•èµ„ç»„åˆçš„é€‰æ‹©æ˜¯ä¸€ä¸ªå…¸å‹çš„å†³ç­–é—®é¢˜ï¼Œå› æ­¤å®ƒçš„è§£å†³æ–¹æ¡ˆåŒ…æ‹¬ä»¥ä¸‹è¦ç´ ï¼šè¯†åˆ«ä¸€ç»„å¤‡é€‰æ–¹æ¡ˆï¼Œä½¿ç”¨é€‰æ‹©æ ‡å‡†å¯¹ä¸åŒçš„å¯èƒ½æ€§è¿›è¡Œæ’åºï¼Œä»¥åŠé—®é¢˜çš„è§£å†³ã€‚ä¸ºäº†ä¼˜åŒ–é‡‘èæŠ•èµ„ç»„åˆï¼Œæˆ‘ä»¬é¦–å…ˆè¦è¡¡é‡å¯ç”¨äº§å“çš„æ”¶ç›Šå’Œé£é™©ã€‚é£é™©ä¸å›æŠ¥å˜é‡å¯ä»¥çœ‹ä½œæ˜¯åŒä¸€æšç¡¬å¸çš„ä¸¤é¢ï¼Œå› ä¸ºä¸€å®šç¨‹åº¦çš„é£é™©å°†å¯¹åº”äºä¸€å®šçš„å›æŠ¥ã€‚
- en: The return can be defined as the sum of the results that are produced by the
    investment in relation to the capital employed, while the concept of risk can
    be translated into the degree of variability of returns associated with a given
    financial instrument. This problem can be modeled as a MAB problem with financial
    products such as arms and product performance as a result.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¶ç›Šå¯ä»¥å®šä¹‰ä¸ºä¸æ‰€ä½¿ç”¨çš„èµ„æœ¬ç›¸å…³çš„æŠ•èµ„ç»“æœçš„æ€»å’Œï¼Œè€Œé£é™©çš„æ¦‚å¿µå¯ä»¥é€šè¿‡ä¸ç‰¹å®šé‡‘èå·¥å…·ç›¸å…³çš„å›æŠ¥æ³¢åŠ¨åº¦æ¥è¡¨ç¤ºã€‚è¿™ä¸ªé—®é¢˜å¯ä»¥è¢«å»ºæ¨¡ä¸ºä¸€ä¸ªå¤šè‡‚è€è™æœºï¼ˆMABï¼‰é—®é¢˜ï¼Œå…¶ä¸­é‡‘èäº§å“ä½œä¸ºâ€œè‡‚â€ï¼Œäº§å“è¡¨ç°ä½œä¸ºç»“æœã€‚
- en: In the next section, we will learn how to estimate the action-value function
    in order to implement the MAB algorithm.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä¼°è®¡åŠ¨ä½œä»·å€¼å‡½æ•°ï¼Œä»¥å®ç°å¤šè‡‚è€è™æœºç®—æ³•ã€‚
- en: Action-value implementation
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ¨ä½œä»·å€¼å®ç°
- en: A general solution to the problem of reinforcement learning is to estimateÂ a
    value function using the learning process. This function must be able to evaluate,
    through the sum of the rewards, the convenience or otherwise of a specific policy.
    To start, we will define the state-value function.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ é—®é¢˜çš„ä¸€èˆ¬è§£å†³æ–¹æ¡ˆæ˜¯é€šè¿‡å­¦ä¹ è¿‡ç¨‹ä¼°è®¡ä¸€ä¸ªä»·å€¼å‡½æ•°ã€‚è¿™ä¸ªå‡½æ•°å¿…é¡»èƒ½å¤Ÿé€šè¿‡å¥–åŠ±çš„æ€»å’Œæ¥è¯„ä¼°ä¸€ä¸ªç‰¹å®šç­–ç•¥çš„ä¾¿åˆ©æ€§æˆ–å…¶ä»–æ–¹é¢ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†å®šä¹‰çŠ¶æ€ä»·å€¼å‡½æ•°ã€‚
- en: State-value function
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: çŠ¶æ€ä»·å€¼å‡½æ•°
- en: A value function represents how good a state is for an agent. It is equal to
    the total reward that's expected for an agent from the status s. The value function
    depends on the policy that the agent selects the actions to be performed on.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä»·å€¼å‡½æ•°è¡¨ç¤ºä¸€ä¸ªçŠ¶æ€å¯¹æ™ºèƒ½ä½“æ¥è¯´æœ‰å¤šå¥½ã€‚å®ƒç­‰äºä»çŠ¶æ€så¼€å§‹ï¼Œæ™ºèƒ½ä½“æ‰€æœŸæœ›çš„æ€»å¥–åŠ±ã€‚ä»·å€¼å‡½æ•°ä¾èµ–äºæ™ºèƒ½ä½“é€‰æ‹©è¦æ‰§è¡Œçš„åŠ¨ä½œçš„ç­–ç•¥ã€‚
- en: 'A policy Ï€ associates the probability Ï€ (s, a) to the pair (s, a),Â thus returning
    the probability that the action a is executed in the state s.Â Based on this, we
    can define a value function VÏ€ (s) as the expected value of the total reinforcement
    R[t] following the policy Ï€ starting from the state s:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥Ï€å°†æ¦‚ç‡Ï€(s, a)ä¸çŠ¶æ€-åŠ¨ä½œå¯¹(s, a)å…³è”ï¼Œä»è€Œè¿”å›åœ¨çŠ¶æ€sä¸‹æ‰§è¡ŒåŠ¨ä½œaçš„æ¦‚ç‡ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªä»·å€¼å‡½æ•°VÏ€(s)ï¼Œå®ƒæ˜¯æ ¹æ®ç­–ç•¥Ï€ä»çŠ¶æ€så¼€å§‹çš„æ€»å¥–åŠ±R[t]çš„æœŸæœ›å€¼ï¼š
- en: '![](img/fadd9841-a3b9-4187-b6cc-f72c5f23dc87.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fadd9841-a3b9-4187-b6cc-f72c5f23dc87.png)'
- en: 'Here, the sequence of rt is generated following the Ï€ policy starting from
    the s state. In other words, the examples of the training pattern must guide the
    learning process toward the evaluation of the optimal Ï€ * policy, as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œåºåˆ—rtæ˜¯åœ¨çŠ¶æ€sä¸‹æŒ‰ç…§ç­–ç•¥Ï€ç”Ÿæˆçš„ã€‚æ¢å¥è¯è¯´ï¼Œè®­ç»ƒæ¨¡å¼çš„ç¤ºä¾‹å¿…é¡»æŒ‡å¯¼å­¦ä¹ è¿‡ç¨‹æœå‘è¯„ä¼°æœ€ä¼˜ç­–ç•¥Ï€*ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](img/6ccd9d16-54bb-4191-a50e-6cfcb2f71ea2.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ccd9d16-54bb-4191-a50e-6cfcb2f71ea2.png)'
- en: 'Assuming Î´ (s, a) the function that determinesÂ the new state generated by the
    pair (s, a), we can perform a lookahead search to choose the best action starting
    from the s state since we can express the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾Î´(s, a)æ˜¯ç”±çŠ¶æ€-åŠ¨ä½œå¯¹(s, a)ç”Ÿæˆçš„æ–°çŠ¶æ€çš„å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œå‰ç»æœç´¢ï¼Œä»çŠ¶æ€så¼€å§‹é€‰æ‹©æœ€ä½³åŠ¨ä½œï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥è¡¨è¾¾ä»¥ä¸‹å†…å®¹ï¼š
- en: '![](img/3a4e33aa-cecc-470e-9dd2-6eefbbc57fbc.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a4e33aa-cecc-470e-9dd2-6eefbbc57fbc.png)'
- en: 'Here, r (s, a) represents the reward that''s obtained from executing an action
    aÂ in the state s. This solution is only acceptable if the functions are known,
    as shown in the following equation:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œr(s, a)è¡¨ç¤ºåœ¨çŠ¶æ€sä¸‹æ‰§è¡ŒåŠ¨ä½œaæ‰€è·å¾—çš„å¥–åŠ±ã€‚åªæœ‰åœ¨å·²çŸ¥è¿™äº›å‡½æ•°çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸ªè§£æ‰æ˜¯å¯æ¥å—çš„ï¼Œå¦‚ä¸‹æ–¹æ–¹ç¨‹æ‰€ç¤ºï¼š
- en: '![](img/37dd3b9c-c5cb-42b3-862d-46671bd3d51e.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37dd3b9c-c5cb-42b3-862d-46671bd3d51e.png)'
- en: '![](img/3d4d0efc-3f05-4ee5-ae31-a01c95de43af.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d4d0efc-3f05-4ee5-ae31-a01c95de43af.png)'
- en: However, this scenario isn't always respected.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¿™ç§æƒ…å†µå¹¶ä¸æ€»æ˜¯è¢«éµå®ˆã€‚
- en: Now that we understand the role that's played by the state-value function, we
    can move on to the definition of the action-value function.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ç†è§£äº†çŠ¶æ€ä»·å€¼å‡½æ•°çš„ä½œç”¨ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­å®šä¹‰åŠ¨ä½œä»·å€¼å‡½æ•°ã€‚
- en: Action-value function
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ¨ä½œä»·å€¼å‡½æ•°
- en: 'When this scenario doesn''t happen, it is necessary to define a new function
    similar to VÏ€ âˆ—:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è¿™ç§æƒ…å†µæ²¡æœ‰å‘ç”Ÿæ—¶ï¼Œéœ€è¦å®šä¹‰ä¸€ä¸ªç±»ä¼¼äºVÏ€ âˆ—çš„æ–°å‡½æ•°ï¼š
- en: '![](img/ea4da394-538a-44c2-8b43-6ec90a2de576.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea4da394-538a-44c2-8b43-6ec90a2de576.png)'
- en: 'If the agent is able to estimate the function Q, it is possible to choose the
    optimal action s, even without knowing the function Î´:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ™ºèƒ½ä½“èƒ½å¤Ÿä¼°è®¡å‡½æ•°Qï¼Œé‚£ä¹ˆå³ä½¿ä¸çŸ¥é“å‡½æ•°Î´ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©æœ€ä¼˜åŠ¨ä½œsï¼š
- en: '![](img/d2fe1391-eca0-4b32-9e17-3b6737e94f11.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d2fe1391-eca0-4b32-9e17-3b6737e94f11.png)'
- en: The functionÂ QÂ is usually referred to as an action-value function. Following
    a policyÂ Ï€,Â the action-value-function returns the expected reward for using action
    a in a certain state s.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•°Qé€šå¸¸è¢«ç§°ä¸ºåŠ¨ä½œå€¼å‡½æ•°ã€‚æ ¹æ®ç­–ç•¥Ï€ï¼ŒåŠ¨ä½œå€¼å‡½æ•°è¿”å›åœ¨ç‰¹å®šçŠ¶æ€sä¸‹ä½¿ç”¨åŠ¨ä½œaçš„æœŸæœ›å¥–åŠ±ã€‚
- en: The main difference between the state-value and the action-value functions is
    that the Q value allows you â€“Â at least in the first phaseÂ â€“ to take a different
    action than the one that was envisaged by the policy.Â This is because Q reasons
    in terms of total reward, so in a specific state, it can also return a reward
    lower than that paid by another action. The state-value function contains the
    value of reaching a certain state, while the action-value-function contains the
    value for choosing an action in a state. How is the best action chosen? Let's
    take a look.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: çŠ¶æ€å€¼å‡½æ•°å’ŒåŠ¨ä½œå€¼å‡½æ•°ä¹‹é—´çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼ŒQå€¼å…è®¸ä½ â€”â€”è‡³å°‘åœ¨ç¬¬ä¸€é˜¶æ®µâ€”â€”é‡‡å–ä¸ç­–ç•¥é¢„æœŸä¸åŒçš„åŠ¨ä½œã€‚è¿™æ˜¯å› ä¸ºQæ˜¯æ ¹æ®æ€»å¥–åŠ±æ¥æ¨ç†çš„ï¼Œå› æ­¤åœ¨ç‰¹å®šçŠ¶æ€ä¸‹ï¼Œå®ƒä¹Ÿå¯èƒ½è¿”å›æ¯”å…¶ä»–åŠ¨ä½œè·å¾—çš„å¥–åŠ±æ›´ä½çš„å¥–åŠ±ã€‚çŠ¶æ€å€¼å‡½æ•°åŒ…å«åˆ°è¾¾æŸä¸€çŠ¶æ€çš„ä»·å€¼ï¼Œè€ŒåŠ¨ä½œå€¼å‡½æ•°åŒ…å«åœ¨æŸä¸€çŠ¶æ€ä¸‹é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œçš„ä»·å€¼ã€‚å¦‚ä½•é€‰æ‹©æœ€ä½³åŠ¨ä½œå‘¢ï¼Ÿè®©æˆ‘ä»¬ä¸€æ¢ç©¶ç«Ÿã€‚
- en: Choosing an action
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ
- en: 'Suppose we select n-actions: a = a[1]â€¦ .a[n]. Each of these actions has its
    own value of the action-value function. Its estimated value at t-t[h] pitch (play)
    is Q[t] (a[k]). Recall that the true value of an action is the average reward
    received when that action is chosen. A natural way to estimate this value is to
    calculate the average of the rewards that was actually received when the action
    was chosen. In other words, if at the t-th game the action a was chosen k times
    before t, obtaining the rewards r[1], r[2], ..., r[ka], then its value is estimated
    to be as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬é€‰æ‹©nä¸ªåŠ¨ä½œï¼ša = a[1]â€¦ .a[n]ã€‚æ¯ä¸ªåŠ¨ä½œéƒ½æœ‰å…¶è‡ªèº«çš„åŠ¨ä½œå€¼å‡½æ•°å€¼ã€‚åœ¨t-t[h]æ—¶åˆ»ï¼ˆå›åˆï¼‰å…¶ä¼°è®¡å€¼ä¸ºQ[t] (a[k])ã€‚å›æƒ³ä¸€ä¸‹ï¼ŒåŠ¨ä½œçš„çœŸå®å€¼æ˜¯å½“é€‰æ‹©è¯¥åŠ¨ä½œæ—¶è·å¾—çš„å¹³å‡å¥–åŠ±ã€‚ä¼°è®¡è¿™ä¸ªå€¼çš„è‡ªç„¶æ–¹æ³•æ˜¯è®¡ç®—å®é™…é€‰æ‹©è¯¥åŠ¨ä½œæ—¶è·å¾—çš„å¥–åŠ±çš„å¹³å‡å€¼ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœåœ¨ç¬¬tå›åˆé€‰æ‹©äº†åŠ¨ä½œaï¼Œå¹¶ä¸”åœ¨tä¹‹å‰é€‰æ‹©äº†kæ¬¡ï¼Œè·å¾—äº†å¥–åŠ±r[1],
    r[2], ..., r[ka]ï¼Œé‚£ä¹ˆå®ƒçš„ä¼°è®¡å€¼ä¸ºï¼š
- en: '![](img/8e395896-9a16-48b1-9d9f-89d358fd694c.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e395896-9a16-48b1-9d9f-89d358fd694c.png)'
- en: 'Here, we have the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰å¦‚ä¸‹å†…å®¹ï¼š
- en: For k[a] = 0, we set Q[t] (a[k]) to a default value, Q[0] (a[k]) = 0 (no estimate
    available).
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºk[a] = 0ï¼Œæˆ‘ä»¬å°†Q[t] (a[k])è®¾ç½®ä¸ºé»˜è®¤å€¼ï¼ŒQ[0] (a[k]) = 0ï¼ˆæ²¡æœ‰å¯ç”¨çš„ä¼°è®¡ï¼‰ã€‚
- en: For k[a] â†’ âˆ, Q[t]Â (a[k])Â â†’ Q ^* (a[k]) (for the law of large numbers).
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºk[a] â†’ âˆï¼ŒQ[t] (a[k]) â†’ Q ^* (a[k])ï¼ˆæ ¹æ®å¤§æ•°æ³•åˆ™ï¼‰ã€‚
- en: In all of these cases, the action-value function is calculated as an average
    (sample-average method).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰€æœ‰è¿™äº›æƒ…å†µä¸‹ï¼ŒåŠ¨ä½œå€¼å‡½æ•°æ˜¯ä½œä¸ºå¹³å‡å€¼ï¼ˆæ ·æœ¬å¹³å‡æ³•ï¼‰è®¡ç®—çš„ã€‚
- en: Now that we've explored the basic concepts of this technology, we will move
    on and explore the possible solutions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»æ¢è®¨äº†è¿™é¡¹æŠ€æœ¯çš„åŸºæœ¬æ¦‚å¿µï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†ç»§ç»­æ¢ç´¢å¯èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚
- en: Understanding problem solution techniques
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†è§£é—®é¢˜è§£å†³æŠ€å·§
- en: 'So far, we have formalized the problem and we have seen what tools are available
    to make the choice of actions that give us the least regret. Now, we can formulate
    selection methods. We will look at the following problem-solving techniques in
    the following subsections:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»å°†é—®é¢˜å½¢å¼åŒ–ï¼Œå¹¶ä¸”æˆ‘ä»¬å·²ç»çœ‹åˆ°æœ‰å“ªäº›å·¥å…·å¯ä»¥å¸®åŠ©æˆ‘ä»¬é€‰æ‹©é‚£äº›èƒ½å¸¦æ¥æœ€å°é—æ†¾çš„åŠ¨ä½œã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥åˆ¶å®šé€‰æ‹©æ–¹æ³•ã€‚æˆ‘ä»¬å°†åœ¨ä»¥ä¸‹å°èŠ‚ä¸­æ¢è®¨ä»¥ä¸‹é—®é¢˜è§£å†³æŠ€å·§ï¼š
- en: Greedy methods
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è´ªå¿ƒæ–¹æ³•
- en: Upper confidence bound
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸Šç½®ä¿¡ç•Œ
- en: Greedy methods
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è´ªå¿ƒæ–¹æ³•
- en: 'This problem, which is much more complex than it may seem, can be tackled by
    using a very naive strategy, though it''s not very effective:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé—®é¢˜ï¼Œæ¯”çœ‹èµ·æ¥æ›´å¤æ‚ï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨ä¸€ä¸ªéå¸¸ç®€å•çš„ç­–ç•¥æ¥è§£å†³ï¼Œå°½ç®¡å®ƒå¹¶ä¸éå¸¸æœ‰æ•ˆï¼š
- en: Initially, each of the levers is played.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ€åˆï¼Œæ¯ä¸ªæ‹‰æ†éƒ½ä¼šè¢«è¯•éªŒã€‚
- en: The lever is played that has returned, on average, the highest reward.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©è¿”å›å¹³å‡æœ€é«˜å¥–åŠ±çš„æ‹‰æ†ã€‚
- en: 'In this algorithm, we can observe the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç®—æ³•ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ä»¥ä¸‹å†…å®¹ï¼š
- en: We allow the agent to have memory
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å…è®¸æ™ºèƒ½ä½“æ‹¥æœ‰è®°å¿†
- en: We store the value associated with different actions
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å­˜å‚¨ä¸ä¸åŒåŠ¨ä½œç›¸å…³çš„å€¼
- en: We choose the action that gave the greatest reward
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€‰æ‹©ç»™å‡ºæœ€å¤§å¥–åŠ±çš„åŠ¨ä½œ
- en: 'The best action is called theÂ **greedy action** and the algorithm based on
    this is called theÂ **greedy** method. The following steps are performed:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ä½³åŠ¨ä½œç§°ä¸º**è´ªå©ªåŠ¨ä½œ**ï¼ŒåŸºäºè¿™ä¸€åŠ¨ä½œçš„ç®—æ³•ç§°ä¸º**è´ªå©ª**æ–¹æ³•ã€‚æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
- en: 'At time step t, estimate a value for each action, as follows:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æ—¶é—´æ­¥tï¼ŒæŒ‰ç…§å¦‚ä¸‹æ–¹å¼ä¼°è®¡æ¯ä¸ªåŠ¨ä½œçš„å€¼ï¼š
- en: '![](img/86e88567-c9b8-4e0b-8b8c-777cea11e2b1.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86e88567-c9b8-4e0b-8b8c-777cea11e2b1.png)'
- en: 'Select the action with the maximum value:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©å…·æœ‰æœ€å¤§å€¼çš„åŠ¨ä½œï¼š
- en: '![](img/7aff49af-1728-4099-8810-0c4a40e4f02b.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7aff49af-1728-4099-8810-0c4a40e4f02b.png)'
- en: In greedy methods, no alternative solutions are explored through this method.
    Why do we have to choose an action that doesn't look the best? This is because
    we explore different solutions since the reward is not deterministic. This implies
    that we could achieve more with other actions because what matters is not the
    instant reward but the sum of the rewards that's obtained. In the greedy solution,
    the algorithm is completely based on exploitation. To improve the performance
    of the model, it is necessary to introduce exploration. Let's see how.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è´ªå©ªæ–¹æ³•ä¸­ï¼Œæ²¡æœ‰é€šè¿‡è¯¥æ–¹æ³•æ¢ç´¢å…¶ä»–æ›¿ä»£è§£å†³æ–¹æ¡ˆã€‚ä¸ºä»€ä¹ˆæˆ‘ä»¬å¿…é¡»é€‰æ‹©ä¸€ä¸ªçœ‹èµ·æ¥ä¸æ˜¯æœ€å¥½çš„åŠ¨ä½œï¼Ÿè¿™æ˜¯å› ä¸ºæˆ‘ä»¬éœ€è¦æ¢ç´¢ä¸åŒçš„è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºå¥–åŠ±å¹¶ä¸æ˜¯ç¡®å®šæ€§çš„ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯èƒ½é€šè¿‡å…¶ä»–åŠ¨ä½œè·å¾—æ›´å¤šçš„å¥–åŠ±ï¼Œå› ä¸ºé‡è¦çš„ä¸æ˜¯ç¬æ—¶çš„å¥–åŠ±ï¼Œè€Œæ˜¯é€šè¿‡è¿™äº›åŠ¨ä½œç´¯è®¡çš„å¥–åŠ±ã€‚åœ¨è´ªå©ªè§£æ³•ä¸­ï¼Œç®—æ³•å®Œå…¨åŸºäºåˆ©ç”¨ã€‚ä¸ºäº†æå‡æ¨¡å‹çš„è¡¨ç°ï¼Œå¿…é¡»å¼•å…¥æ¢ç´¢ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹æ€ä¹ˆåšã€‚
- en: Îµ-greedy methods
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Îµ-è´ªå©ªæ–¹æ³•
- en: Here, it is necessary to maintain a predisposition to explore different actions.
    Once again, we find ourselves dealing with a problem based on the exploration-exploitation
    dilemma we addressed in detail in [Chapter 2](aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml),Â *Building
    Blocks of Reinforcement Learning*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œéœ€è¦ä¿æŒä¸€ç§å€¾å‘äºæ¢ç´¢ä¸åŒåŠ¨ä½œçš„å¿ƒæ€ã€‚æˆ‘ä»¬å†æ¬¡é¢å¯¹ä¸€ä¸ªåŸºäºæ¢ç´¢ä¸åˆ©ç”¨å›°å¢ƒçš„é—®é¢˜ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨[ç¬¬2ç« ](aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml)ã€Šå¼ºåŒ–å­¦ä¹ çš„æ„å»ºæ¨¡å—ã€‹ä¸­è¯¦ç»†è®¨è®ºè¿‡çš„é‚£æ ·ï¼Œ*æ¢ç´¢ä¸åˆ©ç”¨å›°å¢ƒ*ã€‚
- en: 'Ideally, the agent must explore all the possible actions for each state, finding
    the one that is actually most rewarded for exploiting it in achieving its goal.
    Thus, decision-making involves a fundamental choice:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ç†æƒ³æƒ…å†µä¸‹ï¼Œæ™ºèƒ½ä½“å¿…é¡»æ¢ç´¢æ¯ä¸ªçŠ¶æ€ä¸‹æ‰€æœ‰å¯èƒ½çš„åŠ¨ä½œï¼Œæ‰¾å‡ºæœ€èƒ½é€šè¿‡åˆ©ç”¨è¯¥åŠ¨ä½œæ¥è¾¾æˆç›®æ ‡çš„åŠ¨ä½œã€‚å› æ­¤ï¼Œå†³ç­–è¿‡ç¨‹æ¶‰åŠä¸€ä¸ªåŸºæœ¬çš„é€‰æ‹©ï¼š
- en: '**Exploitation**: Making the best decision given the current information'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åˆ©ç”¨**ï¼šæ ¹æ®å½“å‰ä¿¡æ¯åšå‡ºæœ€ä½³å†³ç­–'
- en: '**Exploration**: Collecting more information'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¢ç´¢**ï¼šæ”¶é›†æ›´å¤šä¿¡æ¯'
- en: In this process, the best long-term strategy can lead to considerable sacrifices
    in the short term. Therefore, it is necessary to gather enough information to
    make the best decisions.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæœ€ä½³çš„é•¿æœŸç­–ç•¥å¯èƒ½ä¼šåœ¨çŸ­æœŸå†…å¸¦æ¥ç›¸å½“å¤§çš„ç‰ºç‰²ã€‚å› æ­¤ï¼Œéœ€è¦æ”¶é›†è¶³å¤Ÿçš„ä¿¡æ¯æ¥åšå‡ºæœ€ä½³å†³ç­–ã€‚
- en: 'I suppose that, with probability Îµ, a different action is chosen. This action
    is chosen with a uniform probability between the n possible actions available.
    The following steps are performed:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å‡è®¾ä»¥æ¦‚ç‡Îµï¼Œé€‰æ‹©ä¸€ä¸ªä¸åŒçš„åŠ¨ä½œã€‚è¿™ä¸ªåŠ¨ä½œæ˜¯ä»¥å‡åŒ€æ¦‚ç‡ä»nä¸ªå¯ç”¨åŠ¨ä½œä¸­é€‰æ‹©çš„ã€‚æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
- en: 'At time step t, a value for each action is estimated, as follows:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æ—¶é—´æ­¥tï¼ŒæŒ‰ç…§å¦‚ä¸‹æ–¹å¼ä¼°è®¡æ¯ä¸ªåŠ¨ä½œçš„å€¼ï¼š
- en: '![](img/723adfc6-c091-4d4d-a1fb-0802a8bfddc0.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/723adfc6-c091-4d4d-a1fb-0802a8bfddc0.png)'
- en: 'With probability 1- ğœ€, the action with the maximum value is selected, as follows:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»¥æ¦‚ç‡1-ğœ–ï¼Œé€‰æ‹©æœ€å¤§å€¼çš„åŠ¨ä½œï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](img/3d5ca4b6-5d36-44e8-b0a1-3ab2de6d6134.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d5ca4b6-5d36-44e8-b0a1-3ab2de6d6134.png)'
- en: With probability ğœ€, an action from all the actions with equal probability is
    selected randomly.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»¥æ¦‚ç‡ğœ–ï¼Œä»æ‰€æœ‰åŠ¨ä½œä¸­éšæœºé€‰æ‹©ä¸€ä¸ªï¼Œä¸”æ¯ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ç›¸åŒã€‚
- en: The parameter value that's used the most is ğœ€ =0.1, but this can vary depending
    on the context. In this approach, we introduce an element of exploration that
    improves performance. However, if two actions have a very small difference between
    their Q values, this algorithm will also choose the action that has a higher probability
    than the others.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å¸¸ç”¨çš„å‚æ•°å€¼æ˜¯ğœ– = 0.1ï¼Œä½†è¿™å¯ä»¥æ ¹æ®å…·ä½“æƒ…å†µæœ‰æ‰€ä¸åŒã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¢ç´¢çš„å…ƒç´ ï¼Œä»¥æ”¹å–„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¦‚æœä¸¤ä¸ªåŠ¨ä½œçš„Qå€¼å·®å¼‚éå¸¸å°ï¼Œè¿™ä¸ªç®—æ³•ä¹Ÿä¼šé€‰æ‹©ä¸€ä¸ªæ¦‚ç‡æ¯”å…¶ä»–åŠ¨ä½œæ›´é«˜çš„åŠ¨ä½œã€‚
- en: Îµ-greedy methods with a progressive decrease
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Îµ-è´ªå©ªæ–¹æ³•ä¸æ¸è¿›é€’å‡
- en: The exploration that was introduced with the adoption of ğœ€ offers us the opportunity
    to experiment with options that, so far, are unknown. Nevertheless, the random
    component of the strategy means that actions that have already been taken that
    have received poor results can be explored again. Such inefficient exploration
    can be avoided by progressively reducing the random exploration component.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å¼•å…¥ğœ€è¿›è¡Œæ¢ç´¢ä½¿æˆ‘ä»¬æœ‰æœºä¼šå°è¯•é‚£äº›è‡³ä»ŠæœªçŸ¥çš„é€‰é¡¹ã€‚ç„¶è€Œï¼Œç­–ç•¥ä¸­çš„éšæœºæˆåˆ†æ„å‘³ç€é‚£äº›å·²ç»é‡‡å–å¹¶ä¸”å¾—åˆ°è¾ƒå·®ç»“æœçš„è¡ŒåŠ¨ä»ç„¶å¯èƒ½è¢«é‡æ–°æ¢ç´¢ã€‚é€šè¿‡é€æ¸å‡å°‘éšæœºæ¢ç´¢æˆåˆ†ï¼Œå¯ä»¥é¿å…è¿™ç§ä½æ•ˆçš„æ¢ç´¢ã€‚
- en: In other words, by reducing the parameter Îµ over time, we could explore even
    less since we have gained confidence in the action, which has a strong potential
    of optimal value. This strategy offers a highly exploratory behavior in the beginning
    and a highly exploitative behavior in the end. Let's learn how to carry out exploitation
    and exploration together.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œé€šè¿‡éšç€æ—¶é—´çš„æ¨ç§»å‡å°‘å‚æ•°ğœ–ï¼Œæˆ‘ä»¬å¯ä»¥å‡å°‘æ¢ç´¢ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»å¯¹å…·æœ‰å¼ºå¤§æ½œåŠ›çš„æœ€ä¼˜å€¼çš„è¡ŒåŠ¨äº§ç”Ÿäº†ä¿¡å¿ƒã€‚è¯¥ç­–ç•¥åœ¨å¼€å§‹æ—¶æä¾›äº†é«˜åº¦çš„æ¢ç´¢è¡Œä¸ºï¼Œè€Œåœ¨æœ€ååˆ™è¡¨ç°å‡ºé«˜åº¦çš„åˆ©ç”¨è¡Œä¸ºã€‚è®©æˆ‘ä»¬ä¸€èµ·æ¥å­¦ä¹ å¦‚ä½•åŒæ—¶è¿›è¡Œåˆ©ç”¨ä¸æ¢ç´¢ã€‚
- en: Upper confidence bound
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸Šç½®ä¿¡ç•Œ
- en: At the beginning of the game, we don't know which the best arm is. Therefore,
    we cannot characterize any arm. Thus, the UCB algorithm states that all arms have
    the same observed average value. So, a confidence limit for each arm will be created
    and an arm will be selected at random.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¸¸æˆå¼€å§‹æ—¶ï¼Œæˆ‘ä»¬å¹¶ä¸çŸ¥é“å“ªä¸ªè‡‚æ˜¯æœ€å¥½çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ— æ³•å¯¹ä»»ä½•ä¸€ä¸ªè‡‚è¿›è¡Œæè¿°ã€‚å› æ­¤ï¼ŒUCBç®—æ³•å‡è®¾æ‰€æœ‰è‡‚çš„è§‚å¯Ÿå¹³å‡å€¼éƒ½æ˜¯ç›¸åŒçš„ã€‚äºæ˜¯ï¼Œå°†ä¸ºæ¯ä¸ªè‡‚åˆ›å»ºä¸€ä¸ªç½®ä¿¡åŒºé—´ï¼Œå¹¶éšæœºé€‰æ‹©ä¸€ä¸ªè‡‚ã€‚
- en: In this context, each arm will either give a reward or won't. If the arm that's
    selected returns a mistake, the average that will be observed by the arm will
    decrease, as well as the confidence limit. If the arm that's selected returns
    a reward, the observed average and the confidence limitÂ will increase. By taking
    advantage of the best, we are decreasing the confidence limit. Adding more and
    more rounds, the likelihood that the other arms are doing well also increases.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸ªè‡‚è¦ä¹ˆç»™å‡ºå¥–åŠ±ï¼Œè¦ä¹ˆä¸ç»™ã€‚å¦‚æœé€‰ä¸­çš„è‡‚è¿”å›é”™è¯¯ï¼Œè‡‚çš„è§‚å¯Ÿå¹³å‡å€¼ä»¥åŠç½®ä¿¡åŒºé—´éƒ½ä¼šä¸‹é™ã€‚å¦‚æœé€‰ä¸­çš„è‡‚è¿”å›å¥–åŠ±ï¼Œè§‚å¯Ÿåˆ°çš„å¹³å‡å€¼å’Œç½®ä¿¡åŒºé—´éƒ½ä¼šå¢åŠ ã€‚é€šè¿‡åˆ©ç”¨æœ€ä½³è‡‚ï¼Œæˆ‘ä»¬é™ä½äº†ç½®ä¿¡åŒºé—´ã€‚éšç€æ›´å¤šå›åˆçš„è¿›è¡Œï¼Œå…¶ä»–è‡‚è¡¨ç°è‰¯å¥½çš„å¯èƒ½æ€§ä¹Ÿåœ¨å¢åŠ ã€‚
- en: 'To implement this strategy, follow these steps:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®æ–½è¿™ä¸€ç­–ç•¥ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š
- en: 'At each round, two variables are computed:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¯ä¸€è½®ä¸­ï¼Œéƒ½ä¼šè®¡ç®—ä¸¤ä¸ªå˜é‡ï¼š
- en: '*R[i](n)*: The sum of the rewards obtained by the lever *i* after *n* plays'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R[i](n)*ï¼šæ æ† *i* åœ¨è¿›è¡Œ *n* æ¬¡æ“ä½œåè·å¾—çš„å¥–åŠ±æ€»å’Œ'
- en: '*T**[i]**(n)*: The number of times the lever *i* is played by the strategy
    in the first *n* plays'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*T**[i]**(n)*ï¼šç­–ç•¥åœ¨å‰ *n* æ¬¡æ“ä½œä¸­é€‰æ‹©æ æ† *i* çš„æ¬¡æ•°'
- en: 'We calculate the average rewards obtained by the lever *i* after *n* plays
    using the following formula:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹å…¬å¼è®¡ç®—åœ¨è¿›è¡Œ *n* æ¬¡æ“ä½œåæ æ† *i* çš„å¹³å‡å¥–åŠ±ï¼š
- en: '![](img/ec54ea62-c453-4a2c-b253-41aab5679c82.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec54ea62-c453-4a2c-b253-41aab5679c82.png)'
- en: 'We calculate the confidence interval after *n* plays using the following formula:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹å…¬å¼è®¡ç®—åœ¨è¿›è¡Œ *n* æ¬¡æ“ä½œåçš„ç½®ä¿¡åŒºé—´ï¼š
- en: '![](img/044dd621-9e6e-4245-8f96-f97a5124f599.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/044dd621-9e6e-4245-8f96-f97a5124f599.png)'
- en: 'We select the lever *i* that returns the maximum UCB as follows:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æŒ‰ç…§ä»¥ä¸‹æ–¹å¼é€‰æ‹©è¿”å›æœ€å¤§UCBçš„æ æ† *i*ï¼š
- en: '![](img/6edfa64f-85e2-449f-a9a5-09f9aa3311b0.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6edfa64f-85e2-449f-a9a5-09f9aa3311b0.png)'
- en: Both of these algorithms keep track of how much they know of any available arm
    and pay no attention except to how much reward they have obtained from the arms.
    On the contrary, the algorithms that we've analyzed so far have under-explored
    the options whose initial experiences have not returned significant rewards, even
    if they do not have enough data to be sure of those arms.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ç§ç®—æ³•éƒ½è®°å½•å®ƒä»¬å¯¹ä»»ä½•å¯ç”¨è‡‚çš„äº†è§£ç¨‹åº¦ï¼Œåªå…³æ³¨å®ƒä»¬ä»å„ä¸ªè‡‚è·å¾—çš„å¥–åŠ±ã€‚ä¸æ­¤ç›¸åï¼Œæˆ‘ä»¬è¿„ä»Šä¸ºæ­¢åˆ†æçš„ç®—æ³•ï¼Œå¯¹äºé‚£äº›åˆå§‹ç»éªŒæ²¡æœ‰è¿”å›æ˜¾è‘—å¥–åŠ±çš„é€‰é¡¹ï¼Œè¿›è¡Œäº†ä¸è¶³çš„æ¢ç´¢ï¼Œå³ä½¿å®ƒä»¬æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®æ¥ç¡®å®šè¿™äº›è‡‚ã€‚
- en: In the next section, we will introduce the concept of state, which represents
    a description of the environment that the agent can use to perform targeted actions.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»â€œçŠ¶æ€â€çš„æ¦‚å¿µï¼Œå®ƒä»£è¡¨äº†æ™ºèƒ½ä½“å¯ä»¥ç”¨æ¥æ‰§è¡Œæœ‰é’ˆå¯¹æ€§è¡ŒåŠ¨çš„ç¯å¢ƒæè¿°ã€‚
- en: Implementing the contextual approach
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®ç°æƒ…å¢ƒæ–¹æ³•
- en: So far, in addressing the problem of the MAB, we have generated an action but
    we have not exploited any information on the state of the environment (context).
    The range of actions that are available to the agent consists of pulling one or
    more arms of the bandit. In this way, a reward of +1 or -1 is received.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œåœ¨è§£å†³å¤šè‡‚è€è™æœºï¼ˆMABï¼‰é—®é¢˜æ—¶ï¼Œæˆ‘ä»¬å·²ç»ç”Ÿæˆäº†ä¸€ä¸ªåŠ¨ä½œï¼Œä½†æˆ‘ä»¬å¹¶æ²¡æœ‰åˆ©ç”¨ç¯å¢ƒï¼ˆä¸Šä¸‹æ–‡ï¼‰çš„ä»»ä½•çŠ¶æ€ä¿¡æ¯ã€‚ä»£ç†å¯ç”¨çš„åŠ¨ä½œèŒƒå›´åŒ…æ‹¬æ‹‰åŠ¨ä¸€ä¸ªæˆ–å¤šä¸ªè€è™æœºçš„è‡‚ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥è·å¾—+1æˆ–-1çš„å¥–åŠ±ã€‚
- en: The problem is considered to be solved if the agent chooses the arm that increasingly
    returns a positive reward. In this case, we can design an agent that completely
    ignores the state of the environment since, in effect, there is always only one
    immutable state.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä»£ç†é€‰æ‹©çš„è‡‚ä¸æ–­è¿”å›æ­£å¥–åŠ±ï¼Œé‚£ä¹ˆé—®é¢˜å°±è¢«è®¤ä¸ºå·²è§£å†³ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥è®¾è®¡ä¸€ä¸ªå®Œå…¨å¿½è§†ç¯å¢ƒçŠ¶æ€çš„ä»£ç†ï¼Œå› ä¸ºå®é™…ä¸Šï¼Œå§‹ç»ˆåªæœ‰ä¸€ä¸ªä¸å¯å˜çš„çŠ¶æ€ã€‚
- en: In the contextual bandit, the concept of state is introduced, which represents
    a description of the environment that the agent can use to carry out targeted
    actions. This model extends the original one by linking the decision to the state
    of the environment.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šä¸‹æ–‡è€è™æœºä¸­ï¼Œå¼•å…¥äº†â€œçŠ¶æ€â€æ¦‚å¿µï¼Œå®ƒè¡¨ç¤ºç¯å¢ƒçš„æè¿°ï¼Œä»£ç†å¯ä»¥åˆ©ç”¨è¯¥æè¿°æ‰§è¡Œæœ‰é’ˆå¯¹æ€§çš„åŠ¨ä½œã€‚è¯¥æ¨¡å‹é€šè¿‡å°†å†³ç­–ä¸ç¯å¢ƒçŠ¶æ€ç›¸è”ç³»ï¼Œæ‰©å±•äº†åŸå§‹æ¨¡å‹ã€‚
- en: 'The following diagram shows diagrams of the two models:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾å±•ç¤ºäº†è¿™ä¸¤ä¸ªæ¨¡å‹çš„ç¤ºæ„å›¾ï¼š
- en: '![](img/e65917dc-f7e7-4900-aa21-89c343e8bbec.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e65917dc-f7e7-4900-aa21-89c343e8bbec.png)'
- en: The original problem is substantially modified in the sense that instead of
    a single bandit, which we've considered so far, in the new formalization of the
    problem, there are more bandits. The state of the environment tells us what bandit
    we're dealing with and the agent's goal is to learn the best action for any available
    bandit. Since each bandit will have different probabilities of reward for each
    arm, our agent will have to learn to condition their action on the state of the
    environment. Unless they do, they will not get the maximum possible reward over
    time.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹é—®é¢˜åœ¨æœ¬è´¨ä¸Šå‘ç”Ÿäº†å˜åŒ–ï¼Œå› ä¸ºåœ¨é—®é¢˜çš„æ–°å½¢å¼åŒ–ä¸­ï¼Œä¸å†åªæœ‰ä¸€ä¸ªè€è™æœºï¼ˆæˆ‘ä»¬è‡³ä»Šè€ƒè™‘çš„æƒ…å†µï¼‰ï¼Œè€Œæ˜¯æœ‰å¤šä¸ªè€è™æœºã€‚ç¯å¢ƒçš„çŠ¶æ€å‘Šè¯‰æˆ‘ä»¬æ­£åœ¨å¤„ç†å“ªä¸ªè€è™æœºï¼Œä»£ç†çš„ç›®æ ‡æ˜¯å­¦ä¹ é’ˆå¯¹ä»»ä½•å¯ç”¨è€è™æœºçš„æœ€ä½³åŠ¨ä½œã€‚ç”±äºæ¯ä¸ªè€è™æœºå¯¹æ¯ä¸ªè‡‚çš„å¥–åŠ±æ¦‚ç‡ä¸åŒï¼Œæˆ‘ä»¬çš„ä»£ç†éœ€è¦å­¦ä¹ æ ¹æ®ç¯å¢ƒçŠ¶æ€æ¥è°ƒæ•´å…¶è¡ŒåŠ¨ã€‚é™¤éä»–ä»¬è¿™æ ·åšï¼Œå¦åˆ™æ— æ³•éšç€æ—¶é—´æ¨ç§»è·å¾—æœ€å¤§çš„å¥–åŠ±ã€‚
- en: With the **contextual bandit** model, you not only optimize the decision based
    on the previous observations but also personalize the decisions for each situation.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨**ä¸Šä¸‹æ–‡è€è™æœº**æ¨¡å‹ï¼Œä½ ä¸ä»…åŸºäºå…ˆå‰çš„è§‚å¯Ÿä¼˜åŒ–å†³ç­–ï¼Œè¿˜å¯ä»¥æ ¹æ®æ¯ç§æƒ…å†µä¸ªæ€§åŒ–å†³ç­–ã€‚
- en: 'From the preceding model, we can observe the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å‰é¢çš„æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ä»¥ä¸‹å‡ ç‚¹ï¼š
- en: The algorithm observes a context.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç®—æ³•è§‚å¯Ÿä¸€ä¸ªä¸Šä¸‹æ–‡ã€‚
- en: The algorithm makes a decision by choosing an action from a series of alternative
    actions.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç®—æ³•é€šè¿‡ä»ä¸€ç³»åˆ—å¤‡é€‰åŠ¨ä½œä¸­é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œæ¥åšå‡ºå†³ç­–ã€‚
- en: We can observe the result of this decision, which returns a reward.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°è¯¥å†³ç­–çš„ç»“æœï¼Œå®ƒè¿”å›ä¸€ä¸ªå¥–åŠ±ã€‚
- en: The goal is to maximize the average reward.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ˜¯æœ€å¤§åŒ–å¹³å‡å¥–åŠ±ã€‚
- en: 'An example of applying this algorithm to the real world is the problem of selecting
    advertisements to be displayed on a website to optimize the clickthrough rate.
    The context is information about the user: where it comes from, information about
    the device that was used, pages of the site that were previously visited, geolocation,
    and so on. An action corresponds to the choice of which ad to display. One result
    is whether the user has clicked on a banner or not. A reward is binary: 0 if there
    is no click, 1 if there is a click.Â Now, let''s learn how to alternate the evaluation
    of the policy with the improvement of the policy.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¯¥ç®—æ³•åº”ç”¨äºç°å®ä¸–ç•Œçš„ä¸€ä¸ªä¾‹å­æ˜¯é€‰æ‹©è¦åœ¨ç½‘ç«™ä¸Šå±•ç¤ºçš„å¹¿å‘Šï¼Œä»¥ä¼˜åŒ–ç‚¹å‡»ç‡çš„é—®é¢˜ã€‚ä¸Šä¸‹æ–‡æ˜¯å…³äºç”¨æˆ·çš„ä¿¡æ¯ï¼šç”¨æˆ·æ¥è‡ªå“ªé‡Œã€ä½¿ç”¨çš„è®¾å¤‡ä¿¡æ¯ã€ä¹‹å‰è®¿é—®è¿‡çš„ç½‘ç«™é¡µé¢ã€åœ°ç†ä½ç½®ç­‰ç­‰ã€‚ä¸€ä¸ªåŠ¨ä½œå¯¹åº”äºé€‰æ‹©å±•ç¤ºå“ªæ¡å¹¿å‘Šã€‚ä¸€ä¸ªç»“æœæ˜¯ç”¨æˆ·æ˜¯å¦ç‚¹å‡»äº†å¹¿å‘Šæ¨ªå¹…ã€‚å¥–åŠ±æ˜¯äºŒå…ƒçš„ï¼šå¦‚æœæ²¡æœ‰ç‚¹å‡»ï¼Œåˆ™å¥–åŠ±ä¸º0ï¼›å¦‚æœç‚¹å‡»ï¼Œåˆ™å¥–åŠ±ä¸º1ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬å­¦ä¹ å¦‚ä½•äº¤æ›¿è¯„ä¼°ç­–ç•¥ä¸æ”¹è¿›ç­–ç•¥ã€‚
- en: Understanding asynchronous actor-critic agents
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†è§£å¼‚æ­¥æ¼”å‘˜-è¯„è®ºå‘˜ä»£ç†
- en: Actorâ€“critic methods implement a generalized policy iteration, alternating between
    a policy evaluation and a policy improvement step. There are two closely related
    processes of actor improvement that aim at improving the current policy and critic
    evaluation by evaluating the current policy. If the process that's defined by
    the critic has the bootstrap, then the variance is reduced. By doing this, the
    learning of the algorithm becomes more stable with respect to the methods of the
    policy gradient.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Actor-Critic æ–¹æ³•å®ç°äº†ä¸€ä¸ªå¹¿ä¹‰çš„ç­–ç•¥è¿­ä»£ï¼Œäº¤æ›¿è¿›è¡Œç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ”¹è¿›æ­¥éª¤ã€‚Actor æ”¹è¿›çš„ä¸¤ä¸ªç´§å¯†ç›¸å…³çš„è¿‡ç¨‹æ—¨åœ¨æ”¹å–„å½“å‰ç­–ç•¥ï¼Œè€Œ Critic
    è¯„ä¼°åˆ™æ˜¯é€šè¿‡è¯„ä¼°å½“å‰ç­–ç•¥æ¥å®ç°çš„ã€‚å¦‚æœ Critic æ‰€å®šä¹‰çš„è¿‡ç¨‹å…·æœ‰å¼•å¯¼ä½œç”¨ï¼Œé‚£ä¹ˆæ–¹å·®å°±ä¼šå‡å°‘ã€‚é€šè¿‡è¿™æ ·åšï¼Œç›¸æ¯”äºç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œç®—æ³•çš„å­¦ä¹ ä¼šå˜å¾—æ›´åŠ ç¨³å®šã€‚
- en: These methods have the characteristic of separating the memory structure to
    make the policy independent of the value function. The policy block is known as
    an actor because it chooses actions, while the estimated value function is called
    the critic in the sense that it criticizes the actions that are performed by the
    policy that is being followed. From this, we understand that learning is an on-policy
    typeÂ â€“ in fact, the critic learns and criticizes the work of politics.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ–¹æ³•çš„ç‰¹ç‚¹æ˜¯åˆ†ç¦»å†…å­˜ç»“æ„ï¼Œä½¿å¾—ç­–ç•¥ä¸ä»·å€¼å‡½æ•°ç›¸äº’ç‹¬ç«‹ã€‚ç­–ç•¥æ¨¡å—è¢«ç§°ä¸º Actorï¼Œå› ä¸ºå®ƒé€‰æ‹©è¡ŒåŠ¨ï¼Œè€Œä¼°è®¡çš„ä»·å€¼å‡½æ•°åˆ™è¢«ç§°ä¸º Criticï¼Œä»è¿™ä¸ªè§’åº¦çœ‹ï¼ŒCritic
    æ‰¹è¯„çš„æ˜¯ç­–ç•¥æ‰§è¡Œçš„åŠ¨ä½œã€‚ä»è¿™ä¸€ç‚¹æˆ‘ä»¬å¯ä»¥ç†è§£ï¼Œå­¦ä¹ æ˜¯åŸºäºç­–ç•¥ç±»å‹çš„â€”â€”äº‹å®ä¸Šï¼ŒCritic å­¦ä¹ å¹¶æ‰¹è¯„å½“å‰ç­–ç•¥çš„å·¥ä½œã€‚
- en: 'We have already introduced the actor-critic model, so now, we will explain
    the term asynchronous. This is very simple and has effective intuitions, such
    as the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»ä»‹ç»äº† Actor-Critic æ¨¡å‹ï¼Œç°åœ¨æˆ‘ä»¬å°†è§£é‡Šâ€œå¼‚æ­¥â€è¿™ä¸€æœ¯è¯­ã€‚è¿™éå¸¸ç®€å•ï¼Œå¹¶ä¸”å…·æœ‰æœ‰æ•ˆçš„ç›´è§‰ï¼Œä¾‹å­å¦‚ä¸‹ï¼š
- en: The model has several agents exploring the environment at the same time (each
    agent has a copy of the entire environment).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹æœ‰å¤šä¸ªæ™ºèƒ½ä½“åŒæ—¶æ¢ç´¢ç¯å¢ƒï¼ˆæ¯ä¸ªæ™ºèƒ½ä½“éƒ½æœ‰æ•´ä¸ªç¯å¢ƒçš„å‰¯æœ¬ï¼‰ã€‚
- en: The model gives different starting policies so that the agents are not related.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç»™å‡ºä¸åŒçš„åˆå§‹ç­–ç•¥ï¼Œä½¿å¾—å„ä¸ªæ™ºèƒ½ä½“å½¼æ­¤ç‹¬ç«‹ã€‚
- en: In the model, the global status is updated with the contributions of each agent
    and the process restarts.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è¯¥æ¨¡å‹ä¸­ï¼Œå…¨å±€çŠ¶æ€ä¼šéšç€æ¯ä¸ªæ™ºèƒ½ä½“çš„è´¡çŒ®è€Œæ›´æ–°ï¼Œæ•´ä¸ªè¿‡ç¨‹ä¼šé‡æ–°å¼€å§‹ã€‚
- en: In 2016, the Google DeepMind group proposed an algorithm named **asynchronous
    advantage actor-critic** (**A3C**). The algorithm proved to be faster and simpler
    than most existing algorithms.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 2016å¹´ï¼ŒGoogle DeepMind å°ç»„æå‡ºäº†ä¸€ç§åä¸º**å¼‚æ­¥ä¼˜åŠ¿ Actor-Critic**ï¼ˆ**A3C**ï¼‰çš„ç®—æ³•ã€‚è¯¥ç®—æ³•æ¯”å¤§å¤šæ•°ç°æœ‰ç®—æ³•æ›´å¿«ã€æ›´ç®€æ´ã€‚
- en: In this algorithm, multiple instances of agents are treated, which have been
    initialized differently in their separate environments. Each agent who begins
    to act and learn gathers their own experiences. These experiences are then used
    to update the global neural network shared by all agents. This network affects
    all the agent actions and each new experience of each agent improves the overall
    network faster. Because there are multiple instances of this agent, the training
    will be much faster and more effective.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­¤ç®—æ³•ä¸­ï¼Œå¤„ç†äº†å¤šä¸ªä¸åŒåˆå§‹åŒ–çš„æ™ºèƒ½ä½“å®ä¾‹ï¼Œå®ƒä»¬å¤„äºå„è‡ªçš„ç¯å¢ƒä¸­ã€‚æ¯ä¸ªå¼€å§‹è¡ŒåŠ¨å¹¶å­¦ä¹ çš„æ™ºèƒ½ä½“éƒ½ä¼šç§¯ç´¯è‡ªå·±çš„ç»éªŒã€‚ç„¶åï¼Œè¿™äº›ç»éªŒå°†ç”¨äºæ›´æ–°æ‰€æœ‰æ™ºèƒ½ä½“å…±äº«çš„å…¨å±€ç¥ç»ç½‘ç»œã€‚è¯¥ç½‘ç»œä¼šå½±å“æ‰€æœ‰æ™ºèƒ½ä½“çš„è¡ŒåŠ¨ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“çš„æ–°ç»éªŒä¼šåŠ é€Ÿæ•´ä½“ç½‘ç»œçš„æ›´æ–°ã€‚ç”±äºæœ‰å¤šä¸ªå®ä¾‹çš„æ™ºèƒ½ä½“ï¼Œè®­ç»ƒå°†å˜å¾—æ›´åŠ å¿«é€Ÿå’Œé«˜æ•ˆã€‚
- en: In the next section, we will apply the concepts we've learned about so far by
    addressing a practical case.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡è§£å†³ä¸€ä¸ªå®é™…æ¡ˆä¾‹ï¼Œåº”ç”¨è¿„ä»Šä¸ºæ­¢å­¦åˆ°çš„æ¦‚å¿µã€‚
- en: Online advertising using the MAB model
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ MAB æ¨¡å‹çš„åœ¨çº¿å¹¿å‘Š
- en: Online advertising falls into the new media category and takes advantage of
    the web's ability to reach a significant number of people. Advertising plays a
    decisive role for companies, which can easily reach a wide audience with lower
    costs than traditional means. One of the main advantages of internet advertising
    is the traceability of results or the effect it has on the public. This happens
    thanks to the ad servers that, in the case of banners, measure the number of views
    and the effective number of users, clicks.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨çº¿å¹¿å‘Šå±äºæ–°åª’ä½“èŒƒç•´ï¼Œå¹¶åˆ©ç”¨äº†äº’è”ç½‘èƒ½å¤Ÿæ¥è§¦åˆ°å¤§é‡äººç¾¤çš„ä¼˜åŠ¿ã€‚å¹¿å‘Šå¯¹å…¬å¸è‡³å…³é‡è¦ï¼Œå› ä¸ºå…¬å¸å¯ä»¥ä»¥æ¯”ä¼ ç»Ÿæ–¹å¼æ›´ä½çš„æˆæœ¬ï¼Œè½»æ¾æ¥è§¦åˆ°å¹¿æ³›çš„å—ä¼—ã€‚äº’è”ç½‘å¹¿å‘Šçš„ä¸»è¦ä¼˜åŠ¿ä¹‹ä¸€æ˜¯ç»“æœçš„å¯è¿½è¸ªæ€§ï¼Œæˆ–è€…è¯´å®ƒå¯¹å…¬ä¼—çš„å½±å“ã€‚è¿™å¾—ç›Šäºå¹¿å‘ŠæœåŠ¡å™¨ï¼Œåœ¨æ¨ªå¹…å¹¿å‘Šçš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬å¯ä»¥è¡¡é‡å±•ç¤ºæ¬¡æ•°ã€å®é™…ç”¨æˆ·æ•°é‡å’Œç‚¹å‡»æ¬¡æ•°ã€‚
- en: 'In online advertising, we can distinguish between the following two types of
    macros:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åœ¨çº¿å¹¿å‘Šä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åŒºåˆ†ä»¥ä¸‹ä¸¤ç§å®ç±»å‹ï¼š
- en: Contextual
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸Šä¸‹æ–‡
- en: Behavioral advertising
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¡Œä¸ºå¹¿å‘Š
- en: In contextual advertising, Google is the typical case where you can place ads
    according to the words on the page or the type of site or topic that characterizes
    the website. In behavioral advertising, we select the target using the information
    we've collected regarding the behavior of each user on the web and on the app
    (pages visited, searches made) in order to identify their interests and needs,
    and then submit advertisements in line with them.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šä¸‹æ–‡å¹¿å‘Šä¸­ï¼Œè°·æ­Œæ˜¯å…¸å‹çš„æ¡ˆä¾‹ï¼Œæ‚¨å¯ä»¥æ ¹æ®é¡µé¢ä¸Šçš„è¯è¯­æˆ–ç½‘ç«™çš„ç±»å‹æˆ–ä¸»é¢˜æ”¾ç½®å¹¿å‘Šã€‚åœ¨è¡Œä¸ºå¹¿å‘Šä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ”¶é›†å…³äºæ¯ä¸ªç”¨æˆ·åœ¨ç½‘ç»œå’Œåº”ç”¨ç¨‹åºä¸Šçš„è¡Œä¸ºï¼ˆè®¿é—®çš„é¡µé¢ã€è¿›è¡Œçš„æœç´¢ï¼‰æ¥é€‰æ‹©ç›®æ ‡ï¼Œä»¥è¯†åˆ«ä»–ä»¬çš„å…´è¶£å’Œéœ€æ±‚ï¼Œç„¶åæ ¹æ®è¿™äº›ä¿¡æ¯æŠ•æ”¾ç›¸å…³å¹¿å‘Šã€‚
- en: 'In both cases, it is clear the reference to the context needs to interact with
    the environment. It''s also clear that we won''t know a priori how the user will
    behave before an advertisement. These problems can be addressed through a model
    based on MAB, as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæ˜æ˜¾çš„æ˜¯ï¼Œå¼•ç”¨ä¸Šä¸‹æ–‡éœ€è¦ä¸ç¯å¢ƒäº¤äº’ã€‚ä¹Ÿæ˜¾ç„¶ï¼Œåœ¨å±•ç¤ºå¹¿å‘Šä¹‹å‰ï¼Œæˆ‘ä»¬æ— æ³•å…ˆçŸ¥ç”¨æˆ·çš„è¡Œä¸ºã€‚è¿™äº›é—®é¢˜å¯ä»¥é€šè¿‡åŸºäºMABçš„æ¨¡å‹æ¥è§£å†³ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: The context is represented by the characteristics of visitors and web pages.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸Šä¸‹æ–‡ç”±è®¿é—®è€…å’Œç½‘é¡µçš„ç‰¹å¾è¡¨ç¤ºã€‚
- en: Arms are represented by the types of ads that are available.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰‹è‡‚é€šè¿‡å¯ç”¨çš„å¹¿å‘Šç±»å‹æ¥è¡¨ç¤ºã€‚
- en: An action is equivalent to the type of ad to be shown.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŠ¨ä½œç­‰åŒäºå°†è¦å±•ç¤ºçš„å¹¿å‘Šç±»å‹ã€‚
- en: The rewards are returned by the visitor's behavior. By clicking on the ad shown,
    you receive a reward of 1, while by not clicking on the ad, you receive a reward
    of 0.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¥–åŠ±ç”±è®¿é—®è€…çš„è¡Œä¸ºè¿”å›ã€‚é€šè¿‡ç‚¹å‡»å±•ç¤ºçš„å¹¿å‘Šï¼Œæ‚¨ä¼šæ”¶åˆ°å¥–åŠ±1ï¼Œè€Œä¸ç‚¹å‡»å¹¿å‘Šåˆ™ä¼šæ”¶åˆ°å¥–åŠ±0ã€‚
- en: To make this discussion as understandable as possible, we will limit the number
    of ads we want to evaluate to three and aim to find which strategy offers the
    maximum total click rate after a certain number of impressions. So, let's learn
    how to tackle this problem by adopting the contextual approach.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿è¿™ä¸ªè®¨è®ºå°½å¯èƒ½æ˜“äºç†è§£ï¼Œæˆ‘ä»¬å°†é™åˆ¶æˆ‘ä»¬å¸Œæœ›è¯„ä¼°çš„å¹¿å‘Šæ•°é‡ä¸ºä¸‰ä¸ªï¼Œå¹¶æ—¨åœ¨æ‰¾å‡ºåœ¨ä¸€å®šæ•°é‡çš„å±•ç¤ºåï¼Œå“ªä¸ªç­–ç•¥èƒ½æä¾›æœ€å¤§çš„æ€»ç‚¹å‡»ç‡ã€‚æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬é€šè¿‡é‡‡ç”¨ä¸Šä¸‹æ–‡æ–¹æ³•æ¥å­¦ä¹ å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ã€‚
- en: Implementing the contextual package
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®ç°ä¸Šä¸‹æ–‡åŒ…
- en: In the *Contextual approach* section, we said that, in the contextual bandit,
    the concept of state is introduced, which represents a description of the environment
    that the agent can use to carry out targeted actions. This model extends the original
    one by linking the decision to the state of the environment. In this section,
    we will see some examples of applications of the armed bandit problem addressed
    with the contextual approach. To do this, we will use the contextual package.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*ä¸Šä¸‹æ–‡æ–¹æ³•*éƒ¨åˆ†ï¼Œæˆ‘ä»¬æåˆ°ï¼Œåœ¨ä¸Šä¸‹æ–‡èµŒåšæœºä¸­ï¼Œå¼•å…¥äº†çŠ¶æ€çš„æ¦‚å¿µï¼Œè¡¨ç¤ºä»£ç†å¯ä»¥ç”¨æ¥æ‰§è¡Œæœ‰é’ˆå¯¹æ€§è¡ŒåŠ¨çš„ç¯å¢ƒæè¿°ã€‚è¯¥æ¨¡å‹é€šè¿‡å°†å†³ç­–ä¸ç¯å¢ƒçš„çŠ¶æ€è”ç³»èµ·æ¥ï¼Œæ‰©å±•äº†åŸå§‹æ¨¡å‹ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°ä¸€äº›ä½¿ç”¨ä¸Šä¸‹æ–‡æ–¹æ³•è§£å†³æ­¦è£…èµŒåšæœºé—®é¢˜çš„åº”ç”¨ç¤ºä¾‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`contextual`åŒ…ã€‚
- en: This package facilitates the simulation and evaluation of context-free and contextual
    MAB policies or algorithms to ease the implementation, evaluation, and dissemination
    of existing and new bandit algorithms and policies.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥åŒ…ç®€åŒ–äº†æ— ä¸Šä¸‹æ–‡å’Œä¸Šä¸‹æ–‡MABç­–ç•¥æˆ–ç®—æ³•çš„æ¨¡æ‹Ÿå’Œè¯„ä¼°ï¼Œä¾¿äºç°æœ‰å’Œæ–°ç®—æ³•åŠç­–ç•¥çš„å®ç°ã€è¯„ä¼°å’Œä¼ æ’­ã€‚
- en: 'A brief description of the `diagram` package, which has been extracted from
    the official documentation, is shown in the following table:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`diagram`åŒ…çš„ç®€è¦æè¿°å·²ä»å®˜æ–¹æ–‡æ¡£ä¸­æå–ï¼Œå¹¶æ˜¾ç¤ºåœ¨ä¸‹è¡¨ä¸­ï¼š'
- en: '| Version | 0.9.8.2 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| ç‰ˆæœ¬ | 0.9.8.2 |'
- en: '| Date | 2019-07-08 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| æ—¥æœŸ | 2019-07-08 |'
- en: '| Maintainer | Robin van Emden |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| ç»´æŠ¤è€… | Robin van Emden |'
- en: '| License | GPL-3 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| è®¸å¯è¯ | GPL-3 |'
- en: '| Authors | Robin van Emden, Maurits Kaptein |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| ä½œè€… | Robin van Emden, Maurits Kaptein |'
- en: 'The `contextual` package was presented by the authors in the following paper:
    van Emden, R. and Kaptein, M., 2018\. *Contextual: Evaluating Contextual Multi-Armed
    Bandit Problems in R*. arXiv preprint arXiv:1811.01926.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`contextual`åŒ…ç”±ä½œè€…åœ¨ä»¥ä¸‹è®ºæ–‡ä¸­æå‡ºï¼švan Emden, R. å’Œ Kaptein, M., 2018\. *Contextual: Evaluating
    Contextual Multi-Armed Bandit Problems in R*. arXivé¢„å°æœ¬ arXiv:1811.01926ã€‚'
- en: 'In this package, the MAB problems are addressed under the following assumptions:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­¤åŒ…ä¸­ï¼ŒMABé—®é¢˜æ˜¯åœ¨ä»¥ä¸‹å‡è®¾ä¸‹å¤„ç†çš„ï¼š
- en: The bandit is a set of arms where each arm is defined by some reward function
    mapping dimensional context vector returning a reward for every time step until
    the horizon.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èµŒåšæœºæ˜¯ä¸€ç»„æ‰‹è‡‚ï¼Œæ¯ä¸ªæ‰‹è‡‚ç”±æŸäº›å¥–åŠ±å‡½æ•°å®šä¹‰ï¼Œè¿™äº›å¥–åŠ±å‡½æ•°å°†ç»´åº¦ä¸Šä¸‹æ–‡å‘é‡æ˜ å°„åˆ°æ¯ä¸ªæ—¶é—´æ­¥é•¿çš„å¥–åŠ±ï¼Œç›´åˆ°é¢„å®šçš„æ—¶é—´èŒƒå›´ã€‚
- en: The function of politics is the maximization of the cumulative reward. This
    function is carried out by selecting one of the currently available bandit arms
    in the sequence.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ”¿ç­–çš„åŠŸèƒ½æ˜¯æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚è¿™ä¸ªåŠŸèƒ½æ˜¯é€šè¿‡é€‰æ‹©å½“å‰å¯ç”¨çš„èµŒåšæœºè‡‚ä¹‹ä¸€æ¥å®ç°çš„ã€‚
- en: During the learning process, the policy observes the current state of the environment,
    which is represented by the vectors of the context characteristics. Afterward,
    the policy selects one of the available actions using an arm selection strategy.
    As a result, it receives a reward. This procedure allows the policy to update
    the selection of strategic arms. This procedure is then repeated *T* times.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œç­–ç•¥è§‚å¯Ÿç¯å¢ƒçš„å½“å‰çŠ¶æ€ï¼ŒçŠ¶æ€ç”±ä¸Šä¸‹æ–‡ç‰¹å¾å‘é‡è¡¨ç¤ºã€‚ä¹‹åï¼Œç­–ç•¥ä½¿ç”¨è‡‚é€‰æ‹©ç­–ç•¥é€‰æ‹©ä¸€ä¸ªå¯ç”¨çš„åŠ¨ä½œã€‚ä½œä¸ºç»“æœï¼Œå®ƒä¼šè·å¾—ä¸€ä¸ªå¥–åŠ±ã€‚è¿™ä¸ªè¿‡ç¨‹å…è®¸ç­–ç•¥æ›´æ–°ç­–ç•¥è‡‚çš„é€‰æ‹©ã€‚æ­¤è¿‡ç¨‹éšåä¼šé‡å¤*T*æ¬¡ã€‚
- en: 'To represent what has been said in the algorithm, we can say that, for each
    round, a policy does the following:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¡¨ç¤ºç®—æ³•ä¸­æ‰€è¿°çš„å†…å®¹ï¼Œæˆ‘ä»¬å¯ä»¥è¯´ï¼Œå¯¹äºæ¯ä¸€è½®ï¼Œç­–ç•¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
- en: Observes the current context feature vectors
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è§‚å¯Ÿå½“å‰ä¸Šä¸‹æ–‡ç‰¹å¾å‘é‡
- en: Selects an action
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ
- en: Receives a reward
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è·å¾—å¥–åŠ±
- en: Updates the arm-selection strategy parameters
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ›´æ–°è‡‚é€‰æ‹©ç­–ç•¥å‚æ•°
- en: The goal of the policy is to optimize its cumulative reward.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥çš„ç›®æ ‡æ˜¯ä¼˜åŒ–å…¶ç´¯ç§¯å¥–åŠ±ã€‚
- en: Then, we apply the functions contained in the contextual package to a practical
    case.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å°†åº”ç”¨ä¸Šä¸‹æ–‡åŒ…ä¸­åŒ…å«çš„å‡½æ•°åˆ°å®é™…æ¡ˆä¾‹ä¸­ã€‚
- en: Online advertising context-free policies
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨çº¿å¹¿å‘Šæ— ä¸Šä¸‹æ–‡ç­–ç•¥
- en: 'The first simulation we will perform will not consider the context. We will
    simply evaluate how many clicks each advertisement receives after a certain number
    of impressions. First, we need to set the initial settings. As anticipated, we
    will only consider three announcements that correspond in the MAB formulation
    to three arms of the bandit, each with a different probability of generating a
    click. Let''s get started:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è¿›è¡Œçš„ç¬¬ä¸€æ¬¡æ¨¡æ‹Ÿå°†ä¸è€ƒè™‘ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬å°†ç®€å•åœ°è¯„ä¼°åœ¨ä¸€å®šæ¬¡æ•°å±•ç¤ºåï¼Œæ¯ä¸ªå¹¿å‘Šæ”¶åˆ°çš„ç‚¹å‡»é‡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®åˆå§‹è®¾ç½®ã€‚æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œæˆ‘ä»¬åªè€ƒè™‘ä¸‰ä¸ªå…¬å‘Šï¼Œå®ƒä»¬åœ¨MABæ¨¡å‹ä¸­å¯¹åº”äºä¸‰æ¡è‡‚ï¼Œæ¯æ¡è‡‚éƒ½æœ‰ä¸åŒçš„ç‚¹å‡»æ¦‚ç‡ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼š
- en: 'We will use the following code to perform the analysis:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹ä»£ç è¿›è¡Œåˆ†æï¼š
- en: '[PRE0]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will analyze this code line by line. The first line loads the library:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é€è¡Œåˆ†æè¿™æ®µä»£ç ã€‚ç¬¬ä¸€è¡ŒåŠ è½½äº†åº“ï¼š
- en: '[PRE1]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This library contains many functions that allow for the simulation and evaluation
    of context-free and contextual MAB policies or algorithms.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥åº“åŒ…å«è®¸å¤šå‡½æ•°ï¼Œå…è®¸æ¨¡æ‹Ÿå’Œè¯„ä¼°æ— ä¸Šä¸‹æ–‡å’Œæœ‰ä¸Šä¸‹æ–‡çš„MABç­–ç•¥æˆ–ç®—æ³•ã€‚
- en: 'Now, let''s fix some necessary parameters to set the problem as MAB:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬å›ºå®šä¸€äº›å¿…è¦çš„å‚æ•°ï¼Œä»¥å°†é—®é¢˜è®¾å®šä¸ºMABï¼š
- en: '[PRE2]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The horizon is the number of rounds that must be played. Let''s set the number
    of simulations:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è§†é‡æ˜¯å¿…é¡»ç©çš„è½®æ¬¡æ•°ã€‚è®©æˆ‘ä»¬è®¾ç½®æ¨¡æ‹Ÿçš„æ¬¡æ•°ï¼š
- en: '[PRE3]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The number of simulationsÂ indicates how many times to repeat the simulation.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡æ‹Ÿæ¬¡æ•°è¡¨ç¤ºé‡å¤æ¨¡æ‹Ÿçš„æ¬¡æ•°ã€‚
- en: 'Let''s move on and set the probability that a user clicks on an advertisement:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç»§ç»­å¹¶è®¾ç½®ç”¨æˆ·ç‚¹å‡»å¹¿å‘Šçš„æ¦‚ç‡ï¼š
- en: '[PRE4]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A vector has been defined and contains the probabilities that each ad is clicked.
    In MAB terms, they represent the probabilities associated with the three arms.Â At
    this point, the initial parameters are fixed, so we can move on to defining the
    objects.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: å·²ç»å®šä¹‰äº†ä¸€ä¸ªå‘é‡ï¼ŒåŒ…å«æ¯ä¸ªå¹¿å‘Šè¢«ç‚¹å‡»çš„æ¦‚ç‡ã€‚åœ¨MABæœ¯è¯­ä¸­ï¼Œå®ƒä»¬è¡¨ç¤ºä¸ä¸‰æ¡è‡‚ç›¸å…³è”çš„æ¦‚ç‡ã€‚æ­¤æ—¶ï¼Œåˆå§‹å‚æ•°å·²ç»å›ºå®šï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç»§ç»­å®šä¹‰å¯¹è±¡ã€‚
- en: 'The first will be the bandit:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªå°†æ˜¯èµŒåšæœºï¼š
- en: '[PRE5]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, we used the `BasicBernoulliBandit()` function. This functionÂ simulates
    k Bernoulli arms, where each arm issues a reward of one with uniform probability
    p, otherwise a reward of zero.Â In a bandit scenario, this can be used to simulate
    a hit or miss event, such as if a user clicks on a headline, ad, or recommended
    product.Â Only one argument is expected (weights): it is a numeric vector that
    represents the probability of reward values for each of the bandit''s k arms.
    TheÂ `new()` method generates and instantiates a new `BasicBernoulliBandit` instance.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨äº†`BasicBernoulliBandit()`å‡½æ•°ã€‚è¿™ä¸ªå‡½æ•°æ¨¡æ‹Ÿkä¸ªä¼¯åŠªåˆ©è‡‚ï¼Œå…¶ä¸­æ¯ä¸ªè‡‚ä»¥å‡åŒ€æ¦‚ç‡på‘æ”¾å¥–åŠ±1ï¼Œå¦åˆ™å‘æ”¾å¥–åŠ±0ã€‚åœ¨ä¸€ä¸ªèµŒåšæœºåœºæ™¯ä¸­ï¼Œè¿™å¯ä»¥ç”¨æ¥æ¨¡æ‹Ÿä¸€ä¸ªå‘½ä¸­æˆ–æœªå‘½ä¸­çš„äº‹ä»¶ï¼Œæ¯”å¦‚ç”¨æˆ·æ˜¯å¦ç‚¹å‡»äº†ä¸€ä¸ªæ ‡é¢˜ã€å¹¿å‘Šæˆ–æ¨èçš„äº§å“ã€‚åªéœ€è¦ä¸€ä¸ªå‚æ•°ï¼ˆæƒé‡ï¼‰ï¼šå®ƒæ˜¯ä¸€ä¸ªæ•°å€¼å‘é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªèµŒåšæœºè‡‚çš„å¥–åŠ±æ¦‚ç‡ã€‚`new()`æ–¹æ³•ç”Ÿæˆå¹¶å®ä¾‹åŒ–ä¸€ä¸ªæ–°çš„`BasicBernoulliBandit`å®ä¾‹ã€‚
- en: 'Let''s move on to defining the policy:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥å®šä¹‰ç­–ç•¥ï¼š
- en: '[PRE6]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, we used the `EpsilonFirstPolicy()` function to implement a naive policy
    where a pure exploration phase is followed by a pure exploitation phase.Â Exploration
    happens within the first N time steps that are defined. During this time, at each
    time step t, `EpsilonFirstPolicy` selects an arm at random. Exploitation happens
    in the following steps, where we select the best arm up until N for either the
    remaining N trials or horizon T. Here, we used theÂ `new()` method, which generates
    a new `EpsilonFirstPolicy` object. Let''s move on:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨äº† `EpsilonFirstPolicy()` å‡½æ•°æ¥å®ç°ä¸€ä¸ªç®€å•çš„ç­–ç•¥ï¼Œå…¶ä¸­çº¯æ¢ç´¢é˜¶æ®µåè·Ÿçº¯åˆ©ç”¨é˜¶æ®µã€‚åœ¨å‰ N ä¸ªæ—¶é—´æ­¥å†…è¿›è¡Œæ¢ç´¢ã€‚åœ¨æ­¤æœŸé—´ï¼Œæ¯ä¸ªæ—¶é—´æ­¥
    tï¼Œ`EpsilonFirstPolicy` ä¼šéšæœºé€‰æ‹©ä¸€ä¸ª armã€‚æ¥ä¸‹æ¥çš„æ­¥éª¤ä¸­è¿›è¡Œåˆ©ç”¨ï¼Œæˆ‘ä»¬é€‰æ‹©æœ€ä½³çš„ armï¼Œç›´åˆ° N ä¸ºæ­¢ï¼Œæˆ–è€…åœ¨å‰©ä½™çš„ N æ¬¡è¯•éªŒæˆ–æ—¶é—´æ­¥é•¿
    T å†…è¿›è¡Œåˆ©ç”¨ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨äº† `new()` æ–¹æ³•æ¥ç”Ÿæˆä¸€ä¸ªæ–°çš„ `EpsilonFirstPolicy` å¯¹è±¡ã€‚æ¥ä¸‹æ¥ç»§ç»­ï¼š
- en: '[PRE7]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The agent classÂ is responsible for running one bandit/policy pair. The following
    arguments are available:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: agent ç±»è´Ÿè´£è¿è¡Œä¸€ä¸ª bandit/policy é…å¯¹ã€‚ä»¥ä¸‹æ˜¯å¯ç”¨çš„å‚æ•°ï¼š
- en: '`policy`: A policy instance.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`policy`ï¼šä¸€ä¸ªç­–ç•¥å®ä¾‹ã€‚'
- en: '`bandit`: A bandit instance.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bandit`ï¼šä¸€ä¸ª bandit å®ä¾‹ã€‚'
- en: '`name character`: Sets the name of the Agent. If NULL (default), the agent
    generates a name based on itsÂ policy instance''s name.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name character`ï¼šè®¾ç½® agent çš„åç§°ã€‚å¦‚æœä¸º NULLï¼ˆé»˜è®¤å€¼ï¼‰ï¼Œåˆ™ agent æ ¹æ®å…¶ç­–ç•¥å®ä¾‹çš„åç§°ç”Ÿæˆä¸€ä¸ªåç§°ã€‚'
- en: '`sparse numeric`: Artificially reduces the data size by setting a sparsity
    level for the current bandit and policy pair.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sparse numeric`ï¼šé€šè¿‡ä¸ºå½“å‰çš„ bandit å’Œ policy é…å¯¹è®¾ç½®ç¨€ç–åº¦æ°´å¹³ï¼Œäººå·¥é™ä½æ•°æ®å¤§å°ã€‚'
- en: 'In our case, only two arguments are passed:Â `policy` and `bandit`.Â Let''s move
    on to running the simulation:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œåªæœ‰ä¸¤ä¸ªå‚æ•°è¢«ä¼ é€’ï¼š`policy` å’Œ `bandit`ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬æ¥è¿è¡Œæ¨¡æ‹Ÿï¼š
- en: '[PRE8]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This is the entry point of any simulation.Â The `Simulator` class encloses one
    or more agents, creates an agent clone for each to be repeated simulation, runs
    the agents, and saves the log of all agent interactions in a history object. The
    following arguments are passed:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä»»ä½•æ¨¡æ‹Ÿçš„å…¥å£ç‚¹ã€‚`Simulator` ç±»å°è£…äº†ä¸€ä¸ªæˆ–å¤šä¸ª agentï¼Œé’ˆå¯¹æ¯ä¸ª agent åˆ›å»ºä¸€ä¸ªå…‹éš†è¿›è¡Œé‡å¤æ¨¡æ‹Ÿï¼Œè¿è¡Œè¿™äº› agentï¼Œå¹¶å°†æ‰€æœ‰
    agent äº¤äº’çš„æ—¥å¿—ä¿å­˜åœ¨å†å²å¯¹è±¡ä¸­ã€‚ä»¥ä¸‹å‚æ•°å°†è¢«ä¼ é€’ï¼š
- en: '`agent`:Â An agent instance or a list of agent instances'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`agent`ï¼šä¸€ä¸ª agent å®ä¾‹æˆ–ä¸€ä¸ª agent å®ä¾‹åˆ—è¡¨'
- en: '`horizon`:Â The number of pulls or time steps to run each agent, where t = 1,
    . . . , T (integer value)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`horizon`ï¼šè¿è¡Œæ¯ä¸ª agent çš„æ‹‰å–æ¬¡æ•°æˆ–æ—¶é—´æ­¥æ•°ï¼Œå…¶ä¸­ t = 1, . . . , Tï¼ˆæ•´æ•°å€¼ï¼‰'
- en: '`simulations`:Â How many times to repeat each agent''s simulation over t = 1,
    . . . , T, with a new seed on each repeat (integer value)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`simulations`ï¼šæ¯ä¸ª agent åœ¨ t = 1, . . . , T çš„æ¨¡æ‹Ÿé‡å¤æ¬¡æ•°ï¼Œæ¯æ¬¡é‡å¤æ—¶ä½¿ç”¨æ–°çš„éšæœºç§å­ï¼ˆæ•´æ•°å€¼ï¼‰'
- en: '`do_parallel`: IfÂ running simulator processes in parallel (logical value)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_parallel`ï¼šæ˜¯å¦å¹¶è¡Œè¿è¡Œæ¨¡æ‹Ÿå™¨è¿›ç¨‹ï¼ˆé€»è¾‘å€¼ï¼‰'
- en: 'For a detailed list of all the topics that are covered by the R6 class simulator,
    please refer to the official documentation: [https://CRAN.R-project.org/package=contextual](https://CRAN.R-project.org/package=contextual).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äº R6 ç±»æ¨¡æ‹Ÿå™¨è¦†ç›–çš„æ‰€æœ‰ä¸»é¢˜çš„è¯¦ç»†åˆ—è¡¨ï¼Œè¯·å‚é˜…å®˜æ–¹æ–‡æ¡£ï¼š[https://CRAN.R-project.org/package=contextual](https://CRAN.R-project.org/package=contextual)ã€‚
- en: 'Here, we used theÂ `new()`Â method to generate a newÂ simulator object. It''s
    time to run the simulation:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨äº† `new()` æ–¹æ³•æ¥ç”Ÿæˆä¸€ä¸ªæ–°çš„æ¨¡æ‹Ÿå™¨å¯¹è±¡ã€‚ç°åœ¨æ˜¯æ—¶å€™è¿è¡Œæ¨¡æ‹Ÿäº†ï¼š
- en: '[PRE9]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `run()` method simply runs a simulator instance.Â At this point, we have
    all the history of the simulation recorded in the history variable. Now, we can
    use this data to draw graphs. First, we will analyze the average reward:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`run()` æ–¹æ³•åªæ˜¯è¿è¡Œä¸€ä¸ªæ¨¡æ‹Ÿå™¨å®ä¾‹ã€‚åœ¨æ­¤æ—¶ï¼Œæˆ‘ä»¬å·²ç»å°†æ‰€æœ‰æ¨¡æ‹Ÿå†å²è®°å½•ä¿å­˜åœ¨å†å²å˜é‡ä¸­ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨è¿™äº›æ•°æ®ç»˜åˆ¶å›¾è¡¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†åˆ†æå¹³å‡å¥–åŠ±ï¼š'
- en: '[PRE10]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `plot()` functionÂ generates plots from the history data. The following
    plot types are available:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`plot()` å‡½æ•°æ ¹æ®å†å²æ•°æ®ç”Ÿæˆå›¾è¡¨ã€‚ä»¥ä¸‹æ˜¯å¯ç”¨çš„å›¾è¡¨ç±»å‹ï¼š'
- en: '`cumulative`: Plots the cumulative regret or reward over time. IfÂ regret=TRUE
    is passed, a cumulative regret is returned; ifÂ regret=FALSE is passed, a cumulative
    reward is returned.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cumulative`ï¼šç»˜åˆ¶ç´¯è®¡é—æ†¾æˆ–å¥–åŠ±éšæ—¶é—´å˜åŒ–çš„æ›²çº¿ã€‚å¦‚æœä¼ å…¥regret=TRUEï¼Œåˆ™è¿”å›ç´¯è®¡é—æ†¾ï¼›å¦‚æœä¼ å…¥regret=FALSEï¼Œåˆ™è¿”å›ç´¯è®¡å¥–åŠ±ã€‚'
- en: '`average`: Plots the average regret or reward.Â IfÂ regret=TRUE is passed, a
    cumulative regret is returned; ifÂ regret=FALSE is passed, a cumulative reward
    is returned.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`average`ï¼šç»˜åˆ¶å¹³å‡é—æ†¾æˆ–å¥–åŠ±çš„æ›²çº¿ã€‚å¦‚æœä¼ å…¥regret=TRUEï¼Œåˆ™è¿”å›ç´¯è®¡é—æ†¾ï¼›å¦‚æœä¼ å…¥regret=FALSEï¼Œåˆ™è¿”å›ç´¯è®¡å¥–åŠ±ã€‚'
- en: '`arms`: Plots the percentage of simulations per time step where each arm was
    chosen over time. If multiple agents have been run, it only plots the first agent.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`arms`ï¼šç»˜åˆ¶æ¯ä¸ªæ—¶é—´æ­¥ä¸­æ¯ä¸ª arm è¢«é€‰æ‹©çš„æ¨¡æ‹Ÿç™¾åˆ†æ¯”ã€‚å¦‚æœè¿è¡Œäº†å¤šä¸ª agentï¼Œåˆ™ä»…ç»˜åˆ¶ç¬¬ä¸€ä¸ª agentã€‚'
- en: 'The following plot is printed:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥çš„å›¾è¡¨å°†è¢«æ‰“å°ï¼š
- en: '![](img/24635d77-26f1-40f5-acbe-d84ea5160ff6.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24635d77-26f1-40f5-acbe-d84ea5160ff6.png)'
- en: 'Note that, after the first phase of exploration in which all the arms are tested
    in equal measure, we pass this to the exploitation phase, in which the arms that
    return the greatest rewards are preferred. The passage between the two phases
    can be seen by the net increase in the average reward. Let''s move on the cumulative
    plot:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œåœ¨ç¬¬ä¸€æ¬¡æ¢ç´¢é˜¶æ®µä¸­ï¼Œæ‰€æœ‰çš„è‡‚éƒ½è¢«å¹³ç­‰æµ‹è¯•åï¼Œæˆ‘ä»¬è¿›å…¥äº†åˆ©ç”¨é˜¶æ®µï¼Œåœ¨è¯¥é˜¶æ®µä¸­ï¼Œè¿”å›æœ€é«˜å¥–åŠ±çš„è‡‚ä¼šè¢«ä¼˜å…ˆé€‰æ‹©ã€‚ä¸¤ä¸ªé˜¶æ®µä¹‹é—´çš„è¿‡æ¸¡å¯ä»¥é€šè¿‡å¹³å‡å¥–åŠ±çš„å‡€å¢åŠ çœ‹åˆ°ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬æŸ¥çœ‹ç´¯ç§¯å›¾ï¼š
- en: '[PRE11]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following plot is returned:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å›¾è¡¨è¢«è¿”å›ï¼š
- en: '![](img/a7bc6048-9dcf-420a-85f0-305197e410d5.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7bc6048-9dcf-420a-85f0-305197e410d5.png)'
- en: 'The transition between the exploration and exploitation phases in this graph
    is even more evident. After the first 100 steps in which only the exploration
    phase has been used, the cumulative regret starts increasing with a logarithmic
    profile. Finally, we will plot the arms type:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¼ å›¾ä¸­æ¢ç´¢é˜¶æ®µå’Œåˆ©ç”¨é˜¶æ®µçš„è¿‡æ¸¡æ›´ä¸ºæ˜æ˜¾ã€‚åœ¨å‰ 100 æ­¥ä¸­ï¼Œåªæœ‰æ¢ç´¢é˜¶æ®µè¢«ä½¿ç”¨ï¼Œä¹‹åç´¯ç§¯é—æ†¾å¼€å§‹å‘ˆç°å¯¹æ•°å‹å¢é•¿ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ç»˜åˆ¶è‡‚çš„ç±»å‹ï¼š
- en: '[PRE12]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This type of plot returnsÂ the percentage of simulations per time step each
    arm was chosen overÂ time. The following plot is printed:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ç±»å‹çš„å›¾è¡¨è¿”å›äº†æ¯ä¸ªæ—¶é—´æ­¥éª¤ä¸­æ¯ä¸ªè‡‚è¢«é€‰æ‹©çš„æ¨¡æ‹Ÿæ¬¡æ•°ç™¾åˆ†æ¯”ã€‚ä»¥ä¸‹å›¾è¡¨è¢«æ‰“å°å‡ºæ¥ï¼š
- en: '![](img/0cf25153-6a72-484d-917b-fe1127469258.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0cf25153-6a72-484d-917b-fe1127469258.png)'
- en: From the preceding graph, we can see that the most chosen arm is the number
    3\. In fact, even in this case, after the first exploration phase in which the
    three arms are chosen with comparable percentages, the choice of arm that's passed
    to the exploitation phase falls exclusively on number 3.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å‰é¢çš„å›¾è¡¨ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æœ€å¸¸é€‰æ‹©çš„è‡‚æ˜¯ 3å·è‡‚ã€‚äº‹å®ä¸Šï¼Œå³ä½¿åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåœ¨é¦–æ¬¡æ¢ç´¢é˜¶æ®µä¸­ï¼Œä¸‰ä¸ªè‡‚çš„é€‰æ‹©æ¯”ä¾‹ç›¸è¿‘ï¼Œä½†è¿›å…¥åˆ©ç”¨é˜¶æ®µåï¼Œé€‰æ‹©çš„è‡‚å°±å®Œå…¨é›†ä¸­åœ¨äº†
    3å·è‡‚ä¸Šã€‚
- en: Online advertising Îµ-greedy-based policies
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŸºäº Îµ-greedy ç­–ç•¥çš„åœ¨çº¿å¹¿å‘Š
- en: 'Now, we will address the same problem by adopting an Îµ-greedy policy. As we
    mentioned previously, with this approach, we introduce an element of exploration
    that improves performance.Â The following code shows the analysis process when
    using the Îµ-greedy approach:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å°†é€šè¿‡é‡‡ç”¨ Îµ-greedy ç­–ç•¥æ¥è§£å†³åŒæ ·çš„é—®é¢˜ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ï¼Œä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¢ç´¢å…ƒç´ ï¼Œä»è€Œæé«˜äº†æ€§èƒ½ã€‚ä»¥ä¸‹ä»£ç å±•ç¤ºäº†ä½¿ç”¨
    Îµ-greedy æ–¹æ³•æ—¶çš„åˆ†æè¿‡ç¨‹ï¼š
- en: '[PRE13]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We will analyze this code line by line. The first line loads the library:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é€è¡Œåˆ†æè¿™æ®µä»£ç ã€‚ç¬¬ä¸€è¡ŒåŠ è½½äº†åº“ï¼š
- en: '[PRE14]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, let''s fix some necessary parameters to set the problem as MAB:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬è®¾ç½®ä¸€äº›å¿…è¦çš„å‚æ•°ï¼Œä»¥å°†é—®é¢˜è®¾ä¸º MABï¼š
- en: '[PRE15]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The horizon is the number of rounds that must be played. Let''s set the number
    of simulations:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: horizon æ˜¯å¿…é¡»è¿›è¡Œçš„å›åˆæ•°ã€‚è®©æˆ‘ä»¬è®¾ç½®æ¨¡æ‹Ÿæ¬¡æ•°ï¼š
- en: '[PRE16]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let''s move on and set the probability that a user clicks on an advertisement:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¾ç½®ç”¨æˆ·ç‚¹å‡»å¹¿å‘Šçš„æ¦‚ç‡ï¼š
- en: '[PRE17]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The same probability vector that we used in the previous example has been adopted.
    Let''sÂ move on and define the objects:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨äº†ä¸å‰ä¸€ä¸ªç¤ºä¾‹ç›¸åŒçš„æ¦‚ç‡å‘é‡ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰å¯¹è±¡ï¼š
- en: '[PRE18]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To define the bandit, we used theÂ `BasicBernoulliBandit()` function. This functionÂ simulates
    k Bernoulli arms, where each arm issues a reward of one with a uniform probability
    p, andÂ otherwise a reward of zero.Â Let''s move on to defining the policy:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®šä¹‰èµŒåšæœºï¼Œæˆ‘ä»¬ä½¿ç”¨äº† `BasicBernoulliBandit()` å‡½æ•°ã€‚è¿™ä¸ªå‡½æ•°æ¨¡æ‹Ÿäº† k ä¸ªä¼¯åŠªåˆ©è‡‚ï¼Œå…¶ä¸­æ¯ä¸ªè‡‚ä»¥å‡åŒ€æ¦‚ç‡ p å‘å‡ºå¥–åŠ±
    1ï¼Œå¦åˆ™å‘å‡ºå¥–åŠ± 0ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥å®šä¹‰ç­–ç•¥ï¼š
- en: '[PRE19]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here, we used theÂ `EpsilonGreedyPolicy()` function. This functionÂ chooses an
    arm at random with a probability epsilon in the exploration phase; otherwise,
    it greedily chooses the arm with the highest estimated reward in the exploitation
    phase. Only the epsilon argument is passed, indicating the probability that the
    arms are selected at random. The `new()` method is used toÂ generate a new `EpsilonGreedyPolicy`
    object. Finally, we will create an agent object, as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨äº† `EpsilonGreedyPolicy()` å‡½æ•°ã€‚è¯¥å‡½æ•°åœ¨æ¢ç´¢é˜¶æ®µä»¥æ¦‚ç‡ epsilon éšæœºé€‰æ‹©ä¸€ä¸ªè‡‚ï¼›å¦åˆ™ï¼Œå®ƒä¼šåœ¨åˆ©ç”¨é˜¶æ®µè´ªå©ªåœ°é€‰æ‹©å›æŠ¥æœ€é«˜çš„è‡‚ã€‚åªæœ‰
    epsilon å‚æ•°è¢«ä¼ å…¥ï¼Œè¡¨ç¤ºéšæœºé€‰æ‹©è‡‚çš„æ¦‚ç‡ã€‚`new()` æ–¹æ³•ç”¨äºç”Ÿæˆä¸€ä¸ªæ–°çš„ `EpsilonGreedyPolicy` å¯¹è±¡ã€‚æœ€åï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªä»£ç†å¯¹è±¡ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE20]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The agent classÂ is responsible for running one Bandit/Policy pair. Now, we
    can run the simulation:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç†ç±»è´Ÿè´£è¿è¡Œä¸€ä¸ª Bandit/Policy é…å¯¹ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œæ¨¡æ‹Ÿï¼š
- en: '[PRE21]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following results are printed:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ç»“æœè¢«æ‰“å°å‡ºæ¥ï¼š
- en: '[PRE22]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, we canÂ run the simulation:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œæ¨¡æ‹Ÿï¼š
- en: '[PRE23]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The history of the simulation that we carried out is recorded in the history
    variable. We can use this data to draw graphs.Â First, the regret average is plotted:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿›è¡Œçš„æ¨¡æ‹Ÿå†å²è®°å½•ä¿å­˜åœ¨ history å˜é‡ä¸­ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›æ•°æ®æ¥ç»˜åˆ¶å›¾è¡¨ã€‚é¦–å…ˆï¼Œç»˜åˆ¶äº†é—æ†¾å¹³å‡å€¼å›¾ï¼š
- en: '[PRE24]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following plot is returned:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å›¾è¡¨è¢«è¿”å›ï¼š
- en: '![](img/16345297-ced0-4d21-9e49-785df4a4b960.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16345297-ced0-4d21-9e49-785df4a4b960.png)'
- en: 'Then, we plot the cumulative regret:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬ç»˜åˆ¶ç´¯è®¡é—æ†¾å›¾ï¼š
- en: '[PRE25]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following plot is returned:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›ä»¥ä¸‹å›¾è¡¨ï¼š
- en: '![](img/7d982342-a28b-48ef-998e-b4d89fb3be17.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d982342-a28b-48ef-998e-b4d89fb3be17.png)'
- en: 'Finally, the arm choice in percent is plotted:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œç»˜åˆ¶äº†è‡‚é€‰æ‹©çš„ç™¾åˆ†æ¯”å›¾ï¼š
- en: '[PRE26]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following plot is returned:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›ä»¥ä¸‹å›¾è¡¨ï¼š
- en: '![](img/22cd39d2-2341-4381-967e-3b0d35639a76.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22cd39d2-2341-4381-967e-3b0d35639a76.png)'
- en: In the preceding three graphs, a feature is highlighted (**Arm choice %**).
    The reduction of the regret over time is progressive and does not undergo a discontinuity
    like it did in the previous simulation (context-free policies-based example).
    This is due to the fact that the algorithm simultaneously carries out the exploration
    and exploitation phase. On one hand, it uniformly explores one of the advertisements
    randomly Îµ of the time, while on the other hand, it exploits the ad with the best
    current click rate 1-Îµ of the time.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰é¢ä¸‰ä¸ªå›¾ä¸­ï¼Œçªå‡ºäº†ä¸€ä¸ªç‰¹å¾ï¼ˆ**Arm choice %**ï¼‰ã€‚é—æ†¾éšæ—¶é—´çš„å‡å°‘æ˜¯é€æ¸çš„ï¼Œå¹¶ä¸åƒä¹‹å‰çš„ä»¿çœŸï¼ˆåŸºäºæ— ä¸Šä¸‹æ–‡ç­–ç•¥çš„ä¾‹å­ï¼‰é‚£æ ·å‡ºç°ä¸è¿ç»­ã€‚è¿™æ˜¯å› ä¸ºç®—æ³•åŒæ—¶è¿›è¡Œäº†æ¢ç´¢å’Œåˆ©ç”¨é˜¶æ®µã€‚ä¸€æ–¹é¢ï¼Œå®ƒåœ¨
    Îµ çš„æ—¶é—´é‡Œéšæœºå‡åŒ€åœ°æ¢ç´¢æŸä¸ªå¹¿å‘Šï¼Œè€Œå¦ä¸€æ–¹é¢ï¼Œå®ƒåœ¨ 1-Îµ çš„æ—¶é—´é‡Œåˆ©ç”¨ç‚¹å‡»ç‡æœ€é«˜çš„å¹¿å‘Šã€‚
- en: Online advertising context-based policies
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨çº¿å¹¿å‘ŠåŸºäºä¸Šä¸‹æ–‡çš„ç­–ç•¥
- en: 'What happens if visitors are divided into two categories, male and female,
    or young and adult? We can introduce a reference to the context in the models
    we''ve analyzed so far. Recall that, in the contextual bandit, the concept of
    state was introduced, which represents a description of the environment that the
    agent can use to carry out targeted actions. This model extends the original one
    by linking the decision to the state of the environment.Â The following code performs
    an analysis using the context approach:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå°†è®¿é—®è€…åˆ†ä¸ºç”·æ€§å’Œå¥³æ€§ï¼Œæˆ–è€…å¹´è½»äººå’Œæˆå¹´äººä¸¤ç±»ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆæƒ…å†µï¼Ÿæˆ‘ä»¬å¯ä»¥åœ¨è¿„ä»Šä¸ºæ­¢åˆ†æçš„æ¨¡å‹ä¸­å¼•å…¥å¯¹ä¸Šä¸‹æ–‡çš„å¼•ç”¨ã€‚å›æƒ³ä¸€ä¸‹ï¼Œåœ¨ä¸Šä¸‹æ–‡ Bandit
    æ¨¡å‹ä¸­ï¼Œå¼•å…¥äº†çŠ¶æ€çš„æ¦‚å¿µï¼ŒçŠ¶æ€ä»£è¡¨äº†ä»£ç†å¯ä»¥ç”¨æ¥æ‰§è¡Œæœ‰é’ˆå¯¹æ€§è¡ŒåŠ¨çš„ç¯å¢ƒæè¿°ã€‚è¯¥æ¨¡å‹é€šè¿‡å°†å†³ç­–ä¸ç¯å¢ƒçŠ¶æ€å…³è”ï¼Œæ‰©å±•äº†åŸå§‹æ¨¡å‹ã€‚ä»¥ä¸‹ä»£ç ä½¿ç”¨ä¸Šä¸‹æ–‡æ–¹æ³•è¿›è¡Œåˆ†æï¼š
- en: '[PRE27]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We will analyze this code line by line. The first line loads the library:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é€è¡Œåˆ†æè¿™æ®µä»£ç ã€‚ç¬¬ä¸€è¡ŒåŠ è½½äº†åº“ï¼š
- en: '[PRE28]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, let''s fix some of the necessary parameters to set the problem as MAB:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬è®¾ç½®ä¸€äº›å¿…è¦çš„å‚æ•°ï¼Œå°†é—®é¢˜è®¾ç½®ä¸º MABï¼š
- en: '[PRE29]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The horizon is the number of rounds that must be played. Let''s set the number
    of simulations:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨ªå‘è½´è¡¨ç¤ºå¿…é¡»è¿›è¡Œçš„è½®æ¬¡æ•°ã€‚æˆ‘ä»¬æ¥è®¾ç½®ä»¿çœŸæ¬¡æ•°ï¼š
- en: '[PRE30]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let''s move on and set the probability that a user clicks on an advertisement.
    In this case, the reference to a binary context is introduced. Two possible probability
    distributions are defined, both of which correspond to two user profiles:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ç»§ç»­è®¾ç½®ç”¨æˆ·ç‚¹å‡»å¹¿å‘Šçš„æ¦‚ç‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¼•å…¥äº†å¯¹äºŒå…ƒä¸Šä¸‹æ–‡çš„å¼•ç”¨ã€‚å®šä¹‰äº†ä¸¤ç§å¯èƒ½çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå®ƒä»¬åˆ†åˆ«å¯¹åº”ä¸¤ç§ç”¨æˆ·ç”»åƒï¼š
- en: '[PRE31]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The first profile has a probability vector equal to (0.1, 0.3, 0.7), while
    the second has a probability vector equal to (0.8, 0.4, 0.1). Let''sÂ move on and
    define the objects:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªç”»åƒçš„æ¦‚ç‡å‘é‡ä¸º (0.1, 0.3, 0.7)ï¼Œè€Œç¬¬äºŒä¸ªç”»åƒçš„æ¦‚ç‡å‘é‡ä¸º (0.8, 0.4, 0.1)ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ç»§ç»­å®šä¹‰å¯¹è±¡ï¼š
- en: '[PRE32]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'To define the bandit, we used theÂ `ContextualBinaryBandit()` function. This
    functionÂ simulates a contextual Bernoulli MAB problem, where at least one context
    feature is active at a time. In this case, theÂ weights argument contains a d x
    k numeric matrix with probabilities of reward for d contextual features per kÂ arms.Â Let''s
    move on and define the policy:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®šä¹‰ Banditï¼Œæˆ‘ä»¬ä½¿ç”¨äº† `ContextualBinaryBandit()` å‡½æ•°ã€‚è¯¥å‡½æ•°æ¨¡æ‹Ÿäº†ä¸€ä¸ªä¸Šä¸‹æ–‡ Bernoulli MAB é—®é¢˜ï¼Œå…¶ä¸­è‡³å°‘æœ‰ä¸€ä¸ªä¸Šä¸‹æ–‡ç‰¹å¾åœ¨ä»»ä½•æ—¶åˆ»æ˜¯æ´»åŠ¨çš„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`weights`
    å‚æ•°åŒ…å«ä¸€ä¸ª d x k çš„æ•°å€¼çŸ©é˜µï¼Œå…¶ä¸­åŒ…å«äº†æ¯ä¸ª k ä¸ªè‡‚çš„ d ä¸ªä¸Šä¸‹æ–‡ç‰¹å¾çš„å¥–åŠ±æ¦‚ç‡ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ç»§ç»­å®šä¹‰ç­–ç•¥ï¼š
- en: '[PRE33]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here, we used theÂ `EpsilonGreedyPolicy()` function. Finally, we will create
    an agent object, as follows:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨äº† `EpsilonGreedyPolicy()` å‡½æ•°ã€‚æœ€åï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªä»£ç†å¯¹è±¡ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE34]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The agent classÂ is responsible for running one Bandit/Policy pair. Now, we
    can run the simulation:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç†ç±»è´Ÿè´£è¿è¡Œä¸€ä¸ª Bandit/Policy é…å¯¹ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œä»¿çœŸï¼š
- en: '[PRE35]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The following results are printed:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºä»¥ä¸‹ç»“æœï¼š
- en: '[PRE36]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, we canÂ store the simulation:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å­˜å‚¨ä»¿çœŸç»“æœï¼š
- en: '[PRE37]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The history of the simulation is recorded in the history variable. Now, we
    can use this data to draw graphs.Â In this case, only the arm choice in percent
    is plotted:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¿çœŸçš„å†å²è®°å½•ä¿å­˜åœ¨ history å˜é‡ä¸­ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›æ•°æ®ç»˜åˆ¶å›¾è¡¨ã€‚åœ¨æ­¤æƒ…å†µä¸‹ï¼Œä»…ç»˜åˆ¶äº†è‡‚é€‰æ‹©çš„ç™¾åˆ†æ¯”ï¼š
- en: '[PRE38]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The following plot is returned:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›ä»¥ä¸‹å›¾è¡¨ï¼š
- en: '![](img/d992f12f-5ab6-4a6c-b505-3067ad9155b7.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d992f12f-5ab6-4a6c-b505-3067ad9155b7.png)'
- en: From this analysis, it is clear that the most chosen arm in percentage is number
    1, then number 3, and finally number 2.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™ä¸ªåˆ†æä¸­å¯ä»¥çœ‹å‡ºï¼Œæœ€å¸¸é€‰æ‹©çš„è‡‚æ˜¯ç¬¬1å·è‡‚ï¼Œç„¶åæ˜¯ç¬¬3å·è‡‚ï¼Œæœ€åæ˜¯ç¬¬2å·è‡‚ã€‚
- en: Comparison between solution techniques
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è§£å†³æŠ€æœ¯æ¯”è¾ƒ
- en: 'Several policies are available for the solution of the MAB problem. In the*Â Problem
    solution techniques* section, we analyzed some of them. The contextual packageÂ proposes
    some policies. The following is a list of those available policies:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: è§£å†³MABé—®é¢˜æœ‰å‡ ç§ç­–ç•¥ã€‚åœ¨*é—®é¢˜è§£å†³æŠ€æœ¯*éƒ¨åˆ†ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸€äº›ç­–ç•¥ã€‚ä¸Šä¸‹æ–‡åŒ…æå‡ºäº†ä¸€äº›ç­–ç•¥ã€‚ä»¥ä¸‹æ˜¯å¯ç”¨ç­–ç•¥çš„åˆ—è¡¨ï¼š
- en: '`ContextualEpochGreedyPolicy`'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ä¸Šä¸‹æ–‡æ—¶æœŸè´ªå¿ƒç­–ç•¥`'
- en: '`ContextualEpsilonGreedyPolicy`'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ä¸Šä¸‹æ–‡Îµ-è´ªå¿ƒç­–ç•¥`'
- en: '`ContextualLogitBTSPolicy`'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ä¸Šä¸‹æ–‡LogitBTSç­–ç•¥`'
- en: '`` `ContextualTSProbitPolicy` ``'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`` `ä¸Šä¸‹æ–‡TSProbitç­–ç•¥` ``'
- en: '`EpsilonFirstPolicy`'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Îµ-è´ªå¿ƒç­–ç•¥`'
- en: '`EpsilonGreedyPolicy`'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Îµ-è´ªå¿ƒç­–ç•¥`'
- en: '`Exp3Policy`'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Exp3ç­–ç•¥`'
- en: '`FixedPolicy`'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`å›ºå®šç­–ç•¥`'
- en: '`GittinsBrezziLaiPolicy`'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GittinsBrezziLaiç­–ç•¥`'
- en: '`GradientPolicy`'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`æ¢¯åº¦ç­–ç•¥`'
- en: '`LifPolicy`'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Lifç­–ç•¥`'
- en: '`LinUCBDisjointOptimizedPolicy`'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LinUCBä¸ç›¸äº¤ä¼˜åŒ–ç­–ç•¥`'
- en: '`LinUCBDisjointPolicy`'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LinUCBä¸ç›¸äº¤ç­–ç•¥`'
- en: '`LinUCBGeneralPolicy`'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LinUCBé€šç”¨ç­–ç•¥`'
- en: '`LinUCBHybridOptimizedPolicy`'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LinUCBæ··åˆä¼˜åŒ–ç­–ç•¥`'
- en: '`LinUCBHybridPolicy`'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LinUCBæ··åˆç­–ç•¥`'
- en: '`OraclePolicy`'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Oracleç­–ç•¥`'
- en: '`RandomPolicy`'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`éšæœºç­–ç•¥`'
- en: '`SoftmaxPolicy`'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Softmaxç­–ç•¥`'
- en: '`ThompsonSamplingPolicy`'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`æ±¤æ™®æ£®é‡‡æ ·ç­–ç•¥`'
- en: '`UCB1Policy`'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UCB1ç­–ç•¥`'
- en: '`UCB2Policy`'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UCB2ç­–ç•¥`'
- en: 'For a detailed description of these policies, please refer to the official
    documentation of the package, which is available at the following URL: [https://CRAN.R-project.org/package=contextual](https://cran.r-project.org/package=contextual).'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³è¿™äº›ç­–ç•¥çš„è¯¦ç»†æè¿°ï¼Œè¯·å‚é˜…è¯¥åŒ…çš„å®˜æ–¹æ–‡æ¡£ï¼Œæ–‡æ¡£å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è®¿é—®ï¼š[https://CRAN.R-project.org/package=contextual](https://cran.r-project.org/package=contextual)ã€‚
- en: 'Some of these have already been adopted in the examples we''ve looked at in
    this chapter. To analyze the characteristics of some of these, we can make a comparison
    based on the results we''ve obtained. Here is the full code:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ä¸€äº›ç­–ç•¥å·²ç»åœ¨æˆ‘ä»¬æœ¬ç« æŸ¥çœ‹çš„ç¤ºä¾‹ä¸­é‡‡ç”¨è¿‡ã€‚ä¸ºäº†åˆ†æè¿™äº›ç­–ç•¥çš„ç‰¹ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥åŸºäºå·²ç»è·å¾—çš„ç»“æœè¿›è¡Œæ¯”è¾ƒã€‚ä»¥ä¸‹æ˜¯å®Œæ•´çš„ä»£ç ï¼š
- en: '[PRE39]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We will analyze this code line by line. The first line loads the library:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é€è¡Œåˆ†æè¿™æ®µä»£ç ã€‚ç¬¬ä¸€è¡ŒåŠ è½½äº†åº“ï¼š
- en: '[PRE40]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, let''s fix some of the necessary parameters to set the problem as MAB:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬è®¾ç½®ä¸€äº›å¿…è¦çš„å‚æ•°ï¼Œå°†é—®é¢˜è®¾ç½®ä¸ºMABï¼š
- en: '[PRE41]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let''s move on and set the probability that a user clicks on an advertisement:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬ç»§ç»­è®¾ç½®ç”¨æˆ·ç‚¹å‡»å¹¿å‘Šçš„æ¦‚ç‡ï¼š
- en: '[PRE42]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let''s move on and define the bandit object:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ç»§ç»­å®šä¹‰èµŒåšæœºå¯¹è±¡ï¼š
- en: '[PRE43]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Here, we used theÂ `BasicBernoulliBandit()` function. Now,Â we will create a
    list of agent objects, as follows:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨äº†`BasicBernoulliBandit()`å‡½æ•°ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªä»£ç†å¯¹è±¡çš„åˆ—è¡¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE44]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Six agents were defined with different policies, as follows:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰äº†å…­ä¸ªå…·æœ‰ä¸åŒç­–ç•¥çš„ä»£ç†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '`OraclePolicy`: This policy knows the reward probabilities at all times, and
    will always play the optimal arm. It is often used as a baseline to compare other
    policies.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Oracleç­–ç•¥`ï¼šæ­¤ç­–ç•¥å§‹ç»ˆçŸ¥é“å¥–åŠ±æ¦‚ç‡ï¼Œå¹¶å°†å§‹ç»ˆé€‰æ‹©æœ€ä¼˜è‡‚ã€‚å®ƒé€šå¸¸ä½œä¸ºåŸºå‡†ï¼Œç”¨äºä¸å…¶ä»–ç­–ç•¥è¿›è¡Œæ¯”è¾ƒã€‚'
- en: '`UCB1Policy`:Â This policyÂ constructs an optimistic estimate in the form of
    an Upper Confidence Bound to create an estimate of the expected payoff of each
    action and picks the action with the highest estimate. If the guess is wrong,
    the optimistic guess quickly decreases until another action has a higher estimate.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UCB1ç­–ç•¥`ï¼šæ­¤ç­–ç•¥æ„å»ºäº†ä¸€ä¸ªä¹è§‚çš„ä¼°è®¡ï¼Œä»¥ä¸Šç½®ä¿¡åŒºé—´çš„å½¢å¼åˆ›å»ºæ¯ä¸ªåŠ¨ä½œçš„é¢„æœŸå›æŠ¥ä¼°è®¡ï¼Œå¹¶é€‰æ‹©å…·æœ‰æœ€é«˜ä¼°è®¡å€¼çš„åŠ¨ä½œã€‚å¦‚æœä¼°è®¡é”™è¯¯ï¼Œä¹è§‚ä¼°è®¡ä¼šè¿…é€Ÿä¸‹é™ï¼Œç›´åˆ°å¦ä¸€ä¸ªåŠ¨ä½œçš„ä¼°è®¡å€¼æ›´é«˜ã€‚'
- en: '`ThompsonSamplingPolicy`:Â The procedure that''s followed by this policy exploits
    the memory of the average rewards of the weapons. To do this, use a beta-binomial
    model with alpha and beta parameters, sample the values for each arm from the
    previous step, and select the arm with the highest value. When an arm is pulled
    and a Bernoulli reward is observed, it modifies the prior based on the reward.
    This procedure is repeated for the next arm pull.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`æ±¤æ™®æ£®é‡‡æ ·ç­–ç•¥`ï¼šæ­¤ç­–ç•¥çš„è¿‡ç¨‹åˆ©ç”¨äº†æ­¦å™¨çš„å¹³å‡å¥–åŠ±è®°å¿†ã€‚ä¸ºæ­¤ï¼Œä½¿ç”¨å¸¦æœ‰alphaå’Œbetaå‚æ•°çš„beta-äºŒé¡¹å¼æ¨¡å‹ï¼Œä»ä¸Šä¸€æ­¥ä¸­ä¸ºæ¯ä¸ªè‡‚é‡‡æ ·å€¼ï¼Œå¹¶é€‰æ‹©å…·æœ‰æœ€é«˜å€¼çš„è‡‚ã€‚å½“æ‹‰åŠ¨ä¸€ä¸ªè‡‚å¹¶è§‚å¯Ÿåˆ°ä¸€ä¸ªä¼¯åŠªåˆ©å¥–åŠ±æ—¶ï¼Œå®ƒæ ¹æ®å¥–åŠ±ä¿®æ”¹å…ˆéªŒã€‚æ­¤è¿‡ç¨‹ä¼šä¸ºä¸‹ä¸€æ¬¡æ‹‰è‡‚é‡å¤ã€‚'
- en: '`EpsilonGreedyPolicy`:Â The procedure that''s followed by this policy foresees
    the random choice of an arm with epsilon probability to explore the environment.
    Otherwise, an arm is chosen greedily, with the highest estimated reward. By doing
    this, the agent exploits the information that was acquired in the previous steps.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EpsilonGreedyPolicy`ï¼šè¯¥ç­–ç•¥éµå¾ªçš„ç¨‹åºæ˜¯ä»¥ epsilon æ¦‚ç‡éšæœºé€‰æ‹©ä¸€ä¸ªè‡‚æ¥æ¢ç´¢ç¯å¢ƒã€‚å¦åˆ™ï¼Œå®ƒä¼šè´ªå©ªåœ°é€‰æ‹©ä¼°è®¡å¥–åŠ±æœ€é«˜çš„è‡‚ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä»£ç†å¯ä»¥åˆ©ç”¨å‰ä¸€æ­¥è·å¾—çš„ä¿¡æ¯ã€‚'
- en: '`SoftmaxPolicy`: This policyÂ selects an arm based on the probability from the
    Boltmann distribution. It makes use of a temperature parameter, tau, which specifies
    how many arms we can explore. When tau is high, all the arms are explored equally,
    and, when tau is low, the arms offering higher rewards will be chosen.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SoftmaxPolicy`ï¼šè¯¥ç­–ç•¥åŸºäº Boltmann åˆ†å¸ƒä¸­çš„æ¦‚ç‡é€‰æ‹©ä¸€ä¸ªè‡‚ã€‚å®ƒä½¿ç”¨äº†ä¸€ä¸ªæ¸©åº¦å‚æ•° tauï¼ŒæŒ‡å®šæˆ‘ä»¬å¯ä»¥æ¢ç´¢å¤šå°‘ä¸ªè‡‚ã€‚å½“ tau
    è¾ƒé«˜æ—¶ï¼Œæ‰€æœ‰è‡‚çš„æ¢ç´¢æœºä¼šç›¸åŒï¼›è€Œå½“ tau è¾ƒä½æ—¶ï¼Œç³»ç»Ÿä¼šé€‰æ‹©é‚£äº›æä¾›æ›´é«˜å¥–åŠ±çš„è‡‚ã€‚'
- en: '`Exp3Policy`:Â The procedure that''s followed by this policy uses a probability
    distribution, which is a mixture of a uniform distribution. It also uses a distribution
    that assigns each action an exponential mass of probability in the cumulative
    reward that was estimated for that action.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Exp3Policy`ï¼šè¯¥ç­–ç•¥éµå¾ªçš„ç¨‹åºä½¿ç”¨æ¦‚ç‡åˆ†å¸ƒï¼Œå®ƒæ˜¯å‡åŒ€åˆ†å¸ƒçš„æ··åˆç‰©ã€‚å®ƒè¿˜ä½¿ç”¨ä¸€ç§åˆ†å¸ƒï¼Œå°†æ¯ä¸ªåŠ¨ä½œçš„æ¦‚ç‡è´¨é‡åˆ†é…åœ¨è¯¥åŠ¨ä½œçš„ç´¯ç§¯å¥–åŠ±çš„æŒ‡æ•°å€¼ä¸Šã€‚'
- en: 'Now, we can run the simulation and store its history:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œä»¿çœŸå¹¶å­˜å‚¨å…¶å†å²è®°å½•ï¼š
- en: '[PRE45]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Finally, we will plot the cumulative regrets of the agents that we simulated:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å°†ç»˜åˆ¶æˆ‘ä»¬æ¨¡æ‹Ÿçš„ä»£ç†çš„ç´¯ç§¯æ‚”æ¨å€¼ï¼š
- en: '[PRE46]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The following plot is returned:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›ä»¥ä¸‹å›¾è¡¨ï¼š
- en: '![](img/dd38e685-12b0-4d9c-be60-e1c1a832ae86.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd38e685-12b0-4d9c-be60-e1c1a832ae86.png)'
- en: As we anticipated, the OraclePolicy is the one that presents the lowest values
    in absolute of the regret, which means it succeeds in making better use of the
    arm that supplies better results.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬é¢„æœŸçš„é‚£æ ·ï¼ŒOraclePolicy åœ¨ç»å¯¹æ‚”æ¨å€¼ä¸Šè¡¨ç°æœ€ä½ï¼Œè¿™æ„å‘³ç€å®ƒæˆåŠŸåœ°æ›´å¥½åœ°åˆ©ç”¨äº†æä¾›æ›´å¥½ç»“æœçš„è‡‚ã€‚
- en: Summary
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: In this chapter, we have learned about the basic concepts of the multi-armed
    bandit model. This model is based on the dilemma of exploration and exploitation.
    In the case of limited resources, which is what we base our choices on, it is
    essential to know which competitive alternatives allow us to maximize the expected
    profit. The name derives from the example of a player struggling with a row of
    slot machines, who must decide whether to continue with the current machine or
    try another machine.Â A mathematical model of the problem was described. Then,
    we discovered the meaning of the action-value implementation and how it differs
    from the value function. The state-value-function contains the value of reaching
    a certain state, while the action-value-function contains the value for choosing
    an action in a state.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¤šè‡‚å¼ºç›—æ¨¡å‹çš„åŸºæœ¬æ¦‚å¿µã€‚è¯¥æ¨¡å‹åŸºäºæ¢ç´¢ä¸å¼€å‘çš„å›°å¢ƒã€‚åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼ŒåŸºäºæˆ‘ä»¬åšå‡ºé€‰æ‹©çš„æ ‡å‡†ï¼Œäº†è§£å“ªäº›ç«äº‰æ€§æ›¿ä»£æ–¹æ¡ˆå¯ä»¥æœ€å¤§åŒ–é¢„æœŸåˆ©æ¶¦è‡³å…³é‡è¦ã€‚è¯¥åç§°æºäºä¸€ä¸ªä¾‹å­ï¼Œæè¿°äº†ä¸€ä½ç©å®¶åœ¨é¢å¯¹ä¸€æ’è€è™æœºæ—¶çš„å›°å¢ƒï¼Œä»–å¿…é¡»å†³å®šæ˜¯å¦ç»§ç»­ä½¿ç”¨å½“å‰æœºå™¨è¿˜æ˜¯å°è¯•å¦ä¸€å°æœºå™¨ã€‚æˆ‘ä»¬æè¿°äº†è¯¥é—®é¢˜çš„æ•°å­¦æ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬äº†è§£äº†è¡ŒåŠ¨ä»·å€¼å®ç°çš„å«ä¹‰ï¼Œå¹¶æ¢è®¨äº†å®ƒä¸ä»·å€¼å‡½æ•°çš„åŒºåˆ«ã€‚çŠ¶æ€å€¼å‡½æ•°åŒ…å«åˆ°è¾¾æŸä¸ªçŠ¶æ€çš„å€¼ï¼Œè€Œè¡ŒåŠ¨å€¼å‡½æ•°åŒ…å«åœ¨æŸä¸ªçŠ¶æ€ä¸‹é€‰æ‹©ä¸€ä¸ªè¡ŒåŠ¨çš„å€¼ã€‚
- en: Several problem solution techniques were analyzed and the contextual approach
    was addressed. We also looked at a list of practical applications of the MAB problem.
    Finally, online advertising was treated using several MAB models.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†æäº†å‡ ç§é—®é¢˜æ±‚è§£æŠ€æœ¯ï¼Œå¹¶è®¨è®ºäº†ä¸Šä¸‹æ–‡æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜æŸ¥çœ‹äº† MAB é—®é¢˜çš„ä¸€äº›å®é™…åº”ç”¨åˆ—è¡¨ã€‚æœ€åï¼Œä½¿ç”¨å‡ ä¸ª MAB æ¨¡å‹å¤„ç†äº†åœ¨çº¿å¹¿å‘Šé—®é¢˜ã€‚
- en: In the next chapter, we will learn about the basic concepts of optimization
    techniques. We will learn how to decompose a problem into subproblems and how
    to implement the various optimization techniques. Then, we will understand the
    difference between recursion and memoization and discover how to use the dynamic
    programming approach to make the most convenient choices.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ ä¼˜åŒ–æŠ€æœ¯çš„åŸºæœ¬æ¦‚å¿µã€‚æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•å°†ä¸€ä¸ªé—®é¢˜åˆ†è§£ä¸ºå­é—®é¢˜ï¼Œå¹¶å¦‚ä½•å®ç°å„ç§ä¼˜åŒ–æŠ€æœ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ç†è§£é€’å½’ä¸è®°å¿†åŒ–çš„åŒºåˆ«ï¼Œå¹¶å‘ç°å¦‚ä½•ä½¿ç”¨åŠ¨æ€è§„åˆ’æ–¹æ³•åšå‡ºæœ€åˆé€‚çš„é€‰æ‹©ã€‚
