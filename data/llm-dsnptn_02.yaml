- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Data Cleaning for LLM Training
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM训练的数据清洁
- en: In this chapter, we’ll dive into the **data cleaning** pattern for LLM training.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨LLM训练中的**数据清洁**模式。
- en: Clean, high-quality data is the foundation of robust and reliable language models.
    We’ll explore common data quality issues, preprocessing techniques, and strategies
    for handling diverse data types. *Figure 2**.1* depicts a data cleaning pipeline
    specifically designed for processing raw text data before it’s used to train language
    models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 清洁、高质量的数据是构建稳健和可靠的语言模型的基础。我们将探讨常见的数据质量问题、预处理技术和处理不同数据类型的策略。*图2.1*展示了专门设计用于在用于训练语言模型之前处理原始文本数据的数据清洁流程。
- en: '![Figure 2.1 – Data cleaning pipeline](img/B31249_02_01.jpg)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 – 数据清洁流程](img/B31249_02_01.jpg)'
- en: Figure 2.1 – Data cleaning pipeline
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 数据清洁流程
- en: The process begins with an initial data quality check to assess the raw data’s
    suitability. Following this, text preprocessing and deduplication steps are applied
    to refine and streamline the dataset. If the data fails to meet the required standards
    at any point, it is rerouted through an automated cleaning pipeline for additional
    processing. Successful completion of this stage leads to data validation to ensure
    the dataset’s integrity and compliance with training standards. If the data passes
    validation, it is marked as clean and ready for use in language model training,
    ensuring high-quality input for effective model development.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程从初步的数据质量检查开始，以评估原始数据的适用性。随后，应用文本预处理和去重步骤以精炼和简化数据集。如果在任何点上数据未能达到所需标准，它将通过自动化清洁流程进行额外处理。成功完成此阶段后，进行数据验证以确保数据集的完整性和符合训练标准。如果数据通过验证，则标记为清洁并准备好用于语言模型训练，确保为有效模型开发提供高质量输入。
- en: By the end of this chapter, you’ll be equipped with practical tools and techniques
    to clean your data for LLM training.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将具备用于为LLM训练清洁数据的实用工具和技术。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding the importance of clean data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解清洁数据的重要性
- en: Common data quality issues in language datasets
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言数据集中常见的质量问题
- en: Text preprocessing techniques for LLMs
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于LLM的文本预处理技术
- en: Handling multilingual and code-mixed data
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理多语言和代码混合数据
- en: Deduplication strategies for large text corpora
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型文本语料库的去重策略
- en: Automated data cleaning pipelines
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化数据清洁流程
- en: Data validation and quality assurance
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据验证和质量保证
- en: Understanding the importance of clean data
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解清洁数据的重要性
- en: The quality of data used in training LLMs directly impacts their performance
    and reliability. When we train LLMs on noisy or inconsistent data, we risk introducing
    bias, errors, and inconsistency into the model’s learned representations and outputs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练LLM的数据质量直接影响其性能和可靠性。当我们使用嘈杂或不一致的数据训练LLM时，我们可能会将偏差、错误和不一致性引入模型的学习表示和输出中。
- en: To illustrate the impact of data quality on LLM performance, we can use a simple
    Python script to compare the perplexity scores of models trained on clean and
    noisy data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明数据质量对LLM性能的影响，我们可以使用一个简单的Python脚本来比较在清洁和嘈杂数据上训练的模型的混淆度得分。
- en: 'First, install the necessary packages and import them:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，安装必要的包并导入它们：
- en: '[PRE0]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, define the initial part of the function:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，定义函数的初始部分：
- en: '[PRE1]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `calculate_perplexity` function tokenizes the input text into PyTorch tensors
    using the provided tokenizer. It then passes the tokenized input to the model
    with `input_ids` also used as labels, allowing the model to compute a loss representing
    prediction error. This loss is exponentiated to derive a scalar perplexity score
    and returned as a Python float.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`calculate_perplexity`函数使用提供的分词器将输入文本分词成PyTorch张量。然后，它将分词后的输入传递给模型，其中`input_ids`也用作标签，允许模型计算表示预测错误的损失。该损失被指数化以推导出一个标量混淆度得分，并以Python浮点数的形式返回。'
- en: The second part of the code initializes a language model and tokenizer using
    `GPT4LMHeadModel.from_pretrained("GPT4")` and `GPT4Tokenizer.from_pretrained("GPT4")`,
    which load the model and tokenizer weights from a pre-trained source identified
    as `"GPT4"`.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码的第二部分初始化了一个语言模型和分词器，使用`GPT4LMHeadModel.from_pretrained("GPT4")`和`GPT4Tokenizer.from_pretrained("GPT4")`，从标识为`"GPT4"`的预训练源加载模型和分词器权重。
- en: Perplexity
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆度
- en: '**Perplexity** is a measure used to evaluate language models. It quantifies
    how well a probability model predicts a sample.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**混淆度**是用于评估语言模型的一个度量。它量化了一个概率模型预测样本的能力。'
- en: Lower perplexity indicates that the model is more confident in its predictions
    and considers the text more likely or “normal”. Higher perplexity suggests that
    the model finds the text more surprising or unusual.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 较低的困惑度表明模型对其预测更有信心，并认为文本更可能或“正常”。较高的困惑度表明模型认为文本更令人惊讶或不同寻常。
- en: 'Here are example texts:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里有一些示例文本：
- en: '[PRE2]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, calculate perplexity and print the results:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，计算困惑度并打印结果：
- en: '[PRE3]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This script demonstrates how even small amounts of noise in the input data can
    significantly impact the model’s perplexity.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本演示了输入数据中的微小噪声如何显著影响模型的困惑度。
- en: The perplexity score is calculated as the exponential of the cross-entropy loss.
    In this code, it’s computed using `torch.exp(outputs.loss).item()`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度评分是交叉熵损失的指数。在此代码中，它使用`torch.exp(outputs.loss).item()`进行计算。
- en: 'Here are our possible outcomes:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们的可能结果：
- en: '`The quick brown fox jumps over the lazy dog` is a common, grammatically correct
    English sentence. The clean text perplexity might be something like `10.25`.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “The quick brown fox jumps over the lazy dog”是一个常见的、语法正确的英语句子。干净文本的困惑度可能类似于“10.25”。
- en: '`Th3 qu1ck br0wn f0x jumps 0ver th3 l@zy d0g` contains numbers and symbols
    in place of letters, making it less common and more difficult for the model to
    predict. The noisy text perplexity might be something like `52.87`.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Th3 qu1ck br0wn f0x jumps 0ver th3 l@zy d0g`中包含数字和符号代替字母，使其不那么常见，并且对模型预测来说更困难。噪声文本的困惑度可能类似于“52.87”。'
- en: The exact numbers will depend on the specific model and tokenizer used, but
    the noisy text should consistently have a higher perplexity score than the clean
    text.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 具体的数字将取决于所使用的特定模型和分词器，但噪声文本的困惑度评分应该始终高于干净文本。
- en: This difference in scores demonstrates the model’s ability to distinguish between
    standard, easily predictable text and unusual, harder-to-predict text. It’s a
    useful metric for tasks such as detecting machine-generated or tampered text,
    as such text often has higher perplexity scores compared to human-written text.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分数差异展示了模型区分标准、易于预测的文本和异常、难以预测的文本的能力。这对于检测机器生成或篡改的文本等任务非常有用，因为此类文本的困惑度评分通常高于人类撰写的文本。
- en: Common data quality issues in language datasets
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言数据集中常见的数据质量问题
- en: 'Language datasets often contain various quality issues that can negatively
    impact LLM training:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 语言数据集通常包含各种质量问题，可能会对LLM训练产生负面影响：
- en: Spelling and grammatical errors can introduce noise and inconsistencies in the
    learned representations.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拼写和语法错误可能会在学习的表示中引入噪声和不一致性。
- en: Inconsistent formatting can lead to unnecessary complexity in the model’s learned
    patterns.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不一致的格式可能导致模型学习到的模式中出现不必要的复杂性。
- en: Redundant data can cause models to overfit to specific patterns or bias present
    in the duplicates.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冗余数据可能导致模型过度拟合到重复项中存在的特定模式或偏差。
- en: Irrelevant or low-quality content can dilute the useful information in the dataset.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不相关或低质量的内容会稀释数据集中有用的信息。
- en: Incomplete or truncated sentences can lead to models learning incomplete language
    structures.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不完整或截断的句子可能导致模型学习到不完整的语言结构。
- en: Code-switching and mixed languages can confuse models trained for specific languages.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码切换和混合语言可能会使针对特定语言训练的模型感到困惑。
- en: '**Personally identifiable information** (**PII**) raises privacy concerns and
    can lead to the memorization of sensitive data.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个人身份信息**（**PII**）引发隐私问题，并可能导致敏感数据的记忆化。'
- en: 'To detect these issues, we can use various Python libraries and techniques.
    Here’s an example using spaCy for basic text quality checks:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检测这些问题，我们可以使用各种Python库和技术。以下是一个使用spaCy进行基本文本质量检查的示例：
- en: 'Provide the imports and an overall function definition:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供导入语句和整体函数定义：
- en: '[PRE4]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Check for spelling errors (using spaCy’s built-in spell checker):'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查拼写错误（使用spaCy内置的拼写检查器）：
- en: '[PRE5]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Check for grammatical issues (a simplistic approach using parts of speech (pos)
    tags):'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查语法问题（使用词性（pos）标签的简单方法）：
- en: '[PRE6]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Parts of speech** (**POS**) tags are labels assigned to each word in a sentence
    to indicate its grammatical role. These tags help systems to understand the syntactic
    structure of sentences and are used in tasks such as parsing, machine translation,
    sentiment analysis, and information extraction. Each tag corresponds to a POS
    such as a noun, verb, or adjective, often with finer-grained distinctions to capture
    tense, number, or function.'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**词性**（**POS**）标签是指分配给句子中每个单词的标签，以指示其语法角色。这些标签帮助系统理解句子的句法结构，并在解析、机器翻译、情感分析和信息提取等任务中使用。每个标签对应一个词性，如名词、动词或形容词，通常有更细粒度的区分来捕捉时态、数或功能。'
- en: 'Check for sentence completeness:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查句子完整性：
- en: '[PRE7]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here’s an example usage of the code:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面是代码的一个示例用法：
- en: '[PRE8]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The script provided in *steps 1 to 5* illustrates a basic framework for identifying
    some common text quality issues. We will address other quality issues in the following
    sections.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤1到5中提供的脚本展示了识别一些常见文本质量问题的基本框架。我们将在接下来的章节中讨论其他质量相关问题。
- en: Text preprocessing techniques for LLMs
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM的文本预处理技术
- en: Effective text preprocessing is crucial for preparing data for LLM training.
    We employ various techniques, including lowercasing, punctuation handling, whitespace
    normalization, special character handling, **tokenization**, number normalization,
    and contraction expansion. Tokenization is the process of breaking text into smaller
    units for further analysis or processing. Tokens are the smallest meaningful units
    of text in natural language processing. They can be words, but they could also
    include punctuation, numbers, or other elements depending on the tokenization
    strategy.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的文本预处理对于为LLM训练准备数据至关重要。我们采用各种技术，包括小写化、标点处理、空白字符标准化、特殊字符处理、**分词**、数字标准化和缩写词扩展。分词是将文本分解成更小单元以进行进一步分析或处理的过程。在自然语言处理中，标记是文本的最小有意义的单元。它们可以是单词，但也可以包括标点、数字或其他元素，具体取决于分词策略。
- en: In addition, **subword tokenization** is an advanced text processing technique
    that breaks words into smaller meaningful units (subwords), enabling more efficient
    handling of rare words, compound words, and morphological variations in natural
    language processing tasks. Unlike traditional word-level tokenization, subword
    tokenization can identify common prefixes, suffixes, and root words, allowing
    models to understand and process previously unseen words by recognizing their
    familiar components.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，**子词分词**是一种高级文本处理技术，它将单词分解成更小的有意义的单元（子词），使得在自然语言处理任务中更有效地处理罕见词、复合词和形态变化。与传统词级分词不同，子词分词可以识别常见的词首、词尾和词根，使模型能够通过识别其熟悉的组件来理解和处理之前未见过的单词。
- en: As an example, consider the word “unbelievably”. Traditional word-level tokenization
    would treat this as a single token. If the model has never seen this word before,
    it may struggle to interpret it correctly. In contrast, subword tokenization would
    break it down into smaller components such as “un”, “believ”, and “ably”. These
    subwords are more likely to appear across different contexts—“un-” in “unlikely”,
    “believ” in “believe”, “ably” in “capably”—allowing the model to derive meaning
    even if it encounters “unbelievably” for the first time. This decomposition enhances
    generalization, reduces vocabulary size, and improves the model’s ability to handle
    rare or morphologically complex words.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以“unbelievably”这个词为例。传统的词级分词会将它视为一个单独的标记。如果模型之前从未见过这个单词，它可能难以正确解释它。相比之下，子词分词会将它分解成更小的组件，如“un”、“believ”和“ably”。这些子词在不同上下文中更有可能出现——“un-”在“unlikely”中，“believ”在“believe”中，“ably”在“capably”中——即使模型第一次遇到“unbelievably”，也能从中推导出意义。这种分解增强了泛化能力，减少了词汇量，并提高了模型处理罕见或形态复杂单词的能力。
- en: Popular subword tokenization algorithms include **byte pair encoding** (**BPE**),
    WordPiece, and SentencePiece, which learn to identify frequently occurring character
    sequences in a training corpus and create a vocabulary of subword tokens. This
    approach is particularly valuable for handling morphologically rich languages,
    reducing vocabulary size while maintaining semantic meaning, and has become fundamental
    in modern language models such as Gemini, Claude, GPT, and other transformer-based
    architectures.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的子词分词算法包括**字节对编码**（**BPE**）、WordPiece和SentencePiece，这些算法学习识别训练语料库中频繁出现的字符序列，并创建一个子词标记词汇表。这种方法对于处理形态丰富的语言特别有价值，可以在保持语义意义的同时减少词汇量，并且已成为现代语言模型如Gemini、Claude、GPT和其他基于transformer架构的基本组成部分。
- en: 'These methods help clean and standardize the text data, reducing noise and
    improving the model’s ability to generalize. Here’s a Python script demonstrating
    these preprocessing techniques:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法有助于清理和标准化文本数据，减少噪声并提高模型泛化的能力。下面是一个演示这些预处理技术的Python脚本：
- en: 'First, import the necessary Python packages:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入必要的Python包：
- en: '[PRE9]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, define the overall preprocessing function:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，定义整体预处理函数：
- en: '[PRE10]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Remove stopwords (stopwords are common words such as “the”, “is”, and “at”)
    that are often removed in text processing as they carry little semantic meaning):'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除停用词（停用词是像“the”、“is”和“at”这样的常见词，它们在文本处理中通常被移除，因为它们语义意义很小）：
- en: '[PRE11]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here’s an example usage of the code:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面是一个代码示例的使用方法：
- en: '[PRE12]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This script demonstrates basic text preprocessing techniques. For LLM training,
    we might need to adapt these techniques based on the specific requirements of
    the model and the dataset.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本演示了基本的文本预处理技术。对于LLM训练，我们可能需要根据模型和数据集的具体要求调整这些技术。
- en: Handling multilingual and code-mixed data
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理多语言和代码混合数据
- en: LLMs often encounter multilingual and code-mixed data, which is text that blends
    two or more languages within a single sentence or conversation. This presents
    a challenge as LLMs must interpret linguistic nuances, grammar, and semantic connections
    across multiple languages. To handle code-mixed data, LLMs need to learn language
    switching, vocabulary and syntax variations, and maintain coherent responses,
    which demands strong language modeling and multilingual training data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）经常遇到多语言和代码混合数据，这种数据是在单个句子或对话中混合两种或更多语言。这对LLMs来说是一个挑战，因为它们必须解释跨多种语言的语用学细微差别、语法和语义联系。为了处理代码混合数据，LLMs需要学习语言切换、词汇和句法变化，并保持连贯的回应，这要求强大的语言建模和多语言训练数据。
- en: We need to implement strategies to handle these scenarios effectively. The following
    steps are needed because they create cleaner, more consistent training data that
    helps LLMs better understand and process text across different languages and mixed-language
    scenarios, ultimately improving their performance in real-world applications where
    language mixing is common.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要实施策略来有效处理这些场景。以下步骤是必要的，因为它们创建更干净、更一致的训练数据，有助于LLMs更好地理解和处理不同语言和混合语言场景中的文本，最终提高它们在实际应用中语言混合常见场景下的性能。
- en: 'For multilingual data, certain tasks are crucial:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多语言数据，某些任务至关重要：
- en: '**Language identification**: Detects the primary language of each text sample'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言识别**：检测每个文本样本的主要语言'
- en: '**Script normalization**: Converts text to a consistent script (e.g., transliteration)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**脚本规范化**：将文本转换为一致的脚本（例如，转写）'
- en: '**Language-specific preprocessing**: Applies language-specific tokenization
    and normalization'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特定语言预处理**：应用特定语言的标记化和规范化'
- en: 'Meanwhile, you should carry out the following steps for code-mixed data:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，对于代码混合数据，你应该执行以下步骤：
- en: '**Token-level language identification**: Identifies the language of individual
    tokens'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记级语言识别**：识别单个标记的语言'
- en: '**Consistency enforcement**: Ensures consistent handling of code-switching
    patterns'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性执行**：确保一致地处理代码切换模式'
- en: Here’s a Python script demonstrating language detection and script normalization.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个演示语言检测和脚本规范化的Python脚本。
- en: 'Let’s provide the imports and the overall function definition:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们提供导入和整体函数定义：
- en: '[PRE13]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Tokenize (using NLTK for simplicity, but consider language-specific tokenizers):'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标记化（为了简单起见使用NLTK，但请考虑特定语言的标记化器）：
- en: '[PRE14]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here’s the example usage:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面是一个示例用法：
- en: '[PRE15]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This code iterates through a list of multilingual text strings, including English,
    German, Japanese, and a code-mixed example, and for each string, it calls a `handle_multilingual_text`
    function (presumably defined elsewhere) to process the text, returning a dictionary
    containing the original text, detected language, transliterated text (if applicable),
    and tokenized words, which are then printed to the console.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此代码遍历一个包含英语、德语、日语和代码混合示例的多语言文本字符串列表，并对每个字符串调用一个`handle_multilingual_text`函数（可能定义在其他地方）来处理文本，返回一个包含原始文本、检测到的语言、转写文本（如果适用）和分词单词的字典，然后打印到控制台。
- en: Putting the preceding three code blocks together, we provide a basic framework
    for handling multilingual text. For more advanced scenarios, we would use specialized
    libraries such as Polyglot for language-specific processing and code-mixing analysis
    when multiple languages are used in the same conversation ([https://dl.acm.org/doi/10.1145/3544548.3581445](https://dl.acm.org/doi/10.1145/3544548.3581445)).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 将前面的三个代码块合并，我们提供了一个处理多语言文本的基本框架。对于更高级的场景，我们会使用专门的库，如Polyglot进行特定语言的处理和代码混合分析，当同一对话中使用多种语言时([https://dl.acm.org/doi/10.1145/3544548.3581445](https://dl.acm.org/doi/10.1145/3544548.3581445))。
- en: For example, Polyglot includes built-in language detection, named entity recognition,
    sentiment analysis, and transliteration capabilities across multiple languages,
    all while maintaining a relatively lightweight footprint compared to larger multilingual
    frameworks. The library is particularly valuable for projects dealing with international
    text data, as it provides consistent APIs across languages and comes with pre-trained
    models, making it an efficient choice for multilingual text analysis tasks without
    the complexity of managing multiple language-specific tools.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Polyglot 包含内置的语言检测、命名实体识别、情感分析和跨多种语言的转写功能，同时与较大的多语言框架相比，保持了相对轻量级的性能。该库对于处理国际文本数据的项目尤其有价值，因为它提供了跨语言的统一API，并附带预训练模型，使其成为无需管理多个特定语言工具的复杂性的多语言文本分析任务的效率选择。
- en: Deduplication strategies for large text corpora
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型文本语料库的去重策略
- en: 'Deduplication is a critical step in preparing large text corpora for LLM training.
    Duplicate content can lead to biased models and wasted computational resources.
    We employ various strategies to identify and remove duplicates efficiently:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 去重是准备大型文本语料库进行LLM训练的关键步骤。重复内容可能导致模型偏差和计算资源的浪费。我们采用各种策略来高效地识别和删除重复项：
- en: '**Exact match deduplication**: Remove identical text samples.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确匹配去重**: 删除完全相同的文本样本。'
- en: '**Near-duplicate detection**: Identify and remove highly similar text samples.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**近似重复检测**: 识别并删除高度相似的文字样本。'
- en: '**Shingling**: Create small overlapping sequences of words for comparison.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Shingling**: 创建用于比较的小重叠单词序列。'
- en: '**Locality sensitive hashing**: Efficiently find similar items in large datasets.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部敏感哈希**: 在大型数据集中高效地找到相似项。'
- en: The following sections show examples of each strategy.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分展示了每种策略的示例。
- en: Exact match deduplication
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精确匹配去重
- en: '**Scenario**: You have a list of customer addresses:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**场景**: 你有一份客户地址列表：'
- en: '**Data**:'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据**:'
- en: “123 Main St, Anytown, CA 91234”
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “123 Main St, Anytown, CA 91234”
- en: “456 Oak Ave, Somecity, NY 56789”
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “456 Oak Ave, Somecity, NY 56789”
- en: “123 Main St, Anytown, CA 91234”
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “123 Main St, Anytown, CA 91234”
- en: '**Result**: The third entry, “123 Main St, Anytown, CA 91234”, is removed because
    it is an exact duplicate of the first entry.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结果**: 第三条记录“123 Main St, Anytown, CA 91234”被删除，因为它与第一条记录完全相同。'
- en: '**Remaining data**:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**剩余数据**:'
- en: “123 Main St, Anytown, CA 91234”
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “123 Main St, Anytown, CA 91234”
- en: “456 Oak Ave, Somecity, NY 56789”
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “456 Oak Ave, Somecity, NY 56789”
- en: Near-duplicate detection
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 近似重复检测
- en: '**Scenario**: You have a collection of news articles:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**场景**: 你有一系列新闻文章：'
- en: '**Data**:'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据**:'
- en: 'Article 1: “The company reported a significant increase in quarterly profits.”'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '文章1: “公司报告了季度利润的显著增长。”'
- en: 'Article 2: “Quarterly profits saw a large increase, the company reports.”'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '文章2: “公司报告季度利润大幅增长。”'
- en: '**Result**: A near-duplicate detection algorithm determines that these articles
    are highly similar in content, even though the wording is slightly different.
    One of the articles is removed, based on a similarity threshold.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结果**: 近似重复检测算法确定这些文章在内容上高度相似，尽管措辞略有不同。基于相似度阈值，删除了一篇文章。'
- en: '**Remaining data**: “The company reported a significant increase in quarterly
    profits.”'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**剩余数据**: “公司报告了季度利润的显著增长。”'
- en: Shingling
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Shingling
- en: '**Scenario**: You want to compare the similarity of text documents:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**场景**：您想比较文本文档的相似度：'
- en: '**Data**:'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据**：'
- en: 'Document 1: “The quick brown fox jumps over the lazy dog.”'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档1：“The quick brown fox jumps over the lazy dog。”
- en: k=3 word shingle.
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: k=3词shingle。
- en: '**Result**: The shingles generated are as follows:'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结果**：生成的shingles如下：'
- en: “The quick brown”
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “The quick brown”
- en: “quick brown fox”
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “quick brown fox”
- en: “brown fox jumps”
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “brown fox jumps”
- en: “fox jumps over”
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “fox jumps over”
- en: “jumps over the”
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “jumps over the”
- en: “over the lazy”
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “over the lazy”
- en: “the lazy dog”
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “the lazy dog”
- en: The document is then represented by the set of those shingles. Then another
    document could be turned into shingles, and the sets of shingles can be compared.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后文档被表示为那些shingles的集合。然后另一个文档可以被转换成shingles，shingles的集合可以进行比较。
- en: Locality Sensitive Hashing (LSH)
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局部敏感哈希（LSH）
- en: '**Scenario**: You have a very large database of online product descriptions:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**场景**：您有一个非常大的在线产品描述数据库：'
- en: '**Process**:'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过程**：'
- en: LSH is used to hash the product descriptions.
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSH用于对产品描述进行哈希处理。
- en: Similar product descriptions are more likely to be hashed into the same “buckets.”
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相似的产品描述更有可能被哈希到相同的“桶”中。
- en: Only the descriptions within the same buckets are then compared in detail to
    find near duplicates.
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后只比较同一桶内的描述，以详细查找近似重复项。
- en: '**Result**: Instead of comparing every product description to every other description,
    LSH narrows down the comparisons to only those descriptions within the same buckets,
    greatly increasing the efficiency of finding near duplicates.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结果**：LSH不是将每个产品描述与其他每个描述进行比较，而是将比较缩小到同一桶内的描述，大大提高了查找近似重复项的效率。'
- en: Note
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Deduplicating is computationally very expensive, so techniques such as minhashing
    or parallel processing can be used to scale the deduplicating with the increase
    in corpus data.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 去重计算成本非常高，因此可以使用minhashing或并行处理等技术来扩展去重，以适应语料库数据的增加。
- en: Minhashing efficiently approximates the similarity between documents using smaller,
    more manageable representations, reducing the computational load. Parallel processing
    further distributes the deduplication task across multiple processors or machines,
    allowing for simultaneous comparisons and significantly speeding up the overall
    process, thus enabling effective deduplication of massive corpora.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Minhashing通过使用更小、更易于管理的表示来有效地近似文档之间的相似度，从而减少计算负载。并行处理进一步将去重任务分配到多个处理器或机器上，允许同时比较，从而显著加快整体过程，从而实现大规模语料库的有效去重。
- en: 'Here’s a Python script demonstrating basic deduplication techniques:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个Python脚本演示了基本去重技术：
- en: 'First, define the overall function:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，定义整体函数：
- en: '[PRE16]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, find duplicates:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，查找重复项：
- en: '[PRE17]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Create a deduplicated corpus:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建去重语料库：
- en: '[PRE18]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here’s an example:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里有一个例子：
- en: '[PRE19]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This script demonstrates a basic near-duplicate detection approach using TF-IDF
    and **cosine similarity**. TF-IDF is a numerical statistic used to reflect the
    importance of words in documents within a collection. It combines how often a
    word appears in a document (TF) with how unique it is across all documents (IDF).
    TF-IDF converts text into numerical vectors, enabling mathematical comparisons
    between documents, which is crucial for the similarity calculations used in the
    deduplication process. For large-scale deduplication, we would use more efficient
    algorithms and distributed computing techniques.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本演示了使用TF-IDF和**余弦相似度**的基本近似重复检测方法。TF-IDF是一种数值统计，用于反映集合中文档中单词的重要性。它结合了单词在文档中出现的频率（TF）以及在整个文档中其独特性（IDF）。TF-IDF将文本转换为数值向量，使得可以在文档之间进行数学比较，这对于去重过程中使用的相似度计算至关重要。对于大规模去重，我们会使用更高效的算法和分布式计算技术。
- en: Here, the similarity threshold of `0.9` used in the deduplication function code
    determines how similar documents must be to be considered duplicates, with 90%
    similarity required by default. This value can be adjusted based on specific use
    cases—a higher threshold (e.g., `0.95` or `1`, which is maximum) is stricter and
    reduces false positives, while a lower threshold (e.g., `0` which is minimum or
    `0.8`) is more lenient and catches more potential duplicates.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，去重函数代码中使用的相似度阈值`0.9`决定了文档必须有多相似才能被认为是重复的，默认要求90%的相似度。此值可以根据具体用例进行调整——更高的阈值（例如，`0.95`或`1`，即最大值）更严格，减少了误报，而较低的阈值（例如，`0`即最小值或`0.8`）更宽松，可以捕获更多潜在的重复项。
- en: Next, let’s discuss automated data cleaning pipelines.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论自动化数据清洗管道。
- en: Automated data cleaning pipelines
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化数据清洗管道
- en: To handle the massive datasets required for LLM training, we need to implement
    automated data cleaning pipelines. These pipelines should be scalable, efficient,
    and capable of handling various data quality issues.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理LLM训练所需的庞大数据集，我们需要实现自动化数据清洗流程。这些流程应该是可扩展的、高效的，并且能够处理各种数据质量问题。
- en: 'The key components of an automated data cleaning pipeline are as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化数据清洗流程的关键组件如下：
- en: '**Data ingestion**: Efficiently load and parse large text corpora.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据摄取**：高效地加载和解析大型文本语料库。'
- en: '**Quality assessment**: Automatically detect and flag data quality issues.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质量评估**：自动检测并标记数据质量问题。'
- en: '**Preprocessing**: Apply text cleaning and normalization techniques.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理**：应用文本清洗和规范化技术。'
- en: '**Deduplication**: Remove exact and near-duplicate content.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去重**：移除完全重复和近似重复的内容。'
- en: '**Filtering**: Remove low-quality or irrelevant samples based on predefined
    criteria.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过滤**：根据预定义的标准移除低质量或不相关的样本。'
- en: '**Validation**: Ensure the cleaned data meets quality standards.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证**：确保清洗后的数据符合质量标准。'
- en: '**Output**: Save the cleaned data in an appropriate format for LLM training.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**：将清洗后的数据保存为LLM训练的适当格式。'
- en: 'Here’s a Python script outlining a basic automated data cleaning pipeline:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个概述基本自动化数据清洗流程的Python脚本：
- en: 'We will start by defining the overall class structure:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定义整体类结构：
- en: '[PRE20]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, we will define a preprocess function:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将定义一个预处理函数：
- en: '[PRE21]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`preprocess`: This method takes a text string as input, converts it to lowercase,
    removes punctuation, splits it into words, filters out common stop words, and
    then joins the remaining words into a string, effectively cleaning and normalizing
    the text.'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preprocess`：此方法接收一个文本字符串作为输入，将其转换为小写，删除标点符号，将其拆分为单词，过滤掉常见的停用词，然后将剩余的单词连接成一个字符串，从而有效地清洗和规范化文本。'
- en: '`filter_by_length`: This method takes a pandas DataFrame containing a `text`
    column and filters the DataFrame to include only rows where the length of the
    `text` column falls within a specified minimum and maximum length, allowing the
    selection of text samples within a desired character range.'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter_by_length`：此方法接收一个包含`text`列的pandas DataFrame，并过滤DataFrame，仅包括`text`列长度在指定最小和最大长度范围内的行，从而允许选择所需字符范围内的文本样本。'
- en: 'We then define the deduplication function:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义去重函数：
- en: '[PRE22]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This `deduplicate` method takes a pandas DataFrame as input and removes near-duplicate
    text entries based on their similarity. It first transforms the `text` column
    of the DataFrame into a TF-IDF matrix using a vectorizer, representing each text
    sample as a numerical vector. Then, it calculates the cosine similarity between
    all pairs of text samples using the TF-IDF matrix, resulting in a similarity matrix.
    The code iterates through the similarity matrix, and if the similarity between
    two text samples exceeds a defined `similarity_threshold`, the index of the second
    sample is added to a set of duplicates. Finally, it removes the rows corresponding
    to the identified duplicate indices from the DataFrame and returns the deduplicated
    DataFrame.
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个`deduplicate`方法接收一个pandas DataFrame作为输入，并根据它们的相似性移除近似重复的文本条目。它首先使用向量器将DataFrame的`text`列转换为TF-IDF矩阵，将每个文本样本表示为一个数值向量。然后，它使用TF-IDF矩阵计算所有文本样本对之间的余弦相似度，从而得到一个相似度矩阵。代码遍历相似度矩阵，如果两个文本样本之间的相似度超过定义的`similarity_threshold`，则第二个样本的索引被添加到一个重复集。最后，它从DataFrame中删除对应于已识别重复索引的行，并返回去重后的DataFrame。
- en: 'Putting all the functions together, we can now define a `clean` function:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有函数组合起来，我们现在可以定义一个`clean`函数：
- en: '[PRE23]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This `clean` method orchestrates a series of data cleaning steps on a CSV file.
    It begins by reading the input CSV file into a pandas DataFrame. Then, the `preprocess`
    method is applied to each text entry in the `text` column, normalizing and cleaning
    the text. Subsequently, it filters the DataFrame using the `filter_by_length`
    method to retain only text entries within a specified length range. After length
    filtering, near-duplicate entries are removed using the `deduplicate` method.
    Finally, it saves the cleaned DataFrame to a new CSV file specified by `output_file`,
    excluding the index, and prints a confirmation message indicating the output file’s
    location. Essentially, this method performs a complete text cleaning pipeline,
    encompassing preprocessing, length filtering, and deduplication.
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种`clean`方法在CSV文件上执行一系列数据清洗步骤。它首先将输入的CSV文件读取到一个pandas DataFrame中。然后，对`text`列中的每个文本条目应用`preprocess`方法，对文本进行归一化和清洗。随后，使用`filter_by_length`方法过滤DataFrame，仅保留指定长度范围内的文本条目。长度过滤后，使用`deduplicate`方法移除近似重复的条目。最后，将清洗后的DataFrame保存到由`output_file`指定的新的CSV文件中，排除索引，并打印一个确认消息，指示输出文件的存储位置。本质上，此方法执行了一个完整的文本清洗流程，包括预处理、长度过滤和去重。
- en: 'The following is an example usage:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是一个示例用法：
- en: '[PRE24]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Overall, this script provides a basic framework for an automated data cleaning
    pipeline. In practice, we would extend this pipeline with more sophisticated cleaning
    techniques, error handling, and parallel processing capabilities to handle large-scale
    datasets efficiently.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，此脚本提供了一个自动化数据清洗流程的基本框架。在实际应用中，我们会扩展此流程，以包含更复杂的清洗技术、错误处理和并行处理能力，以有效地处理大规模数据集。
- en: 'The values `10` and `1000` in the code represent the minimum and maximum allowed
    lengths for text documents in the data cleaning pipeline:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的值`10`和`1000`代表数据清洗流程中文本文档允许的最小和最大长度：
- en: '`min_length=10`: This sets the minimum number of characters a document must
    have to be included in the cleaned dataset. It helps to filter out very short
    texts that might not contain meaningful information, such as single words or brief
    phrases.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_length=10`：这设置了文档必须具有的最小字符数，才能包含在清洗后的数据集中。它有助于过滤掉可能不包含有意义信息的非常短的文本，例如单个单词或简短的短语。'
- en: '`max_length=1000`: This establishes the maximum number of characters allowed
    for a document. It excludes extremely long texts that might be atypical or potentially
    problematic for processing, such as entire books or very large documents that
    could skew the analysis.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length=1000`：这确定了文档允许的最大字符数。它排除了可能不典型或可能对处理造成问题的极长文本，例如整本书或非常大的文档，这些文档可能会扭曲分析。'
- en: These length constraints help ensure that the cleaned dataset contains documents
    of a reasonable and consistent size range, which can improve the quality and efficiency
    of subsequent text analysis or machine learning tasks. You can adjust the length
    based on your use cases.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这些长度约束有助于确保清洗后的数据集包含合理且一致的文档大小范围，这可以提高后续文本分析或机器学习任务的质量和效率。您可以根据您的用例调整长度。
- en: Data validation and quality assurance
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据验证和质量保证
- en: After cleaning the data, you need to validate the results and ensure that the
    cleaned dataset meets the required quality standards for LLM training. We implement
    various validation checks and quality assurance measures to verify the effectiveness
    of our cleaning process.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 清洗数据后，您需要验证结果并确保清洗后的数据集符合LLM训练所需的质量标准。我们实施各种验证检查和质量保证措施，以验证我们清洗过程的有效性。
- en: Key aspects include performing statistical analyses, sampling and manual reviews,
    automated tests, consistency verifications, and performance impact assessments.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 关键方面包括执行统计分析、抽样和人工审查、自动测试、一致性验证和性能影响评估。
- en: 'Here’s a Python script demonstrating basic data validation techniques:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个演示基本数据验证技术的Python脚本：
- en: 'First, define the basic function:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，定义基本函数：
- en: '[PRE25]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, check for empty or very short texts:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，检查空或非常短的文本：
- en: '[PRE26]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Sample for a manual review:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行人工审查的抽样：
- en: '[PRE27]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Evaluate the impact on the model’s perplexity:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型困惑度的影响：
- en: '[PRE28]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let’s see an example:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看一个例子：
- en: '[PRE29]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The script defines a function called `validate_cleaned_data` that’s designed
    to perform a basic quality assessment on a text dataset stored in a CSV file (presumably
    after some initial cleaning steps). It loads the data, calculates some basic statistics,
    checks for specific potential issues in the text content, provides a sample for
    manual inspection, and uses a pre-trained language model (hypothetically GPT-4)
    to evaluate the naturalness or quality of a sample of the text via perplexity.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本定义了一个名为 `validate_cleaned_data` 的函数，该函数旨在对存储在CSV文件中的文本数据集（假设在初始清理步骤之后）进行基本质量评估。它加载数据，计算一些基本统计数据，检查文本内容中的特定潜在问题，提供样本以供人工检查，并使用预训练的语言模型（假设为GPT-4）通过困惑度评估文本样本的自然度或质量。
- en: 'The following issues are being checked for:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 检查以下问题：
- en: 'Dataset size and basic properties:'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集大小和基本属性：
- en: '`len(df)`: Checks the total number of samples (rows) in the CSV.'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`len(df)`: 检查CSV中的样本总数（行数）。'
- en: '`df[''text''].str.len().mean()`: Calculates the average length of the text
    entries, which is useful to see if texts are generally long or short.'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`df[''text''].str.len().mean()`: 计算文本条目的平均长度，这有助于判断文本是普遍较长还是较短。'
- en: '`df[''text''].nunique()`: Counts the number of unique text entries. A low number
    compared to the total number of samples might indicate many duplicates.'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`df[''text''].nunique()`: 统计唯一文本条目的数量。与样本总数相比，低数值可能表明存在许多重复项。'
- en: '`df[df[''text''].str.len() < 10]`: Filters the DataFrame to find rows where
    the length of the string in the `text` column is less than 10 characters*   `len(short_texts)`:
    Counts how many such short texts were found*   `df[''text''].str.contains(r''[^a-zA-Z0-9\s]'')`:
    Uses a regular expression (`r''[^a-zA-Z0-9\s]''`) with pandas’ `.str.contains()`
    method. The regex pattern [`^...`] matches any character *not* in the specified
    set (a-z, A-Z, 0-9, whitespace \s).*   `mask.sum()`: Sums the resulting Boolean
    series (true=`1`, false=0) to count how many texts contain at least one such special
    character.*   `df[''text''].str.contains(r''\d'')`: Uses the regular expression
    `\d` (which matches any digit) with `.str.contains()`*   `mask.sum()`: Counts
    how many texts contain at least one digit*   `df[''text''].str.isupper()`: Uses
    the pandas `.str.isupper()` string method, which returns `True` if all cased characters
    in the string are uppercase and there is at least one alphabetic character (i.e.,
    a letter) that is uppercase and not just symbols or digits. If the string is all
    non-alphabetic (like numbers or punctuation), it will return `False`—even though
    those characters aren’t lowercase either.*   `mask.sum()`: Counts how many texts
    are entirely in uppercase*   `df.sample(...)`). Perplexity calculations can be
    computationally expensive, so they are often done on a representative sample rather
    than the whole dataset.*   `GPT4LMHeadModel`) and its corresponding tokenizer
    (`GPT4Tokenizer`) are loaded. (Note: `''GPT4''` here is illustrative; you’d use
    actual model identifiers such as `''gpt2''` or `''bert-base-uncased''` from libraries
    such as Hugging Face Transformers).*   `calculate_perplexity` function tokenizes
    the text, feeds it to the model, obtains the loss (a measure of how surprised
    the model was by the text), and calculates perplexity using `torch.exp(outputs.loss)`.*   `sample_perplexities.mean()`)
    to get a single score representing the sample’s average quality according to the
    model.*   `sample = df.sample(...)`: Takes a random sample of the data*   `print(sample[''text''].head())`:
    Prints the first few text entries from that random sample, making it easy for
    a user running the script to quickly eyeball some examples'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`df[df[''text''].str.len() < 10]`: 过滤DataFrame以找到`text`列中字符串长度小于10个字符的行*   `len(short_texts)`:
    计算找到的此类短文本的数量*   `df[''text''].str.contains(r''[^a-zA-Z0-9\s]'')`: 使用pandas的`.str.contains()`方法和正则表达式（`r''[^a-zA-Z0-9\s]''`）。正则表达式模式`[^...]`匹配不在指定集合（a-z,
    A-Z, 0-9, 空白字符\s）中的任何字符.*   `mask.sum()`: 将结果布尔序列（true=`1`，false=0）求和，以计算包含至少一个此类特殊字符的文本数量.*   `df[''text''].str.contains(r''\d'')`:
    使用`.str.contains()`和正则表达式`\d`（匹配任何数字）*   `mask.sum()`: 计算包含至少一个数字的文本数量*   `df[''text''].str.isupper()`:
    使用pandas的`.str.isupper()`字符串方法，如果字符串中的所有大小写字符都是大写并且至少有一个字母字符是大写的（即字母），则返回`True`。如果字符串全部是非字母字符（如数字或标点符号），则返回`False`——即使这些字符也不是小写的.*   `mask.sum()`:
    计算完全为大写的文本数量*   `df.sample(...)`). 惊奇度计算可能很昂贵，因此通常在代表性样本上而不是整个数据集上进行计算.*   `GPT4LMHeadModel`)及其对应的分词器(`GPT4Tokenizer`)被加载。（注意：这里的`''GPT4''`是示例性的；你会使用实际的模型标识符，例如来自Hugging
    Face Transformers库的`''gpt2''`或`''bert-base-uncased''`。）*   `calculate_perplexity`函数对文本进行分词，将其输入到模型中，获取损失（衡量模型对文本感到惊讶的程度的一个指标），并使用`torch.exp(outputs.loss)`计算惊奇度.*   `sample_perplexities.mean()`)以获得一个代表样本平均质量的单一分数.*   `sample
    = df.sample(...)`: 从数据中随机抽取样本*   `print(sample[''text''].head())`: 打印随机样本中的前几个文本条目，使用户运行脚本时可以快速查看一些示例'
- en: 'To ensure comprehensive quality assurance, you can do the following:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保全面的质量保证，你可以执行以下操作：
- en: Implement more sophisticated automated tests tailored to your specific data
    characteristics and cleaning rules.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施针对你特定数据特征和清洗规则的更复杂的自动化测试。
- en: Develop a systematic process for manual review, including guidelines for human
    annotators to assess data quality consistently.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制定一个系统的手动审查流程，包括为人类标注者提供评估数据质量的一致性指南。
- en: Use a known synthetic dataset with known issues to benchmark and assess the
    performance of the pipeline.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用已知存在问题的合成数据集来基准测试和评估管道的性能。
- en: Compare the cleaned dataset against the original dataset to verify that no unintended
    data loss or alteration occurred during the cleaning process.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将清洗后的数据集与原始数据集进行比较，以验证在清洗过程中是否发生了意外的数据丢失或更改。
- en: Conduct regular audits of your data cleaning pipeline to identify any emerging
    issues or bias introduced during cleaning.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定期审计你的数据清洗管道，以识别在清洗过程中出现的任何新兴问题或偏差。
- en: Maintain detailed logs of the cleaning process, including any decisions made
    and their rationale, to ensure reproducibility and facilitate future improvements.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录详细的清洁过程日志，包括做出的任何决策及其依据，以确保可重复性和便于未来的改进。
- en: By implementing these measures, you can ensure that your cleaned dataset is
    of high quality and suitable for training robust LLMs.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实施这些措施，你可以确保你的清洗数据集具有高质量且适合训练鲁棒的LLMs。
- en: Summary
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored the critical process of data cleaning for LLM training.
    We discussed the importance of clean data in developing robust and reliable language
    models and covered common data quality issues specific to language datasets. We
    provided techniques to address these issues, including text preprocessing, handling
    multilingual and code-mixed data, and deduplication strategies for large text
    corpora.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了LLM训练中数据清洗的关键过程。我们讨论了清洁数据在开发鲁棒和可靠的语言模型中的重要性，并涵盖了针对语言数据集的常见数据质量问题。我们提供了解决这些问题的技术，包括文本预处理、处理多语言和代码混合数据以及大型文本语料库的去重策略。
- en: We also delved into the implementation of automated data cleaning pipelines,
    which are essential for handling the massive datasets used in LLM training. Finally,
    we discussed data validation and quality assurance measures to ensure the effectiveness
    of the cleaning process.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还深入探讨了自动化数据清洗管道的实施，这对于处理LLM训练中使用的海量数据集至关重要。最后，我们讨论了数据验证和质量保证措施，以确保清洗过程的有效性。
- en: In the next chapter, we will focus on the data augmentation pattern for LLMs.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将重点关注LLMs的数据增强模式。
