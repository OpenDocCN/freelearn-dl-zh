- en: Working with Time Series Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理时间序列数据
- en: Classifying images with a neural network is one of the most iconic jobs in deep
    learning. But it certainly isn't the only job that neural networks excel at. Another
    area where there's a lot of research happening is recurrent neural networks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络进行图像分类是深度学习中最具代表性的任务之一。但它当然不是神经网络擅长的唯一任务。另一个有大量研究正在进行的领域是循环神经网络。
- en: In this chapter, we'll dive into recurrent neural networks, and how they can
    be used in scenarios where you have to deal with time series data; for example,
    in an IoT solution where you need to predict temperatures or other important values.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨循环神经网络，以及它们如何应用于需要处理时间序列数据的场景；例如，在物联网解决方案中，你可能需要预测温度或其他重要的数值。
- en: 'The following topics are covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: What are recurrent neural networks?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是循环神经网络？
- en: Usage scenarios for recurrent neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络的应用场景
- en: How do recurrent neural networks work
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络是如何工作的
- en: Building recurrent neural networks with CNTK
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CNTK构建循环神经网络
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: We assume that you have a recent version of Anaconda installed on your computer,
    and have followed the steps in [Chapter 1](9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml),
    *Getting Started with CNTK*, to install CNTK on your computer. The sample code
    for this chapter can be found in our GitHub repository at [https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch6](https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch6).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设你已经在计算机上安装了最新版本的Anaconda，并且按照[第1章](9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml)《*开始使用CNTK*》中的步骤安装了CNTK。本章的示例代码可以在我们的GitHub代码库中找到，网址是[https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch6](https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch6)。
- en: 'In this chapter, we''ll work on an example stored in Jupyter notebooks. To
    access the sample code, run the following commands inside an Anaconda prompt in
    the directory where you''ve downloaded the code:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将处理存储在Jupyter笔记本中的示例代码。要访问示例代码，请在Anaconda提示符下运行以下命令，前提是你已经下载了代码并进入了该目录：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The sample code is stored in the `Training recurrent neural networks.ipynb`
    notebook. Please be aware that running the sample code for this chapter will take
    a long time if you don't have a machine with a GPU that can be used by CNTK.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 示例代码存储在`Training recurrent neural networks.ipynb`笔记本中。如果你没有配备GPU且无法使用CNTK的机器，请注意运行本章的示例代码将需要较长时间。
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 请观看以下视频，查看代码的实际应用：
- en: '[http://bit.ly/2TAdtyr](http://bit.ly/2TAdtyr)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2TAdtyr](http://bit.ly/2TAdtyr)'
- en: What are recurrent neural networks?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是循环神经网络？
- en: Recurrent neural networks are a special breed of neural networks that are capable
    of reasoning over time. They are primarily used in scenarios where you have to
    deal with values that change over time.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络是一种特殊类型的神经网络，能够进行时间推理。它们主要用于需要处理随时间变化的数值的场景。
- en: In a regular neural network, you can provide only one input, which results in
    one prediction. This limits what you can do with a regular neural network. For
    example, regular neural networks are not good at translating text, while there
    have been quite a few successful experiments with recurrent neural networks in
    translation tasks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在常规神经网络中，你只能提供一个输入，这样就只能得到一个预测结果。这限制了你使用常规神经网络的能力。例如，常规神经网络在文本翻译方面表现不佳，而循环神经网络在翻译任务中却取得了不少成功的实验。
- en: In a recurrent neural network, it is possible to provide a sequence of samples
    that result in a single prediction. You can also use a recurrent neural network
    to predict an output sequence based on a single input sample. Finally, you can
    predict an output sequence based on an input sequence.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环神经网络中，可以提供一系列样本，这将生成一个单一的预测结果。你还可以使用循环神经网络根据一个输入样本预测输出序列。最后，你可以根据输入序列预测输出序列。
- en: As with the other types of neural networks, you can use recurrent neural networks
    in classification jobs as well as regression tasks, although it may be harder
    to recognize the kind of job performed with a recurrent network based on the output
    of the network.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 和其他类型的神经网络一样，你也可以在分类任务和回归任务中使用循环神经网络，尽管根据网络输出的结果可能很难识别出循环神经网络执行的任务类型。
- en: Recurrent neural networks variations
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络的变体
- en: 'Recurrent neural networks can be used in a variety of ways. In this section
    we''ll take a look at the different variations of recurrent neural networks and
    how they can be used to solve specific types of problems. Specifically we''ll
    look at the following variations:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络可以以多种方式使用。在本节中，我们将探讨递归神经网络的不同变体，以及它们如何用于解决特定类型的问题。具体来说，我们将关注以下几种变体：
- en: Predicting a single output based on an input sequence
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于输入序列预测单个输出
- en: Predicting a sequence based on a single input value
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于单个输入值预测序列
- en: Predicting sequences based on other sequences
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于其他序列预测序列
- en: Finally we'll also explore stacking multiple recurrent neural networks together
    and how that helps get better performance in a scenario like processing text.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还将探索如何将多个递归神经网络堆叠在一起，以及如何在处理文本等场景中提高性能。
- en: Let's take a look at the scenarios in which recurrent networks can be used,
    as there are several ways in which you can use the unique properties of recurrent
    neural networks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下递归网络可以使用的场景，因为有多种方法可以利用递归神经网络的独特特性。
- en: Predicting a single output based on a sequence
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于序列预测单个输出
- en: 'A recurrent neural network contains a loopback connection to the input. When
    we feed a sequence of values it will process each element in the sequence as time
    steps. Because of the loopback connection it can combine output generated when
    processing one element in the sequence with input for the next element in the
    sequence. By combining the output of previous time steps with the input of the
    next time steps it will build a memory over the whole sequence which can be used
    to make a prediction. Schematically, a basic recurrent neural network looks like
    this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络包含一个反馈连接到输入。当我们输入一系列值时，它将按时间步处理序列中的每个元素。由于反馈连接，它可以将处理一个元素时生成的输出与下一个元素的输入结合起来。通过将前一时间步的输出与下一时间步的输入结合，它将构建一个跨整个序列的记忆，这可以用来进行预测。从示意图来看，基本的递归神经网络如下所示：
- en: '![](img/ca594c5b-ff8b-4a04-8a89-c364e1a1216b.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca594c5b-ff8b-4a04-8a89-c364e1a1216b.png)'
- en: 'This recurrent behavior becomes clearer when we unroll a recurrent neural network
    into its individual steps, as demonstrated in the following diagram:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将递归神经网络展开成其各个步骤时，这种递归行为变得更加清晰，下面的图示展示了这一点：
- en: '![](img/b5319a3f-4a49-4e89-8920-812f00ec7cf7.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5319a3f-4a49-4e89-8920-812f00ec7cf7.png)'
- en: 'To make a prediction with this recurrent neural network we''ll perform the
    following steps:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这个递归神经网络进行预测，我们将执行以下步骤：
- en: First, we feed the first element of the input sequence to create an initial
    hidden state.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将输入序列的第一个元素输入，创建一个初始的隐藏状态。
- en: Then, we take the initial hidden state and combine it with the second element
    in the input sequence to produce an updated hidden state.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将初始隐藏状态与输入序列中的第二个元素结合，生成更新后的隐藏状态。
- en: Finally, we take the third element in the input sequence to produce the final
    hidden state and predict the output for the recurrent neural network.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将输入序列中的第三个元素，生成最终的隐藏状态并预测递归神经网络的输出。
- en: Because of this loopback connection, you can teach a recurrent neural network
    to recognize patterns that happen over time. For example, when you want to predict
    tomorrow's temperature, you will need to look at the weather from the past few
    days to discover a pattern that can be used to determine the temperature for tomorrow.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个反馈连接，您可以训练递归神经网络识别随着时间发生的模式。例如，当你想预测明天的气温时，你需要查看过去几天的天气，以发现一个可以用来确定明天气温的模式。
- en: Predicting a sequence based on a single sample
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于单个样本预测序列
- en: 'The basic model for a recurrent neural network can be extended to other use
    cases as well. For example, you can use the same network architecture to predict
    a sequence of values based on a single input as is shown in the next diagram:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络的基本模型也可以扩展到其他用例。例如，您可以使用相同的网络架构，基于单个输入预测一系列值，如下图所示：
- en: '![](img/60c27a51-468f-4049-8e34-2f12ee8acda0.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60c27a51-468f-4049-8e34-2f12ee8acda0.png)'
- en: In this scenario we have three time steps, each time step will predict one step
    in the output sequence based on the input we provided.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有三个时间步，每个时间步将根据我们提供的输入预测输出序列中的一个步骤。
- en: First, we feed an input sample into the neural network to produce the initial
    hidden state and predict the first element in the output sequence
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将输入样本送入神经网络，生成初始的隐藏状态，并预测输出序列中的第一个元素。
- en: Then, we combine the initial hidden state with the same sample to produce an
    updated hidden state and output for the second element in the output sequence
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将初始隐藏状态与相同的样本结合，生成更新后的隐藏状态和输出，预测输出序列中的第二个元素。
- en: Finally, we feed the sample another time to update the hidden state one more
    time and predict the final element in the output sequence
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们再次输入样本，进一步更新隐藏状态，并预测输出序列中的最后一个元素。
- en: Generating a sequence from one sample is very different from our previous sample
    where we collected information about all time steps in the input sequence to get
    a single prediction. In this scenario we generate output at each time step.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个样本生成一个序列与我们之前的样本非常不同，之前我们收集了输入序列中所有时间步的信息以得到一个单一的预测。而在这种情况下，我们在每个时间步生成输出。
- en: There's one more variation on the recurrent neural network that takes concepts
    of the setup we just discussed with the setup that we discussed in the previous
    section to predict a sequence of values based on a sequence of values.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种递归神经网络的变体，它结合了我们刚刚讨论的设置和前一节中讨论的设置，根据一系列值预测一系列值。
- en: Predicting sequences based on sequences
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于序列预测序列
- en: 'Now that we''ve seen how to predict a single value based on a sequence and
    predicting a sequence based on a single value, let''s take a look at predicting
    sequences for sequences. In this scenario, you perform the same steps as in the
    previous scenario, where we predicted a sequence based on a single sample, as
    is demonstrated in the following diagram:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何根据一个序列预测单个值，或根据一个单独的值预测一个序列，让我们看看如何进行序列到序列的预测。在这种情况下，你执行与前面情景中相同的步骤，其中我们是基于单个样本预测一个序列，如下图所示：
- en: '![](img/d06546de-4155-4584-bed2-a8117e94046a.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d06546de-4155-4584-bed2-a8117e94046a.png)'
- en: 'In this scenario we have three time steps that take in elements from the input
    sequence and predict a corresponding element in the output sequence that we want
    to predict. Let''s go over the scenario step-by-step:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情景下，我们有三个时间步，每个时间步接受输入序列中的元素，并预测一个我们想要预测的输出序列中的对应元素。让我们一步一步地回顾这个情景：
- en: First, we take the first element in the input sequence and create an initial
    hidden state and predict the first element in the output sequence.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们取输入序列中的第一个元素，创建初始的隐藏状态，并预测输出序列中的第一个元素。
- en: Next, we take the initial hidden state and the second element from the input
    sequence to update the hidden state and predict the second element in the output
    sequence.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将初始隐藏状态与输入序列中的第二个元素结合，更新隐藏状态，并预测输出序列中的第二个元素。
- en: Finally, we take the updated hidden state and the final element in the input
    sequence to predict the final element in the output sequence.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将更新后的隐藏状态和输入序列中的最后一个元素一起，预测输出序列中的最后一个元素。
- en: So Instead of repeating the same input sample for each step, like we did in
    the previous section, we feed in the input sequence one element at a time, and
    keep the generated prediction of each step as the output sequence of the model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，和我们在前一节中做的那样，我们不再在每一步重复相同的输入样本，而是一次输入序列中的一个元素，并将每一步生成的预测作为模型的输出序列。
- en: Stacking multiple recurrent layers
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠多个递归层
- en: Recurrent neural networks can have multiple recurrent layers. This makes the
    memory capacity of the recurrent network bigger, enabling the model to learn more
    complex relations.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络可以拥有多个递归层。这使得递归网络的记忆容量增大，模型能够学习到更复杂的关系。
- en: For example, when you want to translate text, you need to stack together at
    least two recurrent layers, one to encode the input text to an intermediate form,
    and another one to decode it in to the language into which you want to translate
    the text. Google has an interesting paper that demonstrates how to use this technique
    to translate from one language to another that is available at [https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当你想翻译文本时，你需要堆叠至少两个递归层，一个用于将输入文本编码为中间形式，另一个用于将其解码为你想翻译成的语言。谷歌有一篇有趣的论文，展示了如何使用这种技术进行语言间的翻译，论文地址是[https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215)。
- en: Because you can use a recurrent neural network in so many ways, it is quite
    versatile in making predictions with time series data. In the next section, we'll
    dive into more detail of how a recurrent network works internally, to get a good
    grasp of how the hidden state works.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于循环神经网络可以以多种方式使用，因此它在处理时间序列数据时具有很强的预测能力。在下一部分，我们将深入了解循环网络如何在内部工作，从而更好地理解隐藏状态的工作原理。
- en: How do recurrent neural networks work?
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络是如何工作的？
- en: In order to understand how recurrent neural networks work, we need to take a
    closer look at how recurrent layers in these networks work. There are several
    different types of recurrent layers you can use in a recurrent neural network.
    Before we dive into the more advanced versions of recurrent units, let's first
    discuss how to predict output with a standard recurrent layer, and how to train
    a neural network that contains recurrent layers.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解循环神经网络是如何工作的，我们需要仔细看看这些网络中循环层的工作原理。在循环神经网络中，你可以使用几种不同类型的循环层。在我们深入讨论更高级的循环单元之前，让我们首先讨论如何使用标准循环层来预测输出，以及如何训练一个包含循环层的神经网络。
- en: Making predictions with a recurrent neural network
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用循环神经网络进行预测
- en: 'A basic recurrent layer is quite different from a regular layer in a neural
    network. Recurrent layers, in general, feature a hidden state that serves as the
    memory for the layer. There''s a loopback connection from the output of the layer
    back to the input of the layer, as demonstrated in the following diagram:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的循环层与神经网络中的常规层非常不同。一般来说，循环层具有一个隐藏状态，作为该层的记忆。该层的输出会通过一个回环连接返回到该层的输入，正如下图所示：
- en: '![](img/562450bb-333b-4dcd-aa44-7374466c7d98.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/562450bb-333b-4dcd-aa44-7374466c7d98.png)'
- en: Now that we've seen what a basic recurrent layer looks like, let's go over how
    this layer type works step-by-step, using a sequence of three elements. Each step
    in the sequence is called a time step. To predict output with a recurrent layer,
    we need to initialize the layer with an initial hidden state. This is usually
    done using all zeros. The hidden state has the same size as the number of features
    in a single time step in the input sequence.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了基本的循环层是什么样的，让我们一步一步地看一下这种层类型是如何工作的，我们将使用一个包含三个元素的序列。序列中的每一步称为一个时间步。为了使用循环层预测输出，我们需要用初始的隐藏状态来初始化该层。这通常是通过全零初始化来完成的。隐藏状态的大小与输入序列中单个时间步的特征数量相同。
- en: 'Next, we will need to update the hidden state for the first time step in the
    sequence. To update the hidden state for the first time step we''ll use the following
    formula:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要更新序列中第一个时间步的隐藏状态。要更新第一个时间步的隐藏状态，我们将使用以下公式：
- en: '![](img/d98b7208-2516-4f5e-a56e-5c283bbb58c4.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d98b7208-2516-4f5e-a56e-5c283bbb58c4.png)'
- en: In this formula we calculate the new hidden state by calculating the dot product
    (that is, the element-wise product) between the initial hidden state (initialized
    with zeros) and a set of weights. We'll add to this the dot product between another
    set of weights and the input for the layer. The result of the sum of both dot
    products is passed through an activation function, just like in a regular neural
    network layer. This gives us the hidden state for the current time step.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，我们通过计算初始隐藏状态（以零初始化）和一组权重之间的点积（即按元素相乘）来计算新的隐藏状态。我们将另加一组权重与该层输入的点积。两个点积的和将通过一个激活函数，就像在常规神经网络层中一样。这样我们就得到了当前时间步的隐藏状态。
- en: 'The hidden state for the current time step is used as the initial hidden state
    for the next time step in the sequence. We''ll repeat the calculations performed
    in the first time step to update the hidden state for the second time step. The
    formula for the second time step is shown below:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当前时间步的隐藏状态将作为序列中下一个时间步的初始隐藏状态。我们将重复在第一个时间步中执行的计算，以更新第二个时间步的隐藏状态。第二个时间步的公式如下所示：
- en: '![](img/def3ec5c-8b66-4c14-b6a6-a4fbfad94c75.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/def3ec5c-8b66-4c14-b6a6-a4fbfad94c75.png)'
- en: We'll calculate the dot product between the weights for the hidden state, and
    the hidden state from step 1, and add to this the dot product between the input
    and the weights for the input. Note that we're reusing the weights from the previous
    time step.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将计算隐藏状态权重与步骤 1 中的隐藏状态的点积，并将其与输入和输入权重的点积相加。请注意，我们正在重用前一个时间步的权重。
- en: 'We''ll repeat the process of updating the hidden state for the third and final
    step in the sequence, as shown in the following formula:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重复更新隐藏状态的过程，作为序列中的第三个也是最后一个步骤，如下式所示：
- en: '![](img/b5096965-de91-411a-bc1b-d3a5014b15c4.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5096965-de91-411a-bc1b-d3a5014b15c4.png)'
- en: 'When we''ve processed all the steps in the sequence we can calculate the output
    using a third set of weights and the hidden state from the final time step, as
    shown in the following formula:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理完序列中的所有步骤后，可以使用第三组权重和最终时间步的隐藏状态来计算输出，如下式所示：
- en: '![](img/459719df-c4d9-4ce5-8fd8-cccb2865b739.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/459719df-c4d9-4ce5-8fd8-cccb2865b739.png)'
- en: When you're using a recurrent network to predict an output sequence, you will
    need to perform this final calculation at every time step, instead of just the
    final time step in the sequence.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用递归网络预测输出序列时，你需要在每个时间步执行这个最终计算，而不仅仅是在序列的最后一个时间步。
- en: Training a recurrent neural network
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练递归神经网络
- en: As with regular neural network layers, you can train a recurrent layer using
    backpropagation. This time, we're going to apply a trick to the regular backpropagation
    algorithm.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 与常规神经网络层一样，你可以通过反向传播来训练递归层。这一次，我们将对常规的反向传播算法应用一个技巧。
- en: In a regular neural network, you'd calculate the gradients based on the `loss`
    function, the input, and the expected output of the model. But this won't work
    for a recurrent neural network. The loss of a recurrent layer can't be calculated
    using a single sample, the target value, and the `loss` function. Because the
    predicted output is based on all time steps in the input of the network, you also
    need to calculate the loss using all time steps of the input sequence. So, instead
    of a single set of gradients, you get a sequence of gradients that results in
    the final loss when summed up.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在常规神经网络中，你会根据`loss`函数、输入和模型的期望输出来计算梯度。但这对递归神经网络不起作用。递归层的损失不能仅通过单个样本、目标值和`loss`函数来计算。因为预测输出是基于网络输入的所有时间步，因此你还需要使用输入序列的所有时间步来计算损失。因此，你得到的不是一组梯度，而是一个梯度序列，当它们加起来时得到最终的损失。
- en: Backpropagation over time is harder than regular backpropagation. To reach the
    global optimum for a `loss` function, we need to work harder to descend down the
    gradients. The hillside for our gradient descent algorithm to walk down is much
    higher than with a regular neural network. Aside from higher losses, it also takes
    longer because we need to process each time step in the sequence to calculate
    and optimize the loss for a single input sequence.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 时间上的反向传播比常规反向传播更为困难。为了达到`loss`函数的全局最优，我们需要更加努力地沿梯度下降。我们梯度下降算法要走的坡度比常规神经网络大得多。除了更高的损失外，它还需要更长时间，因为我们需要处理序列中的每一个时间步，以便为单个输入序列计算和优化损失。
- en: To make things worse, there's a bigger chance we will see exploding gradients
    during backpropagation, because of the addition of gradients over multiple time
    steps. You can resolve the problem with exploding gradients by using a bounded
    activation, such as the **hyperbolic tangent function** (**tanh**) or `sigmoid`.
    These activation functions limit the output value of the recurrent layer to values
    between -1 and 1 for the `tanh` function, and 0 and 1 for the `sigmoid` function.
    The `ReLU` activation function is less useful in a recurrent neural network, because
    the gradients are not limited, which will definitely lead to exploding gradients
    at some point.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，由于梯度在多个时间步的累加，反向传播过程中更容易出现梯度爆炸的问题。你可以通过使用有限制的激活函数来解决梯度爆炸问题，例如**双曲正切函数**（**tanh**）或`sigmoid`。这些激活函数将递归层的输出值限制在`tanh`函数的-1和1之间，以及`sigmoid`函数的0和1之间。`ReLU`激活函数在递归神经网络中不太有用，因为梯度没有限制，这在某个时刻肯定会导致梯度爆炸。
- en: 'Limiting the values produced by the activation function can cause another problem.
    Remember from [Chapter 2](4c9da7a9-6873-4de9-99a9-43de693d65f8.xhtml), *Building
    Neural Networks with CNTK*, that the `sigmoid` has a specific curve where the
    gradients quickly decrease to zero at both ends of the curve. The `tanh` function
    that we''ve been using in the sample in this section has the same type of curve,
    as demonstrated in the following diagram:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 限制激活函数输出值可能会引发另一个问题。记住在[第2章](4c9da7a9-6873-4de9-99a9-43de693d65f8.xhtml)，*使用CNTK构建神经网络*中提到的，`sigmoid`具有一个特定的曲线，梯度在曲线的两端迅速减小到零。我们在本节示例中使用的`tanh`函数也具有相同类型的曲线，如下图所示：
- en: '![](img/73f475dd-54a0-48ea-8e9e-cd17f2598706.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73f475dd-54a0-48ea-8e9e-cd17f2598706.png)'
- en: Input values between -2 and +2 have a reasonably well-defined gradient. This
    means that we can effectively use gradient descent to optimize the weights in
    the neural network. However, when the output of the recurrent layer gets lower
    than -2 or higher than +2, the gradient gets shallower. This can get extremely
    low, to a point where the CPU or GPU starts to round the gradients to zero. This
    means that we are no longer learning.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 输入值在 -2 到 +2 之间时，梯度相对较为明确。这意味着我们可以有效地使用梯度下降来优化神经网络中的权重。然而，当递归层的输出低于 -2 或高于 +2
    时，梯度会变得较浅。这可能会变得极其低，直到 CPU 或 GPU 开始将梯度四舍五入为零。这意味着我们不再进行学习。
- en: Recurrent layers suffer more from the vanishing gradient or saturation problem
    than regular neural network layers because of the multiple time steps involved.
    You can't do much about it when using a regular recurrent layer. There are, however,
    other recurrent layer types that have a more advanced setup that can solve this
    problem to some extent.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于涉及多个时间步，递归层比常规神经网络层更容易受到梯度消失或饱和问题的影响。在使用常规递归层时，你无法对此做太多处理。然而，其他递归层类型具有更为先进的设置，能够在一定程度上解决这个问题。
- en: Using other recurrent layer types
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用其他递归层类型
- en: Because of the vanishing gradient problem, the basic recurrent layer is not
    very good at learning long-term correlations. In other words, it does not handle
    long sequences very well. You run in into this problem when you try to process
    sentences or longer sequences of text and try to classify what they mean. In English
    and other languages, there's quite a long distance between two related words in
    a sentence that give the sentence meaning. When you only use a basic recurrent
    layer in your model, you will quickly discover that your model won't be very good
    at classifying sequences of text.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于梯度消失问题，基本递归层在学习长期相关性方面表现不佳。换句话说，它在处理长序列时不太有效。当你尝试处理句子或更长的文本序列并试图分类它们的含义时，你会遇到这个问题。在英语和其他语言中，句子中的两个相关词之间有较长的距离，它们共同赋予句子意义。当你的模型仅使用基本递归层时，你很快会发现它在分类文本序列时并不优秀。
- en: There are, however, other recurrent layer types that are much more suited for
    working with longer sequences. Also, they tend to combine long and short-term
    correlations better.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有其他递归层类型更适合处理更长的序列。同时，它们通常能更好地结合长短期相关性。
- en: Working with gated recurrent units
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与门控递归单元（GRU）一起工作
- en: 'One alternative to the basic recurrent layer is the **Gated Recurrent Unit**
    (**GRU**). This layer type has two gates that help it handle long-distance correlations
    in sequences, as demonstrated in the following diagram:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 基本递归层的一个替代方案是**门控递归单元**（**GRU**）。这种层类型具有两个门，帮助它处理序列中的长距离相关性，如下图所示：
- en: The GRU is much more complex in shape than the regular recurrent layer. There
    are a lot more lines connecting different inputs to the output. Let's go over
    the diagram and take a look at what the general idea is behind this layer type.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: GRU 的形状比常规递归层复杂得多。有更多的连接线将不同的输入与输出相连接。让我们一起看看这个图表，了解这个层类型背后的总体思路。
- en: Unlike the regular recurrent layer, the GRU layer has an **update gate** and
    **reset gate**. The reset and update gates are the valves that control how much
    of the memory of previous time steps is kept, and how much of the new data is
    used for producing the new memory.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与常规的递归层不同，GRU 层具有**更新门**和**重置门**。重置门和更新门是控制保留多少先前时间步记忆、以及多少新数据用于生成新记忆的阀门。
- en: Predicting output is very similar to predicting with a regular recurrent layer.
    When we feed data into the layer, the previous hidden state is used to calculate
    the value for the new hidden state. When all elements in the sequence have been
    processed, the output is calculated using one additional set of weights, just
    as we did in the regular recurrent layer.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 预测输出与使用常规递归层进行预测非常相似。当我们将数据输入到该层时，先前的隐藏状态将用于计算新隐藏状态的值。当序列中的所有元素都处理完毕时，输出将使用一组额外的权重进行计算，就像我们在常规递归层中所做的那样。
- en: 'Calculating the hidden state over multiple time steps is a lot more complicated
    in a GRU. There are a few steps needed to update the hidden state of the GRU.
    First, we need to calculate the value for the update gate as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GRU 中，计算跨多个时间步的隐藏状态要复杂得多。需要几个步骤来更新 GRU 的隐藏状态。首先，我们需要计算更新门的值，如下所示：
- en: '![](img/1205b70b-a516-409d-9e38-6151b5526369.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1205b70b-a516-409d-9e38-6151b5526369.png)'
- en: The update gate is controlled using two sets of weights, one for the hidden
    state from the previous time step, and one for the current input provided to the
    layer. The value produced by the update gate controls how much of the past time
    steps is kept in the hidden state.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 更新门通过两组权重进行控制，一组用于前一个时间步的隐藏状态，另一组用于当前时间步输入到层的值。更新门产生的值控制着多少过去的时间步数据保留在隐藏状态中。
- en: 'The second step is to update the reset gate. This is done using the following
    formula:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是更新重置门。此操作通过以下公式进行：
- en: '![](img/105e2de9-9e05-4295-aff9-759ca037a53d.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/105e2de9-9e05-4295-aff9-759ca037a53d.png)'
- en: 'The reset gate is also controlled using two sets of weights; one for the input
    value for the current time step and another set of weights for the hidden state.
    The reset gate controls how much of the hidden state is removed. This becomes
    clear when we calculate an initial version of the new hidden state as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 重置门也通过两组权重进行控制；一组用于当前时间步的输入值，另一组用于隐藏状态。重置门控制着从隐藏状态中移除多少信息。当我们计算新隐藏状态的初始版本时，这一点会变得更加清晰：
- en: '![](img/e85f1b8b-4519-401f-8c02-01ee1e254c35.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e85f1b8b-4519-401f-8c02-01ee1e254c35.png)'
- en: First, we multiply the input with its corresponding weights. Then, we multiply
    the previous hidden state with its corresponding weights. We then calculate the
    element-wise or Hadamard product between the reset gate and the weighted hidden
    state. Finally, we add this to the weighted input, and use a `tanh` activation
    over this to calculate the remembered hidden state.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将输入与其对应的权重相乘。然后，将前一个隐藏状态与其对应的权重相乘。接着，我们计算重置门与加权隐藏状态之间的逐元素或Hadamard积。最后，我们将其与加权输入相加，并对其应用`tanh`激活函数来计算记忆的隐藏状态。
- en: The reset gate in this formula controls how much is forgotten of the previous
    hidden state. A reset gate with a low value will remove a lot of data from the
    previous time step. A higher value will help the layer remember a lot from the
    previous time step.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式中的重置门控制着前一个隐藏状态有多少信息被遗忘。一个较低值的重置门会从前一个时间步移除大量数据。较高的值将帮助层保留更多来自前一个时间步的信息。
- en: 'We''re not done yet though—once we have the information coming from the previous
    timestamp increased by the update gate and tempered by the reset gate, this produces
    the remembered information from the previous time step. We can now calculate the
    final hidden state value based on this remembered information as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 但这还没完——一旦我们获得了来自前一个时间戳的信息，并且通过更新门增加并通过重置门调整后，就产生了来自前一个时间步的记忆信息。我们现在可以根据这些记忆信息计算最终的隐藏状态值，如下所示：
- en: '![](img/57708565-093b-49c9-98c2-dda656dad36a.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/57708565-093b-49c9-98c2-dda656dad36a.png)'
- en: First, we take the element-wise product between the previous hidden state and
    the update gate to determine how much information from the previous state should
    be kept. Then we add to that the element-wise product between the update gate
    and the remembered information from the previous state. Note that the update gate
    is used to feed a percentage of new information and a percentage of old information.
    That's why we're using a *1-* operation in the second part of the formula.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们对前一个隐藏状态与更新门进行逐元素相乘，以确定前一个状态应保留多少信息。然后，我们将更新门与前一个状态记忆信息的逐元素乘积相加。注意，更新门用于引入一定比例的新信息和旧信息。这就是为什么在公式的第二部分使用*1-*操作的原因。
- en: The GRU is a large step up from the recurrent layer in terms of the calculations
    involved and its capability to remember long-term and short-term relationships.
    However, it cannot do both at the same time.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: GRU在涉及计算和记忆长期及短期关系的能力方面，比传统的递归层有了很大的提升。然而，它不能同时处理这两者。
- en: Working with long short-term memory units
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用长短期记忆单元
- en: Another alternative to working with basic recurrent layers is to use a **Long
    Short-Term Memory** (**LSTM**) unit. This recurrent layer works with gates just
    like the GRU that we discussed in the previous section, except the LSTM has a
    lot more gates.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种使用基本递归层的替代方法是使用**长短期记忆**（**LSTM**）单元。这个递归层像我们在上一部分讨论的GRU一样，也使用门控机制，区别在于LSTM有更多的门控机制。
- en: 'The following diagram outlines the structure of the LSTM layer:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了LSTM层的结构：
- en: '![](img/5b72f45d-f78f-47ee-8dfa-1840fcfa61c0.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b72f45d-f78f-47ee-8dfa-1840fcfa61c0.png)'
- en: The LSTM unit has a cell state that is central to how this layer type works.
    The cell state is kept over long periods of time and doesn't change much. The
    LSTM layer also has a hidden state, but this state serves a different role in
    the layer.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM单元有一个单元状态，这是该层类型工作原理的核心。单元状态在长时间内保持不变，变化很小。LSTM层还有一个隐藏状态，但这个状态在该层中扮演着不同的角色。
- en: 'In short, the LSTM has a long-term memory modeled as the cell state, and a
    short-term memory modeled as the hidden state. The access to the long-term memory
    is guarded using several gates. There are two gates that control the long-term
    memory access in the LSTM layer:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，LSTM有一个长期记忆，表现为单元状态，以及一个短期记忆，表现为隐藏状态。对长期记忆的访问是通过多个门来保护的。在LSTM层中，有两个门控制长期记忆的访问：
- en: The forget gate, which controls what will be forgotten from the cell state
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遗忘门，控制从单元状态中遗忘什么内容
- en: The input gate, which controls what will be stored from the hidden state and
    input in the cell state
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入门，控制什么内容会从隐藏状态和输入中存储到单元状态中
- en: There's one final gate in the LSTM layer that controls what to take from the
    cell state into the new hidden state. Essentially, we're using the output gate
    to control what to take from long-term memory into short-term memory. Let's go
    over how the layer works step by step.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM层中还有一个最终的门，控制从单元状态中获取哪些信息到新的隐藏状态。实际上，我们使用输出门来控制从长期记忆中取出什么内容到短期记忆中。让我们一步步了解这一层是如何工作的。
- en: 'First, we''ll take a look at the forget gate. The forget gate is the first
    gate that will get updated when you make a prediction with an LSTM layer:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来看看遗忘门。遗忘门是当你使用LSTM层进行预测时，第一个会被更新的门：
- en: '![](img/dfb67acd-f099-4a82-9feb-7c3afa72470e.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfb67acd-f099-4a82-9feb-7c3afa72470e.png)'
- en: 'The forget gate controls how much of the cell state should be forgotten. It
    is updated with the following formula:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 遗忘门控制应该遗忘多少单元状态。它使用以下公式进行更新：
- en: '![](img/c773dd4c-f829-4e0f-9273-a61a20e28fde.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c773dd4c-f829-4e0f-9273-a61a20e28fde.png)'
- en: When you take a closer look at this formula, you will notice that it is essentially
    a dense layer with a `sigmoid` activation function. The forget gate generates
    a vector with values between zero and one to control how much of the elements
    in the cell state are forgotten. A value of one on the forget gate means that
    the value in the cell state is kept. A value of zero on the forget gate makes
    the cell state forget its value.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当你仔细查看这个公式时，你会发现它本质上是一个带有`sigmoid`激活函数的全连接层。遗忘门生成一个在零到一之间的值的向量，用来控制单元状态中多少元素被遗忘。遗忘门的值为一时，表示单元状态中的值被保留。遗忘门的值为零时，表示单元状态中的值被遗忘。
- en: We're concatenating the hidden state from the previous step and the new input
    into one matrix along the column axis. The cell state will essentially store long-term
    information about the input provided to the layer with the hidden state that was
    stored in the layer.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将前一步的隐藏状态和新的输入沿列轴拼接成一个矩阵。单元状态本质上会存储输入提供的长期信息，以及层内存储的隐藏状态。
- en: 'The second gate in the LSTM layer is the input gate. The input gate controls
    how much new data is stored in the cell state. The new data is a combination of
    the hidden state from the previous step and the input for the current time step,
    as is demonstrated in the following diagram:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM层中的第二个门是输入门。输入门控制有多少新数据会被存储在单元状态中。新数据是前一步的隐藏状态和当前时间步的输入的结合，正如以下图示所示：
- en: '![](img/851cfd14-6e8d-4b29-a10b-1980d63593eb.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/851cfd14-6e8d-4b29-a10b-1980d63593eb.png)'
- en: 'We''ll use the following formula to determine the value for the update gate:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下公式来确定更新门的值：
- en: '![](img/b2d16767-7b3a-4143-9d07-19af88809941.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2d16767-7b3a-4143-9d07-19af88809941.png)'
- en: 'Just like the forget gate, the input gate is modeled as a nested dense layer
    within the LSTM layer. You can see the input gate as the left branch within the
    highlighted section of the previous diagram. The input gate is used in the following
    formula to determine the new value to put into the cell state:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 就像遗忘门一样，输入门也被建模为LSTM层内的一个嵌套全连接层。你可以看到输入门作为前面图示中突出部分的左分支。输入门在以下公式中用于确定要放入单元状态的新的值：
- en: '![](img/e5e5aa98-5c77-4939-9761-de781c1fe5dc.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5e5aa98-5c77-4939-9761-de781c1fe5dc.png)'
- en: 'To update the cell state, we need one more step, which is highlighted in the
    next diagram:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更新单元状态，我们还需要一步，下一张图将突出显示这一点：
- en: '![](img/2098dad2-762f-4ec9-b3cf-8e59ce28d043.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2098dad2-762f-4ec9-b3cf-8e59ce28d043.png)'
- en: 'Once we know the values for the forget gate and input gate, we can calculate
    the updated cell state using the following formula:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们知道遗忘门和输入门的值，就可以使用以下公式计算更新后的单元状态：
- en: '![](img/10386cb0-d388-4c08-ad54-b5a68b6f2ea1.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10386cb0-d388-4c08-ad54-b5a68b6f2ea1.png)'
- en: First, we'll multiply the forget gate with the previous cell state to forget
    old information. We then multiply the update gate with the new values for the
    cell state to learn new information. We sum both values up to produce the final
    cell state for the current time step.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将遗忘门与上一个单元状态相乘，以遗忘旧的信息。然后，我们将更新门与新值的单元状态相乘，以学习新信息。我们将两个值相加，生成当前时间步的最终单元状态。
- en: 'The final gate in the LSTM layer is the output gate. This gate controls how
    much information from the cell state is used in the output of the layer and the
    hidden state for the next time step, as demonstrated in the following diagram:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 层中的最后一个门是输出门。这个门控制着从单元状态中有多少信息被用于层的输出和下一个时间步的隐藏状态，具体如下面的示意图所示：
- en: '![](img/428c9e0e-cf19-4656-bea2-3cadc8624716.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/428c9e0e-cf19-4656-bea2-3cadc8624716.png)'
- en: 'The output gate is calculated using the following formula:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 输出门使用以下公式计算：
- en: '![](img/e4613046-8448-455f-8353-2d003fe7e0d2.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4613046-8448-455f-8353-2d003fe7e0d2.png)'
- en: 'The output gate is, just like the input gate and forget gate, a dense layer
    that controls how much of the cell state is copied to the output. We can now calculate
    the new hidden state, or output, of the layer using the following formula:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 输出门就像输入门和遗忘门一样，是一个密集层，控制有多少单元状态被复制到输出中。我们现在可以使用以下公式计算层的新隐藏状态或输出：
- en: '![](img/32364dca-d4db-48a4-9d2f-364ba600154e.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32364dca-d4db-48a4-9d2f-364ba600154e.png)'
- en: You can use this new hidden state to calculate the next time step, or return
    it as the output for the layer.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用这个新的隐藏状态来计算下一个时间步，或者将其作为层的输出返回。
- en: When to use other recurrent layer types
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时使用其他递归层类型
- en: The GRU and LSTM layers are definitely more complex than regular recurrent layers.
    They have a lot more parameters that need to be trained. This will make it harder
    to debug the model when you run into problems.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: GRU 和 LSTM 层的复杂度明显高于常规递归层。它们有更多需要训练的参数。这会使得当你遇到问题时，调试模型变得更加困难。
- en: The regular recurrent layer doesn't hold up well when you work with longer sequences
    of data, because it gets saturated quickly. You can use both the LSTM and GRU
    to resolve this problem. The GRU layer works without an additional memory state.
    The LSTM uses a cell state to model long-term memory.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 常规递归层在处理较长的数据序列时表现不好，因为它会很快饱和。你可以使用 LSTM 和 GRU 来解决这个问题。GRU 层不需要额外的记忆状态，而 LSTM
    使用单元状态来模拟长期记忆。
- en: Because the GRU has fewer gates and no memory, it takes less time to train it.
    So, if you're looking to process longer sequences and need a network that can
    be trained relatively quickly, use the GRU layer.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 GRU 的门较少且没有记忆，它的训练时间较短。因此，如果你需要处理较长的序列并且需要一个相对较快的训练网络，使用 GRU 层。
- en: The LSTM layer has more power to express relationships in the sequences you
    feed it. This means that it will perform better if you have enough data to train
    it. In the end, it comes down to experimentation to determine which layer type
    works best for your solution.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 层在表达你输入序列中的关系方面有更强的能力。这意味着，如果你有足够的数据来训练它，它的表现会更好。最终，还是需要通过实验来确定哪种层类型最适合你的解决方案。
- en: Building recurrent neural networks with CNTK
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CNTK 构建递归神经网络
- en: Now that we've explored the theory behind recurrent neural networks, it's time
    to build one with CNTK. There are several building blocks that CNTK offers for
    building recurrent neural networks. We're going to explore how to build a recurrent
    neural network using a sample dataset containing power measurements from a solar
    panel.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了递归神经网络背后的理论，接下来就该用 CNTK 构建一个递归神经网络了。CNTK 提供了多个构建块用于构建递归神经网络。我们将探索如何使用包含太阳能板功率测量的示例数据集来构建递归神经网络。
- en: The power output of a solar panel changes during the day, so it's hard to predict
    how much power is generated for a typical house. This makes it hard for a local
    energy company to predict how much additional power they should generate to keep
    up with demand.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 太阳能板的功率输出在一天中会发生变化，因此很难预测一个典型家庭能生成多少电力。这使得当地能源公司很难预测他们应该生成多少额外电力以跟上需求。
- en: Luckily, many energy companies offer software that allows customers to keep
    track of the power output of their solar panels. This will allow them to train
    a model based on this historical data, so we can predict what the total power
    output will be per day.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，许多能源公司提供软件，允许客户跟踪太阳能板的功率输出。这将使他们能够基于这些历史数据训练模型，从而预测每天的总功率输出。
- en: We're going to train a power output prediction model using recurrent neural
    networks based on a dataset offered by Microsoft as part of the CNTK documentation.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用递归神经网络来训练一个功率输出预测模型，数据集由微软提供，作为 CNTK 文档的一部分。
- en: The dataset contains multiple measurements per day, and contains the current
    power output at a timestamp, and the total amount of power produced up to that
    timestamp. It's our goal to predict the total power produced for a day, based
    on the measurements collected during the day.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含每天多个测量值，每个时间戳下包含当前的功率输出以及截至该时间戳的总功率。我们的目标是根据当天收集的测量数据，预测当天的总功率输出。
- en: You can use a regular neural network, but that would mean that we would have
    to turn each collected measurement into a feature for the input. Doing so assumes
    that there's no correlation between the measurements. But, in practice, there
    is. Each future measurement depends on a measurement that came before. So, a recurrent
    model that can reason over time is much more practical for this case.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用常规的神经网络，但这意味着我们必须将每个收集的测量值转化为输入特征。这样做假设测量值之间没有相关性。然而，实际上是有的。每一个未来的测量值都依赖于之前的一个测量值。因此，能够进行时间推理的递归模型对于这种情况来说要实用得多。
- en: In the next three sections we'll explore how to build a recurrent neural network
    in CNTK. After that we'll explore how to train the recurrent neural network using
    data from the solar panel dataset. Finally, we'll take a look how to predict output
    with a recurrent neural network.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的三个部分中，我们将探讨如何在 CNTK 中构建递归神经网络。之后，我们将研究如何使用太阳能板数据集中的数据来训练递归神经网络。最后，我们将了解如何用递归神经网络进行输出预测。
- en: Building the neural network structure
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建神经网络结构
- en: 'Before we can start to make predictions about the output of a solar panel we
    need to construct a recurrent neural network. A recurrent neural network is built
    in the same fashion as a regular neural network. Here''s how to build one:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始预测太阳能板的输出之前，我们需要构建一个递归神经网络。递归神经网络的构建方式与常规神经网络相同。以下是构建方法：
- en: '[PRE1]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Follow the given steps:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定步骤操作：
- en: First, create a new input variable to store the input sequence.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，创建一个新的输入变量来存储输入序列。
- en: Then, initialize the default_options for the neural network and provide the
    initial_state setting with a value of 0.1.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，初始化神经网络的default_options，并将initial_state设置为0.1。
- en: Next, Create a Sequential layer set for the neural network.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个神经网络的Sequential层集。
- en: In the Sequential layer set, provide a LSTM recurrent layer with 15 neurons
    wrapped in a Fold layer.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Sequential层集里，提供一个带有15个神经元的LSTM递归层，并将其包装在一个Fold层中。
- en: Finally, add a Dense layer with one neuron.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，添加一个包含一个神经元的Dense层。
- en: There are two ways in which you can model recurrent neural networks in CNTK.
    If you're only interested in the final output of a recurrent layer, you can use
    the `Fold` layer combined with a recurrent layer, such as GRU, LSTM, or even RNNStep.
    The `Fold` layer collects the final hidden state of the recurrent layer, and returns
    that as the output to be used by the next layer.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CNTK 中，你可以用两种方式来建模递归神经网络。如果你只关心递归层的最终输出，可以使用`Fold`层与递归层（例如 GRU、LSTM，甚至是 RNNStep）结合使用。`Fold`层会收集递归层的最终隐藏状态，并将其作为输出返回，供下一个层使用。
- en: As an alternative to the `Fold` layer, you can also use the `Recurrence` block.
    This wrapper returns the full sequence generated by the recurrent layer you wrap
    in it. This is useful if you want to generate sequential output with your recurrent
    neural network.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 作为`Fold`层的替代方案，你也可以使用`Recurrence`模块。这个封装器会返回递归层生成的完整序列。这在你希望用递归神经网络生成序列输出时非常有用。
- en: A recurrent neural network works with sequential input, this is why we're using
    the `sequence.input_variable` function instead of a regular `input_variable` function.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络处理的是顺序输入，这就是为什么我们使用`sequence.input_variable`函数，而不是常规的`input_variable`函数。
- en: The regular `input_variable` function supports only fixed dimensions for the
    input. This means that we have to know the number of features that we want to
    feed into the network for each sample. This applies to both regular models and
    models that process images. In image classification models, we typically use one
    dimension for color channels, and another two dimensions for the width and height
    of the input image. We know all these dimensions upfront. The only dimension that
    is dynamic in the regular `input_variable` function is the batch dimension. This
    dimension gets calculated when you train the model with a certain minibatch size
    setting, which results in a fixed value for the batch dimension.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 常规的`input_variable`函数仅支持固定维度的输入。这意味着我们必须知道每个样本要输入网络的特征数量。这适用于常规模型和处理图像的模型。在图像分类模型中，我们通常使用一个维度表示颜色通道，另外两个维度表示输入图像的宽度和高度。这些维度我们是事先知道的。常规`input_variable`函数中唯一动态的维度是批量维度。这个维度在你使用特定迷你批次大小设置训练模型时计算出来，进而得出批量维度的固定值。
- en: In recurrent neural networks, we don't know how long each sequence will be.
    We only know the shape of each piece of data stored in the sequence as a time
    step. The `sequence.input_variable` function allows us to provide the dimensions
    for each time step, and keep the dimension that models the sequence length dynamic.
    As with the regular `input_variable` function, the batch dimension is also dynamic.
    We configure this dimension when we start training with a particular minibatch
    size setting.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在递归神经网络中，我们不知道每个序列的长度。我们只知道每个时间步中存储的数据的形状。`sequence.input_variable`函数允许我们为每个时间步提供维度，并保持模型序列长度的维度动态。与常规的`input_variable`函数一样，批量维度也是动态的。我们在开始训练时配置此维度，并设置特定的迷你批次大小。
- en: CNTK is unique in the way it handles sequential data. In frameworks such as
    TensorFlow, you have to specify the dimensions for both the sequence length and
    batch upfront, before you can start training. Because you have to use fixed size
    sequences, you will need to add padding to sequences that are shorter than the
    maximum sequence length supported by your model. Also, if you have longer sequences,
    you need to truncate them. This leads to lower quality models, because you ask
    the model to learn information from empty time steps in your sequence. CNTK handles
    dynamic sequences quite well, so you don't have to use padding when working with
    sequences in CNTK.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: CNTK 在处理序列数据方面独具特色。在像 TensorFlow 这样的框架中，你必须在开始训练之前，预先指定序列长度和批量的维度。由于必须使用固定大小的序列，因此你需要对比模型支持的最大序列长度短的序列添加填充。同时，如果序列较长，你需要截断它们。这会导致模型质量较低，因为你要求模型从序列中的空时间步学习信息。CNTK
    对动态序列的处理非常好，因此在使用 CNTK 处理序列时，你不必使用填充。
- en: Stacking multiple recurrent layers
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠多个递归层
- en: 'In the previous section, we only talked about using a single recurrent layer.
    You can, however, stack multiple recurrent layers in CNTK. For example, when we
    want to stack two recurrent layers, we need to use the following combination of
    layers:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一部分中，我们只讨论了使用单一的递归层。然而，你可以在 CNTK 中堆叠多个递归层。例如，当我们想堆叠两个递归层时，需要使用以下层的组合：
- en: '[PRE2]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Follow the given steps:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照以下步骤操作：
- en: First, import the `sequence` module, `default_options` function and `input_variable`
    function from the `cntk` package
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从`cntk`包中导入`sequence`模块、`default_options`函数和`input_variable`函数。
- en: Next, import the layers for the recurrent neural network
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，导入递归神经网络的相关层。
- en: Then, create a new `LSTM` layer with 15 neurons and wrap it in a `Recurrence`
    layer so the layer returns a sequence instead of a single output
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，创建一个新的具有15个神经元的`LSTM`层，并将其包装在`Recurrence`层中，以便该层返回一个序列，而不是单一的输出。
- en: Now, create the second `LSTM` layer with 15 neurons, but this time wrap it in
    a `Fold` layer to return only the final time step as output
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，创建第二个具有15个神经元的`LSTM`层，但这次将其包装在`Fold`层中，仅返回最后一个时间步的输出。
- en: Finally, invoke the created `Sequential` layer stack with the features variable
    to complete the neural network
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用特征变量调用创建的`Sequential`层堆栈，以完成神经网络的构建。
- en: This technique can be extended beyond two layers as well; just wrap the layers
    before the last recurrent layer in `Recurrence` layers and wrap the final layer
    in a `Fold` layer.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术同样可以扩展到超过两层的情况；只需在最后的递归层之前将层包装在`Recurrence`层中，并将最后一层包装在`Fold`层中。
- en: For the sample in this chapter we'll limit ourselves to using one recurrent
    layer as we've constructed it in the previous section, *Building the neural network
    structure*. In the next section we'll talk about training the recurrent neural
    network that we've created.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章中的示例，我们将只使用一个循环层，正如我们在前一节中构建神经网络结构时所做的那样。在下一节中，我们将讨论如何训练我们创建的循环神经网络。
- en: Training the neural network with time series data
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用时间序列数据训练神经网络
- en: Now that we have a model, let's take a look at how to train a recurrent neural
    network in CNTK.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个模型，让我们来看看如何在CNTK中训练一个循环神经网络。
- en: 'First, we need to define what loss function we want to optimize. Since we''re
    predicting a continuous variable—power output—we need to use a mean squared error
    loss. We''ll combine the loss with a mean square error metric to measure the performance
    of our model. Remember, from [Chapter 4](e39df191-73e4-414f-b44b-efca6f0ad4cd.xhtml),
    *Validating Model Performance*, that we can combine the loss and metric in a single
    function object using `@Function`:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要定义我们想要优化的损失函数。由于我们在预测一个连续变量——功率输出——我们需要使用均方误差损失函数。我们将把损失函数与均方误差度量标准结合，以衡量模型的表现。请记住，来自[第4章](e39df191-73e4-414f-b44b-efca6f0ad4cd.xhtml)，*验证模型性能*，我们可以使用`@Function`将损失函数和度量标准结合成一个函数对象：
- en: '[PRE3]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We're going to use the `adam` learner to optimize the model. This learner is
    an extension of the **Stochastic Gradient Descent** (**SGD**) algorithm. While
    SGD uses a fixed learning rate, Adam changes the learning rate over time. In the
    beginning, it will use a high learning rate to get results fast. Once it has run
    for a while, it will start to lower the learning rate to increase accuracy. The
    `adam` optimizer is a lot faster than SGD in optimizing the `loss` function.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`adam`学习器来优化模型。这个学习器是**随机梯度下降**（**SGD**）算法的扩展。虽然SGD使用固定的学习率，但Adam会随着时间的推移调整学习率。在开始时，它会使用较高的学习率来快速得到结果。一段时间后，它会开始降低学习率，以提高准确性。`adam`优化器在优化`loss`函数时比SGD更快。
- en: Now we have a loss, metric, we can use both in-memory and out-of-memory data
    to train the recurrent neural network.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了损失函数和度量标准，我们可以使用内存中的数据和内存外的数据来训练循环神经网络。
- en: 'The data for a recurrent neural network needs to be modeled as sequences. In
    our case, the input data is a sequence of power measurements for each day, stored
    in a **CNTK Text Format** (**CTF**) file.Follow the given steps:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络的数据需要建模为序列。在我们的例子中，输入数据是每天的功率测量序列，存储在**CNTK文本格式**（**CTF**）文件中。请按照给定的步骤操作：
- en: 'In [Chapter 3](f7cd9148-99e8-427c-acf4-d74c3e52df58.xhtml), *Getting Data into
    Your Neural Network*, we discussed how you can store data for training in CNTK
    in CTF format. The CTF file format not only supports storing basic samples, but
    also supports storing sequences. A CTF file for sequences has the following layout:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](f7cd9148-99e8-427c-acf4-d74c3e52df58.xhtml)，*将数据输入神经网络*，我们讨论了如何将数据以CTF格式存储用于CNTK训练。CTF文件格式不仅支持存储基本样本，还支持存储序列。一个用于序列的CTF文件具有以下结构：
- en: '[PRE4]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Each line is prefixed with a unique number to identify the sequence. CNTK will
    consider lines with the same sequence identifier to be one sequence. So, you can
    store one sequence over multiple lines. Each line can contain one time step in
    the sequence.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行都以一个独特的编号为前缀，以标识该序列。CNTK将把具有相同序列标识符的行视为一个序列。所以，你可以将一个序列跨多行存储。每一行可以包含序列中的一个时间步。
- en: 'There''s one important detail you have to keep in mind when storing sequences
    over multiple lines in a CTF file. One of the lines storing the sequence should
    also contain the expected output for the sequence. Let''s take a look at what
    that looks like in practice:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在将序列跨多行存储到CTF文件时，有一个重要的细节需要记住。存储序列的某一行还应该包含该序列的预期输出。让我们来看一下这在实际操作中的表现：
- en: '[PRE5]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The first line for a sequence contains the `target` variable, as well as the
    data for the first time step in the sequence. The `target` variable is used to
    store the expected power output for a particular sequence. The other lines for
    the same sequence only contain the `features` variable. You can't use the input
    file if you do put the `target` variable on a separate line. The minibatch source
    will fail to load.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 序列的第一行包含`target`变量，以及序列中第一时间步的数据。`target`变量用于存储特定序列的预期功率输出。对于同一序列的其他行，只包含`features`变量。如果你将`target`变量放在单独的行中，则无法使用输入文件，迷你批次源将无法加载。
- en: 'You can load the sequence data into your training code, like so:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像这样将序列数据加载到你的训练代码中：
- en: '[PRE6]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Follow the given steps:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤：
- en: 'First, create a new function create_datasource with two parameters: filename,
    and sweeps which has a default of INFINITELY_REPEAT so we can iterate over the
    same dataset multiple times.'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，创建一个新函数`create_datasource`，它有两个参数：`filename`和`sweeps`，其中`sweeps`的默认值为INFINITELY_REPEAT，以便我们可以多次迭代相同的数据集。
- en: In the `create_datasource` function, define two streams for the minibatch source,
    one for the input features and one for the expected output of our model.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`create_datasource`函数中，为小批量数据源定义两个流，一个用于输入特征，一个用于模型的期望输出。
- en: Then use `CTFDeserializer` to read the input file.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后使用`CTFDeserializer`来读取输入文件。
- en: Finally, return a new `MinibatchSource` for the input file provided.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，返回一个新的`MinibatchSource`，用于提供的输入文件。
- en: To train the model, we need to iterate over the same data multiple times to
    train for multiple epochs. That's why you should use an infinity setting for the
    `max_sweeps` of the minibatch source. Testing is done by iterating over a set
    of validation samples so that we configure the minibatch source with just one
    sweep.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型，我们需要多次迭代相同的数据以训练多个周期。这就是为什么你应该为小批量数据源使用无限制的`max_sweeps`设置。测试是通过迭代一组验证样本完成的，所以我们配置小批量数据源时只需要进行一次迭代。
- en: 'Let''s train the neural network with the data sources provided, as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用提供的数据源来训练神经网络，如下所示：
- en: '[PRE7]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Follow the given steps:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤：
- en: First, initialize a `ProgressPrinter` to log the output of the training process.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，初始化一个`ProgressPrinter`来记录训练过程的输出。
- en: Then, create a new test configuration to validate the neural network using data
    from the `test_datasource`.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，创建一个新的测试配置，使用来自`test_datasource`的数据来验证神经网络。
- en: Next, Create a mapping between the input variables of the neural network and
    the streams from the training datasource.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个映射，将神经网络的输入变量与训练数据源中的流进行关联。
- en: Finally, invoke the train method on the loss function to start the training
    process. Provide it the `train_datasource`, settings, the learner, `input_map`
    and the callbacks for logging and testing.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在损失函数上调用训练方法以启动训练过程。为它提供`train_datasource`、设置、学习器、`input_map`以及用于记录和测试的回调函数。
- en: The model needs to train for quite a long time, so grab yourself a coffee or
    two when you plan to run the sample code on your machine.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型需要训练很长时间，所以在你计划在机器上运行示例代码时，可以准备一到两杯咖啡。
- en: 'The `train` method will output the metrics and loss values on screen, because
    we used `ProgressPrinter` as a callback for the `train` method. The output will
    look similar to this:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`train`方法将在屏幕上输出指标和损失值，因为我们将`ProgressPrinter`作为回调传递给`train`方法。输出将类似于如下：'
- en: '[PRE8]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As good practice, you want to validate your model against a separate test set.
    That's why we created the `test_datasource` function earlier. To use this data
    to validate your model, you can use a `TestConfig` object as a callback for the
    `train` method. The testing logic will be called automatically when the training
    process is completed.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种良好的实践，你应该使用单独的测试集来验证你的模型。这就是我们之前创建`test_datasource`函数的原因。要使用这些数据来验证你的模型，你可以将`TestConfig`对象作为回调传递给`train`方法。测试逻辑将在训练过程完成后自动调用。
- en: Predicting output
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测输出
- en: 'When the model is finally done training, you can test it using a few sample
    sequences that can be found in the sample code for this chapter. Remember, a CNTK
    model is a function, so you can invoke it with a numpy array representing the
    sequence for which you want to predict the total output, as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型最终完成训练后，你可以使用一些样本序列进行测试，这些样本可以在本章的示例代码中找到。记住，CNTK模型是一个函数，所以你可以使用一个代表你想要预测总输出的序列的numpy数组来调用它，如下所示：
- en: '[PRE9]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Follow the given steps:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤：
- en: First, import the pickle package
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入pickle包。
- en: Next, define the settings to normalize the data
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义设置以规范化数据。
- en: After that, open the test_samples.pkl file for reading.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，打开`test_samples.pkl`文件以进行读取。
- en: Once the file is opened, load its contents using the pickle.load function.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文件打开后，使用pickle.load函数加载其内容。
- en: Finally, run the samples through the network and multiply them with the NORMALIZE
    constant to obtain the predicted output for the solar panel.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将样本通过网络运行，并用NORMALIZE常数乘以它们，以获得太阳能电池板的预测输出。
- en: The output produced by the model is between zero and one, because that's what
    we stored in the original dataset. The values represent a normalized version of
    the power output of the solar panel. We need to multiply them by the normalization
    value that we used to normalize the original measurements to get the actual power
    output of the solar panel.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输出的值介于零和一之间，因为这正是我们在原始数据集中存储的值。这些值表示太阳能电池板功率输出的规范化版本。我们需要将它们乘以我们用来规范化原始测量值的规范化值，才能得到太阳能电池板的实际功率输出。
- en: 'The final denormalized output for the model looks like this:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的最终反规范化输出如下所示：
- en: '[PRE10]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Predicting with a recurrent neural network is pretty similar to making predictions
    with any other CNTK model, except for the fact that you need to provide sequences
    rather than single samples.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 使用递归神经网络进行预测与使用任何其他CNTK模型进行预测非常相似，区别在于您需要提供的是序列而不是单一样本。
- en: Summary
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we've looked at how to use recurrent neural networks to make
    predictions based on time series data. Recurrent neural networks are useful in
    scenarios where you have to deal with financial data, IoT data, or any other information
    that is collected over time.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何使用递归神经网络根据时间序列数据进行预测。递归神经网络在处理财务数据、物联网数据或任何其他随时间收集的信息的场景中非常有用。
- en: One important building block for recurrent neural networks is the `Fold` and
    the `Recurrence` layer types, which you can combine with any of the recurrent
    layer types, such as RNNStep, GRU, or LSTM, to build a recurrent layer set. Depending
    on whether you want to predict a sequence or single value, you can use the `Recurrence`
    or `Fold` layer types to wrap the recurrent layers.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络的一个重要构建模块是`Fold`和`Recurrence`层类型，您可以将它们与任何递归层类型（如RNNStep、GRU或LSTM）结合使用，以构建递归层集。根据您是要预测序列还是单一值，您可以使用`Recurrence`或`Fold`层类型来包装递归层。
- en: When you're training a recurrent neural network, you can make use of the sequence
    data stored in the CTF file format to make it easier to train the model. But,
    you can just as easily use sequences stored as numpy arrays, as long as you use
    the correct combination of sequence input variables with recurrent layers.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 当您训练递归神经网络时，可以利用存储在CTF文件格式中的序列数据，使训练模型变得更容易。但是，您同样可以使用存储为numpy数组的序列数据，只要您使用正确的序列输入变量与递归层进行组合。
- en: Making predictions with a recurrent neural network is just as easy as it is
    for regular neural networks. The only difference is the input data format, which
    is, just as for training, a sequence.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 使用递归神经网络进行预测和使用常规神经网络一样简单。唯一的区别是输入数据格式，和训练时一样，都是一个序列。
- en: 'In the next chapter, we''ll look at one last topic for this book: *Deploying
    Models to Production*. We''ll explore how to use CNTK models you''ve built in
    C# or Java, and how to properly manage your experiments using tools such as the
    Azure Machine Learning service.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨本书的最后一个主题：*将模型部署到生产环境*。我们将探讨如何在C#或Java中使用您构建的CNTK模型，以及如何使用Azure机器学习服务等工具正确管理实验。
