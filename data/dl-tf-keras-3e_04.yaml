- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Word Embeddings
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入
- en: In the previous chapter, we talked about convolutional networks, which have
    been very successful against image data. Over the next few chapters, we will switch
    tracks to focus on strategies and networks to handle text data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，我们讨论了卷积网络，它们在图像数据中非常成功。在接下来的几章中，我们将切换方向，专注于处理文本数据的策略和网络。
- en: In this chapter, we will first look at the idea behind word embeddings, and
    then cover the two earliest implementations – Word2Vec and GloVe. We will learn
    how to build word embeddings from scratch using the popular library Gensim on
    our own corpus and navigate the embedding space we create.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先了解词嵌入的概念，然后介绍两个最早的实现——Word2Vec 和 GloVe。我们将学习如何使用流行的库 Gensim 在我们自己的语料库上从头开始构建词嵌入，并探索我们创建的嵌入空间。
- en: We will also learn how to use pretrained third-party embeddings as a starting
    point for our own NLP tasks, such as spam detection, that is, learning to automatically
    detect unsolicited and unwanted emails. We will then learn about various ways
    to leverage the idea of word embeddings for unrelated tasks, such as constructing
    an embedded space for making item recommendations.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将学习如何使用预训练的第三方嵌入作为我们自己自然语言处理任务（例如垃圾邮件检测）的起点，即学习如何自动检测未经请求和不需要的电子邮件。接着，我们将了解如何利用词嵌入的概念解决无关的任务，例如为推荐物品构建嵌入空间。
- en: We will then look at extensions to these foundational word embedding techniques
    that have occurred in the last decade since Word2Vec – adding syntactic similarity
    with fastText, adding the effect of context using neural networks such as ELMo
    and Google Universal Sentence Encoder, sentence encodings such as InferSent and
    skip-thoughts, and the introduction of language models such as ULMFiT and BERT.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将了解自 Word2Vec 以来，过去十年间对这些基础词嵌入技术的扩展——通过 fastText 添加句法相似性，通过像 ELMo 和 Google
    Universal Sentence Encoder 这样的神经网络加入上下文的影响，通过 InferSent 和 skip-thoughts 这样的句子编码，此外还引入了像
    ULMFiT 和 BERT 这样的语言模型。
- en: 'In this chapter, we’ll learn about the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下内容：
- en: Word embeddings – origins and fundamentals
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词嵌入——起源与基本原理
- en: Distributed representations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式表示
- en: Static embeddings
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态嵌入
- en: Creating your own embedding with Gensim
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Gensim 创建你自己的嵌入
- en: Exploring the embedding space with Gensim
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Gensim 探索嵌入空间
- en: Using word embedding for spam detection
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用词嵌入进行垃圾邮件检测
- en: Neural embedding – not just for words
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经嵌入——不仅仅是词嵌入
- en: Character and subword embedding
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符和子词嵌入
- en: Dynamic embeddings
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态嵌入
- en: Sentence and paragraph embeddings
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子和段落嵌入
- en: Language-based model embeddings
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于语言的模型嵌入
- en: All the code files for this chapter can be found at [https://packt.link/dltfchp4](https://packt.link/dltfchp4).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码文件可以在[https://packt.link/dltfchp4](https://packt.link/dltfchp4)找到。
- en: Let’s begin!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Word embedding ‒ origins and fundamentals
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入 ‒ 起源与基本原理
- en: Wikipedia defines word embedding as the collective name for a set of language
    modeling and feature learning techniques in **natural language processing** (**NLP**)
    where words or phrases from a vocabulary are mapped to vectors of real numbers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科将词嵌入定义为自然语言处理（**NLP**）中一组语言建模和特征学习技术的统称，其中词汇表中的词或短语被映射为实数向量。
- en: Deep learning models, like other machine learning models, typically don’t work
    directly with text; the text needs to be converted to numbers instead. The process
    of converting text to numbers is a process called vectorization. An early technique
    for vectorizing words was one-hot encoding, which you learned about in *Chapter
    1*, *Neural Network Foundations with TF*. As you will recall, a major problem
    with one-hot encoding is that it treats each word as completely independent from
    all the others, since the similarity between any two words (measured by the dot
    product of the two word vectors) is always zero.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型和其他机器学习模型一样，通常不会直接处理文本；文本需要转换为数字。将文本转换为数字的过程称为向量化。早期的词向量化技术是独热编码，你在*第1章*，*使用TF的神经网络基础*中学习过。正如你会回忆的那样，独热编码的一个主要问题是，它将每个词视为与其他所有词完全独立，因为任何两个词之间的相似度（通过两个词向量的点积来衡量）总是零。
- en: 'The dot product is an algebraic operation that operates on two vectors ![](img/B18331_04_001.png)
    and ![](img/B18331_04_002.png)of equal length and returns a number. It is also
    known as the inner product or scalar product:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 点积是对两个等长向量进行的代数运算！[](img/B18331_04_001.png) 和 ![](img/B18331_04_002.png)，其结果是一个数值。它也被称为内积或标量积：
- en: '![](img/B18331_04_003.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_04_003.png)'
- en: Why is the dot product of one-hot vectors of two words always 0? Consider two
    words *w*[i] and *w*[j]. Assuming a vocabulary size of *V*, their corresponding
    one-hot vectors are a zero vector of rank *V* with positions *i* and *j* set to
    1\. When combined using the dot product operation, the 1 in a[i] is multiplied
    by 0 in b[i], and 1 in b[j] is multiplied by 0 in a[j], and all other elements
    in both vectors are 0, so the resulting dot product is also 0.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么两个词的独热向量的点积总是 0？考虑两个词*w*[i]和*w*[j]。假设词汇表大小为*V*，它们对应的独热向量是一个秩为*V*的零向量，其中位置*i*和*j*设为
    1。当使用点积操作组合时，a[i]中的 1 将与b[i]中的 0 相乘，b[j]中的 1 将与a[j]中的 0 相乘，且两个向量中的其他所有元素都是 0，因此结果点积也为
    0。
- en: To overcome the limitations of one-hot encoding, the NLP community has borrowed
    techniques from **Information Retrieval** (**IR**) to vectorize text using the
    document as the context. Notable techniques are **Term Frequency-Inverse Document
    Frequency** (**TF-IDF**) [35], **Latent Semantic Analysis** (**LSA**) [36], and
    topic modeling [37]. These representations attempt to capture a document-centric
    idea of semantic similarity between words. Of these, one-hot and TF-IDF are relatively
    sparse embeddings, since vocabularies are usually quite large, and a word is unlikely
    to occur in more than a few documents in the corpus.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服独热编码的局限性，NLP 社区借鉴了**信息检索**（**IR**）技术，通过使用文档作为上下文来将文本向量化。值得注意的技术包括**词频-逆文档频率**（**TF-IDF**）[35]、**潜在语义分析**（**LSA**）[36]和主题建模[37]。这些表示方法试图捕捉基于文档的词语语义相似性。在这些方法中，独热编码和
    TF-IDF 是相对稀疏的嵌入，因为词汇通常很大，且一个词在语料库中出现的文档数量通常较少。
- en: The development of word embedding techniques began around 2000\. These techniques
    differ from previous IR-based techniques in that they use neighboring words as
    their context, leading to a more natural semantic similarity from a human understanding
    perspective. Today, word embedding is a foundational technique for all kinds of
    NLP tasks, such as text classification, document clustering, part-of-speech tagging,
    named entity recognition, sentiment analysis, and many more. Word embeddings result
    in dense, low-dimensional vectors, and along with LSA and topic models can be
    thought of as a vector of latent features for the word.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量技术的开发始于 2000 年左右。这些技术与以前基于 IR 的技术不同，因为它们使用邻近的词作为上下文，从而在从人类理解的角度来看产生更自然的语义相似性。今天，词嵌入已成为所有类型的
    NLP 任务的基础技术，如文本分类、文档聚类、词性标注、命名实体识别、情感分析等。词嵌入产生了密集的低维向量，并且与 LSA 和主题模型一起，可以将其视为该词的潜在特征向量。
- en: Word embeddings are based on the distributional hypothesis, which states that
    words that occur in similar contexts tend to have similar meanings. Hence the
    class of word embedding-based encodings is also known as distributed representations,
    which we will talk about next.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量基于分布假设，分布假设认为在相似上下文中出现的词语往往具有相似的意义。因此，基于词嵌入的编码方法也被称为分布式表示，我们将在接下来的部分讨论这一点。
- en: Distributed representations
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式表示
- en: 'Distributed representations attempt to capture the meaning of a word by considering
    its relations with other words in its context. The idea behind the distributed
    hypothesis is captured in this quote from *J. R. Firth*, a linguist, who first
    proposed this idea:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式表示通过考虑一个词与其上下文中其他词的关系，试图捕捉该词的意义。分布假设的思想可以通过语言学家*J. R. Firth*的一句话来表达，他首先提出了这一思想：
- en: You shall know a word by the company it keeps.
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以通过一个词汇周围的词来了解它的含义。
- en: 'How does this work? By way of example, consider the following pair of sentences:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何工作的呢？举个例子，考虑以下一对句子：
- en: '*Paris is the capital of France*.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*巴黎是法国的首都*。'
- en: '*Berlin is the capital of Germany*.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*柏林是德国的首都*。'
- en: 'Even assuming no knowledge of world geography, the sentence pair implies some
    sort of relationship between the entities Paris, France, Berlin, and Germany that
    could be represented as:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 即使假设没有世界地理知识，这对句子对也暗示了巴黎、法国、柏林和德国之间某种关系，可以表示为：
- en: '`"Paris" is to "France" as "Berlin" is to "Germany."`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`"巴黎" 对应 "法国" 就像 "柏林" 对应 "德国"。`'
- en: 'Distributed representations are based on the idea that there exists some transformation,
    as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式表示基于这样一种观点：存在某种变换，如下所示：
- en: '`Paris : France :: Berlin : Germany`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`巴黎 : 法国 :: 柏林 : 德国`'
- en: In other words, a distributed embedding space is one where words that are used
    in similar contexts are close to one another. Therefore, the similarity between
    the word vectors in this space would roughly correspond to the semantic similarity
    between the words.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，分布式嵌入空间是指在相似上下文中使用的词汇彼此靠近。因此，这个空间中的词向量相似度大致对应于词汇的语义相似度。
- en: '*Figure 4.1* shows a TensorBoard visualization of word embedding of words around
    the word “important” in the embedding space. As you can see, the neighbors of
    the word tend to be closely related, or interchangeable with the original word.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.1* 显示了TensorBoard对“重要”这个词在嵌入空间中的词汇嵌入的可视化。从中可以看出，词汇的邻居往往是紧密相关的，或与原词可互换。'
- en: 'For example, “crucial” is virtually a synonym, and it is easy to see how the
    words “historical” or “valuable” could be substituted in certain situations:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，“crucial”几乎是一个同义词，很容易看出在某些情况下，“historical”或“valuable”可以互换使用：
- en: '![Graphical user interface  Description automatically generated with low confidence](img/B18331_04_01.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面  描述自动生成，置信度较低](img/B18331_04_01.png)'
- en: 'Figure 4.1: Visualization of nearest neighbors of the word “important” in a
    word embedding dataset, from the TensorFlow Embedding Guide (https://www.tensorflow.org/guide/embedding)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：词嵌入数据集中“重要”一词最近邻的可视化，来源于TensorFlow嵌入指南（https://www.tensorflow.org/guide/embedding）
- en: In the next section, we will look at various types of distributed representations
    (or word embeddings).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将探讨各种类型的分布式表示（或词嵌入）。
- en: Static embeddings
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 静态嵌入
- en: Static embeddings are the oldest type of word embedding. The embeddings are
    generated against a large corpus but the number of words, though large, is finite.
    You can think of a static embedding as a dictionary, with words as the keys and
    their corresponding vector as the value. If you have a word whose embedding needs
    to be looked up that was not in the original corpus, then you are out of luck.
    In addition, a word has the same embedding regardless of how it is used, so static
    embeddings cannot address the problem of polysemy, that is, words with multiple
    meanings. We will explore this issue further when we cover non-static embeddings
    later in this chapter.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 静态嵌入是最古老的词嵌入类型。这些嵌入是基于一个大型语料库生成的，尽管词汇量很大，但仍然是有限的。你可以把静态嵌入看作一个字典，词汇是键，它们对应的向量是值。如果你有一个词汇需要查找其嵌入，但该词汇不在原始语料库中，那么你就无法找到它。此外，无论一个词如何使用，静态嵌入始终是相同的，因此静态嵌入无法解决多义词的问题，即具有多个含义的词汇。我们将在本章后面讨论非静态嵌入时进一步探讨这个问题。
- en: Word2Vec
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Word2Vec
- en: The models known as Word2Vec were first created in 2013 by a team of researchers
    at Google led by *Tomas Mikolov* [1, 2, 3]. The models are self-supervised, that
    is, they are supervised models that depend on the structure of natural language
    to provide labeled training data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 被称为Word2Vec的模型最早由Google的研究团队于2013年创建，团队由*Tomas Mikolov*领导[1, 2, 3]。这些模型是自监督的，也就是说，它们是依赖自然语言结构来提供标注训练数据的监督模型。
- en: 'The two architectures for Word2Vec are as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec的两种架构如下：
- en: '**Continuous Bag of Words** (**CBOW**)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续词袋模型** (**CBOW**)'
- en: Skip-gram
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skip-gram
- en: '![](img/B18331_04_02.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_04_02.png)'
- en: 'Figure 4.2: Architecture of the CBOW and Skip-gram Word2Vec models'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2：CBOW和Skip-gram Word2Vec模型的架构
- en: In the CBOW architecture, the model predicts the current word given a window
    of surrounding words. The order of context words does not influence the prediction
    (that is, the bag of words assumption, hence the name). In the skip-gram architecture,
    the model predicts the surrounding words given the context word. According to
    the Word2Vec website, CBOW is faster, but skip-gram does a better job at predicting
    infrequent words.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在CBOW架构中，模型根据周围词汇的窗口预测当前词汇。上下文词汇的顺序不会影响预测（即词袋假设，因此得名）。在Skip-gram架构中，模型根据上下文词汇预测周围的词汇。根据Word2Vec网站，CBOW速度较快，但Skip-gram在预测不常见词汇时表现更好。
- en: '*Figure 4.2* summarizes the CBOW and skip-gram architectures. To understand
    the inputs and outputs, consider the following example sentence:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.2* 总结了CBOW和Skip-gram架构。为了理解输入和输出，考虑以下示例句子：'
- en: '*The Earth travels around the Sun once per year.*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*地球每年绕太阳公转一次。*'
- en: 'Assuming a window size of 5, that is, two context words to the left and right
    of the content word, the resulting context windows are shown as follows. The word
    in bold is the word under consideration, and the other words are the context words
    within the window:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 假设窗口大小为5，也就是内容词左右各有两个上下文词，得到的上下文窗口如下所示。加粗的词是正在考虑的词，其他词是窗口中的上下文词：
- en: '[_, _, **The**, Earth, travels]'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[_, _, **这**, 地球, 旅行]'
- en: '[_, The, **Earth**, travels, around]'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[_, 这, **地球**, 旅行, 围绕]'
- en: '[The, Earth, **travels**, around, the]'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[这, 地球, **旅行**, 围绕, 太阳]'
- en: '[Earth, travels, **around**, the, Sun]'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[地球, 旅行, **围绕**, 太阳]'
- en: '[travels, around, **the**, Sun, once]'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[旅行, 围绕, **这**, 太阳, 一次]'
- en: '[around, the, **Sun**, once, per]'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[围绕, 太阳, **这**, 一次, 每]'
- en: '[the, Sun, **once**, per, year]'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[太阳, **一次**, 每年]'
- en: '[Sun, **once**, per, year, _]'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[太阳, **一次**, 每年, _]'
- en: '[**once**, per, year, _, _]'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[**一次**, 每年, _, _]'
- en: 'For the CBOW model, the input and label tuples for the first three context
    windows are as follows. In the following first example, the CBOW model would learn
    to predict the word “The” given the set of words (“Earth,” “travels”), and so
    on. More correctly, the input of sparse vectors for the words “Earth” and “travels.”
    The model will learn to predict a dense vector whose highest value, or probability,
    corresponds to the word “The”:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CBOW模型，前三个上下文窗口的输入和标签元组如下所示。在第一个示例中，CBOW模型将学习在给定词组（“地球”，“旅行”）的情况下预测词“这”，依此类推。更准确地说，是“地球”和“旅行”这两个词的稀疏向量作为输入。模型将学习预测一个稠密向量，其最大值或概率对应于词“这”：
- en: ([Earth, travels], **The**)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ([地球, 旅行], **这**)
- en: ([The, travels, around], **Earth**)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ([这, 旅行, 围绕], **地球**)
- en: ([The, Earth, around, the], **travels**)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ([这, 地球, 围绕, 太阳], **旅行**)
- en: 'For the skip-gram model, the first three context windows correspond to the
    following input and label tuples. We can simplify the skip-gram model objective
    of predicting a context word given a target word to basically predicting if a
    pair of words are contextually related. Contextually related means that a pair
    of words within a context window are somehow related. That is, the input to the
    skip-gram model for the following first example would be the sparse vectors for
    the context words “The” and “Earth,” and the output would be the value 1:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于跳字模型，前三个上下文窗口对应以下输入和标签元组。我们可以简化跳字模型的目标：给定一个目标词，预测一个上下文词，基本上就是预测一对词是否在语境上相关。语境相关意味着一对词在上下文窗口中以某种方式相关。也就是说，跳字模型的输入是上下文词“这”和“地球”的稀疏向量，输出是值1：
- en: ([**The**, Earth], 1)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ([**这**, 地球], 1)
- en: ([**The**, travels], 1)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ([**这**, 旅行], 1)
- en: ([**Earth**, The], 1)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ([**地球**, 这], 1)
- en: ([**Earth**, travels], 1)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ([**地球**, 旅行], 1)
- en: ([**Earth**, around], 1)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ([**地球**, 围绕], 1)
- en: ([**travels**, The], 1)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ([**旅行**, 这], 1)
- en: ([**travels**, Earth], 1)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ([**旅行**, 地球], 1)
- en: ([**travels**, around], 1)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ([**旅行**, 围绕], 1)
- en: ([**travels**, the], 1)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ([**旅行**, 这], 1)
- en: 'We also need negative samples to train a model properly, so we generate additional
    negative samples by pairing each input word with some random word in the vocabulary.
    This process is called negative sampling and might result in the following additional
    inputs:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要负样本来正确训练模型，因此我们通过将每个输入词与词汇表中的某个随机词配对来生成额外的负样本。这一过程称为负采样，可能会产生以下额外的输入：
- en: ([**Earth**, aardvark], 0)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ([**地球**, 土豚], 0)
- en: ([**Earth**, zebra], 0)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ([**地球**, 斑马], 0)
- en: A model trained with all of these inputs is called a **Skip-Gram with Negative
    Sampling** (**SGNS**) model.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所有这些输入训练的模型称为**带有负采样的跳字模型**（**SGNS**）模型。
- en: It is important to understand that we are not interested in the ability of these
    models to classify; rather, we are interested in the side effect of training –
    the learned weights. These learned weights are what we call the embedding.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解，我们并不关心这些模型的分类能力；相反，我们关心的是训练的副作用——学习到的权重。这些学习到的权重就是我们所说的嵌入（embedding）。
- en: While it may be instructive to implement the models on your own as an academic
    exercise, at this point Word2Vec is so commoditized, you are unlikely to ever
    need to do this. For the curious, you will find code to implement the CBOW and
    skip-gram models in the files `tf2_cbow_model.py` and `tf2_cbow_skipgram.py` in
    the source code accompanying this chapter.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自己实现这些模型作为一种学术练习可能会很有启发，但此时Word2Vec已经变得如此商品化，你不太可能再需要自己动手实现。对于好奇者，你将在本章附带的源代码中的`tf2_cbow_model.py`和`tf2_cbow_skipgram.py`文件中找到实现CBOW和跳字模型的代码。
- en: The Word2Vec model was trained in a self-supervised manner by Google on roughly
    100 billion words from the Google News dataset and contains a vocabulary of 3
    million words. Google then released the pretrained model for anyone to download
    and use. The pretrained Word2Vec model is available here ([https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit)).
    The output vector dimensionality is 300\. It is available as a BIN file and can
    be opened using Gensim by using `gensim.models.Word2Vec.load_word2vec_format()`
    or using the `gensim()` data downloader.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec模型是由Google以自监督的方式训练的，使用了约1000亿个来自Google新闻数据集的单词，并包含了300万个单词的词汇表。然后，Google发布了预训练模型，供任何人下载和使用。预训练的Word2Vec模型可以在这里下载（[https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit)）。输出向量的维度是300。它以BIN文件格式提供，并且可以使用Gensim通过`gensim.models.Word2Vec.load_word2vec_format()`或使用`gensim()`数据下载器来打开。
- en: The other early implementation of word embedding is GloVe, which we will talk
    about next.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种早期的词嵌入实现是GloVe，我们接下来将讨论它。
- en: GloVe
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GloVe
- en: The **Global vectors for word representation** (**GloVe**) embeddings were created
    by *Jeffrey Pennington*, *Richard Socher*, and *Christopher Manning* [4]. The
    authors describe GloVe as an unsupervised learning algorithm for obtaining vector
    representations for words. Training is performed on aggregated global word-word
    co-occurrence statistics from a corpus, and the resulting representations show
    similar clustering behavior between similar words as seen in Word2Vec.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**全局词向量表示**（**GloVe**）嵌入是由*Jeffrey Pennington*、*Richard Socher*和*Christopher
    Manning* [4] 创建的。作者将GloVe描述为一种无监督学习算法，用于获取词的向量表示。训练是在从语料库中聚合的全局词-词共现统计数据上进行的，结果的表示显示了类似词之间的聚类行为，类似于Word2Vec。'
- en: GloVe differs from Word2Vec in that Word2Vec is a predictive model while GloVe
    is a count-based model. The first step is to construct a large matrix of (word,
    context) pairs that co-occur in the training corpus. Rows correspond to words
    and columns correspond to contexts, usually a sequence of one or more words. Each
    element of the matrix represents how often the word co-occurs in the context.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe与Word2Vec的区别在于，Word2Vec是一个预测模型，而GloVe是一个基于计数的模型。第一步是构建一个大的（词，上下文）对的矩阵，这些对在训练语料中共现。行对应于单词，列对应于上下文，通常是一个或多个单词的序列。矩阵中的每个元素表示单词在上下文中共现的频率。
- en: 'The GloVe process factorizes this co-occurrence matrix into a pair of (word,
    feature) and (feature, context) matrices. The process is known as matrix factorization
    and is done using **Stochastic Gradient Descent** (**SGD**), an iterative numerical
    method. For example, consider that we want to factorize a matrix *R* into its
    factors *P* and *Q*:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe过程将这个共现矩阵分解为一对（词，特征）和（特征，上下文）矩阵。这个过程被称为矩阵分解，使用**随机梯度下降**（**SGD**）这一迭代数值方法进行。例如，假设我们要将矩阵*R*分解为其因子*P*和*Q*：
- en: '![](img/B18331_04_004.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_04_004.png)'
- en: The SGD process will start with *P* and *Q* composed of random values and attempt
    to reconstruct the matrix *R’* by multiplying them. The difference between the
    matrices *R* and *R’* represents the loss and is usually computed as the mean-squared
    error between the two matrices. The loss dictates how much the values of *P* and
    *Q* need to change for *R’* to move closer to *R* to minimize the reconstruction
    loss. This process is repeated multiple times until the loss is within some acceptable
    threshold. At that point, the (word, feature) matrix *P* is the GloVe embedding.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: SGD过程将从包含随机值的*P*和*Q*开始，并尝试通过将它们相乘来重建矩阵*R’*。矩阵*R*和*R’*之间的差异表示损失，通常通过计算两个矩阵之间的均方误差来得到。损失决定了*P*和*Q*的值需要改变多少，以便使*R’*更接近*R*，从而最小化重建损失。这个过程会重复多次，直到损失在可接受的阈值内为止。此时，（词，特征）矩阵*P*就是GloVe嵌入。
- en: The GloVe process is much more resource-intensive than Word2Vec. This is because
    Word2Vec learns the embedding by training over batches of word vectors, while
    GloVe factorizes the entire co-occurrence matrix in one shot. In order to make
    the process scalable, SGD is often used in parallel mode, as outlined in the HOGWILD!
    paper [5].
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe过程比Word2Vec更具资源消耗性。这是因为Word2Vec通过在词向量批次上训练来学习嵌入，而GloVe则是一次性对整个共现矩阵进行分解。为了使这个过程具有可扩展性，通常会采用SGD并行模式，如HOGWILD!论文中所述
    [5]。
- en: Levy and Goldberg have also pointed out equivalences between the Word2Vec and
    GloVe approaches in their paper [6], showing that the Word2Vec SGNS model implicitly
    factorizes a word-context matrix.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Levy 和 Goldberg 在他们的论文 [6] 中也指出了 Word2Vec 和 GloVe 方法之间的等价性，表明 Word2Vec SGNS
    模型隐式地因子分解了一个词-上下文矩阵。
- en: As with Word2Vec, you are unlikely to ever need to generate your own GloVe embedding,
    and far more likely to use embeddings pre-generated against large corpora and
    made available for download. If you are curious, you will find code to implement
    matrix factorization in `tf2_matrix_factorization.py` in the source code download
    accompanying this chapter.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Word2Vec 类似，你不太可能需要自己生成 GloVe 嵌入，更有可能使用预先生成的针对大型语料库的嵌入并提供下载。如果你感兴趣，你可以在附带本章节源代码下载的地方找到实现矩阵因子分解的代码
    `tf2_matrix_factorization.py`。
- en: GloVe vectors trained on various large corpora (number of tokens ranging from
    6 billion to 840 billion, vocabulary size from 400 thousand to 2.2 million) and
    of various dimensions (50, 100, 200, 300) are available from the GloVe project
    download page ([https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)).
    It can be downloaded directly from the site or using Gensim or spaCy data downloaders.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种大型语料库（标记数从 60 亿到 840 亿，词汇量从 40 万到 220 万）上训练的 GloVe 向量以及各种维度（50、100、200、300）都可以从
    GloVe 项目下载页面 ([https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/))
    获得。它可以直接从该网站下载，或者使用 Gensim 或 spaCy 数据下载器下载。
- en: Creating your own embeddings using Gensim
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Gensim 创建您自己的嵌入
- en: We will create an embedding using Gensim and a small text corpus, called text8.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Gensim 和一个名为 text8 的小型文本语料库创建一个嵌入。
- en: Gensim is an open-source Python library designed to extract semantic meaning
    from text documents. One of its features is an excellent implementation of the
    Word2Vec algorithm, with an easy-to-use API that allows you to train and query
    your own Word2Vec model. To learn more about Gensim, see [https://radimrehurek.com/gensim/index.xhtml](https://radimrehurek.com/gensim/index.xhtml).
    To install Gensim, please follow the instructions at [https://radimrehurek.com/gensim/install.xhtml](https://radimrehurek.com/gensim/install.xhtml).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim 是一个开源的 Python 库，旨在从文本文档中提取语义意义。其特点之一是优秀的 Word2Vec 算法实现，具有易于使用的 API，允许您训练和查询自己的
    Word2Vec 模型。要了解更多关于 Gensim 的信息，请参阅 [https://radimrehurek.com/gensim/index.xhtml](https://radimrehurek.com/gensim/index.xhtml)。要安装
    Gensim，请按照 [https://radimrehurek.com/gensim/install.xhtml](https://radimrehurek.com/gensim/install.xhtml)
    上的说明进行操作。
- en: 'The text8 dataset is the first 10⁸ bytes of the Large Text Compression Benchmark,
    which consists of the first 10⁹ bytes of English Wikipedia [7]. The text8 dataset
    is accessible from within the Gensim API as an iterable of tokens, essentially
    a list of tokenized sentences. To download the text8 corpus, create a Word2Vec
    model from it, and save it for later use, run the following few lines of code
    (available in `create_embedding_with_text8.py` in the source code for this chapter):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: text8 数据集是大型文本压缩基准的前 10⁸ 字节，其中包括英文维基百科的前 10⁹ 字节 [7]。text8 数据集可以作为 Gensim API
    中的一个可迭代的 token 集合访问，基本上是一个标记化句子的列表。要下载 text8 语料库，创建一个 Word2Vec 模型并保存以供以后使用，请运行以下几行代码（在本章节的源代码中的
    `create_embedding_with_text8.py` 中可用）：
- en: '[PRE0]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will train a Word2Vec model on the text8 dataset and save it as a binary
    file. The Word2Vec model has many parameters, but we will just use the defaults.
    In this case, it trains a CBOW model (`sg=0`) with window size 5 (`window=5`)
    and will produce 100 dimensional embeddings (`size=100`). The full set of parameters
    is described on the Word2Vec documentation page [8]. To run this code, execute
    the following commands at the command line:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在 text8 数据集上训练一个 Word2Vec 模型并将其保存为二进制文件。Word2Vec 模型有许多参数，但我们将使用默认值。在这种情况下，它使用
    CBOW 模型 (`sg=0`)，窗口大小为 5 (`window=5`)，并生成 100 维的嵌入 (`size=100`)。详细的参数设置请参阅 Word2Vec
    文档页面 [8]。要运行此代码，请在命令行中执行以下命令：
- en: '[PRE1]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The code should run for 5-10 minutes, after which it will write out a trained
    model into the `data` folder. We will examine this trained model in the next section.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 代码应该运行 5-10 分钟，之后将在 `data` 文件夹中写入一个训练好的模型。我们将在下一节中检查这个训练好的模型。
- en: Word embeddings are central to text processing; however, at the time of writing
    this book, there is no comparable API within TensorFlow that allows you to work
    with embeddings at the same level of abstraction. For this reason, we have used
    Gensim in this chapter to work with Word2Vec models. The online Tensorflow tutorial
    contains an example of how to train a Word2Vec model from scratch ([https://www.tensorflow.org/tutorials/text/word2vec](https://www.tensorflow.org/tutorials/text/word2vec))
    but that is not our focus here.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量在文本处理中的作用至关重要；然而，在本书写作时，TensorFlow中并没有类似的API允许你以相同的抽象层次处理嵌入。因此，在本章中我们使用了Gensim来处理Word2Vec模型。在线TensorFlow教程包含了如何从头开始训练Word2Vec模型的示例（[https://www.tensorflow.org/tutorials/text/word2vec](https://www.tensorflow.org/tutorials/text/word2vec)），但这不是我们关注的重点。
- en: Exploring the embedding space with Gensim
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Gensim探索嵌入空间
- en: 'Let us reload the Word2Vec model we just built and explore it using the Gensim
    API. The actual word vectors can be accessed as a custom Gensim class from the
    model’s `wv` attribute:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新加载刚刚构建的Word2Vec模型，并使用Gensim API进行探索。实际的词向量可以通过模型的`wv`属性作为自定义的Gensim类进行访问：
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can take a look at the first few words in the vocabulary and check to see
    if specific words are available:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看词汇表中的前几个词，并检查是否可以找到特定的词：
- en: '[PRE3]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding snippet of code produces the following output:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码片段产生了以下输出：
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can look for similar words to a given word (“king”), shown as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查找与给定词（“king”）相似的词，如下所示：
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `most_similar()` method with a single parameter produces the following
    output. Here, the floating-point score is a measure of the similarity, higher
    values being better than lower values. As you can see, the similar words seem
    to be mostly accurate:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单个参数的`most_similar()`方法产生了以下输出。在这里，浮点数评分是相似度的衡量标准，较高的值优于较低的值。正如你所看到的，相似词汇看起来大多是准确的：
- en: '[PRE6]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can also do vector arithmetic similar to the country-capital example we
    described earlier. Our objective is to see if the relation Paris : France :: Berlin
    : Germany holds true. This is equivalent to saying that the distance in embedding
    space between Paris and France should be the same as that between Berlin and Germany.
    In other words, France - Paris + Berlin should give us Germany. In code, then,
    this would translate to:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以像我们之前描述的国家-首都示例一样进行向量运算。我们的目标是验证“巴黎：法国”::“柏林：德国”是否成立。这等同于说巴黎和法国之间的嵌入空间距离应该与柏林和德国之间的距离相同。换句话说，法国
    - 巴黎 + 柏林应该给出德国。在代码中，这将转化为：
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This returns the following result, as expected:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下结果，正如预期的那样：
- en: '[PRE8]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The preceding similarity value reported is cosine similarity, but a better measure
    of similarity was proposed by *Levy* and *Goldberg* [9], which is also implemented
    in the Gensim API. This measure essentially computes the distance on a log scale
    thereby amplifying the difference between shorter distances and reducing the difference
    between longer ones.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 前面报告的相似度值是余弦相似度，但*Levy*和*Goldberg* [9]提出了一种更好的相似度度量方法，该方法也在Gensim API中实现。这个度量方法本质上是计算对数尺度上的距离，从而放大较短距离之间的差异，减小较长距离之间的差异。
- en: '[PRE9]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And this also yields the expected result, but with higher similarity:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这也得出了预期的结果，但相似度更高：
- en: '[PRE10]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Gensim also provides a `doesnt_match()` function, which can be used to detect
    the odd one out of a list of words:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim还提供了一个`doesnt_match()`函数，可以用来从一组词中检测出不同的那个词：
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This gives us `singapore` as expected, since it is the only country among a
    set of words identifying religions.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了`singapore`，正如预期的那样，因为它是识别宗教的一组词中唯一的一个国家。
- en: 'We can also calculate the similarity between two words. Here we demonstrate
    that the distance between related words is less than that of unrelated words:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以计算两个词之间的相似度。在这里，我们演示了相关词之间的距离小于不相关词之间的距离：
- en: '[PRE12]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This gives the following interesting result:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下有趣的结果：
- en: '[PRE13]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `similar_by_word()` function is functionally equivalent to `similar()`
    except that the latter normalizes the vector before comparing by default. There
    is also a related `similar_by_vector()` function, which allows you to find similar
    words by specifying a vector as input. Here we try to find words that are similar
    to “singapore”:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`similar_by_word()`函数在功能上与`similar()`等价，唯一的区别是后者默认在比较之前会对向量进行归一化处理。还有一个相关的`similar_by_vector()`函数，它允许你通过指定一个向量作为输入来查找相似的词汇。这里我们尝试查找与“singapore”相似的词：'
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'And we get the following output, which seems to be mostly correct, at least
    from a geographical point of view:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下输出，从地理角度来看，似乎大部分是正确的：
- en: '[PRE15]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can also compute the distance between two words in the embedding space using
    the `distance()` function. This is really just `1 - similarity()`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`distance()`函数计算嵌入空间中两个单词之间的距离。这个实际上就是`1 - similarity()`：
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can also look up vectors for a vocabulary word either directly from the
    `word_vectors` object, or by using the `word_vec()` wrapper, shown as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以直接从`word_vectors`对象中查找词汇表单词的向量，或者使用以下所示的`word_vec()`包装器来查找：
- en: '[PRE17]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: There are a few other functions that you may find useful depending on your use
    case. The documentation page for KeyedVectors contains a list of all the available
    functions [10].
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的使用场景，可能还会有其他一些函数你会觉得有用。KeyedVectors的文档页面列出了所有可用的函数[10]。
- en: The code shown here can be found in the `explore_text8_embedding.py` file in
    the code accompanying this book.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这里显示的代码可以在本书随附的代码中的`explore_text8_embedding.py`文件中找到。
- en: Using word embeddings for spam detection
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用词嵌入进行垃圾邮件检测
- en: Because of the widespread availability of various robust embeddings generated
    from large corpora, it has become quite common to use one of these embeddings
    to convert text input for use with machine learning models. Text is treated as
    a sequence of tokens. The embedding provides a dense fixed dimension vector for
    each token. Each token is replaced with its vector, and this converts the sequence
    of text into a matrix of examples, each of which has a fixed number of features
    corresponding to the dimensionality of the embedding.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由于各种强大嵌入的广泛可用性，这些嵌入是从大型语料库中生成的，因此使用这些嵌入之一将文本输入转化为机器学习模型的输入变得非常普遍。文本被视为一个令牌序列。嵌入为每个令牌提供一个密集的固定维度向量。每个令牌都会被其向量替换，这样就将文本序列转换为一个示例矩阵，每个示例都有一个固定数量的特征，对应于嵌入的维度。
- en: This matrix of examples can be used directly as input to standard (non-neural
    network based) machine learning programs, but since this book is about deep learning
    and TensorFlow, we will demonstrate its use with a one-dimensional version of
    the **Convolutional Neural Network** (**CNN**) that you learned about in *Chapter
    3*, *Convolutional Neural Networks*. Our example is a spam detector that will
    classify **Short Message Service** (**SMS**) or text messages as either “ham”
    or “spam.” The example is very similar to a sentiment analysis example we’ll cover
    in *Chapter 20*, *Advanced Convolutional Neural Networks*, that uses a one-dimensional
    CNN, but our focus here will be on the embedding layer.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例矩阵可以直接作为标准（非神经网络基于）机器学习程序的输入，但由于本书是关于深度学习和TensorFlow的，我们将展示如何使用你在*第3章*《卷积神经网络》中学到的**卷积神经网络**（**CNN**）的一个一维版本。我们的示例是一个垃圾邮件检测器，它将**短消息服务**（**SMS**）或文本消息分类为“ham”或“spam”。这个示例与我们将在*第20章*《高级卷积神经网络》中介绍的情感分析示例非常相似，该示例也使用一维CNN，但我们这里的重点将放在嵌入层上。
- en: Specifically, we will see how the program learns an embedding from scratch that
    is customized to the spam detection task. Next, we will see how to use an external
    third-party embedding like the ones we have learned about in this chapter, a process
    similar to transfer learning in computer vision. Finally, we will learn how to
    combine the two approaches, starting with a third-party embedding and letting
    the network use that as a starting point for its custom embedding, a process similar
    to fine-tuning in computer vision.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将看到程序如何从头开始学习一个嵌入，该嵌入针对垃圾邮件检测任务进行了定制。接下来，我们将看到如何使用像本章中我们学习的外部第三方嵌入，这一过程类似于计算机视觉中的迁移学习。最后，我们将学习如何将这两种方法结合起来，从第三方嵌入开始，让网络将其作为自定义嵌入的起点，这一过程类似于计算机视觉中的微调。
- en: 'As usual, we will start with our imports:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们将从导入开始：
- en: '[PRE18]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Scikit-learn is an open-source Python machine learning toolkit that contains
    many efficient and easy-to-use tools for data mining and data analysis. In this
    chapter, we have used two of its predefined metrics, `accuracy_score` and `confusion_matrix`,
    to evaluate our model after it is trained.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn是一个开源的Python机器学习工具包，包含许多高效且易于使用的数据挖掘和数据分析工具。在本章中，我们使用了其中的两个预定义度量`accuracy_score`和`confusion_matrix`，来评估模型训练后的表现。
- en: You can learn more about scikit-learn at [https://scikit-learn.org/stable/](https://scikit-learn.org/stable/).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://scikit-learn.org/stable/](https://scikit-learn.org/stable/)了解更多关于scikit-learn的信息。
- en: Getting the data
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取数据
- en: 'The data for our model is available publicly and comes from the SMS spam collection
    dataset from the UCI Machine Learning Repository [11]. The following code will
    download the file and parse it to produce a list of SMS messages and their corresponding
    labels:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的数据是公开的，来自UCI机器学习库中的SMS垃圾短信数据集[11]。以下代码将下载该文件并解析它，生成SMS消息及其相应标签的列表：
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The dataset contains 5,574 SMS records, 747 of which are marked as “spam” and
    the other 4,827 are marked as “ham” (not spam). The text of the SMS records is
    contained in the variable `texts`, and the corresponding numeric labels (0 = ham,
    1 = spam) are contained in the variable labels.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含5,574条SMS记录，其中747条标记为“垃圾短信”（spam），其余4,827条标记为“正常短信”（ham）。SMS记录的文本保存在变量`texts`中，相应的数字标签（0
    = 正常短信，1 = 垃圾短信）保存在变量`labels`中。
- en: Making the data ready for use
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据以供使用
- en: The next step is to process the data so it can be consumed by the network. The
    SMS text needs to be fed into the network as a sequence of integers, where each
    word is represented by its corresponding ID in the vocabulary. We will use the
    Keras tokenizer to convert each SMS text into a sequence of words, and then create
    the vocabulary using the `fit_on_texts()` method on the tokenizer.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是处理数据，使其可以被网络使用。SMS文本需要作为整数序列输入网络，其中每个单词由其在词汇表中的相应ID表示。我们将使用Keras的分词器将每条SMS文本转换为单词序列，然后使用`fit_on_texts()`方法在分词器上创建词汇表。
- en: We then convert the SMS messages to a sequence of integers using `texts_to_sequences()`.
    Finally, since the network can only work with fixed-length sequences of integers,
    we call the `pad_sequences()` function to pad the shorter SMS messages with zeros.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`texts_to_sequences()`将SMS消息转换为整数序列。最后，由于网络只能处理固定长度的整数序列，我们调用`pad_sequences()`函数，用零填充较短的SMS消息。
- en: 'The longest SMS message in our dataset has 189 tokens (words). In many applications
    where there may be a few outlier sequences that are very long, we would restrict
    the length to a smaller number by setting the `maxlen` flag. In that case, sentences
    longer than `maxlen` tokens would be truncated, and sentences shorter than `maxlen`
    tokens would be padded:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中最长的SMS消息有189个标记（单词）。在许多应用中，可能会有一些极长的离群序列，我们可以通过设置`maxlen`标志来限制长度为较小的数字。这样，超过`maxlen`个标记的句子将被截断，少于`maxlen`个标记的句子将被填充：
- en: '[PRE20]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We will also convert our labels to categorical or one-hot encoding format,
    because the loss function we would like to choose (categorical cross-entropy)
    expects to see the labels in that format:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将把标签转换为分类格式或独热编码格式，因为我们希望选择的损失函数（分类交叉熵）要求标签采用这种格式：
- en: '[PRE21]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The tokenizer allows access to the vocabulary created through the `word_index`
    attribute, which is basically a dictionary of vocabulary words to their index
    positions in the vocabulary. We also build the reverse index that enables us to
    go from index position to the word itself. In addition, we create entries for
    the `PAD` character:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器允许访问通过`word_index`属性创建的词汇表，该属性基本上是一个词汇单词及其在词汇表中索引位置的字典。我们还构建了反向索引，使我们能够从索引位置找到相应的单词。此外，我们为`PAD`字符创建了条目：
- en: '[PRE22]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, we create the `dataset` object that our network will work with. The
    `dataset` object allows us to set up some properties, such as the batch size,
    declaratively. Here, we build up a dataset from our padded sequence of integers
    and categorical labels, shuffle the data, and split it into training, validation,
    and test sets. Finally, we set the batch size for each of the three datasets:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建了网络将使用的`dataset`对象。`dataset`对象允许我们声明性地设置一些属性，比如批处理大小。在这里，我们从填充后的整数序列和分类标签中构建数据集，打乱数据，并将其拆分为训练集、验证集和测试集。最后，我们为这三个数据集设置了批处理大小：
- en: '[PRE23]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Building the embedding matrix
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建嵌入矩阵
- en: 'The Gensim toolkit provides access to various trained embedding models, as
    you can see from running the following command at the Python prompt:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim工具包提供了对各种训练好的嵌入模型的访问，您可以通过在Python提示符下运行以下命令来查看：
- en: '[PRE24]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This will return (at the time of writing this book) the following trained word
    embeddings:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回（在本书撰写时）以下训练好的词嵌入：
- en: '**Word2Vec**: Two flavors, one trained on Google news (3 million word vectors
    based on 3 billion tokens), and one trained on Russian corpora (word2vec-ruscorpora-300,
    word2vec-google-news-300).'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Word2Vec**：有两种版本，一个是基于Google新闻训练的（包含300万个词向量，基于30亿个标记），另一个是基于俄语语料库训练的（word2vec-ruscorpora-300，word2vec-google-news-300）。'
- en: '**GloVe**: Two flavors, one trained on the Gigawords corpus (400,000 word vectors
    based on 6 billion tokens), available as 50d, 100d, 200d, and 300d vectors, and
    one trained on Twitter (1.2 million word vectors based on 27 billion tokens),
    available as 25d, 50d, 100d, and 200d vectors (glove-wiki-gigaword-50, glove-wiki-gigaword-100,
    glove-wiki-gigaword-200, glove-wiki-gigaword-300, glove-twitter-25, glove-twitter-50,
    glove-twitter-100, glove-twitter-200). Smaller embedding sizes would result in
    greater compression of the input and consequently a greater degree of approximation.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GloVe**：有两种版本，一种在Gigawords语料库上训练（基于60亿标记的40万个词向量），提供50d、100d、200d和300d向量，另一种在Twitter上训练（基于270亿标记的120万个词向量），提供25d、50d、100d和200d向量（glove-wiki-gigaword-50，glove-wiki-gigaword-100，glove-wiki-gigaword-200，glove-wiki-gigaword-300，glove-twitter-25，glove-twitter-50，glove-twitter-100，glove-twitter-200）。较小的嵌入尺寸会导致输入的更大压缩，从而产生更大的近似度。'
- en: '**fastText**: One million word vectors trained with subword information on
    Wikipedia 2017, the UMBC web corpus, and statmt.org news dataset (16B tokens)
    (fastText-wiki-news-subwords-300).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**fastText**：使用子词信息在2017年维基百科、UMBC网络语料库和statmt.org新闻数据集（16B个标记）上训练的一百万个词向量（fastText-wiki-news-subwords-300）。'
- en: '**ConceptNet Numberbatch**: An ensemble embedding that uses the ConceptNet
    semantic network, the **paraphrase database** (**PPDB**), Word2Vec, and GloVe
    as input. Produces 600d vectors [12, 13].'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ConceptNet Numberbatch**：一种集成嵌入，使用ConceptNet语义网络、**释义数据库**（**PPDB**）、Word2Vec和GloVe作为输入。生成600d向量[12,
    13]。'
- en: For our example, we chose the 300d GloVe embeddings trained on the Gigaword
    corpus.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，我们选择了基于Gigaword语料库训练的300d GloVe嵌入。
- en: 'In order to keep our model size small, we want to only consider embeddings
    for words that exist in our vocabulary. This is done using the following code,
    which creates a smaller embedding matrix for each word in the vocabulary. Each
    row in the matrix corresponds to a word, and the row itself is the vector corresponding
    to the embedding for the word:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持我们的模型小，我们只考虑词汇表中存在的词的嵌入。这是通过以下代码完成的，该代码为词汇表中的每个词创建一个较小的嵌入矩阵。矩阵中的每一行对应一个词，行本身就是对应该词的嵌入向量：
- en: '[PRE25]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The output shape for the embedding matrix is (9010, 300), corresponding to the
    9,010 tokens in the vocabulary, and 300 features in the third-party GloVe embeddings.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入矩阵的输出形状是（9010，300），对应词汇表中的9,010个标记，以及第三方GloVe嵌入中的300个特征。
- en: Defining the spam classifier
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义垃圾邮件分类器
- en: We are now ready to define our classifier. We will use a **one-dimensional Convolutional
    Neural Network or ConvNet** (**1D CNN**), similar to the network you have seen
    already in *Chapter 3*, *Convolutional Neural Networks*.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以定义我们的分类器了。我们将使用**一维卷积神经网络或ConvNet**（**1D CNN**），这与您在*第3章*、*卷积神经网络*中已经看到的网络类似。
- en: The input is a sequence of integers. The first layer is an embedding layer,
    which converts each input integer to a vector of size (`embedding_dim`). Depending
    on the run mode, that is, whether we will learn the embeddings from scratch, do
    transfer learning, or do fine-tuning, the embedding layer in the network would
    be slightly different. When the network starts with randomly initialized embedding
    weights (`run_mode == "scratch"`) and learns the weights during the training,
    we set the `trainable` parameter to `True`. In the transfer learning case (`run_mode
    == "vectorizer"`), we set the weights from our embedding matrix `E` but set the
    `trainable` parameter to `False`, so it doesn’t train. In the fine-tuning case
    (`run_mode == "finetuning"`), we set the embedding weights from our external matrix
    `E`, as well as setting the layer to trainable.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是一个整数序列。第一层是一个嵌入层，将每个输入整数转换为大小为（`embedding_dim`）的向量。根据运行模式（即是否从零开始学习嵌入、进行迁移学习或微调），网络中的嵌入层会略有不同。当网络以随机初始化的嵌入权重（`run_mode
    == "scratch"`）开始并在训练过程中学习权重时，我们将`trainable`参数设置为`True`。在迁移学习的情况下（`run_mode ==
    "vectorizer"`），我们从嵌入矩阵`E`中设置权重，但将`trainable`参数设置为`False`，这样它就不会训练。在微调的情况下（`run_mode
    == "finetuning"`），我们从外部矩阵`E`设置嵌入权重，并将该层设置为可训练。
- en: The output of the embedding is fed into a convolutional layer. Here, fixed-size
    3-token-wide 1D windows (`kernel_size=3`), also called time steps, are convolved
    against 256 random filters (`num_filters=256`) to produce vectors of size 256
    for each time step. Thus, the output vector shape is (`batch_size`, `time_steps`,
    `num_filters`).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层的输出被输入到一个卷积层中。这里，固定大小为3个标记宽度的1D窗口（`kernel_size=3`），也称为时间步，被卷积运算与256个随机滤波器（`num_filters=256`）进行运算，从而为每个时间步生成大小为256的向量。因此，输出向量的形状是（`batch_size`，`time_steps`，`num_filters`）。
- en: The output of the convolutional layer is sent to a 1D spatial dropout layer.
    Spatial dropout will randomly drop entire feature maps output from the convolutional
    layer. This is a regularization technique to prevent over-fitting. This is then
    sent through a global max pool layer, which takes the maximum value from each
    time step for each filter, resulting in a vector of shape (`batch_size`, `num_filters`).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层的输出被送入一个1D空间丢弃层。空间丢弃将随机丢弃卷积层输出的整个特征图。这是一种正则化技术，用于防止过拟合。然后，它会经过一个全局最大池化层，该层从每个时间步的每个滤波器中提取最大值，生成形状为（`batch_size`，`num_filters`）的向量。
- en: 'The output of the dropout layer is fed into a pooling layer to flatten it,
    and then into a dense layer, which converts the vector of shape (`batch_size`,
    `num_filters`) to (`batch_size`, `num_classes`). A softmax activation will convert
    the scores for each of (spam, ham) into a probability distribution, indicating
    the probability of the input SMS being spam or ham respectively:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 丢弃层的输出被输入到池化层进行扁平化，然后进入一个全连接层，该层将形状为（`batch_size`，`num_filters`）的向量转换为（`batch_size`，`num_classes`）。Softmax激活函数将（垃圾短信、正常短信）的每个分数转换为概率分布，表示输入的短信是垃圾短信或正常短信的概率：
- en: '[PRE26]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Finally, we compile the model using the categorical cross entropy loss function
    and the Adam optimizer:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用分类交叉熵损失函数和Adam优化器来编译模型：
- en: '[PRE27]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Training and evaluating the model
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和评估模型
- en: One thing to notice is that the dataset is somewhat imbalanced; there are only
    747 instances of spam compared to 4,827 instances of ham. The network could achieve
    close to 87% accuracy simply by always predicting the majority class. To alleviate
    this problem, we set class weights to indicate that an error on a spam SMS is
    eight times as expensive as an error on a ham SMS. This is indicated by the `CLASS_WEIGHTS`
    variable, which is passed into the `model.fit()` call as an additional parameter.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，数据集在某种程度上是不平衡的；垃圾短信只有747个实例，而正常短信有4,827个实例。网络仅通过总是预测多数类即可实现接近87%的准确率。为了解决这个问题，我们设置了类别权重，表示垃圾短信的错误代价是正常短信错误的八倍。这由`CLASS_WEIGHTS`变量表示，并作为额外参数传递给`model.fit()`调用。
- en: 'After training for 3 epochs, we evaluate the model against the test set, and
    report the accuracy and confusion matrix of the model against the test set. However,
    for imbalance data, even with the use of class weights, the model may end up learning
    to always predict the majority class. Therefore, it is generally advisable to
    report accuracy on a per-class basis to make sure that the model learns to distinguish
    each class effectively. This can be done quite easily using the confusion matrix
    by dividing the diagonal element for each row by the sum of elements for that
    row, where each row corresponds to a labeled class:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 训练3个周期后，我们在测试集上评估模型，并报告模型在测试集上的准确率和混淆矩阵。然而，对于不平衡数据，即使使用了类别权重，模型也可能会学习到始终预测多数类。因此，通常建议按类别报告准确率，以确保模型能够有效地区分每个类别。这可以通过使用混淆矩阵来轻松完成，方法是将每行的对角元素除以该行所有元素的和，其中每行对应一个标记类别：
- en: '[PRE28]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Running the spam detector
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行垃圾短信检测器
- en: 'The three scenarios we want to look at are:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要查看的三个场景是：
- en: Letting the network learn the embedding for the task.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让网络为任务学习嵌入。
- en: Starting with a fixed external third-party embedding where the embedding matrix
    is treated like a vectorizer to transform the sequence of integers into a sequence
    of vectors.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从固定的外部第三方嵌入开始，其中嵌入矩阵被视为一个向量化器，用于将整数序列转换为向量序列。
- en: Starting with an external third-party embedding which is further fine-tuned
    to the task during the training.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从外部第三方嵌入开始，在训练过程中进一步微调到任务中。
- en: 'Each scenario can be evaluated by setting the value of the `mode` argument
    as shown in the following command:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 每种场景可以通过设置`mode`参数的值来评估，具体如以下命令所示：
- en: '[PRE29]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The dataset is small, and the model is fairly simple. We were able to achieve
    very good results (validation set accuracies in the high 90s, and perfect test
    set accuracy) with only minimal training (3 epochs). In all three cases, the network
    achieved a perfect score, accurately predicting the 1,111 ham messages, as well
    as the 169 spam cases.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集较小，模型也比较简单。我们通过仅进行少量训练（3 轮），就能取得非常好的结果（验证集准确率接近 100%，测试集准确率完美）。在这三种情况下，网络都取得了完美的成绩，准确预测了
    1,111 条正常消息，以及 169 条垃圾邮件。
- en: 'The change in validation accuracies, shown in *Figure 4.3*, illustrates the
    differences between the three approaches:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4.3* 中显示的验证准确率变化，展示了三种方法之间的差异：'
- en: '![](img/B18331_04_03.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_04_03.png)'
- en: 'Figure 4.3: Comparison of validation accuracy across training epochs for different
    embedding techniques'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：不同嵌入技术在训练周期中的验证准确率对比
- en: In the learning from scratch case, at the end of the first epoch, the validation
    accuracy is 0.93, but over the next two epochs, it rises to 0.98\. In the vectorizer
    case, the network gets something of a head start from the third-party embeddings
    and ends up with a validation accuracy of almost 0.95 at the end of the first
    epoch. However, because the embedding weights are not allowed to change, it is
    not able to customize the embeddings to the spam detection task, and the validation
    accuracy at the end of the third epoch is the lowest among the three. The fine-tuning
    case, like the vectorizer, also gets a head start, but can customize the embedding
    to the task as well, and therefore is able to learn at the most rapid rate among
    the three cases. The fine-tuning case has the highest validation accuracy at the
    end of the first epoch and reaches the same validation accuracy at the end of
    the second epoch that the scratch case achieves at the end of the third.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在从零开始学习的情况下，第一轮结束时，验证准确率为 0.93，但在接下来的两个轮次中，它上升到 0.98。在向量化器的情况下，网络从第三方嵌入中获得了一定的起步优势，并在第一轮结束时达到了接近
    0.95 的验证准确率。然而，由于嵌入权重不允许改变，因此无法将嵌入自定义为垃圾邮件检测任务，第三轮结束时的验证准确率是三者中最低的。微调的情况和向量化器一样，也获得了起步优势，但能够根据任务定制嵌入，因此能够以三者中最快的速度进行学习。微调的情况在第一轮结束时具有最高的验证准确率，并且在第二轮结束时达到了从零开始学习的情况在第三轮结束时所达到的相同验证准确率。
- en: In the next section, we will see that distributional similarity is not restricted
    to word embeddings; it applies to other scenarios as well.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将看到分布式相似性不仅限于单词嵌入，它也适用于其他场景。
- en: Neural embeddings – not just for words
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经嵌入 – 不仅仅是针对单词
- en: Word embedding technology has evolved in various ways since Word2Vec and GloVe.
    One such direction is the application of word embeddings to non-word settings,
    also known as neural embeddings. As you will recall, word embeddings leverage
    the distributional hypothesis that words occurring in similar contexts tend to
    have similar meanings, where context is usually a fixed-size (in number of words)
    window around the target word.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 自 Word2Vec 和 GloVe 以来，词嵌入技术在多个方向上得到了发展。其中一个方向是将词嵌入应用于非单词场景，也就是我们所说的神经嵌入。正如你回忆的那样，词嵌入利用了分布假设，即在相似语境中出现的单词通常具有相似的含义，其中语境通常是围绕目标单词的固定大小（按单词数量计算）窗口。
- en: The idea of neural embeddings is very similar; that is, entities that occur
    in similar contexts tend to be strongly related to each other. The way in which
    these contexts are constructed is usually situation-dependent. We will describe
    two techniques here that are foundational and general enough to be applied easily
    to a variety of use cases.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 神经嵌入的理念非常相似；即在相似语境中出现的实体往往彼此密切相关。这些语境的构建方式通常取决于具体情况。我们将在这里描述两种技术，这些技术是基础且足够通用的，可以轻松应用于各种用例。
- en: Item2Vec
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Item2Vec
- en: The Item2Vec embedding model was originally proposed by Barkan and Koenigstein
    [14] for the collaborative filtering use case, that is, recommending items to
    users based on purchases by other users that have similar purchase histories to
    this user. It uses items in a web-store as the “words” and the itemset (the sequence
    of items purchased by a user over time) as the “sentence” from which the “word
    context” is derived.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Item2Vec 嵌入模型最初由 Barkan 和 Koenigstein [14] 提出，适用于协同过滤用例，即基于其他具有相似购买历史的用户购买的商品来向用户推荐商品。它将网上商店中的商品作为“单词”，将商品集（即用户随时间购买的商品序列）作为“句子”，从中提取“单词语境”。
- en: For example, consider the problem of recommending items to shoppers in a supermarket.
    Assume that our supermarket sells 5,000 items, so each item can be represented
    as a sparse one-hot encoded vector of size 5,000\. Each user is represented by
    their shopping cart, which is a sequence of such vectors. Applying a context window
    similar to the one we saw in the Word2Vec section, we can train a skip-gram model
    to predict likely item pairs. The learned embedding model maps the items to a
    dense low-dimensional space where similar items are close together, which can
    be used to make similar item recommendations.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个向超市购物者推荐商品的问题。假设我们的超市销售 5000 种商品，因此每个商品可以表示为一个大小为 5000 的稀疏独热编码向量。每个用户由他们的购物车表示，购物车是这样的一系列向量。应用类似我们在
    Word2Vec 部分看到的上下文窗口，我们可以训练一个 skip-gram 模型来预测可能的商品对。学习到的嵌入模型将商品映射到一个稠密的低维空间，在这个空间中，类似的商品会聚集在一起，这可以用于进行相似商品推荐。
- en: node2vec
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: node2vec
- en: The node2vec embedding model was proposed by Grover and Leskovec [15], as a
    scalable way to learn features for nodes in a graph. It learns an embedding of
    the structure of the graph by executing a large number of fixed-length random
    walks on the graph. The nodes are the “words” and the random walks are the “sentences”
    from which the “word context” is derived in node2vec.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: node2vec 嵌入模型由 Grover 和 Leskovec [15] 提出，作为一种可扩展的方式，用于学习图中节点的特征。它通过在图上执行大量固定长度的随机游走来学习图结构的嵌入。节点是“单词”，随机游走是从中派生出“单词上下文”的“句子”。
- en: The **Something2Vec** page [40] provides a comprehensive list of ways in which
    researchers have tried to apply the distributional hypothesis to entities other
    than words. Hopefully, this list will spark ideas for your own “Something2Vec”
    representation.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**Something2Vec** 页面 [40] 提供了一个全面的列表，列出了研究人员如何尝试将分布假设应用于除了单词以外的其他实体。希望这个列表能激发你为自己的“Something2Vec”表示法的创意。'
- en: To illustrate how easy it is to create your own neural embedding, we will generate
    a node2vec-like model or, more accurately, a predecessor graph-based embedding
    called DeepWalk, proposed by Perozzi, et al. [42] for papers presented at the
    NeurIPS conference from 1987-2015, by leveraging word co-occurrence relationships
    between them.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明创建你自己的神经嵌入是多么简单，我们将生成一个类似 node2vec 的模型，或者更准确地说，是一个前身基于图的嵌入，叫做 DeepWalk，提出者为
    Perozzi 等人 [42]，用于 1987-2015 年间在 NeurIPS 会议上发表的论文，通过利用它们之间的词共现关系。
- en: The dataset is a 11,463 × 5,812 matrix of word counts, where the rows represent
    words, and columns represent conference papers. We will use this to construct
    a graph of papers, where an edge between two papers represents a word that occurs
    in both of them. Both node2vec and DeepWalk assume that the graph is undirected
    and unweighted. Our graph is undirected, since a relationship between a pair of
    papers is bidirectional. However, our edges could have weights based on the number
    of word co-occurrences between the two documents. For our example, we will consider
    any number of co-occurrences above 0 to be a valid unweighted edge.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是一个 11,463 × 5,812 的词频矩阵，其中行表示单词，列表示会议论文。我们将使用此数据构建论文的图，其中两篇论文之间的边表示它们都出现的单词。node2vec
    和 DeepWalk 假设图是无向且无权的。我们的图是无向的，因为两篇论文之间的关系是双向的。然而，我们的边可以根据两篇文档之间的词共现次数来加权。对于我们的示例，我们将任何超过
    0 的共现次数视为有效的无权边。
- en: 'As usual, we will start by declaring our imports:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们将从声明我们的导入开始：
- en: '[PRE30]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The next step is to download the data from the UCI repository and convert it
    to a sparse term-document matrix, TD, then construct a document-document matrix
    E by multiplying the transpose of the term-document matrix by itself. Our graph
    is represented as an adjacency or edge matrix by the document-document matrix.
    Since each element represents a similarity between two documents, we will binarize
    the matrix `E` by setting any non-zero elements to 1:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是从 UCI 仓库下载数据，并将其转换为稀疏的词项-文档矩阵（TD），然后通过将词项-文档矩阵的转置与其自身相乘来构建文档-文档矩阵 E。我们的图是通过文档-文档矩阵表示为邻接矩阵或边矩阵。由于每个元素表示两个文档之间的相似度，我们将通过将任何非零元素设置为
    1 来二值化矩阵 `E`：
- en: '[PRE31]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Once we have our sparse binarized adjacency matrix, `E`, we can then generate
    random walks from each of the vertices. From each node, we construct 32 random
    walks of a maximum length of 40 nodes. The walks have a random restart probability
    of 0.15, which means that for any node, the particular random walk could end with
    a 15% probability. The following code will construct the random walks and write
    them out to a file given by `RANDOM_WALKS_FILE`. To give an idea of the input,
    we have provided a snapshot of the first 10 lines of this file, showing random
    walks starting from node 0:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有了稀疏二值化邻接矩阵`E`，我们就可以从每个顶点生成随机游走。从每个节点开始，我们构造了32个最大长度为40个节点的随机游走。每次游走有0.15的随机重启概率，这意味着对于任何节点，特定的随机游走有15%的概率会结束。以下代码将构造随机游走，并将它们写入由`RANDOM_WALKS_FILE`指定的文件。为了给出输入的示例，我们提供了该文件前10行的快照，展示了从节点0开始的随机游走：
- en: '[PRE32]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Note that this is a very slow process. A copy of the output is provided along
    with the source code for this chapter in case you prefer to skip the random walk
    generation process:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这是一个非常缓慢的过程。输出的副本随本章的源代码一起提供，以防你希望跳过随机游走生成过程：
- en: '[PRE33]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'A few lines from the `RANDOM_WALKS_FILE` are shown below. You could imagine
    that these look like sentences in a language where the vocabulary of words is
    all the node IDs in our graph. We have learned that word embeddings exploit the
    structure of language to generate a distributional representation for words. Graph
    embedding schemes such as DeepWalk and node2vec do the exact same thing with these
    “sentences” created out of random walks. Such embeddings can capture similarities
    between nodes in a graph that go beyond immediate neighbors, as we shall see:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`RANDOM_WALKS_FILE`中的几行内容。你可以想象，这些看起来像是一种语言中的句子，其中词汇表是我们图中的所有节点ID。我们已经了解到，词嵌入利用语言的结构来生成词的分布式表示。像DeepWalk和node2vec这样的图嵌入方案也做了相同的事情，它们利用从随机游走中生成的“句子”。这些嵌入可以捕捉图中节点之间的相似性，超越了直接邻居的关系，正如我们将看到的那样：
- en: '[PRE34]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We are now ready to create our word embedding model. The Gensim package offers
    a simple API that allows us to declaratively create and train a Word2Vec model,
    using the following code. The trained model will be serialized to the file given
    by `W2V_MODEL_FILE`. The `Documents` class allows us to stream large input files
    to train the Word2Vec model without running into memory issues. We will train
    the Word2Vec model in skip-gram mode with a window size of 10, which means we
    train it to predict up to five neighboring vertices given a central vertex. The
    resulting embedding for each vertex is a dense vector of size 128:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备创建我们的词嵌入模型。Gensim包提供了一个简单的API，允许我们声明性地创建和训练一个Word2Vec模型，使用以下代码。训练后的模型将被序列化到由`W2V_MODEL_FILE`指定的文件中。`Documents`类允许我们流式传输大输入文件，以便训练Word2Vec模型而不会遇到内存问题。我们将在skip-gram模式下训练Word2Vec模型，窗口大小为10，这意味着我们训练它以预测给定一个中心节点时，最多预测五个邻近节点。每个节点的结果嵌入是一个大小为128的稠密向量：
- en: '[PRE35]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Our resulting DeepWalk model is just a Word2Vec model, so anything you can
    do with Word2Vec in the context of words, you can do with this model in the context
    of vertices. Let us use the model to discover similarities between documents:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的DeepWalk模型实际上就是一个Word2Vec模型，因此在处理单词的上下文中，您可以用Word2Vec做的任何事情，也可以在顶点的上下文中使用这个模型来做。让我们使用这个模型来发现文档之间的相似性：
- en: '[PRE36]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following output is shown. The first and second columns are the source
    and target vertex IDs. The third column is the cosine similarity between the term
    vectors corresponding to the source and target documents, and the fourth is the
    similarity score reported by the Word2Vec model. As you can see, cosine similarity
    reports a similarity only between 2 of the 10 document pairs, but the Word2Vec
    model is able to detect latent similarities in the embedding space. This is similar
    to the behavior we have noticed between one-hot encoding and dense embeddings:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果。第一列和第二列是源节点和目标节点的ID。第三列是源文档和目标文档对应的词向量之间的余弦相似度，第四列是Word2Vec模型报告的相似度分数。正如你所看到的，余弦相似度只报告了10对文档中的2对之间的相似性，但Word2Vec模型能够在嵌入空间中检测到潜在的相似性。这与我们注意到的一热编码和稠密嵌入之间的行为类似：
- en: '[PRE37]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The code for this embedding strategy is available in `neurips_papers_node2vec.py`
    in the source code folder accompanying this chapter. Next, we will move on to
    look at character and subword embeddings.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这种嵌入策略的代码可以在本章配套的源代码文件夹中的`neurips_papers_node2vec.py`找到。接下来，我们将继续研究字符和子词嵌入。
- en: Character and subword embeddings
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字符和子词嵌入
- en: Another evolution of the basic word embedding strategy has been to look at character
    and subword embeddings instead of word embeddings. Character-level embeddings
    were first proposed by *Xiang* and *LeCun* [17] and have some key advantages over
    word embeddings.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 基本词嵌入策略的另一种演变是改为使用字符和子词嵌入，而不是词嵌入。字符级嵌入最早由 *Xiang* 和 *LeCun* [17] 提出，相较于词嵌入，它们有一些关键的优势。
- en: First, a character vocabulary is finite and small – for example, a vocabulary
    for English would contain around 70 characters (26 characters, 10 numbers, and
    the rest special characters), leading to character models that are also small
    and compact. Second, unlike word embeddings, which provide vectors for a large
    but finite set of words, there is no concept of out-of-vocabulary for character
    embeddings, since any word can be represented by the vocabulary. Third, character
    embeddings tend to be better for rare and misspelled words because there is much
    less imbalance for character inputs than for word inputs.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，字符词汇表是有限且小的——例如，英语的词汇表大约包含 70 个字符（26 个字母、10 个数字，以及其他特殊字符），这导致字符模型也小而紧凑。其次，不像词嵌入提供一个大而有限的词汇集合的向量，字符嵌入没有“超出词汇表”这一概念，因为任何单词都可以通过词汇表来表示。第三，字符嵌入对于稀有词和拼写错误的单词通常表现更好，因为与词输入相比，字符输入的失衡程度要小得多。
- en: Character embeddings tend to work better for applications that require the notion
    of syntactic rather than semantic similarity. However, unlike word embeddings,
    character embeddings tend to be task-specific and are usually generated inline
    within a network to support the task. For this reason, third-party character embeddings
    are generally not available.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 字符嵌入更适合需要语法相似性而非语义相似性的应用。然而，不同于词嵌入，字符嵌入通常是针对特定任务的，通常是在网络内联生成以支持该任务。因此，第三方字符嵌入通常不可用。
- en: Subword embeddings combine the idea of character and word embeddings by treating
    a word as a bag of character n-grams, that is, sequences of *n* consecutive words.
    They were first proposed by Bojanowski, et al. [18] based on research from **Facebook
    AI Research** (**FAIR**), which they later released as fastText embeddings. fastText
    embeddings are available for 157 languages, including English. The paper has reported
    state-of-the-art performance on a number of NLP tasks, especially word analogies
    and language tasks for languages with rich morphologies.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 子词嵌入结合了字符和词嵌入的思想，通过将单词视为字符 n-gram 的集合，即由 *n* 个连续字符组成的序列。它们最早由 Bojanowski 等人
    [18] 提出，基于 **Facebook AI Research**（**FAIR**）的研究，随后作为 fastText 嵌入发布。fastText 嵌入可用于
    157 种语言，包括英语。相关论文报告了在多个 NLP 任务中，特别是在词类比和语言任务方面，对于形态丰富语言的状态-of-the-art（最先进）性能。
- en: fastText computes embeddings for character n-grams where n is between 3 and
    6 characters (default settings can be changed), as well as for the words themselves.
    For example, character n-grams for n=3 for the word “green” would be “<gr”, “gre”,
    “ree”, “een”, and “en>”. The beginning and end of words are marked with “<” and
    “>” characters respectively, to distinguish between short words and their n-grams
    such as “<cat>” and “cat”.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: fastText 计算字符 n-gram 的嵌入，其中 n 介于 3 到 6 个字符之间（默认设置可以更改），同时也计算单词本身的嵌入。例如，单词“green”的
    n=3 时的字符 n-gram 会是“<gr”、 “gre”、 “ree”、 “een”和“en>”。单词的开始和结束分别用“<”和“>”字符标记，以区分短词和它们的
    n-gram，如“<cat>”与“cat”。
- en: During lookup, you can look up a vector from the fastText embedding using the
    word as the key if the word exists in the embedding. However, unlike traditional
    word embeddings, you can still construct a fastText vector for a word that does
    not exist in the embedding. This is done by decomposing the word into its constituent
    trigram subwords as shown in the preceding example, looking up the vectors for
    the subwords, and then taking the average of these subword vectors. The fastText
    Python API [19] will do this automatically, but you will need to do this manually
    if you use other APIs to access fastText word embeddings, such as Gensim or NumPy.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在查找过程中，如果单词存在于 fastText 嵌入中，你可以通过单词作为键来查找相应的向量。然而，与传统的词嵌入不同，即使单词不在嵌入中，你仍然可以为该单词构建一个
    fastText 向量。这是通过将单词分解为其组成的三元子词（trigram）来实现的，如前面的示例所示，然后查找这些子词的向量，并取这些子词向量的平均值。fastText
    Python API [19] 会自动执行此操作，但如果你使用其他 API 来访问 fastText 词嵌入，如 Gensim 或 NumPy，你需要手动执行此操作。
- en: Next up, we will look at dynamic embeddings.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论动态嵌入。
- en: Dynamic embeddings
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态嵌入
- en: So far, all the embeddings we have considered have been static; that is, they
    are deployed as a dictionary of words (and subwords) mapped to fixed dimensional
    vectors. The vector corresponding to a word in these embeddings is going to be
    the same regardless of whether it is being used as a noun or verb in the sentence,
    for example, the word “ensure” (the name of a health supplement when used as a
    noun, and to make certain when used as a verb). It also provides the same vector
    for polysemous words or words with multiple meanings, such as “bank” (which can
    mean different things depending on whether it co-occurs with the word “money”
    or “river”). In both cases, the meaning of the word changes depending on clues
    available in its context, the sentence. Dynamic embeddings attempt to use these
    signals to provide different vectors for words based on their context.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所考虑的所有嵌入都是静态的；也就是说，它们被部署为一个字典，字典中包含映射到固定维度向量的单词（和子词）。在这些嵌入中，对应某个单词的向量无论它在句子中是作为名词还是动词使用，都将是相同的。例如，单词“ensure”（作为名词时是保健品的名字，作为动词时是“确保”）。它还为多义词或具有多个含义的单词提供相同的向量，例如“bank”（这个词的意思可以根据它与“money”或“river”共现时的上下文不同而不同）。在这两种情况下，单词的意思会根据上下文中的线索发生变化。动态嵌入试图利用这些信号，根据上下文为单词提供不同的向量。
- en: Dynamic embeddings are deployed as trained networks that convert your input
    (typically a sequence of one-hot vectors) into a lower-dimensional dense fixed-size
    embedding by looking at the entire sequence, not just individual words. You can
    either preprocess your input to this dense embedding and then use this as input
    to your task-specific network, or wrap the network and treat it similar to the
    `tf.keras.layers.Embedding` layer for static embeddings. Using a dynamic embedding
    network in this way is usually much more expensive compared to generating it ahead
    of time (the first option) or using traditional embeddings.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 动态嵌入被部署为训练好的网络，它通过查看整个序列（而不仅仅是单个单词），将输入（通常是一个 one-hot 向量序列）转换为一个低维密集的固定大小嵌入。你可以将输入预处理为该密集嵌入，然后将其用作任务特定网络的输入，或者将网络封装起来，将其视为类似于
    `tf.keras.layers.Embedding` 层的静态嵌入。以这种方式使用动态嵌入网络通常比提前生成（第一种选择）或使用传统嵌入要昂贵得多。
- en: The earliest dynamic embedding was proposed by McCann, et al. [20], and was
    called **Contextualized Vectors** (**CoVe**). This involved taking the output
    of the encoder from the encoder-decoder pair of a machine translation network
    and concatenating it with word vectors for the same word.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 最早的动态嵌入是由 McCann 等人 [20] 提出的，称为**上下文化向量**（**CoVe**）。该方法涉及从机器翻译网络的编码器-解码器对中获取编码器的输出，并将其与相同单词的词向量连接起来。
- en: You will learn more about seq2seq networks in the next chapter. The researchers
    found that this strategy improved the performance of a wide variety of NLP tasks.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在下一章中了解更多关于 seq2seq 网络的内容。研究人员发现，这种策略提高了各种自然语言处理（NLP）任务的性能。
- en: Another dynamic embedding proposed by Peters, et al. [21], was **Embeddings
    from Language Models** (**ELMo**). ELMo computes contextualized word representations
    using character-based word representation and bidirectional **Long Short-Term
    Memory** (**LSTM**). You will learn more about LSTMs in the next chapter. In the
    meantime, a trained ELMo network is available from TensorFlow’s model repository
    TensorFlow Hub. You can access it and use it for generating ELMo embeddings as
    follows.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Peters 等人 [21] 提出的另一种动态嵌入是**来自语言模型的嵌入**（**ELMo**）。ELMo 使用基于字符的单词表示和双向**长短期记忆**（**LSTM**）来计算上下文化的单词表示。你将在下一章中了解更多关于
    LSTM 的内容。与此同时，可以从 TensorFlow 的模型库 TensorFlow Hub 获取训练好的 ELMo 网络。你可以访问它并按如下方式使用它来生成
    ELMo 嵌入。
- en: 'The full set of models available on TensorFlow Hub that are TensorFlow 2.0
    compatible can be found on the TensorFlow Hub site for TensorFlow 2.0 [16]. Here
    I have used an array of sentences, where the model will figure out tokens by using
    its default strategy of tokenizing on whitespace:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Hub 上可用的所有与 TensorFlow 2.0 兼容的模型集合可以在 TensorFlow 2.0 的 [16] 网站上找到。在这里，我使用了一系列句子，模型将通过其默认的空格分词策略来确定标记：
- en: '[PRE38]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The output is (`2, 7, 1024`). The first index tells us that our input contained
    2 sentences. The second index refers to the maximum number of words across all
    sentences, in this case, 7\. The model automatically pads the output to the longest
    sentence. The third index gives us the size of the contextual word embedding created
    by ELMo; each word is converted to a vector of size (1024).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是 (`2, 7, 1024`)。第一个索引告诉我们输入包含了 2 个句子。第二个索引指的是所有句子中的最大单词数，在这个例子中是 7。模型会自动将输出填充到最长的句子。第三个索引给出了由
    ELMo 创建的上下文单词嵌入的大小；每个单词被转换为一个大小为 (1024) 的向量。
- en: 'You can also integrate the ELMo embedding layer into your TF2 model by wrapping
    it in a `tf.keras.KerasLayer` adapter. In this simple model, the model will return
    the embedding for the entire string:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过将 ELMo 嵌入层包装在 `tf.keras.KerasLayer` 适配器中，将其集成到你的 TF2 模型中。在这个简单的模型中，模型将返回整个字符串的嵌入：
- en: '[PRE39]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Dynamic embeddings such as ELMo are able to provide different embeddings for
    the same word when used in different contexts and represent an improvement over
    static embeddings such as Word2Vec or GloVe. A logical next step is embeddings
    that represent larger units of text, such as sentences and paragraphs. This is
    what we will look at in the next section.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 动态嵌入，如 ELMo，能够在不同的上下文中为相同的单词提供不同的嵌入，相比于 Word2Vec 或 GloVe 等静态嵌入，具有更好的表现。一个合乎逻辑的下一步是生成表示更大文本单元（如句子和段落）的嵌入。这也是我们在下一部分要探讨的内容。
- en: Sentence and paragraph embeddings
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 句子和段落嵌入
- en: A simple, yet surprisingly effective solution for generating useful sentence
    and paragraph embeddings is to average the word vectors of their constituent words.
    Even though we will describe some popular sentence and paragraph embeddings in
    this section, it is generally always advisable to try averaging the word vectors
    as a baseline.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单却出奇有效的生成有用的句子和段落嵌入的解决方案是将其组成单词的单词向量进行平均。尽管我们在本节中会描述一些流行的句子和段落嵌入，但通常建议始终先尝试将单词向量进行平均，作为一个基准。
- en: Sentence (and paragraph) embeddings can also be created in a task-optimized
    way by treating them as a sequence of words and representing each word using some
    standard word vector. The sequence of word vectors is used as input to train a
    network for some specific task. Vectors extracted from one of the later layers
    of the network just before the classification layer generally tend to produce
    a very good vector representation for the sequence. However, they tend to be very
    task-specific, and are of limited use as a general vector representation.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 句子（和段落）嵌入也可以通过将其视为单词序列并使用标准的单词向量表示每个单词，以任务优化的方式创建。单词向量序列作为输入训练网络来执行某个特定任务。从网络的某个后期层（在分类层之前）提取的向量通常能够很好地表示序列。然而，这些向量往往是非常任务特定的，作为通用的向量表示其使用价值有限。
- en: An idea for generating general vector representations for sentences that could
    be used across tasks was proposed by Kiros, et al. [22]. They proposed using the
    continuity of text from books to construct an encoder-decoder model that is trained
    to predict surrounding sentences given a sentence. The vector representation of
    a sequence of words constructed by an encoder-decoder network is typically called
    a “thought vector.” In addition, the proposed model works on a very similar basis
    to skip-gram, where we try to predict the surrounding words given a word. For
    these reasons, these sentence vectors were called skip-thought vectors. The project
    released a Theano-based model that could be used to generate embeddings from sentences.
    Later, the model was re-implemented with TensorFlow by the Google Research team
    [23]. The Skip-Thoughts model emits vectors of size (2048) for each sentence.
    Using the model is not very straightforward, but the `README.md` file on the repository
    [23] provides instructions if you would like to use it.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: Kiros 等人提出了一种生成通用句子向量表示的思路，这些向量可以跨任务使用[22]。他们建议利用来自书籍的文本连续性来构建一个编码器-解码器模型，该模型经过训练能够根据一个句子预测周围的句子。由编码器-解码器网络构建的单词序列的向量表示通常被称为“思想向量”。此外，提出的模型与
    skip-gram 非常相似，我们试图根据一个单词预测其周围的单词。正因如此，这些句子向量被称为 skip-thought 向量。该项目发布了一个基于 Theano
    的模型，可以用来生成句子的嵌入。后来，Google 研究团队使用 TensorFlow 重新实现了这个模型[23]。Skip-Thoughts 模型为每个句子输出大小为
    (2048) 的向量。使用该模型并不是非常简单，但如果你想使用它，仓库中的 `README.md` 文件[23]提供了详细的使用说明。
- en: A more convenient source of sentence embeddings is the Google Universal Sentence
    Encoder, available on TensorFlow Hub. There are two flavors of the encoder in
    terms of implementation. The first flavor is fast but not so accurate and is based
    on the **Deep Averaging Network** (**DAN**) proposed by Iyer, et al. [24], which
    combines embeddings for words and bigrams and sends it through a fully connected
    network. The second flavor is much more accurate but slower and is based on the
    encoder component of the transformer network proposed by Vaswani, et al. [25].
    We will cover the transformer network in more detail in *Chapter 6*, *Transformers*.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更方便的句子嵌入来源是 Google 的 Universal Sentence Encoder，它可以在 TensorFlow Hub 上获得。该编码器有两种实现方式。第一种实现速度较快，但准确性较差，基于
    Iyer 等人提出的**深度平均网络**（**DAN**）[24]，该网络将单词和二元组的嵌入合并，并通过一个全连接网络进行处理。第二种实现更为准确，但速度较慢，基于
    Vaswani 等人提出的 Transformer 网络的编码器部分[25]。我们将在*第六章*《Transformers》中更详细地讨论 Transformer
    网络。
- en: 'As with ELMo, the Google Universal Sentence Encoder can also be loaded from
    TensorFlow Hub into your TF2 code. Here is some code that calls it with two of
    our example sentences:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 与 ELMo 一样，Google Universal Sentence Encoder 也可以从 TensorFlow Hub 加载到你的 TF2 代码中。下面是一些调用它的代码，使用了我们两个示例句子：
- en: '[PRE40]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The output is (`2`, `512`); that is, each sentence is represented by a vector
    of size (512). It is important to note that the Google Universal Sentence Encoder
    can handle any length of word sequence—so you could legitimately use it to get
    word embeddings on one end as well as paragraph embeddings on the other. However,
    as the sequence length gets longer, the quality of the embeddings tends to get
    “diluted.”
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是（`2`，`512`）；也就是说，每个句子由一个大小为（512）的向量表示。值得注意的是，Google Universal Sentence Encoder
    可以处理任意长度的单词序列——因此，你可以合法地用它来获取一端的单词嵌入以及另一端的段落嵌入。然而，随着序列长度的增加，嵌入的质量往往会变得“稀释”。
- en: A much earlier related line of work in producing embeddings for long sequences
    such as paragraphs and documents was proposed by Le and Mikolov [26] soon after
    Word2Vec was proposed. It is now known interchangeably as Doc2Vec or Paragraph2Vec.
    The Doc2Vec algorithm is an extension of Word2Vec that uses surrounding words
    to predict a word. In the case of Doc2Vec, an additional parameter, the paragraph
    ID, is provided during training. At the end of the training, the Doc2Vec network
    learns an embedding for every word and an embedding for every paragraph. During
    inference, the network is given a paragraph with some missing words. The network
    uses the known part of the paragraph to produce a paragraph embedding, then uses
    this paragraph embedding and the word embeddings to infer the missing words in
    the paragraph. The Doc2Vec algorithm comes in two flavors—the **Paragraph Vectors
    - Distributed Memory** (**PV-DM**) and **Paragraph Vectors - Distributed Bag of
    Words** (**PV-DBOW**), roughly analogous to CBOW and skip-gram in Word2Vec. We
    will not look at Doc2Vec further in this book, except to note that the Gensim
    toolkit provides prebuilt implementations that you can train with your own corpus.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 更早的相关工作是在提出 Word2Vec 后不久，由 Le 和 Mikolov [26] 提出的，旨在为长序列（如段落和文档）生成嵌入。这个方法现在通常被称为
    Doc2Vec 或 Paragraph2Vec。Doc2Vec 算法是 Word2Vec 的扩展，它利用周围的单词来预测一个单词。在 Doc2Vec 的情况下，训练时提供一个额外的参数——段落
    ID。在训练结束时，Doc2Vec 网络会学习每个单词和每个段落的嵌入。在推理阶段，网络接收到一个有缺失单词的段落。网络利用已知部分的段落生成一个段落嵌入，然后使用该段落嵌入和单词嵌入来推断段落中缺失的单词。Doc2Vec
    算法有两种实现方式——**段落向量 - 分布式记忆**（**PV-DM**）和**段落向量 - 分布式词袋模型**（**PV-DBOW**），大致相当于 Word2Vec
    中的 CBOW 和 skip-gram。除了提到 Gensim 工具包提供了可以用你自己的语料库进行训练的预构建实现之外，我们在本书中不会进一步讨论 Doc2Vec。
- en: Having looked at the different forms of static and dynamic embeddings, we will
    now switch gears a bit and look at language model-based embeddings.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在看过不同形式的静态和动态嵌入之后，我们现在稍微换个方向，来看看基于语言模型的嵌入。
- en: Language model-based embeddings
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于语言模型的嵌入
- en: Language model-based embeddings represent the next step in the evolution of
    word embeddings. A language model is a probability distribution over sequences
    of words. Once we have a model, we can ask it to predict the most likely next
    word given a particular sequence of words. Similar to traditional word embeddings,
    both static and dynamic, they are trained to predict the next word (or previous
    word as well, if the language model is bidirectional) given a partial sentence
    from the corpus. Training does not involve active labeling, since it leverages
    the natural grammatical structure of large volumes of text, so in a sense, this
    is a self-supervised learning process.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 基于语言模型的嵌入代表了词嵌入演化的下一步。语言模型是一个单词序列的概率分布。一旦我们有了模型，就可以让它预测给定特定单词序列后最可能出现的下一个单词。与传统的词嵌入类似，无论是静态还是动态，它们都被训练以预测给定语料库中的部分句子后（或前）出现的下一个单词（如果语言模型是双向的，也包括前一个单词）。训练过程中不涉及主动标注，因为它利用了大量文本的自然语法结构，因此在某种意义上，这是一种自监督学习过程。
- en: The main difference between a language model as a word embedding and more traditional
    embeddings is that traditional embeddings are applied as a single initial transformation
    on the data and are then fine-tuned for specific tasks. In contrast, language
    models are trained on large external corpora and represent a model of a particular
    language, say English. This step is called pretraining. The computing cost to
    pretrain these language models is usually fairly high; however, the people who
    pretrain these models generally make them available for use by others so we usually
    do not need to worry about this step. The next step is to fine-tune these general-purpose
    language models for your particular application domain. For example, if you are
    working in the travel or healthcare industry, you would fine-tune the language
    model with text from your own domain. Fine-tuning involves retraining the last
    few layers with your own text. Once fine-tuned, you can reuse this model for multiple
    tasks within your domain. The fine-tuning step is generally much less expensive
    compared to the pretraining step.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 作为词嵌入的语言模型与传统嵌入的主要区别在于，传统嵌入作为数据的单次初始转换应用，然后根据特定任务进行微调。而语言模型则是在大型外部语料库上进行训练，表示特定语言的模型，比如英语。这个步骤被称为预训练。预训练这些语言模型的计算成本通常相对较高；然而，进行预训练的人通常会将模型公开供他人使用，所以我们通常不需要担心这个步骤。下一步是根据你特定的应用领域对这些通用语言模型进行微调。例如，如果你在旅游或医疗行业工作，你会使用自己领域的文本来微调语言模型。微调涉及用你自己的文本重新训练最后几层。微调完成后，你可以将此模型用于该领域的多个任务。与预训练步骤相比，微调步骤通常成本较低。
- en: Once you have the fine-tuned language model, you remove the last layer of the
    language model and replace it with a one-to-two-layer fully connected network
    that converts the language model embedding for your input into the final categorical
    or regression output that your task needs. The idea is identical to transfer learning,
    which you learned about in *Chapter 3*, *Convolutional Neural Networks*, the only
    difference here is that you are doing transfer learning on text instead of images.
    As with transfer learning with images, these language model-based embeddings allow
    us to get surprisingly good results with very little labeled data. Not surprisingly,
    language model embeddings have been referred to as the “ImageNet moment” for natural
    language processing.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你完成了微调语言模型，你就可以去除语言模型的最后一层，替换成一个一到两层的全连接网络，将语言模型嵌入的输入转换为你的任务所需的最终分类或回归输出。这个想法与转移学习相同，你在*第3章*，*卷积神经网络*中已经学习过，唯一的区别是这里你是在文本上进行转移学习，而不是在图像上。与图像的转移学习一样，这些基于语言模型的嵌入允许我们在几乎没有标注数据的情况下获得惊人的结果。不出所料，语言模型嵌入已经被称为自然语言处理领域的“ImageNet时刻”。
- en: The language model-based embedding idea has its roots in the ELMo [21] network,
    which you have already seen in this chapter. ELMo learns about its language by
    being trained on a large text corpus to learn to predict the next and previous
    words given a sequence of words. ELMo is based on a bidirectional LSTM, which
    you will learn more about in *Chapter 8*, *Autoencoders*.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 基于语言模型的嵌入思想起源于ELMo [21] 网络，你在本章中已经见过这个模型。ELMo通过在大量文本语料库上进行训练，学习预测给定一系列单词时下一个和前一个单词的内容，从而了解语言。ELMo基于双向LSTM，你将在*第8章*，*自编码器*中进一步学习有关它的内容。
- en: The first viable language model embedding was proposed by Howard and Ruder [27]
    via their **Universal Language Model Fine-Tuning** (**ULMFiT**) model, which was
    trained on the wikitext-103 dataset consisting of 28,595 Wikipedia articles and
    103 million words. ULMFiT provides the same benefits that transfer learning provides
    for image tasks—better results from supervised learning tasks with comparatively
    less labeled data.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个可行的语言模型嵌入是由 Howard 和 Ruder [27] 提出的，他们通过 **通用语言模型微调**（**ULMFiT**）模型进行训练，该模型使用了包含
    28,595 篇维基百科文章和 1.03 亿个单词的 wikitext-103 数据集。ULMFiT 提供了与图像任务的迁移学习相同的好处——在相对较少的标注数据情况下，通过监督学习任务获得更好的结果。
- en: Meanwhile, the transformer architecture has become the preferred network for
    machine translation tasks, replacing the LSTM network because it allows for parallel
    operations and better handling of long-term dependencies. We will learn more about
    the transformer architecture in *Chapter 6*, *Transformers*. The OpenAI team of
    Radford, et al. [29] proposed using the decoder stack from the standard transformer
    network instead of the LSTM network used in ULMFiT. Using this, they built a language
    model embedding called **Generative Pretraining** (**GPT**) that achieved state
    of the art results for many language processing tasks. The paper proposes several
    configurations for supervised tasks involving single-and multi-sentence tasks
    such as classification, entailment, similarity, and multiple-choice question answering.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，变换器架构已成为机器翻译任务的首选网络，取代了 LSTM 网络，因为它允许并行操作，并更好地处理长期依赖。我们将在 *第六章*，*变换器* 中学习更多关于变换器架构的内容。OpenAI
    团队的 Radford 等人 [29] 提出了使用标准变换器网络的解码器堆栈，取代了 ULMFiT 中使用的 LSTM 网络。利用这一点，他们构建了一个名为
    **生成预训练**（**GPT**）的语言模型嵌入，在许多语言处理任务中取得了最先进的结果。该论文提出了几种配置，用于涉及单句和多句任务的监督任务，如分类、蕴含、相似度和多选问答。
- en: The OpenAI team later followed this up by building even larger language models
    called GPT-2 and GPT-3 respectively. GPT-2 was initially not released because
    of fears of misuse of the technology by malicious operators [30].
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 团队随后构建了更大的语言模型，分别称为 GPT-2 和 GPT-3。由于担心恶意操作员滥用该技术，GPT-2 最初没有发布 [30]。
- en: One problem with the OpenAI transformer architecture is that it is unidirectional
    whereas its predecessors ELMo and ULMFiT were bidirectional. **Bidirectional Encoder
    Representations for Transformers** (**BERT**), proposed by the Google AI team
    [28], uses the encoder stack of the Transformer architecture and achieves bidirectionality
    safely by masking up to 15% of its input, which it asks the model to predict.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 变换器架构的一个问题是它是单向的，而其前身 ELMo 和 ULMFiT 是双向的。由 Google AI 团队 [28] 提出的 **双向编码器表示变换器**（**BERT**）使用了变换器架构的编码器堆栈，并通过对其输入进行最多
    15% 的掩码，要求模型进行预测，从而安全地实现了双向性。
- en: As with the OpenAI paper, BERT proposes configurations for using it for several
    supervised learning tasks such as single- and multiple-sentence classification,
    question answering, and tagging.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 与 OpenAI 的论文一样，BERT 提出了几种配置，用于执行多种监督学习任务，如单句和多句分类、问答和标注。
- en: The BERT model comes in two major flavors—BERT-base and BERT-large. BERT-base
    has 12 encoder layers, 768 hidden units, and 12 attention heads, with 110 million
    parameters in all. BERT-large has 24 encoder layers, 1,024 hidden units, and 16
    attention heads, with 340 million parameters. More details can be found in the
    BERT GitHub repository [33].
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 模型有两种主要版本——BERT-base 和 BERT-large。BERT-base 有 12 层编码器，768 个隐藏单元和 12 个注意力头，总参数量为
    1.1 亿。BERT-large 有 24 层编码器，1,024 个隐藏单元和 16 个注意力头，总参数量为 3.4 亿。更多细节可以在 BERT GitHub
    仓库 [33] 中找到。
- en: BERT pretraining is an expensive process and can currently only be achieved
    using **Tensor Processing Units** (**TPUs**) or large distributed **Graphics Processing
    Units** (**GPUs**) clusters. TPUs are only available from Google via its Colab
    network [31] or Google Cloud Platform [32]. However, fine-tuning the BERT-base
    with custom datasets is usually achievable on GPU instances.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 预训练是一个昂贵的过程，目前只能通过 **张量处理单元**（**TPUs**）或大型分布式 **图形处理单元**（**GPUs**）集群实现。TPUs
    仅通过 Google 的 Colab 网络 [31] 或 Google Cloud Platform [32] 提供。不过，使用自定义数据集对 BERT-base
    进行微调通常可以在 GPU 实例上实现。
- en: Once the BERT model is fine-tuned for your domain, the embeddings from the last
    four hidden layers usually produce good results for downstream tasks. Which embedding
    or combination of embeddings (via summing, averaging, max-pooling, or concatenating)
    to use is usually based on the type of task.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 BERT 模型被微调到你的领域，通常来自最后四个隐藏层的嵌入会对下游任务产生良好的结果。具体使用哪个嵌入，或者哪些嵌入的组合（通过求和、平均、最大池化或拼接）通常取决于任务类型。
- en: In the following section, we will look at how to extract embeddings from the
    BERT language model.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将探讨如何从 BERT 语言模型中提取嵌入。
- en: Using BERT as a feature extractor
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 BERT 作为特征提取器
- en: 'The BERT project [33] provides a set of Python scripts that can be run from
    the command line to fine-tune BERT:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 项目 [33] 提供了一套 Python 脚本，可以通过命令行运行来对 BERT 进行微调：
- en: '[PRE41]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We then download the appropriate BERT model we want to fine-tune. As mentioned
    earlier, BERT comes in two sizes—BERT-base and BERT-large. In addition, each model
    has a cased and uncased version. The cased version differentiates between upper
    and lowercase words, while the uncased version does not. For our example, we will
    use the BERT-base-uncased pretrained model. You can find the download URL for
    this and the other models further down the `README.md` page:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们下载我们想要微调的适当 BERT 模型。如前所述，BERT 有两种尺寸——BERT-base 和 BERT-large。此外，每个模型都有带大小写和不带大小写的版本。带大小写的版本区分大写和小写单词，而不带大小写的版本则不区分。对于我们的例子，我们将使用
    BERT-base-uncased 预训练模型。你可以在 `README.md` 页面下方找到该模型和其他模型的下载链接：
- en: '[PRE42]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This will create the following folder under the `data` directory of your local
    BERT project. The `bert_config.json` file is the configuration file used to create
    the original pretrained model, and the `vocab.txt` is the vocabulary used for
    the model, consisting of 30,522 words and word pieces:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在你本地 BERT 项目的 `data` 目录下创建以下文件夹。`bert_config.json` 文件是用来创建原始预训练模型的配置文件，`vocab.txt`
    是模型使用的词汇表，包含 30,522 个单词和词片段：
- en: '[PRE43]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The pretrained language model can be directly used as a text feature extractor
    for simple machine learning pipelines. This can be useful for situations where
    you want to just vectorize your text input, leveraging the distributional property
    of embeddings to get a denser and richer representation than one-hot encoding.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练语言模型可以直接作为文本特征提取器，应用于简单的机器学习管道。这在你只想将文本输入向量化时非常有用，利用嵌入的分布特性，获取比一热编码更密集、更丰富的表示。
- en: 'The input in this case is just a file with one sentence per line. Let us call
    it `sentences.txt` and put it into our `${CLASSIFIER_DATA}` folder. You can generate
    the embeddings from the last hidden layers by identifying them as -1 (last hidden
    layer), -2 (hidden layer before that), and so on. The command to extract BERT
    embeddings for your input sentences is as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的输入仅是一个每行一个句子的文件。我们称其为 `sentences.txt`，并将其放入 `${CLASSIFIER_DATA}` 文件夹中。你可以通过将它们标识为
    -1（最后一个隐藏层）、-2（前一个隐藏层）等，来生成来自最后隐藏层的嵌入。提取输入句子的 BERT 嵌入的命令如下：
- en: '[PRE44]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The command will extract the BERT embeddings from the last four hidden layers
    of the model and write them out into a line-oriented JSON file called `embeddings.jsonl`
    in the same directory as the input file. These embeddings can then be used as
    input to downstream models that specialize in some specific task, such as sentiment
    analysis. Because BERT was pretrained on large quantities of English text, it
    learns a lot about the nuances of the language, which turn out to be useful for
    these downstream tasks. The downstream model does not have to be a neural network,
    it can be a non-neural model such as SVM or XGBoost as well.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将从模型的最后四个隐藏层提取 BERT 嵌入，并将其写入与输入文件同一目录下名为 `embeddings.jsonl` 的按行排列的 JSON 文件中。这些嵌入随后可以作为下游模型的输入，这些模型专注于某些特定任务，如情感分析。因为
    BERT 是在大量英语文本上预训练的，它学到了许多语言的细微差别，这些知识对下游任务非常有用。下游模型不一定是神经网络，也可以是支持向量机（SVM）或 XGBoost
    等非神经网络模型。
- en: There is much more you can do with BERT. The previous use case corresponds to
    transfer learning in computer vision. As in computer vision, it is also possible
    to fine-tune BERT (and other transformer models) for specific tasks, where the
    appropriate “head” network is attached to BERT, and the combined network is fine-tuned
    for a specific task. You will learn more about these techniques in *Chapter 6*,
    *Transformers*.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用BERT做更多的事情。之前的用例对应于计算机视觉中的迁移学习。像计算机视觉一样，也可以对BERT（以及其他变换器模型）进行微调，以适应特定任务，将适当的“头部”网络附加到BERT上，并将组合网络针对特定任务进行微调。你将在*第六章*，*变换器*中学习更多关于这些技术的内容。
- en: Summary
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have learned about the concepts behind distributional representations
    of words and their various implementations, starting from static word embeddings
    such as Word2Vec and GloVe.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了词的分布表示背后的概念及其各种实现，从静态的词嵌入如Word2Vec和GloVe开始。
- en: We then looked at improvements to the basic idea, such as subword embeddings,
    sentence embeddings that capture the context of the word in the sentence, and
    the use of entire language models for generating embeddings. While language model-based
    embeddings are achieving state-of-the-art results nowadays, there are still plenty
    of applications where more traditional approaches yield very good results, so
    it is important to know them all and understand the tradeoffs.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们看到了对基本概念的改进，例如子词嵌入、能够捕捉词在句子中上下文的句子嵌入，以及使用整个语言模型来生成嵌入。虽然基于语言模型的嵌入方法如今已经取得了最先进的成果，但仍然有很多应用场景中，传统方法能获得非常好的结果，因此了解所有方法并理解它们的权衡是非常重要的。
- en: We also looked briefly at other interesting uses of word embeddings outside
    the realm of natural language, where the distributional properties of other kinds
    of sequences are leveraged to make predictions in domains such as information
    retrieval and recommendation systems.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还简要探讨了词嵌入在自然语言领域之外的其他有趣应用，利用其他类型序列的分布特性，在信息检索和推荐系统等领域做出预测。
- en: You are now ready to use embeddings, not only for your text-based neural networks,
    which we will look at in greater depth in the next chapter, but also in other
    areas of machine learning.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经可以使用嵌入技术，不仅仅是针对文本基础的神经网络，我们将在下一章深入探讨这一点，还可以应用于机器学习的其他领域。
- en: References
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Mikolov, T., et al. (2013, Sep 7) *Efficient Estimation of Word Representations
    in Vector Space*. arXiv:1301.3781v3 [cs.CL].
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mikolov, T., 等. (2013年9月7日)。*高效估计词向量空间中的词表示*。arXiv:1301.3781v3 [cs.CL]。
- en: Mikolov, T., et al. (2013, Sep 17). *Exploiting Similarities among Languages
    for Machine Translation*. arXiv:1309.4168v1 [cs.CL].
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mikolov, T., 等. (2013年9月17日)。*利用语言之间的相似性进行机器翻译*。arXiv:1309.4168v1 [cs.CL]。
- en: Mikolov, T., et al. (2013). *Distributed Representations of Words and Phrases
    and their Compositionality*. Advances in Neural Information Processing Systems
    26 (NIPS 2013).
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mikolov, T., 等. (2013)。*词和短语的分布式表示及其组合性*。神经信息处理系统进展 26 (NIPS 2013)。
- en: 'Pennington, J., Socher, R., Manning, C. (2014). *GloVe: Global Vectors for
    Word Representation*. D14-1162, Proceedings of the 2014 Conference on *Empirical
    Methods in Natural Language Processing* (*EMNLP*).'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pennington, J., Socher, R., Manning, C. (2014). *GloVe：全局词向量表示*。D14-1162，2014年*自然语言处理中的实证方法*会议（*EMNLP*）论文集。
- en: Niu, F., et al (2011, 11 Nov). *HOGWILD! A Lock-Free Approach to Parallelizing
    Stochastic Gradient Descent*. arXiv:1106.5730v2 [math.OC].
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Niu, F., 等. (2011年11月11日)。*HOGWILD！一种无锁并行化随机梯度下降的方法*。arXiv:1106.5730v2 [math.OC]。
- en: Levy, O., Goldberg, Y. (2014). *Neural Word Embedding as Implicit Matrix Factorization*.
    Advances in Neural Information Processing Systems 27 (NIPS 2014).
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Levy, O., Goldberg, Y. (2014). *神经词嵌入作为隐式矩阵分解*。神经信息处理系统进展 27 (NIPS 2014)。
- en: 'Mahoney, M. (2011, 1 Sep). text8 dataset: [http://mattmahoney.net/dc/textdata.xhtml](http://mattmahoney.net/dc/textdata.xhtml)'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mahoney, M. (2011年9月1日)。text8 数据集：[http://mattmahoney.net/dc/textdata.xhtml](http://mattmahoney.net/dc/textdata.xhtml)
- en: 'Rehurek, R. (2019, 10 Apr). gensim documentation for Word2Vec model: [https://radimrehurek.com/gensim/models/word2vec.xhtml](https://radimrehurek.com/gensim/models/word2vec.xhtml)'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rehurek, R. (2019年4月10日)。Word2Vec模型的gensim文档：[https://radimrehurek.com/gensim/models/word2vec.xhtml](https://radimrehurek.com/gensim/models/word2vec.xhtml)
- en: Levy, O., Goldberg, Y. (2014, 26-27 June). *Linguistic Regularities in Sparse
    and Explicit Word Representations*. Proceedings of the Eighteenth Conference on
    Computational Language Learning, pp 171-180 (ACL 2014).
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Levy, O., Goldberg, Y. (2014年6月26-27日). *稀疏和显式词表示中的语言规律性*. 第十八届计算语言学习会议论文集，页171-180（ACL
    2014）。
- en: 'Rehurek, R. (2019, 10 Apr). gensim documentation for KeyedVectors: [https://radimrehurek.com/gensim/models/keyedvectors.xhtml](https://radimrehurek.com/gensim/models/keyedvectors.xhtml)'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rehurek, R. (2019年4月10日). gensim文档，KeyedVectors模块：[https://radimrehurek.com/gensim/models/keyedvectors.xhtml](https://radimrehurek.com/gensim/models/keyedvectors.xhtml)
- en: 'Almeida, T. A., Gamez Hidalgo, J. M., and Yamakami, A. (2011). Contributions
    to the Study of SMS Spam Filtering: New Collection and Results. Proceedings of
    the 2011 ACM Symposium on Document Engineering (DOCENG): [https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/doceng11.pdf?ref=https://githubhelp.com](https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/doceng11.pdf?ref=https://githubhelp.com)'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Almeida, T. A., Gamez Hidalgo, J. M., 和 Yamakami, A. (2011). 短信垃圾信息过滤研究的贡献：新数据集与结果。2011年ACM文档工程研讨会论文集（DOCENG）：[https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/doceng11.pdf?ref=https://githubhelp.com](https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/doceng11.pdf?ref=https://githubhelp.com)
- en: Speer, R., Chin, J. (2016, 6 Apr). *An Ensemble Method to Produce High-Quality
    Word Embeddings*. arXiv:1604.01692v1 [cs.CL].
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Speer, R., Chin, J. (2016年4月6日). *一种生成高质量词嵌入的集成方法*. arXiv:1604.01692v1 [cs.CL].
- en: 'Speer, R. (2016, 25 May). *ConceptNet Numberbatch: a new name for the best
    Word Embeddings you can download*: [http://blog.conceptnet.io/posts/2016/conceptnet-numberbatch-a-new-name-for-the-best-word-embeddings-you-can-download/](http://blog.conceptnet.io/posts/2016/conceptnet-numberbatch-a-new-name-for-the-best-word-embeddings-you-can-download/)'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Speer, R. (2016年5月25日). *ConceptNet Numberbatch：一个新名称，代表你可以下载的最佳词嵌入*：[http://blog.conceptnet.io/posts/2016/conceptnet-numberbatch-a-new-name-for-the-best-word-embeddings-you-can-download/](http://blog.conceptnet.io/posts/2016/conceptnet-numberbatch-a-new-name-for-the-best-word-embeddings-you-can-download/)
- en: 'Barkan, O., Koenigstein, N. (2016, 13-16 Sep). *Item2Vec: Neural Item Embedding
    for Collaborative Filtering*. IEEE 26th International Workshop on Machine Learning
    for Signal Processing (MLSP 2016).'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Barkan, O., Koenigstein, N. (2016年9月13-16日). *Item2Vec：用于协同过滤的神经网络项嵌入*. IEEE第26届国际机器学习与信号处理研讨会（MLSP
    2016）。
- en: 'Grover, A., Leskovec, J. (2016, 13-17 Aug). *node2vec: Scalable Feature Learning
    for Networks*. Proceedings of the 22nd ACM SIGKDD International Conference on
    Knowledge Discovery and Data Mining. (KDD 2016).'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Grover, A., Leskovec, J. (2016年8月13-17日). *node2vec：用于网络的可扩展特征学习*. 第22届ACM SIGKDD国际知识发现与数据挖掘会议论文集（KDD
    2016）。
- en: 'TensorFlow 2.0 Models on TensorFlow Hub: [https://tfhub.dev/s?q=tf2-preview](https://tfhub.dev/s?q=tf2-preview)'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorFlow 2.0 模型在 TensorFlow Hub 上：[https://tfhub.dev/s?q=tf2-preview](https://tfhub.dev/s?q=tf2-preview)
- en: Zhang, X., LeCun, Y. (2016, 4 Apr). *Text Understanding from Scratch*. arXiv
    1502.01710v5 [cs.LG].
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhang, X., LeCun, Y. (2016年4月4日). *从零开始的文本理解*. arXiv 1502.01710v5 [cs.LG].
- en: 'Bojanowski, P., et al. (2017, 19 Jun). *Enriching Word Vectors with Subword
    Information*. arXiv: 1607.04606v2 [cs.CL].'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Bojanowski, P., 等人. (2017年6月19日). *通过子词信息丰富词向量*. arXiv: 1607.04606v2 [cs.CL].'
- en: 'Facebook AI Research, fastText (2017). GitHub repository: [https://github.com/facebookresearch/fastText](https://github.com/facebookresearch/fastText)'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Facebook AI Research, fastText (2017). GitHub 仓库：[https://github.com/facebookresearch/fastText](https://github.com/facebookresearch/fastText)
- en: 'McCann, B., Bradbury, J., Xiong, C., Socher, R. (2017). *Learned in Translation:
    Contextualized Word Vectors*. Neural Information Processing Systems, 2017.'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: McCann, B., Bradbury, J., Xiong, C., Socher, R. (2017). *通过翻译学习：上下文化词向量*. 神经信息处理系统，2017。
- en: 'Peters, M., et al. (2018, 22 Mar). *Deep contextualized word representations*.
    arXiv: 1802.05365v2 [cs.CL].'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Peters, M., 等人. (2018年3月22日). *深度上下文化词表示*. arXiv: 1802.05365v2 [cs.CL].'
- en: 'Kiros, R., et al. (2015, 22 June). *Skip-Thought Vectors*. arXiv: 1506.06727v1
    [cs.CL].'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Kiros, R., 等人. (2015年6月22日). *Skip-Thought 向量*. arXiv: 1506.06727v1 [cs.CL].'
- en: 'Kiros, R, et al (2017). GitHub repository: [https://github.com/ryankiros/skip-thoughts](https://github.com/ryankiros/skip-thoughts)'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kiros, R, 等人. (2017). GitHub 仓库：[https://github.com/ryankiros/skip-thoughts](https://github.com/ryankiros/skip-thoughts)
- en: Iyer, M., Manjunatha, V., Boyd-Graber, J., Daume, H. (2015, July 26-31). *Deep
    Unordered Composition Rivals Syntactic Methods for Text Classification*. Proceedings
    of the 53rd Annual Meeting of the Association for Computational Linguistics and
    the 7th International Joint Conference on Natural Language Processing (ACL 2015).
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Iyer, M., Manjunatha, V., Boyd-Graber, J., Daume, H. (2015年7月26-31日). *深度无序组合在文本分类中超越语法方法*.
    第53届计算语言学协会年会暨第七届国际自然语言处理联合会议（ACL 2015）论文集。
- en: 'Vaswani, A., et al. (2017, 6 Dec). *Attention Is All You Need*. arXiv: 1706.03762v5
    [cs.CL].'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Vaswani, A., et al. (2017年12月6日)。*注意力即全部需求*。arXiv: 1706.03762v5 [cs.CL]。'
- en: 'Le, Q., Mikolov, T. (2014) *Distributed Representation of Sentences and Documents*.
    arXiv: 1405.4053v2 [cs.CL].'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Le, Q., Mikolov, T. (2014) *句子和文档的分布式表示*。arXiv: 1405.4053v2 [cs.CL]。'
- en: 'Howard, J., Ruder, S. (2018, 23 May). *Universal Language Model Fine-Tuning
    for Text Classification*. arXiv: 1801.06146v5 [cs.CL].'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Howard, J., Ruder, S. (2018年5月23日)。*用于文本分类的通用语言模型微调*。arXiv: 1801.06146v5 [cs.CL]。'
- en: 'Devlin, J., Chang, M., Lee, K., Toutanova, K. (2018, 11 Oct). *BERT: Pretraining
    of Deep Bidirectional Transformers for Language Understanding*. arXiv: 1810.04805v1
    [cs.CL]: [https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Devlin, J., Chang, M., Lee, K., Toutanova, K. (2018年10月11日)。*BERT：用于语言理解的深度双向变换器预训练*。arXiv:
    1810.04805v1 [cs.CL]: [https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)'
- en: 'Radford, A., Narasimhan, K., Salimans, T., Sutskever, I. (2018). *Improving
    Language Understanding with Unsupervised Learning*: [https://openai.com/blog/language-unsupervised/](https://openai.com/blog/language-unsupervised/)'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Radford, A., Narasimhan, K., Salimans, T., Sutskever, I. (2018)。*通过无监督学习提高语言理解*：
    [https://openai.com/blog/language-unsupervised/](https://openai.com/blog/language-unsupervised/)
- en: 'Radford, A., et al. (2019). *Language Models are unsupervised Multitask Learners*.
    OpenAI Blog 2019: [http://www.persagen.com/files/misc/radford2019language.pdf](http://www.persagen.com/files/misc/radford2019language.pdf)'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Radford, A., et al. (2019). *语言模型是无监督的多任务学习者*。OpenAI 博客 2019: [http://www.persagen.com/files/misc/radford2019language.pdf](http://www.persagen.com/files/misc/radford2019language.pdf)'
- en: 'Google Collaboratory: [https://colab.research.google.com](https://colab.research.google.com/)'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Google Collaboratory: [https://colab.research.google.com](https://colab.research.google.com/)'
- en: Google Cloud Platform. [https://cloud.google.com/](https://cloud.google.com/)
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Google Cloud Platform. [https://cloud.google.com/](https://cloud.google.com/)
- en: 'Google Research, BERT (2019). GitHub repository: [https://github.com/google-research/bert](https://github.com/google-research/bert)'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Google Research, BERT (2019)。GitHub 仓库：[https://github.com/google-research/bert](https://github.com/google-research/bert)
- en: 'Nemeth (2019). Simple BERT using Tensorflow 2.0\. *Towards Data Science blog*:
    [https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22](https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22)'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Nemeth (2019)。使用 Tensorflow 2.0 的简单 BERT。*Towards Data Science 博客*：[https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22](https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22)
- en: 'TF-IDF. Wikipedia. Retrieved May 2019: [https://en.wikipedia.org/wiki/Tf%E2%80%93idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TF-IDF。维基百科。2019年5月检索：[https://en.wikipedia.org/wiki/Tf%E2%80%93idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)
- en: 'Latent Semantic Analysis. Wikipedia. Retrieved May 2019: [https://en.wikipedia.org/wiki/Latent_semantic_analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 潜在语义分析。维基百科。2019年5月检索：[https://en.wikipedia.org/wiki/Latent_semantic_analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)
- en: 'Topic Model. Wikipedia. Retrieved May 2019: [https://en.wikipedia.org/wiki/Topic_model](https://en.wikipedia.org/wiki/Topic_model)'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主题模型。维基百科。2019年5月检索：[https://en.wikipedia.org/wiki/Topic_model](https://en.wikipedia.org/wiki/Topic_model)
- en: 'Warstadt, A., Singh, A., and Bowman, S. (2018). *Neural Network Acceptability
    Judgements*. arXiv 1805:12471 [cs.CL]: [https://nyu-mll.github.io/CoLA/](https://nyu-mll.github.io/CoLA/)'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Warstadt, A., Singh, A., and Bowman, S. (2018). *神经网络可接受性判断*。arXiv 1805:12471
    [cs.CL]: [https://nyu-mll.github.io/CoLA/](https://nyu-mll.github.io/CoLA/)'
- en: 'Microsoft Research Paraphrase Corpus. (2018): [https://www.microsoft.com/en-us/download/details.aspx?id=52398](https://www.microsoft.com/en-us/download/details.aspx?id=52398)'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Microsoft Research Paraphrase Corpus. (2018): [https://www.microsoft.com/en-us/download/details.aspx?id=52398](https://www.microsoft.com/en-us/download/details.aspx?id=52398)'
- en: 'Nozawa, K. (2019). Something2Vec papers: [https://gist.github.com/nzw0301/333afc00bd508501268fa7bf40cafe4e](https://gist.github.com/nzw0301/333afc00bd508501268fa7bf40cafe4e)'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Nozawa, K. (2019)。Something2Vec 论文： [https://gist.github.com/nzw0301/333afc00bd508501268fa7bf40cafe4e](https://gist.github.com/nzw0301/333afc00bd508501268fa7bf40cafe4e)
- en: 'Perrone, V., et al. (2016). *Poisson Random Fields for Dynamic Feature Models*:
    [https://archive.ics.uci.edu/ml/datasets/NIPS+Conference+Papers+1987-2015](https://archive.ics.uci.edu/ml/datasets/NIPS+Conference+Papers+1987-2015)'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Perrone, V., et al. (2016)。*用于动态特征模型的泊松随机场*：[https://archive.ics.uci.edu/ml/datasets/NIPS+Conference+Papers+1987-2015](https://archive.ics.uci.edu/ml/datasets/NIPS+Conference+Papers+1987-2015)
- en: 'Perozzi, B., Al-Rfou, R., and Skiena, S. (2014). *DeepWalk: Online Learning
    of Social Representations*. arXiv 1403.6652v2 [cs.SI].'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Perozzi, B., Al-Rfou, R., 和 Skiena, S. (2014)。*DeepWalk：社交表示的在线学习*。arXiv 1403.6652v2
    [cs.SI]。
- en: Join our book’s Discord space
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，结识志同道合的人，与超过 2000 名成员一起学习，网址：[https://packt.link/keras](https://packt.link/keras)
- en: '![](img/QR_Code1831217224278819687.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1831217224278819687.png)'
