- en: Restricted Boltzmann Machines
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机
- en: Together, we have seen the power of unsupervised learning and hopefully convinced
    ourselves that it can be applied to different problems. We will finish the topic
    of unsupervised learning with an exciting approach known as **Restricted Boltzmann
    Machines** (**RBMs**). When we do not care about having a large number of layers,
    we can use RBMs to learn from the data and find ways to satisfy an energy function
    that will produce a model that is robust at representing input data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 一起学习后，我们已经看到无监督学习的强大功能，并且希望已经说服自己，它可以应用于不同的问题。我们将以一个激动人心的方法——**受限玻尔兹曼机**（**RBMs**）结束无监督学习的主题。当我们不关心层数过多时，我们可以使用RBMs从数据中学习，并找到满足能量函数的方法，从而产生一个强大的模型来表示输入数据。
- en: This chapter complements [Chapter 8](6677b8b1-806c-4c39-8c1e-371e83501acf.xhtml),
    *Deep Autoencoders*, by introducing the backward-forward nature of RBMs, while
    contrasting it with the forward-only nature of **Autoencoders** (**AEs**). This
    chapter compares RBMs and AEs in the problem of dimensionality reduction, using
    MNIST as the case study. Once you are finished with this chapter, you should be
    able to use an RBM using scikit-learn and implement a solution using a Bernoulli
    RBM. You will be able to perform a visual comparison of the latent spaces of an
    RBM and an AE, and also visualize the learned weights for an inspection of the
    inner workings of an RBM and an AE.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过介绍RBMs的反向-前向特性，补充了[第8章](6677b8b1-806c-4c39-8c1e-371e83501acf.xhtml)《深度自编码器》内容，同时与**自编码器**（**AEs**）的单向特性进行了对比。本章将RBMs与AEs在降维问题上的应用进行比较，使用MNIST作为案例研究。当你完成本章学习后，你应该能够使用scikit-learn实现RBM，并使用伯努利RBM实现一个解决方案。你将能够进行RBM和AE潜在空间的可视化比较，并通过可视化学习到的权重来检查RBM和AE的内部工作原理。
- en: 'This chapter is organized as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章组织结构如下：
- en: Introduction to RBMs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RBM简介
- en: Learning data representations with RBMs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RBM学习数据表示
- en: Comparing RBMs and AEs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较RBMs和AEs
- en: Introduction to RBMs
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RBM简介
- en: RBMs are unsupervised models that can be used in different applications that
    require rich latent representations. They are usually used in a pipeline with
    a classification model with the purpose of extracting features from the data.
    They are based on **Boltzmann Machines** (**BMs**), which we discuss next (Hinton,
    G. E., and Sejnowski, T. J. (1983)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: RBMs是无监督模型，适用于需要丰富潜在表示的各种应用。它们通常与分类模型一起使用，目的是从数据中提取特征。它们基于**玻尔兹曼机**（**BMs**），接下来我们将讨论这一点（Hinton,
    G. E., 和 Sejnowski, T. J.（1983））。
- en: BMs
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 玻尔兹曼机
- en: 'A BM can be thought of as an undirected dense graph, as depicted in *Figure
    10.1*:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机（RBM）可以被看作是一个无向的稠密图，如*图10.1*所示：
- en: '![](img/1dd88078-d42e-4544-8491-77f715f7696f.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1dd88078-d42e-4544-8491-77f715f7696f.png)'
- en: Figure 10.1 – A BM model
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – 玻尔兹曼机模型
- en: 'This undirected graph has some neural units that are modeled to be **visible**,
    ![](img/bc0fc9ef-6923-4dad-905f-d7c40116b3d7.png), and a set of neural units that
    are **hidden**, ![](img/ec3962a8-a124-46ce-b17f-ea74d72f1067.png). Of course,
    there could be many more than these. But the point of this model is that all neurons
    are connected to each other: they all *talk* among themselves. The training of
    this model will not be covered here, but essentially it is an iterative process
    where the input is presented in the visible layers, and every neuron (one at a
    time) adjusts its connections with other neurons to satisfy a loss function (usually
    based on an energy function), and the process repeats until the learning process
    is considered to be satisfactory.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个无向图有一些神经单元，它们被建模为**可见**的，![](img/bc0fc9ef-6923-4dad-905f-d7c40116b3d7.png)，以及一组被建模为**隐藏**的神经单元，![](img/ec3962a8-a124-46ce-b17f-ea74d72f1067.png)。当然，可能会有比这些更多的神经单元。但该模型的关键是所有神经元彼此连接：它们都*互相交流*。本节不会深入讲解此模型的训练过程，但基本上这是一个迭代过程，其中输入数据呈现到可见层，每个神经元（一次调整一个）会调整它与其他神经元之间的连接，以满足一个损失函数（通常基于能量函数），直到学习过程达到满意的程度。
- en: While the RB model was quite interesting and powerful, it took a very long time
    to train! Take into consideration that this was around in the early 1980s and
    performing computations on larger graphs than this and with larger datasets could
    have a significant impact on the training time. However, in 1983, G. E. Hinton
    and his collaborators proposed a simplification of the BM model by restricting
    the communication between neurons, as we will discuss next.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管RB模型非常有趣且强大，但训练时间却非常长！考虑到这是在1980年代初期，当时进行比这更大的图和更大数据集的计算可能会显著影响训练时间。然而，在1983年，G.
    E. Hinton及其合作者提出了通过限制神经元之间的通信来简化BM模型的方法，正如我们接下来将讨论的那样。
- en: RBMs
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RBMs
- en: 'The *restriction* of traditional BMs lies in the communication between neurons;
    that is, visible neurons can only talk to hidden neurons and hidden neurons can
    only talk to visible neurons, as depicted in *Figure 10.2*:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 传统BM的*限制*在于神经元之间的通信；也就是说，可见神经元只能与隐藏神经元通信，隐藏神经元只能与可见神经元通信，正如*图10.2*所示：
- en: '![](img/40d2dc56-c956-418d-a481-ea4ff53f5d42.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40d2dc56-c956-418d-a481-ea4ff53f5d42.png)'
- en: Figure 10.2 – An RBM model. Compare to the BM model in Figure 10.1
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 – 一个RBM模型。与图10.1中的BM模型进行比较。
- en: 'The graph shown in *Figure 10.2* is known as a **dense bipartite graph**. Perhaps
    you are thinking that it looks a lot like the typical dense neural networks that
    we have been using so far; however, it is not quite the same. The main difference
    is that all the neural networks that we have used only communicate information
    going forward from input (visible) to hidden layers, while an RBM can go both
    ways! The rest of the elements are familiar: we have weights and biases that need
    to be learned.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图10.2*所示的图被称为**密集二分图**。也许你会觉得它看起来很像我们迄今为止使用的典型密集神经网络；然而，它并不完全相同。主要的区别在于我们使用的所有神经网络仅仅是从输入（可见层）到隐藏层的信息传递，而RBM则可以双向通信！其余的元素是熟悉的：我们有需要学习的权重和偏置。
- en: If we stick to the simple model shown in *Figure 10.2*, we could explain the
    learning theory behind an RBM in simpler terms.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们坚持使用如*图10.2*所示的简单模型，我们可以用更简单的术语解释RBM背后的学习理论。
- en: Let's interpret every single neural unit as a random variable whose current
    state depends on the state of other neural units.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将每一个神经元单元解释为一个随机变量，其当前状态依赖于其他神经元单元的状态。
- en: This interpretation allows us to use sampling techniques related to **Markov
    Chain Monte Carlo** (**MCMC**) (Brooks, S., et al. (2011)); however, we will not
    go into the details of these in this book.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种解释使我们能够使用与**马尔可夫链蒙特卡洛**（**MCMC**）（Brooks, S., et al. (2011)）相关的采样技术；然而，我们在本书中不会深入讨论这些技术的细节。
- en: 'Using this interpretation, we can define an energy function for the model as
    follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种解释，我们可以为模型定义一个能量函数，如下所示：
- en: '![](img/7eafb87d-5b01-4b10-ab01-98aad9a12ae3.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7eafb87d-5b01-4b10-ab01-98aad9a12ae3.png)'
- en: 'where ![](img/da9561fe-3f4a-4973-9c06-1c311d126aae.png) denote the biases on
    a visible neural unit and a hidden neural unit, respectively. It turns out that
    we can also express the joint probability density function of neural an hidden
    units as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其中！[](img/da9561fe-3f4a-4973-9c06-1c311d126aae.png)分别表示可见神经元和隐藏神经元的偏置。事实证明，我们还可以将神经元和隐藏神经元的联合概率密度函数表示如下：
- en: '![](img/f43952f8-d909-4104-8a8e-c173c9c19d4b.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f43952f8-d909-4104-8a8e-c173c9c19d4b.png)'
- en: 'which has a simple marginal distribution:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 它具有一个简单的边际分布：
- en: '![](img/46d00194-e21e-4dd7-849b-273079cd4c45.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46d00194-e21e-4dd7-849b-273079cd4c45.png)'
- en: 'The denominator in the conditional and marginal is known as a normalizing factor
    that has only the effect of ensuring that probability values add up to one, and
    can be defined as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 条件和边际中的分母被称为归一化因子，它的作用仅仅是确保概率值相加等于1，可以定义如下：
- en: '![](img/c442f2fe-1dfa-4915-8db1-90a8e5b4da7e.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c442f2fe-1dfa-4915-8db1-90a8e5b4da7e.png)'
- en: These formulations allow us to quickly find MCMC techniques for training; most
    notably, you will find in the literature that Contrastive Divergence involving
    Gibbs sampling is the most common approach (Tieleman, T. (2008)).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些公式使我们能够快速找到用于训练的MCMC技术；最著名的是，你会在文献中发现对比散度（Contrastive Divergence）涉及吉布斯采样（Gibbs
    sampling）是最常见的方法（Tieleman, T. (2008)）。
- en: There are only a handful of RBMs implemented that are readily available for
    a learner to get started; one of them is a Bernoulli RBM that is available in
    scikit-learn, which we discuss next.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 目前只有少数实现了的RBM可供学习者使用；其中之一是可以在scikit-learn中使用的伯努利RBM，我们接下来将讨论它。
- en: Bernoulli RBMs
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伯努利RBMs
- en: While the generalized RBM model does not make any assumptions about the data
    that it uses, a Bernoulli RBM does make the assumption that the input data represents
    values in the range [0,1] that can be interpreted as probability values. In the
    ideal case, values are in the set {0,1}, which is closely related to Bernoulli
    trials. If you are interested, there are other approaches that assume that the
    inputs follow a Gaussian distribution. You can find out more by reading Yamashita,
    T. et al. (2014).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然广义的 RBM 模型对其使用的数据不做任何假设，但是 Bernoulli RBM 假设输入数据表示在 [0,1] 范围内的值，可以解释为概率值。在理想情况下，这些值属于集合
    {0,1}，这与伯努利试验密切相关。如果您有兴趣，还有其他方法假设输入遵循高斯分布。您可以通过阅读 Yamashita, T. 等人 (2014) 了解更多信息。
- en: 'There are only a few datasets that we can use for this type of RBM; MNIST is
    an example that can be interpreted as binary inputs where the data is 0 when there
    are no digit traces and 1 where there is digit information. In scikit-learn, the
    `BernoulliRBM` model can be found in the neural network collection: `sklearn.neural_network`.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 只有少数数据集可以用于此类 RBM；MNIST 是一个示例，可以将其解释为二进制输入，其中当没有数字迹象时数据为 0，当存在数字信息时数据为 1。在 scikit-learn
    中，`BernoulliRBM` 模型可以在神经网络集合中找到：`sklearn.neural_network`。
- en: Under the assumption of Bernoulli-like input distribution, this RBM model *approximately*
    optimizes the log likelihood using a method called **Persistent Contrastive Divergence**
    (**PCD**) (Tieleman, T., and Hinton, G. (2009)). It turned out that PCD was much
    faster than any other algorithm at the time and fostered discussions and much
    excitement that was soon overshadowed by the popularization of **backpropagation**
    compared to dense networks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在伯努利样本输入分布的假设下，这个 RBM 模型 *大约* 使用一种称为 **Persistent Contrastive Divergence** (**PCD**)
    的方法优化对数似然。据称，PCD 在当时比任何其他算法都快得多，并引发了讨论和兴奋，很快被背景传播的普及所掩盖，与密集网络相比。
- en: In the next section, we will implement a Bernoulli RBM on MNIST with the purpose
    of learning the representations of the dataset.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将使用 Bernoulli RBM 在 MNIST 上实现，目的是学习数据集的表示。
- en: Learning data representations with RBMs
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 RBM 学习数据表示
- en: Now that you know the basic idea behind RBMs, we will use the `BernoulliRBM`
    model to learn data representations in an unsupervised manner. As before, we will
    do this with the MNIST dataset to facilitate comparisons.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了 RBM 背后的基本思想，我们将使用 `BernoulliRBM` 模型以无监督的方式学习数据表示。与以往一样，我们将使用 MNIST
    数据集进行比较。
- en: For some people, the task of **learning representations** can be thought of
    as **feature engineering**. The latter has an explicability component to the term,
    while the former does not necessarily require us to prescribe meaning to the learned
    representations.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些人来说，**学习表示** 的任务可以被视为 **特征工程**。后者在术语上有一个可解释性的组成部分，而前者并不一定要求我们赋予学到的表示含义。
- en: 'In scikit-learn, we can create an instance of the RBM by invoking the following
    instructions:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，我们可以通过以下方式创建 RBM 的实例：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The default parameters in the constructor of the RBM are the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RBM 的构造函数中，默认参数如下：
- en: '`n_components=256`, which is the number of hidden units, ![](img/73826389-c0a2-4f77-972f-ed916b23f0a9.png),
    while the number of visible units, ![](img/89773bbc-964d-486b-b725-57c2ef2aa47e.png),
    is inferred from the dimensionality of the input.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_components=256`，即隐藏单元的数量，![](img/73826389-c0a2-4f77-972f-ed916b23f0a9.png)，而可见单元的数量，![](img/89773bbc-964d-486b-b725-57c2ef2aa47e.png)，则根据输入的维度进行推断。'
- en: '`learning_rate=0.1` controls the strength of the learning algorithm with respect
    to updates, and it is recommended to explore it with values in the set {*1, 0.1,
    0.01, 0.001*}.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate=0.1` 控制学习算法更新的强度，推荐尝试 `{1, 0.1, 0.01, 0.001}` 中的值。'
- en: '`batch_size=10` controls how many samples are used in the batch-learning algorithm.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size=10` 控制在批处理学习算法中使用的样本数量。'
- en: '`n_iter=10` controls the number of iterations that are run before we stop the
    learning algorithm. The nature of the algorithm allows it to keep going as much
    as we want; however, the algorithm usually finds good solutions in a few iterations.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_iter=10` 控制在停止学习算法之前运行的迭代次数。算法的性质允许我们继续运行它，但通常在少数迭代中找到良好的解决方案。'
- en: We will only change the default number of components to make it 100\. Since
    the original number of dimensions in the MNIST dataset is 784 (because it consists
    of 28 x 28 images), having 100 dimensions does not seem like a bad idea.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需将默认的组件数改为100即可。由于MNIST数据集的原始维度是784（因为它由28 x 28的图像组成），因此使用100个维度似乎是一个不错的主意。
- en: 'To train the RBM with 100 components over MNIST training data loaded into `x_train`,
    we can do the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在MNIST训练数据（已加载到`x_train`中）上使用100个组件训练RBM，我们可以执行以下操作：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output during training might look like this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中的输出可能如下所示：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can look into the representations learned by invoking the `transform()`
    method on the MNIST test data, `x_test`, as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在MNIST测试数据`x_test`上调用`transform()`方法来查看学习到的表示，方法如下：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this case, there are 784 input dimensions, but the `r` variable will have
    100 dimensions. To visualize the test set in the latent space induced by the RBM,
    we can use UMAPs as we did before, which will produce the two-dimensional plot
    shown in *Figure 10.3*:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，有784个输入维度，但`r`变量将具有100个维度。为了在由RBM引起的潜在空间中可视化测试集，我们可以像之前一样使用UMAP，这将产生*图10.3*中所示的二维图：
- en: '![](img/17976fd6-6978-4fe1-aa16-31bfe3ae1cce.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17976fd6-6978-4fe1-aa16-31bfe3ae1cce.png)'
- en: Figure 10.3 – UMAP representation of the learned representations by the RBM
    on MNIST test data
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 – 使用UMAP对RBM在MNIST测试数据上的学习表示进行可视化
- en: 'The full code to produce this plot from the RBM feature space using UMAP is
    as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从RBM特征空间使用UMAP生成此图的完整代码如下：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Compare *Figure 10.3* with the representations shown in previous chapters. From
    the figure, we can appreciate that there are clear class separations and clustering,
    while at the same time there are slight overlaps between classes. For example,
    there is some overlap between the numerals 3 and 8, which is to be expected since
    these numbers look alike. This plot also shows that the RBM generalizes very well
    since the data in *Figure 10.3* is coming from data that is unseen by the model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 将*图10.3*与前几章中展示的表示进行比较。从图中可以看出，存在明显的类别分离和聚类，同时也可以看到类别之间有轻微的重叠。例如，数字3和8之间有一些重叠，这也是可以预料的，因为这两个数字看起来相似。这个图也显示了RBM的良好泛化能力，因为*图10.3*中的数据来自模型未曾见过的数据。
- en: 'We can further inspect the weights (or *components*) learned by the RBM; that
    is, we can retrieve the weights associated with the visible layers as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步检查RBM学习到的权重（或*组件*）；也就是说，我们可以检索与可见层相关的权重，方法如下：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In this case, the `v` variable will be a 784 x 100 matrix describing the learned
    weights. We can visualize every single one of the neurons and reconstruct the
    weights associated with those neurons, which will look like the components in
    *Figure 10.4*:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`v`变量将是一个784 x 100的矩阵，描述了学习到的权重。我们可以可视化每一个神经元，并重构与这些神经元相关的权重，结果将类似于*图10.4*中的组成部分：
- en: '![](img/9ac6b554-5cef-4031-b2af-73bc85f5a4b9.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ac6b554-5cef-4031-b2af-73bc85f5a4b9.png)'
- en: Figure 10.4 – Learned weights of the RBM
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 – RBM学习到的权重
- en: A close examination of *Figure 10.4* informs us that there are weights that
    pay attention to diagonal features, or circular features, or features that are
    very specific to specific digits and edges in general. The bottom row, for example,
    has features that appear to be associated with the numbers 2 and 6.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 细致检查*图10.4*会告诉我们，有一些权重关注于对角线特征、圆形特征或对特定数字和边缘非常特定的特征。例如，底部一行的特征似乎与数字2和6有关。
- en: The weights shown in *Figure 10.4* can be used to transform the input space
    into richer representations that can later be used for classification in a pipeline
    that allows for this task.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10.4*中显示的权重可以用来将输入空间转换为更丰富的表示，这些表示随后可以在允许进行此任务的管道中用于分类。'
- en: 'To satisfy our learning curiosity, we could also look into the RBM and its
    states by sampling the network using the `gibbs()` method. This means that we
    could visualize what happens when we present the input to the visible layer and
    then what the response is from the hidden layer, and then use that as input again
    and repeat to see how the stimulus of the model changes. For example, run the
    following code:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足我们的学习好奇心，我们还可以通过使用`gibbs()`方法对网络进行采样，从而查看RBM及其状态。这意味着我们可以可视化在将输入呈现给可见层时发生了什么，然后是隐藏层的响应，再将其作为输入重复这个过程，观察模型的刺激如何变化。例如，运行以下代码：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will effectively produce a plot like the one shown in *Figure 5*:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这将有效地产生一个类似于*图5*所示的图：
- en: '![](img/8144dac0-0e70-4382-9d00-a6452b5336c7.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8144dac0-0e70-4382-9d00-a6452b5336c7.png)'
- en: Figure 10.5 – Gibbs sampling on an MNIST-based RBM
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 – 基于 MNIST 的 RBM 上的 Gibbs 采样
- en: '*Figure 10.5* shows the input in the first column, and the remaining 10 columns
    are successive sampling calls. Clearly, as the input is propagated back and forth
    within the RBM, it suffers from some slight deformations. Take row number five,
    corresponding to the digit 4; we can see how the input is being deformed until
    it looks like a number 2\. This information has no immediate effect on the learned
    features unless a strong deformation is observed at the first sampling call.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10.5* 显示了第一列的输入，剩余的 10 列是连续的采样调用。显然，随着输入在 RBM 中前后传播，它会出现一些轻微的变形。以第五行（对应数字
    4）为例，我们可以看到输入是如何变形的，直到它看起来像是数字 2。除非在第一次采样调用时观察到强烈的变形，否则这些信息不会对所学习到的特征产生直接影响。'
- en: In the next section, we will use an AE to make a comparison with an RBM.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用 AE 与 RBM 进行比较。
- en: Comparing RBMs and AEs
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较 RBM 和 AE
- en: 'Now that we have seen how RBMs perform, a comparison with AEs is in order.
    To make this comparison fair, we can propose the closest configuration to an RBM
    that an AE can have; that is, we will have the same number of hidden units (neurons
    in the encoder layer) and the same number of neurons in the visible layer (the
    decoder layer), as shown in *Figure 10.6*:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到 RBM 的表现，是时候将其与 AE 进行比较了。为了公平地进行比较，我们可以提出 AE 最接近 RBM 的配置；也就是说，我们将有相同数量的隐藏单元（编码器层中的神经元）和相同数量的可见层神经元（解码器层），如*图
    10.6*所示：
- en: '![](img/d8fefc75-8a3e-40ae-8e91-2ab36d6fea70.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8fefc75-8a3e-40ae-8e91-2ab36d6fea70.png)'
- en: Figure 10.6 – AE configuration that's comparable to RBM
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 – 与 RBM 相当的 AE 配置
- en: 'We can model and train our AE using the tools we covered in [Chapter 7](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml),
    *Autoencoders*, as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用我们在[第 7 章](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml)中讲解的工具，*自动编码器*，来建模和训练我们的
    AE，如下所示：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'There is nothing new here, except that we are training with only two dense
    layers that are large enough to provide nice representations. *Figure 10.7* depicts
    the UMAP visualization of the learned representations on the test set:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有什么新东西，除了我们只用两个足够大的密集层进行训练，这些层足以提供很好的表示。*图 10.7* 展示了在测试集上学习到的表示的 UMAP 可视化：
- en: '![](img/01853091-de4e-420d-a897-43d3f4b7e6b3.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01853091-de4e-420d-a897-43d3f4b7e6b3.png)'
- en: Figure 10.7 – AE-induced representations using a UMAP visualization
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 – 使用 UMAP 可视化的 AE 引导表示
- en: 'The preceding figure is produced with the following code:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图像是通过以下代码生成的：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: From *Figure 10.7*, you can see that the data is nicely clustered; although
    the clusters are closer together than in *Figure 10.3*, the within-cluster separations
    seems to be better. Similarly to the RBM, we can visualize the weights that were
    learned.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 10.7*中可以看到，数据聚类得非常好；尽管这些聚类比*图 10.3*中的更为接近，但聚类内部的分离似乎更好。与 RBM 类似，我们也可以可视化所学习到的权重。
- en: 'Every `Model` object in `tensorflow.keras` has a method called `get_weights()`
    that can retrieve a list with all the weights at every layer. Let''s run this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`tensorflow.keras`中的每个`Model`对象都有一个名为`get_weights()`的方法，可以检索每一层的所有权重列表。让我们运行一下这个：'
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'It gives us access to the weights of the first layer and allows us to visualize
    them in the same way we did for the RBM weights. *Figure 10.8* shows the learned
    weights:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 它使我们能够访问第一层的权重，并允许我们像可视化 RBM 权重一样可视化它们。*图 10.8* 显示了学习到的权重：
- en: '![](img/af57fcd9-06c3-4286-959e-699e42a5904a.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af57fcd9-06c3-4286-959e-699e42a5904a.png)'
- en: Figure 10.8 – AE weights
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8 – AE 权重
- en: The weights shown in *Figure 10.8*, in comparison with those of the RBM in *Figure
    10.4*, have no noticeable digit-specific features. These features seem to be oriented
    to textures and edges in very unique regions. This is very interesting to see
    because it suggests that fundamentally different models will produce fundamentally
    different latent spaces.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与*图 10.4*中的RBM权重相比，*图 10.8*中展示的权重没有明显的数字特定特征。这些特征似乎集中在非常独特的区域，表现为纹理和边缘。看到这一点非常有趣，因为它表明，根本不同的模型会产生根本不同的潜在空间。
- en: If both RBMs and AEs produce interesting latent spaces, imagine what we could
    achieve if we use both of them in our deep learning projects! Try it!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 RBM 和 AE 都能产生有趣的潜在空间，想象一下如果我们在深度学习项目中同时使用它们会取得什么成果！尝试一下吧！
- en: 'Finally, to prove that the AE achieves high-quality reconstructions as modeled,
    we can look at *Figure 10.9*:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了证明 AE 能够实现高质量的重建，如模型所示，我们可以看看*图 10.9*：
- en: '![](img/37f7f0ac-e0ba-498e-80ac-6064f79c94a2.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37f7f0ac-e0ba-498e-80ac-6064f79c94a2.png)'
- en: Figure 10.9 – AE inputs (top row) and reconstructions (bottom row)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9 – AE输入（顶部一行）和重建结果（底部一行）
- en: The reconstructions using 100 components seem to have high quality, as shown
    in *Figure 10.9*. This is, however, not possible for RBMs since their purpose
    is not to reconstruct data, necessarily, as we have explained in this chapter.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用100个组件进行重建的结果似乎具有高质量，如*图10.9*所示。然而，这对于RBM来说是不可能的，因为它们的目的并非一定是重建数据，正如我们在本章中解释的那样。
- en: Summary
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This intermediate-level chapter has shown you the basic theory behind how RBMs
    work and their applications. We paid special attention to a Bernoulli RBM that
    operates on input data that may follow a Bernoulli-like distribution in order
    to achieve fast learning and efficient computations. We used the MNIST dataset
    to showcase how interesting the learned representations are for an RBM, and we
    visualized the learned weights as well. We concluded by comparing the RBM with
    a very simple AE and showed that both learned high-quality latent spaces while
    being fundamentally different models.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中级内容向你展示了RBM的基本理论及其应用。我们特别关注了一个伯努利RBM，它处理可能遵循伯努利分布的输入数据，从而实现快速学习和高效计算。我们使用了MNIST数据集来展示RBM学习到的表示有多么有趣，并且可视化了学习到的权重。最后，我们通过将RBM与一个非常简单的AE进行比较，表明两者都学习到了高质量的潜在空间，尽管它们是根本不同的模型。
- en: At this point, you should be able to implement your own RBM model, visualize
    its learned components, and see the learned latent space by projecting (transforming)
    the input data and looking at the hidden layer projections. You should feel confident
    in using an RBM on large datasets, such as MNIST, and even perform a comparison
    with an AE.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该能够实现自己的RBM模型，可视化其学习到的组件，并通过投影（转换）输入数据，观察隐藏层投影，来看学习到的潜在空间。你应该能够自信地在大型数据集（如MNIST）上使用RBM，甚至与AE进行比较。
- en: The next chapter is the beginning of a new group of chapters about supervised
    deep learning. [Chapter 11](03e9a734-fb56-485d-ae90-66fb98ecd4d1.xhtml), *Deep
    and Wide Neural Networks*, will get us started in a series of exciting and new
    topics surrounding supervised deep learning. The chapter will explain the differences
    in performance and the complexities of deep versus wide neural networks in supervised
    settings. It will introduce the concept of dense networks and sparse networks
    in terms of the connections between neurons. You can't miss it!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章是关于监督深度学习的新一组章节的开始。[第11章](03e9a734-fb56-485d-ae90-66fb98ecd4d1.xhtml)，*深度和宽度神经网络*，将带领我们进入一系列围绕监督深度学习的令人兴奋的新话题。该章节将解释深度神经网络与宽度神经网络在监督学习中的性能差异及其复杂性。它将介绍密集网络和稀疏网络的概念，重点是神经元之间的连接。你不能错过！
- en: Questions and answers
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题与答案
- en: '**Why can''t we perform data reconstructions with an RBM?**'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为什么我们不能用RBM进行数据重建？**'
- en: RBMs are fundamentally different to AEs. An RBM aims to optimize an energy function,
    while an AE aims to optimize a data reconstruction function. Thus, we can't do
    reconstructions with RBMs. However, this fundamental difference allows for new
    latent spaces that are interesting and robust.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: RBM与AE从根本上是不同的。RBM旨在优化能量函数，而AE旨在优化数据重建函数。因此，我们不能使用RBM进行重建。然而，这一根本差异带来了新的潜在空间，这些潜在空间既有趣又强大。
- en: '**Can we add more layers to an RBM?**'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**我们可以给RBM添加更多层吗？**'
- en: No. Not in the current model presented here. The concept of stacked layers of
    neurons fits the concept of deep AEs better.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 不是的。在这里呈现的当前模型中并不适用。神经元堆叠层的概念更适合深度AE的概念。
- en: '**What is so cool about RBMs then?**'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**那RBM到底有什么酷的地方呢？**'
- en: They are simple. They are fast. They provide rich latent spaces. They have no
    equal at this point. The closest competitors are AEs.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 它们简单。它们快速。它们提供丰富的潜在空间。到目前为止没有可比拟的对手。最接近的竞争者是AEs。
- en: References
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Hinton, G. E., and Sejnowski, T. J. (1983, June). Optimal perceptual inference.
    In Proceedings of the *IEEE conference on Computer Vision and Pattern Recognition*
    (Vol. 448). IEEE New York.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton, G. E., 和 Sejnowski, T. J.（1983年6月）。最优感知推理。在*IEEE计算机视觉与模式识别会议论文集*（第448卷）中。IEEE纽约。
- en: Brooks, S., Gelman, A., Jones, G., and Meng, X. L. (Eds.). (2011). *Handbook
    of Markov Chain Monte Carlo*. CRC press.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brooks, S., Gelman, A., Jones, G., 和 Meng, X. L.（主编）。(2011)。*马尔科夫链蒙特卡罗手册*。CRC出版社。
- en: Tieleman, T. (2008, July). Training restricted Boltzmann machines using approximations
    to the likelihood gradient. In Proceedings of the 25th *International Conference
    on Machine Learning* (pp. 1064-1071).
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tieleman, T.（2008年7月）。使用似然梯度的近似方法训练限制玻尔兹曼机。载于第25届*国际机器学习会议*论文集（第1064-1071页）。
- en: Yamashita, T., Tanaka, M., Yoshida, E., Yamauchi, Y., and Fujiyoshii, H. (2014,
    August). To be Bernoulli or to be Gaussian, for a restricted Boltzmann machine.
    In 2014 22nd *International Conference on Pattern Recognition* (pp. 1520-1525).
    IEEE.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yamashita, T.、Tanaka, M.、Yoshida, E.、Yamauchi, Y. 和 Fujiyoshii, H.（2014年8月）。在限制玻尔兹曼机中，选择伯努利分布还是高斯分布。载于2014年第22届*国际模式识别会议*（第1520-1525页）。IEEE。
- en: Tieleman, T., and Hinton, G. (2009, June). Using fast weights to improve persistent
    contrastive divergence. In Proceedings of the 26th Annual *International Conference
    on Machine Learning* (pp. 1033-1040).
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tieleman, T. 和 Hinton, G.（2009年6月）。利用快速权重提高持久对比散度。载于第26届年*国际机器学习会议*论文集（第1033-1040页）。
