- en: Your First Classifier
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你的第一个分类器
- en: With TensorFlow now installed, we need to kick the tires. We will do so by writing
    our first classifier and then training and testing it from start to finish!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow安装完成后，我们需要开始实践。我们将通过编写第一个分类器并从头到尾进行训练和测试来进行实践！
- en: Our first classifier will be a handwriting recognizer. One of the most common
    datasets to train is the **MNIST** handwritten digits dataset. We'll be using
    a similar dataset called `notMNIST`, which features the first ten letters of the
    English alphabet.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个分类器将是一个手写识别器。最常用的训练数据集之一是**MNIST**手写数字数据集。我们将使用一个类似的数据集，名为`notMNIST`，它包含了英文字母表中的前十个字母。
- en: The key parts
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键部分
- en: 'There are three key parts to most machine learning classifiers, which are as
    follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习分类器有三个关键部分，分别如下：
- en: The training pipeline
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练流程
- en: The neural network setup and training outputs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络设置和训练输出
- en: The usage pipeline
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用流程
- en: The training pipeline obtains data, stages it, cleanses it, homogenizes it,
    and puts it in a format acceptable to the neural network. Do not be surprised
    if the training pipeline takes 80% to 85% of your effort initially—this is the
    reality of most machine learning work. Generally, the more realistic the training
    data, the more time spent on the training pipeline. In enterprise settings, the
    training pipeline might be an ongoing effort being enhanced perpetually. This
    is especially true as datasets get larger.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 训练流程获取数据，对数据进行分阶段处理、清洗、标准化，并将其转化为神经网络可接受的格式。不要惊讶，最初训练流程可能占用你80%到85%的工作量——这是大多数机器学习工作的现实。通常，训练数据越真实，训练流程所花费的时间就越多。在企业环境中，训练流程可能是一个持续的工作，不断进行增强。随着数据集的增大，这一点尤为真实。
- en: The second part, the neural network setup, and training, can be quick for routine
    problems and can be a research-grade effort for harder problems. You may find
    yourself making small changes to the network setup, over and over, until you finally
    achieve the desired classifier accuracy. The training is the most computationally
    expensive part, so it takes time before you can evaluate the result of each incremental
    modification.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分是神经网络的设置和训练，对于常规问题，训练过程可能很快，但对于更难的问题，则可能需要进行研究级的努力。你可能会发现自己反复调整网络设置，直到最终达到所需的分类器精度。训练是计算最为密集的部分，因此需要时间才能评估每次增量修改的结果。
- en: Once the initial setup is complete and the network is trained to a sufficient
    level of accuracy, we can just use it over and over. In [Chapter 10](f1a5c9c4-6076-487f-abd1-b5a6e800890f.xhtml),
    *Go Live and Go Big*, we'll explore more advanced topics, such as continuous learning,
    where even usage can feed back into further training the classifier.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦初始设置完成，并且网络训练到足够的精度，我们就可以反复使用它。在[第10章](f1a5c9c4-6076-487f-abd1-b5a6e800890f.xhtml)，*Go
    Live and Go Big*中，我们将探索更多的高级话题，如持续学习，甚至使用过程本身也可以反馈进一步训练分类器。
- en: Obtaining training data
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取训练数据
- en: Machine learning requires training data—often a lot of training data. One of
    the great things about machine learning is the availability of standard training
    datasets. These are often used to benchmark node models and configurations and
    provide a consistent yardstick to gauge performance against previous progress.
    Many of the datasets are also used in annual global competitions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习需要训练数据——通常需要大量的训练数据。机器学习的一大优点是可以使用标准的训练数据集。这些数据集通常用于基准测试节点模型和配置，并提供一致的标准来衡量与之前进展的性能对比。许多数据集也用于年度全球比赛。
- en: This chapter uses training data, which is kindly provided by Yaroslav Bulatov,
    a machine learning researcher.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用的训练数据由机器学习研究员Yaroslav Bulatov慷慨提供。
- en: Downloading training data
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载训练数据
- en: 'You should start by downloading the training data from the following links:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该从以下链接下载训练数据：
- en: '[http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz](http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz](http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz)'
- en: '[http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz](http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz](http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz)'
- en: We will download this programmatically, but we should start with a manual download
    just to peek at the data and structure of the archive. This will be important
    when we write the pipeline, as we'll need to understand the structure so we can
    manipulate the data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过编程下载数据，但首先应该手动下载数据集，先大致了解一下数据和归档的结构。这在我们编写数据流水线时非常重要，因为我们需要理解数据结构，以便操作数据。
- en: 'The small set is ideal for peeking. You can do this via the following command
    line, or just use a browser to download the file with an unarchiver to extract
    the files (I suggest getting familiarized with the command line as all of this
    needs to be automated):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小数据集非常适合快速查看。你可以通过以下命令行操作，或者直接使用浏览器下载文件，利用解压工具提取文件（我建议你熟悉命令行，因为所有这些操作都需要自动化处理）：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The preceding command line will reveal a container folder called `notMNIST_small`
    with ten subfolders underneath, one for each letter of the alphabet `a` through
    `j`. Under each lettered folder, there are thousands of 28x28 pixel images of
    the letter. Additionally, an interesting thing to note is the filename of each
    letter image, (`QnJhbmRpbmcgSXJvbi50dGY=`), suggesting a random string that does
    not contain information of use.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令行将显示一个名为`notMNIST_small`的容器文件夹，下面有十个子文件夹，每个字母（从`a`到`j`）对应一个文件夹。在每个字母文件夹下，有成千上万张28x28像素的字母图片。另外，需要注意的是，每个字母图像的文件名（`QnJhbmRpbmcgSXJvbi50dGY=`）表明它是一个随机字符串，并不包含有用信息。
- en: Understanding classes
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解类
- en: 'The classifier we''re writing seeks to assign unknown images to a class. Classes
    can be of the following types:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在编写的分类器旨在将未知图像分配到某个类。类可以是以下几种类型：
- en: Feline versus canine
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 猫科动物与犬科动物
- en: Two versus seven
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二与七
- en: Tumor versus normal
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 肿瘤与正常
- en: Smiling versus frowning
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微笑与皱眉
- en: In our case, we are considering each letter a class for a total of 10 classes.
    The training set will reveal 10 subfolders with thousands of images underneath
    each subfolder. The name of the subfolder is important as it is the label for
    each of the images. These details will be used by the pipeline to prepare data
    for TensorFlow.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们将每个字母作为一个类别，共有10个类别。训练集将显示10个子文件夹，每个子文件夹下有成千上万的图像。子文件夹的名称很重要，因为它是每张图像的标签。这些细节将被流水线用于准备TensorFlow的数据。
- en: Automating the training data setup
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化训练数据设置
- en: Ideally, we will want the entire process automated. This way, we can easily
    run the process end to end on any computer we use without having to carry around
    ancillary assets. This will be important later, as we will often develop on one
    computer (our development machine) and deploy on a different machine (our production
    server).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望整个过程都能自动化。这样，无论我们在哪台电脑上使用，都可以轻松地端到端地运行这个过程，而不需要携带额外的资产。这个过程在后续会很重要，因为我们通常会在一台电脑（开发机）上进行开发，然后在另一台电脑（生产服务器）上进行部署。
- en: 'I have already written the code for this chapter, as well as all the other
    chapters; it is available at [https://github.com/mlwithtf/MLwithTF](https://github.com/mlwithtf/MLwithTF).
    Our approach will be to rewrite it together while understanding it. Some straightforward
    parts, such as this, may be skipped. I recommend forking the repository and cloning
    a local copy for your projects:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经为本章节以及其他所有章节写好了代码，代码可以在[https://github.com/mlwithtf/MLwithTF](https://github.com/mlwithtf/MLwithTF)找到。我们的方法是边理解边重写代码。像这样直接的部分可以跳过。我建议你分叉这个仓库，并为你的项目克隆一份本地副本：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The code for this specific section is available at— [https://github.com/mlwithtf/mlwithtf/blob/master/chapter_02/download.py.](https://github.com/mlwithtf/mlwithtf/blob/master/chapter_02/download.py)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的代码可通过以下链接获取——[https://github.com/mlwithtf/mlwithtf/blob/master/chapter_02/download.py.](https://github.com/mlwithtf/mlwithtf/blob/master/chapter_02/download.py)
- en: 'Preparing the dataset is an important part of the training process. Before
    we go deeper into the code, we will run `download.py` to automatically download
    and prepare the dataset:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 准备数据集是训练过程中的一个重要部分。在深入代码之前，我们将运行`download.py`来自动下载并准备数据集：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The result will look like this:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '**![](img/12200b7b-6dbd-481d-a663-03ed6c482479.png)**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](img/12200b7b-6dbd-481d-a663-03ed6c482479.png)**'
- en: 'Now, we will take a look at several functions that are used in `download.py`.
    You can find the code in this file:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将查看几个在`download.py`中使用的函数。你可以在这个文件中找到代码：
- en: '[https://github.com/mlwithtf/mlwithtf/blob/master/data_utils.py](https://github.com/mlwithtf/mlwithtf/blob/master/data_utils.py)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/mlwithtf/mlwithtf/blob/master/data_utils.py](https://github.com/mlwithtf/mlwithtf/blob/master/data_utils.py)'
- en: 'The following `downloadFile` function will automatically download the file
    and validate it against an expected file size:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`downloadFile`函数将自动下载文件并验证文件大小是否符合预期：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The function can be called as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数可以按如下方式调用：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The code to extract the contents is as follows (note that the additional import is
    required):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 提取内容的代码如下（请注意，额外的导入是必需的）：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We call the `download` and extract methods in sequence as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按顺序调用`download`和extract方法，代码如下：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Additional setup
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 额外的设置
- en: 'The next part will focus on image processing and manipulation. This requires
    some extra libraries you may not have. At this point, it may make sense to just
    install all the typical packages required in scientific computing, which can be
    done as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分将重点介绍图像处理和操作。这需要一些额外的库，可能你还没有安装。此时，最好安装所有常见的科学计算所需的包，可以按以下方式完成：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Additionally, install the image processing library, some external matrix mathematics
    libraries, and underlying requirements, which can be done as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，安装图像处理库、一些外部矩阵数学库及其底层依赖，方法如下：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Converting images to matrices
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将图像转换为矩阵
- en: Much of machine learning is just operations on matrices. We will start that
    process next by converting our images into a series of matrices—essentially, a
    3D matrix as wide as the number of images we have.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的很多工作其实就是在矩阵上进行操作。接下来，我们将通过将图像转换成一系列矩阵来开始这个过程——本质上是一个3D矩阵，其宽度与我们拥有的图像数量相同。
- en: Almost all matrix operations that we will perform in this chapter, and the entire
    book, use NumPy—the most popular scientific computing package in the Python landscape.
    NumPy is available at [http://www.numpy.org/](http://www.numpy.org/). You should
    install it before running the next series of operations.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本章及全书中几乎所有的矩阵操作都将使用NumPy——Python科学计算领域最流行的库。你可以在[http://www.numpy.org/](http://www.numpy.org/)找到NumPy。你应该在运行接下来的操作之前安装它。
- en: 'The following code opens images and creates the matrices of data (note the
    three extra imports now required):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码打开图像并创建数据矩阵（请注意现在需要额外的三个导入）：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We have our extracted files from the previous section. Now, we can simply run
    this procedure on all our extracted images, as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经从上一节中提取了文件。现在，我们可以对所有提取的图像执行这个过程，代码如下：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The procedure essentially loads letters into a matrix that looks something
    like this:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程基本上将字母加载到一个类似于这样的矩阵中：
- en: '![](img/a75199c9-3913-41b5-9d5a-69e450dd4ee2.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a75199c9-3913-41b5-9d5a-69e450dd4ee2.png)'
- en: 'However, a peek into the matrix reveals more subtlety. Go ahead and take a
    look by printing an arbitrary layer on the stack (for example, `np.set_printoptions(precision=2);
    print (dataset[47]`). You will find a matrix not of bits, but of floating point
    numbers:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，观察矩阵会揭示出更多微妙之处。可以通过打印堆栈中的某一层来查看（例如，`np.set_printoptions(precision=2); print(dataset[47])`）。你会发现一个不是由位组成的矩阵，而是由浮点数组成的：
- en: '![](img/eed77f16-0ef9-4f7c-8cef-8a2501e7fda2.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eed77f16-0ef9-4f7c-8cef-8a2501e7fda2.png)'
- en: 'The images first get loaded into a matrix of values 0 to 255:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图像首先会被加载到一个值范围为0到255的矩阵中：
- en: '![](img/21972f82-50be-496d-b548-73f8ec47175d.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21972f82-50be-496d-b548-73f8ec47175d.png)'
- en: 'These get scaled down to numbers between -0.5 and 0.5, we will revisit the
    reasons why later. We will end up with a stack of images that looks like this:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像将被缩放到-0.5到0.5之间，稍后我们将回顾为什么这样做。最后，我们将得到一个看起来像这样的图像堆栈：
- en: '![](img/9c07636d-2cf7-4b1b-8bbf-75c312f2bef9.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c07636d-2cf7-4b1b-8bbf-75c312f2bef9.png)'
- en: These are all greyscale images, so we will deal with just one layer. We'll deal
    with color images in future chapters; in those cases, each photo will have a matrix
    of height three and a separate matrix for red, green, and blue.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是灰度图像，因此我们只处理一个层级。我们将在后续章节处理中彩色图像；在那些情况下，每张照片将具有三行的矩阵，并且分别包含红色、绿色和蓝色的矩阵。
- en: Logical stopping points
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合理的停止点
- en: Downloading our training file took a long time. Even extracting all the images
    took a while. To avoid repeating all this, we will try to do all the work just
    once and then create **pickle files**—archives of the Python data structures.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 下载我们的训练文件花费了很长时间。即使提取所有图像也花费了一些时间。为了避免重复这些步骤，我们将尽量只做一次所有的工作，然后创建**pickle文件**——这些是Python数据结构的存档。
- en: 'The following procedure runs through each class in our training and test set
    and creates a separate `pickle` file for each. In future runs, we''ll just begin
    from here:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下过程会遍历我们的训练集和测试集中的每个类，并为每个类创建一个单独的`pickle`文件。在以后的运行中，我们将直接从这里开始：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `Pickle` files are essentially persistable and reconstitutable dumps of
    dictionaries.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pickle` 文件本质上是可持久化并可重建的字典转储。'
- en: The machine learning briefcase
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习公文包
- en: We just created nice, clean, `pickle` files with preprocessed images to train
    and test our classifier. However, we've ended up with 20 `pickle` files. There
    are two problems with this. First, we have too many files to keep track of easily.
    Secondly, we've only completed part of our pipeline, where we've processed our
    image sets but have not prepared a TensorFlow consumable file.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚创建了漂亮、干净的`pickle`文件，里面是经过预处理的图像，用于训练和测试我们的分类器。然而，我们最终得到了20个`pickle`文件。这有两个问题。首先，我们有太多文件，难以轻松管理。其次，我们只完成了管道的一部分，即处理了我们的图像集，但还没有准备好一个TensorFlow可用的文件。
- en: Now we will need to create our three major sets—the training set, the validation
    set, and the test set. The training set will be used to nudge our classifier,
    while the validation set will be used to gauge progress on each iteration. The
    test set will be kept secret until the end of the training, at which point, it
    will be used to test how well we've trained the model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要创建三个主要的数据集——训练集、验证集和测试集。训练集将用于训练我们的分类器，验证集用于评估每次迭代的进展。测试集将在训练结束后保密，届时会用来测试我们训练模型的效果。
- en: 'The code to do all this is long, so we''ll leave you to review the Git repository.
    Pay close attention to the following three functions:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这些操作的代码比较长，所以我们将让你自行查看 Git 仓库。请特别注意以下三个函数：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'These three complete our pipeline methods. But, we will still need to use the
    pipeline. To do so, we will first define our training, validation, and test sizes.
    You can change this, but you should keep it less than the full size available,
    of course:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这三部分完成了我们的管道方法。但是，我们仍然需要使用这个管道。为此，我们首先将定义训练、验证和测试集的大小。你可以更改这些参数，但当然应该保持它们小于可用的完整大小：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'These sizes will then be used to construct merged (that is, combining all our
    classes) datasets. We will pass in the list of `pickle` files to source our data
    from and get back a vector of labels and a matrix stack of images. We will finish
    by shuffling our datasets, as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这些尺寸将用于构建合并后的（即，合并我们所有的类别）数据集。我们将传入`pickle`文件的列表作为数据源，并获取一个标签向量和一个图像矩阵堆栈。最后，我们会打乱数据集，如下所示：
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can peek into our newly-merged datasets as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式查看我们新合并的数据集：
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Whew! That was a lot of work we do not want to repeat in the future. Luckily,
    we won''t have to, because we''ll re-pickle our three new datasets into a single,
    giant, `pickle` file. Going forward, all learning will skip the preceding steps
    and work straight off the giant `pickle`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀！这真是太费劲了，我们以后不想再重复这些工作了。幸运的是，我们不必再做这些，因为我们会将三个新数据集重新打包成一个巨大的`pickle`文件。今后，所有的学习都将跳过前面的步骤，直接从这个巨大的`pickle`文件开始：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The ideal way to feed the matrices into TensorFlow is actually as a one-dimensional
    array; so, we''ll reformat our 28x28 matrices into strings of 784 decimals. For
    that, we''ll use the following `reformat` method:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 将矩阵输入 TensorFlow 的理想方式其实是作为一维数组；因此，我们将把28x28的矩阵重新格式化为784个小数的字符串。为此，我们将使用以下`reformat`方法：
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Our images now look like this, with a row for every image in the training,
    validation, and test sets:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的图像现在看起来是这样的，每一行代表训练集、验证集和测试集中的一张图像：
- en: '![](img/c59f15a1-8a00-492b-a94c-4333666cbe79.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c59f15a1-8a00-492b-a94c-4333666cbe79.png)'
- en: 'Finally, to open up and work with the contents of the `pickle` file, we will
    simply read the variable names chosen earlier and pick off the data like a hashmap:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了打开并操作`pickle`文件的内容，我们只需读取之前选定的变量名，并像操作哈希图一样提取数据：
- en: '[PRE18]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Training day
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练日
- en: 'Now, we arrive at the fun part—the neural network. The complete code to train
    this model is available at the following link: [https://github.com/mlwithtf/mlwithtf/blob/master/chapter_02/training.py](https://github.com/mlwithtf/mlwithtf/blob/master/chapter_02/training.py)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来到了有趣的部分——神经网络。训练这个模型的完整代码可以通过以下链接查看：[https://github.com/mlwithtf/mlwithtf/blob/master/chapter_02/training.py](https://github.com/mlwithtf/mlwithtf/blob/master/chapter_02/training.py)
- en: 'To train the model, we''ll import several more modules:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型，我们将导入更多的模块：
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, we will define a few parameters for the training process:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将定义一些用于训练过程的参数：
- en: '[PRE20]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After that, we will use the `data_utils` package to load the dataset that was
    downloaded in the previous section:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将使用`data_utils`包加载前一部分中下载的数据集：
- en: '[PRE21]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We''ll start off with a fully-connected network. For now, just trust the network
    setup (we''ll jump into the theory of setup a bit later). We will represent the
    neural network as a graph, called `graph` in the following code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个完全连接的网络开始。目前，只需相信网络的设置（我们稍后会深入探讨网络的设置理论）。我们将在下面的代码中将神经网络表示为一个图，称为`graph`：
- en: '[PRE22]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `loss` function that is used to train the model is also an important factor
    in this process:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练模型的`loss`函数也是这个过程中的一个重要因素：
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This is the optimizer being used (Stochastic Gradient Descent) along with the
    `learning_rate (0.3)` and the function we're trying to minimize (softmax with
    cross-entropy).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用的优化器（随机梯度下降），以及`learning_rate (0.3)`和我们试图最小化的函数（带交叉熵的softmax）。
- en: 'The real action, and the most time-consuming part, lies in the next and final
    segment—the training loop:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 真实的操作，且最耗时的部分，发生在接下来的最终阶段——训练循环：
- en: '![](img/4ed292fc-aeb9-4402-bf14-a09bcc8a5085.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ed292fc-aeb9-4402-bf14-a09bcc8a5085.png)'
- en: 'We can run this training process using the following command in the `chapter_02`
    directory:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在`chapter_02`目录中使用以下命令运行这个训练过程：
- en: '[PRE24]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Running the procedure produces the following output:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 运行该过程会生成以下输出：
- en: '![](img/feedd131-0973-44c5-9ce0-fd1f31c7ff75.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/feedd131-0973-44c5-9ce0-fd1f31c7ff75.png)'
- en: 'We are running through hundreds of cycles and printing indicative results once
    every 500 cycles. Of course, you are welcome to modify any of these settings.
    The important part is to appreciate the cycle:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在进行数百次的循环，并且每500次循环打印一次指示性结果。当然，你可以修改任何这些设置。重要的是要理解循环的过程：
- en: We will cycle through the process many times.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将多次循环执行这个过程。
- en: Each time, we will create a mini batch of photos, which is a carve-out of the
    full image set.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次，我们都会创建一个小批量的照片，这些照片是完整图像集的一个切割部分。
- en: Each step runs the TensorFlow session and produces a loss and a set of predictions.
    Each step additionally makes a prediction on the validation set.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一步都会运行TensorFlow会话，产生一个损失值和一组预测结果。每一步还会对验证集进行预测。
- en: At the end of the iterative cycle, we will make a final prediction on our test
    set, which is a secret up until now.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在迭代周期的最后，我们将在之前一直保密的测试集上做最终预测。
- en: For each prediction made, we will observe our progress in the form of prediction
    accuracy.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每次做出的预测，我们将通过预测准确度来观察我们的进展。
- en: 'We did not discuss the `accuracy` method earlier. This method simply compares
    the predicted labels against known labels to calculate a percentage score:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前没有讨论`accuracy`方法。这个方法简单地将预测标签与已知标签进行比较，从而计算出一个百分比得分：
- en: '[PRE25]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Just running the preceding classifier will yield accuracy in the general range
    of 85%. This is remarkable because we have just begun! There are much more tweaks
    that we can continue to make.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅运行前面的分类器就能获得大约85%的准确率。这是相当了不起的，因为我们才刚刚开始！我们仍然可以继续进行更多的调整。
- en: Saving the model for ongoing use
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为了持续使用，保存模型
- en: 'To save variables from the TensorFlow session for future use, you can use the
    `Saver()` function, which is as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将TensorFlow会话中的变量保存以便将来使用，你可以使用`Saver()`函数，代码如下：
- en: '[PRE26]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Later, you can retrieve the state of the model and avoid tedious retraining
    by restoring the following checkpoint:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，你可以通过恢复以下检查点来获取模型的状态，避免繁琐的重新训练：
- en: '[PRE27]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Why hide the test set?
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要隐藏测试集？
- en: Notice how we did not use the test set until the last step. Why not? This is
    a pretty important detail to ensure that the test remains a good one. As we iterate
    through the training set and nudge our classifier one way or another, we can sometimes
    *wrap the classifier* around the images or overtrain. This happens when you learn
    the training set rather than learn the features inside each of the classes.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们直到最后一步才使用测试集。为什么不早点使用？这是一个相当重要的细节，确保测试集能够保持良好。随着我们在训练集上迭代并微调分类器的行为，我们有时会*将分类器包裹在图像中*或过度训练。当你学习训练集而非每个类别内的特征时，这种情况就会发生。
- en: When we overtrain, our accuracy on the iterative rounds of the training set
    will look promising, but that is all false hope. Having a never-before-seen test
    set should introduce reality back into the process. Great accuracy on the training
    set followed by poor results on the test set suggests overfitting.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们过度训练时，训练集的迭代轮次中的准确度看起来很有希望，但那完全是虚假的希望。拥有一个从未见过的测试集应该将现实带回到过程当中。在训练集上表现出色却在测试集上结果不佳，通常意味着过拟合。
- en: This is why we've kept a separate test set. It helps indicate the real accuracy
    of our classifier. This is also why you should never shuffle your dataset or intermingle
    the dataset with the test set.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们保留了一个单独的测试集。它帮助指示分类器的真实准确性。这也是为什么你绝对不应将数据集混合或与测试集打乱的原因。
- en: Using the classifier
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用分类器
- en: We will demonstrate the usage of the classifier with `notMNIST_small.tar.gz`,
    which becomes the test set. For ongoing use of the classifier, you can source
    your own images and run them through a similar pipeline to test, not train.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`notMNIST_small.tar.gz`来演示分类器的使用，它将作为测试集。对于分类器的持续使用，你可以自己提供图像，并通过类似的管道对其进行测试，而非训练。
- en: You can create some 28x28 images yourself and place them into the test set for
    evaluation. You will be pleasantly surprised!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以自己创建一些28x28的图像，并将它们放入测试集中进行评估。你会感到惊喜的！
- en: The practical issue with field usage is the heterogeneity of images in the wild.
    You may need to find images, crop them, downscale them, or perform a dozen other
    transformations. This all falls into the usage pipeline, which we discussed earlier.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，遇到的一个问题是野外图像的异质性。你可能需要找到图像、裁剪它们、缩小它们，或者进行其他一系列变换。这些都属于我们之前讨论过的使用管道的一部分。
- en: Another technique to cover larger images, such as finding a letter on a page-sized
    image, is to slide a small window across the large image and feed every subsection
    of the image through the classifier.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种处理大图像的技术是，在图像上滑动一个小窗口，并将图像的每个子部分输入分类器进行分类，这适用于像在页面尺寸的图像上查找字母这样的任务。
- en: We'll be taking our models into production in future chapters but, as a preview,
    one common setup is to move the trained model into a server on the cloud. The
    façade of the system might be a smartphone app that takes photos and sends them
    off for classification behind the scenes. In this case, we will wrap our entire
    program with a web service to accept incoming classification requests and programmatically
    respond to them. There are dozens of popular setups and we will explore several
    of them in [Chapter 9](b38dd75a-b632-4e7b-b581-202500f4e001.xhtml), *Cruise Control
    -Automation*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的章节中，我们将把模型投入生产环境，但作为预览，一种常见的配置是将训练好的模型迁移到云端服务器上。系统的外观可能是一个智能手机应用程序，用户拍照后，照片会被发送到后台进行分类。在这种情况下，我们会用一个网络服务将整个程序包装起来，接受传入的分类请求并进行程序化响应。流行的配置有很多，我们将在[第9章](b38dd75a-b632-4e7b-b581-202500f4e001.xhtml)中探讨其中的几个，*巡航控制
    - 自动化*。
- en: Deep diving into the network
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探索神经网络
- en: 'Notice how we achieved 86% accuracy. This is a great result for two hours of
    work, but we can do much better. Much of the future potential is in changing the
    neural network. Our preceding application used a **fully-connected** setup, where
    each node on a layer is connected to each node on the previous layer and looks
    like this:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们是如何达到86%的准确率的。对于仅仅两小时的工作，这个结果已经很不错，但我们能做得更好。未来的潜力大部分来自于改变神经网络。我们之前的应用使用的是**全连接**设置，在这种设置中，每一层的节点都与上一层的节点相连，形状如下：
- en: '![](img/0be54afc-3577-4dfb-b045-2e916734b1d2.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0be54afc-3577-4dfb-b045-2e916734b1d2.png)'
- en: As you will learn with more complex network setups in coming chapters, this
    setup is fast but not ideal. The biggest issue is the large number of parameters,
    which can cause overfitting of the model on the training data.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你将在后续章节中学到的，随着网络结构变得更复杂，这种设置虽然快速，但并不理想。最大的问题是参数的数量庞大，这可能导致模型在训练数据上过拟合。
- en: Skills learned
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学到的技能
- en: 'You should have learned these skills in the chapter:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该在本章中学到了这些技能：
- en: Preparing training and test data
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备训练数据和测试数据
- en: Creating a training set consumable by TensorFlow
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个TensorFlow可消费的训练集
- en: Setting up a basic neural network graph
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置基本的神经网络图
- en: Training the TensorFlow classifier
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练TensorFlow分类器
- en: Validating the classifier
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证分类器
- en: Piping in real-world data
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入真实世界数据
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Superb progress! We just built a handwriting classifier that would have been
    world class a decade ago. Also, we built an entire pipeline around the process
    to fully automate the training setup and execution. This means that our program
    can be migrated to almost any server and continue to function almost turn-key.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 进展非常顺利！我们刚刚构建了一个十年前也可以被认为是世界级的手写字分类器。此外，我们围绕这个过程构建了一个完整的管道，能够完全自动化训练设置和执行。这意味着我们的程序几乎可以迁移到任何服务器上，并继续正常运行，几乎是开箱即用。
