- en: '*Chapter 18*: Building Secure, Reliable, and Efficient NLP Solutions'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第18章*：构建安全、可靠和高效的NLP解决方案'
- en: Thank you, dear reader, for staying with us through this (hopefully informative)
    journey in building best-in-class **Natural Language Processing** (**NLP**) solutions
    for organizations looking to uncover insights from their text data. Our aim in
    writing this book was to create awareness that **Artificial Intelligence** (**AI**)
    is mainstream and that we are at the cusp of a huge tidal wave of AI adoption
    that many enterprises are moving toward. This exciting technology not only helps
    you advance your career but also provides opportunities to explore new avenues
    of innovation that were not possible previously. For example, according to a BBC
    article ([https://www.bbc.com/future/article/20181129-the-ai-transforming-the-way-aircraft-are-built](https://www.bbc.com/future/article/20181129-the-ai-transforming-the-way-aircraft-are-built)),
    **Autodesk** ([https://www.autodesk.com/](https://www.autodesk.com/)), a global
    leader in designing and making technology, uses **Generative AI** ([https://www.amazon.com/Generative-AI-Python-TensorFlow-Transformer/dp/1800200889](https://www.amazon.com/Generative-AI-Python-TensorFlow-Transformer/dp/1800200889))
    to help aircraft manufacturers design more efficient airframes, a key requirement
    to reduce fuel consumption.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢亲爱的读者在这段（希望对您有帮助的）旅程中陪伴我们，帮助组织构建最佳的**自然语言处理**（**NLP**）解决方案，以从文本数据中发现洞察。本书的写作目的是让人们意识到，**人工智能**（**AI**）已经成为主流，我们正处于一个巨大的人工智能采用浪潮的边缘，许多企业都在朝着这个方向前进。这项激动人心的技术不仅能帮助你提升职业生涯，还为你提供了探索以前无法实现的创新领域的机会。例如，根据BBC的一篇文章（[https://www.bbc.com/future/article/20181129-the-ai-transforming-the-way-aircraft-are-built](https://www.bbc.com/future/article/20181129-the-ai-transforming-the-way-aircraft-are-built)），**Autodesk**（[https://www.autodesk.com/](https://www.autodesk.com/)）这家全球领先的设计和制造技术公司，使用**生成式人工智能**（[https://www.amazon.com/Generative-AI-Python-TensorFlow-Transformer/dp/1800200889](https://www.amazon.com/Generative-AI-Python-TensorFlow-Transformer/dp/1800200889)）帮助飞机制造商设计更高效的机身框架，这是减少燃油消耗的关键要求。
- en: Throughout this book, we reviewed several types of use cases that enterprises
    deal with today to garner useful information from text-based data. This information
    will either directly help them derive insights or serve as a precursor to driving
    downstream decision-making, with operational implications in both cases. We read
    through different business scenarios and discussed different solution design approaches,
    architecture implementation frameworks', and real-time and batch solutions that
    met these requirements.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们回顾了企业今天处理的几种用例类型，旨在从基于文本的数据中获取有用信息。这些信息要么直接帮助他们提取洞察，要么作为推动下游决策的前兆，在这两种情况下都具有操作性影响。我们阅读了不同的商业场景，讨论了满足这些需求的不同解决方案设计方法、架构实施框架，以及实时和批处理解决方案。
- en: We now understand how to design, architect, and build NLP solutions with **AWS**
    **AI** **services**, but we are still missing an important step. How do we ensure
    our solution is production-ready? What are the operational requirements to ensure
    our solution works as expected when dealing with real-world data? What are the
    non-functional requirements that the architecture needs to adhere to? And how
    do we build this into the solution as we go along? To answer these questions,
    we will review the best practices, techniques, and guidance on what makes a good
    NLP solution great.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解如何使用**AWS** **AI** **服务**设计、架构和构建NLP解决方案，但我们仍然缺少一个重要步骤。我们如何确保我们的解决方案已经准备好投入生产？在处理真实世界数据时，确保解决方案按预期工作需要满足哪些操作要求？架构需要遵守哪些非功能性要求？我们该如何在实现过程中将这些融入解决方案？为了回答这些问题，我们将回顾最佳实践、技巧和指导，阐明如何让一个优秀的NLP解决方案更加出色。
- en: 'In this chapter, we will discuss the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Defining best practices for NLP solutions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义NLP解决方案的最佳实践
- en: Applying best practices for optimization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用最佳实践进行优化
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For this chapter, you will need access to an AWS account ([https://aws.amazon.com/console/](https://aws.amazon.com/console/)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，您需要访问一个AWS账户（[https://aws.amazon.com/console/](https://aws.amazon.com/console/)）。
- en: Please refer to the *Signing up for an AWS account* sub-section within the *Setting
    up your AWS environment* section in [*Chapter 2*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027),
    *Introducing Amazon Textract*, for detailed instructions on how you can sign up
    for an AWS account and sign in to the **AWS** **Management Console**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅 [*第2章*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027) 中 *设置您的 AWS 环境*
    部分的 *注册 AWS 账户* 子部分，详细了解如何注册 AWS 账户并登录到**AWS** **管理控制台**的说明。
- en: Defining best practices for NLP solutions
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义自然语言处理（NLP）解决方案的最佳实践。
- en: 'If you have done DIY (do-it-yourself) projects in the past, you know how important
    your tools are for your work. When building an NLP solution, or any solution for
    that matter, you need to keep the following in mind:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您以前做过 DIY（自己动手）项目，您就知道工具对您的工作有多重要。在构建自然语言处理解决方案或任何解决方案时，您需要牢记以下几点：
- en: You need to know your requirements (the "*what*").
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要了解您的需求（"*what*"）。
- en: You need to know the problem that you are trying to solve by building the solution
    (the "*why*").
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要了解构建解决方案解决的问题（"*why*"）。
- en: You need to know the tools and techniques required to build the solution (the
    "*how*").
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要了解构建解决方案所需的工具和技术（"*how*"）。
- en: You need to estimate the time you require to build the solution (the "*when*").
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要估计构建解决方案所需的时间（"*when*"）。
- en: Finally, you need to determine the required skills for the team (the "*who*").
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，您需要确定团队所需的技能（"*who*"）。
- en: But with this approach, you haven't necessarily addressed the needs that will
    make your solution reliable, scalable, efficient, secure, or cost-effective. And
    these are equally important (if not more so) to building long-lasting solutions
    that will delight your customers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，采用这种方法，您未必能够解决使解决方案可靠、可扩展、高效、安全或具有成本效益的需求。而这些同样重要（甚至更重要），以构建能够让客户满意的长久解决方案。
- en: When building with AWS, you have access to prescriptive guidance and valuable
    insights garnered from decades of building and operating highly performant, massive-scale
    applications such as **Amazon**, along with the expertise that comes from helping
    some of the world's largest enterprises with running their cloud workloads on
    AWS. All of this experience and knowledge has been curated into a collection of
    architectural guidelines and best practices called the **AWS Well-Architected
    Framework**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建使用 AWS 时，您可以获得几十年构建和运营高性能、大规模应用程序（如**亚马逊**）的经验中获得的指导和宝贵见解，以及帮助全球最大企业在 AWS
    上运行其云工作负载的专业知识。所有这些经验和知识都已整理成一套架构指南和最佳实践，称为**AWS Well-Architected Framework**。
- en: 'Think of *well-architected* as a comprehensive checklist of questions that
    is defined by five pillars, as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 将*良构建*视为一个由五个支柱定义的全面问题清单。
- en: '**Operational excellence**: The operational excellence pillar recommends automating
    infrastructure provisioning and management (if applicable), modularizing the solution
    architecture into components that can be managed independently, enabling agility,
    implementing CI/CD-based DevOps practices, simulating operational failures, and
    preparing for and learning from it.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运营卓越**：运营卓越支柱建议自动化基础设施的提供和管理（如果适用），将解决方案架构模块化为可以独立管理的组件，实现敏捷性，实施基于 CI/CD
    的 DevOps 实践，模拟运营故障，并为其做准备并从中学习。'
- en: '**Security**: The security pillar recommends making security the top priority
    by implementing least privilege governance measures and associated guardrails
    from the ground up with a focus on identity and access management, compute and
    network security, data protection in transit and at rest, automation, simulation,
    and incident response.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：安全性支柱建议通过从最基础的开始实施最少特权治理措施和相关的保护措施，注重身份和访问管理，计算和网络安全，数据在传输和静态状态下的保护，自动化，模拟和事件响应，将安全置于首要位置。'
- en: '**Reliability**: The reliability pillar requires the setting up of highly resilient
    architectures with the ability to self-heal from failures, with a focus on fail-fast
    and recovery testing, elastic capacity with auto scale-in/scale-out, and a high
    degree of automation.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可靠性**：可靠性支柱要求建立高度弹性的架构，能够从故障中自我修复，注重快速失败和恢复测试，具有自动缩放的弹性能力，并且高度自动化。'
- en: '**Performance efficiency**: The performance efficiency pillar recommends the
    use of **AWS Managed Services** (**AMS**) to remove the undifferentiated heavy
    lifting associated with managing infrastructure, using the global AWS network
    to reduce latency for your end users and remove the need for repeated experimentation
    and decoupling resource interactions by means of APIs.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能效率**：性能效率支柱建议使用**AWS 管理服务**（**AMS**），以去除与管理基础设施相关的无差异重负担，利用全球 AWS 网络减少终端用户的延迟，并通过
    API 去除重复实验和资源交互的耦合需求。'
- en: '**Cost optimization**: The cost optimization pillar provides recommendations
    on measures you can take to track and minimize usage costs.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本优化**：成本优化支柱提供有关你可以采取的措施的建议，帮助你追踪并最小化使用成本。'
- en: 'For more details on the *Well-Architected Framework*, along with the resources
    to get you started, please go to the Amazon documentation: [https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html](https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多有关*良好架构框架*的详细信息，以及帮助你入门的资源，请访问 Amazon 文档：[https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html](https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html)
- en: Taken together, the Well-Architected questions from the various pillars guide
    you through your architecture design, build, and implementation. This enables
    you to include critical design principles, resulting in solutions that are built
    for secure, reliable, cost-effective, and efficient operations (and hence the
    term Well-Architected). So, what does Well-Architected mean in our case with regard
    to NLP solutions and AI services? To help you understand this clearly, we will
    create a matrix of the Well-Architected pillars, aligned with NLP development
    stages such as document preprocessing, prediction, and post-processing, from the
    perspective of the major AWS AI services we used in this book (**Amazon** **Textract**
    and **Amazon** **Comprehend**). We will also look at the application of principles
    for NLP solution builds in general. In each cell of this matrix, we will summarize
    how to apply the Well-Architected principles for design and implementation using
    best practices.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从整体来看，各个支柱的良好架构问题引导你完成架构设计、构建和实施。这使得你能够融入关键设计原则，构建出安全、可靠、经济高效且高效运作的解决方案（因此得名“良好架构”）。那么，在我们的案例中，良好架构对于
    NLP 解决方案和 AI 服务意味着什么呢？为了帮助你清楚地理解这一点，我们将创建一个良好架构支柱矩阵，结合 NLP 开发阶段，如文档预处理、预测和后处理，并从我们在本书中使用的主要
    AWS AI 服务（**Amazon** **Textract** 和 **Amazon** **Comprehend**）的角度进行说明。我们还将探讨在构建
    NLP 解决方案时普遍应用的原则。在矩阵的每个单元格中，我们将总结如何使用最佳实践应用良好架构原则进行设计和实施。
- en: '![Figure 18.1 – Well-Architected NLP solutions matrix'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 18.1 – 良好架构的 NLP 解决方案矩阵'
- en: '](img/B17528_18_01.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17528_18_01.jpg)'
- en: Figure 18.1 – Well-Architected NLP solutions matrix
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.1 – 良好架构的 NLP 解决方案矩阵
- en: As you can see from the preceding matrix, there are a number of design principles
    you can adopt during your solution development to build efficient and secure NLP
    applications. For the sake of clarity, we separated these principles based on
    the Well-Architected Framework pillars and the NLP development stages. However,
    as you might have noticed, some of these principles are repetitive across the
    cells. This is because an application of a principle for a particular pillar may
    automatically also address the needs of a different pillar based on what principle
    we refer to. For example, when using **AWS** **Glue** **ETL** jobs for document
    pre-processing and post-processing tasks, our Well-Architected needs for operational
    excellence, cost optimization, and performance efficiency are addressed without
    the need to do anything else.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的矩阵中可以看出，在解决方案开发过程中，你可以采用一些设计原则来构建高效且安全的 NLP 应用。为了明确起见，我们将这些原则根据良好架构框架支柱和
    NLP 开发阶段分开。但是，正如你可能注意到的，有些原则在各个单元格中是重复的。这是因为某个支柱原则的应用可能会自动解决其他支柱的需求，这取决于我们所参考的原则。例如，当使用**AWS**
    **Glue** **ETL** 作业进行文档预处理和后处理任务时，我们的良好架构需求——包括运营卓越、成本优化和性能效率，都会得到满足，而无需做其他额外工作。
- en: We will explain the reason for this in more detail in the next section. We introduced
    the AWS Well-Architected Framework in this section and reviewed a matrix of how
    the Well-Architected principles can be applied to the AWS AI services we used
    throughout this book. In the next section, we will delve deeper and discuss how
    to implement some of the principles from the matrix.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中更详细地解释这一原因。本节中我们介绍了AWS Well-Architected框架，并回顾了如何将Well-Architected原则应用于本书中使用的AWS
    AI服务的矩阵。在下一节中，我们将深入探讨并讨论如何实现矩阵中的一些原则。
- en: Applying best practices for optimization
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用优化的最佳实践
- en: In this section, we will dive a bit deeper into what each of the design principles
    that we documented in the Well-Architected NLP solutions matrix means, and how
    to implement them for your requirements. Since the scope of this book is primarily
    about AWS AI services, we already have the advantage of using serverless managed
    services, and this addresses a number of suggestions that the Well-Architected
    Framework alludes to. Additionally, as mentioned previously, some of the best
    practices documented in the matrix may appear to be repetitive – this is not a
    mistake but intentional, as the application of one design pattern may have a cascading
    benefit across multiple pillars of the Well-Architected Framework. We will highlight
    these as we come across them. Without further ado, let's dig in.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨我们在《Well-Architected NLP解决方案矩阵》中记录的每个设计原则的含义，以及如何根据您的需求实施这些原则。由于本书的范围主要是关于AWS
    AI服务，我们已经有了使用无服务器托管服务的优势，这解决了《Well-Architected框架》中提到的多个建议。此外，如前所述，矩阵中记录的一些最佳实践可能看起来有些重复——这并不是错误，而是有意为之，因为应用某一种设计模式可能会在《Well-Architected框架》的多个支柱中产生级联效益。我们将在遇到时重点说明这些内容。话不多说，让我们开始深入探讨。
- en: Using an AWS S3 data lake
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用AWS S3数据湖
- en: This section addresses principles *1.1a* and *1.3a* from the Well-Architected
    NLP solutions matrix (*Figure 18.1*).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涉及《Well-Architected NLP解决方案矩阵》中的原则*1.1a*和*1.3a*（*图18.1*）。
- en: A **data lake** is a repository for structured, semi-structured, and unstructured
    data. Initially, it is a collection of data from disparate sources within the
    enterprise and it serves as the data source for downstream analytics, business
    intelligence, **machine learning** (**ML**), and operational needs. However, since
    the data hydrated into a data lake retains its source format (data is not transformed
    before loading into the data lake), the data needs to undergo transformation at
    the time of consumption from the data lake. Building a data lake using **Amazon
    S3** (a fully managed object storage solution in the AWS cloud) makes a lot of
    sense because it scales as much as you want, and data stored in S3 is highly durable
    (for more information, see the section on the *"11 9s"* of durability at ([https://aws.amazon.com/s3/faqs/](https://aws.amazon.com/s3/faqs/)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据湖**是一个用于存储结构化、半结构化和非结构化数据的仓库。最初，它是来自企业不同来源的数据集合，并作为下游分析、商业智能、**机器学习**（**ML**）和运营需求的数据来源。然而，由于数据加载到数据湖时保持其源格式（数据在加载到数据湖之前不会进行转换），因此在从数据湖中获取数据时，数据需要进行转换。使用**Amazon
    S3**（AWS云中的完全托管对象存储解决方案）构建数据湖非常有意义，因为它可以根据需要进行扩展，并且存储在S3中的数据具有高度耐久性（更多信息，请参见关于*“11个9s”*耐久性的部分（[https://aws.amazon.com/s3/faqs/](https://aws.amazon.com/s3/faqs/)）。'
- en: Furthermore, AWS provides a number of ways in which you can get your data into
    S3, and several options to read data from S3, transform it, and feed it to your
    consumers for whatever needs you have, and all of these steps are carried out
    in a highly secure manner. To read in detail about creating a data lake on S3,
    hydrating it with your data sources, creating a data catalog, and securing, managing,
    and transforming the data, please refer to the *Building Big Data Storage Solutions*
    white paper ([https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/building-data-lake-aws.html](https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/building-data-lake-aws.html)).
    For instructions on how to set up your data lake using **AWS** **Lake Formation**
    (a fully managed service to build and manage data lakes), please refer to the
    getting started guide ([https://docs.aws.amazon.com/lake-formation/latest/dg/getting-started.html](https://docs.aws.amazon.com/lake-formation/latest/dg/getting-started.html)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，AWS提供了多种方式，帮助你将数据导入到S3，并提供几种选项，帮助你从S3读取数据、转换数据并将其提供给消费者，以满足你的各种需求，所有这些步骤都以高度安全的方式进行。有关如何在S3上创建数据湖、将数据源注入数据湖、创建数据目录以及如何安全管理和转换数据的详细信息，请参考*《构建大数据存储解决方案》*白皮书（[https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/building-data-lake-aws.html](https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/building-data-lake-aws.html)）。有关如何使用**AWS**
    **Lake Formation**（一个用于构建和管理数据湖的完全托管服务）设置数据湖的说明，请参考入门指南（[https://docs.aws.amazon.com/lake-formation/latest/dg/getting-started.html](https://docs.aws.amazon.com/lake-formation/latest/dg/getting-started.html)）。
- en: Let's now discuss how to use AWS Glue for data processing.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论如何使用AWS Glue进行数据处理。
- en: Using AWS Glue for data processing and transformation tasks
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用AWS Glue进行数据处理和转换任务
- en: This section addresses principles *1.4a*, *1.5a*, *3.1a*, *3.4b*, and *3.5a*
    from the Well-Architected NLP solutions matrix (*Figure 18.1*).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了《Well-Architected NLP解决方案矩阵》中的原则*1.4a*、*1.5a*、*3.1a*、*3.4b*和*3.5a*（*图18.1*）。
- en: AWS Glue is a fully managed and serverless data cataloging, processing, and
    transformation service that enables you to build end-to-end ETL pipelines providing
    ready-made connections to data stores both on-premises and on AWS. The serverless,
    managed nature of AWS Glue removes the costs associated with infrastructure management
    and undifferentiated heavy lifting. AWS Glue enables you to configure connections
    to your data stores and your S3 data lake to directly pull this data for document
    pre-processing. You can use Glue ETL jobs to deliver this data after transformation
    (as required) to an NLP solution pipeline, both for training your custom NLP models
    and for predictions at inference time. This makes your solution more elegant and
    efficient, avoiding the need to create multiple solution components to take care
    of these tasks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue是一个完全托管的无服务器数据目录、处理和转换服务，它使你能够构建端到端的ETL管道，提供现成的连接，以连接本地和AWS上的数据存储。AWS
    Glue的无服务器托管特性消除了与基础设施管理和重复繁重工作相关的成本。AWS Glue使你能够配置与数据存储以及S3数据湖的连接，直接拉取数据进行文档预处理。你可以使用Glue
    ETL作业在数据转换（如有需要）后将数据传送到NLP解决方案管道中，无论是用于训练自定义NLP模型，还是在推理时进行预测。这使得你的解决方案更加优雅和高效，避免了需要创建多个解决方案组件来处理这些任务。
- en: 'AWS Glue ETL jobs can be triggered on-demand or scheduled based on your requirements.
    You can also use it for document post-processing after your NLP solution has completed
    its recognition or classification tasks, for persisting to downstream data stores,
    or for consumption by operational processes that need this data. For a detailed
    demonstration of how AWS Glue can help you, please refer to the following tutorial
    using **AWS** **Glue Studio**, a graphical interface to make it easy to interact
    with Glue when creating and running ETL jobs: [https://docs.aws.amazon.com/glue/latest/ug/tutorial-create-job.html](https://docs.aws.amazon.com/glue/latest/ug/tutorial-create-job.html)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue ETL作业可以根据你的需求按需触发或计划执行。你还可以在NLP解决方案完成识别或分类任务后，使用它进行文档后处理，以将数据持久化到下游数据存储，或供需要这些数据的操作流程使用。有关AWS
    Glue如何帮助你的详细演示，请参考以下使用**AWS** **Glue Studio**的教程，它是一个图形化界面，可以让你在创建和运行ETL作业时更方便地与Glue进行交互：[https://docs.aws.amazon.com/glue/latest/ug/tutorial-create-job.html](https://docs.aws.amazon.com/glue/latest/ug/tutorial-create-job.html)
- en: In the next section, we will review how to use **Amazon** **SageMaker** **Ground
    Truth** for our text labeling tasks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将回顾如何使用**Amazon** **SageMaker** **Ground Truth**来进行文本标注任务。
- en: Using Amazon SageMaker Ground Truth for annotations
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Amazon SageMaker Ground Truth进行标注
- en: This section addresses principle *1.4b* from the Well-Architected NLP solutions
    matrix (*Figure 18.1*).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讲解了 Well-Architected NLP 解决方案矩阵中的原则 *1.4b* (*图 18.1*)。
- en: The accuracy of an NLP model is directly proportional to the quality of the
    labeled data it is based on. Though we primarily use AWS AI services that are
    pre-trained models, we saw quite a few use cases that needed Amazon Comprehend
    custom models for entity recognition and classification tasks. Amazon Comprehend
    custom models use **transfer learning** to train a custom model incrementally
    from its own pre-trained models with data that we provide (that is, data that
    isunique to our business). And so, for these custom training requirements, we
    need to provide high-quality labeled data that influences the accuracy of our
    models. As a best practice, we recommend using Amazon SageMaker Ground Truth ([https://aws.amazon.com/sagemaker/groundtruth/](https://aws.amazon.com/sagemaker/groundtruth/))
    for these labeling tasks. Ground Truth is directly integrated with Amazon Comprehend,
    and all you need to do is point to the location of the Ground Truth manifest when
    you set up your Amazon Comprehend job.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 模型的准确性与其所基于的标注数据的质量成正比。虽然我们主要使用 AWS AI 服务中已预训练的模型，但我们也看到了一些需要使用 Amazon Comprehend
    自定义模型进行实体识别和分类任务的用例。Amazon Comprehend 自定义模型使用**迁移学习**，通过使用我们提供的数据（即对我们业务独特的数据），从其自身的预训练模型中逐步训练自定义模型。因此，对于这些自定义训练需求，我们需要提供高质量的标注数据，这将影响我们模型的准确性。作为最佳实践，我们建议使用
    Amazon SageMaker Ground Truth ([https://aws.amazon.com/sagemaker/groundtruth/](https://aws.amazon.com/sagemaker/groundtruth/))
    来处理这些标注任务。Ground Truth 已与 Amazon Comprehend 直接集成，你只需在设置 Amazon Comprehend 作业时，指向
    Ground Truth 清单的位置。
- en: Ground Truth is a fully managed service, providing easy-to-use capabilities
    for data labeling with options to either use your own private workforce, third-party
    data labelers that you can source from **AWS** **Marketplace**, or with crowdsourced
    public data labelers using **Amazon** **Mechanical Turk**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Ground Truth 是一项完全托管的服务，提供易于使用的数据标注功能，可以选择使用你自己的私有劳动力、来自 **AWS** **Marketplace**
    的第三方数据标注者，或者使用 **Amazon** **Mechanical Turk** 提供的众包公共数据标注者。
- en: 'Ground Truth provides data encryption by default, and it automatically learns
    from the labeling activities conducted by the human labelers by training an ML
    model behind the scenes. This ML model will be applied to automate the labeling
    tasks once a confidence threshold has been reached. Ground Truth provides pre-built
    task templates for various data formats, such as image, text, and video files.
    You can also create custom templates for your own requirements by selecting the
    **Custom** task type when creating a labeling job. Please see the following screenshot
    for different types of text-based labeling tasks supported by Ground Truth:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Ground Truth 默认提供数据加密，并通过在后台训练一个机器学习（ML）模型，自动学习人类标注者执行的标注活动。一旦达到一定的置信度阈值，Ground
    Truth 将应用该 ML 模型来自动化标注任务。Ground Truth 提供了适用于多种数据格式（如图像、文本和视频文件）的预构建任务模板。你还可以通过在创建标注作业时选择**自定义**任务类型，创建适合自己需求的模板。请参见以下截图，了解
    Ground Truth 支持的不同类型的基于文本的标注任务：
- en: '![Figure 18.2 – Types of text labeling tasks in Amazon SageMaker Ground Truth'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 18.2 – Amazon SageMaker Ground Truth 中的文本标注任务类型'
- en: '](img/B17528_18_02.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17528_18_02.jpg)'
- en: Figure 18.2 – Types of text labeling tasks in Amazon SageMaker Ground Truth
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.2 – Amazon SageMaker Ground Truth 中的文本标注任务类型
- en: 'To get started, you create a labeling job, select an S3 location for your dataset,
    specify an **IAM** role (or ask Ground Truth to create one for you), select the
    task category from a list of pre-built templates (or you can select **Custom**
    for your own template), and choose the labeling workforce who will work on your
    request. Please refer to the following documentation for more details on how to
    get started: [https://docs.aws.amazon.com/sagemaker/latest/dg/sms-label-text.html](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-label-text.html)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用，你需要创建一个标注作业，选择数据集的 S3 存储位置，指定 **IAM** 角色（或者让 Ground Truth 为你创建一个），从预构建模板列表中选择任务类别（或者你可以选择
    **自定义** 来创建自己的模板），并选择将处理你请求的标注劳动力。有关如何开始的更多细节，请参考以下文档：[https://docs.aws.amazon.com/sagemaker/latest/dg/sms-label-text.html](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-label-text.html)
- en: Now that we know how to use Ground Truth for annotations, let's review a new
    feature that was launched recently to enable **custom entity recognizer training**
    directly from **PDF** or **Microsoft** **Word** documents.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何使用Ground Truth进行标注，让我们回顾一下最近推出的新功能，它可以直接从**PDF**或**Microsoft** **Word**文档进行**自定义实体识别器训练**。
- en: Using Amazon Comprehend with PDF and Word formats directly
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 直接使用Amazon Comprehend处理PDF和Word格式
- en: This section addresses principle *1.4c* from the Well-Architected NLP solutions
    matrix (*Figure 18.1*).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涉及Well-Architected NLP解决方案矩阵中的原则*1.4c*（*图18.1*）。
- en: 'Note:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：
- en: Amazon Comprehend updated the custom entity recognition feature in September
    2021 to support training and inference with PDF and Word documents directly.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Comprehend在2021年9月更新了自定义实体识别功能，以支持直接使用PDF和Word文档进行训练和推理。
- en: To improve the performance efficiency of your NLP solution pipeline, Amazon
    Comprehend now supports training custom entity recognizers directly from PDF and
    Word document formats, without having to run pre-processing steps to flatten the
    document into a machine-readable format. To use this feature, you follow the same
    steps we specified in [*Chapter 14*](B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162),
    *Auditing Named Entity Recognition Workflows*, to train an Amazon Comprehend custom
    entity recognizer, but with a small difference.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高NLP解决方案管道的性能效率，Amazon Comprehend现在支持直接从PDF和Word文档格式训练自定义实体识别器，而无需执行预处理步骤将文档转换为机器可读的格式。要使用此功能，您可以按照我们在[*第14章*](B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162)中指定的步骤，即*审计命名实体识别工作流*，训练Amazon
    Comprehend自定义实体识别器，但有一个小的不同之处。
- en: 'Note:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：
- en: 'You still need to annotate your entities in the training document and create
    an augmented manifest using Ground Truth. For more details, please refer to the
    instructions in this blog: [https://aws.amazon.com/blogs/machine-learning/custom-document-annotation-for-extracting-named-entities-in-documents-using-amazon-comprehend/](https://aws.amazon.com/blogs/machine-learning/custom-document-annotation-for-extracting-named-entities-in-documents-using-amazon-comprehend/)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 您仍然需要在训练文档中标注实体，并使用Ground Truth创建增强的清单。更多细节，请参考此博客中的说明：[https://aws.amazon.com/blogs/machine-learning/custom-document-annotation-for-extracting-named-entities-in-documents-using-amazon-comprehend/](https://aws.amazon.com/blogs/machine-learning/custom-document-annotation-for-extracting-named-entities-in-documents-using-amazon-comprehend/)
- en: 'Please use the following steps to train and infer from PDF or Word documents
    directly with Amazon Comprehend:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照以下步骤，直接使用Amazon Comprehend从PDF或Word文档中进行训练和推理：
- en: Log in to your AWS Management Console (please refer to the *Technical requirements*
    section for more details) and navigate to the `comprehend` in the **Services**
    search bar.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到您的AWS管理控制台（更多细节请参见*技术要求*部分），并在**服务**搜索栏中输入`comprehend`。
- en: Click on **Custom entity recognition** on the left pane and then click on **Create
    new model**.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧面板中点击**自定义实体识别**，然后点击**创建新模型**。
- en: In the **Model settings** section, provide the name for your model and scroll
    down to the **Data specifications** section to select the **Augmented manifest**
    and **PDF, Word documents** formats for training. Provide the S3 location for
    your augmented manifest. Scroll down to select or create an IAM role and click
    on **Create** to start the training. Once the model is trained, you can run an
    inference using the same steps we discussed in [*Chapter 14*](B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162),
    *Auditing Named Entity Recognition Workflows*. But, provide a PDF or Word document
    as an input instead of a CSV file.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**模型设置**部分，为您的模型提供名称，并向下滚动至**数据规范**部分，选择**增强的清单**和**PDF、Word文档**格式进行训练。提供增强清单的S3位置。向下滚动以选择或创建IAM角色，并点击**创建**以开始训练。一旦模型训练完成，您可以使用我们在[*第14章*](B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162)中讨论的相同步骤进行推理，*审计命名实体识别工作流*。但是，提供PDF或Word文档作为输入，而不是CSV文件。
- en: '![Figure 18.3 – Amazon Comprehend custom entity recognizer training using PDF,
    Word documents'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图18.3 – 使用PDF、Word文档进行Amazon Comprehend自定义实体识别器训练'
- en: '](img/B17528_18_03.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17528_18_03.jpg)'
- en: Figure 18.3 – Amazon Comprehend custom entity recognizer training using PDF,
    Word documents
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.3 – 使用PDF、Word文档进行Amazon Comprehend自定义实体识别器训练
- en: This feature update improves pre-processing efficiency and reduces upfront time
    investments in setting up our NLP solution pipelines. In the next section, we
    will review how to enforce access control when building NLP solutions.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个功能更新提高了预处理效率，并减少了设置我们的NLP解决方案管道时的前期时间投入。在下一部分，我们将回顾如何在构建NLP解决方案时强制执行访问控制。
- en: Enforcing least privilege access
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强制实施最小权限访问
- en: This section addresses principles *1.2c*, *3.2b*, and *4.2a* from the Well-Architected
    NLP solutions matrix (*Figure 18.1*).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了来自“良好架构化NLP解决方案矩阵”（*图18.1*）的原则 *1.2c*、*3.2b* 和 *4.2a*。
- en: One of the core tenets of a highly secure architecture is enforcing what is
    called the *least privilege for access to resources*. **AWS** **Identity and Access
    Management** (**IAM**) is a security service that enables defining and implementing
    your authentication and authorization strategy for secured access to your AWS
    infrastructure for your users. With IAM, you can create permissions policies that
    are attached to roles or users (*identities*) that define what (*actions*) the
    identity can or cannot do with AWS services (*resources*). Least privilege, as
    the name indicates, is all about defining highly restrictive permissions policies
    for your users and roles. The default permission in AWS is a *deny*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 高度安全架构的核心原则之一是强制实施所谓的 *最小权限访问资源*。**AWS** **身份与访问管理**（**IAM**）是一个安全服务，它允许为您的用户定义并实施身份验证和授权策略，以确保对AWS基础设施的安全访问。通过IAM，您可以创建权限策略，这些策略附加到角色或用户（*身份*）上，定义该身份能够或不能使用AWS服务（*资源*）执行哪些（*操作*）。如其名所示，最小权限就是为您的用户和角色定义高度限制性的权限策略。在AWS中，默认的权限是
    *拒绝*。
- en: 'If no policies are specified for a user, the user does not have permission
    to do anything in AWS. So, you add policy statements that allow a user or a role
    to perform specific tasks using AWS services or resources. In our examples in
    this book, due to the nature of our use cases and for the sake of simplicity and
    ease of configuration, we suggest you add managed permissions policies such as
    *TextractFullAccess* or *ComprehendFullAccess* to your **SageMaker execution IAM
    role** for your notebook. When you build your NLP solution and promote it to production,
    as a best practice, you should enforce least privilege access. Let''s discuss
    what this means through an example. The *ComprehendFullAccess* permissions policy
    is defined by the following **JSON** statement:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有为用户指定策略，则该用户没有权限在AWS中执行任何操作。因此，您需要添加策略声明，允许用户或角色使用AWS服务或资源执行特定任务。在本书的示例中，由于用例的性质以及为了简化和方便配置，我们建议您将托管权限策略（例如
    *TextractFullAccess* 或 *ComprehendFullAccess*）添加到您的 **SageMaker执行IAM角色** 中，用于您的笔记本。当您构建您的NLP解决方案并将其推广到生产环境时，作为最佳实践，您应该强制实施最小权限访问。让我们通过一个示例来讨论这意味着什么。*ComprehendFullAccess*
    权限策略由以下 **JSON** 语句定义：
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you refer to the highlighted section in the preceding JSON code, the wildcard
    (`*`) attached to `"comprehend"` indicates that all Amazon Comprehend API actions
    are allowed for the role or the user that wields this policy. This is not a restrictive
    policy but rather provides a broad set of permissions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您参考前面JSON代码中突出显示的部分，附加在 `"comprehend"` 上的通配符（`*`）表示允许角色或用户使用此策略执行所有Amazon
    Comprehend API操作。这不是一个限制性的策略，而是提供了广泛的权限集。
- en: 'To enforce least privilege access, a new policy should be created that should
    be changed as shown in the following JSON statement:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了强制实施最小权限访问，应该创建一个新的策略，并按照以下JSON语句进行更改：
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In this changed JSON statement, we provide restrictive permissions that allow
    a user or a role to only use the **entities detection** feature in Amazon Comprehend.
    A good approach would be to only provide those permissions that are absolutely
    needed for a user or role to perform a task. Also, you will have to ensure that
    you monitor IAM roles and policy assignments to ensure that you clean up the permissions
    once the user or role has completed the task. This way, you avoid the situation
    of an old permission granting a user more access than they need. AWS provides
    a feature called **IAM Access Analyzer** to proactively monitor permissions and
    take actions as required. For a detailed introduction to Access Analyzer, please
    refer to the following documentation: [https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个更改后的JSON声明中，我们提供了限制性权限，允许用户或角色仅使用Amazon Comprehend中的**实体检测**功能。一种好的做法是仅提供完成任务所需的最基本权限。此外，你还需要确保监控IAM角色和策略分配，以确保在用户或角色完成任务后清理权限。这样可以避免因旧权限导致用户获得超出其需求的访问权限。AWS提供了一种名为**IAM访问分析器**的功能，用于主动监控权限并根据需要采取措施。有关Access
    Analyzer的详细介绍，请参考以下文档：[https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html)。
- en: In the next section, we will review how to protect sensitive data during our
    NLP solution building task.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将回顾如何在构建NLP解决方案任务中保护敏感数据。
- en: Obfuscating sensitive data
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混淆敏感数据
- en: This section addresses principles *1.2a* and *4.2b* from the Well-Architected
    NLP solutions matrix (*Figure 18.1*).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了《Well-Architected NLP解决方案矩阵》中的原则 *1.2a* 和 *4.2b*（见 *图18.1*）。
- en: 'Protecting the confidentiality of your data is highly important. Enterprises
    typically classify their data into categories such as *public*, *confidential*,
    *secret*, and *top-secret*, and apply controls and guardrails based on these classifications.
    If you are unsure of how to classify your data, please refer to an existing data
    classification model such as the **US National Classification Scheme**. More details
    on this model, as well as best practices for data classification as recommended
    by AWS, can be found in the following documentation: [https://docs.aws.amazon.com/whitepapers/latest/data-classification/welcome.html](https://docs.aws.amazon.com/whitepapers/latest/data-classification/welcome.html).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 保护数据的机密性非常重要。企业通常将数据分类为*公开*、*机密*、*秘密*和*绝密*等类别，并根据这些分类应用控制措施和保护措施。如果你不确定如何对数据进行分类，请参考现有的数据分类模型，如**美国国家分类方案**。有关此模型的更多细节，以及AWS推荐的数据分类最佳实践，可以在以下文档中找到：[https://docs.aws.amazon.com/whitepapers/latest/data-classification/welcome.html](https://docs.aws.amazon.com/whitepapers/latest/data-classification/welcome.html)。
- en: Once you have classified your data, the next step is to determine the type of
    confidentiality your data contains. Data can be **personally Identifiable Information**
    (**PII**), for example, it may contain social security numbers, credit card numbers,
    bank account numbers, and so on. Or, if your data contains your customers' private
    health records, it is called **protected health information** (**PHI**). If you
    are in the legal industry, the *Attorney-Client Privileged Information* is protected
    data and must be kept confidential.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你对数据进行了分类，下一步是确定数据所包含的保密性类型。数据可以是**个人身份信息**（**PII**），例如，可能包含社会保障号码、信用卡号码、银行账户号码等。或者，如果你的数据包含客户的私人健康记录，它被称为**受保护健康信息**（**PHI**）。如果你从事法律行业，那么*律师-客户特权信息*是受保护数据，必须保密。
- en: 'So, as we can see, data protection is vitally important and must be a key consideration
    in our NLP solution development. When building on AWS, there are multiple ways
    in which you can protect your confidential data, including data encryption at
    rest and in transit, which we cover in subsequent sections in this chapter. For
    details on how AWS supports the highest privacy standards, and information about
    the resources to help you protect your customers'' data, please refer to the following
    link: [https://aws.amazon.com/compliance/data-privacy/](https://aws.amazon.com/compliance/data-privacy/)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，数据保护至关重要，必须成为我们 NLP 解决方案开发中的关键考虑因素。在 AWS 上构建时，有多种方式可以保护你的机密数据，包括静态数据加密和传输数据加密，这些我们将在本章后续部分进行讨论。有关
    AWS 如何支持最高隐私标准的详细信息，以及帮助你保护客户数据的资源信息，请参考以下链接：[https://aws.amazon.com/compliance/data-privacy/](https://aws.amazon.com/compliance/data-privacy/)
- en: The following screenshot shows the results of an Amazon Comprehend PII detection
    in real time.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了 Amazon Comprehend 实时进行 PII 检测的结果。
- en: '![Figure 18.4 – Amazon Comprehend PII detection'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 18.4 – Amazon Comprehend PII 检测'
- en: '](img/B17528_18_04.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17528_18_04.jpg)'
- en: Figure 18.4 – Amazon Comprehend PII detection
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.4 – Amazon Comprehend PII 检测
- en: 'In [*Chapter 4*](B17528_04_Final_SB_ePub.xhtml#_idTextAnchor063), *Automating
    Document Processing Workflows*, we reviewed how Amazon Comprehend provides support
    for detecting PII entities from your data. You can use this capability as part
    of your document pre-processing stage by means of the `Lambda` function automatically
    to either process or transform your data when you fetch it from Amazon S3\. For
    PII detection, two `Lambda` functions are made available to be used with S3 Object
    Access Lambda. The Amazon Comprehend `ContainsPiiEntites` API is used to classify
    documents that contain PII data and the `DetectPiiEntities` API is used to identify
    the actual PII data within the document for the purposes of redaction. For a tutorial
    on how to detect and redact PII data from your documents using the S3 Object Access
    Lambda, please refer to this GitHub repository: [https://github.com/aws-samples/amazon-comprehend-s3-object-lambda-functions](https://github.com/aws-samples/amazon-comprehend-s3-object-lambda-functions)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B17528_04_Final_SB_ePub.xhtml#_idTextAnchor063)，《自动化文档处理工作流》中，我们回顾了
    Amazon Comprehend 如何提供检测数据中的个人身份信息（PII）实体的支持。你可以通过 `Lambda` 函数，在从 Amazon S3 获取数据时，自动对数据进行处理或转换，作为文档预处理阶段的一部分。对于
    PII 检测，提供了两个 `Lambda` 函数，用于与 S3 对象访问 Lambda 一起使用。Amazon Comprehend 的 `ContainsPiiEntites`
    API 用于分类包含 PII 数据的文档，而 `DetectPiiEntities` API 用于识别文档中实际的 PII 数据，以便进行数据屏蔽。关于如何使用
    S3 对象访问 Lambda 检测和屏蔽文档中的 PII 数据的教程，请参考这个 GitHub 仓库：[https://github.com/aws-samples/amazon-comprehend-s3-object-lambda-functions](https://github.com/aws-samples/amazon-comprehend-s3-object-lambda-functions)
- en: In the next section, we will review how to implement data protection at rest
    and in transit.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将回顾如何实施静态数据保护和传输数据保护。
- en: Protecting data at rest and in transit
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保护静态数据和传输中的数据
- en: This section addresses principles *1.2b*, *2.2a*, *3.2a*, and *4.3b* from the
    Well-Architected NLP solutions matrix (*Figure 18.1*).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了《Well-Architected NLP 解决方案矩阵》（*图 18.1*）中的原则 *1.2b*、*2.2a*、*3.2a* 和 *4.3b*。
- en: Now that we have discussed least privilege and data confidentiality, let's review
    the best practices for protecting your data *at rest* and *in transit* (that is,
    when it resides in a data store and during transit, for example, as a result of
    service API calls). When we talk about data protection at rest, we refer to encrypting
    your data during storage in AWS. Your data can reside in an Amazon S3 data lake,
    a relational database in **Amazon** **RDS** (a managed AWS service for relational
    databases), in **Amazon** **Redshift** (an exabyte-scale, cloud-based data warehouse),
    in **Amazon** **DynamoDB**, or in one of the other purpose-built databases such
    as **Amazon** **DocumentDB** (a managed AWS service for **MongoDB**), or **Amazon**
    **Neptune** (a managed AWS service for **Graph** databases), and many more.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了最小权限和数据保密性，现在让我们回顾一下保护数据*静态存储*和*传输中*的最佳实践（也就是说，数据在数据存储中时以及在传输过程中，比如由于服务
    API 调用时）。当我们谈论静态数据保护时，我们指的是在 AWS 中对数据进行加密。你的数据可以存储在 Amazon S3 数据湖中、**Amazon**
    **RDS**（AWS 提供的关系型数据库托管服务）、**Amazon** **Redshift**（一款支持 PB 级数据的云端数据仓库）、**Amazon**
    **DynamoDB**，或其他为特定用途构建的数据库中，比如 **Amazon** **DocumentDB**（为 **MongoDB** 提供的 AWS
    托管服务），或 **Amazon** **Neptune**（为 **图形** 数据库提供的 AWS 托管服务）等。
- en: With AWS, the advantage is that you can enable encryption to protect your data
    at rest easily using **AWS** **Key Management Service** (**KMS**) (a reliable
    and secure service to create, manage, and protect your encryption keys, and for
    applying the encryption of data across many services in AWS). Encryption is supported
    using the **AES-256** standard ([https://en.wikipedia.org/wiki/Advanced_Encryption_Standard](https://en.wikipedia.org/wiki/Advanced_Encryption_Standard)).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 AWS 的优势在于，您可以轻松启用加密来保护静态数据，使用**AWS** **密钥管理服务**（**KMS**）（一个可靠且安全的服务，用于创建、管理和保护加密密钥，并在
    AWS 中的多个服务之间应用数据加密）。加密使用**AES-256**标准（[https://en.wikipedia.org/wiki/Advanced_Encryption_Standard](https://en.wikipedia.org/wiki/Advanced_Encryption_Standard)）得到支持。
- en: For example, when you store objects in Amazon S3, you can request **Server-side
    encryption** (encrypting data at the destination as it is stored in S3) by selecting
    to either use S3-managed encryption keys (that is, keys you create in KMS at the
    bucket level) or your own encryption keys (which you provide when you upload the
    objects to your S3 bucket).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当您将对象存储在 Amazon S3 时，您可以请求**服务器端加密**（在数据存储到 S3 时进行加密），通过选择使用 S3 管理的加密密钥（即您在存储桶级别创建的
    KMS 密钥）或您自己的加密密钥（当您上传对象到 S3 存储桶时提供的密钥）来实现加密。
- en: '![Figure 18.5 – Enabling encryption for your S3 bucket](img/B17528_18_05.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.5 – 为您的 S3 存储桶启用加密](img/B17528_18_05.jpg)'
- en: Figure 18.5 – Enabling encryption for your S3 bucket
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.5 – 为您的 S3 存储桶启用加密
- en: Amazon Redshift provides similar options at cluster creation time to encrypt
    your data ([https://docs.aws.amazon.com/redshift/latest/mgmt/security-server-side-encryption.html](https://docs.aws.amazon.com/redshift/latest/mgmt/security-server-side-encryption.html)).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift 在集群创建时提供类似的选项来加密您的数据（[https://docs.aws.amazon.com/redshift/latest/mgmt/security-server-side-encryption.html](https://docs.aws.amazon.com/redshift/latest/mgmt/security-server-side-encryption.html)）。
- en: Amazon DynamoDB encrypts all your data by default using AWS KMS ([https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html)).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon DynamoDB 默认使用 AWS KMS 加密您的所有数据（[https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html)）。
- en: You can enable encryption for your Amazon RDS databases ([https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html)).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为您的 Amazon RDS 数据库启用加密（[https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html)）。
- en: You can also enable encryption for any purpose-built AWS databases, such as
    Amazon DocumentDB ([https://docs.aws.amazon.com/documentdb/latest/developerguide/encryption-at-rest.html](https://docs.aws.amazon.com/documentdb/latest/developerguide/encryption-at-rest.html)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以为任何专用的 AWS 数据库启用加密，例如 Amazon DocumentDB ([https://docs.aws.amazon.com/documentdb/latest/developerguide/encryption-at-rest.html](https://docs.aws.amazon.com/documentdb/latest/developerguide/encryption-at-rest.html))。
- en: 'All AWS services that handle customer data support encryption. Please refer
    to this blog for more details: [https://aws.amazon.com/blogs/security/importance-of-encryption-and-how-aws-can-help/](https://aws.amazon.com/blogs/security/importance-of-encryption-and-how-aws-can-help/)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 所有处理客户数据的 AWS 服务都支持加密。更多细节请参阅此博客：[https://aws.amazon.com/blogs/security/importance-of-encryption-and-how-aws-can-help/](https://aws.amazon.com/blogs/security/importance-of-encryption-and-how-aws-can-help/)
- en: 'To protect data in transit, you secure your API endpoints using a protocol
    such as **Transport Layer Security** (**TLS**): [https://en.wikipedia.org/wiki/Transport_Layer_Security](https://en.wikipedia.org/wiki/Transport_Layer_Security)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保护数据传输，您可以使用如**传输层安全性**（**TLS**）等协议来保护您的 API 端点：[https://en.wikipedia.org/wiki/Transport_Layer_Security](https://en.wikipedia.org/wiki/Transport_Layer_Security)
- en: Similar to protecting data at rest, AWS provides the means to secure your data
    in transit using **AWS** **Certificate Manager** (a managed service to provision
    and manage TLS certificates) to secure communications, verify identities, and
    implement **HTTPS** endpoints for application interactions. All AWS services that
    handle customer data are secured using TLS with HTTPS.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于保护静态数据，AWS 提供了使用**AWS** **证书管理器**（一个托管服务，用于配置和管理 TLS 证书）来加密通信、验证身份，并实现应用程序交互的**HTTPS**端点。所有处理客户数据的
    AWS 服务都使用 TLS 和 HTTPS 进行加密保护。
- en: Using Amazon API Gateway for request throttling
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Amazon API Gateway 进行请求限制
- en: This section addresses principle *2.1a* from the Well-Architected NLP solutions
    matrix (*Figure 18.1*).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了《Well-Architected NLP 解决方案矩阵》中原则*2.1a*（*图 18.1*）。
- en: 'When we build Amazon Comprehend custom entity recognizers or classifiers, we
    host these models by creating Comprehend real-time endpoints, as shown in the
    following screenshot:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建 Amazon Comprehend 自定义实体识别器或分类器时，我们通过创建 Comprehend 实时端点来托管这些模型，如下图所示：
- en: '![Figure 18.6 – Creating Amazon Comprehend real-time endpoints'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 18.6 – 创建 Amazon Comprehend 实时端点'
- en: '](img/B17528_18_06.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17528_18_06.jpg)'
- en: Figure 18.6 – Creating Amazon Comprehend real-time endpoints
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.6 – 创建 Amazon Comprehend 实时端点
- en: 'You can call these endpoints directly from your code to detect entities or
    for text classification needs, as shown in the following code snippet:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以直接从代码中调用这些端点来检测实体或进行文本分类需求，具体代码片段如下：
- en: '[PRE2]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will talk about how to set up auto scaling for your Amazon Comprehend real-time
    endpoints in a subsequent section, but you are billed by the second for your inference
    endpoints ([https://aws.amazon.com/comprehend/pricing/](https://aws.amazon.com/comprehend/pricing/)),
    and the capacity is measured in inference units that represent a throughput of
    100 characters per second. Throttling requests to the endpoint will allow for
    a more managed use of capacity. **Amazon** **API Gateway** ([https://aws.amazon.com/api-gateway/](https://aws.amazon.com/api-gateway/))
    is a fully managed, secure, and scalable service for API management that can be
    used to create an API to abstract the calls to the Amazon Comprehend endpoint
    by using an AWS Lambda function, as demonstrated in the tutorial in the following
    link: [https://github.com/aws-samples/amazon-comprehend-custom-entity-recognizer-api-example](https://github.com/aws-samples/amazon-comprehend-custom-entity-recognizer-api-example)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在随后的章节中讨论如何为 Amazon Comprehend 实时端点设置自动扩展，但需要注意的是，您将按秒收费（[https://aws.amazon.com/comprehend/pricing/](https://aws.amazon.com/comprehend/pricing/)），且容量以推理单位计量，每个推理单位表示每秒
    100 个字符的吞吐量。对端点进行请求限制将有助于更有效地管理容量。**Amazon** **API Gateway**（[https://aws.amazon.com/api-gateway/](https://aws.amazon.com/api-gateway/)）是一个完全托管的、安全的、可扩展的
    API 管理服务，可以用来创建 API，通过 AWS Lambda 函数抽象调用 Amazon Comprehend 端点，具体操作请参考以下教程：[https://github.com/aws-samples/amazon-comprehend-custom-entity-recognizer-api-example](https://github.com/aws-samples/amazon-comprehend-custom-entity-recognizer-api-example)
- en: 'Apart from throttling, API Gateway also supports traffic management, access
    control, monitoring, and version management, which can help implement a robust
    approach for handling requests for our solution. For more details, please refer
    to the following documentation: [https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html](https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 除了请求限制外，API Gateway 还支持流量管理、访问控制、监控和版本管理，这有助于为我们的解决方案实施一个强健的请求处理方法。有关更多详细信息，请参阅以下文档：[https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html](https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html)
- en: In the next section, we will cover how to set up auto scaling for your Amazon
    Comprehend real-time endpoints.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍如何为 Amazon Comprehend 实时端点设置自动扩展。
- en: Setting up auto scaling for Amazon Comprehend endpoints
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为 Amazon Comprehend 端点设置自动扩展
- en: This section addresses principle *2.3a* from the Well-Architected NLP solutions
    matrix (*Figure 18.1*).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了《Well-Architected NLP 解决方案矩阵》中原则*2.3a*（*图 18.1*）。
- en: 'In the previous section, we discussed that you need endpoints to enable real-time
    predictions from your Amazon Comprehend custom models. The endpoint inference
    capacity is denoted as an **Inference Unit** (**IU**), which represents a throughput
    of 100 characters per second. When you create an endpoint, you specify the number
    of IUs you need, which helps Amazon Comprehend determine the resources to allocate
    to your endpoint. You calculate IUs based on the output throughput you need from
    the endpoint in terms of characters per second, and you are charged for the duration
    of when the endpoint is active, irrespective of whether it is receiving requests
    or not. So, you need to manage the IUs carefully to ensure you receive the required
    capacity when needed (for performance) but can also discard the capacity when
    not needed (to save costs). You can do this using Amazon Comprehend auto scaling:
    [https://docs.aws.amazon.com/comprehend/latest/dg/comprehend-autoscaling.html](https://docs.aws.amazon.com/comprehend/latest/dg/comprehend-autoscaling.html)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了需要终端节点来启用来自 Amazon Comprehend 自定义模型的实时预测。终端节点推理能力用 **推理单元** (**IU**)
    表示，表示每秒 100 个字符的吞吐量。当您创建终端节点时，需要指定所需的 IU 数量，以帮助 Amazon Comprehend 确定为您的终端节点分配的资源。您根据终端节点所需的输出吞吐量（按字符每秒计算）来计算
    IU，并且在终端节点处于活动状态时，无论是否接收请求，都会按持续时间收费。因此，您需要仔细管理 IU，以确保在需要时能够获得所需的容量（以保证性能），并在不需要时释放容量（以节省成本）。您可以通过
    Amazon Comprehend 自动扩展来实现这一点：[https://docs.aws.amazon.com/comprehend/latest/dg/comprehend-autoscaling.html](https://docs.aws.amazon.com/comprehend/latest/dg/comprehend-autoscaling.html)
- en: 'You can set up auto scaling only by using the **AWS** **Command Line Interface**
    (**AWS CLI**). The following example shows how to enable auto scaling for custom
    entity recognition:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 只能通过 **AWS** **命令行界面** (**AWS CLI**) 设置自动扩展。以下示例展示了如何为自定义实体识别启用自动扩展：
- en: 'Register a scalable target by running the following code snippet in the AWS
    CLI. Here `scalable-dimension` refers to the Amazon Comprehend resource type along
    with the unit of measurement for capacity (IUs):'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在 AWS CLI 中运行以下代码片段注册可扩展目标。此处的 `scalable-dimension` 是指 Amazon Comprehend 资源类型以及容量的度量单位（IU）：
- en: '[PRE3]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You then create a JSON configuration for what target you would like to track,
    as shown in the following code snippet:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您需要创建一个 JSON 配置文件，指定您希望跟踪的目标，如下所示的代码片段：
- en: '[PRE4]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, you put this scaling policy into action, as shown in the following
    code snippet:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您将执行此扩展策略，如下所示的代码片段所示：
- en: '[PRE5]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the next section, we will review how to monitor training metrics for our
    custom models to enable proactive actions.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将回顾如何监控自定义模型的训练指标，以便采取主动行动。
- en: Automating monitoring of custom training metrics
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化监控自定义训练指标
- en: This section addresses principle *4.3a* from the Well-Architected NLP solutions
    matrix (*Figure 18.1*).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容涉及 **Well-Architected NLP 解决方案矩阵**（*图 18.1*）中的原则 *4.3a*。
- en: When training your custom classification or entity recognition models, Amazon
    Comprehend generates `DescribeEntityRecognizer` API (for entity recognition) or
    `DescribeDocumentClassifier` API (for classification) to get the evaluation metrics
    for your custom model.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练自定义分类或实体识别模型时，Amazon Comprehend 会生成 `DescribeEntityRecognizer` API（用于实体识别）或
    `DescribeDocumentClassifier` API（用于分类）以获取自定义模型的评估指标。
- en: 'The following is a code snippet of how to use the `DescribeEntityRecognizer`
    API:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用 `DescribeEntityRecognizer` API 的代码片段：
- en: '[PRE6]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To monitor for the completion of the Amazon Comprehend custom training job,
    you can use `DescribeEntityRecognizer` or `DescribeDocumentClassifier` APIs to
    retrieve the evaluation metrics. If the metrics are below a threshold, this function
    can send alerts or notifications using **Amazon** **Simple Notification Service**
    (**SNS**). For details on how to schedule an event using Amazon EventBridge, please
    refer to the documentation: [https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 要监控 Amazon Comprehend 自定义训练作业的完成情况，可以使用 `DescribeEntityRecognizer` 或 `DescribeDocumentClassifier`
    API 来获取评估指标。如果这些指标低于阈值，该功能可以通过 **Amazon** **Simple Notification Service** (**SNS**)
    发送警报或通知。有关如何使用 Amazon EventBridge 安排事件的详细信息，请参考文档：[https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html)。
- en: In the next section, we will look at using Amazon A2I to set up human loops
    to review predictions.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍如何使用 Amazon A2I 设置人工审核流程，以审查预测结果。
- en: Using Amazon A2I to review predictions
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Amazon A2I 审核预测结果
- en: This section addresses principle *2.3b* from the Well-Architected NLP solutions
    matrix (*Figure 18.1*).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了《良好架构化 NLP 解决方案矩阵》（*图 18.1*）中的原则 *2.3b*。
- en: We covered using **Amazon** **Augmented AI** (**Amazon** **A2I**) (a managed
    service to set up human reviews of ML predictions) in great detail in many of
    the previous chapters in this book, starting with [*Chapter 13*](B17528_13_Final_SB_ePub.xhtml#_idTextAnchor151),
    *Improving the Accuracy of Document Processing Workflows*. When your solution
    is newly developed, it is a best practice to set up a human loop for prediction
    reviews, auditing, and making corrections as needed. Your solution should also
    include a feedback loop with model re-training based on human-reviewed data. We
    recommend having a human loop with Amazon A2I for the first three to six months
    to allow your solution to evolve based on direct feedback. Subsequently, you can
    disable the human loop.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书的多个章节中详细介绍了如何使用 **Amazon** **增强 AI** (**Amazon** **A2I**)（一个托管服务，用于设置机器学习预测的人工审核），从
    [*第 13 章*](B17528_13_Final_SB_ePub.xhtml#_idTextAnchor151)，《提高文档处理工作流的准确性》开始。当你的解决方案是新开发时，最佳做法是设置人工审核环节，用于预测审核、审计和根据需要进行修正。你的解决方案还应包括基于人工审核数据的反馈循环，并进行模型再训练。我们建议在前
    3 到 6 个月内使用 Amazon A2I 的人工审核环节，以便根据直接反馈使你的解决方案不断演进。之后，你可以禁用人工审核环节。
- en: In the next section, we will cover how to build modular, loosely coupled solutions
    using **Async** **APIs**.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍如何使用 **异步** **API** 构建模块化、松耦合的解决方案。
- en: Using Async APIs for loose coupling
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用异步 API 实现松耦合
- en: This section addresses principles *2.4a* and *4.4a* from the Well-Architected
    NLP solutions matrix (*Figure 18.1*).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了《良好架构化 NLP 解决方案矩阵》（*图 18.1*）中的原则 *2.4a* 和 *4.4a*。
- en: 'When we set up an NLP solution pipeline that is required to scale to processing
    millions of documents, it is a good idea to use the **Asynchronous Batch APIs**
    to implement this architecture. Synchronous APIs follow the request-response paradigm,
    meaning the requesting application will wait for a response and will be held up
    until a response is received. This approach works well when the need is to process
    a few documents quickly for a real-time or near-real-time, mission-critical requirement.
    However, when the document volume increases, a synchronous approach will hold
    compute resources, and slow down the process. Typically, organizations implement
    two separate NLP solution pipelines: one for real-time processing, and a second
    for batch processing. For batch processing, depending on the number of documents
    to be processed, the inference results are available after a few minutes to a
    few hours, depending on how the architecture is set up.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们设置一个需要扩展到处理数百万文档的 NLP 解决方案流水线时，最好使用 **异步批处理 API** 来实现这一架构。同步 API 遵循请求-响应模式，这意味着请求应用程序将等待响应，直到收到响应为止。当需要快速处理少量文档以满足实时或近实时、关键任务需求时，这种方式效果较好。然而，当文档量增大时，同步方式将占用计算资源，并且会减慢处理速度。通常，组织会实施两个独立的
    NLP 解决方案流水线：一个用于实时处理，另一个用于批处理。对于批处理，根据处理的文档数量，推理结果将在几分钟到几小时后可用，具体取决于架构的设置方式。
- en: With Amazon Comprehend, once the entity recognizer or classifier training is
    completed, use the `Batch` API when you need to run `inference` for large document
    volumes, as shown in the following code snippets for entity recognition.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Amazon Comprehend 时，一旦实体识别器或分类器训练完成，当需要对大量文档进行 `推理` 时，使用 `Batch` API，如下面的实体识别代码片段所示。
- en: 'First, we submit an `entities detection` job (if we provide the endpoint `ARN`,
    it will use our custom entity recognizer). The response returns a `JobId`, a `Job
    ARN`, and a `job status`. Once the job is completed, the results are sent to the
    S3 location you specify in the `OutputDataConfig`:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们提交一个 `实体检测` 任务（如果提供 `ARN` 端点，它将使用我们的自定义实体识别器）。响应返回一个 `JobId`、`Job ARN`
    和 `任务状态`。任务完成后，结果会发送到你在 `OutputDataConfig` 中指定的 S3 位置：
- en: '[PRE7]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: When using Amazon Textract for processing large document volumes, you can use
    the `Batch` APIs to first submit a text analysis or detection job, and then subsequently
    get the extraction results once the analysis job is completed. The following steps
    show how you can use the Amazon Textract Batch APIs.
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用 Amazon Textract 处理大量文档时，可以首先使用 `Batch` API 提交文本分析或检测任务，分析任务完成后，再获取提取结果。以下步骤展示了如何使用
    Amazon Textract 批处理 API。
- en: 'Let''s assume our use case is to process documents that contain tables and
    form data along with text. In this case, we will use the `StartDocumentAnalysis`
    API ([https://docs.aws.amazon.com/textract/latest/dg/API_StartDocumentAnalysis.html](https://docs.aws.amazon.com/textract/latest/dg/API_StartDocumentAnalysis.html))
    as a first step, and ask it to look for table and form contents. Text in paragraphs
    is extracted by default. We also pass an Amazon SNS `topic` and an `IAM role`
    that provides permissions for Amazon Textract to publish a message to the SNS
    `topic`. This API returns a `JobId` that we will use in the next step:'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假设我们的用例是处理包含表格和表单数据的文档以及文本。在这种情况下，我们将首先使用`StartDocumentAnalysis` API（[https://docs.aws.amazon.com/textract/latest/dg/API_StartDocumentAnalysis.html](https://docs.aws.amazon.com/textract/latest/dg/API_StartDocumentAnalysis.html)），并请求它查找表格和表单内容。段落中的文本默认会被提取。我们还会传递一个Amazon
    SNS `topic`和一个`IAM role`，该角色为Amazon Textract提供权限，以便向SNS `topic`发布消息。此API将返回一个`JobId`，我们将在下一步中使用：
- en: '[PRE8]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'When the job completes, Amazon Textract sends a message to the SNS topic indicating
    the job status. You can attach an AWS Lambda function to this SNS topic as an
    event trigger. This Lambda function will call the `GetDocumentAnalysis` API ([https://docs.aws.amazon.com/textract/latest/dg/API_GetDocumentAnalysis.html](https://docs.aws.amazon.com/textract/latest/dg/API_GetDocumentAnalysis.html))
    to retrieve the results from the Amazon Textract job, as shown in the following
    code snippet:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当任务完成时，Amazon Textract会向SNS主题发送一条消息，指示任务状态。你可以将AWS Lambda函数附加到此SNS主题作为事件触发器。此Lambda函数将调用`GetDocumentAnalysis`
    API（[https://docs.aws.amazon.com/textract/latest/dg/API_GetDocumentAnalysis.html](https://docs.aws.amazon.com/textract/latest/dg/API_GetDocumentAnalysis.html)），以从Amazon
    Textract任务中检索结果，如以下代码片段所示：
- en: '[PRE9]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The response is a JSON object of blocks of text data that include both tabular
    and form content. In the next section, we will discuss how we can simplify the
    parsing of the JSON response object using Amazon Textract Response Parser.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 响应是一个JSON对象，包含文本数据块，包括表格和表单内容。在下一节中，我们将讨论如何使用Amazon Textract响应解析器简化JSON响应对象的解析。
- en: Using Amazon Textract Response Parser
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Amazon Textract响应解析器
- en: This section addresses principle *3.4a* from the Well-Architected NLP solutions
    matrix (*Figure 18.1*).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了Well-Architected NLP解决方案矩阵中的原则*3.4a*（*图18.1*）。
- en: 'The JSON documents returned by the Amazon Textract APIs are comprehensive,
    with document contents categorized as blocks that encapsulate information for
    pages, lines, words, tables, forms, and the relationships between them. When using
    Amazon Textract to process complex or descriptive documents, it can seem time-consuming
    to understand the JSON results and parse them to obtain the data we need from
    the various ways text is contained in the document. The following code snippet
    shows the JSON response for a line that Amazon Textract extracted from the document
    we used in [*Chapter 14*](B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162), *Auditing
    Named Entity Recognition Workflows*:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Textract API返回的JSON文档内容非常全面，文档内容被分类为多个块，这些块封装了页面、行、词、表格、表单以及它们之间关系的信息。当使用Amazon
    Textract处理复杂或描述性的文档时，理解JSON结果并解析它们以获取我们需要的数据可能显得有些耗时，因为文本在文档中的存储方式各不相同。以下代码片段显示了Amazon
    Textract从我们在[*第14章*](B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162)使用的文档中提取的一行的JSON响应，*审计命名实体识别工作流*：
- en: '[PRE10]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: So, in order to make the process of retrieving the content we need from the
    JSON output simpler, the **Amazon** **Textract Response Parser** library (or **TRP**)
    ([https://github.com/aws-samples/amazon-textract-response-parser](https://github.com/aws-samples/amazon-textract-response-parser))
    was created. TRP makes it easy to get all the data we need with very few lines
    of code and improves the efficiency of our overall solution. We have already used
    TRP in this book, for example, in [*Chapter 14*](B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162),
    *Auditing Named Entity Recognition Workflows*, and [*Chapter 17*](B17528_17_Final_SB_ePub.xhtml#_idTextAnchor202),
    *Visualizing Insights from Handwritten Content*.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了简化从JSON输出中提取所需内容的过程，**Amazon** **Textract响应解析器**库（或**TRP**）（[https://github.com/aws-samples/amazon-textract-response-parser](https://github.com/aws-samples/amazon-textract-response-parser)）应运而生。TRP使我们能够用极少的代码行轻松获取所需的所有数据，并提高我们整体解决方案的效率。我们在本书中已经使用了TRP，例如在[*第14章*](B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162)中，*审计命名实体识别工作流*，以及[*第17章*](B17528_17_Final_SB_ePub.xhtml#_idTextAnchor202)中，*从手写内容中可视化洞察*。
- en: 'The following code snippets show how to install and use the TRP library:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何安装和使用TRP库：
- en: 'To install the TRP library, use the following code snippet:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要安装 TRP 库，请使用以下代码片段：
- en: '[PRE11]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Import the library, call the `Textract` API to analyze a document, and use
    `TRP` to parse the results:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库，调用 `Textract` API 来分析文档，并使用 `TRP` 解析结果：
- en: '[PRE12]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we can loop through the results to extract data from pages, tables, and
    more. For a **Python** example of how to use TRP, please refer to the code sample:
    [https://github.com/aws-samples/amazon-textract-response-parser/tree/master/src-python#parse-json-response-from-textract](https://github.com/aws-samples/amazon-textract-response-parser/tree/master/src-python#parse-json-response-from-textract)'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以遍历结果以提取页面、表格等数据。有关如何使用 TRP 的**Python** 示例，请参阅代码示例：[https://github.com/aws-samples/amazon-textract-response-parser/tree/master/src-python#parse-json-response-from-textract](https://github.com/aws-samples/amazon-textract-response-parser/tree/master/src-python#parse-json-response-from-textract)
- en: '[PRE13]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the next section, we will review why it is important to persist the prediction
    results from our NLP solution, and how we can make use of this data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将回顾为什么持久化 NLP 解决方案的预测结果非常重要，以及我们如何利用这些数据。
- en: Persisting prediction results
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持久化预测结果
- en: 'During the course of this book, we have seen examples where the results of
    an entity recognition or classification task are sent to an **Amazon** **Elasticsearch**
    instance (for metadata extraction) or to Amazon DynamoDB (for persistence). We
    also saw examples where these results are used to inform decisions that impact
    downstream systems. The reason for this is because we often see with organizations
    that document processing provides important inputs to their mainstream operations.
    So, when you design and build NLP solutions, you have to keep in mind how your
    prediction results are going to be consumed, by whom, and for what purpose. Depending
    on the consumption use case, there are different options available for us to use.
    Let''s review some of these options:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的过程中，我们看到了一些示例，其中实体识别或分类任务的结果被发送到**Amazon** **Elasticsearch** 实例（用于元数据提取）或
    Amazon DynamoDB（用于持久化）。我们还看到了一些示例，这些结果被用来通知影响下游系统的决策。这样做的原因是，因为我们经常看到在组织中，文档处理为其主流业务操作提供了重要的输入。因此，当你设计和构建
    NLP 解决方案时，必须考虑预测结果将如何被消费，谁会使用，以及目的是什么。根据消费的使用案例，我们有不同的选项可以使用。让我们回顾一下这些选项：
- en: If the need is for real-time access to inference results, set up an API Gateway
    and AWS Lambda function to abstract the Amazon Comprehend real-time endpoint.
    Persist the inference request and response in an Amazon S3 bucket or Amazon DynamoDB
    for future reference. Please refer to the *Using API Gateway for request throttling*
    section for more details.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要实时访问推理结果，请设置 API Gateway 和 AWS Lambda 函数，以抽象化 Amazon Comprehend 的实时端点。将推理请求和响应持久化到
    Amazon S3 桶或 Amazon DynamoDB 以供将来参考。有关详细信息，请参考*使用 API Gateway 进行请求限流*部分。
- en: 'If the results are to be sent to downstream applications that need these inputs
    for decision-making or functional requirements, you can persist the results to
    Amazon S3, or an Amazon RDS database, or any of the purpose-built data stores
    in AWS. To notify the applications that new results are available, you can publish
    a message to an Amazon SNS topic or use an event trigger in the data stores. For
    more information, please refer to the following: [https://docs.aws.amazon.com/lambda/latest/dg/services-rds-tutorial.html](https://docs.aws.amazon.com/lambda/latest/dg/services-rds-tutorial.html)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果结果需要发送到需要这些输入进行决策或功能需求的下游应用程序，您可以将结果持久化到 Amazon S3、Amazon RDS 数据库或 AWS 中的任何专用数据存储中。为了通知应用程序有新结果可用，您可以向
    Amazon SNS 主题发布消息，或在数据存储中使用事件触发器。有关更多信息，请参考以下链接：[https://docs.aws.amazon.com/lambda/latest/dg/services-rds-tutorial.html](https://docs.aws.amazon.com/lambda/latest/dg/services-rds-tutorial.html)
- en: 'If you need the results to populate a knowledge repository or make it available
    for user search, send it to an Amazon Elasticsearch (now called **OpenSearch**)
    index. For more information, please refer to the following: [https://docs.aws.amazon.com/opensearch-service/latest/developerguide/search-example.html](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/search-example.html)'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您需要将结果填充到知识库或使其可供用户搜索，请将其发送到 Amazon Elasticsearch（现称为**OpenSearch**）索引。有关更多信息，请参考以下链接：[https://docs.aws.amazon.com/opensearch-service/latest/developerguide/search-example.html](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/search-example.html)
- en: 'If you would like to use the results for business intelligence or visualization,
    you can send the results to an Amazon S3 bucket and use **Amazon** **QuickSight**
    with the data in Amazon S3\. For more information, please refer to the following:
    [https://docs.aws.amazon.com/quicksight/latest/user/getting-started-create-analysis-s3.html](https://docs.aws.amazon.com/quicksight/latest/user/getting-started-create-analysis-s3.html)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想将结果用于商业智能或可视化，可以将结果发送到 Amazon S3 存储桶，并使用 **Amazon** **QuickSight** 对存储在
    Amazon S3 中的数据进行分析。更多信息，请参考以下链接：[https://docs.aws.amazon.com/quicksight/latest/user/getting-started-create-analysis-s3.html](https://docs.aws.amazon.com/quicksight/latest/user/getting-started-create-analysis-s3.html)
- en: If you would like to transform the results before sending them to the data stores,
    use AWS Glue ETL jobs. For more details, please refer to the *Using AWS Glue for
    data processing and transformation* section.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想在将结果发送到数据存储之前进行转换，可以使用 AWS Glue ETL 作业。更多详细信息，请参考*使用 AWS Glue 进行数据处理和转换*部分。
- en: Let's now review how to automate the NLP solution development using **AWS**
    **Step Function**.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下如何使用 **AWS** **Step Function** 自动化 NLP 解决方案的开发。
- en: Using AWS Step Function for orchestration
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 AWS Step Function 进行编排
- en: In a previous section, we read how using Batch APIs can help scale the architecture
    to handle large volumes of documents. We reviewed Amazon Comprehend and Textract
    APIs that can help us implement a batch-processing pipeline. When we start designing
    a batch solution, it may take the shape of an Amazon S3 bucket, to which an AWS
    Lambda event trigger is attached that will call the Amazon Textract API to start
    document analysis. To this, an Amazon SNS topic would be provided, a message will
    be sent by Amazon Textract to this topic, to which an AWS Lambda is attached,
    and so on. You get the point. It can get really difficult to manage all of these
    moving parts in our solution. To design an elegant and efficient NLP solution,
    you can use AWS Step Function to manage the orchestration of your entire pipeline
    ([https://aws.amazon.com/step-functions/](https://aws.amazon.com/step-functions/)).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，我们了解了如何使用 Batch API 帮助扩展架构以处理大量文档。我们回顾了 Amazon Comprehend 和 Textract
    API，这些 API 可以帮助我们实现批处理管道。在开始设计批处理解决方案时，它可能会以一个 Amazon S3 存储桶的形式出现，附加一个 AWS Lambda
    事件触发器，该触发器将调用 Amazon Textract API 开始文档分析。然后，一个 Amazon SNS 主题将被提供，Amazon Textract
    会将消息发送到该主题，AWS Lambda 将附加到该主题，以此类推。你明白了，这样的解决方案很难管理。为了设计一个优雅高效的 NLP 解决方案，你可以使用
    AWS Step Function 来管理整个管道的编排（[https://aws.amazon.com/step-functions/](https://aws.amazon.com/step-functions/)）。
- en: 'AWS Step Function is a serverless, event-driven orchestration service that
    can help tie together several steps in a process and manage it end to end. Error
    handling is built in, so you can configure retries, branching, and compensation
    logic into the orchestration. An example of a Step Function orchestration from
    the samples available in the AWS Console is shown in the following screenshot:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Step Function 是一个无服务器的事件驱动编排服务，可以帮助将多个过程步骤连接起来并进行端到端管理。错误处理是内建的，因此你可以在编排中配置重试、分支和补偿逻辑。以下截图展示了从
    AWS 控制台中可用的示例中的 Step Function 编排：
- en: '![Figure 18.7 – A sample Step Function orchestration'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 18.7 – 一个示例的 Step Function 编排'
- en: '](img/B17528_18_07.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17528_18_07.jpg)'
- en: Figure 18.7 – A sample Step Function orchestration
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.7 – 一个示例的 Step Function 编排
- en: 'To get started, you can run a sample orchestration in your Step Function Console
    in the AWS Management Console by selecting **Run a sample project** in the **State
    Machines** option on the left of the console. You can also try the Step Function
    tutorials available here: [https://aws.amazon.com/getting-started/hands-on/create-a-serverless-workflow-step-functions-lambda/](https://aws.amazon.com/getting-started/hands-on/create-a-serverless-workflow-step-functions-lambda/)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用，你可以在 AWS 管理控制台的 Step Function 控制台中运行一个示例编排，方法是选择控制台左侧的**运行示例项目**，然后选择**状态机**选项。你还可以尝试这里提供的
    Step Function 教程：[https://aws.amazon.com/getting-started/hands-on/create-a-serverless-workflow-step-functions-lambda/](https://aws.amazon.com/getting-started/hands-on/create-a-serverless-workflow-step-functions-lambda/)
- en: In the next section, let's discuss how to automate our deployment process using
    AWS CloudFormation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将讨论如何使用 AWS CloudFormation 自动化部署过程。
- en: Using AWS CloudFormation templates
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 AWS CloudFormation 模板
- en: '**AWS** **CloudFormation** ([https://aws.amazon.com/cloudformation/](https://aws.amazon.com/cloudformation/))
    is an infrastructure-as-code service that helps you automate and manage your resource
    provisioning tasks in AWS. When building NLP solutions using AWS AI services,
    we primarily deal with managed services, but, depending on how our operational
    architecture looks, it makes a lot of sense to use AWS CloudFormation to automate
    our deployment process. This is mainly because it removes a lot of overhead related
    to the setup, makes change management easier, and helps us achieve operational
    excellence. Every solution topology is different, but if your NLP architecture
    includes Amazon S3 buckets and other types of AWS data stores, AWS Step Function,
    AWS Lambda functions, Amazon SNS topics, and so on, you will benefit from using
    AWS CloudFormation. Templates can be written in JSON or **YAML**, and there are
    lots of resources available to help you get started.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**AWS** **CloudFormation** ([https://aws.amazon.com/cloudformation/](https://aws.amazon.com/cloudformation/))
    是一项基础设施即代码服务，帮助你自动化和管理AWS中的资源配置任务。在使用AWS AI服务构建NLP解决方案时，我们主要处理托管服务，但根据我们的操作架构，使用AWS
    CloudFormation来自动化部署过程是非常有意义的。主要原因是它去除了大量与设置相关的开销，使变更管理变得更加简单，并帮助我们实现操作卓越。每个解决方案的拓扑结构都不同，但如果你的NLP架构包括Amazon
    S3桶和其他类型的AWS数据存储、AWS Step Function、AWS Lambda函数、Amazon SNS主题等，那么你将受益于使用AWS CloudFormation。模板可以用JSON或**YAML**编写，且有大量资源可供你入门。'
- en: 'For an example CloudFormation template with AWS Step Function and AWS Lambda
    functions, please refer to the following: [https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-lambda-state-machine-cloudformation.html](https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-lambda-state-machine-cloudformation.html)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要一个包含AWS Step Function和AWS Lambda函数的CloudFormation模板示例，请参考以下链接：[https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-lambda-state-machine-cloudformation.html](https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-lambda-state-machine-cloudformation.html)
- en: 'For template snippet code examples for a variety of AWS services, please refer
    to the following: [https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/CHAP_TemplateQuickRef.html](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/CHAP_TemplateQuickRef.html)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要各种AWS服务的模板代码示例，请参考以下链接：[https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/CHAP_TemplateQuickRef.html](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/CHAP_TemplateQuickRef.html)
- en: As we saw in detail in the preceding sections, these are some of the principles
    and best practices you can adopt to design and build long-lasting NLP solutions
    that are cost-effective, resilient, scalable, secure, and performance-efficient.
    These are the characteristics that make great solutions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中详细看到的，这些是你可以采纳的一些原则和最佳实践，帮助你设计和构建长久有效的NLP解决方案，具备成本效益、弹性、可扩展性、安全性和性能效率。这些特征构成了优秀解决方案的关键。
- en: Summary
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'After having learned how to build NLP solutions for a number of real-world
    use cases in the previous chapters, we spent this chapter reading about how to
    build secure, reliable, and efficient architectures using the AWS Well-Architected
    Framework. We first introduced what the Well-Architected Framework is, and reviewed
    the five pillars it is comprised of: operational excellence, security, reliability,
    performance efficiency, and cost optimization. We read about each of the pillars
    in brief, and then discussed how the Well-Architected Framework can help us build
    better and more efficient NLP solutions by using a matrix of best practices aligned
    with the Well-Architected principles and the different stages of NLP solution
    development.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章学习了如何为多个实际应用案例构建NLP解决方案之后，本章我们阅读了如何利用AWS Well-Architected Framework构建安全、可靠且高效的架构。我们首先介绍了什么是Well-Architected
    Framework，并回顾了它所包含的五大支柱：操作卓越、安全性、可靠性、性能效率和成本优化。我们简要阅读了每个支柱的内容，然后讨论了如何通过使用与Well-Architected原则和NLP解决方案开发不同阶段对齐的最佳实践矩阵，来帮助我们构建更好、更高效的NLP解决方案。
- en: We followed this summary of the best practices by diving deep into each one,
    learning how to implement them using the AWS Management Console, AWS documentation
    references, and some code snippets.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在总结了这些最佳实践后，我们深入探讨了每一个实践，学习如何通过AWS管理控制台、AWS文档参考以及一些代码片段来实现它们。
- en: That brings us to the end of this book. It is with a heavy heart that we bid
    adieu to you, our wonderful readers. We hope that you had as much fun reading
    this book as we had writing it. Please don't forget to check out the *Further
    reading* section for a few references we have included to continue your learning
    journey in the exciting space that is NLP.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这也意味着本书的结束。我们怀着沉重的心情向你告别，我们亲爱的读者。希望你在阅读这本书时，能像我们写书时那样感到愉快。请不要忘记查看*进一步阅读*部分，我们为你准备了一些参考资料，帮助你在令人兴奋的自然语言处理（NLP）领域继续学习。
- en: Further reading
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: AWS Well-Architected Labs ([https://www.wellarchitectedlabs.com/](https://www.wellarchitectedlabs.com/))
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Well-Architected Labs ([https://www.wellarchitectedlabs.com/](https://www.wellarchitectedlabs.com/))
- en: Amazon Textract blogs ([https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-textract/](https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-textract/))
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊 Textract 博客 ([https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-textract/](https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-textract/))
- en: Amazon Comprehend blogs ([https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-comprehend/](https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-comprehend/))
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊 Comprehend 博客 ([https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-comprehend/](https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-comprehend/))
- en: Amazon Comprehend workshops ([https://comprehend-immersionday.workshop.aws/](https://comprehend-immersionday.workshop.aws/))
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊 Comprehend 工作坊 ([https://comprehend-immersionday.workshop.aws/](https://comprehend-immersionday.workshop.aws/))
- en: AWS Automating data processing from documents([https://aws.amazon.com/machine-learning/ml-use-cases/document-processing/](https://aws.amazon.com/machine-learning/ml-use-cases/document-processing/))
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS 自动化文档数据处理([https://aws.amazon.com/machine-learning/ml-use-cases/document-processing/](https://aws.amazon.com/machine-learning/ml-use-cases/document-processing/))
