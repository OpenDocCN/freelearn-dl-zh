- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Unveiling Whisper – Introducing OpenAI’s Whisper
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭开Whisper的面纱——介绍OpenAI的Whisper
- en: '**Automatic speech recognition** (**ASR**) is an area of **artificial intelligence**
    (**AI**) that focuses on the interaction between computers and humans through
    speech. Over the years, ASR has made remarkable progress in speech processing,
    and **Whisper** is one such revolutionary ASR system that has gained popularity
    recently.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**自动语音识别**（**ASR**）是**人工智能**（**AI**）的一个领域，专注于通过语音实现计算机与人类之间的互动。多年来，ASR在语音处理方面取得了显著进展，**Whisper**就是一个近期广受欢迎的革命性ASR系统。'
- en: Whisper is an advanced AI **speech recognition** model developed by OpenAI,
    trained on a massive multilingual dataset. With its ability to accurately transcribe
    speech, Whisper has become a go-to tool for voice applications such as assistants,
    transcription services, and more.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper是由OpenAI开发的先进**语音识别**模型，训练于一个庞大的多语言数据集。凭借其准确的语音转录能力，Whisper已成为语音应用程序的首选工具，如助手、转录服务等。
- en: In this chapter, we will explore the basics of Whisper and its capabilities.
    We will start with an introduction to Whisper and its significance in the ASR
    landscape. Then, we will uncover Whisper’s key features and strengths that set
    it apart from other speech models. We will then cover fundamental guidelines for
    implementing Whisper, including initial system configuration and basic usage walkthroughs
    to get up and running.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探索Whisper的基础知识及其功能。我们将从介绍Whisper及其在ASR领域的重要性开始。接着，我们将揭示Whisper的关键特性和优势，了解它与其他语音模型的不同之处。然后，我们将讲解实现Whisper的基本指导原则，包括初步的系统配置和基础使用教程，帮助你快速上手。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Deconstructing OpenAI’s Whisper
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解构OpenAI的Whisper
- en: Exploring key features and capabilities of Whisper
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索Whisper的关键特性和功能
- en: Setting up Whisper
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置Whisper
- en: By the end of this chapter, you will have first-hand experience with Whisper
    and understand how to leverage its core functionalities for your speech-processing
    needs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将亲自体验Whisper，并了解如何利用其核心功能来满足你的语音处理需求。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: As presented in this chapter, you only need a Google account and internet access
    to run the Whisper AI code in **Google Colaboratory**. No paid subscription is
    required to use the free Colab and the GPU version. Those familiar with Python
    can run this code example in their environment instead of using Colab.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章所述，你只需要一个Google帐户和互联网连接即可在**Google Colaboratory**中运行Whisper AI代码。使用免费的Colab和GPU版本不需要付费订阅。熟悉Python的人可以在自己的环境中运行此代码示例，而不必使用Colab。
- en: We are using Colab in this chapter as it allows for quick setup and running
    of the code without installing Python or Whisper locally. The code in this chapter
    uses the small Whisper model, which works well for testing purposes. In later
    chapters, we will complete the Whisper installation to utilize more advanced ASR
    models and techniques.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们使用Colab，因为它允许快速设置并运行代码，而无需在本地安装Python或Whisper。本章中的代码使用的是小型Whisper模型，适用于测试目的。在后续章节中，我们将完成Whisper的安装，以利用更高级的自动语音识别（ASR）模型和技术。
- en: The code examples from this chapter can be found on GitHub at [https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter01](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter01).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的代码示例可以在GitHub上找到，链接为[https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter01](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter01)。
- en: Deconstructing OpenAI’s Whisper
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解构OpenAI的Whisper
- en: In this section, we embark on a journey through the intricate world of voice
    and speech, unveiling the marvels of human vocalization. Voice and speech are
    more than sounds; they are the symphony of human communication orchestrated through
    a harmonious interplay of physiological processes. This section aims to provide
    a foundational understanding of these processes and their significance in speech
    recognition technology, particularly on Whisper. You will learn how Whisper, an
    advanced speech recognition system, emulates human auditory acuity to interpret
    and transcribe speech accurately. This understanding is crucial, as it lays the
    groundwork for comprehending the complexities and capabilities of Whisper.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将踏上探索语音与言语的复杂世界，揭示人类发声的奇迹。语音与言语不仅仅是声音；它们是人类沟通的交响乐，通过生理过程的和谐交互来演绎。本节旨在为你提供对这些过程的基础理解，以及它们在语音识别技术中的重要性，特别是在Whisper中。你将学习到，Whisper作为一个先进的语音识别系统，如何模拟人类的听觉敏锐度，以准确解读和转录语音。这种理解至关重要，因为它为理解Whisper的复杂性和功能奠定了基础。
- en: The lessons in this section are valuable for multiple reasons. First, they offer
    a deep appreciation of voice and speech’s biological and cognitive intricacies,
    which are fundamental to understanding speech recognition technology. Second,
    they provide a clear perspective on the challenges and limitations inherent in
    these technologies, using Whisper as a prime example. This knowledge is not just
    academic; it’s directly applicable to various real-world scenarios where speech
    recognition can play a transformative role, from enhancing accessibility to breaking
    down language barriers.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的课程有多个价值。首先，它们提供了对语音与言语生物学和认知复杂性的深刻理解，而这些复杂性是理解语音识别技术的基础。其次，它们清晰地展示了这些技术固有的挑战与局限性，以Whisper为典型例子。这些知识不仅仅是学术性的；它们直接适用于现实生活中，尤其是在语音识别能发挥变革作用的各种场景中，从提升可访问性到打破语言障碍。
- en: As we proceed, remember that the journey through voice and speech is a blend
    of art and science – a combination of understanding the natural and mastering
    the technological. This section is your first step into the vast and exciting
    world of speech recognition, with Whisper as your guide.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续前进时，请记住，语音与言语的旅程是艺术与科学的融合——理解自然和掌握技术的结合。本节是你进入广阔且令人兴奋的语音识别世界的第一步，Whisper将作为你的引路人。
- en: The marvel of human vocalization – Understanding voice and speech
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类发声的奇迹——理解语音与言语
- en: In the vast expanse of human capabilities, the ability to produce voice and
    speech is a testament to our biological makeup’s intricate complexity. It’s a
    phenomenon that transcends mere sound production, intertwining biology, emotion,
    and cognition to create a medium through which we express our innermost thoughts
    and feelings. This section invites you to explore the fascinating world of voice
    and speech production, not through the lens of an anatomist but with the curiosity
    of a technologist marveling at one of nature’s most sophisticated instruments.
    As we delve into this subject, consider the immense challenges technologies such
    as OpenAI’s Whisper face in interpreting and understanding these uniquely human
    attributes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在人类能力的广阔范围内，发声和言语的能力是我们生物结构复杂性的见证。这是一个超越单纯声音产生的现象，将生物学、情感和认知交织在一起，创造了一种我们用来表达内心思想和感受的媒介。本节邀请你探索发声与言语产生的迷人世界，不通过解剖学家的视角，而是以技术专家的好奇心去惊叹大自然最复杂的工具之一。当我们深入探讨这一课题时，请思考像OpenAI的Whisper这样的技术，在解读和理解这些独特的人的特性时所面临的巨大挑战。
- en: Have you ever pondered the complexity of the systems at play when you casually
    conversed? The effortless nature of speaking belies the elaborate physiological
    processes that enable it. Similarly, when interacting with a speech recognition
    system such as Whisper, do you consider the intricate coding and algorithmic precision
    that allows it to understand and process your words?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾想过，当你随意交谈时，背后运作的复杂系统？说话的轻松掩盖了支撑这一过程的复杂生理机制。类似地，当你与像Whisper这样的语音识别系统互动时，你是否考虑过支撑它理解和处理你话语的精密编码和算法精确性？
- en: The genesis of voice and speech is rooted in the act of breathing. The diaphragm
    and rib cage play pivotal roles in air inhalation and exhalation, providing the
    necessary airflow for voice production. This process begins with the strategic
    opening and closing of the vocal folds within the larynx, the epicenter of vocalization.
    As air from the lungs flows through the vocal folds, it causes them to vibrate,
    generating sound.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 语言和语音的起源根源于呼吸的行为。横膈膜和肋骨在吸气和呼气中扮演着关键角色，为发声提供必要的气流。这个过程从声带在喉部内的战略性开合开始，喉部是发声的核心。当来自肺部的空气通过声带时，它会导致声带振动，从而产生声音。
- en: Speech, on the other hand, materializes through the meticulous coordination
    of various anatomical structures, including the velum, tongue, jaw, and lips.
    These structures sculpt the raw sounds produced by the vocal folds into recognizable
    linguistic patterns, enabling the expression of thoughts and emotions. Mastering
    the delicate balance of muscular control necessary for intelligible communication
    is a protracted journey that requires extensive practice.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，语言的产生是通过各种解剖结构的精确协调来实现的，包括软腭、舌头、下颚和嘴唇。这些结构将声带产生的原始声音雕刻成可识别的语言模式，使得思想和情感得以表达。掌握为了清晰交流所需的肌肉控制的微妙平衡是一个漫长的过程，需要大量的练习。
- en: Understanding the complexities of human voice and speech production is paramount
    in the context of OpenAI’s Whisper. As an advanced speech recognition system,
    Whisper is engineered to emulate the auditory acuity of the human ear by accurately
    interpreting and transcribing human speech. The challenges faced by Whisper mirror
    the intricacies of speech development in humans, underscoring the complexity of
    the task at hand.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 理解人类声音和语言产生的复杂性在OpenAI的Whisper中至关重要。作为一个先进的语音识别系统，Whisper被设计用来模拟人耳的听觉敏锐度，准确地解释和转录人类语言。Whisper面临的挑战反映了人类语言发展的复杂性，凸显了这一任务的复杂程度。
- en: Understanding the intricacies of speech recognition
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解语音识别的复杂性
- en: The human brain’s capacity for language comprehension is a marvel of cognitive
    processing, which has intrigued scientists and linguists for decades. The average
    20-year-old is estimated to know between 27,000 and 52,000 words, typically increasing
    to 35,000 and 56,000 by age 60\. Each of these words, when spoken, exists for
    a fleeting moment – often less than a second. Yet, the brain is adept at making
    rapid decisions, correctly identifying the intended word approximately *98%* of
    the time. How does the brain accomplish this feat with such precision and speed?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 人类大脑在语言理解方面的能力是认知处理的一个奇迹，几十年来一直吸引着科学家和语言学家的关注。估计一个20岁的人大约知道27,000到52,000个词汇，通常到60岁时，这个数字会增加到35,000到56,000个词汇。每一个词在说出时都会存在一个短暂的瞬间——通常不到一秒钟。然而，大脑能够迅速做出决策，正确识别所说词汇的概率大约为*98%*。大脑是如何以如此高的精度和速度完成这一壮举的呢？
- en: The brain as a parallel neural processor
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大脑作为并行神经处理器
- en: The brain’s function as a **parallel processor** is at the core of our speech
    comprehension abilities. Parallel processing means it can handle multiple tasks
    simultaneously. Unlike sequential processors that handle one operation at a time,
    the brain’s parallel processing allows for the simultaneous activation of numerous
    potential word matches. But what does this look like in the context of neural
    activity?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 大脑作为**并行处理器**的功能是我们语言理解能力的核心。并行处理意味着它可以同时处理多个任务。与一次处理一个操作的顺序处理器不同，大脑的并行处理能力允许同时激活多个可能的词汇匹配。那么，这在神经活动的背景下是怎样的呢？
- en: The general thinking is that each word in our vocabulary is represented by a
    distinctprocessing unit within the brain. These units are not physical entities
    but neuronal firing patterns within the cerebral cortex, **neural representations**
    of words. When we hear the beginning of a word, thousands of these units spring
    into action, each assessing the likelihood that the incoming auditory signal matches
    their corresponding word. As the word progresses, many units deactivate upon realizing
    a mismatch, narrowing down the possibilities. This process continues until a single
    pattern of firing activity remains – this is the **recognition point**. The active
    units suppress the activity of others, a mechanism that saves precious milliseconds,
    allowing us to comprehend speech at a rate of up to eight syllables per second.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一般认为，我们词汇中的每个单词都由大脑中的一个独特处理单元表示。这些单元不是物理实体，而是大脑皮层中的神经元放电模式，**单词的神经表征**。当我们听到一个单词的开头时，这些单元中的成千上万会立即启动，每个单元都在评估即将到来的听觉信号与它们所对应的单词的匹配可能性。随着单词的推进，许多单元会因为发现不匹配而停用，从而缩小可能性范围。这个过程会持续，直到只剩下一个放电活动模式——这就是**识别点**。活跃的单元抑制其他单元的活动，这是一个节省宝贵毫秒的机制，使我们能够以每秒最多八个音节的速度理解语言。
- en: Accessing meaning and context
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取意义和语境
- en: The goal of speech recognition extends beyond mere word identification; it involves
    accessing the word’s meaning. Remarkably, the brain begins considering multiple
    meanings before a word is fully articulated. For instance, upon hearing the fragment
    “cap,” the brain simultaneously entertains various possibilities such as “captain”
    or “capital.” This explosion of potential meanings is refined to a single interpretation
    by the recognition point.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别的目标不仅仅是识别单词本身；它还涉及获取单词的意义。令人惊讶的是，大脑在单词完全发音之前就开始考虑多重含义。例如，当听到“cap”这个片段时，大脑同时考虑到多个可能性，如“captain”或“capital”。这种潜在意义的爆发最终会被识别点精炼为单一解释。
- en: Context plays a pivotal role in guiding our understanding. It allows for quicker
    recognition and helps disambiguate words with multiple meanings or homophones.
    For bilingual or multilingual individuals, the language context is an additional
    cue that filters out words from other languages.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 语境在引导我们理解中起着至关重要的作用。它能加快识别速度，并帮助消除多义词或同音异义词的歧义。对于双语或多语使用者来说，语言语境是一个额外的提示，能够过滤掉其他语言的单词。
- en: The nighttime integration process
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 晚上的整合过程
- en: How does the brain incorporate new vocabulary without disrupting the lexicon?
    The answer lies in the **hippocampus**, a brain region where new words are initially
    stored, separate from the cortex’s central word repository. Through a process
    believed to occur during sleep, these new words are gradually woven into the cortical
    network, ensuring the stability of the existing vocabulary.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 大脑如何在不破坏词汇表的情况下整合新词汇？答案在于**海马体**，这是大脑中最初储存新词的区域，独立于大脑皮层的中心词库。通过一种在睡眠过程中发生的被认为的过程，这些新词逐渐融入大脑皮层的网络，确保现有词汇的稳定性。
- en: While our conscious minds rest at night, the brain actively integrates new words
    into our linguistic framework. This nocturnal activity is crucial for maintaining
    the dynamism of our language capabilities, preparing us for the ever-evolving
    landscape of human communication.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的意识在夜间休息时，大脑却在积极地将新词汇融入我们的语言框架。这一夜间活动对保持我们语言能力的动态性至关重要，为我们应对不断发展的交流景观做好准备。
- en: OpenAI’s Whisper – A technological parallel
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAI 的 Whisper – 一个技术平行体
- en: In AI, OpenAI’s Whisper presents a technological parallel to the human brain’s
    speech recognition capabilities. Whisper is a state-of-the-art speech recognition
    system that leverages deep learning to transcribe and understand spoken language
    with remarkable accuracy. Like the brain processes speech through parallel processing,
    Whisper utilizes **neural networks** to analyze and interpret audio signals.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能领域，OpenAI 的 Whisper 提供了一个与人类大脑语音识别能力相似的技术平行体。Whisper 是一款最先进的语音识别系统，通过深度学习技术以惊人的准确性转录和理解口语。像大脑通过并行处理来处理语言一样，Whisper
    利用**神经网络**来分析和解释音频信号。
- en: Whisper’s neural networks are trained on vast datasets, allowing the system
    to recognize various words and phrases across different languages and accents.
    The system’s architecture mirrors the brain’s recognition point by narrowing down
    potential transcriptions until the most probable one is selected.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper的神经网络在庞大的数据集上进行训练，使得系统能够识别不同语言和口音中的各种词汇和短语。该系统的架构与大脑的识别机制相似，通过不断缩小可能的转录范围，直到选择最可能的那一个。
- en: Whisper also exhibits the brain’s ability to integrate context into comprehension.
    The system can discern context from surrounding speech, improving its accuracy
    in real-time transcription. Moreover, Whisper is designed to learn and adapt continuously,
    just as the human brain integrates new words into its lexicon.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper还展示了大脑将上下文融入理解的能力。该系统能够从周围的语音中辨别出上下文，从而提高其实时转录的准确性。此外，Whisper被设计为不断学习和适应，就像人类大脑将新词汇纳入词汇表一样。
- en: Whisper’s algorithms must navigate a myriad of variables, from accents and intonations
    to background noise and speech irregularities, to convert speech to text accurately.
    By dissecting the nuances of voice and speech recognition, we gain insights into
    the challenges and intricacies that Whisper must navigate to process and understand
    human language effectively.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper的算法必须应对各种变量，从口音、语调到背景噪声和语音不规则性，以便准确地将语音转换为文本。通过解剖语音和语音识别的细微差异，我们可以深入了解Whisper在处理和理解人类语言时必须克服的挑战和复杂性。
- en: As we look to the future, the potential for speech recognition technologies
    such as Whisper is boundless. It holds the promise of breaking down language barriers,
    enhancing accessibility, and creating more natural human-computer interactions.
    The parallels between Whisper and the human brain’s speech recognition processes
    underscore the sophistication of our cognitive abilities and highlight the remarkable
    achievements in AI.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们展望未来，像Whisper这样的语音识别技术潜力无穷。它有望打破语言障碍，提升可访问性，并创造更加自然的人机互动。Whisper与人类大脑语音识别过程的相似性突显了我们认知能力的复杂性，并彰显了人工智能的显著成就。
- en: The evolution of speech recognition and the emergence of OpenAI’s Whisper
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语音识别的演变与OpenAI的Whisper的出现
- en: The quest to endow machines with the ability to recognize and interpret human
    speech has been a formidable challenge that has engaged the brightest minds in
    technology for over a century. From the rudimentary dictation machines of the
    late 19th century to the sophisticated algorithms of today, the journey of speech
    recognition technology is a testament to human ingenuity and perseverance.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为机器赋予识别和理解人类语音的能力一直是一个巨大的挑战，吸引了过去一个多世纪里技术领域最聪明的头脑。从19世纪末期的简单听写机到如今复杂的算法，语音识别技术的发展见证了人类的智慧和毅力。
- en: The genesis of speech recognition
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语音识别的起源
- en: The earliest endeavors in speech recognition concentrated on creating vowel
    sounds, laying the groundwork for systems that could potentially decipher phonemes
    – the fundamental units of speech. The iconic Thomas Edison pioneered in this
    field with his invention of dictation machines that could record speech, a technology
    that found favor among professionals inundated with documentation tasks.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最早的语音识别工作集中在创建元音发音上，为能够破译音素——语音的基本单位——的系统奠定了基础。著名的托马斯·爱迪生在这一领域开创了先河，他发明的听写机能够录制语音，这项技术在大量文书工作中的专业人士中得到了广泛应用。
- en: What are phonemes?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是音素？
- en: 'Phonemes refer to the smallest sound units in a language that hold meaning.
    Changing a phoneme can change the entire meaning of a word. Some examples of phonemes
    are the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 音素是语言中最小的有意义的声音单位。改变一个音素可以改变一个词的全部含义。以下是一些音素的例子：
- en: '- The word “cat” has three phonemes: /c/, /a/, and /t/.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '- 词汇“cat”有三个音素：/c/，/a/，和/t/。'
- en: '- The word “bat” also has three phonemes: /b/, /a/, and /t/. The /b/ phoneme
    changes the meaning from “cat.”'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '- 词汇“bat”也有三个音素：/b/，/a/，和/t/。/b/音素将其含义与“cat”区分开来。'
- en: '- The word “sit” has three phonemes: /s/, /i/, and /t/. Both the /s/ and /i/
    phonemes distinguish it from “cat.”'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '- 词汇“sit”有三个音素：/s/，/i/，和/t/。/s/和/i/音素使其与“cat”有所不同。'
- en: 'It was in the 1950s that the field took a significant leap forward. In 1952,
    Bell Labs created the first viable speech recognition system, Audrey, which recognized
    digits 0–9 spoken by a single voice with 90% accuracy. IBM followed in 1962 with
    Shoebox, which recognized 16 English words. In the 1960s, Japanese researchers
    made advances in phoneme and vowel recognition. However, this accuracy was contingent
    on the speaker, highlighting the inherent challenges of speech recognition: the
    variability of voice, accent, and articulation among individuals.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别领域在1950年代取得了重大进展。1952年，贝尔实验室创造了第一个可行的语音识别系统 Audrey，能够以90%的准确率识别由一个人发音的0至9的数字。1962年，IBM推出了Shoebox，能够识别16个英语单词。在1960年代，日本研究人员在音素和元音识别方面取得了进展。然而，这种准确性取决于说话者，突显了语音识别固有的挑战：声音、口音和发音在不同个体之间的差异性。
- en: The advent of machine understanding
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器理解的到来
- en: A significant breakthrough came in the 1970s from the **Defense Advanced Research
    Projects Agency** (**DARPA**) **Speech Understanding Research** (**SUR**) program.
    At Carnegie Mellon University, Alexander Waibel developed the Harpy system, which
    could understand over 1,000 words, a vocabulary on par with a young child. Harpy
    was notable for using **finite state networks** to reduce the search space and
    **beam search** to pursue only the most promising interpretations.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的突破出现在1970年代，来自**国防高级研究计划局**（**DARPA**）的**语音理解研究**（**SUR**）计划。在卡内基梅隆大学，Alexander
    Waibel 开发了 Harpy 系统，该系统能够理解超过1000个单词，词汇量相当于一个小孩子。Harpy 的亮点是使用**有限状态网络**来减少搜索空间，并使用**光束搜索**来追求最有前景的解释。
- en: Finite state networks
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有限状态网络
- en: Finite state networks are computational models comprising states connected by
    transitions. They can recognize patterns in input while staying within the defined
    states. Their job is to reduce the search space for speech recognition by limiting
    valid transitions between speech components. This simplifies decoding possible
    interpretations.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有限状态网络是由状态和它们之间的转换组成的计算模型。它们可以在保持在定义的状态内的同时识别输入中的模式。它们的作用是通过限制语音组件之间有效的转换，来减少语音识别的搜索空间。这简化了可能解释的解码过程。
- en: 'Examples include the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 示例包括以下内容：
- en: '- Phoneme networks that restrict transition between valid adjacent sounds.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '- 限制有效相邻声音之间转换的音素网络。'
- en: '- Word networks that connect permissible words in a grammar.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '- 连接语法中允许的单词的词汇网络。'
- en: '- Speech recognition uses nested finite state networks spanning different linguistic
    tiers.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '- 语音识别使用嵌套的有限状态网络，跨越不同的语言层次。'
- en: Beam search
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 光束搜索
- en: Beam search is an optimization algorithm that pursues only the most promising
    solutions meeting some criteria, pruning away unlikely candidates. It focuses
    computations on interpretations likely to maximize objective metrics. This is
    more efficient than exhaustively evaluating all options.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 光束搜索是一种优化算法，它只追求满足某些标准的最有前景的解决方案，剔除不太可能的候选项。它将计算集中在可能最大化目标指标的解释上。这比穷举地评估所有选项更高效。
- en: 'Examples include the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 示例包括以下内容：
- en: '- Speech recognition beam search, which pursues probable transcriptions while
    filtering out unlikely word sequences.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '- 语音识别光束搜索，追求可能的转录，同时过滤掉不太可能的词序列。'
- en: '- Machine translation beam search, which ensures translations adhere to target
    language rules.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '- 机器翻译光束搜索，确保翻译符合目标语言规则。'
- en: '- Video captioning beam search, which favors captions that fit the expected
    syntax and semantics.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '- 视频字幕光束搜索，偏向符合预期语法和语义的字幕。'
- en: Waibel was motivated to develop Harpy and subsequent systems such as Hearsay-II
    to enable speech translation, converting speech directly to text in another language
    rather than using dictionaries. Speech translation requires tackling the complexity
    of natural language by leveraging linguistic knowledge.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Waibel 的动机是开发 Harpy 和后续系统，如 Hearsay-II，以实现语音翻译，将语音直接转化为另一种语言的文本，而不是使用字典。语音翻译需要通过利用语言学知识来应对自然语言的复杂性。
- en: Other key developments in the 1970s included Bell Labs building the first multivoice
    system. The 1980s saw the introduction of **hidden Markov models** (**HMMs**)
    and statistical language modeling. IBM’s Tangora could recognize 20,000 words
    by the mid-1980s, enabling early commercial adoption. Conceived initially as a
    voice-operated typewriter for office use, Tangora allows users to speak text aloud
    that would then be transcribed. This functionality drastically boosted productivity
    among office staff. The technology marked meaningful progress toward the voice
    dictation systems we know today.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 1970年代的其他关键发展包括贝尔实验室建立了第一个多语音系统。1980年代，**隐马尔可夫模型**（**HMMs**）和统计语言模型的引入推动了技术进步。到1980年代中期，IBM的Tangora能够识别20,000个单词，为早期的商业化应用奠定了基础。Tangora最初被设计为一款为办公室使用的语音操作打字机，用户可以口述文本，然后由系统转录。这项功能大幅提升了办公人员的工作效率。这项技术标志着我们今天所知的语音听写系统的重大进展。
- en: The era of continuous speech recognition
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连续语音识别的时代
- en: Until the 1990s, speech recognition systems relied heavily on template matching,
    which required precise and slow speech in noise-free environments. This approach
    had obvious limitations, as it needed more flexibility to accommodate the natural
    variations in human speech.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 直到1990年代，语音识别系统主要依赖于模板匹配技术，这要求语音必须清晰且缓慢，并且需要在无噪音的环境中进行。这种方法有明显的局限性，因为它缺乏足够的灵活性来适应人类语音的自然变化。
- en: Accuracy and speed increased rapidly in the 1990s with neural networks and increased
    computing power. IBM’s Tangora, leveraging HMMs, marked a significant advancement.
    This technology allowed for a degree of prediction in phoneme sequences, enhancing
    the system’s adaptability to individual speech patterns. Despite requiring extensive
    training data, Tangora could recognize an impressive lexicon of English words.
    Commercial adoption began.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 1990年代，随着神经网络和计算能力的提升，语音识别的准确性和速度迅速提高。IBM的Tangora利用隐马尔可夫模型（HMMs），标志着一个重要进展。这项技术使得语音单元序列可以进行一定程度的预测，从而提高了系统对个体语音模式的适应性。尽管需要大量的训练数据，Tangora仍能识别出令人印象深刻的英语单词词汇表，开始了商业化应用。
- en: In 1997, Dragon’s NaturallySpeaking software, the world’s first continuous speech
    recognizer, arrived as a watershed moment. This innovation eliminated pauses between
    words, facilitating a more natural interaction with machines. As computing power
    increased, neural networks improved accuracy. Systems such as Dragon NaturallySpeaking
    could process 100 words per minute with 97% accuracy.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 1997年，Dragon公司的NaturallySpeaking软件——世界上首个连续语音识别器——问世，成为一个分水岭时刻。这一创新消除了单词之间的停顿，使得与机器的互动更加自然。随着计算能力的提升，神经网络提高了识别准确性。像Dragon
    NaturallySpeaking这样的系统可以以97%的准确率每分钟处理100个单词。
- en: Google’s foray into speech recognition, with its Voice Search app for iPhone,
    harnessed machine learning and cloud computing to achieve unprecedented accuracy
    levels. Google further refined speech recognition with the introduction of Google
    Assistant, which now resides in many smartphones worldwide. By 2001, consumer
    adoption increased through systems such as BellSouth’s voice-activated portal.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌进军语音识别领域，推出了适用于iPhone的Voice Search应用，利用机器学习和云计算达到了前所未有的准确度。谷歌通过推出Google Assistant进一步完善了语音识别技术，现在这个助手已被安装在全球许多智能手机中。到2001年，通过像BellSouth的语音激活门户这样的系统，消费者的使用不断增加。
- en: However, the most significant impact came after widespread smart device adoption
    in 2007, with accurate voice assistants using cloud-based deep learning. In 2010,
    Apple’s Siri captured the public’s imagination by infusing a semblance of humanity
    into voice recognition. Microsoft’s Cortana and Amazon’s Alexa, introduced in
    2014, ignited a competitive landscape among tech giants in the speech recognition
    domain.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最重大影响出现在2007年智能设备普及后，准确的语音助手使用基于云的深度学习。2010年，苹果的Siri通过在语音识别中加入某种人性化元素，激发了公众的想象。2014年，微软的Cortana和亚马逊的Alexa相继推出，在语音识别领域激起了科技巨头之间的竞争。
- en: The connection to OpenAI’s Whisper
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与OpenAI的Whisper的关系
- en: In this innovation continuum, OpenAI’s Whisper emerges as a pivotal development.
    Whisper is a deep learning-based speech recognition system that builds upon the
    aforementioned historical advancements and challenges. It leverages vast datasets
    and sophisticated models to accurately interpret speech across multiple languages
    and dialects. Whisper embodies the culmination of efforts to create a system that
    is not only highly adaptable to individual speech patterns but also capable of
    contextual understanding, a critical aspect that has long eluded previous technologies.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一创新的连续性中，OpenAI 的 Whisper 成为一个关键的进展。Whisper 是一个基于深度学习的语音识别系统，建立在前述历史进步和挑战的基础上。它利用庞大的数据集和复杂的模型，能够准确地识别多种语言和方言的语音。Whisper
    体现了为创造一个不仅能高度适应个人语音模式，还能具备上下文理解能力的系统而付出的努力，这一关键特性是此前技术所难以实现的。
- en: 'The evolution of speech recognition technology, from Edison’s dictation machines
    to OpenAI’s Whisper, represents a relentless pursuit of a more intuitive and seamless
    interface between humans and machines. As we reflect on this journey, it might
    be timely for us to ask: What new frontiers will the next generation of speech
    recognition technologies explore? The potential for further advancements is vast,
    promising a future where the barriers between human communication and machine
    interpretation are virtually indistinguishable. The progress we have witnessed
    thus far is merely the prologue to an era where voice recognition technology will
    be an integral, ubiquitous part of our daily lives.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从爱迪生的听写机到 OpenAI 的 Whisper，语音识别技术的演变代表了人类与机器之间更加直观和无缝的界面追求。回顾这一历程，我们或许该问问自己：下一代语音识别技术将探索哪些新的前沿？进一步发展的潜力巨大，预示着一个未来，在那里人类交流与机器解读之间的障碍几乎无法区分。到目前为止，我们所见证的进步仅仅是一个序章，标志着一个语音识别技术将成为我们日常生活中不可或缺、普遍存在的时代的开始。
- en: In the next section, you will learn about Whisper’s key features and capabilities
    that enable its precise speech recognition prowess. You’ll discover Whisper’s
    robust capabilities that set it apart in various applications. From its exceptional
    **speech-to-text** (**STT**) conversion to its adeptness in handling diverse languages
    and accents, Whisper exemplifies state-of-the-art performance in ASR. We’ll delve
    into the mechanics of how Whisper converts speech to text using advanced techniques,
    including the encoder-decoder transformer model and its training on a vast and
    varied dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将了解 Whisper 的关键特性和功能，这些特性使其具备了精确的语音识别能力。你将发现 Whisper 强大的功能，使其在各种应用中脱颖而出。从其卓越的**语音转文字**（**STT**）转换到其处理多种语言和口音的能力，Whisper
    在自动语音识别（ASR）领域展示了最前沿的性能。我们将深入探讨 Whisper 如何利用先进的技术，将语音转化为文字，包括编码器-解码器变换器模型及其在庞大且多样化的数据集上进行的训练。
- en: Exploring key features and capabilities of Whisper
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 Whisper 的关键特性和能力
- en: In this section, we dive into the heart of OpenAI’s Whisper, uncovering the
    core elements that make it a standout in ASR. This exploration is not merely a
    listing of features; it is an insightful journey into understanding how Whisper
    transcends traditional boundaries of STT conversion, offering an unparalleled
    blend of accuracy, versatility, and ease of use.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨 OpenAI 的 Whisper，揭示其在自动语音识别（ASR）中脱颖而出的核心要素。这一探索不仅仅是功能的罗列；它是一次深入理解
    Whisper 如何超越传统语音转文字（STT）转换界限的洞察之旅，提供了无与伦比的准确性、多功能性和易用性。
- en: The capabilities of Whisper extend beyond mere transcription. You will learn
    about its prowess in real-time translation, support for a wide array of file formats,
    and ease of integration into various applications. These features collectively
    make Whisper not just a tool for transcription but a comprehensive solution for
    global communication and accessibility.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 的功能不仅限于转录。你将了解它在实时翻译中的强大能力，支持多种文件格式，并且能够轻松集成到各种应用中。这些功能共同使 Whisper 不仅仅是一个转录工具，而是一个全球通信和无障碍解决方案。
- en: This section is crucial for those seeking to understand the practical implications
    of Whisper’s features. Whether you’re a developer looking to integrate Whisper
    into your projects, a researcher exploring the frontiers of ASR technology, or
    simply an enthusiast keen on understanding the latest advancements in AI, the
    lessons here are invaluable. They provide a concrete foundation for appreciating
    the technological marvel that is Whisper and its potential to transform how we
    interact with and process spoken language.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分对于那些寻求理解Whisper功能实际意义的人至关重要。无论你是开发者，想将Whisper集成到项目中，还是研究人员，探索语音识别技术的前沿，或仅仅是一个热衷于理解最新人工智能进展的爱好者，这里的知识都是无价的。它们为理解Whisper这一技术奇迹及其改变我们与口语语言互动和处理方式的潜力提供了坚实的基础。
- en: As you engage with this section, remember that the journey through Whisper’s
    capabilities is more than an academic exercise. It’s a practical guide to harnessing
    the power of one of the most advanced speech recognition technologies available
    today, poised to fuel innovation across diverse fields and applications.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习这一部分内容时，请记住，深入了解Whisper的功能不仅仅是一次学术练习。这是一本实践指南，旨在帮助您利用目前最先进的语音识别技术之一，该技术有望推动各个领域和应用的创新。
- en: Speech-to-text conversion
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语音转文本
- en: The cornerstone feature of Whisper is its capability to transcribe spoken language
    into text. Imagine a journalist recording interviews in the field, where they
    could swiftly convert every word spoken into an editable, searchable, and shareable
    text format. This feature isn’t just convenient; it’s a game-changer in environments
    where quick dissemination of spoken information is crucial.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper的核心功能是将口语转录为文本。想象一下，一个记者在现场录制采访时，可以迅速将每一句话转换成可编辑、可搜索和可共享的文本格式。这一功能不仅方便；它在快速传播口语信息至关重要的环境中是一个游戏改变者。
- en: 'The latest iteration of Whisper, called `large-v3` (Whisper-v3), was released
    on November 6, 2023\. Its architecture uses an encoder-decoder transformer model
    trained on 1 million hours of weakly labeled audio and 4 million hours of pseudo-labeled
    audio collected from real-world speech data from the web, making it adept at handling
    diverse recording conditions. Here’s how Whisper converts speech to text:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最新版本的Whisper，称为`large-v3`（Whisper-v3），于2023年11月6日发布。它的架构使用了一个编码器-解码器变换器模型，经过1百万小时的弱标注音频和4百万小时的伪标注音频训练，这些音频来自互联网上真实世界的语音数据，使其能够应对多种录音条件。以下是Whisper如何将语音转化为文本：
- en: The input audio is split into 30-second chunks and converted into log-Mel spectrograms.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入音频被分割成30秒的片段，并转换成对数-梅尔频谱图。
- en: The encoder receives the spectrograms, creating audio representations.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器接收频谱图，创建音频表示。
- en: Training of the decoder follows to predict the corresponding text transcript
    from the encoder representations, including unique tokens for tasks such as language
    identification and timestamps.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器的训练是根据编码器的表示预测相应的文本转录，包括语言识别和时间戳等任务的独特标记。
- en: Log-Mel spectrograms
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对数-梅尔频谱图
- en: Log-Mel spectrograms are obtained by taking the logarithm of the values in the
    **Mel spectrogram**. This compresses the spectrogram’s dynamic range and makes
    it more suitable for input to machine learning models.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对数-梅尔频谱图是通过对**梅尔频谱图**中的数值取对数得到的。这会压缩频谱图的动态范围，并使其更适合输入到机器学习模型中。
- en: Mel spectrograms represent the power spectrum of an audio signal in the frequency
    domain. They are obtained by applying a **Mel filter bank** to the signal’s power
    spectrum, which groups the frequencies into a set of **Mel** **frequency bins**.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 梅尔频谱图表示音频信号在频域中的功率谱。它们是通过将**梅尔滤波器组**应用于信号的功率谱获得的，将频率分组到一组**梅尔频率桶**中。
- en: Mel frequency bins represent sound information in a way that mimics low-level
    auditory perception. They capture the energy at each frequency band and approximate
    the spectrum shape.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 梅尔频率桶以模拟低级听觉感知的方式表示声音信息。它们捕捉每个频带的能量，并近似频谱形状。
- en: 'Whisper-v3 has the same architecture as the previous large models, except that
    the input uses 128 Mel frequency bins instead of 80\. The increase in the number
    of Mel frequency bins from 80 to 128 in Whisper-v3 is significant for several
    reasons:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper-v3的架构与之前的大型模型相同，只是输入使用了128个梅尔频率桶，而不是80个。在Whisper-v3中，梅尔频率桶数量从80个增加到128个，这在多个方面都具有重要意义：
- en: '- **Improves frequency resolution**: Whisper-v3 can capture finer details in
    the audio spectrum using more Mel frequency bins. This higher resolution allows
    the model to distinguish between closely spaced frequencies, potentially improving
    its ability to recognize subtle acoustic differences between phonemes or words.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '- **改善频率分辨率**：Whisper-v3可以使用更多的Mel频率单元捕捉音频频谱中的更精细细节。更高的分辨率使得模型能够区分彼此接近的频率，这有助于提高其识别音素或单词之间微妙声学差异的能力。'
- en: '- **Enhances speech representation**: The increased number of Mel frequency
    bins provides a more detailed representation of the speech signal. This richer
    representation can help the model learn more discriminative features, leading
    to better speech recognition performance.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '- **增强语音表示**：增加的Mel频率单元数提供了更详细的语音信号表示。这种更丰富的表示可以帮助模型学习到更多区分性特征，从而提高语音识别的性能。'
- en: '- **Increases compatibility with human auditory perception**: The Mel scale
    is designed to mimic the non-linear human perception of sound frequencies. Using
    128 Mel frequency bins, Whisper-v3 can more closely approximate the human auditory
    system’s sensitivity to different frequency ranges. This alignment with human
    perception may contribute to improved speech recognition accuracy.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '- **提高与人类听觉感知的兼容性**：Mel尺度旨在模仿人类对声音频率的非线性感知。通过使用128个Mel频率单元，Whisper-v3能够更接近地模拟人类听觉系统对不同频率范围的敏感性。这种与人类感知的对齐可能有助于提高语音识别的准确性。'
- en: '- **Allows the learning of complex patterns**: The higher-dimensional input
    provided by the 128 Mel frequency bins gives Whisper-v3 more data. This increased
    input dimensionality may enable the model to learn more complex and nuanced patterns
    in the speech signal, potentially improving its ability to handle challenging
    acoustic conditions or speaking styles.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '- **允许学习复杂模式**：128个Mel频率单元提供的高维输入为Whisper-v3提供了更多的数据。这种增加的输入维度可能使得模型能够学习语音信号中的更复杂和微妙的模式，可能有助于提高其在困难的声学条件或说话风格下的处理能力。'
- en: While increasing the number of Mel frequency bins can provide these benefits,
    it also comes with a computational cost. Processing higher-dimensional input requires
    more memory and computation, which may impact the model’s training and inference
    speed. However, the improved speech recognition performance offered by the increased
    frequency resolution can outweigh these computational considerations in many applications.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管增加Mel频率单元数可以带来这些好处，但也伴随着计算成本。处理更高维度的输入需要更多的内存和计算，这可能会影响模型的训练和推理速度。然而，增加的频率分辨率所带来的语音识别性能提升，在许多应用中可能会抵消这些计算考虑。
- en: 'This end-to-end approach allows Whisper to convert speech to text directly
    without any intermediate steps. The large and diverse training dataset enables
    Whisper to handle accents, background noise, and technical language much better
    than previous speech recognition systems. Some critical capabilities regarding
    STT conversion are as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这种端到端的方式允许Whisper直接将语音转化为文本，而不需要任何中间步骤。庞大且多样化的训练数据集使得Whisper比以往的语音识别系统更好地应对口音、背景噪音和技术性语言。以下是一些关于语音转文本（STT）转换的关键能力：
- en: Whisper can transcribe speech to text in nearly 100 languages, including English,
    Mandarin, Spanish, Arabic, Hindi, and Swahili. Whisper-v3 has a new language token
    for Cantonese. This multilingual transcription makes it useful for international
    communications.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Whisper可以将语音转录为近100种语言的文本，包括英语、普通话、西班牙语、阿拉伯语、印地语和斯瓦希里语。Whisper-v3新增了粤语的语言标记。这种多语言转录使其在国际交流中非常有用。
- en: The model is robust with accents, background noise, and technical terminology,
    making it adept at handling diverse recording conditions.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型对口音、背景噪音和技术术语具有强大的鲁棒性，使其能够有效处理多样化的录音条件。
- en: Whisper achieves state-of-the-art performance on many speech recognition benchmarks
    without any fine-tuning. This zero-shot learning capability enables the transcription
    of new languages not seen during training.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Whisper在许多语音识别基准测试中实现了最先进的性能，无需任何微调。这种零样本学习能力使得其能够转录在训练过程中未见过的新语言。
- en: The transcription includes punctuation and capitalization, providing properly
    formatted text output. Timestamps are an option if the goal is to align transcribed
    text with the original audio.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转录内容包括标点符号和大写字母，提供了格式化良好的文本输出。如果目标是将转录文本与原始音频对齐，可以选择使用时间戳。
- en: A streaming API enables real-time transcription with low latency, which is essential
    for live captioning and other applications requiring fast turnaround.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个流式API可以实现低延迟的实时转录，这对于实时字幕和其他需要快速处理的应用至关重要。
- en: The open source release facilitates research into improving speech recognition
    and building customized solutions.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源发布促进了对语音识别改进的研究，并帮助构建定制化解决方案。
- en: Overall, Whisper provides highly robust and accurate STT across many languages
    and use cases. The transcription quality exceeds many commercial offerings without
    requiring any customization.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，Whisper在许多语言和使用场景中提供了高度强大和准确的STT转录质量。其转录质量超过了许多商业产品，且无需任何定制化。
- en: Translation capabilities
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 翻译功能
- en: 'In addition to transcription, Whisper can translate speech from one language
    into another. Key aspects of its translation abilities are as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 除了转录，Whisper还可以将语音从一种语言翻译成另一种语言。其翻译能力的关键特点如下：
- en: Whisper supports STT translation from nearly 100 input languages into English
    text. This feature allows transcription and translation of non-English audio in
    one step.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Whisper支持将近100种输入语言的STT翻译成英文文本。该功能可以在一步操作中实现非英语音频的转录和翻译。
- en: The model auto-detects the input language, so users don’t need to specify the
    language manually during translation.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型会自动检测输入语言，因此用户在翻译过程中无需手动指定语言。
- en: Translated output aims to convey the whole meaning of the original audio, not
    just word-for-word substitution. This feature helps capture nuances and context.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 翻译输出旨在传达原始音频的整体意义，而不仅仅是逐字替换。这一特性有助于捕捉细微差别和上下文。
- en: Multitask training on aligned speech and text data allows the development of
    a single model for transcription and translation instead of separate systems.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在对齐的语音和文本数据上进行多任务训练，使得开发了一个用于转录和翻译的单一模型，而非多个独立的系统。
- en: The translation quality approach uses dedicated machine translation models tailored
    to specific language pairs. However, Whisper covers far more languages with a
    single model.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 翻译质量方法采用了专门针对特定语言对的机器翻译模型。然而，Whisper使用一个模型支持更多的语言。
- en: In summary, Whisper pushes the boundaries of speech translation by enabling
    direct STT translation for many languages within one multitask model without compromising
    accuracy. Whisper makes content globally accessible to English speakers and aids
    international communication.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，Whisper通过在一个多任务模型中实现对多种语言的直接STT翻译，突破了语音翻译的边界，且没有妥协准确性。Whisper使得英语使用者能够访问全球内容，并促进国际间的交流。
- en: Support for diverse file formats
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持多种文件格式
- en: Whisper’s versatility extends to its support for various audio file formats,
    including MP3, MP4, MPEG, MPGA, M4A, WAV, and WebM. This flexibility is essential
    in today’s digital landscape, where audio content comes in many forms. For content
    creators working with diverse media files, this means no extra file conversion
    step, ensuring a smoother workflow.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper的多样性扩展到了支持多种音频文件格式，包括MP3、MP4、MPEG、MPGA、M4A、WAV和WebM。这种灵活性在当今的数字环境中至关重要，因为音频内容形式多样。对于处理各种媒体文件的内容创作者来说，这意味着无需额外的文件转换步骤，确保了更流畅的工作流程。
- en: Specifically, Whisper leverages FFmpeg under the hood to load audio files. As
    FFmpeg supports reading many file containers and codecs, Whisper inherits that
    versatility for inputs. Users can even provide audiovisual formats such as `.mp4`
    as inputs, as Whisper will extract just the audio stream to process.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，Whisper在后台利用FFmpeg加载音频文件。由于FFmpeg支持读取多种文件容器和编解码器，Whisper继承了这一多样性，可以处理各种输入。用户甚至可以提供如`.mp4`这样的视听格式作为输入，Whisper会提取其中的音频流进行处理。
- en: Recent additions to the officially supported formats include the open source
    OGG/OGA and FLAC codecs. Their inclusion underscores Whisper’s commitment to supporting
    community-driven and freely licensed media formats alongside more proprietary
    options.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，官方支持的格式新增了开源OGG/OGA和FLAC编码格式。它们的加入彰显了Whisper在支持社区驱动和自由授权的媒体格式方面的承诺，同时也支持更多专有格式选项。
- en: The current file size limit for uploading files to Whisper’s API service is
    25 MB. Whisper handles larger local files by splitting them into segments under
    25 MB each. The wide range of formats – from standard compressed formats to CD-quality
    lossless ones – combined with the generous file size allowance caters to virtually
    any audio content needs when using Whisper.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当前上传文件到Whisper的API服务的文件大小限制为25 MB。Whisper通过将较大的本地文件拆分为每个小于25 MB的片段来处理更大的文件。各种格式——从标准压缩格式到CD质量的无损格式——再加上宽松的文件大小限制，满足了使用Whisper时几乎所有音频内容的需求。
- en: In summary, Whisper sets itself apart by the breadth of audio formats it accepts
    while maintaining leading-edge speech recognition capability. Whisper empowers
    users to feed their content directly without tedious conversion or conditioning
    steps. Whether producing podcasts, audiobooks, lectures, or other speech-centric
    media, Whisper has users covered on the file support side.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，Whisper通过支持多种音频格式，同时保持领先的语音识别能力，脱颖而出。Whisper使用户能够直接输入内容，而无需繁琐的转换或预处理步骤。无论是制作播客、有声书、讲座，还是其他以语音为核心的媒体，Whisper都能提供文件支持。
- en: Ease of use
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 易用性
- en: OpenAI’s release of Whisper represents a significant step in integrating ASR
    capabilities into applications. The Python code snippets available at OpenAI and
    other sites demonstrate the seamless ease with which developers can incorporate
    Whisper’s functionalities. This simplicity enables innovators to leverage ASR
    technology to create novel tools and services with relative simplicity.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI发布Whisper标志着将自动语音识别（ASR）功能集成到应用程序中的重要一步。OpenAI和其他网站提供的Python代码示例展示了开发者如何轻松地将Whisper的功能融入应用程序。这种简便性使创新者能够相对简单地利用ASR技术创造新的工具和服务。
- en: Specifically, the straightforward process of calling Whisper’s API and passing
    audio inputs showcases the accessibility of the technology. Within minutes, developers
    can integrate a production-grade speech recognition system. Multiple model sizes
    allow the fitting of speech-processing capacity for the infrastructure. Whisper
    scales to the use case from lightweight mobile device apps to heavy-duty backends
    in the cloud.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，调用Whisper API并传递音频输入的简单过程展示了该技术的可接触性。开发者只需几分钟即可集成一套生产级的语音识别系统。多种模型大小可根据基础设施的需求调整语音处理能力。Whisper可根据使用场景扩展，从轻量级的移动设备应用到云端的重型后端系统。
- en: Beyond sheer technical integration, Whisper simplifies the process of leveraging
    speech data. The immense corpus of training data produces remarkable off-the-shelf
    accuracy without user fine-tuning, and built-in multilingualism removes the need
    for language specialization. Together, these attributes lower the barrier to productive
    employment of industrial-strength ASR.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 超越单纯的技术集成，Whisper简化了利用语音数据的过程。庞大的训练数据集提供了出色的现成准确性，无需用户进行微调，内建的多语言支持也免除了语言专业化的需求。综合这些特点，Whisper降低了高强度ASR技术生产性应用的门槛。
- en: In summary, by delivering state-of-the-art speech recognition primed for easy
    assimilation into new systems, Whisper stands poised to fuel a Cambrian explosion
    of voice-enabled applications across domains. Its potential to unlock innovation
    is matched only by the ease with which anyone can tap it. The combination of power
    and accessibility that Whisper provides heralds a new era where speech processing
    becomes a readily available ingredient for inventive problem solvers. OpenAI has
    opened the floodgates wide to innovation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，通过提供最先进的语音识别技术，并便于轻松融入新系统，Whisper有望推动语音启用应用程序在各个领域的“寒武纪大爆发”。其解锁创新的潜力与任何人都能轻松使用它的便捷性不相上下。Whisper所提供的强大功能与易用性相结合，预示着一个新的时代——语音处理成为发明性问题解决者随时可以利用的关键元素。OpenAI已全面开放创新的大门。
- en: Multilingual capabilities
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多语言能力
- en: One of Whisper’s most impressive features is its proficiency in numerous languages.
    As of November 2023, it supports 100 languages, from Afrikaans to Welsh. This
    multilingual capability makes Whisper an invaluable global communication, education,
    and media tool.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper最令人印象深刻的特点之一是其在多种语言上的高效能。截至2023年11月，它支持从南非荷兰语到威尔士语的100种语言。这种多语言能力使Whisper成为全球通信、教育和媒体领域中不可或缺的工具。
- en: For example, educators can use Whisper to transcribe lectures in multiple languages,
    aiding students in language learning and comprehension. Interview journalists
    can transcribe and translate conversations, removing language barriers. Customer
    service agents can communicate with customers in their native tongues using Whisper’s
    speech translation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，教育工作者可以使用 Whisper 将讲座转录成多种语言，帮助学生进行语言学习和理解。采访记者可以转录并翻译对话，消除语言障碍。客服代表可以使用
    Whisper 的语音翻译与客户用母语沟通。
- en: Whisper achieves its multilingual prowess through training on a diverse dataset
    of 680,000 hours of audio in 100 languages collected from the internet. This exposure
    allows the model to handle varied accents, audio quality, and technical vocabulary
    when transcribing and translating.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 通过对来自互联网的 68 万小时、涵盖 100 种语言的音频数据集进行训练，展现了其多语言能力。这些数据使模型能够处理多种口音、音频质量以及专业术语的转录和翻译。
- en: While Whisper’s accuracy varies across languages, it demonstrates competitive
    performance even for low-resource languages such as Swahili. Whisper leverages
    its knowledge of other languages for languages with limited training data to make
    inferences. However, there are still challenges in achieving equal proficiency
    across all languages. Performance is weakest for tonal languages such as Mandarin
    Chinese. Expanding the diversity of Whisper’s training data could further enhance
    its multilingual capabilities.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Whisper 在不同语言的准确性有所差异，但即便是低资源语言如斯瓦希里语，它也展现了竞争力的表现。Whisper 利用其对其他语言的理解，在训练数据较少的语言上进行推断。然而，要在所有语言中实现相等的专业水平仍然面临挑战。对于像普通话这样的声调语言，性能最弱。通过扩大
    Whisper 训练数据的多样性，可能进一步增强其多语言能力。
- en: Whisper’s support for nearly 100 languages in a single model is remarkable.
    As Whisper’s multilingual performance continues improving, it could help bring
    us closer to seamless global communication.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 支持在单一模型中处理近 100 种语言，这一点非常了不起。随着 Whisper 的多语言性能不断提升，它可能帮助我们更接近无缝的全球沟通。
- en: Large input handling
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大文件输入处理
- en: Whisper’s ability to handle audio files of up to 25 MB directly addresses the
    needs of those dealing with lengthy recordings, such as podcasters or oral historians.
    Whisper can process segmented audio for longer files, ensuring no context or content
    quality loss.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 能够处理最大 25 MB 的音频文件，直接满足了处理长时间录音（如播客或口述历史学家）的需求。Whisper 可以处理更长文件的分段音频，确保没有上下文或内容质量的丢失。
- en: Flexible file size limits
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 灵活的文件大小限制
- en: The default 25 MB file size limit covers many standard audio lengths while optimizing
    for fast processing. For files larger than 25 MB, Whisper provides options to
    split the audio into segments under 25 MB each. This chunking approach enables
    Whisper to handle files of any length. Segmenting longer files is recommended
    over compression to avoid degrading audio quality and recognition accuracy. When
    segmenting, it’s best to split on pauses or between speakers to minimize loss
    of context. Libraries such as `pydub` simplify audio segmentation.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的 25 MB 文件大小限制涵盖了许多标准音频长度，同时优化了快速处理。对于超过 25 MB 的文件，Whisper 提供了将音频分割成小于 25
    MB 每段的选项。这种分块方式使 Whisper 能够处理任何长度的文件。推荐将较长文件分段，而非压缩，以避免降低音频质量和识别准确度。在分段时，最好在停顿或发言者之间进行拆分，以最大限度地减少上下文丢失。像
    `pydub` 这样的库可以简化音频分段过程。
- en: Maintaining quality across segments
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保持段落质量
- en: Whisper uses internal algorithms to reconstruct context across audio segments,
    delivering high-quality transcriptions for large files. The OpenAI team continues
    to improve Whisper’s ability to provide coherent transcriptions across segments
    with minimal discrepancies.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 使用内部算法重建音频段之间的上下文，为大文件提供高质量的转录。OpenAI 团队持续改进 Whisper，在不同段落之间提供连贯转录的能力，最大限度减少差异。
- en: Expanding access to long-form content
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展对长篇内容的访问
- en: Whisper’s robustness with large files unlocks transcription capabilities for
    long-form content such as lectures, interviews, and audiobooks. Longer files allow
    creators, researchers, and more to leverage audio content efficiently for various
    downstream applications at any scale. As Whisper’s segmentation capabilities improve,
    users can accurately transcribe even extremely lengthy recordings such as multiday
    conferences.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 对大文件的强大处理能力解锁了长篇内容的转录功能，例如讲座、访谈和有声书。较长的文件使创作者、研究人员等能够高效地利用音频内容进行各种下游应用，无论规模大小。随着
    Whisper 的分段能力提升，用户可以准确转录甚至是极其冗长的录音，如多日的会议记录。
- en: In summary, Whisper provides a flexible transcription solution for short- and
    long-form audio through its segmented processing capabilities. Careful segmentation
    preserves quality while enabling Whisper to handle audio files of any length.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，Whisper通过其分段处理能力，为短音频和长音频提供了灵活的转录解决方案。精心的分段处理能保持质量，同时让Whisper能够处理任何长度的音频文件。
- en: Prompts for specialized vocabularies
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专业词汇的提示词
- en: Whisper’s ability to utilize prompts for enhanced transcription accuracy makes
    it extremely useful for specialized fields such as medicine, law, or technology.
    The model can better recognize niche vocabulary and technical jargon during transcription
    by providing a prompt containing relevant terminology.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper利用提示词来提高转录准确性的能力使其在医学、法律或技术等专业领域极具价值。通过提供包含相关术语的提示词，模型能更好地识别转录过程中涉及的专业词汇和技术术语。
- en: For example, a radiologist could supply Whisper with a prompt full of medical
    terms, anatomical structures, and imaging modalities. The prompt would prime Whisper
    to transcribe radiology reports and interpretive findings accurately. Similarly,
    an attorney could include legal terminology and case citations to improve deposition
    or courtroom proceeding transcriptions.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，放射科医生可以为Whisper提供一个包含医学术语、解剖结构和影像学技术的提示词。这个提示词将帮助Whisper准确转录放射学报告和解释性发现。类似地，律师可以在提示词中加入法律术语和案件引用，以改善证词或法庭程序的转录。
- en: 'Here’s an example of a prompt that a radiologist could supply to Whisper to
    transcribe radiology reports and interpretive findings accurately:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个放射科医生可以提供给Whisper的提示例子，用于准确转录放射学报告和解释性发现：
- en: '[PRE0]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This prompt contains medical terms such as “hypertension,” “hyperlipidemia,”
    “CT scan,” “contrast,” “mass,” “right upper lobe,” “spiculated margins,” “mediastinal
    lymphadenopathy,” and “biopsy.” It also contains anatomical structures such as
    “lung” and “mediastinum.” Finally, it includes imaging modalities such as “CT
    scan” and “contrast.”
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 该提示词包含了“高血压”、“高脂血症”、“CT扫描”、“对比剂”、“肿块”、“右上叶”、“分叶缘”、“纵隔淋巴结肿大”和“活检”等医学术语。它还包括“肺”和“纵隔”等解剖结构。最后，它还涉及了“CT扫描”和“对比剂”等影像学技术。
- en: By providing such a prompt, the radiologist can train Whisper to recognize and
    transcribe these terms accurately. This can help improve the accuracy and speed
    of transcribing radiology reports and interpretive findings, ultimately saving
    time and improving radiologists’ workflow.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供这样的提示，放射科医生可以训练Whisper准确识别和转录这些术语。这有助于提高转录放射学报告和解释性发现的准确性和速度，最终节省时间并改善放射科医生的工作流程。
- en: 'Prompts do not need to be actual transcripts – even fictitious prompts with
    relevant vocabulary can steer Whisper’s outputs. Some techniques for effective
    prompting include the following:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 提示词不需要是实际的抄本——即使是包含相关词汇的虚构提示词也能引导Whisper的输出。以下是一些有效提示的技巧：
- en: Using GPT-3 to generate mock transcripts containing target terminology for Whisper
    to emulate. This trains Whisper on the vocabulary.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GPT-3生成包含目标术语的模拟抄本，以供Whisper模仿。这能训练Whisper掌握相关词汇。
- en: Providing a *spelling guide* with proper spellings of industry-specific names,
    products, procedures, uncommon words, acronyms, etc. This helps Whisper learn
    specialized orthography.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供一个*拼写指南*，列出行业特有名称、产品、程序、少见词汇、缩略语等的正确拼写。这有助于Whisper学习专业的拼写规范。
- en: Submitting long, detailed prompts. More context helps Whisper adapt to the desired
    style and lexicon.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供长篇详细的提示词。更多的上下文帮助Whisper适应所需的风格和词汇。
- en: Editing prompts iteratively based on Whisper’s outputs, including missing terms
    or correct errors, further refine the model.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于Whisper的输出反复编辑提示词，包括补充遗漏的术语或纠正错误，进一步完善模型。
- en: Prompting is not a panacea but can improve accuracy for niche transcription
    tasks. With the technical vocabulary provided upfront, Whisper can produce highly
    accurate transcripts, even for specialized audio content. Its flexibility with
    prompting is a crucial advantage of Whisper over traditional ASR systems.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 提示词不是万能的，但可以提高专业转录任务的准确性。通过提前提供技术性词汇，Whisper能够生成高度准确的抄本，甚至对于专业的音频内容也不例外。其在提示词使用上的灵活性是Whisper相对于传统自动语音识别（ASR）系统的关键优势。
- en: Integration with GPT models
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与GPT模型的集成
- en: Whisper’s integration with large language models such as GPT-4 significantly
    enhances its capabilities by enabling refined transcriptions. GPT-4 can correct
    misspellings, add appropriate punctuation, and improve the overall quality of
    Whisper’s initial transcriptions. This combination of cutting-edge speech recognition
    and advanced language processing creates a robust automated transcription and
    document creation system.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 与像 GPT-4 这样的大型语言模型的整合显著增强了其能力，通过精细化转录使其更加精确。GPT-4 可以纠正拼写错误、添加适当的标点符号，并改善
    Whisper 初始转录的整体质量。这种先进的语音识别与高级语言处理的结合，创造了一个强大的自动化转录和文档创建系统。
- en: By leveraging GPT-4’s contextual understanding and language generation strengths
    to refine Whisper’s STT output, the solution can produce highly accurate written
    documents from audio in a scalable manner. The postprocessing technique using
    GPT-4 is particularly more scalable than that depending solely on Whisper’s prompt
    parameter, which has a token limit.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用 GPT-4 的上下文理解和语言生成优势，来精细化 Whisper 的语音转文本输出，该解决方案可以以可扩展的方式从音频生成高度准确的书面文件。使用
    GPT-4 的后处理技术尤其比单纯依赖 Whisper 的提示参数（其有令牌限制）更具可扩展性。
- en: This integration paves the way for automated documentation of meetings, interviews,
    podcasts, and other verbal content. The resulting transcripts can feed into different
    systems, such as search engines, for enhanced discoverability. They also enable
    detailed analysis of oral communications using **natural language processing**
    (**NLP**) techniques.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这一整合为会议、访谈、播客及其他口语内容的自动化文档制作铺平了道路。生成的文字稿可以输入到不同的系统中，例如搜索引擎，以提高可发现性。这些转录也能通过
    **自然语言处理** (**NLP**) 技术进行详细分析。
- en: Overall, combining Whisper and GPT-4 forms an end-to-end solution that unlocks
    the richness of audio data and makes it accessible for a wide range of applications,
    from personal productivity to enterprise knowledge management. This combination
    showcases the immense potential of composing multiple AI systems to create emergent
    capabilities.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，将 Whisper 和 GPT-4 结合起来形成了一个端到端的解决方案，能够挖掘音频数据的丰富性，并使其可应用于从个人生产力到企业知识管理的广泛应用领域。这个结合展示了将多个
    AI 系统组合在一起以创造新兴能力的巨大潜力。
- en: Fine-tunability
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调能力
- en: 'Fine-tuning is a great way to customize Whisper to improve accuracy, support
    new languages, and adapt the model to specific use cases. At a high level, fine-tuning
    takes a pre-trained model, such as Whisper, and trains it further on a downstream
    task using additional data. To perform the tuning, we need an ASR pipeline consisting
    of three components:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是一种很好的方式来定制 Whisper，以提高准确性、支持新语言，并使模型适应特定的用例。高层次上，微调是对预训练模型（如 Whisper）进行进一步训练，使用额外的数据来完成下游任务。要进行微调，我们需要一个包含三个组件的
    ASR 管道：
- en: A feature extractor for preprocessing the raw audio inputs
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于预处理原始音频输入的特征提取器
- en: The model, which performs sequence-to-sequence mapping
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行序列到序列映射的模型
- en: A tokenizer for postprocessing the model outputs to text format
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于后处理模型输出并转换为文本格式的分词器
- en: Fortunately, the Whisper model has an associated feature extractor and tokenizer
    called *WhisperFeatureExtractor* and *WhisperTokenizer*. We will cover this topic
    in more depth in [*Chapter 4*](B21020_04.xhtml#_idTextAnchor113), *Fine-Tuning
    Whisper for Domain and* *Language Specificity*.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Whisper 模型配有一个名为 *WhisperFeatureExtractor* 和 *WhisperTokenizer* 的特征提取器和分词器。我们将在
    [*第4章*](B21020_04.xhtml#_idTextAnchor113) 中更深入地讨论这一主题，*为领域和* *语言特定性微调 Whisper*。
- en: 'Tuning allows the model to specialize and adapt to a particular use case. The
    main reasons to fine-tune Whisper are the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 微调允许模型专门化并适应特定的用例。微调 Whisper 的主要原因如下：
- en: Improve accuracy for a specific domain or use case such as meetings, call center
    data, and so on
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高特定领域或用例的准确性，如会议、呼叫中心数据等
- en: Support new languages not in the original training data
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持原始训练数据中未包含的新语言
- en: Customize the model for an application’s specific vocabulary, audio conditions,
    and so on
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据应用的特定词汇、音频条件等定制模型
- en: Leverage transfer learning to perform better with less data than training from
    scratch
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用迁移学习，在数据量较少的情况下表现得比从零开始训练更好
- en: Fine-tuning is well suited for Whisper because it is trained on diverse data
    and can benefit from specializing further in a particular task or dataset. Tuning
    can happen over the entire Whisper model or at the higher layers closest to the
    output.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对 Whisper 进行微调非常适合，因为它是在多样化数据上训练的，并且可以从进一步专门化于特定任务或数据集中受益。调整可以在整个 Whisper 模型上或最接近输出的高层次进行。
- en: By leveraging transfer learning instead of training from scratch, fine-tuning
    allows the development of high-quality speech recognition with less data and computing
    resources. The active open source community provides ample resources for fine-tuning
    Whisper using Hugging Face Transformers.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 利用迁移学习而非从头训练，微调使得可以在较少的数据和计算资源下开发高质量的语音识别。活跃的开源社区提供丰富的资源，可用于使用 Hugging Face
    Transformers 对 Whisper 进行微调。
- en: Voice synthesis
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语音合成
- en: Whisper plays a vital role in the one-shot voice synthesis workflow by transcribing
    small voice samples to text for model training. Combined with Ozen and Tortoise
    TTS, it enables high-quality voice synthesis with minimal data.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 在一次性语音合成工作流中扮演重要角色，将小语音样本转录为文本用于模型训练。结合 Ozen 和 Tortoise TTS，它能够以最小的数据实现高质量的语音合成。
- en: One-shot voice synthesis
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一次性语音合成
- en: One-shot voice synthesis is a technique for creating a **text-to-speech** (**TTS**)
    system that can synthesize speech in a target voice using only a single recording
    of that speaker’s voice. The process involves training an ML model on a corpus
    of speech from the target speaker and then using that model to generate new speech
    based on text input. One-shot voice synthesis is an active area of research, and
    there are many different approaches to implementing it.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一次性语音合成是一种创建**文本到语音**（**TTS**）系统的技术，它可以仅使用目标说话者声音的单个录音来合成目标语音。该过程涉及在目标说话者的语音语料库上训练
    ML 模型，然后使用该模型基于文本输入生成新的语音。一次性语音合成是一个活跃的研究领域，有许多不同的实现方法。
- en: The Ozen toolkit leverages Whisper to preprocess audio data by extracting speech
    segments, transcribing them with Whisper, and saving them in the LJSpeech format.
    Tortoise TTS uses the preprocessed data to fine-tune a personalized voice synthesis
    model.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Ozen 工具包利用 Whisper 对音频数据进行预处理，通过提取语音片段、使用 Whisper 转录并将其保存为 LJSpeech 格式。Tortoise
    TTS 使用预处理后的数据对个性化语音合成模型进行微调。
- en: LJSpeech format
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: LJSpeech 格式
- en: This format comes from the one used in the *LJSpeech Dataset*, a public-domain
    speech dataset comprising 13,100 short audio clips of a single speaker reading
    passages from 7 non-fiction books. A transcription is provided for each clip.
    These clips are between 1 and 10 seconds in length, with a total length of approximately
    24 hours ([https://keithito.com/LJ-Speech-Dataset](https://keithito.com/LJ-Speech-Dataset)).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 此格式源自 *LJSpeech 数据集*，一个公有领域的语音数据集，包含一位发音者朗读 7 本非虚构书籍中的段落的 13,100 个短音频剪辑。每个剪辑都提供了转录。这些剪辑长
    1 到 10 秒不等，总长度约为 24 小时 ([https://keithito.com/LJ-Speech-Dataset](https://keithito.com/LJ-Speech-Dataset))。
- en: Tortoise TTS is a neural TTS model that enables high-quality voice synthesis
    with minimal data, even a single audio sample of the target voice. After preprocessing
    data with Ozen and Whisper, Tortoise TTS can be fine-tuned on the new voice and
    used to synthesize speech mimicking that voice.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Tortoise TTS 是一种神经 TTS 模型，可以利用最少的数据实现高质量的语音合成，甚至是目标语音的单个音频样本。在使用 Ozen 和 Whisper
    预处理数据后，Tortoise TTS 可以对新的语音进行微调，并用于合成模仿该语音的语音。
- en: The combination of Whisper, Ozen, and Tortoise TTS enables the building of personalized
    voice synthetic inferences from just a few seconds of audio data without extensive
    data collection or cleaning. Whisper’s robust ASR handles transcription, Ozen
    preprocesses the data, and Tortoise TTS regulates voice synthesis.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper、Ozen 和 Tortoise TTS 的结合，可以从仅有几秒的音频数据中构建个性化语音合成推理，无需进行大量的数据收集或清理。Whisper
    的强大 ASR 处理转录，Ozen 预处理数据，而 Tortoise TTS 调节语音合成。
- en: Speech diarization
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语音分离
- en: Whisper provides robust speech recognition, while external libraries such as
    `pyannote.audio` can be used on top of Whisper for speaker diarization by utilizing
    word-level timestamps from Whisper.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 提供强大的语音识别能力，而像 `pyannote.audio` 这样的外部库可以在 Whisper 之上通过利用 Whisper 的单词级时间戳来进行说话者分离。
- en: Diarization is partitioning an audio recording into homogeneous segments according
    to the speaker’s identity. It answers the question “Who spoke when?” in an audio
    recording. The goal is to separate speech segments belonging to different speakers
    without knowing who the speakers are.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 说话者分段（Diarization）是根据说话者的身份将音频记录划分为同质化的片段。它回答了“谁在什么时候说话？”这个问题。目标是将属于不同说话者的语音片段分开，而不需要知道说话者是谁。
- en: Out of the box, Whisper does not support speaker diarization. It generates transcriptions
    without speaker labels. However, Whisper outputs timestamps at the word level
    in transcriptions. These timestamps, along with external speaker diarization libraries
    such as `pyannote.audio`, match the transcriptions with the speaker segments and
    thus enable speaker labeling.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Whisper 不支持说话者分段。它生成的转录没有说话者标签。然而，Whisper 在转录中输出了按单词级别的时间戳。通过这些时间戳，以及
    `pyannote.audio` 等外部说话者分段库，可以将转录与说话者片段匹配，从而实现说话者标注。
- en: In conclusion, OpenAI’s Whisper is a testament to the incredible advancements
    in speech recognition technology. Its capabilities, from multilingual transcription
    to integration with advanced language models, offer a glimpse into a future where
    the spoken word seamlessly integrates with the digital world. As we continue exploring
    and expanding its applications, Whisper promises to revolutionize our process
    of understanding and utilizing human speech.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，OpenAI 的 Whisper 是语音识别技术惊人进步的见证。它的功能，从多语言转录到与高级语言模型的集成，展示了一个未来的前景——在这个未来，口语与数字世界无缝融合。随着我们继续探索和拓展它的应用，Whisper
    有望彻底改变我们理解和利用人类语言的方式。
- en: In the next section, we take a practical turn, guiding you through the first
    steps of deploying OpenAI’s Whisper. This section is pivotal for anyone eager
    to harness the power of Whisper for audio transcription, as it lays out the straightforward
    procedures to get started. Here, you will learn how to set up and use Whisper
    through a user-friendly web interface and a more hands-on approach using Google
    Colab.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将转向实际操作，指导您部署 OpenAI 的 Whisper 的第一步。这一部分对于那些渴望利用 Whisper 进行音频转录的人来说至关重要，因为它为入门提供了简便的步骤。在这里，您将学习如何通过用户友好的
    Web 界面和更为动手的方式使用 Google Colab 来设置和使用 Whisper。
- en: Setting up Whisper
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 Whisper
- en: The journey begins with exploring how to access Whisper via Hugging Face’s web
    interface, designed for simplicity and convenience. This method is perfect for
    those who prefer to avoid the intricacies of coding and software installation.
    You will learn to easily upload audio files and receive transcriptions directly
    through a web browser, making Whisper accessible to a broader audience.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这段旅程从探索如何通过 Hugging Face 的 Web 界面访问 Whisper 开始，该界面设计简洁便捷，非常适合那些希望避免编码和软件安装复杂性的用户。您将学会轻松上传音频文件，并通过网页浏览器直接接收转录结果，使
    Whisper 更加容易被更广泛的用户群体使用。
- en: Next, we will show you how to install and run Whisper in a cloud environment
    such as Google Colab. This approach is tailored for those who seek a more involved
    experience and wish to understand Whisper’s workings from a closer perspective.
    We will walk through the Whisper and FFmpeg installation for audio and video support,
    demonstrating how to transcribe files and view the results within the Colab environment.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将向您展示如何在 Google Colab 等云环境中安装和运行 Whisper。此方法适用于那些希望更深入了解 Whisper 工作原理的用户。我们将介绍如何安装
    Whisper 和 FFmpeg 以支持音频和视频功能，并演示如何转录文件以及如何在 Colab 环境中查看结果。
- en: Importantly, this section concerns the *how* and the *why*. The ease of setting
    up Whisper underscores its potential for widespread application, from academic
    research to real-world business solutions. By the end of this section, you will
    have gained the technical know-how to start using Whisper and an appreciation
    of its accessibility and versatility. As you progress, remember that these initial
    steps are crucial in unlocking Whisper’s full potential, paving the way for more
    advanced exploration and innovation in speech recognition.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，这一部分涉及到 *如何做* 和 *为什么做*。设置 Whisper 的简便性突显了它在各个领域广泛应用的潜力，从学术研究到现实世界的商业解决方案。到这一部分结束时，您将掌握开始使用
    Whisper 的技术知识，并且能够欣赏到它的可达性和多功能性。在继续前进的过程中，请记住，这些初步步骤对于解锁 Whisper 的全部潜力至关重要，为更高级的探索和语音识别创新铺平道路。
- en: Using Whisper via Hugging Face’s web interface
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 Hugging Face 的 Web 界面使用 Whisper
- en: To use Whisper for audio transcription, you don’t need to create an OpenAI account
    or obtain API keys. Whisper is an open source project available on GitHub, so
    you can use it independently of the OpenAI API. You can install and run Whisper
    on your local machine or in a cloud environment such as Google Colab without any
    OpenAI account or API keys. This accessibility is part of what makes Whisper a
    convenient tool for STT transcription.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Whisper 进行音频转录，你无需创建 OpenAI 账户或获取 API 密钥。Whisper 是一个开源项目，托管在 GitHub 上，因此你可以独立于
    OpenAI API 使用它。你可以在本地机器或像 Google Colab 这样的云环境中安装和运行 Whisper，而不需要任何 OpenAI 账户或
    API 密钥。正是这种便捷性使得 Whisper 成为一个适用于语音转文字（STT）转录的工具。
- en: To provide a more straightforward and user-friendly experience with Whisper,
    we will start by accessing it through web interfaces, which don’t require dealing
    with repositories or Python libraries.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供更简便和用户友好的体验，我们将通过 Web 界面访问 Whisper，这样无需处理代码库或 Python 库。
- en: 'Here’s a simplified guide:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简化的指南：
- en: '**Access Whisper**: Visit the Hugging Face Whisper space at [https://huggingface.co/spaces/openai/whisper](https://huggingface.co/spaces/openai/whisper).'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**访问 Whisper**：访问 Hugging Face Whisper 空间，网址为 [https://huggingface.co/spaces/openai/whisper](https://huggingface.co/spaces/openai/whisper)。'
- en: '**Upload audio**: Upload or record your audio file directly on the website.
    There is an audio file available at [https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter01/Learn_OAI_Whisper_Sample_Audio01.m4a](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter01/Learn_OAI_Whisper_Sample_Audio01.m4a).'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**上传音频**：直接在网站上上传或录制你的音频文件。可以在 [https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter01/Learn_OAI_Whisper_Sample_Audio01.m4a](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter01/Learn_OAI_Whisper_Sample_Audio01.m4a)
    找到一个音频文件。'
- en: '**Transcribe**: Whisper will automatically transcribe the audio into text.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**转录**：Whisper 会自动将音频转录为文本。'
- en: '**Review and download**: If needed, you can review and download the transcription.'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**查看并下载**：如果需要，你可以查看并下载转录内容。'
- en: 'You can see an overview of the Hugging Face Whisper space here:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在此查看 Hugging Face Whisper 空间的概述：
- en: '![Figure 1.1 – Whisper: A Hugging Face space by OpenAI](img/B21020_01_1.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1 – Whisper：OpenAI 提供的 Hugging Face 空间](img/B21020_01_1.jpg)'
- en: 'Figure 1.1 – Whisper: A Hugging Face space by OpenAI'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – Whisper：OpenAI 提供的 Hugging Face 空间
- en: This method provides an easy way to access Whisper’s capabilities without the
    technical requirements of setting up the software locally. It is perfect for those
    who want to transcribe audio quickly without installation hassles.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法提供了一种简便的方式来访问 Whisper 的功能，无需在本地设置软件的技术要求。它非常适合那些希望快速转录音频而不需要安装的用户。
- en: Using Whisper via Google Colaboratory
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 Google Colaboratory 使用 Whisper
- en: 'This following step-by-step guide will help you effectively use Whisper AI
    in Google Colab for transcribing speech to text. Here’s a step-by-step guide on
    using OpenAI’s Whisper AI in Google Colab, based on your provided text and formatted
    with markdown for clarity:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一步一步的指南，将帮助你在 Google Colab 中有效使用 Whisper AI 进行语音转文字。这里有一个基于你提供的文本的指南，已使用
    markdown 格式化，以便于理解：
- en: 'Installing Google Colab:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 Google Colab：
- en: Visit Google Drive and set up your Google account if you don’t already have
    one.
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 Google Drive 并设置你的 Google 账户，如果你还没有的话。
- en: In the top left-hand corner, click `Google Colaboratory`.
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左上角，点击 `Google Colaboratory`。
- en: Select the first option, **Colaboratory**, and click **Install**.
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择第一个选项，**Colaboratory**，然后点击 **安装**。
- en: After installation, click **Done** and close the **Connect more** **apps** window.
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装完成后，点击 **完成** 并关闭 **连接更多** **应用** 窗口。
- en: 'Configuring Google Colab:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置 Google Colab：
- en: Open Google Drive.
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 Google Drive。
- en: Click `Untitled.ipynb` and giving it a more descriptive name.
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 `Untitled.ipynb` 并给它起一个更具描述性的名字。
- en: Click the **Runtime** menu, select **Change runtime type**, and set the **Hardware
    accelerator** option to **T4 GPU**. (If you are using the free version of Google
    Colab, then a T4 GPU should be an option.)
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **运行时** 菜单，选择 **更改运行时类型**，并将 **硬件加速器** 选项设置为 **T4 GPU**。（如果你使用的是 Google Colab
    的免费版本，那么 T4 GPU 应该是一个可选项。）
- en: 'Installing Whisper AI on Google Colab:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Google Colab 上安装 Whisper AI：
- en: Open your Colab notebook.
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你的 Colab 笔记本。
- en: 'Paste the following code to install Whisper and FFmpeg (for audio and video
    file support):'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 粘贴以下代码来安装 Whisper 和 FFmpeg（用于音频和视频文件支持）：
- en: '[PRE1]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Run the code by selecting the **Run** icon.
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **运行** 图标运行代码。
- en: 'Running Whisper AI:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 Whisper AI：
- en: In Colab, click the **Files** icon in the left-hand navigation menu.
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Colab 中，点击左侧导航菜单中的 **文件** 图标。
- en: Drag and drop the audio or video file you want to transcribe.
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拖动并放下您要转录的音频或视频文件。
- en: Click **OK** to acknowledge that uploaded files will be deleted when the runtime
    is recycled.
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **确定** 以确认上传的文件将在运行时回收时被删除。
- en: Your file should now appear under the **Files** section. You might need to press
    the **Refresh** icon to make the file appear.
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您的文件应出现在 **Files** 部分。您可能需要按下 **刷新** 图标才能使文件显示出来。
- en: 'Paste the following code to transcribe the file with Whisper:'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下代码粘贴到 Whisper 中以转录文件：
- en: '[PRE2]'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For testing and based on your memory, processing, and GPU availability, use
    the `small.en` Whisper model. However, there are other model sizes: tiny, base,
    small, medium, and large.'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据您的内存、处理能力和 GPU 可用性，使用 `small.en` Whisper 模型进行测试。不过，还有其他模型大小：tiny、base、small、medium
    和 large。
- en: Run the code by clicking the `your-audio-file-here.txt` (displays the transcription
    text), `your-audio-file-here.vtt` (displays timed text tracks using the WEBVTT
    format), `your-audio-file-here.tsv` (displays text tracks using the tab-separated
    format), `your-audio-file-here.json` (displays the transcription text using the
    JSON format), and `your-audio-file-here.srt` (displays the transcription text
    using the SubRip format) in the **Files** section of Colab. If you do not see
    them, then you might need to press the **Refresh** icon in Colab. To download
    any of these files, hover over the file, select the ellipsis menu, and click **Download**.
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击 `your-audio-file-here.txt`（显示转录文本）、`your-audio-file-here.vtt`（以 WEBVTT
    格式显示时间化文本）、`your-audio-file-here.tsv`（以制表符分隔格式显示文本轨道）、`your-audio-file-here.json`（以
    JSON 格式显示转录文本）和 `your-audio-file-here.srt`（以 SubRip 格式显示转录文本）来运行代码，这些文件位于 Colab
    的 **Files** 部分。如果您没有看到这些文件，您可能需要按下 Colab 中的 **刷新** 图标。要下载这些文件中的任何一个，将鼠标悬停在文件上，选择省略号菜单，然后点击
    **下载**。
- en: Whisper’s output formats
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Whisper 的输出格式
- en: 'In addition to plain text (TXT), Whisper supports various output formats, including
    JSON, WEBVTT, SRT, and TSV. Each format serves a different purpose and is suitable
    for other use cases:'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 除了纯文本（TXT）格式外，Whisper 还支持多种输出格式，包括 JSON、WEBVTT、SRT 和 TSV。每种格式具有不同的用途，并适用于不同的使用场景：
- en: '- **JSON (JavaScript Object Notation)**: This is a versatile and widely used
    data interchange format. In the context of Whisper, the JSON output includes detailed
    information about the transcription, such as the task, language, duration, segments,
    and other metadata. Each segment contains the start and end times, the transcribed
    text, and other details such as average log probability, compression ratio, and
    no-speech probability.'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- **JSON（JavaScript 对象表示法）**：这是一种多用途且广泛使用的数据交换格式。在 Whisper 中，JSON 输出包含有关转录的详细信息，如任务、语言、持续时间、片段和其他元数据。每个片段包含开始和结束时间、转录文本以及其他细节，如平均对数概率、压缩比率和无语音概率。'
- en: '- **WEBVTT (Web Video Text Tracks)**: This is a popular format for displaying
    captions or subtitles for HTML5 videos. It’s designed to be easy to read and write,
    making it a good choice for web developers. Whisper’s output in this format can
    be directly used as video captions.'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- **WEBVTT（网页视频文本轨道）**：这是一种流行的格式，用于显示 HTML5 视频的字幕或说明。它旨在易于阅读和编写，是 Web 开发人员的不错选择。Whisper
    以此格式输出的内容可以直接用作视频字幕。'
- en: '- **SRT (SubRip Text)**: This is another widely used format for subtitles and
    captions. Most video players and video editing software support it. Each entry
    in an SRT file includes a sequence number, start and end times, and the corresponding
    text. Whisper can generate SRT files that can be used to add subtitles to videos.'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- **SRT（SubRip 字幕文本）**：这是另一种广泛使用的字幕和说明格式。大多数视频播放器和视频编辑软件都支持它。SRT 文件中的每个条目包括序列号、开始和结束时间以及相应的文本。Whisper
    可以生成 SRT 文件，这些文件可以用于为视频添加字幕。'
- en: '- **TSV (Tab-Separated Values)**: This is a simple text format for storing
    data in a tabular structure, similar to CSV, but with tabs as separators. It’s
    not as commonly used as the other formats in the context of Whisper, but it can
    be helpful in specific applications where a simple, tabular format is needed.'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- **TSV（制表符分隔值）**：这是一种简单的文本格式，用于以表格结构存储数据，类似于 CSV，但使用制表符作为分隔符。在 Whisper 的上下文中，它不像其他格式那样常用，但在需要简单表格格式的特定应用中可能会很有用。'
- en: Each of these formats has its strengths and is suited to different applications.
    JSON is great for applications needing detailed transcription metadata, while
    WEBVTT and SRT are ideal for video captioning or subtitling applications. TSV,
    on the other hand, provides a simple, tabular representation of the data.
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些格式各有优点，适用于不同的应用场景。JSON 适合需要详细转录元数据的应用，而 WEBVTT 和 SRT 则非常适用于视频字幕或字幕应用。另一方面，TSV
    提供了简单的表格化数据表示。
- en: Now that you have mastered the basics of using OpenAI’s Whisper AI in Google
    Colab, it’s time to explore its more advanced capabilities. The following section
    will introduce you to additional parameters and options you can run in Google
    Colab. These enhancements enable you to customize the transcription process more
    precisely, cater to specific language requirements, and handle various audio conditions.
    Let’s dive deeper and unlock the full potential of Whisper’s advanced features.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在你已经掌握了在 Google Colab 中使用 OpenAI 的 Whisper AI 的基础知识，是时候探索其更高级的功能了。以下部分将介绍你可以在
    Google Colab 中运行的额外参数和选项。这些增强功能使你能够更精确地自定义转录过程，满足特定语言需求，并处理各种音频条件。让我们深入了解，释放 Whisper
    高级功能的全部潜力。
- en: Expanding on the basic usage of Whisper
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展 Whisper 的基本用法
- en: 'You can leverage more advanced parameters with the `!whisper` command in Google
    Colab to customize the transcription process. Here are some additional options
    you can utilize:'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在 Google Colab 中使用 `!whisper` 命令利用更多高级参数，来自定义转录过程。以下是一些你可以使用的附加选项：
- en: '`–model small.en` with the language code. For instance, for Spanish, use `--model
    small –-``language Spanish`.'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`–model small.en` 与语言代码。例如，对于西班牙语，使用 `--model small –-``language Spanish`。'
- en: '`Detecting language using up to the first 30 seconds…`. Try it by running the
    following, for example:'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`正在检测语言，最多使用前 30 秒…`。例如，可以通过运行以下命令来试试：'
- en: '[PRE3]'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '--output_dir flag:'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: --output_dir 标志：
- en: '[PRE4]'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '–-clip_timestamps to process the first 5 seconds of the audio clip:'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: –-clip_timestamps 用于处理音频片段的前 5 秒：
- en: '[PRE5]'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'temperature parameter controls the randomness in generation tasks such as translation.
    Lower values produce more predictable results:'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: temperature 参数控制生成任务（如翻译）中的随机性。较低的值会产生更可预测的结果：
- en: '[PRE6]'
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '!whisper "YOUR_FILE_NAME.mp3" –model medium –-temperature 0 --beam-size 2'
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '!whisper "YOUR_FILE_NAME.mp3" –model medium –-temperature 0 --beam-size 2'
- en: '[PRE7]'
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: These advanced parameters allow you to fine-tune the Whisper AI transcription
    to your specific needs, improving accuracy and tailoring the output to your requirements.
    Experiment with these options to see which combination works best for your audio
    files.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些高级参数允许你根据特定需求对 Whisper AI 转录进行微调，提高准确性并根据要求定制输出。可以尝试这些选项，看看哪种组合最适合你的音频文件。
- en: Summary
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: As we conclude [*Chapter 1*](B21020_01.xhtml#_idTextAnchor016), we have traversed
    a comprehensive path that laid the foundation for understanding and utilizing
    this advanced speech recognition system. Here are the milestones of our journey
    together.
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随着我们结束 [*第 1 章*](B21020_01.xhtml#_idTextAnchor016)，我们已经走过了一段全面的旅程，为理解和利用这个先进的语音识别系统奠定了基础。以下是我们共同旅程的里程碑。
- en: We began our journey with a deep dive into the marvel of human vocalization,
    exploring the complex interplay of biology, emotion, and cognition in voice and
    speech production. This exploration was about understanding the physiological
    processes and appreciating the immense challenges technologies such as OpenAI’s
    Whisper face in interpreting these uniquely human attributes. This understanding
    is vital for enjoying Whisper’s capabilities and the sophistication required to
    transcribe human speech accurately.
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们从深入探讨人类发声的奇迹开始，探索了生物学、情感和认知在语音和语言产生中的复杂相互作用。这一探索旨在理解生理过程，并欣赏像 OpenAI 的 Whisper
    这样技术在解读这些独特的人类特征时所面临的巨大挑战。这一理解对于享受 Whisper 的功能以及准确转录人类语音所需的复杂性至关重要。
- en: Next, we delved into Whisper’s key features and capabilities, which set it apart
    as a significant leap in the realm of ASR. Whisper demonstrates its robustness
    and versatility, from its exceptional ability to convert speech to text across
    nearly 100 languages and handle accents and background noise to its capacity for
    real-time transcription and support for a wide range of audio file formats. This
    section illuminated Whisper’s transformative power in various applications, from
    journalism to international communications, showcasing its state-of-the-art performance
    and ease of integration into diverse projects.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们深入探讨了Whisper的关键特性和能力，这些特性使其成为自动语音识别（ASR）领域的一次重要飞跃。Whisper展示了其强大的稳定性和多功能性，从其卓越的语音转文本能力，涵盖近100种语言、处理各种口音和背景噪音，到实时转录的能力和对多种音频文件格式的支持。本节阐明了Whisper在各种应用中的变革性力量，从新闻业到国际通信，展示了其最先进的性能以及轻松融入各种项目的能力。
- en: Lastly, we explored the practical aspects of setting up and using Whisper through
    Hugging Face’s web interface for a straightforward experience and via Google Colab
    for a more hands-on approach. This section provided a step-by-step guide to effectively
    use Whisper for transcribing speech to text, highlighting its accessibility and
    convenience for users.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们探讨了通过Hugging Face的Web界面和通过Google Colab的更动手式方式，如何设置和使用Whisper，以便获得更简便的体验。本节提供了一个逐步指南，帮助用户有效地使用Whisper进行语音转文本，突出其易用性和便捷性。
- en: Having reached the end of this chapter, you should have gained a comprehensive
    understanding of Whisper’s functionalities and acquired the skills to apply this
    technology in various contexts. The knowledge and insights gleaned here are invaluable
    for anyone looking to harness the power of advanced speech recognition.
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完成本章后，你应该对Whisper的功能有了全面的了解，并掌握了在不同场景中应用这项技术的技能。这里获得的知识和见解对于任何希望利用先进语音识别技术的人来说都具有不可估量的价值。
- en: As we look forward to [*Chapter 2*](B21020_02.xhtml#_idTextAnchor058), *Understanding
    the Core Mechanisms of Whisper*, we prepare to delve into the nuts and bolts of
    Whisper’s ASR system. This chapter will shed light on Whisper’s critical components
    and functions, enhancing our ability to optimize its performance and implement
    best practices. Whether for voice assistants, transcription services, or other
    innovative applications, this foundational knowledge is essential for efficiently
    harnessing Whisper’s capabilities. Prepare to deepen your understanding of how
    Whisper functions at a high level, dissect its components, and discover practical
    techniques for optimizing its performance.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们展望[*第二章*](B21020_02.xhtml#_idTextAnchor058)，*理解Whisper的核心机制*时，我们将深入研究Whisper的ASR系统的基本构造。该章将揭示Whisper的关键组件和功能，提升我们优化其性能和实施最佳实践的能力。无论是用于语音助手、转录服务，还是其他创新应用，这些基础知识对于高效利用Whisper的能力至关重要。准备好深入了解Whisper如何在高级层面上运作，剖析其组件，并发现优化其性能的实用技巧。
