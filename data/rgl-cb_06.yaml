- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Deep Learning Reminders
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习提示
- en: '**Deep learning** is the specific domain of machine learning based on neural
    networks. Deep learning is known to be particularly powerful with unstructured
    data, such as text, audio, and image, but can be useful for time series and structured
    data too. In this chapter, we will review the basics of deep learning, from a
    perceptron to training a neural network. We will provide recipes for training
    neural networks for three main use cases: regression, binary classification, and
    multiclass classification.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习** 是基于神经网络的机器学习特定领域。深度学习被认为在处理非结构化数据（如文本、音频和图像）时特别强大，但对于时间序列和结构化数据也能发挥作用。在本章中，我们将回顾深度学习的基础知识，从感知机到神经网络的训练。我们将提供训练神经网络的三大主要用例的配方：回归、二分类和多类分类。'
- en: 'In this chapter, we’ll cover the following recipes:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下配方：
- en: Training a perceptron
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练感知机
- en: Training a neural network for regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个回归神经网络
- en: Training a neural network for binary classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个二分类神经网络
- en: Training a multiclass classification neural network
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个多类分类神经网络
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, you will train a perceptron, as well as several neural networks.
    To do so, the following libraries are required:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将训练一个感知机以及多个神经网络。为此，需要以下库：
- en: NumPy
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: pandas
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas
- en: scikit-learn
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn
- en: PyTorch
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch
- en: torchvision
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: torchvision
- en: Training a perceptron
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练感知机
- en: The perceptron is arguably the building block of deep learning. Even if the
    perceptron is not directly used in production systems, understanding what it is
    can be an asset for building a strong foundation in deep learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机可以说是深度学习的基石。即便在生产系统中没有直接使用感知机，理解其原理对于构建深度学习的坚实基础是非常有帮助的。
- en: In this recipe, we will review what a perceptron is and then train one using
    scikit-learn on the Iris dataset.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们将回顾感知机的基本概念，然后使用 scikit-learn 在鸢尾花数据集上训练一个感知机。
- en: Getting started
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 入门
- en: The perceptron is a machine learning method first proposed to mimic a biological
    neuron. It was first proposed in the 1940s and then implemented in the 1950s.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机是一种最早提出用于模拟生物神经元的机器学习方法。它最早在1940年代提出，并在1950年代得到了实现。
- en: 'From a high-level point of view, a neuron can be described as a cell that receives
    input signals and fires a signal itself when the sum of the input signals is above
    a given threshold. This is exactly what a perceptron does; all you have to do
    is the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，神经元可以被描述为一种接收输入信号并在输入信号的和超过某个阈值时发出信号的细胞。这正是感知机的工作原理；你只需做以下操作：
- en: Replace the input signals with features
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用特征替换输入信号
- en: Apply a weighted sum to those features and apply an activation function to it
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对这些特征应用加权和，并对其应用激活函数
- en: Replace the output signal with a prediction
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用预测值替代输出信号
- en: 'More formally, assuming *n* input features ![](img/B19629_06_01.png) and n
    weights ![](img/formula_06_002.png), the output ŷ of a perceptron is the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，假设有 *n* 个输入特征 ![](img/B19629_06_01.png) 和 *n* 个权重 ![](img/formula_06_002.png)，则感知机的输出
    ŷ 如下所示：
- en: '![](img/formula_06_003.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/formula_06_003.jpg)'
- en: 'Where ![](img/formula_06_004.png) is the bias and *g* is the activation function;
    historically, this is the step function, which returns 1 for a positive input
    value, 0 otherwise. So, at the end, for *n* input features, a perceptron is made
    of *n+1* parameters: one parameter per feature plus the bias.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/formula_06_004.png) 是偏置，*g* 是激活函数；从历史上看，激活函数通常是步进函数，它对正输入值返回1，其他情况返回0。因此，最终，对于
    *n* 个输入特征，感知机由 *n+1* 个参数组成：每个特征一个参数，再加上偏置。
- en: Tip
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The step function is also called the **Heaviside function** and is widely used
    in other fields, such as physics.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 步进函数也被称为**赫维赛德函数**，并广泛应用于其他领域，如物理学。
- en: The perceptron forward computation is summarized in *Figure 6**.1*. As you can
    see, given a list of features ![](img/formula_06_005.png) and weights ![](img/formula_06_006.png),
    the forward computation is just the weighted sum, to which an activation function
    is applied.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机的前向计算总结在*图6.1*中。如你所见，给定一组特征 ![](img/formula_06_005.png) 和权重 ![](img/formula_06_006.png)，前向计算只是加权和，再应用激活函数。
- en: '![Figure 6.1 – A mathematical representation of a perceptron: from input features
    to output, through weights and the activation function](img/B19629_06_01.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 感知机的数学表示：从输入特征到输出，经过权重和激活函数](img/B19629_06_01.jpg)'
- en: 'Figure 6.1 – A mathematical representation of a perceptron: from input features
    to output, through weights and the activation function'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 感知机的数学表示：从输入特征到输出，通过权重和激活函数
- en: On the practical side, scikit-learn is the only thing required for installation
    for this recipe. It can be installed with the `pip install` `scikit-learn` command.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，安装此配方所需的唯一工具是 scikit-learn。可以通过 `pip install` `scikit-learn` 命令安装。
- en: How to do it…
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'We will use the Iris dataset again since the perceptron does not really perform
    well on complex classification tasks:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用 Iris 数据集，因为感知机在复杂分类任务中表现并不好：
- en: 'Make the required imports from scikit-learn:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 scikit-learn 中导入所需的模块：
- en: '`load_iris`: A function to load the dataset'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_iris`：一个加载数据集的函数'
- en: '`train_test_split`: A function to split the data'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_test_split`：一个用于拆分数据的函数'
- en: '`StandardScaler`: A class allowing us to rescale the data'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StandardScaler`：一个可以重新缩放数据的类'
- en: '`Perceptron`: The class containing the implementation of the perceptron:'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Perceptron`：包含感知机实现的类：'
- en: '[PRE0]'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Load the Iris dataset:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 Iris 数据集：
- en: '[PRE4]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Split the data into training and test sets using the `train_test_split` function,
    with `random state` set to `0` for reproducibility:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `train_test_split` 函数将数据拆分为训练集和测试集，并将 `random state` 设置为 `0` 以确保可重复性：
- en: '[PRE6]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Since all the features are quantitative here, we simply rescale all the features
    with a standard scaler:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这里所有的特征都是定量的，我们只需使用标准缩放器对所有特征进行重新缩放：
- en: '[PRE9]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Instantiate the model with the default parameters and fit it on the training
    set with the `.``fit()` method:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用默认参数实例化模型，并通过 `.fit()` 方法在训练集上进行拟合：
- en: '[PRE13]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Evaluate the model on both the training and test sets with the `.score()` method
    of the `LinearRegression` class, providing the accuracy score:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `LinearRegression` 类的 `.score()` 方法在训练集和测试集上评估模型，并提供准确率得分：
- en: '[PRE14]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here is the output:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE19]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Out of curiosity, we can have a look at the weights in `.coef_` and the bias
    in `.intercept_`.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 出于好奇，我们可以查看 `.coef_` 中的权重和 `.intercept_` 中的偏置。
- en: '[PRE20]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here is the output:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE22]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Important note
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: There are three sets of four weights and one bias, since scikit-learn handles
    on its own the One-vs-Rest multiclass classification, so we have one perceptron
    per class.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 共有三组四个权重和一个偏置，因为 scikit-learn 自动处理 One-vs-Rest 多类分类，所以我们为每个类别使用一个感知机。
- en: There’s more…
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'The perceptron is not only a machine learning model. It can be used to simulate
    logical gates: OR, AND, NOR, NAND, and XOR. Let’s have a look.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机不仅仅是一个机器学习模型。它可以用来模拟逻辑门：OR、AND、NOR、NAND 和 XOR。让我们来看看。
- en: 'We can easily implement a forward propagation for the perceptron with the following
    code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码轻松实现感知机的前向传播：
- en: '[PRE23]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This code does not consider many edge cases, but is used here simply to explain
    and demonstrate simple concepts.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码没有考虑许多边界情况，但在这里仅用于解释和演示简单概念。
- en: 'The AND gate has the inputs and expected outputs defined in the following truth
    table:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: AND 门具有以下真值表中定义的输入和期望输出：
- en: '| **Input 1** | **Input 2** | **Output** |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| **输入 1** | **输入 2** | **输出** |'
- en: '| 0 | 0 | 0 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 |'
- en: '| 0 | 1 | 0 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 0 |'
- en: '| 1 | 0 | 0 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 |'
- en: '| 1 | 1 | 1 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 1 |'
- en: Table 6.1 – AND gate truth table
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.1 – AND 门真值表
- en: 'Let’s reproduce this data with an array `X` that has two features (input 1
    and input 2) and four samples, and an array `y` with the expected outputs:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一个具有两个特征（输入 1 和输入 2）和四个样本的数组 `X` 来重现这个数据，并使用一个包含期望输出的数组 `y`：
- en: '[PRE24]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can now find a set of weights and bias that will allow the perceptron to
    act as an AND gate, and check the results to see whether it’s actually working:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以找到一组权重和偏置，使感知机能够作为 AND 门工作，并检查结果以验证它是否正常工作：
- en: '[PRE25]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Here is the output:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE26]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'With the same logic, most basic logic gates can be created out of a perceptron:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以相同的逻辑，感知机可以创建大多数基本的逻辑门：
- en: 'AND gate: weights [1, 1] and bias -1'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AND 门：权重 [1, 1] 和偏置 -1
- en: 'OR gate: weights [1, 1] and bias 0'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OR 门：权重 [1, 1] 和偏置 0
- en: 'NOR gate: weights [-1, -1] and bias 1'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NOR 门：权重 [-1, -1] 和偏置 1
- en: 'NAND gate: weights [-1, -1] and bias 2'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NAND 门：权重 [-1, -1] 和偏置 2
- en: 'XOR gate: this requires two perceptrons'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XOR 门：这需要两个感知机
- en: Tip
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: You can guess the weights and bias with a trial-and-error approach. But you
    can also use the truth table of a logic gate to make an educated guess or even
    to solve a set of equations.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过试错法猜测权重和偏置，但你也可以使用逻辑门的真值表来做出合理的猜测，甚至可以通过解方程组来求解。
- en: This means that using perceptrons, any logic function can be computed.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着使用感知机可以计算任何逻辑函数。
- en: See also
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'The official documentation of the scikit-learn implementation: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.xhtml).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 实现的官方文档：[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.xhtml)。
- en: Training a neural network for regression
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归任务中训练神经网络
- en: A perceptron is not a powerful and commonly used machine learning model. But
    having many perceptrons employed together in a neural network can become a powerful
    machine learning model. In this recipe, we will review a simple neural network,
    sometimes called a **multi-layer perceptron** or **vanilla neural network**. And
    we will then train such a neural network on a regression task on the California
    housing dataset with PyTorch, a widely used framework in deep learning.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器并不是一个强大且常用的机器学习模型，但将多个感知器结合在神经网络中使用，可以成为一个强大的机器学习模型。在本教程中，我们将回顾一个简单的神经网络，有时称为**多层感知器**或**基础神经网络**。然后，我们将使用广泛应用于深度学习的框架PyTorch，在加利福尼亚住房数据集上进行回归任务的训练。
- en: Getting started
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用
- en: Let’s start by reviewing what a neural network is, and how to feed forward a
    neural network from input features.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先回顾一下什么是神经网络，以及如何从输入特征开始进行神经网络的前向传播。
- en: 'A neural network can be divided into three parts:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以分为三部分：
- en: '**The input layer**, containing the input features'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**，包含输入特征'
- en: '**The hidden layers**, which can be any number of layers and units'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐层**，可以有任意数量的层和单元'
- en: '**The output layer**, which is defined by the expected output of the neural
    network'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**，由神经网络的预期输出定义'
- en: In both the hidden and output layers, we consider each unit (or neuron) to be
    a perceptron, with its own weights and bias.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在隐层和输出层中，我们将每个单元（或神经元）视为一个感知器，具有其自己的权重和偏差。
- en: These three parts are well represented in *Figure 6**.2*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这三部分在*图6.2*中得到了很好的表示。
- en: '![Figure 6.2 – A typical representation of a neural network: on the left the
    input layer, in the middle the hidden layers, on the right the output layer](img/B19629_06_02.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – 神经网络的典型表示：左边是输入层，中间是隐层，右边是输出层](img/B19629_06_02.jpg)'
- en: 'Figure 6.2 – A typical representation of a neural network: on the left the
    input layer, in the middle the hidden layers, on the right the output layer'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 神经网络的典型表示：左边是输入层，中间是隐层，右边是输出层
- en: We will note the input features ![](img/formula_06_007.png), the activation
    of the unit *i* of the layer *l*, and ![](img/formula_06_008.png), the we­­­ights
    of the unit *i* of the layer *l*.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将记录输入特征 ![](img/formula_06_007.png)，第*l*层单位*i*的激活值，以及 ![](img/formula_06_008.png)，第*l*层单位*i*的权重。
- en: Tip
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: We consider a neural network to involve deep learning if there is at least one
    hidden layer.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果神经网络中至少有一个隐层，我们认为它涉及深度学习。
- en: 'Training a neural network in regression is not so different from training a
    linear regression. It is made of the same ingredients:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 训练回归任务中的神经网络与训练线性回归没有太大区别。它由相同的组成部分构成：
- en: Forward propagation, from input features and weights to a prediction
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向传播，从输入特征和权重到预测结果
- en: A loss function to minimize
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个需要最小化的损失函数
- en: An algorithm to update the weights
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新权重的算法
- en: Let’s have a look at those ingredients.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这些组成部分。
- en: Forward propagation
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前向传播
- en: 'The forward propagation is what allows to compute and output from the input
    features. It must be computed from left to right, from the input layer (the input
    features) to the output layer (the output prediction). Each unit being a perceptron,
    the first hidden layer is fairly easy to compute:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播用于从输入特征中计算并输出结果。它必须从左到右计算，从输入层（输入特征）到输出层（输出预测）。每个单元都是感知器，第一隐层的计算相对简单：
- en: '![](img/formula_06_009.jpg)![](img/formula_06_010.jpg)![](img/formula_06_011.jpg)![](img/formula_06_012.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/formula_06_009.jpg)![](img/formula_06_010.jpg)![](img/formula_06_011.jpg)![](img/formula_06_012.jpg)'
- en: Where ![](img/formula_06_013.png) is the bias, and ![](img/formula_06_014.png)
    is the activation function of layer 1.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/formula_06_013.png) 是偏差项，![](img/formula_06_014.png) 是第1层的激活函数。
- en: 'Now, if we want to compute the activations of the second hidden layer ![](img/formula_06_015.png),
    we would use the exact same formulas, but with the activations of the first hidden
    layer as input (![](img/formula_06_016.png)), instead of the input features:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们想计算第二个隐藏层的激活值 ![](img/formula_06_015.png)，我们将使用完全相同的公式，但输入将是第一隐藏层的激活值
    (![](img/formula_06_016.png))，而不是输入特征：
- en: '![](img/formula_06_017.jpg)![](img/formula_06_018.jpg)![](img/formula_06_019.jpg)![](img/formula_06_020.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/formula_06_017.jpg)![](img/formula_06_018.jpg)![](img/formula_06_019.jpg)![](img/formula_06_020.jpg)'
- en: Tip
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: You can easily generalize to any number of hidden layers and any number of units
    per layer – the principle remains the same.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以轻松地推广到任意数量的隐藏层和每层任意数量的单元——原理保持不变。
- en: 'Finally, the output layer would be computed in exactly the same way, except
    in this case we have only one output neuron:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，输出层的计算方式完全相同，只不过在这种情况下我们只有一个输出神经元：
- en: '![](img/formula_06_021.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/formula_06_021.png)'
- en: 'One interesting thing to underline: the activation function is also layer dependent,
    meaning that each layer can have a different activation function. This is particularly
    critical for the output layer, which needs a specific output function depending
    on the task and expected output.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 有一点值得强调：激活函数也是依赖于层的，这意味着每一层可以有不同的激活函数。对于输出层来说，这一点尤为重要，因为它需要根据任务和预期输出使用特定的输出函数。
- en: For a regression task, it is common to have a linear activation function, allowing
    the output values of the neural network to be any number.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归任务，常用的激活函数是线性激活函数，这样神经网络的输出值可以是任意数字。
- en: Tip
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: 'The activation function plays a decisive role in neural networks: it adds non-linearity.
    If we have only linear activation functions for hidden layers, no matter the number
    of layers, it is equivalent to having no hidden layer.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数在神经网络中起着决定性作用：它增加了非线性。如果我们对隐藏层仅使用线性激活函数，无论层数多少，这相当于没有隐藏层。
- en: Loss function
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'The loss function in a regression task can be the same as in a linear regression:
    the mean squared error. In our example, if we consider the prediction *ŷ* to be
    our output value ![](img/formula_06_022.png), the loss *L* is simply the following:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归任务中，损失函数可以与线性回归中的相同：均方误差。在我们的例子中，如果我们认为预测 *ŷ* 是输出值 ![](img/formula_06_022.png)，那么损失
    *L* 只是如下：
- en: '![](img/formula_06_023.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/formula_06_023.jpg)'
- en: Assuming *j* is the sample index.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 *j* 是样本索引。
- en: Updating the weights
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更新权重
- en: Updating the weights is done by trying to minimize the loss function. Again,
    this is almost the same as in a linear regression. The tricky part is that, unlike
    linear regression, we have several layers of units with weights and biases that
    all need to be updated. This is where the so-called backpropagation allows the
    updating of each layer step by step, from the rightmost to the leftmost (following
    the convention in *Figure 6**.2*).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 更新权重是通过尝试最小化损失函数来完成的。这与线性回归几乎相同。难点在于，与线性回归不同，我们有多个层的单元，其中每个单元都有权重和偏置，都需要更新。这就是所谓的反向传播，它允许逐层更新，从最右侧到最左侧（遵循
    *图 6.2* 中的约定）。
- en: The details of the backpropagation, although useful and interesting, are beyond
    this book’s scope.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播的细节，尽管有用且有趣，但超出了本书的范围。
- en: Also, just like there are several algorithms to optimize the weights in a logistic
    regression (the `solver` parameter in scikit-learn’s `LogisticRegression`), there
    are several algorithms to train a neural network. They are commonly called **optimizers**.
    Among the most frequently used are the **Stochastic Gradient Descent** (**SGD**)
    and **Adaptive** **momentum** (**Adam**).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，正如在逻辑回归中有多个算法优化权重（在 `scikit-learn` 的 `LogisticRegression` 中的 `solver` 参数），训练神经网络也有多种算法。这些算法通常被称为**优化器**。其中最常用的有**随机梯度下降**（**SGD**）和**自适应动量**（**Adam**）。
- en: PyTorch
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch
- en: PyTorch is a widely used framework for deep learning, allowing us to easily
    train and reuse deep learning models.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 是一个广泛使用的深度学习框架，使我们能够轻松地训练和重用深度学习模型。
- en: 'It is fairly easy to use and can be easily installed with the following command:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 它非常容易使用，并且可以通过以下命令轻松安装：
- en: '[PRE27]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: For this recipe, we will also need scikit-learn and matplotlib, which can be
    installed with `pip install` `scikit-learn matplotlib`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个食谱，我们还需要 `scikit-learn` 和 `matplotlib`，可以使用 `pip install` `scikit-learn
    matplotlib` 安装它们。
- en: How to do it…
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'In this recipe, we will build and train a neural network on the California
    housing dataset:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将构建并训练一个神经网络来处理加利福尼亚住房数据集：
- en: 'First, we need the required imports. Among the imports are some from scikit-learn
    that we have already used in this book:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要导入所需的模块。在这些导入中，有一些来自我们在本书中已经使用过的scikit-learn：
- en: '`fetch_california_housing` to load the dataset'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fetch_california_housing`用于加载数据集。'
- en: '`train_test_split` to split the data into training and test sets'
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_test_split`用于将数据分割为训练集和测试集。'
- en: '`StandardScaler` to rescale the quantitative data'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StandardScaler`用于重新缩放定量数据。'
- en: '`r2_score` to evaluate the model'
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`r2_score`用于评估模型。'
- en: 'For display purposes, we also import matplotlib:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了显示目的，我们还导入了matplotlib：
- en: '[PRE28]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We also need some imports from torch:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要从torch导入一些模块：
- en: '`torch` itself for some functions at the lower level of the library'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch`本身提供了一些库中较低层级的函数。'
- en: '`torch.nn` containing many useful classes for building a neural network'
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.nn`包含许多用于构建神经网络的有用类。'
- en: '`torch.nn.functional` for some useful functions'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.nn.functional`用于一些有用的函数。'
- en: '`Dataset` and `DataLoader` for handling the data operations:'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dataset`和`DataLoader`用于处理数据操作：'
- en: '[PRE33]'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We need to load the data using the `fetch_california_housing` function and
    return the features and labels:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要使用`fetch_california_housing`函数加载数据，并返回特征和标签：
- en: '[PRE37]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can then split the data into training and test sets using the `train_test_split`
    function. We set a test size of 20% and a random state for reproducibility:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们可以使用`train_test_split`函数将数据分割为训练集和测试集。我们设置测试集大小为20%，并为可重复性指定随机种子：
- en: '[PRE38]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can now rescale the data with a standard scaler:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以使用标准缩放器对数据进行重新缩放：
- en: '[PRE41]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Important note
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Note that we convert the *X* and *y* variables to float32 variables. This is
    necessary to prevent later troubles with PyTorch not properly handling float64
    variables.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将*X*和*y*变量转换为float32类型的变量。这是为了防止后续在PyTorch中处理float64变量时出现问题。
- en: 'For PyTorch, we need to create the dataset class. Nothing complicated here
    though; this class requires only the following to work properly:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于PyTorch，我们需要创建数据集类。这里并不复杂；这个类只需要以下内容才能正常工作：
- en: It has to inherit from the `Dataset` class (imported earlier)
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须继承自`Dataset`类（前面导入过的）。
- en: It has to have a constructor (`__init__` method) that deals with (and optionally
    prepares) the data
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须有一个构造函数（`__init__`方法），处理（并可选地准备）数据。
- en: It has to have a `__len__` method, so that the number of samples can be fetched
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须有一个`__len__`方法，以便能够获取样本的数量。
- en: It has to have a `__getitem__` method, in order to get `X` and `y` for any given
    index
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须有一个`__getitem__`方法，以便获取给定索引的`X`和`y`。
- en: 'Let’s implement this for the California dataset, and let’s call our class `CaliforniaDataset`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为加利福尼亚数据集实现这个，并将我们的类命名为`CaliforniaDataset`：
- en: '[PRE44]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'If we break this class down, we have the following functions:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们分解这个类，我们会看到以下函数：
- en: The `init` constructor simply converts `X` and `y` to torch tensors with the
    `torch.from_numpy` function and stores the results as class attributes
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init`构造函数简单地将`X`和`y`转换为torch张量，使用`torch.from_numpy`函数，并将结果存储为类的属性。'
- en: The `len` method just returns the length of the `X` attribute; it would work
    equally using the length of the `y` attribute
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`len`方法只是返回`X`属性的长度；它同样适用于使用`y`属性的长度。'
- en: The `getitem` method simply returns a tuple with the given item `idx` of the
    `X` and `y` tensors
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`getitem`方法简单地返回一个元组，其中包含给定索引`idx`的`X`和`y`张量。'
- en: This is quite straightforward, and will then allow `pytorch` to know what the
    data is, how many samples are in the dataset, and what the sample `i` is. For
    that, we will need to instantiate a `DataLoader` class.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当简单，然后会让`pytorch`知道数据是什么，数据集中有多少个样本，以及样本`i`是什么。为此，我们需要实例化一个`DataLoader`类。
- en: Tip
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Rescaling can also be computed in this `CaliforniaDataset` class, as well as
    any preprocessing.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 重新缩放也可以在这个`CaliforniaDataset`类中计算，以及任何预处理。
- en: 'Now we instantiate the `CaliforniaDataset` objects for the training and test
    datasets. Then we instantiate the associated loaders using the imported `DataLoader`
    class:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们实例化`CaliforniaDataset`对象用于训练集和测试集。然后，我们使用导入的`DataLoader`类实例化相关的加载器：
- en: '[PRE45]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The data loader instances have a couple of options available. Here, we specify
    the following:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加载器实例有几个可用的选项。在这里，我们指定以下内容：
- en: '`batch_size`: The batch size for training. It may have an impact on the final
    results.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`：训练的批次大小。它可能对最终结果产生影响。'
- en: '`shuffle`: Determines whether to shuffle the data at each epoch.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shuffle`：确定是否在每个epoch时打乱数据。'
- en: 'We can finally create the neural network model class. For this class, we only
    need to fill in two methods:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们最终可以创建神经网络模型类。对于这个类，我们只需要填充两个方法：
- en: The constructor with whatever is useful, such as parameters and attributes
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数，包含所有有用的内容，如参数和属性
- en: 'The `forward` method that computes the forward propagation:'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward`方法计算前向传播：'
- en: '[PRE53]'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'If we break it down, we have designed a class that takes two input parameters:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们细分一下，我们设计了一个类，它接受两个输入参数：
- en: '`input_shape` is the input shape of the neural networks – this is basically
    the number of features in the dataset'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_shape`是神经网络的输入形状——这基本上是数据集中特征的数量'
- en: '`hidden_units` is the number of units in the hidden layers, which defaults
    to 24'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_units`是隐藏层中单元的数量，默认为24'
- en: 'The neural network itself comprises the following:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络本身包括以下内容：
- en: Two hidden layers of `hidden_units` units with ReLU activation functions
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个隐藏层，每个隐藏层有`hidden_units`个单元，激活函数为ReLU
- en: One output layer of one unit, since we need to predict one value
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个输出层，只有一个单元，因为我们只需要预测一个值
- en: Important note
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: More context about ReLU and other activation functions will be given in the
    next *There’s* *more* subsection.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 关于ReLU和其他激活函数的更多内容将在下一个*这里有* *更多*小节中给出。
- en: 'We can now instantiate a neural network and test it on random data of the expected
    shape to check whether the `forward` method is working properly:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以实例化一个神经网络，并在随机数据（形状符合预期）上进行测试，以检查`forward`方法是否正常工作：
- en: '[PRE72]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'We’ll get this output:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到这个输出：
- en: '[PRE78]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: As we can see, the computation of the forward propagation on the random data
    worked well and returns one single value, as expected. Any error in that step
    would mean we did something wrong.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，前向传播在随机数据上的计算工作得很好，按预期返回了一个单一的值。这个步骤中的任何错误都意味着我们做错了什么。
- en: 'Before being able to train the neural network on the data, we need to define
    the loss function and the optimizer. Fortunately, the mean squared error is already
    implemented and available as `nn.MSELoss()`. There are plenty of optimizers available;
    we decided to use Adam here, but other optimizers can also be tested:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在能够在数据上训练神经网络之前，我们需要定义损失函数和优化器。幸运的是，均方误差已经实现，并作为`nn.MSELoss()`提供。有许多优化器可以选择；我们在这里选择了Adam，但也可以测试其他优化器：
- en: '[PRE79]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Important note
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The optimizer needs the network parameters as input to its constructor.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器需要将网络参数作为输入传递给其构造函数。
- en: 'Finally, we can train the neural networks on 10 epochs with the following piece
    of code:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以使用以下代码训练神经网络10个epoch：
- en: '[PRE81]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Hopefully, the comments are self-explanatory. Basically, there are two nested
    loops:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 希望注释是自解释的。基本上，有两个嵌套循环：
- en: 'One outer loop over the epochs: the number of times the model is trained over
    the whole dataset'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个外部循环遍历所有的epoch：即模型在整个数据集上训练的次数
- en: 'One inner loop over the samples: for each step, a batch of `batch_size` samples
    is used to train the model'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个内循环遍历样本：每个步骤中，使用`batch_size`样本的批次来训练模型
- en: 'For each step in the inner loop, we have the following main steps:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在内循环的每一步中，我们有以下主要步骤：
- en: 'Get a batch of the data: both features and labels'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取一批数据：包括特征和标签
- en: Forward propagate on this data and get output predictions
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据进行前向传播并获取输出预测
- en: 'Compute the loss: the mean squared error between predictions and labels'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算损失：预测值与标签之间的均方误差
- en: Update the weights of the network with backpropagation
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用反向传播更新网络的权重
- en: At the end of each step, we print the loss, which hopefully decreases with each
    epoch.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个步骤结束时，我们打印损失值，希望它随着每个epoch的进行而减少。
- en: 'We can plot the loss as a function of the epoch. This is quite visual and lets
    us ensure the network is learning if the loss is decreasing:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以将损失作为epoch的函数进行绘制。这非常直观，并且让我们能够确保网络在学习，如果损失在减少的话：
- en: '[PRE109]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Here is the resulting graph:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果图：
- en: '![Figure 6.3 – Resulting MSE loss as a function of the epoch](img/B19629_06_03.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – MSE损失与epoch的关系](img/B19629_06_03.jpg)'
- en: Figure 6.3 – Resulting MSE loss as a function of the epoch
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – MSE损失与epoch的关系
- en: Important note
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: We could also keep track of the loss on the test set and display it at this
    step for more information. We will do that in the next recipe to avoid being drowned
    in too much information.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在这一步追踪测试集上的损失，并显示出来，以获取更多信息。为了避免信息过载，我们将在下一个食谱中进行这个操作。
- en: 'We can finally evaluate the model on both the training and test sets. As we
    did previously in this book with regression tasks, we will use the R2-score. Any
    other relevant metric can be used too:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以在训练集和测试集上评估模型。正如本书前面在回归任务中所做的那样，我们将使用 R2-score。其他相关指标也可以使用：
- en: '[PRE112]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'Here’s the output:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '[PRE123]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: As we can see here, we have a reasonable R2-score of 0.74 on the training set,
    with minor overfitting.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这里看到的，我们在训练集上得到了合理的 R2-score 值 0.74，存在轻微的过拟合。
- en: There’s more…
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: In this recipe, we mentioned activation functions without really explaining
    what they are or why they are needed.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提到了激活函数，但并没有真正解释它们是什么或为什么需要它们。
- en: Put simply, they add non-linearities, allowing the model to learn more complex
    patterns. Indeed, if we had a neural network with no activation function, the
    whole model would be equivalent to a linear model (e.g., a linear regression),
    no matter the number of layers or the number of units. This is summarized in *Figure
    6**.4*.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，激活函数添加了非线性因素，使模型能够学习更复杂的模式。事实上，如果我们有一个没有激活函数的神经网络，无论层数或单元数多少，整个模型都等同于一个线性模型（例如线性回归）。这一点在
    *图 6.4* 中得到了总结。
- en: '![Figure 6.4 – On the left, a neural network with no activation functions can
    only learn linearly separable decision functions. On the right, a neural network
    with activation functions can learn complex decision functions](img/B19629_06_04.jpg)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – 左侧的神经网络没有激活函数，只能学习线性可分的决策函数；右侧的神经网络有激活函数，可以学习复杂的决策函数](img/B19629_06_04.jpg)'
- en: Figure 6.4 – On the left, a neural network with no activation functions can
    only learn linearly separable decision functions. On the right, a neural network
    with activation functions can learn complex decision functions
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 左侧的神经网络没有激活函数，只能学习线性可分的决策函数；右侧的神经网络有激活函数，可以学习复杂的决策函数
- en: There are many available activation functions, but some of the most common ones
    for hidden layers are sigmoid, ReLU, and tanh.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多可用的激活函数，但最常见的隐藏层激活函数包括 sigmoid、ReLU 和 tanh。
- en: Sigmoid
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Sigmoid
- en: 'The sigmoid function is the same as that used in logistic regression:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数与逻辑回归中使用的函数相同：
- en: '![](img/formula_06_024.jpg)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![](img/formula_06_024.jpg)'
- en: This function’s values range from 0 to 1, and outputs 0.5 if *x =* *0*.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的值范围从 0 到 1，当 *x =* *0* 时输出 0.5。
- en: tanh
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: tanh
- en: 'The tanh or hyperbolic tangent function ranges from -1 to 1, with a value of
    0 if *x* is 0:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: tanh 或双曲正切函数的值范围从 -1 到 1，当 *x* 为 0 时值为 0：
- en: '![](img/formula_06_025.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](img/formula_06_025.jpg)'
- en: ReLU
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ReLU
- en: 'The **ReLU** or **Rectified Linear Unit** function just returns 0 for any input
    negative value, and *x* for any positive input value *x*. Unlike sigmoid and tanh,
    it does not plateau and thus limits vanishing gradient problems. Its formula is
    the following:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReLU** 或 **修正线性单元** 函数对于任何负输入值返回 0，对于任何正输入值 *x* 返回 *x*。与 sigmoid 和 tanh 不同，它不会出现平台效应，因此能够避免梯度消失问题。其公式如下：'
- en: '![](img/formula_06_026.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![](img/formula_06_026.jpg)'
- en: Visualization
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化
- en: 'We can visualize these three activation functions (sigmoid, tanh, and ReLU)
    together for a more intuitive understanding with the following code:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码将这三个激活函数（sigmoid、tanh 和 ReLU）一起可视化，从而更直观地理解它们：
- en: '[PRE124]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'You’ll get this output upon running the previous code, which computes the output
    values of these functions in the [-2, 2] range:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 运行之前的代码时，你会得到这个输出，计算的是这些函数在 [-2, 2] 范围内的输出值：
- en: '![Figure 6.5 – Resulting plot of the sigmoid, tanh, and ReLU activation functions
    in the [-2, 2] input range](img/B19629_06_05.jpg)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5 – Sigmoid、tanh 和 ReLU 激活函数在 [-2, 2] 输入范围内的结果图](img/B19629_06_05.jpg)'
- en: Figure 6.5 – Resulting plot of the sigmoid, tanh, and ReLU activation functions
    in the [-2, 2] input range
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – Sigmoid、tanh 和 ReLU 激活函数在 [-2, 2] 输入范围内的结果图
- en: 'For more about the available activation functions in PyTorch, have a look at
    the following link: [https://pytorch.org/docs/stable/nn.xhtml#non-linear-activations-weighted-sum-nonlinearity.](https://pytorch.org/docs/stable/nn.xhtml#non-linear-activations-weighted-sum-nonlinearity%0D)'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于 PyTorch 中可用的激活函数，请查看以下链接：[https://pytorch.org/docs/stable/nn.xhtml#non-linear-activations-weighted-sum-nonlinearity.](https://pytorch.org/docs/stable/nn.xhtml#non-linear-activations-weighted-sum-nonlinearity%0D)
- en: See also
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'Here are several links to PyTorch tutorials that can be helpful for gaining
    familiarity with it, along with a deeper understanding of how it works:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个 PyTorch 教程的链接，它们对于熟悉 PyTorch 和深入理解其工作原理非常有帮助：
- en: '[https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.xhtml](https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.xhtml)'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.xhtml](https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.xhtml)'
- en: '[https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.xhtml](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.xhtml)'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.xhtml](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.xhtml)'
- en: '[https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.xhtml](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.xhtml)'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.xhtml](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.xhtml)'
- en: '[https://pytorch.org/tutorials/beginner/basics/data_tutorial.xhtml?highlight=dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.xhtml?highlight=dataset)'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pytorch.org/tutorials/beginner/basics/data_tutorial.xhtml?highlight=dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.xhtml?highlight=dataset)'
- en: 'And the following link is for a very well-written website about deep learning,
    for those who wish to have a better understanding of neural networks, gradient
    descent, and backpropagation: [http://neuralnetworksanddeeplearning.com/.](http://neuralnetworksanddeeplearning.com/%0D)'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接是一个关于深度学习的优秀网站，适合那些希望更好理解神经网络、梯度下降和反向传播的人：[http://neuralnetworksanddeeplearning.com/.](http://neuralnetworksanddeeplearning.com/%0D)
- en: Training a neural network for binary classification
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个用于二分类的神经网络
- en: In this recipe, let’s train our first neural network for a binary classification
    task on the breast cancer dataset. We will also learn more about the impact of
    the learning rate and the optimizer on the optimization, as well as how to evaluate
    the model against the test set.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将训练第一个用于乳腺癌数据集的二分类任务神经网络。我们还将了解学习率和优化器对优化过程的影响，以及如何通过测试集评估模型。
- en: Getting ready
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'As we will see in this recipe, training a neural network for binary classification
    is not so different from training a neural network for regression. Primarily,
    two changes have to be made:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这个食谱中所看到的，训练一个用于二分类的神经网络与训练一个回归神经网络并没有太大不同。主要有两个变化需要进行：
- en: The output layer’s activation function
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层的激活函数
- en: The loss function
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数
- en: In the previous recipe for a regression task, the output layer had no activation
    function. Indeed, for a regression, one can expect the prediction to take any
    value.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的回归任务食谱中，输出层没有激活函数。实际上，对于回归任务，可以期望预测值取任意值。
- en: 'For a binary classification, we expect the output to be a probability, so a
    value between 0 and 1, just like the logistic regression. This is why when doing
    a binary classification, the output layer’s activation function is usually the
    sigmoid function. The resulting predictions will be just like those of a logistic
    regression: a number on which to apply a threshold (e.g., 0.5) above which we
    consider the prediction to be class 1.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二分类任务，我们期望输出是一个概率值，也就是介于 0 和 1 之间的值，就像逻辑回归一样。这就是为什么在做二分类时，输出层的激活函数通常是 sigmoid
    函数。最终的预测结果将与逻辑回归的预测结果类似：一个数值，我们可以应用一个阈值（例如 0.5），超过该阈值时我们认为预测为类别 1。
- en: As the labels are 0s and 1s, and the predictions are values between 0 and 1,
    the mean squared error is no longer suited to train such a model. So, just like
    for a logistic regression, we would use binary cross-entropy loss.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 由于标签是 0 和 1，而预测值是介于 0 和 1 之间的值，因此均方误差不再适用于训练此类模型。因此，就像逻辑回归一样，我们将使用二元交叉熵损失函数。
- en: The required libraries for this recipe are matplotlib, scikit-learn, and PyTorch,
    they can be installed with `pip install matplotlib` `scikit-learn torch`.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱所需的库是 matplotlib、scikit-learn 和 PyTorch，可以通过 `pip install matplotlib` `scikit-learn
    torch` 来安装。
- en: How to do it…
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'We will train a simple neural network with two hidden layers on a binary classification
    task on the breast cancer dataset. Even though this dataset is not really suited
    for deep learning since it is a small dataset, it allows us to easily understand
    all the steps involved in training a neural network for binary classification:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练一个具有两个隐藏层的简单神经网络，用于乳腺癌数据集上的二分类任务。尽管这个数据集不太适合深度学习，因为它是一个小型数据集，但它使我们能够轻松理解训练二分类神经网络的所有步骤：
- en: 'We have the following required imports from scikit-learn:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从 scikit-learn 导入了以下所需的库：
- en: '`load_breast_cancer` to load the dataset'
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_breast_cancer` 用于加载数据集'
- en: '`train_test_split` to split the data into training and test sets'
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_test_split` 用于将数据拆分为训练集和测试集'
- en: '`StandardScaler` to rescale the quantitative data'
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于重新缩放定量数据的 `StandardScaler`
- en: '`accuracy_score` to evaluate the model'
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于评估模型的 `accuracy_score`
- en: 'We also need matplotlib for display, and we need the following from torch:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要 matplotlib 来进行显示，并且需要从 torch 中引入以下内容：
- en: '`torch` itself'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch` 本身'
- en: '`torch.nn` containing required classes for building a neural network'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含构建神经网络所需类的 `torch.nn`
- en: '`torch.nn.functional` for activation functions such as ReLU'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于激活函数（如 ReLU）的 `torch.nn.functional`
- en: '`Dataset` and `DataLoader` for handling the data'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理数据的 `Dataset` 和 `DataLoader`
- en: '[PRE125]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'Load the features and labels with the `load_breast_cancer` function:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `load_breast_cancer` 函数加载特征和标签：
- en: '[PRE133]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: 'Split the data into training and test sets, specifying the random state for
    reproducibility. Also cast the features and labels for `float32` for later compatibility
    with PyTorch:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拆分为训练集和测试集，指定随机状态以确保结果可复现。同时将特征和标签转换为 `float32`，以便与 PyTorch 后续操作兼容：
- en: '[PRE134]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '[PRE136]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'Create the `Dataset` class for handling the data. Note that in this recipe
    we integrate the data rescaling in this step, unlike in the previous recipe:'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建处理数据的 `Dataset` 类。请注意，在这个食谱中，我们将数据重新缩放集成到此步骤中，不像在之前的食谱中那样：
- en: '[PRE137]'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: '[PRE139]'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '[PRE140]'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE140]'
- en: '[PRE141]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '[PRE143]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE143]'
- en: '[PRE144]'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE144]'
- en: '[PRE145]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '[PRE146]'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '[PRE147]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '[PRE148]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE148]'
- en: '[PRE149]'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE150]'
- en: '[PRE151]'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE151]'
- en: Important note
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Having the scaler in the class has pros and cons, where a pro is properly handling
    data leakage between train and test sets.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 将缩放器包含在类中有利有弊，其中一个优点是可以正确处理训练集和测试集之间的数据泄露。
- en: 'Instantiate the training and test sets and loaders. Note that no scaler is
    provided to the training dataset, while the test dataset is given the training
    set scaler to ensure that all the data is processed the same with no data leakage:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化训练集和测试集及其加载器。请注意，训练数据集没有提供缩放器，而测试数据集则提供了训练集的缩放器，以确保所有数据以相同方式处理，避免数据泄露：
- en: '[PRE152]'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE152]'
- en: '[PRE153]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '[PRE155]'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE155]'
- en: '[PRE156]'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE156]'
- en: '[PRE157]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE157]'
- en: '[PRE158]'
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE158]'
- en: 'Build the neural network. Here, we build a neural network with two hidden layers.
    In the `forward` method, the `torch.sigmoid()` function is applied to the output
    layer before returning the value, ensuring we have a prediction between 0 and
    1\. The only parameter needed to instantiate the model is the input shape, which
    is simply the number of features here:'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建神经网络。在这里，我们构建一个具有两个隐藏层的神经网络。在 `forward` 方法中，`torch.sigmoid()` 函数被应用于输出层，确保返回的值在
    0 和 1 之间。实例化模型所需的唯一参数是输入形状，这里仅为特征的数量：
- en: '[PRE159]'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE160]'
- en: '[PRE161]'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE162]'
- en: '[PRE163]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE163]'
- en: '[PRE164]'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE164]'
- en: '[PRE165]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE165]'
- en: '[PRE166]'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE166]'
- en: '[PRE167]'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE167]'
- en: '[PRE168]'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE168]'
- en: '[PRE169]'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE169]'
- en: '[PRE170]'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE170]'
- en: '[PRE171]'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '[PRE172]'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE172]'
- en: '[PRE173]'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE173]'
- en: '[PRE174]'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE174]'
- en: '[PRE175]'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE175]'
- en: '[PRE176]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE176]'
- en: '[PRE177]'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE177]'
- en: 'We can now instantiate the model with the right input shape and check that
    the forward propagation works properly on a given random tensor:'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以使用正确的输入形状实例化模型，并检查给定随机张量上的前向传播是否正常工作：
- en: '[PRE178]'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE178]'
- en: '[PRE179]'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE179]'
- en: '[PRE180]'
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE180]'
- en: '[PRE181]'
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE181]'
- en: '[PRE182]'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE182]'
- en: '[PRE183]'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE183]'
- en: 'After running the previous code, we get the following output:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，我们得到以下输出：
- en: '[PRE184]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: 'Define the loss function and the optimizer. As stated, we will use the binary
    cross-entropy loss, available as `nn.BCELoss()` in PyTorch. The chosen optimizer
    is `Adam`, but other optimizers can be tested too:'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义损失函数和优化器。如前所述，我们将使用二元交叉熵损失，PyTorch 中提供的 `nn.BCELoss()` 实现此功能。选择的优化器是 `Adam`，但也可以测试其他优化器：
- en: '[PRE185]'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE185]'
- en: '[PRE186]'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE186]'
- en: '[PRE187]'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE187]'
- en: Important note
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: More explanations about the optimizer are provided in the next *There’s* *more*
    subsection.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 有关优化器的更多解释将在下一节 *There’s* *more* 中提供。
- en: 'We can now train the neural network for 50 epochs. We also compute both the
    training and test set loss at each epoch, so we can plot them afterward. To do
    so, we need to switch mode for the model:'
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以训练神经网络 50 个 epoch。我们还会在每个 epoch 计算训练集和测试集的损失，以便之后绘制它们。为此，我们需要切换模型的模式：
- en: Before training on the training set, switch to train mode with `model.train()`
  id: totrans-444
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练训练集之前，使用 `model.train()` 切换到训练模式
- en: Before evaluating the test set, switch to the `eval` model with `model.eval()`
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在评估测试集之前，使用 `model.eval()` 切换到 `eval` 模式
- en: '[PRE188]'
  id: totrans-446
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE188]'
- en: '[PRE189]'
  id: totrans-447
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE189]'
- en: '[PRE190]'
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE190]'
- en: '[PRE191]'
  id: totrans-449
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE191]'
- en: '[PRE192]'
  id: totrans-450
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE192]'
- en: '[PRE193]'
  id: totrans-451
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE193]'
- en: '[PRE194]'
  id: totrans-452
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE194]'
- en: '[PRE195]'
  id: totrans-453
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE195]'
- en: '[PRE196]'
  id: totrans-454
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE196]'
- en: '[PRE197]'
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE197]'
- en: '[PRE198]'
  id: totrans-456
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE198]'
- en: '[PRE199]'
  id: totrans-457
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE199]'
- en: '[PRE200]'
  id: totrans-458
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE200]'
- en: '[PRE201]'
  id: totrans-459
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE201]'
- en: '[PRE202]'
  id: totrans-460
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE202]'
- en: '[PRE203]'
  id: totrans-461
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE203]'
- en: '[PRE204]'
  id: totrans-462
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE204]'
- en: '[PRE205]'
  id: totrans-463
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE205]'
- en: '[PRE206]'
  id: totrans-464
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE206]'
- en: '[PRE207]'
  id: totrans-465
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE207]'
- en: '[PRE208]'
  id: totrans-466
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE208]'
- en: '[PRE209]'
  id: totrans-467
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE209]'
- en: '[PRE210]'
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE210]'
- en: '[PRE211]'
  id: totrans-469
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE211]'
- en: '[PRE212]'
  id: totrans-470
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE212]'
- en: '[PRE213]'
  id: totrans-471
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE213]'
- en: '[PRE214]'
  id: totrans-472
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE214]'
- en: '[PRE215]'
  id: totrans-473
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE215]'
- en: '[PRE216]'
  id: totrans-474
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE216]'
- en: '[PRE217]'
  id: totrans-475
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE217]'
- en: '[PRE218]'
  id: totrans-476
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE218]'
- en: '[PRE219]'
  id: totrans-477
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE219]'
- en: '[PRE220]'
  id: totrans-478
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE220]'
- en: '[PRE221]'
  id: totrans-479
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE221]'
- en: '[PRE222]'
  id: totrans-480
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE222]'
- en: '[PRE223]'
  id: totrans-481
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE223]'
- en: '[PRE224]'
  id: totrans-482
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE224]'
- en: '[PRE225]'
  id: totrans-483
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE225]'
- en: '[PRE226]'
  id: totrans-484
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE226]'
- en: '[PRE227]'
  id: totrans-485
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE227]'
- en: '[PRE228]'
  id: totrans-486
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE228]'
- en: '[PRE229]'
  id: totrans-487
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE229]'
- en: '[PRE230]'
  id: totrans-488
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE230]'
- en: '[PRE231]'
  id: totrans-489
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE231]'
- en: '[PRE232]'
  id: totrans-490
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE232]'
- en: '[PRE233]'
  id: totrans-491
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE233]'
- en: '[PRE234]'
  id: totrans-492
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE234]'
- en: '[PRE235]'
  id: totrans-493
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE235]'
- en: '[PRE236]'
  id: totrans-494
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE236]'
- en: '[PRE237]'
  id: totrans-495
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE237]'
- en: Important note
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Notice the use of `with` `torch.no_grad()` around the evaluation part. This
    line of code allows us to deactivate the autograd engine and speed up processing.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意在评估部分使用 `with` `torch.no_grad()`。这行代码允许我们禁用自动求导引擎，从而加速处理。
- en: 'Now we plot the losses for the train and test sets as a function of the epoch,
    using the two computed lists in the previous step:'
  id: totrans-498
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将训练集和测试集的损失作为 epoch 的函数绘制，使用上一步骤中计算的两个列表：
- en: '[PRE238]'
  id: totrans-499
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE238]'
- en: '[PRE239]'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE239]'
- en: '[PRE240]'
  id: totrans-501
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE240]'
- en: '[PRE241]'
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE241]'
- en: '[PRE242]'
  id: totrans-503
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE242]'
- en: 'Here’s the output:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 6.6 – Resulting MSE loss for the train and test sets](img/B19629_06_06.jpg)'
  id: totrans-505
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6 – 训练集和测试集的 MSE 损失](img/B19629_06_06.jpg)'
- en: Figure 6.6 – Resulting MSE loss for the train and test sets
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – 训练集和测试集的 MSE 损失
- en: As we can see, both losses are decreasing. At first, the train and test losses
    are almost equal, but after 10 epochs, the train loss keeps decreasing while the
    test does not, meaning the model has overfit on the training set.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，两个损失都在减少。一开始，训练损失和测试损失几乎相等，但在10个epoch后，训练损失继续减少，而测试损失没有变化，意味着模型在训练集上发生了过拟合。
- en: 'It is possible to evaluate the model using the accuracy scores from both the
    training and test sets, via the `accuracy_score` function of scikit-learn. It
    requires a few more steps to compute the predictions, since we have to do the
    following operations before getting actual class predictions:'
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用训练集和测试集的准确度分数来评估模型，通过scikit-learn的`accuracy_score`函数。计算预测结果时需要更多步骤，因为我们必须执行以下操作才能获得实际的类别预测：
- en: Rescale the data with the scaler used for training, available in the `training_data.x_scaler`
    attribute
  id: totrans-509
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用用于训练的标准化器对数据进行重新缩放，该标准化器可在`training_data.x_scaler`属性中找到
- en: Cast the NumPy data to the torch tensor with `torch.tensor()`
  id: totrans-510
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`torch.tensor()`将NumPy数据转换为torch张量
- en: Apply forward propagation to the model
  id: totrans-511
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将前向传播应用到模型上
- en: Cast the output torch tensor back to NumPy with `.detach().numpy()`
  id: totrans-512
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`.detach().numpy()`将输出的torch张量转换回NumPy
- en: Apply a threshold to convert a probability prediction (between 0 and 1) to a
    class prediction with `>` `0.5`
  id: totrans-513
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`>` `0.5`应用阈值，将概率预测（介于0和1之间）转换为类别预测
- en: '[PRE243]'
  id: totrans-514
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE243]'
- en: '[PRE244]'
  id: totrans-515
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE244]'
- en: '[PRE245]'
  id: totrans-516
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE245]'
- en: '[PRE246]'
  id: totrans-517
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE246]'
- en: '[PRE247]'
  id: totrans-518
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE247]'
- en: '[PRE248]'
  id: totrans-519
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE248]'
- en: '[PRE249]'
  id: totrans-520
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE249]'
- en: '[PRE250]'
  id: totrans-521
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE250]'
- en: '[PRE251]'
  id: totrans-522
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE251]'
- en: '[PRE252]'
  id: totrans-523
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE252]'
- en: '[PRE253]'
  id: totrans-524
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE253]'
- en: '[PRE254]'
  id: totrans-525
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE254]'
- en: 'Here is the output of the preceding code:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前面代码的输出：
- en: '[PRE255]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE255]'
- en: We get an accuracy of 99% on the training and 96% on the test set, proving there
    is indeed overfitting, as expected from the curve of the train and test losses
    as a function of the epoch.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练集上获得了99%的准确率，在测试集上获得了96%的准确率，这证明了模型确实存在过拟合现象，正如从训练和测试损失随epoch变化的曲线所预期的那样。
- en: There’s more…
  id: totrans-529
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: As we have seen here, the loss is decreasing over time, meaning the model is
    actually learning.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这里看到的，损失随着时间的推移而减少，意味着模型实际上在学习。
- en: Important note
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Even if it’s sometimes a bit noisy with bumps in the loss, as long as the overall
    trend remains good, there is nothing to worry about.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有时损失出现一些波动，只要总体趋势保持良好，也不必担心。
- en: 'There are two important notions that may alter these results, somewhat related
    to each other: the learning rate and the optimizer. As with logistic regression
    or linear regression, the optimizer’s goal is to find the parameters that provide
    the lowest possible loss value. Therefore, this is a minimization problem and
    can be represented as in *Figure 6**.7*: we seek to find the set of parameters
    that give the lowest possible value.'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个重要的概念可能会影响这些结果，它们在某种程度上是相互关联的：学习率和优化器。与逻辑回归或线性回归类似，优化器的目标是找到提供最低可能损失值的参数。因此，这是一个最小化问题，可以如*图
    6.7*所示表示：我们寻求找到一组参数，能给出最低的损失值。
- en: '![Figure 6.7 – Representation of the loss L as a function of the parameters
    w. The red cross is the optimal point, while the blue cross is a random arbitrary
    set of weights](img/B19629_06_07.jpg)'
  id: totrans-534
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – 损失L作为参数w的函数的表示。红色交叉点是最优点，而蓝色交叉点是一个随机的任意权重集合](img/B19629_06_07.jpg)'
- en: Figure 6.7 – Representation of the loss L as a function of the parameters w.
    The red cross is the optimal point, while the blue cross is a random arbitrary
    set of weights
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – 损失L作为参数w的函数的表示。红色交叉点是最优点，而蓝色交叉点是一个随机的任意权重集合
- en: Let’s see how the learning rate can impact the learning curve.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看学习率如何影响学习曲线。
- en: Learning rate
  id: totrans-537
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习率
- en: The learning rate is set in PyTorch when instantiating the optimizer, with the
    *lr=0.001* parameter for example. Arguably, we can have four main cases for the
    learning rate value, as presented in *Figure 6**.8*, from a low learning rate
    to a very high learning rate.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，通过实例化优化器时设置学习率，例如使用*lr=0.001*参数。可以说，学习率的值主要有四种情况，正如在*图 6.8*中所展示的，从较低的学习率到非常高的学习率。
- en: '![Figure 6.8 – The four main categories of learning rate: too low learning
    rate, good learning rate, high learning rate, very high learning rate (diverging
    loss)](img/B19629_06_08.jpg)'
  id: totrans-539
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8 – 学习率的四个主要类别：学习率过低、合适的学习率、学习率过高、学习率非常高（损失发散）](img/B19629_06_08.jpg)'
- en: 'Figure 6.8 – The four main categories of learning rate: too low learning rate,
    good learning rate, high learning rate, very high learning rate (diverging loss)'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 – 学习率的四个主要类别：学习率过低、合适的学习率、学习率过高、学习率非常高（损失发散）
- en: In terms of loss, the rates can be intuited from *Figure 6**.9*, presenting
    the evolution of the weights and the loss for several epochs.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 就损失而言，学习率的变化可以从*图 6**.9*中直观感受到，图中展示了多个epoch中权重和损失的变化过程。
- en: '![Figure 6.9 – A visual interpretation of the four cases of learning rate:
    a) a low learning rate, b) a good learning rate, c) a high learning rate, d) a
    very high learning rate](img/B19629_06_09.jpg)'
  id: totrans-542
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9 – 学习率四种情况的可视化解释：a) 低学习率，b) 合适的学习率，c) 高学习率，d) 非常高的学习率](img/B19629_06_09.jpg)'
- en: 'Figure 6.9 – A visual interpretation of the four cases of learning rate: a)
    a low learning rate, b) a good learning rate, c) a high learning rate, d) a very
    high learning rate'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – 学习率四种情况的可视化解释：a) 低学习率，b) 合适的学习率，c) 高学习率，d) 非常高的学习率
- en: '*Figure 6**.9* can be further explained with the following:'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6**.9* 可以通过以下内容进一步解释：'
- en: '**A low learning rate (a)**: The loss will decrease over the epochs but too
    slowly and may take a very long time to converge. It may also get the model stuck
    in a local minimum.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低学习率 (a)**：损失会随着epoch的进行而逐渐减少，但速度太慢，可能需要非常长的时间才能收敛，也可能使模型陷入局部最小值。'
- en: '**A good learning rate (b)**: The loss will decrease steadily until it gets
    close enough to the global minimum.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合适的学习率 (b)**：损失将稳定下降，直到接近全局最小值。'
- en: '**A slightly too large learning rate (c)**: The loss will decrease steeply
    at first but may soon jump over the global minimum without being able to ever
    reach it.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**略大的学习率 (c)**：损失一开始会急剧下降，但可能很快跳过全局最小值，无法再到达它。'
- en: '**A very high learning rate (d)**: The loss will rapidly diverge, taking learning
    steps that are way too large.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非常高的学习率 (d)**：损失将迅速发散，学习步伐过大。'
- en: Tuning the learning rate may sometimes help to produce the best results. Several
    techniques, such as the so-called learning rate decay, decrease the learning rate
    over time to hopefully more accurately catch the global minimum.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 调整学习率有时有助于产生最佳结果。几种技术，如所谓的学习率衰减，会随着时间推移逐渐降低学习率，以期更加准确地捕捉到全局最小值。
- en: Optimizer
  id: totrans-550
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化器
- en: 'There are many robust and useful optimizers in deep learning besides the arguably
    most famous ones (the stochastic gradient descent and Adam). Without getting into
    the details of those optimizers, let’s just give some insight into how they work
    and their differences, summarized in *Figure 6**.10*:'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 除了可能最著名的随机梯度下降和Adam优化器，深度学习中还有许多强大且有用的优化器。在不深入讨论这些优化器的细节的前提下，让我们简要了解它们的工作原理和它们之间的差异，如*图
    6**.10*所总结：
- en: '**Stochastic gradient descent** simply computes the gradients from the loss
    for each batch, without any further sophistication. It means that sometimes, the
    optimization of one batch may be almost in the opposite direction of another batch.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降** 只是根据每个批次的损失计算梯度，没有进一步的复杂处理。这意味着，有时一个批次的优化方向可能几乎与另一个批次相反。'
- en: '**Adam** uses momentum, meaning that for each batch, not only the gradient
    from this batch is used, but also the momentum of the previously computed gradients.
    This allows Adam to keep an overall more consistent direction, and hopefully to
    converge faster.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Adam** 使用动量，这意味着对于每个批次，除了使用该批次的梯度外，还会使用之前计算过的梯度的动量。这使得Adam能够保持一个更加一致的方向，并且有望更快地收敛。'
- en: '![Figure 6.10 – A visual representation of training towards a global minimum,
    on the left with stochastic gradient descent; on the right with Adam keeping the
    momentum of previous steps](img/B19629_06_10.jpg)'
  id: totrans-554
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.10 – 向全局最小值训练的可视化表现，左边是随机梯度下降，右边是Adam保持之前步骤的动量](img/B19629_06_10.jpg)'
- en: Figure 6.10 – A visual representation of training towards a global minimum,
    on the left with stochastic gradient descent; on the right with Adam keeping the
    momentum of previous steps
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 – 向全局最小值训练的可视化表现，左边是随机梯度下降，右边是Adam保持之前步骤的动量
- en: 'The optimization process can be summarized with quite a simple metaphor. This
    is like hiking somewhere up a mountain, and then trying to go down (to the global
    minimum) while surrounded by fog. You can either go down with stochastic gradient
    descent or Adam:'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 优化过程可以用一个简单的比喻来概括。这就像在雾霾中爬山，试图向下走（到达全局最小值）。你可以通过随机梯度下降或Adam来进行下降：
- en: With stochastic gradient descent, you look around you, choose the direction
    with the steepest slope downward, and take a step in that direction. And then
    do it again.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机梯度下降时，你观察周围的情况，选择向下坡度最陡的方向，并沿该方向迈出一步。然后再重复一次。
- en: With Adam, you do the same as stochastic gradient descent but running. You quickly
    look around you, see the direction with the steepest slope downward, and try to
    take a step in that direction while keeping the inertia from your previous steps
    since you’re running. And then do it again.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Adam，你做的和随机梯度下降一样，但速度更快。你迅速环顾四周，看到朝下坡度最陡的方向，然后试图朝那个方向迈出一步，同时保持之前步骤的惯性，因为你是在跑步。然后再重复这一过程。
- en: Note that in this analogy, the step size would be the learning rate.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这个类比中，步长就是学习率。
- en: See also
  id: totrans-560
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'A list of available optimizers on PyTorch: [https://pytorch.org/docs/stable/optim.xhtml#algorithms](https://pytorch.org/docs/stable/optim.xhtml#algorithms)'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch上可用的优化器列表：[https://pytorch.org/docs/stable/optim.xhtml#algorithms](https://pytorch.org/docs/stable/optim.xhtml#algorithms)
- en: Training a multiclass classification neural network
  id: totrans-562
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个多类别分类神经网络
- en: 'In this recipe, we will have a look at another very common task: multiclass
    classification with neural networks, in this instance using PyTorch. We will work
    on a very iconic dataset in deep learning: **MNIST handwritten digit recognition**.
    This dataset is a set of small grayscale images of 28x28 pixels, depicting handwritten
    digits between 0 and 9, having thus 10 classes.'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将关注另一个非常常见的任务：使用神经网络进行多类别分类，在本例中使用PyTorch。我们将处理一个深度学习中的经典数据集：**MNIST手写数字识别**。该数据集包含28x28像素的小灰度图像，描绘的是0到9之间的手写数字，因此有10个类别。
- en: Getting ready
  id: totrans-564
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In classical machine learning, multiclass classification is usually not handled
    natively. For example, when training logistic regression with scikit-learn on
    a three-class task (e.g., the Iris dataset), scikit-learn will automatically train
    three models, using the one-versus-the-rest method.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典机器学习中，多类别分类通常不会原生处理。例如，在使用scikit-learn训练一个三分类任务（例如，Iris数据集）时，scikit-learn将自动训练三个模型，采用一对其余法（one-versus-the-rest）。
- en: 'In deep learning, it is possible for the model to natively handle more than
    two classes. To do so, only a few changes are required compared to binary classification:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，模型可以原生处理超过两个类别。为了实现这一点，与二分类相比，只需进行少量的更改：
- en: 'The output layer has as many units as classes: this way, each unit will be
    responsible for predicting the probability of one class'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层的单元数与类别数相同：这样，每个单元将负责预测一个类别的概率
- en: The output layer’s activation function is the softmax function, a function such
    that the sum of the units is equal to 1, allowing us to consider it as a probability
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层的激活函数是softmax函数，该函数使得所有单元的和等于1，从而允许我们将其视为概率
- en: The loss function is the cross-entropy loss, considering multiple classes, unlike
    the binary cross entropy
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数是交叉熵损失，考虑多个类别，而不是二元交叉熵
- en: 'In our case, we will need a few other changes in the code that are specific
    to the data itself. Since the input is now an image, some transformations are
    required:'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们还需要对代码进行一些其他特定于数据本身的更改。由于输入现在是图像，因此需要进行一些转换：
- en: The image, a 2D (or 3D if RGB color image) array, must be flattened to be 1D
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像是一个二维（或三维，如果是RGB彩色图像）数组，必须将其展平为一维
- en: The data must be normalized, just like rescaling for quantitative data
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据必须标准化，就像定量数据的重缩放一样
- en: 'To do that, we will require the following libraries: torch, torchvision (for
    dataset loading and image transformation), and matplotlib for visualization. They
    can be installed with `pip install torch` `torchvision matplotlib`.'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们将需要以下库：torch、torchvision（用于数据集加载和图像转换）和matplotlib（用于可视化）。可以通过`pip
    install torch` `torchvision matplotlib`来安装。
- en: How to do it…
  id: totrans-574
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'In this recipe, we will reuse the same pattern as previously in this chapter:
    we will train a two-hidden-layer neural network. But a few things will change,
    though:'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将重用本章之前相同的模式：训练一个有两个隐藏层的神经网络。但有一些事情会有所不同：
- en: The input data is a grayscale image from the MNIST handwritten digits dataset,
    so it’s a 2D array that needs to be flattened
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数据是来自MNIST手写数字数据集的灰度图像，因此它是一个需要展平的二维数组
- en: The output layer will have not one, but ten units for the ten classes of the
    dataset; the loss will change accordingly
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层将不只有一个单元，而是为数据集的十个类别准备十个单元；损失函数也会相应改变
- en: We will not only compute the training and test losses in the training loop,
    but also the accuracy
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不仅会在训练循环中计算训练和测试损失，还会计算准确率
- en: 'Let’s see how to do that in practice now:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何在实践中做到这一点：
- en: 'Import the required libraries. As in previous recipes, we import several useful
    torch modules and functions:'
  id: totrans-580
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库。和以前的食谱一样，我们导入了几个有用的torch模块和函数：
- en: '`torch` itself'
  id: totrans-581
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch`本身'
- en: '`torch.nn` containing the required classes for building a neural network'
  id: totrans-582
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.nn`包含构建神经网络所需的类'
- en: '`torch.nn.functional` for activation functions such as ReLU'
  id: totrans-583
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.nn.functional`用于激活函数，例如ReLU'
- en: '`DataLoader` for handling the data'
  id: totrans-584
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataLoader`用于处理数据'
- en: 'We also need some imports from torchvision:'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要从`torchvision`导入一些内容：
- en: '`MNIST` for loading the dataset'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MNIST`用于加载数据集'
- en: '`transforms` for transforming the dataset, both rescaling and flattening the
    data:'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transforms`用于转换数据集，包括重新缩放和展平数据：'
- en: '[PRE256]'
  id: totrans-588
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE256]'
- en: '[PRE257]'
  id: totrans-589
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE257]'
- en: '[PRE258]'
  id: totrans-590
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE258]'
- en: '[PRE259]'
  id: totrans-591
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE259]'
- en: '[PRE260]'
  id: totrans-592
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE260]'
- en: '[PRE261]'
  id: totrans-593
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE261]'
- en: '[PRE262]'
  id: totrans-594
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE262]'
- en: 'Instantiate the transformations. We use the `Compose` class, allowing us to
    compose two or more transformations. Here, we compose three transformations:'
  id: totrans-595
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化转换。我们使用`Compose`类，可以将两个或更多的转换组合起来。这里，我们组合了三个转换：
- en: '`transforms.ToTensor()`: Converts the input image to `torch.Tensor` format.'
  id: totrans-596
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transforms.ToTensor()`：将输入图像转换为`torch.Tensor`格式。'
- en: '`transforms.Normalize()`: Normalizes the image with a mean value and standard
    deviation. It will subtract the mean (i.e., 0.1307) and then divide by the standard
    deviation (i.e., 0.3081) for each pixel value.'
  id: totrans-597
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transforms.Normalize()`：使用均值和标准差对图像进行归一化。它将减去均值（即0.1307），然后除以标准差（即0.3081），对每个像素值进行处理。'
- en: '`transforms.Lambda(torch.flatten)`: Flattens the 2D tensor to a 1D tensor:'
  id: totrans-598
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transforms.Lambda(torch.flatten)`：将2D张量展平为1D张量：'
- en: '[PRE263]'
  id: totrans-599
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE263]'
- en: '[PRE264]'
  id: totrans-600
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE264]'
- en: '[PRE265]'
  id: totrans-601
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE265]'
- en: Important note
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Images are commonly normalized with a mean and standard deviation of 0.5\. We
    normalize with the specific values used in the preceding code block because the
    dataset is made with specific images, but 0.5 would work fine too. Check the *See
    also* subsection of this recipe for an explanation.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 图像通常会用均值和标准差为0.5进行归一化。我们使用前面代码块中使用的特定值进行归一化，因为数据集是基于特定图像创建的，但0.5也能正常工作。有关解释，请查看本食谱中的*另见*小节。
- en: 'Load the train and test sets, as well as the train and data loaders. Using
    the `MNIST` class, we both get the train and test sets using the `train` parameter
    `as` `True` and `False`, respectively. We directly apply the previously defined
    transformations while loading the data with the MNIST class too. Then we instantiate
    the data loaders with a batch size of 64:'
  id: totrans-604
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载训练集和测试集，以及训练数据加载器和测试数据加载器。通过使用`MNIST`类，我们分别使用`train`参数为`True`和`False`来获取训练集和测试集。在加载数据时，我们也直接应用之前定义的转换。然后，我们以批大小64实例化数据加载器：
- en: '[PRE266]'
  id: totrans-605
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE266]'
- en: '[PRE267]'
  id: totrans-606
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE267]'
- en: '[PRE268]'
  id: totrans-607
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE268]'
- en: '[PRE269]'
  id: totrans-608
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE269]'
- en: '[PRE270]'
  id: totrans-609
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE270]'
- en: '[PRE271]'
  id: totrans-610
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE271]'
- en: '[PRE272]'
  id: totrans-611
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE272]'
- en: '[PRE273]'
  id: totrans-612
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE273]'
- en: 'Define the neural network. We define here by default a neural network with
    2 hidden layers of 24 units. The output layer has 10 units for the 10 classes
    of the data (our digits between 0 and 9). Note that the softmax function is applied
    to the output layer, allowing the sum of the 10 units to be strictly equal to
    1:'
  id: totrans-613
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义神经网络。这里我们默认定义了一个具有2个隐藏层、每层24个单元的神经网络。输出层有10个单元，表示数据的10个类别（我们的是0到9之间的数字）。请注意，Softmax函数应用于输出层，使得10个单元的和严格等于1：
- en: '[PRE274]'
  id: totrans-614
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE274]'
- en: '[PRE275]'
  id: totrans-615
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE275]'
- en: '[PRE276]'
  id: totrans-616
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE276]'
- en: '[PRE277]'
  id: totrans-617
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE277]'
- en: '[PRE278]'
  id: totrans-618
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE278]'
- en: '[PRE279]'
  id: totrans-619
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE279]'
- en: '[PRE280]'
  id: totrans-620
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE280]'
- en: '[PRE281]'
  id: totrans-621
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE281]'
- en: '[PRE282]'
  id: totrans-622
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE282]'
- en: '[PRE283]'
  id: totrans-623
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE283]'
- en: '[PRE284]'
  id: totrans-624
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE284]'
- en: '[PRE285]'
  id: totrans-625
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE285]'
- en: '[PRE286]'
  id: totrans-626
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE286]'
- en: '[PRE287]'
  id: totrans-627
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE287]'
- en: '[PRE288]'
  id: totrans-628
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE288]'
- en: '[PRE289]'
  id: totrans-629
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE289]'
- en: '[PRE290]'
  id: totrans-630
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE290]'
- en: '[PRE291]'
  id: totrans-631
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE291]'
- en: '[PRE292]'
  id: totrans-632
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE292]'
- en: '[PRE293]'
  id: totrans-633
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE293]'
- en: '[PRE294]'
  id: totrans-634
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE294]'
- en: 'We can now instantiate the model with the right input shape of 784 (28x28 pixels),
    and check the forward propagation works properly on a given random tensor:'
  id: totrans-635
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以用正确的输入形状784（28x28像素）来实例化模型，并检查前向传播在给定的随机张量上是否正常工作：
- en: '[PRE295]'
  id: totrans-636
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE295]'
- en: '[PRE296]'
  id: totrans-637
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE296]'
- en: '[PRE297]'
  id: totrans-638
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE297]'
- en: '[PRE298]'
  id: totrans-639
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE298]'
- en: '[PRE299]'
  id: totrans-640
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE299]'
- en: '[PRE300]'
  id: totrans-641
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE300]'
- en: '[PRE301]'
  id: totrans-642
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE301]'
- en: 'Here is the output:'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '[PRE302]'
  id: totrans-644
  prefs: []
  type: TYPE_PRE
  zh: '[PRE302]'
- en: Tip
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Note the output is a tensor of 10 values, with a sum of 1.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输出是一个包含10个值的张量，其和为1。
- en: 'Define the loss function as the cross-entropy loss, available as `nn.CrossEntropyLoss()`
    in PyTorch, and the optimizer as Adam:'
  id: totrans-647
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义损失函数为交叉熵损失，在PyTorch中可以通过`nn.CrossEntropyLoss()`获得，并将优化器定义为Adam：
- en: '[PRE303]'
  id: totrans-648
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE303]'
- en: '[PRE304]'
  id: totrans-649
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE304]'
- en: '[PRE305]'
  id: totrans-650
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE305]'
- en: 'Before training, we implement an `epoch_step` helper function that works for
    both the train and test sets, allowing us to loop over all the data, compute the
    loss and the accuracy, and train the model for the training set:'
  id: totrans-651
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练之前，我们实现了一个`epoch_step`辅助函数，适用于训练集和测试集，允许我们遍历所有数据，计算损失和准确度，并在训练集上训练模型：
- en: '[PRE306]'
  id: totrans-652
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE306]'
- en: '[PRE307]'
  id: totrans-653
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE307]'
- en: '[PRE308]'
  id: totrans-654
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE308]'
- en: '[PRE309]'
  id: totrans-655
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE309]'
- en: '[PRE310]'
  id: totrans-656
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE310]'
- en: '[PRE311]'
  id: totrans-657
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE311]'
- en: '[PRE312]'
  id: totrans-658
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE312]'
- en: '[PRE313]'
  id: totrans-659
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE313]'
- en: '[PRE314]'
  id: totrans-660
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE314]'
- en: '[PRE315]'
  id: totrans-661
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE315]'
- en: '[PRE316]'
  id: totrans-662
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE316]'
- en: '[PRE317]'
  id: totrans-663
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE317]'
- en: '[PRE318]'
  id: totrans-664
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE318]'
- en: '[PRE319]'
  id: totrans-665
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE319]'
- en: '[PRE320]'
  id: totrans-666
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE320]'
- en: '[PRE321]'
  id: totrans-667
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE321]'
- en: '[PRE322]'
  id: totrans-668
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE322]'
- en: '[PRE323]'
  id: totrans-669
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE323]'
- en: '[PRE324]'
  id: totrans-670
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE324]'
- en: '[PRE325]'
  id: totrans-671
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE325]'
- en: '[PRE326]'
  id: totrans-672
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE326]'
- en: 'We can now train the neural network on 20 epochs. For each epoch, we also compute
    the following:'
  id: totrans-673
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以在20个epoch上训练神经网络。在每个epoch中，我们还会计算以下内容：
- en: The loss for both train and test sets
  id: totrans-674
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集和测试集的损失
- en: The accuracy for both train and test sets
  id: totrans-675
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集和测试集的准确率
- en: 'As for the previous recipe, before training, the model is switched to train
    mode with `model.train()`, while before evaluating on the test set, it is switched
    to `eval` model with `model.eval()`:'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 和前一个食谱一样，在训练之前，模型会通过`model.train()`切换到训练模式，而在评估测试集之前，它会通过`model.eval()`切换到评估模式：
- en: '[PRE327]'
  id: totrans-677
  prefs: []
  type: TYPE_PRE
  zh: '[PRE327]'
- en: 'We can plot the loss for both the train and test sets as a function of the
    epoch, since we stored those values for each epoch:'
  id: totrans-678
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以绘制损失随时代变化的图像，因为我们已经存储了每个时代的这些值，并且可以同时查看训练集和测试集的变化：
- en: '[PRE328]'
  id: totrans-679
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE328]'
- en: '[PRE329]'
  id: totrans-680
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE329]'
- en: '[PRE330]'
  id: totrans-681
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE330]'
- en: '[PRE331]'
  id: totrans-682
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE331]'
- en: 'Here is the output:'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '![Figure 6.11 – Resulting cross-entropy loss as a function of the epoch, for
    both the train and test sets](img/B19629_06_11.jpg)'
  id: totrans-684
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.11 – 交叉熵损失随时代变化的结果，适用于训练集和测试集](img/B19629_06_11.jpg)'
- en: Figure 6.11 – Resulting cross-entropy loss as a function of the epoch, for both
    the train and test sets
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 – 交叉熵损失随时代变化的结果，适用于训练集和测试集
- en: Since the loss seems to keep improving on both the **train** and **test** sets
    after 20 epochs, it could be interesting in terms of performance to keep training
    more epochs.
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在经过 20 个时代后，**训练集**和**测试集**的损失似乎仍在不断改进，从性能角度来看，继续训练更多的时代可能会很有趣。
- en: 'It is also possible to do the same with the accuracy score, showing the equivalent
    results:'
  id: totrans-687
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，也可以通过显示相应结果，使用准确度分数进行相同操作：
- en: '[PRE332]'
  id: totrans-688
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE332]'
- en: '[PRE333]'
  id: totrans-689
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE333]'
- en: '[PRE334]'
  id: totrans-690
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE334]'
- en: '[PRE335]'
  id: totrans-691
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE335]'
- en: 'Here are the results:'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是结果：
- en: '![Figure 6.12 – Resulting accuracy as a function of the epoch for the train
    and test sets](img/B19629_06_12.jpg)'
  id: totrans-693
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.12 – 训练集和测试集准确度随时代变化的结果](img/B19629_06_12.jpg)'
- en: Figure 6.12 – Resulting accuracy as a function of the epoch for the train and
    test sets
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 – 训练集和测试集准确度随时代变化的结果
- en: At the end, the accuracy is about 97% on the train set and 96% on the test set.
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，训练集的准确度大约为97%，测试集的准确度为96%。
- en: 'Once the model has been trained, it is, of course, possible to store it so
    that it can be used on new data directly. There are several ways to save a model:'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，当然可以将其存储起来，以便在新数据上直接使用。有几种保存模型的方法：
- en: '`net` class must first be instantiated.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须首先实例化`net`类。
- en: '**Saving the entire model**: This saves both the weights and the architecture,
    meaning only the file needs to be loaded.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保存整个模型**：这会保存权重和架构，意味着只需要加载文件。'
- en: '**Saving in torchscript format**: This saves the entire model using a more
    efficient representation. This method is more suited for deployment and inference
    at scale.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**以torchscript格式保存**：这会使用更高效的表示方式保存整个模型。此方法更适合于大规模的部署和推断。'
- en: 'For now, let’s just save the `state` dict, reload it, and then compute inferences
    on an image:'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需保存`state`字典，重新加载它，然后对图像进行推断：
- en: '[PRE336]'
  id: totrans-701
  prefs: []
  type: TYPE_PRE
  zh: '[PRE336]'
- en: 'It is now possible to compute inferences using that loaded, already-trained
    model on a given image:'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，可以使用已加载的训练好模型对给定图像进行推断：
- en: '[PRE337]'
  id: totrans-703
  prefs: []
  type: TYPE_PRE
  zh: '[PRE337]'
- en: 'This is what we get:'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们得到的结果：
- en: '![Figure 6.13 – The resulting output with six input images and their predictions
    from the trained model](img/B19629_06_13.jpg)'
  id: totrans-705
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.13 – 使用六个输入图像及其来自训练模型的预测结果](img/B19629_06_13.jpg)'
- en: Figure 6.13 – The resulting output with six input images and their predictions
    from the trained model
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13 – 使用六个输入图像及其来自训练模型的预测结果
- en: As expected, the loaded model can correctly predict the right number on most
    images.
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，加载的模型能够正确预测大多数图像上的正确数字。
- en: There’s more…
  id: totrans-708
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: 'Deep learning is often used in heavily computational tasks requiring a lot
    of resources. In such cases, the use of a GPU is often a necessity. PyTorch of
    course allows us to train and infer models on GPUs. Only a few steps are required
    to do so: declaring a device variable and moving both the model and data to this
    device. Let’s have a quick look at how to do it.'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习通常用于需要大量计算资源的任务。在这种情况下，使用GPU往往是必须的。当然，PyTorch允许我们在GPU上训练和推断模型。只需要几个步骤：声明一个设备变量并将模型和数据移到该设备。让我们快速了解一下如何实现这一过程。
- en: Choosing the device
  id: totrans-710
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择设备
- en: 'Declaring a device variable to be the GPU can be done with the following Python
    code:'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 声明一个设备变量为GPU可以通过以下Python代码完成：
- en: '[PRE338]'
  id: totrans-712
  prefs: []
  type: TYPE_PRE
  zh: '[PRE338]'
- en: This line instantiates a `torch.device` object, containing `"cuda"` if CUDA
    is available, else it contains `"cpu"`. Indeed, if CUDA is not installed, or if
    there is no GPU on your hardware, the CPU will be used (which is the default behavior).
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 这一行实例化了一个`torch.device`对象，如果CUDA可用，它将包含`"cuda"`，否则包含`"cpu"`。事实上，如果未安装CUDA，或者硬件上没有GPU，将使用CPU（这是默认行为）。
- en: If the GPU has been correctly detected, the output of `print(device)` is `"cuda"`.
    Otherwise, the output is `"cpu"`.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 如果GPU已被正确检测，`print(device)`的输出将是`"cuda"`。否则，输出将是`"cpu"`。
- en: Moving the model and data to the GPU
  id: totrans-715
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将模型和数据移到GPU
- en: 'Once the device is correctly set to the GPU, both the model and data have to
    be moved to the GPU memory. To do so, you only need to call the`.to(device)` method
    on both the model and the data. For example, the training and evaluation code
    that we used in this recipe would become the following:'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设备正确设置为GPU，模型和数据都必须移动到GPU内存中。为此，只需要在模型和数据上调用`.to(device)`方法。例如，我们在此食谱中使用的训练和评估代码将变成以下内容：
- en: '[PRE339]'
  id: totrans-717
  prefs: []
  type: TYPE_PRE
  zh: '[PRE339]'
- en: At the beginning, the model is moved to the GPU device once with `net =` `net.to(device)`.
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，模型通过`net =` `net.to(device)`将其一次性移动到GPU设备。
- en: At each iteration loop for both the training and evaluation, the inputs and
    labels tensors are moved to the device with `tensor =` `tensor.to(device)`.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次训练和评估的迭代循环中，输入和标签张量都通过`tensor =` `tensor.to(device)`移动到设备上。
- en: Tip
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The data can be either fully loaded on the GPU at loading, or done one batch
    at a time during training. However, since only rather small datasets can be fully
    loaded in the GPU memory, we did not present this solution here.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以在加载时完全加载到GPU中，或者在训练过程中一次加载一个批次。然而，由于只有较小的数据集可以完全加载到GPU内存中，因此我们在此并未展示此解决方案。
- en: See also
  id: totrans-722
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'The official documentation on saving and loading PyTorch models: [https://pytorch.org/tutorials/beginner/saving_loading_models.xhtml](https://pytorch.org/tutorials/beginner/saving_loading_models.xhtml)'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存和加载 PyTorch 模型的官方文档：[https://pytorch.org/tutorials/beginner/saving_loading_models.xhtml](https://pytorch.org/tutorials/beginner/saving_loading_models.xhtml)
- en: 'The reason the images are transformed with such specific values for the MNIST
    dataset: [https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457](https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457)'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像使用如此具体的值进行转换是因为 MNIST 数据集：[https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457](https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457)
