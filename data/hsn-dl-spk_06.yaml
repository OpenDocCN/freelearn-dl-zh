- en: Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: In this chapter, we are going to learn more about **Recurrent Neural Networks**
    (**RNNs**), an overview of their most common use cases, and, finally, a possible
    implementation by starting to be hands-on using the DeepLearning4j framework.
    This chapter's code examples involve Apache Spark too. As stated in the previous
    chapter for CNNs, training and evaluation strategies for RNNs will be covered
    in [Chapter 7](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml), *Training Neural Networks
    with Spark*, [Chapter 8](b30120ea-bd42-4cb7-95d9-5ecaa2b7c181.xhtml), *Monitoring
    and Debugging Neural Network Training*, and [Chapter 9](869a9495-e759-4810-8623-d8b76ba61398.xhtml),
    *Interpreting Neural Network Output*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将进一步了解**循环神经网络**（**RNNs**），它们最常见的应用场景概述，以及最后通过使用 DeepLearning4j 框架进行实际操作的可能实现。本章的代码示例还涉及到
    Apache Spark。如同前一章关于 CNNs 的内容所述，RNN 的训练和评估策略将在[第 7 章](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml)，*使用
    Spark 训练神经网络*，[第 8 章](b30120ea-bd42-4cb7-95d9-5ecaa2b7c181.xhtml)，*监控与调试神经网络训练*，以及[第
    9 章](869a9495-e759-4810-8623-d8b76ba61398.xhtml)，*解释神经网络输出*中详细介绍。
- en: In this chapter, I have tried to reduce the usage of math concepts and formulas
    as much as possible in order to make the reading and comprehension easier for
    developers and data analysts who might have no math or data science background.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我尽量减少了数学概念和公式的使用，以便让没有数学或数据科学背景的开发人员和数据分析师能够更容易地阅读和理解。
- en: 'The chapter covers the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: '**Long short-term memory** (**LSTM**)'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长短期记忆**（**LSTM**）'
- en: Use cases
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用场景
- en: Hands-on RNN with Spark
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实战 RNN 与 Spark
- en: LSTM
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM
- en: 'RNNs are multilayer neural networks that are used to recognize patterns in
    sequences of data. By sequences of data, we mean text, handwriting, numerical
    times series (coming for example from sensors), log entries, and so on. The algorithms
    involved here have a temporal dimension too: they take time (and this is the main
    difference with CNNs) and sequence both into account. For a better understanding
    of the need for RNNs, we have to look at the basics of feedforward networks first.
    Similar to RNNs, these networks channel information through a series of mathematical
    operations performed at the nodes of the network, but they feed information straight
    through, never touching a given node twice. The network is fed with input examples
    that are then transformed into an output: in simple words, they map raw data to
    categories. Training happens for them on labeled inputs, until the errors made
    when guessing input categories has been minimized. This is the way a network learns
    to categorize new data it has never seen before. A feedforward network hasn''t
    any notion of order in time: the only input it considers is the current one it
    has been exposed to, and it doesn''t necessarily alter how it classifies the next
    one. RNNs take as input the current example they see, plus anything they have
    perceived previously. A RNN can be then be seen as multiple feedforward neural
    networks passing information from one to the other.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 是多层神经网络，用于识别数据序列中的模式。这里的数据序列可以是文本、手写字、数字时间序列（例如来自传感器的数据）、日志条目等。涉及的算法也具有时间维度：它们会考虑时间（这与
    CNN 的主要区别）和序列。为了更好地理解为什么需要 RNN，我们首先要看一下前馈网络的基础。与 RNN 类似，这些网络通过一系列数学操作在网络的节点上处理信息，但它们是将信息直接传递，且每个节点不会被重复访问。网络接收输入示例，然后将其转化为输出：简而言之，它们将原始数据映射到类别。训练过程发生在有标签的输入上，直到猜测输入类别时所犯的错误最小化。这是网络学习如何对它从未见过的新数据进行分类的方式。前馈网络没有时间顺序的概念：它仅考虑当前输入，并不一定会改变它如何分类下一个输入。RNN
    则接收当前示例以及它之前感知到的任何信息作为输入。可以将 RNN 视为多个前馈神经网络，将信息从一个网络传递到另一个网络。
- en: In the RNNs' use case scenarios, a sequence could be a finite or infinite stream
    of interdependent data. CNNs can't work well in those cases because they don’t
    have any correlation between previous and next input. From [Chapter 5](fbec1d8a-a92f-4899-af0f-11f3d545e0eb.xhtml), *Convolutional
    Neural Networks*, you have learned that a CNN takes an input and then outputs
    based on the trained model. Running a given number of different inputs, none of
    them would be biased by taking into account any of the previous outputs. But if
    you consider a case like that presented in the last sections of this chapter (a
    sentence generation case), where all the generated words are dependent on the
    those generated before, there is definitely a need to bias based on previous output.
    This is where RNNs come to the rescue, because they have memory of what happened
    earlier in the sequence of data and this helps them to get the context. RNNs in
    theory can look back indefinitely at all of the previous steps, but really, for
    performance reasons, they have to restrict looking back at the last few steps
    only.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在RNN的应用场景中，一个序列可能是有限的或无限的、相互依赖的数据流。CNN在这些情况下表现不好，因为它们没有考虑前一个和下一个输入之间的相关性。根据[第5章](fbec1d8a-a92f-4899-af0f-11f3d545e0eb.xhtml)，你已经了解到，CNN接收输入并根据训练的模型进行输出。对于给定数量的不同输入，任何一个都不会受到之前输出的影响。但如果考虑到本章最后几节提出的情况（一个句子生成的案例），其中所有生成的单词都依赖于之前生成的单词，那么就一定需要根据之前的输出进行偏置。这时，RNN就派上用场了，因为它们能记住数据序列中之前发生的事情，这有助于它们获取上下文。理论上，RNN可以无限回顾所有前一步骤，但实际上，出于性能考虑，它们只能回顾最后几步。
- en: Let's go into the details of RNNs. For this explanation, I am going to start
    from a **Multilayer Perception** (**MLP**), a class of feedforward ANN. The minimal
    implementation of an MLP has at least three layers of nodes. But for the input
    nodes, each node is a neuron that uses a nonlinear activation function. The input
    layer, of course, takes the input. It is the first hidden layer that does the
    activation, passing onto the next hidden layers, and so on. Finally, it reaches
    the output layer. This is responsible for providing the output. All of the hidden
    layers behave differently, because each one has different weights, bias, and activation
    functions. In order to make it possible and easier to merge them, all the layers
    need to be replaced with the same weights (and also same biases and activation
    function). This is the only way to combine all the hidden layers into a single
    recurrent layer. They start looking as shown in the following diagram.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解RNN的细节。为了进行这个解释，我将从**多层感知机**（**MLP**）开始，它是一个前馈人工神经网络（ANN）类别。MLP的最小实现至少有三层节点。但对于输入节点，每个节点是一个使用非线性激活函数的神经元。输入层当然是接收输入的部分。第一个隐藏层进行激活操作，将信息传递给下一个隐藏层，以此类推。最终，信息到达输出层，负责提供输出。所有隐藏层的行为不同，因为每一层都有不同的权重、偏置和激活函数。为了使这些层能够合并并简化这一过程，所有层需要替换为相同的权重（以及相同的偏置和激活函数）。这是将所有隐藏层合并成一个单一的循环层的唯一方法。它们开始看起来如下图所示。
- en: '![](img/f7fe7d22-1b68-4038-bf16-8b4e3c859d78.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f7fe7d22-1b68-4038-bf16-8b4e3c859d78.png)'
- en: Figure 6.1
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1
- en: With reference to the preceding diagram, the network **H** receives some input
    **x** and produces an output **o**. Any info passes from one step of the network
    to the next through a loop mechanism. An input is provided to the hidden layer
    of the network at each step. Any neuron of an RNN stores the inputs it receives
    during all of the previous steps and then can merge that information with input
    passed to it at the current step. This means that a decision taken at a time step
    *t-1* affects the decision that is taken at a time *t*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的图示，网络**H**接收一些输入**x**并产生输出**o**。信息通过循环机制从网络的一个步骤传递到下一个步骤。在每个步骤中，输入会被提供给网络的隐藏层。RNN的任何神经元都会存储它在所有前一步骤中接收到的输入，然后可以将这些信息与当前步骤传递给它的输入进行合并。这意味着在时间步*t-1*做出的决策会影响在时间步*t*做出的决策。
- en: 'Let’s rephrase the preceding explanation with an example: let''s say we want
    to predict what the next letter would be after a sequence of letters. Let''s assume
    the input word is **pizza**, which is of five letters. What happens when the network
    tries to figure out the fifth letter after the first four letters have been fed
    to the network? Five iterations happen for the hidden layer. If we unfold the
    network, it would be a five layers network, one for each letter of the input word
    (see [Chapter 2](a177e128-f96b-4a3a-8e3e-24f133492cb0.xhtml), *Deep Learning Basics*, *Figure
    2.11* as reference). We can see it then as a normal neural network repeated multiple
    times (5). The number of times we unroll it has a direct correlation with how
    far in the past the network can remember. Going back to the **pizza** example,
    the total vocabulary of the input data is *{p, i, z, a}*. The hidden layer or
    the RNN applies a formula to the current input as well as the previous state.
    In our example, the letter *p* from the word *pizza*, being the first letter,
    has nothing preceding it, so nothing is done and we can move on to the next letter,
    which is *i*. The formula is applied by the hidden layer at the time between letter
    *i* and the previous state, which was letter *p*. If at a given time *t*, the
    input is *i*, then at time *t-1*, the input is *p*. By applying the formula to
    both *p* and *i* we get a new state. The formula to calculate the current state
    can be written as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子重新表述前面的解释：假设我们想预测在一系列字母之后，接下来的字母是什么。假设输入的单词是 **pizza**，它由五个字母组成。当网络尝试推测第五个字母时，会发生什么呢？前四个字母已经输入到网络中。对于隐藏层来说，会进行五次迭代。如果我们展开网络，它将变成一个五层网络，每一层对应输入单词的一个字母（参考
    [第2章](a177e128-f96b-4a3a-8e3e-24f133492cb0.xhtml)，*深度学习基础*，*图2.11*）。我们可以将它看作是一个重复多次（5）的普通神经网络。展开的次数与网络能够记住多远的过去有直接关系。回到
    **pizza** 的例子，输入数据的词汇表是 *{p, i, z, a}*。隐藏层或 RNN 会对当前输入和前一个状态应用一个公式。在我们的例子中，单词
    *pizza* 中的字母 *p* 作为第一个字母，它前面没有任何字母，所以什么也不做，然后我们可以继续处理下一个字母 *i*。在字母 *i* 和前一个状态（字母
    *p*）之间，隐藏层应用公式。如果在某个时刻 *t*，输入是 *i*，那么在时刻 *t-1*，输入是 *p*。通过对 *p* 和 *i* 应用公式，我们得到一个新的状态。计算当前状态的公式可以写成如下：
- en: '*h[t] = f(h[t-1], x[t])*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*h[t] = f(h[t-1], x[t])*'
- en: 'where *h[t]* is the new state, *h[t-1]* is the previous state and *x[t]* is
    the current input. From the previous formula, we can understand that the current
    state is a function of the previous input (the input neuron has applied transformations
    on the previous input). Any successive input is used as a time step. In this *pizza*
    example we have four inputs to the network. The same function and the same weights
    are applied to the network at each time step. Considering the simplest implementation
    of an RNN, the activation function is *tanh*, a hyperbolic tangent that ranges
    from *-1* to *1*, which is one of the most common sigmoid activation function
    for MLPs. So, the formula looks as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *h[t]* 是新的状态，*h[t-1]* 是前一个状态，*x[t]* 是当前输入。从之前的公式可以理解，当前状态是前一个输入的函数（输入神经元对前一个输入进行了变换）。任何连续的输入都会作为时间步长。在这个
    *pizza* 的例子中，我们有四个输入进入网络。在每个时间步长，都会应用相同的函数和相同的权重。考虑到 RNN 的最简单实现，激活函数是 *tanh*，即双曲正切函数，其值范围在
    *-1* 到 *1* 之间，这是 MLP 中最常见的 S 型激活函数之一。因此，公式如下：
- en: '*h[t] = tanh(W[hh]h[t-1] + W[xh]x[t])*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*h[t] = tanh(W[hh]h[t-1] + W[xh]x[t])*'
- en: 'Here *W[hh]* is the weight at the recurrent neuron and *W[xh]* is the weight
    at the input neuron. That formula means that the immediate previous state is taken
    into account by a recurrent neuron. Of course, the preceding equation can involve
    multiple states in cases of longer sequence than *pizza*. Once the final state
    is calculated then the output *y[t]* can be obtained this way:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 *W[hh]* 是递归神经元的权重，*W[xh]* 是输入神经元的权重。这个公式意味着递归神经元会考虑到前一个状态。当然，前面的公式可以在更长的序列情况下涉及多个状态，而不仅仅是
    *pizza*。一旦计算出最终状态，就可以通过以下方式获得输出 *y[t]*：
- en: '*y[t] = W[hy]h[t]*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*y[t] = W[hy]h[t]*'
- en: One final note about the error. It is calculated by comparing the output to
    the actual output. Once the error has been calculated, then the learning process
    happens by backpropagating it through the network in order to update the network
    weights.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 关于误差的最后一点说明。误差通过将输出与实际输出进行比较来计算。一旦计算出误差，就通过反向传播将其传播到网络中，以更新网络的权重。
- en: Backpropagation Through Time (BPTT)
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播通过时间（BPTT）
- en: Multiple variant architectures have been proposed for RNNs (some of them have
    been listed in [Chapter 2](a177e128-f96b-4a3a-8e3e-24f133492cb0.xhtml), *Deep
    Learning Basics*, in the section *Recurrent Neural Networks*). Before entering
    into details of the LSTM implementation, a few words must be spent about the problems
    with the generic RNN architecture described previously. In general for neural
    networks, forward propagation is the technique used to get the output of a model
    and check if it is correct or not. Likewise, backward propagation is a technique
    to go backwards through a neural network to find the partial derivatives of the
    error over the weights (this makes it possible to subtract the found value from
    the weights). These derivatives are then used by the Gradient Descent Algorithm,
    which, in an iterative way, minimizes a function and then does up or down adjustments
    to the weights (the direction depends on which one decreases the error). At training
    time, backpropagation is then the way in which it is possible to adjust the weights
    of a model. BPTT is just a way to define the process of doing backpropagation
    on an unrolled RNN. With reference to [Chapter 2](a177e128-f96b-4a3a-8e3e-24f133492cb0.xhtml),
    *Deep Learning Basics*, *Figure 2.11*, in doing BPTT, it is mandatory to do the
    formulation of unrolling, this being the error of a given time step, depending
    on the previous one. In the BPTT technique, the error is backpropagated from the
    last time step to the first one, while unrolling all of them. This allows error
    calculation for each time step, making it possible to update the weights. Please
    be aware that BPTT can be computationally expensive in those cases where the number
    of time steps is high.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为RNN提出了多种变体架构（其中一些已在[第二章](a177e128-f96b-4a3a-8e3e-24f133492cb0.xhtml)，*深度学习基础*，*循环神经网络*一节中列出）。在详细介绍LSTM实现之前，需要先简要讨论一下之前描述的通用RNN架构的问题。对于神经网络，一般使用前向传播技术来获得模型的输出并检查其是否正确。同样，反向传播是一种通过神经网络向后传播，找出误差对权重的偏导数的技术（这使得可以从权重中减去找到的值）。这些偏导数随后被梯度下降算法使用，梯度下降算法以迭代的方式最小化一个函数，然后对权重进行上下调整（调整的方向取决于哪个方向能减少误差）。在训练过程中，反向传播是调整模型权重的方式。BPTT只是定义在展开的RNN上执行反向传播过程的一种方法。参考[第二章](a177e128-f96b-4a3a-8e3e-24f133492cb0.xhtml)，*深度学习基础*，*图2.11*，在执行BPTT时，必须进行展开的公式化，即某一时间步的误差依赖于前一个时间步。在BPTT技术中，误差是从最后一个时间步反向传播到第一个时间步，同时展开所有时间步。这使得可以为每个时间步计算误差，从而更新权重。请注意，在时间步数较多的情况下，BPTT可能会计算非常耗时。
- en: RNN issues
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN问题
- en: The two major issues affecting RNNs are the **Exploding Gradients** and **Vanishing
    Gradients**. We talk about Exploding Gradients when an algorithm assigns, without
    a reason, a high importance to the model weights. But, the solution to this problem
    is easy, as this would require just truncating or compressing the gradients. We
    talk about Vanishing Gradients when the values of a gradient are so small that
    they cause a model to stop or take too long to learn. This is a major problem
    if compared with the Exploding Gradients, but it has now been solved through the
    **LSTM** (Long Short-Term Memory) neural networks. LSTMs are a special kind of
    RNN, capable of learning long-term dependencies, that were introduced by Sepp
    Hochreiter ([https://en.wikipedia.org/wiki/Sepp_Hochreiter](https://en.wikipedia.org/wiki/Sepp_Hochreiter))
    & Juergen Schmidhuber ([https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber](https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber))
    in 1997.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 影响RNN的两个主要问题是**梯度爆炸**和**梯度消失**。当算法在没有理由的情况下给模型权重赋予过高的重要性时，我们称之为梯度爆炸。但解决这个问题的方法很简单，只需要截断或压缩梯度即可。我们称之为梯度消失，是指梯度的值非常小，以至于它导致模型停止学习或学习速度过慢。如果与梯度爆炸相比，这是一个主要问题，但现在已经通过**LSTM**（长短期记忆）神经网络得到了解决。LSTM是一种特殊类型的RNN，能够学习长期依赖关系，1997年由Sepp
    Hochreiter（[https://en.wikipedia.org/wiki/Sepp_Hochreiter](https://en.wikipedia.org/wiki/Sepp_Hochreiter)）和Juergen
    Schmidhuber（[https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber](https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber)）提出。
- en: 'They are explicitly designed with the default ability to remember information
    for long periods of time. This can be achieved because LSTMs retain their information
    in a memory, which is pretty much like that of a computer: a LSTM can read, write,
    and delete information from it. The LSTM''s memory can be considered as a gated
    cell: it decides whether or not to store or delete information (open gates or
    not), depending on the importance it puts on a given information. The process
    of assigning importance happens through weights: consequently a network learns
    over time which information has to be considered important and which not. An LSTM
    has three gates: the input, the forget, and the output gate. The **Input Gate**
    determines if a new input in should be let in, the **Forget Gate** deletes the
    non-important information, and the **Output Gate** influences the output of the
    network at the current time step, as shown in the following diagram:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 它们明确设计为具有默认的长期记忆能力。之所以能够实现这一点，是因为LSTM会在一个内存中保持信息，这个内存的功能类似于计算机的内存：LSTM可以从中读取、写入和删除信息。LSTM的内存可以被视为一个带门单元：它决定是否存储或删除信息（是否打开门），这取决于它对给定信息的重要性赋予了多少权重。赋予重要性的过程通过权重进行：因此，网络随着时间的推移学习哪些信息需要被认为是重要的，哪些不重要。LSTM有三个门：输入门、遗忘门和输出门。**输入门**决定是否让新输入进入，**遗忘门**删除不重要的信息，**输出门**影响网络当前时间步的输出，如下图所示：
- en: '![](img/1cebafbc-4831-480b-b561-f880b3a85ac0.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1cebafbc-4831-480b-b561-f880b3a85ac0.png)'
- en: 'Figure 6.2: The three gates of an LSTM'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：LSTM的三个门
- en: 'You can think of each of these three gates as a conventional artificial neuron,
    as in a feedforward MNN: they compute an activation (using an activation function)
    of a weighted sum. What enables the LSTM gates to do backpropagation is the fact
    that they are analog (sigmoids, they range from zero to one). This implementation
    solves the problems of Vanishing Gradients because it keeps the gradients steep
    enough, and consequently the training completes in a relatively short time, while
    maintaining an high accuracy.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将这三个门看作是传统的人工神经元，就像在前馈神经网络（MNN）中一样：它们计算一个加权和的激活（使用激活函数）。使得LSTM门能够进行反向传播的原因在于它们是模拟的（sigmoid函数，其范围从零到一）。这种实现解决了梯度消失的问题，因为它保持了足够陡峭的梯度，从而使得训练能够在相对较短的时间内完成，同时保持较高的准确性。
- en: Use cases
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用案例
- en: 'RNNs have several use cases. Here is a list of the most frequently used:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: RNN有多个使用场景。以下是最常见的几种：
- en: '**Language modelling and text generation**: This is the attempt to predict
    the likelihood of the next word, given a sequence of words. This is useful for
    language translation: the most likely sentence would be the one that is correct.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言建模与文本生成**：这是一种尝试，根据一系列单词预测下一个单词的概率。这对于语言翻译非常有用：最有可能的句子通常是正确的句子。'
- en: '**Machine translation**: This is the attempt to translate text from one language
    to another.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器翻译**：这是一种尝试将文本从一种语言翻译成另一种语言的方法。'
- en: '**Anomaly detection in time series**: It has been demonstrated that LSTM networks
    in particular are useful for learning sequences containing longer term patterns
    of unknown length, due to their ability to maintain long-term memory. For this
    reason they are useful for anomaly or fault detection in time series. Practical
    use cases are in log analysis and sensor data analysis.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列中的异常检测**：研究表明，特别是LSTM网络非常适合学习包含未知长度的长期模式的序列，因为它们能够保持长期记忆。由于这一特性，它们在时间序列中的异常或故障检测中非常有用。实际应用案例包括日志分析和传感器数据分析。'
- en: '**Speech recognition**: This is the attempt to predict phonetic segments based
    on input sound waves and then to formulate a word.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音识别**：这是一种基于输入声波预测语音片段，然后形成单词的尝试。'
- en: '**Semantic parsing**: Converting a natural language utterance to a logical
    form—a machine-understandable representation of its meaning. Practical applications
    include question answering and programming language code generation.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义解析**：将自然语言表达转换为逻辑形式——一种机器可理解的意义表示。实际应用包括问答系统和编程语言代码生成。'
- en: '**Image captioning**: This is a case that usually involves a combination of
    a CNN and an RNN. The first makes the segmentation, while the other then uses
    the data segmented by the CNN to recreate the descriptions.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像描述**：这通常涉及CNN和RNN的组合。CNN进行图像分割，RNN则利用CNN分割后的数据来重建描述。'
- en: '**Video tagging**: RNNs can be used for video search when doing frame by frame
    image captioning of a video.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视频标注**：RNN可以用于视频搜索，当进行逐帧视频图像说明时，RNN可以发挥作用。'
- en: '**Image generation**: This is the process of creating parts of a scene independently
    from others and to successively refine approximate sketches, generating at the
    end, images that cannot be distinguished from real data with the naked eye.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像生成**：这是一个将场景的各部分独立生成并逐步改进大致草图的过程，最终生成的图像在肉眼下无法与真实数据区分。'
- en: Hands-on RNNs with Spark
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark动手实践RNN
- en: Let's start now being hands-on with RNNs. This section is divided into two parts—the
    first one is about using DL4J to implement a network, while the second one will
    introduce using both DL4J and Spark for the same purpose. As with CNNs, you will
    discover that, thanks to the DL4J framework, lots of high-level facilities come
    out-of-the-box with it, so that the implementation process is easier than you
    might expect.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始动手使用RNN。本节分为两部分——第一部分是关于使用DL4J实现网络，第二部分将介绍使用DL4J和Spark实现同样目标的方法。与CNN一样，借助DL4J框架，许多高级功能都可以开箱即用，因此实现过程比你想象的要容易。
- en: RNNs with DL4J
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DL4J的RNN
- en: The first example presented in this chapter is an LSTM which, after the training,
    will recite the following characters once the first character of the learning
    string has been used as input for it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本章展示的第一个示例是一个LSTM，经过训练后，当学习字符串的第一个字符作为输入时，它将会复述接下来的字符。
- en: 'The dependencies for this example are the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的依赖项如下：
- en: Scala 2.11.8
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala 2.11.8
- en: DL4J NN 0.9.1
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL4J NN 0.9.1
- en: ND4J Native 0.9.1 and the specific classifier for the OS of the machine where
    you would run it
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ND4J Native 0.9.1以及你运行该模型的机器操作系统专用分类器
- en: ND4J jblas 0.4-rc3.6
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ND4J jblas 0.4-rc3.6
- en: 'Assuming we have a learn string that is specified through an immutable variable
    `LEARNSTRING`, let''s start creating a dedicated list of possible characters from
    it, as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个通过不可变变量`LEARNSTRING`指定的学习字符串，接下来我们开始创建一个由它生成的可能字符列表，如下所示：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s configure the network, as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始配置网络，如下所示：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You will notice that we are using the same `NeuralNetConfiguration.Builder`
    class as for the CNN example presented in the previous chapter. This same abstraction
    is used for any network you need to implement through DL4J. The optimization algorithm
    used is the Stochastic Gradient Descent ([https://en.wikipedia.org/wiki/Stochastic_gradient_descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)).
    The meaning of the other parameters will be explained in the next chapter that
    will focus on training.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，我们正在使用与上一章中CNN示例相同的`NeuralNetConfiguration.Builder`类。这个抽象类用于任何你需要通过DL4J实现的网络。使用的优化算法是随机梯度下降（[https://en.wikipedia.org/wiki/Stochastic_gradient_descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)）。其他参数的意义将在下一章中进行讲解，该章将重点介绍训练过程。
- en: 'Let''s now define the layers for this network. The model we are implementing
    is based on the LSTM RNN by Alex Graves ([https://en.wikipedia.org/wiki/Alex_Graves_(computer_scientist)](https://en.wikipedia.org/wiki/Alex_Graves_(computer_scientist))).
    After deciding their total number assigning a value to an immutable variable `HIDDEN_LAYER_CONT`,
    we can define the hidden layers of our network, as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义这个网络的各层。我们实现的模型基于Alex Graves的LSTM RNN（[https://en.wikipedia.org/wiki/Alex_Graves_(computer_scientist)](https://en.wikipedia.org/wiki/Alex_Graves_(computer_scientist))）。在决定它们的总数并将一个值分配给不可变变量`HIDDEN_LAYER_CONT`后，我们可以定义网络的隐藏层，如下所示：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The activation function is `tanh` (hyperbolic tangent).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是`tanh`（双曲正切）。
- en: 'We need then to define the `outputLayer` (choosing softmax as the activation
    function), as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要定义`outputLayer`（选择softmax作为激活函数），如下所示：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Before completing the configuration, we must specify that this model isn''t
    pre-trained and that we use backpropagation, as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成配置之前，我们必须指定该模型没有经过预训练，并且我们使用反向传播，如下所示：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The network (`MultiLayerNetwork`) can be created starting from the preceding
    configuration, as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 网络（`MultiLayerNetwork`）可以从上述配置开始创建，如下所示：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Some training data can be generated programmatically starting from the learning
    string character list, as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一些训练数据可以通过编程方式从学习字符串字符列表生成，如下所示：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The way the training for this RNN happens will be covered in the next chapter
    (and the code example will be completed there)—the focus in this section is to
    show how to configure and build an RNN network using the DL4J API.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 该 RNN 训练的过程将在下一章中介绍（代码示例将在那里完成）——本节的重点是展示如何使用 DL4J API 配置和构建 RNN 网络。
- en: RNNs with DL4J and Spark
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DL4J 和 Spark 进行 RNN 训练
- en: The example presented in this section is an LSTM that would be trained to generate
    text, one character at a time. The training is done using Spark.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中展示的示例是一个 LSTM 模型，它将被训练以一次生成一个字符的文本。训练通过 Spark 进行。
- en: 'The dependencies for this example are the following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 该示例的依赖项如下：
- en: Scala 2.11.8
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala 2.11.8
- en: DL4J NN 0.9.1
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL4J NN 0.9.1
- en: ND4J Native 0.9.1 and the specific classifier for the OS of the machine where
    you would run it
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ND4J Native 0.9.1 和适用于运行环境操作系统的特定分类器。
- en: ND4J jblas 0.4-rc3.6
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ND4J jblas 0.4-rc3.6
- en: Apache Spark Core 2.11, release 2.2.1
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark Core 2.11，版本 2.2.1
- en: DL4J Spark 2.11, release 0.9.1_spark_2
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL4J Spark 2.11，版本 0.9.1_spark_2
- en: 'We start configuring the network as usual through the `NeuralNetConfiguration.Builder`
    class, as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像往常一样通过 `NeuralNetConfiguration.Builder` 类开始配置网络，具体如下：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As for the example presented in the *RNNs with DL4J* section, the LSTM RNN implementation
    used here is that by Alex Graves. So the configuration, the hidden layers, and
    the output layer are pretty similar to those for the previous example.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 *RNNs with DL4J* 部分中展示的示例，这里使用的 LSTM RNN 实现是 Alex Graves 的版本。所以配置、隐藏层和输出层与前一个示例非常相似。
- en: 'Now this is where Spark comes into play. Let''s set up the Spark configuration
    and context, as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Spark 开始发挥作用了。让我们设置 Spark 配置和上下文，如下所示：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Assuming we got some training data and have created a `JavaRDD[DataSet]` named
    `trainingData` from them, we need to set up for data parallel training. In particular,
    we need to set up the `TrainingMaster` ([https://deeplearning4j.org/doc/org/deeplearning4j/spark/api/TrainingMaster.html](https://deeplearning4j.org/doc/org/deeplearning4j/spark/api/TrainingMaster.html)).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经获得了一些训练数据，并从中创建了一个名为 `trainingData` 的 `JavaRDD[DataSet]`，我们需要为数据并行训练进行设置。特别是，我们需要设置
    `TrainingMaster`（[https://deeplearning4j.org/doc/org/deeplearning4j/spark/api/TrainingMaster.html](https://deeplearning4j.org/doc/org/deeplearning4j/spark/api/TrainingMaster.html)）。
- en: 'It is an abstraction that controls how learning is actually executed on Spark
    and allows for multiple different training implementations to be used with `SparkDl4jMultiLayer`
    ([https://deeplearning4j.org/doc/org/deeplearning4j/spark/impl/multilayer/SparkDl4jMultiLayer.html](https://deeplearning4j.org/doc/org/deeplearning4j/spark/impl/multilayer/SparkDl4jMultiLayer.html)).
    Set up for data parallel training, as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个抽象，控制学习如何在 Spark 上执行，并允许使用多种不同的训练实现与 `SparkDl4jMultiLayer`（[https://deeplearning4j.org/doc/org/deeplearning4j/spark/impl/multilayer/SparkDl4jMultiLayer.html](https://deeplearning4j.org/doc/org/deeplearning4j/spark/impl/multilayer/SparkDl4jMultiLayer.html)）一起使用。为数据并行训练设置如下：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Currently, the DL4J framework has only one implementation of the `TrainingMaster`,
    the `ParameterAveragingTrainingMaster` ([https://deeplearning4j.org/doc/org/deeplearning4j/spark/impl/paramavg/ParameterAveragingTrainingMaster.html](https://deeplearning4j.org/doc/org/deeplearning4j/spark/impl/paramavg/ParameterAveragingTrainingMaster.html)).
    The parameters that we have set for it in the current example are:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，DL4J 框架仅实现了一个 `TrainingMaster`，即 `ParameterAveragingTrainingMaster`（[https://deeplearning4j.org/doc/org/deeplearning4j/spark/impl/paramavg/ParameterAveragingTrainingMaster.html](https://deeplearning4j.org/doc/org/deeplearning4j/spark/impl/paramavg/ParameterAveragingTrainingMaster.html)）。我们在当前示例中为其设置的参数如下：
- en: '`workerPrefetchNumBatches`: The number of Spark workers capable of prefetching
    in an asynchronous way; a number of mini-batches (Dataset objects), in order to
    avoid waiting for the data to be loaded. Setting this parameter to `0` means disabling
    this prefetching. Setting it to `2` (such as in our example) is a good compromise
    (a sensible default with a non-excessive use of memory).'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`workerPrefetchNumBatches`：能够异步预取的 Spark 工作节点数量；一组 mini-batches（数据集对象），以避免等待数据加载。将该参数设置为
    `0` 表示禁用预取。将其设置为 `2`（如我们的示例中）是一个较好的折中方案（在不过度使用内存的情况下，合理的默认值）。'
- en: '`batchSizePerWorker`: This is the number of examples used for each parameter
    update in each Spark worker.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batchSizePerWorker`：这是每个 Spark 工作节点在每次参数更新时使用的示例数量。'
- en: '`averagingFrequency`: To control how frequently the parameters are averaged
    and redistributed, in terms of a number of mini-batches of size `batchSizePerWorker`.
    Setting a low averaging period may be inefficient, because of the high network
    communication and initialization overhead, relative to computation, while setting
    a large averaging period may result in poor performance. So, a good compromise
    is to keep its value between `5` and `10`.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`averagingFrequency`：控制参数平均和重新分发的频率，以`batchSizePerWorker`大小的迷你批次数量为单位。设置较低的平均周期可能效率较低，因为网络通信和初始化开销相较于计算较高，而设置较大的平均周期可能导致性能较差。因此，良好的折衷方案是将其值保持在`5`到`10`之间。'
- en: The `SparkDl4jMultiLayer` requires as parameters the Spark context, the Spark
    configuration, and the `TrainingMaster`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkDl4jMultiLayer`需要的参数包括Spark上下文、Spark配置和`TrainingMaster`。'
- en: The training through Spark can now start. The way it happens will be covered
    in the next chapter (and this code example will be completed there)—again, the
    focus in this section is to show how to configure and build an RNN network using
    the DL4J and Spark API.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以开始通过Spark进行训练。训练过程将在下一章中详细介绍（并将在那里完成此代码示例）——本节的重点是展示如何使用DL4J和Spark API配置和构建RNN网络。
- en: Loading multiple CSVs for RNN data pipelines
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载多个CSV用于RNN数据管道
- en: Before wrapping up this chapter, here are a few notes about how we can load
    multiple CSV files, each containing one sequence, for RNN training and testing
    data. We are assuming to have a dataset made of multiple CSV files stored in a
    cluster (it could be HDFS or an object storage such as Amazon S3 or Minio), where
    each file represents a sequence, each row of one file contains the values for
    one time step only, the number of rows could be different across files, and the
    header row could be present or missing in all files.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束前，这里有一些关于如何加载多个CSV文件的注意事项，每个文件包含一个序列，用于RNN训练和测试数据。我们假设有一个由多个CSV文件组成的数据集，这些文件存储在集群中（可以是HDFS或像Amazon
    S3或Minio这样的对象存储），每个文件表示一个序列，文件中的每一行仅包含一个时间步的值，各个文件的行数可能不同，头行可能存在也可能缺失。
- en: 'With reference to CSV files saved in an S3-based object storage (refer to [Chapter
    3](44fab060-12c9-4eec-9e15-103da589a510.xhtml), *Extract, Transform, Load*, *Data
    Ingestion from S3,* for more details), the Spark context has been created as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 参考保存在S3基础对象存储中的CSV文件（更多细节请参考[第3章](44fab060-12c9-4eec-9e15-103da589a510.xhtml)，*提取、转换、加载*，*从S3加载数据*），Spark上下文已如下创建：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The Spark job configuration has been set up to access the object storage (as
    explained in [Chapter 3](44fab060-12c9-4eec-9e15-103da589a510.xhtml), *Extract,
    Transform, Load*), and we can get the data as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Spark作业配置已设置为访问对象存储（如[第3章](44fab060-12c9-4eec-9e15-103da589a510.xhtml)，*提取、转换、加载*中所述），我们可以如下获取数据：
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '(`dl4j-bucket` is the bucket containing the CSV files). Next we create a DataVec
    `CSVSequenceRecordReader` specifying if all the CSV files in the bucket have the
    header row or not (use the value `0` for no, `1` for yes) and the values separator,
    as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: （`dl4j-bucket`是包含CSV文件的存储桶）。接下来，我们创建一个DataVec `CSVSequenceRecordReader`，并指定所有CSV文件是否有头行（如果没有头行，使用值`0`；如果有头行，使用值`1`），以及值分隔符，如下所示：
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally we get the sequence by applying a `map` transformation to the original
    data in `seqRR`, as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过对`seqRR`中的原始数据应用`map`转换来获取序列，如下所示：
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It is very similar in the case of RNN training with non-sequence CSV files,
    by using the `DataVecDataSetFunction` class of `dl4j-spark` and specifying the
    index of the label column and the number of labels for classification, as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用非序列CSV文件进行RNN训练时也非常相似，使用`dl4j-spark`的`DataVecDataSetFunction`类并指定标签列的索引和分类的标签数，如下所示：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Summary
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小结
- en: In this chapter, we first went deeper into the RNNs' main concepts, before understanding
    how many practical use cases these particular NNs have, and, finally, we started
    going hands-on, implementing some RNNs using DL4J and Spark.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先深入探讨了RNN的主要概念，然后了解了这些特定神经网络在许多实际应用中的使用案例，最后，我们开始动手实践，使用DL4J和Spark实现一些RNN。
- en: The next chapter will focus on training techniques for CNN and RNN models. Training
    techniques have just been mentioned, or skipped from [Chapter 3](44fab060-12c9-4eec-9e15-103da589a510.xhtml),
    *Extract, Transform, Load*, to this chapter because the main goal so far has been
    on understanding how training data can be retrieved and prepared and how models
    can be implemented through DL4J and Spark.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将重点介绍CNN和RNN模型的训练技巧。训练技巧在[第三章](44fab060-12c9-4eec-9e15-103da589a510.xhtml)中已经提到，或者从*提取、转换、加载*跳过到本章，因为迄今为止，主要的目标是理解如何获取和准备训练数据，以及如何通过DL4J和Spark实现模型。
