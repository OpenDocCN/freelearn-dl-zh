- en: 'Chapter 8:'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第8章：
- en: Best Practices for Model Training and Performance
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练和性能的最佳实践
- en: In order for a supervised machine learning model to be well trained, it requires
    large volumes of training data. In this chapter, we are going to look at a few
    common examples and patterns for handling input data. We will specifically learn
    how to access training data regardless of its size and train the model with it.
    After that, we will look at regularization techniques that help to prevent overfitting.
    Having large volumes of training data is no guarantee of a well-trained model.
    In order to prevent overfitting, we may need to apply various regularization techniques
    in our training processes. We will take a look at a number of such techniques,
    starting with the typical Lasso (**L1**), Ridge (**L2**), and elastic net regularizations,
    before moving on to a modern regularization technique known as adversarial regularization.
    With these techniques at our disposal, we put ourselves in a good position vis-à-vis
    reducing overfitting as a result of training.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使一个监督式机器学习模型得到良好的训练，需要大量的训练数据。在本章中，我们将查看处理输入数据的一些常见示例和模式。我们将特别学习如何访问训练数据，不论其大小，并使用这些数据训练模型。之后，我们将看看有助于防止过拟合的正则化技术。拥有大量的训练数据并不能保证得到一个良好的训练模型。为了防止过拟合，我们可能需要在训练过程中应用各种正则化技术。我们将逐步探讨这些技术，从典型的Lasso（**L1**）、Ridge（**L2**）和弹性网正则化开始，然后深入到一种现代的正则化技术——对抗正则化。通过这些技术的应用，我们可以在减少训练过程中的过拟合方面处于有利地位。
- en: 'When it comes to regularization, there is no straightforward way to determine
    which method works best. It certainly depends on other factors, such as the distribution
    or sparsity of features and the volume of data. The purpose of this chapter is
    to provide various examples and give you several choices to try during your own
    model training process. In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在谈到正则化时，并没有一种简单的方法来确定哪种方法最有效。这肯定取决于其他因素，例如特征的分布或稀疏性以及数据的体量。本章的目的是提供各种示例，并为您在自己的模型训练过程中提供多种选择。在本章中，我们将涵盖以下主题：
- en: Input handling for loading data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载数据的输入处理
- en: Regularization to reduce overfitting
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化以减少过拟合
- en: Input handling for loading data
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据的输入处理
- en: Many common examples that we typically see tend to focus on the modeling aspect,
    such as how to build a deep learning model using TensorFlow with various layers
    and patterns. In these examples, the data used is almost always loaded into the
    runtime memory directly. This is fine as long as the training data is sufficiently
    small. But what if it is much larger than your runtime memory can handle? The
    solution is data streaming. We have been using this technique to feed data into
    our model in the previous chapters, and we are going to take a closer look at
    data streaming and generalize it to more data types.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常看到的许多常见示例往往侧重于建模方面，比如如何使用TensorFlow构建一个深度学习模型，并结合不同的层次和模式。在这些示例中，所使用的数据几乎总是直接加载到运行时内存中。只要训练数据足够小，这没问题。但如果数据远远超过了您的运行时内存处理能力怎么办呢？解决方案是数据流技术。我们在前几章中已经使用了这种技术将数据输入到模型中，接下来我们将更详细地探讨数据流，并将其推广到更多的数据类型。
- en: The streaming data technique is very similar to a Python generator. Data is
    ingested into the model training process in batches, meaning that all the data
    is not sent at one time. In this chapter, we are going to use an example of flower
    image data. Even though this data is not big by any means, it is a convenient
    tool for our teaching and learning purposes in this regard. It is multiclass and
    contains images of different sizes. This reflects what we usually have to deal
    with in reality, where available training images may be crowdsourced or provided
    at different scales or dimensions. In addition, an efficient data ingestion workflow
    is needed as the frontend to the model training process.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 数据流技术非常类似于Python生成器。数据是按批次输入到模型训练过程中，也就是说，所有数据并不会一次性发送。在本章中，我们将使用一个花卉图像数据的示例。尽管这些数据本身并不大，但它在教学和学习方面是一个非常方便的工具。该数据集是多类别的，并包含不同大小的图像。这反映了我们在实际应用中通常需要处理的情况，即可用的训练图像可能是众包的，或提供的尺度和尺寸各不相同。此外，模型训练过程的前端还需要一个高效的数据摄取工作流。
- en: Working with the generator
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用生成器
- en: When it comes to the generator, TensorFlow now has a very convenient `ImageDataGenerator`
    API that greatly simplifies and speeds up the code development process. From our
    experience in using pretrained models for image classification, we have seen that
    it is often necessary to standardize image dimensions (height and width as measured
    by the number of pixels) and normalize image pixel values to within a certain
    range (from [`0`, `255`] to [`0`, `1`]).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成器方面，TensorFlow 现在提供了一个非常方便的 `ImageDataGenerator` API，极大简化并加速了代码开发过程。根据我们在使用预训练模型进行图像分类的经验，我们发现通常需要对图像尺寸（以像素数衡量的高度和宽度）进行标准化，并将图像像素值归一化到特定范围内（从
    [`0`，`255`] 到 [`0`，`1`]）。
- en: 'The `ImageDataGenerator` API provides optional input parameters to make these
    tasks almost routine and reduce the work of writing your own functions to perform
    standardization and normalization. So, let''s take a look at how to use this API:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`ImageDataGenerator` API 提供了可选的输入参数，使得这些任务几乎成为常规操作，并减少了编写自定义函数来执行标准化和归一化的工作。那么，接下来我们来看看如何使用这个
    API：'
- en: 'Organize raw images. Let''s begin by setting up our image collection. For convenience,
    we are going to use the flower images directly from the `tf.keras` API:'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 组织原始图像。让我们首先设置我们的图像集合。为了方便起见，我们将直接使用 `tf.keras` API 提供的花卉图像：
- en: '[PRE0]'
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding code, we use the `tf.keras` API to download the images of five
    flower types.
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用 `tf.keras` API 下载五种花卉类型的图像。
- en: 'Next, we will set up `ImageDataGenerator` and streaming objects with `flow_from_directory`.
    In this step, several operations are defined:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 `flow_from_directory` 设置 `ImageDataGenerator` 和流对象。在这个步骤中，我们定义了几个操作：
- en: a. Image pixel intensity is scaled to a range of [`0`, `255`], along with a
    cross-validation fraction. The `ImageDataGenerator` API comes with optional input
    argument rescaling and `validation_split`. These arguments have to be in a dictionary
    format. Therefore, we can organize the rescale (normalization) factor and fraction
    for cross-validation together in `datagen_kwargs`.
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 图像像素强度被缩放到 [`0`，`255`] 范围内，并且伴随有交叉验证分数。`ImageDataGenerator` API 提供了可选的输入参数，包括重新缩放和
    `validation_split`。这些参数需要以字典格式提供。因此，我们可以将重缩放（归一化）因子和交叉验证的比例一起组织到 `datagen_kwargs`
    中。
- en: b. The image height and width are both reformatted to `224` pixels. The `flow_
    from_directory` API contains the optional `target_size`, `batch_size`, and `interpolation`
    `arguments`. These arguments are designed in a dictionary format. We may use these
    input arguments to set image size standardization, batch size, and the resampling
    interpolation algorithm in `dataflow_kwargs`.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 图像的高度和宽度都被重新格式化为 `224` 像素。`flow_from_directory` API 包含可选的 `target_size`、`batch_size`
    和 `interpolation` 参数。这些参数以字典格式设计。我们可以使用这些输入参数来设置图像尺寸标准化、批量大小和重采样插值算法在 `dataflow_kwargs`
    中。
- en: 'c. The preceding settings are passed to the generator instance. We then pass
    these into `ImageDataGenerator` and `flow_from_directory`:'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. 前述设置传递给生成器实例。然后，我们将这些设置传递给 `ImageDataGenerator` 和 `flow_from_directory`：
- en: '[PRE1]'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding code demonstrates a typical workflow for creating image generators
    as a means of ingesting training data in to a model. Two dictionaries are defined
    and hold the arguments we need. Then, the `ImageDataGenerator` API is invoked,
    followed by the `flow_from_directory` API. The process is repeated for training
    data as well. For the results, we have set up an ingestion workflow for training
    and cross-validation data through `train_generator` and `valid_generator`.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码展示了创建图像生成器的典型工作流，将训练数据传递到模型中。定义了两个字典并存储我们需要的参数。接着调用了 `ImageDataGenerator`
    API，随后调用 `flow_from_directory` API。对于训练数据，流程也会重复进行。最后，我们通过 `train_generator` 和
    `valid_generator` 设置了一个用于训练和交叉验证数据的摄取工作流。
- en: 'Retrieve mapping for labels. Since we use `ImageDataGenerator` to create a
    data pipeline for training, we may use it to retrieve image labels as well:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索标签映射。由于我们使用 `ImageDataGenerator` 创建训练数据管道，我们也可以使用它来检索图像标签：
- en: '[PRE2]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the preceding code, `idx_labels` is a dictionary that maps the classification
    model output, which is an index, to the `flower` class. This is `idx_labels`:'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，`idx_labels` 是一个字典，将分类模型的输出（即索引）映射到 `flower` 类。以下是 `idx_labels`：
- en: '[PRE3]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Since this is a multiclass classification problem, our model prediction will
    be an array of five probabilities. Therefore, we want the position of the class
    with the highest probability, and then we will map the position to the name of
    the corresponding class using `idx_labels`.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于这是一个多类分类问题，我们的模型预测将是一个包含五个概率的数组。因此，我们需要获取概率最高类别的位置，然后使用`idx_labels`将该位置映射到相应类别的名称。
- en: 'Build and train the model. This step is the same as we performed in the previous
    chapter, [*Chapter 7*](B16070_07_Final_JM_ePub.xhtml#_idTextAnchor200), *Model
    Optimization*, where we will build a model by means of transfer learning. The
    model of choice is a ResNet feature vector, and the final classification layer
    is a dense layer with five nodes (`NUM_CLASSES` is defined to be `5`, as indicated
    in *step 2*), and these five nodes output probabilities for each of the five classes:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并训练模型。这个步骤与我们在前一章[*第7章*](B16070_07_Final_JM_ePub.xhtml#_idTextAnchor200)中执行的相同，*模型优化*，在其中我们将通过迁移学习构建一个模型。选择的模型是ResNet特征向量，最终的分类层是一个包含五个节点的密集层（`NUM_CLASSES`定义为`5`，如*步骤2*所示），这五个节点输出每个类别的概率：
- en: '[PRE4]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The preceding code shows the general flow of setting up a model's architecture
    through training. We started by building an `mdl` model using the `tf.keras` sequential
    API. Once the `loss` function and optimizer were designated, we compiled the model.
    Since we want to include cross-validation as part of the training routine, we
    need to set up `step_per_epoch`, which is the total number of data batches for
    the generator to yield as one epoch. This process is repeated for cross-validation
    data. Then we call the Fit API to launch the training process for five epochs.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码展示了通过训练设置模型架构的基本流程。我们首先使用`tf.keras`的顺序API构建了一个`mdl`模型。一旦指定了`loss`函数和优化器，就编译了模型。由于我们希望将交叉验证作为训练流程的一部分，我们需要设置`step_per_epoch`，即生成器为每个epoch生成的总数据批次数。这个过程会为交叉验证数据重复执行。然后，我们调用Fit
    API启动五个epoch的训练过程。
- en: The preceding steps demonstrate how to start with `ImageDataGenerator` to build
    a pipeline that flows image data from the image directory via `flow_from_directory`,
    and we are also able to handle image normalization and standardization routines
    as input arguments.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 上述步骤展示了如何从`ImageDataGenerator`开始，构建一个将图像数据从图像目录通过`flow_from_directory`流入的管道，我们还能够处理图像标准化和归一化过程，作为输入参数。
- en: TFRecord dataset – ingestion pipeline
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFRecord数据集 – 数据摄取管道
- en: Another means of streaming training data into the model during the training
    process is through the TFRecord dataset. TFRecord is a protocol buffer format.
    Data stored in this format may be used in **Python**, **Java**, and **C++**. In
    enterprise or production systems, this format may provide versatility and promote
    reusability of data across different applications. Another caveat for TFRecord
    is that if you wish to use TPU as your compute target, and you wish to use a pipeline
    to ingest training data, then TFRecord is the means to achieve it. Currently,
    TPU does not work with generators. Therefore, the only way to stream data through
    a pipeline approach is by means of TFRecord. Again, the size of this dataset does
    not require TFRecord in reality. This is only used for learning purposes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中，另一种将训练数据流入模型的方式是通过TFRecord数据集。TFRecord是一种协议缓冲格式。以这种格式存储的数据可以在**Python**、**Java**和**C++**中使用。在企业或生产系统中，这种格式可能提供灵活性，并促进数据在不同应用程序中的重用。TFRecord的另一个注意事项是，如果你希望使用TPU作为计算目标，并希望使用管道摄取训练数据，那么TFRecord就是实现这一目标的方式。目前，TPU不支持生成器。因此，通过管道流式传输数据的唯一方法是使用TFRecord。再次强调，实际上该数据集的大小并不需要TFRecord，这仅用于学习目的。
- en: 'We are going to start with a TFRecord dataset already prepared. It contains
    the same flower images and classes as seen in the previous section. In addition,
    this TFRecord dataset is partitioned into training, validation, and test datasets.
    This TFRecord dataset is available in this book''s GitHub repository. You may
    clone this repository with the following command:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从已经准备好的TFRecord数据集开始。它包含了与前一节中看到的相同的花卉图像和类别。此外，这个TFRecord数据集已被划分为训练、验证和测试数据集。该TFRecord数据集可以在本书的GitHub仓库中找到。你可以使用以下命令克隆该仓库：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once this command is complete, get in the following path:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦该命令完成，请进入以下路径：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You will see the following TFRecord datasets:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到以下TFRecord数据集：
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Make a note of the file path where these datasets are stored.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 记下这些数据集存储的文件路径。
- en: 'We will refer to this path as `<PATH_TO_TFRECORD>`. This could be the path
    in your local system or any cloud notebook environment where you uploaded and
    mounted these TFRecord files:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这个路径称为`<PATH_TO_TFRECORD>`。这可以是你本地系统中的路径，或者是你上传并挂载这些TFRecord文件的任何云端笔记本环境中的路径：
- en: Set up the file path. As you can see, in this TFRecord collection, there are
    multiple parts (two) of `train.tfrecord`. We will use the wildcard (`*`) symbol
    to denote multiple filenames that follow the same naming pattern. We may use `glob`
    to keep track of the pattern, pass it to `list_files` to create a list of files,
    and then let `TFRecordDataset` create a dataset object.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置文件路径。如你所见，在这个TFRecord集合中，有多个部分（两个）`train.tfrecord`。我们将使用通配符（`*`）符号来表示遵循相同命名模式的多个文件名。我们可以使用`glob`来跟踪模式，将其传递给`list_files`以创建文件列表，然后让`TFRecordDataset`创建数据集对象。
- en: 'Recognize and encode the filename convention. We want to have a pipeline that
    can handle the data ingestion process. Therefore, we have to create variables
    to hold the file path and naming convention:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别并编码文件名约定。我们希望拥有一个能够处理数据导入过程的管道。因此，我们必须创建变量来保存文件路径和命名约定：
- en: '[PRE11]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, we encoded text string representations of the file path to training, validation,
    and test data in the `train_file_pattern`, `val_file_pattern`, and `test_file_pattern`
    variables. Notice that we used the wildcard operator `*` to handle multiple file
    parts, if any. This is an important way to achieve scalability in data ingestion
    pipelines. It doesn't matter how many files there are, because now you have a
    way to find all of them by means of the path pattern.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们将训练、验证和测试数据的文件路径以文本字符串的形式编码到`train_file_pattern`、`val_file_pattern`和`test_file_pattern`变量中。注意，我们使用了通配符操作符`*`来处理多个文件部分（如果有的话）。这是实现数据导入管道可扩展性的一个重要方法。无论有多少个文件，因为现在你有了一种通过路径模式找到所有文件的方法。
- en: 'Create a file list. To create an object that can handle multiple parts of TFRecord
    files, we will use `list_files` to keep track of these files:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建文件列表。为了创建一个能够处理多个部分TFRecord文件的对象，我们将使用`list_files`来跟踪这些文件：
- en: '[PRE12]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding code, we use the `tf.io` API to make a reference to the training,
    validation, and test files. The path to these files is defined by `train_file_pattern`,
    `val_file_pattern`, and `test_file_pattern`.
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用`tf.io` API引用了训练、验证和测试文件。这些文件的路径由`train_file_pattern`、`val_file_pattern`和`test_file_pattern`定义。
- en: 'Create a dataset object. We will use `TFRecordDataset` to create dataset objects
    from training, validation, and test list objects:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建数据集对象。我们将使用`TFRecordDataset`从训练、验证和测试列表对象创建数据集对象：
- en: '[PRE13]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `TFRecordDataset` API reads the `TFRecord` file referenced by the file path
    variables.
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`TFRecordDataset` API读取由文件路径变量引用的`TFRecord`文件。'
- en: 'Inspect the sample size. So far, there is no quick way to establish the sample
    size in each TFRecord. The only way to do so is by iterating it:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查样本大小。到目前为止，尚无快速方法来确定每个TFRecord中的样本大小。唯一的方法是通过迭代它：
- en: '[PRE14]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The preceding code prints and verifies the sample sizes in each of our TFRecord
    datasets.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上面的代码打印并验证了我们每个TFRecord数据集中的样本大小。
- en: 'The output should be as follows:'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '[PRE15]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Since we are able to count samples in TFRecord datasets, we know that our data
    pipeline for TFRecord is set up correctly.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于我们能够统计TFRecord数据集中的样本数，我们知道我们的TFRecord数据管道已经正确设置。
- en: TFRecord dataset – feature engineering and training
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFRecord数据集 - 特征工程与训练
- en: 'When we used a generator as the ingestion pipeline, the generator took care
    of batching and matching data and labels during the training process. However,
    unlike the generator, in order to use the TFRecord dataset, we have to parse it
    and perform some necessary feature engineering tasks, such as normalization and
    standardization, ourselves. The creator of TFRecord has to provide a feature description
    dictionary as a **template** for parsing the samples. In this case, the following
    feature dictionary is provided:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用生成器作为数据导入管道时，生成器在训练过程中会负责批处理和数据与标签的匹配。然而，与生成器不同，为了使用TFRecord数据集，我们必须自己解析它并执行一些必要的特征工程任务，例如归一化和标准化。TFRecord的创建者必须提供一个特征描述字典作为**模板**来解析样本。在这种情况下，提供了以下特征字典：
- en: '[PRE16]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We will go through the following steps to parse the dataset, perform feature
    engineering tasks, and submit the dataset for training. These steps follow the
    completion of the *TFRecord dataset – ingestion pipeline* section:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过以下步骤来解析数据集，执行特征工程任务，并提交数据集进行训练。这些步骤是在完成*TFRecord数据集 - 导入管道*部分后进行的：
- en: 'Parse TFRecord and resize the images. We will use the preceding dictionary
    to parse TFRecord in order to extract a single image as a NumPy array and its
    corresponding label. We will define a `decode_and_resize` function that should
    be used:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析TFRecord并调整图像大小。我们将使用前面的字典来解析TFRecord，以提取单张图像作为NumPy数组及其对应的标签。我们将定义一个`decode_and_resize`函数来执行此操作：
- en: '[PRE27]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `decode_and_resize` function takes a dataset in TFRecord format, parses
    it, extracts the metadata and actual image, and then returns the image and its
    label.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`decode_and_resize`函数接受TFRecord格式的数据集，解析它，提取元数据和实际图像，然后返回图像及其标签。'
- en: At a more detailed level inside this function, the TFRecord dataset is parsed
    with `parsed_feature`. This is how we extract different metadata from the dataset.
    The image is decoded by the `decode_jpeg` API, and is resized to 224 x 224 pixels.
    As for the label, it is extracted and one-hot encoded. Finally, the function returns
    the resized image and the corresponding one-hot label.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在该函数的更详细层面内，TFRecord数据集使用`parsed_feature`进行解析。这是我们从数据集中提取不同元数据的方法。图像通过`decode_jpeg`
    API进行解码，并调整为224 x 224像素。至于标签，它被提取并进行独热编码。最后，函数返回调整后的图像和相应的独热标签。
- en: 'Normalize the pixel value. We also need to normalize pixel values within the
    range [`0`, `255`]. Here, we define a `normalize` function to do this:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标准化像素值。我们还需要将像素值标准化到[`0`, `255`]范围内。这里，我们定义了一个`normalize`函数来实现这一目标：
- en: '[PRE28]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Here, the image is rescaled, pixel-wise, to a range of [`0`, `1.0`] by dividing
    each pixel by `255`. The results are cast to `float32` to represent floating-point
    values. This function returns the rescaled image with its label.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，图像按像素进行重新缩放，范围为[`0`, `1.0`]，方法是将每个像素值除以`255`。结果被转换为`float32`，以表示浮动点值。该函数返回重新缩放后的图像及其标签。
- en: 'Execute these functions. These functions (`decode_and_resize` and `normalize`)
    are designed to be applied to each sample within TFRecord. We use a map to accomplish
    this:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行这些函数。这些函数（`decode_and_resize`和`normalize`）被设计为应用于TFRecord中的每个样本。我们使用`map`来完成这一任务：
- en: '[PRE29]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Here, we apply `decode_and_resize` to all the datasets, and then normalize the
    dataset at a pixel-wise level.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们对所有数据集应用`decode_and_resize`，然后在像素级别对数据集进行标准化。
- en: 'Batch datasets for training processes. The final step to be performed on the
    TFRecord dataset is batching. We will define a few variables for this purpose,
    and define a function, `prepare_for_model`, for batching:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批量处理用于训练过程的数据集。对TFRecord数据集执行的最后一步是批量处理。我们将为此定义一些变量，并定义一个函数`prepare_for_model`来进行批量处理：
- en: '[PRE30]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Cross-validation and test data are not separated into batches. Therefore, the
    entire cross-validation data is a single batch, and likewise for test data.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 交叉验证和测试数据没有被分成批次。因此，整个交叉验证数据集是一个单独的批次，测试数据集也是如此。
- en: The `prepare_for_model` function takes a dataset and then caches it in memory
    and prefetches it. If this function is applied to the training data, it also repeats
    it infinitely to make sure you don't run out of data during the training process.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`prepare_for_model`函数接受一个数据集，然后将其缓存在内存中并进行预取。如果此函数应用于训练数据，它还会无限次重复数据，以确保在训练过程中不会用完数据。'
- en: 'Execute batching. Use the `map` function to apply the `batching` function:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行批量处理。使用`map`函数来应用`batching`函数：
- en: '[PRE31]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The preceding code sets up batches of training, validation, and test data. These
    are ready to be fed into the training routine. We have now completed the data
    ingestion pipeline.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的代码设置了训练、验证和测试数据的批次。这些数据已经准备好可以输入到训练过程中。我们现在已经完成了数据输入管道。
- en: 'Build and train the model. This part does not vary from the previous section.
    We will build and train a model with the same architecture as seen in the generator:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建和训练模型。这部分与前面的部分没有区别。我们将构建并训练一个与生成器中看到的架构相同的模型：
- en: '[PRE32]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Notice that the training and validation datasets are passed into the model as
    `prepped_train_ds` and `prepped_val_ds`, respectively. In this regard, it is no
    different to how we passed generators into the model for training. However, the
    extra work we had to do in terms of parsing, standardizing, and normalizing these
    datasets is substantially more complex compared to generators.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，训练数据集和验证数据集分别作为`prepped_train_ds`和`prepped_val_ds`传递给模型。在这方面，它与我们在训练时将生成器传递给模型的方式没有区别。然而，相较于生成器，我们在解析、标准化和规范化这些数据集方面所做的额外工作要复杂得多。
- en: The benefit of TFRecord is that if you have a large dataset, then breaking it
    up and storing it as TFRecord in multiple parts will help you stream the data
    into the model faster than using a generator. Also, if your compute target is
    TPU, then you cannot stream training data using a generator; you will have to
    use the TFRecord dataset to stream training data into the model for training.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: TFRecord 的好处在于，如果您有大型数据集，那么将其分成多个部分并将其存储为 TFRecord，将比使用生成器更快地将数据流入模型。此外，如果您的计算目标是
    TPU，那么您不能使用生成器来流式传输训练数据；您将必须使用 TFRecord 数据集来流式传输训练数据到模型进行训练。
- en: Regularization
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: During the training process, the model is learning to find the best set of weights
    and biases that minimize the `loss` function. As the model architecture becomes
    more complex, or simply starts to take on more layers, the model is being fitted
    with more parameters. Although this may help to produce a better fit during training,
    having to use more parameters may also lead to overfitting.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，模型正在学习找到最佳的权重和偏置集合，以最小化 `loss` 函数。随着模型架构变得更复杂或者简单地开始增加更多层次，模型正在装配更多的参数。尽管这可能有助于在训练期间产生更好的拟合，但使用更多的参数也可能导致过拟合。
- en: In this section, we will dive into some regularization techniques that can be
    implemented in a straightforward fashion in the `tf.keras` API.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨一些可以在 `tf.keras` API 中直接实现的正则化技术。
- en: L1 and L2 regularization
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L1 和 L2 正则化
- en: 'Traditional methods to address the concern of overfitting involve introducing
    a penalty term in the `loss` function. This is known as regularization. The penalty
    term is directly related to model complexity, which is largely determined by the
    number of non-zero weights. To be more specific, there are three traditional types
    of regularization used in machine learning:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 传统方法解决过拟合问题涉及在 `loss` 函数中引入惩罚项。这称为正则化。惩罚项直接与模型复杂性相关，主要由非零权重的数量决定。更具体地说，机器学习中通常使用三种传统的正则化类型：
- en: '`loss` function with L1 regularization:![](img/Formula_08_001.jpg)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` 函数与 L1 正则化：![](img/Formula_08_001.jpg)'
- en: It uses the sum of the absolute values of the weights, `w`, multiplied by a
    user-defined penalty value, `λ`, to measure complexity (that is, the number of
    parameters that are fitted to the model indicate how complex it is). The idea
    is that the more parameters, or weights, that are used, the higher the penalty
    applied. We want the best model with the fewest parameters.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用权重 `w` 的绝对值之和，乘以用户定义的惩罚值 `λ`，来衡量复杂性（即模型拟合的参数数量表明其复杂性）。其思想是，使用的参数或权重越多，施加的惩罚就越高。我们希望用尽可能少的参数得到最佳模型。
- en: '`loss` function with L2 regularization:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` 函数与 L2 正则化：'
- en: '![](img/Formula_08_002.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_002.jpg)'
- en: It uses the sum of the squares of the weights, `w`, multiplied by a user-defined
    penalty value, `λ`, to measure complexity.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用权重 `w` 的平方和，乘以用户定义的惩罚值 `λ`，来衡量复杂性。
- en: '`loss` function with L1 and L2 regularization:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` 函数与 L1 和 L2 正则化：'
- en: '![](img/Formula_08_003.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_003.jpg)'
- en: It uses a combination of L1 and L2 to measure complexity. Each regularization
    term has its own penalty factor.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用 L1 和 L2 的组合来衡量复杂性。每个正则化项都有自己的惩罚因子。
- en: '(Reference: *pp. 38-39*, *Antonio Gulli and Sujit Pal*, *Deep Learning with
    Keras*, *Packt 2017*, [https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/))'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: （参考：*pp. 38-39*，*Antonio Gulli 和 Sujit Pal*，*Deep Learning with Keras*，*Packt
    2017*，[https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/)）
- en: 'These are the keyword input parameters available for model layer definition,
    including dense or convolutional layers, such as Conv1D, Conv2D, and Conv3D:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是模型层定义的关键字输入参数，包括密集或卷积层，例如 Conv1D、Conv2D 和 Conv3D：
- en: '`kernel_regularizer`: A regularizer applied to the weight matrix'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernel_regularizer`：应用于权重矩阵的正则化器'
- en: '`bias_regularizer`: A regularizer applied to the bias vector'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bias_regularizer`：应用于偏置向量的正则化器'
- en: '`activity_regularizer`: A regularizer applied to the output of the layer'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activity_regularizer`：应用于层输出的正则化器'
- en: '(Reference: *p. 63*, *Antonio Gulli and Sujit Pal*, *Deep Learning with Keras*,
    *Packt 2017*, [https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/Regularizer](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/Regularizer))'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: （参考：*p. 63*，*Antonio Gulli 和 Sujit Pal*，*Deep Learning with Keras*，*Packt 2017*，[https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/Regularizer](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/Regularizer)）
- en: 'Now we will take a look at how to implement some of these parameters. As an
    example, we will leverage the model architecture built in the previous section,
    namely, a ResNet feature vector layer followed by a dense layer as the classification
    head:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看看如何实现其中一些参数。例如，我们将利用前一节中构建的模型架构，即ResNet特征向量层，后跟密集层作为分类头部：
- en: '[PRE33]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Notice that we are using an alias to define regularizers of interest to us
    outside the layer. This will make it easy to adjust the hyperparameters (`l1`,
    `l2`) that determine how strongly we want the regularization term to penalize
    the `loss` function for potential overfit:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用别名来定义我们感兴趣的正则化器。这将使我们能够调整决定正则化项如何惩罚潜在过度拟合的超参数（`l1`，`l2`）：
- en: '[PRE46]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This is followed by the addition of these `regularizer` definitions in the
    dense layer definition:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在密集层定义中添加这些`regularizer`定义：
- en: '[PRE48]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: These are the only changes that are required to the code used in the previous
    section.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在前一节使用的代码中所需的唯一更改。
- en: Adversarial regularization
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗正则化
- en: An interesting technique known as adversarial learning emerged in 2014 (if interested,
    read the seminal paper published by *Goodfellow et al*., *2014*). This idea stems
    from the fact that a machine learning model's accuracy can be greatly compromised,
    and will produce incorrect predictions, if the inputs are slightly noisier than
    expected. Such noise is known as adversarial perturbation. Therefore, if the training
    dataset is augmented with some random variation in the data, then we can use this
    technique to make our model more robust.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的技术被称为对抗学习于2014年出现（如果感兴趣，请阅读*Goodfellow et al*., *2014*发表的开创性论文）。这个想法源于这样一个事实，即如果输入比预期稍微嘈杂，机器学习模型的准确性可能会大大降低，从而产生错误预测。这种噪声称为对抗扰动。因此，如果训练数据集增加了一些数据的随机变化，我们可以利用这种技术使我们的模型更加健壮。
- en: 'TensorFlow''s `AdversarialRegularization` API is designed to complement the
    `tf.keras` API and simplify model building and training processes. We are going
    to reuse the TFRecord dataset downloaded as the original training data. Then we
    will apply a data augmentation technique to this dataset, and finally we will
    train the model. To do so follow the given steps:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的`AdversarialRegularization` API旨在补充`tf.keras` API并简化模型构建和训练过程。我们将重新使用下载的TFRecord数据集作为原始训练数据。然后我们将对该数据集应用数据增强技术，最后我们将训练模型。为此，请按照以下步骤操作：
- en: 'Download and unzip the training data (if you didn''t do so at the start of
    this chapter). You need to download flower_tfrecords.zip, the TFRecord dataset
    that we will use from Harvard Dataverse ([https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/1ECTVN](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/1ECTVN)).
    Put it in the compute node you intend to use. It may be your local compute environment
    or a cloud-based environment such as JupyterLab in Google AI Platform, or Google
    Colab. Unzip the file once you have downloaded it, and make a note of its path.
    We will refer to this path as `<PATH_TO_TFRECORD>`. In this path, you will see
    these TFRecord datasets:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并解压训练数据（如果您在本章开头没有这样做）。您需要下载flower_tfrecords.zip，即我们将从Harvard Dataverse使用的TFRecord数据集（[https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/1ECTVN](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/1ECTVN)）。将其放置在您打算使用的计算节点上。它可以是您的本地计算环境或云计算环境，如Google
    AI平台的JupyterLab或Google Colab。下载后解压文件，并记下其路径。我们将称此路径为`<PATH_TO_TFRECORD>`。在此路径中，您将看到这些TFRecord数据集：
- en: '[PRE54]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Install the library. We need to make sure that the neural structured learning
    module is available in our environment. If you haven''t done so yet, you should
    install this module using the following `pip` command:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装库。我们需要确保神经结构化学习模块在我们的环境中可用。如果您还没有这样做，应使用以下`pip`命令安装此模块：
- en: '[PRE55]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Create a file pattern object for the data pipeline. There are multiple files
    (two). Therefore, we may leverage the file naming convention and wildcard `*`
    qualifier during the data ingestion process:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为数据管道创建文件模式对象。有多个文件（两个）。因此，在数据摄取过程中，我们可以利用文件命名约定和通配符`*`限定符：
- en: '[PRE56]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'For convenience, the path to these TFRecord files is designated as the following
    variables: `train_file_pattern`, `val_file_pattern`, and `test_file_pattern`.
    These paths are represented as text strings. The wildcard symbol `*` is used to
    handle multiple file parts, in case there are any.'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了方便起见，TFRecord文件的路径被指定为以下变量：`train_file_pattern`、`val_file_pattern`和`test_file_pattern`。这些路径以文本字符串的形式表示。通配符`*`用于处理多个文件部分，以防有多个文件。
- en: 'Take an inventory of all the filenames. We may use the `glob` API to create
    a dataset object that tracks all parts of the file:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清点所有文件名。我们可以使用`glob` API创建一个数据集对象，用于追踪文件的所有部分：
- en: '[PRE57]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Here, we use the `tf.io` API to refer to the file paths indicated in the previous
    step. The filenames referred to by the `glob` API of `tf.io` are then encoded
    in a list of filenames by the `list_files` API of `tf.data`.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用`tf.io` API引用前一步中指示的文件路径。`tf.io`的`glob` API所引用的文件名将通过`tf.data`的`list_files`
    API编码成文件名列表。
- en: 'Establish the loading pipeline. Now we may establish the reference to our data
    source via `TFRecordDataset`:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立加载管道。现在我们可以通过`TFRecordDataset`建立与数据源的引用：
- en: '[PRE58]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Here, we use the `TFRecordDataset` API to create respective datasets from our
    source.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用`TFRecordDataset` API从源数据创建各自的数据集。
- en: 'To check whether we have total visibility of the data, we will count the sample
    sizes in each dataset:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了检查我们是否完全看到了数据，我们将统计每个数据集中的样本大小：
- en: '[PRE59]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Currently, the way to find out how many samples are in a TFRecord file is by
    iterating through it. In the code:'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目前，了解TFRecord文件中有多少样本的方法是通过遍历文件。在代码中：
- en: '[PRE60]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: We use the `for` loop to iterate through the dataset, and sum up the iteration
    count to obtain the final count as the sample size. This coding pattern is also
    used to determine validation and test dataset sample sizes. The sizes of these
    datasets are then stored as variables.
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用`for`循环遍历数据集，并通过累加迭代次数来获得最终的样本大小。此编码模式也用于确定验证集和测试集的样本大小。然后，这些数据集的大小将存储为变量。
- en: 'The output of the preceding code will look like this:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码的输出将如下所示：
- en: '[PRE61]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'As regards data transformation, we need to transform all images to the same
    size, which is `224` pixels in height and `224` pixels in width. The intensity
    level of each pixel should be in the range [`0`, `1`]. Therefore, we need to divide
    each pixel''s value by `255`. We need these two functions for these transformation
    operations:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于数据转换，我们需要将所有图像转换为相同的大小，即高度为`224`像素，宽度为`224`像素。每个像素的强度值应在[`0`，`1`]的范围内。因此，我们需要将每个像素的值除以`255`。我们需要以下两个函数来进行这些转换操作：
- en: '[PRE62]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The `decode_and_resize` function takes a dataset in TFRecord format, parses
    it, extracts the metadata and actual image, and returns the image and its label.
    At a more detailed level, inside this function, the TFRecord dataset is parsed
    with `parsed_feature`. This is how we extract different metadata from the dataset.
    The image is decoded by the `decode_jpeg` API, and it is resized to `224` by `224`
    pixels. As for the label, it is extracted and one-hot encoded.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`decode_and_resize`函数接受TFRecord格式的数据集，解析数据，提取元数据和实际图像，并返回图像及其标签。更具体地，在该函数内部，TFRecord数据集通过`parsed_feature`进行解析。通过这种方式，我们提取数据集中的不同元数据。图像通过`decode_jpeg`
    API解码，并调整大小为`224`×`224`像素。至于标签，它会被提取并进行独热编码。'
- en: Finally, the function returns the resized image and the corresponding one-hot
    label.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，函数返回调整大小后的图像及其对应的独热标签。
- en: '[PRE63]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: This function takes a JPEG image and normalizes pixel values (dividing each
    pixel by `255`) in the range of [`0`, `1.0`], and casts it to `tf.float32` to
    represent floating-point values. It returns the normalized image with its corresponding
    label.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个函数接受一张JPEG图像，并将像素值（将每个像素值除以`255`）归一化到[`0`，`1.0`]范围内，并将其转换为`tf.float32`表示浮动值。它返回归一化后的图像及其对应的标签。
- en: 'Execute data transformation. We will use the `map` function to apply the preceding
    transformation routines to each element in our dataset:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行数据转换。我们将使用`map`函数将之前的转换操作应用到数据集中的每个元素：
- en: '[PRE64]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: In the preceding code, we apply `decode_and_resize` to each dataset, and then
    we rescale it by applying the `normalize` function to each pixel in the dataset.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们将`decode_and_resize`应用于每个数据集，然后通过对每个像素应用`normalize`函数来重新缩放数据集。
- en: 'Define the parameters for training. We need to specify the batch size for our
    dataset, as well as the parameters that define the epochs:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练参数。我们需要指定数据集的批量大小，以及定义周期的参数：
- en: '[PRE65]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: In the preceding code, we defined the parameters required to set up the training
    process. Datasets are also batched and fetched for consumption.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们定义了设置训练过程所需的参数。数据集也会被批量化并提取出来以供使用。
- en: Now we have built our dataset pipeline that will fetch a batch of data at a
    time to ingest into the model training process.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们已经构建了数据集管道，每次都会获取一批数据并输入到模型训练过程中。
- en: 'Build your model. We will build an image classification model using the ResNet
    feature vector:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建你的模型。我们将使用ResNet特征向量来构建一个图像分类模型：
- en: '[PRE66]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: We use the `tf.keras` sequential API to build an image classification model.
    It first uses the input layer to accept the training data as `224` by `224` by
    `3` pixels. Then we leverage the feature vector of `ResNet_V2_50` as the middle
    layer. We will use it as is (`trainable` = `FINE_TUNING_CHOICE`. `FINE_TUNING_CHOICE`
    is set to `False` in the previous step. If you wish, you may set it to `True`.
    However, this would increase your training time significantly). Finally, the output
    layer is represented by a dense layer with five nodes (`NUM_CLASSES = 5`). Each
    node represents a probability value for the respective flower type.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用`tf.keras`的顺序API来构建一个图像分类模型。首先使用输入层接受训练数据，尺寸为`224`乘`224`乘`3`像素。接着我们利用`ResNet_V2_50`的特征向量作为中间层。我们将其保持原样（`trainable`
    = `FINE_TUNING_CHOICE`。`FINE_TUNING_CHOICE`在前一步已设为`False`，如果你愿意，也可以将其设置为`True`，但这样会显著增加训练时间）。最后，输出层由一个具有五个节点的全连接层表示（`NUM_CLASSES
    = 5`）。每个节点表示各类花卉类型的概率值。
- en: So far, there is nothing specific to adversarial regularization. Starting with
    the next step, we will begin by building a configuration object that specifies
    adversarial training data and launch the training process.
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 到目前为止，还没有涉及到对抗正则化。从下一步开始，我们将通过构建一个配置对象来指定对抗训练数据并启动训练过程。
- en: 'Convert the training samples to a dictionary. A particular requirement for
    adversarial regularization is to have training data and labels combined as a dictionary
    and then streamed into the training process. This can easily be accomplished with
    the following function:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练样本转换为字典。对抗正则化的一个特殊要求是将训练数据和标签合并为一个字典，并将其流式传输到训练过程中。这可以通过以下函数轻松实现：
- en: '[PRE67]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: This function accepts the image and corresponding label, and then reformats
    these as key-value pairs in a dictionary.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数接受图像和对应的标签，然后将其重新格式化为字典中的键值对。
- en: 'Convert the data and label collection into a dictionary. For the batched dataset,
    we may use the `map` function again to apply `examples_to_dict` to each element
    in the dataset:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据和标签集合转换为字典。对于批量数据集，我们可以再次使用`map`函数，将`examples_to_dict`应用于数据集中的每个元素：
- en: '[PRE68]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: In this code, each sample in the dataset is also converted to a dictionary.
    This is done via the `map` function. The `map` function applies the `examples_to_dict`
    function to each element (sample) in the dataset.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这段代码中，数据集中的每个样本也会被转换为字典。这是通过`map`函数实现的。`map`函数将`examples_to_dict`函数应用到数据集中的每个元素（样本）上。
- en: 'Create an adversarial regularization object. Now we are ready to create an
    `adv_config` object that specifies adversarial configuration. Then we wrap the
    `mdl` base model we created in a previous step with `adv_config`:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建对抗正则化对象。现在我们准备创建一个指定对抗配置的`adv_config`对象。然后我们将前一步创建的`mdl`基础模型与`adv_config`进行包装：
- en: '[PRE69]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Now we have a model, `adv_mdl`, that contains the base model structure as defined
    by `mdl`. `adv_mdl` includes knowledge of the adversarial configuration, `adv_config`,
    which will be used to create adversarial images during the training process.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们有了一个模型，`adv_mdl`，它包含了由`mdl`定义的基础模型结构。`adv_mdl`包括了对抗配置`adv_config`的知识，该配置将在训练过程中用于生成对抗图像。
- en: 'Compile and train the model. This part is similar to what we did previously.
    It is no different to training the base model, except for the input dataset:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译并训练模型。这部分与之前我们做的类似，训练基础模型时也是如此，唯一的不同在于输入数据集：
- en: '[PRE70]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Notice now the input to the `fit` function for training is `train_set_for_adv_model`
    and `val_set_for_adv_model`, which is a dataset that streams each sample as a
    dictionary into the training process.
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，现在传递给`fit`函数的训练输入是`train_set_for_adv_model`和`val_set_for_adv_model`，这两个数据集会将每个样本作为字典流式传输到训练过程中。
- en: It doesn't take a lot of work to set up adversarial regularization with `tf.keras`
    and adversarial regularization APIs. Basically, an extra step is required to reformat
    the sample and label into a dictionary. Then, we wrap our model using the `nsl.keras.AdversarialRegularization`
    API, which encapsulates the model architecture and adversarial regularization
    object. This makes it very easy to implement this type of regularization.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`tf.keras`和对抗性正则化API设置对抗性正则化并不需要很多工作。基本上，只需要额外一步将样本和标签重新格式化为字典。然后，我们使用`nsl.keras.AdversarialRegularization`
    API包装我们的模型，该API封装了模型架构和对抗性正则化对象。这使得实现这种类型的正则化变得非常简单。
- en: Summary
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'This chapter presented some common practices for enhancing and improving your
    model building and training processes. One of the most common issues in dealing
    with training data handling is to stream or fetch training data in an efficient
    and scalable manner. In this chapter, you have seen two methods to help you build
    such an ingestion pipeline: generators and datasets. Each has its strengths and
    purposes. Generators manage data transformation and batching quite well, while
    a dataset API is designed where a TPU is the target.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一些增强和改善模型构建与训练过程的常见实践。在处理训练数据时，最常见的问题之一是以高效和可扩展的方式流式传输或获取训练数据。在本章中，你已经看到两种方法帮助你构建这样的数据获取管道：生成器和数据集。每种方法都有其优点和用途。生成器能够很好地管理数据转换和批处理，而数据集API则是为TPU目标而设计的。
- en: We also learned how to implement various regularization techniques using the
    traditional L1 and L2 regularization, as well as a modern regularization technique
    known as adversarial regularization, which is applicable to image classification.
    Adversarial regularization also manages data transformation and augmentation on
    your behalf to save you the effort of generating noisy images. These new APIs
    and capabilities enhance TensorFlow Enterprise's user experience and help save
    on development time.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学习了如何使用传统的L1和L2正则化技术，以及一种现代的正则化技术——对抗性正则化，它适用于图像分类。对抗性正则化还会为你处理数据转换和增强，省去你生成噪声图像的麻烦。这些新的API和功能增强了TensorFlow
    Enterprise的用户体验，并帮助节省开发时间。
- en: In the next chapter, we are going to see how to serve a TensorFlow model with
    TensorFlow Serving.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习如何使用TensorFlow Serving服务TensorFlow模型。
