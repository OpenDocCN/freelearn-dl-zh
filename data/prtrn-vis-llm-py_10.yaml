- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Fine-Tuning and Evaluating
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调和评估
- en: In this chapter, you’ll learn how to fine-tune your model on use case-specific
    datasets, comparing its performance to that of off-the-shelf public models. You
    should be able to see a quantitative and qualitative boost from your pretraining
    regime. You’ll dive into some examples involving language, text, and everything
    in between. You’ll also learn how to think about and design a human-in-the-loop
    evaluation system, including the same RLHF that makes ChatGPT tick! This chapter
    focuses on updating the trainable weights of the model. For techniques that mimic
    learning but don’t update the weights, such as prompt tuning and standard retrieval
    augmented generation, see [*Chapter 13*](B18942_13.xhtml#_idTextAnchor198) on
    prompt engineering.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何在用例特定数据集上微调您的模型，比较其性能与现成的公共模型。您应该能够从预训练中看到定量和定性的提升。您将深入一些涉及语言、文本及其间关系的例子。您还将学习如何思考和设计一个人在循环评估系统，包括使ChatGPT运行的RLHF！本章重点在于更新模型的可训练权重。对于模仿学习但不更新权重的技术，如提示调整和标准检索增强生成，请参阅[*第13章*](B18942_13.xhtml#_idTextAnchor198)中的提示工程。
- en: 'We are going to cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中涵盖以下主题：
- en: Fine-tuning for language, text, and everything in between
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言、文本及其间的微调
- en: LLM fine-tuning breakdown – instruction fine-tuning, parameter efficient fine-tuning,
    and reinforcement learning with human feedback
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM微调分解 - 指令微调、参数高效微调以及带有人类反馈的强化学习
- en: Vision fine-tuning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉微调
- en: Evaluating foundation models in vision, language, and joint tasks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在视觉、语言和联合任务中评估基础模型
- en: Fine-tuning for language, text, and everything in between
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言、文本及其间的微调
- en: 'At this point in the book, we’ve already covered a lot of ground. We’ve focused
    primarily on the pretraining aspect, looking at everything from finding the right
    use cases and datasets to defining your loss functions, preparing your models
    and datasets, defining progressively larger experiments, parallelization basics,
    working with GPUs, finding the right hyperparameters, advanced concepts, and more!
    Here, we’ll explore how to make your models even more targeted to a specific application:
    **fine-tuning**.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的这一部分，我们已经涵盖了很多内容。我们主要关注预训练方面，涵盖了从寻找合适的用例和数据集到定义损失函数，准备您的模型和数据集，定义逐渐扩展的更大实验，基本并行化，与GPU合作，寻找合适的超参数，高级概念等！在这里，我们将探讨如何使您的模型更加针对特定应用：**微调**。
- en: 'Presumably, if you are embarking on a large-scale training project, you might
    have one of the following goals:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，如果您正在进行大规模的训练项目，您可能会有以下一些目标：
- en: You might be pretraining your own foundation model
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能正在预训练您自己的基础模型
- en: You might be designing a novel method for autonomous vehicles
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能正在设计自动驾驶车辆的新方法
- en: You might be classifying and segmenting 3D data, such as in real estate or manufacturing
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能正在对3D数据进行分类和分割，比如在房地产或制造业中
- en: You might be training a large text classification model or designing a novel
    image-text generation model
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能正在训练一个大型文本分类模型或设计一个新颖的图像文本生成模型
- en: You might be building a text-to-music generator, or working on a completely
    new jointly trained modality, as yet undiscovered by the machine learning community
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能正在构建一个文本到音乐的生成器，或者正在开发一个机器学习社区尚未发现的全新联合训练模态
- en: You might be training a large language model to solve general-purpose search
    and qquestion and answering for the entire world, or for specific communities,
    languages, organizations, and purposes
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能正在训练一个大型语言模型，以解决全球范围内的通用搜索和问答问题，或者针对特定社区、语言、组织和目的
- en: All these use cases have something in common; they use one large-scale model
    that achieves general-purpose intelligence through learning patterns in extreme-scale
    datasets and model sizes. In many cases, however, *these models only become extraordinarily
    useful when fine-tuned to solve a specific problem*. This is not to say that you
    cannot simply deploy one of them and use specialized *prompt engineering* to immediately
    get useful results, because you can. In fact, we will dive into that later in
    this book. But prompt engineering alone can only get you so far. It is much more
    common to *combine prompt engineering with fine-tuning* to both focus your model
    on a target application and use all your creativity and skill to solve the actual
    human problem.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些使用案例有一个共同点；它们使用一个大规模模型，通过在极大规模的数据集和模型规模中学习模式来实现通用智能。然而，在许多情况下，*这些模型只有在经过微调以解决特定问题时才会变得特别有用*。这并不是说你不能简单地部署其中一个并使用专门的*提示工程*来立即获得有用的结果，因为你可以。事实上，我们将在本书后续内容中深入探讨这个话题。但仅仅依靠提示工程是有限的。通常，*将提示工程与微调相结合*，以便将你的模型集中于目标应用，并运用你的创造力和技能解决实际的人工问题，这种方式更加常见。
- en: I like to think about this pretraining and fine-tuning paradigm almost like
    the difference between general and specialized education, whether that is in a
    formal undergraduate and graduate program, online coursework, or on-the-job training.
    General education is broad. When done well, it encompasses a broad array of skills
    across many disciplines. Arguably, the primary output of generalized education
    is critical thinking itself.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢将这种预训练和微调范式视为一种类似于通用和专业教育之间差异的思考方式，无论是正式的本科和研究生课程、在线课程还是在职培训。通用教育是广泛的。如果做得好，它涵盖了多个学科的广泛技能。可以说，通用教育的主要产出就是批判性思维本身。
- en: Specialized training is very different; it is hyper-focused on excellence in
    a sometimes narrow domain. Examples of specialized training include a master’s
    degree, a certificate, a seminar, or a boot camp. Its output is usually critical
    thinking applied to one specific vertical.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 专业化培训非常不同，它通常专注于某个狭窄领域的卓越表现。专业化培训的例子包括硕士学位、证书、研讨会或训练营。它的产出通常是应用于某一特定领域的批判性思维。
- en: While this intuitive difference is easy to grasp, in practice, it is less obvious
    how to design machine learning applications and experiments that are gracefully
    optimized for both kinds of knowledge and keep this up to date. Personally, I
    would argue that a combination of pretrained and fine-tuned models presents the
    best solution for this to date. It is possible this will be how we continue to
    deal with ML for years, if not decades, to come.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种直观差异容易理解，但在实践中，设计既优化了通用知识又优化了专业知识的机器学习应用程序和实验，并保持这些内容的更新，却并不那么明显。从个人观点出发，我认为预训练和微调模型的结合是迄今为止最佳的解决方案。很可能，这将是我们未来几年，甚至几十年，继续处理机器学习的方式。
- en: As you should start to feel quite comfortable with by this point in the book,
    *the art of building a fantastic machine learning application or effective experiment
    lies in using the best of both general and specialized models*. Do not limit yourself
    to just a single model; this is not terribly different from limiting yourself
    to a single world view or perspective. Increasing the number of types of models
    you use has the possibility to increase the overall intelligence of your application.
    Just make sure you are phasing each experiment and sprint with clear objectives
    and deliverables.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在本书中这一部分应该开始感到非常熟悉的那样，*构建一个出色的机器学习应用程序或有效实验的艺术在于同时利用通用模型和专业模型的优势*。不要局限于使用单一模型；这与限制自己只拥有单一的世界观或视角没有太大不同。增加你使用的模型种类有可能提高你应用程序的整体智能水平。只要确保你在每个实验和迭代中都设定清晰的目标和可交付成果。
- en: You might use one single pretrained model, such as GPT-2, then fine-tune it
    to generate text in your vernacular. Or you might use a pretrained model to featurize
    your input text, such as in Stable Diffusion, and then pass it to a downstream
    model such as KNN.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能使用一个单一的预训练模型，比如GPT-2，然后对其进行微调，以生成你自己的语言风格的文本。或者你可能使用预训练模型对输入文本进行特征化，比如在稳定扩散（Stable
    Diffusion）中，然后将其传递给下游模型，如KNN。
- en: Hint
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: This is a great way to solve an image search! Or you might use any of the following
    fine-tuning regimes outlined.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是解决图像搜索的一个很好的方法！或者你可以使用下文所列的任何微调方法。
- en: Fine-tuning a language-only model
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调一个仅处理语言的模型
- en: If your model is a language-only project – something inspired by BERT or GPT
    – then once you’ve either finished pretraining or have reached a significant milestone
    (maybe a few hundred steps or so), you’ll want to fine-tune that pretrained base
    model on a more specific dataset. I would start to think about which use case
    to apply my model to – likely wherever I have the most supervised training data.
    This will also likely be a part of my business that has the strongest customer
    impact – from customer support to search, question answering to translation. You
    might also explore product ideation, feature request prioritization, documentation
    generation, text autocompletion, and so on.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的模型是一个仅处理语言的项目——比如受 BERT 或 GPT 启发的模型——那么一旦你完成了预训练或达到了一个重要的里程碑（可能是几百步），你就需要在更具体的数据集上微调这个预训练的基础模型。我会开始考虑将模型应用于哪个用例——很可能是在我拥有最多监督训练数据的地方。这也很可能是我的业务中影响客户最强的部分——从客户支持到搜索，从问答到翻译。你也可以探索产品创意、功能请求优先级、文档生成、文本自动补全等方面。
- en: Collect your supervised data and follow the steps in the previous chapter about
    analyzing your data. I would have many notebooks comparing the datasets, running
    summary statistics, and comparing distributions on key characteristics. After
    this basic analysis, I’d run some training jobs! These could be full-fledged SageMaker
    training jobs, or just using your notebook instances or Studio resources. Usually,
    fine-tuning jobs are not huge; it’s more common to fine-tune only a few GBs or
    so. If you have significantly more than this, I would probably consider just adding
    this to my pretraining dataset altogether.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 收集你的监督数据，并按照上一章关于分析数据的步骤进行操作。我会有许多笔记本来比较数据集，运行摘要统计，并对关键特征的分布进行比较。完成这基本分析后，我会运行一些训练任务！这些任务可以是完整的
    SageMaker 训练任务，也可以是使用你的笔记本实例或 Studio 资源。通常，微调任务并不庞大；通常只微调几 GB 的数据。如果你有明显更多的数据，我可能会考虑将其直接加入到我的预训练数据集中。
- en: Your final model should combine both the output from your pretraining project
    and the generalized data with your target use case. If you’ve done your job right,
    you should have many use cases lined up, so fine-tuning will let you use your
    pretrained model with all of them!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你的最终模型应结合预训练项目的输出和与目标用例相关的通用数据。如果你做得对，你应该已经有了多个用例排队等待，因此微调将让你可以将预训练模型应用于所有这些用例！
- en: Personally, I would use Hugging Face for a language-only project, pointing to
    my new pretrained model as the base object. You can follow steps from its SDK
    to point to different *downstream tasks*. What’s happening is that we’re using
    the pretrained model as the base of the neural network, then simply adding extra
    layers at the end to render the output tokens in a format that more closely resembles
    and solves the use case you want to handle.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 就个人而言，我会使用 Hugging Face 来处理仅涉及语言的项目，将我的新预训练模型作为基础对象。你可以按照其 SDK 中的步骤来指向不同的*下游任务*。发生的事情是，我们将预训练模型作为神经网络的基础，然后在末尾简单地添加额外的层，以便将输出的标记渲染成与你想处理的用例更为接近的格式。
- en: You’ll get to choose all the hyperparameters again. This is another time hyperparameter
    tuning is extremely useful; make it your friend to easily loop through tens to
    hundreds of iterations of your model and find the best version.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你将再次选择所有的超参数。这是超参数调优极为有用的又一个时刻；把它当作朋友，轻松地进行几十到几百次的模型迭代，找到最佳版本。
- en: Now, let’s break down different fine-tuning strategies for language that explicitly
    update the model parameters. Please note that the commentary that follows simply
    describes common scenarios for these techniques; I have no doubt that there are
    better ways of approaching these in development today.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们详细拆解几种显式更新模型参数的语言微调策略。请注意，接下来的评论只是描述了这些技术的常见场景；我毫不怀疑今天的开发中会有更好的方法来处理这些问题。
- en: '| **Name** | **Method** | **Result** |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| **名称** | **方法** | **结果** |'
- en: '| Classic fine-tuning | This takes a set of supervised text pairs and a pretrained
    foundation model and adds a new downstream head to the model. The new head, and
    possibly a few layers of the original model, is updated. | The new model performs
    well on the given task and dataset but fails outside of this. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 经典微调 | 这种方法需要一组监督学习的文本对和一个预训练的基础模型，并向模型添加一个新的下游头部。新的头部，可能还会更新原始模型的几层。 | 新模型在给定任务和数据集上表现良好，但在其他方面表现不佳。
    |'
- en: '| Instruction fine-tuning | This technique is essentially normal fine-tuning,
    but the crux is using a dataset with explicit instructions and the desired response
    provided. For a sample dataset, see Stanford’s Alpaca project *(1)*. The instructions
    are commands such as “tell me a story,” “create a plan,” or “summarize an article.”
    | A base generative model produces arbitrary text and performs well only in few-shot
    learning cases with complex prompt engineering. Once the instructions have been
    fine-tuned, the model can respond well in zero-shot cases without any examples
    in the prompt itself. Naturally, humans strongly prefer this, as it is much easier
    and faster to use. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 指令微调 | 这项技术本质上是常规的微调，但关键在于使用包含明确指令和期望回应的数据集。有关样本数据集，请参见斯坦福的 Alpaca 项目 *(1)*。指令包括“讲个故事”，“制定一个计划”或“总结一篇文章”等命令。
    | 基础生成模型生成任意文本，并且仅在少量示例学习情况下，结合复杂的提示工程时表现良好。一旦指令经过微调，模型就能在零-shot 情况下很好地响应，而无需在提示中提供示例。自然地，人们更喜欢这种方式，因为它使用起来更简单、更快捷。
    |'
- en: '| **Parameter-efficient** **fine-tuning** (**PEFT**) | Instead of updating
    all the weights of the original model, PEFT-based techniques, as inspired by LoRA
    *(2)*, inject new trainable matrices into the original model. This makes training
    and storage much more efficient and cost-effective, as much as three times so
    | For similar datasets, PEFT-based methods seem to meet accuracy levels of full
    fine-tuning, while requiring an order of magnitude less computation. The newly
    trained layers can be reused similarly to a classically fine-tuned model. Personally,
    I wonder whether this method could unlock hyperparameter tuning at scale for foundation
    models! |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| **参数高效** **微调**（**PEFT**） | PEFT 基于 LoRA *(2)* 启发，通过将新的可训练矩阵注入原始模型，而不是更新原始模型的所有权重。这使得训练和存储变得更加高效且具有成本效益，效率提高了三倍
    | 对于相似数据集，PEFT 基础的方法似乎能达到与完全微调相当的准确度，同时所需的计算量少了一个数量级。新训练的层可以像经典微调模型一样被重复使用。个人而言，我在想，这种方法是否能够为基础模型的大规模超参数调优开辟新的可能！
    |'
- en: '| Domain adaptation | Using largely unsupervised data, in language, this technique
    lets you continue pretraining the model. This is most relevant for focusing the
    performance of the model on a new domain, such as a particular industry vertical
    or proprietary dataset. | This results in an updated foundation model that should
    know new vocabulary and terminology based on its updated domain. It will still
    require task-specific fine-tuning to achieve the best performance on a specific
    task. |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 领域适应 | 通过大量无监督数据，这项技术可以在语言领域继续对模型进行预训练。这对于将模型性能集中在新领域（如特定行业垂直或专有数据集）尤为重要。
    | 这会导致一个更新后的基础模型，它应该掌握基于更新领域的新词汇和术语。但要在特定任务上实现最佳性能，仍然需要针对任务的微调。 |'
- en: '| **Reinforcement learning with human** **feedback** (**RLHF**) | This technique
    lets you quantify human preferences on generated content at scale. The process
    puts multiple model responses in front of human labelers and asks them to rank
    them. This is used to train a reward model, which serves as the guide for a reinforcement
    learning procedure to train a new LLM. We discuss this in detail shortly. | OpenAI
    shows *(3)* that models trained with RLHF are consistently preferred by humans,
    even over instruction fine-tuning. This is because the reward model learns what
    a group of humans sees, on average, as better-generated content. This preference
    is then incorporated into an LLM through RL. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **强化学习与人类** **反馈**（**RLHF**） | 该技术允许你在大规模上量化人类对生成内容的偏好。这个过程将多个模型响应展示给人类标注员，并让他们对其进行排序。这用于训练奖励模型，奖励模型作为强化学习过程的指导，用来训练新的
    LLM。我们稍后会详细讨论这一点。 | OpenAI *(3)* 表明，通过 RLHF 训练的模型在人类偏好上始终优于其他模型，甚至超过指令微调。这是因为奖励模型学习了人类群体平均认为更好的生成内容。这种偏好随后通过
    RL 融入 LLM 中。 |'
- en: If you’d like to jump straight into these techniques, including how they work
    with examples, then head straight over to the repository. You can also jump right
    to [*Chapter 15*](B18942_15.xhtml#_idTextAnchor229) for a deeper dive on parameter
    efficient fine tuning Remember, in [*Chapter 13*](B18942_13.xhtml#_idTextAnchor198),
    we’ll learn all about techniques that mimic learning but do so without updating
    any parameters in the model itself. This includes prompt-tuning, prompt engineering,
    prefix tuning, and more. For now, let’s learn about fine-tuning vision-only models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想直接了解这些技术，包括它们如何与示例一起工作，那么请直接前往代码库。你也可以直接跳转到[*第15章*](B18942_15.xhtml#_idTextAnchor229)以深入了解参数高效微调。记住，在[*第13章*](B18942_13.xhtml#_idTextAnchor198)中，我们将学习所有关于模仿学习的技术，但这些技术并不更新模型本身的任何参数。这包括提示调优、提示工程、前缀调优等。现在，让我们先了解一下微调视觉专用模型。
- en: Fine-tuning vision-only models
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调视觉专用模型
- en: Vision is a completely different world than language in terms of fine-tuning.
    In language, it’s a somewhat reliable practice to take a large pretrained model,
    such as BERT or GPT, add an extra dataset, fine-tune it, and get reasonably good
    performance out of the box. This isn’t to say that there aren’t countless other
    nuances and issues in language, because there are, but the general likelihood
    of getting pretty good performance with simple fine-tuning is high.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调方面，视觉和语言是完全不同的世界。在语言中，使用一个大型预训练模型，如BERT或GPT，添加一个额外的数据集，进行微调，并能在开箱即用的情况下获得相对较好的表现，已成为一种较为可靠的做法。这并不是说语言中没有无数其他细微差别和问题，因为它们确实存在，但通过简单的微调获得较好表现的普遍可能性较高。
- en: In vision, the likelihood of getting good performance right away is not as high.
    You might have a model from ImageNet that you’d like to use as your base model,
    then pair with a different set of labeled images. If your images already look
    like they came from ImageNet, then you are in good shape. However, if your images
    are completely different, with a different style, tone, character, nuance, or
    mode, then it’s likely your model won’t perform as well immediately. This is an
    age-old problem in vision that predates foundation models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉中，立即获得良好表现的可能性并不是很高。你可能有一个来自ImageNet的模型，想用它作为你的基础模型，然后与另一组标注图像配对。如果你的图像看起来已经像是来自ImageNet，那你就处于有利位置。然而，如果你的图像完全不同，风格、语调、特征、细微差别或模式都不一样，那么你的模型很可能不会立即表现得很好。这是一个早在基础模型之前就存在的视觉问题。
- en: '![Figure 10.1 – From Kate Saenko’s WACV Pretrain Workshop 2023 Keynote (4)](img/B18942_Figure_10_01.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1 – 来自凯特·塞恩科的WACV预训练研讨会2023年主旨演讲 (4)](img/B18942_Figure_10_01.jpg)'
- en: Figure 10.1 – From Kate Saenko’s WACV Pretrain Workshop 2023 Keynote (4)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – 来自凯特·塞恩科的WACV预训练研讨会2023年主旨演讲 (4)
- en: Leading vision researcher Kate Saenko pioneered approaches to solving this problem,
    which she calls **distribution bias**. As she identified in her first paper on
    this in 2010 *(5)*, the core issue is the massive gap between the **domains**.
    What happens in computer vision is that using a pretrained model, focused on one
    particular dataset, doesn’t translate as well to the downstream task. Even after
    fine-tuning the pretrained base model on a new set of labeled samples, the model
    is likely to simply overfit or not even learn the new domain well.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 领先的视觉研究者凯特·塞恩科（Kate Saenko）开创了解决这一问题的方法，她称之为**分布偏差**。正如她在2010年的第一篇论文中所指出的*(5)*，核心问题在于**领域**之间的巨大差距。在计算机视觉中，使用专注于某一特定数据集的预训练模型并不能很好地转化到下游任务。即使在新一组标注样本上对预训练基础模型进行微调，模型也很可能只是过拟合，或者甚至无法很好地学习新领域。
- en: Kate’s work identified that, in fact, using a more recently pretrained foundation
    model is very helpful in overcoming this domain adaptation problem *(6)*. She
    found that “*simply using a state-of-the-art backbone outperforms existing state-of-the-art
    domain adaptation baselines and sets new baselines on OfficeHome and DomainNet
    improving by 10.7% and 5.5%*”. In this case, *backbone* refers to the model, here
    ConvNext-T, DeiT-S, and Swin-S.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 凯特的研究发现，事实上，使用最近预训练的基础模型对于克服领域适应问题非常有帮助*(6)*。她发现“*仅仅使用最先进的骨干网络就超过了现有的最先进领域适应基准，并在OfficeHome和DomainNet上设置了新的基准，分别提高了10.7%和5.5%*”。在这种情况下，*骨干网络*指的是模型，这里是ConvNext-T、DeiT-S和Swin-S。
- en: In the same work, Kate also found that larger models tended to perform better.
    In the following visual, you can see that by increasing the model size by tens
    of millions of parameters, she was also able to increase accuracy.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一项工作中，Kate还发现，较大的模型往往表现得更好。在下图中，你可以看到，通过将模型的参数数量增加数千万，她也提高了准确性。
- en: '![Figure 10.2 – Saenko’s results on the impact of increasing model size in
    vision](img/B18942_Figure_10_02.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图10.2 – Saenko关于在视觉中增加模型大小的影响的结果](img/B18942_Figure_10_02.jpg)'
- en: Figure 10.2 – Saenko’s results on the impact of increasing model size in vision
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 – Saenko关于在视觉中增加模型大小的影响的结果
- en: Now that we’ve learned about some fine-tuning regimes relating to vision-only,
    let’s explore fine-tuning regimes in the combination of vision and language!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了一些与仅视觉相关的微调机制，让我们来探索视觉与语言结合的微调机制吧！
- en: Fine-tuning vision-language models
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调视觉-语言模型
- en: First, let’s recap a few interesting tasks that are unique to the explicit combination
    of vision and language. These include visual question-answering, text-to-image,
    text-to-music, and as Allie Miller likes to say, “text-to-everything”. They also
    include image captioning, video captioning, visual entailment, grounding, and
    more. You might use vision-language models in an e-commerce application to make
    sure the right product is on the page, or even in the film industry to generate
    new points on a storyboard.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们回顾一下几项与视觉和语言明确结合独特相关的有趣任务。这些任务包括视觉问答、文本到图像、文本到音乐，正如Allie Miller所说的，“文本到一切”。它们还包括图像描述、视频描述、视觉蕴含、基础对接等。你可能会在电商应用中使用视觉-语言模型，确保页面上的产品正确，甚至在电影行业中用于生成新的故事板点。
- en: Fine-tuning a pretrained vision-language model, at the most basic level, should
    follow the same pattern as we discussed for each of the other paradigms. You need
    a base model, then you need a set of data that follows the same labeling schema.
    If you’ve used the *Lensa* app, then you’ll already be somewhat familiar with
    fine-tuning a vision-language model! Lensa asks you to upload photos of yourself
    to its app. Just hypothesizing here, I would guess that it takes these photos
    and quickly fine-tunes Stable Diffusion on these new images of you. Then, it probably
    uses a prompting tool along with a content filter to send images back to you.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 微调预训练的视觉-语言模型，在最基本的层面上，应该遵循我们为其他范式讨论的相同模式。你需要一个基础模型，然后需要一组符合相同标签方案的数据。如果你使用过*Lensa*应用，那么你已经对微调视觉-语言模型有些了解了！Lensa会要求你上传自己的照片到应用中。假设一下，我猜它会利用这些照片，并迅速在这些新图像上微调Stable
    Diffusion。然后，它可能使用一个提示工具，并配合内容过滤器将图像发送回给你。
- en: Another recent case study of vision-language fine-tuning I’m really impressed
    by is *Riffusion* *(7)*. As of right now, you can use their free website to listen
    to music generated from text, and it’s pretty good! They built an open source
    framework that takes audio clips and converts them into images. The images are
    called **spectrograms**, which are the result of using a *Short-time Fourier transform*,
    which is an approximation for converting the audio into a two-dimensional image.
    This then serves as a visual signature for the sound itself. Spectrograms can
    also be transformed back into the audio itself, producing the sound.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我最近非常印象深刻的另一个视觉-语言微调的案例研究是*Riffusion* *(7)*。目前，你可以使用他们的免费网站来听从文本生成的音乐，而且效果相当不错！他们建立了一个开源框架，能够将音频片段转换成图像。这些图像被称为**声谱图**，是通过使用*短时傅里叶变换*得出的，这是一种将音频转换为二维图像的近似方法。这样就能为声音本身提供一种视觉特征。声谱图也可以转回音频，重新生成声音。
- en: Naturally, they used short textual descriptions of the audio clips as the textual
    label for each image, and voila! They had a labeled dataset to fine-tune Stable
    Diffusion. Using the spectrograms and textual descriptions of these, they fine-tuned
    the model and hosted it. Now you can quite literally write a textual prompt such
    as “Jamaican dancehall vocals” or “Sunrise DJ Set”, and their model will generate
    that audio for you!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，他们使用了音频片段的简短文本描述作为每张图像的文本标签，瞧！他们拥有了一个标注数据集来微调Stable Diffusion。通过使用这些声谱图和其文本描述，他们对模型进行了微调并进行了托管。现在，你几乎可以写下像“牙买加舞厅人声”或“日出DJ集”的文本提示，他们的模型就会为你生成这些音频！
- en: 'I love this project because the authors went a step further: they designed
    a novel smoothing function to seamlessly transition from one spectrogram to another.
    This means when you’re using their website, you can very naturally transition
    from one musical mode to another. All of this was made possible by using the large
    pretrained Stable Diffusion base model and fine-tuning it with their novel image/text
    dataset. For the record, there are quite a few other music generation projects,
    including MusicLM *(8)*, DiffusionLM *(9)*, MuseNet *(10)*, and others.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢这个项目，因为作者更进一步：他们设计了一种新颖的平滑函数，可以无缝地从一个谱图过渡到另一个谱图。这意味着当你使用他们的网站时，你可以非常自然地从一种音乐模式过渡到另一种音乐模式。所有这一切都得益于使用了大规模预训练的稳定扩散基础模型，并用他们的新型图像/文本数据集进行了微调。值得一提的是，还有其他一些音乐生成项目，包括
    MusicLM *(8)*、DiffusionLM *(9)*、MuseNet *(10)* 等等。
- en: Now that you’ve learned about the great variety of pretraining and fine-tuning
    regimes, you should be getting pretty excited about identifying ways to use the
    model you’ve been working on training until now. Let’s learn how to compare performance
    with open source models!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了各种预训练和微调策略，应该对找到利用你一直在训练的模型的方式感到非常兴奋。让我们学习如何与开源模型进行性能比较吧！
- en: Evaluating foundation models
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估基础模型
- en: 'As we’ve discussed many times in this book so far, the primary reason to engage
    in large-scale training is that open source models aren’t cutting it for you.
    Before you start your own large-scale training project, you should have already
    completed the following steps:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中多次讨论的那样，进行大规模训练的主要原因是开源模型无法满足你的需求。在开始自己的大规模训练项目之前，你应该已经完成了以下步骤：
- en: Tested an open source model on your specific use case
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的特定使用场景中测试过开源模型
- en: Identified performance gaps
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定了性能差距
- en: Fine-tuned that same open source model on a *small* subset of your data
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你数据的*小*子集上微调了同一个开源模型
- en: Identified *smaller* performance gaps
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定了*更小*的性能差距
- en: The point is that you should have some empirical reason to believe that the
    open source model solves *some* of your business problem but not *all* of it.
    You need to also empirically prove that small-scale fine-tuning is in the same
    boat; it should increase system performance but still leave room for improvement.
    This entire next section is about evaluating that room for improvement. Let’s
    try to understand how we can evaluate foundation models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是，你应该有一些经验性的理由相信开源模型可以解决你*部分*的业务问题，但并不是*全部*问题。你还需要经验性地证明小规模微调也在同样的情况下；它应该提高系统性能，但仍然有改进的空间。接下来的整个部分将讨论如何评估这种改进空间。让我们试着理解如何评估基础模型。
- en: As you are no doubt suspecting, evaluating foundation models falls into two
    phases. First, we care about the pretraining performance. You want to see the
    pretraining loss drop, be that masked language modeling loss, causual modeling
    loss, diffusion loss, perplexity, FID, or anything else. Second, we care about
    the downstream performance. That can be classification, named entity recognition,
    recommendation, pure generation, question answering, chat, or anything else. We
    covered evaluating the pretraining loss function in earlier chapters. In the following
    section, we’ll mostly cover the evaluation of downstream tasks. Let’s start with
    some top terms in vision models.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你无疑已经猜到的那样，评估基础模型分为两个阶段。首先，我们关注的是预训练性能。你希望看到预训练损失下降，无论是掩码语言建模损失、因果建模损失、扩散损失、困惑度、FID
    还是其他任何指标。其次，我们关注的是下游性能。下游任务可以是分类、命名实体识别、推荐、纯生成、问答、聊天等。我们在前面的章节中已经讨论了如何评估预训练损失函数。在接下来的部分，我们将主要讨论下游任务的评估。让我们从一些视觉模型中的重要术语开始吧。
- en: Model evaluation metrics for vision
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉模型评估指标
- en: In vision projects, as in all machine learning, evaluation completely depends
    on the task at hand. Common vision tasks include image classification, object
    detection, classification, segmentation, facial recognition, pose estimation,
    segmentation maps, and more. For an image classification problem, you’ll be happy
    to know the primary evaluation metric tends to be accuracy! Precision and recall
    also continue to be relevant here, as they are with any classification task.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉项目中，和所有机器学习任务一样，评估完全依赖于当前任务。常见的视觉任务包括图像分类、物体检测、分类、分割、人脸识别、姿态估计、分割图等。对于图像分类问题，你会很高兴知道，主要的评估指标通常是准确率！精确率和召回率在这里也同样重要，因为它们在任何分类任务中都相关。
- en: For object detection, as you can see in the figure, the question is much harder.
    It’s not enough to know whether the given class is in the image anywhere; you
    need the model to also know which part of the image includes the object.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于物体检测，如你在图中所见，问题要复杂得多。仅仅知道给定类别是否在图像中的某个位置是不够的；你还需要模型知道图像的哪一部分包含了该物体。
- en: '![Figure 10.3 – Intersection over union](img/B18942_Figure_10_03.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.3 – 交集与并集的比值](img/B18942_Figure_10_03.jpg)'
- en: Figure 10.3 – Intersection over union
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 – 交集与并集的比值
- en: Object detection is useful in self-driving, manufacturing, security, retail,
    and other applications. Usually, it’s not enough to just identify an object; you
    want to jointly minimize the amount of incorrect pixels your box consumes while
    maximizing the amount of correct pixels.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测在自动驾驶、制造业、安全、零售和其他应用中非常有用。通常，仅仅识别物体是不够的；你希望共同最小化边界框包含的错误像素数量，同时最大化包含的正确像素数量。
- en: Here, we are using a term called **IOU**, which literally means **intersection
    over union**. As you can see, the term corresponds to the area of the overlap
    of the two bounding boxes, divided by the area of the union of the two. As you
    might imagine, a larger IOU is better, because it means your bounding boxes are
    more consistent. A smaller IOU means there is still a wide degree of difference
    between the two, and your classifiers may not be capturing similar amounts of
    information. You might find this interesting if your object detector has many
    different classes and you want to compare these. You can also take the weighted
    average IOU of all classes, giving you the **mean** **IOU** (**mIOU**).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了一个叫做**IOU**的术语，它的字面意思是**交集与并集的比值**。如你所见，该术语对应的是两个边界框的重叠区域面积，除以这两个边界框并集的面积。正如你可能想象的那样，较大的IOU值更好，因为它意味着你的边界框更加一致。较小的IOU值则意味着两者之间的差异较大，你的分类器可能没有捕捉到相似的内容。如果你的物体检测器包含多个类别，并且你想对这些类别进行比较，你可能会对这个概念感兴趣。你还可以计算所有类别的加权平均IOU，从而得到**均值**
    **IOU**（**mIOU**）。
- en: Another common way of aggregating the overall performance of the many classifiers
    in your object detection algorithm is **mAP**, or **mean average precision**.
    For a single model, you’d call this the average precision, because it’s an average
    of the results across all classification thresholds. For multiple models, you’d
    take the average of each class, hence **mean average** **precision** (**mAP**).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的聚合物体检测算法中多个分类器整体性能的方式是**mAP**，即**均值平均精度**。对于单个模型，你会称其为平均精度，因为它是跨所有分类阈值的结果平均值。对于多个模型，你会取每个类别的平均值，因此就是**均值平均精度**（**mAP**）。
- en: Another really interesting vision solution in the foundation model space is
    **Segment** **Anything Model** (**SAM**) by Meta *(9)*. As shown in the following
    figure from its work, it presents a novel task, dataset, and model to *enable
    prompt-driven mask generation*. A segmentation map is a helpful construct in computer
    vision to identify pixels of an image that belong to a certain class. In this
    work, SAM learns how to generate new segmentation maps from both a given image
    and a natural language prompt. It then isolates pixels provided in the image that
    solve the question posed by the natural language prompt.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在基础模型领域，另一个非常有趣的视觉解决方案是Meta的**Segment** **Anything Model**（**SAM**）*(9)*。如下面图中的展示，它提出了一项新的任务、数据集和模型，旨在*启用基于提示的掩码生成*。分割图是计算机视觉中的一种有用结构，用来识别图像中属于某个特定类别的像素。在这项工作中，SAM学习如何根据给定图像和自然语言提示生成新的分割图。然后，它会从图像中隔离出解决自然语言提示问题的像素。
- en: "![Figure 10.4 – Meta’s \uFEFFSAM](img/B18942_Figure_10_04.jpg)"
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.4 – Meta的SAM](img/B18942_Figure_10_04.jpg)'
- en: Figure 10.4 – Meta’s SAM
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 – Meta的SAM
- en: To evaluate the segmentation maps generated by the model, the Meta team randomly
    sampled 50,000 masks and asked their professional annotators to improve the quality
    of these masks using image-editing tools, such as “brush” and “eraser”. Then,
    they computed the IoU between the original and the final map.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型生成的分割图，Meta团队随机抽取了50,000个掩码，并要求他们的专业标注员使用图像编辑工具（如“画笔”和“橡皮擦”）来改进这些掩码的质量。然后，他们计算了原始图和最终图之间的IOU值。
- en: Now that we’ve looked at a few evaluation examples in vision, let’s do the same
    for language.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过了一些视觉领域的评估示例，接下来我们也来看看语言领域的类似评估。
- en: Model evaluation metrics in language
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言中的模型评估指标
- en: While many of the classification-type metrics still apply to language, the question
    of how to evaluate generated language is inherently challenging. One could argue
    that many disciplines within the humanities, such as literary criticism, history,
    and philosophy, come down to evaluating a given corpora of written text. It’s
    not immediately obvious how to apply all this learning to improve the outputs
    of large language models.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然许多分类型指标仍适用于语言，但如何评估生成语言本身是一个固有的挑战。可以说，人文学科中的许多领域，比如文学批评、历史学和哲学，归根结底都是对给定的文本语料库进行评估。如何将这些知识应用到改善大型语言模型的输出并不显而易见。
- en: One attempt at providing a standardized framework for this is the *HELM* *(12)*
    project from Stanford’s Center for Research on Foundation Models. **HELM** stands
    for **Holistic Evaluation of Language Models**. It provides an incredibly rich
    taxonomy of multiple evaluation metrics, including accuracy, fairness, bias, toxicity,
    and so on, along with results from nearly 30 LLMs available today. The following
    is a short example of this from their work *(13)*. It standardizes metrics evaluated
    across models and datasets.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 提供一个标准化框架的一个尝试是斯坦福大学基础模型研究中心的*HELM* *(12)*项目。**HELM**代表的是**语言模型的整体评估**。它提供了一个极为丰富的多个评估指标的分类法，包括准确性、公平性、偏见、毒性等，以及来自近30个现有LLM的结果。以下是他们工作中的一个简短示例*(13)*。它标准化了跨模型和数据集评估的指标。
- en: '![Figure 10.5 – Taxonomy of multiple metrics from HELM](img/B18942_Figure_10_05.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图10.5 – 来自HELM的多个指标分类](img/B18942_Figure_10_05.jpg)'
- en: Figure 10.5 – Taxonomy of multiple metrics from HELM
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 – 来自HELM的多个指标分类
- en: The HELM ratings are open sourced and available both in a web interface *(14)*
    and a GitHub repository *(15)*. In many cases, when you are hunting for the best
    model to use as your base, HELM is a great starting point. Now, let’s explore
    a few more of these evaluation metrics in detail. We’ll start with translation,
    then move on to summarization, question answering, and finally, pure generation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: HELM评级是开源的，并且可以在一个网页界面*(14)*和GitHub仓库*(15)*中找到。在很多情况下，当你寻找最合适的模型作为基础时，HELM是一个很好的起点。现在，让我们更详细地探讨这些评估指标。我们将从翻译开始，然后继续总结、问答，最后是纯生成。
- en: One of the first applications for natural language processing was translation,
    also known as *machine translation*. Quite literally, this means training a large
    language model to learn the relationships between strings you provide in pairs,
    such as translations across natural languages, such as English into German. One
    early metric used to compare the quality of the generated translations is **bleu**,
    which was proposed at the ACL conference in 2002 by a team from IBM *(16)*. They
    called their approach a **bilingual evaluation understudy**, hence **bleu**. Generally,
    this refers to comparing the precise words generated by the model, and whether
    or not that exact same set of words appears in the target sentence.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理的第一个应用之一就是翻译，也叫做*机器翻译*。字面意思就是训练一个大型语言模型，学习你提供的成对字符串之间的关系，例如不同自然语言之间的翻译，例如从英语到德语。一个早期用来比较生成翻译质量的指标是**bleu**，这个指标由IBM的一个团队在2002年的ACL会议上提出*(16)*。他们将其方法称为**双语评估替代方法**，因此叫做**bleu**。通常，这个指标是通过比较模型生成的精确单词，以及这些单词是否出现在目标句子中来进行评估。
- en: Bleu has many drawbacks, however, such as not being able to adequately handle
    synonyms, small variants of the same word, the importance of words, or their order.
    For those reasons, many practitioners use more recently developed evaluation metrics,
    such as rouge *(17)*. Rather than anticipating a literal translation, as bleu
    does, rouge anticipates a summary of a text. This counts the number of lapping
    sub-words, word sequences, and word pairs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Bleu有许多缺点，例如不能充分处理同义词、相同单词的小变体、单词的重要性或它们的顺序。因此，许多实践者使用了最近开发的评估指标，例如rouge*(17)*。与bleu预测字面翻译不同，rouge则预测文本的总结。它计算了重叠的子词、单词序列和单词对的数量。
- en: Evaluation in question answering is interesting because, fundamentally, you
    can break the problem into two parts. First, based on a provided question, usually,
    you want to retrieve a document that relates to that question. Years ago, this
    was commonly solved with term frequency/inverse document frequency terms (TF-IDF
    scoring), which of course was famously ousted by Google’s page rank, which up-voted
    pages based on how many times they were linked by pages that linked other high-quality
    sites. Today, the NLP start-up deepset has an interesting solution, called **haystack**,
    which provides a convenient wrapper around your own pretrained NLP models to retrieve
    the document most relevant to the question.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 问答评估非常有趣，因为从根本上讲，你可以将问题分解为两部分。首先，根据给定问题，通常你希望检索与该问题相关的文档。多年前，这个问题通常通过词频/逆文档频率（TF-IDF）来解决，当然，这一方法后来被Google的页面排名取代，页面排名是根据其他页面链接的高质量站点链接的次数来对页面进行评分的。今天，NLP初创公司deepset提供了一个有趣的解决方案，叫做**haystack**，它为你自己预训练的NLP模型提供了一个方便的封装，用来检索与问题最相关的文档。
- en: The second part of evaluating a question-answering system is really the quality
    of your rendered text as the answer. You might simply try to find the part of
    the original document that most relates to the question, using techniques from
    information retrieval. Or you might try to summarize the document, or some part
    of the document, that most closely resembles the question. If you have large amounts
    of labeled data, such as clickstream data, you can actually point exactly to the
    part of the document that receives the most click-through data and provide that
    as the answer. Obviously, this appears to be what Google does today.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 评估问答系统的第二部分实际上是你呈现的文本质量作为答案。你可能只是尝试找到与问题最相关的原始文档部分，使用信息检索的技术。或者你可能尝试总结文档，或者文档的某部分，它最接近问题的内容。如果你有大量标注数据，比如点击流数据，你实际上可以准确指出文档中获得最多点击的数据部分，并将其作为答案。显然，这似乎就是今天Google的做法。
- en: Evaluating the quality of generated text is especially challenging. While classification
    and some other ML problems have an inherently objective answer, where the human
    label is clearly right or wrong, this really isn’t the case in literary analysis.
    There are many right answers, because reasonable people have different perspectives
    on how they interpret a given story or text. This is an example of subjectivity.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 评估生成文本的质量尤其具有挑战性。尽管分类和一些其他机器学习问题有着固有的客观答案，人在标注时是明确对错的，但在文学分析中并非如此。因为有很多正确的答案，合理的人们会根据他们对给定故事或文本的不同解读，得出不同的结论。这就是主观性的一个例子。
- en: 'How do we handle this discrepancy between subjectivity and objectivity in evaluating
    generated text? I can think of at least three ways. First, I’m quite fond of training
    a discriminator. This could be a classifier trained with positive and negative
    samples in cases where that is accurate, such as trying to mimic a certain author’s
    style. You can easily fine-tune a BERT-based model with a small sample of an author’s
    work, as compared with random outputs from GPT-3, for example, and get a very
    reliable way to evaluate generated text. Another interesting project is GPTScore,
    which uses zero-shot prompting to test other LLMs: [https://arxiv.org/pdf/2302.04166.pdf](https://arxiv.org/pdf/2302.04166.pdf)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何处理在评估生成文本时主观性与客观性之间的这种差异呢？我能想到至少三种方法。首先，我非常喜欢训练一个判别器。这可以是一个使用正负样本训练的分类器，在某些情况下（例如试图模仿某位作者的风格）是准确的。你可以很容易地用一小部分作者作品对基于BERT的模型进行微调，和GPT-3的随机输出进行对比，从而获得一种非常可靠的方式来评估生成的文本。另一个有趣的项目是GPTScore，它使用零-shot提示测试其他LLM：[https://arxiv.org/pdf/2302.04166.pdf](https://arxiv.org/pdf/2302.04166.pdf)
- en: You might also have humans label the responses, and then simply aggregate the
    labels. Personally, I am really impressed by ChatGPT’s interesting approach to
    this problem. They simply asked humans to rank the responses from their GPT-3
    model, and then trained the model to be optimized for the best of all responses
    using reinforcement learning! We’ll dive into that in the last section of this
    chapter.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以让人类标注回答，然后简单地汇总这些标签。就个人而言，我对ChatGPT在这个问题上的有趣方法感到非常印象深刻。他们直接让人类对来自GPT-3模型的回答进行排序，然后使用强化学习训练模型，以优化所有回答中的最佳答案！我们将在本章最后一部分深入探讨这个方法。
- en: Now that you’ve learned about a few evaluation metrics in language, let’s explore
    the same in jointly trained vision-language tasks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了一些语言评估指标，让我们在共同训练的视觉-语言任务中探讨相同的内容。
- en: Model evaluation metrics in joint vision-language tasks
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 联合视觉-语言任务中的模型评估指标
- en: As you’ve no doubt witnessed since their release, diffusion-based models are
    a fascinating space to watch. These models typically are jointly trained vision-and-language
    models that learn how to generate images using a process called **diffusion**.
    This process learns about the relationship between the provided words and the
    image itself, enabling the consumer to then easily produce a new image simply
    by providing a new set of words. After achieving a low loss during training on
    the validation set, evaluation is typically done manually by the consumer offline.
    Most people simply guess and check, testing the model with a few different hyperparameters
    and ultimately just picking their favorite picture. An ambitious team might train
    a discriminator, similar to what I mentioned previously for evaluating generated
    text.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你自发布以来无疑已经见证的那样，基于扩散的模型是一个非常引人注目的领域。这些模型通常是联合训练的视觉-语言模型，通过一种叫做**扩散**的过程来学习如何生成图像。这个过程学习了提供的文字与图像之间的关系，使得使用者可以通过简单地提供一组新文字来轻松生成新图像。在验证集上训练时实现低损失后，评估通常是由使用者离线手动完成的。大多数人只是通过猜测和检查，测试模型的几个不同超参数，最终选择自己最喜欢的图片。一支雄心勃勃的团队可能会训练一个鉴别器，类似于我之前提到的，用于评估生成的文本。
- en: But what if you wanted to focus on a specific object, and simply put that object
    onto a different background? Or stylize that object? Or change its emotion or
    pose? Fortunately, now you can! Nataniel Ruiz from Boston University, while interning
    at Google, developed a project to do just that called **DreamBooth** *(18)*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果你想专注于一个特定对象，并简单地将该对象放到另一个背景上呢？或者将该对象进行风格化？或者改变它的情绪或姿势？幸运的是，现在你可以了！来自波士顿大学的
    Nataniel Ruiz，在谷歌实习时，开发了一个名为**DreamBooth** *(18)*的项目，专门实现这一目标。
- en: '![Figure 10.6 – Preserving loss with DreamBooth fine-tuning](img/B18942_Figure_10_06.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.6 – 使用 DreamBooth 微调保持损失](img/B18942_Figure_10_06.jpg)'
- en: Figure 10.6 – Preserving loss with DreamBooth fine-tuning
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 – 使用 DreamBooth 微调保持损失
- en: DreamBooth accomplishes this through custom tokens and a *specialized loss function*.
    The loss function is called a **prior preservation**, and it’s built to counter
    the overfitting that can commonly happen in vision fine-tuning, along with the
    *language-drift* issue that is known to happen in language fine-tuning. I’ll spare
    you the mathematical details of the loss function, but if you’re curious, please
    feel free to read the paper directly! Generally speaking, this new loss function
    retains its own generated samples during the fine-tuning process and uses these
    during supervision. This helps it retain the prior. They found that roughly 200
    epochs and just 3-5 input training images were enough to deliver excellent images
    as a result. You could consider this custom loss function another type of evaluation
    for image-text models.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: DreamBooth 通过自定义标记和*专业损失函数*来实现这一点。这个损失函数被称为**先验保持**，它旨在对抗在视觉微调过程中常见的过拟合问题，以及语言微调中已知的*语言漂移*问题。我将省略损失函数的数学细节，但如果你感兴趣，欢迎直接阅读论文！一般来说，这个新的损失函数在微调过程中保留了其生成的样本，并在监督中使用这些样本。这有助于它保持先验。他们发现大约
    200 次迭代和仅 3-5 张输入训练图像就足以生成优秀的图像了。你可以将这个自定义损失函数视为图像-文本模型的另一种评估方式。
- en: Now that we’ve explored a large variety of evaluation methods for vision and
    language models, let’s learn about ways to keep humans in the loop!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了多种视觉和语言模型的评估方法，让我们来学习如何保持人类在循环中的方法！
- en: Incorporating the human perspective with labeling through SageMaker Ground Truth
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 SageMaker Ground Truth 结合人类标注视角
- en: Obviously, a critical way to incorporate the human perspective into your work
    is with labeling! At AWS, we have both a low-level labeling service called **Mechanical
    Turk** (**MTurk**) and a more managed feature called **SageMaker Ground Truth**.
    As we’ve discussed, MTurk has already impacted the ML domain by being used to
    create datasets as famous as ImageNet! Personally, I’m a fan of SageMaker Ground
    Truth because it’s much easier to use for pre-built image labeling tasks such
    as object detection, image classification, and semantic segmentation. It comes
    with tasks for NLP, such as text classification and named entity recognition,
    tasks for video, and tasks for 3D point clouds.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，将人工视角融入工作的一种关键方式就是标注！在 AWS，我们有一个低级标注服务 **机械土耳其人** (**MTurk**) 和一个更为管理化的功能叫做
    **SageMaker Ground Truth**。正如我们讨论的那样，MTurk 已经通过用于创建像 ImageNet 这样著名的数据集，影响了 ML
    领域！就个人而言，我更喜欢 SageMaker Ground Truth，因为它在使用预建的图像标注任务（如物体检测、图像分类和语义分割）时更加方便。它还提供
    NLP 任务，如文本分类和命名实体识别，视频任务以及 3D 点云任务。
- en: '![Figure 10.7 – Manage data labeling with SageMaker Ground Truth](img/B18942_Figure_10_07.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.7 – 使用 SageMaker Ground Truth 管理数据标注](img/B18942_Figure_10_07.jpg)'
- en: Figure 10.7 – Manage data labeling with SageMaker Ground Truth
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 – 使用 SageMaker Ground Truth 管理数据标注
- en: You can bring your own HTML frame to any arbitrary ML task and even make use
    of the *active labelling* *(19)* feature to dynamically train a model using records
    you’ve already labeled and speed up the whole labeling process. For supported
    built-in tasks such as image classification, semantic segmentation, object detection,
    and text classification, this means you will actually train an ML model on the
    data you’ve already labeled, then run inference against the unlabeled data. When
    the model is at least 80% confident in its response, it’s considered a labeled
    sample. When it’s not, it’s routed to the manual teams to label. Overall, this
    can dramatically reduce the cost of your project.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以为任何任意的 ML 任务带入自己的 HTML 框架，甚至利用*主动标注* *(19)* 功能，动态训练一个模型，使用你已标注的记录并加速整个标注过程。对于支持的内建任务，如图像分类、语义分割、物体检测和文本分类，这意味着你实际上会在已标注的数据上训练
    ML 模型，然后对未标注的数据进行推理。当模型对其响应的置信度达到至少 80% 时，它被视为已标注样本。如果没有，它会被转交给人工团队进行标注。总体来说，这可以大幅度降低你的项目成本。
- en: Another nice feature of SageMaker Ground Truth is that it automatically consolidates
    any discrepancies in labelers on your behalf. You can define how many people you
    want to label your objects, and it will look at, on average, how accurate each
    of those labelers is. Then, it’ll use that per-person average accuracy to consolidate
    the votes per object.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Ground Truth 的另一个不错的功能是，它会自动为你整合标注者之间的任何差异。你可以定义想要多少人标注你的对象，它会根据每个标注者的准确性进行平均，然后使用这个每个人的平均准确度来整合每个对象的投票。
- en: For hosted models, you can also connect them to SageMaker Ground Truth via the
    *augmented artificial intelligence* solution. This means you can set a trigger
    to route model inference responses to a team of manual labelers, via SageMaker
    Ground Truth, to audit the response and ensure it’s accurate and not harmful to
    humans.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于托管模型，你还可以通过*增强型人工智能*解决方案将其与 SageMaker Ground Truth 连接。这意味着你可以设置触发器，通过 SageMaker
    Ground Truth 将模型推理响应引导至人工标注团队，审查响应并确保其准确且不对人类造成伤害。
- en: Now that you have some idea of how to incorporate human labeling across your
    ML projects, let’s break down the method that makes ChatGPT tick!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你大致了解了如何在 ML 项目中融入人工标注，让我们来分解一下让 ChatGPT 运作的关键方法！
- en: Reinforcement learning from human feedback
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来自人类反馈的强化学习
- en: At least two things are undeniable about ChatGPT. First, its launch was incredibly
    buzzy. If you follow ML topics on social and general media, you probably remember
    being overloaded with content about people using it for everything from writing
    new recipes to start-up growth plans, and from website code to Python data analysis
    tips. However, there’s a good reason for the buzz. It’s actually so much better
    in terms of performance than any other prompt-based NLP solution the world has
    seen before. It establishes a new state of the art in question answering, text
    generation, classification, and so many other domains. It’s so good, in some cases
    it’s even better than a basic Google search! How did they do this? **RLHF** is
    the answer!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 关于ChatGPT，有至少两件事是不可否认的。首先，它的发布引起了极大的关注。如果你在社交和大众媒体上关注机器学习话题，你可能记得被关于人们用它做从写新食谱到制定创业计划、从网站代码到Python数据分析技巧等各类内容的文章淹没。然而，围绕它的热议是有充分理由的。就性能而言，它实际上比世界上任何其他基于提示的自然语言处理（NLP）解决方案都要好得多。它在问题回答、文本生成、分类等多个领域树立了新的技术标准。它表现得如此出色，在某些情况下，甚至比基本的Google搜索更好！他们是怎么做到的呢？**RLHF**就是答案！
- en: While RLHF is not a new concept in and of itself, certainly the most obviously
    successful application of RLHF in the large language model domain is ChatGPT.
    The predecessor to ChatGPT was InstructGPT *(20)*, where OpenAI developed a novel
    framework to improve the model responses from GPT-3\. Despite being 100x smaller
    in terms of parameters, InstructGPT actually outperforms GPT-3 in many text-generation
    scenarios. ChatGPT takes this a step further by adding an explicit dialogue framework
    to the training data. This dialogue helps maintain the context of the entire chat,
    referring the model back to the top data points provided by the consumer.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RLHF本身并不是一个新概念，但在大型语言模型领域，RLHF最显著的成功应用无疑是ChatGPT。ChatGPT的前身是InstructGPT *(20)*，OpenAI在其中开发了一种新的框架来改进GPT-3的模型响应。尽管在参数数量上InstructGPT小100倍，但在许多文本生成场景中，InstructGPT实际上优于GPT-3。ChatGPT更进一步，通过为训练数据添加明确的对话框架来增强这一点。这种对话有助于保持整个聊天的上下文，将模型引导回消费者提供的顶部数据点。
- en: '![Figure 10.8 – Reinforcement learning from human feedback](img/B18942_Figure_10_08.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.8 – 来自人类反馈的强化学习](img/B18942_Figure_10_08.jpg)'
- en: Figure 10.8 – Reinforcement learning from human feedback
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8 – 来自人类反馈的强化学习
- en: 'Let’s break down the reinforcement learning! To simplify the process, we can
    break it down into three key steps:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来解析强化学习吧！为了简化过程，我们可以将其分解为三个关键步骤：
- en: First, collect data from a pretrained model hosted live with humans. From actual
    questions provided by humans, OpenAI sends these same questions to a team of manual
    labelers. This set of labeled data is then used to fine-tune the large GPT-3 type
    model – in this case, it was GPT-3.5 specifically.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从由人类主持的预训练模型中收集数据。OpenAI将人类提供的实际问题发送给一组人工标注员进行标注。这些标注数据随后被用于微调大型GPT-3类型的模型——在这种情况下，特别是GPT-3.5。
- en: Next, take the fine-tuned model and submit prompts. After this, OpenAI asks
    the human labelers to *simply rank the outputs*. I love this approach because
    it does a good job of bridging the inherently subjective task of labeling freeform
    generated tasks, with the objective goal of producing a better ML model. When
    humans rank the responses from best to worst, it avoids the subjective question
    *“Is this good or not?”* and replaces it with an objective question, *“Which is
    your favourite?”* Armed with these ranked responses, it trains a *reward model*,
    which is just an ML model that takes a given prompt and rates it based on human
    responses. I’d imagine this is a regression model, though classification would
    also work.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用微调后的模型并提交提示。之后，OpenAI要求人工标注者*简单地对输出进行排名*。我喜欢这种方法，因为它很好地弥合了标注自由生成任务这一固有主观性的工作与生产更好机器学习模型这一客观目标之间的差距。当人类将响应从最好到最差进行排名时，它避免了主观问题*“这好不好？”*，而是用一个客观问题*“哪个是你最喜欢的？”*来代替。通过这些排名的响应，它训练出一个*奖励模型*，这只是一个机器学习模型，接收给定的提示并根据人类的响应对其进行评分。我想这应该是一个回归模型，尽管分类模型也可以使用。
- en: Finally, OpenAI uses a reinforcement learning algorithm, PPO specifically, to
    connect the dots. It generates a prompt response from the LLM, and in the reinforcement
    learning literature, we’d call that *takes an action*. The reward for this response
    is produced by running it against the reward model we just trained in the previous
    step. That reward is used to update the PPO algorithm, which in turn ensures that
    the next response it provides is closer to the highest reward it can get.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，OpenAI 使用一种强化学习算法，具体来说是 PPO，将各个环节连接起来。它从大语言模型（LLM）生成一个提示响应，在强化学习文献中，我们称之为*采取行动*。这个响应的奖励是通过将其与我们在前一步训练的奖励模型进行对比得出的。这个奖励用于更新
    PPO 算法，进而确保下一次生成的响应更接近能够获得的最高奖励。
- en: And that is RLHF in a nutshell! I think it’s a brilliant way of integrating
    nuanced human preferences with machine learning models, and I can’t wait to try
    it in my next project.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 RLHF 的精髓！我认为这是将细致的人类偏好与机器学习模型结合起来的绝妙方式，迫不及待想在下一个项目中尝试它。
- en: 'Before we move on to the next chapter, in which we will detect and mitigate
    bias, let’s do a quick recap of all the concepts we’ve covered in this chapter.
    Hint: there are a lot!'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入下一章，检测和缓解偏差之前，让我们快速回顾一下本章中讨论的所有概念。提示：内容很多！
- en: Summary
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The goal of this chapter was to give you a better understanding of fine-tuning
    and evaluating ML models overall, comparing them with open source options, and
    ultimately keeping humans in the loop.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是让你更好地理解微调和评估机器学习模型，比较它们与开源选项，最终确保人类始终在决策环节中参与。
- en: We started with a recap of fine-tuning for language, text, and everything in
    between, discussing the benefits of both general and specialized knowledge. We
    learned about fine-tuning a language-only model, and how generally this is possible
    with even a small amount of data. We also talked about fine-tuning vision-only
    models, and how generally it is much more likely to overfit, making it a challenging
    proposition. We looked at fine-tuning jointly trained vision-language models,
    including Stable Diffusion and an interesting open source project called Riffusion.
    We talked about comparing performance with off-the-shelf public models. We learned
    about model evaluation metrics for vision specifically, along with language, and
    the emerging joint vision-language space. We also looked at a variety of ways
    to keep humans in the loop across this entire spectrum, culminating in a discussion
    about RLHF as used in ChatGPT!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从回顾语言、文本及其相关领域的微调开始，讨论了通用知识和专业知识的优势。我们学习了如何微调仅限语言的模型，并了解即使数据量较小，通常也能实现这一点。我们还讨论了如何微调仅限视觉的模型，并认识到它们更容易出现过拟合，从而使得这种方法更加具有挑战性。我们研究了微调联合训练的视觉-语言模型，包括
    Stable Diffusion 和一个名为 Riffusion 的有趣开源项目。我们讨论了与现成的公共模型进行性能比较。我们了解了视觉、语言以及新兴的联合视觉-语言空间的模型评估指标。我们还探索了在整个过程中如何确保人类参与，最终讨论了
    ChatGPT 中使用的 RLHF（人类反馈强化学习）！
- en: Now, you’re ready to learn about detecting and mitigating bias in your ML projects
    in the next chapter.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已准备好在下一章学习如何在你的机器学习项目中检测和缓解偏差。
- en: References
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'tatsu-lab/stanford_alpaca: [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'tatsu-lab/stanford_alpaca: [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)'
- en: 'LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS: [https://arxiv.org/pdf/2106.09685.pdf](https://arxiv.org/pdf/2106.09685.pdf)'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LORA：大规模语言模型的低秩适配： [https://arxiv.org/pdf/2106.09685.pdf](https://arxiv.org/pdf/2106.09685.pdf)
- en: 'Training language models to follow instructions with human feedback: [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '用人类反馈训练语言模型以遵循指令: [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)'
- en: 'Keynote Speakers: [https://sites.google.com/view/wacv2023-workshop/speakers](https://sites.google.com/view/wacv2023-workshop/speakers)'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '主旨演讲人: [https://sites.google.com/view/wacv2023-workshop/speakers](https://sites.google.com/view/wacv2023-workshop/speakers)'
- en: 'Adapting Visual Category Models to New Domains: [https://link.springer.com/content/pdf/10.1007/978-3-642-15561-1_16.pdf?pdf=inline%20link](https://link.springer.com/content/pdf/10.1007/978-3-642-15561-1_16.pdf?pdf=inline%20link)'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '将视觉类别模型适配到新领域: [https://link.springer.com/content/pdf/10.1007/978-3-642-15561-1_16.pdf?pdf=inline%20link](https://link.springer.com/content/pdf/10.1007/978-3-642-15561-1_16.pdf?pdf=inline%20link)'
- en: 'A Broad Study of Pre-training for Domain Generalization and Adaptation: [https://arxiv.org/pdf/2203.11819.pdf](https://arxiv.org/pdf/2203.11819.pdf)'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '针对领域泛化和适应性进行预训练的广泛研究: [https://arxiv.org/pdf/2203.11819.pdf](https://arxiv.org/pdf/2203.11819.pdf)'
- en: 'RIFFUSION: [https://www.riffusion.com/about](https://www.riffusion.com/about)'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'RIFFUSION: [https://www.riffusion.com/about](https://www.riffusion.com/about)'
- en: 'MusicLM: Generating Music From Text: [https://arxiv.org/pdf/2301.11325.pdf](https://arxiv.org/pdf/2301.11325.pdf)'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'MusicLM: 从文本生成音乐: [https://arxiv.org/pdf/2301.11325.pdf](https://arxiv.org/pdf/2301.11325.pdf)'
- en: 'Diffusion-LM on Symbolic Music Generation with Controllability: [http://cs230.stanford.edu/projects_fall_2022/reports/16.pdf](http://cs230.stanford.edu/projects_fall_2022/reports/16.pdf)'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '以符号音乐生成为重点的Diffusion-LM: [http://cs230.stanford.edu/projects_fall_2022/reports/16.pdf](http://cs230.stanford.edu/projects_fall_2022/reports/16.pdf)'
- en: 'OpenAI: [https://openai.com/research/musenet](https://openai.com/research/musenet)'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'OpenAI: [https://openai.com/research/musenet](https://openai.com/research/musenet)'
- en: Segment Anything [https://arxiv.org/pdf/2304.02643.pdf](https://arxiv.org/pdf/2304.02643.pdf)
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Segment Anything [https://arxiv.org/pdf/2304.02643.pdf](https://arxiv.org/pdf/2304.02643.pdf)
- en: 'HELM: [https://crfm.stanford.edu/helm/latest/](https://crfm.stanford.edu/helm/latest/)'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'HELM: [https://crfm.stanford.edu/helm/latest/](https://crfm.stanford.edu/helm/latest/)'
- en: 'Holistic Evaluation of Language Models: [https://arxiv.org/pdf/2211.09110.pdf](https://arxiv.org/pdf/2211.09110.pdf)'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '语言模型的整体评估: [https://arxiv.org/pdf/2211.09110.pdf](https://arxiv.org/pdf/2211.09110.pdf)'
- en: 'HELM: [https://crfm.stanford.edu/helm/latest/?groups=1](https://crfm.stanford.edu/helm/latest/?groups=1)'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'HELM: [https://crfm.stanford.edu/helm/latest/?groups=1](https://crfm.stanford.edu/helm/latest/?groups=1)'
- en: 'stanford-crfm/helm: [https://github.com/stanford-crfm/helm](https://github.com/stanford-crfm/helm)'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'stanford-crfm/helm: [https://github.com/stanford-crfm/helm](https://github.com/stanford-crfm/helm)'
- en: 'BLEU: a Method for Automatic Evaluation of Machine Translation: [https://aclanthology.org/P02-1040.pdf](https://aclanthology.org/P02-1040.pdf)'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'BLEU: 一种用于机器翻译自动评估的方法: [https://aclanthology.org/P02-1040.pdf](https://aclanthology.org/P02-1040.pdf)'
- en: 'ROUGE: A Package for Automatic Evaluation of Summaries: [https://aclanthology.org/W04-1013.pdf](https://aclanthology.org/W04-1013.pdf)'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'ROUGE: 自动摘要评估包: [https://aclanthology.org/W04-1013.pdf](https://aclanthology.org/W04-1013.pdf)'
- en: 'DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation:
    [https://arxiv.org/pdf/2208.12242.pdf](https://arxiv.org/pdf/2208.12242.pdf)'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation:
    [https://arxiv.org/pdf/2208.12242.pdf](https://arxiv.org/pdf/2208.12242.pdf)'
- en: 'Automate Data Labeling: [https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html)'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '自动化数据标注: [https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html)'
- en: 'Training language models to follow instructions with human feedback: [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '用人类反馈训练语言模型遵循指令: [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)'
