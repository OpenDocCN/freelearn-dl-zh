- en: Long Short-Term Memory Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期记忆网络
- en: '"When I was young, I often pondered over what to do in my life. The most exciting
    thing, to me, seemed to be able to solve the riddles of the universe. That entailed
    becoming a physicist. However, I soon realized that there might be something even
    grander. What if I were to try build a machine, which becomes a much better physicist
    than I could ever hope to be. Perhaps, this is how I can multiply my tiny bit
    of creativity, into eternity."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “当我年轻时，我常常思考自己的人生该做什么。对我来说，最激动人心的事情似乎是能够解开宇宙的谜团。这意味着要成为一名物理学家。然而，我很快意识到，也许有更宏伟的目标。假如我能尝试去构建一台机器，让它成为比我任何时候都更优秀的物理学家呢？也许，这就是我能将自己微不足道的创造力，扩展到永恒的方法。”
- en: – Jeurgen Schmidthuber, co-inventor of the Long Short-Term Memory network
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: – Jeurgen Schmidthuber，长短期记忆网络的共同发明人
- en: 'In his diploma thesis in 1987, Schmidthuber theorized a mechanism of meta-learning
    that would be capable of inspecting its own learning algorithm and subsequently
    modifying it to effectively optimize the very mechanism of learning it employs.
    This idea entails opening up the learning space to the system itself so it can
    iteratively improve its learning as it sees new data: a system that would learn
    to learn, if you will. Schmidthuber even named this machine the Gödel machine,
    after the founder of the mathematical concept behind recursive self-improvement
    algorithms. Unfortunately, we are yet to build a self-learning universal problem-solver
    as described by Schmidthuber. However, that might not be as big a disappointment
    as you think it is. Some may argue that nature itself is yet to succeed in building
    such a system, given the current state of human affairs.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在1987年的毕业论文中，Schmidthuber提出了一种元学习机制的理论，该机制能够检查自身的学习算法，并随后对其进行修改，以有效地优化所使用的学习机制。这个想法意味着将学习空间开放给系统本身，以便它能够在看到新数据时不断改进自身的学习：可以说是一个“学习如何学习”的系统。Schmidthuber甚至将这台机器命名为Gödel机器，命名灵感来自于递归自我改进算法背后的数学概念创始人Gödel。不幸的是，我们至今尚未构建出Schmidthuber描述的那种自学习的通用问题解决器。然而，这可能并不像你想象的那么令人失望。有人可能会认为，鉴于当前人类事务的状态，自然界本身还未成功构建出这样的系统。
- en: On the other hand, Schmidthuber and his colleagues did succeed in developing
    something else that is quite novel. We speak, of course, of the **Long Short-Term
    Memory** (**LSTM**) network. Funnily enough, the LSTM is the older sibling of
    the **Gated Recurrent Unit** (**GRU**), seen previously, in many ways. Not only
    was the LSTM network conceived earlier (Hochreiter and Schmidthuber, 1997) than
    the GRU (Cho et al, 2014), but it is also computationally more intensive to run.
    This computational burden does come with a benefit, bringing a deluge of representational
    power for long-term dependency modeling when compared to the other **recurrent
    neural network** (**RNN**) counterparts we have seen so far.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Schmidthuber和他的同事确实成功开发了一些相当新颖的东西。我们当然指的是**长短期记忆**（**LSTM**）网络。有趣的是，LSTM在很多方面是**门控递归单元**（**GRU**）的“哥哥”。LSTM网络不仅比GRU（Cho等人，2014）早（Hochreiter和Schmidthuber，1997）提出，而且它的计算复杂度也更高。虽然计算负担较重，但与我们之前看到的其他**递归神经网络**（**RNN**）相比，它在长期依赖建模方面带来了大量的表示能力。
- en: The LSTM network provides a more complex solution to the problems of exploding
    and vanishing gradients we reviewed earlier. You may think of the GRU as a simplified
    version of the LSTM.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM网络为我们早前回顾过的梯度爆炸和梯度消失问题提供了一种更为复杂的解决方案。你可以将GRU看作是LSTM的简化版本。
- en: 'Following are the topics that we will be covering in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章将涉及的主题：
- en: The LSTM network
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM网络
- en: Dissecting the LSTM
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剖析LSTM
- en: LSTM memory block
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM记忆块
- en: Visualizing the flow of information
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化信息流动
- en: Computing contender memory
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算竞争者记忆
- en: Variations of LSTM and performance
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM的变种及其性能
- en: Understanding peephole connections
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解窥视孔连接
- en: Importance of timing and counting
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时机与计数的重要性
- en: Putting our knowledge to use
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将我们的知识付诸实践
- en: On modeling stock market data
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于建模股市数据
- en: Denoising the data
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据去噪
- en: Implementing exponential smoothing
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现指数平滑
- en: The problem with one-step ahead predictions
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一步预测问题
- en: Creating sequences of observation
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建观察序列
- en: Building LSTMs
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建LSTM
- en: Closing comments
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结束语
- en: On processing complex sequences
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理复杂序列
- en: In the last chapter, we discussed how humans tend to process events in a sequential
    manner. We break down our daily tasks into a sequence of smaller actions, without giving
    it much thought. When you get up in the morning, you may choose to visit the bathroom
    before making yourself breakfast. In the bathroom, you may choose to shower first
    before brushing your teeth. Some may choose to execute both tasks simultaneously.
    Often, these choices boil down to our individual preferences and time restrictions.
    From another perspective, a lot of how we go about doing the things we do has
    to do with how our brain has chosen to represent the importance of these relative
    tasks, governed by information it has saved about the near and distant past. For
    example, when you wake up in the morning, you may be inclined to shower first
    if you live in an apartment block with shared water supply.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了人类如何倾向于按顺序处理事件。我们将日常任务分解成一系列较小的行动，而不会过多考虑。当你早上起床时，可能会选择先去洗手间，再做早餐。在洗手间，你可能会先洗澡，然后刷牙。有些人可能会选择同时完成这两个任务。通常，这些选择归结为个人偏好和时间限制。从另一个角度来看，我们做事的方式往往与大脑如何选择表示这些相对任务的重要性有关，这种选择是由它对近过去和远过去保存的信息所支配的。例如，当你早上醒来时，如果你住在一个有共享水供应的公寓楼里，你可能会倾向于先洗澡。
- en: On the other hand, you may delay this task on some days if you know that your
    neighbors are on vacation. As it turns out, our own brains are very good at selecting,
    reducing, categorizing, and making available the information that is most advantageous
    to make predictions about the world around us.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你知道你的邻居正在度假，你可能会在某些日子推迟完成某些任务。事实证明，我们的大脑非常擅长选择、减少、分类并提供最有利的信息，以便对周围世界做出预测。
- en: Breaking down memory
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分解记忆
- en: We humans have layers of neurons aggregated in specific parts of our brain tasked
    with maintaining detailed and distinct representations of different types of important
    events that we may perceive. Take the temporal lobe, for instance, which consists
    of structures responsible for our declarative, or long-term, memory. This is what
    is widely believed to form the span of our conscious recollection of incidents.
    It reminds us of all general happenings going on in our mental model of the world,
    forming notions of both semantic facts about it (in semantic memory), and the
    occurrence of events (in episodic memory) within it. A semantic fact could be
    that the molecular compound of water represents one hydrogen and two oxygen atoms.
    Conversely, an episodic fact could be that a particular pool of water is tainted
    with chemicals, and hence is not potable. These distinctions in memory help us
    effectively navigate our information-abundant environment, as we make decisions
    to optimize our goals, whatever they may be. Moreover, some may even argue that
    making such distinctions to partition information is paramount to processing complex
    time-dependent sequences of data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们人类的大脑中有层次化的神经元，聚集在特定的区域，负责维护我们可能感知到的各种重要事件的详细且独特的表示。例如，考虑到颞叶，它包含了负责我们陈述性记忆或长期记忆的结构。这一部分通常被认为构成了我们对事件的意识回忆的范围。它提醒我们在世界的心理模型中，所有正在发生的一般事件，形成了对这些事件的语义记忆（有关其语义事实）以及事件发生的回忆（在情节记忆中）。一个语义事实可能是水的分子化合物由一个氢原子和两个氧原子组成。相反，一个情节事实可能是某个水池的水被污染了，因此不能饮用。记忆中的这些区分帮助我们有效地应对信息丰富的环境，使我们能够做出决策，优化我们可能有的任何目标。而且，有人甚至可能认为，做出这样的区分来划分信息，对于处理复杂的时间依赖数据序列至关重要。
- en: Ultimately, we need to maintain relevance in our predictive models over long
    periods of time, be it for the creation of interactive chatbots, or to predict
    the movement of stock prices. Being relevant involves not only knowing what has
    recently occurred, but also how history has unfolded. After all, as the old saying
    goes, history tends to repeat itself. Therefore, it can be useful to maintain
    a representation of this so-called history in memory. As we will soon see, this
    is precisely what the LSTM has set out to achieve.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们需要保持预测模型在长时间内的相关性，无论是用于创建互动聊天机器人，还是预测股价的走势。保持相关性不仅仅意味着了解最近发生了什么，还需要知道历史是如何展开的。毕竟，正如老话所说，历史往往会重演。因此，保持对所谓历史的记忆表示是很有用的。正如我们即将看到的，LSTM正是为了实现这一目标而设计的。
- en: The LSTM network
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM 网络
- en: Behold, the LSTM architecture. This model, iconic in its use of complex information
    paths and gates, is capable of learning informative time dependent representations
    from the inputs it is shown.  Each line in the following diagram represents the
    propagation of an entire vector from one node to another in the direction denoted
    by the arrows. When these lines split, the value they carry is copied to each
    pathway. Memory from previous time steps are shown to enter from the top-left
    of the unit, while activations from previous timesteps enter from the bottom-left
    corner.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 看啊，LSTM 架构。这一模型以其复杂的信息路径和门控机制而著称，能够从它所接收到的输入中学习时间依赖的有意义表示。下图中的每一条线代表一个向量从一个节点传递到另一个节点，方向由箭头指示。当这些线条分开时，它们所携带的值会被复制到每一条路径上。来自前一时间步的记忆从单元的左上方进入，而来自前一时间步的激活值则从左下角进入。
- en: 'The boxes represent the dot products of learned weight matrices and some inputs
    passed through an activation function. The circles represent point-wise operations,
    such as element-wise vector multiplication (*) or addition (+):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些框表示学习到的权重矩阵与某些通过激活函数传递的输入的点积。圆圈表示逐点操作，如逐元素向量乘法（*）或加法（+）：
- en: '![](img/570d2652-759a-463b-8066-fc280d0015af.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/570d2652-759a-463b-8066-fc280d0015af.png)'
- en: In the last chapter, we saw how RNNs may use a feedback connection through time
    to store representations of recent inputs through activations. These activations
    can essentially be thought of as the short-term memory of the unit, as it is mostly
    influenced by the activations from immediately preceding timesteps. Sadly, the
    vanishing gradients problem prohibited us from leveraging information that had
    occurred at very early timesteps (long-term memory) to inform later predictions.
    We saw that the weights comprising the hidden state have a propensity to decay
    or explode, as the errors are backpropagated through more and more timesteps.
    How can we solve this? How can we effectively allow information to flow through
    the timesteps, as it were, to inform predictions very late in the sequence? The
    answer, of course, came from Hochreiter and Schmidthuber, and consisted of using
    long-term memory (*c^((t-1))*) along with short-term memory (*a^((t-1))*) in RNNs.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，我们看到了 RNN 如何通过时间上的反馈连接来存储近期输入的表示，通过激活值。这些激活值本质上可以被视为单元的短期记忆，因为它们主要受紧接着的前一时间步激活值的影响。遗憾的是，梯度消失问题使得我们无法利用发生在非常早期时间步（长期记忆）的信息来指导后续的预测。我们看到，构成隐藏状态的权重倾向于衰减或爆炸，因为误差在更多时间步中进行反向传播。我们该如何解决这个问题？我们如何才能有效地让信息流经时间步，像是让它流动，来影响序列中后期的预测？答案，当然，来自
    Hochreiter 和 Schmidthuber，他们提出了在 RNN 中同时使用长期记忆 (*c^((t-1))*) 和短期记忆 (*a^((t-1))*)
    的方法。
- en: This approach allowed them to effectively overcome the problem of predicting
    relevantly over long sequences, by implementing an RNN design that is adept at
    conserving relevant memories of distant events. Practically speaking, this is
    done by employing a set of information gates that perform very well at conserving
    and passing forward the cell state, which encodes relevant representations from
    the distant past. This significant breakthrough has been shown to be applicable
    for various use cases, including speech processing, language modeling, non-Markovian
    control, and music generation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法使得它们能够有效克服在长序列中进行相关预测的问题，通过实现一种能够有效保存远程事件相关记忆的 RNN 设计。实际上，这是通过采用一组信息门来完成的，这些信息门在保存和传递细胞状态方面表现出色，细胞状态编码了来自遥远过去的相关表示。这一重大突破已被证明适用于多种应用场景，包括语音处理、语言建模、非马尔可夫控制和音乐生成。
- en: 'A source for further reading is given here:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提供了进一步阅读的来源：
- en: '**Original LSTM Paper Hochreiter and Schmidthuber**: [https://www.bioinf.jku.at/publications/older/2604.pdf](https://www.bioinf.jku.at/publications/older/2604.pdf)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**原始 LSTM 论文 Hochreiter 和 Schmidthuber**：[https://www.bioinf.jku.at/publications/older/2604.pdf](https://www.bioinf.jku.at/publications/older/2604.pdf)'
- en: Dissecting the LSTM
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解构 LSTM
- en: 'As mentioned, the LSTM architecture relies on a series of gates that can independently
    influence the activation values (*a^((t-1))*), as well as the memory (*c^(**^(t-1))*),
    from previous timesteps as information flows through an LSTM unit. These values
    are transformed as the unit spits out the activations (*a^t*) and memory (*c^t*)
    vectors pertaining to the current timestep at each iteration. While their earlier
    counterparts enter the unit separately, they are allowed to interact with each
    other in two broad manners. In the following diagram, the gates (denoted with
    the capital Greek letter gama, or Γ) represent sigmoid activation functions applied
    to the dot product of their respectively initialized weight matrix, with previous
    activations and current input:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，LSTM 架构依赖于一系列门，这些门可以独立地影响来自前一个时间步的激活值（*a^((t-1))*）以及记忆值（*c^(**^(t-1)*)）。这些值在信息流经
    LSTM 单元时被转化，最终在每次迭代中输出当前时间步的激活（*a^t*）和记忆（*c^t*）向量。虽然它们的早期版本分别进入单元，但它们允许以两种大致的方式相互作用。在下面的图示中，门（用大写希腊字母
    gama，或 Γ 表示）代表了对它们各自初始化的权重矩阵与先前激活和当前输入的点积应用的 sigmoid 激活函数：
- en: '![](img/669d8d78-7f7b-4489-99d3-711e16221bfb.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/669d8d78-7f7b-4489-99d3-711e16221bfb.png)'
- en: Comparing the closest known relative
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较最接近的已知亲戚
- en: Let's try to understand how an LSTM works by leveraging our pre-existing knowledge
    of the GRU architecture, which we saw in the last chapter. As we will soon discover,
    the LSTM is nothing but a more complex version of the GRU, albeit obeying similar
    principles that govern its operation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试通过运用我们之前对 GRU 架构的知识来理解 LSTM 是如何工作的，我们在上一章中已经看到过它。正如我们很快会发现的那样，LSTM 只是 GRU
    的一个更复杂版本，尽管它遵循了与 GRU 操作相同的基本原理。
- en: GRU memory
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GRU 记忆
- en: 'Recall that the GRU architecture computed its cell state (or memory) by leveraging
    two vectors through an update gate. These two vectors were activations from earlier
    timesteps (**c****t-1**), as well as a contender vector (**c ̴****t** ). The contender
    vector presents itself as a candidate for the current cell state, at each timestep.
    The activations, on the other hand, essentially represent the hidden state of
    the GRU from previous timesteps. The degree to which each of these two vectors
    influence the current cell state was determined by the update gate. This gate
    controlled the flow of information, allowing the memory cell to relevantly update
    itself with new representations to inform subsequent predictions. Using the update
    gate, we were able to calculate the new cell state at a given time step (**c^t**),
    as shown here:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 记得 GRU 架构是通过更新门利用两个向量来计算其单元状态（或记忆）的。这两个向量分别是来自先前时间步的激活（**c****t-1**），以及一个候选向量（**c
    ̴****t**）。候选向量在每个时间步表现为当前单元状态的候选者，而激活则代表了 GRU 从前一个时间步的隐藏状态。这两个向量对当前单元状态的影响程度由更新门决定。这个门控制信息流，允许记忆单元用新的表示来更新自身，从而为后续的预测提供相关的信息。通过使用更新门，我们能够计算出给定时间步的新单元状态（**c^t**），如下所示：
- en: '![](img/dab24003-383f-4f6b-bc4c-1dfbab4104b8.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dab24003-383f-4f6b-bc4c-1dfbab4104b8.png)'
- en: As we observe, the GRU used the update gate (**Γu**) and its inverse (**1- Γu**)
    to decide whether to update the memory cell with a new value (**c ̴****^t** )
    or conserve the old values from the previous timestep (**c****^(t-1)**). More
    importantly, the GRU leveraged a single update gate, along with its inverse value,
    to control the memory value (**c^t**). The LSTM architecture presents a more complex
    mechanism, and at the core uses an equation similar to the GRU architecture to
    maintain relevant state. But how exactly does it do this?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所观察到的，GRU 使用更新门（**Γu**）及其逆门（**1- Γu**）来决定是用新值（**c ̴****^t**）更新记忆单元，还是保留前一个时间步的旧值（**c****^(t-1)**）。更重要的是，GRU
    利用一个更新门及其逆值来控制记忆值（**c^t**）。LSTM 架构则提出了一种更复杂的机制，并且在核心部分使用与 GRU 架构类似的方程来维持相关状态。但它到底是如何做到的呢？
- en: LSTM memory cell
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM 记忆单元
- en: In the following diagram, you will notice the straight line at the top of the
    LSTM unit that denotes its memory or cell state (*c^t*). More technically, the
    cell state here is defined by the **Constant Error Carousel** (**CEC**), which
    is essentially a recurrently self-connected linear unit. This implementation is
    a core component of the LSTM layer that allows the enforcement of a constant flow
    of error during backpropagation. Essentially, this allows the mitigation of the
    vanishing gradient problem suffered by other RNNs.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图示中，您会注意到LSTM单元顶部的直线，它表示该单元的记忆或细胞状态（*c^t*）。更技术性地讲，细胞状态由**常数误差旋转环**（**CEC**）定义，它本质上是一个递归自连接的线性单元。这个实现是LSTM层的核心组件，使得在反向传播过程中能够强制执行恒定的误差流动。本质上，它允许缓解其他RNN所遭遇的梯度消失问题。
- en: The CEC prevents the error signals from decaying too quickly during backpropagation,
    allowing earlier representations to be well maintained and carried forward into
    future timesteps. It can be thought of as the information highway that lets this
    architecture learn to bridge time intervals in excess of 1,000 steps with relevant
    information. This has been shown to hold true in a variety of time series prediction
    tasks, effectively addressing problems faced by previous architectures, and dealing
    with noisy input data. While the exploding gradients issue can be addressed through
    gradient clipping (as we saw in the last chapter), the vanishing gradient problem
    is shown to be equally addressable by the CEC implementation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: CEC防止误差信号在反向传播过程中迅速衰减，从而使得早期的表示能够得到良好保持，并传递到未来的时间步。可以将其视为信息高速公路，使得这种架构能够学习在超过1,000步的时间间隔内传递相关信息。研究表明，这在各种时间序列预测任务中是有效的，能够有效解决以前架构面临的问题，并处理噪声输入数据。尽管通过梯度裁剪（如我们在上一章所见）可以解决梯度爆炸问题，但梯度消失问题同样可以通过CEC实现来解决。
- en: 'Now we have a high-level understanding of how the cell state is represented
    by the activation of the CEC. This activation (that is, *c^t*) is computed using
    inputs from several information gates. The use of different gates in the LSTM
    architecture permits it to control the error flow through the separate units,
    aiding in maintaining a relevant cell state (**c** for short):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对细胞状态如何通过CEC的激活来表示有了一个高层次的理解。这个激活（即 *c^t*）是通过多个信息门的输入来计算的。LSTM架构中不同门的使用使其能够控制通过各个单元的误差流动，从而帮助维持相关的细胞状态（简写为**c**）：
- en: '![](img/1a78c040-8e81-4b70-9dc0-7b8436506d55.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a78c040-8e81-4b70-9dc0-7b8436506d55.png)'
- en: Treating activations and memory separately
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将激活值和记忆单独处理
- en: 'Notice how both the short-term memory (*a**^(t-1)*) and the long-term memory
    (*c**^(t-1)*) are allowed to flow into the architecture separately. The memory
    from previous timesteps flows in through the top-left corner, while the activations
    from previous timesteps flows in from the bottom-left corner of the depicted illustration.
    This is the first key difference we may note from the GRU architecture that we
    are already familiar with. Doing so permits the LSTM to leverage both the short-term
    activations and the long-term memory (cell state) of our network, while computing
    the current memory (*c**^t*) and activations (*a**^t*). This dichotomous architecture
    aids in maintain a constant error flow through time, while letting relevant representations
    be carried forward to inform future predictions. An example of such predictions,
    in the case of **natural language processing** (**NLP**), could be identifying
    the presence of different genders or the fact that there are plural entities in
    a given sequence of words. Yet, what if we wanted to remember multiple things
    from a given sequence of words? What if we wanted to remember multiple facts about
    a subject in a given sequence over longer sets of sequences? Consider the case
    of machine question-answering, with the following two sentences:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意观察短期记忆（*a**^(t-1)*)和长期记忆（*c**^(t-1)*)是如何分别流入该架构的。来自前一时刻的记忆通过图示的左上角流入，而来自前一时刻的激活值则从左下角流入。这是我们从已经熟悉的GRU架构中能够注意到的第一个关键区别。这样做使得LSTM能够同时利用短期激活值和网络的长期记忆（细胞状态），同时计算当前记忆（*c**^t*）和激活值（*a**^t*）。这种二元结构有助于维持时间上的持续误差流动，同时让相关的表示被传递到未来的预测中。在**自然语言处理**（**NLP**）中，这样的预测可能是识别不同性别的存在，或者某个词序列中存在复数实体的事实。然而，如果我们希望从一个给定的词序列中记住多个信息呢？如果我们想在较长的词序列中记住一个主题的多个事实呢？考虑机器问答的情况，以下是两个句子：
- en: It had been several months since Napoleon was exiled to St. Helen. His spirit
    was already weak, his body feeble, yet it would be the arsenic, from the damp
    mold forming on the pale green wallpaper around his room, that would slowly lead
    to his demise.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拿破仑被流放到圣赫勒拿岛已经有几个月了。他的精神已经衰弱，身体虚弱，但正是从他房间四周苍白绿色墙纸上滋生的潮湿霉菌中的砒霜，慢慢导致了他的死亡。
- en: Where was Napoleon? How did Napoleon die?
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拿破仑在哪里？拿破仑是如何去世的？
- en: LSTM memory block
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM记忆块
- en: To be able to answer these questions, our network must have several memory cells,
    where each can store quasi-dependent bits of information regarding the subject
    of our enquiry, the French emperor Napoleon Bonaparte. In practice, an LSTM unit
    can have multiple memory cells, each storing different representations from the
    input sequence. One may store the gender of the subject, another may store the
    fact that there are multiple subjects, and so on. For the purpose of having clear
    illustrations, we have taken the liberty of depicting only one memory cell per
    diagram in this chapter. We do this because understanding the principle behind
    the workings of one cell will suffice to extrapolate the functioning of a memory
    block with multiple memory cells. The part of the LSTM that contains all its memory
    cells is referred to as a memory block. The adaptive information gates of the
    architecture are shared by all cells in the memory block, and serve to control
    the flow of information between the short-term activations (*a^(t-1)*), current
    inputs (*X^t*), and the long-term state (*c**^t*) of the LSTM.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够回答这些问题，我们的网络必须有多个记忆单元，每个单元可以存储与我们研究对象——法国皇帝拿破仑·波拿巴——相关的准依赖信息。实际上，一个LSTM单元可以有多个记忆单元，每个单元存储输入序列中的不同表示。一个可能存储主题的性别，另一个可能存储有多个主题的事实，依此类推。为了清晰地展示，我们在本章中只描绘了每个图示中的一个记忆单元。我们这么做是因为理解一个单元的工作原理足以推断出一个包含多个记忆单元的记忆块的工作方式。LSTM中包含所有记忆单元的部分被称为记忆块。架构的自适应信息门控由记忆块中的所有单元共享，并用于控制短期激活值（*a^(t-1)*）、当前输入（*X^t*）和LSTM的长期状态（*c**^t*）之间的信息流动。
- en: Importance of the forget gate
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 忘记门的重要性
- en: 'As we noted, the equation defining the memory cell''s state (*c**^t*) of an
    LSTM is similar in spirit to the one of the GRU. A key difference, however, is
    that it leverages a new gate (Γf), namely the forget gate, along with the update
    gate, to decide whether to forget the value stored at previous timesteps (*c**^(t-1)*)
    or include it in the computation of the new cell memory. The following formula
    depicts the CEC responsible for conserving the cell state of our LSTM. It is the
    very formula that makes LSTMs so effective at remembering long-term dependencies.
    As mentioned earlier, the CEC is a neuron specific to each memory cell in an LSTM
    that defines the cell state at any given time. We will start with how the LSTM
    unit computes the value (**C^t**) that refers to what is stored in its memory
    cell (**C**) at time (**t**):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所注意到的，定义LSTM记忆单元状态（*c**^t*）的方程与GRU的状态方程在本质上是相似的。然而，一个关键的区别是，LSTM利用了一个新的门（Γf），即遗忘门，以及更新门来决定是否忘记在前一个时间步存储的值（*c**^(t-1)*），或者将其包含在新单元记忆的计算中。以下公式描述了负责保持我们LSTM单元状态的CEC（记忆单元控制单元）。它正是让LSTM能够有效记住长期依赖关系的公式。如前所述，CEC是每个LSTM记忆单元特有的神经元，定义了在任何给定时间的单元状态。我们将从LSTM单元如何计算它的记忆单元中存储的值（**C^t**）开始：
- en: '![](img/4a1b073e-3a2a-46fc-80af-c981b0c76cfe.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a1b073e-3a2a-46fc-80af-c981b0c76cfe.png)'
- en: This lets us incorporate information from both the contender value (*c ^̴**^t*)
    and the memory at the previous timestep (*c^(t-1)*) to the current memory value.
    As we will soon see, this forget gate is nothing but a sigmoid applied to matrix-level
    dot products along with a bias term that helps us control the flow of information
    from previous timesteps.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得我们可以将来自候选值(*c ^̴**^t*)和前一个时间步的记忆值(*c^(t-1)*)的信息，结合到当前的记忆值中。正如我们很快会看到的，这个遗忘门其实就是一个对矩阵级别的点积应用sigmoid激活函数，并加上一个偏置项，帮助我们控制从前一个时间步传递过来的信息流。
- en: Conceptualizing the difference
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概念化差异
- en: It is worth noting that the forget gate represents an important conceptual difference
    in maintaining the cell state when compared to the mechanism employed in the GRU
    architecture to achieve similar ends. One way to think about this gate is that
    it allows us to control how much of the previous cell state (or memory) should
    influence the current cell state. In the case of the GRU architecture, we simply
    exposed either the entire memory from previous timesteps, or just the new contender
    value, seldom making a compromise between the two.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，遗忘门在保持单元状态方面与GRU架构所采用的机制存在一个重要的概念性区别，它们的目标是实现相似的效果。可以这样理解：遗忘门允许我们控制前一个单元状态（或记忆）在多大程度上影响当前的单元状态。而在GRU架构中，我们只是简单地暴露前一个时间步的全部记忆，或者只是新的候选值，很少在两者之间做出妥协。
- en: 'GRU cell state calculation is as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: GRU单元状态的计算如下：
- en: '![](img/b8e8a603-503a-45e1-a0b8-ec11a26fc563.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b8e8a603-503a-45e1-a0b8-ec11a26fc563.png)'
- en: 'This binary trade-off between exposing the entire memory or a new contender
    value can actually be avoided, as is the case with the LSTM architecture. This
    is achieved by using two separate gates, each with its own learnable weight matrix,
    to control the cell state of our LSTM. LSTM cell state computation is as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种在暴露整个记忆和新的候选值之间的二元权衡实际上是可以避免的，正如LSTM架构所展示的那样。通过使用两个独立的门，每个门都有自己可学习的权重矩阵，来控制我们LSTM的单元状态，从而实现这一点。LSTM单元状态的计算如下：
- en: '![](img/57675529-9aea-4c31-8d51-066a15df3631.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/57675529-9aea-4c31-8d51-066a15df3631.png)'
- en: Walking through the LSTM
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 走进LSTM
- en: 'So, let''s have a closer look at the entire set of equations that describe
    the LSTM architecture. The first set of gates that we will examine are the forget
    gate and the update gate. Unlike the GRU, the LSTM uses both these gates to determine
    the memory values (*c^t*) at each timestep:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们仔细看一下描述LSTM架构的整个方程组。我们将首先研究的门是遗忘门和更新门。与GRU不同，LSTM使用这两个门来确定每个时间步的记忆值(*c^t*)：
- en: '![](img/049a8aaa-21cf-4b1d-aeee-5650ef72f1a9.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/049a8aaa-21cf-4b1d-aeee-5650ef72f1a9.png)'
- en: 'First, let''s see how these gates themselves are computed. The following formulations
    reveal to us that these gates are simply the result of a sigmoid function being
    applied to the dot products of previous activations, and current inputs, with
    respective weight matrices (*Wf* and *Wu* for the forget and output gates):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看这些门是如何计算的。以下公式表明，这些门实际上只是将前一时刻的激活值与当前输入的点积，通过对应的权重矩阵（*Wf*和*Wu*分别用于遗忘门和输出门），再应用sigmoid函数的结果：
- en: '*Forget gate (ΓF) = sigmoid ( Wf [ at-1, ![](img/0e51b3c3-3c8a-4270-aa33-e3bd678717f4.png)
    t ] + bF)*'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*遗忘门 (ΓF) = sigmoid ( Wf [ at-1, ![](img/0e51b3c3-3c8a-4270-aa33-e3bd678717f4.png)
    t ] + bF)*'
- en: '*Update gate (ΓU) = sigmoid ( Wu [ at-1, ![](img/39ad1f6b-908f-4e4a-acf2-6257ced8f68e.png)
    t ] + bu)*'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*更新门 (ΓU) = sigmoid ( Wu [ at-1, ![](img/39ad1f6b-908f-4e4a-acf2-6257ced8f68e.png)
    t ] + bu)*'
- en: '![](img/c7c64f11-8f75-4c4f-98ae-f5b2f0bbf27c.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7c64f11-8f75-4c4f-98ae-f5b2f0bbf27c.png)'
- en: Visualizing the flow of information
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化信息流
- en: The two vectors (*a**^(t-1)* and ![](img/5c963b23-c24f-448e-ae07-1e67f1f82c77.png)
    t, respectively) enter the LSTM unit from the bottom-left corner, and are copied
    to each gate (ΓF and ΓU) upon their arrival. Then, they are each multiplied with
    the weight matrix of the respective gate, before a sigmoid is applied to their
    dot products, and a bias term. As we know, the sigmoid is famous for compressing
    its input between the range of zero and one, so each gate holds a value between
    this range. Importantly, each weight matrix is unique to a given gate (*Wf* for
    the forget gate, or *Wu* for the update gate). The weight matrices (*Wf* and *Wu*)
    represent a subset of the learnable parameters within an LSTM unit, and are updated
    iteratively during the backpropagation procedure, just as we have been doing all
    along.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个向量 (*a**^(t-1)* 和 ![](img/5c963b23-c24f-448e-ae07-1e67f1f82c77.png) t) 分别从LSTM单元的左下角进入，并在到达时被复制到每个门（ΓF和ΓU）。然后，它们分别与各自门的权重矩阵相乘，再对它们的点积应用sigmoid，并加上偏置项。正如我们所知，sigmoid函数以其将输入压缩到零和一之间而闻名，因此每个门的值都在这个范围内。重要的是，每个权重矩阵是特定于给定门的（*Wf*用于遗忘门，*Wu*用于更新门）。权重矩阵（*Wf*和*Wu*）代表LSTM单元中的一部分可学习参数，并在反向传播过程中迭代更新，就像我们一直在做的那样。
- en: Computing cell state
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算单元状态
- en: 'Now that we know what both gates (update and forget) represent, and how they
    are computed, we can move on to understand how they influence our LSTM''s memory
    (or state) at a given timestep. Please do take another moment to note the different
    information pathways flowing towards and away from the gates.  The inputs, entering
    from the left hand side of the cell, are transformed and propagated forward until
    they reach the end of the LSTM unit, on the right hand side of the illustration
    provided here:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了两个门（更新门和遗忘门）分别代表什么，它们是如何计算的，我们可以继续理解它们如何在给定时间步影响我们LSTM的记忆（或状态）。请再次注意流向和流出门的不同信息路径。输入从单元格的左侧进入，经过转换并传播，直到它们到达LSTM单元的右侧，如下图所示：
- en: '![](img/52c932fa-8c88-435f-8e96-5f5a4d47b4d9.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52c932fa-8c88-435f-8e96-5f5a4d47b4d9.png)'
- en: 'As we saw, the forget gate (*ΓF*) is used to, quite literally, forget the memory
    values from previous timesteps. Similarly, the update gate (*Γu *) is used to
    determine whether or not to allow the potential contender values of (*c ^(̴t)*)
    to be incorporated at the given timestep. Both these gates are, in conjunction,
    responsible for conserving the state of our LSTM memory (*c**^t*) at a given timestep.
    Mathematically, this translates to the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，遗忘门（*ΓF*）的作用，字面上就是忘记来自前一个时间步的记忆值。同样，更新门（*Γu*）决定是否允许将潜在的候选值（*c ^(̴t)*)
    纳入当前时间步。这两个门共同负责在给定时间步保留我们LSTM记忆的状态（*c**^t*）。在数学上，这可以转化为以下公式：
- en: '*Current memory value* *(c^t*** ) =* ( Γu * c ^(̴t) ) + (ΓF * c^(t-1) )*'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*当前记忆值* *(c^t*) = ( Γu * c ^(̴t) ) + (ΓF * c^(t-1) )*'
- en: As we mentioned, each gate essentially represents a value between zero and one,
    since we squished our values through the non-linear sigmoid. We know that most
    values tend to be either very close to zero or to one given the operating range
    of the sigmoid, hence we can imagine the gates as binary values. This is useful,
    as we can visualize these gates as being open (one) for information to flow through,
    or closed (zero). Any value in between would let some information in, but not
    the entirety of it.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们提到的，每个门本质上表示一个介于零和一之间的值，因为我们通过非线性sigmoid函数将值压缩。我们知道，由于sigmoid的工作范围，大多数值往往非常接近零或接近一，因此我们可以将这些门看作是二进制值。这是有用的，因为我们可以将这些门想象成打开（1）让信息流通，或者关闭（0）。介于零和一之间的任何值都能让部分信息流入，但并不是全部。
- en: So, now we understand how the values of these gates are computed, as well as
    how they are used to control the degree of influence that either the contender
    value (*c ^̴**^t*) or the previous memory state (*c**^(t-1)*) should have on the
    computation of the current state (*c**^t*). The state of an LSTM memory (*c**^t*)
    is defined by the straight line at the top of the previously shown LSTM illustration.
    In practice, this straight line (that is, the constant error carousel)  is very
    good at conserving relevant information and carrying it forward to future timesteps,
    to assist with predictions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们理解了这些门值是如何计算的，以及它们如何控制候选者值(*c ^̴**^t*)或前一个记忆状态(*c**^(t-1)*)在当前状态计算中应具有的影响程度。LSTM
    记忆的状态(*c**^t*)由之前展示的 LSTM 图中顶部的直线定义。实际上，这条直线（即常数误差环）非常擅长保持相关信息并将其传递到未来的时间步，以协助预测。
- en: Computing contender memory
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算候选者记忆
- en: We now know how the memory at time (*t*) is calculated, but how about the contender
    (*c ^(̴t)*) itself? After all, it is partially responsible for maintaining a relevant
    state of memory, characterized by possibly useful representations occurring at
    each timestep.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道了如何计算时间点(*t*)的记忆，但那么候选者(*c ^(̴t)*)本身呢？毕竟，它在维护相关的记忆状态方面起着部分作用，特点是每个时间步出现的可能有用的表示。
- en: 'This is the same idea that we saw in the GRU unit, where we allow the possibility
    for memory values to be updated using a contender value at each timestep. Earlier,
    with the GRU, we used a relevance gate that helped us compute it for the GRU.
    However, that is not necessary in the case of the LSTM, and we get a much simpler
    and arguably more elegant formulation as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在 GRU 单元中看到的想法相同，在那里我们允许在每个时间步使用候选者值更新记忆值。早些时候，在 GRU 中，我们使用了一个相关性门来帮助我们为
    GRU 计算它。然而，在 LSTM 的情况下，这是不必要的，我们得到了一个更加简单且可以说更优雅的公式，如下所示：
- en: '*Contender memory value (c ^(̴t) ) = tanh ( Wc [ a^(t-1), ![](img/07d7a599-b99a-4933-90bb-634bca4e98af.png)
    t ] + bc)*'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*候选者记忆值 (c ^(̴t)) = tanh ( Wc [ a^(t-1), ![](img/07d7a599-b99a-4933-90bb-634bca4e98af.png)
    t ] + bc)*'
- en: 'Here, *Wc* is a weight matrix that is initialized at the beginning of a training
    session, and iteratively updated as the network trains. The dot product of this
    matrix, with the previous activations (*a^(*t*-1)*) and current inputs (*x^t*),
    along with a bias term (*bc*), are passed through a tanh activation function to
    arrive to the contender value (*c* *^̴^t*).  This contender vector is then multiplied
    (element-wise) with the value of the update gate that we saw form a part of the
    memory state (*c**^t*) at the current time. In the next diagram, we illustrate
    the computation of the contender memory vector, and show how the information is
    carried forward to influence the final state of the memory cell (*c**t*):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*Wc* 是一个权重矩阵，在训练开始时初始化，并随着网络训练而迭代更新。这个矩阵与前一时刻的激活值(*a^(*t*-1)*)和当前输入(*x^t*)的点积，再加上偏置项(*bc*)，通过
    tanh 激活函数得出候选者值(*c* *^̴^t*)。然后，这个候选者向量与我们在当前时间看到的内存状态(*c**^t*)的更新门值进行逐元素相乘。在下图中，我们说明了候选者记忆向量的计算，并展示了如何将信息传递到下一个时间步，影响最终的记忆单元状态(*c**t*)：
- en: '![](img/1c2024ce-67fd-4906-8a94-2f324c3efd1a.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c2024ce-67fd-4906-8a94-2f324c3efd1a.png)'
- en: Do recall that the tanh activation function effectively compresses its outputs
    between -1 and 1, hence the values of the contender vector (*c ^(̴t)*) will always
    appear within this range. Now we understand how to compute an LSTMs cell state
    (or memory) at a given timestep. We also learned how the contender value is computed
    before it is regulated by the update gate and passed forward into the computation
    of the current memory, (*c**^t*).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 请记得，tanh 激活函数有效地将输出压缩到 -1 和 1 之间，因此候选者向量(*c ^(̴t)*)的值总是出现在这个范围内。现在我们理解了如何计算
    LSTM 的单元状态（或记忆）在给定时间步的值。我们还了解了在更新门调整之前，候选者值是如何计算的，然后传递到当前记忆的计算中，(*c**^t*)。
- en: Computing activations per timestep
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算每个时间步的激活值
- en: 'As we previously pointed out in the LSTM architecture, it is fed the memory
    and activation values from the previous timestep separately. This is distinctly
    separate from the assumption we made with the GRU unit, where *a**t = ct*. This
    dual manner of data processing is what lets us conserve relevant representations
    in memory across very long sequences, potentially even 1,000 timesteps! The activations
    are, however, always functionally related to the memory (*c^t*) at each time step.
    So, we can compute the activations at a given timestep by first applying a tanh function
    to the memory (*c^t*), then performing an element-wise computation of the result
    with the output gate value (Γo). Note that we do not initialize a weight matrix
    at this step, but simply apply tanh to each element in the (*c^t*) vector. This
    can be mathematically represented as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前在LSTM架构中指出的，它分别接收来自前一时间步的记忆和激活值。这与我们在GRU单元中做出的假设不同，在GRU中我们有 *a^t = ct*。这种双重数据处理方式使得我们能够在很长的序列中保留相关的表示，甚至可能达到1,000个时间步！然而，激活值始终与每个时间步的记忆（*c^t*）功能相关。因此，我们可以通过首先对记忆（*c^t*）应用tanh函数，然后将结果与输出门值（Γo）进行逐元素计算，来计算某个时间步的激活值。请注意，在这一步我们并不初始化权重矩阵，而只是对（*c^t*）向量中的每个元素应用tanh函数。数学表达式如下：
- en: '*Current activations (a^t ) = Γo * tanh(c^t)*'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*当前激活值 (a^t ) = Γo * tanh(c^t)*'
- en: '![](img/f9dbdd70-4dcf-4421-9c98-f56cf1e2876b.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9dbdd70-4dcf-4421-9c98-f56cf1e2876b.png)'
- en: 'Here, the output gate is nothing but another sigmoid, applied to a dot product
    of a learnable weight matrix, with the activations from previous timesteps and
    the input at the current time as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，输出门不过是另一个sigmoid函数，应用于一个可学习的权重矩阵的点积，其中包含来自前一时间步的激活值和当前时刻的输入，具体如下：
- en: '*Output gate (Γo) = sigmoid ( Wo [ a^(t-1), ![](img/623ea13b-345c-4274-8caf-eba9d43163bc.png)
    t ] + bo)*'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出门 (Γo) = sigmoid ( Wo [ a^(t-1), ![](img/623ea13b-345c-4274-8caf-eba9d43163bc.png)
    t ] + bo)*'
- en: 'Each of the weight matrices (*Wf*, *Wu*, *Wc*, and *Wo*) that exist for each
    separate gate (forget, update, contender, and output, respectively) can be considered
    the learnable parameters of the LSTM unit, and are iteratively updated during
    the training process. In the diagram provided here, we can observe each of these
    weight matrices, as it moulds the inputs entering their respective gates, before
    passing the result along to other sections of the architecture:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 存在于每个单独门（分别为遗忘门、更新门、候选门和输出门）的权重矩阵（*Wf*, *Wu*, *Wc*, 和 *Wo*）可以被视为LSTM单元的可学习参数，并在训练过程中不断更新。在这里提供的图示中，我们可以观察到每个权重矩阵是如何塑造进入各自门的输入，随后将结果传递到架构的其他部分：
- en: '![](img/bc1c2d48-1925-4cba-af50-f80f56351588.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc1c2d48-1925-4cba-af50-f80f56351588.png)'
- en: Variations of LSTM and performance
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM的变种与性能
- en: You already saw a variation of the LSTM, namely the GRU. We have, extensively
    discussed how these two architectures differ. There are other variations that
    also exist and are quite noteworthy. One of these is the LSTM variation, that
    includes something known as a **peephole connections**. These connections permit
    information to flow from the cell state all the way back to the information gates
    (forget, update, and output). This simply lets our LSTM gates peek at the memory
    values from previous timesteps while it computes the current gate values at the
    current time.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到了LSTM的一个变种，即GRU。我们已经广泛讨论了这两种架构的不同。还有其他一些变种同样值得注意。其中之一是LSTM的变种，它包含了被称为**窥视连接**（peephole
    connections）的东西。这些连接允许信息从细胞状态流回到信息门（遗忘门、更新门和输出门）。这使得我们的LSTM门在计算当前时间的门值时，可以“窥视”来自前一时间步的记忆值。
- en: Understanding peephole connections
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解窥视连接（peephole connections）
- en: The point behind peephole connections is the need to capture the information
    of time lags. In other words, we wish to include the information conveyed by time
    intervals between sub-patterns of sequences in our modeling efforts. This is relevant
    not only for certain language processing tasks (such as *speech recognition*),
    but also for numerous other tasks ranging from machine motor control to maintaining
    elaborate rhythms in computer-generated music. Previous approaches to tasks such
    as speech recognition employed the use of **Hidden Markov Models** (**HMMs**).
    These are essentially statistical models that estimate the probability of a set
    of observations based on the sequence of hidden state transitions. In the case
    of speech processing, observations are defined as segments of digital signals
    corresponding to speech, while Markov hidden states are the sequences of phonemes
    that we are looking to recognize as words. As you will notice, nowhere in this
    model are we able to incorporate the delay between phonemes to see whether a given
    digital signal corresponds to a certain word. This information is typically discarded
    in HMMs, yet can be of paramount importance for us in determining whether we have
    heard sentence *I want to open my storage unit before...* or *I want to open my
    storage unit, B-4*. In these examples, the delay between the phonemes could well
    distinguish the detection of either *B-4*, or *before*. While the HMM is beyond
    the scope of this chapter, it helps us understand the how the LSTM overcomes previous
    modeling limitations by leveraging delays between time sequences.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 窥视孔连接的核心思想是捕捉时间延迟信息。换句话说，我们希望在建模过程中包括序列子模式之间时间间隔传递的信息。这不仅对于某些语言处理任务（如*语音识别*）相关，而且对于从机器运动控制到计算机生成音乐中保持复杂节奏的其他众多任务也非常重要。以前处理语音识别等任务的方法使用了**隐马尔可夫模型**（**HMMs**）。这些本质上是统计模型，基于隐藏状态转移序列估计一组观察值的概率。在语音处理的例子中，观察值被定义为对应语音的数字信号片段，而马尔可夫隐藏状态则是我们希望识别为单词的音素序列。如你所见，这个模型中并未考虑音素之间的延迟，无法判断某一数字信号是否对应某个特定的单词。这些信息在HMM中通常会被丢弃，但在我们判断是听到句子*I
    want to open my storage unit before...*还是*I want to open my storage unit, B-4*时，延迟信息可能至关重要。在这些例子中，音素之间的延迟很可能区分出*B-4*和*before*。虽然HMM超出了本章讨论的范围，但它帮助我们理解了LSTM如何通过利用时间序列之间的延迟，克服了以往模型的局限。
- en: 'You can see the peephole paper at: [ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf](ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下链接查看窥视孔论文：[ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf](ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf)：
- en: '![](img/55abc71a-14fc-4aa2-ba7e-629c00ee0ab2.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55abc71a-14fc-4aa2-ba7e-629c00ee0ab2.png)'
- en: Do note that the peephole modification can be made to either gate. You may choose
    to implement this for all gates, or just a subset thereof.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，窥视孔修改可以应用于任意一个门。你可以选择对所有门实施该修改，或者仅对其中的一部分实施。
- en: 'The following equations demonstrate the computations performed to obtain the
    respective gate values when a peephole connection is added to include the previous
    cell states:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方程展示了在添加窥视孔连接以包含前一单元状态时，计算各门值时执行的计算：
- en: '*Forget gate (ΓF) = sigmoid ( Wf [ c^(t-1) , a^(t-1), ![](img/8b9f0d7e-71a2-40b8-942d-5b72c2a49b42.png)
    t ] + bF)*'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*遗忘门 (ΓF) = sigmoid ( Wf [ c^(t-1) , a^(t-1), ![](img/8b9f0d7e-71a2-40b8-942d-5b72c2a49b42.png)
    t ] + bF)*'
- en: '*Update gate (ΓU) = sigmoid ( Wu [ c^(t-1) , a^(t-1), ![](img/6594fc16-f906-42e3-b893-ef25808cb580.png)
    t ] + bu)*'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*更新门 (ΓU) = sigmoid ( Wu [ c^(t-1) , a^(t-1), ![](img/6594fc16-f906-42e3-b893-ef25808cb580.png)
    t ] + bu)*'
- en: '*Output gate (Γo) = sigmoid ( Wo [c^(t-1) , a^(t-1), ![](img/70de5906-85ac-4d0d-9b26-9c8fae3b5457.png)
    t ] + bo)*'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出门 (Γo) = sigmoid ( Wo [c^(t-1) , a^(t-1), ![](img/70de5906-85ac-4d0d-9b26-9c8fae3b5457.png)
    t ] + bo)*'
- en: So, the peephole modification mathematically boils down to performing an additional
    matrix-level multiplication in the computation of a given gate value. In other
    words, the value of a gate now can accommodate the previous cell state by computing
    its dot product with the weight matrix of the given gate. Then, the resulting
    dot product is summed up along with the first two dot products and the bias term,
    before being all squished through a sigmoid function.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，窥视孔修改在数学上简化为在计算给定门值时执行额外的矩阵级别乘法。换句话说，门的值现在可以通过与给定门的权重矩阵计算点积来容纳前一单元状态。然后，得到的点积与前两个点积及偏置项一起求和，再通过sigmoid函数进行处理。
- en: Importance of timing and counting
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时序和计数的重要性
- en: 'Let''s solidify the idea of using time-interval, dependent information to inform
    sequential predictions with another conceptual example, where such information
    is considered crucial for accurate predictions. Consider how a human drummer,
    for example, must execute a precise sequence of motor commands corresponding to
    a precise flow of rhythm. They time their actions and count their progressions
    in a sequentially dependent order. Here, the information representing patterns
    of generated sequences is, at least partially, conveyed through the time delays
    between these respective events. Naturally, we would be interested in artificially
    replicating the sophisticated sequence modeling task occurring in such interactions.
    In theory, we could even use such an approach to sample novel rhyming schemes
    from computer-generated poetry, or create robot athletes capable of competing
    alongside humans at future iterations of the Olympic games (for whatever reasons
    we collectively decide that this would be a good idea). If you wish to further
    research the topic of how peephole connections may be used to augment predictions
    over complex time-delayed sequences, we encourage you to read the original LSTM
    peephole modification paper, given here:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过另一个概念性例子，进一步巩固使用时间间隔相关信息来指导顺序预测的理念，在这个例子中，这类信息被认为对于准确预测至关重要。举个例子，考虑一个人类鼓手，必须执行一系列精确的运动指令，这些指令对应着精确的节奏流。鼓手根据时间来安排他们的动作，并按顺序依赖地计数他们的进度。在这里，代表生成序列模式的信息，至少部分地，是通过这些事件之间的时间延迟来传达的。自然，我们会有兴趣人工复制这种复杂的序列建模任务，这种任务在这些互动中发生。从理论上讲，我们甚至可以利用这种方法从计算机生成的诗歌中提取新的押韵模式，或者创造可以在未来的奥运会中与人类竞争的机器人运动员（无论我们集体决定出于什么理由认为这是个好主意）。如果你希望进一步研究如何通过窥视连接来增强对复杂时间延迟序列的预测，我们鼓励你阅读原始的LSTM窥视点修改论文，链接如下：
- en: '[http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf](http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf](http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf)'
- en: Exploring other architectural variations
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索其他架构变体
- en: 'Many other variations of RNNs exist besides the ones addressed in this book
    (see *Depth Gated RNNs* by *Yao et al*, 2015; or *Clockwork RNNs* by *Koutnik
    et al.* 2014). Each of these can be suitable in an array of niche tasks—the general
    consensus is that LSTMs excel at most time series prediction tasks, and can be
    considerably modified to suit most common and more complex use cases. In fact,
    as further reading, we recommend an excellent article (*LSTM: A Search Space Odyssey*,
    2017: [https://arxiv.org/abs/1503.04069](https://arxiv.org/abs/1503.04069)) that
    compares the performance of different variations of LSTMs at various tasks, such
    as speech recognition and language modeling. Due to the fact that it used approximately
    15 years of GPU time to conduct their experiments, this study is a one-of-a-kind
    exploratory resource for researchers wanting to better understand different LSTM
    architectural considerations and their effects when modeling sequential data.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 除了本书中涉及的RNN变体外，还有许多其他RNN变体（参见*深度门控RNNs*，*姚等人*，2015；或*时钟RNNs*，*Koutnik等人*，2014）。这些变体每个都适合在特定任务中使用——普遍的共识是LSTM在大多数时间序列预测任务中表现出色，并且可以相当修改以适应大多数常见和更复杂的使用案例。事实上，作为进一步阅读，我们推荐一篇优秀的文章（*LSTM：一次搜索空间奥德赛*，2017：[https://arxiv.org/abs/1503.04069](https://arxiv.org/abs/1503.04069)），该文章比较了不同LSTM变体在多种任务中的表现，如语音识别和语言建模。由于该研究使用了大约15年的GPU时间来进行实验，因此它成为了一项独特的探索性资源，供研究人员更好地理解不同LSTM架构的考虑因素及其在建模顺序数据时的效果。
- en: Putting our knowledge to use
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运用我们的知识
- en: Now that we have achieved a good understanding of how an LSTM works and what
    kind of tasks they particularly tend to excel at, it is time to implement a real-world
    example. Of course, time series data can appear in a vast array of settings, ranging
    from sensor data from industrial machinery to spectrometric data representing
    light arriving from distant stars. Today, however, we will simulate a more common,
    yet notorious, use case. We will implement an LSTM to predict the movement of
    stock prices. For this purpose, we will employ the Standard & Poor (S&P) 500 dataset,
    and select a random stock to prepare for sequential modeling. The dataset can
    be found on Kaggle, and comprises historical stock prices (opening, high, low,
    and closing prices) for all current S&P 500 large capital companies traded on
    the American stock market.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经充分理解了LSTM的工作原理，以及它在特定任务中尤为擅长的方面，是时候实施一个真实世界的例子了。当然，时间序列数据可以出现在各种场景中，从工业机器的传感器数据到表示来自遥远星辰的光谱数据。然而，今天我们将模拟一个更常见但臭名昭著的用例。我们将使用LSTM来预测股价的波动。为此，我们将使用标准普尔（S&P）500数据集，并随机选择一只股票准备进行序列建模。该数据集可以在Kaggle上找到，包含了所有当前S&P
    500大市值公司在美国股市交易的历史股价（开盘价、最高价、最低价和收盘价）。
- en: On modeling stock market data
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于建模股市数据
- en: Before moving forward, we must remind ourselves about the inherent stochasticity
    that lies embedded in market trends. Perhaps you are more of an efficient market
    hypothesis type of a person than an irrational market type. Whatever may be your
    personal convictions on the inner logic motivating stock movements, the reality
    of the matter is that there is a lot of randomness that often escapes even the
    most predictive of models. Investor behavior is hard to foresee, as investors
    tend to capitalize for various motives. Even general trends can be deceptive,
    as proven most recently by the Bitcoin asset bubble toward the end of 2017; many
    other examples exist (the 2008 global crisis, post-unrest inflation in Zimbabwe,
    the 1970s oil crisis, post-WWI Germany, the tulip mania during the Dutch golden
    age, and so forth, all the way back to antiquity).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们必须提醒自己，市场趋势中蕴含着固有的随机性。也许你更倾向于相信有效市场假说，而不是非理性市场理论。无论你个人对股票波动背后的内在逻辑持何种信念，现实是，市场中有大量的随机性，常常连最具预测性的模型也无法捕捉。投资者行为难以预见，因为投资者往往出于不同的动机进行操作。即使是一般的趋势也可能具有欺骗性，正如最近比特币资产泡沫在2017年底的崩溃所证明的那样；还有许多其他例子（2008年全球危机、津巴布韦的战后通货膨胀、1970年代的石油危机、一战后德国的经济困境、荷兰黄金时代的郁金香狂热，等等，甚至可以追溯到古代）。
- en: In fact, many economists have been quoted on the seemingly inherent randomness
    involved in stock market movements. Princeton University economist Burton Malkiel
    drove home this point almost half a century ago, in his book titled *A Random
    Walk Down Wall Street*. However, just because we can't get a perfect predictive
    score does not mean we cannot attempt to steer our guesses into the metaphorical
    ballpark. In other words, such sequence modeling efforts may still be of use in
    predicting the general trend of movements in the market for the near future. So,
    let's import our data and have a look at what we are dealing with here without
    much further ado. Please do feel free to follow along with your own market data,
    or with the same dataset as we use, which you can find at: [https://www.kaggle.com/camnugent/sandp500](https://www.kaggle.com/camnugent/sandp500).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，许多经济学家曾引用股市波动中似乎固有的随机性。普林斯顿大学经济学家伯顿·马尔基尔在近半个世纪前的著作《*华尔街的随机漫步*》中强调了这一点。然而，仅仅因为我们无法获得完美的预测结果，并不意味着我们不能尝试将我们的猜测引导到比喻上的“正确方向”。换句话说，这种序列建模的尝试在预测市场短期内的整体趋势时，仍然可能是有用的。那么，我们现在就导入数据，看看我们在这里处理的是什么内容，不再赘述。请随时跟随您的市场数据，或者使用我们所用的相同数据集，您可以在以下网址找到： [https://www.kaggle.com/camnugent/sandp500](https://www.kaggle.com/camnugent/sandp500)。
- en: Importing the data
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入数据
- en: 'The data is stored in **Comma Separated Value** (**CSV**) files, and can be
    imported by the pandas CSV reader. We will also import the standard NumPy and
    Matplotlib libraries, along with the `MinMaxScaler` library from sklearn, to be
    able to reshape and plot out and normalize our data when the time is right, as
    shown in the following code:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储在**逗号分隔值**（**CSV**）文件中，可以通过pandas的CSV读取器导入。我们还将导入标准的NumPy和Matplotlib库，并使用来自sklearn的`MinMaxScaler`库，以便在合适的时候重塑、绘制和归一化我们的数据，如以下代码所示：
- en: '[PRE0]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We get the output as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的输出如下：
- en: '![](img/ba7364d1-4b01-48be-9ee8-a972145e560f.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba7364d1-4b01-48be-9ee8-a972145e560f.png)'
- en: Sorting and visualizing the trend
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 排序并可视化趋势
- en: 'First, we will select a random stock out of the 505 different stocks in our
    dataset. You may choose any of them to repeat this experiment with. We will also
    sort our DataFrame by date, since we deal with a time series prediction problem
    where the order of the sequence is of paramount importance to the predictive value
    of our task. Then we may proceed to visually display our data by plotting out
    the high and low prices (on a given day) in sequential order of occurrence. This
    helps us visualize the general trend of stock prices for the American airlines
    group (ticker name: `AAL`), over the period of five years (2013-2017) as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从数据集中505只不同的股票中随机选择一只。你可以选择任何一只来重复进行这个实验。我们还将按日期对DataFrame进行排序，因为我们处理的是时间序列预测问题，在这种问题中，序列的顺序对于预测任务的价值至关重要。接着，我们可以通过按顺序绘制高价和低价（某一天）的走势图来可视化我们的数据。这有助于我们直观地查看美国航空集团（股票代码：`AAL`）在五年期（2013-2017年）内的股票价格走势，如下所示：
- en: '[PRE1]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](img/5866d5f8-356c-48c1-ab7b-5f06f5d40d8c.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5866d5f8-356c-48c1-ab7b-5f06f5d40d8c.png)'
- en: From DataFrame to tensor
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从DataFrame到张量
- en: 'We observe that, while slightly different from one another, the high and low
    prices both clearly follow the same pattern. Hence, it would be redundant to use
    both these variables for predictive modeling, as they are highly correlated. We
    could, of course, pick just one out of the two, but we could also take some sort
    of average between the two price indicators on any given market day. We will convert
    the columns containing the high and low prices of a given observation day into
    NumPy arrays. We do so by calling values on the respective columns, which returns
    a NumPy representation of each column. Then, we can use each of these newly defined
    columns to compute a third NumPy array that stores the mid-price values (calculated
    as *(high + low) /2)* of all the given observations as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，尽管高价和低价略有不同，但它们的走势显然遵循相同的模式。因此，使用这两个变量进行预测建模是多余的，因为它们高度相关。当然，我们可以从这两个中选择一个，但也可以在任何给定的市场日，将两个价格指标取个平均值。我们将把包含某一天的高价和低价的列转换为NumPy数组。具体做法是通过调用各自列的`values`，这将返回每列的NumPy表示。然后，我们可以使用这些新定义的列来计算一个第三个NumPy数组，存储所有给定观察值的中间价（计算方式为 *(high
    + low) /2)*，如下面所示：
- en: '[PRE2]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We note that there are `1259` total observations, each corresponding to the
    mid-price of our AAL stock on a given day. We will use this array to define our
    training and testing data, before we proceed to prepare them in batches of sequences
    for our LSTM to ingest.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，总共有`1259`个观察值，每个观察值对应于我们AAL股票在某一天的中间价。我们将使用这个数组来定义我们的训练数据和测试数据，然后将它们按序列批次准备好，供LSTM模型使用。
- en: Splitting up the data
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拆分数据
- en: 'Let''s split our entire span of instances (that is, the `mid_prices` variable)
    into training and testing sets of instances. Later, we will use these sets to
    generate the training and testing sequences separately:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将把整个实例范围（即`mid_prices`变量）拆分为训练集和测试集。稍后，我们将分别使用这些数据集生成训练和测试序列：
- en: '[PRE3]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Plotting out training and testing splits
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制训练和测试集的划分
- en: 'In the following screenshot, we simply illustrate two sub-plots to visualize
    the unnormalized training and testing segments of the AAL stock data. You may
    note that the plots are not to scale, as the training data represents 1,000 observations,
    while the test data has only about a quarter of that. Similarly, the test data
    appears between the price range of 40 to 57 USD in the time frame of observations
    it represents, while training data appears in the range between 0 to 50+ USD in
    its respectively longer span of observation. Recall that the test data is simply
    the time series sequence following the first 1,000 observations from our preprocessed
    AAL mid-stock prices data:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的截图中，我们简单地展示了两个子图，用于可视化未归一化的AAL股票数据的训练和测试部分。你可能会注意到，图表的比例并不一致，因为训练数据包含1,000个观察值，而测试数据仅包含大约四分之一的观察值。同样，测试数据的价格区间出现在40到57美元之间，而训练数据则出现在0到50+美元之间，并覆盖了一个更长时间范围的观察期。请记住，测试数据只是我们预处理后的AAL中间股票价格数据中，紧跟前1,000个观察值后的时间序列：
- en: '[PRE4]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding code block generates the following output:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码块生成了以下输出：
- en: '![](img/2a4ece85-8d88-4874-b5b4-fe46784ea04b.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a4ece85-8d88-4874-b5b4-fe46784ea04b.png)'
- en: Windowed normalization
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窗口归一化
- en: Before we can segment our data into smaller sequences for training, we must
    scale all data points between the intervals of zero and one as we have been doing
    thus far. Recall that this representation makes it easier for our network to capture
    relevant representations from the data it is shown, and is a common normalization
    practice within and outside of the deep learning community for various **machine
    learning** (**ML**) tasks.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将数据划分为更小的序列进行训练之前，我们必须将所有数据点缩放到零和一之间，正如我们迄今为止所做的那样。回想一下，这种表示方式使得我们的网络更容易从展示的数据中捕捉到相关的表示，并且这是深度学习社区内外针对各种**机器学习**（**ML**）任务的常见标准化方法。
- en: Unlike previous approaches, however, we must adjust our normalization strategy
    for this particular time series problem. To do this, we adopt a windowed normalization
    approach. Why? Well, this simply allows us to normalize our data in smaller batches,
    instead of normalizing the entire dataset at the same time. Earlier, when we visualized
    the entire time series of our stock data, we noticed something. It turns out that
    data from different years had different value ranges at drastically different
    times. So, an overall normalization procedure will cause values occurring early
    in the time series to be extremely close to zero. This will prohibit our model
    from distinguishing relevant trends as we want it to, and severely diminishes
    the representations that can be captured while training a network. You could,
    of course, choose a wider feature range—however, this would also detrimentally
    affect the learning process as **artificial neural networks** (**ANNs**) tend
    to work best when dealing with values between zero and one.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与之前的方法不同，我们必须调整我们的标准化策略来适应这个特定的时间序列问题。为此，我们采用了窗口标准化方法。为什么？因为这使我们能够在较小的批次中对数据进行标准化，而不是一次性对整个数据集进行标准化。早些时候，当我们可视化整个股票数据的时间序列时，我们注意到了一些问题。事实证明，不同年份的数据在不同时间段内具有不同的值范围。因此，整体标准化过程会导致时间序列早期出现的值接近零。这会阻止我们的模型按预期区分相关的趋势，并且在训练网络时，严重削弱了能够捕捉到的表示。你当然可以选择更宽的特征范围——但是，这也会对学习过程产生不利影响，因为**人工神经网络**（**ANNs**）在处理零到一之间的值时效果最好。
- en: 'So lets implement this windowed normalization scheme, as shown in the following
    code blocks:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们实现这个窗口标准化方案，如下代码块所示：
- en: '[PRE5]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: One issue with the windowed normalization approach we just undertook is worth
    mentioning. Normalizing our data in batches can introduce a break in continuity
    at the end of each batch, since each batch is normalized independently. So, it
    is recommended to choose a reasonable window size that does not introduce too
    many breaks in our training data. In our case, we will choose a window size of
    250 days, as this not only perfectly divides our training and test sets, but also
    only introduces only four potential breaks in continuity, while normalizing our
    entire dataset (that is, 1000 / 250 = 4). We deem this manageable for the demonstrative
    use case at hand.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才采用的窗口标准化方法有一个问题值得提及。批量标准化数据可能在每个批次的结尾引入连续性的中断，因为每个批次都是独立标准化的。因此，建议选择一个合理的窗口大小，以避免在训练数据中引入过多的中断。就我们的情况而言，我们将选择250天的窗口大小，因为这不仅能完美地划分我们的训练集和测试集，而且在标准化整个数据集时（即1000
    / 250 = 4）只会引入四个潜在的连续性中断。我们认为这对于当前的演示用例是可控的。
- en: Denoising the data
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据去噪
- en: 'Next, we will denoise our stock price data to remove the somewhat irrelevant
    market fluctuations that are currently present. We can do this by weighting the
    data points in an exponentially decreasing manner (otherwise known as **exponential
    smoothing**). This allows us to let recent events have a higher influence on the
    current data point than events from the distant past so that each data point can
    be expressed (or smoothened) as a weighted recursive function of the current value
    and preceding values in the time series. This can be expressed mathematically
    as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将去噪我们的股票价格数据，以去除当前存在的那些不太相关的市场波动。我们可以通过以指数递减的方式加权数据点来做到这一点（也就是**指数平滑**）。这使得我们能够让近期的事件对当前数据点的影响大于远古的事件，从而每个数据点都可以作为当前值和时间序列中前面值的加权递归函数来表示（或平滑）。这可以用数学公式表示如下：
- en: '![](img/df08fb55-0a55-41d4-a7c1-1c160813228c.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df08fb55-0a55-41d4-a7c1-1c160813228c.png)'
- en: The preceding equation denotes the smoothing transformation of a given data
    point (*x[t]*) as a function of a weighted term, gamma. The result (*S[t]*) is
    the smoothened value of a given data point, while the gamma term denotes a smoothing
    factor between zero and one. The decay term allows us to encode prior assumptions
    we may have on the presence of data variations occurring in specific time intervals
    (that is, seasonality) into our predictive modeling efforts. Consequently, we
    will be smoothing the curvature of the mid-stock prices plotted against time.
    This is a common signal preprocessing technique employed in time series modeling
    that helps in removing high-frequency noise from data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的公式表示了给定数据点（*x[t]*）的平滑转换，作为加权项gamma的函数。结果（*S[t]*）是给定数据点的平滑值，而gamma项表示一个介于零和一之间的平滑因子。衰减项使我们能够将可能对特定时间间隔内数据变化（即季节性）存在的假设编码到我们的预测模型中。因此，我们将平滑绘制的股价曲线与时间的关系。这是时间序列建模中常用的信号预处理技术，有助于去除数据中的高频噪声。
- en: Implementing exponential smoothing
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施指数平滑
- en: 'So, we transform our training data by looping over each mid-price value, updating
    the smoothing coefficient, and then applying it to the current price value. Note
    that we update the smoothing coefficient using the previously shown formula, which
    allows us to weight each observation in the time series as a function weighting
    the current and previous observations:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们通过循环遍历每个中间价格值来转换我们的训练数据，更新平滑系数，然后将其应用于当前价格值。请注意，我们使用之前展示的公式来更新平滑系数，这使我们能够根据当前和前一个观测值的加权函数对时间序列中的每个观测值进行加权：
- en: '[PRE6]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Visualizing the curve
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化曲线
- en: Using the following diagram, we can visualize the difference in curvature before
    and after smoothing our data points. As you can see, the purple graph displays
    a much smoother curve while maintaining the general movement of stock prices over
    time.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用下面的图示，我们可以可视化平滑前后数据点曲率的差异。如您所见，紫色图表显示了一个更加平滑的曲线，同时保持了股价随时间变化的总体走势。
- en: 'If we were to use the unsmoothed data points, we would very likely have a hard
    time training a predictive model using any type of ML technique:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用未平滑的数据点，我们很可能会很难使用任何类型的机器学习技术来训练预测模型：
- en: '![](img/ea4c43a3-f7f1-429a-a0f7-ed1f54abae42.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea4c43a3-f7f1-429a-a0f7-ed1f54abae42.png)'
- en: Representation is key, and there will always exist an optimal trade-off between
    accuracy and efficiency. On one hand, using reduced representations may allow
    machines to learn much faster from data. Yet, the very process of down sampling
    to a more manageable representation may cause the loss of valuable information
    that may no longer be captured by our statistical model. On the other hand, dealing
    with the full spectrum of information invites a deluge of computational complexity
    that is neither paralleled by the necessary resources to model, nor is often necessary
    to consider to solve the problem at hand.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 表示是关键，且始终存在准确性和效率之间的最佳平衡。一方面，使用简化的表示可能使机器更快地从数据中学习。然而，简化到更易管理的表示的过程可能会导致有价值的信息丢失，这些信息可能不再被我们的统计模型捕捉到。另一方面，处理全部信息会引发计算复杂性的洪流，这种复杂性既没有必要的资源来建模，也不常需要考虑来解决眼前的问题。
- en: Performing one-step-ahead predictions
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行一步预测
- en: Next, we will interpret some baseline models. This will help us better assess
    the effectiveness of the LSTM network. The smoothing process we performed will
    help us implement these baseline models, which will be used to benchmark the performance
    of our LSTM model. We will try to use some relatively simple algorithms. To do
    this, we will use two techniques, known as the simple moving average and the exponential
    moving average algorithms. Both methods essentially perform one-step-ahead predictions,
    predicting the next time series value in our training data as an average of previous
    sequence of values.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将解释一些基准模型。这将帮助我们更好地评估LSTM网络的有效性。我们进行的平滑处理将帮助我们实现这些基准模型，这些模型将用于基准测试我们LSTM模型的性能。我们将尝试使用一些相对简单的算法。为此，我们将使用两种技术，分别是简单移动平均和指数移动平均算法。这两种方法本质上都是进行一步预测，将训练数据中下一个时间序列的值预测为前一序列值的平均值。
- en: To evaluate the effectiveness of each method, we may use the **mean squared
    error** (**MSE**) function to assess the difference in predicted and actual values.
    Recall that this function, quite literally, squares the errors between predicted
    and actual outcomes at a given timestep. We will also visually verify our predictions
    by superimposing the predicted time series progression over the actual time series
    progression of our stock prices.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估每种方法的有效性，我们可以使用**均方误差**（**MSE**）函数来评估预测值与实际值之间的差异。回顾一下，这个函数实际上是在每个时间步长上，对预测值和实际结果之间的误差进行平方运算。我们还将通过将预测的时间序列进程与实际的股票价格时间序列进程叠加，直观地验证我们的预测。
- en: Simple moving average prediction
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单移动平均预测
- en: 'In the case of the simple moving average, we weight past observations equally
    in a given window when predicting the next value in the time series sequence.
    Here, we calculate the arithmetic average of the stock prices over a given interval
    of time. This simple algorithm can be mathematically expressed as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单移动平均法，我们在预测时间序列中的下一个值时，会对给定窗口内的过去观测值进行等权重处理。在这里，我们计算给定时间间隔内股票价格的算术平均值。这个简单的算法可以用以下数学公式表示：
- en: '![](img/cb9f19b3-8518-4e73-a8fd-3c3055a981f7.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb9f19b3-8518-4e73-a8fd-3c3055a981f7.png)'
- en: 'Taking short-term averages (that is, over the course of months) will allow
    the model to respond quickly to price changes, while long-term averages (that
    is, over the course of years) tend to react slowly to the change in price. In
    Python, this operation translates to the following:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 采用短期平均（即在几个月内）将使模型能够快速响应价格变化，而长期平均（即在几年内）通常对价格变化的反应较慢。在Python中，这一操作可以转化为以下代码：
- en: '[PRE7]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We collected the simple average predictions by, once again, looping through
    our training data using a predefined window size, and collecting the batch-wise
    mean as well as the MSE for each data point in our training set. As indicated
    by the MSE value, our simple averaging prediction model is not performing too
    badly. Next, we can plot out these predictions and superimpose it over the true
    time series progression of our stock prices, giving us a visual illustration of
    this method''s performance:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过再次遍历我们的训练数据，使用预定义的窗口大小，并收集每个数据点的批次均值以及均方误差（MSE），从而收集了简单平均预测。正如MSE值所示，我们的简单平均预测模型表现得还不错。接下来，我们可以将这些预测值绘制出来，并将其与我们股票价格的真实时间序列进程叠加在一起，从而直观地展示这种方法的表现：
- en: '[PRE8]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We get the following graph:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下图表：
- en: '![](img/8dee4561-2e58-48fc-b755-477c6c970f24.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8dee4561-2e58-48fc-b755-477c6c970f24.png)'
- en: In the simple average prediction graph, we note that our predictions do indeed
    catch the general trends of the stock prices, yet do not really provide an accurate
    and reliable prediction at all separate points of the time series. Some predictions
    may seem spot on, yet most are off their mark, and the rate at which they change
    relative to the true counterparts is too slow to make any profitable predictions.
    You may also print out separate values of the prediction array and compare them
    with the actual values from the training data if you wish to get a more numerical
    sense of how far off the predictions actually are. Next, we will move on to our
    second baseline.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单平均预测图中，我们注意到我们的预测确实捕捉到了股票价格的总体趋势，但在时间序列的各个独立点上并没有提供准确且可靠的预测。有些预测可能看起来非常准确，但大多数都偏离了实际值，而且它们相对于真实值变化的速度也太慢，无法做出任何有利可图的预测。如果你想更直观地了解预测的准确度，可以单独打印出预测数组的各个值，并与训练数据中的实际值进行比较。接下来，我们将继续进行第二个基准测试。
- en: Exponential moving average prediction
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指数移动平均预测
- en: 'The exponential moving average is a bit trickier than its simple counterpart;
    however, we are already familiar with the formula we will be using. In essence,
    we will use the same equation as the one we employed to smooth our data. This
    time, however, we will use exponential averaging in order to predict the next
    data point in our time series, instead of rescaling the current data point:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 指数移动平均比简单的移动平均稍微复杂一些；然而，我们已经熟悉我们将使用的公式。从本质上讲，我们将使用与平滑数据时相同的方程式。不过，这次我们将使用指数平均法来预测时间序列中的下一个数据点，而不是重新调整当前的数据点：
- en: '[PRE9]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As we can see, the simple moving average ([https://en.wikipedia.org/wiki/Moving_average#Simple_moving_average](https://en.wikipedia.org/wiki/Moving_average#Simple_moving_average))
    weighs the past observations equally. Contrarily, we use exponential functions
    to control the degree of influence held by previous data points when predicting
    future data points. In other words, we are able to assign exponentially decreasing
    weights to earlier data points over time. Such a technique allows the modeler
    to encode prior assumptions (such as seasonal demand) into the predictive algorithm,
    by modifying the decay rate (gamma). The MSE between the one-step-ahead exponential
    averages and the true price is considerably lower when compared to the one achieved
    from simple averaging. Let''s plot out a graph to visually inspect our results:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，简单移动平均（[https://en.wikipedia.org/wiki/Moving_average#Simple_moving_average](https://en.wikipedia.org/wiki/Moving_average#Simple_moving_average)）赋予过去的观察值相同的权重。相反，我们使用指数函数来控制先前数据点对未来数据点的影响程度。换句话说，我们能够随着时间推移，给早期数据点分配逐渐减少的权重。这种技术允许建模者通过修改衰减率（gamma），将先验假设（例如季节性需求）编码到预测算法中。当与简单平均法计算得到的MSE相比，基于一步前的指数平均法得到的MSE要低得多。让我们绘制一张图表，直观地检查我们的结果：
- en: '[PRE10]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We get the following graph:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到如下图表：
- en: '![](img/efdfe4ce-e94c-4e7e-ba21-f67c9c527fe0.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/efdfe4ce-e94c-4e7e-ba21-f67c9c527fe0.png)'
- en: The problem with one-step-ahead predictions
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一步前预测的问题
- en: Phenomenal! It appears that we are able to almost perfectly predict the stock
    price on the next day given a set of previous days. We didn't even have to train
    a fancy neural network! So, why bother to continue? Well, as it turns out, predicting
    the stock price one day in advance does not really make us millionaires. Moving
    averages are inherently lagging indicators. They are metrics that reflect significant
    changes in the market only after the stock price has started to follow a particular
    trend. Due to the short time span between our predictions and the actual occurrence
    of the event, the optimal point for market entry would have already passed by
    the time such a model would reflect a significant trend.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 非常棒！看起来，给定一组前几天的数据，我们几乎可以完美预测第二天的股价。我们甚至不需要训练复杂的神经网络！那么，为什么还要继续呢？事实证明，提前一天预测股价并不能让我们变成百万富翁。移动平均线本质上是滞后指标，它们仅在股价开始跟随某一特定趋势后，才能反映市场的重大变化。由于我们的预测与事件实际发生之间时间跨度较短，当这种模型反映出明显的趋势时，市场的最佳进入点往往已经过去。
- en: 'On the other hand, using this method to try to predict multiple timesteps into
    the future will also not work. We can actually illustrate this concept mathematically.
    Let''s say we have a data point, and we wanted to use the exponential moving average
    method to predict two steps in advance. In other words, we will not be using the
    true value of (X[t + 1]), but our predictions to compute the subsequent day''s
    stock price. Recall that the equation defining a one-step-ahead prediction is
    defined as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，使用这种方法尝试预测多个时间步的未来股价也不会成功。我们实际上可以用数学来说明这一概念。假设我们有一个数据点，我们希望使用指数移动平均法预测两步以后的股价。换句话说，我们将不会使用(X[t
    + 1])的真实值，而是使用我们的预测值来计算接下来一天的股价。回顾一下，一步前预测的方程式定义如下：
- en: '![](img/396dd755-ac1c-4402-92d6-19d4a6086e5e.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/396dd755-ac1c-4402-92d6-19d4a6086e5e.png)'
- en: 'Let''s assume that the value of data point *X[t]* is 0.6, the *EMA* at *X[t-1]*
    is given as 0.2, and the decay rate we have chosen (gamma) is 0.3\. Then, our
    prediction for *X[t-1]* can be computed as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据点*X[t]*的值为0.6，*X[t-1]*的*EMA*为0.2，我们选择的衰减率（gamma）为0.3。那么，我们对*X[t-1]*的预测可以如下计算：
- en: = 0.3 x 0.2 + (1 – 0.3) x 0.6
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: = 0.3 x 0.2 + (1 – 0.3) x 0.6
- en: = 0.06 + (0.7 x 0.6)
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: = 0.06 + (0.7 x 0.6)
- en: = 0.06 + 0.42 = 0.48
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: = 0.06 + 0.42 = 0.48
- en: 'So, 0.48 is both our prediction for *X[t-1]* and the *EMA* of the current timestep.
    If we are to use the same formulation to compute our prediction for the stock
    price at the following timestep (X[t-2]), we run into some problems. The following
    equation illustrates this difficulty, where *EMA[t] = X[t + 1] = 0.48*:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，0.48既是我们对*X[t-1]*的预测值，也是当前时间步的*EMA*。如果我们使用相同的公式来计算下一时间步（X[t-2]）的股价预测，就会遇到一些问题。以下方程式展示了这一难题，其中*EMA[t]
    = X[t + 1] = 0.48*：
- en: '![](img/d195eaa5-f652-404b-b976-5f0541dbf653.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d195eaa5-f652-404b-b976-5f0541dbf653.png)'
- en: Due to this, whatever gamma we choose to have, since both *EMA[t]* and *X[t
    + 1]* hold the same values, the prediction of *X[t + 2]* will be the same as the
    prediction of *X[t + 1]*. This holds true for any attempt at predicting *X[t]* that
    exceeds one timestep. In practice, exponential moving averages are commonly employed
    by intraday traders as a sanity check, which they use to assess and validate significant
    market moves, often in potentially fast-moving markets. So, now that we have established
    a simple baseline using one-step-ahead moving average predictions, we may move
    to building more complex models that can see much further into the future.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，无论我们选择什么样的gamma，由于*EMA[t]*和*X[t + 1]*持有相同的值，*X[t + 2]*的预测将与*X[t + 1]*的预测相同。这对于任何超过一个时间步长的*X[t]*预测都适用。实际上，指数加权移动平均线（EMA）常常被日内交易者用作理性检查，他们用它来评估和验证市场的重要波动，尤其是在可能快速变化的市场中。所以，现在我们已经使用一步前移平均预测建立了一个简单的基准，我们可以开始构建更复杂的模型，来预测未来更远的价格。
- en: Soon, we will build a set of neural networks and evaluate their performance
    to see how LSTMs perform at the task of predicting the movement of stock prices.
    We will again establish a baseline with a simple feedforward neural network, and
    progressively build more complex LSTMs to compare their performances. Before we
    can proceed with this, however, we must prepare our data. We need to ensure that
    our network may ingest a sequence of training data before it can make a prediction
    on the following sequence value (the scaled mid-price of our stock).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 很快，我们将构建一组神经网络并评估它们的性能，看看LSTM在预测股价走势任务中的表现如何。我们将再次使用一个简单的前馈神经网络作为基准，并逐步构建更复杂的LSTM以比较它们的性能。然而，在我们继续之前，必须准备好数据。我们需要确保我们的网络能够接收一系列训练数据，才能对下一个序列值（我们股票的缩放中价）进行预测。
- en: Creating sequences of observations
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建观测序列
- en: We use the following function to create the training and test sequences that
    we will use to train and test our networks. The function takes a set of time series
    stock prices, and organizes them into segments of *n* consecutive values in a
    given sequence. The key difference will be that the label for each training sequence
    will correspond to the stock price four timesteps into the future! This is quite
    different from what we did with the moving average methods, as they were only
    able to predict the stock price one timestep in advance. So, we generate our sequences
    of data so that our model is trained to foresee the stock price four time steps
    ahead.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下函数来创建训练和测试序列，供我们训练和测试网络。该函数接受一组时间序列股价，并将其组织成一组*n*个连续值的片段，形成一个给定的序列。关键的不同之处在于，每个训练序列的标签将对应于四个时间步后的股价！这与我们之前使用移动平均法有所不同，因为移动平均法只能预测股价一个时间步的变化。因此，我们生成数据序列，使得我们的模型能够预测未来四个时间步后的股价。
- en: 'We define a `look_back` value, which refers to the number of stock prices we
    keep in a given observation. In our case, we are actually allowing the network
    to `look_back` at the past `7` price values, before we ask it to predict what
    happens to our stock price four timesteps later:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个`look_back`值，它表示我们在给定的观测中保留的股价数量。在我们的案例中，我们实际上允许网络回顾过去`7`个价格值，然后再让它预测四个时间步后我们的股价会发生什么变化：
- en: '[PRE11]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We employ the `create_dataset` function to generate a dataset of sequences and
    their corresponding labels. This function is called on our time series data (that
    is, the `train_data` variable) and takes two additional arguments. The first one
    (`look_back`) refers to the number of data points we want per observed sequence.
    In our case, we will create sequences with seven data points in each, referring
    to the past seven mid-price values at a given point in the time series. Similarly,
    the second (`foresight`) variable is the number of steps between the last data
    point in the observed sequence, and the data point we aim to predict. So, our
    labels will reflect a lag of four timesteps into the future for each training
    and test sequence. We repeat this methodology of creating training sequences and
    their labels, from the original training data, with a stride of one. So, we are
    left with a training data of 990 sequences of observations, each with a label
    corresponding to the stock price achieved four timesteps in the future. While
    our `look_back` and `foresight` values are somewhat arbitrary, we encourage you
    experiment with different values to assess how larger `look_back` and `foresight`
    values each affect the predictive prowess of your model. In practice, you will
    experience diminishing returns on either side for both values.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`create_dataset`函数来生成序列及其相应标签的数据集。此函数在我们的时间序列数据（即`train_data`变量）上调用，并接受两个额外的参数。第一个参数（`look_back`）表示每个观察序列中希望有多少个数据点。在我们的例子中，我们将创建包含七个数据点的序列，表示时间序列中某一点之前的七个中间价格值。同样，第二个参数（`foresight`）表示从观察序列的最后一个数据点到我们想要预测的目标数据点之间的步数。因此，我们的标签将反映每个训练和测试序列未来四个时间步的滞后。我们通过使用步长为一的方式，从原始训练数据中重复创建训练序列及其标签的方法。最终，我们将得到一个包含990个观察序列的训练数据集，每个序列的标签对应于四个时间步后达到的股票价格。虽然我们的`look_back`和`foresight`值在某种程度上是任意的，但我们鼓励你尝试不同的值，以评估较大的`look_back`和`foresight`值如何分别影响模型的预测能力。在实际操作中，你会发现这两个值在两端会有递减的回报。
- en: Reshaping the data
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整数据形状
- en: 'Next, we simply reshape our training and test sequences for our network. We
    prepare a 3D tensor of dimensions (timesteps, 1, features), which will be functionally
    useful for testing out different neural network models:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们简单地调整训练集和测试集的序列形状以适应我们的网络。我们准备一个三维张量，维度为（时间步长，1，特征），这将对测试不同的神经网络模型非常有用：
- en: '[PRE12]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Making some imports
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行一些导入
- en: 'Now we are ready to finally build and test some neural network architectures
    and see how they hold up to the task of predicting stock trends. We will start
    by importing the relevant Keras layers, as well as some callbacks that let us
    interact with models in training to save them or cease the training session when
    we deem appropriate:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好最终构建和测试一些神经网络架构，并看看它们在预测股市趋势任务中的表现。我们将从导入相关的Keras层开始，以及一些回调函数，回调函数可以让我们在模型训练过程中进行交互，以便保存模型或在我们认为合适的时候停止训练：
- en: '[PRE13]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Baseline neural networks
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准神经网络
- en: As we mentioned earlier, it is always good to perform sanity checks by starting
    off with simpler models before progressing to more complex ones. Data modelers
    often tend to be attracted to so-called **powerful models**, yet many times they
    may just not be necessary for the task at hand. In these scenarios, it is better
    to employ less powerful (and often less computationally intensive) models to form
    a proper baseline to benchmark the value-added benefit of using anything more
    complex. In such spirits, we will construct two baseline models. Each baseline
    will indicate the performance of a particular type of network on the task at hand.
    We will use the simple feedforward network to establish the preliminary baseline
    for all neural networks. Then, we will use a basic GRU network to establish a
    recurrent network baseline.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，开始时使用较简单的模型进行检查总是好的，然后再逐步过渡到更复杂的模型。数据建模人员往往容易被所谓的**强大模型**吸引，但很多时候，这些复杂的模型可能并不一定是完成任务所必需的。在这些情况下，使用较简单（且通常计算开销较小）的模型来建立一个合适的基准，进而评估使用更复杂模型的增值效果，会更好。抱着这种精神，我们将构建两个基准模型。每个基准模型将展示特定类型网络在任务中的表现。我们将使用简单的前馈网络来建立所有神经网络的初步基准。然后，我们将使用基本的GRU网络来建立递归网络的基准。
- en: Building a feedforward network
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建前馈网络
- en: 'While the feedforward network is a network you are quite familiar with, this
    architecture carries with it a few modifications, allowing it to be suitable for
    the task at hand. The last layer, for instance, is a regressor layer with only
    one neuron. It also uses a linear activation function. As for the loss function,
    we choose the **mean absolute error** (**MAE**). We also choose the `adam` optimizer
    for this task. All future networks will have the same last layer, loss, and optimizer
    implemented. We will also nest the building and compiling of a model in a function,
    to allow us to easily test multiple networks, as we have been doing so far. The
    following code block shows how this can be achieved:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前馈网络是您非常熟悉的网络，但这种体系结构进行了一些修改，使其适合当前的任务。例如，最后一层是一个只有一个神经元的回归器层。它还使用线性激活函数。至于损失函数，我们选择了**平均绝对误差**（**MAE**）。我们还选择了`adam`优化器来执行此任务。所有未来的网络将实现相同的最后一层、损失和优化器。我们还将在一个函数中嵌套构建和编译模型，以便我们可以轻松测试多个网络，就像我们迄今为止所做的那样。以下代码块显示了如何实现这一点：
- en: '[PRE14]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Recurrent baseline
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归基准
- en: 'Next, we will build a simple GRU network to establish a recurrent baseline.
    We specify the correct input shape, and add a small fraction of recurrent dropout.
    Recall that this applies the same dropout scheme to subsequent timesteps, better
    preserving temporal information than its simple dropout counterpart. We have also
    included a small fraction of neurons that randomly drop out. We encourage you
    to separately perform experiments save the one we are currently undertaking, to
    understand the difference in performance of RNNs under different dropout strategies:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将构建一个简单的GRU网络来建立一个递归基准。我们指定正确的输入形状，并添加了一个小的递归dropout比例。请记住，这将对后续时间步骤应用相同的dropout方案，比简单的dropout方案更好地保留时间信息。我们还包括了一个小比例的神经元随机dropout。我们建议您分别进行实验，保存我们当前正在进行的实验，以了解在不同dropout策略下RNN性能的差异：
- en: '[PRE15]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Building LSTMs
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建LSTM模型
- en: 'Now that we have some baseline models in place, let''s proceed by constructing
    what this chapter is all about: an LSTM. We will first start with a plain one-layer
    LSTM with no dropout strategy, equipping it with 32 neurons as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一些基准模型，让我们继续构建这一章的重点：一个LSTM模型。我们首先从一个普通的单层LSTM开始，没有使用任何dropout策略，配置32个神经元，如下所示：
- en: '[PRE16]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We connect the LSTM layer to our dense regressor layer, and continue to use
    the same loss and optimizer and loss functions.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将LSTM层连接到我们的密集回归层，并继续使用相同的损失和优化器函数。
- en: Stacked LSTM
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠的LSTM
- en: 'Next, we simply stack two LSTM layers on top of each other, just like we did
    with the GRUs in the previous chapter. We will see whether this helps the network
    remember more complex time-dependent signals in our stock data. We apply both
    dropout and recurrent dropout schemes to both LSTM layers as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们简单地将两个LSTM层堆叠在一起，就像我们在上一章中对GRU所做的那样。我们将看到这是否有助于网络记住我们股票数据中更复杂的时间相关信号。我们对两个LSTM层都应用了dropout和递归dropout方案，如下所示：
- en: '[PRE17]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now we are ready to run our experiments and evaluate the results. We can evaluate
    them through the MSE metric, as well as visually interpret the model's predictions
    imposed over the actual predictions. We went ahead and constructed a few functions
    that help us visualize our results at the end of each training session.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备运行实验并评估结果。我们可以通过MSE指标进行评估，以及通过将模型的预测结果与实际预测结果叠加进行视觉解释。我们已经构建了一些函数，帮助我们在每次训练会话结束时可视化我们的结果。
- en: Using helper functions
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用辅助函数
- en: 'Before we begin training our networks, we can construct a few helper functions
    that may inform us upon the model''s performance once they have been trained.
    The former `plot_losses` function simply plots the training loss and the validation
    loss, using the `history` object of our model. Recall that this is a default callback
    that provides access to a dictionary containing the training and validation losses
    computed in a session:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始训练我们的网络之前，我们可以构建一些辅助函数，这些函数可以在模型训练完成后帮助我们了解模型的性能。前者`plot_losses`函数简单地绘制训练损失和验证损失，使用我们模型的`history`对象。请记住，这是一个默认的回调函数，提供了在会话中计算的训练和验证损失的字典：
- en: '[PRE18]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we will use the `plot_predictions` function to plot out the model''s
    predictions on our secluded test set, and superimpose them over the actual labels
    of our test set. This is similar in spirit to what we did earlier with one-step-ahead
    predictions. The only difference now is that we will be visualizing a trend predicted
    three timesteps in advance by our network as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`plot_predictions`函数绘制模型在我们隔离的测试集上的预测，并将其叠加到测试集的实际标签上。这与我们之前在一步预测中的做法精神相似。唯一的区别是，现在我们将可视化由我们的网络预测的三步预测趋势，如下所示：
- en: '[PRE19]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Training the model
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: Finally, we build a training function that will help us initiate the training
    session for each network, save their model weights at each epoch, and visualize
    the model performance when the training session has ceased.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们构建了一个训练函数，帮助我们为每个网络启动训练会话，在每个周期保存模型权重，并在训练会话结束时可视化模型表现。
- en: 'This function may take a list of models and execute the described steps on
    each model. So, get ready to take a brief/extensive stroll (depending on your
    hardware configuration) after running the following cells of code:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数可以接受一个模型列表，并对每个模型执行上述步骤。因此，在运行以下代码单元后，请准备好进行一次简短/长时间的散步（取决于你的硬件配置）。
- en: '[PRE20]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Visualizing results
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化结果
- en: Finally, we will display the predictions of our model vis-à-vis the actual prices,
    as shown in the following diagram. Note that although the simple LSTM performs
    the best (with MAE of 0.0809), it is quite closely matched by the simple feedforward
    neural network that, by design, has fewer trainable parameters than the LSTM network.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将展示模型的预测与实际价格的对比，如下图所示。请注意，虽然简单的LSTM表现最佳（MAE为0.0809），但它与简单的前馈神经网络的表现非常接近，后者在设计上具有比LSTM网络更少的可训练参数。
- en: 'How so? you may wonder. Well, while LSTMs are extremely good at encoding complex
    time-dependent signals, those signals have to be present in our data in the first
    place:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，怎么做呢？嗯，虽然LSTM在编码复杂的时间依赖信号方面非常擅长，但这些信号必须首先出现在我们的数据中：
- en: '![](img/9a5f02c4-8376-4e8e-b919-06aabdc980c1.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a5f02c4-8376-4e8e-b919-06aabdc980c1.png)'
- en: There can be only so much information conveyed through viewing the past seven
    mid-prices in predicting the future. In our case, it seems that the type of representations
    our LSTM could conjure for the predictive task was more or less matched by the
    representations conjured by the feedforward network. There might be a lot of complex
    signals that the LSTM could model in this context, but they don't seem to be present
    in our dataset. For instance, we are not, by design, incorporating any information
    about what happened to the market at time *t+1* or *t+2* when predicting the label
    of *x[t + 3]*. Moreover, there may exist variables other than past mid-stock prices
    that would better correlate with the future movement of the stock market. Social
    media sentiment (on Twitter, read: [https://arxiv.org/pdf/1010.3003.pdf](https://arxiv.org/pdf/1010.3003.pdf)),
    for instance, has been shown to correlate with the movement of stock prices up
    to seven days in advance! It turns out that the winning emotion was calmness,
    rather than happiness or neuroticism, which lined up best with market movements
    up to a week in advance. So, including features that represent other types and
    sources of information may help increase our LSTM's performance in comparison
    to the baseline models.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看过去七个中间价格来预测未来，能够传递的信息是有限的。在我们的案例中，似乎LSTM能够为预测任务构建的表示与前馈网络所构建的表示相匹配。LSTM在此上下文中可能能够建模很多复杂信号，但这些信号似乎在我们的数据集中并不存在。例如，在预测标签*x[t
    + 3]*时，我们并未设计性地包含关于市场在时间*t+1*或*t+2*的任何信息。此外，可能还存在其他变量，而非过去的中间股票价格，更能与股市的未来走势相关。例如，社交媒体情绪（如Twitter，参考：[https://arxiv.org/pdf/1010.3003.pdf](https://arxiv.org/pdf/1010.3003.pdf)）已被证明能与股票价格的变动相关，最多提前七天！事实证明，获胜的情感是冷静，而非快乐或神经质，这与股市在最多提前一周的变动最为一致。因此，包含代表其他类型和信息来源的特征，可能有助于提高我们的LSTM模型的表现，相比于基准模型。
- en: Closing comments
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束评论
- en: Note that this does not necessarily mean that the movement of all stocks, in
    all industries, can be better predicted through inclusion of social media data.
    However, it does illustrate our point that there is some room for heuristic-based
    feature generation that may allow additional signals to be leveraged for better
    predictive outcomes. To provide some closing comments on our experiments, we also
    notice that the simple GRU and the stacked LSTMs both have smoother predictive
    curves, and are less likely to be swayed by noisy input sequences. They perform
    remarkably well at conserving the general trend of the stock. The out-of-set accuracy
    of these models (assessed with the MAE between the predicted and actual value)
    tells us that they perform slightly worse than the feedforward network and the
    simple LSTM. However, we may prefer to employ the models with the smoother curve
    for decision making compared to the noisier predictors, depending on the specific
    use case.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这并不一定意味着通过引入社交媒体数据，可以更好地预测所有行业中所有股票的走势。然而，这确实说明了我们的观点，即基于启发式的特征生成还有一些空间，可能允许利用额外的信号来实现更好的预测结果。为了对我们的实验做一些总结评论，我们还注意到，简单的
    GRU 和堆叠的 LSTM 都具有更平滑的预测曲线，并且不太容易受到噪声输入序列的影响。它们在保持股票整体趋势方面表现得非常好。这些模型的外部精度（通过预测值与实际值之间的
    MAE 来评估）告诉我们，它们的表现略逊色于前馈网络和简单的 LSTM。然而，根据具体的使用案例，我们可能更倾向于使用具有更平滑曲线的模型进行决策，而不是噪声较大的预测模型。
- en: Summary
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we dived deep into the inner workings of the LSTM network.
    We explored both the concepts and mathematical implementation related to these
    networks, understanding how information is processed in an LSTM cell and using
    short-term and long-term memory of events. We also saw why the network gets its
    name, being adept at conserving relevant cell states over very distant timesteps.
    While we discussed some variants to the architecture, such as the peephole connection,
    it is seldom seen in most common LSTM candidate scenarios. Although we executed
    our demonstrations with a simple time series dataset, we highly encourage you
    to implement this architecture to tackle other problems that you may already be
    familiar with (such as the IMDB sentiment classification dataset), and compare
    results with our earlier efforts.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了 LSTM 网络的内部工作原理。我们探索了与这些网络相关的概念和数学实现，理解了信息是如何在 LSTM 单元中处理的，并使用短期和长期记忆来存储事件。我们还了解了为什么这个网络得名，因为它擅长在非常远的时间步长中保持相关的单元状态。虽然我们讨论了该架构的一些变体，如窥视孔连接，但在大多数常见的
    LSTM 候选场景中很少见到它。尽管我们使用了一个简单的时间序列数据集进行演示，但我们强烈建议你实现这个架构来解决你可能已经熟悉的其他问题（例如 IMDB
    情感分类数据集），并将结果与我们早期的工作进行比较。
- en: LSTMs have really been shown to shine at **natural language processing** (**NLP**)
    tasks. You could try generating movie scripts with the Wikipedia movies dataset,
    or even try generating music using the music21 library and some MIDI files with
    training songs.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 在**自然语言处理**（**NLP**）任务中确实表现突出。你可以尝试使用维基百科电影数据集生成电影剧本，或者尝试使用 music21 库和一些
    MIDI 文件来生成音乐，并用训练歌曲进行训练。
- en: 'Some further coding can be found here:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的编码可以在这里找到：
- en: '**Peephole pseudocode**: [https://gist.github.com/EderSantana/f07fa7a0371d0e1c4ef1](https://gist.github.com/EderSantana/f07fa7a0371d0e1c4ef1)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**窥视孔伪代码**：[https://gist.github.com/EderSantana/f07fa7a0371d0e1c4ef1](https://gist.github.com/EderSantana/f07fa7a0371d0e1c4ef1)'
- en: The theoretical notion behind LSTMs remain quite eliciting—even more so in light
    of their excellent performance on a variety of sequential and non-sequential tasks.
    Are we then to crown LSTMs as the ultimate champions, as far as RNNs go? Well,
    not exactly. One of the next big ideas, bordering the realms of RNNs, comes from
    the area of attention models, where we, quite literally, try to steer the attention
    of our neural network while it processes a collection of information. This approach
    is quite useful in the case of image captioning, as we need to correlate important
    parts of an image in a given input with must-include words sequenced in a coherent
    output. We will explore the topic of attention models in further detail in the
    coming chapters. For interested readers, you may follow up on the task of machine
    image captioning by reading an excellent paper, titled *Image captioning with
    semantic attention*, by *Fang et al.* 2016.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM背后的理论概念仍然相当引人注目——尤其是考虑到它们在各种顺序和非顺序任务中的出色表现。那么，是否可以将LSTM冠以RNN领域的终极冠军称号呢？嗯，答案并不完全是。下一个接近RNN领域的重要思想来源于注意力模型的领域，在这个领域中，我们字面上地试图引导神经网络在处理一组信息时的注意力。这种方法在图像描述任务中非常有用，因为我们需要将输入图像的关键部分与输出中必须包含的、按序排列的单词相关联。我们将在接下来的章节中详细探讨注意力模型的相关话题。对于感兴趣的读者，你可以通过阅读Fang等人于2016年发表的优秀论文*Image
    captioning with semantic attention*来进一步了解机器图像描述任务。
- en: 'In the next chapter, however, we will focus our attention on another part of
    neural networks and deep learning: reinforcement learning. This is an extremely
    interesting area of machine learning that deals with how artificial agents must
    act in a designed environment for them to be able to cumulatively maximize some
    reward. This approach can be applied to a myriad of use cases, such as teaching
    machines to perform surgery, generate jokes, or play video games. Having machines
    capable of leveraging a level of physical or psychological dexterity comparable
    to (or beyond) that of humans can allow us to build very complex and intelligent
    systems. Such systems maintain internal states that are relevant to the environment
    in which the system operates, and are able to update their internal state by studying
    the consequences of their actions upon the environment while optimizing a specific
    goal. So, each combination of actions triggers different reward signals that the
    learning system may leverage for self-improvement.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在下一章中，我们将把注意力集中在神经网络和深度学习的另一个部分：强化学习。这是机器学习中一个极为有趣的领域，它研究人工智能体如何在一个设计好的环境中行动，以便能够累积性地最大化某个奖励。这个方法可以应用于各种各样的使用案例，例如教机器进行手术、生成笑话或玩视频游戏。让机器能够利用与人类相当（甚至超越）水平的身体或心理灵活性，能够帮助我们构建非常复杂和智能的系统。这些系统维护与其操作环境相关的内部状态，并能够通过研究其行为对环境的影响来更新内部状态，同时优化特定目标。因此，每一种行动组合都会触发不同的奖励信号，学习系统可以利用这些信号进行自我提升。
- en: As we will soon see, designing systems that are allowed to be reinforced through
    reward signals can lead to very complex behavior, leading machines to perform
    highly intelligent actions even where humans tend to dominate. The tale of AlphaGo
    versus Lee Sedol (the once-revered world champion of the ancient Chinese board
    game Go) comes to mind. As the AlphaGo system beat its human contender five to
    one in 2016, the event itself was very different to the victory of IBM's Deep
    Blue over Gary Kasparov (1997). Many who watched the AlphaGo matches against Lee
    Sedol saw something special in the machine's modus operandi. Some even called
    it **intuition**.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们很快会看到的，设计允许通过奖励信号来强化的系统，可以导致非常复杂的行为，从而使机器能够执行高度智能的行动，甚至在人类通常占优势的领域也能表现突出。AlphaGo与李世石的对决故事浮现在脑海中。2016年，AlphaGo以五比一战胜了李世石，而这一事件与1997年IBM的Deep
    Blue战胜加里·卡斯帕罗夫（Gary Kasparov）大不相同。许多观看AlphaGo与李世石对局的人都看到了机器操作方式的特殊性。有些人甚至称之为**直觉**。
- en: In the next chapter, we will see how such systems, operating on some fairly
    straightforward statistical properties of environments and possible actions, can
    produce beautifully complex outcomes, at times transcending our own expectations.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将看到这样的系统，基于环境和可能行动的一些相当简单的统计属性，如何产生出美丽而复杂的结果，有时甚至超出我们自己的预期。
- en: Exercises
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Examine the time taken for models to converge. Is there a big difference between
    different models?
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查模型收敛所需的时间。不同模型之间有很大的差异吗？
- en: Examine the training and validation losses between the models. What do you notice?
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查模型之间的训练和验证损失。你注意到了什么？
- en: Experiment with downscaling and upscaling the architecture, note how this affects
    learning.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试缩小和放大架构，注意这如何影响学习过程。
- en: Experiment with different optimizers and loss metrics and note how this affects
    learning.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的优化器和损失度量，并注意这如何影响学习过程。
- en: Implement an LSTM on the IMBD dataset for sentiment classification.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在IMBD数据集上实现LSTM进行情感分类。
- en: Implement an LSTM on the Wikimovies dataset to build a character/word-level
    language model and generate artificial movie plots.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Wikimovies数据集上实现LSTM，构建字符/词级语言模型并生成人工电影剧情。
