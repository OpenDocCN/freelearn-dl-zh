- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Generative Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成模型
- en: Generative models are a type of machine learning algorithm that is used to create
    data. They are used to generate new data that is similar to the data that was
    used to train the model. They can be used to create new data for testing or to
    fill in missing data. Generative models are used in many applications, such as
    density estimation, image synthesis, and natural language processing. The VAE
    discussed in *Chapter 8*, *Autoencoders*, was one type of generative model; in
    this chapter, we will discuss a wide range of generative models, **Generative
    Adversarial Networks** (**GANs**) and their variants, flow-based models, and diffusion
    models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型是一种机器学习算法，用于生成数据。它们用于生成与训练模型时使用的数据相似的新数据。这些模型可以用于生成新数据以供测试，或填补缺失的数据。生成模型被广泛应用于许多领域，例如密度估计、图像合成和自然语言处理。在*第8章*《自编码器》中讨论的VAE就是一种生成模型；在本章中，我们将讨论各种生成模型，包括**生成对抗网络**（**GANs**）及其变种、基于流的模型和扩散模型。
- en: 'GANs have been defined as *the most interesting idea in the last 10 years in
    ML* ([https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning](https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning))
    by Yann LeCun, one of the fathers of deep learning. GANs are able to learn how
    to reproduce synthetic data that looks real. For instance, computers can learn
    how to paint and create realistic images. The idea was originally proposed by
    Ian Goodfellow (for more information, refer to *NIPS 2016 Tutorial: Generative
    Adversarial Networks*, by I. Goodfellow, 2016); he has worked with the University
    of Montreal, Google Brain, and OpenAI, and is presently working in Apple Inc.
    as the Director of Machine Learning.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: GANs被Yann LeCun（深度学习的奠基人之一）定义为*过去10年中机器学习领域最有趣的想法*（[https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning](https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning)）。GANs能够学习如何重现看似真实的合成数据。例如，计算机可以学习如何绘画并创建逼真的图像。这个想法最初由Ian
    Goodfellow提出（有关更多信息，请参见*2016年NIPS教程：生成对抗网络*，I. Goodfellow，2016）；他曾与蒙特利尔大学、Google
    Brain和OpenAI合作，现在在Apple Inc.担任机器学习总监。
- en: 'In this chapter, we will cover different types of GANs; the chapter will introduce
    you to flow-based models and diffusion models, and additionally, you will see
    some of their implementation in TensorFlow. Broadly, we will cover the following
    topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍不同类型的GANs；本章将向你介绍基于流的模型和扩散模型，此外，你还将看到它们在TensorFlow中的一些实现。总体来说，我们将涵盖以下主题：
- en: What is a GAN?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是GAN？
- en: Deep convolutional GANs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度卷积GANs
- en: InfoGAN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: InfoGAN
- en: SRGAN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SRGAN
- en: CycleGAN
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CycleGAN
- en: Applications of GANs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN的应用
- en: Flow-based generative models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于流的生成模型
- en: Diffusion models for data generation
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于数据生成的扩散模型
- en: All the code files for this chapter can be found at [https://packt.link/dltfchp9](https://packt.link/dltfchp9)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码文件可以在[https://packt.link/dltfchp9](https://packt.link/dltfchp9)找到
- en: Let’s begin!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: What is a GAN?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是GAN？
- en: The ability of GANs to learn high-dimensional, complex data distributions has
    made them very popular with researchers in recent years. Between 2016, when they
    were first proposed by Ian Goodfellow, to March 2022, we have more than 100,000
    research papers related to GANs, just in the space of 6 years!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: GANs能够学习高维复杂的数据分布，这使它们在近年来受到研究人员的广泛关注。从2016年Ian Goodfellow首次提出GANs，到2022年3月，关于GANs的研究论文已超过10万篇，仅在短短6年的时间里！
- en: The applications of GANs include creating images, videos, music, and even natural
    languages. They have been employed in tasks like image-to-image translation, image
    super-resolution, drug discovery, and even next-frame prediction in video. They
    have been especially successful in the task of synthetic data generation – both
    for training the deep learning models and assessing the adversarial attacks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: GANs的应用包括创建图像、视频、音乐，甚至自然语言。它们已被应用于图像到图像的转换、图像超分辨率、药物发现，甚至视频中的下一帧预测任务。在合成数据生成任务中，GANs特别成功——无论是在训练深度学习模型，还是评估对抗性攻击中。
- en: The key idea of GAN can be easily understood by considering it analogous to
    “art forgery,” which is the process of creating works of art that are falsely
    credited to other usually more famous artists. GANs train two neural nets simultaneously.
    The generator *G(Z)* is the one that makes the forgery, and the discriminator
    *D(Y)* is the one that can judge how realistic the reproductions are, based on
    its observations of authentic pieces of art and copies. *D(Y)* takes an input
    *Y* (for instance, an image), and expresses a vote to judge how real the input
    is. In general, a value close to 1 denotes “real,” while a value close to 0 denotes
    “forgery.” *G(Z)* takes an input from random noise *Z* and it trains itself to
    fool *D* into thinking that whatever *G(Z)* produces is real.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的关键思想可以通过将其类比为“艺术伪造”来轻松理解，艺术伪造是指创造被错误归于其他通常更著名艺术家的艺术作品的过程。GAN同时训练两个神经网络。生成器*G(Z)*负责进行伪造，判别器*D(Y)*则负责根据其对真实艺术品和复制品的观察判断复制作品的真实性。*D(Y)*接收输入*Y*（例如一张图像），并通过表达一个投票来判断输入的真实性。一般来说，接近1的值表示“真实”，而接近0的值表示“伪造”。*G(Z)*接收来自随机噪声*Z*的输入，并通过训练自己来欺骗*D*，让其认为*G(Z)*生成的内容是真实的。
- en: 'The goal of training the discriminator *D(Y)* is to maximize *D(Y)* for every
    image from the true data distribution and to minimize *D(Y)* for every image not
    from the true data distribution. So, *G* and *D* play opposite games, hence the
    name **adversarial training**. Note that we train *G* and *D* in an alternating
    manner, where each one of their objectives is expressed as a loss function optimized
    via a gradient descent. The generative model continues to improve its forgery
    capabilities, and the discriminative model continues to improve its forgery recognition
    capabilities. The discriminator network (usually a standard convolutional neural
    network) tries to classify if an input image is real or generated. The important
    new idea is to backpropagate through both the discriminator and the generator
    to adjust the generator’s parameters in such a way that the generator can learn
    how to fool the discriminator more often. In the end, the generator will learn
    how to produce images that are indistinguishable from the real ones:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 训练判别器*D(Y)*的目标是最大化*D(Y)*对于真实数据分布中的每一张图像，同时最小化*D(Y)*对于每一张非真实数据分布的图像。因此，*G*和*D*进行对立的博弈，这也是**对抗训练**这一名称的由来。需要注意的是，我们以交替的方式训练*G*和*D*，它们的每个目标都通过一个梯度下降优化的损失函数来表达。生成模型不断提高其伪造能力，而判别模型则不断提高其伪造识别能力。判别网络（通常是标准的卷积神经网络）尝试分类输入的图像是否真实。重要的新想法是通过判别器和生成器反向传播，从而调整生成器的参数，使得生成器能够学会如何更频繁地欺骗判别器。最终，生成器将学会生成与真实图像无法区分的图像：
- en: '![A black sign with white text  Description automatically generated](img/B18331_09_01.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![一块黑色的标牌，白色文字  描述自动生成](img/B18331_09_01.png)'
- en: 'Figure 9.1: Basic architecture of a GAN'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9.1: GAN的基本架构'
- en: Of course, GANs involve working towards equilibrium in a game involving two
    players. Let us first understand what we mean by equilibrium here. When we start,
    one of the two players is hopefully better than the other. This pushes the other
    to improve and this way, both the generator and discriminator push each other
    towards improvement.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，GAN涉及在两个玩家之间的博弈中朝着平衡发展。让我们首先理解这里的“平衡”是什么意思。当我们开始时，希望其中一个玩家优于另一个，这样会推动另一个玩家进步，从而使生成器和判别器互相推动着彼此改进。
- en: 'Eventually, we reach a state where the improvement is not significant in either
    player. We check this by plotting the loss function, to see when the two losses
    (gradient loss and discriminator loss) reach a plateau. We don’t want the game
    to be skewed too heavily one way; if the forger were to immediately learn how
    to fool the judge on every occasion, then the forger has *nothing more to learn*.
    Practically training GANs is really hard, and a lot of research is being done
    in analyzing GAN convergence; check this site: [https://avg.is.tuebingen.mpg.de/projects/convergence-and-stability-of-gan-training](https://avg.is.tuebingen.mpg.de/projects/convergence-and-stability-of-gan-training)
    for details on convergence and stability of different types of GANs. In generative
    applications of GAN, we want the generator to learn a little better than the discriminator.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们达到了一个状态，在这个状态下，无论是哪一方，改进都不再显著。我们通过绘制损失函数来检查这一点，查看两个损失（梯度损失和判别器损失）何时达到平台期。我们不希望游戏向某一方过于倾斜；如果伪造者每次都能立即学会如何欺骗判别器，那么伪造者就*再也没有什么可以学习的了*。实际上，训练
    GAN 是非常困难的，很多研究都在分析 GAN 的收敛性；可以查看这个网站：[https://avg.is.tuebingen.mpg.de/projects/convergence-and-stability-of-gan-training](https://avg.is.tuebingen.mpg.de/projects/convergence-and-stability-of-gan-training)
    了解不同类型 GAN 的收敛性和稳定性的详细信息。在 GAN 的生成应用中，我们希望生成器的学习效果略好于判别器。
- en: 'Let’s now delve deep into how GANs learn. Both the discriminator and generator
    take turns to learn. The learning can be divided into two steps:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入探讨 GAN 是如何学习的。判别器和生成器轮流进行学习。学习过程可以分为两个步骤：
- en: Here the discriminator, *D(x)*, learns. The generator, *G(z)*, is used to generate
    fake images from random noise *z* (which follows some prior distribution *P(z)*).
    The fake images from the generator and the real images from the training dataset
    are both fed to the discriminator, and it performs supervised learning trying
    to separate fake from real. If *P*[data] (*x*) is the training dataset distribution,
    then the discriminator network tries to maximize its objective so that *D(x)*
    is close to 1 when the input data is real and close to zero when the input data
    is fake.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里是判别器，*D(x)*，进行学习。生成器，*G(z)*，用于从随机噪声*z*（遵循某个先验分布*P(z)*)中生成假的图像。生成器产生的假图像和训练数据集中的真实图像都被输入到判别器中，判别器进行监督学习，尝试将真假图像分开。如果*P*[数据]
    (*x*) 是训练数据集分布，那么判别器网络会最大化其目标，使得*D(x)*在输入数据为真实时接近 1，在输入数据为虚假时接近 0。
- en: In the next step, the generator network learns. Its goal is to fool the discriminator
    network into thinking that generated *G(z)* is real, that is, force *D(G(z))*
    close to 1.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一步中，生成器网络进行学习。它的目标是欺骗判别器网络，使其认为生成的*G(z)*是真实的，即强迫*D(G(z))*接近 1。
- en: The two steps are repeated sequentially. Once the training ends, the discriminator
    is no longer able to discriminate between real and fake data and the generator
    becomes a pro in creating data very similar to the training data. The stability
    between discriminator and generator is an actively researched problem.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个步骤会依次重复。一旦训练结束，判别器就不再能够区分真实和虚假的数据，生成器则变得非常擅长生成与训练数据非常相似的数据。判别器和生成器之间的稳定性是一个活跃的研究问题。
- en: Now that you have got an idea of what GANs are, let’s look at a practical application
    of a GAN in which “handwritten” digits are generated.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对 GAN 有了基本了解，让我们看一个实际应用，其中使用 GAN 生成“手写”数字。
- en: MNIST using GAN in TensorFlow
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中使用 GAN 生成 MNIST 数据
- en: 'Let us build a simple GAN capable of generating handwritten digits. We will
    use the MNIST handwritten digits to train the network. We will need to import
    TensorFlow modules; to keep the code clean, we export all the classes that we
    will require from the TensorFlow framework:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个简单的 GAN，能够生成手写数字。我们将使用 MNIST 手写数字来训练网络。我们需要导入 TensorFlow 模块；为了保持代码的简洁，我们将从
    TensorFlow 框架中导出所有需要的类：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We use the TensorFlow Keras dataset to access the MNIST data. The data contains
    60,000 training images of handwritten digits each of size 28 × 28\. The pixel
    value of the digits lies between 0-255; we normalize the input values such that
    each pixel has a value in the range [-1, 1]:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 TensorFlow Keras 数据集来访问 MNIST 数据。数据包含 60,000 张手写数字的训练图像，每张图像的大小为 28 × 28。数字的像素值范围为
    0-255；我们将输入值标准化，使得每个像素的值在 [-1, 1] 范围内：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will use a simple **multi-layered perceptron** (**MLP**) and we will feed
    it an image as a flat vector of size 784, so we reshape the training data:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个简单的**多层感知器**（**MLP**），并将其输入一个大小为 784 的平面向量图像，因此我们需要重塑训练数据：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now we will need to build a generator and discriminator. The purpose of the
    generator is to take in a noisy input and generate an image similar to the training
    dataset. The size of the noisy input is decided by the variable `randomDim`; you
    can initialize it to any integral value. Conventionally, people set it to 100\.
    For our implementation, we tried a value of 10\. This input is fed to a dense
    layer with `256` neurons with LeakyReLU activation. We next add another dense
    layer with `512` hidden neurons, followed by the third hidden layer with `1024`
    neurons, and finally the output layer with `784` neurons. You can change the number
    of neurons in the hidden layers and see how the performance changes; however,
    the number of neurons in the output unit has to match the number of pixels in
    the training images. The corresponding generator is then:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要构建生成器和鉴别器。生成器的目的是接收一个噪声输入，并生成一张与训练数据集相似的图像。噪声输入的大小由变量`randomDim`决定；你可以将其初始化为任何整数值。通常人们将其设置为100。对于我们的实现，我们尝试了一个值为10。这个输入会传入一个有`256`个神经元并使用LeakyReLU激活函数的全连接层。接下来，我们添加另一个有`512`个隐藏神经元的全连接层，紧接着是第三个有`1024`个神经元的隐藏层，最后是输出层，包含`784`个神经元。你可以改变隐藏层中神经元的数量，看看性能如何变化；然而，输出层神经元的数量必须与训练图像的像素数量匹配。对应的生成器如下：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Similarly, we build a discriminator. Notice now (*Figure 9.1*) that the discriminator
    takes in the images, either from the training set or images generated by the generator,
    thus its input size is `784`. Additionally, here we are using a TensorFlow initializer
    to initialize the weights of the dense layer, we are using a normal distribution
    with a standard deviation of 0.02 and a mean of 0\. As mentioned in *Chapter 1*,
    *Neural Network Foundations with TF*, there are many initializers available in
    the TensorFlow framework. The output of the discriminator is a single bit, with
    `0` signifying a fake image (generated by generator) and `1` signifying that the
    image is from the training dataset:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们构建了一个鉴别器。现在注意（*图9.1*）鉴别器接收的图像，可能来自训练集，也可能是由生成器生成的图像，因此其输入大小为`784`。此外，在这里我们使用了TensorFlow的初始化器来初始化全连接层的权重，使用的是标准差为0.02，均值为0的正态分布。如*第1章：TensorFlow神经网络基础*中所述，TensorFlow框架中提供了许多初始化器。鉴别器的输出是一个单一的比特，`0`表示假图像（由生成器生成），`1`表示图像来自训练数据集：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we combine the generator and discriminator together to form a GAN. In
    the GAN, we ensure that the discriminator weights are fixed by setting the `trainable`
    argument to `False`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将生成器和鉴别器结合在一起，形成一个GAN（生成对抗网络）。在GAN中，我们确保鉴别器的权重是固定的，通过将`trainable`参数设置为`False`来实现：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The trick to training the two is that we first train the discriminator separately;
    we use binary cross-entropy loss for the discriminator. Later, we freeze the weights
    of the discriminator and train the combined GAN; this results in the training
    of the generator. The loss this time is also binary cross-entropy:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这两个网络的技巧是，我们首先单独训练鉴别器；我们对鉴别器使用二元交叉熵损失函数。随后，我们冻结鉴别器的权重，并训练组合的GAN；这时的训练目标是生成器。此时的损失函数仍然是二元交叉熵：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let us now perform the training. For each epoch, we take a sample of random
    noise first, feed it to the generator, and the generator produces a fake image.
    We combine the generated fake images and the actual training images in a batch
    with their specific labels and use them to train the discriminator first on the
    given batch:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始进行训练。对于每一个训练轮次（epoch），我们首先取一个随机噪声样本，输入到生成器中，生成器会产生一张假图像。然后，我们将生成的假图像与实际的训练图像以及它们对应的标签一起放入一个批次，并用这些数据先对鉴别器进行训练：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you notice, while assigning labels, instead of `0`/`1` we used `0`/`0.9`
    – this is called label smoothing. It has been found that keeping a soft target
    improves both generalization and learning speed (*When does label smoothing help?*,
    Muller et al. NeurIPS 2019).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你注意到，在分配标签时，我们并没有使用`0`/`1`，而是使用了`0`/`0.9` —— 这叫做标签平滑。研究发现，使用软目标有助于提高模型的泛化能力和学习速度（*标签平滑何时有用？*，Muller
    等，NeurIPS 2019）。
- en: 'Now, in the same `for` loop, we will train the generator. We want the images
    generated by the generator to be detected as real by the discriminator, so we
    use a random vector (noise) as input to the generator; this generates a fake image
    and then trains the GAN such that the discriminator perceives the image as real
    (the output is `1`):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在同一个`for`循环中，我们将训练生成器。我们希望生成器生成的图像被鉴别器判断为真实的，因此我们使用一个随机向量（噪声）作为输入给生成器；生成器生成一张假图像，然后训练GAN，使得鉴别器将这张图像判断为真实（输出为`1`）：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Cool trick, right? If you wish to, you can save the generator and discriminator
    loss as well as the generated images. Next, we are saving the losses for each
    epoch and generating images after every 20 epochs:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 很酷的技巧，对吧？如果你愿意，你还可以保存生成器和判别器的损失，以及生成的图像。接下来，我们将保存每个epoch的损失，并在每20个epoch后生成图像：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can now train the GAN by calling the `train` function. In the following
    graph, you can see the plot of both generative and discriminative loss as the
    GAN is learning:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过调用`train`函数来训练GAN。在下图中，你可以看到生成损失和判别损失的曲线图，随着GAN的学习进行：
- en: '![Chart, line chart  Description automatically generated](img/B18331_09_02.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图 说明自动生成](img/B18331_09_02.png)'
- en: 'Figure 9.2: Discriminator and generator loss plots'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：判别器和生成器的损失图
- en: 'And handwritten digits generated by our GAN:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由我们GAN生成的手写数字：
- en: '![Calendar  Description automatically generated](img/B18331_09_03_01.png)![A
    picture containing text, grater, kitchenware  Description automatically generated](img/B18331_09_03_02.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![日历 说明自动生成](img/B18331_09_03_01.png)![一张包含文字的图片，擦菜板，厨房用品 说明自动生成](img/B18331_09_03_02.png)'
- en: 'Figure 9.3: Generated handwritten digits'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：生成的手写数字
- en: You can see from the preceding figures that as the epochs increase, the handwritten
    digits generated by the GAN become more and more realistic.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的图中可以看出，随着训练轮次的增加，GAN生成的手写数字越来越逼真。
- en: 'To plot the loss and the generated images of the handwritten digits, we define
    two helper functions, `plotLoss()` and `saveGeneratedImages()`. Their code is
    given as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绘制损失和生成的手写数字图像，我们定义了两个辅助函数，`plotLoss()`和`saveGeneratedImages()`。它们的代码如下：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `saveGeneratedImages` function saves images in the `images` folder, so make
    sure you have created the folder in your current working directory. The complete
    code for this can be found in the notebook `VanillaGAN.ipynb` at the GitHub repo
    for this chapter. In the coming sections, we will cover some recent GAN architectures
    and implement them in TensorFlow.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`saveGeneratedImages`函数将图像保存在`images`文件夹中，因此请确保你已在当前工作目录中创建了该文件夹。完整代码可以在本章节的GitHub仓库中的笔记本`VanillaGAN.ipynb`中找到。在接下来的部分，我们将介绍一些近期的GAN架构，并在TensorFlow中实现它们。'
- en: Deep convolutional GAN (DCGAN)
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度卷积生成对抗网络（DCGAN）
- en: Proposed in 2016, DCGANs have become one of the most popular and successful
    GAN architectures. The main idea of the design was using convolutional layers
    without the use of pooling layers or the end classifier layers. The convolutional
    strides and transposed convolutions are employed for the downsampling (the reduction
    of dimensions) and upsampling (the increase of dimensions. In GANs, we do this
    with the help of a transposed convolution layer. To know more about transposed
    convolution layers, refer to the paper *A guide to convolution arithmetic for
    deep learning* by Dumoulin and Visin) of images.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN于2016年提出，已成为最流行且成功的GAN架构之一。设计的主要思想是使用卷积层，而不使用池化层或最终的分类层。卷积步幅和转置卷积被用来进行下采样（即减少维度）和上采样（即增加维度。在GAN中，我们通过转置卷积层来实现这一点。有关转置卷积层的更多信息，请参考Dumoulin和Visin的论文《深度学习卷积算术指南》）。
- en: 'Before going into the details of the DCGAN architecture and its capabilities,
    let us point out the major changes that were introduced in the paper:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解DCGAN架构及其功能之前，让我们先指出该论文中介绍的主要变化：
- en: The network consisted of all convolutional layers. The pooling layers were replaced
    by strided convolutions (i.e., instead of one single stride while using the convolutional
    layer, we increased the number of strides to two) in the discriminator and transposed
    convolutions in the generator.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络由所有卷积层组成。池化层被替换为判别器中的步幅卷积（即，在使用卷积层时，我们将步幅数从1增加到2），而生成器中使用转置卷积。
- en: The fully connected classifying layers after the convolutions are removed.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层后的全连接分类层已被移除。
- en: To help with the gradient flow, batch normalization is done after every convolutional
    layer.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了帮助梯度流动，每个卷积层后都进行了批归一化。
- en: 'The basic idea of DCGANs is the same as the vanilla GAN: we have a generator
    that takes in noise of 100 dimensions; the noise is projected and reshaped, and
    is then passed through convolutional layers. *Figure 9.4* shows the generator
    architecture:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN的基本理念与基础GAN相同：我们有一个生成器，它接受100维的噪声；噪声被投影并重新塑形，然后通过卷积层。*图9.4*显示了生成器的架构：
- en: '![Diagram  Description automatically generated](img/B18331_09_04.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图表 描述自动生成](img/B18331_09_04.png)'
- en: 'Figure 9.4: Visualizing the architecture of a generator'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4：可视化生成器的架构
- en: The discriminator network takes in the images (either generated by the generator
    or from the real dataset), and the images undergo convolution followed by batch
    normalization. At each convolution step, the images get downsampled using strides.
    The final output of the convolutional layer is flattened and feeds a one-neuron
    classifier layer.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器网络接受图像（无论是由生成器生成的还是来自真实数据集的），这些图像会经过卷积和批量归一化处理。在每个卷积步骤中，图像会通过步幅进行下采样。卷积层的最终输出被展平并传递到一个单神经元分类器层。
- en: 'In *Figure 9.5*, you can see the discriminator:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 9.5*中，您可以看到判别器：
- en: '![Diagram  Description automatically generated](img/B18331_09_05.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图表 描述自动生成](img/B18331_09_05.png)'
- en: 'Figure 9.5: Visualizing the architecture of a discriminator'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：可视化判别器的架构
- en: The generator and the discriminator are combined together to form the DCGAN.
    The training follows in the same manner as before; that is, we first train the
    discriminator on a mini-batch, then freeze the discriminator and train the generator.
    The process is repeated iteratively for a few thousand epochs. The authors found
    that we get more stable results with the Adam optimizer and a learning rate of
    0.002.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器和判别器结合在一起形成了DCGAN。训练过程与之前相同；也就是说，我们首先在一个小批量上训练判别器，然后冻结判别器并训练生成器。这个过程会反复进行几千个周期。作者发现，使用Adam优化器和学习率0.002可以获得更稳定的结果。
- en: Next, we’ll implement a DCGAN for generating handwritten digits.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现一个DCGAN，用于生成手写数字。
- en: DCGAN for MNIST digits
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于MNIST数字的DCGAN
- en: Let us now build a DCGAN for generating handwritten digits. We first see the
    code for the generator. The generator is built by adding the layers sequentially.
    The first layer is a dense layer that takes the noise of 100 dimensions as an
    input. The 100-dimensional input is expanded to a flat vector of size 128 × 7
    × 7\. This is done so that finally, we get an output of size 28 × 28, the standard
    size of MNIST handwritten digits. The vector is reshaped to a tensor of size 7
    × 7 × 128\. This vector is then upsampled using the TensorFlow Keras `UpSampling2D`
    layer. Please note that this layer simply scales up the image by doubling rows
    and columns. The layer has no weights, so it is computationally cheap.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们构建一个用于生成手写数字的DCGAN。我们首先来看生成器的代码。生成器是通过按顺序添加层来构建的。第一层是一个密集层，输入是100维的噪声。这个100维的输入被扩展为一个大小为128
    × 7 × 7的平坦向量。这样做是为了最终得到一个28 × 28的输出，这是MNIST手写数字的标准大小。该向量被重塑为大小为7 × 7 × 128的张量。然后，使用TensorFlow
    Keras的`UpSampling2D`层对该张量进行上采样。请注意，这一层仅通过将行和列加倍来缩放图像。该层没有权重，因此计算开销很小。
- en: 'The Upsampling2D layer will now double the rows and columns of the 7 × 7 ×
    128 (rows × columns × channels) image, yielding an output of size 14 × 14 × 128\.
    The upsampled image is passed to a convolutional layer. This convolutional layer
    learns to fill in the details in the upsampled image. The output of a convolution
    is passed to batch normalization for better gradient flow. The batch normalized
    output then undergoes ReLU activation in all the intermediate layers. We repeat
    the structure, that is, upsampling | convolution | batch normalization | ReLU.
    In the following generator, we have two such structures, the first with 128 filters,
    and the second with 64 filters in the convolution operation. The final output
    is obtained from a pure convolutional layer with 3 filters and tan hyperbolic
    activation, yielding an image of size 28 × 28 × 1:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Upsampling2D层现在将7 × 7 × 128（行 × 列 × 通道）图像的行和列加倍，得到14 × 14 × 128的输出。上采样后的图像将传递到一个卷积层，该卷积层学习填充上采样图像中的细节。卷积的输出传递到批量归一化层，以改善梯度流。批量归一化后的输出在所有中间层中会经过ReLU激活。我们重复这种结构，即上采样
    | 卷积 | 批量归一化 | ReLU。在接下来的生成器中，我们有两个这样的结构，第一个卷积操作使用128个滤波器，第二个使用64个滤波器。最终的输出通过一个纯卷积层得到，该层使用3个滤波器和双曲正切激活，输出大小为28
    × 28 × 1的图像：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The resultant generator model is as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 结果生成器模型如下：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You can also experiment with the transposed convolution layer. This layer not
    only upsamples the input image but also learns how to fill in details during the
    training. Thus, you can replace upsampling and convolution layers with a single
    transposed convolution layer. The transpose convolutional layer performs an inverse
    convolution operation. You can read about it in more detail in the paper: *A guide
    to convolution arithmetic for deep learning* ([https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285)).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以尝试使用转置卷积层。这个层不仅能上采样输入图像，还能在训练过程中学习如何填充细节。因此，你可以用一个转置卷积层替换上采样和卷积层。转置卷积层执行的是反向卷积操作。你可以在论文中阅读更详细的内容：*深度学习卷积算术指南*（[https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285)）。
- en: 'Now that we have a generator, let us see the code to build the discriminator.
    The discriminator is similar to a standard convolutional neural network but with
    one major change: instead of max pooling, we use convolutional layers with strides
    of 2\. We also add dropout layers to avoid overfitting, and batch normalization
    for better accuracy and fast convergence. The activation layer is leaky ReLU.
    In the following network, we use three such convolutional layers, with filters
    of 32, 64, and 128 respectively. The output of the third convolutional layer is
    flattened and fed to a dense layer with a single unit.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了生成器，让我们看看构建判别器的代码。判别器与标准卷积神经网络相似，但有一个重大变化：我们使用步长为 2 的卷积层代替最大池化。我们还加入了
    dropout 层以避免过拟合，并使用批归一化来提高准确性和加速收敛。激活层使用的是 leaky ReLU。在接下来的网络中，我们使用了三个这样的卷积层，过滤器分别为
    32、64 和 128。第三个卷积层的输出被展平并输入到一个带有单个单元的全连接层。
- en: 'The output of this unit classifies the image as fake or real:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 该单元的输出将图像分类为伪造或真实：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The resultant discriminator network is:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 结果判别器网络是：
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The complete GAN is made by combining the two:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的 GAN 是通过将两者结合而成的：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you might have noticed, we are defining here the `binary_crossentropy` loss
    object, which we will use later to define the generator and discriminator losses.
    Optimizers for both the generator and discriminator is defined in this `init`
    method. And finally, we define a TensorFlow checkpoint that we will use to save
    the two models (generator and discriminator) as the model trains.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们在这里定义了`binary_crossentropy`损失对象，稍后我们将用它来定义生成器和判别器的损失。生成器和判别器的优化器在此`init`方法中定义。最后，我们定义了一个
    TensorFlow 检查点，用于在模型训练过程中保存两个模型（生成器和判别器）。
- en: The GAN is trained in the same manner as before; at each step, first, random
    noise is fed to the generator. The output of the generator is added with real
    images to initially train the discriminator, and then the generator is trained
    to give an image that can fool the discriminator.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 的训练方式与之前相同；每一步，首先将随机噪声输入生成器。生成器的输出与真实图像结合，初步训练判别器，然后训练生成器使其生成能够欺骗判别器的图像。
- en: 'The process is repeated for the next batch of images. The GAN takes between
    a few hundred to thousands of epochs to train:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程会对下一批图像重复进行。GAN 的训练通常需要几百到几千轮：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Lastly, we need a helper function to save images:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要一个辅助函数来保存图像：
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let us now train our GAN:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始训练我们的 GAN：
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The images generated by our GAN as it learned to fake handwritten digits are:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 GAN 在学习如何伪造手写数字后生成的图像是：
- en: '![](img/B18331_09_06.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_09_06.png)'
- en: 'Figure 9.6: Images generated by GAN – initial attempt'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：GAN 生成的图像——初步尝试
- en: 'The preceding images were the initial attempts by the GAN. As it learned through
    the following 10 epochs, the quality of digits generated improved manyfold:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图像是 GAN 的初步尝试。随着它在接下来的 10 轮训练中学习，生成的数字质量大幅提升：
- en: '![](img/B18331_09_07.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_09_07.png)'
- en: 'Figure 9.7: Images generated by GAN after 6, 8, and 10 epochs'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：GAN 在 6、8 和 10 轮训练后的生成图像
- en: 'The complete code is available in `DCGAN.ipynb` in the GitHub repo. We can
    take the concepts discussed here and apply them to images in other domains. One
    of the interesting works on images was reported in the paper, *Unsupervised Representation
    Learning with Deep Convolutional Generative Adversarial Networks*, Alec Radford,
    Luke Metz, Soumith Chintala, 2015\. Quoting the abstract:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在 GitHub 仓库中的`DCGAN.ipynb`找到。我们可以将这里讨论的概念应用到其他领域的图像上。关于图像的一个有趣研究工作发表在论文
    *无监督表示学习与深度卷积生成对抗网络* 中，作者是 Alec Radford、Luke Metz 和 Soumith Chintala，2015年。以下是论文摘要：
- en: In recent years, supervised learning with convolutional networks (CNNs) has
    seen huge adoption in computer vision applications. Comparatively, unsupervised
    learning with CNNs has received less attention. In this work we hope to help bridge
    the gap between the success of CNNs for supervised learning and unsupervised learning.
    We introduce a class of CNNs called deep convolutional generative adversarial
    networks (DCGANs), that have certain architectural constraints, and demonstrate
    that they are a strong candidate for unsupervised learning. Training on various
    image datasets, we show convincing evidence that our deep convolutional adversarial
    pair learns a hierarchy of representations from object parts to scenes in both
    the generator and discriminator. Additionally, we use the learned features for
    novel tasks - demonstrating their applicability as general image representations.
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 近年来，卷积网络（CNN）在计算机视觉应用中的监督学习得到了广泛应用。相比之下，CNN的无监督学习关注较少。在这项工作中，我们希望帮助弥合CNN在监督学习和无监督学习上的成功之间的差距。我们介绍了一类称为深度卷积生成对抗网络（DCGAN）的CNN，它们具有某些架构限制，并证明它们是无监督学习的有力候选。通过在各种图像数据集上进行训练，我们展示了令人信服的证据，表明我们的深度卷积对抗对可以在生成器和判别器中学习从物体部分到场景的层次化表示。此外，我们还使用学习到的特征进行新的任务，展示它们作为通用图像表示的适用性。
- en: ''
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Radford et al., 2015
  id: totrans-107
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —Radford等人，2015年
- en: 'Following are some of the interesting results of applying DCGANs to a celebrity
    image dataset:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是将DCGAN应用于名人图像数据集的一些有趣结果：
- en: '![A collage of a person''s face  Description automatically generated](img/B18331_09_08_1.png)![A
    collage of a person''s face  Description automatically generated](img/B18331_09_08_2.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![一个人的面部拼贴图，描述自动生成](img/B18331_09_08_1.png)![一个人的面部拼贴图，描述自动生成](img/B18331_09_08_2.png)'
- en: 'Figure 9.8: Generated celebrity images using DCGAN'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8：使用DCGAN生成的名人图像
- en: Another interesting paper is *Semantic Image Inpainting with Perceptual and
    Contextual Losses*, by Raymond A. Yeh et al. in 2016\. Just as content-aware fill
    is a tool used by photographers to fill in unwanted or missing parts of images,
    in this paper they used a DCGAN for image completion.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 另一篇有趣的论文是由Raymond A. Yeh等人于2016年发表的*《基于感知和上下文损失的语义图像修复》*。就像摄影师使用内容感知填充工具来填补图像中不需要或缺失的部分一样，本文中他们使用了DCGAN进行图像补全。
- en: As mentioned earlier, a lot of research is happening around GANs. In the next
    section, we will explore some of the interesting GAN architectures proposed in
    recent years.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，围绕GAN的研究正在进行大量探索。在下一节中，我们将探讨近年来提出的一些有趣的GAN架构。
- en: Some interesting GAN architectures
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些有趣的GAN架构
- en: Since their inception, a lot of interest has been generated in GANs, and as
    a result, we are seeing a lot of modifications and experimentation with GAN training,
    architecture, and applications. In this section, we will explore some interesting
    GANs proposed in recent years.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 自从它们问世以来，GAN（生成对抗网络）引起了大量关注，因此我们看到在GAN的训练、架构和应用方面进行了许多修改和实验。在这一节中，我们将探讨近年来提出的一些有趣的GAN。
- en: SRGAN
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SRGAN
- en: 'Remember seeing a crime thriller where our hero asks the computer guy to magnify
    the faded image of the crime scene? With the zoom, we can see the criminal’s face
    in detail, including the weapon used and anything engraved upon it! Well, **Super
    Resolution GANs** (**SRGANs**) can perform similar magic. Magic in the sense that
    because GANs show that it is possible to get high-resolution images, the final
    results depend on the camera resolution used. Here, a GAN is trained in such a
    way that it can generate a photorealistic high-resolution image when given a low-resolution
    image. The SRGAN architecture consists of three neural networks: a very deep generator
    network (which uses Residual modules; see ResNets in *Chapter 20*, *Advanced Convolutional
    Neural Networks*), a discriminator network, and a pretrained VGG-16 network.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否记得在某部犯罪惊悚片中，主角让电脑专家放大模糊的犯罪现场图像？通过放大，我们可以详细看到犯罪嫌疑人的面孔，包括使用的武器以及上面刻的任何东西！好吧，**超分辨率生成对抗网络**（**SRGANs**）可以执行类似的魔法。魔法的意思是，GAN表明通过它可以获得高分辨率图像，而最终的结果取决于所使用的相机分辨率。在这里，GAN被训练成在给定低分辨率图像时，可以生成逼真的高分辨率图像。SRGAN架构由三个神经网络组成：一个非常深的生成器网络（使用残差模块；参见*第20章*，*高级卷积神经网络*），一个判别器网络，以及一个预训练的VGG-16网络。
- en: 'SRGANs use the perceptual loss function (developed by Johnson et al; you can
    find the link to the paper in the *References* section). In SRGAN, the authors
    first downsampled a high-resolution image and used the generator to get its “high-resolution”
    version. The discriminator was trained to differentiate between the real high-resolution
    image and the generated high-resolution image. The difference in the feature map
    activations in high layers of a VGG network between the network output and the
    high-resolution parts comprises the perceptual loss function. Besides perceptual
    loss, the authors further added content loss and an adversarial loss so that images
    generated look more natural and the finer details more artistic. The perceptual
    loss is defined as the weighted sum of the content loss and adversarial loss:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: SRGAN使用感知损失函数（由Johnson等人开发；你可以在*参考文献*部分找到该论文的链接）。在SRGAN中，作者首先对高分辨率图像进行了下采样，并使用生成器生成其“高分辨率”版本。判别器被训练来区分真实的高分辨率图像和生成的高分辨率图像。在VGG网络的高层中，网络输出与高分辨率部分之间的特征图激活差异构成了感知损失函数。除了感知损失外，作者进一步添加了内容损失和对抗损失，以使得生成的图像看起来更自然，细节更具艺术感。感知损失定义为内容损失和对抗损失的加权和：
- en: '![](img/B18331_09_001.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_09_001.png)'
- en: 'The first term on the right-hand side is the content loss, obtained using the
    feature maps generated by pretrained VGG 19\. Mathematically, it is the Euclidean
    distance between the feature map of the reconstructed image (that is, the one
    generated by the generator) and the original high-resolution reference image.
    The second term on the RHS is the adversarial loss. It is the standard generative
    loss term, designed to ensure that images generated by the generator can fool
    the discriminator. You can see in the following figure that the image generated
    by the SRGAN is much closer to the original high-resolution image with a PSNR
    value of 37.61:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 右边第一个项是内容损失，通过使用预训练的VGG 19生成的特征图来获得。从数学角度来看，它是重建图像（即由生成器生成的图像）和原始高分辨率参考图像之间的欧几里得距离。右侧的第二项是对抗损失。它是标准的生成损失项，旨在确保生成器生成的图像能够欺骗判别器。你可以从下面的图像中看到，由SRGAN生成的图像在PSNR值为37.61时，已经非常接近原始高分辨率图像：
- en: '![](img/B18331_09_09.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_09_09.png)'
- en: 'Figure 9.9: An example following the paper Photo-Realistic Single Image Super-Resolution
    Using a Generative Adversarial Network, Ledig et al.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9：来自论文《使用生成对抗网络的照片级单图像超分辨率》中的示例，Ledig等人。
- en: Another noteworthy architecture is CycleGAN; proposed in 2017, it can perform
    the task of image translation. Once trained you can translate an image from one
    domain to another domain. For example, when trained on a horse and zebra dataset,
    if you give it an image with horses in the foreground, the CycleGAN can convert
    the horses to zebras with the same background. We will explore it next.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得注意的架构是CycleGAN；它在2017年提出，能够执行图像翻译任务。经过训练后，你可以将一张图像从一个领域翻译到另一个领域。例如，在训练了马和斑马的数据集后，如果你给它一张前景是马的图像，CycleGAN可以将马转化为斑马，并保持相同的背景。我们接下来将进一步探讨它。
- en: CycleGAN
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CycleGAN
- en: Have you ever imagined how some scenery would look if Van Gogh or Manet had
    painted it? We have many scenes and landscapes painted by Van Gogh/Manet, but
    we do not have any collection of input-output pairs. A CycleGAN performs the image
    translation, that is, transfers an image given in one domain (scenery, for example)
    to another domain (a Van Gogh painting of the same scene, for instance) in the
    absence of training examples. The CycleGAN’s ability to perform image translation
    in the absence of training pairs is what makes it unique.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你有没有想过如果梵高或马奈画了某个景象，它会是什么样子？我们有很多梵高/马奈画的景象和风景，但没有任何输入输出对的集合。CycleGAN执行图像翻译，即在没有训练样本的情况下，将给定领域（例如景象）的图像转换为另一个领域（例如同一场景的梵高画作）。CycleGAN在没有训练对的情况下进行图像翻译的能力使它独特。
- en: To achieve image translation, the authors used a very simple yet effective procedure.
    They made use of two GANs, the generator of each GAN performing the image translation
    from one domain to another.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现图像翻译，作者使用了一种非常简单但有效的过程。他们利用了两个GAN，每个GAN的生成器执行图像从一个领域到另一个领域的翻译。
- en: 'To elaborate, let us say the input is *X*, then the generator of the first
    GAN performs a mapping ![](img/B18331_09_002.png); thus, its output would be *Y
    = G(X)*. The generator of the second GAN performs an inverse mapping ![](img/B18331_09_003.png),
    resulting in *X = F(Y)*. Each discriminator is trained to distinguish between
    real images and synthesized images. The idea is shown as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，假设输入为 *X*，则第一个 GAN 的生成器执行映射 ![](img/B18331_09_002.png)；因此，其输出为 *Y = G(X)*。第二个
    GAN 的生成器执行反向映射 ![](img/B18331_09_003.png)，得到 *X = F(Y)*。每个判别器的训练目标是区分真实图像与合成图像。其原理如下所示：
- en: '![A picture containing text, clock, watch  Description automatically generated](img/B18331_09_10.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图片，包含文本、时钟、手表](img/B18331_09_10.png)'
- en: 'Figure 9.10: Cycle-consistency loss'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10：循环一致性损失
- en: To train the combined GANs, the authors added, besides the conventional GAN
    adversarial loss, a forward cycle-consistency loss (left figure) and a backward
    cycle-consistency loss (right figure). This ensures that if an image *X* is given
    as input, then after the two translations *F(G(X)) ~ X* the obtained image is
    the same, *X* (similarly the backward cycle-consistency loss ensures that *G(F(Y))
    ~ Y*).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练组合的 GAN，作者除了常规的 GAN 对抗损失外，还加入了正向循环一致性损失（左图）和反向循环一致性损失（右图）。这确保了如果输入图像为*X*，则经过两次转换
    *F(G(X)) ~ X* 后，得到的图像与原图像相同，*X*（同样，反向循环一致性损失确保 *G(F(Y)) ~ Y*）。
- en: 'Following are some of the successful image translations by CycleGANs [7]:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些由 CycleGAN 成功实现的图像转换示例 [7]：
- en: '![A picture containing text  Description automatically generated](img/B18331_09_11.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图片，包含文本](img/B18331_09_11.png)'
- en: 'Figure 9.11: Examples of some successful CycleGAN image translations'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11：一些成功的 CycleGAN 图像转换示例
- en: 'Following are a few more examples; you can see the translation of seasons (summer
    ![](img/B18331_09_004.png) winter), photo ![](img/B18331_09_004.png) painting
    and vice versa, and horses ![](img/B18331_09_004.png) zebras and vice versa [7]:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是更多示例；你可以看到季节的转换（夏季 ![](img/B18331_09_004.png) 冬季），照片 ![](img/B18331_09_004.png)
    绘画及其反向转换，马 ![](img/B18331_09_004.png) 斑马及其反向转换 [7]：
- en: '![A picture containing text, screenshot, display, different  Description automatically
    generated](img/B18331_09_12.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图片，包含文本、截图、显示器、不同内容](img/B18331_09_12.png)'
- en: 'Figure 9.12: Further examples of CycleGAN translations'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12：CycleGAN 转换的更多示例
- en: Later in the chapter, we will also explore a TensorFlow implementation of CycleGANs.
    Next, we talk about the InfoGAN, a conditional GAN where the GAN not only generates
    an image, but you also have a control variable to control the images generated.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后面，我们还将探索 CycleGAN 的 TensorFlow 实现。接下来，我们将讨论 InfoGAN，它是一种条件 GAN，其中 GAN 不仅生成图像，还可以通过控制变量来控制生成的图像。
- en: InfoGAN
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: InfoGAN
- en: The GAN architectures that we have considered up to now provide us with little
    or no control over the generated images. The InfoGAN changes this; it provides
    control over various attributes of the images generated. The InfoGAN uses the
    concepts from information theory such that the noise term is transformed into
    latent code that provides predictable and systematic control over the output.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们考虑的 GAN 架构几乎没有或完全没有对生成图像的控制。InfoGAN 改变了这一点；它提供了对生成图像各种属性的控制。InfoGAN
    使用信息理论的概念，将噪声项转化为潜在编码，从而对输出进行可预测且系统化的控制。
- en: 'The generator in an InfoGAN takes two inputs: the latent space *Z* and a latent
    code *c*, thus the output of the generator is *G(Z,c)*. The GAN is trained such
    that it maximizes the mutual information between the latent code *c* and the generated
    image *G(Z,c)*. The following figure shows the architecture of the InfoGAN:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: InfoGAN 中的生成器接受两个输入：潜在空间 *Z* 和潜在编码 *c*，因此生成器的输出是 *G(Z,c)*。GAN 的训练目标是最大化潜在编码
    *c* 和生成图像 *G(Z,c)* 之间的互信息。以下图展示了 InfoGAN 的架构：
- en: '![Diagram  Description automatically generated](img/B18331_09_13.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表](img/B18331_09_13.png)'
- en: 'Figure 9.13: The architecture of the InfoGAN, visualized'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13：InfoGAN 的架构，已可视化
- en: 'The concatenated vector *(Z,c)* is fed to the generator. *Q(c|X)* is also a
    neural network. Combined with the generator, it works to form a mapping between
    random noise *Z* and its latent code *c_hat*. It aims to estimate *c* given *X*.
    This is achieved by adding a regularization term to the objective function of
    the conventional GAN:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 拼接向量*(Z,c)*被输入到生成器中。*Q(c|X)* 也是一个神经网络。与生成器结合后，它工作于形成随机噪声 *Z* 和其潜在编码 *c_hat*
    之间的映射。目标是给定 *X* 来估计 *c*。这通过在传统 GAN 的目标函数中添加正则化项来实现：
- en: '![](img/B18331_09_007.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_09_007.png)'
- en: The term *V*[G]*(D,G)* is the loss function of the conventional GAN, and the
    second term is the regularization term, where ![](img/B18331_09_008.png) is a
    constant. Its value was set to 1 in the paper, and *I(c;G(Z,c))* is the mutual
    information between the latent code *c* and the generator-generated image *G(Z,c)*.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 术语 *V*[G]*(D,G)* 是传统 GAN 的损失函数，第二项是正则化项，其中 ![](img/B18331_09_008.png) 是一个常数。论文中将其值设为
    1，*I(c;G(Z,c))* 是潜在编码 *c* 和生成器生成的图像 *G(Z,c)* 之间的互信息。
- en: 'Following are the exciting results of the InfoGAN on the MNIST dataset:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 InfoGAN 在 MNIST 数据集上的一些令人兴奋的结果：
- en: '![](img/B18331_09_14.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_09_14.png)'
- en: 'Figure 9.14: Results of using the InfoGAN on the MNIST dataset. Here, different
    rows correspond to different random samples of fixed latent codes and noise'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14：使用 InfoGAN 在 MNIST 数据集上的结果。这里，不同的行对应于不同的固定潜在编码和噪声的随机样本
- en: Now, that we have seen some exciting GAN architectures, let us explore some
    cool applications of GAN.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然我们已经看过一些令人兴奋的 GAN 架构，让我们来探索一些 GAN 的酷应用。
- en: Cool applications of GANs
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAN 的酷应用
- en: 'We have seen that the generator can learn how to forge data. This means that
    it learns how to create new synthetic data that is created by the network that
    appears to be authentic and human-made. Before going into the details of some
    GAN code, we would like to share the results of the paper [6] (code is available
    online at [https://github.com/hanzhanggit/StackGAN](https://github.com/hanzhanggit/StackGAN))
    where a GAN has been used to synthesize forged images starting from a text description.
    The results are impressive: the first column is the real image in the test set
    and all the rest of the columns are the images generated from the same text description
    by Stage-I and Stage-II of StackGAN. More examples are available on YouTube ([https://www.youtube.com/watch?v=SuRyL5vhCIM&feature=youtu.be](https://www.youtube.com/watch?v=SuRyL5vhCIM&feature=youtu.be)):'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到生成器能够学会如何伪造数据。这意味着它学会了如何创建由网络生成的看似真实且人工制作的新合成数据。在深入探讨一些 GAN 代码的细节之前，我们想分享论文
    [6] 的结果（代码可以在 [https://github.com/hanzhanggit/StackGAN](https://github.com/hanzhanggit/StackGAN)
    在线获得），该论文使用 GAN 从文本描述中合成伪造图像。结果令人印象深刻：第一列是测试集中真实的图像，所有其他列是由 StackGAN 的第一阶段和第二阶段从相同的文本描述生成的图像。更多例子可以在
    YouTube 上找到（[https://www.youtube.com/watch?v=SuRyL5vhCIM&feature=youtu.be](https://www.youtube.com/watch?v=SuRyL5vhCIM&feature=youtu.be)）：
- en: '![A picture containing text, bird, outdoor, standing  Description automatically
    generated](img/B18331_09_15.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text, bird, outdoor, standing  Description automatically
    generated](img/B18331_09_15.png)'
- en: 'Figure 9.15: Image generation of birds, using GANs'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15：使用 GAN 生成的鸟类图像
- en: '![A group of flowers  Description automatically generated with low confidence](img/B18331_09_16.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![A group of flowers  Description automatically generated with low confidence](img/B18331_09_16.png)'
- en: 'Figure 9.16: Image generation of flowers, using GANs'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16：使用 GAN 生成的花卉图像
- en: 'Now let us see how a GAN can learn to “forge” the MNIST dataset. In this case,
    it is a combination of GAN and CNNs used for the generator and discriminator networks.
    In the beginning, the generator creates nothing understandable, but after a few
    iterations, synthetic forged numbers are progressively clearer and clearer. In
    this image, the panels are ordered by increasing training epochs and you can see
    the quality improving among the panels:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看 GAN 如何学会“伪造”MNIST 数据集。在这种情况下，生成器和判别器网络使用的是 GAN 和 CNN 的结合。在开始时，生成器生成的内容没有任何可理解的形式，但经过几轮迭代后，合成的伪造数字逐渐变得越来越清晰。在这张图中，面板按训练轮次递增排序，你可以看到面板之间的质量在不断提高：
- en: '![A picture containing text, furniture  Description automatically generated](img/B18331_09_17.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text, furniture  Description automatically generated](img/B18331_09_17.png)'
- en: 'Figure 9.17: Illegible initial outputs of the GAN'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.17：GAN 的初始输出难以辨认
- en: 'As the training progresses, you can see in *Figure 9.17* that the digits start
    taking a more recognizable form:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练的进行，你可以在 *图 9.17* 中看到数字开始变得更具可识别性：
- en: '![A picture containing text, computer, keyboard, electronics  Description automatically
    generated](img/B18331_09_18.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含文本、计算机、键盘、电子设备的图片  描述自动生成](img/B18331_09_18.png)'
- en: 'Figure 9.18: Improved outputs of the GAN, following further iterations'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.18：GAN 的改进输出，经过进一步迭代后
- en: '![A picture containing keyboard, computer, desk, electronics  Description automatically
    generated](img/B18331_09_19.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含键盘、计算机、桌子、电子设备的图片  描述自动生成](img/B18331_09_19.png)'
- en: 'Figure 9.19: Final outputs of the GAN, showing significant improvement from
    previous iterations'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.19：GAN 的最终输出，显示出比之前迭代显著的改进
- en: After 10,000 epochs, you can see that the handwritten digits are even more realistic.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 经过 10,000 次训练周期后，你可以看到手写数字变得更加真实。
- en: 'One of the coolest uses of GANs is doing arithmetic on faces in the generator’s
    vector *Z*. In other words, if we stay in the space of synthetic forged images,
    it is possible to see things like this: *[smiling woman] - [neutral woman] + [neutral
    man] = [smiling man]*, or like this: *[man with glasses] - [man without glasses]
    + [woman without glasses] = [woman with glasses]*. This was shown in the paper
    *Unsupervised Representation Learning with Deep Convolutional Generative Adversarial
    Networks* by Alec Radford and his colleagues in 2015\. All images in this work
    are generated by a version of GAN. They are NOT REAL. The full paper is available
    here: [http://arxiv.org/abs/1511.06434](http://arxiv.org/abs/1511.06434). Following
    are some examples from the paper. The authors also share their code in this GitHub
    repo: [https://github.com/Newmu/dcgan_code](https://github.com/Newmu/dcgan_code):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 最酷的应用之一是在生成器的向量 *Z* 中对人脸进行运算。换句话说，如果我们停留在合成伪造图像的空间中，就可以看到类似这样的事情： *[微笑的女人]
    - [中立的女人] + [中立的男人] = [微笑的男人]*，或者像这样： *[戴眼镜的男人] - [不戴眼镜的男人] + [不戴眼镜的女人] = [戴眼镜的女人]*。这一点在
    Alec Radford 及其同事于 2015 年发布的论文 *《使用深度卷积生成对抗网络的无监督表示学习》* 中得到了展示。本文中的所有图像都是通过 GAN
    的一个版本生成的。它们并不是真实的。完整论文请访问：[http://arxiv.org/abs/1511.06434](http://arxiv.org/abs/1511.06434)。以下是论文中的一些示例。作者还在这个
    GitHub 仓库中分享了他们的代码：[https://github.com/Newmu/dcgan_code](https://github.com/Newmu/dcgan_code)：
- en: '![A collage of a person''s face  Description automatically generated with medium
    confidence](img/B18331_09_20.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含人脸的拼贴图  描述自动生成，置信度较高](img/B18331_09_20.png)'
- en: 'Figure 9.20: Image arithmetic using GANs'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.20：使用 GAN 进行图像运算
- en: '**Bedrooms**: Generated bedrooms after five epochs of training:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**卧室**：经过五次训练周期后生成的卧室：'
- en: '![A collage of a house  Description automatically generated with low confidence](img/B18331_09_21.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![一张房子的拼贴图  描述自动生成，置信度较低](img/B18331_09_21.png)'
- en: 'Figure 9.21: Generated bedrooms using GAN after 5 epochs of training'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.21：使用 GAN 在经过 5 次训练周期后生成的卧室
- en: '**Album covers**: These images are generated by the GAN, but look like authentic
    album covers:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**专辑封面**：这些图像是通过 GAN 生成的，但看起来像真实的专辑封面：'
- en: '![A picture containing text, different, bunch, booth  Description automatically
    generated](img/B18331_09_22.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含文本、不同物体、摊位的图片  描述自动生成](img/B18331_09_22.png)'
- en: 'Figure 9.22: Album covers generated using DCGAN'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.22：使用 DCGAN 生成的专辑封面
- en: 'Another cool application of GANs is the generation of artificial faces. NVIDIA
    introduced a model in 2018, which it named StyleGAN (the second version, StyleGAN2,
    was released in February 2020, and the third version in 2021), which it showed
    can be used to generate realistic-looking images of people. Below you can see
    some of the realistic-looking fake people’s faces generated by StyleGAN obtained
    after training of 1,000 epochs; for better results, you will need to train more:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 的另一个酷应用是生成虚拟人脸。NVIDIA 于 2018 年推出了一种名为 StyleGAN 的模型（第二版 StyleGAN2 于 2020
    年 2 月发布，第三版于 2021 年发布），该模型展示了可以用来生成逼真的人类图像。下面是通过 StyleGAN 生成的一些逼真的虚假人物面孔，这些面孔是在经过
    1,000 次训练周期后获得的；为了获得更好的效果，你需要进行更多的训练：
- en: '![](img/B18331_09_23.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_09_23.png)'
- en: 'Figure 9.23: Fake faces generated by StyleGAN'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.23：通过 StyleGAN 生成的虚假面孔
- en: 'Not only does it generate fake images but like InfoGAN, you can control the
    features from coarse to grain. This is the official video released by NVIDIA showing
    how features affect the results: [https://www.youtube.com/watch?v=kSLJriaOumA](https://www.youtube.com/watch?v=kSLJriaOumA).
    They were able to do this by adding a non-linear mapping network after the Latent
    variable *Z*. The mapping network transformed the latent variable to a mapping
    of the same size; the output of the mapping vector is fed to different layers
    of the generator network, and this allows the StyleGAN to control different visual
    features. To know more about StyleGAN, you should read the paper *A style-based
    generator architecture for Generative Adversarial Networks* from NVIDIA Labs [10].'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 它不仅生成假图像，像InfoGAN一样，你可以从粗到细地控制特征。这是NVIDIA发布的官方视频，展示了特征如何影响结果：[https://www.youtube.com/watch?v=kSLJriaOumA](https://www.youtube.com/watch?v=kSLJriaOumA)。他们通过在潜变量*Z*之后添加一个非线性映射网络来实现这一点。映射网络将潜变量转换为相同大小的映射；映射向量的输出被馈送到生成器网络的不同层，从而允许StyleGAN控制不同的视觉特征。要了解更多关于StyleGAN的内容，您应该阅读NVIDIA实验室的论文《A
    style-based generator architecture for Generative Adversarial Networks》[10]。
- en: CycleGAN in TensorFlow
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow中的CycleGAN
- en: 'In this section, we will implement a CycleGAN in TensorFlow. The CycleGAN requires
    a special dataset, a paired dataset, from one domain of images to another domain.
    So, besides the necessary modules, we will use `tensorflow_datasets` as well.
    Also, we will make use of the library `tensorflow_examples`, we will directly
    use the generator and the discriminator from the `pix2pix` model defined in `tensorflow_examples`.
    The code here is adapted from the code here [https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现一个基于TensorFlow的CycleGAN。CycleGAN需要一个特殊的数据集，一个成对数据集，来自一个图像领域到另一个领域。因此，除了必要的模块外，我们还将使用`tensorflow_datasets`。另外，我们将使用`tensorflow_examples`库，直接使用`tensorflow_examples`中定义的`pix2pix`模型中的生成器和判别器。这里的代码来自于这里的[https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb)：
- en: '[PRE19]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'TensorFlow’s `Dataset` API contains a list of datasets. It has many paired
    datasets for CycleGANs, such as horse to zebra, apples to oranges, and so on.
    You can access the complete list here: [https://www.tensorflow.org/datasets/catalog/cycle_gan](https://www.tensorflow.org/datasets/catalog/cycle_gan).
    For our code, we will be using `summer2winter_yosemite`, which contains images
    of Yosemite (USA) in summer (Dataset A) and winter (Dataset B). We will train
    the CycleGAN to convert an input image of summer to winter and vice versa.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的`Dataset` API包含了一个数据集列表。它有许多适用于CycleGAN的成对数据集，例如从马到斑马、从苹果到橘子等等。你可以在这里访问完整的列表：[https://www.tensorflow.org/datasets/catalog/cycle_gan](https://www.tensorflow.org/datasets/catalog/cycle_gan)。在我们的代码中，我们将使用`summer2winter_yosemite`，它包含了约塞米蒂（美国）在夏季（数据集A）和冬季（数据集B）的图像。我们将训练CycleGAN将夏季图像转换为冬季图像，反之亦然。
- en: 'Let us load the data and get train and test images:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载数据并获取训练和测试图像：
- en: '[PRE20]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We need to set some hyperparameters:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要设置一些超参数：
- en: '[PRE21]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The images need to be normalized before we train the network. For better performance,
    we also add random jittering to the train images; the images are first resized
    to size 286x286, then we randomly crop them back to the size 256x256, and finally
    apply the random jitter:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们训练网络之前，图像需要进行归一化。为了更好的性能，我们还会对训练图像添加随机抖动；这些图像首先被调整为286x286的大小，然后我们随机裁剪回256x256的大小，最后应用随机抖动：
- en: '[PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The augmentation (random crop and jitter) is done only to the train images;
    therefore, we will need to separate functions for preprocessing the images, one
    for train data, and the other for test data:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强（随机裁剪和抖动）仅应用于训练图像；因此，我们需要为图像预处理分离出两个函数，一个用于训练数据，另一个用于测试数据：
- en: '[PRE23]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The preceding functions, when applied to images, will normalize them in the
    range [-1,1] and apply augmentation to train images. Let us apply this to our
    train and test datasets and create a data generator that will provide images for
    training in batches:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的函数应用于图像时，将会将其归一化到[-1,1]的范围内，并对训练图像应用增强。让我们将这些应用到我们的训练和测试数据集，并创建一个数据生成器，它将批量提供训练图像：
- en: '[PRE24]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the preceding code, the argument `num_parallel_calls` allows one to take
    benefit from multiple CPU cores in the system; one should set its value to the
    number of CPU cores in your system. If you are not sure, use the `AUTOTUNE = tf.data.AUTOTUNE`
    value so that TensorFlow dynamically determines the right number for you.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，参数`num_parallel_calls`使得可以利用系统中的多个 CPU 核心；应将其值设置为系统中 CPU 核心的数量。如果不确定，请使用`AUTOTUNE
    = tf.data.AUTOTUNE`，让 TensorFlow 动态地为你确定合适的数量。
- en: 'As mentioned in the beginning, we use a generator and discriminator from the
    `pix2pix` model defined in the `tensorflow_examples` module. We will have two
    generators and two discriminators:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们使用的是从`pix2pix`模型中提取的生成器和判别器，定义在`tensorflow_examples`模块中。我们将有两个生成器和两个判别器：
- en: '[PRE25]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Before moving ahead with the model definition, let us see the images. Each
    image is processed before plotting so that its intensity is normal:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续进行模型定义之前，让我们看看图像。每张图像在绘制之前都会进行处理，以确保其强度是正常的：
- en: '[PRE26]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![A picture containing painted  Description automatically generated](img/B18331_09_24.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing painted  Description automatically generated](img/B18331_09_24.png)'
- en: 'Figure 9.24: The input of GAN 1 and output of GAN 2 in CycleGAN architecture
    before training'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.24：训练前 CycleGAN 架构中 GAN 1 的输入和 GAN 2 的输出
- en: 'We next define the loss and optimizers. We retain the same loss functions for
    generator and discriminator as we did in DCGAN:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们定义损失和优化器。我们保留与 DCGAN 相同的生成器和判别器的损失函数：
- en: '[PRE27]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Since there are now four models, two generators and two discriminators, we
    need to define four optimizers:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 由于现在有四个模型，两个生成器和两个判别器，我们需要定义四个优化器：
- en: '[PRE28]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Additionally, in the CycleGAN, we require to define two more loss functions,
    first the cycle-consistency loss; we can use the same function for forward and
    backward cycle-consistency loss calculation. The cycle-consistency loss ensures
    that the result is close to the original input:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在 CycleGAN 中，我们需要定义两个额外的损失函数，第一个是循环一致性损失；我们可以使用相同的函数来计算前向和反向的循环一致性损失。循环一致性损失确保结果接近原始输入：
- en: '[PRE29]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We also need to define an identity loss, which ensures that if an image *Y*
    is fed to the generator, it would yield the real image *Y* or an image similar
    to *Y*. Thus, if we give our summer image generator an image of summer as input,
    it should not change it much:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要定义一个身份损失，确保如果输入给生成器一个图像*Y*，它将输出真实图像*Y*或与*Y*相似的图像。因此，如果我们给夏季图像生成器输入一张夏季图像，它不应该有太大变化：
- en: '[PRE30]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now we define the function that trains the generator and discriminator in a
    batch, a pair of images at a time. The two discriminators and the two generators
    are trained via this function with the help of the tape gradient. The training
    step can be divided into four parts:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义一个函数，在批次中训练生成器和判别器，每次处理一对图像。这两个判别器和两个生成器将通过该函数以及带有梯度带的帮助进行训练。训练步骤可以分为四个部分：
- en: Get the output images from the two generators.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取两个生成器的输出图像。
- en: Calculate the losses.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失。
- en: Calculate the gradients.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度。
- en: 'And finally, apply the gradients:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，应用梯度：
- en: '[PRE31]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We define checkpoints to save the model weights. Since it can take a while
    to train a sufficiently good CycleGAN, we save the checkpoints, and if we start
    next, we can start with loading the existing checkpoints – this will ensure that
    model starts learning from where it left:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了检查点来保存模型权重。由于训练一个足够好的 CycleGAN 可能需要一些时间，因此我们保存检查点，如果我们下次开始时，可以加载现有的检查点——这将确保模型从上次停止的地方继续学习：
- en: '[PRE32]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let us now combine it all and train the network for 100 epochs. Please remember
    that in the paper, the test network was trained for 200 epochs, so our results
    will not be that good:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将所有部分结合起来，并训练网络 100 轮。请记住，在论文中，测试网络被训练了 200 轮，因此我们的结果不会那么好：
- en: '[PRE33]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You can see some of the images generated by our CycleGAN. Generator *A* takes
    in summer photos and converts them to winter, while generator *B* takes in winter
    photos and converts them to summer:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到一些由我们的 CycleGAN 生成的图像。生成器*A*接收夏季照片并将其转换为冬季，而生成器*B*接收冬季照片并将其转换为夏季：
- en: '![A picture containing window, indoor, different  Description automatically
    generated](img/B18331_09_25.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing window, indoor, different  Description automatically
    generated](img/B18331_09_25.png)'
- en: 'Figure 9.25: Images using CycleGAN after training'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.25：训练后使用 CycleGAN 生成的图像
- en: 'We suggest you experiment with other datasets in the TensorFlow CycleGAN datasets.
    Some will be easy like apples and oranges, but some will require much more training.
    The authors also maintain a GitHub repository where they have shared their own
    implementation in PyTorch along with the links to implementations in other frameworks
    including TensorFlow: [https://github.com/junyanz/CycleGAN](https://github.com/junyanz/CycleGAN).'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议您在TensorFlow CycleGAN数据集中尝试其他数据集。一些数据集会像苹果和橙子一样容易，但一些数据集则需要更多的训练。作者还维护了一个GitHub仓库，分享了他们在PyTorch中的实现，并提供了其他框架（包括TensorFlow）中实现的链接：[https://github.com/junyanz/CycleGAN](https://github.com/junyanz/CycleGAN)。
- en: Flow-based models for data generation
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流基模型用于数据生成
- en: While both VAEs (*Chapter 8*, *Autoencoders*) and GANs do a good job of data
    generation, they do not explicitly learn the probability density function of the
    input data. GANs learn by converting the unsupervised problem to a supervised
    learning problem.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然VAE（*第8章*，*自编码器*）和GAN在数据生成方面做得很好，但它们并未显式地学习输入数据的概率密度函数。GAN通过将无监督问题转化为有监督学习问题来进行学习。
- en: VAEs try to learn by optimizing the maximum log-likelihood of the data by maximizing
    the **Evidence Lower Bound** (**ELBO**). Flow-based models differ from the two
    in that they explicitly learn data distribution ![](img/B18331_09_009.png). This
    offers an advantage over VAEs and GANs, because this makes it possible to use
    flow-based models for tasks like filling incomplete data, sampling data, and even
    identifying bias in data distributions. Flow-based models accomplish this by maximizing
    the log-likelihood estimation. To understand how, let us delve a little into its
    math.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: VAE通过最大化**证据下界**（**ELBO**）来优化数据的最大对数似然。流基模型与这两者的不同之处在于，它们显式地学习数据分布![](img/B18331_09_009.png)。这一点优于VAE和GAN，因为它使得流基模型能够用于填补缺失数据、采样数据，甚至识别数据分布中的偏差。流基模型通过最大化对数似然估计来实现这一点。为了理解如何实现这一点，我们稍微深入一些数学内容。
- en: 'Let ![](img/B18331_09_010.png) be the probability density of data *D*, and
    let ![](img/B18331_09_011.png) be the probability density approximated by our
    model *M*. The goal of a flow-based model is to find the model parameters ![](img/B18331_09_012.png)
    such that the distance between two is minimum, i.e.:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让![](img/B18331_09_010.png)表示数据*D*的概率密度，![](img/B18331_09_011.png)表示由我们的模型*M*近似的概率密度。流基模型的目标是找到模型参数![](img/B18331_09_012.png)，使得两者之间的距离最小，即：
- en: '![](img/B18331_09_013.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_09_013.png)'
- en: 'If we use the KL divergence as our distance metrics, the expression above reduces
    to:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用KL散度作为距离度量，上面的表达式可以简化为：
- en: '![](img/B18331_09_015.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_09_015.png)'
- en: This equation represents minimizing the **Negative Log-Likelihood** (**NLL**)
    (equivalent to maximizing log-likelihood estimation.)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程表示最小化**负对数似然**（**NLL**）（等同于最大化对数似然估计）。
- en: 'The basic architecture of flow-based models consists of a series of invertible
    functions, as shown in the figure below. The challenge is to find the function
    *f(x)*, such that its inverse *f*^(-1)*(x)* generates *x*’, the reconstructed
    version of the input *x*:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 流基模型的基本架构由一系列可逆函数组成，如下图所示。挑战在于找到函数*f(x)*，使得其逆函数*f*^(-1)*(x)*能够生成*x*’，即输入*x*的重构版本：
- en: '![](img/B18331_09_26.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_09_26.png)'
- en: 'Figure 9.26: Architecture of flow-based model'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.26：流基模型的架构
- en: 'There are mainly two ways flow-based models are implemented:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 流基模型的实现主要有两种方式：
- en: 'Normalized Flow: Here, the basic idea is to use a series of simple invertible
    functions to transform the complex input. As we flow through the sequence of transformations,
    we repeatedly substitute the variable with a new one, as per the change of variables
    theorem ([https://archive.lib.msu.edu/crcmath/math/math/c/c210.htm](https://archive.lib.msu.edu/crcmath/math/math/c/c210.htm)),
    and finally, we obtain a probability distribution of the target variable. The
    path that the variables z[i] traverse is the flow and the complete chain formed
    by the successive distributions is called the normalizing flow.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归一化流：这里的基本思想是使用一系列简单的可逆函数来转换复杂的输入。当我们通过一系列变换时，我们根据变量变化定理（[https://archive.lib.msu.edu/crcmath/math/math/c/c210.htm](https://archive.lib.msu.edu/crcmath/math/math/c/c210.htm)）反复地用新变量替代旧变量，最终得到目标变量的概率分布。变量z[i]所经过的路径即为流，而由连续分布形成的完整链条称为归一化流。
- en: 'The **RealNVP** (**Real-valued Non-Volume Preserving**) model proposed by Dinh
    et al., 2017, **NICE** (**Non-linear Independent Components Estimation**) by Dinh
    et al., 2015, and Glow by Knigma and Dhariwal, 2018, use the normalized flow trick:'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**RealNVP**（**Real-valued Non-Volume Preserving**）模型由Dinh等人（2017年）提出，**NICE**（**Non-linear
    Independent Components Estimation**）由Dinh等人（2015年）提出，Glow由Knigma和Dhariwal（2018年）提出，使用了归一化流技巧：'
- en: '![A picture containing diagram  Description automatically generated](img/B18331_09_27.png)'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![自动生成的图示](img/B18331_09_27.png)'
- en: 'Figure 9.27: Normalizing flow model: https://lilianweng.github.io/posts/2018-10-13-flow-models/'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.27：归一化流模型：[https://lilianweng.github.io/posts/2018-10-13-flow-models/](https://lilianweng.github.io/posts/2018-10-13-flow-models/)
- en: 'Autoregressive Flow: Models like **MADE** (**Masked Autoencoder for Distribution
    Estimation**), PixelRNN, and wavenet are based on autoregressive models. Here,
    each dimension in a vector variable is dependent on the previous dimensions. Thus,
    the probability of observing ![](img/B18331_09_016.png) depends only on ![](img/B18331_09_017.png),
    and therefore, the product of these conditional probabilities gives us the probability
    of the entire sequence.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自回归流：像**MADE**（**Masked Autoencoder for Distribution Estimation**）、PixelRNN和wavenet这样的模型基于自回归模型。在这里，向量变量中的每一维都依赖于之前的维度。因此，观察
    ![](img/B18331_09_016.png) 的概率只依赖于 ![](img/B18331_09_017.png)，因此这些条件概率的乘积给出了整个序列的概率。
- en: Lilian Weng’s blog ([https://lilianweng.github.io/posts/2018-10-13-flow-models/](https://lilianweng.github.io/posts/2018-10-13-flow-models/))
    provides a very good description of flow-based models.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Lilian Weng的博客 ([https://lilianweng.github.io/posts/2018-10-13-flow-models/](https://lilianweng.github.io/posts/2018-10-13-flow-models/))
    提供了关于基于流的模型的非常好的描述。
- en: Diffusion models for data generation
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据生成的扩散模型
- en: The 2021 paper *Diffusion Models Beat GANs on Image synthesis* by two OpenAI
    research scientists Prafulla Dhariwal and Alex Nichol garnered a lot of interest
    in diffusion models for data generation.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年，OpenAI的两位研究科学家Prafulla Dhariwal和Alex Nichol发表的论文 *扩散模型在图像合成上超越GAN* 引起了对扩散模型用于数据生成的极大关注。
- en: 'Using the **Frechet Inception Distance** (**FID**) as the metrics for evaluation
    of generated images, they were able to achieve an FID score of 3.85 on a diffusion
    model trained on ImageNet data:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**Frechet Inception Distance**（**FID**）作为生成图像评估的度量标准，他们在一个基于ImageNet数据训练的扩散模型上达到了3.85的FID分数：
- en: '![A collage of animals  Description automatically generated with medium confidence](img/B18331_09_28.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的动物拼图](img/B18331_09_28.png)'
- en: 'Figure 9.28: Selected samples of images generated from ImageNet (FID 3.85).
    Image Source: Dhariwal, Prafulla, and Alexander Nichol. “Diffusion models beat
    GANs on image synthesis.” *Advances in Neural Information Processing Systems*
    34 (2021)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.28：从ImageNet生成的图像的选定样本（FID 3.85）。图像来源：Dhariwal, Prafulla 和 Alexander Nichol.
    “扩散模型在图像合成上超越GAN。” *神经信息处理系统进展* 34（2021）
- en: 'The idea behind diffusion models is very simple. We take our input image ![](img/B18331_09_018.png),
    and at each time step (forward step), we add a Gaussian noise to it (diffusion
    of noise) such that after ![](img/B18331_09_019.png) time steps, the original
    image is no longer decipherable. And then find a model that can, starting from
    a noisy input, perform the reverse diffusion and generate a clear image:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型背后的想法非常简单。我们从输入图像 ![](img/B18331_09_018.png) 开始，在每一个时间步（正向步）中向其添加高斯噪声（噪声扩散），使得在
    ![](img/B18331_09_019.png) 步后，原始图像无法再被解读。然后找到一个模型，它可以从一个噪声输入开始，执行反向扩散并生成清晰图像：
- en: '![Diagram  Description automatically generated](img/B18331_09_29.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图示](img/B18331_09_29.png)'
- en: 'Figure 9.29: Graphical model as a Markov chain for the forward and reverse
    diffusion process'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.29：作为马尔可夫链的图模型，用于正向和反向扩散过程
- en: 'The only problem is that while the conditional probabilities ![](img/B18331_09_020.png)
    can be obtained using the reparameterization trick, the reverse conditional probability
    ![](img/B18331_09_021.png) is unknown. We train a neural network model ![](img/B18331_09_022.png)
    to approximate these conditional probabilities. Below is the training and the
    sampling algorithm used by Ho et al., 2020, in their *Denoising Diffusion Probabilistic
    Models* paper:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的问题是，虽然可以使用重新参数化技巧来获得条件概率 ![](img/B18331_09_020.png)，但反向条件概率 ![](img/B18331_09_021.png)
    是未知的。我们训练一个神经网络模型 ![](img/B18331_09_022.png) 来近似这些条件概率。以下是Ho等人（2020年）在他们的 *去噪扩散概率模型*
    论文中使用的训练和采样算法：
- en: '| **Algorithm 1 Training** | **Algorithm 2 Sampling** |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| **算法1 训练** | **算法2 采样** |'
- en: '| 1\. **repeat**2\. ![](img/B18331_09_023.png)3\. ![](img/B18331_09_024.png)4\.
    ![](img/B18331_09_025.png)5\. Take gradient descent step on ![](img/B18331_09_026.png)6.
    **until** converged | 1\. ![](img/B18331_09_027.png)2. **for** *t* = *T*, ...,
    1 **do**3\. ![](img/B18331_09_028.png)4\. ![](img/B18331_09_029.png)5. **end for**6.
    **return x**[0] |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 1\. **重复** 2\. ![](img/B18331_09_023.png) 3\. ![](img/B18331_09_024.png)
    4\. ![](img/B18331_09_025.png) 5\. 在 ![](img/B18331_09_026.png) 上执行梯度下降步骤 6\.
    **直到** 收敛 | 1\. ![](img/B18331_09_027.png) 2\. **对于** *t* = *T*, ..., 1 **执行**
    3\. ![](img/B18331_09_028.png) 4\. ![](img/B18331_09_029.png) 5\. **结束** 6\. **返回
    x**[0] |'
- en: 'Table 9.1: Training and sampling steps used by Ho et al., 2020'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1：Ho等人（2020年）使用的训练和采样步骤
- en: Diffusion models offer both tractability and flexibility – two conflicting objectives
    in generative models. However, they rely on a long Markov chain of diffusion steps
    and thus are computationally expensive. There is a lot of traction in diffusion
    models, and we hope that in the near future there will be algorithms that can
    give as fast sampling as GANs.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型提供了可处理性和灵活性这两个相互矛盾的目标，这在生成模型中是非常难得的。然而，它们依赖于一长串的扩散步骤的马尔可夫链，因此计算开销较大。扩散模型正受到广泛关注，我们希望在不久的将来会出现能够像GAN一样快速采样的算法。
- en: Summary
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'This chapter explored one of the most exciting deep neural networks of our
    times: GANs. Unlike discriminative networks, GANs have the ability to generate
    images based on the probability distribution of the input space. We started with
    the first GAN model proposed by Ian Goodfellow and used it to generate handwritten
    digits. We next moved to DCGANs where convolutional neural networks were used
    to generate images and we saw the remarkable pictures of celebrities, bedrooms,
    and even album artwork generated by DCGANs. Finally, the chapter delved into some
    awesome GAN architectures: the SRGAN, CycleGAN, InfoGAN, and StyleGAN. The chapter
    also included an implementation of the CycleGAN in TensorFlow 2.0.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了我们这个时代最令人兴奋的深度神经网络之一：GAN。与判别网络不同，GAN有能力基于输入空间的概率分布生成图像。我们从Ian Goodfellow提出的第一个GAN模型开始，并用它生成手写数字。接着，我们介绍了DCGAN，其中使用卷积神经网络生成图像，我们看到了由DCGAN生成的名人肖像、卧室甚至专辑封面等令人惊叹的图片。最后，本章深入探讨了一些令人惊叹的GAN架构：SRGAN、CycleGAN、InfoGAN和StyleGAN。本章还包括了在TensorFlow
    2.0中实现CycleGAN的代码。
- en: In this chapter and the ones before it, we have been continuing with different
    unsupervised learning models, with both autoencoders and GANs examples of self-supervised
    learning; the next chapter will further detail the difference between self-supervised,
    joint, and contrastive learning.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章以及之前的章节中，我们一直在继续探讨不同的无监督学习模型，其中自编码器和GAN是自监督学习的例子；下一章将进一步详细阐述自监督学习、联合学习和对比学习之间的区别。
- en: References
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Goodfellow, Ian J. (2014). *On Distinguishability Criteria for Estimating Generative
    Models*. arXiv preprint arXiv:1412.6515: [https://arxiv.org/pdf/1412.6515.pdf](https://arxiv.org/pdf/1412.6515.pdf)'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Goodfellow, Ian J. (2014). *生成模型估计的可区分性标准*. arXiv预印本 arXiv:1412.6515: [https://arxiv.org/pdf/1412.6515.pdf](https://arxiv.org/pdf/1412.6515.pdf)'
- en: 'Dumoulin, Vincent, and Visin, Francesco. (2016). *A guide to convolution arithmetic
    for deep learning*. arXiv preprint arXiv:1603.07285: [https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285)'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Dumoulin, Vincent, 和 Visin, Francesco. (2016). *深度学习的卷积算术指南*. arXiv预印本 arXiv:1603.07285:
    [https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285)'
- en: 'Salimans, Tim, et al. (2016). *Improved Techniques for Training GANs*. Advances
    in neural information processing systems: [http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf](http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf%20)'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Salimans, Tim 等人. (2016). *改进的GAN训练技术*. 神经信息处理系统进展: [http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf](http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf%20)'
- en: 'Johnson, Justin, Alahi, Alexandre, and Fei-Fei, Li. (2016). *Perceptual Losses
    for Real-Time Style Transfer and Super-Resolution*. European conference on computer
    vision. Springer, Cham: [https://arxiv.org/abs/1603.08155](https://arxiv.org/abs/1603.08155)'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Johnson, Justin, Alahi, Alexandre, 和 Fei-Fei, Li. (2016). *实时风格迁移与超分辨率的感知损失*.
    欧洲计算机视觉会议. Springer, Cham: [https://arxiv.org/abs/1603.08155](https://arxiv.org/abs/1603.08155)'
- en: 'Radford, Alec, Metz, Luke., and Chintala, Soumith. (2015). *Unsupervised Representation
    Learning with Deep Convolutional Generative Adversarial Networks*. arXiv preprint
    arXiv:1511.06434: [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Radford, Alec, Metz, Luke., 和 Chintala, Soumith. (2015). *使用深度卷积生成对抗网络进行无监督表示学习*.
    arXiv预印本 arXiv:1511.06434: [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)'
- en: 'Ledig, Christian, et al. (2017). *Photo-Realistic Single Image Super-Resolution
    Using a Generative Adversarial Network*. Proceedings of the IEEE conference on
    computer vision and pattern recognition: [http://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf)'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ledig, Christian, 等人。（2017）。*使用生成对抗网络进行逼真的单幅图像超分辨率*。IEEE计算机视觉与模式识别会议论文集：[http://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf)
- en: 'Zhu, Jun-Yan, et al. (2017). *Unpaired Image-to-Image Translation using Cycle-Consistent
    Adversarial Networks*. Proceedings of the IEEE international conference on computer
    vision: [http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf)'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhu, Jun-Yan, 等人。（2017）。*使用循环一致对抗网络进行未配对的图像到图像翻译*。IEEE国际计算机视觉会议论文集：[http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf)
- en: Karras, Tero, Laine, Samuli, and Aila, Timo. (2019). *A style-based generator
    architecture for generative adversarial networks*. In Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition, pp. 4401-4410.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Karras, Tero, Laine, Samuli 和 Aila, Timo。（2019）。*一种基于风格的生成对抗网络生成器架构*。IEEE/CVF计算机视觉与模式识别会议论文集，页码4401-4410。
- en: 'Chen, Xi, et al. (2016). *InfoGAN: Interpretable Representation Learning by
    Information Maximizing Generative Adversarial Nets*. Advances in neural information
    processing systems: [https://arxiv.org/abs/1606.03657](https://arxiv.org/abs/1606.03657)'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Chen, Xi, 等人。（2016）。*InfoGAN：通过信息最大化生成对抗网络进行可解释的表示学习*。神经信息处理系统进展：[https://arxiv.org/abs/1606.03657](https://arxiv.org/abs/1606.03657)
- en: 'TensorFlow implementation of the StyleGAN: [https://github.com/NVlabs/stylegan](https://github.com/NVlabs/stylegan)'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: StyleGAN的TensorFlow实现：[https://github.com/NVlabs/stylegan](https://github.com/NVlabs/stylegan)
- en: Join our book’s Discord space
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord社区
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区，和志同道合的人一起学习，和超过2000名成员一起成长，链接：[https://packt.link/keras](https://packt.link/keras)
- en: '![](img/QR_Code18312172242788196873.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code18312172242788196873.png)'
