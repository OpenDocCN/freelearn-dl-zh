- en: 9\. Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9. 循环神经网络
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, you will learn how to handle real sequential data. You will
    extend your knowledge of **artificial neural network** (**ANN**) models and **recurrent
    neural network** (**RNN**) architecture for training sequential data. You will
    also learn how to build an RNN model with an LSTM layer for natural language processing.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何处理真实的序列数据。您将扩展对**人工神经网络**（**ANN**）模型和**循环神经网络**（**RNN**）架构的理解，以便训练序列数据。您还将学习如何构建一个具有
    LSTM 层的 RNN 模型，用于自然语言处理。
- en: By the end of this chapter, you will have gained hands-on experience of applying
    multiple LSTM layers to build RNNs for stock price predictions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将通过实际操作获得应用多个 LSTM 层构建 RNN 用于股票价格预测的经验。
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: Sequential data refers to datasets in which each data point is dependent on
    the previous ones. Think of it like a sentence, which is composed of a sequence
    of words that are related to each other. A verb will be linked to a subject and
    an adverb will be related to a verb. Another example is a stock price, where the
    price on a particular day is related to the price of the previous days. Traditional
    neural networks are not fit for processing this kind of data. There is a specific
    type of architecture that can ingest sequences of data. This chapter will introduce
    you to such models—known as **recurrent neural networks** (**RNNs**).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 序列数据指的是每个数据点都依赖于前一个数据点的数据集。可以把它想象成一句话，由一系列彼此相关的单词组成。动词将与主语相关，副词则与动词相关。另一个例子是股票价格，其中某一天的价格与前几天的价格相关。传统的神经网络并不适合处理这类数据。存在一种特定类型的架构，可以处理数据序列。本章将介绍这种模型——即**循环神经网络**（**RNN**）。
- en: An RNN model is a specific type of deep learning architecture in which the output
    of the model feeds back into the input. Models of this kind have their own challenges
    (known as vanishing and exploding gradients) that will be addressed later in the chapter.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 模型是一种特定类型的深度学习架构，其中模型的输出会反馈到输入中。这类模型有其自身的挑战（称为消失梯度和爆炸梯度），这些问题将在本章稍后讨论。
- en: In many ways, an RNN is a representation of how a brain might work. RNNs use
    memory to help them learn. But how can they do this if information only flows
    in one direction? To understand this, you'll need to first review sequential data.
    This is a type of data that requires a working memory to process data effectively.
    Until now, you have only explored non-sequential models, such as a perceptron
    or CNN. In this chapter, you will look at sequential models such as RNN, LSTM,
    or GRU.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多方面，RNN 代表了大脑可能如何工作。RNN 使用记忆来帮助它们学习。但是，信息只沿一个方向流动，它们怎么做到这一点呢？要理解这一点，您需要先回顾序列数据。这是一种需要工作记忆才能有效处理数据的数据类型。到目前为止，您只探索了非序列模型，如感知机或
    CNN。在本章中，您将研究诸如 RNN、LSTM 或 GRU 之类的序列模型。
- en: '![Figure 9.1: Sequential versus non-sequential models'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.1：序列模型与非序列模型'
- en: '](img/B16341_09_01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_09_01.jpg)'
- en: 'Figure 9.1: Sequential versus non-sequential models'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1：序列模型与非序列模型
- en: Sequential Data
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列数据
- en: Sequential data is information that happens in a sequence and is related to
    past and future data. An example of sequential data is time series data; as you
    perceive it, time only travels in one direction.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 序列数据是按顺序发生并与过去和未来数据相关的信息。一个序列数据的例子是时间序列数据；就像你感知的时间一样，时间只朝一个方向流动。
- en: Suppose you have a ball (as in *Figure 9.2*), and you want to predict where
    this ball will travel next. If you have no prior information about the direction
    from which the ball was thrown, you will simply have to guess. However, if in
    addition to the ball's current location, you also had information about its previous
    location, the problem would be much simpler. To be able to predict the ball's
    next location, you need the previous location information in a sequential (or
    ordered) form to make a prediction about future events.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有一个球（如*图 9.2*所示），并且您想预测这个球接下来会朝哪个方向移动。如果您没有关于球被投掷的方向的任何先前信息，那么您只能猜测。然而，如果除了球的当前位置之外，您还拥有球之前位置的信息，问题就变得简单多了。为了能够预测球的下一个位置，您需要将之前位置的信息以序列（或有序）形式提供，以便对未来事件做出预测。
- en: '![Figure 9.2: Direction of the ball'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.2：球的运动方向'
- en: '](img/B16341_09_02.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_09_02.jpg)'
- en: 'Figure 9.2: Direction of the ball'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：球的运动方向
- en: RNNs function in a way that allows the sequence of the information to retain
    value with the help of internal memory.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）能够通过内部记忆帮助信息的顺序保持有效，从而发挥作用。
- en: You'll take a look at some examples of sequential data in the following section.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将展示一些顺序数据的示例。
- en: Examples of Sequential Data
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 顺序数据的示例
- en: Sequential data is a specific type of data where the order of each piece of
    information is important, and they all depend on each other.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序数据是一种特殊类型的数据，其中每一条信息的顺序很重要，它们彼此依赖。
- en: One example of sequential data is financial data, such as stock prices. If you
    want to predict future data values for a given stock, you need to use previous
    values in time. In fact, you will work on stock prediction in *Exercise 9.01*,
    *Training an ANN for Sequential Data – Nvidia Stock Prediction*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序数据的一个例子是金融数据，比如股票价格。如果你想预测给定股票的未来数据值，就需要使用先前的时间值。事实上，你将在*练习 9.01*中进行股票预测，*为顺序数据训练一个人工神经网络——Nvidia
    股票预测*。
- en: 'Audio and text can also be considered sequential data. Audio can be split up
    into a sequence of sound waves, and text can be split up into sequences of either
    characters or words. The sound waves or sequences of characters or words should
    be processed in order to convey the desired result. Beyond these two examples
    that you encounter every day, there are many more examples in which sequential
    processing may be useful, from analyzing medical signals such as EEGs, projecting
    stock prices, and inferring and understanding genomic sequences. There are three
    categories of sequential data:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 音频和文本也可以视为顺序数据。音频可以分割成一系列声波，而文本可以分割成一系列字符或单词。声波或字符、单词序列应该按照顺序处理，以传达预期的结果。除了这两个日常遇到的例子外，顺序处理在很多其他领域也非常有用，比如分析医学信号（如脑电图）、预测股票价格以及推断和理解基因序列等。顺序数据有三种类型：
- en: '**Many-to-One** produces one output from many inputs.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对一** 从多个输入产生一个输出。'
- en: '**One-to-Many** produces many outputs from one input.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一对多** 从一个输入产生多个输出。'
- en: '**Many-to-Many** produces many outputs from many inputs.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对多** 从多个输入产生多个输出。'
- en: '![Figure 9.3: Categories of sequential data'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.3：顺序数据的分类'
- en: '](img/B16341_09_03.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_09_03.jpg)'
- en: 'Figure 9.3: Categories of sequential data'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3：顺序数据的分类
- en: 'Consider another example. Suppose you have a language model with a sentence
    or a phrase and you are trying to predict the word that comes next, as in the
    following figure:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是，假设你有一个语言模型，输入是一个句子或短语，你正在尝试预测接下来的单词，如下图所示：
- en: '![Figure 9.4: Sentence example'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.4：句子示例'
- en: '](img/B16341_09_04.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_09_04.jpg)'
- en: 'Figure 9.4: Sentence example'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4：句子示例
- en: Say you're given the words `yesterday I took my car out for a…`, and you want
    to try to predict the next word, `drive`. One way you could do this is by building
    a deep neural network such as a feed-forward neural network. However, you would
    immediately run into a problem. A feed-forward network can only take a fixed-length
    input vector as its input; you have to specify the size of that input right from
    the start.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，你被给定了这句话 `yesterday I took my car out for a…`，并且你想预测下一个单词是 `drive`。你可以通过构建一个深度神经网络（如前馈神经网络）来实现这一点。然而，你会立即遇到一个问题：前馈网络只能接受固定长度的输入向量；你必须从一开始就指定输入的大小。
- en: Because of this, your model needs a way to be able to handle variable-length
    inputs. One way you can do this is by using a fixed window. That means that you
    force your input vector to be just a certain length. For example, you can split
    the sentence into groups of two consecutive words (also called a **bi-gram**)
    and predict the next one. This means that no matter where you're trying to make
    that next prediction, your model will only be taking in the previous two words
    as its input. You need to consider how you can numerically represent this data.
    One way you can do this is by taking a fixed-length vector and allocating some
    space in that vector for the first word and some space in that vector for the
    second word. In those spaces, encode the identity of each word. However, this
    is problematic.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你的模型需要能够处理可变长度输入的方式。你可以通过使用固定窗口来实现这一点。这意味着你强制将输入向量限制为特定的长度。例如，你可以将句子拆分为两个连续单词的组（也称为**双元语法**），然后预测下一个单词。这意味着，无论你尝试在哪个位置进行下一步预测，模型仅会将前两个单词作为输入。你需要考虑如何以数字的形式表示这些数据。一种方法是使用固定长度的向量，并为第一个单词分配一些空间，为第二个单词分配一些空间。在这些空间中，编码每个单词的身份。然而，这种方法存在问题。
- en: Why? Because you're using only a portion of the information available (that
    is, two consecutive words only). You have access to a limited window of data that
    doesn't give enough context to accurately predict what will be the next word.
    That means you cannot effectively model long-term dependencies. This is important
    in sentences like the one in *Figure 9.5* where you clearly need information from
    much earlier in the sentence to be able to accurately predict the next word.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么？因为你只使用了部分可用信息（即仅仅是两个连续的单词）。你只能访问有限的数据窗口，这不足以提供足够的上下文来准确预测下一个单词。这意味着你无法有效地建模长期依赖性。这在像*图
    9.5*中的句子中非常重要，在那里你显然需要来自句子早期的信息，以便准确预测下一个单词。
- en: '![Figure 9.5: Sentence example'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.5：句子示例'
- en: '](img/B16341_09_05.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_09_05.jpg)'
- en: 'Figure 9.5: Sentence example'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：句子示例
- en: If you were only looking at the past two or three words, you wouldn't be able
    to make this next prediction, which you know is `Italian`. So, this means that
    you really need a way to integrate the information in the sentence from start
    to finish.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只看过去的两个或三个单词，你就无法做出下一个预测，而你知道这个预测是`意大利语`。因此，这意味着你确实需要一种方法来整合从句子开始到结束的所有信息。
- en: To do this, you could use a set of counts as a fixed-length vector and use the
    entire sentence. This method is known as **bag of words**.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，你可以使用一组计数作为固定长度向量，并使用整个句子。这种方法被称为**词袋模型**。
- en: You have a fixed-length vector regardless of the identity of the sentence, but
    what differs is adding the counts over this vocabulary. You can feed this into
    your model as an input to generate a prediction.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一个固定长度的向量，不管句子的身份是什么，但不同之处在于，添加了对这个词汇表的计数。你可以将其作为输入传递给模型，以生成预测结果。
- en: However, there's another big problem with this. Using just the counts means
    that you lose all sequential information and all information about the prior history.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里还有一个大问题。仅仅使用计数意味着你失去了所有的顺序信息以及所有关于前历史的信息。
- en: Consider *Figure 9.6*. So, these two sentences, which have completely opposite
    semantic meanings would have the exact same representations in this bag of words
    format. This is because they have the exact same list of words, just in a different
    order. So, obviously, this isn't going to work. Another idea could be simply to
    extend the fixed window.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考*图 9.6*。所以，这两句话，尽管语义完全相反，但在这种词袋格式中会有完全相同的表示。因为它们有完全相同的单词列表，只是顺序不同。所以，显然，这种方法行不通。另一个想法是简单地扩展固定窗口。
- en: '![Figure 9.6: Bag of words example'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.6：词袋示例'
- en: '](img/B16341_09_06.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_09_06.jpg)'
- en: 'Figure 9.6: Bag of words example'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：词袋示例
- en: Now, consider *Figure 9.7*. You can represent your sentence in this way, feed
    the sentence into your model, and generate your prediction. The problem is that
    if you were to feed this vector into a feed-forward neural network, each of these
    inputs, `yesterday I took my car`, would have a separate weight connecting it
    to the network. So, if you were to repeatedly see the word `yesterday` at the
    beginning of the sentence, the network may be able to learn that `yesterday` represents
    a time or a setting. However, if `yesterday` were to suddenly appear later in
    that fixed-length vector, at the end of a sentence, the network may have difficulty
    understanding the meaning of `yesterday`. This is because the parameters that
    are at the end of a vector may never have seen the term `yesterday` before, and
    the parameters from the beginning of the sentence weren't shared across the entire
    sequence.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑*图9.7*。你可以这样表示你的句子，将句子输入到模型中，并生成预测。问题是，如果你将这个向量输入到一个前馈神经网络中，这些输入（`yesterday
    I took my car`）每个都会有一个单独的权重与网络连接。因此，如果你反复看到句子开头的单词`yesterday`，网络可能会学会`yesterday`代表一个时间或环境。然而，如果`yesterday`突然出现在固定长度向量的后面，在句子的末尾，网络可能会很难理解`yesterday`的含义。这是因为位于向量末尾的参数可能以前从未见过`yesterday`这个词，而句子开头的参数没有在整个序列中共享。
- en: '![Figure 9.7: Sentence example'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.7：句子示例'
- en: '](img/B16341_09_07.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_09_07.jpg)'
- en: 'Figure 9.7: Sentence example'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7：句子示例
- en: 'So, you need to be able to handle variable-length input and long-term dependencies,
    track sequential order, and have parameters that can be shared across the entirety
    of your sequence. Specifically, you need to develop models that can do the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你需要能够处理可变长度的输入和长期依赖，跟踪顺序，并且具有可以在整个序列中共享的参数。具体来说，你需要开发能够执行以下操作的模型：
- en: Handle variable-length input sequences.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理可变长度的输入序列。
- en: Track long-term dependencies in the data.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪数据中的长期依赖。
- en: Maintain information about the sequence's order.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持序列顺序的信息。
- en: Share parameters across the entirety of the sequence.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在整个序列中共享参数。
- en: How can you do this with a model where information only flows in one direction?
    You need a different kind of neural network. You need a recursive model. You will
    practice processing sequential data in the following exercise.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在信息仅朝一个方向流动的模型中做到这一点？你需要一种不同的神经网络。你需要一个递归模型。你将在以下练习中实践处理顺序数据。
- en: 'Exercise 9.01: Training an ANN for Sequential Data – Nvidia Stock Prediction'
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习9.01：为顺序数据训练ANN——Nvidia股票预测
- en: In this exercise, you will build a simple ANN model to predict the Nvidia stock
    price. But unlike examples from previous chapters, this time the input data is
    sequential. So, you need to manually do some processing to create a dataset that
    will contain the price of the stock for a given day as the target variable and
    the price for the previous 60 days as features. You are required to split the
    data into training and testing sets before and after the date `2019-01-01`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，你将构建一个简单的人工神经网络（ANN）模型来预测Nvidia股票价格。但与之前章节的示例不同，这次输入数据是顺序的。因此，你需要手动处理一些数据，创建一个数据集，该数据集将包含给定日期的股票价格作为目标变量，并且将前60天的价格作为特征。你需要在`2019-01-01`之前和之后将数据分为训练集和测试集。
- en: Note
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the `NVDA.csv` dataset here: [https://packt.link/Mxi80](https://packt.link/Mxi80).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到`NVDA.csv`数据集：[https://packt.link/Mxi80](https://packt.link/Mxi80)。
- en: Open a new Jupyter or Colab notebook.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter或Colab笔记本。
- en: 'Import the libraries needed. Use `numpy` for computation, `matplotlib` for
    plotting visualization, `pandas` to help work with your dataset, and `MinMaxScaler`
    to scale the dataset between zero and one:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库。使用`numpy`进行计算，`matplotlib`用于绘制可视化，`pandas`帮助处理数据集，`MinMaxScaler`用于将数据集缩放到0和1之间：
- en: '[PRE0]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Use the `read_csv()` function to read in the CSV file and store your dataset
    in a pandas DataFrame, `data`, for manipulation:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`read_csv()`函数读取CSV文件，并将数据集存储在pandas的DataFrame中，命名为`data`，以便进行操作：
- en: '[PRE1]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Call the `head()` function on your data to take a look at the first five rows
    of your DataFrame:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对你的数据调用`head()`函数，查看DataFrame的前五行：
- en: '[PRE2]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You should get the following output:'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.8: First five rows of output'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图9.8：输出的前五行'
- en: '](img/B16341_09_08.jpg)'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_08.jpg)'
- en: 'Figure 9.8: First five rows of output'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.8：输出的前五行
- en: The preceding table shows the raw data. You can see that each row represents
    a day where you have information about the stock price when the market opened
    and closed, the highest price, the lowest price, and the adjusted close price
    of the stock (taking into account dividend or stock split, for instance).
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上表展示了原始数据。你可以看到，每一行代表一天，包含了当天开盘价和收盘价、最高价、最低价以及调整后的收盘价（例如考虑到股息或股票拆分等因素）。
- en: 'Now, split the training data. Use all data that is older than `2019-01-01`
    using the `Date` column for your training data. Save it as `data_training`. Save
    this in a separate file by using the `copy()` method:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，拆分训练数据。使用`Date`列中所有2019年1月1日之前的数据作为训练数据。将其保存为`data_training`。通过使用`copy()`方法，将其保存在单独的文件中：
- en: '[PRE3]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, split the test data. Use all data that is more recent than or equal to
    `2019-01-01` using the `Date` column. Save it as `data_test`. Save this in a separate
    file by using the `copy()` method:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，拆分测试数据。使用`Date`列中所有2019年1月1日及之后的数据，将其保存为`data_test`。通过使用`copy()`方法，将其保存在单独的文件中：
- en: '[PRE4]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Use `drop()` to remove your `Date` and `Adj Close` columns in your DataFrame.
    Remember that you used the `Date` column to split your training and test sets,
    so the date information is not needed. Use `axis = 1` to specify that you also
    want to drop labels from your columns. To make sure it worked, call the `head()`
    function to take a look at the first five rows of the DataFrame:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`drop()`方法删除数据框中的`Date`和`Adj Close`列。记住，你使用了`Date`列来划分训练集和测试集，因此不需要日期信息。使用`axis
    = 1`来指定你还希望删除列标签。为了确保操作成功，调用`head()`函数查看数据框的前五行：
- en: '[PRE5]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You should get the following output:'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到如下输出：
- en: '![Figure 9.9: New training data'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.9：新的训练数据'
- en: '](img/B16341_09_09.jpg)'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_09.jpg)'
- en: 'Figure 9.9: New training data'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.9：新的训练数据
- en: This is the output you should get after removing those two columns.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是你在删除那两列之后应该得到的输出：
- en: 'Create a scaler from `MinMaxScaler` to scale `training_data` to numbers between
    zero and one. Use the `fit_transform` function to fit the model to the data and
    then transform the data according to the fitted model:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个来自`MinMaxScaler`的缩放器，将`training_data`缩放到0和1之间。使用`fit_transform`函数将模型拟合到数据并根据拟合的模型转换数据：
- en: '[PRE6]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You should get the following output:'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到如下输出：
- en: '![Figure 9.10: Scaled training data'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.10：已缩放的训练数据'
- en: '](img/B16341_09_10.jpg)'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_10.jpg)'
- en: 'Figure 9.10: Scaled training data'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.10：已缩放的训练数据
- en: 'Split your data into `X_train` and `y_train` datasets:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拆分为`X_train`和`y_train`数据集：
- en: '[PRE7]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Check the shape of `training_data`:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查`training_data`的形状：
- en: '[PRE8]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You should get the following output:'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到如下输出：
- en: '[PRE9]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can see there are 868 observations in the training set.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以看到训练集包含868个观测值。
- en: 'Create a training dataset that has the previous 60 days'' stock prices so that
    you can predict the closing stock price for day 61\. Here, `X_train` will have
    two columns. The first column will store the values from 0 to 59, and the second
    will store values from 1 to 60\. In the first column of `y_train`, store the 61st
    value at index 60, and in the second column, store the 62nd value at index 61\.
    Use a `for` loop to create data in 60 time steps:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个训练数据集，包含过去60天的股价数据，这样你就可以预测第61天的收盘价。在这里，`X_train`将有两列。第一列将存储从0到59的值，第二列将存储从1到60的值。在`y_train`的第一列存储第61个值（索引为60），在第二列存储第62个值（索引为61）。使用`for`循环创建60个时间步的数据：
- en: '[PRE10]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Convert `X_train` and `y_train` into NumPy arrays:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`X_train`和`y_train`转换为NumPy数组：
- en: '[PRE11]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Call the `shape()` function on `X_train` and `y_train`:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对`X_train`和`y_train`调用`shape()`函数：
- en: '[PRE12]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You should get the following output:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到如下输出：
- en: '[PRE13]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The preceding snippet shows that the prepared training set contains `808` observations
    with `60` days of data for the five features you kept (`Open`, `Low`, `High`,
    `Close`, and `Volume`).
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上面的代码片段显示，准备好的训练集包含`808`个观测值，数据集包含60天的五个特征（`Open`、`Low`、`High`、`Close`和`Volume`）。
- en: 'Transform the data into a 2D matrix with the shape of the sample (the number
    of samples and the number of features in each sample). Stack the features for
    all 60 days on top of each other to get an output size of `(808, 300)`. Use the
    following code for this purpose:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据转换为形状为样本矩阵（每个样本的样本数和特征数）的二维矩阵。将60天的所有特征堆叠在一起，得到输出形状为`(808, 300)`。使用以下代码完成此操作：
- en: '[PRE14]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You should get the following output:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到如下输出：
- en: '[PRE15]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, build an ANN. You will need some additional libraries for this. Use `Sequential`
    to initialize the neural net, `Input` to add an input layer, `Dense` to add a
    dense layer, and `Dropout` to help prevent overfitting:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，构建一个ANN。你需要一些额外的库来实现这一点。使用`Sequential`来初始化神经网络，`Input`来添加输入层，`Dense`来添加全连接层，`Dropout`来帮助防止过拟合：
- en: '[PRE16]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Initialize the neural network by calling `regressor_ann = Sequential()`.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`regressor_ann = Sequential()`初始化神经网络。
- en: '[PRE17]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Add an input layer with `shape` as `300`:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个输入层，`shape`设置为`300`：
- en: '[PRE18]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, add the first dense layer. Set it to `512` units, which will be your
    dimensionality for the output space. Use a ReLU activation function. Finally,
    add a dropout layer that will remove 20% of the units during training to prevent overfitting:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，添加第一层全连接层，设置为`512`个单元，这将是你输出空间的维度。使用ReLU激活函数。最后，添加一个丢弃层，在训练过程中移除20%的单元，以防止过拟合：
- en: '[PRE19]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Add another dense layer with `128` units, ReLU as the activation function,
    and a dropout of `0.3`:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加另一个具有`128`个单元的全连接层，使用ReLU作为激活函数，丢弃率为`0.3`：
- en: '[PRE20]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Add another dense layer with `64` units, ReLU as the activation function, and
    a dropout of `0.4`:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加另一个具有`64`个单元的全连接层，使用ReLU作为激活函数，丢弃率为`0.4`：
- en: '[PRE21]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Again, add another dense layer with `128` units, ReLU as the activation function,
    and a dropout of `0.3`:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次添加一个具有`128`个单元的全连接层，使用ReLU作为激活函数，丢弃率为`0.3`：
- en: '[PRE22]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Add a final dense layer with one unit:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个具有一个单元的最终全连接层：
- en: '[PRE23]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Check the summary of the model:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看模型的摘要：
- en: '[PRE24]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You will get valuable information about your model layers and parameters.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得关于模型层和参数的宝贵信息。
- en: '![Figure 9.11: Model summary'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.11: 模型摘要'
- en: '](img/B16341_09_11.jpg)'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_11.jpg)'
- en: 'Figure 9.11: Model summary'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 9.11: 模型摘要'
- en: 'Use the `compile()` method to configure your model for training. Choose Adam
    as your optimizer and mean squared error to measure your loss function:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`compile()`方法配置你的模型以进行训练。选择Adam作为优化器，并使用均方误差作为损失函数的度量标准：
- en: '[PRE25]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, fit your model and set it to run on `10` epochs. Set your batch size
    to `32`:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，拟合你的模型并设置训练周期为`10`。将批量大小设置为`32`：
- en: '[PRE26]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You should get the following output:'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.12: Training the model'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.12: 训练模型'
- en: '](img/B16341_09_12.jpg)'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_12.jpg)'
- en: 'Figure 9.12: Training the model'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 9.12: 训练模型'
- en: 'Test and predict the stock price and prepare the dataset. Check your data by
    calling the `head()` method:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试并预测股票价格，并准备数据集。通过调用`head()`方法检查数据：
- en: '[PRE27]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You should get the following output:'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.13: First five rows of a DataFrame'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.13: DataFrame的前五行'
- en: '](img/B16341_09_13.jpg)'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_13.jpg)'
- en: 'Figure 9.13: First five rows of a DataFrame'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 9.13: DataFrame的前五行'
- en: 'Use the `tail(60)` method to create a `past_60_days` variable, which consists
    of the last 60 days of data in the training set. Add the `past_60_days` variable
    to the test data with the `append()` function. Assign `True` to `ignore_index`:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tail(60)`方法创建一个`past_60_days`变量，包含训练集中最后60天的数据。将`past_60_days`变量通过`append()`函数添加到测试数据中，并将`ignore_index`设置为`True`：
- en: '[PRE28]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, prepare your test data for predictions by repeating what you did for the
    training data in *steps 8* to *15*:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，通过重复你在*步骤8*到*15*中的操作，准备你的测试数据进行预测：
- en: '[PRE29]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You should get the following output:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE30]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Test some predictions for your stock prices by calling the `predict()` method
    on `X_test`:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对你的股票价格进行一些预测，通过对`X_test`调用`predict()`方法进行测试：
- en: '[PRE31]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Before looking at the results, reverse the scaling you did earlier so that
    the number you get as output will be at the correct scale using the `StandardScaler`
    utility class that you imported with `scaler.scale_`:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在查看结果之前，先使用你之前导入的`StandardScaler`工具类中的`scaler.scale_`方法反转之前的缩放操作，这样输出的数值将处于正确的尺度：
- en: '[PRE32]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'You should get the following output:'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.14: Using StandardScaler'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.14: 使用StandardScaler'
- en: '](img/B16341_09_14.jpg)'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_14.jpg)'
- en: 'Figure 9.14: Using StandardScaler'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 9.14: 使用StandardScaler'
- en: 'Use the first value in the preceding array to set your scale in preparation
    for the multiplication of `y_pred` and `y_test`. Recall that you are converting
    your data back from your earlier scale, in which you converted all values to between
    zero and one:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前面数组中的第一个值来设置你的缩放比例，为`y_pred`和`y_test`的乘法做好准备。回想一下，你正在将数据从早期的缩放状态中转换回来，当时你将所有值转换为0到1之间：
- en: '[PRE33]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You should get the following output:'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE34]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Multiply `y_pred` and `y_test` by `scale` to convert your data back to the
    proper values:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`y_pred`和`y_test`乘以`scale`，以将数据转换回正确的数值：
- en: '[PRE35]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Review the real Nvidia stock price and your predictions:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回顾真实的Nvidia股票价格和你的预测：
- en: '[PRE36]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You should get the following output:'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.15: Real Nvidia stock price versus your predictions'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.15：真实的Nvidia股价与你的预测值](img/B16341_09_15.jpg)'
- en: '](img/B16341_09_15.jpg)'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_15.jpg)'
- en: 'Figure 9.15: Real Nvidia stock price versus your predictions'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15：真实的Nvidia股价与你的预测值
- en: In the preceding graph, you can see that your trained model is able to capture
    some of the trends of the Nvidia stock price. Observe that the predictions are
    quite different from the real values. It is evident from this result that ANNs
    are not suited for sequential data.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，你可以看到你训练的模型能够捕捉到一些Nvidia股价的趋势。可以观察到，预测值与真实值相差很大。从这个结果可以明显看出，人工神经网络并不适合处理顺序数据。
- en: In this exercise, you saw the inability of simple ANNs to deal with sequential
    data. In the next section, you will learn about recurrent neural networks, which
    are designed to learn from the temporal dimensionality of sequential data. Then,
    in *Exercise 9.02*, *Building an RNN with LSTM Layer Nvidia Stock Prediction*,
    you will perform predictions on the same Nvidia stock price dataset using RNNs
    and compare your results.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，你看到了简单的人工神经网络无法处理顺序数据的局限性。在下一节中，你将了解递归神经网络，它们被设计用来从顺序数据的时间维度中学习。然后，在*练习9.02*，*构建具有LSTM层的RNN进行Nvidia股票预测*中，你将使用RNN对相同的Nvidia股价数据集进行预测，并比较你的结果。
- en: Recurrent Neural Networks
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归神经网络
- en: 'The first formulation of a recurrent-like neural network was created by John
    Hopfield in 1982\. He had two motivations for doing so:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 递归型神经网络的第一个形式是由John Hopfield在1982年创建的。他有两个动机来做这件事：
- en: Sequential processing of data
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的顺序处理
- en: Modeling of neuronal connectivity
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经连接建模
- en: Essentially, an RNN processes input data at each time step and stores information
    in its memory that will be used for the next step. Information is first transformed
    into vectors that can be processed by machines. The RNN then processes the vector
    sequence one at a time. As it processes each vector, it passes the previous hidden
    state. The hidden state retains information from the previous step, acting as
    a type of memory. It does this by combining the input and the previous hidden
    state with a tanh function that compresses the values between `-1` and `1`.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，RNN在每个时间步骤处理输入数据，并将信息存储在其内存中，这些信息将用于下一步。信息首先被转换成向量，以便机器进行处理。然后，RNN逐一处理向量序列。在处理每个向量时，它会传递前一个隐藏状态。隐藏状态保留了来自前一步的信息，充当一种记忆。它通过结合输入和前一个隐藏状态与tanh函数来完成这一过程，该函数将值压缩在`-1`和`1`之间。
- en: Essentially, this is how the RNN functions. RNNs don't need a lot of computation
    and work well with short sequences.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，这就是RNN的功能。RNN不需要大量计算，并且在处理短序列时效果很好。
- en: '![Figure 9.16: RNN data flow'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.16：RNN数据流](img/B16341_09_16.jpg)'
- en: '](img/B16341_09_16.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_09_16.jpg)'
- en: 'Figure 9.16: RNN data flow'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16：RNN数据流
- en: Now turn your attention to applying neural networks to problems that involve
    sequential processing of data. You've already learned a bit about why these sorts
    of tasks require a fundamentally different type of network architecture from what
    you've seen so far.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将注意力转向将神经网络应用于涉及数据顺序处理的问题。你已经稍微了解过为什么这些任务需要与之前所见的网络架构有根本不同的原因。
- en: RNN Architecture
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN架构
- en: This section will go through the key principles behind RNNs, how they are fundamentally
    different from what you've learned so far, and how RNN computation actually works.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍RNN的关键原理，它们与之前所学的内容有何根本不同，以及RNN计算是如何工作的。
- en: But before you do that, take one step back and consider the standard feed-forward
    neural network that was discussed previously.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 但在此之前，先回顾一下之前讨论过的标准前馈神经网络。
- en: In feed-forward neural networks, data propagates in one direction only, that
    is, from input to output.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在前馈神经网络中，数据只在一个方向上传播，即从输入到输出。
- en: Therefore, you need a different kind of network architecture to handle sequential
    data. RNNs are particularly well-suited to handling cases in which you have a
    sequence of inputs rather than a single input. These are great for problems in
    which a sequence of data is being propagated to give a single output.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你需要一种不同的网络架构来处理顺序数据。RNN特别适合处理一系列输入的情况，而不是单一输入。对于那些需要将数据序列传播以获得单一输出的问题来说，RNN非常适用。
- en: For example, imagine that you are training a model that takes a sequence of
    words as input and outputs an emotion associated with that sequence. Similarly,
    consider cases in which, instead of returning a single output, you could have
    a sequence of inputs and propagate them through your network, where each time
    step in the sequence generates an output.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设你正在训练一个模型，输入是一个单词序列，输出是与该序列相关的情感。同样，考虑那些情况，在这些情况下，你不仅仅返回一个输出，而是拥有一个输入序列，并将其传播通过你的网络，其中序列中的每个时间步都会生成一个输出。
- en: Simply put, RNNs are networks that offer a mechanism to persist previously processed
    data over time and use it to make future predictions.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，RNN 是一种网络，它提供了一种机制，可以使先前处理过的数据在时间上持续存在，并利用这些数据进行未来的预测。
- en: '![Figure 9.17: RNN computation'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.17：RNN 计算'
- en: '](img/B16341_09_17.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_09_17.jpg)'
- en: 'Figure 9.17: RNN computation'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.17：RNN 计算
- en: In the preceding diagram, at some time step denoted by t, the RNN takes in `X`t
    as the input, and at that time step, it computes a prediction value, `Y`t, which
    is the output of the network.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图中，在某个时间步 `t`，RNN 接收 `X`t 作为输入，并在该时间步计算预测值 `Y`t，这就是网络的输出。
- en: In addition to that output, it saved an internal state, called update, `H`t.
    This internal state from time step `t` can then be used to complement the input
    of the next time step `t+1`. So, basically, it provides information about the
    previous step to the next one. This mechanism is called **recurrent** because
    information is being passed from one time step to the next within the network.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这个输出外，它还保存了一个内部状态，叫做更新 `H`t。这个时间步 `t` 的内部状态随后可以用来补充下一个时间步 `t+1` 的输入。所以，基本上，它为下一个步骤提供了前一个步骤的信息。这个机制被称为
    **递归**，因为信息在网络内从一个时间步传递到下一个时间步。
- en: What's really happening here? This is done by using a simple recurrence relation
    to process the sequential data. RNNs maintain internal state, `H`t, and combine
    it with the next input data, `X`t+1, to make a prediction, `Y`t+1, and store the
    new internal state, `H`t+1\. The key idea is that the state update is a combination
    of the previous state time step as well as the current input that the network
    is receiving.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这里究竟发生了什么呢？这是通过使用一个简单的递归关系来处理顺序数据。RNN 保持内部状态 `H`t，并将其与下一个输入数据 `X`t+1 结合，做出预测
    `Y`t+1，并存储新的内部状态 `H`t+1。关键的思想是，状态更新是前一个状态时间步与当前网络接收到的输入的组合。
- en: 'It''s important to note that, in this computation, it''s the same function
    `f` of `W` and the same set of parameters that are used at every time step, and
    it''s those sets of parameters that you learn during the course of training. To
    get a better sense of how these networks work, step through the RNN algorithm:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，在这个计算过程中，使用的是相同的 `f` 函数和 `W`，以及每个时间步都使用相同的一组参数，而这些参数是在训练过程中学习得到的。为了更好地理解这些网络是如何工作的，可以逐步执行
    RNN 算法：
- en: You begin by initializing your RNN and the hidden state of that network. You
    can denote a sentence for which you are interested in predicting the next word.
    The RNN computation simply consists of them looping through the words in this sentence.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你首先需要初始化你的 RNN 以及该网络的隐藏状态。你可以指定一个句子，并基于该句子预测下一个单词。RNN 计算实际上就是它们循环处理这个句子中的每个单词。
- en: At each time step, you feed both the current word that you're considering, as
    well as the previous hidden state of your RNN into the network. This can then
    generate a prediction for the next word in the sequence and use this information
    to update its hidden state.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个时间步，你将当前正在考虑的单词以及 RNN 的上一个隐藏状态一起输入到网络中。然后，它可以根据这些信息预测序列中的下一个单词，并用来更新其隐藏状态。
- en: Finally, after you've looped through all the words in the sentence, your prediction
    for that missing word is simply the RNN's output at that final time step.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在你循环完句子中的所有单词后，你对缺失单词的预测就是该最终时间步的 RNN 输出。
- en: As you can see in the following diagram, this RNN computation includes both
    the internal state update and the formal output vector.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，这个 RNN 计算包含了内部状态的更新以及正式的输出向量。
- en: '![Figure 9.18: RNN data flow'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.18：RNN 数据流'
- en: '](img/B16341_09_18.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_09_18.jpg)'
- en: 'Figure 9.18: RNN data flow'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.18：RNN 数据流
- en: Given the input vector, `X`t, the RNN applies a function to update its hidden
    state. This function is simply a standard neural net operation. It consists of
    multiplication by a weight matrix and the application of a non-linearity activation
    function. The key difference is that, in this case, you're feeding in both the
    input vector, `X`t, and the previous state as inputs to this function, `H`t-1.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入向量 `X`t，RNN 应用一个函数来更新它的隐藏状态。这个函数只是一个标准的神经网络操作，它包括乘以权重矩阵和应用非线性激活函数。关键的区别在于，在这种情况下，你同时将输入向量
    `X`t 和前一个状态作为输入传递给这个函数 `H`t-1。
- en: Next, you apply a non-linearity activation function such as tanh to the previous
    step. You have these two weight matrices, and finally, your output, `y`t, at a
    given time step is then a modified, transformed version of this internal state.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你应用一个非线性激活函数，如 tanh，来处理上一步的输出。你有这两个权重矩阵，最后，在给定时间步的输出 `y`t 就是这个内部状态的一个修改和转化版本。
- en: After you've looped through all the words in the sentence, your prediction for
    that missing word is simply the RNN's output at that final time step, after all
    the words have been fed through the model. So, as mentioned, RNN computation includes
    both internal state updates and formal output vectors.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 当你遍历完句子中的所有单词后，你对缺失单词的预测就是在所有单词通过模型后，RNN 在最后一个时间步的输出。如前所述，RNN 计算包括内部状态更新和正式的输出向量。
- en: Another way you can represent RNNs is by unrolling their modules over time.
    You can think of RNNs as having multiple copies of the same network, where each
    passes a message on to its descendant.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种表示 RNN 的方式是将其模块沿时间展开。你可以将 RNN 看作是多个相同网络的副本，每个副本将信息传递给它的后代。
- en: '![Figure 9.19: Computational graph with time'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.19：带时间的计算图'
- en: '](img/B16341_09_19.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_09_19.jpg)'
- en: 'Figure 9.19: Computational graph with time'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.19：带时间的计算图
- en: In this representation, you can make your weight matrices explicit, beginning
    with the weights that transform the input to the `H` weights that are used to
    transform the previous hidden state to the current hidden state, and finally the
    hidden state to the output.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种表示方式中，你可以明确显示你的权重矩阵，从变换输入的权重开始，到 `H` 权重，它们用于将前一个隐藏状态转换为当前隐藏状态，最后是将隐藏状态转换为输出。
- en: It's important to note that you use the same weight matrices at every time step.
    From these outputs, you can compute a loss at each time step. The computation
    of the loss will then complete your forward propagation through the network. Finally,
    to define the total loss, you simply sum the losses from all of the individual
    time steps. Since your loss is dependent on each time step, this means that, in
    training the network, you will have to also involve time as a component.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，你在每个时间步使用相同的权重矩阵。从这些输出中，你可以在每个时间步计算损失。损失的计算将完成你的网络前向传播。最后，要定义总损失，你只需将所有时间步的损失相加。由于你的损失依赖于每个时间步，这意味着在训练网络时，你还必须将时间作为一个组件来考虑。
- en: Now that you've got a bit of a sense of how these RNNs are constructed and how
    they function, you can walk through a simple example of how to implement an RNN
    from scratch in TensorFlow.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你对这些 RNN 是如何构建和运作的有了一些基本的了解，可以逐步演示如何从头开始在 TensorFlow 中实现一个简单的 RNN。
- en: 'The following snippet uses a simple RNN from `keras.models.Sequential`. You
    specify the number of units as `1` and set the first input dimension to `None`
    as an RNN can process any number of time steps. A simple RNN uses tanh activation
    by default:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码段使用了来自 `keras.models.Sequential` 的简单 RNN。你将单元数指定为 `1`，并将第一个输入维度设置为 `None`，因为
    RNN 可以处理任何数量的时间步。简单的 RNN 默认使用 tanh 激活函数：
- en: '[PRE37]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The preceding code creates a single layer with a single neuron.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码创建了一个只有一个神经元的单层网络。
- en: 'That was easy enough. Now you need to stack some additional recurrent layers.
    The code is similar, but there is a key difference here. You will notice `return_sequences=True`
    on all but the last layer. This is to ensure that the output is a 3D array. As
    you can see, the first two layers each have `20` units:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单。现在你需要堆叠一些额外的递归层。代码类似，但这里有一个关键区别。你会注意到，除了最后一层外，所有层都有 `return_sequences=True`。这是为了确保输出是一个三维数组。如你所见，前两层每层都有
    `20` 个单元：
- en: '[PRE38]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The RNN is defined as a layer, and you can build it by inheriting it from the
    layer class. You can also initialize your weight matrices and the hidden state
    of your RNN cell to zero.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 被定义为一个层，你可以通过继承层类来构建它。你还可以将权重矩阵和 RNN 单元的隐藏状态初始化为零。
- en: The key step here is defining the call function, which describes how you make
    a forward pass through the network given an input `X`. And, to break down this
    call function, you would first update the hidden state according to the equation
    discussed previously.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键步骤是定义调用函数，它描述了如何在给定输入`X`的情况下，通过网络进行前向传播。为了分解这个调用函数，你首先需要根据之前讨论的方程更新隐藏状态。
- en: Take the previous hidden state and the input `X`, multiply them by the relevant
    weight matrices, add them together, and then pass them through a non-linearity,
    like a hyperbolic tangent (tanh).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 取上一个隐藏状态和输入`X`，将它们与相关的权重矩阵相乘，求和后通过一个非线性函数，如双曲正切（tanh），然后传递下去。
- en: Then, the output is simply a transformed version of the hidden state, and at
    each time step, you return both the current output and the updated hidden state.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，输出只是隐藏状态的变换版本，在每个时间步，你返回当前的输出和更新后的隐藏状态。
- en: TensorFlow has made it easy by having a built-in dense layer. The same applies
    to RNNs. TensorFlow has implemented these types of RNN cells with the simple RNN
    layer. But this type of layer has some limitations, such as vanishing gradients.
    You will look at this problem in the next section before exploring different types
    of recurrent layers.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow通过内置的全连接层使得这一过程变得简单。RNN也适用同样的情况。TensorFlow已经实现了这些类型的RNN单元，使用简单的RNN层。但这种类型的层存在一些限制，比如梯度消失问题。在接下来的章节中，你将深入探讨这个问题，然后再研究不同类型的递归层。
- en: Vanishing Gradient Problem
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度消失问题
- en: If you take a closer look at how gradients flow in this chain of repeating modules,
    you can see that between each time step you need to perform matrix multiplication.
    That means that the computation of the gradient—that is, the derivative of the
    loss with respect to the parameters, tracing all the way back to your initial
    state—requires many repeated multiplications of this weight matrix, as well as
    repeated use of the derivative of your activation function.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仔细观察梯度在这个重复模块链中的流动，你会发现，在每个时间步之间，你需要进行矩阵乘法。这意味着，梯度的计算——即关于参数的损失函数的导数，追溯到你的初始状态——需要多次进行这个权重矩阵的乘法运算，以及反复使用激活函数的导数。
- en: 'You can have one of two scenarios that could be particularly problematic: the
    exploding gradient problem or the vanishing gradient problem.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会遇到两种特别有问题的情况：梯度爆炸问题或梯度消失问题。
- en: The exploding gradients problem is when gradients become continuously larger
    and larger due to the matrix multiplication operation, and you can't optimize
    them anymore. One way you may be able to mitigate this is by performing what's
    called gradient clipping. This amounts to scaling back large gradients so that
    their values are smaller and closer to `1`.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度爆炸问题是指梯度由于矩阵乘法操作变得越来越大，导致无法再进行优化。缓解这个问题的一种方法是执行所谓的梯度裁剪。这相当于缩放大梯度，使它们的值变小并接近`1`。
- en: You can also have the opposite problem where your gradients are too small. This
    is what is known as the vanishing gradient problem. This is when gradients become
    increasingly smaller (close to `0`) as you make these repeated multiplications,
    and you can no longer train the network. This is a very real problem when it comes
    to training RNNs.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可能遇到相反的问题，即梯度过小。这就是所谓的梯度消失问题。当你进行这些重复的乘法时，梯度会变得越来越小（接近`0`），以至于你无法继续训练网络。这在训练RNN时是一个非常真实的问题。
- en: For example, consider a scenario in which you keep multiplying a number by some
    number that's in between zero and one. As you keep doing this repeatedly, that
    number is constantly shrinking until, eventually, it vanishes and becomes 0\.
    When this happens to gradients, it's hard to propagate errors further back into
    the past because the gradients are becoming smaller and smaller.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一种情况，你不断将一个数字与介于零和一之间的数字相乘。随着不断重复这个过程，这个数字会不断缩小，直到最终消失变为0。当这种情况发生在梯度上时，很难将误差传播到更远的过去，因为梯度越来越小。
- en: Consider the earlier example from the language model where you were trying to
    predict the next word. If you're trying to predict the last word in the following
    phrase, it's relatively clear what the next word is going to be. There's not that
    much of a gap between the key relevant information, such as the word "fish," and
    the place where the prediction is needed.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑之前的语言模型示例，你在尝试预测下一个单词。如果你要预测以下短语中的最后一个单词，那么接下来的单词是什么就比较明确了。关键的相关信息之间没有太大的间隔，比如单词“鱼”和需要预测的位置之间。
- en: '![Figure 9.20: Word prediction'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.20：词预测'
- en: '](img/B16341_09_20.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_09_20.jpg)'
- en: 'Figure 9.20: Word prediction'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.20：词预测
- en: However, there are other cases where more context is necessary, like in the
    following example. Information from early in the sentence, `She lived in Spain`,
    suggests that the next word of the sentence after `she speaks fluent` is most
    likely the name of a language, `Spanish`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也有一些情况需要更多的上下文信息，比如以下的例子。句子开头的信息，`她住在西班牙`，表明紧接在`她说流利的`之后的单词很可能是某种语言的名称，`西班牙语`。
- en: '![Figure 9.21: Sentence example'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.21：句子示例'
- en: '](img/B16341_09_21.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_09_21.jpg)'
- en: 'Figure 9.21: Sentence example'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.21：句子示例
- en: But you need the context of `Spain`, which is located at a much earlier position
    in this sentence, to be able to fill in the relevant gaps and identify which language
    is correct. As this gap between words that are semantically important grows, RNNs
    become increasingly unable to connect the dots and link these relevant pieces
    of information together. That is due to the vanishing gradient problem.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，你需要句子开头的上下文信息，即`西班牙`，才能填补相关的空白并确定正确的语言。随着语义上重要的单词之间的空隙增大，RNN（循环神经网络）变得越来越无法连接这些点并将这些相关信息联系起来。这就是梯度消失问题的表现。
- en: How can you alleviate this? The first trick is simple. You can choose either
    tanh or sigmoid as your activation function. Both of these functions have derivatives
    that are less than `1`.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 如何缓解这个问题呢？第一个技巧很简单。你可以选择tanh或sigmoid作为激活函数。这两种函数的导数都小于`1`。
- en: Another simple trick you can use is to initialize the weights for the parameters
    of your network. It turns out that initializing the weights to the identity matrix
    helps prevent them shrinking to zero too rapidly during back-propagation.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可以使用的简单技巧是初始化网络参数的权重。事实证明，将权重初始化为单位矩阵有助于在反向传播过程中防止它们迅速收敛到零。
- en: But the final and most robust solution is to use a slightly more complex recurrent
    unit that can track long-term dependencies in the data more effectively. It can
    do this by controlling what information is passed through and what information
    is used to update its internal state. Specifically, this is the concept of a gated
    cell, like in the LSTM layer, which is the focus of the next section.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 但最终最稳健的解决方案是使用一个稍微复杂一点的循环单元，它可以更有效地跟踪数据中的长期依赖关系。它通过控制哪些信息被传递，哪些信息被用来更新其内部状态来实现这一点。具体而言，这就是“门控单元”的概念，类似于LSTM层，这是下一节的重点。
- en: Long Short-Term Memory Network
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 长短期记忆网络
- en: LSTMs are well-suited to learning long-term dependencies and overcoming the
    vanishing gradient problem. They are very performant models for sequential data,
    and they're widely used by the deep learning community.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM（长短期记忆网络）非常适合学习长期依赖关系并克服梯度消失问题。它们是处理序列数据的高效模型，在深度学习领域广泛使用。
- en: LSTMs have a chain-like structure. In an LSTM, the repeating unit contains different
    interacting layers. The key point is that these layers interact to selectively
    control the flow of information within the cell.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM具有链式结构。在LSTM中，重复单元包含不同的交互层。关键点是，这些层相互作用，选择性地控制信息在单元内的流动。
- en: The key building block of the LSTM is a structure called a gate, which functions
    to enable the LSTM to selectively add or remove information from its cell state.
    Gates consist of a neural net layer like a sigmoid.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM的关键构建块是称为“门”的结构，它使LSTM能够选择性地向其单元状态中添加或删除信息。门由一个神经网络层组成，例如sigmoid层。
- en: '![Figure 9.22: LSTM architecture'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.22：LSTM架构'
- en: '](img/B16341_09_22.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_09_22.jpg)'
- en: 'Figure 9.22: LSTM architecture'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.22：LSTM架构
- en: Take a moment to think about what a gate like this would do in an LSTM. In this
    case, the sigmoid function would force its input to be between `0` and `1`. You
    can think of this mechanism as capturing how much of the information that's passed
    through the gate should be retained. It's between zero and one. This effectively
    gates the flow of information.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 花点时间思考一下这样的门在 LSTM 中会执行什么操作。在这种情况下，sigmoid 函数会强制其输入值保持在`0`到`1`之间。你可以将这个机制视为捕捉通过门传递的信息应该保留多少。它介于零和一之间。这有效地控制了信息的流动。
- en: 'LSTMs process information through four simple steps:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 通过四个简单的步骤处理信息：
- en: The first step in the LSTM is to decide what information is going to be thrown
    away from the cell state, to forget irrelevant history. This is a function of
    both the prior internal state, `H`t-1, and the input, `X`t, because some of that
    information may not be important.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM 的第一步是决定将从单元状态中丢弃哪些信息，以忘记无关的历史信息。这是先前内部状态`H`t-1 和输入`X`t 的函数，因为其中一些信息可能不重要。
- en: Next, the LSTM decides what part of the new information is relevant and uses
    this to store this information in its cell state.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，LSTM 决定哪些部分的新信息是相关的，并将其用于将信息存储在单元状态中。
- en: Then, it takes both the relevant parts of the prior information, as well as
    the current input, and uses this to selectively update its cell state.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它结合了先前信息中的相关部分以及当前输入，利用这些信息选择性地更新其单元状态。
- en: 'Finally, it returns an output, and this is known as the output gate, which
    controls what information encoded in the cell state is sent to the network.![Figure
    9.23: LSTM processing steps'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，它返回一个输出，这就是输出门，控制哪些编码在单元状态中的信息被发送到网络。[图 9.23：LSTM 处理步骤]
- en: '](img/B16341_09_23.jpg)'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_23.jpg)'
- en: 'Figure 9.23: LSTM processing steps'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.23：LSTM 处理步骤
- en: 'The key takeaway here for LSTMs is the sequence of how they regulate information
    flow and storage. Once again, LSTMs operate as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 的关键点是它们调节信息流动和存储的顺序。再次说明，LSTM 的操作过程如下：
- en: Forgetting irrelevant history
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忘记无关的历史信息
- en: Storing what's new and what's important
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储新的和重要的信息
- en: Using its internal memory to update the internal state
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用其内部记忆来更新内部状态
- en: Generating an output
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成输出
- en: An important property of LSTMs is that all these different gating and update
    mechanisms work to create an internal cell state, `C`, which allows the uninterrupted
    flow of gradients through time. You can think of it as sort of a highway of cell
    states where gradients can flow uninterrupted. This enables you to alleviate and
    mitigate the vanishing gradient problem that's seen with standard RNNs.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 的一个重要特性是，所有这些不同的门控和更新机制共同作用，创建了一个内部单元状态`C`，它允许梯度在时间中不间断地流动。你可以将其看作是一条单元状态的高速公路，梯度可以在其中不受阻碍地流动。这使得你能够缓解和减轻标准
    RNN 中出现的梯度消失问题。
- en: LSTMs are able to maintain this separate cell state independently of what is
    output, and they use gates to control the flow of information by forgetting irrelevant
    history, storing relevant new information, selectively updating their cell state,
    and then returning a filtered version as the output.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 能够独立于输出维持这个独立的单元状态，并通过使用门控来控制信息流动，通过忘记无关的历史信息、存储相关的新信息、选择性更新其单元状态，然后返回一个经过过滤的版本作为输出。
- en: The key point in terms of training and LSTMs is that maintaining the separate
    independent cell state allows the efficient training of an LSTM to backpropagate
    through time, which is discussed later.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和 LSTM 的应用中，关键点是保持独立的单元状态，使得 LSTM 可以高效地进行时间反向传播，稍后会进一步讨论。
- en: Now that you've gone through the fundamental workings of RNNs, the backpropagation
    through time algorithm, and a bit about the LSTM architecture, you can put some
    of these concepts to work in the following example.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了 RNN 的基本工作原理、时间反向传播算法以及一些 LSTM 架构的内容，你可以在以下示例中应用这些概念。
- en: 'Consider the following LSTM model:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑以下 LSTM 模型：
- en: '[PRE39]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'First, you have initialized a neural network by calling `regressor = Sequential()`.
    Again, it''s important to note that in the last line you omit `return_sequences
    = True` because it is the final output:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你通过调用`regressor = Sequential()`初始化了神经网络。同样需要注意的是，在最后一行中，你省略了`return_sequences
    = True`，因为这是最终输出：
- en: '[PRE40]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Then, the LSTM layer is added. In the first instance, set the LSTM layer to
    `50` units. Use a relu activation function and specify the shape of the training
    set. Finally, the dropout layer is added with `regressor.add(Dropout(0.2)`. The
    `0.2` means that 20% of the layers will be removed. Set `return_sequences = True`,
    which allows the return of the last output.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，添加LSTM层。首先，将LSTM层设置为`50`个单元。使用relu激活函数，并指定训练集的形状。最后，添加dropout层，使用`regressor.add(Dropout(0.2))`。`0.2`表示20%的层会被移除。设置`return_sequences
    = True`，以便返回最后一个输出。
- en: Similarly, add three more LSTM layers and one dense layer to the LSTM model.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，向LSTM模型中添加三个LSTM层和一个全连接层。
- en: Now that you are familiar with the basic concepts surrounding working with sequential
    data, it's time to complete the following exercise using some real data.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经熟悉了处理顺序数据的基本概念，接下来是时候使用一些真实数据来完成以下练习了。
- en: 'Exercise 9.02: Building an RNN with an LSTM Layer – Nvidia Stock Prediction'
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习9.02：构建带有LSTM层的RNN – 英伟达股价预测
- en: In this exercise, you will be working on the same dataset as for *Exercise 9.01*,
    *Training an ANN for Sequential Data – Nvidia Stock Prediction*. You will still
    try to predict the Nvidia stock price based on the data of the previous 60 days.
    But this time, you will be training an LSTM model. You will need to split the
    data into training and testing sets before and after the date `2019-01-01`.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，你将使用与*练习9.01*相同的数据集，即*为顺序数据训练ANN – 英伟达股价预测*。你将继续尝试基于过去60天的数据预测英伟达的股价。但这次，你将训练一个LSTM模型。你需要在和`2019-01-01`日期之前和之后将数据拆分为训练集和测试集。
- en: Note
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the `NVDA.csv` dataset here: [https://packt.link/Mxi80](https://packt.link/Mxi80).'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到`NVDA.csv`数据集：[https://packt.link/Mxi80](https://packt.link/Mxi80)。
- en: 'You will need to prepare the dataset like in *Exercise 9.01*, *Training an
    ANN for Sequential Data – Nvidia Stock Prediction* (*steps 1* to *15*) before
    applying the following code:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要像在*练习9.01*中一样准备数据集，即*为顺序数据训练ANN – 英伟达股价预测*（*步骤1*至*步骤15*），然后再应用以下代码：
- en: 'Start building the LSTM. You will need some additional libraries for this.
    Use `Sequential` to initialize the neural net, `Dense` to add a dense layer, `LSTM`
    to add an LSTM layer, and `Dropout` to help prevent overfitting:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始构建LSTM。你需要一些额外的库来实现。使用`Sequential`初始化神经网络，使用`Dense`添加全连接层，使用`LSTM`添加LSTM层，并使用`Dropout`来帮助防止过拟合：
- en: '[PRE41]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Initialize the neural network by calling `regressor = Sequential()`. Add four
    LSTM layers with `50`, `60`, `80`, and `120` units each. Use a ReLU activation
    function and assign `True` to `return_sequences` for all but the last LSTM layer.
    Provide the shape of your training set to the first LSTM layer. Finally, add dropout
    layers with 20%, 30%, 40%, and 50% dropouts:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`regressor = Sequential()`来初始化神经网络。添加四个LSTM层，分别为`50`、`60`、`80`和`120`个单元。使用ReLU激活函数，并将`return_sequences`设置为`True`，除了最后一个LSTM层。为第一个LSTM层提供训练集的形状。最后，添加dropout层，分别使用20%、30%、40%和50%的丢弃率：
- en: '[PRE42]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Check the summary of the model using the `summary()` method:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`summary()`方法检查模型的摘要：
- en: '[PRE43]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'You should get the following output:'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.24: Model summary'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图9.24：模型摘要'
- en: '](img/B16341_09_24.jpg)'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_24.jpg)'
- en: 'Figure 9.24: Model summary'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.24：模型摘要
- en: As you can see from the preceding figure, the summary provides valuable information
    about all model layers and parameters. This is a good way to make sure that your
    layers are in the order you wish and that they have the proper output shapes and
    parameters.
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前图所示，摘要提供了有关所有模型层和参数的有价值信息。这是确保层次顺序正确，且它们具有适当的输出形状和参数的好方法。
- en: 'Use the `compile()` method to configure your model for training. Choose Adam
    as your optimizer and mean squared error to measure your loss function:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`compile()`方法配置你的模型以进行训练。选择Adam作为优化器，使用均方误差作为损失函数的度量：
- en: '[PRE44]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Fit your model and set it to run on `10` epochs. Set your batch size equal
    to `32`:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合你的模型，并设置运行`10`个epoch。将批次大小设置为`32`：
- en: '[PRE45]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'You should get the following output:'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.25: Training the model'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图9.25：训练模型'
- en: '](img/B16341_09_25.jpg)'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_25.jpg)'
- en: 'Figure 9.25: Training the model'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.25：训练模型
- en: 'Test and predict the stock price and prepare the dataset. Check your data by
    calling the `head()` function:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试并预测股价，并准备数据集。通过调用`head()`函数检查你的数据：
- en: '[PRE46]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'You should get the following output:'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.26: First five rows of the DataFrame'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图9.26：DataFrame的前五行'
- en: '](img/B16341_09_26.jpg)'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_26.jpg)'
- en: 'Figure 9.26: First five rows of the DataFrame'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.26：DataFrame的前五行
- en: 'Call the `tail(60)` method to look at the last 60 days of data. You will use
    this information in the next step:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`tail(60)`方法查看最后60天的数据。你将在下一步中使用这些信息：
- en: '[PRE47]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'You should get the following output:'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.27: Last 10 rows of the DataFrame'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图9.27：DataFrame的最后10行'
- en: '](img/B16341_09_27.jpg)'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_27.jpg)'
- en: 'Figure 9.27: Last 10 rows of the DataFrame'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.27：DataFrame的最后10行
- en: 'Use the `tail(60)` method to create the `past_60_days` variable:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tail(60)`方法创建`past_60_days`变量：
- en: '[PRE48]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Add the `past_60_days` variable to your test data with the `append()` function.
    Set `True` to `ignore_index`. Drop the `Date` and `Adj Close` columns as you will
    not need that information:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`append()`函数将`past_60_days`变量添加到你的测试数据中。将`True`设置为`ignore_index`。删除`Date`和`Adj
    Close`列，因为你不需要这些信息：
- en: '[PRE49]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Check the DataFrame to make sure that you successfully dropped `Date` and `Adj
    Close` by using the `head()` function:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查DataFrame，确保你已经成功删除了`Date`和`Adj Close`列，可以通过使用`head()`函数来验证：
- en: '[PRE50]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'You should get the following output:'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.28: Checking the first five rows of the DataFrame'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图9.28：检查DataFrame的前五行'
- en: '](img/B16341_09_28.jpg)'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_28.jpg)'
- en: 'Figure 9.28: Checking the first five rows of the DataFrame'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.28：检查DataFrame的前五行
- en: 'Use `scaler.transform` from `StandardScaler` to perform standardization on
    inputs:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`scaler.transform`来自`StandardScaler`对输入数据进行标准化：
- en: '[PRE51]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You should get the following output:'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.29: DataFrame standardization'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图9.29：DataFrame标准化'
- en: '](img/B16341_09_29.jpg)'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_29.jpg)'
- en: 'Figure 9.29: DataFrame standardization'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.29：DataFrame标准化
- en: From the preceding results, you can see that after standardization, all values
    are close to `0` now.
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从前面的结果来看，你可以看到标准化后，所有的值现在都接近`0`。
- en: 'Split your data into `X_test` and `y_test` datasets. Create a test dataset
    that has the previous 60 days'' stock prices, so that you can test the closing
    stock price for the 61st day. Here, `X_test` will have two columns. The first
    column will store the values from 0 to 59\. The second column will store values
    from 1 to 60\. In the first column of `y_test`, store the 61st value at index
    60, and in the second column, store the 62nd value at index 61\. Use a `for` loop
    to create data in 60 time steps:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你的数据分割成`X_test`和`y_test`数据集。创建一个测试数据集，其中包含前60天的股票价格，以便你可以测试第61天的收盘股票价格。在这里，`X_test`将有两列。第一列将存储从0到59的值，第二列将存储从1到60的值。在`y_test`的第一列，存储第61个值（索引为60），在第二列，存储第62个值（索引为61）。使用`for`循环在60个时间步骤内创建数据：
- en: '[PRE52]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Convert `X_test` and `y_test` into NumPy arrays:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`X_test`和`y_test`转换为NumPy数组：
- en: '[PRE53]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'You should get the following output:'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE54]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The preceding result shows that there are `391` observations and for each of
    them you have the last `60` days'' data for the following five features: `Open`,
    `High`, `Low`, `Close`, and `Volume`. The target variable, on the other hand,
    contains `391` values.'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述结果显示共有`391`个观测值，对于每一个观测值，你有过去`60`天的以下五个特征数据：`Open`、`High`、`Low`、`Close`和`Volume`。目标变量则包含`391`个值。
- en: 'Test some predictions for stock prices by calling `regressor.predict(X_test)`:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`regressor.predict(X_test)`测试一些股票价格的预测：
- en: '[PRE55]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Before looking at the results, reverse the scaling you did earlier so that
    the number you get as output will be at the correct scale using the `StandardScaler`
    utility class that you imported with `scaler.scale_`:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在查看结果之前，使用`StandardScaler`工具类中导入的`scaler.scale_`，反转你之前进行的缩放操作，以便得到正确尺度的输出：
- en: '[PRE56]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'You should get the following output:'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.30: Using StandardScaler'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图9.30：使用StandardScaler'
- en: '](img/B16341_09_30.jpg)'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_30.jpg)'
- en: 'Figure 9.30: Using StandardScaler'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.30：使用StandardScaler
- en: 'Use the first value in the preceding array to set your scale in preparation
    for the multiplication of `y_pred` and `y_test`. Recall that you are converting
    your data back from the scale you did earlier when converting all values to between
    zero and one:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前面数组中的第一个值来设置你的缩放值，为乘法运算`y_pred`和`y_test`做准备。回想一下，你正在将数据从之前将所有值转换到0和1之间的缩放状态中转换回来：
- en: '[PRE57]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'You should get the following output:'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE58]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Multiply `y_pred` and `y_test` by `scale` to convert your data back to the
    proper values:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`y_pred`和`y_test`乘以`scale`以将数据转换回正确的值：
- en: '[PRE59]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Use `y_pred` to view predictions for NVIDIA stock:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`y_pred`查看NVIDIA股票的预测结果：
- en: '[PRE60]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'You should get the following output:'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.31: Checking prediction'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图9.31：检查预测结果'
- en: '](img/B16341_09_31.jpg)'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_31.jpg)'
- en: 'Figure 9.31: Checking prediction'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.31：检查预测
- en: The preceding results show the predicted Nvidia stock price for the future dates.
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述结果显示了预测的未来日期的 Nvidia 股票价格。
- en: 'Plot the real Nvidia stock price and your predictions:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制实际的 Nvidia 股票价格和你的预测：
- en: '[PRE61]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'You should get the following output:'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.32: NVIDIA stock price visualization'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.32：NVIDIA 股票价格可视化'
- en: '](img/B16341_09_32.jpg)'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_32.jpg)'
- en: 'Figure 9.32: NVIDIA stock price visualization'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.32：NVIDIA 股票价格可视化
- en: As you can see from the gray line in *Figure 9.32*, your prediction model is
    pretty accurate, when compared to the actual stock price, which is shown by the
    black line.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在 *图 9.32* 中的灰色线所示，与你的预测模型相比，实际的股票价格（由黑线表示）显示你的预测模型相当准确。
- en: In this exercise, you built an RNN with an LSTM layer for Nvidia stock prediction
    and completed the training, testing, and prediction steps.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，你构建了一个带有 LSTM 层的 RNN 模型，用于 Nvidia 股票预测，并完成了训练、测试和预测步骤。
- en: Now, test the knowledge you've gained so far in this chapter in the following
    activity.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，测试你在本章中所学到的知识，完成以下活动。
- en: 'Activity 9.01: Building an RNN with Multiple LSTM Layers to Predict Power Consumption'
  id: totrans-364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 9.01：构建一个具有多个 LSTM 层的 RNN 来预测电力消耗
- en: The `household_power_consumption.csv` dataset contains information related to
    electric power consumption measurements for a household over 4 years with a 1-minute
    sampling rate. You are required to predict the power consumption of a given minute
    based on previous measurements.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '`household_power_consumption.csv` 数据集包含一个家庭四年内，每分钟采样率的电力消耗测量数据。你需要基于之前的测量值预测给定分钟的电力消耗。'
- en: You are tasked with adapting an RNN model with additional LSTM layers to predict
    household power consumption at the minute level. You will be building an RNN model
    with three LSTM layers.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 你的任务是调整一个带有额外 LSTM 层的 RNN 模型，以便按分钟级别预测家庭电力消耗。你将构建一个包含三层 LSTM 的 RNN 模型。
- en: Note
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the dataset here: [https://packt.link/qrloK](https://packt.link/qrloK).'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到数据集：[https://packt.link/qrloK](https://packt.link/qrloK)。
- en: 'Perform the following steps to complete this activity:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以完成此活动：
- en: Load the data.
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据。
- en: Prepare the data by combining the `Date` and `Time` columns to form one single
    `Datetime` column that can be used then to sort the data and fill in missing values.
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将 `Date` 和 `Time` 列合并成一个单一的 `Datetime` 列来准备数据，然后可以使用这个列来排序数据并填补缺失值。
- en: Standardize the data and remove the `Date`, `Time`, `Global_reactive_power`,
    and `Datetime` columns as they won't be needed for the predictions.
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据进行标准化，并删除 `Date`、`Time`、`Global_reactive_power` 和 `Datetime` 列，因为它们在预测中不需要。
- en: Reshape the data for a given minute to include the previous 60 minutes' values.
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对给定的分钟数据进行重塑，包含前 60 分钟的值。
- en: Split the data into training and testing sets with, respectively, data before
    and after the index `217440`, which corresponds to the last month of data.
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集，分别为索引 `217440` 之前和之后的数据，该索引对应最后一个月的数据。
- en: Define and train an RNN model composed of three different layers of LSTM with
    `20`, `40`, and `80` units, followed by `50%` dropout and ReLU as the activation function.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义并训练一个由三层不同 LSTM 层组成的 RNN 模型，分别为 `20`、`40` 和 `80` 单元，后接 `50%` 的 dropout 和 ReLU
    作为激活函数。
- en: Make predictions on the testing set with the trained model.
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练好的模型对测试集进行预测。
- en: Compare the predictions against the actual values on the entire dataset.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将预测值与整个数据集中的实际值进行比较。
- en: 'You should get the following output:'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.33: Expected output of Activity 9.01'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.33：活动 9.01 的预期输出'
- en: '](img/B16341_09_33.jpg)'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_33.jpg)'
- en: 'Figure 9.33: Expected output of Activity 9.01'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.33：活动 9.01 的预期输出
- en: Note
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found via [this link](B16341_Solution_ePub.xhtml#_idTextAnchor280).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 这个活动的解决方案可以通过[此链接](B16341_Solution_ePub.xhtml#_idTextAnchor280)找到。
- en: In the next section, you will learn how to apply RNNs to text.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将学习如何将 RNN 应用于文本。
- en: Natural Language Processing
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: '**Natural Language Processing** (**NLP**) is a quickly growing field that is
    both challenging and rewarding. NLP takes valuable data that has traditionally
    been very difficult for machines to make sense of and turns it into information
    that can be used. This data can take the form of sentences, words, characters,
    text, and audio, to name a few. Why is this such a difficult task for machines?
    To answer that question, consider the following examples.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）是一个快速发展的领域，既充满挑战又具有回报。NLP 将传统上对机器来说很难理解的有价值数据转化为可以使用的信息。这些数据可以是句子、单词、字符、文本和音频等形式。为什么对机器来说这是一个如此困难的任务？为了回答这个问题，请考虑以下例子。'
- en: 'Recall the two sentences: *it is what it is* and *is it what it is*. These
    two sentences, though they have completely opposite semantic meanings, would have
    the exact same representations in this bag of words format. This is because they
    have the exact same words, just in a different order. So, you know that you need
    to use a sequential model to process this, but what else? There are several tools
    and techniques that have been developed to solve these problems. But before you
    get to that, you need to learn how to preprocess sequential data.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这两句话：*it is what it is* 和 *is it what it is*。这两句话虽然语义完全相反，但在这种词袋模型格式中，它们会有完全相同的表示。这是因为它们包含完全相同的单词，只是顺序不同。因此，你知道需要使用顺序模型来处理这个问题，但还有什么需要注意的呢？有几种工具和技术已经被开发出来来解决这些问题。但在此之前，你需要学习如何预处理顺序数据。
- en: Data Preprocessing
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'As a quick review, preprocessing generally entails all the steps needed to
    train your model. Some common steps include data cleaning, data transformation,
    and data reduction. For natural language processing, more specifically, the steps
    could be all, some, or none of the following:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 简单回顾，预处理通常包含训练模型所需的所有步骤。一些常见的步骤包括数据清洗、数据转换和数据降维。对于自然语言处理，更具体地说，这些步骤可能包括以下所有、部分或没有：
- en: Tokenization
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记化
- en: Padding
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充
- en: Lowercase conversion
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小写转换
- en: Removing stop words
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除停用词
- en: Removing punctuation
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除标点符号
- en: Stemming
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词干提取
- en: 'The following sections provide a more in-depth description of the steps that
    you will be using. For now, here''s an overview of each step:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分提供了对你将要使用的步骤的更深入描述。目前，这是每个步骤的概述：
- en: '**Dataset cleaning** encompasses the conversion of case to lowercase, the removal
    of punctuation marks, and so on.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集清理**包括将大小写转换为小写、移除标点符号等。'
- en: '**Tokenization** is breaking up a character sequence into specified units called tokens.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记化**是将字符序列分割成指定单元的过程，这些单元称为标记（tokens）。'
- en: '**Padding** is a way to make input sentences of different sizes the same by
    padding them. Padding the sequences means ensuring that the sequences have a uniform
    length.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**填充**是通过填充使输入句子大小相同的一种方法。填充序列意味着确保序列具有统一的长度。'
- en: '**Stemming** is truncating words down to their stem. For example, the words
    "rainy" and "raining" both have the stem "rain".'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词干提取**是将单词截断到它们的词干。例如，单词“rainy”和“raining”都具有词干“rain”。'
- en: Dataset Cleaning
  id: totrans-401
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集清理
- en: 'Here, you create the `clean_text` function, which returns a list containing
    words once it has been cleaned. You will save all text as lowercase with `lower()`
    and encode it with `utf8` for character standardization:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你创建了一个 `clean_text` 函数，它在清理后返回一个包含单词的列表。你将所有文本转换为小写并使用 `lower()` 方法保存，并使用
    `utf8` 编码进行字符标准化：
- en: '[PRE62]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Generating a Sequence and Tokenization
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成序列和标记化
- en: 'TensorFlow provides a dedicated class for generating a sequence of N-gram tokens
    – `Tokenizer` from `keras.preprocessing.text`:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 提供了一个专门的类来生成 N-gram 标记序列 —— `Tokenizer` 来自 `keras.preprocessing.text`：
- en: '[PRE63]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Once you have instantiated a `Tokenizer()`, you can use the `fit_on_texts()`
    method to extract tokens from a corpus. This step will attribute an integer index
    to each unique word from the corpus:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你实例化了 `Tokenizer()`，你可以使用 `fit_on_texts()` 方法从语料库中提取标记。此步骤会为语料库中的每个唯一单词分配一个整数索引：
- en: '[PRE64]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'After the tokenizer has been trained on a corpus, you can access the indexes
    allocated to each word from your corpus with the `word_index` attribute:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记化器经过语料库训练后，你可以通过 `word_index` 属性访问分配给语料库中每个单词的索引：
- en: '[PRE65]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'You can convert a sentence into a tokenized version using the `texts_to_sequences()`
    method:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`texts_to_sequences()`方法将句子转换为标记化版本：
- en: '[PRE66]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'You can create a function that will generate an N-gram sequence of tokenized
    sentences from an input corpus with the following snippet:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以创建一个函数，通过以下代码片段从输入语料库生成标记化句子的N-gram序列：
- en: '[PRE67]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The `get_seq_of_tokens()` function trains a `Tokenizer()` on the given corpus.
    Then you need to iterate through each line of the corpus and convert them into
    their tokenized equivalents. Finally, for each tokenized sentence, you create
    the different sequences of N-gram from it.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_seq_of_tokens()`函数在给定的语料库上训练一个`Tokenizer()`。然后，你需要遍历语料库中的每一行，并将它们转换为标记化的等效形式。最后，对于每个标记化的句子，你可以从中创建不同的N-gram序列。'
- en: Next, you will see how you can deal with variable sentence length with padding.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将看到如何通过填充处理可变长度的句子。
- en: Padding Sequences
  id: totrans-417
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 填充序列
- en: 'As discussed previously, deep learning models expect fixed-length input. But
    with text, the length of a sentence can vary. One way to overcome this is to transform
    all sentences to have the same length. You will need to set the maximum length
    of sentences. Then, for sentences that are shorter than this threshold, you can
    add padding, which will add a specific token value to fill the gap. On the other
    hand, longer sentences will be truncated to fit this constraint. You can use `pad_sequences()`
    to achieve this:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，深度学习模型期望固定长度的输入。但对于文本来说，句子的长度可以变化。一种克服这种情况的方法是将所有句子转换为相同的长度。你需要设置句子的最大长度。然后，对于短于此阈值的句子，你可以添加填充，填充将通过添加特定的标记值来填补空白。另一方面，较长的句子将被截断以适应这一约束。你可以使用`pad_sequences()`来实现这一点：
- en: '[PRE68]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'You can create the `generate_padded_sequences` function, which will take `input_sequences`
    and generate the padded version of it:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以创建`generate_padded_sequences`函数，该函数将接受`input_sequences`并生成其填充版本：
- en: '[PRE69]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Now that you know how to process raw text, have a look at the modeling step
    in the next section.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道如何处理原始文本，接下来请查看下一节中的建模步骤。
- en: Back Propagation Through Time (BPTT)
  id: totrans-423
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间反向传播（BPTT）
- en: There are many types of sequential models. You've already used simple RNNs,
    deep RNNs, and LSTMs. Let's take a look at a couple of additional models used
    for NLP.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多类型的序列模型。你已经使用了简单的RNN、深度RNN和LSTM。我们来看看几个用于自然语言处理的附加模型。
- en: Remember that you trained feed-forward models by first making a forward pass
    through the network that goes from input to output. This is the standard feed-forward
    model where the layers are densely connected. To train this kind of model, you
    can backpropagate the gradients through the network, taking the derivative of
    the loss of each weight parameter in the network. Then, you can adjust the parameters
    to minimize the loss.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，你是通过首先进行网络的前向传播，从输入到输出，来训练前馈模型的。这是标准的前馈模型，其中层之间是密集连接的。要训练这种模型，你可以通过网络反向传播梯度，计算每个权重参数的损失的导数。然后，你可以调整参数以最小化损失。
- en: But in RNNs, as discussed earlier, your forward pass through the network also
    consists of going forward in time, updating the cell state based on the input
    and the previous state, and generating an output, `Y`. At that time step, computing
    a loss and then finally summing these losses from the individual time steps gets
    your total loss.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 但在RNN中，如前所述，网络的前向传播也包括时间上的前进，根据输入和先前状态更新单元状态，并生成输出`Y`。在该时间步长，计算损失并最终将来自各个时间步的这些损失加总起来，得到总损失。
- en: This means that instead of backpropagating errors through a single feed-forward
    network at a single time step, errors are backpropagated at each individual time
    step, and then, finally, across all time steps—all the way from where you are
    currently, to the beginning of the sequence.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，与其在单个时间步长通过单个前馈网络反向传播误差，不如在每个单独的时间步长进行反向传播误差，最后，再跨越所有时间步长——从当前所在位置一直回溯到序列的开始。
- en: This is why it's called backpropagation through time. As you can see, all errors
    are flowing back in time to the beginning of your data sequence.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么它被称为时间反向传播（Backpropagation through Time）。正如你所看到的，所有误差都在时间上反向流动，回到数据序列的开始。
- en: A great example of machine translation and one of the most powerful and widely
    used applications of RNNs in industry is Google Translate. In machine translation,
    you input a sequence in one language and the task is to train the RNN to output
    that sequence in a new language. This is done by employing a dual structure with
    an encoder that encodes the sentence in its original language into a state vector
    and a decoder. This then takes that encoded representation as input and decodes
    it into a new language.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译的一个伟大例子，以及RNN在工业中最强大且广泛使用的应用之一，就是谷歌翻译。在机器翻译中，你输入一个语言中的序列，任务是训练RNN将该序列转换为新的语言。这是通过使用一个双重结构来完成的，该结构有一个编码器，它将原语言中的句子编码成状态向量，然后是一个解码器。解码器将这个编码表示作为输入并解码成新语言。
- en: 'There''s a key problem though in this approach: all content that is fed into
    the encoder structure must be encoded into a single vector. This can become a
    huge information bottleneck in practice because you may have a large body of text
    that you want to translate. To get around this problem the researchers at Google
    developed an extension of RNN called **attention**.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法存在一个关键问题：所有输入到编码器结构中的内容必须编码为单一向量。在实际操作中，这可能成为一个巨大的信息瓶颈，因为你可能有大量的文本需要翻译。为了解决这个问题，谷歌的研究人员开发了一种称为**注意力机制**的RNN扩展。
- en: Now, instead of the decoder only having access to the final encoded state, it
    can access the states of all the time steps in the original sentence. The weights
    of these vectors that connect the encoder states to the decoder are learned by
    the network during training. This is called attention because when the network
    learns, it places its attention on different parts of the input sentence.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，解码器不再仅仅访问最终的编码状态，它可以访问原始句子中所有时间步的状态。这些将编码器状态连接到解码器的向量权重是在训练过程中由网络学习的。这就是所谓的注意力机制，因为在网络学习过程中，它会将注意力集中在输入句子的不同部分。
- en: In this way, it effectively captures a sort of memory access to the important
    information in that original sentence. So, with building blocks such as attention
    and gated cells, like LSTMs, RNNs have really taken off in recent years and are
    being used in the real world quite successfully.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，它有效地捕捉了对原始句子中重要信息的某种记忆访问。因此，借助注意力机制和门控单元等构建块，像LSTM和RNN这样的模型近年来取得了巨大进展，并在现实世界中被成功应用。
- en: You should have by now gotten a sense of how RNNs work and why they are so powerful
    for processing sequential data. You've seen why and how you can use RNNs to perform
    sequence modeling tasks by defining this recurrence relation. You also learned
    how you can train RNNs and looked at how gated cells such as LSTMs can help us
    model long-term dependencies.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该已经对RNN是如何工作的以及它们为何如此强大以处理序列数据有了一定的了解。你已经了解了如何通过定义递归关系使用RNN执行序列建模任务。你还学习了如何训练RNN，并看到了像LSTM这样的门控单元如何帮助我们建模长期依赖性。
- en: In the following exercise, you will see how to use an LSTM model for predicting
    the next word of a text.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，你将看到如何使用LSTM模型来预测文本中的下一个单词。
- en: 'Exercise 9.03: Building an RNN with an LSTM Layer for Natural Language Processing'
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 9.03：构建一个带LSTM层的RNN进行自然语言处理
- en: In this exercise, you will use an RNN with an LSTM layer to predict the final
    word of a news headline.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，你将使用带LSTM层的RNN来预测新闻标题的最后一个单词。
- en: The `Articles.csv` dataset contains raw text that consists of news titles. You
    will be training an LTSM model that will predict the next word of a given sentence.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '`Articles.csv`数据集包含由新闻标题组成的原始文本。你将训练一个LTSM模型，该模型将预测给定句子的下一个单词。'
- en: Note
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the dataset here: [https://packt.link/RQVoB](https://packt.link/RQVoB).'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到数据集：[https://packt.link/RQVoB](https://packt.link/RQVoB)。
- en: 'Perform the following steps to complete this exercise:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以完成此练习：
- en: 'Import the libraries needed:'
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE70]'
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'You should get the following output:'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE71]'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Load the dataset locally by setting `curr_dir` to `content`. Create the `all_headlines`
    variable. Use a `for` loop to iterate over the files contained in the folder,
    and extract the headlines. Remove all headlines with the `Unknown` value. Print
    the length of `all_headlines`:'
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将`curr_dir`设置为`content`，在本地加载数据集。创建`all_headlines`变量。使用`for`循环遍历文件夹中的文件，并提取标题。删除所有值为`Unknown`的标题。打印`all_headlines`的长度：
- en: '[PRE72]'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The output will be as follows:'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE73]'
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Create the `clean_text` method to return a list containing words once it has
    been cleaned. Save all text as lowercase with the `lower()` method and encode
    it with `utf8` for character standardization. Finally, output 10 headlines from
    your corpus:'
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `clean_text` 方法，在清洗文本后返回一个包含单词的列表。使用 `lower()` 方法将所有文本转换为小写，并使用 `utf8` 编码进行字符标准化。最后，从你的语料库中输出
    10 条头条新闻：
- en: '[PRE74]'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'You should get the following output:'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.34: Corpus'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.34：语料库'
- en: '](img/B16341_09_34.jpg)'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_34.jpg)'
- en: 'Figure 9.34: Corpus'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.34：语料库
- en: 'Use `tokenizer.fit` to extract tokens from the corpus. Each integer output corresponds
    with a specific word. With `input_sequences`, train features that will be a `list
    []`. With `token_list = tokenizer.texts_to_sequences`, convert each sentence into
    its tokenized equivalent. With `n_gram_sequence = token_list`, generate the N-gram
    sequences. Using `input_sequences.append(n_gram_sequence)`, append each N-gram
    sequence to the list of your features:'
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `tokenizer.fit` 从语料库中提取词元。每个整数输出对应于一个特定的单词。使用 `input_sequences`，训练出将是一个 `list
    []` 的特征。通过 `token_list = tokenizer.texts_to_sequences`，将每个句子转换为其标记化的等效形式。使用 `n_gram_sequence
    = token_list`，生成 N-gram 序列。通过 `input_sequences.append(n_gram_sequence)`，将每个 N-gram
    序列追加到特征列表中：
- en: '[PRE75]'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'You should get the following output:'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.35: N-gram tokens'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.35：N-gram 词元'
- en: '](img/B16341_09_35.jpg)'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_35.jpg)'
- en: 'Figure 9.35: N-gram tokens'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.35：N-gram 词元
- en: 'Pad the sequences and obtain the `predictors` and `target` variables. Use `pad_sequence`
    to pad the sequences and make their lengths equal:'
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填充序列并获取 `predictors` 和 `target` 变量。使用 `pad_sequence` 来填充序列，使它们的长度相等：
- en: '[PRE76]'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Prepare your model for training. Add an input embedding layer with `model.add(Embedding)`.
    Add a hidden LSTM layer with `100` units and add a dropout of 10%. Then, add a
    dense layer with a softmax activation function. With the `compile` method, configure
    your model for training, setting your loss function to `categorical_crossentropy`,
    and use the Adam optimizer:'
  id: totrans-463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备你的模型进行训练。添加一个输入嵌入层 `model.add(Embedding)`。添加一个具有 `100` 单元的 LSTM 隐藏层，并添加 10%
    的 dropout。然后，添加一个具有 softmax 激活函数的全连接层。使用 `compile` 方法配置你的模型进行训练，将损失函数设置为 `categorical_crossentropy`，并使用
    Adam 优化器：
- en: '[PRE77]'
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'You should get the following output:'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.36: Model summary'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.36：模型总结'
- en: '](img/B16341_09_36.jpg)'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_36.jpg)'
- en: 'Figure 9.36: Model summary'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.36：模型总结
- en: 'Fit your model with `model.fit` and set it to run on `100` epochs. Set `verbose`
    equal to `5`:'
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `model.fit` 拟合你的模型，并将其设置为运行 `100` 个训练周期。将 `verbose` 设置为 `5`：
- en: '[PRE78]'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'You should get the following output:'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.37: Training the model'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.37：训练模型'
- en: '](img/B16341_09_37.jpg)'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_37.jpg)'
- en: 'Figure 9.37: Training the model'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.37：训练模型
- en: 'Write a function that will receive an input text, a model, and the number of
    next words to be predicted. This function will prepare the input text to be fed
    into the model that will predict the next word:'
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个函数，该函数将接收输入文本、一个模型以及要预测的下一个单词的数量。该函数将准备输入文本，以便将其输入到模型中，模型将预测下一个单词：
- en: '[PRE79]'
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Output some of your generated text with the `print` function. Add your own words
    for the model to use and generate from. For example, in `the hottest new`, the
    integer `5` is the number of words output by the model:'
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `print` 函数输出一些生成的文本。添加你自己的单词让模型使用并生成。例如，在 `the hottest new` 中，整数 `5` 是模型输出的单词数：
- en: '[PRE80]'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'You should get the following output:'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.38: Generated text'
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.38：生成的文本'
- en: '](img/B16341_09_38.jpg)'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_38.jpg)'
- en: 'Figure 9.38: Generated text'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.38：生成的文本
- en: In this result, you can see the text generated by your model for each sentence.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个结果中，你可以看到你的模型为每个句子生成的文本。
- en: In this exercise, you have successfully predicted some news headlines. Not surprisingly,
    some of them may not be very impressive, but some are not too bad.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，你成功地预测了一些新闻头条。不出所料，其中一些可能并不太令人印象深刻，但也有一些还不错。
- en: Now that you have all the essential knowledge about RNNs, try to test yourself
    by performing the next activity.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经掌握了关于 RNN 的所有基础知识，尝试通过执行下一个活动来测试自己。
- en: 'Activity 9.02: Building an RNN for Predicting Tweets'' Sentiment'
  id: totrans-486
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 9.02：构建一个 RNN 用于预测推文的情感
- en: The `tweets.csv` dataset contains a list of tweets related to an airline company.
    Each of the tweets has been classified as having positive, negative, or neutral sentiment.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '`tweets.csv` 数据集包含与一家航空公司相关的推文列表。每条推文都被分类为正面、负面或中立情感。'
- en: 'You have been tasked to analyze a sample of tweets for the company. Your goal
    is to build an RNN model that will be able to predict the sentiment of each tweet:
    either positive or negative.'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 你被分配了分析公司推文样本的任务。你的目标是构建一个RNN模型，能够预测每条推文的情感：正面或负面。
- en: Note
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find `tweets.csv` here: [https://packt.link/dVUd2](https://packt.link/dVUd2).'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到`tweets.csv`：[https://packt.link/dVUd2](https://packt.link/dVUd2)。
- en: Perform the following steps to complete this activity.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以完成此活动。
- en: Import the necessary packages.
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包。
- en: Prepare the data (combine the `Date` and `Time` columns, name it `datetime`,
    sort the data, and fill in missing values).
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备数据（合并`Date`和`Time`列，命名为`datetime`，对数据进行排序，并填补缺失值）。
- en: Prepare the text data (tokenize words and add padding).
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备文本数据（对单词进行标记化并添加填充）。
- en: Split the dataset into training and testing sets with, respectively, the first
    10,000 tweets and the remaining tweets.
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集拆分为训练集和测试集，训练集包含前10,000条推文，测试集包含剩余的推文。
- en: Define and train an RNN model composed of two different layers of LSTM with,
    respectively, `50` and `100` units followed by 20% dropout and ReLU as the activation
    function.
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义并训练一个由两层不同的LSTM组成的RNN模型，分别包含`50`和`100`个单元，后跟20%的dropout和ReLU激活函数。
- en: Make predictions on the testing set with the trained model.
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练好的模型对测试集进行预测。
- en: 'You should get the following output:'
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 9.39: Expected output of Activity 9.02'
  id: totrans-499
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.39：活动 9.02 的预期输出'
- en: '](img/B16341_09_39.jpg)'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_39.jpg)'
- en: 'Figure 9.39: Expected output of Activity 9.02'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.39：活动 9.02 的预期输出
- en: Note
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found via [this link](B16341_Solution_ePub.xhtml#_idTextAnchor281).
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以通过[此链接](B16341_Solution_ePub.xhtml#_idTextAnchor281)找到。
- en: Summary
  id: totrans-504
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you explored different recurrent models for sequential data.
    You learned that each sequential data point is dependent on the prior sequence
    of data points, such as natural language text. You also learned why you must use
    models that allow for the sequence of data to be used by the model, and sequentially
    generate the next output.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你探讨了不同的递归模型用于处理序列数据。你了解到每个序列数据点都依赖于之前的序列数据点，例如自然语言文本。你还学会了为什么必须使用允许模型使用数据序列并按顺序生成下一个输出的模型。
- en: This chapter introduced RNN models that can make predictions for sequential
    data. You observed the way RNNs can loop back on themselves, which allows the
    output of the model to feed back into the input. You reviewed the types of challenges
    that you face with these models, such as vanishing and exploding gradients, and
    how to address them.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了可以对序列数据进行预测的RNN模型。你观察到RNN可以进行自我循环，这使得模型的输出可以反馈到输入中。你回顾了使用这些模型时所面临的挑战，如梯度消失和爆炸，并学习了如何解决这些问题。
- en: In the next chapter, you will learn how to utilize custom TensorFlow components
    to use within your models, including loss functions and layers.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习如何利用自定义的TensorFlow组件在模型中使用，包括损失函数和层。
