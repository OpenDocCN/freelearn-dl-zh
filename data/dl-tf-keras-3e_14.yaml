- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: The Math Behind Deep Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习背后的数学
- en: In this chapter, we discuss the math behind deep learning. This topic is quite
    advanced and not necessarily required for practitioners. However, it is recommended
    reading if you are interested in understanding what is going on *under the hood*
    when you play with neural networks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论深度学习背后的数学。这个话题相当深入，可能并非所有从业者都需要掌握。然而，如果你有兴趣了解当你操作神经网络时，*背后的工作原理*，那么这篇内容是值得一读的。
- en: 'Here is what you will learn:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学到的内容：
- en: A historical introduction
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 历史介绍
- en: The concepts of derivatives and gradients
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导数和梯度的概念
- en: Gradient descent and backpropagation algorithms commonly used to optimize deep
    learning networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降和反向传播算法通常用于优化深度学习网络
- en: Let’s begin!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: History
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 历史
- en: The basics of continuous backpropagation were proposed by Henry J. Kelley [1]
    in 1960 using dynamic programming. Stuart Dreyfus proposed using the chain rule
    in 1962 [2]. Paul Werbos was the first to use backpropagation (backprop for short)
    for neural nets in his 1974 PhD thesis [3]. However, it wasn’t until 1986 that
    backpropagation gained success with the work of David E. Rumelhart, Geoffrey E.
    Hinton, and Ronald J. Williams published in Nature [4]. In 1987, Yann LeCun described
    the modern version of backprop currently used for training neural networks [5].
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 连续反向传播的基本原理由亨利·J·凯利（Henry J. Kelley）于1960年提出，他使用了动态规划方法。斯图尔特·德雷福斯（Stuart Dreyfus）在1962年提出使用链式法则。保罗·韦尔博斯（Paul
    Werbos）是第一个在1974年博士论文中将反向传播（简称backprop）应用于神经网络的人。然而，直到1986年，反向传播才在大卫·E·鲁梅尔哈特（David
    E. Rumelhart）、杰弗里·E·辛顿（Geoffrey E. Hinton）和罗纳德·J·威廉姆斯（Ronald J. Williams）在《自然》杂志上发表的论文中取得成功。1987年，扬·勒昆（Yann
    LeCun）描述了现代版本的反向传播算法，现用于神经网络的训练。
- en: The basic intuition of **Stochastic Gradient Descent** (**SGD**) was introduced
    by Robbins and Monro in 1951 in a context different from neural networks [6].
    In 2012 – or 52 years after the first time backprop was first introduced – AlexNet
    [7] achieved a top-5 error of 15.3% in the ImageNet 2012 Challenge using GPUs.
    According to The Economist [8], *Suddenly people started to pay attention, not
    just within the AI community but across the technology industry as a whole.* Innovation
    in this field was not something that happened overnight. Instead, it was a long
    walk lasting more than 50 years!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机梯度下降**（**SGD**）的基本直觉由罗宾斯和蒙罗（Robbins and Monro）在1951年提出，这一概念的背景与神经网络不同。2012年——即反向传播首次提出52年后——AlexNet
    [7]在2012年ImageNet挑战赛中使用GPU实现了15.3%的前五名错误率。根据《经济学人》[8]的报道，*突然间，人们开始关注这一领域，不仅仅是AI社区，整个技术行业都开始关注。*
    这一领域的创新并非一蹴而就，而是经历了超过50年的漫长探索！'
- en: Some mathematical tools
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些数学工具
- en: Before introducing backpropagation, we need to review some mathematical tools
    from calculus. Don’t worry too much; we’ll briefly review a few areas, all of
    which are commonly covered in high school-level mathematics.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍反向传播之前，我们需要回顾一些微积分中的数学工具。别担心，我们将简要回顾几个常见的数学领域，这些内容通常都在高中数学中涉及。
- en: Vectors
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量
- en: 'We will review two basic concepts of geometry and algebra that are quite useful
    for machine learning: vectors and the cosine of angles. We start by giving an
    explanation of vectors. Fundamentally, a vector is a list of numbers. Given a
    vector, we can interpret it as a direction in space. Mathematicians most often
    write vectors as either a column *x* or row vector *x*^T. Given two column vectors
    *u* and *v*, we can form their dot product by computing ![](img/B18331_14_001.png).
    It can be easily proven that ![](img/B18331_14_002.png) where ![](img/B18331_10_024.png)
    is the angle between the two vectors.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将回顾两个在机器学习中非常有用的基本几何和代数概念：向量和角度的余弦。我们首先解释向量的概念。根本上，向量是一个数字列表。给定一个向量，我们可以将其解释为空间中的一个方向。数学家们通常将向量写成列向量
    *x* 或行向量 *x*^T。给定两个列向量 *u* 和 *v*，我们可以通过计算它们的点积来得到它们的结果！[](img/B18331_14_001.png)。可以很容易证明，！[](img/B18331_14_002.png)，其中！[](img/B18331_10_024.png)是两个向量之间的夹角。
- en: Here are two easy questions for you. What is the result when the two vectors
    are very close? And what is the result when the two vectors are the same?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个简单的问题：当两个向量非常接近时，结果是什么？当两个向量相同的时候，结果是什么？
- en: Derivatives and gradients everywhere
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处处都有导数和梯度
- en: Derivatives are a powerful mathematical tool. We are going to use derivatives
    and gradients to optimize our network. Let’s look at the definition. The derivative
    of a function *y* = *f*(*x*) of a variable *x* is a measure of the rate at which
    the value *y* of the function changes with respect to the change of the variable
    *x*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 导数是一个强大的数学工具。我们将使用导数和梯度来优化我们的网络。让我们来看一下定义。函数*y* = *f*(*x*) 的导数是衡量函数值*y*相对于变量*x*变化的变化速率。
- en: If *x* and *y* are real numbers, and if the graph of *f* is plotted against
    *x*, the derivative is the “slope” of this graph at each point.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*x* 和 *y* 是实数，并且绘制了 *f* 与 *x* 的图像，则导数是该图像在每个点的“斜率”。
- en: 'If the function is linear ![](img/B18331_14_004.png), the slope is ![](img/B18331_14_005.png).
    This is a simple result of calculus, which can be derived by considering that:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果函数是线性的 ![](img/B18331_14_004.png)，则斜率为 ![](img/B18331_14_005.png)。这是微积分的一个简单结果，可以通过考虑得到：
- en: '![](img/B18331_14_006.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_006.png)'
- en: '![](img/B18331_14_007.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_007.png)'
- en: '![](img/B18331_14_008.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_008.png)'
- en: 'In *Figure 14.1*, we show the geometrical meaning of ![](img/B18331_14_009.png),
    ![](img/B18331_14_010.png), and the angle ![](img/B18331_10_024.png) between the
    linear function and the *x*-cartesian axis:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图14.1*中，我们展示了 ![](img/B18331_14_009.png)、![](img/B18331_14_010.png) 和角度 ![](img/B18331_10_024.png)
    之间的几何意义，角度是线性函数与*x*-笛卡尔坐标轴之间的角度：
- en: '![Chart  Description automatically generated](img/B18331_14_01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![Chart  Description automatically generated](img/B18331_14_01.png)'
- en: 'Figure 14.1: An example of a linear function and rate of change'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1：线性函数及其变化率示例
- en: 'If the function is not linear, then computing the rate of change as the mathematical
    limit value of the ratio of the differences ![](img/B18331_14_012.png) as ![](img/B18331_14_013.png)
    becomes infinitely small. Geometrically, this is the tangent line at ![](img/B18331_14_014.png)
    as shown in *Figure 14.2*:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果函数不是线性的，那么通过将差异的比值 ![](img/B18331_14_012.png) 作为 ![](img/B18331_14_013.png)
    的数学极限值计算变化速率，即差异变得无限小。从几何角度来看，这就是*图14.2*中所示的切线：
- en: '![A picture containing chart  Description automatically generated](img/B18331_14_02.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing chart  Description automatically generated](img/B18331_14_02.png)'
- en: 'Figure 14.2: Rate of change for ![](img/B18331_14_015.png) and the tangential
    line as ![](img/B18331_14_016.png)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2：![](img/B18331_14_015.png) 的变化速率和当 ![](img/B18331_14_016.png) 时的切线
- en: 'For instance, considering ![](img/B18331_14_015.png) and the derivative ![](img/B18331_14_018.png)
    in a given point, say *x* = 2, we can see that the derivative is positive ![](img/B18331_14_019.png),
    as shown in *Figure 14.3*:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑 ![](img/B18331_14_015.png) 和在给定点（比如*x* = 2）处的导数 ![](img/B18331_14_018.png)，我们可以看到导数为正
    ![](img/B18331_14_019.png)，如*图14.3*所示：
- en: '![Chart, line chart  Description automatically generated](img/B18331_14_03.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, line chart  Description automatically generated](img/B18331_14_03.png)'
- en: 'Figure 14.3: ![](img/B18331_14_015.png) and ![](img/B18331_14_018.png)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3：![](img/B18331_14_015.png) 和 ![](img/B18331_14_018.png)
- en: 'A gradient is a generalization of the derivative for multiple variables. Note
    that the derivative of a function of a single variable is a scalar-valued function,
    whereas the gradient of a function of several variables is a vector-valued function.
    The gradient is denoted with an upside-down delta ![](img/B18331_14_022.png),
    and called “del” or *nabla* from the Greek alphabet. This makes sense as delta
    indicates the change in one variable, and the gradient is the change in all variables.
    Suppose ![](img/B18331_14_023.png) (e.g. the space of real numbers with *m* dimensions)
    and *f* maps from ![](img/B18331_14_024.png) to ![](img/B18331_14_025.png); the
    gradient is defined as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度是多个变量的导数的推广。请注意，单一变量的函数的导数是标量值函数，而多个变量的函数的梯度是向量值函数。梯度用倒三角符号 ![](img/B18331_14_022.png)
    表示，称为“德尔”或来自希腊字母的*nabla*。这有道理，因为delta表示单一变量的变化，而梯度表示所有变量的变化。假设 ![](img/B18331_14_023.png)（例如具有*m*维度的实数空间）且*f*
    从 ![](img/B18331_14_024.png) 映射到 ![](img/B18331_14_025.png)；梯度定义如下：
- en: '![](img/B18331_14_026.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_026.png)'
- en: In math, a partial derivative ![](img/B18331_14_027.png) of a function of several
    variables is its derivative with respect to one of those variables, with the others
    held constant.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，多个变量的函数的偏导数 ![](img/B18331_14_027.png) 是该函数相对于其中一个变量的导数，其他变量保持不变。
- en: 'Note that it is possible to show that the gradient is a vector (a direction
    to move) that:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，可以证明梯度是一个向量（移动的方向），它：
- en: Points in the direction of the greatest increase of a function.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指向函数最大增量方向的点。
- en: Is 0 at a local maximum or local minimum. This is because if it is 0, it cannot
    increase or decrease further.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在局部最大值或局部最小值处为 0。这是因为如果它为 0，它就无法继续增大或减小。
- en: 'The proof is left as an exercise to the interested reader. (Hint: consider
    *Figure 14.2* and *Figure 14.3*.)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 证明留给有兴趣的读者作为练习。（提示：考虑 *图 14.2* 和 *图 14.3*。）
- en: Gradient descent
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降
- en: If the gradient points in the direction of the greatest increase for a function,
    then it is possible to move toward a local minimum for the function by simply
    moving in a direction opposite to the gradient. That’s the key observation used
    for gradient descent algorithms, which will be used shortly.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果梯度指向一个函数的最大增量方向，那么只需沿着梯度的反方向移动，就有可能朝着函数的局部最小值前进。这是梯度下降算法的关键观察，接下来将会使用该算法。
- en: 'An example is provided in *Figure 14.4*:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14.4* 中提供了一个例子：'
- en: '![Chart, radar chart  Description automatically generated](img/B18331_14_04.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, radar chart  Description automatically generated](img/B18331_14_04.png)'
- en: 'Fig.14.4: Gradient descent for a function in 3 variables'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4：三变量函数的梯度下降
- en: Chain rule
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 链式法则
- en: 'The chain rule says that if we have a function *y* = *g*(*x*) and ![](img/B18331_14_028.png),
    then the derivative is defined as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则指出，如果我们有一个函数 *y* = *g*(*x*) 且 ![](img/B18331_14_028.png)，那么导数定义如下：
- en: '![](img/B18331_14_029.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_029.png)'
- en: 'This chaining can be generalized beyond the scalar case. Suppose ![](img/B18331_14_023.png)
    and ![](img/B18331_14_031.png) with *g*, which maps from ![](img/B18331_14_032.png)
    to ![](img/B18331_14_024.png), and *f*, which maps from ![](img/B18331_14_024.png)
    to ![](img/B18331_14_025.png). With *y* = *g*(*x*) and *z* = *f*(*y*), we can
    deduce:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这种链式操作可以超越标量情况进行推广。假设 ![](img/B18331_14_023.png) 和 ![](img/B18331_14_031.png)
    与 *g* 相关，*g* 将 ![](img/B18331_14_032.png) 映射到 ![](img/B18331_14_024.png)，而 *f*
    将 ![](img/B18331_14_024.png) 映射到 ![](img/B18331_14_025.png)。若 *y* = *g*(*x*) 且
    *z* = *f*(*y*)，我们可以推导出：
- en: '![](img/B18331_14_036.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_036.png)'
- en: The generalized chain rule using partial derivatives will be used as a basic
    tool for the backpropagation algorithm when dealing with functions in multiple
    variables. Stop for a second and make sure that you fully understand it.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用偏导数的广义链式法则将在处理多变量函数时作为反向传播算法的基本工具。稍作停顿，确保你完全理解它。
- en: A few differentiation rules
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些求导法则
- en: 'It might be useful to remind ourselves of a few additional differentiation
    rules that will be used later:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要提醒自己一些将在后面使用的额外求导法则：
- en: 'Constant differentiation: *c’ = 0*, where *c* is a constant.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常数求导：*c' = 0*，其中 *c* 是常数。
- en: 'Variable differentiation: ![](img/B18331_14_037.png), when deriving the differentiation
    of a variable.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量求导：![](img/B18331_14_037.png)，用于求变量的导数。
- en: 'Linear differentiation: ![](img/B18331_14_038.png)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性求导：![](img/B18331_14_038.png)
- en: 'Reciprocal differentiation: ![](img/B18331_14_039.png)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 倒数求导：![](img/B18331_14_039.png)
- en: 'Exponential differentiation: ![](img/B18331_14_040.png)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指数求导：![](img/B18331_14_040.png)
- en: Matrix operations
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵运算
- en: 'There are many books about matrix calculus. Here we focus only on only a few
    basic operations used for neural networks. Recall that a matrix ![](img/B18331_14_041.png)
    can be used to represent the weights *w*[ij], with ![](img/B18331_14_042.png),
    ![](img/B18331_14_043.png) associated with the arcs between two adjacent layers.
    Note that by adjusting the weights we can control the “behavior” of the network
    and that a small change in a specific *w*[ij] will be propagated through the network
    following its topology (see *Figure 14.5*, where the edges in bold are the ones
    impacted by the small change in a specific *w*[ij]):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 关于矩阵微积分的书籍有很多。这里我们仅关注神经网络中使用的几种基本运算。回顾一下，矩阵 ![](img/B18331_14_041.png) 可用来表示权重
    *w*[ij]，其中 ![](img/B18331_14_042.png)、![](img/B18331_14_043.png) 与相邻两层之间的连接相关。请注意，通过调整权重，我们可以控制网络的“行为”，而且对特定的
    *w*[ij] 的微小变化将沿着网络拓扑结构传播（见 *图 14.5*，其中粗体边是受特定 *w*[ij] 微小变化影响的边）：
- en: '![Diagram  Description automatically generated](img/B18331_14_05.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B18331_14_05.png)'
- en: 'Figure 14.5: Propagating *w*[ij] changes through the network via the edges
    in bold'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5：通过粗体边传播 *w*[ij] 的变化
- en: Now that we have reviewed some basic concepts of calculus, let’s start applying
    them to deep learning. The first question is how to optimize activation functions.
    Well, I am pretty sure that you are thinking about computing the derivative, so
    let’s do it!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了一些微积分的基本概念，让我们开始将它们应用于深度学习。第一个问题是如何优化激活函数。嗯，我敢肯定你正在考虑计算导数，所以我们来做吧！
- en: Activation functions
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: In *Chapter 1*, *Neural Network Foundations with TF*, we saw a few activation
    functions including sigmoid, tanh, and ReLU. In the section below, we compute
    the derivative of these activation functions.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第 1 章*，*神经网络基础与 TF* 中，我们看到了一些激活函数，包括 sigmoid、tanh 和 ReLU。在下面的部分中，我们将计算这些激活函数的导数。
- en: Derivative of the sigmoid
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sigmoid 的导数
- en: 'Remember that the sigmoid is defined as ![](img/B18331_14_044.png) (see *Figure
    14.6*):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，Sigmoid 函数定义为 ![](img/B18331_14_044.png)（见 *图 14.6*）：
- en: '![](img/B18331_14_06.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_06.png)'
- en: 'Figure 14.6: Sigmoid activation function'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.6：Sigmoid 激活函数
- en: 'The derivative can be computed as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 导数可以按如下方式计算：
- en: '![](img/B18331_14_045.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_045.png)'
- en: 'Therefore the derivative of ![](img/B18331_14_046.png) can be computed as a
    very simple form: ![](img/B18331_14_047.png).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，![](img/B18331_14_046.png) 的导数可以计算为一个非常简单的形式：![](img/B18331_14_047.png)。
- en: Derivative of tanh
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tanh 的导数
- en: 'Remember that the arctan function is defined as ![](img/B18331_14_048.png)
    as seen in *Figure 14.7*:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，arctan 函数定义为 ![](img/B18331_14_048.png)，如 *图 14.7* 所示：
- en: '![Chart, line chart  Description automatically generated](img/B18331_14_07.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图 自动生成的描述](img/B18331_14_07.png)'
- en: 'Figure 14.7: Tanh activation function'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.7：Tanh 激活函数
- en: 'If you remember that ![](img/B18331_14_049.png) and ![](img/B18331_14_050.png),
    then the derivative is computed as:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得 ![](img/B18331_14_049.png) 和 ![](img/B18331_14_050.png)，那么导数可以计算为：
- en: '![](img/B18331_14_051.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_051.png)'
- en: 'Therefore the derivative of ![](img/B18331_14_052.png) can be computed as a
    very simple form: ![](img/B18331_14_053.png).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，![](img/B18331_14_052.png) 的导数可以计算为一个非常简单的形式：![](img/B18331_14_053.png)。
- en: Derivative of ReLU
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReLU 的导数
- en: 'The ReLU function is defined as ![](img/B18331_14_054.png) (see *Figure 14.8*).
    The derivative of ReLU is:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 函数定义为 ![](img/B18331_14_054.png)（见 *图 14.8*）。ReLU 的导数是：
- en: '![](img/B18331_14_055.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_055.png)'
- en: 'Note that ReLU is non-differentiable at zero. However, it is differentiable
    anywhere else, and the value of the derivative at zero can be arbitrarily chosen
    to be a 0 or 1, as demonstrated in *Figure 14.8*:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，ReLU 在零点处不可微分。然而，在其他地方它是可微分的，并且零点处的导数值可以任意选择为 0 或 1，如 *图 14.8* 中所示：
- en: '![Chart, line chart  Description automatically generated](img/B18331_14_08.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图 自动生成的描述](img/B18331_14_08.png)'
- en: 'Figure 14.8: ReLU activation function'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.8：ReLU 激活函数
- en: Backpropagation
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: Now that we have computed the derivative of the activation functions, we can
    describe the backpropagation algorithm — the mathematical core of deep learning.
    Sometimes, backpropagation is called *backprop* for short.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算了激活函数的导数，可以描述反向传播算法——深度学习的数学核心。有时，反向传播简称为 *backprop*。
- en: Remember that a neural network can have multiple hidden layers, as well as one
    input layer and one output layer.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，一个神经网络可以有多个隐藏层，以及一个输入层和一个输出层。
- en: In addition to that, recall from *Chapter 1*, *Neural Network Foundations with
    TF*, that backpropagation can be described as a way of progressively correcting
    mistakes as soon as they are detected. In order to reduce the errors made by a
    neural network, we must train the network. The training needs a dataset including
    input values and the corresponding true output value. We want to use the network
    for predicting output as close as possible to the true output value. The key intuition
    of the backpropagation algorithm is to update the weights of the connections based
    on the measured error at the output neuron(s). In the remainder of this section,
    we will explain how to formalize this intuition.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，请回想一下 *第 1 章*，*神经网络基础与 TF*，中提到的，反向传播可以被描述为一种在错误被发现后，逐步纠正错误的方式。为了减少神经网络的错误，我们必须训练网络。训练需要一个包含输入值及其相应真实输出值的数据集。我们希望使用这个网络来预测尽可能接近真实输出值的结果。反向传播算法的关键直觉是根据输出神经元的误差来更新连接的权重。在本节的剩余部分，我们将解释如何形式化这个直觉。
- en: 'When backpropagation starts, all the weights have some random assignment. Then
    the net is activated for each input in the training set; values are propagated
    forward from the input stage through the hidden stages to the output stage where
    a prediction is made (note that we keep the figure below simple by only representing
    a few values with green dotted lines, but in reality, all the values are propagated
    forward through the network):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当反向传播开始时，所有权重都有一些随机赋值。然后，网络对训练集中的每个输入进行激活；值从输入阶段通过隐藏阶段传播到输出阶段，最终做出预测（请注意，为了简化示意图，我们只表示了一些带绿色虚线的值，但实际上所有值都会通过网络向前传播）：
- en: '![Diagram, schematic  Description automatically generated](img/B18331_14_09.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图示，示意图 描述自动生成](img/B18331_14_09.png)'
- en: 'Figure 14.9: Forward step in backpropagation'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.9：反向传播中的正向步骤
- en: 'Since we know the true observed value in the training set, it is possible to
    calculate the error made in the prediction. The easiest way to think about backtracking
    is to propagate the error back (see *Figure 14.10*), using an appropriate optimizer
    algorithm such as gradient descent to adjust the neural network weights, with
    the goal of reducing the error (again, for the sake of simplicity only a few error
    values are represented here):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道训练集中的真实观察值，可以计算预测中所犯的错误。回溯的最简单方式是将错误反向传播（见*图14.10*），使用合适的优化算法（如梯度下降）来调整神经网络的权重，目的是减少误差（为了简化起见，这里仅表示少量误差值）：
- en: '![Diagram  Description automatically generated](img/B18331_14_10.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图示 描述自动生成](img/B18331_14_10.png)'
- en: 'Figure 14.10: Backward step in backpropagation'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.10：反向传播中的反向步骤
- en: The process of forward propagation from input to output and backward propagation
    of errors is repeated several times until the error goes below a predefined threshold.
    The whole process is represented in *Figure 14.11*. A set of features is selected
    as input to a machine learning model that produces predictions.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入到输出的正向传播和误差的反向传播过程会重复多次，直到误差降到预定的阈值以下。整个过程如*图14.11*所示。选择一组特征作为机器学习模型的输入，模型根据这些输入生成预测结果。
- en: 'The predictions are compared with the (true) label, and the resulting loss
    function is minimized by the optimizer, which updates the weights of the model:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 将预测结果与（真实的）标签进行比较，生成的损失函数由优化器最小化，优化器更新模型的权重：
- en: '![Diagram  Description automatically generated](img/B18331_14_11.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图示 描述自动生成](img/B18331_14_11.png)'
- en: 'Figure 14.11: Forward propagation and backward propagation'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.11：正向传播与反向传播
- en: Let’s see in detail how the forward and backward steps are realized. It might
    be useful to have a look back at *Figure 14.5* and recall that a small change
    in a specific *w*[ij] will be propagated through the network following its topology
    (see *Figure 14.5*, where the edges in bold are the ones impacted by the small
    change in specific weights).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看正向和反向步骤是如何实现的。回顾一下*图14.5*可能会有帮助，并回忆一下特定*w*[ij]的小变化将如何通过网络传播，遵循其拓扑结构（参见*图14.5*，其中粗体的边是受特定权重小变化影响的部分）。
- en: Forward step
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正向步骤
- en: 'During the forward steps, the inputs are multiplied with the weights and then
    all summed together. Then the activation function is applied (see *Figure 14.12*).
    This step is repeated for each layer, one after another. The first layer takes
    the input features as input and it produces its output. Then, each subsequent
    layer takes as input the output of the previous layer:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在正向步骤中，输入与权重相乘，然后所有结果加起来。接着应用激活函数（见*图14.12*）。此步骤对每一层依次重复。第一层将输入特征作为输入并产生输出。然后，每一层的输入是前一层的输出：
- en: '![Arrow  Description automatically generated with medium confidence](img/B18331_14_12.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![箭头 描述自动生成，置信度中等](img/B18331_14_12.png)'
- en: 'Figure 14.12: Forward propagation'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.12：正向传播
- en: 'If we look at one single layer, mathematically we have two equations:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只看一个单独的层，数学上我们有两个方程：
- en: The transfer equation ![](img/B18331_14_056.png), where *x*[i] are the input
    values, *w*[i] are the weights, and *b* is the bias. In vector notation ![](img/B18331_14_057.png).
    Note that *b* can be *absorbed* in the summatory by setting ![](img/B18331_14_058.png)
    and ![](img/B18331_14_059.png).
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转移方程 ![](img/B18331_14_056.png)，其中*x*[i]为输入值，*w*[i]为权重，*b*为偏置。以向量表示为 ![](img/B18331_14_057.png)。注意，*b*可以通过设置
    ![](img/B18331_14_058.png) 和 ![](img/B18331_14_059.png) 来*吸收*进求和式中。
- en: 'The activation function: ![](img/B18331_14_060.png), where ![](img/B18331_07_010.png)
    is the chosen activation function.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数： ![](img/B18331_14_060.png)，其中 ![](img/B18331_07_010.png)是选择的激活函数。
- en: An artificial neural network consists of an input layer *I*, an output layer
    *O*, and any number of hidden layers *H*[i] situated between the input and the
    output layers. For the sake of simplicity, let’s assume that there is only one
    hidden layer, since the results can be easily generalized.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一个人工神经网络由输入层*I*、输出层*O*以及位于输入层和输出层之间的任意数量的隐藏层*H*[i]组成。为了简化起见，假设只有一个隐藏层，因为结果可以很容易地推广。
- en: As shown in *Figure 14.12*, the features *x*[i] from the input layer are multiplied
    by a set of fully connected weights *w*[ij] connecting the input layer to the
    hidden layer (see the left side of *Figure 14.12*). The weighted signals are summed
    together and with the bias to calculate the result ![](img/B18331_14_062.png)
    (see the center of *Figure 14.12*). The result is passed through the activation
    function ![](img/B18331_14_063.png), which leaves the hidden layer to the output
    layer (see the right side of *Figure 14.12*).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 14.12*所示，来自输入层的特征*x*[i]与连接输入层和隐藏层的全连接权重*w*[ij]相乘（见*图 14.12*的左侧）。加权信号与偏置一起求和以计算结果
    ![](img/B18331_14_062.png)（见*图 14.12*的中间）。结果通过激活函数 ![](img/B18331_14_063.png)传递，最终从隐藏层流向输出层（见*图
    14.12*的右侧）。
- en: 'In summary, during the forward step we need to run the following operations:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，在前向传播过程中，我们需要执行以下操作：
- en: For each neuron in a layer, multiply each input by its corresponding weight.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于一层中的每个神经元，将每个输入乘以其对应的权重。
- en: Then for each neuron in the layer, sum all input weights together.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后对于层中的每个神经元，将所有输入权重加在一起。
- en: Finally, for each neuron, apply the activation function on the result to compute
    the new output.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，对于每个神经元，对结果应用激活函数来计算新的输出。
- en: 'At the end of the forward step, we get a predicted vector ![](img/B18331_14_064.png)
    from the output layer *o* given the input vector *x* presented at the input layer.
    Now the question is: how close is the predicted vector ![](img/B18331_14_064.png)
    to the true value vector *t*?'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播结束时，我们从输出层*o*获得一个预测向量 ![](img/B18331_14_064.png)，该向量是给定输入向量*x*的结果，输入向量在输入层给出。现在的问题是：预测向量
    ![](img/B18331_14_064.png)与真实值向量*t*有多接近？
- en: That’s where the backstep comes in.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是反向传播的作用。
- en: Backstep
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播
- en: 'To understand how close the predicted vector ![](img/B18331_14_064.png) is
    to the true value vector *t*, we need a function that measures the error at the
    output layer *o*. That is the *loss function* defined earlier in the book. There
    are many choices for loss function. For instance, we can define the mean squared
    error defined as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解预测向量 ![](img/B18331_14_064.png)与真实值向量*t*有多接近，我们需要一个函数来衡量输出层*o*的误差。这就是书中早期定义的*损失函数*。损失函数有很多选择。例如，我们可以定义均方误差，如下所示：
- en: '![](img/B18331_14_067.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_067.png)'
- en: Note that *E* is a quadratic function and, therefore, the difference is quadratically
    larger when *t* is far away from ![](img/B18331_14_064.png), and the sign is not
    important. Note that this quadratic error (loss) function is not the only one
    that we can use. Later in this chapter, we will see how to deal with cross-entropy.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*E*是一个二次函数，因此，当*t*与 ![](img/B18331_14_064.png)相距较远时，差值的平方会更大，且符号不重要。注意，这个二次误差（损失）函数并不是唯一可以使用的函数。稍后在本章中，我们将看到如何处理交叉熵。
- en: 'Now, remember that the key point is that during the training, we want to adjust
    the weights of the network to minimize the final error. As discussed, we can move
    toward a local minimum by moving in the opposite direction to the gradient ![](img/B18331_14_069.png).
    Moving in the opposite direction to the gradient is the reason why this algorithm
    is called *gradient descent*. Therefore, it is reasonable to define the equation
    for updating the weight *w*[ij] as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，记住关键点是，在训练过程中，我们希望调整网络的权重，以最小化最终误差。如前所述，我们可以通过沿着梯度的反方向移动来接近局部最小值！[](img/B18331_14_069.png)。沿梯度的反方向移动是为什么这个算法叫做*梯度下降*的原因。因此，定义更新权重*w*[ij]的方程是合理的，如下所示：
- en: '![](img/B18331_14_070.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_070.png)'
- en: For a function in multiple variables, the gradient is computed using partial
    derivatives. We introduce the hyperparameter ![](img/B18331_01_025.png) – or,
    in ML lingo, the learning rate – to account for how large a step should be in
    the direction opposite to the gradient.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多变量的函数，梯度通过偏导数来计算。我们引入超参数 ![](img/B18331_01_025.png) —— 或者在机器学习术语中称为学习率 ——
    来衡量在梯度的反方向上应该走多大的步长。
- en: 'Considering the error, *E*, we have the equation:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑误差 *E*，我们得到以下方程：
- en: '![](img/B18331_14_072.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_072.png)'
- en: 'The preceding equation is simply capturing the fact that a slight change will
    impact the final error, as seen in *Figure 14.13*:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程只是捕捉到一个微小变化将影响最终误差的事实，如*图14.13*所示：
- en: '![Diagram  Description automatically generated](img/B18331_14_13.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图示 说明自动生成](img/B18331_14_13.png)'
- en: 'Figure 14.13: A small change in *w*[ij] will impact the final error *E*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.13：*w*[ij] 的小变化将影响最终的误差 *E*
- en: 'Let’s define the notation used throughout our equations in the remaining section:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义在剩余部分中方程中使用的符号：
- en: '![](img/B18331_14_073.png) is the input to node *j* for layer *l*.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B18331_14_073.png) 是层 *l* 中节点 *j* 的输入。'
- en: '![](img/B18331_14_074.png) is the activation function for node *j* in layer
    *l* (applied to ![](img/B18331_14_073.png)).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B18331_14_074.png) 是层 *l* 中神经元 *j* 的激活函数（应用于 ![](img/B18331_14_073.png)）。'
- en: '![](img/B18331_14_076.png) is the output of the activation of node *j* in layer
    *l*.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B18331_14_076.png) 是层 *l* 中神经元 *j* 的激活输出。'
- en: '![](img/B18331_14_077.png) is the matrix of weights connecting the neuron *i*
    in layer ![](img/B18331_14_078.png) to the neuron *j* in layer *l*.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B18331_14_077.png) 是连接层 ![](img/B18331_14_078.png) 中神经元 *i* 到层 *l*
    中神经元 *j* 的权重矩阵。'
- en: '![](img/B18331_14_079.png) is the bias for unit *j* in layer *l*.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B18331_14_079.png) 是层 *l* 中单元 *j* 的偏置。'
- en: '![](img/B18331_14_080.png) is the target value for node *o* in the output layer.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B18331_14_080.png) 是输出层节点 *o* 的目标值。'
- en: 'Now we need to compute the partial derivative for the error at the output layer
    ![](img/B18331_14_081.png) when the weights change by ![](img/B18331_14_082.png).
    There are two different cases:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要计算当权重变化为 ![](img/B18331_14_082.png) 时，输出层 ![](img/B18331_14_081.png) 处误差的偏导数。这里有两种不同的情况：
- en: '**Case 1:** Weight update equation for a neuron from hidden (or input) layer
    to output layer.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情况 1：** 从隐藏（或输入）层到输出层的神经元权重更新方程。'
- en: '**Case 2:** Weight update equation for a neuron from hidden (or input) layer
    to hidden layer.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情况 2：** 从隐藏（或输入）层到隐藏层的神经元权重更新方程。'
- en: We’ll begin with Case 1.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从情况 1 开始。
- en: 'Case 1: From hidden layer to output layer'
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 情况 1：从隐藏层到输出层
- en: 'In this case, we need to consider the equation for a neuron from hidden layer
    *j* to output layer *o*. Applying the definition of *E* and differentiating we
    have:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们需要考虑从隐藏层 *j* 到输出层 *o* 的神经元的方程。应用 *E* 的定义并对其求导，我们得到：
- en: '![](img/B18331_14_083.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_083.png)'
- en: 'Here the summation disappears because when we take the partial derivative with
    respect to the *j*-th dimension, the only term that is not zero in the error is
    the *j*-th. Considering that differentiation is a linear operation and that ![](img/B18331_14_084.png)
    – because the true ![](img/B18331_14_085.png) value does not depend on ![](img/B18331_14_086.png)
    – we have:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这里求和项消失了，因为当我们对 *j* 维度求偏导时，误差中唯一不为零的项是 *j* 维度。考虑到微分是线性操作，并且 ![](img/B18331_14_084.png)
    —— 因为真实的 ![](img/B18331_14_085.png) 值不依赖于 ![](img/B18331_14_086.png) —— 我们得到：
- en: '![](img/B18331_14_087.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_087.png)'
- en: 'Applying the chain rule again and remembering that ![](img/B18331_14_088.png),
    we have:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 再次应用链式法则，并记住！[](img/B18331_14_088.png)，我们得到：
- en: '![](img/B18331_14_089.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_089.png)'
- en: 'Remembering that ![](img/B18331_14_090.png), we have ![](img/B18331_14_091.png)
    again because when we take the partial derivative with respect to the *j*-th dimension
    the only term that is not zero in the error is the *j*-th. By definition ![](img/B18331_14_092.png),
    so putting everything together we have:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 记住 ![](img/B18331_14_090.png)，我们再次得到 ![](img/B18331_14_091.png)，因为当我们对 *j* 维度求偏导时，误差中唯一不为零的项是
    *j* 维度。根据定义，![](img/B18331_14_092.png)，因此将所有内容合并后我们得到：
- en: '![](img/B18331_14_093.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_093.png)'
- en: 'The gradient of the error *E* with respect to the weights *w*[j] from the hidden
    layer *j* to the output layer *o* is therefore simply the product of three terms:
    the difference between the prediction ![](img/B18331_14_064.png) and the true
    value ![](img/B18331_14_080.png), the derivative ![](img/B18331_14_096.png) of
    the output layer activation function, and the activation output ![](img/B18331_14_097.png)
    of node *j* in the hidden layer. For simplicity we can also define ![](img/B18331_14_098.png)
    and get:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，误差 *E* 相对于从隐藏层 *j* 到输出层 *o* 的权重 *w*[j] 的梯度，实际上是三项的乘积：预测值 ![](img/B18331_14_064.png)
    与真实值 ![](img/B18331_14_080.png) 之间的差值、输出层激活函数的导数 ![](img/B18331_14_096.png)，以及隐藏层节点
    *j* 的激活输出 ![](img/B18331_14_097.png)。为了简化，我们还可以定义 ![](img/B18331_14_098.png)，得到：
- en: '![](img/B18331_14_099.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_099.png)'
- en: 'In short, for Case 1, the weight update equation for each of the hidden-output
    connections is:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，对于案例 1，每个隐藏层-输出层连接的权重更新方程为：
- en: '![](img/B18331_14_100.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_100.png)'
- en: 'Note: if we want to explicitly compute the gradient with respect to the output
    layer biases, the steps to follow are similar to the ones above with only one
    difference:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果我们要显式计算相对于输出层偏置的梯度，遵循的步骤与上述类似，唯一的区别是：
- en: '![](img/B18331_14_101.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_101.png)'
- en: so in this case ![](img/B18331_14_102.png).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这种情况下，![](img/B18331_14_102.png)。
- en: Next, we’ll look at Case 2.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看案例 2。
- en: 'Case 2: From hidden layer to hidden layer'
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 案例 2：从隐藏层到隐藏层
- en: In this case, we need to consider the equation for a neuron from a hidden layer
    (or the input layer) to a hidden layer. *Figure 14.13* showed that there is an
    indirect relationship between the hidden layer weight change and the output error.
    This makes the computation of the gradient a bit more challenging. In this case,
    we need to consider the equation for a neuron from hidden layer *i* to hidden
    layer *j*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们需要考虑从隐藏层（或输入层）到隐藏层的神经元方程。*图 14.13* 显示了隐藏层权重变化与输出误差之间的间接关系。这使得梯度的计算变得有些复杂。在这种情况下，我们需要考虑从隐藏层
    *i* 到隐藏层 *j* 的神经元方程。
- en: 'Applying the definition of *E* and differentiating we have:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 应用 *E* 的定义并进行微分，我们得到：
- en: '![](img/B18331_14_103.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_103.png)'
- en: 'In this case, the sum will not disappear because the change of weights in the
    hidden layer is directly affecting the output. Substituting ![](img/B18331_14_104.png)
    and applying the chain rule we have:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，和不会消失，因为隐藏层中权重的变化直接影响输出。替换 ![](img/B18331_14_104.png) 并应用链式法则，我们得到：
- en: '![](img/B18331_14_105.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_105.png)'
- en: 'The indirect relation between ![](img/B18331_14_106.png) and the internal weights
    *w*[ij] (*Figure 14.13*) is mathematically expressed by the expansion:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B18331_14_106.png) 和内部权重 *w*[ij] (*图 14.13*) 之间的间接关系可以通过以下展开式数学表达：'
- en: '![](img/B18331_14_107.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_107.png)'
- en: since ![](img/B18331_14_108.png).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 ![](img/B18331_14_108.png)。
- en: 'This suggests applying the chain rule again:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明需要再次应用链式法则：
- en: '![](img/B18331_14_109.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_109.png)'
- en: 'Applying the chain rule:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 应用链式法则：
- en: '![](img/B18331_14_110.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_110.png)'
- en: 'Substituting ![](img/B18331_14_106.png):'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 替换 ![](img/B18331_14_106.png)：
- en: '![](img/B18331_14_112.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_112.png)'
- en: 'Deriving:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 推导：
- en: '![](img/B18331_14_113.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_113.png)'
- en: 'Substituting ![](img/B18331_14_076.png):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 替换 ![](img/B18331_14_076.png)：
- en: '![](img/B18331_14_115.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_115.png)'
- en: 'Applying the chain rule:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 应用链式法则：
- en: '![](img/B18331_14_116.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_116.png)'
- en: 'Substituting ![](img/B18331_14_117.png):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 替换 ![](img/B18331_14_117.png)：
- en: '![](img/B18331_14_118.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_118.png)'
- en: 'Deriving:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 推导：
- en: '![](img/B18331_14_119.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_119.png)'
- en: 'Now we can combine the above two results:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以结合上述两个结果：
- en: '![](img/B18331_14_120.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_120.png)'
- en: '![](img/B18331_14_121.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_121.png)'
- en: 'and get:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 并得到：
- en: '![](img/B18331_14_122.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_122.png)'
- en: 'Remembering the definition: ![](img/B18331_14_098.png), we get:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 记住定义：![](img/B18331_14_098.png)，我们得到：
- en: '![](img/B18331_14_124.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_124.png)'
- en: 'This last substitution with ![](img/B18331_14_125.png) is particularly interesting
    because it backpropagates the signal ![](img/B18331_14_125.png) computed in the
    subsequent layer. The rate of change ![](img/B18331_14_081.png) with respect to
    the rate of change of the weights *w*[ij] is therefore the multiplication of three
    factors: the output activations *y*[i] from the layer below, the derivative of
    hidden layer activation function ![](img/B18331_14_128.png), and the sum of the
    backpropagated signal ![](img/B18331_14_125.png) previously computed in the subsequent
    layer weighted by ![](img/B18331_14_086.png). We can use this idea of backpropagating
    the error signal by defining ![](img/B18331_14_131.png) and therefore ![](img/B18331_14_132.png).
    This suggests that in order to calculate the gradients at any layer ![](img/B18331_14_133.png)
    in a deep neural network, we can simply multiply the backpropagated error signal
    ![](img/B18331_14_134.png) and multiply it by the feed-forward signal ![](img/B18331_14_135.png),
    arriving at the layer *l*. Note that the math is a bit complex but the result
    is indeed very very simple! The intuition is given in *Figure 14.14*. Given a
    function ![](img/B18331_14_136.png), computed locally to a neuron with the input
    ![](img/B18331_14_137.png), and ![](img/B18331_14_138.png), the gradients ![](img/B18331_14_139.png)
    are backpropagated. Then, they are combined via the chain rule with the local
    gradients ![](img/B18331_14_140.png) and ![](img/B18331_14_141.png) for further
    backpropagation.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后一个替代公式与 ![](img/B18331_14_125.png) 特别有趣，因为它反向传播了后续层中计算得到的信号 ![](img/B18331_14_125.png)。相对于权重
    *w*[ij] 的变化率 ![](img/B18331_14_081.png) 因此是三个因子的乘积：来自下面一层的输出激活值 *y*[i]，隐藏层激活函数的导数
    ![](img/B18331_14_128.png)，以及通过 ![](img/B18331_14_086.png) 权重加权的之前在后续层计算的反向传播信号
    ![](img/B18331_14_125.png)。我们可以通过定义 ![](img/B18331_14_131.png) 来利用这种反向传播误差信号的思想，从而得出
    ![](img/B18331_14_132.png)。这表明，为了计算深度神经网络中任何一层 ![](img/B18331_14_133.png) 的梯度，我们只需将反向传播的误差信号
    ![](img/B18331_14_134.png) 与前馈信号 ![](img/B18331_14_135.png) 相乘，就能到达 *l* 层。注意，数学公式稍显复杂，但结果实际上是非常非常简单的！直观理解见
    *图 14.14*。给定一个函数 ![](img/B18331_14_136.png)，在神经元处局部计算得到输入 ![](img/B18331_14_137.png)
    和 ![](img/B18331_14_138.png)，梯度 ![](img/B18331_14_139.png) 被反向传播。然后，通过链式法则与局部梯度
    ![](img/B18331_14_140.png) 和 ![](img/B18331_14_141.png) 结合，进一步进行反向传播。
- en: 'Here, *L* denotes the error from the generic previous layer:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*L* 表示来自上一层的误差：
- en: '![Diagram  Description automatically generated](img/B18331_14_14.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B18331_14_14.png)'
- en: 'Figure 14.14: An example of the math behind backpropagation'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.14：反向传播背后的数学示例
- en: 'Note: if we want to explicitly compute the gradient with respect to the output
    layer biases, it can be proven that ![](img/B18331_14_142.png). We leave this
    as an exercise for you.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果我们想显式地计算输出层偏置的梯度，可以证明 ![](img/B18331_14_142.png)。我们将这作为练习留给你。
- en: 'In short, for Case 2 (hidden-to-hidden connection) the weight delta is ![](img/B18331_14_143.png)
    and the weight update equation for each of the hidden-hidden connections is simply:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，对于案例 2（隐藏层到隐藏层的连接），权重的变化量是 ![](img/B18331_14_143.png)，每个隐藏层连接的权重更新方程仅为：
- en: '![](img/B18331_14_144.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_144.png)'
- en: 'We have arrived at the end of this section and all the mathematical tools are
    defined to make our final statement. The essence of the backstep is nothing more
    than applying the weight update rule one layer after another, starting from the
    last output layer and moving back toward the first input layer. Difficult to derive,
    to be sure, but extremely easy to apply once defined. The whole forward-backward
    algorithm at the core of deep learning is then the following:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经到达本节的结尾，所有数学工具都已定义好，以便做出最终的陈述。反向传播的本质无非是从最后的输出层开始，一层一层地应用权重更新规则，一直到第一层输入层。虽然推导过程很困难，但一旦定义清楚，应用起来极其简单。深度学习的前向-反向算法的核心可以总结为以下内容：
- en: Compute the feedforward signals from the input to the output.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算从输入到输出的前馈信号。
- en: Compute the output error *E* based on the predictions ![](img/B18331_14_064.png)
    and the true value ![](img/B18331_14_080.png).
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据预测值 ![](img/B18331_14_064.png) 和真实值 ![](img/B18331_14_080.png)，计算输出误差 *E*。
- en: Backpropagate the error signals; multiply them with the weights in previous
    layers and with the gradients of the associated activation functions.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播误差信号；将它们与前一层的权重和相关激活函数的梯度相乘。
- en: Compute the gradients ![](img/B18331_14_147.png) for all of the parameters ![](img/B18331_10_024.png)
    based on the backpropagated error signal and the feedforward signals from the
    inputs.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有参数 ![](img/B18331_10_024.png) 的梯度 ![](img/B18331_14_147.png)，基于反向传播的误差信号和来自输入的前向传播信号。
- en: Update the parameters using the computed gradients ![](img/B18331_14_149.png).
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用计算出的梯度 ![](img/B18331_14_149.png) 更新参数。
- en: Note that the above algorithm will work for any choice of differentiable error
    function *E* and for any choice of differentiable activation ![](img/B18331_14_150.png)
    function. The only requirement is that both must be differentiable.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述算法适用于任何可微的误差函数 *E* 和任何可微的激活 ![](img/B18331_14_150.png) 函数。唯一的要求是它们都必须是可微的。
- en: Gradient descent with backpropagation is not guaranteed to find the global minimum
    of the loss function, but only a local minimum. However, this is not necessarily
    a problem observed in practical application.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 使用反向传播的梯度下降不能保证找到损失函数的全局最小值，而只是局部最小值。然而，这在实际应用中不一定是一个问题。
- en: Cross entropy and its derivative
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉熵及其导数
- en: 'Gradient descent can be used when cross-entropy is adopted as the loss function.
    As discussed in *Chapter 1*, *Neural Network Foundations with TF*, the logistic
    loss function is defined as:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 当交叉熵作为损失函数时，可以使用梯度下降。正如在 *第一章* 中讨论的，*《使用 TF 的神经网络基础》*，逻辑回归损失函数定义为：
- en: '![](img/B18331_14_151.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_151.png)'
- en: 'Where *c* refers to one-hot-encoded classes (or labels) whereas *p* refers
    to softmax-applied probabilities. Since cross-entropy is applied to softmax-applied
    probabilities and to one-hot-encoded classes, we need to take into account the
    chain rule for computing the gradient with respect to the final weights *score*[i].
    Mathematically, we have:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *c* 指的是独热编码类（或标签），而 *p* 指的是 softmax 应用后的概率。由于交叉熵应用于 softmax 应用后的概率和独热编码类，我们需要考虑计算相对于最终权重
    *score*[i] 的梯度的链式法则。从数学上来说，我们有：
- en: '![](img/B18331_14_152.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_152.png)'
- en: 'Computing each part separately, let’s start from ![](img/B18331_14_153.png):'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 分别计算每一部分，让我们从 ![](img/B18331_14_153.png) 开始：
- en: '![](img/B18331_14_154.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_154.png)'
- en: (noting that for a fixed ![](img/B18331_14_155.png) all the terms in the sum
    are constant except the chosen one).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: （注意，对于固定的 ![](img/B18331_14_155.png)，求和中的所有项都是常数，除了选择的项）。
- en: 'Therefore, we have:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到：
- en: '![](img/B18331_14_156.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_156.png)'
- en: (applying the partial derivative to the sum and considering that ![](img/B18331_14_157.png))
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: （将偏导数应用到求和式中，考虑到 ![](img/B18331_14_157.png)）
- en: 'Therefore, we have:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到：
- en: '![](img/B18331_14_158.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_158.png)'
- en: 'Now let’s compute the other part ![](img/B18331_14_159.png) where *p*[i] is
    the softmax function defined as:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算另一个部分 ![](img/B18331_14_159.png)，其中 *p*[i] 是定义为以下的 softmax 函数：
- en: '![](img/B18331_14_160.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_160.png)'
- en: 'The derivative is:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 导数是：
- en: '![](img/B18331_14_161.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_161.png)'
- en: and
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![](img/B18331_14_162.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_162.png)'
- en: 'Using the Kronecker delta ![](img/B18331_14_163.png) we have:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 使用克罗内克 delta ![](img/B18331_14_163.png) 我们得到：
- en: '![](img/B18331_14_164.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_164.png)'
- en: 'Therefore, considering that we are computing the partial derivative, all the
    components are zeroed with the exception of only one, and we have:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，考虑到我们正在计算偏导数，除了一个分量外，所有其他分量都会归零，我们得到：
- en: '![](img/B18331_14_165.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_165.png)'
- en: 'Combining the results, we have:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 将结果结合起来，我们得到：
- en: '![](img/B18331_14_166.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_166.png)'
- en: 'Where *c*[i] denotes the one-hot-encoded classes and *p*[i] refers to the softmax
    probabilities. In short, the derivative is both elegant and easy to compute:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *c*[i] 表示独热编码类，而 *p*[i] 指的是 softmax 概率。简而言之，导数既优雅又容易计算：
- en: '![](img/B18331_14_167.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_167.png)'
- en: Batch gradient descent, stochastic gradient descent, and mini-batch
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量梯度下降、随机梯度下降和小批量
- en: 'If we generalize the previous discussion, then we can state that the problem
    of optimizing a neural network consists of adjusting the weights *w* of the network
    in such a way that the loss function is minimized. Conveniently, we can think
    about the loss function in the form of a sum, as in this form it’s indeed representing
    all the loss functions commonly used:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将之前的讨论进行概括，那么我们可以说优化神经网络的问题就是调整网络的权重 *w*，使得损失函数最小化。方便的是，我们可以将损失函数看作一个求和的形式，因为这种形式实际上代表了所有常用的损失函数：
- en: '![](img/B18331_14_168.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_168.png)'
- en: 'In this case, we can perform a derivation using steps very similar to those
    discussed previously, following the update rule, where ![](img/B18331_01_025.png)
    is the learning rate and ![](img/B18331_14_022.png) is the gradient:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以使用与之前讨论的步骤非常相似的推导方法，遵循更新规则，其中![](img/B18331_01_025.png)是学习率，![](img/B18331_14_022.png)是梯度：
- en: '![](img/B18331_14_171.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_171.png)'
- en: In many cases, evaluating the above gradient might require an expensive evaluation
    of the gradients from all summand functions. When the training set is very large,
    this can be extremely expensive. If we have three million samples, we have to
    loop through three million times or use the dot product. That’s a lot! How can
    we simplify this? There are three types of gradient descent, each different in
    the way they handle the training dataset.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，评估上述梯度可能需要对所有加和函数的梯度进行昂贵的评估。当训练集非常大时，这可能非常昂贵。如果我们有三百万个样本，我们就得循环三百万次或使用点积。这可真不简单！我们该如何简化这个过程呢？有三种梯度下降方法，它们在处理训练数据集的方式上各不相同。
- en: Batch gradient descent
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量梯度下降
- en: '**Batch Gradient Descent** (**BGD**) computes the change of error but updates
    the whole model only once the entire dataset has been evaluated. Computationally
    it is very efficient, but it requires that the results for the whole dataset be
    held in the memory.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**批量梯度下降**（**BGD**）计算误差的变化，但只有在整个数据集评估完之后，才会更新整个模型。从计算上讲，它非常高效，但需要将整个数据集的结果保存在内存中。'
- en: Stochastic gradient descent
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: 'Instead of updating the model once the dataset has been evaluated, **Stochastic
    Gradient Descent** (**SGD**) does so after every single training example. The
    key idea is very simple: SGD samples a subset of summand functions at every step.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 与在整个数据集评估完之后更新模型不同，**随机梯度下降**（**SGD**）在每个训练样本之后进行更新。其关键思想非常简单：SGD在每一步都抽样一小部分加和函数。
- en: Mini-batch gradient descent
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小批量梯度下降
- en: '**Mini-Batch Gradient Descent** (**MBGD**) is very frequently used in deep
    learning. MBGD (or mini-batch) combines BGD and SGD in one single heuristic. The
    dataset is divided into small batches of about size *bs*, generally 64 to 256\.
    Then each of the batches is evaluated separately.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**小批量梯度下降**（**MBGD**）在深度学习中非常常用。MBGD（或小批量）将BGD和SGD结合在一个单一的启发式方法中。数据集被分成小批量，大小约为*bs*，通常为64到256。然后，每个批量会单独进行评估。'
- en: Note that *bs* is another hyperparameter to fine-tune during training. MBGD
    lies between the extremes of BGD and SGD – by adjusting the batch size and the
    learning rate parameters, we sometimes find a solution that descends closer to
    the global minimum than what can be achieved by either of the extremes.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*bs*是另一个需要在训练过程中微调的超参数。MBGD位于BGD和SGD的两个极端之间——通过调整批量大小和学习率参数，我们有时可以找到比这两种极端方法更接近全局最小值的解。
- en: In contrast with gradient descent, where the cost function is minimized more
    smoothly, the mini-batch gradient has a bit more of a noisy and bumpy descent,
    but the cost function still trends downhill. The reason for the noise is that
    mini-batches are a sample of all the examples and this sampling can cause the
    loss function to oscillate.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 与梯度下降平滑地最小化代价函数不同，小批量梯度下降具有较为噪声和崎岖的下降趋势，但代价函数仍然是下降的。噪声的原因在于小批量是所有示例的一个子集，这种抽样可能会导致损失函数出现波动。
- en: Thinking about backpropagation and ConvNets
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思考反向传播和卷积神经网络（ConvNets）
- en: In this section, we will examine backprop and ConvNets. For the sake of simplicity,
    we will focus on an example of convolution with input *X* of size 3x3, one single
    filter *W* of size 2x2 with no padding, stride 1, and no dilation (see *Chapter
    3*, *Convolutional Neural Networks*). The generalization is left as an exercise.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将讨论反向传播和卷积神经网络。为了简化起见，我们将重点关注一个卷积例子，输入*X*大小为3x3，一个大小为2x2的单一滤波器*W*，没有填充，步长为1，且没有扩张（见*第3章*，*卷积神经网络*）。推广部分留作练习。
- en: 'The standard convolution operation is represented in *Figure 14.15*. Simply
    put, the convolutional operation is the forward pass:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的卷积操作如*图14.15*所示。简单来说，卷积操作是前向传播过程：
- en: '| **Input****X11****X12****X13****X21****X22****X23****X31****X32****X33**
    | **Weights****W11****W12****W21****W22** | **Convolution****W11X11+W12X12+W21X21+W22X22****W11X12+W12X13+W21X21+W22X23****W11X21+W12X22+W21X31+W22X32****W11X22+W12X23+W21X32+W22X33**
    |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| **输入** **X11** **X12** **X13** **X21** **X22** **X23** **X31** **X32** **X33**
    | **权重** **W11** **W12** **W21** **W22** | **卷积** **W11X11+W12X12+W21X21+W22X22**
    **W11X12+W12X13+W21X21+W22X23** **W11X21+W12X22+W21X31+W22X32** **W11X22+W12X23+W21X32+W22X33**
    |'
- en: 'Figure 14.15: Forward pass for our ConvNet toy example'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.15：ConvNet 范例的前向传播
- en: Following the examination of *Figure 14.15*, we can now focus our attention
    on the backward pass for the current layer. The key assumption is that we receive
    a backpropagated signal ![](img/B18331_14_172.png) as input, and we need to compute
    ![](img/B18331_14_173.png) and ![](img/B18331_14_174.png). This computation is
    left as an exercise, but please note that each weight in the filter contributes
    to each pixel in the output map or, in other words, any change in a weight of
    a filter affects all the output pixels.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查完 *图 14.15* 后，我们现在可以将注意力集中在当前层的反向传播上。关键假设是，我们接收到一个反向传播信号 ![](img/B18331_14_172.png)
    作为输入，需要计算 ![](img/B18331_14_173.png) 和 ![](img/B18331_14_174.png)。这个计算留作练习，请注意，滤波器中的每个权重都影响输出地图中的每个像素，或者说，滤波器权重的任何变化都会影响所有输出像素。
- en: Thinking about backpropagation and RNNs
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思考反向传播和 RNN
- en: Remember from *Chapter 5*, *Recurrent Neural Networks*, the basic equation for
    an RNN is ![](img/B18331_14_175.png), the final prediction is ![](img/B18331_14_176.png)
    at step *t*, the correct value is *y*[t], and the error *E* is the cross-entropy.
    Here *U*, *V*, and *W* are learning parameters used for the RNN’s equations. These
    equations can be visualized as shown in *Figure 14.16*, where we unroll the recurrency.
    The core idea is that total error is just the sum of the errors at each time step.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *第 5 章*，*递归神经网络*，我们记得一个 RNN 的基本方程是 ![](img/B18331_14_175.png)，第 *t* 步的最终预测是
    ![](img/B18331_14_176.png)，正确值为 *y*[t]，误差 *E* 是交叉熵。这里 *U*、*V* 和 *W* 是用于 RNN 方程的学习参数。这些方程可以像
    *图 14.16* 中展示的那样进行可视化，其中我们展开了递归。核心思想是总误差只是每个时间步的误差的总和。
- en: 'If we used SGD, we need to sum the errors and the gradients at each time step
    for one given training example:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用 SGD，需要对给定训练样本的每个时间步骤的误差和梯度进行求和：
- en: '![](img/B18331_14_16.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_16.png)'
- en: 'Figure 14.16: RNN unrolled with equations'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.16：展开的 RNN 方程式
- en: 'We are not going to write all the tedious math behind all the gradients but
    rather focus only on a few peculiar cases. For instance, with math computations
    similar to the ones made in the previous sections, it can be proven by using the
    chain rule that the gradient for *V* depends only on the value at the current
    time step *s*[3], *y*[3] and ![](img/B18331_14_177.png):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不打算详细写出所有梯度背后繁琐的数学计算，而是只专注于几个特殊情况。例如，通过与前几节相似的数学计算，可以使用链式法则证明 *V* 的梯度仅取决于当前时间步
    *s*[3]、*y*[3] 和 ![](img/B18331_14_177.png)：
- en: '![](img/B18331_14_178.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_178.png)'
- en: 'However, ![](img/B18331_14_179.png) has dependencies carried across time steps
    because, for instance, ![](img/B18331_14_180.png) depends on *s*[2], which depends
    on *W*[2] and *s*[1]. As a consequence, the gradient is a bit more complicated
    because we need to sum up the contributions of each time step:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，![](img/B18331_14_179.png) 的依赖关系跨越时间步骤，因为例如，![](img/B18331_14_180.png) 依赖于
    *s*[2]，而 *s*[2] 又依赖于 *W*[2] 和 *s*[1]。因此，梯度会稍微复杂一些，因为我们需要对每个时间步的贡献进行求和：
- en: '![](img/B18331_14_181.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_181.png)'
- en: To understand the preceding equation, imagine that we are using the standard
    backpropagation algorithm used for traditional feedforward neural networks but
    for RNNs. We need to additionally add the gradients of *W* across time steps.
    That’s because we can effectively make the dependencies across time explicit by
    unrolling the RNN. This is the reason why backprop for RNNs is frequently called
    **Backpropagation Through Time** (**BPTT**).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解上述方程，可以想象我们正在使用传统前馈神经网络用于 RNN 的标准反向传播算法，但需要额外添加跨时间步的 *W* 梯度。这是因为通过展开 RNN，我们可以有效地使时间上的依赖关系显式化。这也是为什么
    RNN 的反向传播经常被称为**递归神经网络的时序反向传播**（**BPTT**）。
- en: 'The intuition is shown in *Figure 14.17*, where the backpropagated signals
    are represented:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 直觉在 *图 14.17* 中展示，其中显示了反向传播信号：
- en: '![Diagram  Description automatically generated](img/B18331_14_17.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图示说明](img/B18331_14_17.png)'
- en: 'Figure 14.17: RNN equations and backpropagated signals'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.17：循环神经网络方程和反向传播信号
- en: 'I hope that you have been following up to this point because now the discussion
    will be slightly more difficult. If we consider:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 希望您迄今为止都有跟上，因为现在的讨论将会稍微困难一些。如果我们考虑：
- en: '![](img/B18331_14_182.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_182.png)'
- en: 'then we notice that ![](img/B18331_14_183.png) should be again computed with
    the chain rule, producing a number of multiplications. In this case, we take the
    derivative of a vector function with respect to a vector, so we need a matrix
    whose elements are all the pointwise derivatives (in math, this matrix is called
    a Jacobian). Mathematically, it can be proven that:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们注意到，![](img/B18331_14_183.png) 应该再次使用链式法则来计算，产生一系列的乘法。在这种情况下，我们需要计算一个向量函数关于另一个向量的导数，因此我们需要一个矩阵，其元素是所有逐点导数（在数学中，这个矩阵称为雅可比矩阵）。从数学上可以证明：
- en: '![](img/B18331_14_184.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_184.png)'
- en: 'Therefore, we have:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得出：
- en: '![](img/B18331_14_185.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_14_185.png)'
- en: The multiplication in the above equation is particularly problematic since both
    the sigmoid and tanh get saturated at both ends and their derivative goes to 0\.
    When this happens, they drive other gradients in previous layers toward 0\. This
    makes the gradient vanish completely after a few time steps and the network stops
    learning from “far away.”
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程中的乘法尤其具有问题，因为sigmoid和tanh在两端都会饱和，其导数会趋近于0。当这种情况发生时，它们会使得前面层的其他梯度趋近于0。这会导致梯度在几次迭代后完全消失，网络也就无法从“远距离”学习。
- en: '*Chapter 5*, *Recurrent Neural Networks*, discussed how to use **Long Short-Term
    Memory** (**LSTM**) and **Gated Recurrent Units** (**GRUs**) to deal with the
    problem of vanishing gradients and efficiently learn long-range dependencies.
    In a similar way, the gradient can explode when one single term in the multiplication
    of the Jacobian matrix becomes large. *Chapter 5* discussed how to use gradient
    clipping to deal with this problem.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '*第5章*，*循环神经网络*，讨论了如何使用**长短期记忆网络**（**LSTM**）和**门控循环单元**（**GRU**）来处理梯度消失问题，并高效地学习长期依赖关系。类似地，当雅可比矩阵中的某个单一项变得很大时，梯度也可能爆炸。*第5章*讨论了如何使用梯度裁剪来处理这个问题。'
- en: We have now concluded this journey, and you should now understand how backpropagation
    works and how it is applied in neural networks for dense networks, CNNs, and RNNs.
    In the next section, we will discuss how TensorFlow computes gradients, and why
    this is useful for backpropagation.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成了这次旅程，你应该已经理解了反向传播的工作原理，以及它如何在密集网络、卷积神经网络（CNN）和循环神经网络（RNN）中应用。在接下来的章节中，我们将讨论TensorFlow如何计算梯度，以及为什么这对反向传播很有用。
- en: A note on TensorFlow and automatic differentiation
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于TensorFlow和自动微分的说明
- en: TensorFlow can automatically calculate derivatives, a feature called automatic
    differentiation. This is achieved by using the chain rule. Every node in the computational
    graph has an attached gradient operation for calculating the derivatives of input
    with respect to output. After that, the gradients with respect to parameters are
    automatically computed during backpropagation.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow可以自动计算导数，这一特性被称为自动微分。通过使用链式法则来实现这一功能。计算图中的每个节点都有一个附加的梯度操作，用于计算输入与输出之间的导数。之后，参数的梯度会在反向传播过程中自动计算。
- en: Automatic differentiation is a very important feature because you do not need
    to hand-code new variations of backpropagation for each new model of a neural
    network. This allows for quick iteration and running many experiments faster.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 自动微分是一个非常重要的特性，因为你不需要为每个新的神经网络模型手动编写新的反向传播变种。这使得快速迭代和进行许多实验变得更加高效。
- en: Summary
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we discussed the math behind deep learning. Put simply, a
    deep learning model computes a function given an input vector to produce the output.
    The interesting part is that it can literally have billions of parameters (weights)
    to be tuned. Backpropagation is a core mathematical algorithm used by deep learning
    for efficiently training artificial neural networks, following a gradient descent
    approach that exploits the chain rule. The algorithm is based on two steps repeated
    alternatively: the forward step and the backstep.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了深度学习背后的数学原理。简单来说，深度学习模型根据输入向量计算一个函数，进而生成输出。令人感兴趣的是，它实际上可能拥有数十亿个需要调整的参数（权重）。反向传播是深度学习中用于高效训练人工神经网络的核心数学算法，它采用梯度下降法，并利用链式法则。该算法基于两个交替重复的步骤：前向步骤和后向步骤。
- en: During the forward step, inputs are propagated through the network to predict
    the outputs. These predictions might be different from the true values given to
    assess the quality of the network. In other words, there is an error and our goal
    is to minimize it. This is where the backstep plays a role, by adjusting the weights
    of the network to minimize the error. The error is computed via loss functions
    such as **Mean Squared Error** (**MSE**), or cross-entropy for non-continuous
    values such as Boolean (*Chapter 1*, *Neural Network Foundations with TF*). A
    gradient-descent-optimization algorithm is used to adjust the weight of neurons
    by calculating the gradient of the loss function. Backpropagation computes the
    gradient, and gradient descent uses the gradients for training the model. A reduction
    in the error rate of predictions increases accuracy, allowing machine learning
    models to improve. SGD is the simplest thing you could possibly do by taking one
    step in the direction of the gradient. This chapter does not cover the math behind
    other optimizers such as Adam and RMSProp (*Chapter 1*). However, they involve
    using the first and the second moments of the gradients. The first moment involves
    the exponentially decaying average of the previous gradients, and the second moment
    involves the exponentially decaying average of the previous squared gradients.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向步骤中，输入通过网络传播以预测输出。这些预测可能与评估网络质量所需的真实值不同。换句话说，存在误差，我们的目标是最小化它。此时，反向传播发挥了作用，通过调整网络的权重来最小化误差。误差通过损失函数进行计算，如**均方误差**（**MSE**）或用于非连续值（如布尔值）的交叉熵（*第1章*，*使用TF的神经网络基础*）。一种梯度下降优化算法用于通过计算损失函数的梯度来调整神经元的权重。反向传播计算梯度，梯度下降使用梯度来训练模型。通过减少预测的误差率，可以提高准确性，使机器学习模型不断改进。SGD是最简单的做法，通过在梯度方向上迈出一步来实现。此章节不涉及其他优化算法（如Adam和RMSProp，*第1章*）背后的数学原理。然而，它们涉及使用梯度的第一阶和第二阶矩。第一阶矩涉及先前梯度的指数衰减平均，第二阶矩则涉及先前平方梯度的指数衰减平均。
- en: 'There are three big properties of our data that justify using deep learning;
    otherwise, we might as well use regular machine learning:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个数据的主要特性证明了使用深度学习的必要性；否则，我们完全可以使用常规机器学习：
- en: Very-high-dimensional input (text, images, audio signals, videos, and temporal
    series are frequently a good example).
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高维输入（如文本、图像、音频信号、视频和时间序列，通常是一个很好的例子）。
- en: Dealing with complex decision surfaces that cannot be approximated with a low-order
    polynomial function.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理无法用低阶多项式函数近似的复杂决策面。
- en: Having a large amount of training data available.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有大量可用的训练数据。
- en: Deep learning models can be thought of as a computational graph made up of stacking
    together several basic components such as dense networks (*Chapter 1*), CNNs (*Chapter
    3*), embeddings (*Chapter 4*), RNNs (*Chapter 5*), GANs (*Chapter 9*), autoencoders
    (*Chapter 8*) and, sometimes, adopting shortcut connections such as “peephole,”
    “skip,” and “residual” because they help data flow a bit more smoothly. Each node
    in the graph takes tensors as input and produces tensors as output. As discussed,
    training happens by adjusting the weights in each node with backprop, where the
    key idea is to reduce the error in the final output node(s) via gradient descent.
    GPUs and TPUs (*Chapter 15*) can significantly accelerate the optimization process
    since it is essentially based on (hundreds of) millions of matrix computations.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型可以看作是由多个基本组件堆叠在一起的计算图，例如密集网络（*第1章*）、CNN（*第3章*）、嵌入层（*第4章*）、RNN（*第5章*）、GAN（*第9章*）、自编码器（*第8章*），有时还采用像“窥视孔”、“跳跃”和“残差”这样的捷径连接，因为它们有助于数据更平滑地流动。图中的每个节点接收张量作为输入，并生成张量作为输出。如前所述，训练通过调整每个节点中的权重来进行，使用反向传播，其中关键思想是通过梯度下降减少最终输出节点的误差。GPU和TPU（*第15章*）可以显著加速优化过程，因为它本质上依赖于（数百万）矩阵计算。
- en: There are a few other mathematical tools that might be helpful to improve your
    learning process. Regularization (L1, L2, and Lasso (*Chapter 1*)) can significantly
    improve learning by keeping weights normalized. Batch normalization (*Chapter
    1*) helps to basically keep track of the mean and the standard deviation of your
    dataset across multiple deep layers. The key idea is to have data resembling a
    normal distribution while it flows through the computational graph. Dropout (Chapters
    *1*, *3*, *5*, *6*, *9*, and *20*) helps by introducing some elements of redundancy
    in your computation; this prevents overfitting and allows better generalization.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他数学工具可能有助于提高你的学习过程。正则化（L1, L2 和 Lasso (*第1章*)）可以通过保持权重归一化显著改善学习。批量归一化 (*第1章*)
    基本上帮助追踪数据集在多个深度层次之间的均值和标准差。关键思想是使数据在通过计算图时呈现近似正态分布。丢弃法（*第1章*，*第3章*，*第5章*，*第6章*，*第9章*，和*第20章*）通过在计算中引入一些冗余元素来帮助防止过拟合，从而提高泛化能力。
- en: This chapter has presented the mathematical foundation behind intuition. As
    discussed, this topic is quite advanced and not necessarily required for practitioners.
    However, it is recommended reading if you are interested in understanding what
    is going on “under the hood” when you play with neural networks.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了直觉背后的数学基础。如前所述，这个话题相当高级，并不一定是从业者必需的。然而，如果你有兴趣了解在玩转神经网络时，“引擎盖下”究竟发生了什么，建议阅读。
- en: The next chapter will introduce the **Tensor Processing Unit** (**TPU**), a
    special chip developed at Google for ultra-fast execution of many mathematical
    operations described in this chapter.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将介绍**张量处理单元**（**TPU**），这是谷歌开发的一种特殊芯片，旨在超快执行本章描述的许多数学运算。
- en: References
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Kelley, Henry J. (1960). *Gradient theory of optimal flight paths*. ARS Journal.
    30 (10): 947–954\. Bibcode:1960ARSJ...30.1127B. doi:10.2514/8.5282.'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Kelley, Henry J. (1960). *最优飞行路径的梯度理论*. ARS Journal. 30 (10): 947–954\. Bibcode:1960ARSJ...30.1127B.
    doi:10.2514/8.5282.'
- en: 'Dreyfus, Stuart. (1962). *The numerical solution of variational problems*.
    Journal of Mathematical Analysis and Applications. 5 (1): 30–45\. doi:10.1016/0022-247x(62)90004-5.'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Dreyfus, Stuart. (1962). *变分问题的数值解法*. 《数学分析与应用杂志》. 5 (1): 30–45\. doi:10.1016/0022-247x(62)90004-5.'
- en: 'Werbos, P. (1974). *Beyond Regression: New Tools for Prediction and Analysis
    in the Behavioral Sciences*. PhD thesis, Harvard University.'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Werbos, P. (1974). *超越回归：行为科学中的新预测与分析工具*. 哈佛大学博士论文.
- en: 'Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J. (1986-10-09).
    *Learning representations by back-propagating errors*. Nature. 323 (6088): 533–536\.
    Bibcode:1986Natur.323..533R. doi:10.1038/323533a0.'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J. (1986-10-09).
    *通过反向传播误差学习表示*. 《自然》. 323 (6088): 533–536\. Bibcode:1986Natur.323..533R. doi:10.1038/323533a0.'
- en: LeCun, Y. (1987). *Modèles Connexionnistes de l’apprentissage (Connectionist
    Learning Models)*, Ph.D. thesis, Université P. et M. Curie.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LeCun, Y. (1987). *连接主义学习模型（Modèles Connexionnistes de l’apprentissage）*，博士论文，巴黎P.
    et M. Curie大学。
- en: Herbert Robbins and Sutton Monro. (1951). *A Stochastic Approximation Method.
    The Annals of Mathematical Statistics, Vol. 22, No. 3*. pp. 400–407.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Herbert Robbins 和 Sutton Monro. (1951). *一种随机逼近方法. 数学统计学年鉴, 第22卷, 第3期*. 第400–407页.
- en: 'Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey E. (June 2017). *ImageNet
    classification with deep convolutional neural networks* (PDF). Communications
    of the ACM. 60 (6): 84–90\. doi:10.1145/3065386\. ISSN 0001-0782\.'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey E. (2017年6月). *使用深度卷积神经网络进行ImageNet分类*（PDF）。《ACM通讯》。60
    (6): 84–90\. doi:10.1145/3065386\. ISSN 0001-0782\.'
- en: '*From not working to neural networking*. The Economist. (25 June 2016)'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*从无所作为到神经网络*. 《经济学人》. (2016年6月25日)'
- en: Join our book’s Discord space
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 频道
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人一起学习，并与超过2000名成员一起成长，网址：[https://packt.link/keras](https://packt.link/keras)
- en: '![](img/QR_Code1831217224278819687.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1831217224278819687.png)'
