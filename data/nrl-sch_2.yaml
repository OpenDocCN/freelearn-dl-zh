- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Introducing Foundations of Vector Representation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入向量表示基础
- en: '**Vectors** and **vector representation** are at the very core of neural search
    since the quality of vectors determines the quality of search results. In this
    chapter, you will learn about the concept of **vectors** within **machine learning**
    (**ML**). You will see common search algorithms using vector representation as
    well as their weaknesses and strengths.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**向量**和**向量表示**是神经搜索的核心，因为向量的质量决定了搜索结果的质量。在本章中，您将了解**机器学习**（**ML**）中**向量**的概念。您将看到常见的搜索算法，它们使用向量表示，以及它们的优缺点。'
- en: 'We’re going to cover the following main topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要主题：
- en: Introducing vectors in ML
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器学习中引入向量
- en: Measuring the similarity between two vectors
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 衡量两个向量之间的相似度
- en: Local and distributed representations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部与分布式表示
- en: By the end of this chapter, you will have a solid understanding of how every
    type of data can be represented in vectors and why this concept is at the very
    core of neural search.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，您将对如何将每种类型的数据表示为向量以及为什么这一概念是神经搜索核心有一个扎实的理解。
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter has the following technical requirements:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章有以下技术要求：
- en: A laptop with a minimum of 4 GB RAM (8 GB or more is preferred)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台最低配置为4GB RAM的笔记本电脑（推荐8GB或更多）
- en: Python installed with version 3.7, 3.8, or 3.9 on a Unix-like operating system,
    such as macOS or Ubuntu
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装有版本3.7、3.8或3.9的Python，并且运行在类似Unix的操作系统上，如macOS或Ubuntu
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter02](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter02).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在[https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter02](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter02)找到。
- en: Introducing vectors in ML
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在机器学习中引入向量
- en: Text is an important means of recording human knowledge. As of June 2021, the
    number of web pages indexed by mainstream search engines such as Google and Bing
    has reached 2.4 billion, and the majority of information is stored as text. How
    to store this textual information, and even how to efficiently retrieve the required
    information from the repository, has become a major issue in information retrieval.
    The first step in solving these problems lies in representing text in a format
    that is *comprehensible* to computers.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 文本是记录人类知识的重要手段。截止2021年6月，主流搜索引擎如Google和Bing索引的网页数量已达到24亿，并且大多数信息以文本形式存储。如何存储这些文本信息，甚至如何从存储库中高效地检索所需信息，已成为信息检索中的一个重要问题。解决这些问题的第一步在于以计算机*能够理解*的格式表示文本。
- en: As network-based information has become increasingly diverse, in addition to
    text, web pages contain a large amount of multimedia information, such as pictures,
    music, and video files. These files are more diverse than text in terms of form
    and content and satisfy users’ needs from different perspectives. How to represent
    and retrieve these types of information, as well as how to pinpoint the multimodal
    information needed by users from the vast mass of data available on the internet
    is also an important factor to be considered in the design of search engines.
    To achieve this, we need to represent each document as its vector representation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 随着基于网络的信息日益多样化，除了文本，网页还包含大量的多媒体信息，如图片、音乐和视频文件。这些文件在形式和内容上比文本更加多样，满足了用户从不同角度的需求。如何表示和检索这些类型的信息，以及如何从互联网上海量的数据中准确找到用户所需的多模态信息，也是搜索引擎设计中需要考虑的一个重要因素。为了实现这一点，我们需要将每个文档表示为其向量表示。
- en: 'A *vector* is an object that has both a magnitude and a direction, as you may
    remember learning in school. If we can represent our data using vector representation,
    then we’re able to use the angle to measure the similarity of two pieces of information.
    To be more concrete, we can say the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*向量*是具有大小和方向的对象，就像你在学校学到的那样。如果我们能够使用向量表示我们的数据，那么我们就能够通过角度来衡量两条信息的相似性。更具体地说，我们可以这样说：'
- en: Two pieces of information are represented as vectors
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两条信息被表示为向量
- en: Both vectors start from the origin [*0, 0*] (assuming two dimensions)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个向量都从原点[*0, 0*]开始（假设为二维）
- en: Two vectors form an angle
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个向量形成一个角度
- en: '*Figure 2.1* illustrates the relationship between two vectors with respect
    to their angle:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.1*说明了两个向量之间的关系及其角度：'
- en: '![Figure 2.1 – An example of vector representation ](img/Figure_2.1_B17488.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 – 向量表示示例](img/Figure_2.1_B17488.jpg)'
- en: Figure 2.1 – An example of vector representation
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 向量表示示例
- en: '**vec1** and **vec2** have the same direction but different lengths. **vec2**
    and **vec3** have the same lengths but point in opposite directions. If the angle
    is 0 degrees, the two vectors are identical. If the vector is 180 degrees, the
    two vectors are completely opposite. We can measure the similarity between two
    vectors by the angle: the smaller the angle, the closer the vectors are. This
    method is also called **cosine similarity**.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**vec1**和**vec2**有相同的方向，但长度不同。**vec2**和**vec3**长度相同，但方向相反。如果角度为0度，则两个向量完全相同；如果向量的角度为180度，则两个向量完全相反。我们可以通过角度来衡量两个向量的相似度：角度越小，两个向量越接近。这个方法也称为**余弦相似度**。'
- en: In reality, cosine similarity is one of the most commonly used similarity measurements
    to determine the similarity between two vectors, but not the only one. We’ll dive
    into it in more detail, as well as other similarity metrics, in the *Measuring
    the similarity between two vectors* section. Before that, you might be wondering
    how we can encode our raw information, such as text or audio, into a vector of
    numeric values. In this section, we’re going to do that.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，余弦相似度是最常用的相似度度量之一，用于确定两个向量的相似度，但并不是唯一的方法。我们将在*测量两个向量之间的相似度*章节中更详细地探讨它以及其他相似度度量。在此之前，你可能会想知道我们如何将原始信息（如文本或音频）编码成数值向量。在这一部分，我们将实现这一目标。
- en: We’ll dive into the details of cosine similarity using *Python* and the *NumPy
    library*. As well as that, we will introduce other similarity metrics and briefly
    cover local and distributed vector representation in the following subsections.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用*Python*和*NumPy库*深入探讨余弦相似度的细节。此外，我们还将介绍其他相似度度量，并在以下小节中简要讨论局部和分布式向量表示。
- en: Using vectors to represent data
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用向量表示数据
- en: 'Let’s start with the most common scenario: **representing text information**.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最常见的场景开始：**表示文本信息**。
- en: First of all, let’s define the concept of a **feature vector**. Let’s say we
    want to build a search system for Wikipedia (in English). As of July 2022, English
    Wikipedia has over 6.5 million articles containing over 4 billion words (180,000
    unique words). We can call these unique words the Vocabulary of Wikipedia.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来定义**特征向量**的概念。假设我们想为维基百科（英语版）构建一个搜索系统。截至2022年7月，英语维基百科有超过650万篇文章，包含超过40亿个单词（18万个唯一单词）。我们可以将这些唯一单词称为维基百科的词汇表。
- en: Each of the articles in this Wikipedia collection should be encoded into a series
    of numerical values; this is referred to as a feature vector. To this end, we
    can encode 6.5 million articles into 6.5 million indexed feature vectors, then
    use a similarity metric, such as cosine similarity, to measure the similarity
    between the encoded query feature vector and the indexed 6.5 million feature vectors.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇维基百科文集中的每篇文章都应该被编码为一系列数值，这被称为特征向量。为此，我们可以将650万篇文章编码为650万个索引的特征向量，然后使用相似度度量（如余弦相似度）来衡量编码后的查询特征向量与索引的650万个特征向量之间的相似度。
- en: The encoding process involves finding an optimal function to transform the original
    data into its vector representation. How can we achieve this goal?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 编码过程涉及找到一个最优函数，将原始数据转换为其向量表示。那么我们如何实现这一目标呢？
- en: 'Again, we start with the simplest method: using a **bit vector**. A bit vector
    means all the values inside the vector will be either 0 or 1, depending on the
    occurrence of the word. Let’s say we loop over all unique words in the Vocabulary;
    if the word occurs in this particular document, *d*, then we set the value of
    the location of this unique word to be 1, otherwise 0.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从最简单的方法开始：使用**位向量**。位向量意味着向量中的所有值将是0或1，具体取决于单词的出现情况。假设我们遍历词汇表中的所有唯一单词；如果该单词出现在特定文档*d*中，那么我们将该唯一单词位置的值设置为1，否则为0。
- en: 'Let’s refresh what we introduced in [*Chapter 1*](B17488_01.xhtml#_idTextAnchor014),
    *Neural Networks for Neural Search*, in the *How does the traditional search system
    work?* section, imagining we have two documents:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下在[*第1章*](B17488_01.xhtml#_idTextAnchor014)，*神经网络与神经搜索*一节中介绍的内容，假设我们有两个文档：
- en: '`doc1` = *Jina is a neural search framework*'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc1` = *Jina 是一个神经搜索框架*'
- en: '`doc2` = *Jina is built with cutting edge technology called deep learning*'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc2` = *Jina是用名为深度学习的前沿技术构建的*'
- en: 'If we merge these two documents, we have a Vocabulary (of unique words), as
    follows:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们将这两个文档合并，我们将得到如下的词汇表（唯一单词）：
- en: '[PRE0]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Imagining the preceding variable, `vocab`, is our Vocabulary, after preprocessing
    (tokenizing and stemming), we get a list of tokens, as follows:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设前述变量`vocab`是我们的词汇表，在经过预处理（分词和词干提取）后，我们得到如下的令牌列表：
- en: '[PRE1]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that the aforementioned Vocabulary has been sorted alphabetically.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述词汇表已经按字母顺序排序。
- en: 'To encode `doc1` into a vector representation, we loop through all the words
    inside the `doc1`, and create the bit vector:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要将`doc1`编码成向量表示，我们遍历`doc1`中的所有单词，并创建位向量：
- en: '[PRE2]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding code block encodes `doc1` into a bit vector. In the `encode`
    function, we firstly created a Python list filled with 0s; the length of the list
    is identical to the size of Vocabulary. Then, we loop over the Vocabulary to check
    the occurrence of the word inside the document to encode. If present, we set the
    value of the encoded vector as `1`. In the end, we get this:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码块将`doc1`编码为位向量。在`encode`函数中，我们首先创建了一个充满0的Python列表；该列表的长度与词汇表的大小相同。然后，我们遍历词汇表，检查文档中单词的出现情况。如果单词存在，我们就将编码向量的值设置为`1`。最后，我们得到如下结果：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this way, we’ve successfully encoded a document into its bit vector representation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们成功地将文档编码成了其位向量表示。
- en: Important Note
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You might have noticed that in the preceding example, the output of the bit
    vector contains a lot of 0s values. In a real-world scenario, as the size of the
    Vocabulary gets much larger, and the dimensionality of the vector gets very high,
    there is a high chance that most of the dimensions in the encoded documents are
    filled with 0s, which is extremely inefficient to store and retrieve. This is
    also called a **sparse vector**. Some Python libraries, such as SciPy, have strong
    sparse vector support. Some deep learning libraries, such as TensorFlow and PyTorch,
    have built-in sparse tensor support. Meanwhile, Jina primitive data types support
    SciPy, TensorFlow, and PyTorch sparse representations.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到，在前面的示例中，位向量的输出包含大量的0值。在实际应用中，随着词汇表的大小不断增大，向量的维度变得非常高，编码文档中的大多数维度将被填充为0，这对于存储和检索非常低效。这也叫做**稀疏向量**。一些Python库，例如SciPy，具有强大的稀疏向量支持。一些深度学习库，如TensorFlow和PyTorch，内置了稀疏张量支持。同时，Jina原始数据类型支持SciPy、TensorFlow和PyTorch的稀疏表示。
- en: So far, we have learned that a vector is an object that has both a magnitude
    and a direction. We also managed to create the simplest form of vector representation
    of two text documents using a bit vector. Now, it would be very interesting to
    know how similar these two documents are. Let us learn more about this in the
    next section.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学到，向量是一个既有大小又有方向的对象。我们还成功地使用位向量创建了两个文本文档的最简单形式的向量表示。现在，了解这两个文档的相似度将会非常有趣。我们将在下一节中深入学习这一点。
- en: Measuring similarity between two vectors
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 衡量两个向量之间的相似性
- en: Measuring similarity between two vectors is important in a neural search system.
    Once all of the documents have been indexed into their vector representation,
    given a user query, we carry out the same encoding process to the query. In the
    end, we compare the encoded query vector against all the encoded document vectors
    to find out what the most similar documents are.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量两个向量之间的相似性在神经搜索系统中至关重要。一旦所有文档都被索引为向量表示，给定用户查询时，我们对查询执行相同的编码过程。最后，我们将编码后的查询向量与所有编码后的文档向量进行比较，以找出最相似的文档。
- en: 'We can continue our example from the previous section, trying to measure the
    similarity between `doc1` and `doc2`. First of all, we need to run the script
    two times to encode both `doc1` and `doc2`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以继续上一节中的示例，尝试衡量`doc1`和`doc2`之间的相似性。首先，我们需要运行脚本两次，分别编码`doc1`和`doc2`：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we can produce a vector representation for both of them:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以为它们生成向量表示：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Since the dimension of the encoded result is always identical to the size of
    Vocabulary, the problem has been converted to how to measure the similarity between
    two vector representations: `encoded_doc1` and `encoded_doc2`.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于编码结果的维度始终与词汇表的大小相同，因此问题已转化为如何衡量两个向量表示之间的相似度：`encoded_doc1`和`encoded_doc2`。
- en: Important Note
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The aforementioned vector representation of `encoded_doc1` and `encoded_doc2`
    has a depth of 15\. It is easy for us to visualize 1D data as a point, 2D data
    as a line, or 3D data, but not for high-dimensional data. Practically, we might
    perform dimensionality reduction to reduce high-dimensional vectors to 3D or 2D
    in order to plot them. The most common technique is called **t-sne**.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 上述的`encoded_doc1`和`encoded_doc2`的向量表示具有15维。我们很容易将1D数据可视化为一个点，2D数据为一条线，3D数据也能可视化，但对于高维数据就不容易了。实际上，我们可能会进行降维，将高维向量降至3D或2D以便绘制它们。最常见的技术叫做**t-sne**。
- en: 'Imaging two encoded vector representations can be plotted in a 2D vector space.
    We can visualize `encoded_doc1` and `encoded_doc2` as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 想象两个编码后的向量表示可以绘制在二维向量空间中。我们可以如下方式可视化`encoded_doc1`和`encoded_doc2`：
- en: '![Figure 2.2 – Cosine similarity ](img/Figure_2.2_B17488.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – 余弦相似度](img/Figure_2.2_B17488.jpg)'
- en: Figure 2.2 – Cosine similarity
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – 余弦相似度
- en: 'Then, we can measure the similarity between `encoded_doc1` and `encoded_doc2`
    using their angles, specifically, the cosine similarity. The law of cosine tells
    us that:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过它们的角度来衡量`encoded_doc1`和`encoded_doc2`之间的相似度，具体来说，就是余弦相似度。余弦定理告诉我们：
- en: '![](img/Formula_2.1_B17488.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_2.1_B17488.jpg)'
- en: 'Let’s say *p* is represented as [x1, y1] and *q* is represented as [x2, y2];
    then, the aforementioned formula can be rewritten as:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 假设*p*表示为[x1, y1]，*q*表示为[x2, y2]；那么，前述公式可以改写为：
- en: '![](img/Formula_2.2_B17488.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_2.2_B17488.jpg)'
- en: 'Since cosine similarity also works for high-dimensional data, the aforementioned
    formula can be again rewritten as:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于余弦相似度也适用于高维数据，上述公式可以再次改写为：
- en: '![](img/Formula_2.3_B17488.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_2.3_B17488.jpg)'
- en: 'Based on the formula, we can compute the cosine similarity between `encoded_doc1`
    and `encoded_doc2`, as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基于公式，我们可以计算`encoded_doc1`和`encoded_doc2`之间的余弦相似度，如下所示：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If we print out the result of the similarity between `encoded_doc1` and `encoded_doc2`,
    we get the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打印出`encoded_doc1`和`encoded_doc2`之间的相似度结果，得到如下：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, we get the cosine similarity between two encoded vectors, roughly equal
    to *0.405*. In a search system, when the user submits a query, we will encode
    the query into its vector representation. We have encoded all the documents (that
    we want to search) into their vector representations individually offline. In
    this way, we can compute the similarity score of the query vector against all
    document vectors to produce the final ranking list.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们得到了两个编码向量之间的余弦相似度，约等于*0.405*。在搜索系统中，当用户提交查询时，我们将查询编码为其向量表示。我们已经将所有文档（我们想要搜索的文档）各自离线编码为向量表示。通过这种方式，我们可以计算查询向量与所有文档向量之间的相似度得分，进而生成最终的排名列表。
- en: Important Note
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The preceding code illustrates how you can compute the cosine similarity. The
    code is not optimized. In reality, you should always use NumPy to perform vectorized
    operations over vectors (NumPy arrays) to achieve higher performance.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码演示了如何计算余弦相似度。该代码没有经过优化。实际上，你应该始终使用NumPy对向量（NumPy数组）执行向量化操作，以获得更高的性能。
- en: Metrics beyond cosine similarity
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越余弦相似度的度量
- en: Through cosine similarity is the most commonly used similarity/distance metric,
    there are some other commonly used metrics as well. We will cover another two
    commonly used distance functions in this section, namely, **Euclidean distance**
    and **Manhattan distance**.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管余弦相似度是最常用的相似度/距离度量，但也有一些其他常用的度量方法。我们将在本节中介绍另外两种常用的距离函数，即**欧几里得距离**和**曼哈顿距离**。
- en: Important Note
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Similarity metrics measure how alike two documents are. On the other hand, distance
    metrics measure the dissimilarity between two documents. In the search scenario,
    you always want to get the top k matches against your query. So, if you are using
    similarity metrics, always get the first k items from the ranked list. On the
    other hand, while using distance metrics, always get the last k items from the
    ranked list or reverse the ranked list and get the first k items.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 相似度度量衡量两个文档之间的相似程度。另一方面，距离度量衡量两个文档之间的差异。在搜索场景中，你总是希望获得与查询最匹配的前k个结果。因此，如果你使用的是相似度度量，始终从排名列表中获取前k项。另一方面，在使用距离度量时，始终从排名列表中获取最后k项，或者反转排名列表并获取前k项。
- en: 'Unlike cosine similarity, which takes the angle of two vectors as its similarity
    measure, the Euclidean distance takes the length of the line segment between two
    data points. For instance, consider two 2D docs in the following figure:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 与计算余弦相似度（通过测量两个向量之间的角度）不同，欧几里得距离通过计算两个数据点之间的线段长度来衡量相似度。例如，考虑下图中的两个二维文档：
- en: '![Figure 2.3 – Euclidean distance ](img/Figure_2.3_B17488.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – 欧几里得距离](img/Figure_2.3_B17488.jpg)'
- en: Figure 2.3 – Euclidean distance
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 欧几里得距离
- en: 'As you can see in *Figure 2.3*, previously, we used the angle between `vec1`
    and `vec2` to compute their cosine similarity. For Euclidean distance, we compute
    it in a different way. Both `vec1` and `vec2` have a starting point of 0 and the
    `p` and `q` endpoints, respectively. Now, the distance between these two vectors
    becomes:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如在*图 2.3*中所示，之前我们使用了`vec1`和`vec2`之间的角度来计算它们的余弦相似度。而对于欧几里得距离，我们的计算方式有所不同。`vec1`和`vec2`的起点都是0，终点分别为`p`和`q`。现在，这两个向量之间的距离为：
- en: '![](img/Formula_2.4_B17488.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_2.4_B17488.jpg)'
- en: 'Another distance metric is called `p` at (p1, p2) and `q` at (q1, q2), the
    distance between these two vectors becomes:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个距离度量叫做`p`（位于(p1, p2)）和`q`（位于(q1, q2)），这两个向量之间的距离为：
- en: '![](img/Formula_2.5_B17488.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_2.5_B17488.jpg)'
- en: 'As can be seen in *Figure 2.4*, the hyperplane has been split into small blocks.
    Each block has a width of 1 and a height of 1\. The distance between `p` and `q`
    becomes 4:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如在*图 2.4*中所示，超平面已被分割成小块。每个块的宽度为1，高度也为1。`p`和`q`之间的距离变为4：
- en: '![Figure 2.4 – Manhattan distance ](img/Figure_2.4_B17488.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.4 – 曼哈顿距离](img/Figure_2.4_B17488.jpg)'
- en: Figure 2.4 – Manhattan distance
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 – 曼哈顿距离
- en: 'There are many other distance metrics as well, such as the **Hamming distance**
    and **angular distance**, but we won’t cover each of them given the fact that
    cosine and Euclidean are the most commonly used similarity metrics. This, in turn,
    leads to an interesting question: which distance/similarity metric should I use
    to make vector similarity computation more effective? The answer is *it depends*.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他的距离度量，比如**汉明距离**和**角度距离**，但我们不会在这里深入探讨它们，因为余弦和欧几里得距离是最常用的相似度度量。这也引出了一个有趣的问题：我应该使用哪种距离/相似度度量来使向量相似度计算更有效？答案是*视情况而定*。
- en: First of all, it depends on your task and your data. But, in general, when performing
    text retrieval and related tasks, cosine similarity will be your first choice.
    It has been widely adopted for applications such as measuring similarity between
    two pieces of encoded text documents.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，这取决于你的任务和数据。但一般来说，在执行文本检索及相关任务时，余弦相似度将是你的首选。它已经广泛应用于诸如衡量两篇编码文本文档之间的相似度等任务。
- en: 'The deep learning model might also impact your similarity/distance metric choice.
    For instance, if you applied metric learning techniques to fine-tune your ML model
    to optimize certain similarity metrics, then you might stick with the same similarity
    metric that you optimized. To be more specific, note the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型也可能会影响你选择的相似度/距离度量。例如，如果你应用了度量学习技术来微调你的机器学习模型，以优化特定的相似度度量，那么你可能会坚持使用你优化过的相似度度量。更具体地说，注意以下几点：
- en: You can apply *Siamese neural networks* to optimize pairs of inputs (query and
    document) based on the Euclidean distance and get a new model
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用*孪生神经网络*，基于欧几里得距离优化输入对（查询和文档），从而得到一个新的模型。
- en: When extracting features with the model, it’s better to use the *Euclidean distance*
    as the similarity measure
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提取特征时，最好使用*欧几里得距离*作为相似度度量。
- en: If your vectors have extremely high dimensions, it might be a good idea to switch
    from the Euclidean distance to the *Manhattan distance* since it delivers more
    robust results
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的向量具有极高的维度，可能需要考虑将欧几里得距离转换为*曼哈顿距离*，因为它能够提供更强的鲁棒性。
- en: Important Note
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In application, different ANN libraries might use different distance metrics
    as default configuration. For instance, Annoy encourages users to use the angular
    distance to compute vector distances. It is a variation of the Euclidean distance.
    More about ANN will be introduced in [*Chapter 3*](B17488_03.xhtml#_idTextAnchor044),
    *System Design and Engineering Challenges*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，不同的人工神经网络（ANN）库可能会使用不同的距离度量作为默认配置。例如，Annoy库鼓励用户使用角度距离来计算向量距离，这是一种欧几里得距离的变种。关于ANN的更多内容将在[*第
    3 章*](B17488_03.xhtml#_idTextAnchor044)《系统设计与工程挑战》中介绍。
- en: 'There are multiple ways to encode data into vector representations. Generally
    speaking, this can be classified into two forms: **local representation** and
    **distributed representation**. The aforementioned way of encoding data into vector
    representation can be classified into local representation since it treats each
    unique word as one dimension.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据编码为向量表示的方法有多种。一般来说，这可以分为两种形式：**局部表示**和**分布式表示**。上述数据编码为向量表示的方法可以归类为局部表示，因为它将每个唯一单词视为一个维度。
- en: In the next section, we’ll introduce the most important local representation
    and distributed representation algorithms.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍最重要的局部表示和分布式表示算法。
- en: Local and distributed representations
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 局部和分布式表示
- en: In this section, we’ll dive into **local representations** and **distributed
    representations**. We will go through the characteristics of two different representations
    and list the most widely used local and global representations to encode different
    modalities of data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨 **局部表示** 和 **分布式表示**。我们将介绍这两种不同表示的特点，并列出用于编码不同数据模态的最广泛使用的局部和全局表示。
- en: Local vector representation
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局部向量表示
- en: As a classic method of text representation, **local representation** only makes
    use of the **disjointed dimensions** in the vector for a certain word when it
    is represented as a vector. Disjointed dimension means that each dimensionality
    of the vector represents a single token.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 作为经典的文本表示方法，**局部表示**仅利用向量中表示某个单词的 **不相交维度**。不相交维度意味着向量的每个维度代表一个单一的标记。
- en: When only one dimension is used, it is called **one-hot representation**. *One-hot*
    means that the word is represented as a long vector, and the dimension of the
    vector is the total number of words to be represented. Most dimensions are 0,
    while only one dimension has a value of 1\. Different words with a dimension of
    1 are not used. If this method of representation is stored sparsely, that is,
    assigning a digital ID to each word based on the dimension of 1, it will be concise.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当只使用一个维度时，这被称为 **one-hot 表示**。*One-hot* 意味着单词被表示为一个长向量，向量的维度是要表示的单词总数。大多数维度为
    0，而只有一个维度的值为 1。不同的单词具有维度为 1 的值并不会重复。如果这种表示方式以稀疏的方式存储，即根据维度为 1 为每个单词分配一个数字 ID，那么它将更加简洁。
- en: One-hot also means that no additional learning process is required under the
    assumption that all words are independent of each other. This maintains the orthogonality
    between vectors representing words and therefore has a strong discriminative ability.
    With maximum entropy, a support vector machine, conditional random field, and
    other ML algorithms, the one-hot representation has great effects on multiple
    aspects, such as text classification, text clustering, and part-of-speech tagging.
    For an application scenario of ad hoc retrieval where keyword matching plays a
    leading role, the bag-of-words model based on one-hot representation is still
    the mainstream choice.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: One-hot 还意味着在假设所有单词彼此独立的情况下，不需要额外的学习过程。这保持了表示单词的向量之间的正交性，因此具有很强的区分能力。通过最大熵、支持向量机、条件随机场和其他机器学习算法，one-hot
    表示在文本分类、文本聚类和词性标注等多个方面具有显著效果。对于关键字匹配主导的特定检索应用场景，基于 one-hot 表示的词袋模型仍然是主流选择。
- en: However, one-hot representation ignores the semantic relationships between words.
    In addition, when representing a Vocabulary, **V**, that contains **N** words,
    the one-hot representation needs to construct a vector of dimension **N**. This
    leads to the problems of parameter explosion and data sparseness.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，one-hot 表示忽略了单词之间的语义关系。此外，当表示一个包含 **N** 个单词的词汇表 **V** 时，one-hot 表示需要构建一个维度为
    **N** 的向量。这导致了参数爆炸和数据稀疏性的问题。
- en: Another type of local representation is referred to as **bag-of-words**, or
    *bit vector representation*, which we introduced earlier in the chapter.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种局部表示方法被称为 **词袋模型**，或 *比特向量表示*，我们在本章前面已经介绍过。
- en: 'As a method of vector representation, the bag-of-words model regards the text
    as a collection of words, only documenting whether the words appear in the text
    or not but ignoring the word order and grammar in a body of text. Based on the
    one-hot representation of words, bag-of-words represents the text as a vector
    composed of 0s and 1s, and offers great support for bit operations. This method
    can conduct regular query processing in retrieval scenarios. Because it also maintains
    the orthogonality between words, it still works well for tasks such as text classification.
    Now, we will build a bit vector representation using a *Python ML framework* called
    **scikit-learn**:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种向量表示方法，词袋模型将文本视为单词集合，仅记录单词是否出现在文本中，而忽略文本中的单词顺序和语法。基于单词的 one-hot 表示，词袋模型将文本表示为由
    0 和 1 组成的向量，并为位操作提供了很好的支持。该方法可以在检索场景中进行常规查询处理。由于它仍然保持了单词之间的正交性，因此在文本分类等任务中表现良好。现在，我们将使用一种名为
    **scikit-learn** 的 *Python 机器学习框架* 构建一个位向量表示：
- en: '[PRE8]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output looks like this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Based on the bag-of-words (bit vector) model, the bag-of-words representation
    algorithm takes into account the frequency of words appearing in a body of text.
    Therefore, the bag-of-words encoded feature values corresponding to different
    words are no longer 0 or 1, but the frequency of such words appears in the body
    of text. Generally speaking, the more frequently a word appears in the text, the
    more important the word is to the text. To get the representation, you can simply
    put `binary=False` in the preceding implementation:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 基于词袋（位向量）模型，词袋表示算法考虑了词语在文本中的出现频率。因此，不同单词对应的词袋编码特征值不再是 0 或 1，而是该单词在文本中出现的频率。一般来说，单词在文本中出现得越频繁，它对文本的贡献就越重要。为了获得表示，你只需要在前面的实现中将
    `binary=False` 设置即可：
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can discover from the following output, the term frequency has been taken
    into consideration. For example, since the `neural` token occurred two times,
    the value of the encoded result has increased by `1:`
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下输出中，你可以发现已考虑了词频。例如，由于 `neural` 这个词出现了两次，编码结果的值增加了 `1`：
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Last but not least, we have one of the most-used local representations, called
    **term frequency-inverse document frequency** (**tf-idf**) **representation**.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，我们有一个最常用的局部表示方法，称为**词频-逆文档频率**（**tf-idf**）**表示法**。
- en: 'tf-idf is a common representation method for information retrieval and data
    mining. The TF-IDF value of word *i* in text j is as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: tf-idf 是一种常见的信息检索和数据挖掘表示方法。词 *i* 在文本 j 中的 TF-IDF 值如下所示：
- en: '![](img/Formula_2.6_B17488.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_2.6_B17488.jpg)'
- en: 'Here, *ni, j* denotes the frequency of word *i* appearing in the text *j*;
    *|d_j |* denotes the total number of words in the text; *|D|* indicates the number
    of tokens in the corpus, and ![](img/Formula_2.7_B17488.png) represents the number
    of documents containing the word *i*. By factoring in the frequency of words appearing
    in the text, the TF-IDF algorithm further considers the universal importance of
    the word in the entire body of text by calculating the IDF of the word. That is,
    the more frequently a word appears in the text, the less frequently it appears
    in other parts of the body of text. This shows that the more important the word
    is for the current text, the higher weight it will be given. The scikit-learn
    implementation of this algorithm is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*ni, j* 表示词 *i* 在文本 *j* 中出现的频率；*|d_j |* 表示文本中词的总数；*|D|* 表示语料库中的词汇数量，而 ![](img/Formula_2.7_B17488.png)
    表示包含词 *i* 的文档数量。通过考虑词语在文本中的出现频率，TF-IDF 算法通过计算词的 IDF 来进一步考虑词在整个文本中的普遍重要性。也就是说，词语在文本中出现得越频繁，它在其他部分的文本中出现的频率就越低。这表明，词语对当前文本的重要性越大，它的权重就越高。此算法的
    scikit-learn 实现如下：
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The Tf-Idf weighted encoding result looks like this:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Tf-Idf 加权编码结果如下所示：
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Up until now, we have introduced local vector representation. In the next section,
    we will dive deep into a distributed vector representation, why we need it, and
    the commonly used algorithms.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了局部向量表示。接下来的章节，我们将深入探讨分布式向量表示、它为何需要以及常用的算法。
- en: Distributed vector representation
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式向量表示
- en: Although the local representation of texts has advantages in tasks such as text
    classification and data recall, it has the problem of data sparseness.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管文本的局部表示在文本分类和数据召回等任务中具有优势，但它存在数据稀疏性的问题。
- en: To be more specific, if a corpus has 100,000 distinct tokens, the dimensionality
    of the vector will become 100,000\. Suppose we have a document that contains 200
    tokens. In order to represent this document, only 200 entries of the vector out
    of 100,000 are non-zero. All other dimensions still get a 0 value since the tokens
    of the Vocabulary did not occur in the document.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，如果语料库有 100,000 个不同的标记，向量的维度将变为 100,000。假设我们有一个包含 200 个标记的文档。为了表示这个文档，向量中只有
    200 个条目不为零。由于词汇表的标记没有出现在文档中，所有其他维度仍然得到零值。
- en: This has posed great challenges to data storage and retrieval. Accordingly,
    a natural idea is to obtain a low-dimensional dense vector of the text, which
    is called a **distributed representation** of the text.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这给数据存储和检索带来了巨大挑战。因此，一个自然的想法是获取文本的低维稠密向量，称为文本的 **分布式表示**。
- en: In this section, the distributed representation of single modalities, such as
    text, images, and audio, is first described; then, the distributed representation
    method of multimodal joint learning is presented. We’ll also selectively introduce
    several important representation learning algorithms based on the modality of
    the data, that is, text, image, audio, and cross-modal representation learning.
    Let’s first look at text-based algorithms.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，首先描述了单模态（如文本、图像和音频）的分布式表示；然后，介绍了多模态联合学习的分布式表示方法。我们还会选择性地介绍几种重要的表示学习算法，基于数据模态，即文本、图像、音频和跨模态表示学习。让我们先看看基于文本的算法。
- en: 'In the following table, we have listed some selected models to encode different
    modalities of data:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在下表中，我们列出了用于编码不同数据模态的一些选定模型：
- en: '| **Model** | **Modality** | **Domain** | **Application** |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **模态** | **领域** | **应用** |'
- en: '| `BERT` | Text | Dense retrieval | Text-to-text search, question answering
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| `BERT` | 文本 | 密集检索 | 文本到文本搜索，问答 |'
- en: '| `VGGNet` | Image | Content-based image retrieval | Image-to-image search
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| `VGGNet` | 图像 | 基于内容的图像检索 | 图像到图像搜索 |'
- en: '| `ResNet` | Image | Content-based image retrieval | Image-to-image search
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| `ResNet` | 图像 | 基于内容的图像检索 | 图像到图像搜索 |'
- en: '| `Wave2Vec` | Acoustic | Content-based audio retrieval | Audio-to-audio search
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| `Wave2Vec` | 声音 | 基于内容的音频检索 | 音频到音频搜索 |'
- en: '| `CLIP` | Text and image | Cross-modal retrieval | Text-to-image search |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| `CLIP` | 文本和图像 | 跨模态检索 | 文本到图像搜索 |'
- en: Table 1.1 – Selected models that can be served as encoders for different modality
    of inputs
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.1 – 可用作不同输入模态编码器的选定模型
- en: Text-based algorithms
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于文本的算法
- en: Because text carries important information, the distributed representation of
    texts serves as a major function of search engines and has been extensively studied
    in academic works and the industry. Given the fact that we have a huge amount
    of unlabeled text data (such as Wikipedia), when it comes to text-based algorithms,
    we normally employ unsupervised pretraining on a large corpus.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因为文本携带着重要信息，文本的分布式表示在搜索引擎中起着重要作用，并且在学术界和工业界得到广泛研究。考虑到我们拥有大量未标记的文本数据（如维基百科），在涉及基于文本的算法时，我们通常对大语料库进行无监督预训练。
- en: 'Based on the belief that similar words have a similar context, Mikolov et al.
    proposed the *word2vec* algorithm, which includes two simple neural network models
    for learning: the **Continuous-Bag-of-Words** (**CBOW**) and **skip-gram** (**SG**)
    models.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 基于类似单词具有类似上下文的信念，Mikolov 等人提出了 *word2vec* 算法，其中包括两个简单的神经网络模型用于学习：**Continuous-Bag-of-Words**
    (**CBOW**) 和 **skip-gram** (**SG**) 模型。
- en: 'Specifically, the CBOW model is used to derive the representation of a word,
    ![](img/WT.png), using its surrounding words, such as two words before and two
    words after. For example, given a sentence in a Wikipedia document, we randomly
    mask out one token inside this sentence. We try to predict the masked token by
    its surrounding tokens:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，CBOW 模型用于推导单词的表示，![](img/WT.png)，使用其周围的词，例如单词前两个和后两个。例如，给定维基百科文档中的一个句子，我们随机屏蔽该句子内的一个标记。我们尝试通过其周围的标记预测屏蔽的标记：
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the preceding document, we masked the token search and tried to predict
    the vector representation of the masked token, *u*, by summing up the representation
    of surrounding tokens, ![](img/Formula_2.8_B17488.png), and conducting the dot
    product between *u* and ![](img/Formula_2.8_B174881.png). At training time, we’ll
    select a token, *y*, to maximize the dot product:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述文档中，我们屏蔽了标记搜索，并试图预测被屏蔽的标记*u*的向量表示，通过周围标记的表示之和，![](img/Formula_2.8_B17488.png)，并计算*u*与![](img/Formula_2.8_B174881.png)之间的点积。在训练时，我们将选择一个标记*y*，以最大化点积：
- en: '![](img/Formula_2.10_B17488.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_2.10_B17488.jpg)'
- en: Important Note
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: It should be noted that before training, we will randomly initialize the vector
    values.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，在训练之前，我们将随机初始化向量值。
- en: 'On the other hand, SG tries to predict the vector representations of the surrounding
    tokens from the current token. The difference between CBOW and SG is illustrated
    in *Figure 2.5*:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，SG试图从当前标记预测周围标记的向量表示。CBOW和SG之间的区别在*图2.5*中有所说明：
- en: '![Figure 2.5 – CBOW and SG (source: Efficient estimation of word representations
    in vector space) ](img/Figure_2.5_B17488.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图2.5 – CBOW和SG（来源：Efficient estimation of word representations in vector
    space）](img/Figure_2.5_B17488.jpg)'
- en: 'Figure 2.5 – CBOW and SG (source: Efficient estimation of word representations
    in vector space)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – CBOW和SG（来源：Efficient estimation of word representations in vector space）
- en: Both models are used to learn the word representation by maximizing the log-likelihood
    of the objective function on the entire corpus. To alleviate the burden of numerous
    calculations caused by the softmax function at the output layer, Mikolov et al.
    created two optimization methods, namely, **hierarchical softmax** and **negative
    sampling**. Conventional deep neural networks predict each next word as a classification
    task. This network must have many output classes as unique tokens. For example,
    when predicting the next word in the English Wikipedia, the number of classes
    is over 160,000\. This is extremely inefficient. Hierarchical softmax and negative
    sampling replace the flat softmax layer with a hierarchical layer that has the
    words as leaves and convert the multiclass classification problem into a binary
    classification problem by classifying whether two tokens are a true pair (semantically
    similar) or a false pair (independent tokens). This greatly improves the prediction
    speed of word embeddings.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型都用于通过最大化整个语料库上的目标函数的对数似然来学习词表示。为了减轻由输出层softmax函数造成的大量计算负担，Mikolov等人创建了两种优化方法，即**分层softmax**和**负采样**。传统的深度神经网络将每个下一个词预测为一个分类任务。这个网络必须有许多输出类作为唯一的标记。例如，在预测英文维基百科中的下一个词时，类的数量超过160,000。这是极其低效的。分层softmax和负采样用分层层替换了平坦的softmax层，该层以词作为叶子，并通过分类判断两个标记是否为真对（语义上相似）或假对（独立标记），将多类分类问题转换为二元分类问题。这大大提高了词嵌入的预测速度。
- en: After pretraining, we can give a token to this word2vec model and get a so-called
    word embedding. This word embedding is represented by a vector. Some pretrained
    `word2vec` vectors are represented as 300D word vectors. The dimensionality is
    much smaller than the sparse vector space model we introduced before. So, we also
    refer to these vectors as dense vectors.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练之后，我们可以给这个word2vec模型一个标记，得到所谓的词嵌入。这个词嵌入由一个向量表示。一些预训练的`word2vec`向量被表示为300维的词向量。维度远远小于我们之前介绍的稀疏向量空间模型。因此，我们也将这些向量称为密集向量。
- en: In algorithms such as *word2vec* and *GloVe*, the representation vector of a
    word generally remains unchanged after training and can be applied to downstream
    applications, such as named entity recognition.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在诸如*word2vec*和*GloVe*的算法中，一个词的表示向量在训练后通常保持不变，并且可以应用于下游应用，例如命名实体识别。
- en: However, the semantics of the same word in different contexts may vary or even
    have significantly different meanings. In 2019, Google announced **Bidirectional
    Encoder Representations from Transformers** (**BERT**), a transformer-based neural
    network for natural language processing. BERT uses a transformer network to represent
    the text and obtains the contextual information of the text through a masked language
    model. In addition, BERT also employs **next sentence prediction** (**NSP**) to
    strengthen the textual representation of relationships and has achieved good results
    for many textual representation tasks.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，相同单词在不同上下文中的语义可能会有所不同，甚至可能有显著不同的含义。2019年，谷歌宣布了**双向编码器表示的转换器**（**BERT**），这是一个基于转换器的神经网络，用于自然语言处理。BERT使用转换器网络表示文本，并通过掩蔽语言模型获取文本的上下文信息。此外，BERT还使用**下一个句子预测**（**NSP**）来增强文本关系的表示，并在许多文本表示任务中取得了良好的效果。
- en: Similar to word2vec, BERT has been pretrained on the Wikipedia dataset and some
    other datasets, such as BookCorpus. They form a Vocabulary of above 3 billion
    tokens. BERT has also been trained in different languages, such as English and
    German, as well as on multilingual datasets.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于word2vec，BERT已经在Wikipedia数据集和其他一些数据集（如BookCorpus）上进行了预训练。它们组成了一个超过30亿标记的词汇表。BERT还在不同的语言（如英语和德语）以及多语言数据集上进行了训练。
- en: 'BERT can be trained on a large amount of corpus without any annotations through
    the pretrain and fine-tune paradigm. During prediction, the text to be predicted
    is put in a well-trained network again to obtain a dynamic vector representation
    containing contextual information. During training, BERT replaces the words in
    the original text according to a certain ratio and uses the training model to
    make correct predictions. BERT will also add some special characters, such as
    `[CLS]` and `[SEP]`, to help the model correctly determine whether the two input
    sentences are continuous. Again, we have `doc1` and `doc2`, as follows; `doc2`
    is the next sentence of `doc1`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: BERT可以通过预训练和微调范式在大量语料库上进行训练，而不需要任何标注。在预测过程中，待预测的文本再次被输入到训练好的网络中，以获得包含上下文信息的动态向量表示。在训练过程中，BERT根据一定的比例替换原始文本中的词语，并利用训练模型做出正确的预测。BERT还会添加一些特殊字符，如`[CLS]`和`[SEP]`，以帮助模型正确判断两个输入句子是否是连续的。再一次，我们有`doc1`和`doc2`，如下所示；`doc2`是`doc1`的下一个句子：
- en: '`doc1` = *Jina is a neural search framework*'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc1` = *Jina是一个神经搜索框架*'
- en: '`doc2` = *Jina is built with cutting edge technology called deep learning*'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc2` = *Jina是基于深度学习的前沿技术构建的*'
- en: 'During pretraining, we consider two documents as two sentences, and represent
    documents as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练期间，我们将两个文档视为两个句子，并表示文档如下：
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After the text is input, BERT’s input consists of three types of vectors, that
    is, `[MASK]` token. According to the author of the BERT paper (*BERT: Pre-training
    of Deep Bidirectional Transformers for Language Understanding*), around 15% of
    tokens are masked out (Jacob et al.).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '在文本输入后，BERT的输入由三种类型的向量组成，即`[MASK]`标记。根据BERT论文的作者（*BERT: Pre-training of Deep
    Bidirectional Transformers for Language Understanding*）的描述，大约15%的标记会被掩盖（Jacob等人）。'
- en: '![Figure. 2.6 – BERT input representation. Each input embedding is the sum
    of three embeddings ](img/Figure_2.6_B17488.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.6 – BERT 输入表示。每个输入嵌入是三个嵌入的总和](img/Figure_2.6_B17488.jpg)'
- en: Figure. 2.6 – BERT input representation. Each input embedding is the sum of
    three embeddings
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 – BERT 输入表示。每个输入嵌入是三个嵌入的总和
- en: At the pretraining time, since we use NSP as the training objective, around
    50% of the second sentences are the “true” next sentence, while another 50% of
    the sentences are randomly selected from the corpus, which means they’re not the
    sentence that follows on from the first sentence. This helps us provide positive
    pairs and negative pairs to improve model pretraining. The objective function
    of BERT is to correctly predict the masked token as well as whether the next sentence
    is the correct one.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练阶段，由于我们使用NSP作为训练目标，第二个句子中大约50%是“真实”的下一个句子，而另外50%的句子是从语料库中随机选择的，这意味着它们不是紧接着第一个句子的句子。这帮助我们提供正负样本对以改善模型的预训练。BERT的目标函数是正确预测被掩盖的词汇以及判断下一个句子是否是正确的句子。
- en: As was mentioned before, after pretraining BERT, we can fine-tune the model
    for specific tasks. The author of the BERT paper fine-tuned a pretrained model
    on different downstream tasks, such as question answering and language understanding,
    and it achieved state-of-the-art performance on 11 downstream datasets.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，预训练BERT后，我们可以针对特定任务微调该模型。BERT论文的作者在不同的下游任务上微调了预训练模型，如问答和语言理解，并在11个下游数据集上取得了最先进的性能。
- en: Vision-based algorithms
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于视觉的算法
- en: With the rapid development of the internet, information carriers on the internet
    are increasingly diversified and images provide a variety of visual features.
    Many researchers expect to encode images as vectors for representation. The most
    widely used model architecture for imagery analysis is called **convolutional
    neural network** (**CNN**).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 随着互联网的快速发展，互联网中的信息载体日益多样化，图像提供了各种视觉特征。许多研究人员期望将图像编码为向量进行表示。最广泛使用的图像分析模型架构被称为**卷积神经网络**（**CNN**）。
- en: A CNN receives an image of shape (`Height`, `Width`, `Num_Channels`) as input
    (normally, it’s a three-channel RGB image or a one-channel grayscale image). The
    image will be passing through one of multiple convolutional layers. This takes
    a kernel (or filter) and slides through the input, and the image becomes an abstracted
    activation map.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一个CNN接收形状为(`Height`, `Width`, `Num_Channels`)的图像作为输入（通常是一个三通道的RGB图像或一个单通道的灰度图像）。图像将通过多个卷积层之一进行处理。这一过程使用一个内核（或滤波器）并在输入上滑动，图像变成一个抽象的激活图。
- en: After one of multiple convolutional operations, the output of the activation
    map will be sent through a pooling layer. The pooling layer takes a small cluster
    of neurons in the feature map and applies max or mean operations in this cluster.
    This is referred to as max pooling and mean pooling. The pooling layer can significantly
    reduce the dimensionality of the feature map into a more compact representation.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个卷积操作之一之后，激活图的输出将通过一个池化层。池化层对特征图中的一小群神经元应用最大值或均值操作，这被称为最大池化和均值池化。池化层可以显著降低特征图的维度，将其转换为更紧凑的表示。
- en: Normally, a combination of one of multiple convolutional layers and one pooling
    layer is named a convolutional block. For example, three convolutional layers
    plus one pooling layer make a convolutional block. At the end of the convolutional
    block, we normally apply a flatten operation to get the vector representation
    of the image data.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，多个卷积层和一个池化层的组合被称为卷积块。例如，三个卷积层加一个池化层构成一个卷积块。在卷积块的末尾，我们通常会应用展平操作，以获取图像数据的向量表示。
- en: 'In the following screenshot, we demonstrate a beautifully designed CNN model
    named VGG16\. As can be seen, it consists of five convolutional blocks, each one
    containing two or three convolutional layers and a max pooling layer. At the end
    of these blocks, the activation map is flattened as a feature vector:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的截图中，我们展示了一个精美设计的CNN模型，名为VGG16。如图所示，它由五个卷积块组成，每个卷积块包含两到三个卷积层和一个最大池化层。在这些块的末尾，激活图被展平为一个特征向量：
- en: '![Figure. 2.7 – VGG16 consists of five convolutional blocks and produces classification
    results with a softmax classification head ](img/Figure_2.7_B17488.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图2.7 – VGG16由五个卷积块组成，并通过softmax分类头生成分类结果](img/Figure_2.7_B17488.jpg)'
- en: Figure. 2.7 – VGG16 consists of five convolutional blocks and produces classification
    results with a softmax classification head
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 – VGG16由五个卷积块组成，并通过softmax分类头生成分类结果
- en: It is worth mentioning that VGG16 is designed for ImageNet classification. So,
    after the activation map is flattened as a feature vector, it is connected to
    two fully connected layers (dense layers) and a softmax classification head.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，VGG16是为ImageNet分类设计的。因此，在激活图展平为特征向量后，它会连接到两个全连接层（稠密层）和一个softmax分类头。
- en: In practice, we will remove the softmax classification head to turn this classification
    model into an embedding model. Given an input image, this embedding model produces
    a flattened feature map rather than the classified classes of the objects in the
    image. Besides, ResNet is a more complicated but frequently used vision feature
    extractor compared with VGGNet.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，我们将移除softmax分类头，将该分类模型转换为嵌入模型。给定一张输入图像，这个嵌入模型会输出一个展平的特征图，而不是图像中物体的分类结果。此外，与VGGNet相比，ResNet是一个更复杂但更常用的视觉特征提取器。
- en: Apart from text and images, audio search is an important search application,
    for instance, to identify music from a short clip or search for music with a similar
    style. In the next section, we will list several deep learning models in this
    direction.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 除了文本和图像，音频搜索也是一种重要的搜索应用，例如，用于识别短片中的音乐或搜索具有相似风格的音乐。在下一节中，我们将列出该方向上的几种深度学习模型。
- en: Acoustic-based algorithms
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于声学的算法
- en: Given a sequence of acoustic inputs, deep learning-powered algorithms have a
    huge impact on the acoustic domain. For instance, they have been widely used for
    text-to-speech tasks. Given a piece of music as query, finding similar (or the
    same) music is commonly used for music applications.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一系列声学输入，深度学习驱动的算法在声学领域产生了巨大影响。例如，它们已广泛应用于文本转语音任务。给定一段音乐作为查询，寻找相似（或相同）的音乐在音乐应用中是常见的需求。
- en: One of the latest state-of-the-art algorithms trained on audio data is called
    **wave2vec 2.0**. Similar to BERT, wave2vec is trained in an unsupervised fashion.
    Taking a piece of audio data, during pretraining, wave2vec masks out parts of
    the audio inputs and tries to learn what has been masked out.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一种最新的基于音频数据训练的最先进算法叫做**wave2vec 2.0**。与BERT类似，wave2vec是以无监督方式训练的。通过一段音频数据，在预训练过程中，wave2vec会遮掩音频输入的部分内容，并尝试学习这些被遮掩的部分。
- en: The major difference between wave2vec and BERT is that audio is a continuous
    signal with no clear segmentation into tokens. Wave2vec considers each 25 ms-long
    audio as a basic unit and feeds each 25 ms basic unit into a CNN model to learn
    a unit-level feature representation. Then, part of the input is masked out and
    fed into a BERT-like transformer model to predict the masked output. The training
    objective is to minimize the contrastive loss between the original audio and predicted
    audio.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: wave2vec与BERT之间的主要区别在于音频是一种连续信号，没有明确的标记切分为tokens。Wave2vec将每个25毫秒长的音频视为一个基本单元，并将每个25毫秒的基本单元输入到CNN模型中，以学习单元级的特征表示。然后，部分输入被遮掩，并输入到类似BERT的Transformer模型中，以预测被遮掩的输出。训练目标是最小化原始音频与预测音频之间的对比损失。
- en: 'It is worth mentioning that contrastive (self-supervised) pretraining is also
    widely used in the representation learning of text or images. For example, given
    an image as input, we can augment the image content a little bit to produce two
    views of the same image: even though these two views look different, we know they
    come from the same image.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，对比（自监督）预训练也广泛应用于文本或图像的表征学习。例如，给定一张图像作为输入，我们可以稍微增强图像内容，生成同一图像的两个视图：尽管这两个视图看起来不同，但我们知道它们来自同一张图像。
- en: 'This self-supervised contrastive learning has been widely used for representation
    learning: to learn a good feature vector given any kind of input. When applying
    the model to a specific domain, it is still recommended to give some labeled data
    to fine-tune the model with some extra labels.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这种自监督对比学习已经广泛应用于表征学习：即在给定任何类型输入的情况下，学习出一个好的特征向量。当将模型应用于特定领域时，仍然建议提供一些标注数据，以通过额外的标签对模型进行微调。
- en: Algorithms beyond text, visual, and acoustic
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超越文本、视觉和声学的算法
- en: In real life, many kinds of information carriers exist. In addition to text,
    images, and speech, videos, actions, and even proteins contain a wealth of information.
    Therefore, many attempts have been made to obtain vector representations. Researchers
    at DeepMind have developed the *AlphaFold* and *AlphaFold2* algorithms. Based
    on traditional features, such as those of an amino acid sequence, AlphaFold algorithms
    can be used to obtain protein expression vectors and calculate its 3D structure
    in space, which greatly improves experiment efficiency in the field of protein
    analysis.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，存在多种信息载体。除了文本、图像和语音，视频、动作甚至蛋白质都包含了丰富的信息。因此，许多尝试已经被做出来以获取向量表示。DeepMind的研究人员开发了*AlphaFold*和*AlphaFold2*算法。基于传统特征，如氨基酸序列，AlphaFold算法可以用于获取蛋白质表达向量并计算其在空间中的三维结构，极大地提高了蛋白质分析领域的实验效率。
- en: Moreover, in 2021, GitHub launched Copilot to help programmers with the automatic
    completion of code. Prior to this, OpenAI developed the *Codex* model, which was
    able to convert natural language into code. Based on Codex’s model architecture,
    GitHub uses their open source TB-level code base to train the model on a large
    scale and completes the Copilot model to help programmers write new code. Copilot
    also supports the generation and completion of multiple programming languages,
    such as Python, JavaScript, and Go. In the search field, if we want to perform
    a code search or evaluate the similarity of two pieces of code, the Codex model
    can be employed to encode the source code into a vector representation.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在2021年，GitHub推出了Copilot，帮助程序员自动完成代码。在此之前，OpenAI开发了*Codex*模型，它能够将自然语言转换为代码。基于Codex的模型架构，GitHub利用其开源的TB级代码库大规模训练该模型，并完成了Copilot模型，帮助程序员编写新代码。Copilot还支持多种编程语言的生成和补全，如Python、JavaScript和Go。在搜索框中，如果我们想进行代码搜索或评估两段代码的相似性，可以使用Codex模型将源代码编码为向量表示。
- en: The aforesaid operations mostly focus on separate encodings of text, images,
    or audio, so the encoded vector space may vary significantly. To map the information
    of different modalities to the same vector space, OpenAI researchers proposed
    the CLIP model, which can effectively map an image to text. Specifically, CLIP
    includes an image encoder and a text encoder. After inputting an image and multiple
    texts, CLIP encodes them at the same time and hopes to find the text most suitable
    for each image. By training CLIP on a large-scale dataset, CLIP can acquire an
    excellent representation of images and text and map them in the same vector space.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 前述操作大多专注于文本、图像或音频的单独编码，因此编码后的向量空间可能会有显著差异。为了将不同模态的信息映射到相同的向量空间，OpenAI的研究人员提出了CLIP模型，该模型能够有效地将图像映射到文本。具体来说，CLIP包括一个图像编码器和一个文本编码器。在输入图像和多个文本后，CLIP同时对它们进行编码，并希望找到与每张图像最匹配的文本。通过在大规模数据集上训练，CLIP能够获得图像和文本的优秀表示，并将它们映射到相同的向量空间。
- en: Summary
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter described the method of vector representation, which is a major
    step in the operation of search engines.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了向量表示的方法，这是搜索引擎运作中的一个重要步骤。
- en: First, we introduced the importance of vector representation and how to use
    it, and then addressed local and distributed vector representation algorithms.
    In terms of distributed vector representation, the commonly used representation
    algorithms for text, images, and audio were covered, and common representation
    methods for other modalities and multimodality were summarized. Hence, we found
    that the dense vector representation method often entails relatively rich contextual
    information when compared with sparse vectors.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们介绍了向量表示的重要性及其使用方法，接着讨论了局部和分布式向量表示算法。在分布式向量表示方面，介绍了文本、图像和音频的常用表示算法，并总结了其他模态和多模态的常见表示方法。因此，我们发现，与稀疏向量相比，密集向量表示方法通常包含相对丰富的上下文信息。
- en: When building a scalable neural search system, it is important to create an
    encoder that can encode raw documents into high-quality embeddings. This encoding
    process needs to be performed fast to reduce the indexing time. At search time,
    it is critical to apply the same encoding process and find the top-ranked documents
    in a reasonable amount of time. In the next chapter, we’ll utilize the ideas in
    this chapter and build a mental map on how to create a scalable neural search
    system.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建可扩展的神经搜索系统时，创建一个能够将原始文档编码为高质量嵌入的编码器非常重要。这个编码过程需要快速执行，以减少索引时间。在搜索时，必须应用相同的编码过程，并在合理的时间内找到排名靠前的文档。在下一章中，我们将利用本章的思想，构建创建可扩展神经搜索系统的思维导图。
- en: Further reading
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers
    for language understanding.” *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin, Jacob 等人。“Bert：用于语言理解的深度双向变换器的预训练。”*arXiv预印本 arXiv:1810.04805*（2018年）。
- en: Simonyan, Karen and Andrew Zisserman. “Very deep convolutional networks for
    large-scale image recognition.” *arXiv preprint arXiv:1409.1556* (2014).
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan, Karen 和 Andrew Zisserman。“非常深的卷积网络用于大规模图像识别。”*arXiv预印本 arXiv:1409.1556*（2014年）。
- en: He, Kaiming et al. “Deep residual learning for image recognition.” *Proceedings
    of the IEEE conference on computer vision and pattern recognition*. 2016.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He, Kaiming 等人。“深度残差学习用于图像识别。”*IEEE计算机视觉与模式识别会议论文集*，2016年。
- en: 'Schneider, Steffen, et al. “wav2vec: Unsupervised pre-training for speech recognition.”
    *arXiv preprint arXiv:1904.05862* (2019).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schneider, Steffen 等人. “wav2vec: 用于语音识别的无监督预训练。” *arXiv 预印本 arXiv:1904.05862*
    (2019)。'
- en: Radford, Alec et al. “Learning transferable visual models from natural language
    supervision.” *International Conference on Machine Learning*. PMLR, 2021.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford, Alec 等人. “从自然语言监督中学习可转移的视觉模型。” *国际机器学习会议*。PMLR, 2021。
