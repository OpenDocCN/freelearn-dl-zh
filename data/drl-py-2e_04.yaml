- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Monte Carlo Methods
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡罗方法
- en: In the previous chapter, we learned how to compute the optimal policy using
    two interesting dynamic programming methods called value and policy iteration.
    Dynamic programming is a model-based method and it requires the model dynamics
    of the environment to compute the value and Q functions in order to find the optimal policy.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何使用两种有趣的动态规划方法：值迭代和策略迭代来计算最优策略。动态规划是一种基于模型的方法，需要环境的模型动态来计算价值和Q函数，从而找到最优策略。
- en: But let's suppose we don't have the model dynamics of the environment. In that
    case, how do we compute the value and Q functions? Here is where we use model-free
    methods. Model-free methods do not require the model dynamics of the environment
    to compute the value and Q functions in order to find the optimal policy. One
    such popular model-free method is the **Monte Carlo** (**MC**) method.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 但假设我们没有环境的模型动态。在这种情况下，我们如何计算价值和Q函数呢？这时我们就会使用无模型的方法。无模型方法不需要环境的模型动态来计算价值和Q函数，从而找到最优策略。一个常见的无模型方法就是**蒙特卡罗**（**MC**）方法。
- en: We will begin the chapter by understanding what the MC method is, then we will
    look into two important types of tasks in reinforcement learning called prediction
    and control tasks. Later, we will learn how the Monte Carlo method is used in
    reinforcement learning and how it is beneficial compared to the dynamic programming
    method we learned about in the previous chapter. Moving forward, we will understand
    what the MC prediction method is and the different types of MC prediction methods.
    We will also learn how to train an agent to play blackjack with the MC prediction
    method.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从理解蒙特卡罗方法开始，然后介绍强化学习中两个重要的任务类型：预测任务和控制任务。随后，我们将学习蒙特卡罗方法如何在强化学习中应用，并了解它相比于上一章我们学习的动态规划方法有哪些优势。接下来，我们将了解蒙特卡罗预测方法及其不同类型，并学习如何使用蒙特卡罗预测方法训练智能体玩二十一点。
- en: Going ahead, we will learn about the Monte Carlo control method and different
    types of Monte Carlo control methods. Following this, we will learn how to train
    an agent to play blackjack with the MC control method.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习蒙特卡罗控制方法以及不同类型的蒙特卡罗控制方法。之后，我们将学习如何使用蒙特卡罗控制方法训练智能体玩二十一点。
- en: 'To summarize, in this chapter, we will learn about the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在本章中，我们将学习以下内容：
- en: Understanding the Monte Carlo method
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解蒙特卡罗方法
- en: Prediction and control tasks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测与控制任务
- en: The Monte Carlo prediction method
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒙特卡罗预测方法
- en: Playing blackjack with the MC prediction method
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用蒙特卡罗预测方法玩二十一点
- en: The Monte Carlo control method
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒙特卡罗控制方法
- en: Playing blackjack with the MC control method
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用蒙特卡罗控制方法玩二十一点
- en: Understanding the Monte Carlo method
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解蒙特卡罗方法
- en: Before understanding how the Monte Carlo method is useful in reinforcement learning,
    first, let's understand what the Monte Carlo method is and how it works. The Monte
    Carlo method is a statistical technique used to find an approximate solution through
    sampling.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解蒙特卡罗方法在强化学习中的应用之前，首先让我们了解蒙特卡罗方法是什么以及它是如何工作的。蒙特卡罗方法是一种统计技术，通过抽样来找到近似解。
- en: 'For instance, the Monte Carlo method approximates the expectation of a random
    variable by sampling, and when the sample size is greater, the approximation will
    be better. Let''s suppose we have a random variable *X* and say we need to compute
    the expected value of *X*; that is *E(X)*, then we can compute it by taking the
    sum of the values of *X* multiplied by their respective probabilities as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，蒙特卡罗方法通过抽样来近似随机变量的期望值，当样本量增大时，近似效果会更好。假设我们有一个随机变量*X*，并且需要计算*X*的期望值；也就是*E(X)*，那么我们可以通过将*X*的值乘以它们各自的概率并求和来计算，如下所示：
- en: '![](img/B15558_04_001.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_001.png)'
- en: 'But instead of computing the expectation like this, can we approximate it with
    the Monte Carlo method? Yes! We can estimate the expected value of *X* by just
    sampling the values of *X* for some *N* times and compute the average value of
    *X* as the expected value of *X* as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们能否用蒙特卡罗方法来近似计算期望值呢？可以！我们可以通过对*X*进行*N*次抽样，并计算*X*的平均值来估算*X*的期望值，具体如下：
- en: '![](img/B15558_04_002.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_002.png)'
- en: When *N* is larger our approximation will be better. Thus, with the Monte Carlo
    method, we can approximate the solution through sampling and our approximation
    will be better when the sample size is large.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当*N*更大时，我们的近似值将更准确。因此，通过蒙特卡罗方法，我们可以通过采样来逼近解决方案，并且当样本量较大时，我们的近似值会更好。
- en: In the upcoming sections, we will learn how exactly the Monte Carlo method is
    used in reinforcement learning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将学习蒙特卡罗方法在强化学习中是如何被使用的。
- en: Prediction and control tasks
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测与控制任务
- en: 'In reinforcement learning, we perform two important tasks, and they are:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，我们执行两个重要的任务，它们是：
- en: The prediction task
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测任务
- en: The control task
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制任务
- en: Prediction task
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测任务
- en: In the prediction task, a policy ![](img/B15558_03_172.png) is given as an input
    and we try to predict the value function or Q function using the given policy.
    But what is the use of doing this? Our goal is to evaluate the given policy. That
    is, we need to determine whether the given policy is good or bad. How can we determine
    that? If the agent obtains a good return using the given policy then we can say
    that our policy is good. Thus, to evaluate the given policy, we need to understand
    what the return the agent would obtain if it uses the given policy. To obtain
    the return, we predict the value function or Q function using the given policy.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测任务中，给定一个策略 ![](img/B15558_03_172.png) 作为输入，我们尝试使用给定策略预测值函数或Q函数。那么这样做有什么用呢？我们的目标是评估给定的策略。也就是说，我们需要判断给定策略是好是坏。我们如何判断呢？如果代理使用给定策略获得了良好的回报，那么我们可以说这个策略是好的。因此，为了评估给定的策略，我们需要了解代理使用该策略时能获得的回报。为了获得回报，我们通过给定策略预测值函数或Q函数。
- en: That is, we learned that the value function or value of a state denotes the
    expected return an agent would obtain starting from that state following some
    policy ![](img/B15558_03_140.png). Thus, by predicting the value function using
    the given policy ![](img/B15558_03_084.png), we can understand what the expected
    return the agent would obtain in each state if it uses the given policy ![](img/B15558_03_050.png).
    If the return is good then we can say that the given policy is good.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们学到值函数或状态的值表示代理从该状态开始，遵循某个策略 ![](img/B15558_03_140.png) 所获得的期望回报。因此，通过使用给定策略
    ![](img/B15558_03_084.png) 预测值函数，我们可以理解代理在每个状态下，如果使用给定策略 ![](img/B15558_03_050.png)
    时所获得的期望回报。如果回报很好，那么我们可以说给定的策略是好的。
- en: Similarly, we learned that the Q function or Q value denotes the expected return
    the agent would obtain starting from the state *s* and an action *a* following
    the policy ![](img/B15558_03_082.png). Thus, predicting the Q function using the
    given policy ![](img/B15558_03_008.png), we can understand what the expected return
    the agent would obtain in each state-action pair if it uses the given policy.
    If the return is good then we can say that the given policy is good.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们已经学到，Q函数或Q值表示代理从状态*s*和动作*a*开始，遵循策略 ![](img/B15558_03_082.png) 时获得的期望回报。因此，通过使用给定策略
    ![](img/B15558_03_008.png) 预测Q函数，我们可以理解代理在每个状态-动作对中，如果使用给定策略时所获得的期望回报。如果回报很好，那么我们可以说给定的策略是好的。
- en: Thus, we can evaluate the given policy ![](img/B15558_03_008.png) by computing
    the value and Q functions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以通过计算值函数和Q函数来评估给定策略 ![](img/B15558_03_008.png)。
- en: Note that, in the prediction task, we don't make any change to the given input
    policy. We keep the given policy as fixed and predict the value function or Q
    function using the given policy and obtain the expected return. Based on the expected
    return, we evaluate the given policy.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在预测任务中，我们不会对给定的输入策略做任何修改。我们保持给定的策略不变，并使用给定策略预测值函数或Q函数，得到期望的回报。基于期望回报，我们评估给定的策略。
- en: Control task
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制任务
- en: Unlike the prediction task, in the control task, we will not be given any policy
    as an input. In the control task, our goal is to find the optimal policy. So,
    we will start off by initializing a random policy and we try to find the optimal
    policy iteratively. That is, we try to find an optimal policy that gives the maximum
    return.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与预测任务不同，在控制任务中，我们不会获得任何策略作为输入。在控制任务中，我们的目标是找到最优策略。因此，我们将从初始化一个随机策略开始，并尝试迭代地找到最优策略。也就是说，我们尝试找到一个能提供最大回报的最优策略。
- en: Thus, in a nutshell, in the prediction task, we evaluate the given input policy
    by predicting the value function or Q function, which helps us to understand the
    expected return an agent would get if it uses the given policy, while in the control
    task our goal is to find the optimal policy and we will not be given any policy
    as input; so we will start off by initializing a random policy and we try to find
    the optimal policy iteratively.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在预测任务中，我们通过预测价值函数或Q函数来评估给定的输入策略，这有助于我们理解如果一个智能体使用给定的策略，它所获得的期望回报；而在控制任务中，我们的目标是找到最优策略，并且不会提供任何策略作为输入；因此，我们将从初始化一个随机策略开始，并且通过迭代的方式来寻找最优策略。
- en: Now that we have understood what prediction and control tasks are, in the next
    section, we will learn how to use the Monte Carlo method for performing the prediction
    and control tasks.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了预测和控制任务是什么，在下一节中，我们将学习如何使用蒙特卡罗方法执行预测和控制任务。
- en: Monte Carlo prediction
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡罗预测
- en: In this section, we will learn how to use the Monte Carlo method to perform
    the prediction task. We have learned that in the prediction task, we will be given
    a policy and we predict the value function or Q function using the given policy
    to evaluate it. First, we will learn how to predict the value function using the
    given policy with the Monte Carlo method. Later, we will look into predicting
    the Q function using the given policy. Alright, let's get started with the section.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用蒙特卡罗方法来执行预测任务。我们已经学到，在预测任务中，我们会给定一个策略，然后通过预测价值函数或Q函数来评估该策略。首先，我们将学习如何使用蒙特卡罗方法预测给定策略的价值函数。接下来，我们将学习如何预测给定策略的Q函数。好了，让我们开始这一节。
- en: Why do we need the Monte Carlo method for predicting the value function of the
    given policy? Why can't we predict the value function using the dynamic programming
    methods we learned about in the previous chapter? We learned that in order to
    compute the value function using the dynamic programming method, we need to know
    the model dynamics (transition probability), and when we don't know the model
    dynamics, we use the model-free methods.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要使用蒙特卡罗方法来预测给定策略的价值函数？为什么我们不能使用前一章学到的动态规划方法来预测价值函数呢？我们学到，要使用动态规划方法计算价值函数，我们需要知道模型的动态（转移概率），而当我们不知道模型动态时，就需要使用无模型方法。
- en: The Monte Carlo method is a model-free method, meaning that it doesn't require
    the model dynamics to compute the value function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡罗方法是一种无模型方法，这意味着它不需要模型动态来计算价值函数。
- en: 'First, let''s recap the definition of the value function. The value function
    or the value of the state *s* can be defined as the expected return the agent
    would obtain starting from the state *s* and following the policy ![](img/B15558_04_010.png).
    It can be expressed as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们回顾一下价值函数的定义。价值函数，或者说状态*s*的价值，可以定义为从状态*s*开始并遵循策略的智能体所获得的期望回报！[](img/B15558_04_010.png)。它可以表示为：
- en: '![](img/B15558_04_011.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_011.png)'
- en: Okay, how can we estimate the value of the state (value function) using the
    Monte Carlo method? At the beginning of the chapter, we learned that the Monte
    Carlo method approximates the expected value of a random variable by sampling,
    and when the sample size is greater, the approximation will be better. Can we
    leverage this concept of the Monte Carlo method to predict the value of a state?
    Yes!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们如何使用蒙特卡罗方法来估计状态的价值（价值函数）呢？在本章开始时，我们了解到蒙特卡罗方法通过采样来近似随机变量的期望值，并且当样本量增大时，近似效果会更好。我们能否利用蒙特卡罗方法的这个概念来预测状态的价值呢？可以！
- en: 'In order to approximate the value of the state using the Monte Carlo method,
    we sample episodes (trajectories) following the given policy ![](img/B15558_04_012.png)
    for some *N* times and then we compute the value of the state as the average return
    of a state across the sampled episodes, and it can be expressed as:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用蒙特卡罗方法近似状态的价值，我们将按照给定策略！[](img/B15558_04_012.png)进行若干*N*次试验采样（轨迹），然后我们计算状态的价值，作为这些试验中状态的平均回报，它可以表示为：
- en: '![](img/B15558_04_013.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_013.png)'
- en: From the preceding equation, we can understand that the value of a state *s*
    can be approximated by computing the average return of the state *s* across some
    *N* episodes. Our approximation will be better when *N* is higher.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的方程中，我们可以理解，状态*s*的价值可以通过计算在若干*N*次试验中，状态*s*的平均回报来近似。当*N*增大时，我们的近似会更准确。
- en: In a nutshell, in the Monte Carlo prediction method, we approximate the value
    of a state by taking the average return of a state across *N* episodes instead
    of taking the expected return.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在蒙特卡洛预测方法中，我们通过在*N*个回合中取一个状态的平均回报来近似该状态的值，而不是取期望回报。
- en: 'Okay, let''s get a better understanding of how the Monte Carlo method estimates
    the value of a state (value function) with an example. Let''s take our favorite
    grid world environment we covered in *Chapter 1*, *Fundamentals of Reinforcement
    Learning*, as shown in *Figure 4.1*. Our goal is to reach the state **I** from
    the state **A** without visiting the shaded states, and the agent receives +1
    reward when it visits the unshaded states and -1 reward when it visits the shaded
    states:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们通过一个例子更好地理解蒙特卡洛方法如何估计状态的值（价值函数）。我们以我们最喜欢的网格世界环境为例，该环境在*第1章*《强化学习基础》中已介绍，如*图4.1*所示。我们的目标是从状态**A**到达状态**I**，而不经过阴影状态，代理在访问未阴影状态时获得+1奖励，在访问阴影状态时获得-1奖励：
- en: '![](img/B15558_04_01.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_01.png)'
- en: 'Figure 4.1: Grid world environment'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：网格世界环境
- en: Let's say we have a stochastic policy ![](img/B15558_03_050.png). Let's suppose,
    in state **A,** our stochastic policy ![](img/B15558_03_140.png) selects action
    *down* 80% of time and action *right* 20% of the time, and it selects action *right*
    in states **D** and **E** and action *down* in states **B** and **F** 100% of
    the time.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个随机策略 ![](img/B15558_03_050.png)。假设在状态**A**中，我们的随机策略 ![](img/B15558_03_140.png)
    80%的时间选择动作*下*，20%的时间选择动作*右*，并且在状态**D**和**E**中选择动作*右*，在状态**B**和**F**中选择动作*下*，且选择的概率为100%。
- en: 'First, we generate an episode ![](img/B15558_04_016.png) using our given stochastic
    policy ![](img/B15558_03_140.png) as *Figure 4.2* shows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用给定的随机策略 ![](img/B15558_03_140.png) 生成一个回合 ![](img/B15558_04_016.png)，如*图4.2*所示：
- en: '![](img/B15558_04_02.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_02.png)'
- en: 'Figure 4.2: Episode ![](img/B15558_04_018.png)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2：回合 ![](img/B15558_04_018.png)
- en: For a better understanding, let's focus only on state **A**. Let's now compute
    the return of state **A**. The return of a state is the sum of the rewards of
    the trajectory starting from that state. Thus, the return of state **A** is computed
    as *R*[1](*A*) = 1+1+1+1 = 4 where the subscript 1 in *R*[1] indicates the return
    from episode 1.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，让我们只关注状态**A**。现在让我们计算状态**A**的回报。一个状态的回报是从该状态开始的轨迹上的奖励总和。因此，状态**A**的回报计算为
    *R*[1](*A*) = 1+1+1+1 = 4，其中*R*[1]中的下标1表示来自回合1的回报。
- en: 'Say we generate another episode ![](img/B15558_04_019.png) using the same given
    stochastic policy ![](img/B15558_03_084.png) as *Figure 4.3* shows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们生成了另一集 ![](img/B15558_04_019.png)，使用与*图4.3*所示相同的随机策略 ![](img/B15558_03_084.png)：
- en: '![](img/B15558_04_03.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_03.png)'
- en: 'Figure 4.3: Episode ![](img/B15558_04_019.png)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3：回合 ![](img/B15558_04_019.png)
- en: Let's now compute the return of state **A**. The return of state **A** is *R*[2](*A*)
    = -1+1+1+1 = 2.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算状态**A**的回报。状态**A**的回报是 *R*[2](*A*) = -1+1+1+1 = 2。
- en: 'Say we generate another episode ![](img/B15558_04_022.png) using the same given
    stochastic policy ![](img/B15558_03_082.png) as *Figure 4.4* shows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们生成了另一集 ![](img/B15558_04_022.png)，使用与*图4.4*所示相同的随机策略 ![](img/B15558_03_082.png)：
- en: '![](img/B15558_04_04.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_04.png)'
- en: 'Figure 4.4: Episode ![](img/B15558_04_024.png)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4：回合 ![](img/B15558_04_024.png)
- en: Let's now compute the return of state **A**. The return of state **A** is *R*[3](*A*)
    = 1+1+1+1 = 4.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算状态**A**的回报。状态**A**的回报是 *R*[3](*A*) = 1+1+1+1 = 4。
- en: 'Thus, we generated three episodes and computed the return of state **A** in
    all three episodes. Now, how can we compute the value of the state **A**? We learned
    that in the Monte Carlo method, the value of a state can be approximated by computing
    the average return of the state across some *N* episodes (trajectories):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们生成了三个回合，并计算了状态**A**在所有三个回合中的回报。现在，我们如何计算状态**A**的值呢？我们了解到，在蒙特卡洛方法中，状态的值可以通过计算该状态在*N*个回合中的平均回报来近似（轨迹）：
- en: '![](img/B15558_04_013.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_013.png)'
- en: 'We need to compute the value of state **A**, so we can compute it by just taking
    the average return of the state **A** across the *N* episodes as:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要计算状态**A**的值，因此我们可以通过取状态**A**在*N*个回合中的平均回报来计算它，如下所示：
- en: '![](img/B15558_04_026.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_026.png)'
- en: 'We generated three episodes, thus:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成了三个回合，因此：
- en: '![](img/B15558_04_027.png)![](img/B15558_04_028.png)![](img/B15558_04_029.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_027.png)![](img/B15558_04_028.png)![](img/B15558_04_029.png)'
- en: Thus, the value of state **A** is 3.3\. Similarly, we can compute the value
    of all other states by just taking the average return of the state across the
    three episodes.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，状态**A**的价值为 3.3。同样地，我们可以通过仅仅计算该状态在三个回合中的平均回报来计算所有其他状态的价值。
- en: For easier understanding, in the preceding example, we only generated three
    episodes. In order to find a better and more accurate estimate of the value of
    the state, we should generate many episodes (not just three) and compute the average
    return of the state as the value of the state.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于理解，在前面的例子中，我们只生成了三个回合。为了找到更好、更准确的状态价值估计，我们应该生成更多回合（而不仅仅是三个），并计算状态的平均回报作为该状态的价值。
- en: Thus, in the Monte Carlo prediction method, to predict the value of a state
    (value function) using the given input policy ![](img/B15558_03_139.png), we generate
    some *N* episodes using the given policy and then we compute the value of a state
    as the average return of the state across these *N* episodes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在蒙特卡洛预测方法中，使用给定的输入策略 ![](img/B15558_03_139.png) 来预测一个状态的价值（价值函数），我们通过给定的策略生成一些
    *N* 个回合，然后计算该状态的价值作为这些 *N* 个回合中该状态的平均回报。
- en: Note that while computing the return of the state, we can also include the discount
    factor and compute the discounted return, but for simplicity let's not include
    the discount factor.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在计算状态的回报时，我们也可以包括折扣因子并计算折扣回报，但为了简化起见，我们这里不包括折扣因子。
- en: Now, that we have a basic understanding of how the Monte Carlo prediction method
    predicts the value function of the given policy, let's look into more detail by
    understanding the algorithm of the Monte Carlo prediction method in the next section.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经对蒙特卡洛预测方法如何预测给定策略的价值函数有了基本的了解，接下来让我们通过理解蒙特卡洛预测方法的算法来更详细地探讨这一方法。
- en: MC prediction algorithm
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MC 预测算法
- en: 'The Monte Carlo prediction algorithm is given as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛预测算法如下所示：
- en: Let total_return(*s*) be the sum of return of a state across several episodes
    and *N*(*s*) be the counter, that is, the number of times a state is visited across
    several episodes. Initialize total_return(*s*) and *N*(*s*) as zero for all the
    states. The policy ![](img/B15558_03_140.png) is given as input.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让 total_return(*s*) 表示一个状态在多个回合中的回报总和，*N*(*s*) 表示计数器，即该状态在多个回合中被访问的次数。将 total_return(*s*)
    和 *N*(*s*) 对所有状态初始化为零。策略 ![](img/B15558_03_140.png) 作为输入。
- en: 'For *M* number of iterations:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *M* 次迭代：
- en: Generate an episode using the policy ![](img/B15558_04_032.png)
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略 ![](img/B15558_04_032.png) 生成一个回合
- en: Store all the rewards obtained in the episode in the list called rewards
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将回合中获得的所有奖励存储在名为 rewards 的列表中
- en: 'For each step *t* in the episode:'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于回合中的每一步 *t*：
- en: Compute the return of state *s*[t] as *R*(*s*[t]) = sum(rewards[t:])
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将状态 *s*[t] 的回报计算为 *R*(*s*[t]) = sum(rewards[t:])
- en: Update the total return of state *s*[t] as total_returns(*s*[t]) = total_return(*s*[t])
    + R(*s*[t])
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新状态 *s*[t] 的回报总和为 total_returns(*s*[t]) = total_return(*s*[t]) + R(*s*[t])
- en: Update the counter as *N*(*s*[t]) = *N*(*s*[t]) + 1
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新计数器为 *N*(*s*[t]) = *N*(*s*[t]) + 1
- en: Compute the value of a state by just taking the average, that is:![](img/B15558_04_033.png)
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过仅仅计算平均值来求一个状态的价值，即：![](img/B15558_04_033.png)
- en: The preceding algorithm implies that the value of the state is just the average
    return of the state across several episodes.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 上述算法表明，状态的价值就是该状态在多个回合中的平均回报。
- en: To get a better understanding of how exactly the preceding algorithm works,
    let's take a simple example and compute the value of each state manually. Say
    we need to compute the value of three states *s*[0], *s*[1], and *s*[2]. We know
    that we obtain a reward when we transition from one state to another. Thus, the
    reward for the final state will be 0 as we don't make any transitions from the
    final state. Hence, the value of the final state *s*[2] will be zero. Now, we
    need to find the value of two states *s*[0] and *s*[1].
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解前面的算法是如何工作的，我们通过一个简单的例子手动计算每个状态的价值。假设我们需要计算三个状态 *s*[0]、*s*[1] 和 *s*[2]
    的价值。我们知道，在从一个状态过渡到另一个状态时，我们会获得奖励。因此，最终状态的奖励将为 0，因为我们不会从最终状态进行任何过渡。因此，最终状态 *s*[2]
    的价值为零。现在，我们需要找到两个状态 *s*[0] 和 *s*[1] 的价值。
- en: The upcoming sections are explained with manual calculations, for a better understanding,
    follow along with a pen and paper.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的章节将通过手动计算进行说明，便于理解，请准备好笔和纸跟随练习。
- en: '**Step 1**:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1**：'
- en: 'Initialize the total_returns(*s*) and *N*(*s*) for all the states to zero as
    *Table 4.1* shows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如*表 4.1*所示，将所有状态的 total_returns(*s*) 和 *N*(*s*) 初始化为零：
- en: '![](img/B15558_04_05.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_05.png)'
- en: 'Table 4.1: Initial values'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1：初始值
- en: Say we are given a stochastic policy ![](img/B15558_03_082.png); in state *s*[0]
    our stochastic policy selects the action 0 for 50% of the time and action 1 for
    50% of the time, and it selects action 1 in state *s*[1] for 100% of the time.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们给定一个随机策略 ![](img/B15558_03_082.png)；在状态 *s*[0]，我们的随机策略以 50% 的概率选择动作 0，50%
    的概率选择动作 1，而在状态 *s*[1] 以 100% 的概率选择动作 1。
- en: '**Step 2: Iteration 1**:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2：第一次迭代**：'
- en: 'Generate an episode using the given input policy ![](img/B15558_03_038.png),
    as *Figure 4.5* shows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用给定的输入策略生成一个回合，如*图 4.5*所示：
- en: '![](img/B15558_04_06.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_06.png)'
- en: 'Figure 4.5: Generating an episode using the given policy ![](img/B15558_03_084.png)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5：使用给定策略生成回合 ![](img/B15558_03_084.png)
- en: Store all rewards obtained in the episode in the list called rewards. Thus,
    rewards = [1, 1].
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 将在该回合中获得的所有奖励存储在名为 rewards 的列表中。因此，rewards = [1, 1]。
- en: 'First, we compute the return of the state *s*[0] (sum of rewards from *s*[0]):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算状态 *s*[0] 的回报（即从 *s*[0] 开始的奖励总和）：
- en: '![](img/B15558_04_037.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_037.png)'
- en: 'Update the total return of the state *s*[0] in our table as:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 更新表格中状态 *s*[0] 的总回报如下：
- en: '![](img/B15558_04_038.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_038.png)'
- en: 'Update the number of times the state *s*[0] is visited in our table as:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 更新表格中状态 *s*[0] 被访问的次数如下：
- en: '![](img/B15558_04_039.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_039.png)'
- en: 'Now, let''s compute the return of the state *s*[1] (sum of rewards from *s*[1]):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算状态 *s*[1] 的回报（即从 *s*[1] 开始的奖励总和）：
- en: '![](img/B15558_04_040.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_040.png)'
- en: 'Update the total return of the state *s*[1] in our table as:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 更新表格中状态 *s*[1] 的总回报如下：
- en: '![](img/B15558_04_041.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_041.png)'
- en: 'Update the number of times the state *s*[1] is visited in our table as:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 更新表格中状态 *s*[1] 被访问的次数如下：
- en: '![](img/B15558_04_042.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_042.png)'
- en: 'Our updated table, after iteration 1, is as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 经过第一次迭代后的更新表格如下：
- en: '![](img/B15558_04_07.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_07.png)'
- en: 'Table 4.2: Updated table after the first iteration'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.2：第一次迭代后的更新表格
- en: '**Iteration 2:**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二次迭代：**'
- en: 'Say we generate another episode using the same given policy ![](img/B15558_03_140.png)
    as *Figure 4.6* shows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用相同的给定策略生成另一个回合，正如*图 4.6*所示：
- en: '![](img/B15558_04_08.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_08.png)'
- en: 'Figure 4.6: Generating an episode using the given policy ![](img/B15558_03_172.png)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6：使用给定策略生成回合 ![](img/B15558_03_172.png)
- en: Store all rewards obtained in the episode in the list called rewards. Thus,
    rewards = [3, 1].
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 将在该回合中获得的所有奖励存储在名为 rewards 的列表中。因此，rewards = [3, 1]。
- en: 'First, we compute the return of the state *s*[0] (sum of rewards from *s*[0]):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算状态 *s*[0] 的回报（即从 *s*[0] 开始的奖励总和）：
- en: '![](img/B15558_04_045.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_045.png)'
- en: 'Update the total return of the state *s*[0] in our table as:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 更新表格中状态 *s*[0] 的总回报如下：
- en: '![](img/B15558_04_046.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_046.png)'
- en: 'Update the number of times the state *s*[0] is visited in our table as:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 更新表格中状态 *s*[0] 被访问的次数如下：
- en: '![](img/B15558_04_047.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_047.png)'
- en: 'Now, let''s compute the return of the state *s*[1] (sum of rewards from *s*[1]):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算状态 *s*[1] 的回报（即从 *s*[1] 开始的奖励总和）：
- en: '![](img/B15558_04_040.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_040.png)'
- en: 'Update the return of the state *s*[1] in our table as:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 更新表格中状态 *s*[1] 的回报如下：
- en: '![](img/B15558_04_049.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_049.png)'
- en: 'Update the number of times the state is visited:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 更新状态被访问的次数：
- en: '![](img/B15558_04_050.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_050.png)'
- en: 'Our updated table after the second iteration is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第二次迭代后的更新表格如下：
- en: '![](img/B15558_04_09.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_09.png)'
- en: 'Table 4.3: Updated table after the second iteration'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.3：第二次迭代后的更新表格
- en: Since we are computing manually, for simplicity, let's stop at two iterations;
    that is, we just generate only two episodes.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在手动计算，为了简单起见，我们就止步于两次迭代；也就是说，我们只生成了两个回合。
- en: '**Step 3:**'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3：**'
- en: 'Now, we can compute the value of the state as:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以计算状态的值如下：
- en: '![](img/B15558_04_051.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_051.png)'
- en: 'Thus:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 因此：
- en: '![](img/B15558_04_052.png)![](img/B15558_04_053.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_052.png)![](img/B15558_04_053.png)'
- en: Thus, we computed the value of the state by just taking the average return across
    multiple episodes. Note that in the preceding example, for our manual calculation,
    we just generated two episodes, but for a better estimation of the value of the
    state, we generate several episodes and then we compute the average return across
    those episodes (not just 2).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们通过在多个回合中计算平均回报来估算状态的值。注意，在上述示例中，为了进行手动计算，我们只生成了两个回合，但为了更好地估算状态的值，我们应该生成多个回合，然后计算这些回合的平均回报（而不仅仅是
    2 个回合）。
- en: Types of MC prediction
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蒙特卡洛预测的类型
- en: 'We just learned how the Monte Carlo prediction algorithm works. We can categorize
    the Monte Carlo prediction algorithm into two types:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学到了蒙特卡洛预测算法的工作原理。我们可以将蒙特卡洛预测算法分为两种类型：
- en: First-visit Monte Carlo
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首次访问蒙特卡洛方法
- en: Every-visit Monte Carlo
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次访问蒙特卡洛方法
- en: First-visit Monte Carlo
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 首次访问蒙特卡洛方法
- en: We learned that in the MC prediction method, we estimate the value of the state
    by just taking the average return of the state across multiple episodes. We know
    that in each episode a state can be visited multiple times. In the first-visit
    Monte Carlo method, if the same state is visited again in the same episode, we
    don't compute the return for that state again. For example, consider a case where
    an agent is playing snakes and ladders. If the agent lands on a snake, then there
    is a good chance that the agent will return to a state that it had visited earlier.
    So, when the agent revisits the same state, we don't compute the return for that
    state for the second time.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学过，在蒙特卡洛预测方法中，我们通过在多个回合中取一个状态的平均回报来估计该状态的值。我们知道，在每个回合中，一个状态可能会被访问多次。在首次访问蒙特卡洛方法中，如果同一状态在同一个回合中再次被访问，我们不会再次计算该状态的回报。例如，考虑一个代理玩蛇梯游戏的情况。如果代理踩到蛇，那么很有可能它会回到一个之前访问过的状态。因此，当代理重新访问相同的状态时，我们不会第二次计算该状态的回报。
- en: 'The following shows the algorithm of first-visit MC; as the point in bold says,
    we compute the return for the state *s*[t] only if it is occurring for the first
    time in the episode:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了首次访问蒙特卡洛算法；正如粗体部分所述，只有在状态 *s*[t] 在回合中第一次出现时，我们才会计算该状态的回报：
- en: Let total_return(*s*) be the sum of return of a state across several episodes
    and *N*(*s*) be the counter, that is, the number of times a state is visited across
    several episodes. Initialize total_return(*s*) and *N*(*s*) as zero for all the
    states. The policy ![](img/B15558_04_054.png) is given as input
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设 total_return(*s*) 为该状态在多个回合中的回报总和，*N*(*s*) 为计数器，即该状态在多个回合中被访问的次数。将所有状态的 total_return(*s*)
    和 *N*(*s*) 初始化为零。输入给定策略 ![](img/B15558_04_054.png)
- en: 'For *M* number of iterations:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *M* 次迭代：
- en: Generate an episode using the policy ![](img/B15558_03_084.png)
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略生成一个回合 ![](img/B15558_03_084.png)
- en: Store all the rewards obtained in the episode in the list called rewards
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将回合中获得的所有奖励存储在名为 rewards 的列表中
- en: 'For each step *t* in the episode:'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于回合中的每一步 *t*：
- en: '**If** **the state s**[t] **is occurring for the first time in the episode:**'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**如果** **状态 s**[t] **在回合中首次出现：**'
- en: Compute the return of the state *s*[t] as *R*(*s*[t]) = sum(rewards[t:])
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算状态 *s*[t] 的回报为 *R*(*s*[t]) = sum(rewards[t:])
- en: Update the total return of the state *s*[t] as total_return(*s*[t]) = total_return(*s*[t])
    + R(*s*[t])
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将状态 *s*[t] 的总回报更新为 total_return(*s*[t]) = total_return(*s*[t]) + R(*s*[t])
- en: Update the counter as *N*(*s*[t]) = *N*(*s*[t]) + 1
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新计数器为 *N*(*s*[t]) = *N*(*s*[t]) + 1
- en: Compute the value of a state by just taking the average, that is:![](img/B15558_04_056.png)
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过取平均值来计算状态的值，即：![](img/B15558_04_056.png)
- en: Every-visit Monte Carlo
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每次访问蒙特卡洛方法
- en: 'As you might have guessed, every-visit Monte Carlo is just the opposite of
    first-visit Monte Carlo. Here, we compute the return every time a state is visited
    in the episode. The algorithm of every-visit Monte Carlo is the same as the one
    we saw earlier at the beginning of this section and it is as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所料，每次访问蒙特卡洛方法正好是首次访问蒙特卡洛方法的相反。在这里，我们每次访问到一个状态时，都会计算该状态的回报。每次访问蒙特卡洛算法与我们之前在本节开始时看到的算法是相同的，具体如下：
- en: Let total_return(*s*) be the sum of the return of a state across several episodes
    and *N*(*s*) be the counter, that is, the number of times a state is visited across
    several episodes. Initialize total_return(*s*) and *N*(*s*) as zero for all the
    states. The policy ![](img/B15558_04_012.png) is given as input
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设 total_return(*s*) 为该状态在多个回合中的回报总和，*N*(*s*) 为计数器，即该状态在多个回合中被访问的次数。将所有状态的 total_return(*s*)
    和 *N*(*s*) 初始化为零。输入给定策略 ![](img/B15558_04_012.png)
- en: 'For *M* number of iterations:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *M* 次迭代：
- en: Generate an episode using the policy ![](img/B15558_03_140.png)
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略生成一个回合 ![](img/B15558_03_140.png)
- en: Store all the rewards obtained in the episode in the list called rewards
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将回合中获得的所有奖励存储在名为 rewards 的列表中
- en: 'For each step *t* in the episode:'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于回合中的每一步 *t*：
- en: Compute the return of the state *s*[t] as *R*(*s*[t]) = sum(rewards[t:])
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算状态 *s*[t] 的回报为 *R*(*s*[t]) = sum(rewards[t:])
- en: Update the total return of the state *s*[t] as total_return(*s*[t]) = total_return(*s*[t])
    + R(*s*[t])
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新状态 *s*[t] 的总回报：total_return(*s*[t]) = total_return(*s*[t]) + R(*s*[t])
- en: Update the counter as *N*(*s*[t]) = *N*(*s*[t]) + 1
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新计数器：*N*(*s*[t]) = *N*(*s*[t]) + 1
- en: Compute the value of a state by just taking the average, that is:![](img/B15558_04_056.png)
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过计算一个状态的平均值来获得该状态的值，即：![](img/B15558_04_056.png)
- en: Remember that the only difference between the first-visit MC and every-visit
    MC methods is that in the first-visit MC method, we compute the return for a state
    only for its first time of occurrence in the episode but in the every-visit MC
    method, the return of the state is computed every time the state is visited in
    an episode. We can choose between first-visit MC and every-visit MC based on the
    problem that we are trying to solve.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，第一次访问MC（First-Visit MC）和每次访问MC（Every-Visit MC）方法之间的唯一区别在于，在第一次访问MC方法中，我们只计算某个状态在该回合第一次出现时的回报；而在每次访问MC方法中，我们会在每次访问该状态时计算回报。我们可以根据所解决的问题选择使用第一次访问MC或每次访问MC方法。
- en: Now that we have understood how the Monte Carlo prediction method predicts the
    value function of the given policy, in the next section, we will learn how to
    implement the Monte Carlo prediction method.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了蒙特卡洛预测方法如何预测给定策略的值函数，在下一节中，我们将学习如何实现蒙特卡洛预测方法。
- en: Implementing the Monte Carlo prediction method
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现蒙特卡洛预测方法
- en: If you love playing card games then this section is definitely going to be interesting
    for you. In this section, we will learn how to play blackjack with the Monte Carlo
    prediction method. Before diving in, let's understand how the blackjack game works
    and its rules.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢玩纸牌游戏，那么这一节肯定会让你感兴趣。在这一节中，我们将学习如何使用蒙特卡洛预测方法来玩二十一点。在深入之前，先让我们了解一下二十一点游戏的规则和玩法。
- en: Understanding the blackjack game
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解二十一点游戏
- en: Blackjack, also known as **21**, is one of the most popular card games. The
    game consists of a player and a dealer. The goal of the player is to have the
    value of the sum of all their cards be 21 or a larger value than the sum of the
    dealer's cards while not exceeding 21\. If one of these criteria is met then the
    player wins the game; else the dealer wins the game. Let's understand this in
    more detail.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 二十一点，也被称为**21点**，是最受欢迎的纸牌游戏之一。游戏由玩家和庄家组成。玩家的目标是使所有牌的总和为21或比庄家的牌总和更大，但不得超过21。如果满足其中一个条件，玩家就赢得游戏；否则庄家赢得游戏。让我们更详细地了解这一点。
- en: The values of the cards **Jack (J)**, **King (K)**, and **Queen (Q)** will be
    considered as 10\. The value of the **Ace (A)** can be 1 or 11, depending on the
    player's choice. That is, the player can decide whether the value of an **Ace**
    should be 1 or 11 during the game. The value of the rest of the cards (**2** **to**
    **10**) is just their face value. For instance, the value of the card **2** will
    be 2, the value of the card **3** will be 3, and so on.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jack（J）**、**King（K）** 和 **Queen（Q）** 的牌值都将视为10。**Ace（A）** 的值可以是1或11，这取决于玩家的选择。也就是说，玩家可以决定在游戏中
    **Ace** 的值是1还是11。其他牌（**2** 到 **10**）的值即为其面值。例如，牌 **2** 的值为2，牌 **3** 的值为3，依此类推。'
- en: We learned that the game consists of a player and a dealer. There can be many
    players at a time but only one dealer. All the players compete with the dealer
    and not with other players. Let's consider a case where there is only one player
    and a dealer. Let's understand blackjack by playing the game along with different
    cases. Let's suppose we are the player and we are competing with the dealer.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，这个游戏由玩家和庄家组成。游戏中可以有多个玩家，但只有一个庄家。所有玩家与庄家竞争，而不是与其他玩家竞争。我们来考虑一个只有一个玩家和一个庄家的情况。让我们通过玩游戏并分析不同的情况来理解二十一点。假设我们是玩家，正在与庄家竞争。
- en: '**Case 1: When the player wins the game**'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例 1：当玩家赢得游戏时**'
- en: Initially, a player is given two cards. Both of these cards are face up, that
    is, both of the player's cards are visible to the dealer. Similarly, the dealer
    is also given two cards. But one of the dealer's cards is face up, and the other
    is face down. That is, the dealer shows only one of their cards.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，玩家会被发两张牌。两张牌都面朝上，也就是说，玩家的两张牌对庄家都是可见的。类似地，庄家也会发两张牌，但庄家的一张牌面朝上，另一张面朝下。也就是说，庄家只展示他们的一张牌。
- en: 'As we can see in *Figure 4.7*, the player has two cards (both face up) and
    the dealer also has two cards (only one face up):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在*图4.7*中所见，玩家有两张卡牌（都面朝上），而庄家也有两张卡牌（其中一张面朝上）：
- en: '![](img/B15558_04_10.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_10.png)'
- en: 'Figure 4.7: The player has 20, and the dealer has 2 with one card face down'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7：玩家的总牌面为20，庄家有一张牌面朝下的2
- en: Now, the player performs either of the two actions, which are **Hit** and **Stand**.
    If we (the player) perform the action **hit,** then we get one more card. If we
    perform **stand,** then it implies that we don't need any more cards and tells
    the dealer to show all their cards. Whoever has a sum of cards value equal to
    21 or a larger value than the other player but not exceeding 21 wins the game.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，玩家执行两个动作中的一个，即**要牌**和**停牌**。如果我们（玩家）选择执行**要牌**，我们将再拿一张牌。如果我们选择**停牌**，则表示我们不再需要任何牌，并告诉庄家亮出他们的所有牌。无论谁的牌面总和等于21，或者比对方更大但不超过21，就赢得游戏。
- en: We learned that the value of **J**, **K**, and **Q** is 10\. As shown in *Figure
    4.7*, we have cards **J** and **K**, which sums to 20 (10+10). Thus, the total
    value our cards is already a large number and it didn't exceed 21\. So we **stand**,
    and this action tells the dealer to show their cards. As we can observe in *Figure
    4.8*, the dealer has now shown all their cards and the total value of the dealer's
    cards is 12 and the total value of our (the player's) cards is 20, which is larger
    and also didn't exceed 21, so we win the game.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，**J**、**K**和**Q**的牌值为10。如*图4.7*所示，我们手中有**J**和**K**两张牌，它们的总和是20（10+10）。因此，我们的牌面总和已经是一个大数，并且没有超过21。所以我们选择**停牌**，这一动作告诉庄家亮出他们的牌。如*图4.8*所示，庄家现在已展示出他们的所有牌，庄家的牌面总和为12，而我们的（玩家的）牌面总和为20，这个总和更大，并且也没有超过21，因此我们赢得了游戏。
- en: '![](img/B15558_04_11.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_11.png)'
- en: 'Figure 4.8: The player wins!'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8：玩家获胜！
- en: '**Case 2: When the player loses the game**'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例2：玩家输掉游戏**'
- en: '*Figure 4.9* shows we have two cards and the dealer also has two cards and
    only one of the dealer''s card is visible to us:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.9*显示我们有两张牌，庄家也有两张牌，但庄家只有一张牌是我们可见的：'
- en: '![](img/B15558_04_12.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_12.png)'
- en: 'Figure 4.9: The player has 13, and the dealer has 7 with one card face down'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9：玩家的总牌面为13，庄家有一张牌面朝下的7
- en: 'Now, we have to decide whether we should (perform the action) hit or stand.
    *Figure 4.9* shows we have two cards, **K** and **3**, which sums to 13 (10+3).
    Let''s be a little optimistic and hope that the total value of the dealer''s cards
    will not be greater than ours. So we **stand,** and this action tells the dealer
    to show their cards. As we can observe in *Figure 4.10*, the sum of the dealer''s
    card is 17, but ours is only 13, so we lose the game. That is, the dealer has
    got a larger value than us, and it did not exceed 21, so the dealer wins the game,
    and we lose:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须决定是（执行动作）“要牌”还是“停牌”。*图4.9*显示我们有两张牌，**K**和**3**，它们的总和是13（10+3）。让我们稍微乐观一些，希望庄家的牌面总和不会超过我们的。因此，我们选择**停牌**，这一动作告诉庄家亮出他们的牌。如*图4.10*所示，庄家的牌面总和为17，而我们的总和只有13，所以我们输掉了游戏。也就是说，庄家的牌面总和比我们大，并且没有超过21，因此庄家赢得了游戏，而我们输了：
- en: '![](img/B15558_04_13.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_13.png)'
- en: 'Figure 4.10: The dealer wins!'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10：庄家获胜！
- en: '**Case 3: When the player goes bust**'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例3：玩家爆掉**'
- en: '*Figure 4.11* shows we have two cards and the dealer also has two cards but
    only one of the dealer''s cards is visible to us:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.11*显示我们有两张牌，庄家也有两张牌，但庄家只有一张牌是我们可见的：'
- en: '![](img/B15558_04_14.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_14.png)'
- en: 'Figure 4.11: The player has 8, and the dealer has 10 with one card face down'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11：玩家的总牌面为8，庄家有一张牌面朝下的10
- en: 'Now, we have to decide whether we should (perform the action) hit or stand.
    We learned that the goal of the game is to have a sum of cards value of 21, or
    a larger value than the dealer while not exceeding 21\. Right now, the total value
    of our cards is just 3+5 = 8\. Thus, we (perform the action) **hit** so that we
    can make our sum value larger. After we **hit,** we receive a new card as shown
    in *Figure 4.12*:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须决定是（执行动作）“要牌”还是“停牌”。我们已经了解到，游戏的目标是让牌面总和为21，或者比庄家的牌面总和大，但不超过21。目前，我们的牌面总和是3+5
    = 8。因此，我们选择**要牌**，以便增加我们的牌面总和。在我们**要牌**之后，我们收到了如*图4.12*所示的新牌：
- en: '![](img/B15558_04_15.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_15.png)'
- en: 'Figure 4.12: The player has 18, and the dealer has 10 with one card face down'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12：玩家的总牌面为18，庄家有一张牌面朝下的10
- en: 'As we can observe, we got a new card. Now, the total value of our cards is
    3+5+10 = 18\. Again, we need to decide whether we should (perform the action)
    hit or stand. Let''s be a little greedy and (perform the action) **hit** so that
    we can make our sum value a little larger. As shown in *Figure 4.13*, we **hit**
    and received one more card but now the total value of our cards becomes 3+5+10+10
    = 28, which exceeds 21, and this is called a **bust** and we lose the game:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们得到了新的一张牌。现在，我们的牌的总点数是 3+5+10 = 18。再次，我们需要决定是否（执行操作）要牌或停牌。让我们有点贪心，选择（执行操作）**要牌**，以便我们可以稍微增加我们的点数。如*图
    4.13*所示，我们**要牌**并得到了一张新牌，但现在我们的牌的总点数变为 3+5+10+10 = 28，超过了 21，这就是所谓的**爆掉**，我们输掉了游戏：
- en: '![](img/B15558_04_16.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_16.png)'
- en: 'Figure 4.13: The player goes bust!'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.13：玩家爆掉了！
- en: '**Case 4: Useable Ace**'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例 4：可用的 A**'
- en: 'We learned that the value of the **Ace** can be either 1 or 11, and the player
    can decide the value of the **ace** during the game. Let''s learn how this works.
    As *Figure 4.14* shows, we have been given two cards and the dealer also has two
    cards and only one of the dealer''s cards is face up:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道**A**的点数可以是 1 或 11，玩家可以在游戏过程中决定**A**的点数。让我们学习一下这是如何运作的。如*图 4.14*所示，我们被发了两张牌，庄家也有两张牌，其中只有一张庄家的牌是面朝上的：
- en: '![](img/B15558_04_17.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_17.png)'
- en: 'Figure 4.14: The player has 10, and the dealer has 5 with one card face down'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14：玩家的点数是 10，庄家的点数是 5，且有一张牌是面朝下的
- en: As we can see, the total value of our cards is 5+5 = 10\. Thus, we **hit** so
    that we can make our sum value larger. As *Figure 4.15* shows, after performing
    the hit action we received a new card, which is an **Ace**. Now, we can decide
    the value of the **Ace** to be either 1 or 11\. If we consider the value of **Ace**
    to be 1, then the total value of our cards will be 5+5+1 = 11\. But if we consider
    the value of the **Ace** to be 11, then the total value of our cards will be 5+5+11
    = 21\. In this case, we consider the value of our **Ace** to be 11 so that our
    sum value becomes 21.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们的牌的总点数是 5+5 = 10。因此，我们选择**要牌**以便可以增加我们的点数。如*图 4.15*所示，在执行要牌操作后，我们获得了一张新牌，它是一张**A**。现在，我们可以决定将**A**的点数定为
    1 或 11。如果我们将**A**的点数定为 1，那么我们的牌的总点数将是 5+5+1 = 11。如果我们将**A**的点数定为 11，那么我们的牌的总点数将是
    5+5+11 = 21。在这种情况下，我们将**A**的点数定为 11，以便我们的点数变为 21。
- en: 'Thus, we set the value of the **Ace** to be 11 and win the game, and in this
    case, the **Ace** is called the usable **Ace** since it helped us to win the game:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将**A**的点数定为 11 并赢得了游戏，在这种情况下，**A**被称为可用的**A**，因为它帮助我们赢得了游戏：
- en: '![](img/B15558_04_18.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_18.png)'
- en: 'Figure 4.15: The player uses the **Ace** as 11 and wins the game'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15：玩家将**A**视为 11 并赢得了游戏
- en: '**Case 5: Non-usable Ace**'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例 5：不可用的 A**'
- en: '*Figure 4.16* shows we have two cards and the dealer has two cards with one
    face up:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4.16*显示我们有两张牌，庄家也有两张牌，其中一张是面朝上的：'
- en: '![](img/B15558_04_19.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_19.png)'
- en: 'Figure 4.16: The player has 13, and the dealer has 10 with one card face down'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.16：玩家的点数是 13，庄家的点数是 10，且有一张牌是面朝下的
- en: 'As we can observe, the total value of our cards is 13 (10+3). We (perform the
    action) **hit** so that we can make our sum value a little larger:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们的牌的总点数是 13（10+3）。我们（执行操作）**要牌**，以便我们可以稍微增加我们的点数：
- en: '![](img/B15558_04_20.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_20.png)'
- en: 'Figure 4.17: The player has to use the **Ace** as a 1 else they go bust'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.17：玩家必须将**A**视为 1，否则将爆掉
- en: As *Figure 4.17* shows, we **hit** and received a new card, which is an **Ace**.
    Now we can decide the value of **Ace** to be 1 or 11\. If we choose 11, then our
    sum value becomes 10+3+11 = 23\. As we can observe, when we set our ace to 11,
    then our sum value exceeds 21, and we lose the game. Thus, instead of choosing
    **Ace** = 11, we set the **Ace** value to be 1; so our sum value becomes 10+3+1
    = 14.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 4.17*所示，我们**要牌**并获得了一张新牌，它是一张**A**。现在我们可以决定将**A**的点数定为 1 或 11。如果我们选择 11，那么我们的点数将变为
    10+3+11 = 23。如我们所观察到的，当我们将**A**定为 11时，我们的总点数超过了 21，因此我们输了游戏。因此，我们不选择将**A**定为 11，而是将**A**的点数定为
    1，这样我们的总点数就变为 10+3+1 = 14。
- en: Again, we need to decide whether we should (perform the action) hit or stand.
    Let's say we stand hoping that the dealer sum value will be lower than ours. As
    *Figure 4.18* shows, after performing the stand action, both of the dealer's cards
    are shown, and the sum of the dealer's card is 20, but ours is just 14, and so
    we lose the game, and in this case, the **Ace** is called a **non-usable** **Ace**
    since it did not help us to win the game.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们需要决定是执行（执行动作）hit 还是 stand。假设我们选择站立，希望庄家的牌面总和低于我们的牌面总和。正如 *图 4.18* 所示，执行站立动作后，庄家的两张牌都会显示，庄家的牌总和是
    20，而我们的只有 14，所以我们输了游戏，在这种情况下，**Ace** 被称为**不可用的** **Ace**，因为它并没有帮助我们赢得游戏。
- en: '![](img/B15558_04_21.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_21.png)'
- en: 'Figure 4.18: The player has 14, and the dealer has 20 and wins'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.18：玩家的牌是 14，庄家的牌是 20，庄家获胜
- en: '**Case 6: When the game is a draw**'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例 6：当游戏是平局时**'
- en: If both the player and the dealer's sum of cards value is the same, say 20,
    then the game is called a draw.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果玩家和庄家的牌面总和相同，比如都是 20，则游戏称为平局。
- en: Now that we have understood how to play blackjack, let's implement the Monte
    Carlo prediction method in the blackjack game. But before going ahead, first,
    let's learn how the blackjack environment is designed in Gym.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何玩 blackjack，接下来让我们在 blackjack 游戏中实现蒙特卡洛预测方法。但在继续之前，首先让我们了解一下 Gym
    中 blackjack 环境的设计。
- en: The blackjack environment in the Gym library
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Gym 库中的 blackjack 环境
- en: 'Import the Gym library:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 Gym 库：
- en: '[PRE0]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The environment id of blackjack is `Blackjack-v0`. So, we can create the blackjack
    game using the `make` function as shown as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: blackjack 环境的 ID 是 `Blackjack-v0`。因此，我们可以使用如下的 `make` 函数来创建 blackjack 游戏：
- en: '[PRE1]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let''s look at the state of the blackjack environment; we can just reset
    our environment and look at the initial state:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 blackjack 环境的状态；我们可以重置环境并查看初始状态：
- en: '[PRE2]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that every time we run the preceding code, we might get a different result,
    as the initial state is randomly initialized. The preceding code will print something
    like this:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每次运行前面的代码时，我们可能会得到不同的结果，因为初始状态是随机初始化的。前面的代码将打印如下内容：
- en: '[PRE3]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we can observe, our state is represented as a tuple, but what does this mean?
    We learned that in the blackjack game, we will be given two cards and we also
    get to see one of the dealer's cards. Thus, `15` implies that the value of the
    sum of our cards, `9` implies the face value of one of the dealer's cards, `True`
    implies that we have a usable ace, and it will be `False` if we don't have a usable
    ace.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，状态表示为一个元组，那么这意味着什么呢？我们了解到，在 blackjack 游戏中，玩家会得到两张牌，并且还可以看到庄家的其中一张牌。因此，`15`表示我们手中牌的总值，`9`表示庄家一张牌的面值，`True`表示我们有可用的
    Ace，如果没有可用的 Ace，则为`False`。
- en: 'Thus, in the blackjack environment the state is represented as a tuple consisting
    of three values:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在 blackjack 环境中，状态表示为由三个值组成的元组：
- en: The value of the sum of our cards
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们手中牌的总和
- en: The face value of one of the dealer's card
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 庄家一张牌的面值
- en: Boolean value—`True` if we have a useable ace and `False` if we don't have a
    useable ace
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 布尔值——如果我们有可用的 Ace，则为`True`，如果没有可用的 Ace，则为`False`
- en: 'Let''s look at the action space of our blackjack environment:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 blackjack 环境的动作空间：
- en: '[PRE4]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding code will print:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将打印：
- en: '[PRE5]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As we can observe, it implies that we have two actions in our action space,
    which are 0 and 1:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这意味着我们的动作空间中有两个动作，分别是 0 和 1：
- en: The action **stand** is represented by 0
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作**stand**用 0 来表示
- en: The action **hit** is represented by 1
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作**hit**用 1 来表示
- en: 'Okay, what about the reward? The reward will be assigned as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么奖励如何呢？奖励将按照如下方式分配：
- en: '**+1.0** reward if we win the game'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们赢得游戏，奖励为**+1.0**
- en: '**-1.0** reward if we lose the game'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们输掉游戏，奖励为**-1.0**
- en: '**0** reward if the game is a draw'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果游戏是平局，奖励为**0**
- en: Now that we have understood how the blackjack environment is designed in Gym,
    let's start implementing the MC prediction method in the blackjack game. First,
    we will look at every-visit MC and then we will learn how to implement first-visit
    MC prediction.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了 Gym 中 blackjack 环境的设计，接下来让我们在 blackjack 游戏中实现 MC 预测方法。首先，我们将研究每次访问的
    MC，然后学习如何实现首次访问的 MC 预测。
- en: Every-visit MC prediction with the blackjack game
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每次访问的 MC 预测与 blackjack 游戏
- en: 'To understand this section clearly, you should recap the every-visit Monte
    Carlo method we learned earlier. Let''s now understand how to implement every-visit
    MC prediction with the blackjack game step by step:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰理解这一部分，你应该回顾一下我们之前学习的每次访问蒙特卡罗方法。现在，让我们一步步理解如何用黑杰克游戏实现每次访问的蒙特卡罗预测：
- en: 'Import the necessary libraries:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 导入必要的库：
- en: '[PRE6]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Create a blackjack environment:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个黑杰克环境：
- en: '[PRE7]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Defining a policy
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义策略
- en: We learned that in the prediction method, we will be given an input policy and
    we predict the value function of the given input policy. So, now, we first define
    a policy function that acts as an input policy. That is, we define the input policy
    whose value function will be predicted in the upcoming steps.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在预测方法中，我们将获得一个输入策略，并预测该输入策略的价值函数。因此，现在我们首先定义一个策略函数，作为输入策略。也就是说，我们定义了一个输入策略，其价值函数将在接下来的步骤中进行预测。
- en: 'As shown in the following code, our policy function takes the state as an input
    and if the **state[0]**, the sum of our cards, value, is greater than 19, then
    it will return action **0** (stand), else it will return action **1** (hit):'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示的代码，我们的策略函数以状态作为输入，如果**state[0]**，即我们牌的总和，值大于 19，那么它将返回动作**0**（停牌），否则它将返回动作**1**（要牌）：
- en: '[PRE8]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We defined an optimal policy: it makes more sense to perform an action 0 (stand)
    when our sum value is already greater than 19\. That is, when the sum value is
    greater than 19 we don''t have to perform a 1 (hit) action and receive a new card,
    which may cause us to lose the game or bust.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个最优策略：当我们的牌的总和已经大于 19 时，执行动作 0（停牌）更有意义。也就是说，当总和大于 19 时，我们不必执行动作 1（要牌），以避免再拿到一张牌可能导致我们输掉游戏或爆掉。
- en: 'For example, let''s generate an initial state by resetting the environment
    as shown as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们通过重置环境生成一个初始状态，如下所示：
- en: '[PRE9]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Suppose the preceding code prints the following:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 假设前面的代码打印了以下内容：
- en: '[PRE10]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As we can notice, `state[0] = 20`; that is, the value of the sum of our cards
    is 20, so in this case, our policy will return the action 0 (stand) as the following
    shows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所注意到的，`state[0] = 20`；也就是说，我们牌的总和是 20，因此在这种情况下，我们的策略将返回动作 0（停牌），如下所示：
- en: '[PRE11]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The preceding code will print:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将打印：
- en: '[PRE12]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now that we have defined the policy, in the next sections, we will predict the
    value function (state values) of this policy.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了策略，在接下来的部分中，我们将预测该策略的价值函数（状态值）。
- en: Generating an episode
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生成一集
- en: Next, we generate an episode using the given policy, so we define a function
    called `generate_episode`, which takes the policy as an input and generates the
    episode using the given policy.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用给定的策略生成一集，因此我们定义了一个名为`generate_episode`的函数，该函数以策略作为输入，并使用给定的策略生成一集。
- en: 'First, let''s set the number of time steps:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们设定时间步数：
- en: '[PRE13]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'For a clear understanding, let''s look into the function line by line:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地理解，让我们逐行查看这个函数：
- en: '[PRE14]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s define a list called `episode` for storing the episode:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个名为`episode`的列表，用于存储集：
- en: '[PRE15]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Initialize the state by resetting the environment:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重置环境来初始化状态：
- en: '[PRE16]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then for each time step:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于每一个时间步：
- en: '[PRE17]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Select the action according to the given policy:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 根据给定的策略选择动作：
- en: '[PRE18]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Perform the action and store the next state information:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 执行动作并存储下一个状态信息：
- en: '[PRE19]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Store the state, action, and reward into our episode list:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态、动作和奖励存入我们的集列表：
- en: '[PRE20]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If the next state is a final state then break the loop, else update the next
    state to the current state:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如果下一个状态是最终状态，则跳出循环，否则将下一个状态更新为当前状态：
- en: '[PRE21]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let''s take a look at what the output of our `generate_episode` function looks
    like. Note that we generate an episode using the policy we defined earlier:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`generate_episode`函数的输出是什么样子的。请注意，我们使用之前定义的策略生成了一集：
- en: '[PRE22]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The preceding code will print something like the following:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码会打印如下内容：
- en: '[PRE23]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As we can observe our output is in the form of **[(state, action, reward)]**.
    As shown previously, we have two states in our episode. We performed action 1
    (hit) in the state `(10, 2, False)` and received a 0 reward, and we performed
    action 0 (stand) in the state `(20, 2, False)` and received a reward of 1.0.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们的输出是**[(state, action, reward)]**的形式。如前所述，我们的集里有两个状态。在状态 `(10, 2, False)`
    下，我们执行了动作 1（要牌），并且收到了 0 的奖励；在状态 `(20, 2, False)` 下，我们执行了动作 0（停牌），并且收到了 1.0 的奖励。
- en: Now that we have learned how to generate an episode using the given policy,
    next, we will look at how to compute the value of the state (value function) using
    the every-visit MC method.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何使用给定的策略生成回合，接下来，我们将学习如何使用每次访问MC方法计算状态的值（值函数）。
- en: Computing the value function
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算值函数
- en: We learned that in order to predict the value function, we generate several
    episodes using the given policy and compute the value of the state as an average
    return across several episodes. Let's see how to implement that.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，为了预测值函数，我们通过给定的策略生成多个回合，并将状态的值计算为多个回合中的平均回报。让我们看看如何实现这一点。
- en: 'First, we define the `total_return` and `N` as a dictionary for storing the
    total return and the number of times the state is visited across episodes respectively:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将`total_return`和`N`定义为字典，分别用于存储回合中每个状态的总回报和该状态被访问的次数：
- en: '[PRE24]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Set the number of iterations, that is, the number of episodes, we want to generate:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 设置我们希望生成的迭代次数，即回合数：
- en: '[PRE25]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, for every iteration:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于每次迭代：
- en: '[PRE26]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Generate the episode using the given policy; that is, generate an episode using
    the policy function we defined earlier:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 使用给定的策略生成回合；即，使用我们之前定义的策略函数生成回合：
- en: '[PRE27]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Store all the states, actions, and rewards obtained from the episode:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 存储从回合中获得的所有状态、动作和奖励：
- en: '[PRE28]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then, for each step in the episode:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于回合中的每一步：
- en: '[PRE29]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Compute the return `R` of the state as the sum of rewards, *R*(*s*[t]) = sum(rewards[t:]):'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 计算状态的回报`R`，作为奖励的总和，*R*(*s*[t]) = sum(rewards[t:])：
- en: '[PRE30]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Update the `total_return` of the state as total_return(*s*[t]) = total_return(*s*[t])
    + R(*s*[t]):'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态的`total_return`更新为total_return(*s*[t]) = total_return(*s*[t]) + R(*s*[t])：
- en: '[PRE31]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Update the number of times the state is visited in the episode as *N*(*s*[t])
    = *N*(*s*[t]) + 1:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态在回合中被访问的次数更新为*N*(*s*[t]) = *N*(*s*[t]) + 1：
- en: '[PRE32]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: After computing the `total_return` and `N` we can just convert them into a pandas
    data frame for a better understanding. Note that this is just to give a clear
    understanding of the algorithm; we don't necessarily have to convert to the pandas
    data frame, we can also implement this efficiently just by using the dictionary.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算了`total_return`和`N`后，我们可以将它们转换为pandas数据框，以便更好地理解。请注意，这只是为了清晰地理解算法；我们不一定非得转换为pandas数据框，我们也可以仅通过使用字典来高效实现。
- en: 'Convert the `total_returns` dictionary into a data frame:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 将`total_returns`字典转换为数据框：
- en: '[PRE33]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Convert the counter `N` dictionary into a data frame:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 将计数器`N`字典转换为数据框：
- en: '[PRE34]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Merge the two data frames on states:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 按状态合并两个数据框：
- en: '[PRE35]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Look at the first few rows of the data frame:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 查看数据框的前几行：
- en: '[PRE36]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The preceding code will display the following. As we can observe, we have the
    total return and the number of times the state is visited:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将显示如下内容。正如我们所观察到的，我们有状态的总回报和访问次数：
- en: '![](img/B15558_04_22.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_22.png)'
- en: 'Figure 4.19: The total return and the number of times a state has been visited'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.19：状态的总回报和被访问的次数
- en: 'Next, we can compute the value of the state as the average return:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以计算状态的值作为平均回报：
- en: '![](img/B15558_04_056.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_056.png)'
- en: 'Thus, we can write:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以写出：
- en: '[PRE37]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let''s look at the first few rows of the data frame:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一下数据框的前几行：
- en: '[PRE38]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The preceding code will display something like this:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将显示类似这样的内容：
- en: '![](img/B15558_04_23.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_23.png)'
- en: 'Figure 4.20: The value is calculated as the average of the return of each state'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.20：值被计算为每个状态回报的平均值
- en: As we can observe, we now have the value of the state, which is just the average
    of a return of the state across several episodes. Thus, we have successfully predicted
    the value function of the given policy using the every-visit MC method.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们可以观察到的，现在我们有了状态的值，这只是该状态在多个回合中的回报平均值。因此，我们成功地使用每次访问MC方法预测了给定策略的值函数。
- en: Okay, let's check the value of some states and understand how accurately our
    value function is estimated according to the given policy. Recall that when we
    started off, to generate episodes, we used the optimal policy, which selects the
    action 0 (stand) when the sum value is greater than 19 and the action 1 (hit)
    when the sum value is lower than 19\.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，接下来我们检查一些状态的值，并了解根据给定策略我们估算的值函数有多准确。回想一下，当我们开始时，为了生成回合，我们使用了最优策略，当总和大于19时选择动作0（停牌），当总和低于19时选择动作1（要牌）\。
- en: 'Let''s evaluate the value of the state `(21,9,False)`, as we can observe, the
    value of the sum of our cards is already 21 and so this is a good state and should
    have a high value. Let''s see what our estimated value of the state is:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们评估状态 `(21,9,False)` 的值。正如我们所看到的，我们的牌面总和已经是 21，因此这是一个好状态，应该有较高的值。让我们看看我们估算的状态值：
- en: '[PRE39]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The preceding will print something like this:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将打印类似如下内容：
- en: '[PRE40]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As we can observe, the value of the state is high.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所观察到的，状态的值较高。
- en: 'Now, let''s check the value of the state `(5,8,False)`. As we can notice, the
    value of the sum of our cards is just 5 and even the one dealer''s single card
    has a high value, 8; in this case, the value of the state should be lower. Let''s
    see what our estimated value of the state is:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查状态 `(5,8,False)` 的值。正如我们所注意到的，牌面总和仅为 5，即使庄家的单张牌值很高，达到了 8，在这种情况下，状态的值应该较低。让我们看看我们估算的状态值：
- en: '[PRE41]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The preceding code will print something like this:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将打印类似如下内容：
- en: '[PRE42]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: As we can notice, the value of the state is lower.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，状态的值较低。
- en: Thus, we learned how to predict the value function of the given policy using
    the every-visit MC prediction method. In the next section, we will look at how
    to compute the value of the state using the first-visit MC method.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经学会了如何使用每次访问蒙特卡洛预测方法预测给定策略的值函数。在下一节中，我们将学习如何使用首次访问蒙特卡洛方法计算状态的值。
- en: First-visit MC prediction with the blackjack game
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用黑杰克游戏的首次访问蒙特卡洛（MC）预测
- en: 'Predicting the value function using the first-visit MC method is exactly the
    same as how we predicted the value function using the every-visit MC method, except
    that here we compute the return of a state only for its first time of occurrence
    in the episode. The code for first-visit MC is the same as what we have seen in
    every-visit MC except here, we compute the return only for its first time of occurrence
    as shown in the following highlighted code:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 使用首次访问蒙特卡洛方法预测值函数的方法与使用每次访问蒙特卡洛方法预测值函数的方式完全相同，唯一的区别是，这里我们只计算状态第一次在回合中出现时的返回值。首次访问蒙特卡洛的代码与我们在每次访问蒙特卡洛中看到的代码相同，只不过在这里，我们只计算状态第一次出现时的返回值，具体代码如下所示：
- en: '[PRE43]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: You can obtain the complete code from the GitHub repo of the book and you will
    get results similar to what we saw in the every-visit MC section.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从本书的 GitHub 仓库获取完整代码，并将获得与我们在每次访问蒙特卡洛部分看到的类似结果。
- en: Thus, we learned how to predict the value function of the given policy using
    the first-visit and every-visit MC methods.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经学会了如何使用首次访问和每次访问蒙特卡洛方法预测给定策略的值函数。
- en: Incremental mean updates
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增量均值更新
- en: 'In both first-visit MC and every-visit MC, we estimate the value of a state
    as an average (arithmetic mean) return of the state across several episodes as
    shown as follows:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在首次访问蒙特卡洛和每次访问蒙特卡洛中，我们通过计算多个回合中状态的平均返回值（算术均值）来估算状态的值，具体如下所示：
- en: '![](img/B15558_04_056.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_056.png)'
- en: 'Instead of using the arithmetic mean to approximate the value of the state,
    we can also use the incremental mean, and it is expressed as:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅可以使用算术均值来近似状态的值，还可以使用增量均值，公式如下所示：
- en: '![](img/B15558_04_062.png)![](img/B15558_04_063.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_062.png)![](img/B15558_04_063.png)'
- en: 'But why do we need incremental mean? Consider our environment as non-stationary.
    In that case, we don''t have to take the return of the state from all the episodes
    and compute the average. As the environment is non-stationary we can ignore returns
    from earlier episodes and use only the returns from the latest episodes for computing
    the average. Thus, we can compute the value of the state using the incremental
    mean as shown as follows:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我们需要增量均值呢？考虑到我们的环境是非平稳的。在这种情况下，我们不需要将所有回合的状态返回值取平均。由于环境是非平稳的，我们可以忽略早期回合的返回值，仅使用最新回合的返回值来计算平均值。因此，我们可以使用增量均值来计算状态的值，具体如下所示：
- en: '![](img/B15558_04_064.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_064.png)'
- en: Where ![](img/B15558_04_065.png) and *R*[t] is the return of the state *s*[t].
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_04_065.png) 和 *R*[t] 是状态 *s*[t] 的返回值。
- en: MC prediction (Q function)
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蒙特卡洛预测（Q 函数）
- en: So far, we have learned how to predict the value function of the given policy
    using the Monte Carlo method. In this section, we will see how to predict the
    Q function of the given policy using the Monte Carlo method.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学会了如何使用蒙特卡洛方法预测给定策略的值函数。在本节中，我们将看到如何使用蒙特卡洛方法预测给定策略的 Q 函数。
- en: Predicting the Q function of the given policy using the MC method is exactly
    the same as how we predicted the value function in the previous section except
    that here we use the return of the state-action pair, whereas in the case of the
    value function we used the return of the state. That is, just like we approximated
    the value of a state (value function) by computing the average return of the state
    across several episodes, we can also approximate the value of a state-action pair
    (Q function) by computing the average return of the state-action pair across several
    episodes.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 使用蒙特卡罗（MC）方法预测给定策略的Q函数与我们在前一节中预测价值函数的方式完全相同，只是这里我们使用状态-动作对的回报，而在价值函数的情况下，我们使用状态的回报。也就是说，就像我们通过计算多个回合中状态的平均回报来逼近状态的价值（价值函数），我们也可以通过计算多个回合中状态-动作对的平均回报来逼近状态-动作对的价值（Q函数）。
- en: 'Thus, we generate several episodes using the given policy ![](img/B15558_03_140.png),
    then, we calculate the total_return(*s*, *a*), the sum of the return of the state-action
    pair across several episodes, and also we calculate *N*(*s*, *a*), the number
    of times the state-action pair is visited across several episodes. Then we compute
    the Q function or Q value as the average return of the state-action pair as shown
    as follows:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用给定的策略 ![](img/B15558_03_140.png) 生成多个回合，然后计算 total_return(*s*, *a*)，即跨多个回合的状态-动作对回报之和，并且我们还计算
    *N*(*s*, *a*)，即状态-动作对跨多个回合被访问的次数。然后，我们计算Q函数或Q值，即状态-动作对的平均回报，如下所示：
- en: '![](img/B15558_04_067.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_067.png)'
- en: 'For instance, let consider a small example. Say we have two states *s*[0] and
    *s*[1] and we have two possible actions 0 and 1\. Now, we compute total_return(*s*,
    *a*) and *N*(*s*, *a*). Let''s say our table after computation looks like *Table
    4.4*:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个小示例。假设我们有两个状态 *s*[0] 和 *s*[1]，并且我们有两个可能的动作 0 和 1。现在，我们计算 total_return(*s*,
    *a*) 和 *N*(*s*, *a*)。假设我们计算后的表格如下所示，见 *表 4.4*：
- en: '![](img/B15558_04_24.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_24.png)'
- en: 'Table 4.4: The result of two actions in two states'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.4：两个状态中两个动作的结果
- en: 'Once we have this, we can compute the Q value by just taking the average, that
    is:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到这个，就可以通过简单地取平均来计算Q值，即：
- en: '![](img/B15558_04_067.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_067.png)'
- en: 'Thus, we can compute the Q value for all state-action pairs as:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以计算所有状态-动作对的Q值，如下所示：
- en: '![](img/B15558_04_069.png)![](img/B15558_04_070.png)![](img/B15558_04_071.png)![](img/B15558_04_072.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_069.png)![](img/B15558_04_070.png)![](img/B15558_04_071.png)![](img/B15558_04_072.png)'
- en: 'The algorithm for predicting the Q function using the Monte Carlo method is
    as follows. As we can see, it is exactly the same as how we predicted the value function
    using the return of the state except that here we predict the Q function using
    the return of a state-action pair:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 使用蒙特卡罗方法预测Q函数的算法如下所示。如我们所见，它与我们使用状态回报预测价值函数的方式完全相同，只是这里我们使用状态-动作对的回报来预测Q函数：
- en: Let total_return(*s*, *a*) be the sum of the return of a state-action pair across
    several episodes and *N*(*s*, *a*) be the number of times a state-action pair
    is visited across several episodes. Initialize total_return(*s*, *a*) and *N*(*s*,
    *a*) for all state-action pairs to zero. The policy ![](img/B15558_03_185.png)
    is given as input
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让 total_return(*s*, *a*) 为跨多个回合的状态-动作对回报之和，*N*(*s*, *a*) 为状态-动作对跨多个回合被访问的次数。将所有状态-动作对的
    total_return(*s*, *a*) 和 *N*(*s*, *a*) 初始化为零。策略 ![](img/B15558_03_185.png) 作为输入给定。
- en: 'For *M* number of iterations:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *M* 次迭代：
- en: Generate an episode using policy ![](img/B15558_03_084.png)
  id: totrans-377
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略 ![](img/B15558_03_084.png) 生成一个回合
- en: Store all rewards obtained in the episode in the list called rewards
  id: totrans-378
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将回合中获得的所有奖励存储在名为rewards的列表中
- en: 'For each step *t* in the episode:'
  id: totrans-379
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于回合中的每一步 *t*：
- en: Compute return for the state-action pair, *R*(*s*[t], *a*[t]) = sum(rewards[t:])
  id: totrans-380
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算状态-动作对的回报，*R*(*s*[t], *a*[t]) = sum(rewards[t:])
- en: Update the total return of the state-action pair, total_return(*s*[t], *a*[t])
    = total_return(*s*[t], *a*[t]) + R(*s*[t], *a*[t])
  id: totrans-381
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新状态-动作对的总回报，total_return(*s*[t], *a*[t]) = total_return(*s*[t], *a*[t]) + R(*s*[t],
    *a*[t])
- en: Update the counter as *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
  id: totrans-382
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新计数器，*N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
- en: Compute the Q function (Q value) by just taking the average, that is:![](img/B15558_04_067.png)
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过简单地取平均来计算Q函数（Q值），即：![](img/B15558_04_067.png)
- en: Recall that in the MC prediction of the value function, we learned two types
    of MC—first-visit MC and every-visit MC. In first-visit MC, we compute the return
    of the state only for the first time the state is visited in the episode and in
    every-visit MC we compute the return of the state every time the state is visited
    in the episode.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在MC预测值函数时，我们学到了两种类型的MC——首次访问MC和每次访问MC。在首次访问MC中，我们仅计算状态第一次在回合中被访问时的回报，而在每次访问MC中，我们计算状态每次在回合中被访问时的回报。
- en: Similarly, in the MC prediction of the Q function, we have two types of MC—first-visit
    MC and every-visit MC. In first-visit MC, we compute the return of the state-action
    pair only for the first time the state-action pair is visited in the episode and in every-visit
    MC we compute the return of the state-action pair every time the state-action
    pair is visited in the episode.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在MC预测Q函数时，我们有两种类型的MC——首次访问MC和每次访问MC。在首次访问MC中，我们仅计算状态-动作对第一次在回合中被访问时的回报，而在每次访问MC中，我们计算状态-动作对每次在回合中被访问时的回报。
- en: 'As mentioned in the previous section, instead of using the arithmetic mean,
    we can also use the incremental mean. We learned that the value of a state can
    be computed using the incremental mean as:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，除了使用算术平均值外，我们还可以使用增量平均值。我们学到的是，状态的值可以通过增量平均值来计算，公式如下：
- en: '![](img/B15558_04_064.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_064.png)'
- en: 'Similarly, we can also compute the Q value using the incremental mean as shown
    as follows:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们也可以使用增量平均值计算Q值，如下所示：
- en: '![](img/B15558_04_077.png)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_077.png)'
- en: Now that we have learned how to perform the prediction task using the Monte
    Carlo method, in the next section, we will learn how to perform the control task
    using the Monte Carlo method.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何使用蒙特卡洛方法执行预测任务，在接下来的部分，我们将学习如何使用蒙特卡洛方法执行控制任务。
- en: Monte Carlo control
  id: totrans-391
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛控制
- en: In the control task, our goal is to find the optimal policy. Unlike the prediction
    task, here, we will not be given any policy as an input. So, we will begin by
    initializing a random policy, and then we try to find the optimal policy iteratively.
    That is, we try to find an optimal policy that gives the maximum return. In this
    section, we will learn how to perform the control task to find the optimal policy
    using the Monte Carlo method.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制任务中，我们的目标是找到最优策略。与预测任务不同，在这里，我们不会给定任何策略作为输入。因此，我们将从初始化一个随机策略开始，然后我们尝试通过迭代找到最优策略。也就是说，我们尝试找到一个能带来最大回报的最优策略。在本节中，我们将学习如何使用蒙特卡洛方法执行控制任务以找到最优策略。
- en: 'Okay, we learned that in the control task our goal is to find the optimal policy.
    First, how can we compute a policy? We learned that the policy can be extracted
    from the Q function. That is, if we have a Q function, then we can extract policy
    by selecting an action in each state that has the maximum Q value as the following
    shows:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们学到的是，在控制任务中我们的目标是找到最优策略。首先，我们如何计算一个策略？我们学到的是，策略可以从Q函数中提取。也就是说，如果我们有一个Q函数，那么我们可以通过在每个状态中选择具有最大Q值的动作来提取策略，正如下图所示：
- en: '![](img/B15558_04_078.png)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_078.png)'
- en: So, to compute a policy, we need to compute the Q function. But how can we compute
    the Q function? We can compute the Q function similarly to what we learned in
    the MC prediction method. That is, in the MC prediction method, we learned that
    when given a policy, we can generate several episodes using that policy and compute
    the Q function (Q value) as the average return of the state-action pair across
    several episodes.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了计算策略，我们需要计算Q函数。那么，我们如何计算Q函数呢？我们可以像在MC预测方法中那样计算Q函数。也就是说，在MC预测方法中，我们学到的是，当给定一个策略时，我们可以使用该策略生成多个回合，并计算Q函数（Q值），作为状态-动作对在多个回合中的平均回报。
- en: 'We can perform the same step here to compute the Q function. But in the control
    method, we are not given any policy as input. So, we will initialize a random
    policy, and then we compute the Q function using the random policy. That is, just
    like we learned in the prediction method, we generate several episodes using our
    random policy. Then we compute the Q function (Q value) as the average return
    of a state-action pair across several episodes as the following shows:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这里执行相同的步骤来计算Q函数。但在控制方法中，我们并没有给定任何策略作为输入。因此，我们将初始化一个随机策略，然后使用这个随机策略计算Q函数。也就是说，就像我们在预测方法中学到的那样，我们使用随机策略生成多个回合。然后，我们计算Q函数（Q值），作为状态-动作对在多个回合中的平均回报，正如下图所示：
- en: '![](img/B15558_04_067.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_067.png)'
- en: 'Let''s suppose after computing the Q function as the average return of the
    state-action pair, our Q function looks like *Table 4.5*:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在计算Q函数作为状态-动作对的平均回报后，我们的Q函数看起来像*表4.5*：
- en: '![](img/B15558_04_25.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_25.png)'
- en: 'Table 4.5: The Q table'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.5：Q表
- en: From the preceding Q function, we can extract a new policy by selecting an action
    in each state that has the maximum Q value. That is, ![](img/B15558_04_078.png).
    Thus, our new policy selects action 0 in state *s*[0] and action 1 in state *s*[1]
    as it has the maximum Q value.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的Q函数中，我们可以通过选择每个状态下具有最大Q值的动作来提取新的策略。也就是说，![](img/B15558_04_078.png)。因此，我们的新策略在状态*s*[0]选择动作0，在状态*s*[1]选择动作1，因为它们具有最大的Q值。
- en: However, this new policy will not be an optimal policy because this new policy
    is extracted from the Q function, which is computed using the random policy. That
    is, we initialized a random policy and generated several episodes using the random
    policy, then we computed the Q function by taking the average return of the state-action
    pair across several episodes. Thus, we are using the random policy to compute
    the Q function and so the new policy extracted from the Q function will not be
    an optimal policy.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个新策略不一定是最优策略，因为这个新策略是从Q函数中提取的，而Q函数是使用随机策略计算的。也就是说，我们初始化了一个随机策略，并使用该随机策略生成了多个回合，然后通过取多个回合中的状态-动作对的平均回报来计算Q函数。因此，我们使用随机策略计算Q函数，所以从Q函数中提取出的新策略将不会是最优策略。
- en: 'But now that we have extracted a new policy from the Q function, we can use
    this new policy to generate episodes in the next iteration and compute the new
    Q function. Then, from this new Q function, we extract a new policy. We repeat
    these steps iteratively until we find the optimal policy. This is explained clearly
    in the following steps:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，现在我们已经从Q函数中提取出了新的策略，我们可以使用这个新策略在下一次迭代中生成新的回合并计算新的Q函数。然后，从这个新的Q函数中，我们提取出新的策略。我们反复执行这些步骤，直到找到最优策略。以下步骤清晰地解释了这一过程：
- en: '**Iteration 1**—Let ![](img/B15558_04_081.png) be the random policy. We use
    this random policy to generate an episode, and then we compute the Q function
    ![](img/B15558_04_082.png) by taking the average return of the state-action pair.
    Then, from this Q function ![](img/B15558_04_083.png), we extract a new policy
    ![](img/B15558_04_084.png). This new policy ![](img/B15558_04_085.png) will not
    be an optimal policy since it is extracted from the Q function, which is computed
    using the random policy.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '**第1次迭代—**设！[](img/B15558_04_081.png)为随机策略。我们使用这个随机策略生成一个回合，然后通过取状态-动作对的平均回报来计算Q函数！[](img/B15558_04_082.png)。然后，从这个Q函数！[](img/B15558_04_083.png)中，我们提取出新的策略！[](img/B15558_04_084.png)。这个新策略！[](img/B15558_04_085.png)将不会是最优策略，因为它是从Q函数中提取的，而Q函数是使用随机策略计算的。'
- en: '**Iteration 2—**So, we use the new policy ![](img/B15558_04_086.png) derived
    from the previous iteration to generate an episode and compute the new Q function
    ![](img/B15558_04_087.png) as average return of a state-action pair. Then, from
    this Q function ![](img/B15558_04_088.png), we extract a new policy ![](img/B15558_03_158.png).
    If the policy ![](img/B15558_03_158.png) is optimal we stop, else we go to iteration
    3.'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '**第2次迭代—**因此，我们使用从上一次迭代中推导出的新策略！[](img/B15558_04_086.png)来生成回合并计算新的Q函数！[](img/B15558_04_087.png)，作为状态-动作对的平均回报。然后，从这个Q函数！[](img/B15558_04_088.png)中，我们提取出新的策略！[](img/B15558_03_158.png)。如果策略！[](img/B15558_03_158.png)是最优的，我们就停止，否则进入第3次迭代。'
- en: '**Iteration 3—**Now, we use the new policy ![](img/B15558_03_159.png) derived
    from the previous iteration to generate an episode and compute the new Q function
    ![](img/B15558_04_092.png). Then, from this Q function ![](img/B15558_04_092.png),
    we extract a new policy ![](img/B15558_04_094.png). If ![](img/B15558_04_094.png)
    is optimal we stop, else we go to the next iteration.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '**第3次迭代—**现在，我们使用从上一次迭代中推导出的新策略！[](img/B15558_03_159.png)来生成回合并计算新的Q函数！[](img/B15558_04_092.png)。然后，从这个Q函数！[](img/B15558_04_092.png)中，我们提取出新的策略！[](img/B15558_04_094.png)。如果！[](img/B15558_04_094.png)是最优的，我们就停止，否则进入下一次迭代。'
- en: 'We repeat this process for several iterations until we find the optimal policy
    ![](img/B15558_04_096.png) as shown in *Figure 4.21*:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复这个过程进行多次迭代，直到找到最优策略！[](img/B15558_04_096.png)，如*图4.21*所示：
- en: '![](img/B15558_04_097.png)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_097.png)'
- en: 'Figure 4.21: The path to finding the optimal policy'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.21：寻找最优策略的路径
- en: This step is called policy evaluation and improvement and is similar to the
    policy iteration method we covered in *Chapter 3*, *The Bellman Equation and Dynamic
    Programming*. Policy evaluation implies that at each step we evaluate the policy.
    Policy improvement implies that at each step we are improving the policy by taking
    the maximum Q value. Note that here, we select the policy in a greedy manner meaning
    that we are selecting policy ![](img/B15558_03_055.png) by just taking the maximum
    Q value, and so we can call our policy a greedy policy.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步称为策略评估与改进，类似于我们在*第3章*《贝尔曼方程与动态规划》中讨论的策略迭代方法。策略评估意味着在每一步我们评估策略；策略改进意味着在每一步我们通过选择最大Q值来改进策略。请注意，这里我们采用贪婪方式选择策略，即通过选择最大Q值来选择策略
    ![](img/B15558_03_055.png)，因此我们可以称我们的策略为贪婪策略。
- en: Now that we have a basic understanding of how the MC control method works, in
    the next section, we will look into the algorithm of the MC control method and
    learn about it in more detail.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对MC控制方法的基本工作原理有了基本了解，在下一节中，我们将深入探讨MC控制方法的算法，并对其进行更详细的学习。
- en: MC control algorithm
  id: totrans-412
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MC控制算法
- en: The following steps show the Monte Carlo control algorithm. As we can observe,
    unlike the MC prediction method, here, we will not be given any policy. So, we
    start off by initializing the random policy and use the random policy to generate
    an episode in the first iteration. Then, we will compute the Q function (Q value)
    as the average return of the state-action pair.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤展示了蒙特卡洛控制算法。正如我们观察到的，与MC预测方法不同，这里我们不会给出任何策略。因此，我们首先通过初始化随机策略并使用该随机策略生成一个回合作为第一轮迭代。然后，我们将计算Q函数（Q值），作为状态-动作对的平均回报。
- en: Once we have the Q function, we extract a new policy by selecting an action
    in each state that has the maximum Q value. In the next iteration, we use the
    extracted new policy to generate an episode and compute the new Q function (Q
    value) as the average return of the state-action pair. We repeat these steps for
    many iterations to find the optimal policy.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦得到Q函数，我们通过选择每个状态中具有最大Q值的动作来提取新策略。在下一次迭代中，我们使用提取出的新策略生成一个回合，并计算新的Q函数（Q值），作为状态-动作对的平均回报。我们重复这些步骤多次迭代，以找到最优策略。
- en: One more thing, we need to observe that just as we learned in the first-visit
    MC prediction method, here, we compute the return of the state-action pair only
    for the first time a state-action pair is visited in the episode.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一点，我们需要注意，就像我们在首次访问MC预测方法中学习到的那样，这里我们只计算状态-动作对在回合中首次访问时的回报。
- en: 'For a better understanding, we can compare the MC control algorithm with the
    MC prediction of the Q function. One difference we can observe is that, here,
    we compute the Q function in each iteration. But if you notice, in the MC prediction
    of the Q function, we compute the Q function after all the iterations. The reason
    for computing the Q function in every iteration here is that we need the Q function
    to extract the new policy so that we can use the extracted new policy in the next
    iteration to generate an episode:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，我们可以将MC控制算法与Q函数的MC预测进行比较。我们可以观察到的一个区别是，在这里，我们在每次迭代中计算Q函数。但如果你注意到，在Q函数的MC预测中，我们是在所有迭代完成后计算Q函数。这里在每次迭代中计算Q函数的原因是，我们需要Q函数来提取新策略，以便在下一次迭代中使用提取的新策略生成回合：
- en: Let total_return(*s*, *a*) be the sum of the return of a state-action pair across
    several episodes and *N*(*s*, *a*) be the number of times a state-action pair
    is visited across several episodes. Initialize total_return(*s*, *a*) and *N*(*s*,
    *a*) for all state-action pairs to zero and initialize a random policy ![](img/B15558_04_099.png)
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让total_return(*s*, *a*)表示一个状态-动作对在多个回合中的回报总和，*N*(*s*, *a*)表示该状态-动作对在多个回合中被访问的次数。将所有状态-动作对的total_return(*s*,
    *a*)和*N*(*s*, *a*)初始化为零，并初始化一个随机策略 ![](img/B15558_04_099.png)
- en: 'For *M* number of iterations:'
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于*M*次迭代：
- en: Generate an episode using policy ![](img/B15558_04_099.png)
  id: totrans-419
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略生成一个回合 ![](img/B15558_04_099.png)
- en: Store all rewards obtained in the episode in the list called rewards
  id: totrans-420
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将回合中获得的所有奖励存储在名为rewards的列表中
- en: 'For each step *t* in the episode:'
  id: totrans-421
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个回合中的步骤*t*：
- en: 'If (*s*[t], *a*[t]) is occurring for the first time in the episode:'
  id: totrans-422
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果(*s*[t], *a*[t])在该回合中是第一次出现：
- en: Compute the return of a state-action pair, *R*(*s*[t], *a*[t]) = sum(rewards[t:])
  id: totrans-423
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算状态-动作对的回报，*R*(*s*[t], *a*[t]) = sum(rewards[t:])
- en: Update the total return of the state-action pair as, total_return(*s*[t], *a*[t])
    = total_return(*s*[t], *a*[t]) + R(*s*[t], *a*[t])
  id: totrans-424
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新状态-行动对的总回报，公式为：total_return(*s*[t], *a*[t]) = total_return(*s*[t], *a*[t])
    + R(*s*[t], *a*[t])
- en: Update the counter as *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
  id: totrans-425
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新计数器，公式为：*N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
- en: Compute the Q value by just taking the average, that is,![](img/B15558_04_101.png)
  id: totrans-426
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过简单取平均值来计算Q值，即：![](img/B15558_04_101.png)
- en: Compute the new updated policy ![](img/B15558_04_032.png) using the Q function:![](img/B15558_04_078.png)
  id: totrans-427
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Q函数计算新的更新策略 ![](img/B15558_04_032.png)：![](img/B15558_04_078.png)
- en: From the preceding algorithm, we can observe that we generate an episode using
    the policy ![](img/B15558_04_054.png). Then for each step in the episode, we compute
    the return of state-action pair and compute the Q function *Q*(*s*[t], *a*[t])
    as an average return, then from this Q function, we extract a new policy ![](img/B15558_03_038.png).
    We repeat this step iteratively to find the optimal policy ![](img/B15558_03_172.png).
    Thus, we learned how to perform the control task using the Monte Carlo method.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的算法中，我们可以观察到，我们使用策略![](img/B15558_04_054.png)生成一个历程。然后，对于历程中的每一步，我们计算状态-行动对的回报，并计算Q函数*Q*(*s*[t],
    *a*[t])作为平均回报，然后从这个Q函数中提取出新的策略![](img/B15558_03_038.png)。我们重复这一步骤，迭代地找到最优策略![](img/B15558_03_172.png)。这样，我们就学会了如何使用蒙特卡洛方法执行控制任务。
- en: 'We can classify the control methods into two types:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将控制方法分为两类：
- en: On-policy control
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在策略控制
- en: Off-policy control
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 脱策略控制
- en: '**On-policy control—**In the on-policy control method, the agent behaves using
    one policy and also tries to improve the same policy. That is, in the on-policy
    method, we generate episodes using one policy and also improve the same policy
    iteratively to find the optimal policy. For instance, the MC control method, which
    we just learned above, can be called on-policy MC control as we are generating
    episodes using a policy ![](img/B15558_03_139.png), and we also try to improve
    the same policy ![](img/B15558_03_185.png) on every iteration to compute the optimal
    policy.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '**在策略控制—**在在策略控制方法中，智能体使用一种策略进行行为，并尝试改进相同的策略。也就是说，在在策略方法中，我们使用一种策略生成历程，并迭代地改进相同的策略，以找到最优策略。例如，刚刚学习过的MC控制方法可以称为在策略MC控制，因为我们使用策略![](img/B15558_03_139.png)生成历程，并且在每次迭代中尝试改进相同的策略![](img/B15558_03_185.png)，以计算最优策略。'
- en: '**Off-policy control—**In the off-policy control method, the agent behaves
    using one policy *b* and tries to improve a different policy *![](img/B15558_04_099.png)*.
    That is, in the off-policy method, we generate episodes using one policy and we
    try to improve the different policy iteratively to find the optimal policy.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '**脱策略控制—**在脱策略控制方法中，智能体使用一种策略*b*进行行为，并尝试改进另一种策略*![](img/B15558_04_099.png)*。也就是说，在脱策略方法中，我们使用一种策略生成历程，并迭代地尝试改进另一种策略，以找到最优策略。'
- en: We will learn how exactly the preceding two control methods work in detail in
    the upcoming sections.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将详细学习前面两种控制方法的具体工作原理。
- en: On-policy Monte Carlo control
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在策略蒙特卡洛控制
- en: 'There are two types of on-policy Monte Carlo control methods:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的在策略蒙特卡洛控制方法：
- en: Monte Carlo exploring starts
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒙特卡洛探索启动
- en: Monte Carlo with the epsilon-greedy policy
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用epsilon-greedy策略的蒙特卡洛方法
- en: Monte Carlo exploring starts
  id: totrans-439
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 蒙特卡洛探索启动
- en: 'We have already learned how the Monte Carlo control method works. One thing
    we may want to take into account is exploration. There can be several actions
    in a state: some actions will be optimal, while others won''t. To understand whether
    an action is optimal or not, the agent has to explore by performing that action.
    If the agent never explores a particular action in a state, then it will never
    know whether it is a good action or not. So, how can we solve this? That is, how
    can we ensure enough exploration? Here is where Monte Carlo exploring starts helps
    us.'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了蒙特卡洛控制方法的工作原理。有一点我们可能需要考虑的是探索。一个状态下可能有多个动作：有些动作是最优的，而有些则不是。为了理解一个动作是否最优，智能体必须通过执行该动作来进行探索。如果智能体从不探索一个特定的动作，它就永远不知道该动作是否是一个好动作。那么，如何解决这个问题呢？也就是说，我们如何确保足够的探索？这就是蒙特卡洛探索启动帮助我们的地方。
- en: In the MC exploring starts method, we set all state-action pairs to a non-zero
    probability for being the initial state-action pair. So before generating an episode,
    first, we choose the initial state-action pair randomly and then we generate the
    episode starting from this initial state-action pair following the policy ![](img/B15558_03_055.png).
    Then, in every iteration, our policy will be updated as a greedy policy (selecting
    the max Q value; see the next section on *Monte Carlo with the epsilon-greedy
    policy* for more details).
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MC 探索起始状态方法中，我们为所有状态-动作对设置一个非零的概率，使其成为初始状态-动作对。因此，在生成情节之前，首先我们随机选择初始状态-动作对，然后按照策略
    ![](img/B15558_03_055.png) 从该初始状态-动作对开始生成情节。接着，在每次迭代中，我们的策略将更新为贪心策略（选择最大 Q 值；更多细节请参见下一节关于
    *蒙特卡洛与 epsilon-greedy 策略*）。
- en: 'The following steps show the algorithm of MC control exploring starts. It is
    essentially the same as what we learned earlier for the MC control algorithm section,
    except that here, we select an initial state-action pair and generate episodes
    starting from this initial state-action pair as shown in the bold point:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤展示了 MC 控制探索起始状态的算法。它本质上与我们之前在 MC 控制算法部分学到的相同，只是这里我们选择一个初始状态-动作对，并从该初始状态-动作对开始生成情节，如粗体部分所示：
- en: Let total_return(*s*, *a*) be the sum of the return of a state-action pair across
    several episodes and *N*(*s*, *a*) be the number of times a state-action pair
    is visited across several episodes. Initialize total_return(*s*, *a*) and *N*(*s*,
    *a*) for all state-action pairs to zero and initialize a random policy ![](img/B15558_03_084.png)
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 令 total_return(*s*, *a*) 为多个情节中状态-动作对的回报之和，*N*(*s*, *a*) 为多个情节中状态-动作对的访问次数。将所有状态-动作对的
    total_return(*s*, *a*) 和 *N*(*s*, *a*) 初始化为零，并初始化一个随机策略 ![](img/B15558_03_084.png)
- en: 'For *M* number of iterations:'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *M* 次迭代：
- en: '**Select the initial state s****0** **and initial action a****0** **randomly
    such that all state-action pairs have a probability greater than 0**'
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**随机选择初始状态 s****0** **和初始动作 a****0** **，使得所有状态-动作对的概率大于 0**'
- en: Generate an episode from the selected initial state *s*[0] and action *a*[0]
    using policy ![](img/B15558_03_084.png)
  id: totrans-446
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略 ![](img/B15558_03_084.png) 从选择的初始状态 *s*[0] 和初始动作 *a*[0] 开始生成一个情节
- en: Store all the rewards obtained in the episode in the list called rewards
  id: totrans-447
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将情节中获得的所有奖励存储在名为 rewards 的列表中
- en: 'For each step *t* in the episode:'
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于情节中的每一步 *t*：
- en: 'If (*s*[t], *a*[t]) is occurring for the first time in the episode:'
  id: totrans-449
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果 (*s*[t], *a*[t]) 在该情节中第一次出现：
- en: Compute the return of a state-action pair, *R*(*s*[t], *a*[t]) = sum(rewards[t:])
  id: totrans-450
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算状态-动作对的回报，*R*(*s*[t], *a*[t]) = sum(rewards[t:])
- en: Update the total return of the state-action pair as total_return(*s*[t], *a*[t])
    = total_return(*s*[t], *a*[t]) + R(*s*[t], *a*[t])
  id: totrans-451
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新状态-动作对的总回报：total_return(*s*[t], *a*[t]) = total_return(*s*[t], *a*[t]) + R(*s*[t],
    *a*[t])
- en: Update the counter as *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
  id: totrans-452
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新计数器：*N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
- en: Compute the Q value by just taking the average, that is,![](img/B15558_04_101.png)
  id: totrans-453
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过直接取平均值来计算 Q 值，即：![](img/B15558_04_101.png)
- en: Compute the updated policy ![](img/B15558_03_084.png) using the Q function:![](img/B15558_04_078.png)
  id: totrans-454
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Q 函数计算更新后的策略 ![](img/B15558_03_084.png)：![](img/B15558_04_078.png)
- en: One of the major drawbacks of the exploring starts method is that it is not
    applicable to every environment. That is, we can't just randomly choose any state-action
    pair as an initial state-action pair because in some environments there can be
    only one state-action pair that can act as an initial state-action pair. So we
    can't randomly select the state-action pair as the initial state-action pair.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 探索起始状态方法的主要缺点之一是它不适用于每个环境。也就是说，我们不能随机选择任何状态-动作对作为初始状态-动作对，因为在某些环境中，可能只有一个状态-动作对可以作为初始状态-动作对。因此，我们不能随机选择状态-动作对作为初始状态-动作对。
- en: For example, suppose we are training an agent to play a car racing game; we
    can't start the episode in a random position as the initial state and a random
    action as the initial action because we have a fixed single starting state and
    action as the initial state and action.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们正在训练一个智能体玩赛车游戏；我们不能将情节从一个随机位置作为初始状态开始，也不能随机选择一个动作作为初始动作，因为我们有一个固定的单一起始状态和动作作为初始状态和动作。
- en: Thus, to overcome the problem in exploring starts, in the next section, we will
    learn about the Monte Carlo control method with a new type of policy called the
    epsilon-greedy policy.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了解决探索起始状态的问题，在下一节中，我们将学习蒙特卡洛控制方法，采用一种新的策略类型——epsilon-greedy 策略。
- en: Monte Carlo with the epsilon-greedy policy
  id: totrans-458
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带有 epsilon-贪婪策略的蒙特卡洛方法
- en: Before going ahead, first, let us understand what the epsilon-greedy policy
    is as it is ubiquitous in reinforcement learning.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，首先让我们理解什么是 epsilon-贪婪策略，因为它在强化学习中无处不在。
- en: 'First, let''s learn what a greedy policy is. A greedy policy is one that selects
    the best action available at the moment. For instance, let''s say we are in some
    state **A** and we have four possible actions in the state. Let the actions be
    *up, down, left,* and *right*. But let''s suppose our agent has explored only
    two actions, *up* and *right*, in the state **A**; the Q value of actions *up*
    and *right* in the state **A** are shown in *Table 4.6*:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解什么是贪婪策略。贪婪策略是选择当前时刻可用的最佳动作。例如，假设我们处于某个状态**A**，在该状态中有四个可能的动作。设这些动作为*上*、*下*、*左*和*右*。但假设我们的代理只在状态**A**中探索了两个动作，*上*和*右*，这两个动作在状态**A**中的
    Q 值如*表 4.6*所示：
- en: '![](img/B15558_04_26.png)'
  id: totrans-461
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_26.png)'
- en: 'Table 4.6: The agent has only explored two actions in state A'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.6：代理在状态 A 中仅探索了两个动作
- en: We learned that the greedy policy selects the best action available at the moment.
    So the greedy policy checks the Q table and selects the action that has the maximum
    Q value in state **A**. As we can see, the action *up* has the maximum Q value.
    So our greedy policy selects the action *up* in state **A**.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，贪婪策略选择当前时刻可用的最佳动作。因此，贪婪策略会检查 Q 表并选择在状态**A**中具有最大 Q 值的动作。正如我们所看到的，动作*上*具有最大的
    Q 值。所以我们的贪婪策略在状态**A**中选择动作*上*。
- en: But one problem with the greedy policy is that it never explores the other possible
    actions; instead, it always picks the best action available at the moment. In
    the preceding example, the greedy policy always selects the action *up*. But there
    could be other actions in state **A** that might be more optimal than the action
    *up* that the agent has not explored yet. That is, we still have two more actions—*down*
    and *left*—in state **A** that the agent has not explored yet, and they might
    be more optimal than the action *up*.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 但贪婪策略有一个问题，那就是它从不探索其他可能的动作；相反，它总是选择当前时刻可用的最佳动作。在前面的例子中，贪婪策略总是选择动作*上*。但在状态**A**中，可能有其他动作比代理尚未探索的动作*上*更优。也就是说，在状态**A**中，我们仍然有两个未探索的动作——*下*和*左*——它们可能比动作*上*更优。
- en: So, now the question is whether the agent should explore all the other actions
    in the state and select the best action as the one that has the maximum Q value
    or exploit the best action out of already-explored actions. This is called an
    **exploration-exploitation dilemma.**
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在的问题是，代理是否应该探索状态中的所有其他动作，并将具有最大 Q 值的动作作为最佳动作，还是仅从已探索的动作中利用最优动作。这就是所谓的**探索-利用困境**。
- en: Say there are many routes from our work to home and we have explored only two
    routes so far. Thus, to reach home, we can select the route that takes us home
    most quickly out of the two routes we have explored. However, there are still
    many other routes that we have not explored yet that might be even better than
    our current optimal route. The question is whether we should explore new routes
    (exploration) or whether we should always use our current optimal route (exploitation).
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 假设从我们工作地点到家有很多条路线，而到目前为止我们只探索了两条路线。因此，为了回家，我们可以从已探索的两条路线中选择最快的那一条。然而，还有许多其他我们尚未探索的路线，它们可能比我们目前的最优路线更好。问题是我们是否应该探索新路线（探索），还是应该始终使用当前的最优路线（利用）。
- en: To avoid this dilemma, we introduce a new policy called the epsilon-greedy policy.
    Here, all actions are tried with a non-zero probability (epsilon). With a probability
    epsilon, we explore different actions randomly and with a probability 1-epsilon,
    we choose an action that has the maximum Q value. That is, with a probability
    epsilon, we select a random action (exploration) and with a probability 1-epsilon
    we select the best action (exploitation).
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种困境，我们引入了一种新的策略，称为 epsilon-贪婪策略。在这种策略中，所有动作都有一个非零的概率（epsilon）被尝试。以概率 epsilon，我们随机探索不同的动作；以概率
    1-epsilon，我们选择具有最大 Q 值的动作。也就是说，以概率 epsilon，我们选择一个随机动作（探索），以概率 1-epsilon，我们选择最佳动作（利用）。
- en: In the epsilon-greedy policy, if we set the value of epsilon to 0, then it becomes
    a greedy policy (only exploitation), and when we set the value of epsilon to 1,
    then we will always end up doing only the exploration. So, the value of epsilon
    has to be chosen optimally between 0 and 1\.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在epsilon-greedy策略中，如果我们将epsilon的值设置为0，那么它就变成了一个贪婪策略（仅利用），而当我们将epsilon的值设置为1时，我们将总是进行探索。因此，epsilon的值必须在0和1之间进行最佳选择。
- en: Say we set epsilon = 0.5; then we will generate a random number from the uniform
    distribution and if the random number is less than epsilon (0.5), then we select
    a random action (exploration), but if the random number is greater than or equal
    to epsilon then we select the best action, that is, the action that has the maximum
    Q value (exploitation).
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们设置epsilon = 0.5；然后我们将从均匀分布中生成一个随机数，如果该随机数小于epsilon（0.5），则选择一个随机动作（探索）；但如果随机数大于或等于epsilon，则选择最佳动作，也就是具有最大Q值的动作（利用）。
- en: 'So, in this way, we explore actions that we haven''t seen before with the probability
    epsilon and select the best actions out of the explored actions with the probability
    1-epsilon. As *Figure 4.22* shows, if the random number we generated from the
    uniform distribution is less than epsilon, then we choose a random action. If
    the random number is greater than or equal to epsilon, then we choose the best
    action:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过这种方式，我们以epsilon的概率探索我们之前未见过的动作，并以1-epsilon的概率从已探索的动作中选择最佳动作。正如*图 4.22*所示，如果我们从均匀分布中生成的随机数小于epsilon，则选择一个随机动作。如果随机数大于或等于epsilon，则选择最佳动作：
- en: '![](img/B15558_04_27.png)'
  id: totrans-471
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_27.png)'
- en: 'Figure 4.22: Epsilon-greedy policy'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.22：epsilon-greedy策略
- en: 'The following snippet shows the Python code for the epsilon-greedy policy:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了epsilon-greedy策略的Python实现：
- en: '[PRE44]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Now that we have understood what an epsilon-greedy policy is, and how it is
    used to solve the exploration-exploitation dilemma, in the next section, we will
    look at how to use the epsilon-greedy policy in the Monte Carlo control method.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了什么是epsilon-greedy策略，以及它是如何用来解决探索与利用困境的，在下一节中，我们将研究如何在蒙特卡洛控制方法中使用epsilon-greedy策略。
- en: The MC control algorithm with the epsilon-greedy policy
  id: totrans-476
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用epsilon-greedy策略的MC控制算法
- en: 'The algorithm of Monte Carlo control with the epsilon-greedy policy is essentially
    the same as the MC control algorithm we learned earlier except that here we select
    actions based on the epsilon-greedy policy to avoid the exploration-exploitation
    dilemma. The following steps show the algorithm of Monte Carlo with the epsilon-greedy
    policy:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 使用epsilon-greedy策略的蒙特卡洛控制算法本质上与我们之前学习的MC控制算法相同，唯一不同的是，在这里我们基于epsilon-greedy策略选择动作，以避免探索与利用的困境。以下步骤展示了使用epsilon-greedy策略的蒙特卡洛算法：
- en: Let total_return(*s*, *a*) be the sum of the return of a state-action pair across
    several episodes and *N*(*s*, *a*) be the number of times a state-action pair
    is visited across several episodes. Initialize total_return(*s*, *a*) and *N*(*s*,
    *a*) for all state-action pairs to zero and initialize a random policy ![](img/B15558_03_055.png)
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 令 total_return(*s*, *a*) 为某状态-动作对在多个回合中的回报总和，*N*(*s*, *a*) 为该状态-动作对在多个回合中被访问的次数。初始化所有状态-动作对的
    total_return(*s*, *a*) 和 *N*(*s*, *a*) 为零，并初始化一个随机策略！[](img/B15558_03_055.png)
- en: 'For *M* number of iterations:'
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于*M*次迭代：
- en: Generate an episode using policy ![](img/B15558_04_117.png)
  id: totrans-480
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略生成一个回合！[](img/B15558_04_117.png)
- en: Store all rewards obtained in the episode in the list called rewards
  id: totrans-481
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将回合中获得的所有奖励存储在名为 rewards 的列表中
- en: 'For each step *t* in the episode:'
  id: totrans-482
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对回合中的每一步* t *：
- en: 'If (*s*[t], *a*[t]) is occurring for the first time in the episode:'
  id: totrans-483
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果 (*s*[t], *a*[t]) 是回合中第一次出现的情况：
- en: Compute the return of a state-action pair, *R*(*s*[t], *a*[t]) = sum(rewards[t:])
  id: totrans-484
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算状态-动作对的回报，*R*(*s*[t], *a*[t]) = sum(rewards[t:])
- en: Update the total return of the state-action pair as total_return(*s*[t], *a*[t])
    = total_return(*s*[t], *a*[t]) + R(*s*[t], *a*[t])
  id: totrans-485
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新状态-动作对的总回报为 total_return(*s*[t], *a*[t]) = total_return(*s*[t], *a*[t]) +
    R(*s*[t], *a*[t])
- en: Update the counter as *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
  id: totrans-486
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新计数器为 *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
- en: Compute the Q value by just taking the average, that is,![](img/B15558_04_101.png)
  id: totrans-487
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过直接取平均值来计算Q值，即！[](img/B15558_04_101.png)
- en: Compute the updated policy ![](img/B15558_04_119.png) using the Q function.
    Let ![](img/B15558_04_120.png). The policy ![](img/B15558_04_054.png) selects
    the best action ![](img/B15558_04_122.png) with probability ![](img/B15558_04_123.png)
    and random action with probability ![](img/B15558_04_124.png)
  id: totrans-488
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Q 函数计算更新后的策略 ![](img/B15558_04_119.png)。设 ![](img/B15558_04_120.png)。策略 ![](img/B15558_04_054.png)
    以概率 ![](img/B15558_04_123.png) 选择最佳动作 ![](img/B15558_04_122.png)，以概率 ![](img/B15558_04_124.png)
    选择随机动作：
- en: As we can observe, in every iteration, we generate the episode using the policy
    ![](img/B15558_04_125.png) and also we try to improve the same policy ![](img/B15558_03_140.png)
    in every iteration to compute the optimal policy.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所观察到的，在每次迭代中，我们使用策略 ![](img/B15558_04_125.png) 生成回合，并且我们在每次迭代中都尝试改进相同的策略
    ![](img/B15558_03_140.png) 来计算最优策略。
- en: Implementing on-policy MC control
  id: totrans-490
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现基于策略的 MC 控制：
- en: Now, let's learn how to implement the MC control method with the epsilon-greedy
    policy to play the blackjack game; that is, we will see how can we use the MC
    control method to find the optimal policy in the blackjack game.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何使用 epsilon-greedy 策略实现 MC 控制方法来玩黑杰克游戏；也就是说，我们将看到如何使用 MC 控制方法在黑杰克游戏中找到最优策略。
- en: 'First, let''s import the necessary libraries:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，导入必要的库：
- en: '[PRE45]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Create a blackjack environment:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个黑杰克环境：
- en: '[PRE46]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Initialize the dictionary for storing the Q values:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化字典，用于存储 Q 值：
- en: '[PRE47]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Initialize the dictionary for storing the total return of the state-action
    pair:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化字典，用于存储状态-动作对的总回报：
- en: '[PRE48]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Initialize the dictionary for storing the count of the number of times a state-action
    pair is visited:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化字典，用于存储状态-动作对的访问次数：
- en: '[PRE49]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Define the epsilon-greedy policy
  id: totrans-502
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义 epsilon-greedy 策略：
- en: 'We learned that we select actions based on the epsilon-greedy policy, so we
    define a function called `epsilon_greedy_policy`, which takes the state and Q
    value as an input and returns the action to be performed in the given state:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，我们是基于 epsilon-greedy 策略选择动作的，因此我们定义了一个名为 `epsilon_greedy_policy` 的函数，它接受状态和
    Q 值作为输入，并返回在给定状态下要执行的动作：
- en: '[PRE50]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Set the epsilon value to 0.5:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 将 epsilon 值设置为 0.5：
- en: '[PRE51]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Sample a random value from the uniform distribution; if the sampled value is
    less than epsilon then we select a random action, else we select the best action
    that has the maximum Q value as shown:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 从均匀分布中随机采样一个值；如果采样值小于 epsilon，则选择一个随机动作，否则选择具有最大 Q 值的最佳动作，如下所示：
- en: '[PRE52]'
  id: totrans-508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Generating an episode
  id: totrans-509
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生成一个回合：
- en: Now, let's generate an episode using the epsilon-greedy policy. We define a
    function called `generate_episode`, which takes the Q value as an input and returns
    the episode.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 epsilon-greedy 策略生成一个回合。我们定义一个名为 `generate_episode` 的函数，它接受 Q 值作为输入并返回回合。
- en: 'First, let''s set the number of time steps:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，设置时间步数：
- en: '[PRE53]'
  id: totrans-512
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now, let''s define the function:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义函数：
- en: '[PRE54]'
  id: totrans-514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Initialize a list for storing the episode:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化一个列表，用于存储回合：
- en: '[PRE55]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Initialize the state using the `reset` function:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `reset` 函数初始化状态：
- en: '[PRE56]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Then for each time step:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于每个时间步：
- en: '[PRE57]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Select the action according to the epsilon-greedy policy:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 epsilon-greedy 策略选择动作：
- en: '[PRE58]'
  id: totrans-522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Perform the selected action and store the next state information:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 执行所选动作并存储下一个状态信息：
- en: '[PRE59]'
  id: totrans-524
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Store the state, action, and reward in the episode list:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 在回合列表中存储状态、动作和奖励：
- en: '[PRE60]'
  id: totrans-526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'If the next state is the final state then break the loop, else update the next
    state to the current state:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 如果下一个状态是终止状态，则跳出循环，否则将下一个状态更新为当前状态：
- en: '[PRE61]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Computing the optimal policy
  id: totrans-529
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算最优策略：
- en: 'Now, let''s learn how to compute the optimal policy. First, let''s set the
    number of iterations, that is, the number of episodes, we want to generate:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何计算最优策略。首先，设置我们想要生成的迭代次数，即回合数：
- en: '[PRE62]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'For each iteration:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次迭代：
- en: '[PRE63]'
  id: totrans-533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: We learned that in the on-policy control method, we will not be given any policy
    as an input. So, we initialize a random policy in the first iteration and improve
    the policy iteratively by computing the Q value. Since we extract the policy from
    the Q function, we don't have to explicitly define the policy. As the Q value
    improves, the policy also improves implicitly. That is, in the first iteration,
    we generate the episode by extracting the policy (epsilon-greedy) from the initialized
    Q function. Over a series of iterations, we will find the optimal Q function,
    and hence we also find the optimal policy.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在策略控制方法中，我们不会提供任何策略作为输入。因此，在第一次迭代时，我们初始化一个随机策略，并通过计算 Q 值迭代地改进策略。由于我们从
    Q 函数中提取策略，所以不需要显式定义策略。随着 Q 值的改进，策略也会隐式改进。也就是说，在第一次迭代时，我们通过从初始化的 Q 函数中提取策略（epsilon-greedy）来生成回合。经过一系列迭代后，我们会找到最优的
    Q 函数，因此也找到了最优的策略。
- en: 'So, here we pass our initialized Q function to generate an episode:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在这里我们传递初始化的 Q 函数来生成一个回合：
- en: '[PRE64]'
  id: totrans-536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Get all the state-action pairs in the episode:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 获取回合中的所有状态-动作对：
- en: '[PRE65]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Store all the rewards obtained in the episode in the rewards list:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 将回合中获得的所有奖励存储在奖励列表中：
- en: '[PRE66]'
  id: totrans-540
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'For each step in the episode:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 每个回合中的每一步：
- en: '[PRE67]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'If the state-action pair is occurring for the first time in the episode:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 如果状态-动作对在该回合中首次出现：
- en: '[PRE68]'
  id: totrans-544
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Compute the return `R` of the state-action pair as the sum of rewards, *R*(*s*[t],
    *a*[t]) = sum(rewards[t:]):'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 计算状态-动作对的回报 `R`，作为奖励的总和，*R*(*s*[t], *a*[t]) = sum(rewards[t:])：
- en: '[PRE69]'
  id: totrans-546
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Update the total return of the state-action pair as total_return(*s*[t], *a*[t])
    = total_return(*s*[t], *a*[t]) + R(*s*[t], *a*[t]):'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 更新状态-动作对的总回报为 total_return(*s*[t], *a*[t]) = total_return(*s*[t], *a*[t]) +
    R(*s*[t], *a*[t])：
- en: '[PRE70]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Update the number of times the state-action pair is visited as *N*(*s*[t],
    *a*[t]) = *N*(*s*[t], *a*[t]) + 1:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 更新状态-动作对被访问的次数为 *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1：
- en: '[PRE71]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Compute the Q value by just taking the average, that is,
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简单地取平均值来计算 Q 值，即：
- en: '![](img/B15558_04_127.png)'
  id: totrans-552
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_127.png)'
- en: '[PRE72]'
  id: totrans-553
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Thus on every iteration, the Q value improves and so does the policy.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在每次迭代中，Q 值都会改进，策略也会随之改进。
- en: 'After all the iterations, we can have a look at the Q value of each state-action
    pair in the pandas data frame for more clarity. First, let''s convert the Q value
    dictionary into a pandas data frame:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 所有迭代结束后，我们可以查看 pandas 数据框中每个状态-动作对的 Q 值，以获得更清晰的了解。首先，让我们将 Q 值字典转换为 pandas 数据框：
- en: '[PRE73]'
  id: totrans-556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Let''s look at the first few rows of the data frame:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下数据框的前几行：
- en: '[PRE74]'
  id: totrans-558
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '![](img/B15558_04_28.png)'
  id: totrans-559
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_28.png)'
- en: 'Figure 4.23: The Q values of the state-action pairs'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.23：状态-动作对的 Q 值
- en: As we can observe, we have the Q values for all the state-action pairs. Now
    we can extract the policy by selecting the action that has the maximum Q value
    in each state. For instance, say we are in the state `(21,8, True)`. Now, should
    we perform action 0 (stand) or action 1 (hit)? It makes more sense to perform
    action 0 (stand) here, since the value of the sum of our cards is already 21,
    and if we perform action 1 (hit) our game will lead to a bust.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所观察到的，我们得到了所有状态-动作对的 Q 值。现在我们可以通过选择每个状态中具有最大 Q 值的动作来提取策略。例如，假设我们在状态 `(21,8,
    True)` 中。现在，我们应该执行动作 0（stand）还是动作 1（hit）？在这里执行动作 0（stand）更有意义，因为我们卡牌的总值已经是 21，如果执行动作
    1（hit），游戏就会爆掉。
- en: Note that due to stochasticity, you might get different results than those shown
    here.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于随机性，你可能会得到与这里显示的不同的结果。
- en: 'Let''s look at the Q values of all the actions in this state, `(21,8, True)`:'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看该状态 `(21,8, True)` 中所有动作的 Q 值：
- en: '[PRE75]'
  id: totrans-564
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The preceding code will print the following:'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将打印出以下内容：
- en: '![](img/B15558_04_29.png)'
  id: totrans-566
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_29.png)'
- en: 'Figure 4.24: The Q values of the state (21,8, True)'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.24：状态 (21,8, True) 的 Q 值
- en: As we can observe, we have a maximum Q value for action 0 (stand) compared to
    action 1 (hit). So, we perform action 0 in the state `(21,8, True)`. Similarly,
    in this way, we can extract the policy by selecting the action in each state that
    has the maximum Q value.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所观察到的，与动作 1（hit）相比，动作 0（stand）的 Q 值最大。因此，我们在状态 `(21,8, True)` 下执行动作 0。同样地，通过这种方式，我们可以通过选择每个状态中具有最大
    Q 值的动作来提取策略。
- en: In the next section, we will learn about an off-policy control method that uses
    two different policies.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习一种使用两种不同策略的非策略控制方法。
- en: Off-policy Monte Carlo control
  id: totrans-570
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非策略蒙特卡洛控制
- en: Off-policy Monte Carlo is another interesting Monte Carlo control method. In
    the off-policy method, we use two policies called the behavior policy and the
    target policy. As the name suggests, we behave (generate episodes) using the behavior
    policy and we try to improve the other policy called the target policy.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 离策略蒙特卡洛方法是另一种有趣的蒙特卡洛控制方法。在离策略方法中，我们使用两个策略，分别称为行为策略和目标策略。顾名思义，我们使用行为策略来进行操作（生成情节），而我们试图改进另一种策略，称为目标策略。
- en: In the on-policy method, we generate an episode using the policy ![](img/B15558_03_140.png)
    and we improve the same policy ![](img/B15558_04_129.png) iteratively to find
    the optimal policy. But in the off-policy method, we generate an episode using
    a policy called the behavior policy *b* and we try to iteratively improve a different
    policy called the target policy ![](img/B15558_04_130.png).
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 在在策略方法中，我们使用策略 ![](img/B15558_03_140.png)生成情节，并且我们迭代地改进相同的策略 ![](img/B15558_04_129.png)，以找到最优策略。但在离策略方法中，我们使用一个名为行为策略*b*的策略生成情节，并且我们试图迭代地改进一个不同的策略，称为目标策略
    ![](img/B15558_04_130.png)。
- en: That is, in the on-policy method, we learned that the agent generates an episode
    using the policy ![](img/B15558_03_008.png). Then for each step in the episode,
    we compute the return of the state-action pair and compute the Q function *Q*(*s*[t],
    *a*[t]) as an average return, then from this Q function, we extract a new policy
    ![](img/B15558_03_055.png). We repeat this step iteratively to find the optimal
    policy ![](img/B15558_04_032.png).
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，在在策略方法中，我们已经了解到，智能体使用策略 ![](img/B15558_03_008.png)生成情节。然后，对于情节中的每个步骤，我们计算状态-动作对的回报，并将Q函数*Q*(*s*[t],
    *a*[t])作为平均回报来计算，然后从这个Q函数中提取一个新策略 ![](img/B15558_03_055.png)。我们重复这一步骤，迭代地找到最优策略
    ![](img/B15558_04_032.png)。
- en: But in the off-policy method, the agent generates an episode using a policy
    called the behavior policy *b*. Then for each step in the episode, we compute
    the return of the state-action pair and compute the Q function *Q*(*s*[t], *a*[t])
    as an average return, then from this Q function, we extract a new policy called
    the target policy ![](img/B15558_03_084.png). We repeat this step iteratively
    to find the optimal target policy ![](img/B15558_04_032.png).
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 但是在离策略方法中，智能体使用称为行为策略*b*的策略生成情节。然后，对于情节中的每个步骤，我们计算状态-动作对的回报，并将Q函数*Q*(*s*[t],
    *a*[t])作为平均回报来计算，然后从这个Q函数中提取一个新策略，称为目标策略 ![](img/B15558_03_084.png)。我们重复这一步骤，迭代地找到最优的目标策略
    ![](img/B15558_04_032.png)。
- en: The behavior policy will usually be set to the epsilon-greedy policy and thus
    the agent explores the environment with the epsilon-greedy policy and generates
    an episode. Unlike the behavior policy, the target policy is set to be the greedy
    policy and so the target policy will always select the best action in each state.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 行为策略通常设置为ε-greedy策略，因此智能体会使用ε-greedy策略探索环境并生成情节。与行为策略不同，目标策略设置为贪婪策略，因此目标策略在每个状态下始终选择最佳动作。
- en: 'Let''s now understand how the off-policy Monte Carlo method works exactly.
    First, we will initialize the Q function with random values. Then we generate
    an episode using the behavior policy, which is the epsilon-greedy policy. That
    is, from the Q function we select the best action (the action that has the max
    Q value) with probability 1-epsilon and we select the random action with probability
    epsilon. Then for each step in the episode, we compute the return of the state-action
    pair and compute the Q function *Q*(*s*[t], *a*[t]) as an average return. Instead
    of using the arithmetic mean to compute the Q function, we can use the incremental
    mean. We can compute the Q function using the incremental mean as shown as follows:'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们具体了解离策略蒙特卡洛方法是如何工作的。首先，我们将Q函数初始化为随机值。然后，我们使用行为策略生成情节，行为策略为ε-greedy策略。也就是说，从Q函数中，我们以1-ε的概率选择最佳动作（即具有最大Q值的动作），以ε的概率选择随机动作。然后，对于情节中的每个步骤，我们计算状态-动作对的回报，并将Q函数*Q*(*s*[t],
    *a*[t])作为平均回报来计算。为了计算Q函数，我们可以使用增量均值，而不是使用算术均值。我们可以按照以下方式使用增量均值来计算Q函数：
- en: '![](img/B15558_04_077.png)'
  id: totrans-577
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_077.png)'
- en: 'After computing the Q function, we extract the target policy ![](img/B15558_03_140.png)
    by selecting an action in each state that has the maximum Q value as shown as
    follows:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算Q函数后，我们通过选择每个状态下具有最大Q值的动作来提取目标策略 ![](img/B15558_03_140.png)，如下所示：
- en: '![](img/B15558_04_138.png)'
  id: totrans-579
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_138.png)'
- en: 'The algorithm is given as follows:'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 算法如下所示：
- en: Initialize the Q function *Q*(*s*, *a*) with random values, set the behavior
    policy *b* to be epsilon-greedy, and also set the target policy ![](img/B15558_03_055.png)
    to be greedy policy.
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Q 函数 *Q*(*s*, *a*) 初始化为随机值，将行为策略 *b* 设置为ε-贪婪策略，同时将目标策略 ![](img/B15558_03_055.png)
    设置为贪婪策略。
- en: 'For *M* number of episodes:'
  id: totrans-582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *M* 次回合：
- en: Generate an episode using the behavior policy *b*
  id: totrans-583
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用行为策略 *b* 生成一个回合
- en: Initialize return *R* to 0
  id: totrans-584
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化返回值 *R* 为 0
- en: 'For each step *t* in the episode, *t* = *T*-1, *T*-2,…, 0:'
  id: totrans-585
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于回合中的每一步 *t*，*t* = *T*-1, *T*-2,…, 0：
- en: Compute the return as *R* = *R*+*r*[t][+1]
  id: totrans-586
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算返回值为 *R* = *R*+*r*[t][+1]
- en: Compute the Q value as ![](img/B15558_04_077.png)
  id: totrans-587
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 Q 值为 ![](img/B15558_04_077.png)
- en: Compute the target policy ![](img/B15558_04_138.png)
  id: totrans-588
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标策略 ![](img/B15558_04_138.png)
- en: Return the target policy ![](img/B15558_04_142.png)
  id: totrans-589
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回目标策略 ![](img/B15558_04_142.png)
- en: As we can observe from the preceding algorithm, first we set the Q values of
    all the state-action pairs to random values and then we generate an episode using
    the behavior policy. Then on each step of the episode, we compute the updated
    Q function (Q values) using the incremental mean and then we extract the target
    policy from the updated Q function. As we can notice, on every iteration, the
    Q function is constantly improving and since we are extracting the target policy
    from the Q function, our target policy will also be improving on every iteration.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从前面的算法中观察到的，首先我们将所有状态-动作对的 Q 值设置为随机值，然后使用行为策略生成一个回合。接着在每个回合的每一步，我们使用增量平均来计算更新后的
    Q 函数（Q 值），然后从更新后的 Q 函数中提取目标策略。正如我们所注意到的，在每次迭代中，Q 函数在不断改进，且由于我们是从 Q 函数中提取目标策略，我们的目标策略在每次迭代中也会不断改进。
- en: Also, note that since it is an off-policy method, the episode is generated using
    the behavior policy and we try to improve the target policy.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 同时需要注意的是，由于这是一种非策略方法，回合是使用行为策略生成的，而我们则尝试改进目标策略。
- en: But wait! There is a small issue here. Since we are finding the target policy
    ![](img/B15558_04_142.png) from the Q function, which is computed based on the
    episodes generated by a different policy called the behavior policy, our target
    policy will be inaccurate. This is because the distribution of the behavior policy
    and the target policy will be different. So, to correct this, we introduce a new
    technique called **importance sampling**. This is a technique for estimating the
    values of one distribution when given samples from another.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 但是等等！这里有一个小问题。由于我们是从 Q 函数中找到目标策略 ![](img/B15558_04_142.png)，而 Q 函数是基于由另一种策略——行为策略生成的回合计算出来的，因此我们的目标策略会不准确。原因是行为策略和目标策略的分布会有所不同。因此，为了纠正这一点，我们引入了一种新的技术，叫做**重要性采样**。这是一种通过给定来自另一个分布的样本来估计一个分布的值的技术。
- en: 'Let us say we want to compute the expectation of a function *f*(*x*) where
    the value of *x* is sampled from the distribution *p*(*x*) that is, ![](img/B15558_04_143.png);
    then we can write:'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要计算一个函数 *f*(*x*) 的期望，其中 *x* 的值是从分布 *p*(*x*) 中采样的，也就是 ![](img/B15558_04_143.png)；那么我们可以写成：
- en: '![](img/B15558_04_144.png)'
  id: totrans-594
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_144.png)'
- en: 'With the importance sampling method, we estimate the expectation using a different
    distribution *q*(*x*); that is, instead of sampling *x* from *p*(*x*) we use a
    different distribution *q*(*x*) as shown as follows:'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 使用重要性采样方法，我们通过一个不同的分布 *q*(*x*) 来估计期望；也就是说，我们不从 *p*(*x*) 中采样 *x*，而是使用一个不同的分布
    *q*(*x*)，如下面所示：
- en: '![](img/B15558_04_145.png)'
  id: totrans-596
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_145.png)'
- en: The ratio ![](img/B15558_04_146.png) is called the importance sampling ratio
    or importance correction.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 该比率 ![](img/B15558_04_146.png) 被称为重要性采样比率或重要性修正。
- en: Okay, how does importance sampling help us? We learned that with importance
    sampling, we can estimate the value of one distribution by sampling from another
    using the importance sampling ratio. In off-policy control, we can estimate the
    target policy with the samples (episodes) from the behavior policy using the importance
    sampling ratio.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，重要性采样怎么帮助我们呢？我们了解到，使用重要性采样，我们可以通过使用重要性采样比率从另一个分布中采样来估计一个分布的值。在非策略控制中，我们可以使用来自行为策略的样本（回合）通过重要性采样比率来估计目标策略。
- en: 'Importance sampling has two types:'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 重要性采样有两种类型：
- en: Ordinary importance sampling
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 普通重要性采样
- en: Weighted importance sampling
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加权重要性采样
- en: In ordinary importance sampling, the importance sampling ratio will be the ratio
    of the target policy to the behavior policy ![](img/B15558_04_147.png) and in
    weighted importance sampling, the importance sampling ratio will be the weighted
    ratio of the target policy to the behavior policy ![](img/B15558_04_148.png).
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 在普通重要性采样中，重要性采样比率将是目标策略与行为策略的比率 ![](img/B15558_04_147.png)，而在加权重要性采样中，重要性采样比率将是目标策略与行为策略的加权比率
    ![](img/B15558_04_148.png)。
- en: 'Let''s now understand how we use weighted importance sampling in the off-policy
    Monte Carlo method. Let *W* be the weight and *C*(*s*[t], *a*[t]) denote the cumulative
    sum of weights across all the episodes. We learned that we compute the Q function
    (Q values) using the incremental mean as:'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来理解如何在脱策略蒙特卡罗方法中使用加权重要性采样。设*W*为权重，*C*(*s*[t], *a*[t])表示所有回合的权重累加和。我们了解到，计算Q函数（Q值）时使用增量平均值，如下所示：
- en: '![](img/B15558_04_077.png)'
  id: totrans-604
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_077.png)'
- en: 'Now, we slightly modify our Q function computation with the weighted importance
    sampling as shown as follows:'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们略微修改了Q函数计算，采用加权重要性采样，如下所示：
- en: '![](img/B15558_04_150.png)'
  id: totrans-606
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_04_150.png)'
- en: The algorithm of the off-policy Monte Carlo method is shown next. First, we
    generate an episode using the behavior policy and then we initialize return *R*
    to 0 and the weight *W* to 1\. Then on every step of the episode, we compute the
    return and update the cumulative weight as *C*(*s*[t], *a*[t]) = *C*(*s*[t], *a*[t])
    + *W*. After updating the cumulative weights, we update the Q value as ![](img/B15558_04_150.png).
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 脱策略蒙特卡罗方法的算法如下。首先，我们使用行为策略生成一个回合，然后将返回值*R*初始化为0，权重*W*初始化为1。接着，在回合的每一步，我们计算返回值并更新累积权重为
    *C*(*s*[t], *a*[t]) = *C*(*s*[t], *a*[t]) + *W*。更新累积权重后，我们更新Q值为 ![](img/B15558_04_150.png)。
- en: From the Q value, we extract the target policy as ![](img/B15558_04_138.png).
    When the action *a*[t] given by the behavior policy and the target policy is not
    the same then we break the loop and generate the next episode; else we update
    the weight as ![](img/B15558_04_153.png).
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 从Q值中，我们提取目标策略为 ![](img/B15558_04_138.png)。当行为策略和目标策略给定的动作 *a*[t] 不同，则退出循环并生成下一个回合；否则，我们将权重更新为
    ![](img/B15558_04_153.png)。
- en: 'The complete algorithm of the off-policy Monte Carlo method is explained in
    the following steps:'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 脱策略蒙特卡罗方法的完整算法在以下步骤中进行了说明：
- en: Initialize the Q function *Q*(*s*, *a*) with random values, set the behavior
    policy *b* to be epsilon-greedy, and target policy ![](img/B15558_03_084.png)
    to be greedy policy and initialize the cumulative weights as *C*(*s*, *a*) = 0
  id: totrans-610
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Q函数 *Q*(*s*, *a*) 初始化为随机值，将行为策略 *b* 设置为epsilon-贪婪策略，目标策略 ![](img/B15558_03_084.png)
    设置为贪婪策略，并将累积权重初始化为 *C*(*s*, *a*) = 0
- en: 'For *M* number of episodes:'
  id: totrans-611
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于*M*个回合：
- en: Generate an episode using the behavior policy *b*
  id: totrans-612
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用行为策略 *b* 生成一个回合
- en: Initialize return *R* to 0 and weight *W* to 1
  id: totrans-613
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将返回值*R*初始化为0，权重*W*初始化为1
- en: 'For each step *t* in the episode, *t* = *T*-1, *T*-2,…, 0:'
  id: totrans-614
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于回合中的每一步 *t*，*t* = *T*-1, *T*-2,…, 0：
- en: Compute the return as *R* = *R*+*r*[t][+1]
  id: totrans-615
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将返回值计算为 *R* = *R* + *r*[t][+1]
- en: Update the cumulative weights *C*(*s*[t], *a*[t]) = *C*(*s*[t], *a*[t]) + *W*
  id: totrans-616
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新累积权重 *C*(*s*[t], *a*[t]) = *C*(*s*[t], *a*[t]) + *W*
- en: Update the Q value as ![](img/B15558_04_150.png)
  id: totrans-617
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新Q值，如 ![](img/B15558_04_150.png)
- en: Compute the target policy ![](img/B15558_04_138.png)
  id: totrans-618
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标策略 ![](img/B15558_04_138.png)
- en: If ![](img/B15558_04_157.png) then break
  id: totrans-619
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 ![](img/B15558_04_157.png)，则退出
- en: Update the weight as ![](img/B15558_04_153.png)
  id: totrans-620
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新权重为 ![](img/B15558_04_153.png)
- en: Return the target policy ![](img/B15558_03_139.png)
  id: totrans-621
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回目标策略 ![](img/B15558_03_139.png)
- en: Is the MC method applicable to all tasks?
  id: totrans-622
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MC方法适用于所有任务吗？
- en: We learned that Monte Carlo is a model-free method, and so it doesn't require
    the model dynamics of the environment to compute the value and Q function in order
    to find the optimal policy. The Monte Carlo method computes the value function
    and Q function by just taking the average return of the state and the average
    return of the state-action pair, respectively.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，蒙特卡罗是一种无模型方法，因此它不需要环境的模型动态来计算值函数和Q函数，从而找到最优策略。蒙特卡罗方法通过仅计算状态的平均返回值和状态-动作对的平均返回值，分别来计算值函数和Q函数。
- en: But one issue with the Monte Carlo method is that it is applicable only to episodic
    tasks. We learned that in the Monte Carlo method, we compute the value of the
    state by taking the average return of the state and the return is the sum of rewards
    of the episode. But when there is no episode, that is, if our task is a continuous
    task (non-episodic task), then we cannot apply the Monte Carlo method.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 但是蒙特卡洛方法的一个问题是，它仅适用于回合任务。我们了解到，在蒙特卡洛方法中，我们通过取状态的平均回报来计算状态的值，而回报是回合的奖励总和。但是当没有回合时，也就是当我们的任务是连续任务（非回合任务）时，就无法应用蒙特卡洛方法。
- en: Okay, how do we compute the value of the state where we have a continuous task
    and also where we don't know the model dynamics of the environment? Here is where
    we use another interesting model-free method called temporal difference learning.
    In the next chapter, we will learn exactly how temporal difference learning works.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，当我们有一个连续任务并且不知道环境的模型动态时，如何计算该状态的值呢？在这里，我们使用另一种有趣的无模型方法，称为时间差分学习。下一章，我们将准确了解时间差分学习是如何工作的。
- en: Summary
  id: totrans-626
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started the chapter by understanding what the Monte Carlo method is. We learned
    that in the Monte Carlo method, we approximate the expectation of a random variable
    by sampling, and when the sample size is greater, the approximation will be better.
    Then we learned about the prediction and control tasks. In the prediction task,
    we evaluate the given policy by predicting the value function or Q function, which
    helps us to understand the expected return an agent would get if it uses the given
    policy. In the control task, our goal is to find the optimal policy, and we will
    not be given any policy as input, so we start by initializing a random policy
    and we try to find the optimal policy iteratively.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过理解蒙特卡洛方法开始了本章的学习。我们了解到，在蒙特卡洛方法中，我们通过采样来近似随机变量的期望，当样本量更大时，近似结果会更好。接着，我们学习了预测和控制任务。在预测任务中，我们通过预测价值函数或Q函数来评估给定的策略，这有助于我们理解如果智能体使用给定的策略，预期会得到的回报。在控制任务中，我们的目标是找到最优策略，且不会给出任何策略作为输入，因此我们从初始化一个随机策略开始，并通过迭代方法寻找最优策略。
- en: Moving forward, we learned how to use the Monte Carlo method to perform the
    prediction task. We learned that the value of a state and the value of a state-action
    pair can be computed by just taking the average return of the state and an average
    return of state-action pair across several episodes, respectively.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们学习了如何使用蒙特卡洛方法执行预测任务。我们了解到，状态的值和状态-动作对的值可以通过分别取状态和状态-动作对在多个回合中的平均回报来计算。
- en: We also learned about the first-visit MC and every-visit MC methods. In first-visit
    MC, we compute the return only for the first time the state is visited in the
    episode, and in every-visit MC, we compute the return every time the state is
    visited in the episode.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学习了首次访问MC和每次访问MC方法。在首次访问MC中，我们仅计算状态在回合中第一次访问时的回报，而在每次访问MC中，我们每次访问状态时都会计算回报。
- en: Following this, we explored how to perform a control task using the Monte Carlo
    method. We learned about two different types of control methods—on-policy and
    off-policy control.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们探讨了如何使用蒙特卡洛方法执行控制任务。我们学习了两种不同的控制方法——在策略控制和离策略控制。
- en: In the on-policy method, we generate episodes using one policy and also improve
    the same policy iteratively to find the optimal policy. We first learned about
    the Monte Carlo control exploring starts method where we set all the state-action
    pairs to a non-zero probability to ensure exploration. Later, we learned about
    Monte Carlo control with an epsilon-greedy policy where we select a random action
    (exploration) with probability epsilon, and with probability 1-epsilon we select
    the best action (exploitation).
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 在策略梯度方法中，我们使用一种策略生成回合，并且通过迭代改进同一策略来找到最优策略。我们首先学习了蒙特卡洛控制探索起始方法，在这种方法中，我们将所有状态-动作对的概率设置为非零概率，以确保进行探索。后来，我们学习了使用ε-贪婪策略的蒙特卡洛控制方法，在这种方法中，我们以概率ε选择一个随机动作（探索），以概率1-ε选择最优动作（利用）。
- en: At the end of the chapter, we discussed the off-policy Monte Carlo control method
    where we use two different policies called the behavior policy, for generating
    the episode, and the target policy, for finding the optimal policy.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后，我们讨论了离策略蒙特卡洛控制方法，在这种方法中，我们使用两种不同的策略：行为策略（用于生成回合）和目标策略（用于找到最优策略）。
- en: Questions
  id: totrans-633
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s assess our knowledge of the Monte Carlo methods by answering the following
    questions:'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回答以下问题来评估我们对蒙特卡洛方法的理解：
- en: What is the Monte Carlo method?
  id: totrans-635
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是蒙特卡洛方法？
- en: Why is the Monte Carlo method preferred over dynamic programming?
  id: totrans-636
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么蒙特卡洛方法比动态规划更受青睐？
- en: How do prediction tasks differ from control tasks?
  id: totrans-637
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测任务与控制任务有什么区别？
- en: How does the MC prediction method predict the value function?
  id: totrans-638
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 蒙特卡洛预测方法是如何预测价值函数的？
- en: What is the difference between first-visit MC and every-visit MC?
  id: totrans-639
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首次访问蒙特卡洛（first-visit MC）与每次访问蒙特卡洛（every-visit MC）有何不同？
- en: Why do we use incremental mean updates?
  id: totrans-640
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们使用增量均值更新（incremental mean updates）？
- en: How does on-policy control differ from off-policy control?
  id: totrans-641
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于策略控制（on-policy control）与离策略控制（off-policy control）有什么区别？
- en: What is the epsilon-greedy policy?
  id: totrans-642
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是ε-贪婪策略（epsilon-greedy policy）？
