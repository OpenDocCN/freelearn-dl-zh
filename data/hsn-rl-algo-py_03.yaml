- en: Implementing RL Cycle and OpenAI Gym
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现RL循环和OpenAI Gym
- en: In every machine learning project, an algorithm learns rules and instructions
    from a training dataset, with a view to performing a task better. In **reinforcement
    learning** (**RL**), the algorithm is called the agent, and it learns from the
    data provided by an environment. Here, the environment is a continuous source
    of information that returns data according to the agent's actions. And, because
    the data returned by an environment could be potentially infinite, there are many
    conceptual and practical differences among the supervised settings that arise
    while training. For the purpose of this chapter, however, it is important to highlight
    the fact that different environments not only provide different tasks to accomplish,
    but can also have different types of input, output, and reward signals, while
    also requiring the adaptation of the algorithm in each case. For example, a robot
    could either sense its state from a visual input, such as an RGB camera, or from
    discrete internal sensors.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个机器学习项目中，算法从训练数据集中学习规则和指令，以更好地执行任务。在**强化学习**（**RL**）中，算法被称为代理程序，并从环境提供的数据中学习。在这里，环境是一个连续提供信息的来源，根据代理的动作返回数据。由于环境返回的数据可能是潜在无限的，因此在训练时会涉及许多监督设置中出现的概念和实际差异。但是，对于本章的目的，重要的是强调不同的环境不仅提供不同的任务完成，还可能具有不同类型的输入、输出和奖励信号，同时需要在每种情况下调整算法。例如，机器人可以通过视觉输入（如RGB摄像头）或离散的内部传感器感知其状态。
- en: In this chapter, you'll set up the environment required to code RL algorithms
    and build your first algorithm. Despite being a simple algorithm that plays CartPole,
    it offers a useful baseline to master the basic RL cycle before moving on to more
    advanced RL algorithms. Also, because, in the later chapters, you'll code many
    deep neural networks, here, we'll give you a brief recap about TensorFlow and
    introduce TensorBoard, a visualization tool.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将设置编写强化学习算法所需的环境，并构建您的第一个算法。尽管这只是一个玩CartPole的简单算法，但在深入研究更高级别的强化学习算法之前，它提供了一个有用的基准。此外，在后续章节中，您将编写许多深度神经网络，因此在这里我们将简要回顾TensorFlow并介绍TensorBoard，一个可视化工具。
- en: Almost all the environments used throughout the book are based on the interface
    open sourced by OpenAI called **Gym**. Therefore, we'll take a look at it and
    use some of its built-in environments. Then, before moving on to an in-depth examination
    of RL algorithms in the next chapters, we'll list and explain the strengths and
    differences of a number of open source environments. In this way, you'll have
    a broad and practical overview of the problems that can be tackled with RL.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中几乎所有使用的环境都基于OpenAI开源的接口**Gym**。因此，我们将介绍它并使用其中一些内置环境。然后，在深入探讨后续章节中的强化学习算法之前，我们将列出并解释多个开源环境的优势和差异。这样一来，您将对可以使用强化学习解决的问题有一个广泛而实用的概述。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Setting up the environment
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置环境
- en: OpenAI Gym and RL cycles
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI Gym和RL循环
- en: TensorFlow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: TensorBoard
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorBoard
- en: Types of RL environments
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习环境的种类
- en: Setting up the environment
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置环境
- en: 'The following are the three main tools required to create deep RL algorithms:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是创建深度强化学习算法所需的三个主要工具：
- en: '**Programming language**: Python is the first choice for the development of
    machine learning algorithms on account of its simplicity and the third-party libraries
    that are built around it.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编程语言**：Python是开发机器学习算法的首选，因其简单性和建立在其周围的第三方库。'
- en: '**Deep learning framework**: In this book, we use TensorFlow because, as we''ll
    see in the *TensorFlow* section, it is scalable, flexible, and very expressive.
    Despite this, many other frameworks can be used in its place, including PyTorch
    and Caffe.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度学习框架**：在本书中，我们使用TensorFlow，因为如同我们将在*TensorFlow*部分看到的那样，它可扩展、灵活且非常表达力强。尽管如此，也可以使用许多其他框架，包括PyTorch和Caffe。'
- en: '**Environment**: Throughout the book, we''ll use many different environments
    to demonstrate how to deal with different types of problems and to highlight the
    strengths of RL algorithms.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境**：在本书中，我们将使用许多不同的环境来演示如何处理不同类型的问题，并突出强化学习算法的优势。'
- en: In this book, we use Python 3.7, but all versions above 3.5 should work. We
    also assume that you've already installed `numpy` and `matplotlib`.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们使用Python 3.7，但所有3.5以上的版本也应该可以工作。我们还假设您已经安装了`numpy`和`matplotlib`。
- en: 'If you haven''t already installed TensorFlow, you can do so through their website
    or by typing the following in a Terminal window:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有安装TensorFlow，可以通过他们的网站安装，或者在终端窗口中输入以下命令：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Alternatively, you can type the following command, if your machine has GPUs:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你的机器有GPU，你可以输入以下命令：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can find all the installation instructions and the exercises relating to
    this chapter on the GitHub repository, which can be found here: [https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python)[.](https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-Algorithms-with-Python)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub仓库中找到所有的安装说明和与本章相关的练习，仓库地址是：[https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python)
- en: Now, let's look at how to install the environments.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看如何安装这些环境。
- en: Installing OpenAI Gym
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装OpenAI Gym
- en: OpenAI Gym offers a general interface as well as a broad variety of environments.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym提供了一个通用接口以及多种不同的环境。
- en: To install it, we will use the following commands.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装它，我们将使用以下命令。
- en: 'On OSX, we can use the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在OSX上，我们可以使用以下命令：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'On Ubuntu 16.04, we will use the following command:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在Ubuntu 16.04上，我们将使用以下命令：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'On Ubuntu 18.04, we will use the following command:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在Ubuntu 18.04上，我们将使用以下命令：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After running the preceding command for your respective OS, the following command
    is used:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行完前面的命令之后，针对你的操作系统，可以使用以下命令：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Certain Gym environments also require the installation of `pybox2d`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 某些Gym环境还需要安装`pybox2d`：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Installing Roboschool
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Roboschool
- en: 'The final environment we are interested in is Roboschool, a simulator for robots.
    It''s easy to install, but if you encounter any errors, take a look at its GitHub
    repository:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的最终环境是Roboschool，它是一个机器人模拟器。它容易安装，但如果遇到任何错误，可以查看其GitHub仓库：[https://github.com/Roboschool/roboschool](https://github.com/Roboschool/roboschool)
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: OpenAI Gym and RL cycles
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI Gym与强化学习周期
- en: Since RL requires an agent and an environment to interact with each other, the
    first example that may spring to mind is the earth, the physical world we live
    in. Unfortunately, for now, it is actually used in only a few cases. With the
    current algorithms, the problems stem from the large number of interactions that
    an agent has to execute with the environment in order to learn good behaviors.
    It may require hundreds, thousands, or even millions of actions, requiring way too
    much time to be feasible. One solution is to use simulated environments to start
    the learning process and, only at the end, fine-tune it in the real world. This
    approach is way better than learning just from the world around it, but still
    requires slow real-world interactions. However, in many cases, the task can be
    fully simulated. To research and implement RL algorithms, games, video games,
    and robot simulators are a perfect testbed because, in order to be solved, they
    require capabilities such as planning, strategy, and long-term memory. Moreover,
    games have a clear reward system and can be completely simulated in an artificial
    environment (computers), allowing fast interactions that accelerate the learning
    process. For these reasons, in this book, we'll use mostly video games and robot
    simulators to demonstrate the capabilities of RL algorithms.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于强化学习需要一个代理和环境相互作用，第一个可能想到的例子是地球——我们所生活的物理世界。不幸的是，目前它仅在少数几个案例中使用。由于当前算法的问题来自于代理必须与环境进行大量互动才能学习到良好的行为，这可能需要数百、数千甚至数百万次操作，导致所需时间过长，难以实现。一种解决方案是使用模拟环境来启动学习过程，最后才在真实世界中进行微调。这种方法比仅从周围世界中学习要好得多，但仍然需要缓慢的现实世界交互。然而，在许多情况下，任务可以完全模拟。为了研究和实现强化学习算法，游戏、视频游戏和机器人模拟器是完美的测试平台，因为它们的解决方案需要规划、策略和长期记忆等能力。此外，游戏有明确的奖励系统，可以在人工环境（计算机）中完全模拟，允许快速交互，从而加速学习过程。正因为如此，在本书中，我们将主要使用视频游戏和机器人模拟器来展示强化学习算法的能力。
- en: OpenAI Gym, an open source toolkit for developing and researching RL algorithms,
    was created to provide a common and shared interface for environments, while making
    a large and diverse collection of environments available. These include Atari
    2600 games, continuous control tasks, classic control theory problems, simulated
    robotic goal-based tasks, and simple text games. Owing to its generality, many
    environments created by third parties are using the Gym interface.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym 是一个开源工具包，用于开发和研究 RL 算法，旨在提供一个统一的环境接口，同时提供一个庞大且多样化的环境集合。这些环境包括 Atari
    2600 游戏、连续控制任务、经典控制理论问题、模拟机器人目标导向任务以及简单的文本游戏。由于其通用性，许多第三方创建的环境都使用 Gym 接口。
- en: Developing an RL cycle
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发一个 RL 循环
- en: 'A basic RL cycle is shown in the following code block. This essentially makes
    the RL model play for 10 moves while rendering the game at each step:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块展示了一个基本的 RL 循环。这本质上让 RL 模型进行 10 步操作，并在每一步渲染游戏：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This leads to the following output:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '![](img/6b77b90c-3aad-4e87-a1fc-2de8c5ec6805.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b77b90c-3aad-4e87-a1fc-2de8c5ec6805.png)'
- en: 'Figure 2.1: Rendering of CartPole'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：CartPole 渲染
- en: Let's take a closer look at the code. It starts by creating a new environment
    named `CartPole-v1`, a classic game used in control theory problems. However,
    before using it, the environment is initialized by calling `reset()`*.* After
    doing so, the cycle loops 10 times. In each iteration, `env.action_space.sample()`
    samples a random action, executes it in the environment with `env.step()`*,* and
    displays the result with the `render()` method; that is, the current state of
    the game, as in the preceding screenshot. In the end, the environment is closed
    by calling `env.close()`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看代码。它通过创建一个名为 `CartPole-v1` 的新环境开始，这是一个经典的用于控制理论问题的游戏。然而，在使用它之前，环境需要通过调用
    `reset()` 进行初始化。初始化后，循环执行 10 次。在每次迭代中，`env.action_space.sample()` 会随机选择一个动作，通过
    `env.step()` 执行该动作，并通过 `render()` 方法显示结果；也就是游戏的当前状态，如前面的截图所示。最后，环境通过调用 `env.close()`
    被关闭。
- en: Don't worry if the following code outputs deprecation warnings; they are there
    to notify you that some functions have been changed. The code will still be functioning
    correctly.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下代码输出弃用警告，不用担心；这些警告是用来通知你某些函数已被更改，代码仍然能够正常运行。
- en: This cycle is the same for every environment that uses the Gym interface, but
    for now, the agent can only play random actions without having any feedback, which
    is essential to any RL problem.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个循环对于所有使用 Gym 接口的环境都是相同的，但现在，智能体只能进行随机动作，而没有任何反馈，这是任何 RL 问题中至关重要的部分。
- en: In RL, you may see the terms **state** and **observation** being used almost
    interchangeably, but they are not the same. We talk about state when all the information
    pertaining to the environment is encoded in it. We talk about observation when
    only a part of the actual state of the environment is visible to the agent, such
    as the perception of a robot. To simplify this, OpenAI Gym always uses the term
    observation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RL 中，你可能会看到**状态**和**观察**这两个术语几乎可以互换使用，但它们并不相同。当所有与环境相关的信息都被编码在其中时，我们称之为状态。当只有部分实际环境状态对智能体可见时，我们称之为观察，例如机器人的感知。为了简化这一点，OpenAI
    Gym 始终使用“观察”一词。
- en: 'The following diagram shows the flow of the cycle:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示显示了循环的流程：
- en: '![](img/81b29847-365f-4f0f-be9f-48fdc91fe028.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/81b29847-365f-4f0f-be9f-48fdc91fe028.png)'
- en: 'Figure 2.2: Basic RL cycle according to OpenAI Gym. The environment returns
    the next state, a reward, a done flag, and some additional information'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：根据 OpenAI Gym 的基本 RL 循环。环境返回下一个状态、奖励、完成标志和一些附加信息。
- en: Indeed, the `step()` method returns four variables that provide information
    about the interaction with the environment. The preceding diagram shows the loop
    between the agent and environment, as well as the variables exchanged; namely, **Observation**,
    **Reward**, **Done**, and **Info**. **Observation** is an object that represents
    the new observation (or state) of the environment. **Reward** is a float number
    that represents the number of rewards obtained in the last action. **Done** is
    a Boolean value that is used on tasks that are episodic; that is, tasks that are limited
    in terms of the number of interactions. Whenever `done`is `True`, this means that
    the episode has terminated and that the environment should be reset. For example,
    `done` is `True` when the task has been completed or the agent has died. **Info**,
    on the other hand, is a dictionary that provides extra information about the environment
    but that usually isn't used.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，`step()` 方法返回四个变量，它们提供关于与环境交互的信息。上面的图示展示了代理与环境之间的循环，以及交换的变量；即 **观察**、**奖励**、**完成**
    和 **信息**。**观察** 是一个表示环境新观察（或状态）的对象。**奖励** 是一个浮动数值，表示上一个动作获得的奖励数值。**完成** 是一个布尔值，用于表示任务是否是阶段性任务；也就是交互次数有限的任务。每当
    `done` 为 `True` 时，意味着该回合已经结束，环境应当被重置。例如，`done` 为 `True` 时，表示任务已完成或代理已失败。另一方面，**信息**
    是一个字典，提供有关环境的额外信息，但通常不会使用。
- en: If you have never heard of CartPole, it's a game with the goal of balancing
    a pendulum acting on a horizontal cart. A reward of +1 is provided for every timestep
    when the pendulum is in the upright position. The episode ends when it is too
    unbalanced or it manages to balance itself for more than 200 timesteps (collecting
    a maximum cumulative reward of 200).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从未听说过 CartPole，它是一款目标是平衡作用在水平小车上的摆锤的游戏。每当摆锤处于竖直位置时，都会获得 +1 的奖励。游戏结束时，如果摆锤失去平衡，或者它成功平衡超过
    200 个时间步（累积奖励最大为 200），游戏就会结束。
- en: 'We can now create a more complete algorithm that plays 10 games and prints
    the accumulated reward for each game using the following code:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以创建一个更完整的算法，使用以下代码来进行 10 局游戏，并打印每局游戏的累积奖励：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output will be similar to the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following table shows the output of the `step()` method over the last four
    actions of a game:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 下表显示了 `step()` 方法在游戏最后四个动作中的输出：
- en: '| **Observation** | **Reward** | **Done** | **Info** |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| **观察** | **奖励** | **完成** | **信息** |'
- en: '| [-0.05356921, -0.38150626, 0.12529277, 0.9449761 ] | 1.0  | False  | {} |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| [-0.05356921, -0.38150626, 0.12529277, 0.9449761 ] | 1.0  | False  | {} |'
- en: '| [-0.06119933, -0.57807287, 0.14419229, 1.27425449] | 1.0 | False  | {} |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| [-0.06119933, -0.57807287, 0.14419229, 1.27425449] | 1.0 | False  | {} |'
- en: '| [-0.07276079, -0.38505429, 0.16967738, 1.02997704] | 1.0 | False  | {} |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| [-0.07276079, -0.38505429, 0.16967738, 1.02997704] | 1.0 | False  | {} |'
- en: '| [-0.08046188, -0.58197758, 0.19027692, 1.37076617] | 1.0 | False  | {} |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| [-0.08046188, -0.58197758, 0.19027692, 1.37076617] | 1.0 | False  | {} |'
- en: '| [-0.09210143, -0.3896757, 0.21769224, 1.14312384] | 1.0 | True | {} |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| [-0.09210143, -0.3896757, 0.21769224, 1.14312384] | 1.0 | True | {} |'
- en: Notice that the environment's observation is encoded in a 1 x 4 array; that
    the reward, as we expected, is always 1; and that `done` is `True` only in the
    last row when the game is terminated. Also, **Info**, in this case, is empty.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，环境的观察以 1 x 4 的数组进行编码；正如我们预期的那样，奖励始终为 1；并且只有在游戏结束时（即最后一行），`done` 为 `True`。此外，**信息**在此情况下为空。
- en: In the upcoming chapters, we'll create agents that play CartPole by taking more
    intelligent actions depending on the current state of the pole.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将创建代理，根据摆锤的当前状态做出更智能的决策来玩 CartPole 游戏。
- en: Getting used to spaces
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 习惯于空间
- en: 'In OpenAI Gym, actions and observations are mostly instances of the `Discrete`
    or `Box` class. These two classes represent different spaces. `Box` represents
    an *n*-dimensional array, while `Discrete`, on the other hand, is a space that
    allows a fixed range of non-negative numbers. In the preceding table, we have
    already seen that the observation of CartPole is encoded by four floats, meaning
    that it''s an instance of the `Box` class. It is possible to check the type and
    dimension of the observation spaces by printing the `env.observation_space` variable:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenAI Gym 中，动作和观察大多是 `Discrete` 或 `Box` 类的实例。这两个类表示不同的空间。`Box` 代表一个 *n* 维数组，而
    `Discrete` 是一个允许固定范围非负数的空间。在前面的表格中，我们已经看到，CartPole 的观察由四个浮动数值编码，意味着它是 `Box` 类的一个实例。可以通过打印
    `env.observation_space` 变量来检查观察空间的类型和维度：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Indeed, as we expected, the output is as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，正如我们预期的那样，输出如下：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this book, we mark the output of `print()` by introducing the printed text
    with `>>`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们通过在`print()`输出的文本前添加`>>`来标记输出。
- en: 'In the same way, it is possible to check the dimension of the action space:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，可以检查动作空间的维度：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This results in the following output:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In particular, `Discrete(2)` means that the actions could either have the value
    `0` or `1`. Indeed, if we use the sampling function used in the preceding example,
    we obtain `0` or `1` (in CartPole, this means left or right*)*:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，`Discrete(2)`意味着动作的值可以是`0`或`1`。实际上，如果我们使用前面示例中的采样函数，我们将获得`0`或`1`（在CartPole中，这意味着向左或向右*）*：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `low`and `high` instance attributes return the minimum and maximum values
    allowed by a `Box` space:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`low`和`high`实例属性返回`Box`空间允许的最小值和最大值：'
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Development of ML models using TensorFlow
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow开发机器学习模型
- en: TensorFlow is a machine learning framework that performs high-performance numerical
    computations. TensorFlow owes its popularity to its high quality and vast amount
    of documentation, its ability to easily serve models at scale in production environments,
    and the friendly interface to GPUs and TPUs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是一个执行高性能数值计算的机器学习框架。TensorFlow之所以如此流行，得益于其高质量和丰富的文档、能够轻松在生产环境中部署模型的能力，以及友好的GPU和TPU接口。
- en: TensorFlow, to facilitate the development and deployment of ML models, has many
    high-level APIs, including Keras, Eager Execution, and Estimators. These APIs
    are very useful in many contexts, but, in order to develop RL algorithms, we'll
    only use low-level APIs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow，为了便于机器学习模型的开发和部署，提供了许多高级API，包括Keras、Eager Execution和Estimators。这些API在许多场合非常有用，但为了开发强化学习算法，我们将只使用低级API。
- en: 'Now, let''s code immediately using **TensorFlow**. The following lines of code
    execute the sum of the constants, `a` and `b`, created with `tf.constant()`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们立即使用**TensorFlow**编写代码。以下代码行执行了常量`a`和`b`的加法，`a`和`b`是使用`tf.constant()`创建的：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'A particularity of TensorFlow is the fact that it expresses all computations
    as a computational graph that has to first be defined and later executed. Only
    after execution will the results be available. In the following example, after
    the operation, `c = a + b`, `c` doesn''t hold the end value. Indeed, if you print
    `c` before creating the session, you''ll obtain the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的一个特点是，它将所有计算表达为一个计算图，首先需要定义该图，之后才能执行。只有在执行后，结果才会可用。在以下示例中，在操作`c =
    a + b`之后，`c`并不持有最终值。实际上，如果你在创建会话之前打印`c`，你将获得以下内容：
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This is the class of the `c` variable, not the result of the addition.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`c`变量的类，而不是加法的结果。
- en: 'Moreover, execution has to be done inside a session that is instantiated with `tf.Session()`.
    Then, to perform the computation, the operation has to be passed as input to the
    `run` function of the session just created. Thus, to actually compute the graph
    and consequently sum `a` and `b`, we need to create a session and pass `c` as
    an input to `session.run`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，执行必须在通过`tf.Session()`实例化的会话内进行。然后，为了执行计算，操作必须作为输入传递给刚创建的会话的`run`函数。因此，为了实际计算图并最终求和`a`和`b`，我们需要创建一个会话，并将`c`作为输入传递给`session.run`：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If you are using Jupyter Notebook, make sure to reset the previous graph by
    running `tf.reset_default_graph()`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用Jupyter Notebook，请确保通过运行`tf.reset_default_graph()`来重置之前的图。
- en: Tensor
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量
- en: The variables in TensorFlow are represented as tensors that are arrays of any
    number of dimensions. There are three main types of tensors—`tf.Variable`, `tf.constant`,
    and `tf.placeholder`. Except for `tf.Variable`, all the other tensors are immutable.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow中的变量表示为张量，是任何维度数组。张量有三种主要类型——`tf.Variable`、`tf.constant`和`tf.placeholder`。除了`tf.Variable`，其他张量都是不可变的。
- en: 'To check the shape of a tensor, we will use the following code:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查张量的形状，我们将使用以下代码：
- en: '[PRE20]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The elements of a tensor are easily accessible, and the mechanisms are similar
    to those employed by Python:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的元素非常容易访问，机制与Python中使用的类似：
- en: '[PRE21]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Constant
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常量
- en: 'As we have already seen, a constant is an immutable type of tensor that can
    be easily created using `tf.constant`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们已经看到的，常量是不可变的张量类型，可以使用`tf.constant`轻松创建：
- en: '[PRE22]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Placeholder
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 占位符
- en: 'A placeholder is a tensor that is fed at runtime. Usually, placeholders are
    used as input for models. Every input passed to a computational graph at runtime
    is fed with `feed_dict`. `feed_dict` is an optional argument that allows the caller
    to override the value of tensors in the graph. In the following snippet, the `a` placeholder
    is overridden by `[[0.1,0.2,0.3]]`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 占位符是一个张量，在运行时被提供输入。通常，占位符作为模型的输入。在运行时传递给计算图的每个输入都通过`feed_dict`进行传递。`feed_dict`是一个可选参数，允许调用者覆盖图中张量的值。在以下代码片段中，`a`占位符的值被`[[0.1,0.2,0.3]]`覆盖：
- en: '[PRE23]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If the size of the first dimension of the input is not known during the creation
    of the graph, TensorFlow can take care of it. Just set it to `None`:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入的第一维的大小在创建图时尚不确定，TensorFlow可以处理它。只需将其设置为`None`：
- en: '[PRE24]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This feature is useful when the number of training examples is not known initially.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这个功能在训练示例数量最初不确定时特别有用。
- en: Variable
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变量
- en: A **variable** is a mutable tensor that can be trained using an optimizer. For
    example, they can be the free variables that constitute the weights and biases
    of a neural network.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**变量**是一个可变的张量，可以使用优化器进行训练。例如，它们可以是神经网络的权重和偏置所构成的自由变量。'
- en: 'We will now create two variables, one uniformly initialized, and one initialized
    with constant values:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建两个变量，一个使用均匀初始化，另一个使用常数值初始化：
- en: '[PRE25]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The variables aren't initialized until `global_variables_initializer()` is called.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 变量在调用`global_variables_initializer()`之前不会被初始化。
- en: 'All the variables created in this way are set as `trainable`, meaning that
    the graph can modify them, for example, after an optimization operation. The variables
    can be set as non-trainable, as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式创建的所有变量都会被设置为`可训练`，意味着图可以修改它们，例如，在优化操作之后。也可以将变量设置为不可训练，如下所示：
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'An easy way to access all the variables is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 访问所有变量的简便方法如下：
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Creating a graph
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建图表
- en: A **graph** represents low-level computations in terms of the dependencies between
    operations. In TensorFlow, you first define a graph, and then create a session
    that executes the operations in the graph.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**图**表示低级计算，这些计算基于操作之间的依赖关系。在TensorFlow中，首先定义一个图，然后创建一个会话来执行图中的操作。'
- en: The way a graph is built, computed, and optimized in TensorFlow allows a high
    degree of parallelism, distributed execution, and portability, all very important
    properties when building machine learning models.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow中图的构建、计算和优化方式支持高度的并行性、分布式执行和可移植性，这些都是构建机器学习模型时非常重要的属性。
- en: 'To give you an idea of the structure of a graph produced internally by TensorFlow,
    the following program produces the computational graph demonstrated in the following
    diagram:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你了解TensorFlow内部生成的计算图的结构，以下程序将生成如下图所示的计算图：
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This results in the following graph:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下图表：
- en: '![](img/34e69931-f42d-439c-b29e-656073e965d4.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/34e69931-f42d-439c-b29e-656073e965d4.png)'
- en: 'Figure 2.3: Example of a computational graph'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：计算图示例
- en: Simple linear regression example
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单线性回归示例
- en: 'To better digest all the concepts, let''s now create a simple linear regression
    model. First, we have to import all the libraries and set a random seed, both
    for NumPy and TensorFlow (so that we''ll all have the same results):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解所有概念，我们现在创建一个简单的线性回归模型。首先，我们必须导入所有库并设置随机种子，这对于NumPy和TensorFlow都是必要的（这样我们得到的结果都是相同的）：
- en: '[PRE29]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, we can create a synthetic dataset consisting of 100 examples, as shown
    in the following screenshot:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以创建一个由100个示例组成的合成数据集，如下图所示：
- en: '![](img/a558b08f-c230-46e9-b908-aae0172ee7e8.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a558b08f-c230-46e9-b908-aae0172ee7e8.png)'
- en: 'Figure 2.4: Dataset used in the linear regression example'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4：线性回归示例中使用的数据集
- en: 'Because this is a linear regression example, *y = W * X + b*, where *W*and
    *b* are arbitrary values. In this example, we set `W` = `0.5` and `b` = `1.4`.
    Additionally, we add some normal random noise:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是一个线性回归示例，*y = W * X + b*，其中*W*和*b*是任意值。在这个示例中，我们设置`W` = `0.5`和`b` = `1.4`。此外，我们还加入了一些正态随机噪声：
- en: '[PRE30]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The next step involves creating the placeholders for the input and the output,
    and the variables of the weight and bias of the linear model. During training,
    these two variables will be optimized to be as similar as possible to the weight
    and bias of the dataset:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建输入和输出的占位符，以及线性模型的权重和偏置变量。在训练过程中，这两个变量将被优化，使其尽可能与数据集的权重和偏置相似：
- en: '[PRE31]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, we build the computational graph defining the linear operation and the
    **mean squared error** (**MSE**) loss:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们构建了定义线性操作和 **均方误差** (**MSE**) 损失的计算图：
- en: '[PRE32]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can now instantiate the optimizer and call `minimize()` to minimize the
    MSE loss. `minimize()` first computes the gradients of the variables (`v_weight`
    and `v_bias`) and then applies the gradient, updating the variables:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以实例化优化器并调用 `minimize()` 来最小化 MSE 损失。`minimize()` 首先计算变量（`v_weight` 和 `v_bias`）的梯度，然后应用梯度更新变量：
- en: '[PRE33]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, let''s create a session and initialize all the variables:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个会话并初始化所有变量：
- en: '[PRE34]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The training is done by running the optimizer multiple times while feeding
    the dataset to the graph. To keep track of the state of the model, the MSE loss
    and the model variables (weight and bias) are printed every 40 epochs:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 训练通过多次运行优化器并将数据集输入图表来完成。为了跟踪模型的状态，MSE 损失和模型变量（权重和偏差）每 40 个 epochs 打印一次：
- en: '[PRE35]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In the end, we can print the final values of the variables:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以打印变量的最终值：
- en: '[PRE36]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output will be similar to the following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '[PRE37]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: During the training phase, it's possible to see that the MSE loss would decrease
    toward a non-zero value (of about 3.71). That's because we added random noise
    to the dataset that prevents the MSE from reaching a perfect value of 0.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，可以看到 MSE 损失会逐渐减少到一个非零值（大约为 3.71）。这是因为我们在数据集中添加了随机噪声，防止了 MSE 达到完美的 0 值。
- en: 'Also, as anticipated, with regard to the weight and bias of the model approach,
    the values of `0.500` and `1.473` are precisely the values around which the dataset
    has been built. The blue line visible in the following screenshot is the prediction
    of the trained linear model, while the points are our training examples:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，如预期的那样，关于模型方法的权重和偏差，`0.500`和`1.473`的值正是构建数据集时围绕的值。下图中可见的蓝色线条是训练好的线性模型的预测值，而这些点则是我们的训练示例：
- en: '![](img/c1eeea8a-b731-4fd5-bff9-6679bd86cdea.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c1eeea8a-b731-4fd5-bff9-6679bd86cdea.png)'
- en: 'Figure 2.5: Linear regression model predictions'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：线性回归模型预测
- en: For all the color references in the chapter, please refer to the color images
    bundle: [http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf.).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所有颜色引用的详细信息，请参考颜色图片包：[http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf.)。
- en: Introducing TensorBoard
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 TensorBoard
- en: Keeping track of how variables change during the training of a model can be
    a tedious job. For instance, in the linear regression example, we kept track of
    the MSE loss and of the parameters of the model by printing them every 40 epochs.
    As the complexity of the algorithms increases, there is an increase in the number
    of variables and metrics to be monitored. Fortunately, this is where TensorBoard
    comes to the rescue.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，跟踪变量的变化可能是一项繁琐的工作。例如，在线性回归示例中，我们通过每 40 个 epochs 打印 MSE 损失和模型的参数来跟踪它们。随着算法复杂性的增加，需监控的变量和指标也会增多。幸运的是，这时
    TensorBoard 就派上了用场。
- en: 'TensorBoard is a suite of visualization tools that can be used to plot metrics,
    visualize TensorFlow graphs, and visualize additional information. A typical TensorBoard
    screen is similar to the one shown in the following screenshot:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 是一套可视化工具，能够用于绘制指标、可视化 TensorFlow 图表以及展示额外信息。一个典型的 TensorBoard 页面类似于以下截图所示：
- en: '![](img/0c541147-d088-4565-9993-2e32ab112850.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c541147-d088-4565-9993-2e32ab112850.png)'
- en: 'Figure 2.6: Scalar TensorBoard page'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6：标量 TensorBoard 页面
- en: 'The integration of TensorBoard with TensorFlow code is pretty straightforward as
    it involves only a few tweaks to the code. In particular, to visualize the MSE
    loss over time and monitor the weight and bias of our linear regression model
    using TensorBoard, it is first necessary to attach the loss tensor to `tf.summar.scalar()`
    and the model''s parameters to `tf.summary.histogram()`*. *The following snippet
    should be added after the call to the optimizer:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 与 TensorFlow 代码的集成非常简单，只需对代码做少量调整。特别是，为了使用 TensorBoard 可视化 MSE 损失随时间变化，并监控线性回归模型的权重和偏差，首先需要将损失张量附加到
    `tf.summary.scalar()`，并将模型的参数附加到 `tf.summary.histogram()`*。以下代码片段应添加在优化器调用之后：
- en: '[PRE38]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then, to simplify the process and handle them as a single summary, we can merge
    them:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了简化过程并将其作为一个总结处理，我们可以将它们合并：
- en: '[PRE39]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'At this point, we have to instantiate a `FileWriter` instance that will log
    all the summary information in a file:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们需要实例化一个`FileWriter`实例，它将把所有摘要信息记录到文件中：
- en: '[PRE40]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The first two lines create a unique filename using the current date and time.
    In the third line, the path of the file and the TensorFlow graph are passed to `FileWriter()`.
    The second parameter is optional and represents the graph to visualize.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 前两行通过当前的日期和时间创建一个独特的文件名。在第三行，文件的路径和TensorFlow图形会传递给`FileWriter()`。第二个参数是可选的，表示要可视化的图形。
- en: 'The final change is done in the training loop by replacing the previous line, `train_loss,
    _ = session.run(..)`, with the following:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的修改是在训练循环中进行的，通过将之前的行`train_loss, _ = session.run(..)`替换为以下内容：
- en: '[PRE41]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: First, `all_summary` is executed in the current session, and then the result
    is added to `file_writer` to be saved in the file. This procedure will run the
    three summaries that were merged previously and log them in the log file. TensorBoard
    will then read from this file and visualize the scalar, the two histograms, and
    the computation graph.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在当前会话中执行`all_summary`，然后将结果添加到`file_writer`中保存到文件。这一过程将运行之前合并的三个摘要，并将它们记录到日志文件中。TensorBoard随后会从该文件读取并可视化标量、两个直方图以及计算图。
- en: 'Remember to close `file_writer` at the end, as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在结束时关闭`file_writer`，如下所示：
- en: '[PRE42]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Finally, we can open TensorBoard by going to the working directory and typing
    the following in a terminal:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过进入工作目录并在终端中输入以下命令来打开TensorBoard：
- en: '[PRE43]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This command creates a web server that listens to port `6006`. To start TensorBoard,
    you have to go to the link that TensorBoard shows you:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令创建一个监听`6006`端口的Web服务器。要启动TensorBoard，您需要访问TensorBoard显示的链接：
- en: '![](img/7ca1b057-02de-40bc-994b-2822f44fecae.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ca1b057-02de-40bc-994b-2822f44fecae.png)'
- en: 'Figure 2.7: Histogram of the linear regression model''s parameters'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7：线性回归模型参数的直方图
- en: 'You can now browse TensorBoard by clicking on the tabs at the top of the page
    to access the plots, the histograms, and the graph. In the preceding—as well as
    the following—screenshots, you can see some of the results visualized on those
    pages. The plots and the graphs are interactive, so take some time to explore
    them in order to improve your understanding of their use. Also check the TensorBoard
    official documentation ([https://www.tensorflow.org/guide/summaries_and_tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard)) to
    learn more about the additional features included in TensorBoard:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以通过点击页面顶部的选项卡来浏览TensorBoard，访问图形、直方图和计算图。在之前和之后的截图中，您可以看到在这些页面上可视化的部分结果。图形和计算图是交互式的，因此请花时间浏览它们，以帮助您更好地理解它们的使用方法。还可以查看TensorBoard的官方文档（[https://www.tensorflow.org/guide/summaries_and_tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard)），了解更多关于TensorBoard附加功能的内容：
- en: '![](img/6a568965-c104-40d1-94b3-536ae8e0bc0d.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a568965-c104-40d1-94b3-536ae8e0bc0d.png)'
- en: 'Figure 2.8: Scalar plot of the MSE loss'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8：MSE损失的标量图
- en: Types of RL environments
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习（RL）环境的类型
- en: Environments, similar to labeled datasets in supervised learning, are the essential
    part of RL as they dictate the information that has to be learned and the choice
    of algorithms. In this section, we'll take a look at the main differences between
    the types of environments and list some of the most important open source environments.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 环境，类似于监督学习中的标签数据集，是强化学习（RL）中的核心部分，因为它们决定了需要学习的信息和算法的选择。在本节中，我们将查看不同类型环境之间的主要区别，并列出一些最重要的开源环境。
- en: Why different environments?
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要有不同的环境？
- en: While, for real applications, the choice of environment is dictated by the task
    to be learned, for research applications, usually, the choice is dictated by intrinsic
    features of the environment. In this latter case, the end goal is not to train
    the agent on a specific task, but to show some task-related capabilities.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在实际应用中，环境的选择由要学习的任务决定，但在研究应用中，通常环境的选择由其内在特性决定。在后者的情况下，最终目标不是让智能体在特定任务上进行训练，而是展示一些与任务相关的能力。
- en: For instance, if the goal is to create a multi-agent RL algorithm, the environment
    should have at least two agents with a means to communicate with one another,
    regardless of the end task. Instead, to create a lifelong learner (agents that
    continuously create and learn more difficult tasks using the knowledge acquired
    in previous easier tasks), the primary quality that the environment should have
    is the ability to adapt to new situations and a realistic domain.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果目标是创建一个多智能体 RL 算法，那么环境至少应包含两个智能体，并且要有彼此交流的方式，而与最终任务无关。相反，如果目标是创建一个终身学习者（即那些利用在之前更容易的任务中获得的知识，持续创建并学习更难任务的智能体），那么环境应该具备的主要特性是能够适应新情况，并且拥有一个现实的领域。
- en: 'Task aside, environments can differ by other characteristics, such as complexity,
    observation space, action space, and reward function:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 除了任务外，环境还可以在其他特征上有所不同，如复杂性、观察空间、动作空间和奖励函数：
- en: '**Complexity**: Environments can spread across a wide spectrum, from the balance
    of a pole to the manipulation of physical objects with a robot hand. More complex
    environments can be chosen to show the capability of an algorithm to deal with
    a large state space that mimics the complexity of the world. On the other hand,
    simpler ones can be used to show only some specific qualities.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂性**：环境可以涵盖从平衡杆到用机器人手臂操控物理物体的广泛范围。可以选择更复杂的环境来展示算法处理大型状态空间的能力，模拟世界的复杂性。另一方面，也可以选择更简单的环境来展示算法的一些特定特性。'
- en: '**Observation space**: As we have already seen, the observation space can range
    from the full state of the environment to only a partial observation perceived
    by the perception systems, such as row images.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察空间**：正如我们已经看到的，观察空间可以从环境的完整状态到由感知系统感知的部分观察（如原始图像）。'
- en: '**Action space**: Environments with a large continuous action space challenge
    the agent to deal with real-value vectors, whereas discrete actions are easier
    to learn as they have only a limited number of actions available.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作空间**：具有大规模连续动作空间的环境挑战智能体处理实值向量，而离散动作则更容易学习，因为它们只有有限的可用动作。'
- en: '**Reward function**: Environments with hard explorations and delayed rewards,
    such as Montezuma''s revenge, are very challenging to solve. Surprisingly, only
    a few algorithms are able to reach human levels. For this reason, these environments
    are used as a test bed for algorithms that propose to address the exploration
    problem.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励函数**：具有难度较大的探索和延迟奖励的环境，例如《蒙特祖玛的复仇》，非常具有挑战性，只有少数算法能够达到人类水平。因此，这些环境常用于测试旨在解决探索问题的算法。'
- en: Open source environments
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源环境
- en: 'How can we design an environment that meets our requirements? Fortunately,
    there are many open source environments that are built to tackle specific or broader
    problems. By way of an example, CoinRun, shown in the following screenshot, was
    created to measure the generalization capabilities of an algorithm:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何设计一个符合要求的环境？幸运的是，有许多开源环境专门为解决特定或更广泛的问题而构建。举个例子，CoinRun，如下图所示，旨在衡量算法的泛化能力：
- en: '![](img/a82a2e04-618c-49bb-b0b8-3f7a74f3cb1c.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a82a2e04-618c-49bb-b0b8-3f7a74f3cb1c.png)'
- en: 'Figure 2.9: The CoinRun environment'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9：CoinRun 环境
- en: 'We will now list some of the main open source environments available. These
    are created by different teams and companies, but almost all of them use the OpenAI
    Gym interface:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将列出一些主要的开源环境。这些环境由不同的团队和公司创建，但几乎所有环境都使用 OpenAI Gym 接口：
- en: '![](img/63951647-17fb-4b34-b107-63834c6408da.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/63951647-17fb-4b34-b107-63834c6408da.png)'
- en: 'Figure 2.10: Roboschool environment'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10：Roboschool 环境
- en: '**Gym Atari** ([https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari)):
    Includes Atari 2600 games with screen images as input. They are useful for measuring
    the performance of RL algorithms on a wide variety of games with the same observation
    space.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gym Atari** ([https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari))：包括使用屏幕图像作为输入的
    Atari 2600 游戏。它们对于衡量 RL 算法在具有相同观察空间的各种游戏中的表现非常有用。'
- en: '**Gym Classic control** ([https://gym.openai.com/envs/#classic_control](https://gym.openai.com/envs/#classic_control)):
    Classic games that can be used for the easy evaluation and debugging of an algorithm.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gym 经典控制** ([https://gym.openai.com/envs/#classic_control](https://gym.openai.com/envs/#classic_control))：经典游戏，可以用于轻松评估和调试算法。'
- en: '**Gym MuJoCo** ([https://gym.openai.com/envs/#mujoco](https://gym.openai.com/envs/#mujoco)):
    Includes continuous control tasks (such as Ant, and HalfCheetah) built on top
    of MuJoCo, a physics engine that requires a paid license (a free license is available
    for students).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gym MuJoCo** ([https://gym.openai.com/envs/#mujoco](https://gym.openai.com/envs/#mujoco)):
    包含建立在 MuJoCo 上的连续控制任务（如 Ant 和 HalfCheetah），MuJoCo 是一个需要付费许可证的物理引擎（学生可以获得免费的许可证）。'
- en: '**MalmoEnv** ([https://github.com/Microsoft/malmo](https://github.com/Microsoft/malmo)):
    An environment built on top of Minecraft.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MalmoEnv** ([https://github.com/Microsoft/malmo](https://github.com/Microsoft/malmo)):
    基于 Minecraft 构建的环境。'
- en: '**Pommerman** ([https://github.com/MultiAgentLearning/playground](https://github.com/MultiAgentLearning/playground)):
    A great environment for training multi-agent algorithms. Pommerman is a variant
    of the famous Bomberman.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pommerman** ([https://github.com/MultiAgentLearning/playground](https://github.com/MultiAgentLearning/playground)):
    一个非常适合训练多智能体算法的环境。Pommerman 是著名游戏 Bomberman 的变种。'
- en: '**Roboschool** ([https://github.com/openai/roboschool](https://github.com/openai/roboschool)):
    A robot simulation environment integrated with OpenAI Gym. It includes an environment
    replica of MuJoCo, as shown in the preceding screenshot, two interactive environments
    to improve the robustness of the agent, and one multiplayer environment.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Roboschool** ([https://github.com/openai/roboschool](https://github.com/openai/roboschool)):
    一个与 OpenAI Gym 集成的机器人仿真环境。它包含一个 MuJoCo 环境复制，如前面的截图所示，两个用于提高智能体鲁棒性的互动环境，以及一个多人游戏环境。'
- en: '**Duckietown** ([https://github.com/duckietown/gym-duckietown](https://github.com/duckietown/gym-duckietown)):
    A self-driving car simulator with different maps and obstacles.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Duckietown** ([https://github.com/duckietown/gym-duckietown](https://github.com/duckietown/gym-duckietown)):
    一个具有不同地图和障碍物的自动驾驶汽车模拟器。'
- en: '**PLE** ([https://github.com/ntasfi/PyGame-Learning-Environment](https://github.com/ntasfi/PyGame-Learning-Environment)):
    PLE includes many different arcade games, such as Monster Kong, FlappyBird, and
    Snake.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PLE** ([https://github.com/ntasfi/PyGame-Learning-Environment](https://github.com/ntasfi/PyGame-Learning-Environment)):
    PLE 包括许多不同的街机游戏，例如 Monster Kong、FlappyBird 和 Snake。'
- en: '**Unity ML-Agents** ([https://github.com/Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents)):
    Environments built on top of Unity with realistic physics. ML-agents allow a great
    degree of freedom and the possibility to create your own environment using Unity.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Unity ML-Agents** ([https://github.com/Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents)):
    基于 Unity 构建的环境，具有逼真的物理效果。ML-agents 提供了高度的自由度，并且可以使用 Unity 创建自己的环境。'
- en: '**CoinRun** ([https://github.com/openai/coinrun](https://github.com/openai/coinrun)):
    An environment that addresses the problem of overfitting in RL. It generates different
    environments for training and testing.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CoinRun** ([https://github.com/openai/coinrun](https://github.com/openai/coinrun)):
    一个解决强化学习中过拟合问题的环境。它生成不同的环境用于训练和测试。'
- en: '**DeepMind Lab** ([https://github.com/deepmind/lab](https://github.com/deepmind/lab)):
    Provides a suite of 3D environments for navigation and puzzle tasks.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DeepMind Lab** ([https://github.com/deepmind/lab](https://github.com/deepmind/lab)):
    提供了一套用于导航和解谜任务的 3D 环境。'
- en: '**DeepMind PySC2** ([https://github.com/deepmind/pysc2](https://github.com/deepmind/pysc2)):
    An environment for learning the complex game, StarCraft II.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DeepMind PySC2** ([https://github.com/deepmind/pysc2](https://github.com/deepmind/pysc2)):
    一个用于学习复杂游戏《星际争霸 II》的环境。'
- en: Summary
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: Hopefully, in this chapter, you have learned about all the tools and components
    needed to build RL algorithms. You set up the Python environment required to develop
    RL algorithms and programmed your first algorithm using an OpenAI Gym environment.
    As the majority of state-of-the-art RL algorithms involve deep learning, you have
    been introduced to TensorFlow, a deep learning framework that you'll use throughout
    the book. The use of TensorFlow speeds up the development of deep RL algorithms
    as it deals with complex parts of deep neural networks such as backpropagation.
    Furthermore, TensorFlow is provided with TensorBoard, a visualization tool that
    is used to monitor and help the algorithm debugging process.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 希望在本章中，你已经了解了构建强化学习算法所需的所有工具和组件。你已经设置了开发强化学习算法所需的 Python 环境，并使用 OpenAI Gym 环境编写了你的第一个算法。由于大多数先进的强化学习算法都涉及深度学习，你也已经接触到了
    TensorFlow——一本贯穿全书使用的深度学习框架。使用 TensorFlow 加速了深度强化学习算法的开发，因为它处理了深度神经网络中复杂的部分，比如反向传播。此外，TensorFlow
    提供了 TensorBoard，这是一个用于监控和帮助算法调试过程的可视化工具。
- en: Because we'll be using many environments in the subsequent chapters, it's important
    to have a clear understanding of their differences and distinctiveness. By now,
    you should also be able to choose the best environments for your own projects,
    but bear in mind that despite the fact that we provided you with a comprehensive
    list, there may be many others that could better suit your problem.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在接下来的章节中我们将使用许多不同的环境，所以理解它们的差异和独特性非常重要。到现在为止，你应该能够为自己的项目选择最佳的环境，但请记住，尽管我们为你提供了一个全面的列表，可能还有许多其他环境更适合你的问题。
- en: That being said, in the following chapters, you'll finally learn how to develop
    RL algorithms. Specifically, in the next chapter, you will be presented with algorithms
    that can be used in simple problems where the environment is completely known.
    After those, we'll build more sophisticated ones that can deal with more complex
    cases.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，在接下来的章节中，你将最终学会如何开发强化学习（RL）算法。具体来说，在下一章中，你将接触到可以用于环境完全已知的简单问题的算法。之后，我们将构建更复杂的算法，以应对更复杂的情况。
- en: Questions
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What's the output of the `step()` function in Gym?
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gym 中 `step()` 函数的输出是什么？
- en: How can you sample an action using the OpenAI Gym interface?
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何通过 OpenAI Gym 接口采样一个动作？
- en: What's the main difference between the `Box` and `Discrete` classses?
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Box` 和 `Discrete` 类之间的主要区别是什么？'
- en: Why are deep learning frameworks used in RL?
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么深度学习框架在强化学习中被使用？
- en: What's a tensor?
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是张量？
- en: What can be visualized in TensorBoard?
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 TensorBoard 中可以可视化什么内容？
- en: To create a self-driving car, which of the environments mentioned in the chapter
    would you use?
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果要创建一辆自动驾驶汽车，你会使用本章中提到的哪些环境？
- en: Further reading
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For the TensorFlow official guide, refer to the following link: [https://www.tensorflow.org/guide/low_level_intro](https://www.tensorflow.org/guide/low_level_intro).'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关 TensorFlow 官方指南，请参考以下链接：[https://www.tensorflow.org/guide/low_level_intro](https://www.tensorflow.org/guide/low_level_intro)。
- en: For the TensorBoard official guide, refer to the following link: [https://www.tensorflow.org/guide/summaries_and_tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard).
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关 TensorBoard 官方指南，请参考以下链接：[https://www.tensorflow.org/guide/summaries_and_tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard)。
