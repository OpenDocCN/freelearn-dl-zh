- en: '*Chapter 5*: Text Classification and Regression Using AutoKeras'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 5 章*：使用 AutoKeras 进行文本分类和回归'
- en: In this chapter, we will focus on the use of AutoKeras to work with text (a
    sequence of words).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点介绍使用 AutoKeras 处理文本（即一系列单词）的方式。
- en: In the previous chapter, we saw that there was a specialized type of network
    suitable for image processing, called a **convolutional neural network** (**CNN**).
    In this chapter, we will see what **recurrent neural networks** (**RNNs**) are
    and how they work. An RNN is a type of neural network that is very suited to working
    with text.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到有一种专门适用于图像处理的网络类型，叫做**卷积神经网络**（**CNN**）。在本章中，我们将了解什么是**递归神经网络**（**RNN**），以及它们如何工作。RNN
    是一种非常适合处理文本的神经网络类型。
- en: We will also use a classifier and a regressor to solve text-based tasks. By
    the end of the chapter, you will have learned how to use AutoKeras to solve a
    wide variety of problems that are text-based, such as extracting emotions from
    tweets, detecting spam in emails, and so on.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用分类器和回归器来解决基于文本的任务。到本章结束时，你将学会如何使用 AutoKeras 解决各种基于文本的问题，例如从推文中提取情绪、检测电子邮件中的垃圾邮件等。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主题：
- en: Working with text data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理文本数据
- en: Understanding RNNs—what are these neural networks and how do they work?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 RNN——这些神经网络是什么，它们如何工作？
- en: One-dimensional CNNs (Conv1D)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一维 CNN（Conv1D）
- en: Creating an email spam detector
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建电子邮件垃圾邮件检测器
- en: Predicting news popularity in social media
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测社交媒体新闻的流行度
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'All coding examples in this book are available as Jupyter notebooks that can
    be downloaded from the following link: [https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的所有代码示例都可以作为 Jupyter 笔记本下载，下载链接如下：[https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras)。
- en: As code cells can be executed, each notebook can be self-installable, by adding
    a code snippet with the requirements you need. For this reason, at the beginning
    of each notebook there is a code cell for environmental setup, which installs
    AutoKeras and its dependencies.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于代码单元格可以执行，每个笔记本都可以自我安装，只需添加包含所需依赖项的代码片段。因此，在每个笔记本的开始部分都有一个代码单元格用于环境设置，安装 AutoKeras
    及其依赖项。
- en: 'So, to run the coding examples, you only need a computer with Ubuntu Linux
    as the operating system and can install the Jupyter Notebook with the following
    command line:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要运行这些代码示例，你只需要一台操作系统为 Ubuntu Linux 的计算机，并且可以使用以下命令行安装 Jupyter Notebook：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Alternatively, you can also run these notebooks using Google Colaboratory, in
    which case you will only need a web browser. For further details, see the *AutoKeras
    with Google Colaboratory* section in [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029),
    *Getting Started with AutoKeras*. Furthermore, in the *Installing AutoKeras* section,
    you will also find other installation options.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你也可以使用 Google Colaboratory 运行这些笔记本，这时只需要一个网页浏览器。有关详细信息，请参见 [*第 2 章*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029)
    中的 *AutoKeras 与 Google Colaboratory* 部分，*AutoKeras 入门*。此外，在 *安装 AutoKeras* 部分，你还会找到其他安装选项。
- en: Working with text data
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理文本数据
- en: AutoKeras allows us to quickly and easily create high-performance models for
    solving text-based tasks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: AutoKeras 使我们能够快速轻松地创建高性能模型来解决基于文本的任务。
- en: 'Text is an excellent source of information to feed DL models, and there is
    a multitude of sources that are text-based, such as social media, chats, emails,
    articles, books, and countless tasks to automate based on text, such as the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 文本是喂养深度学习模型的优秀信息来源，存在大量基于文本的资源，例如社交媒体、聊天记录、电子邮件、文章、书籍，以及无数需要基于文本自动化的任务，如下所示：
- en: '**Translation**: Convert source text in one language to text in another language.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**翻译**：将一种语言的源文本转换为另一种语言的文本。'
- en: '**Conversational bots**: Simulate human conversation using ML models.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对话机器人**：使用机器学习模型模拟人类对话。'
- en: '**Sentiment analysis**: Classification of emotions by analyzing text data.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**：通过分析文本数据来分类情绪。'
- en: '**Spam classifiers**: Email classification using machine learning models.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**垃圾邮件分类器**：使用机器学习模型进行电子邮件分类。'
- en: '**Document summarizers**: Generate summaries of documents automatically.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档摘要生成器**：自动生成文档摘要。'
- en: '**Text generators**: Generate text from scratch automatically.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本生成器**：从零开始自动生成文本。'
- en: As with other types of data, AutoKeras will do all the preprocessing so that
    we can pass the text directly to our model, but before starting with the practical
    examples, let's take a look at what it does under the hood.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他类型的数据一样，AutoKeras会完成所有的预处理工作，这样我们就可以直接将文本传递给模型。在开始实际示例之前，让我们先看看它在幕后做了什么。
- en: Tokenization
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记化
- en: As we already know, neural networks take vectors of numbers as input, so the
    text must be converted to numerical Tensors in a process called **vectorization**.
    Before that, however, we must cut the text into segments.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，神经网络输入的是数值向量，因此文本必须通过一个叫做**向量化**的过程转换为数值张量。不过，在这之前，我们必须先将文本切分成片段。
- en: 'This text segmentation can be done with the help of different units, such as
    the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这种文本分割可以通过以下不同的单位来完成：
- en: '**Word**: Divide the text by words.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单词**：将文本按单词进行划分。'
- en: '**Character**: Fragment the text into characters.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字符**：将文本切分成字符。'
- en: '**N-gram**: Extract N-grams of words or characters. N-grams are overlapping
    groupings of multiple consecutive words or characters.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**N-gram**：提取词或字符的N-gram。N-gram是多个连续词或字符的重叠分组。'
- en: The units mentioned previously are called **tokens**, and the process of dividing
    the text into said tokens is called **tokenization**. This is a necessary step
    to convert the text to tensors in the vectorization process, which we explain
    next.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 前面提到的单位称为**标记**，将文本划分为这些标记的过程称为**标记化**。这是将文本转换为张量的必要步骤，接下来我们将解释这一过程。
- en: Vectorization
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量化
- en: Once the text is tokenized, vectorization is performed. This process transforms
    each word/character/N-gram into a vector.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文本被标记化，就会进行向量化。这个过程将每个单词/字符/N-gram转换为一个向量。
- en: 'All text vectorization processes consist of the following steps:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 所有文本向量化过程包括以下步骤：
- en: Applying some tokenization scheme
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用某种标记化方案
- en: Associating numeric vectors with the generated tokens
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数值向量与生成的标记关联
- en: These vectors, packed into sequence tensors, feed into **deep neural networks**
    (**DNNs**).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些向量被打包成序列张量，输入到**深度神经网络**（**DNNs**）中。
- en: 'There are multiple ways to associate a token with a vector. Let''s see two
    of the most important ones, as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方式可以将标记与向量关联。让我们来看两种最重要的方式，如下所示：
- en: '`1` and all the remaining values of the vector are zeros.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1` 和向量中的所有其他值都是零。'
- en: '**Token embedding** is another form of token-vector association that is widely
    used and is more powerful than one-hot encoding. While the vectors obtained by
    one-hot encoding are binary (one input with value *1* and the rest of values *0*)
    and large (they must have the same length as the number of words in the vocabulary),
    the embeddings of words are low-dimensional floating-point vectors.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记嵌入**是另一种广泛使用的标记-向量关联方式，且比One-hot编码更为强大。虽然One-hot编码得到的向量是二进制的（一个输入值为*1*，其余值为*0*），且长度很大（必须与词汇表中单词的数量相同），而词嵌入则是低维的浮点向量。'
- en: The word vectors obtained by one-hot encoding are static (the position in the
    array determines the word and these values never change), while the word-embedding
    vectors are dynamic (they are learned from the data), in such a way that their
    values are modified during learning, in the same way that they are with the weights
    of the neural network layers.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过One-hot编码得到的词向量是静态的（数组中的位置决定了单词，这些值永远不会改变），而词嵌入向量是动态的（它们是从数据中学习得到的），以至于它们的值在学习过程中会发生变化，就像神经网络层的权重一样。
- en: 'It is this dynamism that allows it to store more information in a much smaller
    size, as you can see in the following screenshot:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正是这种动态性使它能够以更小的尺寸存储更多信息，正如你在以下截图中看到的那样：
- en: '![Figure 5.1 – One-hot encoding versus embedding comparison](img/B16953_05_01.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – One-hot 编码与嵌入比较](img/B16953_05_01.jpg)'
- en: Figure 5.1 – One-hot encoding versus embedding comparison
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – One-hot 编码与嵌入比较
- en: Just as convolutional networks were the most appropriate choice for image-based
    tasks, when we talk about word processing, the most optimal type of network is
    an RNN. Let's see what this consists of in the following section.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 就像卷积网络是处理基于图像任务的最合适选择一样，在谈到文本处理时，最优化的网络类型是RNN。让我们在接下来的部分看看它的组成。
- en: Understanding RNNs
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解RNN
- en: 'A common feature of all the neural networks seen so far is that they don''t
    have a memory. Networks formed by either fully connected layers or convolutional
    layers process each input independently so that it is isolated from the other
    layers. However, in RNNs, "the past" is taken into account, and this is done using
    its previous output as the state; so, an RNN layer will have two inputs, one is
    which is the standard input of the current vector, and the other being the output
    of the previous vector, as seen in the following diagram:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，所有神经网络的共同特点是它们没有记忆。由全连接层或卷积层组成的网络独立地处理每个输入，使得每一层都是孤立的。然而，在RNN中，“过去”会被考虑进去，这通过使用上一个输出作为状态来完成；因此，RNN层将有两个输入，一个是当前向量的标准输入，另一个是前一个向量的输出，如下图所示：
- en: '![Figure 5.2 – RNN loop unfolded](img/B16953_05_02.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – RNN循环展开](img/B16953_05_02.jpg)'
- en: Figure 5.2 – RNN loop unfolded
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – RNN循环展开
- en: 'The RNN implements this memory feature with an internal loop over the entire
    sequence of elements. Let''s explain it with some pseudocode, as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: RNN通过对整个元素序列的内部循环来实现这个记忆特性。我们用一些伪代码来解释，如下所示：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: There are several types of RNN architectures with much more complex systems
    than the one presented here, but this is beyond the scope of the book. Understanding
    the concepts explained here is enough, since both the configuration and the choice
    of architecture to be used will be handled by AutoKeras.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有许多比本书所展示的更复杂的RNN架构，但这超出了本书的范围。理解这里解释的概念就足够了，因为配置和架构的选择将由AutoKeras来处理。
- en: One-dimensional CNNs (Conv1D)
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一维卷积神经网络（Conv1D）
- en: Another architecture to take into account when working with texts is one-dimensional
    CNNs (Conv1D). The principle on which they are based is similar to the 2D CNN
    that we saw in the previous chapter, [*Chapter 4*](B16953_04_Final_PG_ePub.xhtml#_idTextAnchor063),
    *Image Classification and Regression Using AutoKeras*. These neural networks manage
    to learn patterns in text through filters, in the same way as they did with images
    in the previous chapter.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 处理文本时需要考虑的另一种架构是一维卷积神经网络（Conv1D）。它们的原理与我们在上一章中看到的二维卷积神经网络类似，参见 [*第四章*](B16953_04_Final_PG_ePub.xhtml#_idTextAnchor063)，*使用AutoKeras的图像分类与回归*。这些神经网络通过滤波器学习文本中的模式，就像它们在上一章中学习图像模式一样。
- en: 'An example of a one-dimensional CNN is shown in the following diagram:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个一维卷积神经网络的示例：
- en: '![Figure 5.3 – 1D convolution over text sequences](img/B16953_05_03.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 1D卷积在文本序列上的应用](img/B16953_05_03.jpg)'
- en: Figure 5.3 – 1D convolution over text sequences
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 1D卷积在文本序列上的应用
- en: It is good to know that if the chronological order of the elements in the sequence
    is important for the prediction, the RNNs are much more effective, thus one-dimensional
    CNNs are often combined with the RNNs to create high-performance models. The exhaustive
    search performed by AutoKeras takes both into account to find the best model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 需要知道的是，如果序列中元素的时间顺序对于预测很重要，那么RNN的效果会更好，因此一维卷积神经网络常常与RNN结合使用，以创建高性能模型。AutoKeras进行的全面搜索会同时考虑这两者，以找到最佳模型。
- en: Now, let's put the learned concepts into practice with some practical examples.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过一些实际例子来实践所学的概念。
- en: Creating an email spam detector
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建电子邮件垃圾邮件检测器
- en: The model we are going to create will detect spam emails from an `emails` dataset.
    This is a little dataset of 5,572 emails, labeled with a `spam` column.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建的模型将从 `emails` 数据集中检测垃圾邮件。这是一个包含 5,572 封邮件的小型数据集，带有 `spam` 列标签。
- en: 'The notebook with the complete source code can be found at the following link:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 包含完整源代码的笔记本可以通过以下链接找到：
- en: '[https://colab.research.google.com/github/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter05/Chapter5_SpamDetector.ipynb](https://colab.research.google.com/github/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter05/Chapter5_SpamDetector.ipynb)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://colab.research.google.com/github/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter05/Chapter5_SpamDetector.ipynb](https://colab.research.google.com/github/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter05/Chapter5_SpamDetector.ipynb)'
- en: 'Let''s now have a look at the relevant cells of the notebook in detail, as
    follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们详细查看笔记本中的相关单元格，具体如下：
- en: '`pip` package manager:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pip` 包管理器：'
- en: '[PRE2]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`tensorflow`, `pandas`, `numpy`, and `autokeras` as needed dependencies for
    this project:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow`、`pandas`、`numpy` 和 `autokeras` 作为此项目所需的依赖项：'
- en: '[PRE3]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`emails` spam dataset from our GitHub repository, as follows:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自我们 GitHub 仓库的 `emails` 垃圾邮件数据集，详见如下：
- en: '[PRE4]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We now prepare our dataset by renaming the relevant columns and removing unnecessary
    ones, as follows:'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们现在通过重命名相关列并删除不必要的列来准备我们的数据集，如下所示：
- en: '[PRE5]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here is the output of the preceding code:'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是前面代码的输出：
- en: '![Figure 5.4 – Notebook output of dataset preview](img/B16953_05_04.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 数据集预览的笔记本输出](img/B16953_05_04.jpg)'
- en: Figure 5.4 – Notebook output of dataset preview
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 数据集预览的笔记本输出
- en: 'Let''s now split the dataset into `train` and `test` datasets, as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将数据集拆分为`train`和`test`数据集，如下所示：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We are ready to create the spam classifier.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已准备好创建垃圾邮件分类器。
- en: Creating the spam predictor
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建垃圾邮件预测器
- en: 'Now, we will use the AutoKeras `TextClassifier` class to find the best classification
    model. Just for this little example, we set `max_trials` (the maximum number of
    different Keras models to try) to 2, and we do not set the `epochs` parameter
    but rather define an `EarlyStopping` callback of `2` epochs, such that the training
    process stops if the loss of validation does not improve in two consecutive epochs.
    The code is shown in the following snippet:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用AutoKeras的`TextClassifier`类来找到最佳的分类模型。仅为了这个小示例，我们将`max_trials`（尝试的最大不同Keras模型数量）设置为2，并且不设置`epochs`参数，而是定义了一个`EarlyStopping`回调，当验证损失在连续两个epoch内没有改善时，训练过程就会停止。代码如下所示：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s run the training to search for the optimal classifier for the training
    dataset, as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始训练，以寻找训练数据集的最佳分类器，如下所示：
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here is the output of the preceding code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前面代码的输出：
- en: '![Figure 5.5 – Notebook output of text classifier training'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.5 – 文本分类器训练的笔记本输出](img/B16953_05_04.jpg)'
- en: '](img/B16953_05_05.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16953_05_05.jpg)'
- en: Figure 5.5 – Notebook output of text classifier training
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 文本分类器训练的笔记本输出
- en: The previous output shows that the accuracy with the training dataset is increasing.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的输出显示，使用训练数据集时，准确率在不断提高。
- en: As we can see, we achieved a loss value of `0.080` in the validation set. It's
    a really good number just for one minute of training. We have limited the search
    to two architectures (`max_trials = 2`). Increasing this number would give us
    a more accurate model, although it would also take longer to finish.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在验证集上我们达到了`0.080`的损失值。仅仅经过一分钟的训练，结果已经非常好。我们将搜索范围限制在两种架构内（`max_trials =
    2`）。增加该数字会使模型更准确，但也会增加训练时间。
- en: Evaluating the model
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'It''s time to evaluate the best model with the testing dataset. We can do this
    by running the following command:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候使用测试数据集评估最佳模型了。我们可以通过运行以下命令来实现：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here is the output of the preceding command:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前面命令的输出：
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As we can see, `0.9849` as prediction accuracy in the test set is a really good
    final prediction score for the time invested.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，`0.9849`的测试集预测准确率是非常好的最终预测得分，考虑到投入的时间，这个结果非常令人满意。
- en: Visualizing the model
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化模型
- en: 'Now, we can see a little summary of the architecture of the best generated
    model. We can do this by running the following code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到最佳生成模型的架构的简要总结。我们可以通过运行以下代码来实现：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here is the output of the preceding code:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前面代码的输出：
- en: '![Figure 5.6 – Best model architecture summary'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.6 – 最佳模型架构总结](img/B16953_05_07.jpg)'
- en: '](img/B16953_05_06.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16953_05_06.jpg)'
- en: Figure 5.6 – Best model architecture summary
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – 最佳模型架构总结
- en: As we can see here, AutoKeras has chosen a convolution model (Conv1D) to do
    the task. As we explained in [*Chapter 4*](B16953_04_Final_PG_ePub.xhtml#_idTextAnchor063),
    *Image Classification and Regression Using AutoKeras*, this kind of architecture
    works great when the order of the elements in the sequence is not important for
    the prediction, as in this case.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，AutoKeras选择了卷积模型（Conv1D）来完成这个任务。正如我们在[*第 4 章*](B16953_04_Final_PG_ePub.xhtml#_idTextAnchor063)《使用AutoKeras进行图像分类与回归》中解释的那样，这种架构在序列中元素的顺序对预测结果不重要时效果非常好，正如本例所示。
- en: 'Here is a visual representation of the architecture:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是架构的可视化表示：
- en: '![Figure 5.7 – Best model architecture visualization'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.7 – 最佳模型架构可视化](img/B16953_05_05.jpg)'
- en: '](img/B16953_05_07.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16953_05_07.jpg)'
- en: Figure 5.7 – Best model architecture visualization
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – 最佳模型架构可视化
- en: As you already know, generating the models and choosing the best one is a task
    done by AutoKeras automatically, but let's briefly explain these blocks.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你们所知，生成模型并选择最佳模型是AutoKeras自动完成的任务，但让我们简要解释一下这些模块。
- en: Each block represents a layer, and the output of each is connected to the input
    of the next, except the first block (whose input is the text) and the last block
    (whose output is the predicted number). The blocks before the Conv1D block are
    all data-preprocessing blocks to vectorize the text-generating embeddings to feed
    this Conv1D block, as well as reduce the dimension of the filters through the
    max pooling layer. Notice that AutoKeras has also added several dropout blocks
    to reduce overfitting.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 每个块代表一层，且每层的输出连接到下一层的输入，除了第一个块（其输入是文本）和最后一个块（其输出是预测的数字）。Conv1D块之前的所有块都是数据预处理块，用于将文本生成的嵌入向量化以馈送到Conv1D块，并通过最大池化层降低过滤器的维度。请注意，AutoKeras还添加了多个dropout块以减少过拟合。
- en: 'In the next section, we are going to resolve a text regression problem with
    a practical example: we are going to create a news popularity predictor.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将通过一个实际示例来解决文本回归问题：我们将创建一个新闻流行度预测器。
- en: Predicting news popularity in social media
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在社交媒体上预测新闻的流行度
- en: In this section, we will create a model that will find out the popularity score
    for an article on social media platforms, based on its text. For this, we will
    train the model with a *News Popularity* dataset collected between 2015 and 2016
    ([https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms](https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms)).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将创建一个模型，基于文章的文本来预测其在社交媒体平台上的流行度评分。为此，我们将使用2015至2016年间收集的*新闻流行度*数据集来训练模型（[https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms](https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms)）。
- en: As we want to approximate a score (number of likes), we will use a text regressor
    for this task.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望预测一个分数（点赞数），因此我们将使用文本回归器来完成这项任务。
- en: 'In the next screenshot, you can see some samples taken from this dataset:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个截图中，您可以看到从该数据集中提取的一些样本：
- en: '![Figure 5.8 – A few samples from the News Popularity dataset'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.8 – 来自新闻流行度数据集的几个样本](img/B16953_05_08.jpg)'
- en: '](img/B16953_05_08.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16953_05_08.jpg)'
- en: Figure 5.8 – A few samples from the News Popularity dataset
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 – 来自新闻流行度数据集的几个样本
- en: This notebook with the complete source code can be found at [https://colab.research.google.com/github/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter05/Chapter5_SpamDetector.ipynb](https://colab.research.google.com/github/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter05/Chapter5_SpamDetector.ipynb).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个包含完整源代码的笔记本可以在[https://colab.research.google.com/github/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter05/Chapter5_SpamDetector.ipynb](https://colab.research.google.com/github/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter05/Chapter5_SpamDetector.ipynb)找到。
- en: 'We will now explain the relevant code cells of the notebook in detail, as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将详细解释笔记本中的相关代码单元，如下所示：
- en: '**Getting the articles dataset**: Before training, we have to download the
    dataset that contains the text of each article, as well as the popularity score.
    Here is the code to do this:'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**获取文章数据集**：在训练之前，我们必须下载包含每篇文章文本及其流行度评分的数据集。以下是实现这一目的的代码：'
- en: '[PRE12]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`Title` and `Headline` text columns to feed our regression model.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Title`和`Headline`文本列将用来馈送给我们的回归模型。'
- en: 'Now, we extract the popularity score of each article on LinkedIn, to be used
    as labels. We have decided to use only the LinkedIn scores to simplify the example.
    The code is shown in the following snippet:'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们提取每篇文章在LinkedIn上的流行度评分，作为标签。为了简化示例，我们决定只使用LinkedIn上的评分。代码如下所示：
- en: '[PRE13]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we will create the train and test datasets, as follows:'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们将创建训练集和测试集，如下所示：
- en: '`train` and `test` set using the `sklearn` function, as follows:'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`sklearn`函数创建`train`和`test`数据集，如下所示：
- en: '[PRE14]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Once we have created our datasets we are ready to feed our model, but first,
    we have to create it.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了数据集，就可以将其输入模型，但首先，我们需要创建模型。
- en: Creating a text regressor
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建文本回归器
- en: 'Because we want to predict a popularity score from a set of text sentences,
    and this score is a scalar value, we are going to use AutoKeras `TextRegressor`.
    For this example, we set `max_trials` to `2`, and we do not set the `epochs` parameter
    but rather define an `EarlyStopping` callback of `2` epochs of patience, such
    that the training process stops if the validation loss does not decrease in two
    consecutive epochs. The code can be seen in the following snippet:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们想从一组文本句子中预测一个流行度分数，而这个分数是一个标量值，所以我们将使用AutoKeras的`TextRegressor`。在这个示例中，我们将`max_trials`设置为`2`，并且没有设置`epochs`参数，而是定义了一个`EarlyStopping`回调，耐心等待`2`个epoch，即如果验证损失在两个连续的epoch中没有减少，训练过程将停止。代码可以在以下代码片段中看到：
- en: '[PRE15]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s run the training to search for the optimal regressor for the training
    dataset, as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行训练，搜索训练数据集的最佳回归器，如下所示：
- en: '[PRE16]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here is the output of the preceding code:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前面代码的输出：
- en: '![Figure 5.9 – Notebook output of the training of our news popularity predictor](img/B16953_05_09.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图5.9 – 我们的新闻流行度预测器训练的笔记本输出](img/B16953_05_09.jpg)'
- en: Figure 5.9 – Notebook output of the training of our news popularity predictor
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 – 我们的新闻流行度预测器训练的笔记本输出
- en: As we can see from the previous output, after a few minutes we have a model
    with `14726` as the best validation loss (`121` (square root of `14726`) in the
    final score, which is not a bad result for the time invested. Let's see how it's
    working with the test set.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的输出中可以看到，几分钟后，我们得到一个模型，最佳验证损失为`14726`（最终得分中的`121`是`14726`的平方根，对于投入的时间来说，这是一个不错的结果。接下来看看它在测试集上的表现。
- en: Evaluating the model
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Time to evaluate the best model with the testing dataset. We do this by running
    the following code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候使用测试数据集评估最佳模型了。我们通过运行以下代码来完成：
- en: '[PRE17]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here is the output of the preceding code:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前面代码的输出：
- en: '[PRE18]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As we can see, `13944` is a really good prediction score for the time invested.
    If we run AutoKeras with more trials, we will get better results.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，`13944`是一个非常好的预测得分，考虑到投入的时间。如果我们使用更多的试验来运行AutoKeras，我们会得到更好的结果。
- en: Visualizing the model
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化模型
- en: 'Now, it''s time to take a look at what we have under the hood. We''ll run the
    following code:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候看看我们引擎盖下的内容了。我们将运行以下代码：
- en: '[PRE19]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here is the output of the preceding code:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前面代码的输出：
- en: '![Figure 5.10 – Best model architecture summary'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.10 – 最佳模型架构摘要'
- en: '](img/B16953_05_10.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16953_05_10.jpg)'
- en: Figure 5.10 – Best model architecture summary
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 – 最佳模型架构摘要
- en: As in the previous classification example, AutoKeras has chosen a convolution
    model (Conv1D) to do the task. As we explained before, this is a less time-consuming
    architecture than RNN and is most suitable when the order of the elements in the
    sequence is not important for the prediction.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如同前面的分类示例一样，AutoKeras选择了卷积模型（Conv1D）来完成任务。正如我们之前解释的，这比RNN架构更节省时间，且当序列中元素的顺序对于预测并不重要时，它是最合适的。
- en: Improving the model performance
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升模型性能
- en: As we did in previous examples, if we need more precision in less time, we can
    fine-tune our model using an advanced AutoKeras feature that allows you to customize
    your search space.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在之前的示例中所做的那样，如果我们需要更高的精度并且希望节省时间，我们可以通过使用AutoKeras的高级功能来微调我们的模型，允许你自定义搜索空间。
- en: By using `AutoModel` with `TextBlock` instead of `TextRegressor`, we can create
    high-level configurations, such as `block_type` for the type of neural network
    to look for; or, if your text source has a larger vocabulary (number of distinct
    words), you may need to create a custom pipeline in AutoKeras to increase the
    `max_tokens` parameter.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`AutoModel`与`TextBlock`代替`TextRegressor`，我们可以创建高级配置，例如`block_type`来指定要查找的神经网络类型；或者，如果你的文本源有更大的词汇表（不同词汇的数量），你可能需要在AutoKeras中创建一个自定义的流水线来增加`max_tokens`参数。
- en: 'See the following example for more details:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 详情请见以下示例：
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In the previous code block, we have done the following with the settings:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们已经根据设置进行了以下操作：
- en: The `EarlyStopping` block will stop the training if the validation loss doesn't
    decrease in two consecutive epochs.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EarlyStopping`块将在验证损失在两个连续的epoch中没有减少时停止训练。'
- en: The `max_token` parameter is set to `20000` because our text source has a larger
    vocabulary (number of distinct words).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_token`参数设置为`20000`，因为我们的文本源有一个较大的词汇表（不同词汇的数量）。'
- en: With `TextBlock(block_type="ngram"`, we are telling AutoKeras to only scan models
    using N-gram embeddings.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`TextBlock(block_type="ngram"`时，我们告诉AutoKeras仅扫描使用N-gram嵌入的模型。
- en: You can also not specify any of these arguments, in which case these different
    options would be tuned automatically.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以选择不指定这些参数，这样不同的选项将会自动调整。
- en: Evaluating the model with the test set
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用测试集评估模型
- en: 'After training, it is time to measure the actual prediction of our model using
    the reserved test dataset. In this way, we can rule out that the good results
    obtained with the training set are due to overfitting. Run the following code
    to do this:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，是时候使用保留的测试数据集来测量我们模型的实际预测效果了。这样，我们可以排除训练集上获得的良好结果是由于过拟合造成的。运行以下代码来实现这一点：
- en: '[PRE21]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here is the output of the preceding code:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是前面代码的输出：
- en: '[PRE22]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The performance is slightly better than in the model without fine-tuning, but
    training it for a longer time surely improves it.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 性能稍微好于没有微调的模型，但肯定通过更长时间的训练会进一步提升它。
- en: Summary
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have learned how neural networks work with text data, what
    recurrent neural networks are and how they work.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了神经网络如何与文本数据一起工作，什么是循环神经网络以及它们如何工作。
- en: We've also put the concept of neural network into practice, using the power
    of AutoKeras, by implementing a spam predictor and a news popularity regressor,
    in just a few lines of code.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还通过使用AutoKeras的强大功能，将神经网络的概念付诸实践，实现了一个垃圾邮件预测器和一个新闻热门度回归器，只用了几行代码。
- en: Now that we have learned how to work with text, we are ready to move on to the
    next chapter, where you will learn how to work with structured data by implementing
    classification and regression models using AutoKeras.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经学会了如何处理文本数据，我们准备进入下一章，在这一章中你将学习如何通过使用AutoKeras实现分类和回归模型来处理结构化数据。
