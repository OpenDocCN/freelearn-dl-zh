- en: '*Chapter 11*: Generalization and Domain Randomization'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第11章*：泛化与领域随机化'
- en: Deep **reinforcement** **learning** (**RL**) has achieved what was impossible
    with earlier AI methods, such as beating world champions in games such as Go,
    Dota 2, and StarCraft II. Yet, applying RL to real-world problems is still challenging.
    Two key obstacles that stand in the way of this goal are the generalization of
    trained policies to a broad set of environment conditions and developing policies
    that can handle partial observability. As we will see in this chapter, these are
    closely related challenges, for which we will present solution approaches.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 深度**强化学习**（**RL**）已经实现了早期 AI 方法无法做到的事情，例如在围棋、Dota 2 和星际争霸 II 等游戏中击败世界冠军。然而，将
    RL 应用于现实问题仍然充满挑战。实现这一目标的两个关键障碍是将训练好的策略推广到广泛的环境条件，并开发能够处理部分可观测性的策略。正如我们将在本章中看到的，这两个挑战紧密相关，我们将提出解决方案。
- en: 'Here is what we will cover in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将覆盖以下内容：
- en: An overview of generalization and partial observability
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泛化与部分可观测性的概述
- en: Domain randomization for generalization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泛化的领域随机化
- en: Using memory to overcome partial observability
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用记忆克服部分可观测性
- en: These topics are critical to understand to ensure the successful implementation
    of RL in real-world settings. So, let's dive right in!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这些主题对理解 RL 在现实世界应用中的成功实施至关重要。让我们立即深入探讨吧！
- en: An overview of generalization and partial observability
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 泛化与部分可观测性的概述
- en: As in all machine learning, we want our RL models to work beyond training, and
    under a broad set of conditions during test time. Yet, when you start learning
    about RL, the concept of overfitting is not at the forefront of the discussion
    as opposed to supervised learning. In this section, we will compare overfitting
    and generalization between supervised learning and RL, describe how generalization
    is closely related to partial observability, and present a general recipe to handle
    these challenges.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如所有机器学习一样，我们希望我们的 RL 模型不仅能在训练数据上工作，还能在测试时应对广泛的条件。然而，当你开始学习 RL 时，过拟合的概念不像在监督学习中那样被优先讨论。在本节中，我们将比较监督学习和
    RL 中的过拟合与泛化，描述泛化与部分可观测性之间的密切关系，并提出一个通用方法来应对这些挑战。
- en: Generalization and overfitting in supervised learning
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习中的泛化与过拟合
- en: 'One of the most important goals in supervised learning, such as in image recognition
    and forecasting, is to prevent overfitting and achieve high accuracy on unseen
    data – after all, we already know the labels in our training data. We use various
    methods to this end:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习中最重要的目标之一，例如在图像识别和预测中，就是防止过拟合，并在未见过的数据上获得高准确性——毕竟，我们已经知道训练数据中的标签。为此，我们使用各种方法：
- en: We use separate training, dev, and test sets for model training, hyperparameter
    selection, and model performance assessment, respectively. The model should not
    be modified based on the test set so that it gives a fair assessment of the model's
    performance.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们为模型训练、超参数选择和模型性能评估分别使用独立的训练集、开发集和测试集。模型不应根据测试集进行修改，以确保对模型性能的公正评估。
- en: We use various regularization methods, such as penalizing model variance (for
    example, L1 and L2 regularization) and dropout methods, to prevent overfitting.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用各种正则化方法，例如惩罚模型方差（例如 L1 和 L2 正则化）和丢弃法，以防止过拟合。
- en: We use as much data as possible to train the model, which itself has a regularization
    effect. In the absence of enough data, we leverage data augmentation techniques
    to generate more.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们尽可能使用大量数据来训练模型，这本身就具有正则化作用。在数据不足的情况下，我们利用数据增强技术生成更多数据。
- en: We aim to have a dataset that is diverse and of the same distribution as the
    kind of data we expect to see after model deployment.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的目标是拥有一个多样化的数据集，并且该数据集与我们在模型部署后期望看到的数据分布相同。
- en: These concepts come up at the very beginning when you learn about supervised
    learning, and they form the basis of how to train a model. In RL, though, we don't
    seem to have the same mindset when it comes to overfitting.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念在你学习监督学习时就会出现，并且它们构成了如何训练模型的基础。然而，在 RL 中，我们似乎并没有以相同的心态来看待过拟合。
- en: Let's go into a bit more detail about what is different in RL, why, and whether
    generalization is actually of secondary importance or not.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解 RL 中的不同之处，探讨原因，以及泛化是否真的不那么重要。
- en: Generalization and overfitting in RL
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RL 中的泛化与过拟合
- en: Deep supervised learning notoriously requires a lot of data. But the hunger
    for data in deep RL dwarfs the need in deep supervised learning due to the noise
    in feedback signals and the complex nature of RL tasks. It is not uncommon to
    train RL models over billions of data points, and over many months. Since it is
    not quite possible to generate such large data using physical systems, deep RL
    research has leveraged digital environments, such as simulations and video games.
    This has blurred the distinction between training and testing.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 深度监督学习众所周知需要大量的数据。但深度强化学习对数据的需求远远超过了深度监督学习，因为反馈信号中存在噪声，而且强化学习任务本身的复杂性。训练强化学习模型常常需要数十亿的数据点，并且持续多个月。由于使用物理系统生成如此庞大的数据几乎不可能，深度强化学习研究已经利用了数字环境，如仿真和视频游戏。这模糊了训练与测试之间的界限。
- en: 'Think about it: if you train an RL agent to play an Atari game, such as Space
    Invaders, well (which is what most RL algorithms are benchmarked against) and
    use a lot of data for that, then if the agent plays Space Invaders very well after
    training, is there any problem with it? Well, in this case, no. However, as you
    may have realized, such a training workflow does not involve any effort to prevent
    overfitting as opposed to a supervised learning model training workflow. It may
    just be that your agent could have memorized a wide variety of scenes in the game.
    If you only care about Atari games, it may seem like overfitting is not a concern
    in RL.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 想一想：如果你训练一个强化学习代理来玩雅达利游戏，比如《太空入侵者》（Space Invaders），并且训练得非常好（这是大多数强化学习算法的基准测试），并且使用了大量的数据，那么如果代理在训练后玩《太空入侵者》非常好，这样有问题吗？嗯，在这种情况下，没有问题。然而，正如你可能已经意识到的，这样的训练工作流程并没有像监督学习模型训练那样采取任何措施来防止过拟合。可能你的代理已经记住了游戏中的各种场景。如果你只关心雅达利游戏，那么看起来强化学习中的过拟合似乎并不是一个问题。
- en: When we depart from Atari settings and, say, train an agent to beat human competitors,
    such as in Go, Dota 2, and StarCraft II, overfitting starts to appear as a bigger
    concern. As we saw in [*Chapter 9*](B14160_09_Final_SK_ePub.xhtml#_idTextAnchor200),
    *Multi-Agent Reinforcement Learning*, these agents are usually trained through
    self-play. A major danger for the agents in this setting is them overfitting to
    each other's strategies. To prevent this, we usually train multiple agents and
    make them play against each other in phases so that a single agent sees a diverse
    set of opponents (the environment from the agent's perspective) and there is less
    chance of overfitting.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们离开雅达利（Atari）环境，例如训练一个代理去击败人类竞争者，比如围棋、Dota 2 和星际争霸 II，过拟合开始成为一个更大的问题。正如我们在[*第9章*](B14160_09_Final_SK_ePub.xhtml#_idTextAnchor200)中看到的，*多智能体强化学习*，这些代理通常是通过自我对弈进行训练的。在这种环境下，一个主要的危险是代理会过拟合彼此的策略。为了防止这种情况，我们通常会训练多个代理，并让它们分阶段地相互对战，这样一个代理就能遇到多样化的对手（从代理的视角来看就是环境），从而减少过拟合的机会。
- en: Overfitting becomes a huge concern in RL, much more than the two situations
    we mentioned previously, when we train models in a simulation and deploy them
    in a physical environment. This is because, no matter how high-fidelity it is,
    a simulation is (almost) never the same as the real world. This is called the
    **sim2real gap**. A simulation involves many assumptions, simplifications, and
    abstractions. It is only a model of the real world, and as we all know, all models
    are wrong. We work with models everywhere, so why has this all of a sudden become
    a major concern in RL? Well, again, the training and RL agent require lots and
    lots of data (and that's why we needed a simulation in the first place), and training
    any ML model over similar data for a long time is a recipe for overfitting. So,
    RL models are extremely likely to overfit to the patterns and quirks of the simulation.
    There, we really need RL policies to generalize beyond the simulation for them
    to be useful. This is a serious challenge for RL in general, and one of the biggest
    obstacles in the way of bringing RL to real applications.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合在强化学习（RL）中成为了一个巨大的问题，远远超过了我们之前提到的两种情况，即我们在仿真环境中训练模型并将其部署到物理环境中。这是因为，无论仿真有多高保真度，它（几乎）永远不会与现实世界完全相同。这就是所谓的**sim2real
    gap**（仿真到现实的差距）。仿真涉及许多假设、简化和抽象。它只是现实世界的一个模型，正如我们都知道的，所有模型都是错误的。我们无处不在地使用模型，那为什么这一下子在强化学习中变成了一个主要问题呢？嗯，原因在于，训练和强化学习代理需要大量的数据（这也是我们最初需要仿真环境的原因），而在相似数据上长期训练任何机器学习模型都会导致过拟合。因此，强化学习模型极有可能会过拟合仿真环境中的模式和怪癖。在这种情况下，我们真的需要强化学习策略能够超越仿真环境，从而使其变得有用。这对强化学习来说是一个严峻的挑战，也是将强化学习应用于实际应用中的最大障碍之一。
- en: The sim2real gap is a concept that is closely related to partial observability.
    Let's look at this connection next.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: sim2real 差距是一个与部分可观察性密切相关的概念。接下来我们将讨论这一联系。
- en: The connection between generalization and partial observability
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 泛化与部分可观察性之间的联系
- en: 'We mentioned that a simulation is never the same as the real world. Two forms
    that this can manifest itself in are as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到过，模拟永远不可能和现实世界完全相同。这种差异可以通过以下两种形式表现出来：
- en: In some problems, you will never get the exact same observations in a simulation
    that you would get in the real world. Training a self-driving car is an example
    of that. The real scenes will always be different.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一些问题中，你永远无法在模拟中得到与现实世界完全相同的观察结果。训练自动驾驶汽车就是一个例子。现实场景总是不同的。
- en: In certain problems, you can train the agent on the same observations as you
    would see in the real world – for example, a factory production planning problem
    where the observation is the demand outlook, current inventory levels, and machine
    states. If you know what the ranges for your observations are, you can design
    your simulation to reflect that. Still, simulation and real life will be different.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些问题中，你可以训练代理在与现实世界中看到的相同观察下进行操作——例如一个工厂生产规划问题，其中观察到的内容包括需求预测、当前库存水平和机器状态。如果你知道观察数据的范围，你可以设计模拟以反映这一点。然而，模拟和现实生活总会有所不同。
- en: 'In the former case, it is more obvious why your trained agent may not generalize
    well beyond a simulation. However, the situation with the latter is a bit more
    subtle. In that case, although observations are the same, the world dynamics may
    not be between the two environments (admittedly, this will be also a problem for
    the former case). Can you recall what this is similar to? Partial observability.
    You can think of the gap between simulation and real world as a result of partial
    observability: there is a state of the environment hidden from the agent affecting
    the transition dynamics. So, even though the agent makes the same observation
    in simulation and the real world, it does not see this hidden state that we assume
    to capture the differences between them.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一种情况中，你的训练代理可能在模拟之外无法很好地泛化，这一点更为明显。然而，后一种情况就有些微妙了。在这种情况下，尽管观察结果相同，但两种环境之间的世界动态可能并不一致（诚然，这对于前一种情况也会是一个问题）。你能回忆起这和什么相似吗？部分可观察性。你可以将模拟与现实世界之间的差距视为部分可观察性的结果：有一个环境的状态被隐藏，影响了转移动态。所以，即使代理在模拟和现实世界中做出相同的观察，它并未看到这个我们假设用来捕捉两者差异的隐藏状态。
- en: Because of this connection, we cover generalization and partial observability
    together in this chapter. Having said that, generalization could be still of concern
    even in fully observable environments, and we may have to deal with partial observability
    even when generalization is not a major concern. We will look into these dimensions
    later.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正因为有这种联系，我们在本章中将泛化和部分可观察性一并讨论。话虽如此，即使在完全可观察的环境中，泛化仍然可能是一个问题，而即便泛化不是主要问题，我们也可能需要处理部分可观察性。我们稍后会探讨这些维度。
- en: Next, let's briefly discuss how we can tackle each of these challenges, before
    going into the details in later sections.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们简要讨论如何解决这些挑战，然后再深入到后续章节的细节中。
- en: Overcoming partial observability with memory
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过记忆克服部分可观察性
- en: Do you remember what it was like when you first entered your high school classroom?
    Chances are there were a lot of new faces and you wanted to make friends. However,
    you would not want to approach people just based on your first impression. Although
    a first impression does tell us something about people, it reflects only part
    of who they are. What you really want to do is to make observations over time
    before you make a judgment.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你还记得第一次进入高中教室时的感觉吗？很可能有很多新面孔，你希望交朋友。然而，你不会仅凭第一印象就接近别人。尽管第一印象确实能让我们了解一些关于人的信息，但它仅仅反映了他们的一部分。你真正想做的是随着时间的推移进行观察，然后再做判断。
- en: 'The situation is similar in the context of RL. Take this example from the Atari
    game Breakout:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）背景下情况类似。以下是来自 Atari 游戏《Breakout》的一个例子：
- en: '![Figure 11.1 – A single frame of an Atari game gives a partial observation
    of the environment'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.1 – Atari 游戏的单一帧画面提供了环境的部分观察'
- en: '](img/B14160_11_01.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_11_01.jpg)'
- en: Figure 11.1 – A single frame of an Atari game gives a partial observation of
    the environment
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 – Atari 游戏的单一帧画面提供了环境的部分观察
- en: It is not very clear where the ball is moving toward from a single game scene.
    If we had another snapshot from a previous timestep, we could estimate the change
    in position and the velocity of the ball. One more snapshot could help us estimate
    the change in velocity, acceleration, and so on. So, when an environment is partially
    observable, taking actions based on not a single observation but a sequence of
    them results in more informed decisions. In other words, having a **memory** allows
    an RL agent to uncover what is not observable in a single observation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从单一游戏场景中并不十分清楚球的移动方向。如果我们有来自先前时刻的另一张快照，我们就能估算球的位置变化和速度变化。再多一张快照，能够帮助我们估算速度变化、加速度等等。因此，当环境是部分可观察的时，根据不仅是单一观察，而是一系列观察采取行动，会导致更有依据的决策。换句话说，拥有**记忆**使得RL代理能够发现单一观察中不可见的部分。
- en: There are different ways of maintaining memory in RL models, and we will discuss
    them in more detail later in the chapter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在RL模型中，有多种方式可以保持记忆，我们将在本章稍后详细讨论这些方法。
- en: Before doing so, let's also briefly discuss how to overcome overfitting.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之前，让我们简要讨论如何克服过拟合。
- en: Overcoming overfitting with randomization
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过随机化克服过拟合
- en: If you are a car driver, think about how you have gained experience over time
    about driving under different conditions. You may have driven small cars, large
    vehicles, ones that accelerate fast and slow, ones that are high and low riding,
    and so on. In addition, you probably have had driving experience in the rain,
    maybe snow, and on asphalt and gravel. I personally have had these experiences.
    So when I took a test drive with a Tesla Model S for the first time, it felt like
    a quite different experience at first. But after several minutes, I got used to
    it and started driving pretty comfortably.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一名司机，想想你是如何在不同条件下获得驾驶经验的。你可能开过小车、大车、加速快慢不一的车、车身高低不同的车，等等。此外，你可能还在雨天、雪天、沥青路面和碎石路面上开过车。我个人就有过这些经历。所以，当我第一次试驾特斯拉Model
    S时，刚开始确实感觉是一种完全不同的体验。但几分钟后，我就习惯了，并开始驾驶得相当舒适。
- en: 'Now, as a driver, we often cannot define precisely what differs from one vehicle
    to another: what the exact differences in weight, torque, traction, and so on
    are that make the environment partially observable for us. But our past experience
    under diverse driving conditions helps us quickly adapt to new ones after several
    minutes of driving. How does this happen? Our brains are able to develop a general
    physics model for driving (and act accordingly) when the experience is diverse,
    rather than "overfitting" the driving style to a particular car model and condition.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，作为一名司机，我们通常无法精确地定义一辆车与另一辆车之间的差异：它们在重量、扭矩、牵引力等方面的确切差异，使得我们在面对环境时会有部分不可见的感知。但是，我们在多样化驾驶条件下的过去经验帮助我们在开车几分钟后快速适应新的环境。这是如何发生的呢？我们的脑袋能够为驾驶过程建立一个通用的物理模型（并作出相应的行为），当经历足够多样时，而不是“过拟合”某种驾驶风格到特定的车和条件上。
- en: The way we will deal with overfitting and achieve generalization will be similar
    for our RL agents. We will expose the agent to many different environment conditions,
    including ones that it cannot necessarily fully observe, which is called **domain
    randomization** (**DR**). This will give the agent the experience that is necessary
    to generalize beyond the simulation and the conditions it is trained under.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们处理过拟合并实现泛化的方法在我们的RL（强化学习）代理中是类似的。我们将使代理暴露于许多不同的环境条件，包括那些它不一定能够完全观察到的条件，这被称为**领域随机化**（**DR**）。这将为代理提供超越模拟及其训练条件的泛化所必需的经验。
- en: In addition to that, the regularization methods we use in supervised learning
    are also helpful with RL models, as we will discuss.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，我们在监督学习中使用的正则化方法对于RL模型也非常有帮助，正如我们将要讨论的那样。
- en: Next, let's summarize our discussion related to generalization in the next section.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们在下一节中总结与泛化相关的讨论。
- en: Recipe for generalization
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 泛化的配方
- en: 'As it should be clear now after going through the preceding examples, we need
    three ingredients to achieve generalization:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如今通过前面的示例，应该已经很清楚，我们实现泛化需要三种成分：
- en: Diverse environment conditions to help the agent enrich its experience
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多样化的环境条件帮助代理丰富其经验
- en: Model memory to help the agent uncover the underlying conditions in the environment,
    especially if the environment is partially observable
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型记忆帮助代理发现环境中的潜在条件，尤其是当环境是部分可观察时
- en: Using regularization methods as in supervised learning
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用类似于监督学习中的正则化方法
- en: Now it is time to make the discussion more concrete and talk about specific
    methods to handle generalization and partial observability. We start with using
    domain randomization for generalization, and then discuss using memory to overcome
    partial observability.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候让讨论更具体，谈论如何处理泛化和部分可观测性的方法了。我们从使用域随机化进行泛化开始，然后讨论使用记忆克服部分可观测性。
- en: Domain randomization for generalization
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 域随机化用于泛化
- en: 'We mentioned previously how diversity of experience helps with generalization.
    In RL, we achieve this by randomizing the environment parameters during training,
    known as DR. Examples of these parameters, say, for a robot hand that carries
    and manipulates objects, could be as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，经验的多样性有助于泛化。在强化学习（RL）中，我们通过在训练过程中随机化环境参数来实现这一点，这被称为 DR。这些参数的例子，比如对于一个搬运和操作物体的机器人手来说，可能如下：
- en: Friction coefficient on the object surface
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体表面的摩擦系数
- en: Gravity
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重力
- en: Object shape and weight
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体的形状和重量
- en: Power going into actuators
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入执行器的功率
- en: DR is especially popular in robotics applications to overcome the sim2real gap
    since the agents are usually trained in a simulation and deployed in the physical
    world. However, whenever generalization is of concern, DR is an essential part
    of the training process.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DR）在机器人应用中尤其受欢迎，因为它能够克服仿真与现实之间的差距，因为智能体通常在仿真环境中训练，并在现实世界中部署。然而，每当涉及到泛化时，DR
    是训练过程中必不可少的一部分。
- en: In order to get more specific about how environment parameters are randomized,
    we need to discuss how two environments representing the same type of problems
    could differ.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更具体地说明环境参数是如何随机化的，我们需要讨论如何表示相同类型问题的两个环境可能会有所不同。
- en: Dimensions of randomization
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机化的维度
- en: Borrowed from *Rivilin*, *2019*, a useful categorization of differences between
    similar environments is shown in the following sections.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从*Rivilin*（2019）借鉴的，下面的章节展示了相似环境之间差异的有用分类。
- en: Different observations for the same/similar states
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相同/相似状态下的不同观察结果
- en: In this case, two environments emit different observations although the underlying
    state and transition functions are the same or very similar. An example of this
    is the same Atari game scene but with different backgrounds and texture colors.
    Nothing is different in the game state, but the observations are different. A
    more realistic example would be, while training a self-driving car, having a more
    "cartoonish" look in the simulation versus a realistic one from the real camera
    input even for the exact same scene.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，两个环境发出的观察结果不同，尽管底层的状态和转移函数是相同的或非常相似的。一个例子是相同的Atari游戏场景，但背景和纹理颜色不同。游戏状态没有变化，但观察结果有所不同。一个更现实的例子是，当训练自动驾驶汽车时，仿真中的场景呈现“卡通化”外观，而在真实摄像头输入下却呈现真实的外观，即使是完全相同的场景。
- en: Solution – adding noise to observations
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解决方案——向观察结果中添加噪声
- en: In these cases, what can help with generalization is to add noise to observations,
    so that the RL model focuses on the patterns in the observations that actually
    matter, rather than overfitting to irrelevant details. Soon, we will discuss a
    specific approach, called **network randomization**, that will address this problem.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，帮助泛化的方法是向观察结果中添加噪声，这样强化学习模型就能专注于那些实际重要的观察模式，而不是对无关细节进行过拟合。很快，我们将讨论一种具体的方法，叫做**网络随机化**，它将解决这个问题。
- en: Same/similar observations for different states
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相同/相似的观察结果对应不同的状态
- en: 'Another way that POMDPs could differ from each other is when observations are
    the same/similar, but the underlying states are actually different, also called
    **aliased states**. A simple example is two separate versions of the mountain
    car environment with the exact same look but different gravity. These situations
    are pretty common in practice with robotics applications. Consider the manipulation
    of objects with a robot hand, as in the famous OpenAI work (which we will discuss
    in detail later in the chapter): the friction, weight, actual power going into
    the actuators, and so on could differ between different environment versions in
    ways that are not observable to the agent.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: POMDP（部分可观察马尔可夫决策过程）之间的另一个区别是，当观察结果相同或相似时，底层状态实际上是不同的，这也叫做**同态状态**（aliased states）。一个简单的例子是两个不同版本的山地车环境，外观完全相同，但重力不同。这种情况在机器人应用中非常常见。考虑用机器人手操作物体，正如OpenAI的著名工作所展示的（我们将在本章后面详细讨论）：摩擦、重量、输入执行器的实际功率等，可能在不同的环境版本之间有所不同，而这些差异对智能体是不可观察的。
- en: Solution – training with randomized environment parameters and using memory
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解决方案 – 在随机化的环境参数下进行训练，并使用记忆
- en: 'The approach to take here is to train the agent in many different versions
    of the environment with different underlying parameters, along with memory in
    the RL model. This will help the agent uncover the underlying environment characteristics.
    A famous example here is OpenAI''s robot hand manipulating objects trained in
    simulation, which is prone to the sim2real gap. We can overcome this by randomizing
    simulation parameters for the following scenario:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里应该采取的方法是，在多个不同版本的环境中进行训练，每个版本有不同的底层参数，并在强化学习（RL）模型中加入记忆。这将帮助智能体揭示环境的潜在特征。一个著名的例子是OpenAI的机器人手
    manipulating objects（操作物体）在模拟中训练，容易受到模拟与现实之间差距（sim2real gap）的影响。我们可以通过随机化模拟参数来克服这种差距，适用于以下情境：
- en: When used with memory, and having been trained in a vast majority of environment
    conditions, the policy will hopefully gain the ability of adapting to the environment
    it finds itself in.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当与记忆结合使用，并且经过大多数环境条件的训练后，策略有望获得适应所处环境的能力。
- en: Different levels of complexity for the same problem class
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 同一问题类别的不同复杂度层次
- en: This is the case where we are essentially dealing with the same type of problem
    but at different levels of complexity. A nice example from *Rivlin*, *2019*, is
    the **traveling** **salesman** **problem** (**TSP**) with a different number of
    nodes on a graph. The RL agent's job in this environment is to decide which node
    to visit next at every time step so that all nodes are visited exactly once at
    minimum cost, while going back to the initial node at the end. In fact, many problems
    we deal within RL naturally face this type of challenge, such as training a chess
    agent against opponents of different levels of expertise and so on.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们本质上处理相同类型问题，但在不同复杂度层次下的情况。一个来自*Rivlin*，*2019*的好例子是带有不同节点数量的**旅行商问题**（**TSP**）。在这个环境中，RL智能体的任务是在每个时间步决策下一个访问的节点，使得所有节点都被访问一次且以最小成本完成，同时在最后回到初始节点。事实上，我们在RL中处理的许多问题自然面临这种挑战，比如训练一个象棋智能体与不同水平的对手对战等。
- en: Solution – training at varying levels of complexity with a curriculum
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解决方案 – 通过课程在不同复杂度层次下进行训练
- en: Not so surprisingly, training the agent in environments with varying levels
    of difficulty is necessary to achieve generalization here. Having said that, using
    a curriculum that starts with easy environment configurations and gets gradually
    more difficult, as we described earlier in the book. This will potentially make
    the learning more efficient, and even feasible in some cases that would have been
    infeasible without a curriculum.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，在不同难度层次的环境中训练智能体是实现泛化的必要条件。也就是说，使用一种从简单环境配置开始，逐渐增加难度的课程，如我们之前在书中所描述的。这将可能使学习更加高效，甚至在某些没有课程时无法实现的情况下变得可行。
- en: Now that we have covered high-level approaches to achieve generalization in
    different dimensions, we will go into some specific algorithms. But first, let's
    introduce a benchmark environment used to quantify generalization.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了实现不同维度泛化的高级方法，接下来将讨论一些具体的算法。但首先，让我们介绍一个用于量化泛化的基准环境。
- en: Quantifying generalization
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化泛化
- en: 'There are various ways of testing whether certain algorithms/approaches generalize
    to unseen environment conditions better than others, such as the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以测试某些算法/方法在未见过的环境条件下是否比其他方法更好地进行泛化，例如以下几种：
- en: Creating validation and test environments with separate sets of environment
    parameters
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建具有不同环境参数集的验证和测试环境
- en: Assessing policy performance in a real-life deployment
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在现实生活中的部署中评估策略性能
- en: It is not always practical to do the latter as a real-life deployment may not
    necessarily be an option. The challenge with the former is to have consistency
    and to ensure that validation/test data is indeed not used in training. Also,
    it is possible to overfit to the validation environment when too many models are
    tried based on validation performance. One approach to overcome these challenges
    is to use procedurally generated environments. To this end, OpenAI has created
    the CoinRun environment to benchmark algorithms on their generalization capabilities.
    Let's look into it in more detail.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 执行后者并不总是可行的，因为现实生活中的部署可能不一定是一个选项。前者的挑战是确保一致性，并确保验证/测试数据在训练过程中没有被使用。此外，当基于验证性能尝试太多模型时，也可能会对验证环境进行过拟合。克服这些挑战的一种方法是使用程序化生成的环境。为此，OpenAI
    创建了 CoinRun 环境，用于基准测试算法的泛化能力。我们来更详细地了解一下。
- en: CoinRun environment
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CoinRun 环境
- en: 'The CoinRun environment is about a character trying to collect the coin at
    the level it is in without colliding with any obstacles. The character starts
    at the far left and the coin is at the far right. The levels are generated procedurally
    from an underlying probability distribution at various levels of difficulty, as
    shown here:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: CoinRun 环境是一个角色尝试在没有碰到任何障碍物的情况下收集硬币的游戏。角色从最左边开始，硬币在最右边。关卡是根据底层概率分布程序化生成的，难度各异，如下所示：
- en: '![Figure 11.2: Two levels in CoinRun with different levels of difficulty (source:
    Cobbe et al., 2018)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.2：CoinRun 中的两个关卡，难度不同（来源：Cobbe 等，2018）'
- en: '](img/B14160_11_02.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_11_02.jpg)'
- en: 'Figure 11.2: Two levels in CoinRun with different levels of difficulty (source:
    Cobbe et al., 2018)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2：CoinRun 中的两个关卡，难度不同（来源：Cobbe 等，2018）
- en: 'Here are more details about the environment''s reward function and terminal
    conditions:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于环境奖励函数和终止条件的更多细节：
- en: There are dynamic and static obstacles, causing the character to die when it
    collides with them, which terminates the episode.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境中有动态和静态障碍物，当角色与其碰撞时会死亡，从而终止该回合。
- en: The reward is only given when the coin is collected, which also terminates the
    episode.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有在收集到硬币时才会给予奖励，这也会终止回合。
- en: There is a time limit of 1,000 time steps before the episode terminates, unless
    the character dies or the coin is reached.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个回合有 1,000 步的时间限制，回合会在时间到达时终止，除非角色死亡或到达硬币。
- en: Note that the CoinRun environment generates all (training and test) levels from
    the same distribution, so it does not test the out-of-distribution (extrapolation)
    performance of a policy.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，CoinRun 环境生成的所有（训练和测试）关卡都来自相同的分布，因此它不会测试策略的超出分布（外推）性能。
- en: Next, let's install this environment and experiment with it.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来安装这个环境并进行实验。
- en: Installing the CoinRun environment
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 CoinRun 环境
- en: 'You can follow these steps to install the CoinRun environment:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以按照以下步骤安装 CoinRun 环境：
- en: 'We start with setting up and activating a virtual Python environment since
    CoinRun needs specific packages. So, in your directory of choice, run the following:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从设置和激活一个虚拟 Python 环境开始，因为 CoinRun 需要特定的包。因此，在你选择的目录中运行以下命令：
- en: '[PRE0]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we install the necessary Linux packages, including a famous parallel
    computing interface, MPI:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们安装必要的 Linux 包，包括一个著名的并行计算接口 MPI：
- en: '[PRE1]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, we install the Python dependencies and the CoinRun package fetched from
    the GitHub repo:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们安装从 GitHub 仓库获取的 Python 依赖项和 CoinRun 包：
- en: '[PRE2]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that we had to install an old TensorFlow version. Officially, TensorFlow
    1.12.0 is suggested by the creators of CoinRun. However, using a later TensorFlow
    1.x version could help you avoid CUDA conflicts while using the GPU.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们需要安装一个旧版本的 TensorFlow。官方建议使用 CoinRun 的创建者推荐的 TensorFlow 1.12.0 版本。然而，使用较新的
    TensorFlow 1.x 版本可能有助于避免使用 GPU 时出现的 CUDA 冲突。
- en: 'You can test the environment out with your keyboard''s arrow keys with the
    following command:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用键盘上的箭头键通过以下命令来测试环境：
- en: '[PRE3]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Great; enjoy playing CoinRun! I suggest you familiarize yourself with it to
    gain a better understanding of the comparisons we will go into later.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒；祝你玩得开心，玩CoinRun愉快！我建议你先熟悉一下这个环境，以便更好地理解我们接下来要讨论的比较。
- en: Info
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: You can visit the CoinRun GitHub repo for the full set of commands at [https://github.com/openai/coinrun](https://github.com/openai/coinrun).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以访问CoinRun的GitHub仓库，获取完整的命令集：[https://github.com/openai/coinrun](https://github.com/openai/coinrun)。
- en: The paper that introduced the environment (*Cobbe et al., 2018*) also mentions
    how various regularization techniques affects generalization in RL. We will discuss
    this next and then go into other approaches for generalization.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 引入该环境的论文（*Cobbe等，2018*）还提到了各种正则化技术如何影响RL中的泛化能力。我们将接下来讨论这一点，然后介绍其他的泛化方法。
- en: The effect of regularization and network architecture on the generalization
    of RL policies
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化和网络架构对RL策略泛化的影响
- en: The authors found that several techniques used in supervised learning to prevent
    overfitting are also helpful in RL. Since reproducing the results in the paper
    takes a really long time, where each experiment takes hundreds of millions of
    steps, we will not attempt to do it here. Instead, we have provided you with a
    summary of the results and the commands to run various versions of algorithms.
    But you can observe that, even after 500k time steps, applying the regularization
    techniques we mention as follows improves the test performance.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 作者发现，在监督学习中用于防止过拟合的几种技术，在强化学习（RL）中也同样有效。由于在论文中复现实验结果需要非常长的时间，每个实验需要数亿步，因此我们在此不再尝试复现。相反，我们为你提供了结果摘要和运行不同版本算法的命令。但你可以观察到，即使在500k时间步之后，应用我们接下来提到的正则化技术也能提高测试表现。
- en: 'You can see all the training options for this environment using the following
    command:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下命令查看该环境的所有训练选项：
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let's start with running a baseline Define acronym without any regularizations
    applied.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从运行一个没有任何正则化应用的基准开始。
- en: Vanilla training
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基础训练
- en: 'You can train an RL agent using PPO with an Impala architecture without any
    improvements for generalization, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用PPO和Impala架构训练一个RL代理，而不对泛化能力进行任何改进，如下所示：
- en: '[PRE5]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, `BaseAgent` is an ID for your agent that is decided by you, `--num-levels
    500` says to use 500 game levels during training with the default seed used in
    the paper, and `--num-envs 60` kicks off 60 parallel environments for rollouts,
    which you can adjust according to the number of CPUs available on your machine.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`BaseAgent`是你为代理决定的ID，`--num-levels 500`表示训练时使用500个游戏关卡，并使用论文中默认的种子，`--num-envs
    60`启动60个并行环境进行回滚，你可以根据机器上可用的CPU数量调整该参数。
- en: 'In order to test the trained agent in three parallel sessions, with 20 parallel
    environments in each and five levels for each environment, you can run the following
    command:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在三个并行会话中测试训练好的代理，每个会话有20个并行环境，每个环境有五个关卡，你可以运行以下命令：
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The average test reward will be specified in `mpi_out`. In my case, the reward
    went from around 5.5 in training after 300K time steps to 0.8 in the test environment
    to give you an idea.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 平均测试奖励将显示在`mpi_out`中。在我的情况下，奖励从训练后的300K时间步的约5.5降至测试环境中的0.8，以给你一个大概的参考。
- en: 'Also, you can watch your trained agent by running the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以通过运行以下命令来观察你的训练代理：
- en: '[PRE7]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is actually quite fun to do.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是很有趣的。
- en: Using a larger network
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用更大的网络
- en: The authors found that, as in supervised learning, using a larger neural network
    increases generalization by successfully solving more test episodes as it comes
    with a higher capacity. They also note that, however, there are diminishing returns
    for generalization with the increase in size, so generalization won't improve
    linearly with the network size.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 作者发现，正如在监督学习中一样，使用更大的神经网络通过更高的容量成功解决更多的测试场景，从而提高了泛化能力。他们还指出，然而，随着网络规模的增大，泛化能力的提升是递减的，因此泛化能力并不会随着网络规模线性提升。
- en: 'To use an architecture with five residual blocks instead of three with double
    the number of channels in each layer, you can add the `impalalarge` argument:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用一个具有五个残差块的架构，而不是三个，每层中的通道数是原来的两倍，你可以添加`impalalarge`参数：
- en: '[PRE8]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Again, you can run test evaluations with the run ID that you provided for the
    large agent case.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，你可以使用为大代理案例提供的运行ID进行测试评估。
- en: Diversity in training data
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练数据的多样性
- en: In order to test the importance of diverse training data, the authors compared
    two types of training, both with a total of 256M time steps, across 100 and then
    10,000 game levels (controlled by `--num-levels`). The test performance went from
    30% to a 90%+ success rate (which is also on par with the training performance)
    with the more diverse data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试多样化训练数据的重要性，作者比较了两种类型的训练，均包含256M时间步数，跨越100个和10,000个游戏关卡（通过`--num-levels`控制）。使用更多样化的数据后，测试性能从30%提升到90%以上（这也与训练性能相当）。
- en: Tip
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Increasing data diversity acts as a regularizer in supervised learning and RL.
    This is because the model would have to explain more variation with the same model
    capacity as diversity increases, forcing it to use its capacity to focus on the
    most important patterns in the input, rather than overfitting to noise.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 增加数据多样性在监督学习和强化学习（RL）中起到了正则化的作用。这是因为随着多样性的增加，模型必须在相同的模型容量下解释更多的变化，从而迫使模型利用其容量专注于输入中的最重要模式，而不是过拟合噪声。
- en: This emphasizes the importance of the randomization of the environment parameters
    to achieve generalization, which we will talk about separately later in the chapter.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这强调了环境参数随机化在实现泛化中的重要性，稍后我们将在本章中单独讨论这一点。
- en: Dropout and L2 regularization
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dropout和L2正则化
- en: The experiment results show both dropout and L2 regularization improve generalization,
    bringing in around an additional 5% and 8% success on top of around a 79% baseline
    test performance.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果表明，dropout和L2正则化都能改善泛化能力，分别增加了约5%和8%的成功率，基准测试性能约为79%。
- en: Tip
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: If you need a refresher on dropout and L2 regularization, check out Chitta Ranjan's
    blog at [https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275](https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要复习dropout和L2正则化，可以查看Chitta Ranjan的博客：[https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275](https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275)。
- en: 'We can explore this in more detail as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式更详细地探讨这个问题：
- en: Empirically, the authors find the best L2 weight as ![](img/Formula_11_001.png)
    and the best dropout rate as 0.1.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从经验上来看，作者发现最佳的L2权重为 ![](img/Formula_11_001.png)，最佳的dropout率为0.1。
- en: L2 regularization, empirically, has a higher impact on generalization compared
    to dropout.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从经验来看，L2正则化对泛化的影响比dropout更大。
- en: As expected, training with dropout is slower to converge, to which twice as
    many time steps (512M) are allocated.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如预期所示，使用dropout的训练收敛速度较慢，因此分配了两倍的时间步数（512M）。
- en: 'To use a dropout with a rate of 0.1 on top of the vanilla agent, you can use
    the following command:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要在普通代理的基础上使用0.1的dropout率，可以使用以下命令：
- en: '[PRE9]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Similarly, to use L2 normalization with a weight of 0.0001, execute the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，要使用L2正则化，权重为0.0001，可以执行以下操作：
- en: '[PRE10]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In your TensorFlow RL model, you can add dropout using the `Dropout` layer,
    as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的TensorFlow RL模型中，您可以通过以下方式使用`Dropout`层来添加dropout：
- en: '[PRE11]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To add L2 regularization, do something like this:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 要添加L2正则化，可以做类似以下操作：
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Info
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: 'TensorFlow has a very nice tutorial on overfitting and underfitting, which
    you might want to check out: [https://www.tensorflow.org/tutorials/keras/overfit_and_underfit](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow有一个非常好的关于过拟合和欠拟合的教程，您可以查看：[https://www.tensorflow.org/tutorials/keras/overfit_and_underfit](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit)。
- en: Next, let's discuss data augmentation.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论数据增强。
- en: Using data augmentation
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用数据增强
- en: 'A common method to prevent overfitting is data augmentation, which is modifying/distorting
    the input, mostly randomly, so that the diversity in the training data increases.
    When used on images, these techniques include random cropping, changing the brightness
    and sharpness, and so on. An example CoinRun scene with data augmentation used
    looks like the following:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的防止过拟合的方法是数据增强，即对输入进行修改/扭曲，通常是随机的，以增加训练数据的多样性。当应用于图像时，这些技术包括随机裁剪、改变亮度和锐度等。以下是使用数据增强的CoinRun场景示例：
- en: '![Figure 11.3 – CoinRun with data augmentation (source: Cobbe et al., 2018)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.3 – 带数据增强的CoinRun（来源：Cobbe等，2018年）'
- en: '](img/B14160_11_03.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_11_03.jpg)'
- en: 'Figure 11.3 – CoinRun with data augmentation (source: Cobbe et al., 2018)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 – 带数据增强的CoinRun（来源：Cobbe等，2018年）
- en: Info
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: For a TensorFlow tutorial on data augmentation, check out [https://www.tensorflow.org/tutorials/images/data_augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据增强的TensorFlow教程，请查看[https://www.tensorflow.org/tutorials/images/data_augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation)。
- en: Data augmentation, as it turns out, is also helpful in RL, and it gives a lift
    in the test performance that is slightly worse than L2 regularization.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强，事实证明，在强化学习（RL）中也很有帮助，它能够提升测试性能，虽然略逊色于L2正则化。
- en: Using batch normalization
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用批量归一化
- en: Batch normalization is one of the key tools in deep learning to stabilize training
    as well as to prevent overfitting.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化是深度学习中的关键工具之一，它有助于稳定训练并防止过拟合。
- en: Info
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: If you need a refresher on batch normalization, check out Chris Versloot's blog
    at [https://bit.ly/3kjzjno](https://bit.ly/3kjzjno).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要复习批量归一化的知识，可以查看Chris Versloot的博客：[https://bit.ly/3kjzjno](https://bit.ly/3kjzjno)。
- en: 'In the CoinRun environment, you can enable the batch normalization layers in
    the training as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在CoinRun环境中，你可以通过如下方式启用训练中的批量归一化层：
- en: '[PRE13]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This adds a batch normalization layer after every convolutional layer.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在每个卷积层后添加一个批量归一化层。
- en: When you implement your own TensorFlow model, the syntax for the batch normalization
    layer is `layers.BatchNormalization()`, with some optional arguments you can pass.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当你实现自己的TensorFlow模型时，批量归一化层的语法是`layers.BatchNormalization()`，并且可以传入一些可选的参数。
- en: The reported results show that using batch normalization gives the second-best
    lift to the test performance among all of the other regularization methods (except
    increasing the diversity in the training data).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 报告结果显示，使用批量归一化在所有其他正则化方法中（除了增加训练数据的多样性）能为测试性能提供第二大的提升。
- en: Adding stochasticity
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加随机性
- en: 'Finally, introducing stochasticity/noise into the environment turns out to
    be the most useful generalization technique, with around a 10% lift to the test
    performance. Two methods are tried in the paper with the PPO algorithm during
    training:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，向环境中引入随机性/噪声被证明是最有用的泛化技术，能够将测试性能提高约10%。本文在训练中使用了两种方法与PPO算法：
- en: Using ![](img/Formula_11_002.png)-greedy actions (which are usually used with
    Q-learning approaches)
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用！[](img/Formula_11_002.png)-贪婪策略（通常与Q学习方法一起使用）
- en: Increasing the entropy bonus coefficient (![](img/Formula_11_003.png)) in PPO,
    which encourages variance in the actions suggested by the policy
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在PPO中增加熵奖励系数（![](img/Formula_11_003.png)），以鼓励策略提出的动作具有更多的方差
- en: Some good hyperparameter choices for these methods are ![](img/Formula_11_004.png)
    and ![](img/Formula_11_005.png), respectively. Something worth noting is that
    if the environment dynamics are already highly stochastic, introducing more stochasticity
    may not be as impactful.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法的良好超参数选择分别是！[](img/Formula_11_004.png) 和 ![](img/Formula_11_005.png)。需要注意的是，如果环境的动态已经高度随机化，那么引入更多的随机性可能没有那么显著的影响。
- en: Combining all the methods
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结合所有方法
- en: Using all these regularization methods together during training only slightly
    improved the boost obtained from the individual methods, suggesting that each
    of these methods plays a similar role to prevent overfitting. Note that it is
    not quite possible to say that these methods will have a similar impact on all
    RL problems. What we need to keep in mind, though, is that the traditional regularization
    methods used in supervised learning can have a significant impact on generalizing
    RL policies as well.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中同时使用所有这些正则化方法，仅稍微改善了单个方法带来的提升，表明这些方法各自都在防止过拟合方面起到了类似的作用。需要注意的是，无法确定这些方法对所有RL问题的影响完全相同。但我们需要记住的是，传统的监督学习正则化方法也能显著影响RL策略的泛化能力。
- en: This concludes our discussion on the fundamental regularization techniques for
    RL. Next, we will look into another method that followed the original CoinRun
    paper, which is network randomization.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们关于RL的基本正则化技术的讨论。接下来，我们将研究另一种方法，这种方法紧随原始CoinRun论文之后，即网络随机化。
- en: Network randomization and feature matching
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络随机化和特征匹配
- en: 'Network randomization, proposed by Lee et al., 2020, simply involves using
    a random transformation of observations, ![](img/Formula_11_006.png), as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 网络随机化，由Lee等人于2020年提出，简单地涉及使用观测的随机变换，！[](img/Formula_11_006.png)，如下所示：
- en: '![](img/Formula_11_007.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_11_007.jpg)'
- en: 'Then, the transformed observation, ![](img/Formula_11_008.png), is fed as input
    to the regular policy network used in the RL algorithm. Here, ![](img/Formula_11_009.png)
    is the parameter of this transformation, which is randomly initialized at every
    training iteration. This can be simply achieved by adding, right after the input
    layer, a layer that is not trainable and re-initialized periodically. In TensorFlow
    2, a randomization layer that transforms the input after each call could be implemented
    as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，变换后的观察值![](img/Formula_11_008.png)作为输入被送入RL算法中使用的常规策略网络。在这里，![](img/Formula_11_009.png)是此变换的参数，每次训练迭代时都会随机初始化。通过在输入层之后添加一个不可训练并且定期重新初始化的层，可以简单实现这一点。在TensorFlow
    2中，可以按如下方式实现一个在每次调用后转换输入的随机化层：
- en: '[PRE14]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Note that this custom layer exhibits the following traits:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个自定义层具有以下特性：
- en: Has weights that are not trainable
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重是不可训练的
- en: Assigns random weights to the layer at each call
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每次调用时为层分配随机权重
- en: 'A further improvement to this architecture is to do two forward passes, with
    and without randomized inputs, and enforce the network to give similar outputs.
    This can be achieved by adding a loss to the RL objective that penalizes the difference:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对该架构的进一步改进是进行两次前向传递，一次带有随机输入，一次不带，并强制网络给出相似的输出。这可以通过向RL目标添加一个损失来实现，该损失惩罚输出差异：
- en: '![](img/Formula_11_010.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_11_010.jpg)'
- en: Here, ![](img/Formula_11_011.png) is the parameters of the policy network, and
    ![](img/Formula_11_012.png) is the second-from-last layer in the policy network
    (that is, the one right before the layer that outputs the actions). This is called
    **feature matching** and makes the network differentiate noise from the signal
    in the input.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_11_011.png)是策略网络的参数，而![](img/Formula_11_012.png)是策略网络中倒数第二层（即在输出动作的层之前的那一层）。这被称为**特征匹配**，它使得网络能够区分输入中的噪声和信号。
- en: Info
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: The TensorFlow 1.x implementation of this architecture for the CoinRun environment
    is available at [https://github.com/pokaxpoka/netrand](https://github.com/pokaxpoka/netrand).
    Compare it to the original CoinRun environment by comparing `random_ppo2.py` with
    `ppo2.py` and the `random_impala_cnn` method to the `impala_cnn` method under
    `policies.py`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构在CoinRun环境中的TensorFlow 1.x实现可以在[https://github.com/pokaxpoka/netrand](https://github.com/pokaxpoka/netrand)找到。通过将`random_ppo2.py`与`ppo2.py`进行比较，并将`random_impala_cnn`方法与`impala_cnn`方法在`policies.py`中的对比，您可以将其与原始的CoinRun环境进行比较。
- en: Going back to the dimensions of generalization we mentioned earlier, network
    randomization helps RL policies generalize in all of the three dimensions.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们之前提到的泛化维度，网络随机化有助于RL策略在这三个维度上进行泛化。
- en: Next, we will discuss one of the key approaches to achieve generalization with
    proven real-life successes.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论一种实现泛化的关键方法，该方法在现实生活中已被证明有效。
- en: Curriculum learning for generalization
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 泛化的课程学习
- en: 'We already discussed how a rich training experience helps an RL policy to better
    generalize. Let''s assume that for your robotics application, you have identified
    two parameters to randomize in your environment with some minimum and maximum
    values:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论过，丰富的训练经验有助于RL策略更好地泛化。假设在您的机器人应用中，您已识别出需要在环境中随机化的两个参数，并为它们指定了最小值和最大值：
- en: 'Friction: ![](img/Formula_11_013.png)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摩擦：![](img/Formula_11_013.png)
- en: 'Actuator torque: ![](img/Formula_11_014.png)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行器扭矩：![](img/Formula_11_014.png)
- en: The goal here is to prepare the agent to act in an environment with an unknown
    friction-torque combination at test time.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标是使代理准备好在测试时应对具有未知摩擦-扭矩组合的环境。
- en: It turns out that, as we mentioned in the previous chapter when we discussed
    curriculum learning, the training may result in a mediocre agent if you try to
    train it over the entire range for these parameters right at the beginning. That
    is because the extreme values of the parameter ranges are likely to be too challenging
    (assuming they are centered around some reasonable values for the respective parameters)
    for the agent who has not even figured out the basics of the task. The idea behind
    curriculum learning is to start with easy scenarios, such as having the first
    lesson as ![](img/Formula_11_015.png) and ![](img/Formula_11_016.png), and gradually
    increase the difficulty in the next lessons by expanding the ranges.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，正如我们在上一章讨论课程学习时提到的，训练可能会导致一个平庸的智能体，如果你在一开始就尝试在这些参数的整个范围内进行训练。这是因为参数范围的极端值可能对尚未掌握任务基础的智能体来说过于具有挑战性（假设这些极端值围绕着某些合理的参数值）。课程学习的核心思想是从简单的场景开始，比如第一课可以是
    ![](img/Formula_11_015.png) 和 ![](img/Formula_11_016.png)，然后通过扩展范围逐步增加难度。
- en: Then, a key question is how we should construct the curriculum, what the lessons
    should look like (that is, what the next range of parameters after the agent succeeds
    in the current lesson should be), and when to declare success for the existing
    lesson. In this section, we discuss two methods for curriculum learning that automatically
    generate and manage the curriculum for effective domain randomization.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，一个关键问题是我们应该如何构建课程，课程应该是什么样子（也就是说，当智能体在当前课题中成功后，下一步的参数范围应是什么），以及何时宣布当前课题的成功。在本节中，我们将讨论两种自动生成和管理课程的课程学习方法，以有效进行领域随机化。
- en: Automatic domain randomization
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动领域随机化
- en: '**Automatic domain randomization** (**ADR**) is a method proposed by OpenAI
    in their research of using a robot hand to manipulate a Rubik''s cube. It is one
    of the most successful robotics applications of RL for several reasons:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**自动领域随机化**（**ADR**）是 OpenAI 在其研究使用机器人手操控魔方时提出的一种方法。这是 RL 在机器人技术应用中最成功的案例之一，原因有很多：'
- en: Dexterous robots are notoriously difficult to control due to their high degrees
    of freedom.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灵活的机器人由于其高度的自由度，控制起来 notoriously 困难。
- en: The policy is trained completely in simulation and then successfully transferred
    to a physical robot, successfully bridging the sim2real gap.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略完全在仿真中训练，然后成功地转移到物理机器人上，成功弥合了模拟与现实之间的差距。
- en: During test time, the robot succeeded under conditions that it had never seen
    during training, such as the fingers being tied, having a rubber glove on, perturbations
    with various objects, and so on.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在测试时，机器人成功地在训练时未曾见过的条件下完成任务，例如手指被绑住、戴上橡胶手套、与各种物体发生扰动等。
- en: These are phenomenal results in terms of the generalization capability of the
    trained policy.
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些结果在训练策略的泛化能力方面是非常出色的。
- en: Info
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信息
- en: You should check out the blog post for this important research at [https://openai.com/blog/solving-rubiks-cube/](https://openai.com/blog/solving-rubiks-cube/).
    It contains great visualizations and insights into the results.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该查看这篇关于这一重要研究的博客文章，[https://openai.com/blog/solving-rubiks-cube/](https://openai.com/blog/solving-rubiks-cube/)。它包含了很好的可视化和对结果的深刻见解。
- en: ADR was the key method in the success of this application. Next, we will discuss
    how ADR works.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**ADR** 是该应用成功的关键方法。接下来，我们将讨论 ADR 是如何工作的。'
- en: The ADR algorithm
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ADR 算法
- en: Each environment we create during training is randomized over certain parameters,
    such as friction and torque, in the preceding example. To denote this formally,
    we say that an environment, ![](img/Formula_11_017.png), is parametrized by ![](img/Formula_11_018.png),
    where ![](img/Formula_11_019.png) is the number of parameters (2, in this example).
    When an environment is created, we sample ![](img/Formula_11_020.png) from a distribution,
    ![](img/Formula_11_021.png). What ADR adjusts is ![](img/Formula_11_022.png) of
    the parameter distribution, therefore changing the likelihood of different parameter
    samples, making the environment more difficult or easier, depending on whether
    the agent is successful in the current difficulty.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练过程中创建的每个环境都会对某些参数进行随机化，例如在前面的例子中，摩擦力和扭矩。为了正式表示这一点，我们说一个环境，![](img/Formula_11_017.png)，是由![](img/Formula_11_018.png)来参数化的，其中![](img/Formula_11_019.png)是参数的数量（在这个例子中为2）。当一个环境被创建时，我们从一个分布中抽取![](img/Formula_11_020.png)，![](img/Formula_11_021.png)。ADR调整的是参数分布的![](img/Formula_11_022.png)，从而改变不同参数样本的可能性，使得环境变得更难或更容易，具体取决于智能体在当前难度下是否成功。
- en: 'An example, ![](img/Formula_11_023.png), would consist of uniform distributions
    for each parameter dimension, ![](img/Formula_11_024.png), with ![](img/Formula_11_025.png).
    Connecting with our example, ![](img/Formula_11_026.png) would correspond to the
    friction coefficient, ![](img/Formula_11_027.png). Then, for the initial lesson,
    we would have ![](img/Formula_11_028.png). This would be similar for the torque
    parameter, ![](img/Formula_11_029.png). Then, ![](img/Formula_11_030.png) becomes
    the following:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子，![](img/Formula_11_023.png)，将包括每个参数维度的均匀分布，![](img/Formula_11_024.png)，其中有![](img/Formula_11_025.png)。与我们的例子相联系，![](img/Formula_11_026.png)对应摩擦系数，![](img/Formula_11_027.png)。然后，对于初始课程，我们将有![](img/Formula_11_028.png)。对于扭矩参数也类似，![](img/Formula_11_029.png)。然后，![](img/Formula_11_030.png)变为以下内容：
- en: '![](img/Formula_11_031.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_11_031.jpg)'
- en: 'ADR suggests the following:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ADR建议如下：
- en: As the training continues, allocate some of the environments for evaluation
    to decide whether to update ![](img/Formula_11_032.png).
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着训练的进行，分配一些环境用于评估，以决定是否更新![](img/Formula_11_032.png)。
- en: In each evaluation environment, pick a dimension, ![](img/Formula_11_033.png),
    then pick either the upper or lower bound to focus on, such as ![](img/Formula_11_034.png)
    and ![](img/Formula_11_035.png).
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个评估环境中，选择一个维度，![](img/Formula_11_033.png)，然后选择上限或下限之一进行关注，例如![](img/Formula_11_034.png)和![](img/Formula_11_035.png)。
- en: Fix the environment parameter for the dimension picked to the bound chosen.
    Sample the rest of the parameters from ![](img/Formula_11_036.png).
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将选定维度的环境参数固定在选定的边界上。其余参数从![](img/Formula_11_036.png)中抽样。
- en: Evaluate the agent's performance in the given environment and keep the total
    reward obtained in that episode in a buffer associated with the dimension and
    the bound (for example, ![](img/Formula_11_037.png)).
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估智能体在给定环境中的表现，并将该回合中获得的总奖励保存在与维度和边界相关的缓冲区中（例如，![](img/Formula_11_037.png)）。
- en: When there are enough results in the buffer, compare the average reward to the
    success and failure thresholds you had chosen a priori.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当缓冲区中的结果足够时，将平均奖励与您事先设定的成功和失败阈值进行比较。
- en: If the average performance for the given dimension and bound is above your success
    threshold, expand the parameter range for the dimension; if it is below the failure
    threshold, decrease the range.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果给定维度和边界的平均表现高于您的成功阈值，则扩展该维度的参数范围；如果低于失败阈值，则缩小范围。
- en: In summary, ADR systematically evaluates the agent performance for each parameter
    dimension at the boundaries of the parameter range, then expands or shrinks the
    range depending on the agent performance. You can refer to the paper for pseudo-code
    of the ADR algorithm, which should be easy to follow with the preceding explanations.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，ADR系统地评估每个参数维度在参数范围边界上的智能体表现，然后根据智能体的表现扩大或缩小范围。您可以参考论文中的伪代码，它应该与前面的解释配合使用时很容易理解。
- en: Next, let's discuss another important method for automatic curriculum generation.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论另一种重要的自动课程生成方法。
- en: Absolute learning progress with Gaussian mixture models
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用高斯混合模型的绝对学习进展
- en: 'Another method for automatic curriculum generation is the **absolute learning
    progress with Gaussian mixture models** (**ALP-GMM**) method. The essence of this
    approach is as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种自动课程生成的方法是**使用高斯混合模型的绝对学习进展**（**ALP-GMM**）方法。该方法的本质如下：
- en: To identify the parts of the environment parameter space that show the most
    learning progress (called the ALP value)
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于识别环境参数空间中表现出最多学习进展的部分（称为ALP值）
- en: To fit multiple GMM models to the ALP data, with ![](img/Formula_11_038.png)number
    of kernels, then select the best one
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了将多个GMM模型拟合到ALP数据上，使用![](img/Formula_11_038.png)个核，然后选择最佳的一个
- en: To sample the environment parameters from the best GMM model
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从最佳GMM模型中采样环境参数
- en: This idea has roots in cognitive science and is used to model the early vocal
    developments of infants.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法源于认知科学，用来建模婴儿早期的语言发展。
- en: 'The ALP score for a newly sampled parameter vector, ![](img/Formula_11_039.png),
    is calculated as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 新采样的参数向量的ALP分数，![](img/Formula_11_039.png)，计算公式如下：
- en: '![](img/Formula_11_040.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_11_040.jpg)'
- en: Here, ![](img/Formula_11_041.png) is the episode reward obtained with ![](img/Formula_11_042.png),
    ![](img/Formula_11_043.png) is the closest parameter vector that was obtained
    in a previous episode, and ![](img/Formula_11_044.png) is the episode reward associated
    with ![](img/Formula_11_045.png). All the ![](img/Formula_11_046.png)) pairs are
    kept in a database, denoted by ![](img/Formula_11_047.png), with which the ALP
    scores are calculated. The GMM model, however, is obtained using the most recent
    ![](img/Formula_11_048.png) ![](img/Formula_11_049.png) pairs.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_11_041.png)是通过![](img/Formula_11_042.png)获得的回合奖励，![](img/Formula_11_043.png)是先前回合获得的最接近的参数向量，![](img/Formula_11_044.png)是与![](img/Formula_11_045.png)相关的回合奖励。所有的![](img/Formula_11_046.png)对都保存在一个数据库中，表示为![](img/Formula_11_047.png)，通过它来计算ALP分数。然而，GMM模型是使用最新的![](img/Formula_11_048.png)![](img/Formula_11_049.png)对来获得的。
- en: Note that the parts of the parameter space that have high ALP scores are more
    likely to be sampled to generate new environments. A high ALP score shows potential
    for learning for that region, and it can be obtained by observing a large drop
    or increase in episode reward with the newly sampled ![](img/Formula_11_050.png).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，具有较高ALP分数的参数空间部分更有可能被采样以生成新环境。高ALP分数表明该区域有学习潜力，可以通过观察新采样的![](img/Formula_11_050.png)下回合奖励的大幅下降或增加来获得。
- en: Info
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: The code repo of the ALP-GMM paper is available at [https://github.com/flowersteam/teachDeepRL](https://github.com/flowersteam/teachDeepRL),
    which also contains animations that show how the algorithm works. We are not able
    to go into the details of the repo here due to space limitations, but I highly
    recommend you take a look at the implementation and the results.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ALP-GMM论文的代码库可以在[https://github.com/flowersteam/teachDeepRL](https://github.com/flowersteam/teachDeepRL)找到，其中还包含了展示算法如何工作的动画。由于篇幅限制，我们无法在这里详细讲解代码库，但我强烈建议你查看实现和结果。
- en: Finally, we will present some additional resources on generalization for further
    reading.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将提供一些关于泛化的额外资源，供进一步阅读。
- en: Sunblaze environment
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sunblaze环境
- en: We don't have space to cover all methods for generalization in this book, but
    a useful resource for you to check out is a blog by Packer & Gao, 2019, which
    introduces the sunblaze environments to systematically assess generalization approaches
    for RL. These environments are modifications to the classic OpenAI Gym environments,
    which are parametrized to test the interpolation and extrapolation performance
    of algorithms.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中无法覆盖所有的泛化方法，但一个有用的资源是Packer & Gao, 2019年发布的博客，介绍了Sunblaze环境，旨在系统地评估强化学习（RL）的泛化方法。这些环境是经典的OpenAI
    Gym环境的修改版，经过参数化以测试算法的插值和外推性能。
- en: Info
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: You can find the blog post describing the sunblaze environments and the results
    at [https://bair.berkeley.edu/blog/2019/03/18/rl-generalization/](https://bair.berkeley.edu/blog/2019/03/18/rl-generalization/).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://bair.berkeley.edu/blog/2019/03/18/rl-generalization/](https://bair.berkeley.edu/blog/2019/03/18/rl-generalization/)找到描述Sunblaze环境及其结果的博客文章。
- en: Terrific job! You have learned about one of the most important topics concerning
    real-world RL! Next, we will look at a closely connected topic, which is overcoming
    partial observability.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 做得非常棒！你已经了解了有关真实世界强化学习中最重要的主题之一！接下来，我们将讨论一个紧密相关的话题，即克服部分可观察性。
- en: Using memory to overcome partial observability
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用记忆克服部分可观察性
- en: 'At the beginning of the chapter, we described how memory is a useful structure
    to handle partial observability. We also mentioned that the problem of generalization
    can often be thought of as the result of partial observability:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开始时，我们描述了内存作为一种处理部分可观察性的有用结构。我们还提到过，泛化问题往往可以看作是部分可观察性的结果：
- en: A hidden state differentiating two environments, such as a simulation and the
    real world, can be uncovered through memory.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个区分两个环境（例如模拟环境与真实世界）的隐藏状态可以通过内存揭示出来。
- en: When we implement domain randomization, we aim to create many versions of our
    training environment in which we expect the agent to build an overarching model
    for the world dynamics.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们实现领域随机化时，我们的目标是创建许多版本的训练环境，其中我们希望代理为世界动态建立一个总体模型。
- en: Through memory, we hope that the agent can identify the characteristics of the
    environment it is in, even if it had not seen that particular environment during
    training, and then act accordingly.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过内存，我们希望代理能够识别它所在环境的特征，即使它在训练过程中没有看到过这个特定的环境，然后相应地进行行动。
- en: Now, memory for a model is nothing more than a way of processing a sequence
    of observations as the input to the agent policy. If you have worked with other
    types of sequence data with neural networks, such as in time-series prediction
    or **natural language processing** (**NLP**), you can adopt similar approaches
    to use observation memory as the input for your RL model.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，模型的内存不过是将一系列观察作为输入处理的方式。如果你曾经处理过其他类型的序列数据与神经网络结合的任务，比如时间序列预测或**自然语言处理**（**NLP**），你可以采用类似的方法，将观察内存作为RL模型的输入。
- en: Let's go into more details of how this can be done.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解如何实现这一点。
- en: Stacking observations
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠观察
- en: 'A simple way of passing an observation sequence to the model is to stitch them
    together and treat this stack as a single observation. Denoting the observation
    at time ![](img/Formula_11_051.png) as ![](img/Formula_11_052.png), we can form
    a new observation, ![](img/Formula_11_053.png), to be passed to the model, as
    follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 将观察序列传递给模型的一种简单方法是将它们拼接在一起，并将这个堆叠视为单一的观察。将时间点上的观察表示为![](img/Formula_11_051.png)，记作![](img/Formula_11_052.png)，我们可以形成一个新的观察，![](img/Formula_11_053.png)，并将其传递给模型，如下所示：
- en: '![](img/Formula_11_054.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_11_054.jpg)'
- en: Here, ![](img/Formula_11_055.png) is the length of the memory. Of course, for
    ![](img/Formula_11_056.png), we need to somehow initialize the earlier parts of
    the memory, such as using vectors of zeros that are the same dimension as ![](img/Formula_11_057.png).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_11_055.png)是内存的长度。当然，对于![](img/Formula_11_056.png)，我们需要以某种方式初始化内存的早期部分，例如使用与![](img/Formula_11_057.png)维度相同的零向量。
- en: 'In fact, simply stacking observations is how the original DQN work handled
    the partial observability in the Atari environments. In more detail, the steps
    of that preprocessing are as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，简单地堆叠观察就是原始DQN工作处理Atari环境中部分可观察性的方法。更详细地说，该预处理的步骤如下：
- en: A rescaled ![](img/Formula_11_058.png) RGB frame of the screen is obtained.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取一个重新缩放的![](img/Formula_11_058.png) RGB屏幕帧。
- en: The Y channel (luminance) is extracted to further compress the frame into an
    ![](img/Formula_11_059.png) image. That makes a single observation, ![](img/Formula_11_060.png).
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取Y通道（亮度）进一步将帧压缩成![](img/Formula_11_059.png)图像。这样就得到了一个单一的观察![](img/Formula_11_060.png)。
- en: '![](img/Formula_11_061.png) most recent frames are concatenated to an ![](img/Formula_11_062.png)image,
    forming an observation with memory for the model, ![](img/Formula_11_063.png).'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![](img/Formula_11_061.png)最新的帧被拼接成一个![](img/Formula_11_062.png)图像，形成一个具有内存的模型观察![](img/Formula_11_063.png)。'
- en: Note that only the last step is about the memory and the former steps are not
    strictly necessary.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，只有最后一步涉及内存，前面的步骤并不是严格必要的。
- en: The obvious benefit of this method is that it is super simple, and the resulting
    model is easy to train. The downside is, though, that this is not the best way
    of handling sequence data, which should not be surprising to you if you have dealt
    with time-series problems or NLP before. Here is an example of why.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的明显优点是非常简单，生成的模型也很容易训练。然而，缺点是，这并不是处理序列数据的最佳方法，如果你曾经处理过时间序列问题或NLP，应该不会对这一点感到惊讶。以下是一个原因的示例。
- en: 'Consider the following sentence you speak to your virtual voice assistant,
    such as Apple''s Siri:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你对虚拟语音助手（如苹果的Siri）说的以下句子：
- en: '*"Buy me a plane ticket from San Francisco to Boston."*'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '*“帮我买一张从旧金山到波士顿的机票。”*'
- en: 'This is the same as saying the following:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这与以下说法是相同的：
- en: '*"Buy me a plane ticket to Boston from San Francisco"*'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '*"给我买一张从旧金山到波士顿的机票"*'
- en: Assuming each word is passed to an input neuron, it is hard for the neural network
    to readily interpret them as the same sentences because normally, each input neuron
    expects a specific input. In this structure, you would have to train your network
    with all different combinations of this sentence. A further complexity is that
    your input size is fixed, but each sentence could be of a different length. You
    can extend this thought to RL problems as well.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 假设每个单词都被传递到输入神经元，神经网络很难将它们轻松地理解为相同的句子，因为通常每个输入神经元期望特定的输入。在这种结构中，你需要使用该句子的所有不同组合来训练网络。更复杂的是，输入大小是固定的，但每个句子的长度可能不同。你也可以将这个思路扩展到强化学习问题中。
- en: Now, stacking observations is good enough in most problems, such as Atari games.
    But if you are trying to teach your model how to play Dota 2, a strategy video
    game, then you are out of luck.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在大多数问题中，堆叠观察值就足够了，比如 Atari 游戏。但是如果你试图教会你的模型如何玩 Dota 2 这种策略视频游戏，那么你就会遇到困难。
- en: Fortunately, **recurrent** **neural** **networks** (**RNNs**) come to the rescue.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，**递归** **神经** **网络**（**RNN**）来救场了。
- en: Using RNNs
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 RNN
- en: 'RNNs are designed to handle sequence data. A famous RNN variant, **long** **short-term**
    **memory** (**LSTM**) networks, can be effectively trained to handle long sequences.
    LSTM is usually the choice when it comes to handling partial observability in
    complex environments: it is used in OpenAI''s Dota 2 and DeepMind''s StarCraft
    II models, among many others.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 设计用于处理序列数据。一个著名的 RNN 变体，**长短期记忆**（**LSTM**）网络，能够有效地训练来处理长序列。当涉及到处理复杂环境中的部分可观察性时，LSTM
    通常是首选：它被应用于 OpenAI 的 Dota 2 和 DeepMind 的 StarCraft II 模型中，当然还有许多其他模型。
- en: Info
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: 'Describing the full details of how RNNs and LSTMs work is beyond the scope
    of this chapter. If you want a resource to learn more about them, Christopher
    Olah''s blog is the place to go: [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 详细描述 RNN 和 LSTM 如何工作的内容超出了本章的范围。如果你想了解更多关于它们的知识，Christopher Olah 的博客是一个不错的资源：[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)。
- en: 'When using RLlib, the LSTM layer can be enabled as follows, say while using
    PPO, followed by some optional hyperparameter changes over the default values:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 RLlib 时，可以按如下方式启用 LSTM 层，比如在使用 PPO 时，并在默认值的基础上进行一些可选的超参数调整：
- en: '[PRE15]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that the input is first fed to the (preprocessing) "model" in RLlib, which
    is typically a sequence of fully connected layers. The output of the preprocessing
    is then passed to the LSTM layer.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输入首先被传递到 RLlib 中的（预处理）"模型"，该模型通常是由一系列全连接层组成。预处理的输出随后会传递给 LSTM 层。
- en: 'The fully connected layer hyperparameters can be similarly overwritten:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接层的超参数也可以类似地被覆盖：
- en: '[PRE16]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After specifying the environment within the config as a Gym environment name
    or your custom environment class, the config dictionary is then passed to the
    trainer class:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置文件中指定环境为 Gym 环境名称或自定义环境类后，配置字典将传递给训练器类：
- en: '[PRE17]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'There are couple things to keep in mind when using an LSTM model as opposed
    to the simple stacking of observations:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LSTM 模型时，有几个需要注意的事项，与简单地堆叠观察值不同：
- en: LSTM is often slower to train due to the sequential processing of a multi-step
    input.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于需要对多步输入进行顺序处理，LSTM 的训练通常较慢。
- en: Training LSTM may require more data compared to a feedforward network.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与前馈网络相比，训练 LSTM 可能需要更多的数据。
- en: Your LSTM model could be more sensitive to hyperparameters, so you may have
    to do some hyperparameter tuning.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的 LSTM 模型可能对超参数更敏感，因此你可能需要进行一些超参数调优。
- en: 'Speaking of hyperparameters, here are some values to try if your training is
    not progressing well for an algorithm like PPO:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 说到超参数，如果你的训练在像 PPO 这样的算法上进展不顺利，以下是一些可以尝试的值：
- en: 'Learning rate (`config["lr"]`): ![](img/Formula_11_064.png).'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率（`config["lr"]`）：![](img/Formula_11_064.png)。
- en: 'LSTM cell size (`config["model"]["lstm_cell_size"]`): 64, 128, 256.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM 单元大小（`config["model"]["lstm_cell_size"]`）：64，128，256。
- en: 'Layer sharing between the value and policy network (`config["vf_share_layers"]`):
    Try to make this false if your episode rewards are in the hundreds or above to
    prevent the value function loss from dominating the policy loss. Alternatively,
    you can also reduce `config["vf_loss_coeff"]`.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值网络和策略网络之间的层共享（`config["vf_share_layers"]`）：如果你的回报是几百或更多，请尝试将其设为假，以防值函数损失主导策略损失。或者，你也可以减少
    `config["vf_loss_coeff"]`。
- en: 'Entropy coefficient (`config["entropy_coeff"]`): ![](img/Formula_11_065.png)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熵系数（`config["entropy_coeff"]`）：![](img/Formula_11_065.png)
- en: 'Passing reward and previous actions as input (`config["model"]["lstm_use_prev_action_reward"]`):
    Try to make this true to provide more information to the agent in addition to
    observations.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将奖励和之前的动作作为输入（`config["model"]["lstm_use_prev_action_reward"]`）：尝试将其设为真，以便在观察之外为智能体提供更多信息。
- en: 'Preprocessing model architecture (`config["model"]["fcnet_hiddens"]` and `config["model"]["fcnet_activation"]`):
    Try single linear layers.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理模型架构（`config["model"]["fcnet_hiddens"]` 和 `config["model"]["fcnet_activation"]`）：尝试使用单一线性层。
- en: Hopefully, these will be helpful to come up with a good architecture for your
    model.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这些内容有助于为你的模型构建一个良好的架构。
- en: 'Finally, let''s discuss one of the most popular architectures: Transformer.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来讨论一下最流行的架构之一：Transformer。
- en: Transformer architecture
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer 架构
- en: Over the past several years, the Transformer architecture has essentially replaced
    RNNs in NLP applications.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，Transformer 架构基本上取代了 RNNs 在自然语言处理（NLP）应用中的地位。
- en: 'The Transformer architecture has several advantages over LSTM, the most used
    RNN type:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构相较于最常用的 RNN 类型 LSTM，具有几个优势：
- en: An LSTM encoder packs all the information obtained from the input sequence into
    a single embedding layer that is then passed to the decoder. This creates a bottleneck
    between the encoder and the decoder. The Transformer model, however, allows the
    decoder to look at each of the elements of the input sequence (at their embeddings,
    to be precise).
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM 编码器将从输入序列中获得的所有信息压缩到一个单一的嵌入层，然后传递给解码器。这在编码器和解码器之间创建了一个瓶颈。而 Transformer
    模型允许解码器查看输入序列的每个元素（准确来说，是查看它们的嵌入）。
- en: Since LSTM relies on backpropagation through time, the gradients are likely
    to explode or vanish throughout the update. The Transformer model, on the other
    hand, simultaneously looks at each of the input steps and does not run into a
    similar problem.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 LSTM 依赖时间反向传播，梯度很可能在更新过程中爆炸或消失。而 Transformer 模型则同时查看每个输入步骤，并不会遇到类似的问题。
- en: As a result, Transformer models can effectively use much longer input sequences.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果是，Transformer 模型能够有效地处理更长的输入序列。
- en: For these reasons, Transformer is potentially a competitive alternative to RNNs
    for RL applications as well.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 正因为如此，Transformer 也有可能成为强化学习应用中，RNNs的竞争替代方案。
- en: Info
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: A great tutorial on the Transformer architecture is by Jay Alammar, available
    at [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/),
    if you would like to catch up on the topic.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解该主题的更多内容，Jay Alammar 提供了一篇关于 Transformer 架构的优秀教程，网址为 [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)。
- en: Despite the advantages of the original Transformer model, it has been proven
    to be unstable in RL applications. An improvement has been proposed (*Parisotto
    et al., 2019*), named **Gated Transformer-XL** (**GTrXL**).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管原始的 Transformer 模型有其优势，但已被证明在强化学习应用中不稳定。已有改进方案被提出（*Parisotto et al., 2019*），名为**Gated
    Transformer-XL**（**GTrXL**）。
- en: 'RLlib has GTrXL implemented as a custom model. It can be used as follows:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: RLlib 已将 GTrXL 实现为自定义模型。它可以如下使用：
- en: '[PRE18]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This gives us another powerful architecture to try in RLlib.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了另一个强大的架构，可以在 RLlib 中尝试。
- en: Congratulations! We have arrived at the end of the chapter! We have covered
    important topics that deserve more attention than what our limited space allows
    here. Go ahead and read the sources in the *References* section and experiment
    with the repos we introduced to deepen your understanding of the topic.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你！我们已经到达本章的结尾！我们已经涵盖了几个重要主题，这些内容远比我们的篇幅所能表达的要更为深刻。请继续阅读*参考文献*部分的来源，并尝试我们介绍的仓库，以加深你对该主题的理解。
- en: Summary
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we covered an important topic in RL: generalization and partial
    observability, which are key for real-world applications. Note that this is an
    active research area: keep our discussion here as suggestions and the first methods
    to try for your problem. New approaches come out periodically, so watch out for
    them. The important thing is you should always keep an eye on generalization and
    partial observability for a successful RL implementation outside of video games.
    In the next section, we will take our expedition to yet another advanced level
    with meta-learning. So, stay tuned!'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了强化学习中的一个重要主题：泛化和部分可观测性，这对于现实世界的应用至关重要。请注意，这是一个活跃的研究领域：保持我们在这里的讨论作为建议，并尝试为您的问题尝试的第一种方法。新方法定期推出，所以请留意。重要的是，您应始终关注泛化和部分可观测性，以便在视频游戏之外成功实施强化学习。在下一节中，我们将通过元学习将我们的探险提升到另一个高级水平。所以，请继续关注！
- en: References
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Cobbe, K., Klimov, O., Hesse, C., Kim, T., & Schulman, J. (2018). *Quantifying
    Generalization in Reinforcement Learning*: [https://arxiv.org/abs/1812.02341](https://arxiv.org/abs/1812.02341)'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cobbe, K., Klimov, O., Hesse, C., Kim, T., & Schulman, J. (2018). *强化学习中的泛化量化*:
    [https://arxiv.org/abs/1812.02341](https://arxiv.org/abs/1812.02341)'
- en: 'Lee, K., Lee, K., Shin, J., & Lee, H. (2020). *Network Randomization: A Simple
    Technique for Generalization in Deep Reinforcement Learning*: [https://arxiv.org/abs/1910.05396](https://arxiv.org/abs/1910.05396)'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee, K., Lee, K., Shin, J., & Lee, H. (2020). *网络随机化：深度强化学习中的一种简单技术用于泛化*: [https://arxiv.org/abs/1910.05396](https://arxiv.org/abs/1910.05396)'
- en: 'Rivlin, O. (2019, November 21). *Generalization in Deep Reinforcement Learning*.
    Retrieved from Towards Data Science: [https://towardsdatascience.com/generalization-in-deep-reinforcement-learning-a14a240b155b](https://towardsdatascience.com/generalization-in-deep-reinforcement-learning-a14a240b155b)'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rivlin, O. (2019年11月21日). *深度强化学习中的泛化*。从Towards Data Science检索：[https://towardsdatascience.com/generalization-in-deep-reinforcement-learning-a14a240b155b](https://towardsdatascience.com/generalization-in-deep-reinforcement-learning-a14a240b155b)
- en: 'Rivlin, O. (2019). *Generalization in Deep Reinforcement Learning*. Retrieved
    from Towards Data Science: [https://towardsdatascience.com/generalization-in-deep-reinforcement-learning-a14a240b155](https://towardsdatascience.com/generalization-in-deep-reinforcement-learning-a14a240b155)'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rivlin, O. (2019). *深度强化学习中的泛化*。从Towards Data Science检索：[https://towardsdatascience.com/generalization-in-deep-reinforcement-learning-a14a240b155](https://towardsdatascience.com/generalization-in-deep-reinforcement-learning-a14a240b155)
- en: 'Cobbe, K., Klimov, O., Hesse, C., Kim, T., & Schulman, J. (2018). *Quantifying
    Generalization in Reinforcement Learning*: [https://arxiv.org/abs/1812.0234](https://arxiv.org/abs/1812.0234)'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cobbe, K., Klimov, O., Hesse, C., Kim, T., & Schulman, J. (2018). *强化学习中的泛化量化*:
    [https://arxiv.org/abs/1812.0234](https://arxiv.org/abs/1812.0234)'
- en: 'Lee, K., Lee, K., Shin, J., & Lee, H. (2020). *Network Randomization: A Simple
    Technique for Generalization in Deep Reinforcement Learning*: [https://arxiv.org/abs/1910.0539](https://arxiv.org/abs/1910.0539)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee, K., Lee, K., Shin, J., & Lee, H. (2020). *网络随机化：深度强化学习中的一种简单技术用于泛化*: [https://arxiv.org/abs/1910.0539](https://arxiv.org/abs/1910.0539)'
- en: 'Parisotto, E., et al. (2019). *Stabilizing Transformers for Reinforcement Learning*:
    [http://arxiv.org/abs/1910.06764](http://arxiv.org/abs/1910.06764)'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Parisotto, E., 等人 (2019). *用于强化学习的稳定变换器*: [http://arxiv.org/abs/1910.06764](http://arxiv.org/abs/1910.06764)'
