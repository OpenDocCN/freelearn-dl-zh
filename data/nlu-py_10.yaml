- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Machine Learning Part 2 – Neural Networks and Deep Learning Techniques
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习第二部分 – 神经网络与深度学习技术
- en: '**Neural networks** (**NNs**) have only became popular in **natural language
    understanding** (**NLU**) around 2010 but have since been widely applied to many
    problems. In addition, there are many applications of NNs to non-**natural language
    processing** (**NLP**) problems such as image classification. The fact that NNs
    are a general approach that can be applied across different research areas has
    led to some interesting synergies across these fields.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经网络**（**NN**）直到2010年左右才在**自然语言理解**（**NLU**）领域变得流行，但此后广泛应用于许多问题。此外，神经网络还被广泛应用于非**自然语言处理**（**NLP**）问题，如图像分类。神经网络作为一种可以跨领域应用的通用方法，已经在这些领域之间产生了一些有趣的协同效应。'
- en: In this chapter, we will cover the application of **machine learning** (**ML**)
    techniques based on NNs to problems such as NLP classification. We will also cover
    several different kinds of commonly used NNs—specifically, fully connected **multilayer
    perceptrons** (**MLPs**), **convolutional NNs** (**CNNs**), and **recurrent NNs**
    (**RNNs**)—and show how they can be applied to problems such as classification
    and information extraction. We will also discuss fundamental NN concepts such
    as hyperparameters, learning rate, activation functions, and epochs. We will illustrate
    NN concepts with a classification example using the TensorFlow/Keras libraries.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖基于神经网络（NN）的**机器学习**（**ML**）技术应用，解决诸如自然语言处理（NLP）分类等问题。我们还将介绍几种常用的神经网络——特别是全连接**多层感知器**（**MLP**）、**卷积神经网络**（**CNN**）和**循环神经网络**（**RNN**）——并展示它们如何应用于分类和信息提取等问题。我们还将讨论一些基本的神经网络概念，如超参数、学习率、激活函数和训练轮数（epochs）。我们将通过使用TensorFlow/Keras库的分类示例来说明神经网络的概念。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Basics of NNs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络基础
- en: Example—MLP for classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例——用于分类的MLP
- en: Hyperparameters and tuning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数与调优
- en: Moving beyond MLPs—RNNs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超越MLP——循环神经网络（RNN）
- en: Looking at another approach—CNNs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看另一种方法——卷积神经网络（CNN）
- en: Basics of NNs
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络基础
- en: The basic concepts behind NNs have been studied for many years but have only
    fairly recently been applied to NLP problems on a large scale. Currently, NNs
    are one of the most popular tools for solving NLP tasks. NNs are a large field
    and are very actively researched, so we won’t be able to give you a comprehensive
    understanding of NNs for NLP. However, we will attempt to provide you with some
    basic knowledge that will let you apply NNs to your own problems.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的基本概念已经研究了许多年，但直到最近才在大规模自然语言处理（NLP）问题中得到应用。目前，神经网络是解决NLP任务的最流行工具之一。神经网络是一个庞大的领域，且研究非常活跃，因此我们无法为你提供关于NLP神经网络的全面理解。然而，我们将尽力为你提供一些基本知识，帮助你将神经网络应用到自己的问题中。
- en: NNs are inspired by some properties of the animal nervous system. Specifically,
    animal nervous systems consist of a network of interconnected cells, called *neurons*,
    that transmit information throughout the network with the result that, given an
    input, the network produces an output that represents a decision about the input.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的灵感来自于动物神经系统的某些特性。具体而言，动物神经系统由一系列互联的细胞组成，这些细胞被称为*神经元*，它们通过网络传递信息，从而在给定输入时，网络产生一个输出，代表了对该输入的决策。
- en: '**Artificial NNs** (**ANNs**) are designed to model this process in some respects.
    The decision about how to react to the inputs is determined by a sequence of processing
    steps starting with units (*neurons*) that receive inputs and create outputs (or
    *fire*) if the correct conditions are met. When a neuron fires, it sends its output
    to other neurons. These next neurons receive inputs from a number of other neurons,
    and they in turn fire if they receive the right inputs. Part of the decision process
    about whether to fire involves *weights* on the neurons. The way that the NN learns
    to do its task—that is, the *training* process—is the process of adjusting the
    weights to produce the best results on the training data.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（**ANN**）旨在从某些方面模拟这一过程。如何反应输入的决定由一系列处理步骤决定，这些步骤从接收输入并在满足正确条件时产生输出（或**激活**）的单元（*神经元*）开始。当神经元激活时，它将其输出发送给其他神经元。接下来的神经元从多个其他神经元接收输入，并且当它们收到正确的输入时，它们也会激活。决定是否激活的部分过程涉及神经元的*权重*。神经网络学习完成任务的方式——也就是*训练*过程——是调整权重以在训练数据上产生最佳结果的过程。'
- en: The training process consists of a set of *epochs*, or passes through the training
    data, adjusting the weights on each pass to try to reduce the discrepancy between
    the result produced by the NN and the correct results.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程由一组*周期*组成，或者说是训练数据的遍历，每次遍历都会调整权重，试图减少神经网络产生的结果与正确结果之间的差异。
- en: The neurons in an NN are arranged in a series of layers, with the final layer—the
    output layer—producing the decision. Applying these concepts to NLP, we will start
    with an input text that is fed to the input layer, which represents the input
    being processed. Processing proceeds through all the layers, continuing through
    the NN until it reaches the output layer, which provides the decision—for example,
    is this movie review positive or negative?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的神经元按层排列，最终层——输出层——产生决策。将这些概念应用于自然语言处理（NLP），我们从输入文本开始，该文本被传递到输入层，表示正在处理的输入。处理通过所有层进行，直到到达输出层，输出决策——例如，这篇电影评论是正面还是负面？
- en: '*Figure 10**.1* represents a schematic diagram of an NN with an input layer,
    two hidden layers, and an output layer. The NN in *Figure 10**.1* is a **fully
    connected NN** (**FCNN**) because every neuron receives inputs from every neuron
    in the preceding layer and sends outputs to every neuron in the following layer:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10.1*表示一个包含输入层、两个隐藏层和输出层的神经网络示意图。*图10.1*中的神经网络是**全连接神经网络**（**FCNN**），因为每个神经元都接收来自前一层的每个神经元的输入，并将输出传递给后一层的每个神经元：'
- en: "![Figure 10.1 – A\uFEFFn FCNN with two hidden layers](img/B19005_10_01.jpg)"
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1 – 一个包含两个隐藏层的全连接神经网络](img/B19005_10_01.jpg)'
- en: Figure 10.1 – An FCNN with two hidden layers
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – 一个包含两个隐藏层的全连接神经网络
- en: 'The field of NNs uses a lot of specialized vocabulary, which can sometimes
    make it difficult to read documentation on the topic. In the following list, we’ll
    provide a brief introduction to some of the most important concepts, referring
    to *Figure 10**.1* as needed:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络领域使用了大量的专业词汇，有时会使得阅读相关文档变得困难。在下面的列表中，我们将简要介绍一些最重要的概念，并根据需要参考*图10.1*：
- en: '**Activation function**: The activation function is the function that determines
    when a neuron has enough inputs to fire and transmit its output to the neurons
    in the next layer. Some common activation functions are sigmoid and **rectified
    linear** **unit** (**ReLU**).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**：激活函数是确定神经元何时拥有足够输入以触发并将输出传递给下一层神经元的函数。一些常见的激活函数有sigmoid和**修正线性单元**（**ReLU**）。'
- en: '**Backpropagation**: The process of training an NN where the loss is fed back
    through the network to train the weights.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向传播**：训练神经网络的过程，其中损失通过网络反馈，训练权重。'
- en: '**Batch**: A batch is a set of samples that will be trained together.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批次**：批次是一组将一起训练的样本。'
- en: '**Connection**: A link between neurons, associated with a weight that represents
    the strength of the connection. The lines between neurons in *Figure 10**.1* are
    connections.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连接**：神经元之间的链接，关联着一个权重，代表连接的强度。在*图10.1*中的神经元之间的线是连接。'
- en: '**Convergence**: A network has converged when additional epochs do not appear
    to produce any reduction in the loss or improvements in accuracy.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收敛**：当额外的训练周期不再减少损失或提高准确度时，网络已收敛。'
- en: '**Dropout**: A technique for preventing overfitting by randomly removing neurons.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout**：通过随机删除神经元来防止过拟合的技术。'
- en: '**Early stopping**: Ending training before the planned number of epochs because
    training appears to have converged.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提前停止**：在计划的训练周期数之前结束训练，因为训练似乎已经收敛。'
- en: '**Epoch**: One pass through the training data, adjusting the weights to minimize
    loss.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**周期**：一次训练数据的遍历，调整权重以最小化损失。'
- en: '**Error**: The difference between the predictions produced by an NN and the
    reference labels. Measures how well the network predicts the classification of
    the data.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**误差**：神经网络产生的预测结果与参考标签之间的差异。衡量网络对数据分类的预测效果。'
- en: '**Exploding gradients**: Exploding gradients occur when gradients become unmanageably
    large during training.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度爆炸**：梯度爆炸发生在训练过程中，当梯度变得极大，无法控制时。'
- en: '**Forward propagation**: Propagation of inputs forward through an NN from the
    input layer through the hidden layers to the output layer.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前向传播**：输入通过神经网络的传播过程，从输入层经过隐藏层到输出层。'
- en: '**Fully connected**: An FCNN is an NN “*with every neuron in one layer connecting
    to every neuron in the next layer*” ([https://en.wikipedia.org/wiki/Artificial_neural_network](https://en.wikipedia.org/wiki/Artificial_neural_network)),
    as shown in *Figure 10**.1*.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全连接**：FCNN是一种神经网络，"*每一层的每个神经元都与下一层的每个神经元相连接*" ([https://en.wikipedia.org/wiki/Artificial_neural_network](https://en.wikipedia.org/wiki/Artificial_neural_network))，如*图
    10.1*所示。'
- en: '**Gradient descent**: Optimizing weights by adjusting them in a direction that
    will minimize loss.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度下降**：通过调整权重的方向来优化，以最小化损失。'
- en: '**Hidden layer**: A layer of neurons that is not the input or output layer.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：不是输入层或输出层的神经元层。'
- en: '**Hyperparameters**: Parameters that are not learned and are usually adjusted
    in a manual tuning process in order for the network to produce optimal results.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数**：在训练过程中未学习的参数，通常需要通过手动调整来优化，以使网络产生最佳结果。'
- en: '**Input layer**: The layer in an NN that receives the initial data. This is
    the layer on the left in *Figure 10**.1*.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：神经网络中的层，接收初始数据。这是*图 10.1*中左侧的层。'
- en: '**Layer**: A set of neurons in an NN that takes information from the previous
    layer and passes it on to the next layer. *Figure 10**.1* includes four layers.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层**：神经网络中的一组神经元，接收来自前一层的信息并将其传递到下一层。*图 10.1*包括了四个层。'
- en: '**Learning**: Assigning weights to connections in the training process in order
    to minimize loss.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习**：在训练过程中为连接分配权重，以最小化损失。'
- en: '**Learning rate/adaptive learning rate**: Amount of adjustment to the weights
    after each epoch. In some approaches, the learning rate can adapt as the training
    progresses; for example, if learning starts to slow, it can be useful to decrease
    the learning rate.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率/自适应学习率**：每个训练周期后对权重的调整量。在某些方法中，学习率可以随着训练的进行而自适应调整；例如，如果学习过程开始变慢，降低学习率可能会有所帮助。'
- en: '**Loss**: A function that provides a metric that quantifies the distance between
    the current model’s predictions and the goal values. The training process attempts
    to minimize loss.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失**：提供度量，用于量化当前模型预测与目标值之间的距离的函数。训练过程的目标是最小化损失。'
- en: '**MLP**: As described on *Wikipedia*, “*a fully connected class of feedforward
    artificial neural network (ANN). An MLP consists of at least three layers of nodes:
    an input layer, a hidden layer and an output layer*” ([https://en.wikipedia.org/wiki/Multilayer_perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)).
    *Figure 10**.1* shows an example of an MLP.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLP**：如*维基百科*所述，"*一种完全连接的前馈人工神经网络（ANN）类别。MLP至少包含三层节点：输入层、隐藏层和输出层*" ([https://en.wikipedia.org/wiki/Multilayer_perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron))。*图
    10.1*显示了一个MLP的示例。'
- en: '**Neuron (unit)**: A unit in an NN that receives inputs and computes outputs
    by applying an activation function.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经元（单元）**：神经网络中的单元，接收输入并通过应用激活函数计算输出。'
- en: '**Optimization**: Adjustment to the learning rate during training.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化**：在训练过程中调整学习率。'
- en: '**Output layer**: The final layer in an NN that produces a decision about the
    input. This is the layer on the right in *Figure 10**.1*.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：神经网络中的最终层，基于输入做出决策。这是*图 10.1*中右侧的层。'
- en: '**Overfitting**: Tuning the network too closely to the training data so that
    it does not generalize to previously unseen test or validation data.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合**：将网络调得过于紧密地适应训练数据，从而导致它不能很好地泛化到之前未见过的测试或验证数据。'
- en: '**Underfitting**: Underfitting occurs when an NN is unable to obtain good accuracy
    for training data. It can be addressed by using more training epochs or more layers.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欠拟合**：欠拟合发生在神经网络无法为训练数据获得良好准确度时。可以通过增加训练周期或增加层数来解决。'
- en: '**Vanishing gradients**: Gradients that become so small that the network is
    unable to make progress.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度消失**：梯度变得非常小，导致网络无法进行有效的进展。'
- en: '**Weights**: A property of the connection between neurons that represents the
    strength of the connection. Weights are learned during training.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重**：神经元之间连接的特性，表示连接的强度。权重是在训练过程中学习得到的。'
- en: In the next section, we will make these concepts concrete by going through an
    example of text classification with a basic MLP.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将通过一个基本的MLP文本分类示例，来具体化这些概念。
- en: Example – MLP for classification
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 – 用于分类的MLP
- en: We will review basic NN concepts by looking at the MLP, which is conceptually
    one of the most straightforward types of NNs. The example we will use is the classification
    of movie reviews into reviews with positive and negative sentiments. Since there
    are only two possible categories, this is a *binary* classification problem. We
    will use the *Sentiment Labelled Sentences Data Set* (*From Group to Individual
    Labels using Deep Features*, *Kotzias et al.*, *KDD 2015* [https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)),
    available from the University of California, Irvine. Start by downloading the
    data and unzipping it into a directory in the same directory as your Python script.
    You will see a directory called `sentiment labeled sentences` that contains the
    actual data in a file called `imdb_labeled.txt`. You can install the data into
    another directory of your choosing, but if you do, be sure to modify the `filepath_dict`
    variable accordingly.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过查看多层感知器（MLP）来回顾基本的神经网络概念，它是概念上最直观的神经网络类型之一。我们将使用的例子是电影评论的分类，将评论分为正面和负面情感。由于只有两个可能的类别，这是一个
    *二分类* 问题。我们将使用 *情感标注句子数据集*（*从群体到个体标签的深度特征*，*Kotzias 等人*，*KDD 2015* [https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)），该数据集来自加利福尼亚大学欧文分校。首先下载数据并将其解压到与
    Python 脚本相同的目录中。你会看到一个名为 `sentiment labeled sentences` 的目录，其中包含实际数据文件 `imdb_labeled.txt`。你也可以将数据安装到你选择的其他目录，但如果这样做，记得相应修改
    `filepath_dict` 变量。
- en: 'You can take a look at the data using the following Python code:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下 Python 代码查看数据：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The output from the last `print` statement will include the first sentence in
    the corpus, its label (`1` or `0`—that is, positive or negative), and its source
    (`Internet Movie` `Database IMDB`).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个 `print` 语句的输出将包含语料库中的第一句，其标签（`1` 或 `0`——即正面或负面），以及其来源（`互联网电影数据库 IMDB`）。
- en: In this example, we will vectorize the corpus using the scikit-learn count `CountVectorizer`),
    which we saw earlier in [*Chapter 7*](B19005_07.xhtml#_idTextAnchor144).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用 scikit-learn 的 `CountVectorizer` 来对语料库进行向量化，这个方法我们在 [*第7章*](B19005_07.xhtml#_idTextAnchor144)
    中已经提到过。
- en: 'The following code snippet shows the start of the vectorization process, where
    we set up some parameters for the vectorizer:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了向量化过程的开始，我们为向量化器设置了一些参数：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `CountVectorizer` function has some useful parameters that control the maximum
    number of words that will be used to build the model, as well as make it possible
    to exclude words that are considered to be too frequent or too rare to be very
    useful in distinguishing documents.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer` 函数有一些有用的参数，能够控制用于构建模型的最大单词数，还可以排除那些被认为过于频繁或过于稀有、不太有用的词语。'
- en: 'The next step is to do the train-test split, as shown in the following code
    block:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是进行训练集和测试集的拆分，如下代码块所示：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The preceding code shows the splitting of the training and test data, reserving
    20% of the total data for testing.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码展示了训练数据和测试数据的拆分，保留了总数据的 20% 用于测试。
- en: 'The `reviews` variable holds the actual documents, and the `y` variable holds
    their labels. Note that `X` and `y` are frequently used in the literature to represent
    the data and the categories in an ML problem, respectively, although we’re using
    `reviews` for the `X` data here:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`reviews` 变量保存实际文档，而 `y` 变量保存它们的标签。请注意，`X` 和 `y` 在文献中常常用来分别表示机器学习问题中的数据和类别，尽管我们在这里使用
    `reviews` 作为 `X` 数据：'
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The preceding code shows the process of vectorizing the data, or converting
    each document to a numerical representation, using the vectorizer that was defined
    previously. You can review vectorization by going back to [*Chapter 7*](B19005_07.xhtml#_idTextAnchor144).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码展示了数据的向量化过程，即使用先前定义的向量化器将每个文档转换为数值表示。你可以通过回顾 [*第7章*](B19005_07.xhtml#_idTextAnchor144)
    来复习向量化。
- en: The result is `X_train`, the count BoW of the dataset. You will recall the count
    BoW from [*Chapter 7*](B19005_07.xhtml#_idTextAnchor144).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是 `X_train`，即数据集的词频（BoW）。你可以回忆起在 [*第7章*](B19005_07.xhtml#_idTextAnchor144)
    中提到的词频模型（BoW）。
- en: 'The next step is to set up the NN. We will be using the Keras package, which
    is built on top of Google’s TensorFlow ML package. Here’s the code we need to
    execute:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是建立神经网络（NN）。我们将使用 Keras 包，它建立在 Google 的 TensorFlow 机器学习包之上。以下是我们需要执行的代码：
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The code first prints the input dimension, which in this case is the number
    of words in each document vector. The input dimension is useful to know because
    it’s computed from the corpus, as well as the parameters we set in the `CountVectorizer`
    function. If it is unexpectedly large or small, we might want to change the parameters
    to make the vocabulary larger or smaller.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 代码首先打印输入维度，在本例中是每个文档向量中的单词数。了解输入维度非常重要，因为它是从语料库中计算出来的，并且与我们在 `CountVectorizer`
    函数中设置的参数相关。如果输入维度异常地大或小，我们可能需要调整参数，以便使词汇表变大或变小。
- en: 'The following code defines the model:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码定义了模型：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The model built in the preceding code includes the input layer, two hidden layers,
    and one output layer. Each call to the `model.add()` method adds a new layer to
    the model. All the layers are dense because, in this fully connected network,
    every neuron receives inputs from every neuron in the previous layer, as illustrated
    in *Figure 10**.1*. The 2 hidden layers each contain 16 neurons. Why do we specify
    16 neurons? There is no hard and fast rule for how many neurons to include in
    the hidden layers, but a general approach would be to start with a smaller number
    since the training time will increase as the number of neurons increases. The
    final output layer will only have one neuron because we only want one output for
    this problem, whether the review is positive or negative.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中构建的模型包括输入层、两个隐藏层和一个输出层。每次调用 `model.add()` 方法都会向模型中添加一个新层。所有层都是全连接的，因为在这个全连接的网络中，每个神经元都会接收来自前一层每个神经元的输入，如
    *图 10.1* 所示。两个隐藏层各包含 16 个神经元。为什么指定 16 个神经元？隐藏层神经元的数量没有硬性规定，但一般的方法是从较小的数字开始，因为随着神经元数量的增加，训练时间也会增加。最终的输出层只包含一个神经元，因为我们只需要一个输出，判断评论是正面还是负面。
- en: 'Another very important parameter is the **activation function**. The activation
    function is the function that determines how the neuron responds to its inputs.
    For all of the layers in our example, except the output layer, this is the ReLU
    activation function. The ReLU function can be seen in *Figure 10**.2*. ReLU is
    a very commonly used activation function:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常重要的参数是 **激活函数**。激活函数决定了神经元如何响应其输入。在我们的示例中，除了输出层外，所有层的激活函数都是 ReLU 函数。ReLU
    函数可以在 *图 10.2* 中看到。ReLU 是一种非常常用的激活函数：
- en: '![Figure 10.2 – Values of the ReLU function for inputs between -15 and 15](img/B19005_10_02.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.2 – ReLU 函数在输入范围从 -15 到 15 之间的值](img/B19005_10_02.jpg)'
- en: Figure 10.2 – Values of the ReLU function for inputs between -15 and 15
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 – ReLU 函数在输入范围从 -15 到 15 之间的值
- en: One of the most important benefits of the ReLU function is that it is very efficient.
    It has also turned out to generally give good results in practice and is normally
    a reasonable choice as an activation function.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 函数最重要的优点之一是它非常高效。它也证明在实践中通常能提供良好的结果，并且通常是作为激活函数的合理选择。
- en: 'The other activation function that’s used in this NN is the sigmoid function,
    which is used in the output layer. We use the sigmoid function here because in
    this problem we want to predict the probability of a positive or negative sentiment,
    and the value of the sigmoid function will always be between `0` and `1`. The
    formula for the sigmoid function is shown in the following equation:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此神经网络中使用的另一个激活函数是 sigmoid 函数，它用于输出层。我们在这里使用 sigmoid 函数，因为在这个问题中，我们需要预测正面或负面情感的概率，而
    sigmoid 函数的值始终介于 `0` 和 `1` 之间。sigmoid 函数的公式如下所示：
- en: S(x) =  1 _ 1 + e −x
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: S(x) = 1 / (1 + e^−x)
- en: 'A plot of the sigmoid function is shown in *Figure 10**.3*, and it is easy
    to see that its output value will always be between `0` and `1` regardless of
    the value of the input:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数的图像显示在 *图 10.3* 中，很容易看出，无论输入值如何，其输出值始终介于 `0` 和 `1` 之间：
- en: '![Figure 10.3 – Values of the sigmoid  function for inputs between -10 and
    10](img/B19005_10_03.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.3 – Sigmoid 函数在输入范围从 -10 到 10 之间的值](img/B19005_10_03.jpg)'
- en: Figure 10.3 – Values of the sigmoid function for inputs between -10 and 10
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 – Sigmoid 函数在输入范围从 -10 到 10 之间的值
- en: 'The sigmoid and ReLU activation functions are popular and practical activation
    functions, but they are only two examples of the many possible NN activation functions.
    If you wish to investigate this topic further, the following *Wikipedia* article
    is a good place to start: [https://en.wikipedia.org/wiki/Activation_function](https://en.wikipedia.org/wiki/Activation_function).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 和 ReLU 激活函数是常见且实用的激活函数，但它们只是众多可能的神经网络激活函数中的两个示例。如果你希望进一步研究这个话题，以下的 *维基百科*
    文章是一个不错的起点：[https://en.wikipedia.org/wiki/Activation_function](https://en.wikipedia.org/wiki/Activation_function)。
- en: 'Once the model has been defined, we can compile it, as shown in the following
    code snippet:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型定义完成，我们就可以编译它，如下所示的代码片段：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `model.compile()` method requires the `loss`, `optimizer`, and `metrics`
    parameters, which supply the following information:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.compile()` 方法需要 `loss`、`optimizer` 和 `metrics` 参数，这些参数提供以下信息：'
- en: The `loss` parameter, in this case, tells the compiler to use `binary_crossentropy`
    to compute the loss. `categorical_crossentropy`, is used for problems when there
    are two or more label classes in the output. For example, if the task were to
    assign a star rating to reviews, we might have five output classes corresponding
    to the five possible star ratings, and in that case, we would use categorical
    cross-entropy.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` 参数在这里告诉编译器使用 `binary_crossentropy` 来计算损失。`categorical_crossentropy`
    用于输出中有两个或更多标签类别的问题。例如，如果任务是给评论分配星级评分，我们可能会有五个输出类别，分别对应五个可能的星级评分，在这种情况下，我们会使用类别交叉熵。'
- en: The `optimizer` parameter adjusts the learning rate during training. We will
    not go into the mathematical details of `adam` here, but generally speaking, the
    optimizer we use here, `adam`, normally turns out to be a good choice.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer` 参数在训练过程中调整学习率。我们在这里不讨论 `adam` 的数学细节，但一般来说，我们使用的优化器 `adam` 通常是一个不错的选择。'
- en: Finally, the `metrics` parameter tells the compiler how we will evaluate the
    quality of the model. We can include multiple metrics in this list, but we will
    just include `accuracy` for now. In practice, the metrics you use will depend
    on your problem and dataset, but `accuracy` is a good metric to use for the purposes
    of our example. In [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226), we will explore
    other metrics and the reasons that you might want to select them in particular
    situations.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，`metrics` 参数告诉编译器我们将如何评估模型的质量。我们可以在这个列表中包含多个指标，但现在我们仅包括 `accuracy`。在实际应用中，你使用的指标将取决于你的问题和数据集，但对于我们这个示例来说，`accuracy`
    是一个不错的选择。在[*第13章*](B19005_13.xhtml#_idTextAnchor226)中，我们将探讨其他指标，以及在特定情况下你可能希望选择它们的原因。
- en: 'It is also helpful to display a summary of the model to make sure that the
    model is structured as intended. The `model.summary()` method will produce a summary
    of the model, as shown in the following code snippet:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 显示模型的摘要也很有帮助，以确保模型按预期的方式构建。`model.summary()` 方法将生成模型的摘要，如下所示的代码片段：
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this output, we can see that the network, consisting of four dense layers
    (which are the input layer, the two hidden layers, and the output layer), is structured
    as expected.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个输出中，我们可以看到网络由四个全连接层（包括输入层、两个隐藏层和输出层）组成，且结构符合预期。
- en: 'The final step is to fit or train the network, using the following code:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是使用以下代码来拟合或训练网络：
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Training is the iterative process of putting the training data through the network,
    measuring the loss, adjusting the weights to reduce the loss, and putting the
    training data through the network again. This step can be quite time-consuming,
    depending on the size of the dataset and the size of the model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 训练是一个迭代过程，将训练数据通过网络，测量损失，调整权重以减少损失，然后再次将训练数据通过网络。这个步骤可能非常耗时，具体取决于数据集的大小和模型的大小。
- en: Each cycle through the training data is an epoch. The number of epochs in the
    training process is a *hyperparameter*, which means that it’s adjusted by the
    developer based on the training results. For example, if the network’s performance
    doesn’t seem to be improving after a certain number of epochs, the number of epochs
    can be reduced since additional epochs are not improving the result. Unfortunately,
    there is no set number of epochs after which we can stop training. We have to
    observe the improvements in accuracy and loss over epochs to decide whether the
    system is sufficiently trained.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 每次遍历训练数据的过程叫做一个周期。训练过程中的周期数是一个*超参数*，意味着它是由开发者根据训练结果调整的。例如，如果网络的表现似乎在经过一定数量的周期后没有改善，则可以减少周期数，因为额外的周期不会改善结果。不幸的是，并没有一个固定的周期数可以决定我们何时停止训练。我们必须观察准确率和损失随周期的变化，来决定系统是否已经充分训练。
- en: Setting the `verbose = True` parameter is optional but useful because this will
    produce a trace of the results after each epoch. If the training process is long,
    the trace can help you verify that the training is making progress. The batch
    size is another hyperparameter that defines how many data samples are to be processed
    before updating the model. When the following Python code is executed, with `verbose`
    set to `True`, at the end of every epoch, the loss, the accuracy, and the validation
    loss and accuracy will be computed. After training is complete, the `history`
    variable will contain information about the progress of the training process,
    and we can see plots of the training progress.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 设置`verbose = True`参数是可选的，但非常有用，因为它会在每个周期后生成结果追踪。如果训练过程很长，追踪信息可以帮助你验证训练是否在进展。批量大小是另一个超参数，它定义了在更新模型之前要处理多少数据样本。当执行以下Python代码时，设置`verbose`为`True`，每个周期结束时，损失、准确率、验证损失和验证准确率都会被计算出来。训练完成后，`history`变量将包含训练过程的进展信息，我们可以看到训练进展的图表。
- en: 'It is important to display how the plots of accuracy and loss change with each
    epoch because it will give us an idea of how many epochs are needed to get this
    training to converge and will make it very clear when the data is overfitting.
    The following code shows how to plot the accuracy and loss changes over epochs::'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 显示准确率和损失在每个周期的变化非常重要，因为这可以帮助我们了解训练收敛需要多少个周期，并且能清楚地看到数据是否过拟合。以下代码展示了如何绘制准确率和损失随周期变化的图表：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can see the results of the progress of our example through training over
    20 epochs in *Figure 10**.4*:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过训练20个周期（*图10.4*）来看到我们示例的进展结果：
- en: '![Figure 10.4 – Accuracy and loss over 20 epochs of training](img/B19005_10_04.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图10.4 – 训练20个周期的准确率和损失](img/B19005_10_04.jpg)'
- en: Figure 10.4 – Accuracy and loss over 20 epochs of training
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 – 训练20个周期的准确率和损失
- en: Over the 20 epochs of training, we can see that the training accuracy approaches
    **1.0** and the training loss approaches **0**. However, this apparently good
    result is misleading because the really important results are based on the validation
    data. Because the validation data is not being used to train the network, it is
    the performance on the validation data that actually predicts how the network
    will perform in use. We can see from the plots of the changes in validation accuracy
    and loss that doing more training epochs after about 10 is not improving the model’s
    performance on the validation data. In fact, it is increasing the loss and therefore
    making the model worse. This is clear from the increase in the validation loss
    in the graph on the right in *Figure 10**.4*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练的20个周期中，我们可以看到训练准确率接近**1.0**，训练损失接近**0**。然而，这个看似好的结果是具有误导性的，因为真正重要的结果是基于验证数据的。由于验证数据没有用来训练网络，验证数据上的表现实际上预测了网络在实际使用中的表现。我们从验证准确率和损失变化的图表中可以看到，在大约第10个周期后，继续训练并没有改善模型在验证数据上的表现。事实上，这反而增加了损失，使得模型变得更差。从*图10.4*右侧的图表中验证损失的增加可以看出这一点。
- en: Improving performance on this task will involve modifying other factors, such
    as hyperparameters and other tuning processes, which we will go over in the next
    section.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 提高任务性能将涉及修改其他因素，如超参数和其他调优过程，我们将在下一节中讨论这些内容。
- en: Hyperparameters and tuning
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数和调优
- en: '*Figure 10**.4* clearly shows that increasing the number of training epochs
    is not going to improve performance on this task. The best validation accuracy
    seems to be about 80% after 10 epochs. However, 80% accuracy is not very good.
    How can we improve it? Here are some ideas. None of them is guaranteed to work,
    but it is worth experimenting with them:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10.4*清楚地显示，增加训练周期数并不会提升任务的性能。在10个周期后，最佳的验证准确率似乎是大约80%。然而，80%的准确率并不算很好。我们该如何改进呢？以下是一些想法。虽然没有任何一个方法能保证有效，但值得尝试：'
- en: If more training data is available, the amount of training data can be increased.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果有更多的训练数据可用，可以增加训练数据的数量。
- en: Preprocessing techniques that can remove noise from the training data can be
    investigated—for example, stopword removal, removing non-words such as numbers
    and HTML tags, stemming and lemmatization, and lowercasing. Details on these techniques
    were covered in [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107).
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以研究一些预处理技术，以去除训练数据中的噪音——例如，去除停用词、移除数字和HTML标签等非词项、词干提取、词形还原以及小写化等。这些技术的细节在[*第5章*](B19005_05.xhtml#_idTextAnchor107)中有介绍。
- en: Changes to the learning rate—for example, lowering the learning rate might improve
    the ability of the network to avoid local minima.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率的变化——例如，降低学习率可能改善网络避免局部最小值的能力。
- en: Decreasing the batch size.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少批量大小。
- en: Changing the number of layers and the number of neurons in each layer is something
    that can be tried, but having too many layers is likely to lead to overfitting.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以尝试改变层数和每层神经元的数量，但层数过多可能会导致过拟合。
- en: Adding dropout by specifying a hyperparameter that defines the probability that
    the outputs from a layer will be ignored. This can help make the network more
    robust to overfitting.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过指定一个超参数来添加dropout，这个超参数定义了层输出被忽略的概率。这有助于提高网络对过拟合的鲁棒性。
- en: Improvements in vectorization—for example, by using **term frequency-inverse
    document frequency** (**TF-IDF**) instead of count BoW.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量化的改进——例如，使用**词频-逆文档频率**（**TF-IDF**）而不是计数型的BoW。
- en: A final strategy for improving performance is to try some of the newer ideas
    in NNs—specifically, RNNs, CNNs, and transformers.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 提高性能的最终策略是尝试一些新型的神经网络方法——特别是RNN、CNN和transformer。
- en: We will conclude this chapter by briefly reviewing RNNs and CNNs. We will cover
    transformers in [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的最后简要回顾RNN和CNN。我们将在[*第11章*](B19005_11.xhtml#_idTextAnchor193)讨论transformer。
- en: Moving beyond MLPs – RNNs
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越多层感知机（MLP）——递归神经网络（RNN）
- en: RNNs are a type of NN that is able to take into account the order of items in
    an input. In the example of the MLP that was discussed previously, the vector
    representing the entire input (that is, the complete document) was fed to the
    NN at once, so the network had no way of taking into account the order of words
    in the document. However, this is clearly an oversimplification in the case of
    text data since the order of words can be very important to the meaning. RNNs
    are able to take into account the order of words by using earlier outputs as inputs
    to later layers. This can be especially helpful in certain NLP problems where
    the order of words is very important, such as **named entity recognition** (**NER**),
    **part-of-speech (POS) tagging**, or **slot labeling**.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: RNN是一种能够考虑输入中项次序的神经网络。在之前讨论的MLP示例中，表示整个输入（即完整文档）的向量一次性输入神经网络，因此网络无法考虑文档中单词的顺序。然而，在文本数据中，这显然是过于简化的，因为单词的顺序可能对含义非常重要。RNN通过将早期的输出作为后续层的输入，能够考虑单词的顺序。在某些自然语言处理（NLP）问题中，单词顺序非常重要，例如**命名实体识别**（**NER**）、**词性标注**（POS）或**槽标签**（slot
    labeling），RNN尤为有用。
- en: 'A diagram of a unit of an RNN is shown in *Figure 10**.5*:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: RNN单元的示意图如*图10.5*所示：
- en: '![Figure 10.5 – A unit of an RNN](img/B19005_10_05.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图10.5 – 一个RNN单元](img/B19005_10_05.jpg)'
- en: Figure 10.5 – A unit of an RNN
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 – 一个RNN单元
- en: The unit is shown at time *t*. The input at time *t*, *x(t)*, is passed to the
    activation function as in the case of the MLP, but the activation function also
    receives the output from time *t-1*—that is, *x(t-1)*. For NLP, the earlier input
    would most likely have been the previous word. So, in this case, the input is
    the current word and one previous word. Using an RNN with Keras is very similar
    to the MLP example that we saw earlier, with the addition of a new RNN layer in
    the layer stack.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 单元显示在时间 *t*。时间 *t* 的输入，*x(t)*，与MLP中的情况一样传递给激活函数，但激活函数还会接收到来自时间 *t-1* 的输出——即
    *x(t-1)*。对于NLP来说，早期的输入很可能是前一个词。因此，在这种情况下，输入是当前词和一个前一个词。使用Keras中的RNN与我们之前看到的MLP示例非常相似，只是在层堆栈中添加了一个新的RNN层。
- en: However, as the length of the input increases, the network will tend to *forget*
    information from earlier inputs, because the older information will have less
    and less influence over the current state. Various strategies have been designed
    to overcome this limitation, such as **gated recurrent units** (**GRUs**) and
    **long short-term memory** (**LSTM**). If the input is a complete text document
    (as opposed to speech), we have access not only to previous inputs but also to
    future inputs, and a bidirectional RNN can be used.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着输入长度的增加，网络往往会*忘记*早期输入的信息，因为较早的信息对当前状态的影响会越来越小。为克服这一限制，已经设计了各种策略，如**门控循环单元**（**GRU**）和**长短期记忆**（**LSTM**）。如果输入是完整的文本文档（而非语音），我们不仅可以访问到之前的输入，还能访问未来的输入，这时可以使用双向RNN。
- en: 'We will not cover these additional variations of RNNs here, but they do often
    improve performance on some tasks, and it would be worth researching them. Although
    there is a tremendous amount of resources available on this popular topic, the
    following *Wikipedia* article is a good place to start: [https://en.wikipedia.org/wiki/Recurrent_neural_network](https://en.wikipedia.org/wiki/Recurrent_neural_network).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在这里详细讲解这些RNN的额外变种，但它们确实在某些任务中提高了性能，值得进行研究。尽管关于这个热门话题有大量的资源，以下的*Wikipedia*文章是一个很好的起点：[https://en.wikipedia.org/wiki/Recurrent_neural_network](https://en.wikipedia.org/wiki/Recurrent_neural_network)。
- en: Looking at another approach – CNNs
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 看看另一种方法——CNN
- en: CNNs are very popular for image recognition tasks, but they are less often used
    for NLP tasks than RNNs because they don’t take into account the temporal order
    of items in the input. However, they can be useful for document classification
    tasks. As you will recall from earlier chapters, the representations that are
    often used in classification depend only on the words that occur in the document—BoW
    and TF-IDF, for example—so, effective classification can often be accomplished
    without taking word order into account.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: CNN在图像识别任务中非常流行，但在NLP任务中使用的频率低于RNN，因为它们没有考虑输入项的时间顺序。然而，它们在文档分类任务中可以很有用。如你从前面的章节中回忆到的，分类中常用的表示方法仅依赖于文档中出现的词汇——例如BoW和TF-IDF——因此，通常可以在不考虑词序的情况下完成有效的分类。
- en: To classify documents with CNNs, we can represent a text as an array of vectors,
    where each word is mapped to a vector in a space made up of the full vocabulary.
    We can use word2vec, which we discussed in [*Chapter 7*](B19005_07.xhtml#_idTextAnchor144),
    to represent word vectors. Training a CNN for text classification with Keras is
    very similar to the training process that we worked through in MLP classification.
    We create a sequential model as we did earlier, but we add new convolutional layers
    and pooling layers.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 要用CNN对文档进行分类，我们可以将文本表示为一个向量数组，其中每个词都映射到由完整词汇表构成的空间中的一个向量。我们可以使用我们在[*第7章*](B19005_07.xhtml#_idTextAnchor144)中讨论过的word2vec来表示词向量。使用Keras训练CNN进行文本分类与我们在MLP分类中使用的训练过程非常相似。我们像之前一样创建一个顺序模型，但我们添加了新的卷积层和池化层。
- en: We will not cover the details of using CNNs for classification, but they are
    another option for NLP classification. As in the case of RNNs, there are many
    available resources on this topic, and a good starting point is *Wikipedia* ([https://en.wikipedia.org/wiki/Convolutional_neural_network](https://en.wikipedia.org/wiki/Convolutional_neural_network)).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在这里详细讲解CNN用于分类的细节，但它们是NLP分类的另一种选择。和RNN一样，关于这个话题有许多可用的资源，一个很好的起点是*Wikipedia*（[https://en.wikipedia.org/wiki/Convolutional_neural_network](https://en.wikipedia.org/wiki/Convolutional_neural_network)）。
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we have explored applications of NNs to document classification
    in NLP. We covered the basic concepts of NNs, reviewed a simple MLP, and applied
    it to a binary classification problem. We also provided some suggestions for improving
    performance by modifying hyperparameters and tuning. Finally, we discussed the
    more advanced types of NNs—RNNs and CNNs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了神经网络（NNs）在NLP中文档分类中的应用。我们介绍了神经网络的基本概念，回顾了一个简单的多层感知机（MLP），并将其应用于二分类问题。我们还提供了一些通过修改超参数和调整来提高性能的建议。最后，我们讨论了更高级的神经网络类型——循环神经网络（RNNs）和卷积神经网络（CNNs）。
- en: In [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193), we will cover the currently
    best-performing techniques in NLP—transformers and pretrained models.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第11章*](B19005_11.xhtml#_idTextAnchor193)中，我们将介绍目前在自然语言处理（NLP）中表现最好的技术——变压器（transformers）和预训练模型（pretrained
    models）。
