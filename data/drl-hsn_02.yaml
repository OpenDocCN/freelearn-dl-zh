- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: OpenAI Gym API and Gymnasium
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI Gym API 和 Gymnasium
- en: After talking so much about the theoretical concepts of reinforcement learning
    (RL) in Chapter [1](ch005.xhtml#x1-190001), let’s start doing something practical.
    In this chapter, you will learn the basics of Gymnasium, a library used to provide
    a uniform API for an RL agent and lots of RL environments. Originally, this API
    was implemented in the OpenAI Gym library, but it is no longer maintained. In
    this book, we’ll use Gymnasium—a fork of OpenAI Gym implementing the same API.
    In any case, having a uniform API for environments removes the need to write boilerplate
    code and allows you to implement an agent in a general way without worrying about
    environment details.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们讨论了强化学习（RL）的理论概念后，接下来让我们开始一些实际操作。在本章中，你将学习 Gymnasium 的基础知识，这是一种为 RL
    智能体和大量 RL 环境提供统一 API 的库。最初，这个 API 是在 OpenAI Gym 库中实现的，但它不再维护。在本书中，我们将使用 Gymnasium——OpenAI
    Gym 的一个分支，实现在同一 API 下的功能。无论如何，统一的 API 让环境的细节无须担心，避免了编写冗余代码，从而可以用更通用的方式实现智能体。
- en: 'You will also write your first randomly behaving agent and become more familiar
    with the basic concepts of RL that we have covered so far. By the end of the chapter,
    you will have an understanding of:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你还将编写第一个随机行为的智能体，并进一步熟悉我们迄今为止覆盖的强化学习基本概念。到本章结束时，你将理解：
- en: The high-level requirements that need to be implemented to plug the agent into
    the RL framework
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要实现的高级要求，以便将智能体接入强化学习框架
- en: A basic, pure-Python implementation of the random RL agent
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个基础的纯 Python 实现的随机强化学习智能体
- en: The OpenAI Gym API and its implementation – the Gymnasium library
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI Gym API 及其实现 —— Gymnasium 库
- en: The anatomy of the agent
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 智能体的结构
- en: 'As you learned in the previous chapter, there are several fundamental concepts
    in RL:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在上一章中学到的，强化学习中有几个基本概念：
- en: 'The agent: A thing, or person, that takes an active role. In practice, the
    agent is some piece of code that implements some policy. Basically, this policy
    decides what action is needed at every time step, given our observations.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体：执行主动角色的事物或人。在实践中，智能体是实现某种策略的一段代码。基本上，这个策略决定了在每个时间步上，根据我们的观察需要采取什么行动。
- en: 'The environment: Everything that is external to the agent and has the responsibility
    of providing observations and giving rewards. The environment changes its state
    based on the agent’s actions.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境：一切外部于智能体的事物，负责提供观察和奖励。环境根据智能体的行为改变其状态。
- en: Let’s explore how both can be implemented in Python for a simple situation.
    We will define an environment that will give the agent random rewards for a limited
    number of steps, regardless of the agent’s actions. This scenario is not very
    useful in the real world, but it will allow us to focus on specific methods in
    both the environment and agent classes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索如何在 Python 中实现这两者，针对一个简单的情况。我们将定义一个环境，它会根据智能体的行为，在有限的步骤内给智能体随机奖励。这个场景在现实世界中并不十分有用，但它将帮助我们集中精力在环境和智能体类中的特定方法上。
- en: 'Please note that the code snippets shown in this book are not full examples.
    You can find the full examples on the GitHub page: [https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition)
    and run them.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本书中展示的代码片段并不是完整示例。你可以在 GitHub 页面找到完整的示例：[https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition)
    并运行它们。
- en: 'Let’s start with the environment:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从环境开始：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding code, we allowed the environment to initialize its internal
    state. In our case, the state is just a counter that limits the number of time
    steps that the agent is allowed to take to interact with the environment.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们允许环境初始化其内部状态。在我们的例子中，状态只是一个计数器，限制了智能体与环境交互的时间步数。
- en: 'The get_observation() method is supposed to return the current environment’s
    observation to the agent. It is usually implemented as some function of the internal
    state of the environment:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: get_observation() 方法应该返回当前环境的观察信息给智能体。它通常实现为环境内部状态的某种函数：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you’re curious about what is meant by -> List[float], that’s an example
    of Python type annotations, which were introduced in Python 3.5\. You can find
    out more in the documentation at [https://docs.python.org/3/library/typing.xhtml](https://docs.python.org/3/library/typing.xhtml).
    In our example, the observation vector is always zero, as the environment basically
    has no internal state. The get_actions() method allows the agent to query the
    set of actions it can execute:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对`-> List[float]`的含义感到好奇，那是Python类型注解的一个示例，这一功能是在Python 3.5中引入的。你可以在[https://docs.python.org/3/library/typing.xhtml](https://docs.python.org/3/library/typing.xhtml)中了解更多信息。在我们的示例中，观察向量始终为零，因为环境基本上没有内部状态。`get_actions()`方法允许代理查询它可以执行的动作集合：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Normally, the set of actions does not change over time, but some actions can
    become impossible in different states (for example, not every move is possible
    in any position of the tic-tac-toe game). In our simplistic example, there are
    only two actions that the agent can carry out, which are encoded with the integers
    0 and 1.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，动作集合不会随时间变化，但某些动作在不同状态下可能变得不可行（例如，在井字棋的任何位置并不是每一步都可以走）。在我们的简单示例中，代理能执行的动作只有两种，它们分别用整数0和1表示。
- en: 'The following method signals the end of the episode to the agent:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方法向代理发出回合结束的信号：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you saw in Chapter [1](ch005.xhtml#x1-190001), the series of environment-agent
    interactions is divided into a sequence of steps called episodes. Episodes can
    be finite, like in a game of chess, or infinite, like the Voyager 2 mission (a
    famous space probe that was launched over 46 years ago and has traveled beyond
    our solar system). To cover both scenarios, the environment provides us with a
    way to detect when an episode is over and there is no way to communicate with
    it anymore.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在第[1](ch005.xhtml#x1-190001)章中看到的，环境与代理之间的一系列交互被分为一系列步骤，称为回合（episodes）。回合可以是有限的，比如棋局中的回合，或者是无限的，比如“旅行者2号”任务（这是一项著名的太空探测任务，发射已超过46年，且已越过我们的太阳系）。为了涵盖这两种情况，环境提供了一种方法，用来检测回合何时结束，并且无法再与其通信。
- en: 'The action() method is the central piece in the environment’s functionality:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`action()`方法是环境功能的核心部分：'
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: It does two things – handles an agent’s action and returns the reward for this
    action. In our example, the reward is random and its action is discarded. Additionally,
    we update the count of steps and don’t continue the episodes that are over.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 它做了两件事——处理代理的动作并返回该动作的奖励。在我们的示例中，奖励是随机的，其动作被丢弃。此外，我们更新了步骤计数，并且不会继续已经结束的回合。
- en: 'Now, when looking at the agent’s part, it is much simpler and includes only
    two methods: the constructor and the method that performs one step in the environment:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，查看代理的部分会简单得多，仅包括两个方法：构造函数和执行环境中一步操作的方法：
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the constructor, we initialize the counter that will keep the total reward
    accumulated by the agent during the episode.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在构造函数中，我们初始化了一个计数器，用于记录代理在回合过程中累积的总奖励。
- en: 'The step() function accepts the environment instance as an argument:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`step()`函数接受环境实例作为参数：'
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This function allows the agent to perform the following actions:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 该功能允许代理执行以下操作：
- en: Observe the environment
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察环境
- en: Make a decision about the action to take based on the observations
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据观察结果做出关于采取哪种动作的决策
- en: Submit the action to the environment
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将动作提交给环境
- en: Get the reward for the current step
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取当前步骤的奖励
- en: 'For our example, the agent is dull and ignores the observations obtained during
    the decision-making process about which action to take. Instead, every action
    is selected randomly. The final piece is the glue code, which creates both classes
    and runs one episode:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，代理很迟钝，在做出采取哪个动作的决策过程中忽略了获得的观察结果。相反，每个动作都是随机选择的。最后一部分是粘合代码，它创建了两个类并运行一个回合：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can find the full code in this book’s GitHub repository at [https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition)
    in the Chapter02/01_agent_anatomy.py file. It has no external dependencies and
    should work with any relatively modern Python version. By running it several times,
    you’ll get different amounts of reward gathered by the agent. The following is
    an output I got on my machine:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的 GitHub 仓库中找到完整的代码，地址是 [https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition)，文件位于
    Chapter02/01_agent_anatomy.py 中。它没有外部依赖，并且应该能在任何相对现代的 Python 版本中运行。通过多次运行，你将获得代理收集的不同数量的奖励。以下是我在我的机器上得到的输出：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The simplicity of the preceding code illustrates the important basic concepts
    of the RL model. The environment could be an extremely complicated physics model,
    and an agent could easily be a large neural network (NN) that implements the latest
    RL algorithm, but the basic pattern will stay the same – at every step, the agent
    will take some observations from the environment, do its calculations, and select
    the action to take. The result of this action will be a reward and a new observation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码的简洁性展示了强化学习（RL）模型中重要的基本概念。环境可以是一个极其复杂的物理模型，而代理可以轻松地是一个大型神经网络（NN），实现最新的 RL
    算法，但基本模式始终不变——在每一步，代理会从环境中获取一些观测，进行计算，并选择要执行的动作。这个动作的结果将是一个奖励和一个新的观测。
- en: You may ask, if the pattern is the same, why do we need to write it from scratch?
    What if it is already implemented by somebody and could be used as a library?
    Of course, such frameworks exist, but before we spend some time discussing them,
    let’s prepare your development environment.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，如果模式是一样的，为什么我们需要从头开始编写它？如果已经有人实现了它并且可以作为库使用呢？当然，确实存在这样的框架，但在我们花时间讨论它们之前，让我们先准备好你的开发环境。
- en: Hardware and software requirements
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬件和软件要求
- en: The examples in this book were implemented and tested using Python version 3.11\.
    I assume that you’re already familiar with the language and common concepts such
    as virtual environments, so I won’t cover in detail how to install packages and
    how to do this in an isolated way. The examples will use the previously mentioned
    Python type annotations, which will allow us to provide type signatures for functions
    and class methods.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的示例是使用 Python 3.11 版本实现并测试的。我假设你已经熟悉该语言以及虚拟环境等常见概念，因此我不会详细介绍如何安装软件包以及如何以隔离的方式进行操作。示例将使用之前提到的
    Python 类型注解，这将使我们能够为函数和类方法提供类型签名。
- en: Nowadays, there are lots of ML and RL libraries available, but in this book,
    I tried to keep the list of dependencies to a minimum, giving a preference to
    our own implementation of methods over the blind import of third-party libraries.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，市面上有很多机器学习（ML）和强化学习（RL）库，但在本书中，我尽量将依赖项的数量保持在最低限度，优先考虑我们自己实现的方法，而不是盲目导入第三方库。
- en: 'The external libraries that we will use in this book are open source software,
    and they include the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中使用的外部库都是开源软件，包括以下内容：
- en: 'NumPy: This is a library for scientific computing and implementing matrix operations
    and common functions.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy：这是一个用于科学计算和实现矩阵运算及常用函数的库。
- en: 'OpenCV Python bindings: This is a computer vision library and provides many
    functions for image processing.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenCV Python 绑定：这是一个计算机视觉库，提供了许多图像处理功能。
- en: 'Gymnasium from the Farama Foundation ([https://farama.org](https://farama.org)):
    This is a maintained fork of the OpenAI Gym library ( [https://github.com/openai/gym](https://github.com/openai/gym))
    and an RL framework that has various environments that can be communicated with
    in a unified way.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 Farama Foundation 的 Gymnasium（[https://farama.org](https://farama.org)）：这是
    OpenAI Gym 库（[https://github.com/openai/gym](https://github.com/openai/gym)）的一个维护版本，它是一个
    RL 框架，拥有可以以统一方式进行通信的各种环境。
- en: 'PyTorch: This is a flexible and expressive deep learning (DL) library. A short
    crash course on it will be given in Chapter [3](ch007.xhtml#x1-530003).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch：这是一个灵活且富有表现力的深度学习（DL）库。第 [3](ch007.xhtml#x1-530003) 章将简要介绍它。
- en: 'PyTorch Ignite: This is a set of high-level tools on top of PyTorch used to
    reduce boilerplate code. It will be covered briefly in Chapter [3](ch007.xhtml#x1-530003).
    The full documentation is available here: [https://pytorch-ignite.ai/](https://pytorch-ignite.ai/).'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PyTorch Ignite: 这是一个基于 PyTorch 的高层次工具集，用于减少样板代码。在第[3](ch007.xhtml#x1-530003)章中将简要介绍。完整文档可在此处查看：[https://pytorch-ignite.ai/](https://pytorch-ignite.ai/)。'
- en: 'PTAN: ([https://github.com/Shmuma/ptan](https://github.com/Shmuma/ptan)): This
    is an open source extension to the OpenAI Gym API that I created to support modern
    deep RL methods and building blocks. All classes used will be described in detail
    together with the source code.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PTAN: ([https://github.com/Shmuma/ptan](https://github.com/Shmuma/ptan)): 这是我创建的一个开源扩展，用于支持现代深度强化学习方法和构建模块。所有使用的类将详细描述，并附带源代码。'
- en: Other libraries will be used for specific chapters; for example, we will use
    Microsoft TextWorld to play text-based games, PyBullet and MuJoCo for robotic
    simulations, Selenium for browser-based automation problems, and so on. Those
    specialized chapters will include installation instructions for those libraries.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其他库将用于特定章节；例如，我们将使用 Microsoft TextWorld 来玩基于文本的游戏，PyBullet 和 MuJoCo 用于机器人仿真，Selenium
    用于基于浏览器的自动化问题，等等。那些专门的章节将包括这些库的安装说明。
- en: 'A significant portion of this book (Parts 2, 3, and 4) is focused on the modern
    deep RL methods that have been developed over the past few years. The word ”deep”
    in this context means that DL is heavily used. You may be aware that DL methods
    are computationally hungry. One modern graphics processing unit (GPU) can be 10
    to 100 times faster than even the fastest multiple central processing unit (CPU)
    systems. In practice, this means that the same code that takes one hour to train
    on a system with a GPU could take from half a day to one week even on the fastest
    CPU system. It doesn’t mean that you can’t try the examples from this book without
    having access to a GPU, but it will take longer. To experiment with the code on
    your own (the most useful way to learn anything), it is better to get access to
    a machine with a GPU. This can be done in various ways:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的很大一部分内容（第 2、3 和 4 部分）专注于过去几年中开发的现代深度强化学习（RL）方法。在这个上下文中，“深度”一词意味着深度学习（DL）的广泛应用。你可能已经知道，深度学习方法对计算资源的需求很高。一块现代图形处理单元（GPU）可以比即使是最快的多核中央处理单元（CPU）系统快
    10 到 100 倍。实际上，这意味着在一个 GPU 系统上训练一小时的代码，即使是在最快的 CPU 系统上也可能需要半天到一周的时间。这并不意味着没有 GPU
    你就不能尝试本书中的示例，但时间会更长。为了自己进行代码实验（学习任何东西最有用的方式），最好使用有 GPU 的机器。你可以通过以下几种方式来实现：
- en: Buying a modern GPU suitable for CUDA and supported by the PyTorch framework.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 购买适合 CUDA 并支持 PyTorch 框架的现代 GPU。
- en: Using cloud instances. Both Amazon Web Services and Google Cloud Platform can
    provide you with GPU-powered instances.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用云实例。Amazon Web Services 和 Google Cloud Platform 都可以提供 GPU 驱动的实例。
- en: Google Colab offers free GPU access to its Jupyter notebooks.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Colab 提供免费的 GPU 访问权限，适用于其 Jupyter 笔记本。
- en: The instructions on how to set up the system are beyond the scope of this book,
    but there are plenty of manuals available on the Internet. In terms of an operating
    system (OS) , you should use Linux or macOS. Windows is supported by PyTorch and
    Gymnasium, but the examples in the book were not fully tested on the Windows OS.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 系统设置的说明超出了本书的范围，但互联网上有很多手册可以参考。在操作系统（OS）方面，你应该使用 Linux 或 macOS。Windows 被 PyTorch
    和 Gymnasium 支持，但本书中的示例未在 Windows 操作系统上经过充分测试。
- en: 'To give you the exact versions of the external dependencies that we will use
    throughout the book, here is a requirements.txt file (please note that it was
    tested on Python 3.11; different versions might require you to tweak the dependencies
    or not work at all):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给你提供本书中将使用的外部依赖项的准确版本，以下是一个 requirements.txt 文件（请注意，它是用 Python 3.11 测试过的；不同版本可能需要调整依赖项或根本无法工作）：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: All the examples in the book were written and tested with PyTorch 2.5.0, which
    can be installed by following the instructions on the [https://pytorch.org](https://pytorch.org)
    website (normally, that’s just the conda install pytorch torchvision -c pytorch
    or even just pip install torch command, depending on your OS).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的所有示例都是用 PyTorch 2.5.0 编写和测试的，可以通过访问[https://pytorch.org](https://pytorch.org)
    网站上的说明进行安装（通常，只需使用 conda install pytorch torchvision -c pytorch 命令，或者根据你的操作系统，直接使用
    pip install torch 命令）。
- en: Now, let’s go into the details of the OpenAI Gym API, which provides us with
    tons of environments, from trivial to challenging ones.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解OpenAI Gym API，它为我们提供了从简单到挑战性强的各种环境。
- en: The OpenAI Gym API and Gymnasium
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI Gym API与Gymnasium
- en: The Python library called Gym was developed by OpenAI ([www.openai.com](https://www.openai.com)).
    The first version was released in 2017 and since then, lots of environments were
    developed or adopted to this original API, which became a de facto standard for
    RL.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由OpenAI开发的Python库Gym ([www.openai.com](https://www.openai.com))。第一个版本发布于2017年，从那时起，许多环境都已被开发或适配到这个原始API，后者也成为了强化学习（RL）的事实标准。
- en: In 2021, the team that developed OpenAI Gym moved the development to Gymnasium
    ([github.com/Farama-Foundation/Gymnasium](https://github.com/Farama-Foundation/Gymnasium))
    – the fork of the original Gym library. Gymnasium provides the same API and is
    supposed to be a “drop-in replacement” for Gym (you can write import gymnasium
    as gym and most likely your code will work).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在2021年，开发OpenAI Gym的团队将开发工作转移到了Gymnasium ([github.com/Farama-Foundation/Gymnasium](https://github.com/Farama-Foundation/Gymnasium))——原始Gym库的一个分支。Gymnasium提供相同的API，并被认为是Gym的“直接替代品”（你可以写`import
    gymnasium as gym`，大部分情况下你的代码将正常运行）。
- en: Examples in this book are using Gymnasium, but in the text, I’ll use ”Gym” for
    brevity. In rare cases when the difference does matter, I’ll use ”Gymnasium.”
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的示例使用的是Gymnasium，但为了简洁起见，文中会使用“Gym”。在极少数情况下，当差异确实重要时，我会使用“Gymnasium”。
- en: 'The main goal of Gym is to provide a rich collection of environments for RL
    experiments using a unified interface. So, it is not surprising that the central
    class in the library is an environment, which is called Env. Instances of this
    class expose several methods and fields that provide the required information
    about its capabilities. At a high level, every environment provides these pieces
    of information and functionality:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Gym的主要目标是通过统一的接口为RL实验提供丰富的环境集合。因此，库中的核心类是环境类，称为Env。该类的实例暴露了几个方法和字段，提供关于其功能的必要信息。从高层次来看，每个环境都提供这些信息和功能：
- en: A set of actions that is allowed to be executed in the environment. Gym supports
    both discrete and continuous actions, as well as their combination.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许在环境中执行的动作集合。Gym支持离散和连续动作，以及它们的组合。
- en: The shape and boundaries of the observations that the environment provides the
    agent with.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境向代理提供的观察的形状和边界。
- en: A method called step to execute an action, which returns the current observation,
    the reward, and a flag indicating that the episode is over.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个名为step的方法用于执行一个动作，该方法返回当前的观察、奖励以及指示该回合是否结束的标志。
- en: A method called reset, which returns the environment to its initial state and
    obtains the first observation.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个名为reset的方法，它将环境恢复到初始状态并获取第一个观察。
- en: Let’s now talk about these components of the environment in detail.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来详细讨论一下环境的这些组件。
- en: The action space
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作空间
- en: As mentioned, the actions that an agent can execute can be discrete, continuous,
    or a combination of the two.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，代理可以执行的动作可以是离散的、连续的，或者是两者的组合。
- en: Discrete actions are a fixed set of things that an agent can do, for example,
    directions in a grid like left, right, up, or down. Another example is a push
    button, which could be either pressed or released. Both states are mutually exclusive
    and this is the main characteristic of a discrete action space, where only one
    action from a finite set of actions is possible at a time.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 离散动作是一组固定的、代理可以执行的动作，例如，在一个网格中的方向：左、右、上或下。另一个例子是按钮，按钮可以是按下或释放。两个状态是互斥的，这是离散动作空间的主要特征，在该空间中，每次只能从有限的动作集合中选择一个动作。
- en: A continuous action has a value attached to it, for example, a steering wheel,
    which can be turned at a specific angle, or an accelerator pedal, which can be
    pressed with different levels of force. A description of a continuous action includes
    the boundaries of the value that the action could have. In the case of a steering
    wheel, it could be from −720 degrees to 720 degrees. For an accelerator pedal,
    it’s usually from 0 to 1.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 连续动作附带一个值，例如，方向盘可以转动到特定角度，或油门踏板可以以不同的力量踩下。连续动作的描述包括该动作可能具有的值的边界。对于方向盘来说，可能的值范围是-720度到720度。对于油门踏板，通常范围是从0到1。
- en: Of course, we are not limited to a single action; the environment could take
    multiple actions, such as pushing multiple buttons simultaneously or steering
    the wheel and pressing two pedals (the brake and the accelerator). To support
    such cases, Gym defines a special container class that allows the nesting of several
    action spaces into one unified action.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们不仅仅局限于单个动作；环境可以执行多个动作，例如同时按下多个按钮或同时转动方向盘和踩两个踏板（刹车和油门）。为了支持这种情况，Gym定义了一个特殊的容器类，允许将多个动作空间嵌套成一个统一的动作。
- en: The observation space
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 观测空间
- en: As discussed in Chapter [1](ch005.xhtml#x1-190001), observations are pieces
    of information that an environment provides the agent with, on every timestamp,
    besides the reward. Observations can be as simple as a bunch of numbers or as
    complex as several multidimensional tensors containing color images from several
    cameras. An observation can even be discrete, much like action spaces. An example
    of a discrete observation space is a lightbulb, which could be in two states –
    on or off – given to us as a Boolean value.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[1](ch005.xhtml#x1-190001)章所讨论，观测是环境在每个时间戳提供给智能体的信息，除了奖励之外。观测可以像一堆数字一样简单，或者像几个多维张量一样复杂，这些张量包含来自多个相机的彩色图像。观测甚至可以是离散的，类似于动作空间。离散观测空间的一个例子是灯泡，它可以处于两种状态——开或关——并以布尔值的形式给我们提供。
- en: 'So, you can see the similarity between actions and observations, and that is
    how they have been represented in Gym’s classes. Let’s look at a class diagram:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您可以看到动作和观测之间的相似性，这就是它们在Gym类中表示的方式。让我们来看一个类图：
- en: '![tsuhpalpee::Dl TsaioSuSTmsnBwpppupc:o:alaplrxcecleeinfleeettos[S[(eapin)tatc,e
    .,..]...] cohnigtha:inflso(axt) seed () ](img/B22150_02_01.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![tsuhpalpee::Dl TsaioSuSTmsnBwpppupc:o:alaplrxcecleeinfleeettos[S[(eapin)tatc,e
    .,..]...] cohnigtha:inflso(axt) seed () ](img/B22150_02_01.png)'
- en: 'Figure 2.1: The hierarchy of the Space class in Gym'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：Gym中Space类的层级结构
- en: 'The basic abstract Space class includes one property and three methods that
    are relevant to us:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的抽象Space类包括一个属性和三个对我们有用的方法：
- en: 'shape: This property contain the shape of the space, identical to NumPy arrays.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: shape：此属性包含空间的形状，与NumPy数组相同。
- en: 'sample(): This returns a random sample from the space.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: sample()：此方法返回空间中的一个随机样本。
- en: 'contains(x): This checks whether the argument, x, belongs to the space’s domain.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: contains(x)：此方法检查参数x是否属于该空间的领域。
- en: 'seed(): This method allows us to initialize a random number generator for the
    space and all subspaces. This is useful if you want to get reproducible environment
    behavior across several runs.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: seed()：此方法允许我们为空间及其所有子空间初始化一个随机数生成器。如果您希望在多个运行中获得可重复的环境行为，这非常有用。
- en: 'All these methods are abstract and reimplemented in each of the Space subclasses:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些方法都是抽象方法，并在每个Space子类中重新实现：
- en: The Discrete class represents a mutually exclusive set of items, numbered from
    0 to n-1\. If needed, you can redefine the starting index with the optional constructor
    argument start. The value n is a count of the items our Discrete object describes.
    For example, Discrete(n=4) can be used for an action space of four directions
    to move in [left, right, up, or down].
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Discrete类表示一个互斥的项目集合，编号从0到n-1。如果需要，您可以通过可选的构造函数参数start重新定义起始索引。值n是我们Discrete对象描述的项目数量。例如，Discrete(n=4)可以用于四个方向的动作空间[左、右、上、下]。
- en: 'The Box class represents an n-dimensional tensor of rational numbers with intervals
    [low, high]. For instance, this could be an accelerator pedal with one single
    value between 0.0 and 1.0, which could be encoded by Box(low=0.0, high=1.0, shape=(1,),
    dtype=np.float32). Here, the shape argument is assigned a tuple of length 1 with
    a single value of 1, which gives us a one-dimensional tensor with a single value.
    The dtype parameter specifies the space’s value type, and here, we specify it
    as a NumPy 32-bit float. Another example of Box could be an Atari screen observation
    (we will cover lots of Atari environments later), which is an RGB (red, green,
    and blue) image of size 210 × 160: Box(low=0, high=255, shape=(210, 160, 3), dtype=np.uint8).
    In this case, the shape argument is a tuple of three elements: the first dimension
    is the height of the image, the second is the width, and the third equals 3, which
    all correspond to three color planes for red, green, and blue, respectively. So,
    in total, every observation is a three-dimensional tensor with 100,800 bytes.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Box类表示一个具有区间[low, high]的有理数n维张量。例如，这可以是一个油门踏板，其值介于0.0和1.0之间，可以通过Box(low=0.0,
    high=1.0, shape=(1,), dtype=np.float32)来编码。在这里，shape参数被赋值为长度为1的元组，元组中只有一个值1，这样就给我们一个一维的张量，其中包含一个值。dtype参数指定空间的值类型，在这里，我们指定它为NumPy
    32位浮动类型。另一个Box的例子可能是Atari屏幕的观察（稍后我们会涉及许多Atari环境），它是一个大小为210 × 160的RGB（红色、绿色和蓝色）图像：Box(low=0,
    high=255, shape=(210, 160, 3), dtype=np.uint8)。在这种情况下，shape参数是一个包含三个元素的元组：第一个维度是图像的高度，第二个是宽度，第三个是3，分别对应于红色、绿色和蓝色的三个色彩通道。因此，总体来说，每个观察是一个具有100,800字节的三维张量。
- en: 'The final child of Space is a Tuple class, which allows us to combine several
    Space class instances together. This enables us to create action and observation
    spaces of any complexity that we want. For example, imagine we want to create
    an action space specification for a car. The car has several controls that can
    be changed at every timestamp, including the steering wheel angle, brake pedal
    position, and accelerator pedal position. These three controls can be specified
    by three float values in one single Box instance. Besides these essential controls,
    the car has extra discrete controls, like a turn signal (which could be off, right,
    or left) or horn (on or off). To combine all of this into one action space specification
    class, we can use the following code:'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空间的最终子类是Tuple类，它允许我们将多个Space类实例组合在一起。这使我们能够创建我们想要的任何复杂度的动作和观察空间。例如，假设我们想为一辆汽车创建一个动作空间的规范。汽车有多个控制项，每个控制项都可以在每个时间戳进行改变，包括方向盘角度、刹车踏板位置和油门踏板位置。这三个控制项可以通过一个单独的Box实例中的三个浮动值来指定。除了这些基本的控制项外，汽车还有额外的离散控制项，如转向信号（可以是关闭、右转或左转）或喇叭（开或关）。为了将这一切组合成一个动作空间规范类，我们可以使用以下代码：
- en: '[PRE10]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This flexibility is rarely used; for example, in this book, you will see only
    the Box and Discrete actions and observation spaces, but the Tuple class can be
    handy in some cases.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种灵活性很少被使用；例如，在本书中，你只会看到Box和离散的动作和观察空间，但在某些情况下，Tuple类会很有用。
- en: There are other Space subclasses defined in Gym, for example, Sequence (representing
    variable-length sequences), Text (strings), and Graph (where space is a set of
    nodes with connections between them). But the three that we have described are
    the most useful ones.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在Gym中还定义了其他的Space子类，例如Sequence（表示可变长度序列）、Text（字符串）和Graph（空间是一个节点集合，节点之间有连接）。但我们所描述的这三个子类是最常用的。
- en: 'Every environment has two members of type Space: the action_space and observation_space.
    This allows us to create generic code that could work with any environment. Of
    course, dealing with the pixels of the screen is different from handling discrete
    observations (as in the former case, we may want to preprocess images with convolutional
    layers or with other methods from the computer vision toolbox); so, most of the
    time, this means optimizing the code for a particular environment or group of
    environments, but Gym doesn’t prevent us from writing generic code.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 每个环境都有两个类型为Space的成员：action_space和observation_space。这使我们能够创建通用代码，可以与任何环境一起使用。当然，处理屏幕的像素与处理离散观察不同（因为在前一种情况下，我们可能希望通过卷积层或计算机视觉工具箱中的其他方法来预处理图像）；因此，大多数时候，这意味着要为特定环境或环境组优化代码，但Gym并不禁止我们编写通用代码。
- en: The environment
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境
- en: 'The environment is represented in Gym by the Env class, which has the following
    members:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 环境在Gym中由Env类表示，该类具有以下成员：
- en: 'action_space: This is the field of the Space class and provides a specification
    for allowed actions in the environment.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: action_space：这是 Space 类的字段，提供有关环境中允许执行的动作的规范。
- en: 'observation_space: This field has the same Space class, but specifies the observations
    provided by the environment.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: observation_space：这个字段属于相同的 Space 类，但指定了环境提供的观察。
- en: 'reset(): This resets the environment to its initial state, returning the initial
    observation vector and the dict with extra information from the environment.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: reset()：此方法将环境重置为初始状态，返回初始观察向量以及来自环境的额外信息字典。
- en: 'step(): This method allows the agent to take the action and returns information
    about the outcome of the action:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: step()：这个方法允许智能体采取行动并返回有关行动结果的信息：
- en: The next observation
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个观察
- en: The local reward
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地奖励
- en: The end-of-episode flag
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回合结束标志
- en: The flag indicating a truncated episode
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标志，指示回合是否被截断
- en: A dictionary with extra information from the environment
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含环境额外信息的字典
- en: This method is a bit complicated; we will look at it in detail later in this
    section.
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个方法有点复杂，我们稍后会在本节中详细讨论。
- en: 'There are extra utility methods in the Env class, such as render(), which allows
    us to obtain the observation in a human-friendly form, but we won’t use them.
    You can find the full list in Gym’s documentation, but let’s focus on the core
    Env methods: reset() and step().'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Env 类中有额外的实用方法，比如 render()，它允许我们以人类友好的形式获取观察数据，但我们不会使用它们。你可以在 Gym 的文档中找到完整列表，但我们将专注于核心的
    Env 方法：reset() 和 step()。
- en: As reset is much simpler, we will start with it. The reset() method has no arguments;
    it instructs an environment to reset into its initial state and obtain the initial
    observation. Note that you have to call reset() after the creation of the environment.
    As you may remember from Chapter [1](ch005.xhtml#x1-190001), the agent’s communication
    with the environment may have an end (like a “Game Over” screen). Such sessions
    are called episodes, and after the end of the episode, an agent needs to start
    over. The value returned by this method is the first observation of the environment.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 reset 方法相对简单，我们将从它开始。reset() 方法没有参数；它指示环境重置为初始状态并获取初始观察。请注意，在创建环境后，你必须调用
    reset()。正如你在第[1](ch005.xhtml#x1-190001)章中记得的那样，智能体与环境的交互可能会有结束（比如“游戏结束”屏幕）。这种会话称为回合，在回合结束后，智能体需要重新开始。此方法返回的值是环境的第一次观察。
- en: Besides the observation, reset() returns the second value – the dictionary with
    extra environment-specific information. Most standard environments return nothing
    in this dictionary, but more complicated ones (like TextWorld, an emulator for
    interactive-fiction games; we’ll take a look at it later in the book) might return
    additional information that doesn’t fit into standard observation.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 除了观察外，reset() 返回第二个值——包含额外环境特定信息的字典。大多数标准环境在此字典中不返回任何内容，但更复杂的环境（如 TextWorld——一个交互式小说游戏的模拟器；我们将在本书后面了解它）可能会返回一些不适合标准观察的数据。
- en: 'The step() method is the central piece in the environment’s functionality.
    It does several things in one call, which are as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: step() 方法是环境功能的核心部分。它在一次调用中执行多个操作，具体如下：
- en: Telling the environment which action we will execute in the next step
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 告诉环境我们将在下一步执行的动作
- en: Getting the new observation from the environment after this action
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取这个行动后从环境中得到的新观察
- en: Getting the reward the agent gained with this step
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取智能体通过此步获得的奖励
- en: Getting the indication that the episode is over
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取回合是否结束的指示
- en: Getting the flag which signals an episode truncation (when time limit is enabled,
    for example)
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取信号，指示一个回合是否已被截断（例如启用时间限制时）
- en: Getting the dict with extra environment-specific information
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取包含额外环境特定信息的字典
- en: 'The first item in the preceding list (action) is passed as the only argument
    to the step() method, and the rest are returned by this method. More precisely,
    this is a tuple (Python tuple and not the Tuple class we discussed in the previous
    section) of five elements (observation, reward, done, truncated, and info). They
    have these types and meanings:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 前述列表中的第一个项目（action）作为唯一参数传递给 step() 方法，其余内容由此方法返回。更准确地说，这是一个包含五个元素（observation,
    reward, done, truncated 和 info）的元组（Python 元组，而不是我们在上一节讨论的 Tuple 类）。它们具有以下类型和含义：
- en: 'observation: This is a NumPy vector or a matrix with observation data.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: observation：这是一个包含观察数据的 NumPy 向量或矩阵。
- en: 'reward: This is the float value of the reward.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: reward：这是奖励的浮动值。
- en: 'done: This is a Boolean indicator, which is True when the episode is over.
    If this value is True, we have to call reset() in the environment, as no more
    actions are possible.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'done: 这是一个布尔指示符，当回合结束时值为True。如果这个值为True，我们必须在环境中调用reset()，因为不再可能进行任何动作。'
- en: 'truncated: This is a Boolean indicator, which is True when the episode is truncated.
    For most environments, this is a TimeLimit (which is a way to limit length of
    episodes), but might have different meaning in some environments. This flag is
    separated from done, because in some scenarios it might be useful to distinguish
    situations ”agent reached the end of episode” and ”agent has reached the time
    limit of the environment.” If truncated is True, we also have to call reset()
    in the environment, the same as with the done flag.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'truncated: 这是一个布尔指示符，当回合被截断时值为True。对于大多数环境，这通常是一个TimeLimit（限制回合时长的方式），但在某些环境中它可能有不同的含义。这个标志与done标志分开，因为在某些场景下，区分“代理到达回合结束”与“代理到达环境时间限制”可能会很有用。如果truncated为True，我们还需要在环境中调用reset()，就像处理done标志一样。'
- en: 'info: This could be anything environment-specific with extra information about
    the environment. The usual practice is to ignore this value in general RL methods.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'info: 这可能是与环境特定的额外信息，通常做法是在一般强化学习方法中忽略此值。'
- en: You may have already got the idea of environment usage in an agent’s code –
    in a loop, we call the step() method with an action to perform until the done
    or truncated flags become True. Then, we can call reset() to start over. There
    is only one piece missing – how we create Env objects in the first place.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经对环境在代理代码中的使用方式有了一些了解——在循环中，我们调用step()方法并执行一个动作，直到done或truncated标志变为True。然后，我们可以调用reset()重新开始。还有一个部分缺失——我们如何首先创建Env对象。
- en: Creating an environment
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建环境
- en: Every environment has an unique name of the EnvironmentName-vN form, where N
    is the number used to distinguish between different versions of the same environment
    (when, for example, some bugs get fixed or some other major changes are made).
    To create an environment, the gymnasium package provides the make(name) function,
    whose only argument is the environment’s name in string form.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 每个环境都有一个唯一的名称，格式为EnvironmentName-vN，其中N是区分同一环境不同版本的数字（例如，当修复了某些错误或做了其他重大更改时）。为了创建一个环境，gymnasium包提供了make(name)函数，其唯一参数是环境的名称字符串。
- en: 'At the time of writing, Gymnasium version 0.29.1 (being installed with the
    [atari] extension) contains 1,003 environments with different names. Of course,
    all of these are not unique environments, as this list includes all versions of
    an environment. Additionally, the same environment can have different variations
    in the settings and observations spaces. For example, the Atari game Breakout
    has these environment names:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Gymnasium版本0.29.1（安装了[atari]扩展）包含了1,003个不同名称的环境。当然，并非所有这些环境都是独立的，因为这个列表包括了环境的所有版本。此外，相同的环境也可能在设置和观察空间中有所不同。例如，Atari游戏Breakout有以下这些环境名称：
- en: 'Breakout-v0, Breakout-v4: The original Breakout with a random initial position
    and direction of the ball.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Breakout-v0, Breakout-v4: 原版Breakout，球的位置和方向是随机的。'
- en: 'BreakoutDeterministic-v0, BreakoutDeterministic-v4: Breakout with the same
    initial placement and speed vector of the ball.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'BreakoutDeterministic-v0, BreakoutDeterministic-v4: 初始位置和球速向量相同的Breakout。'
- en: 'BreakoutNoFrameskip-v0, BreakoutNoFrameskip-v4: Breakout with every frame displayed
    to the agent. Without this, every action is executed for several consecutive frames.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'BreakoutNoFrameskip-v0, BreakoutNoFrameskip-v4: 每帧都展示给代理的Breakout环境。没有这个设置时，每个动作会执行多个连续帧。'
- en: 'Breakout-ram-v0, Breakout-ram-v4: Breakout with the observation of the full
    Atari emulation memory (128 bytes) instead of screen pixels.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Breakout-ram-v0, Breakout-ram-v4: 使用完整Atari模拟内存（128字节）而非屏幕像素的Breakout。'
- en: 'Breakout-ramDeterministic-v0, Breakout-ramDeterministic-v4: Memory observation
    with the same initial state.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Breakout-ramDeterministic-v0, Breakout-ramDeterministic-v4: 使用相同初始状态的内存观察。'
- en: 'Breakout-ramNoFrameskip-v0, Breakout-ramNoFrameskip-v4: Memory observation
    without frame skipping.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Breakout-ramNoFrameskip-v0, Breakout-ramNoFrameskip-v4: 无跳帧的内存观察。'
- en: 'In total, there are 12 environments for a single game. In case you’ve never
    seen it before, here is a screenshot of its gameplay:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 总共为一个游戏有12个环境。如果你之前没见过，这是它的游戏截图：
- en: '![PIC](img/file6.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file6.png)'
- en: 'Figure 2.2: The gameplay of Breakout'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：Breakout的游戏画面
- en: 'Even after the removal of such duplicates, Gymnasium comes with an impressive
    list of 198 unique environments, which can be divided into several groups:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 即便去除这些重复项，Gymnasium 依然提供了一个令人印象深刻的 198 个独特环境的列表，这些环境可以分为几个组：
- en: 'Classic control problems: These are toy tasks that are used in optimal control
    theory and RL papers as benchmarks or demonstrations. They are usually simple,
    with low-dimension observation and action spaces, but they are useful as quick
    checks when implementing algorithms. Think about them as the ”MNIST for RL” (MNIST
    is a handwriting digit recognition dataset from Yann LeCun, which you can find
    at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)).'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经典控制问题：这些是玩具任务，用于最优控制理论和强化学习论文中的基准测试或演示。它们通常简单，观察和动作空间的维度较低，但在实现算法时，它们作为快速检查是非常有用的。可以把它们看作是强化学习领域的“MNIST”（MNIST
    是 Yann LeCun 提供的手写数字识别数据集，网址是 [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)）。
- en: 'Atari 2600: These are games from the classic game platform from the 1970s.
    There are 63 unique games.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Atari 2600：这些是来自 1970 年代经典游戏平台的游戏，共有 63 款独特游戏。
- en: 'Algorithmic: These are problems that aim to perform small computation tasks,
    such as copying the observed sequence or adding numbers.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法问题：这些是旨在执行小型计算任务的问题，如复制观察到的序列或加法运算。
- en: 'Box2D: These are environments that use the Box2D physics simulator to learn
    walking or car control.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Box2D：这些是使用 Box2D 物理仿真器来学习行走或汽车控制的环境。
- en: 'MuJoCo: This is another physics simulator used for several continuous control
    problems.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MuJoCo：这是另一种物理仿真器，用于解决多个连续控制问题。
- en: 'Parameter tuning: This is RL being used to optimize NN parameters.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数调整：这是利用强化学习来优化神经网络参数。
- en: 'Toy text: These are simple grid world text environments.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩具文本：这些是简单的网格世界文本环境。
- en: Of course, the total number of RL environments supporting the Gym API is much
    larger. For example, The Farama Foundation maintains several repositories related
    to special RL topics like multi-agent RL, 3D navigation, robotics, and web automation.
    In addition, there are lots of third-party repositories. To get the idea, you
    can check out [https://gymnasium.farama.org/environments/third_party_environments](https://gymnasium.farama.org/environments/third_party_environments)
    in the Gymnasium documentation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，支持 Gym API 的强化学习环境的总数要大得多。例如，Farama 基金会维护了多个与特殊强化学习主题相关的代码库，如多智能体强化学习、3D
    导航、机器人技术和网页自动化。此外，还有许多第三方代码库。你可以查看 [https://gymnasium.farama.org/environments/third_party_environments](https://gymnasium.farama.org/environments/third_party_environments)
    了解相关信息。
- en: But enough theory! Let’s now look at a Python session working with one of Gym’s
    environments.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 够了！让我们来看看一个 Python 会话，演示如何使用 Gym 的环境。
- en: The CartPole session
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CartPole 会话
- en: Let’s apply our knowledge and explore one of the simplest RL environments that
    Gym provides.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用我们的知识，探索 Gym 提供的最简单的强化学习（RL）环境之一。
- en: '[PRE11]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, we have imported the gymnasium package and created an environment called
    CartPole. This environment is from the classic control group and its gist is to
    control the platform with a stick attached to its bottom part (see the following
    figure).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们导入了 gymnasium 包并创建了一个名为 CartPole 的环境。这个环境来自经典控制组，核心思想是控制底部附有杆子的平衡平台（见下图）。
- en: The trickiness is that this stick tends to fall right or left and you need to
    balance it by moving the platform to the right or left at every step.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的难点在于，这根杆子容易向左或向右倒，你需要通过每一步将平台移动到右侧或左侧来保持平衡。
- en: '![PIC](img/B22150_02_03.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_02_03.png)'
- en: 'Figure 2.3: The CartPole environment'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：CartPole 环境
- en: The observation of this environment is four floating-point numbers containing
    information about the x coordinate of the stick’s center of mass, its speed, its
    angle to the platform, and its angular speed. Of course, by applying some math
    and physics knowledge, it won’t be complicated to convert these numbers into actions
    when we need to balance the stick, but our problem is different – how do we learn
    how to balance this system without knowing the exact meaning of the observed numbers
    and only by getting the reward? The reward in this environment is 1, and it is
    given on every time step. The episode continues until the stick falls, so to get
    a more accumulated reward, we need to balance the platform in a way to avoid the
    stick falling.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这个环境的观察结果是包含有关杆质心 x 坐标、速度、与平台的角度以及角速度的四个浮点数。当然，通过一些数学和物理知识，将这些数字转换为动作来平衡杆并不复杂，但我们的问题是不同的——在不知道观察到的数字确切含义的情况下，只通过获取奖励来学习如何平衡这个系统。这个环境中的奖励为
    1，在每个时间步上都会给出。本集结束直到杆子倒下，因此为了获得更多的累积奖励，我们需要以一种避免杆子倒下的方式平衡平台。
- en: This problem may look difficult, but in just two chapters, we will write the
    algorithm that will easily solve CartPole in minutes, without any idea about what
    the observed numbers mean. We will do it only by trial and error and using a bit
    of RL magic.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题看起来可能很难，但在仅仅两章之内，我们将编写一个算法，能够在几分钟内轻松解决 CartPole，而不需要理解观察到的数字意味着什么。我们将只通过试错和一点强化学习的魔法来完成。
- en: But now, let’s continue with our session.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在，让我们继续我们的会话。
- en: '[PRE12]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here, we reset the environment and obtained the first observation (we always
    need to reset the newly created environment). As I said, the observation is four
    numbers, so no surprises here. Let’s now examine the action and observation space
    of the environment:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们重置了环境并获得了第一个观察结果（我们始终需要重置新创建的环境）。正如我所说，观察结果是四个数字，所以这里没有什么意外。现在让我们来检查一下环境的动作和观察空间：
- en: '[PRE13]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The action_space field is of the Discrete type, so our actions will be just
    0 or 1, where 0 means pushing the platform to the left and 1 is pushing to the
    right. The observation space is of Box(4,), which means a vector of four numbers.
    The first list shown in the observation_space field is the low bound and the second
    is the high bound of parameters.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: action_space 字段是离散类型，所以我们的动作只能是 0 或 1，其中 0 表示向左推动平台，1 表示向右推动。观察空间是 Box(4,)，意味着一个四个数字的向量。在
    observation_space 字段中显示的第一个列表是参数的低边界，第二个列表是高边界。
- en: 'If you’re curious, you can peek at the source code of the environment in the
    Gymnasium repository in the cartpole.py file at [https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py#L40](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py#L40).
    Documentation strings of the CartPole class provide all the details, including
    semantics of observation:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你好奇的话，你可以查看 Gymnasium 仓库中 cartpole.py 文件中的环境源代码，位于 [https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py#L40](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py#L40)。CartPole
    类的文档字符串提供了所有细节，包括观察的语义：
- en: 'Cart position: Value in −4.8…4.8 range'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小车位置：值在 −4.8…4.8 范围内
- en: 'Cart velocity: Value in −∞…∞ range'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小车速度：值在 −∞…∞ 范围内
- en: 'Pole angle: Value in radians in −0.418…0.418 range'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杆角度：弧度值在 −0.418…0.418 范围内
- en: 'Pole angular velocity: Value in −∞…∞ range'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杆角速度：值在 −∞…∞ 范围内
- en: 'Python uses float32 maximum and minimum values to represent infinity, which
    is why some entries in boundary vectors have values of scale 10^(38). All those
    internal details are interesting to know, but absolutely not needed to solve the
    environment using RL methods. Let’s go further and send an action to the environment:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Python 使用 float32 的最大和最小值来表示无穷大，这就是为什么边界向量中的某些条目具有 10^(38) 规模值的内部细节。这些内部细节很有趣，但绝对不需要使用
    RL 方法来解决环境问题。让我们进一步发送一个动作到环境中：
- en: '[PRE14]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here, we pushed our platform to the left by executing the action 0 and got
    the tuple of five elements:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过执行动作 0 将平台向左推动，并得到了一个五个元素的元组：
- en: A new observation, which is a new vector of four numbers
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个新的观察结果，即一个新的四个数字的向量
- en: A reward of 1.0
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励为 1.0
- en: The done flag with value False, which means that the episode is not over yet
    and we are more or less okay with balancing the pole
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: done 标志值为 False，表示本集尚未结束，我们对平衡杆的掌握还算可以。
- en: The truncated flag with value False, meaning that the episode was not truncated
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 截断标志值为 False，表示本集未被截断
- en: Extra information about the environment, which is an empty dictionary
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于环境的额外信息，这是一个空字典
- en: Next, we will use the sample() method of the Space class on the action_space
    and observation_space.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 Space 类的 sample() 方法，分别作用于 action_space 和 observation_space。
- en: '[PRE15]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This method returned a random sample from the underlying space, which in the
    case of our Discrete action space means a random number of 0 or 1, and for the
    observation space means a random vector of four numbers. The random sample of
    the observation space is not very useful, but the sample from the action space
    could be used when we are not sure how to perform an action. This feature is especially
    handy because you don’t know any RL methods yet, but we still want to play around
    with the Gym environment. Now that you know enough to implement your first randomly
    behaving agent for CartPole, let’s do it.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法返回了底层空间的一个随机样本，对于我们的离散动作空间来说，意味着一个随机的 0 或 1，而对于观测空间来说，意味着一个四个数字的随机向量。观测空间的随机样本并不特别有用，但来自动作空间的样本可以在我们不确定如何执行某个动作时使用。这个功能特别方便，因为你还不懂任何强化学习方法，但我们仍然想在
    Gym 环境中玩玩。既然你已经学到了足够的知识来实现你的第一个随机行为的 CartPole 智能体，那么我们开始吧。
- en: The random CartPole agent
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机 CartPole 智能体
- en: Although the environment is much more complex than our first example in section [2.1](#x1-390002.1),
    the code of the agent is much shorter. This is the power of reusability, abstractions,
    and third-party libraries!
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管环境比我们在[2.1节](#x1-390002.1)中第一个例子要复杂得多，但智能体的代码要简短得多。这就是可重用性、抽象和第三方库的强大之处！
- en: 'So, here is the code (you can find it in Chapter02/02_cartpole_random.py):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码（你可以在 Chapter02/02_cartpole_random.py 中找到它）：
- en: '[PRE16]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here, we created the environment and initialized the counter of steps and the
    reward accumulator. On the last line, we reset the environment to obtain the first
    observation (which we will not use, as our agent is stochastic):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了环境并初始化了步数计数器和奖励累加器。在最后一行，我们重置了环境以获得第一个观测值（我们不会使用它，因为我们的智能体是随机的）：
- en: '[PRE17]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In the preceding loop, after sampling a random action, we asked the environment
    to execute it and return to us the next observation (obs), the reward, the is_done,
    and the is_trunc flags. If the episode is over, we stop the loop and show how
    many steps we have taken and how much reward has been accumulated. If you start
    this example, you will see something like this (not exactly, though, due to the
    agent’s randomness):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的循环中，采样一个随机动作后，我们要求环境执行该动作并返回下一个观测值（obs）、奖励、is_done 和 is_trunc 标志。如果回合结束，我们就停止循环，并显示我们走了多少步，累计了多少奖励。如果你运行这个示例，你会看到类似这样的输出（虽然不完全相同，因为智能体是随机的）：
- en: '[PRE18]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: On average, our random agent takes 12 to 15 steps before the pole falls and
    the episode ends. Most of the environments in Gym have a ”reward boundary,” which
    is the average reward that the agent should gain during 100 consecutive episodes
    to ”solve” the environment. For CartPole, this boundary is 195, which means that,
    on average, the agent must hold the stick for 195 time steps or longer. Using
    this perspective, our random agent’s performance looks poor. However, don’t be
    disappointed; we are just at the beginning, and soon you will solve CartPole and
    many other much more interesting and challenging environments.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，我们的随机智能体在杆子倒下并且回合结束之前大约需要 12 到 15 步。Gym 中的大多数环境都有一个“奖励边界”，这是智能体在 100 个连续回合中应获得的平均奖励，以“解决”该环境。对于
    CartPole，这个边界是 195，这意味着，平均而言，智能体必须保持杆子 195 个时间步长或更长时间。用这个角度来看，我们的随机智能体表现得很差。然而，不要失望；我们才刚刚开始，很快你就能解决
    CartPole 和许多更有趣、更具挑战性的环境。
- en: Extra Gym API functionality
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 额外的 Gym API 功能
- en: What we have discussed so far covers two-thirds of the Gym core API and the
    essential functions required to start writing agents. The rest of the API you
    can live without, but it will make your life easier and the code cleaner. So,
    let’s briefly cover the rest of the API.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的内容涵盖了 Gym 核心 API 的三分之二以及开始编写智能体所需的基本功能。其余的 API 你可以不使用，但它会让你的生活更轻松，代码更简洁。所以，让我们简要地讲解一下剩下的
    API。
- en: Wrappers
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 包装器
- en: Very frequently, you will want to extend the environment’s functionality in
    some generic way. For example, imagine an environment gives you some observations,
    but you want to accumulate them in some buffer and provide to the agent the N
    last observations. This is a common scenario for dynamic computer games, when
    one single frame is just not enough to get the full information about the game
    state. Another example is when you want to be able to crop or preprocess an image’s
    pixels to make it more convenient for the agent to digest, or if you want to normalize
    reward scores somehow. There are many such situations that have the same structure
    – you want to ”wrap” the existing environment and add some extra logic for doing
    something. Gym provides a convenient framework for this – the Wrapper class.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 很多时候，你可能希望以某种通用的方式扩展环境的功能。例如，假设一个环境给你一些观察结果，但你希望将这些结果积累到某个缓冲区中，并提供给智能体最近的 N
    个观察结果。这是动态计算机游戏中的常见场景，因为单一的帧画面不足以获取游戏状态的完整信息。另一个例子是，当你希望能够裁剪或预处理图像的像素，使其更方便智能体处理，或者你希望以某种方式对奖励分数进行归一化处理。这类情况有很多，它们的结构相同——你想“包装”现有的环境，并添加一些额外的逻辑来完成某些操作。Gym
    提供了一个方便的框架——Wrapper 类。
- en: The class structure is shown in Figure [2.4](#x1-49003r4).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 类的结构如图 [2.4](#x1-49003r4) 所示。
- en: '![ObAsRceoetrbwivsaWeoaaerrrnnctrdeEavWtivWwnp:rioaravpaontareEpnWiopdrnp(rnp(veaa(err)por)pbesr)
    unwrapped: Env ](img/B22150_02_01.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![ObAsRceoetrbwivsaWeoaaerrrnnctrdeEavWtivWwnp:rioaravpaontareEpnWiopdrnp(rnp(veaa(err)por)pbesr)
    unwrapped: Env ](img/B22150_02_01.png)'
- en: 'Figure 2.4: The hierarchy of the Wrapper classes in Gym'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4：Gym 中 Wrapper 类的层次结构
- en: 'The Wrapper class inherits the Env class. Its constructor accepts the only
    argument – the instance of the Env class to be ”wrapped.” To add extra functionality,
    you need to redefine the methods you want to extend, such as step() or reset().
    The only requirement is to call the original method of the superclass. To simplify
    accessing the environment being wrapped, Wrapper has two properties: env, of the
    immediate environment we’re wrapping (which could be another wrapper as well),
    and property unwrapped, which is an Env without any wrappers.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Wrapper 类继承自 Env 类。它的构造函数接受一个参数——要“包装”的 Env 类实例。为了添加额外的功能，你需要重新定义想要扩展的方法，例如
    step() 或 reset()。唯一的要求是调用父类的原始方法。为了简化对被包装环境的访问，Wrapper 类有两个属性：env，表示我们正在包装的直接环境（它也可以是另一个
    wrapper），以及 unwrapped，表示没有任何包装器的 Env 环境。
- en: 'To handle more specific requirements, such as a Wrapper class that wants to
    process only observations from the environment, or only actions, there are subclasses
    of Wrapper that allow the filtering of only a specific portion of information.
    They are as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理更具体的需求，例如一个只想处理环境中的观察结果或仅仅处理动作的 Wrapper 类，Gym 提供了一些 Wrapper 的子类，它们允许过滤特定的信息部分。它们如下所示：
- en: 'ObservationWrapper: You need to redefine the observation(obs) method of the
    parent. The obs argument is an observation from the wrapped environment, and this
    method should return the observation that will be given to the agent.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ObservationWrapper：你需要重新定义父类的 observation(obs) 方法。obs 参数是来自被包装环境的观察结果，该方法应返回将提供给智能体的观察值。
- en: 'RewardWrapper: This exposes the reward(rew) method, which can modify the reward
    value given to the agent, for example, scale it to the needed range, add a discount
    based on some previous actions, or something like this.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RewardWrapper：这个类暴露了 reward(rew) 方法，可以修改赋予智能体的奖励值，例如，将其缩放到所需的范围，基于某些之前的动作添加折扣，或类似的操作。
- en: 'ActionWrapper: You need to override the action(a) method, which can tweak the
    action passed to the wrapped environment by the agent.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ActionWrapper：你需要重写 action(a) 方法，它可以调整智能体传递给被包装环境的动作。
- en: 'To make it slightly more practical, let’s imagine a situation where we want
    to intervene in the stream of actions sent by the agent and, with a probability
    of 10%, replace the current action with a random one. It might look like an unwise
    thing to do, but this simple trick is one of the most practical and powerful methods
    for solving the exploration/exploitation problem that we mentioned in Chapter [1](ch005.xhtml#x1-190001).
    By issuing the random actions, we make our agent explore the environment and from
    time to time drift away from the beaten track of its policy. This is an easy thing
    to do using the ActionWrapper class (a full example is in Chapter02/03_random_action_wrapper.py):'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其稍微更具实用性，让我们想象一种情况，我们希望干预智能体发送的动作流，并且以10%的概率将当前动作替换为随机动作。这可能看起来是一个不明智的做法，但这个简单的技巧是我们在第[1](ch005.xhtml#x1-190001)章提到的探索/利用问题的最实用和最强大的解决方法之一。通过发出随机动作，我们让智能体探索环境，并时不时地偏离其策略的固有轨迹。这是一个通过使用ActionWrapper类（完整示例见Chapter02/03_random_action_wrapper.py）轻松实现的事情：
- en: '[PRE19]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Here, we initialized our wrapper by calling a parent’s __init__ method and saving
    epsilon (the probability of a random action).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过调用父类的__init__方法并保存epsilon（随机动作的概率）来初始化我们的包装器。
- en: 'The following is a method that we need to override from a parent’s class to
    tweak the agent’s actions:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们需要从父类重写的方法，用于调整智能体的动作：
- en: '[PRE20]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Every time we roll the die, and with the probability of epsilon, we sample a
    random action from the action space and return it instead of the action the agent
    has sent to us. Note that using action_space and wrapper abstractions, we were
    able to write abstract code, which will work with any environment from Gym. We
    also print the message on the console, just to illustrate that our wrapper is
    working. In the production code, this won’t be necessary, of course.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我们掷骰子时，凭借epsilon的概率，我们从动作空间中采样一个随机动作并返回，而不是返回智能体发送给我们的动作。请注意，使用action_space和包装器抽象，我们能够编写抽象代码，这段代码可以与Gym中的任何环境一起工作。我们还在控制台上打印了消息，仅仅是为了说明我们的包装器正在工作。在生产代码中，当然不需要这么做。
- en: 'Now it’s time to apply our wrapper. We will create a normal CartPole environment
    and pass it to our Wrapper constructor:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候应用我们的包装器了。我们将创建一个普通的CartPole环境，并将其传递给我们的Wrapper构造函数：
- en: '[PRE21]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: From here on, we will use our wrapper as a normal Env instance, instead of the
    original CartPole. As the Wrapper class inherits the Env class and exposes the
    same interface, we can nest our wrappers as deep as we want. This is a powerful,
    elegant, and generic solution.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在起，我们将把我们的包装器当作一个普通的Env实例来使用，而不是原始的CartPole。由于Wrapper类继承了Env类并暴露了相同的接口，我们可以根据需要将包装器嵌套得很深。这是一个强大、优雅和通用的解决方案。
- en: 'Here is almost the same code as in the random agent, except that every time,
    we issue the same action, 0, so our agent is dull and does the same thing:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的代码几乎与随机智能体中的代码相同，只不过每次我们发出相同的动作0，所以我们的智能体显得呆板，一直做同样的事情：
- en: '[PRE22]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'By running the code, you should see that the wrapper is indeed working:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，你应该能看到包装器确实在工作：
- en: '[PRE23]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We should move on now and look at how you can render your environment during
    execution.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应该继续，看看在执行期间如何渲染你的环境。
- en: Rendering the environment
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 渲染环境
- en: 'Another possibility that you should be aware of is rendering the environment.
    It is implemented with two wrappers: HumanRendering and RecordVideo.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个你应该了解的可能性是渲染环境。它是通过两个包装器实现的：HumanRendering和RecordVideo。
- en: Those two classes replace the original Monitor wrapper in the OpenAI Gym library,
    which was removed. This class was able to record the information about your agent’s
    performance in a file, with an optional video recording of your agent in action.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个类替代了OpenAI Gym库中已被移除的原始Monitor包装器。这个类能够将有关智能体表现的信息记录到文件中，并可选地记录智能体动作的视频。
- en: With the Gymnasium library, you have two classes to check what’s going on inside
    the environment. The first one is HumanRendering, which opens a separate graphical
    window in which the image from the environment is being shown interactively. To
    be able to render the environment (CartPole in our case), it has to be initialized
    with the render_mode="rgb_array" argument. This argument tells the environment
    to return pixels from its render() method, which is being called by the HumanRendering
    wrapper.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Gymnasium 库，你可以通过两个类来检查环境内部的情况。第一个是 HumanRendering，它打开一个单独的图形窗口，在该窗口中，环境中的图像会以交互方式显示。为了能够渲染环境（在我们的例子中是
    CartPole），必须使用 render_mode="rgb_array" 参数进行初始化。这个参数告诉环境返回来自其 render() 方法的像素，而该方法由
    HumanRendering 包装器调用。
- en: 'So, to use the HumanRenderer wrapper, you need to change the random agent’s
    code (the full code is in Chapter02/04_cartpole_random_monitor.py):'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要使用 HumanRenderer 包装器，你需要修改随机代理的代码（完整代码位于 Chapter02/04_cartpole_random_monitor.py）：
- en: '[PRE24]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: If you start the code, the window with environment rendering will appear. As
    our agent cannot balance the pole for too long (10-30 steps max), the window will
    disappear quite quickly, once the env.close() method is called.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你启动代码，带有环境渲染的窗口将会出现。由于我们的代理无法保持平衡杆太长时间（最多 10-30 步），一旦调用 env.close() 方法，窗口会很快消失。
- en: '![PIC](img/file8.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file8.png)'
- en: 'Figure 2.5: CartPole environment rendered by HumanRendering'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：通过 HumanRendering 渲染的 CartPole 环境
- en: 'Another wrapper that might be useful is RecordVideo, which captures the pixels
    from the environment and produces a video file of your agent in action. It is
    used in the same way as the human renderer, but requires an extra argument specifying
    the directory to store video files. If the directory doesn’t exist, it will be
    created:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能有用的包装器是 RecordVideo，它捕获环境中的像素并生成一个展示代理行为的视频文件。它与 human renderer 的使用方式相同，但需要一个额外的参数来指定存储视频文件的目录。如果目录不存在，它会被创建：
- en: '[PRE25]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'After starting the code, it reports the name of the video produced:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 启动代码后，它会报告所生成视频的名称：
- en: '[PRE26]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This wrapper is especially useful in situations when you’re running your agent
    on a remote machine without the GUI.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这个包装器特别有用，当你在没有 GUI 的远程机器上运行代理时。
- en: More wrappers
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多包装器
- en: Gymnasium provides lots of other wrappers, which we’ll use in the upcoming chapters.
    It can do standardized preprocessing of Atari game images, do reward normalization,
    stack observation frames, do vectorization of an environment, do time limiting
    and much more.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Gymnasium 提供了许多其他的包装器，我们将在接下来的章节中使用。它可以对 Atari 游戏图像进行标准化预处理，进行奖励归一化，堆叠观察帧，进行环境向量化，设置时间限制等。
- en: The full list of available wrappers is available in the documentation, [https://gymnasium.farama.org/api/wrappers/](https://gymnasium.farama.org/api/wrappers/),
    and in the source code.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的完整包装器列表可以在文档中找到，[https://gymnasium.farama.org/api/wrappers/](https://gymnasium.farama.org/api/wrappers/)，也可以在源代码中查看。
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: You have started to learn about the practical side of RL! In this chapter, we
    experimented with Gymnasium, with its tons of environments to play with. We studied
    its basic API and created a randomly behaving agent.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经开始学习强化学习的实践部分！在这一章中，我们使用了 Gymnasium，探索了其众多可以使用的环境。我们研究了它的基本 API，并创建了一个随机行为的代理。
- en: You also learned how to extend the functionality of existing environments in
    a modular way and became familiar with a way to render our agent’s activity using
    wrappers. This will be heavily used in the upcoming chapters.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 你还学习了如何以模块化的方式扩展现有环境的功能，并且熟悉了通过包装器渲染代理活动的方式。这将在接下来的章节中得到广泛应用。
- en: In the next chapter, we will do a quick DL recap using PyTorch, which is one
    of the most widely used DL toolkits.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用 PyTorch 进行快速的深度学习回顾，PyTorch 是最广泛使用的深度学习工具包之一。
