- en: Deploying and Maintaining AI Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署与维护 AI 应用
- en: Throughout this book, we've learned all about how to create **Artificial Intelligence**
    (**AI**) applications to perform a variety of tasks. While writing these applications
    has been a considerable feat in itself, it's often only a small portion of what
    it takes to turn your model into a serviceable production system. For many practitioners,
    the workflow for deep learning models often ends at the validation stage. You've
    created a network that performs extremely well; We're done, right?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们已经了解了如何创建**人工智能**（**AI**）应用以执行各种任务。尽管编写这些应用本身是一项不小的成就，但它通常只是将模型转化为可用生产系统所需工作的一个小部分。对于许多从业者来说，深度学习模型的工作流通常在验证阶段结束。你已经创建了一个表现非常好的网络；我们完成了吗？
- en: It's becoming increasingly common for data scientists and machine learning engineers
    to handle their applications from the discovery to deployment stages. According
    to Google, more than 60-70% of the time it takes to build ...
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家和机器学习工程师从发现阶段到部署阶段处理应用的情况变得越来越普遍。根据谷歌的说法，构建应用程序所需的时间中，超过 60-70% 的时间用于...
- en: Technical requirements
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'While this chapter will contain some materials that are typically part of the
    job of a DevOps engineer, we''ll touch on these tools and topics on a need-to-know
    basis, and refer to other resources and tools that can help you learn about the
    topics in more depth. In this chapter, we''ll be utilizing the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章虽然包含了一些通常是 DevOps 工程师工作的一部分的内容，但我们会根据需要了解的程度涉及这些工具和话题，并参考其他资源和工具，以帮助您更深入地学习相关话题。本章将使用以下内容：
- en: TensorFlow
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: PyTorch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch
- en: Docker, a containerization service for deploying our models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker，一个用于部署我们模型的容器化服务
- en: Amazon Web Services or Google Cloud Platform as a cloud provider
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊 Web 服务或谷歌云平台作为云提供商
- en: Introductory knowledge of Kubernetes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 入门知识
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The deployment and maintenance of AI applications is more than just a single
    action; it's a process. In this section, we will work through creating sustainable
    applications in the cloud by creating a **deep learning deployment architecture**.These
    architectures will help us create end-to-end systems: **deep learning systems**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: AI 应用的部署与维护不仅仅是一个单一的动作；它是一个过程。在这一部分，我们将通过创建**深度学习部署架构**来在云中创建可持续的应用。这些架构将帮助我们创建端到端的系统：**深度学习系统**。
- en: 'In many machine learning/AI applications, the typical project pipeline and
    workflow might look something like the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多机器学习/AI 应用中，典型的项目流程和工作流可能类似于以下内容：
- en: '![](img/43eaa596-8282-48b3-bd65-a7f5273c2e90.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43eaa596-8282-48b3-bd65-a7f5273c2e90.png)'
- en: The training processes are strictly offline, and serialized models are pushed
    to the cloud and interact with a user through an API. These processes often leave
    us with several different ...
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程是严格离线的，序列化的模型会推送到云端，并通过 API 与用户交互。这些过程通常会给我们带来几个不同的...
- en: Deploying your applications
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署您的应用
- en: So, what does it mean to deploy a model? Deployment is an all-encompassing term
    that covers the process of taking a tested and validated model from your local
    computer, and setting it up in a sustainable environment where it's accessible.
    Deployment can be handled in a myriad of ways; in this chapter, we'll focus on
    the knowledge and best practices that you should know about to get your models
    up into production.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，部署一个模型意味着什么呢？部署是一个涵盖广泛的术语，指的是将一个经过测试和验证的模型从本地计算机中拿出来，并在一个可持续的环境中进行设置，使其可以访问。部署有多种方式可以处理；在本章中，我们将重点介绍您应该了解的知识和最佳实践，以帮助您将模型投入生产。
- en: 'Your choice of deployment architecture depends on a few things:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您选择的部署架构取决于几个因素：
- en: Is your model being trained in one environment and productionalized in another?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的模型是否在一个环境中训练，并在另一个环境中进行生产化？
- en: How many times are you expecting your model to be called predictions to be made
    from it?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您期望您的模型被调用多少次以进行预测？
- en: Is your data changing over time or is it static? Will you need to handle large
    inflows of data?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的数据是随时间变化的，还是静态的？您是否需要处理大量的数据流入？
- en: 'Each of these questions can be answered by breaking down our model selection
    options into two buckets. We can break down our models by the location from which
    they are served, as well as the way in which they are trained. The following figure
    shows these options in a matrix, as well as the costs and benefits of each method:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 每个问题可以通过将我们的模型选择选项分为两个类别来回答。我们可以根据模型的服务位置以及训练方式来划分模型。下图展示了这些选项的矩阵，以及每种方法的成本和收益：
- en: '![](img/b900735b-b3a0-486f-a85e-4244fb800d8c.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b900735b-b3a0-486f-a85e-4244fb800d8c.png)'
- en: Models that are trained on a specific piece of data, in a separate environment
    from where they are deployed, are called **offline models**, whereas models that
    actively learn from new data in their deployment environment are called **online
    models**.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个与部署环境分离的特定数据上训练的模型，称为 **离线模型**，而在其部署环境中主动从新数据中学习的模型，称为 **在线模型**。
- en: The simplest form of serving for offline models is called **batch serving**.
    If you are a data scientist or come from academia, you're probably very familiar
    with this model. Batch serving involves simply taking a static dataset, feeding
    it to your model, and receiving predictions back. Typically, you'll probably do
    this on your local machine, perhaps with a Jupyter Notebook or simply by running
    a script from your terminal or Command Prompt. In the majority of cases, we want
    our models to be accessible to larger groups of users so that we encase them in
    a **web service**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 离线模型最简单的服务形式称为 **批量服务**。如果你是数据科学家或来自学术界，可能非常熟悉这种模型。批量服务就是将静态数据集输入到模型中，然后获取预测结果。通常，你可能会在本地机器上做这个，可能通过
    Jupyter Notebook 或仅仅通过从终端或命令提示符运行脚本。在大多数情况下，我们希望我们的模型能够为更多用户所访问，因此我们将其封装成一个 **Web
    服务**。
- en: Online models are more difficult to manage due to the complications that can
    arise from the handling of data flows and potentially bad input data. Microsoft's
    fated Tay Twitter bot was an example of a Fully online learning model which took
    tweets as input, and quickly became racist and crude. Managing these models can
    become complicated because of the open training process, and many safeguards must
    be put in place to ensure that your model does not deviate too far from its desired
    output.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在线模型更难以管理，因为处理数据流和潜在的坏输入数据时可能会出现复杂问题。微软著名的 Tay Twitter 机器人就是一个完全在线学习模型的例子，它接受推文作为输入，迅速变得种族主义和粗俗。管理这些模型变得复杂，因为训练过程是开放的，必须采取许多防护措施来确保模型不会偏离预期输出太远。
- en: Automated machine learning models, on the other hand, are becoming increasingly
    popular. They have controlled input, but are actively retraining to consider new
    data. Think about Netflix's recommendation system – it actively responds to your
    behavior by training on the data you generate based on your browsing and viewing
    activity.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化机器学习模型则变得越来越流行。它们有控制的输入，但会主动重新训练以考虑新的数据。想想 Netflix 的推荐系统——它通过根据你浏览和观看活动生成的数据进行训练，积极响应你的行为。
- en: Now that we have a grasp of our ecosystem, let's get started by learning how
    to set up a common web service deployment architecture with TensorFlow. If you
    are not interested in learning about manual deployment processes, and only wish
    to use deployment service, please feel free to skip the next section.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了我们的生态系统，让我们开始学习如何使用 TensorFlow 设置一个常见的 Web 服务部署架构。如果你不感兴趣手动部署过程，只想使用部署服务，可以跳过下一部分。
- en: Deploying models with TensorFlow Serving
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Serving 部署模型
- en: In general, when deploying models, we want the inner machinations of the model
    to be isolated from the public behind an HTTP interface. With a traditional machine
    learning model, we would wrap this serialized model in a deployment framework
    such as Flask to create an API, and serve our model from there. This could lead
    us to a myriad of issues with dependencies, versioning, and performance, so instead,
    we are going to use a tool provided to us by the TensorFlow authors called **TensorFlow
    Serving**. This spins up a small server that runs a TensorFlow model and provides
    access to it.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，在部署模型时，我们希望将模型的内部机制通过 HTTP 接口与公众隔离。对于传统的机器学习模型，我们会将序列化的模型包装在像 Flask 这样的部署框架中，创建一个
    API，然后从那里提供我们的模型服务。这可能会导致依赖关系、版本控制和性能等一系列问题，因此，我们将使用由 TensorFlow 作者提供的工具，叫做 **TensorFlow
    Serving**。这个工具启动一个小型服务器，运行 TensorFlow 模型并提供访问接口。
- en: TensorFlow Serving implements a specific type of remote procedure call known
    as **GPRC**.In computer science, remote procedure ...
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving 实现了一种特定类型的远程过程调用，称为 **GPRC**。在计算机科学中，远程过程...
- en: Utilizing docker
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Docker
- en: Since we'll be deploying our model to the cloud, we'll need some type of mechanism
    to run the model itself. While we could spin up a virtual machine on AWS, it's
    a bit overkill for what we need and there are many simpler (and cheaper) processes
    that can help us.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将把模型部署到云端，我们需要某种机制来运行模型。虽然我们可以在 AWS 上启动一个虚拟机，但这对于我们的需求来说有点过头，而且有许多更简单（且更便宜）的方法可以帮助我们。
- en: Instead, we will utilize a tool known as a **container**. Containers are a lightweight
    virtualization technique that contain all of the necessary runtime packages and
    methods for running an application. The most popular container service is called
    **Docker**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们将使用一种称为**容器**的工具。容器是一种轻量级的虚拟化技术，包含了运行应用程序所需的所有必要运行时包和方法。最流行的容器服务被称为**Docker**。
- en: 'While we won''t cover the Docker installation process here, you can install
    Docker by following the official installation guidelines:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不会在这里讲解 Docker 的安装过程，但你可以按照官方安装指南来安装 Docker：
- en: Create a **Docker image**
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 **Docker 镜像**
- en: Create a **container from** the Docker image
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Docker 镜像创建一个**容器**
- en: Build TensorFlow Serving on the container
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在容器中构建 TensorFlow Serving
- en: The configuration of a Docker image is defined in something called a **Docker
    file**. TensorFlow Serving gives these files to us, one for utilizing CPUs and
    one for utilizing GPUs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 镜像的配置定义在一个叫做 **Docker 文件** 的文件中。TensorFlow Serving 给我们提供了这些文件，一个用于利用
    CPU，另一个用于利用 GPU。
- en: 'Google''s TensorFlow team maintains a Docker image that is ready to use for
    TensorFlow Serving:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌的 TensorFlow 团队维护了一个适用于 TensorFlow Serving 的 Docker 镜像，可以直接使用：
- en: 'Once you have Docker installed, you can grab it easily with the `docker pull`
    command in your terminal or command prompt:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你安装了 Docker，可以通过终端或命令提示符中的 `docker pull` 命令轻松获取：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You should see a series of messages that look something as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到一系列类似以下的消息：
- en: '![](img/b62de008-df45-4acb-a98d-01528bbfe313.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b62de008-df45-4acb-a98d-01528bbfe313.png)'
- en: 'Once you''ve downloaded the Docker image, you can move on to creating a container
    on the image. We can easily do that by running the build command:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你下载了 Docker 镜像，就可以继续在该镜像上创建一个容器。我们可以通过运行构建命令轻松完成：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Building the docker container can take a while—don't worry, this is normal.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 构建 Docker 容器可能需要一段时间——别担心，这是正常的。
- en: 'Once the container is built, go ahead and run the container:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦容器构建完成，继续运行容器：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You should now have shell access to your Docker container. Next, we''ll download
    the actual TensorFlow serving files into the container:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你应该可以访问 Docker 容器的 shell。接下来，我们将在容器中下载实际的 TensorFlow Serving 文件：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Lastly, we''ll need to install the TensorFlow modelserver on the container.
    Modelserver will be doing the actual serving for our model:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要在容器中安装 TensorFlow 模型服务器。模型服务器将实际为我们的模型提供服务：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Once we have a container, our environment is configured. The next thing to do
    is place our saved model inside the docker container.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了一个容器，环境就配置好了。接下来要做的是将我们保存的模型放入 Docker 容器中。
- en: When you exit the shell of a Docker container, the container shuts down. If
    you'd like to start the container again, you can do so by running `docker start
    -i nn_container` in the container's directory.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当你退出 Docker 容器的 shell 时，容器会关闭。如果你想再次启动容器，可以在容器的目录中运行 `docker start -i nn_container`
    命令。
- en: 'Let''s create a directory to place our model in. While you are still in the
    command line for the container, create a new directory with:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个目录来放置我们的模型。还在容器的命令行中时，使用以下命令创建一个新目录：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Next, we'll upload our saved model to this directory. From wherever you saved
    the classifier from previously, run the following commend. You'll replace `output_directory`
    with whatever the sub-folder is that the TensorFlow SavedModel binaries are saved
    in.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把保存的模型上传到这个目录。从你之前保存分类器的地方，运行以下命令。你需要将 `output_directory` 替换为 TensorFlow
    SavedModel 二进制文件所在的子文件夹。
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s try serving our model. Run the following command inside the docker container:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试为模型提供服务。在 Docker 容器中运行以下命令：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Your model should now be running locally with TensorFlow serving. We're not
    done, however, as we need to create a way that the model can interact with requests
    once it is deployed in the cloud. For that, we'll need to create something called
    a **client**, which is a small program that acts as a gatekeeper for the model
    to talk with the outside world.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您的模型现在应该在本地运行，使用TensorFlow服务。然而，我们还没有完成，因为在部署到云端后，我们需要创建一种模型可以与请求交互的方式。为此，我们需要创建一个称为**客户端**的东西，它是一个小程序，作为模型与外部世界交流的门卫。
- en: Building a TensorFlow client
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建TensorFlow客户端
- en: 'Lastly, we need to build a client to interact with our TensorFlow model. Building
    a custom client to interact with your model is a bit beyond the scope of this
    book, so we''ve provided this in the corresponding GitHub repository:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要构建一个客户端来与我们的TensorFlow模型进行交互。构建一个自定义客户端来与您的模型进行交互，这有点超出了本书的范围，因此我们在对应的GitHub存储库中提供了这个功能：
- en: 'Go ahead and download it with the following code:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请继续使用以下代码下载它：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s try using the client to send a request to the model:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们尝试使用客户端向模型发送请求：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: What's happening here?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了什么？
- en: The first line imports the client itself, ...
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一行导入了客户端本身，...
- en: Training and deploying with the Google Cloud Platform
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Google Cloud Platform进行训练和部署
- en: For a much simpler deployment procedure, we can deploy a TensorFlow SavedModel
    to production with the **Google Cloud Platform** (**GCP**). In this section, we'll
    cover the basics of how to both train and deploy a model using GCP.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更简单的部署过程，我们可以使用**Google Cloud Platform**（**GCP**）将TensorFlow SavedModel部署到生产环境。在本节中，我们将介绍如何使用GCP来训练和部署模型的基础知识。
- en: The GCP currently provides one of the most straightforward and easy interfaces
    for training and deploying models. If you are interested in getting your model
    up to production as quickly as possible, GCP is often your answer. Specifically,
    we'll be using the Cloud ML service, which is a compliment to AWS SageMaker that
    we just learned previously. Cloud ML is enabled currently enabled to run TensorFlow,
    Scikit-learn, and XGBoost right out of the box, although you can add your own
    packages manually. Compared to SageMaker, Cloud ML receives updates automatic
    updates to TensorFlow much at a rapid speed due to the library's Google integration,
    and hence it is recommended to use it for TensorFlow-based applications.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: GCP目前提供了最简单和易于使用的接口之一，用于训练和部署模型。如果您有兴趣尽快将模型投入生产，GCP通常是您的选择。具体来说，我们将使用Cloud ML服务，这是对我们刚刚学习过的AWS
    SageMaker的一个补充。Cloud ML目前能够直接运行TensorFlow、Scikit-learn和XGBoost，虽然您可以手动添加自己的包。与SageMaker相比，由于该库与Google集成，Cloud
    ML能够更快地接收TensorFlow的自动更新，因此建议您用于基于TensorFlow的应用程序。
- en: 'Before we get started, let''s set up a new Google Cloud Storage Bucket that
    will be the basis for our application. Go ahead and log onto your GCP account,
    look for Cloud Storage, and click Create bucket. You should see a screen that
    looks like the one as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，让我们设置一个新的Google Cloud存储桶，作为我们应用程序的基础。请登录您的GCP账号，找到Cloud Storage，然后点击创建存储桶。您应该看到一个类似以下内容的屏幕：
- en: '![](img/173abe83-964f-4d1a-b7a4-55dec4314d84.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/173abe83-964f-4d1a-b7a4-55dec4314d84.png)'
- en: This bucket will act as the staging ground for our data, model, training checkpoints,
    and model binaries. Go ahead and upload the `creditcard.csv` file that we've been
    using to the bucket , we'll be using it soon!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这个存储桶将作为我们数据、模型、训练检查点和模型二进制文件的临时存储区。继续将我们一直在使用的`creditcard.csv`文件上传到这个存储桶中，我们很快就会用到它！
- en: 'Next, let''s make our model ready for training on GCP, we''ll have to give
    it a couple lines of code so that it can run from the command line. In a script
    that contains the model code from previously, we''ll add this to the bottom:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们准备好在GCP上训练我们的模型，我们需要添加几行代码，以便它可以从命令行运行。在之前包含模型代码的脚本中，我们将其添加到底部：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This script will enable us to pass in the only parameter for the model, `job_dir`,
    from the command line. For the full GCP-ready code, check out the `simple_classifier.py`
    script in this chapter's GitHub. Once you have your Cloud Storage and script set
    up, we're ready to start our training and deployment!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本将使我们能够通过命令行传递模型的唯一参数`job_dir`。要查看完整的GCP就绪代码，请查看本章GitHub中的`simple_classifier.py`脚本。一旦您设置好Cloud
    Storage和脚本，我们就可以开始训练和部署！
- en: Training on GCP
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在GCP上训练
- en: Google has made the entire deep learning training / deployment process streamlined
    and simple by allowing us to train models, store them, and deploy them with minimal
    code. Before we start training in the cloud, let's train our model locally to
    ensure that everything is working as intended. First, we need to set some environment
    variables. First and foremost, we'll have to put our files in a particular structure
    to train with Cloud ML. Look for the training folder in the chapter `GitHub` folder,
    and you will find the correct file structure. The `__init__.py` file that you
    see there will simply tell GCP that our file is an executable Python program.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Google 通过允许我们训练模型、存储模型并以最少的代码进行部署，简化了整个深度学习训练/部署过程。在我们开始在云端训练之前，先在本地训练我们的模型，确保一切按预期工作。首先，我们需要设置一些环境变量。最重要的是，我们需要将文件放入特定的结构中，以便与
    Cloud ML 一起训练。在本章的 `GitHub` 文件夹中查找训练文件夹，你将找到正确的文件结构。你看到的 `__init__.py` 文件将告诉 GCP，我们的文件是一个可执行的
    Python 程序。
- en: First we'll define a job directory, which should be the folder where your `simple_classifier.py
    ...`
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义一个作业目录，这应该是你的 `simple_classifier.py ...` 文件所在的文件夹。
- en: Deploying for online learning on GCP
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 GCP 上部署在线学习
- en: When we deploy a TensorFlow SavedModel to the GCP platform, we either need to
    upload the entire SavedModel directory to a storage location on GCP or train in
    the cloud as we did previously. Regardless of what method you main use, your TensorFlow
    model's binaries should be stored in a Google Cloud Storage location.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将 TensorFlow SavedModel 部署到 GCP 平台时，我们需要将整个 SavedModel 目录上传到 GCP 上的存储位置，或者像之前一样在云端进行训练。无论你选择哪种方法，你的
    TensorFlow 模型二进制文件都应存储在 Google Cloud Storage 中。
- en: Your model binaries will be the final that was created after training, and will
    have the extension `.pb`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你的模型二进制文件将是训练后创建的最终文件，并将具有 `.pb` 扩展名。
- en: 'To start our deployment process, we first need to create a deployed model object.
    You can create it with the command as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始我们的部署过程，我们首先需要创建一个部署的模型对象。你可以通过以下命令创建它：
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we''ll create an environment variable that will let GCP know where our
    saved model binaries are:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个环境变量，让 GCP 知道我们的保存模型二进制文件的位置：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'All we have to do now is run the command as follows, and our classifier will
    be deployed! Keep in mind that deployment will take a few minutes to configure:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需要运行如下命令，我们的分类器就会被部署！请记住，部署需要几分钟的时间来配置：
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you see, we've done with a few lines of code what took us an entire section
    precedingly; platforms as services like AWS SageMaker and Google Cloud ML are
    extreme time savers in the modeling process.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，我们通过几行代码完成了前面一整节内容的工作；像 AWS SageMaker 和 Google Cloud ML 这样的平台作为服务在建模过程中节省了大量时间。
- en: 'Now, let''s try getting predictions from our model. Before we try asking for
    a prediction, we''ll need to go ahead and setups a few variables. The input data
    file will be a json file that contains a single line of data. To make it easy,
    we''ve included a line from the dataset as `test.json` in the `GitHub` folder:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试从我们的模型中获取预测。在请求预测之前，我们需要设置一些变量。输入数据文件将是一个包含一行数据的 json 文件。为了简便起见，我们已经将数据集中的一行作为
    `test.json` 包含在 `GitHub` 文件夹中：
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Lastly, go ahead and run the prediction request:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，运行预测请求：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Congratulations! Your model is now hosted in the cloud. You should see a returned
    json object, with probabilities for both of the potential classifications, fraud
    or not-fraud. While the `gcloud` command previously is great for issuing individual
    requests, we often want to return requests as part of a web application. In the
    next segment, we'll run through a simple example of how we can do this with a
    simple Flask application.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你的模型现在已托管在云端。你应该会看到一个返回的 json 对象，其中包含两种潜在分类的概率：欺诈或非欺诈。虽然之前的 `gcloud` 命令非常适合发出单个请求，但我们通常希望将请求作为
    Web 应用的一部分返回。在下一部分，我们将演示如何通过一个简单的 Flask 应用来做到这一点。
- en: Using an API to Predict
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 API 进行预测
- en: 'To get started, you''ll have to create a Google Cloud service account key so
    that your application can access the model. Navigate to the link [https://console.cloud.google.com/apis/credentials/serviceaccountkey](https://console.cloud.google.com/apis/credentials/serviceaccountkey),
    and create a new account. Be sure to download the key as a JSON file. To connect
    to GCP, you''ll need to setup your account credentials as an environment variable
    that GCP can access. Go ahead and set it''s location as an environment variable:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，您需要创建一个 Google Cloud 服务账户密钥，以便您的应用程序能够访问该模型。请访问链接 [https://console.cloud.google.com/apis/credentials/serviceaccountkey](https://console.cloud.google.com/apis/credentials/serviceaccountkey)，并创建一个新账户。记得将密钥下载为
    JSON 文件。要连接到 GCP，您需要将账户凭证设置为 GCP 可以访问的环境变量。请将其位置设置为环境变量：
- en: '[PRE16]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Let's create a script called `predict.py` (you can find the completed script
    in the `chapter` folder). First, we'll import the Google libraries that will allow
    our program to connect to the API. `GoogleCredidentials` will discover ...
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个名为 `predict.py` 的脚本（你可以在 `chapter` 文件夹中找到完整的脚本）。首先，我们将导入一些 Google 库，允许我们的程序连接到
    API。`GoogleCredentials` 将帮助我们发现 ...
- en: Scaling your applications
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展您的应用程序
- en: 'Scalability is the capacity for a system to handle greater and greater workloads.
    When we create a system or program, we want to make sure that it is scalable so
    that it doesn''t crash upon receiving too many requests. Scaling can be done in
    one of two ways:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性是指系统处理越来越多工作负载的能力。当我们创建一个系统或程序时，我们希望确保它是可扩展的，以便在接收到过多请求时不会崩溃。扩展可以通过两种方式进行：
- en: '**Scaling up**: Increasing the hardware of your existing workers, such as upgrading
    from CPUs to GPUs.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展上行**：提升现有工作节点的硬件配置，例如从 CPU 升级到 GPU。'
- en: '**Scaling out**: Distributing the workload among many workers. Spark is a common
    framework for doing this.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展外部**：将工作负载分配给多个工作节点。Spark 是一个常用的框架来实现这一点。'
- en: Scaling up can be as easy as moving your model to a larger cloud instance. In
    this section, we'll be focus on how to distribute TensorFlow to scale out our
    applications.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展上行可以像将模型迁移到更大云实例一样简单。在本节中，我们将专注于如何分布 TensorFlow，以便扩展我们的应用程序。
- en: Scaling out with distributed TensorFlow
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用分布式 TensorFlow 扩展计算能力
- en: What if we'd like to scale out our compute resources? We can distribute our
    TensorFlow processes over multiple workers to make training faster and easier.
    There are actually three frameworks for distributing TensorFlow: *native distributed
    TensorFlow*, *TensorFlowOnSpark*, and *Horovod*. In this section, we will be exclusively
    focusing on native distributed TensorFlow.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望扩展计算资源怎么办？我们可以将 TensorFlow 进程分布到多个工作节点上，从而加速和简化训练。实际上，有三种框架可以用来分布 TensorFlow：*原生分布式
    TensorFlow*、*TensorFlowOnSpark* 和 *Horovod*。在本节中，我们将专注于原生分布式 TensorFlow。
- en: 'In the world of distributed processing, there are two approaches that we can
    take to distribute the computational load of our model, that is, **model parallelism**
    and **data parallelism**:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式处理领域，我们可以采用两种方法来分配模型的计算负载，即**模型并行**和**数据并行**：
- en: '**Model parallelism**: The distribution of the training layers of a model across
    various devices.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型并行**：将模型的训练层分布到多个设备上。'
- en: '**Data parallelism**: The distribution of the entire model across various ...'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据并行**：将整个模型分布到多个设备上 ...'
- en: Testing and maintaining your applications
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试和维护您的应用程序
- en: With either online or offline learning, we should always institute systems and
    safety checks that will tell us when our model's predictions, or even its critical
    deployment architecture, are out of whack. By testing, we are referring to the
    hard-coded checking of inputs, outputs, and errors to ensure that our model is
    performing as intended. In standard software testing, for every input, there should
    be a defined output. This becomes difficult in the field of machine learning,
    where models will have variable outputs depending on a host of factors - not the
    great for standard testing procedures, is it? In this section, we'll talk about
    the process of testing machine learning code, and discuss best practices.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是在线学习还是离线学习，我们都应该建立系统和安全检查机制，及时告知我们模型的预测结果，甚至是关键部署架构是否出现问题。这里的测试指的是对输入、输出和错误进行硬编码检查，以确保我们的模型按照预期执行。在标准软件测试中，对于每一个输入，都应该有一个定义好的输出。但在机器学习领域，由于模型的输出会受到多种因素的影响，输出结果会有变动——这对于标准测试程序来说并不理想，是吗？在本节中，我们将讨论机器学习代码的测试过程，并探讨最佳实践。
- en: Once deployed, AI applications also have to be maintained. DevOps tools like
    Jenkins can help ensure that tests pass before new versions of a model are pushed
    to production. While we certainly aren't expecting you as a machine learning engineer
    to create development pipelines, we'll review their principals in this section
    so that you can be familiar with best practices.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦部署，AI 应用程序还需要维护。像 Jenkins 这样的 DevOps 工具可以帮助确保在将新版本的模型推送到生产环境之前通过测试。虽然我们当然不期望你作为机器学习工程师去创建开发管道，但我们将在本节中回顾其基本原理，以便你熟悉最佳实践。
- en: Testing deep learning algorithms
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试深度学习算法
- en: The AI community is severely behind on adopting appropriate testing procedures.
    Some of the most advanced AI companies rely on manual checks instead of automating
    tests on their algorithms. You've already done a form of a test throughout this
    book; cross-validating our models with training, testing, and validation sets
    helps to verify that everything is working as intended. In this section, we'll
    instead focus on **unit tests**, which seek to test software at the smallest computational
    level possible. In other words, we want to test the little parts of our algorithms,
    so we can ensure the larger platform is running smoothly.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: AI 社区在采用适当的测试程序方面严重滞后。一些最先进的 AI 公司依赖人工检查，而不是对他们的算法进行自动化测试。你已经在本书中进行了一种测试形式；通过交叉验证我们的模型，使用训练集、测试集和验证集来确保一切按预期工作。在本节中，我们将重点讨论
    **单元测试**，它旨在以尽可能小的计算单元测试软件。换句话说，我们要测试算法的各个小部分，以确保更大的平台能够顺利运行。
- en: Testing our algorithms helps us keep track of **non**-**breaking bugs**, which
    have become ubiquitous ...
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 测试我们的算法有助于我们追踪 **非**-**破坏性 bug**，这些 bug 已经变得无处不在……
- en: Summary
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Building AI applications goes beyond the basics of model construction – it takes
    deploying your models to a production environment in the cloud where they persist.In
    this chapter, we've discussed how to take a validated TensorFlow model and deploy
    it to production in the cloud. We also discussed ways that you can scale these
    models, and how you can test your applications for resiliency.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 构建 AI 应用程序不仅仅是模型构建的基础内容 —— 它还涉及将模型部署到云端的生产环境中，在那里它们得以持久化。在这一章中，我们讨论了如何将一个经过验证的
    TensorFlow 模型部署到云端的生产环境中。我们还讨论了如何扩展这些模型，以及如何测试你的应用程序的弹性。
- en: When taking a TensorFlow application from development to production, the first
    step is to create a TensorFlow SavedModel that can be stored in the cloud. From
    here, there are several services, including AWS Lambda and Google Cloud ML Engine,
    that can help make your deployment process easily.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在将 TensorFlow 应用程序从开发环境迁移到生产环境时，第一步是创建一个可以存储在云中的 TensorFlow SavedModel。从这里开始，有几个服务可以帮助简化你的部署过程，包括
    AWS Lambda 和 Google Cloud ML Engine。
- en: Applications can be scaled up or out for more computing power and faster processing.
    By scaling up, we provide our algorithms with a larger computing resource. By
    scaling out, we provide our application with more resources at once. Remember,
    models that are deployed to production should also be tested, and basic tests
    like unit tests can help prevent your entire application from crashing!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序可以通过纵向或横向扩展来获得更多的计算能力和更快的处理速度。通过纵向扩展，我们为算法提供更大的计算资源；通过横向扩展，我们一次性为应用程序提供更多的资源。记住，已部署到生产环境的模型也应进行测试，基本的测试，如单元测试，可以帮助防止你的整个应用程序崩溃！
- en: We've now reached the end of the book. As you've worked through the content
    in the chapters, I hope you have been enlightened to the exciting possibilities
    that deep learning is creating for the artificial intelligence field. While there
    is no doubt that research will continue into many of these topics, and that new
    methods will we be created and used, the fundamental concepts that you've learned
    throughout these chapters will hold steady, and provide the basis for groundbreaking
    work going forward. Who knows, the person doing that groundbreaking work could
    be you!
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经到达本书的结尾。随着你逐章学习，我希望你已经意识到深度学习为人工智能领域创造的激动人心的可能性。尽管毫无疑问，许多这些主题的研究将继续进行，并且新的方法将会被创造和使用，但你在本书中学到的基本概念将始终保持不变，为未来的突破性工作奠定基础。谁知道呢，也许进行这些突破性工作的人就是你！
- en: References
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Mention Quora
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提到 Quora
- en: Tensorflow client citation
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorFlow 客户端引用
