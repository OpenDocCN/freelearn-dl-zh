- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Rule-Based Techniques
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于规则的技术
- en: '**Rule-based techniques** are a very important and useful tool in **natural
    language processing** (**NLP**). Rules are used to examine text and decide how
    it should be analyzed in an all-or-none fashion, as opposed to the statistical
    techniques we will be reviewing in later chapters. In this chapter, we will discuss
    how to apply rule-based techniques to NLP. We will look at examples such as regular
    expressions, syntactic parsing, and semantic role assignment. We will primarily
    use the NLTK and spaCy libraries, which we have seen in earlier chapters.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于规则的技术**是**自然语言处理**（**NLP**）中非常重要且有用的工具。规则用于检查文本并决定如何以全有或全无的方式进行分析，这与我们将在后续章节中回顾的统计技术不同。在本章中，我们将讨论如何将基于规则的技术应用于自然语言处理。我们将查看一些示例，如正则表达式、句法分析和语义角色分配。我们将主要使用在前几章中见过的NLTK和spaCy库。'
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Rule-based techniques
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于规则的技术
- en: Why use rules?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么使用规则？
- en: Exploring regular expressions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索正则表达式
- en: Sentence-level analysis – syntactic parsing and semantic role assignment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子级别分析——句法分析和语义角色分配
- en: Rule-based techniques
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于规则的技术
- en: Rule-based techniques in NLP are, as the name suggests, based on rules written
    by human developers, as opposed to machine-learned models derived from data. Rule-based
    techniques were, for many years, the most common approach to NLP, but as we saw
    in [*Chapter 7*](B19005_07.xhtml#_idTextAnchor144), rule-based approaches have
    largely been superseded by numerical, machine-learned approaches for the overall
    design of most NLP applications. There are many reasons for this; for example,
    since rules are written by humans, it is possible that they might not cover all
    situations if the human developer has overlooked something.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理中的基于规则的技术顾名思义，依赖于由人工开发者编写的规则，而非从数据中派生的机器学习模型。基于规则的技术曾是多年来自然语言处理的最常见方法，但正如我们在[*第7章*](B19005_07.xhtml#_idTextAnchor144)中看到的，基于规则的方法在大多数自然语言处理应用程序的整体设计中，已经被数值型的机器学习方法大多取代。出现这种情况的原因有很多；例如，由于规则是由人类编写的，如果开发者忽视了某些情况，规则可能无法覆盖所有场景。
- en: However, for practical applications, rules can be very useful, either by themselves
    or, more likely, along with machine-learned models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于实际应用来说，规则可能非常有用，无论是单独使用，还是与机器学习模型结合使用。
- en: The next section will discuss the motivations for using rules in NLP applications.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将讨论在自然语言处理应用中使用规则的动机。
- en: Why use rules?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么使用规则？
- en: 'Rules are a useful approach in one or more of the following situations:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 规则在以下一种或多种情况下非常有用：
- en: The application you are developing has a requirement to analyze fixed expressions
    that include thousands or even millions of variants when it would be extremely
    difficult to provide enough data to learn a machine model. These kinds of expressions
    include numbers, monetary amounts, dates, and addresses, for example. It is hard
    for systems to learn models when the data is so diverse. Moreover, it is usually
    not necessary, as rules to analyze these expressions are not difficult to write
    because their formats are very structured. For both of these reasons, a rule-based
    approach is a simpler solution for recognizing fixed expressions.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您正在开发的应用程序要求分析包含成千上万甚至百万种变体的固定表达式，而提供足够的学习数据来训练机器模型将极为困难。这些固定表达式包括数字、货币金额、日期和地址等。例如，当数据如此多样化时，系统很难学习模型。此外，通常不需要这样做，因为这些表达式的格式非常结构化，编写分析这些表达式的规则并不困难。出于这两个原因，基于规则的方法是识别固定表达式的一个更简单的解决方案。
- en: Very little training data is available for the application, and creating new
    data would be expensive. For example, annotating new data might require very specialized
    expertise. Although there are now techniques (such as few-shot or zero-shot learning)
    that can adapt large pre-trained models to cover specialized domains, if the domain-specific
    data is very different from the original training data in terms of syntax or vocabulary,
    it will be difficult for the adaptation process to work well. Medical reports
    and air traffic control messages are examples of these kinds of data.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序可用的训练数据非常少，而创建新数据将是昂贵的。例如，注释新的数据可能需要非常专业的专长。尽管现在有一些技术（如少量学习或零-shot学习）可以使大规模预训练模型适应特定领域，但如果领域特定的数据在语法或词汇上与原始训练数据有很大不同，那么适应过程可能无法顺利进行。医学报告和空中交通管制信息就是这类数据的例子。
- en: There are existing, well-tested rules and libraries available that can easily
    be used in new applications, such as the Python `datetime` package for recognizing
    dates and times.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已经有现成的、经过充分测试的规则和库可以轻松地用于新的应用程序，例如用于识别日期和时间的Python `datetime`包。
- en: The goal is to bootstrap a machine-learned model by preliminary annotation of
    a corpus. Corpus annotation is needed in preparation for using that corpus as
    training data for a machine learning model, or for using the corpus as a *gold
    standard* in NLP system evaluation. In this process, the data is first annotated
    by applying some hand-written rules. Then, the resulting annotated corpus is usually
    reviewed and corrected by human annotators, since it is likely to contain errors.
    Even though the corpus requires a review process, an initial annotation by a rule-based
    system will save time over manual annotation from scratch.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标是通过对语料库的初步标注来启动一个机器学习模型。语料库标注是为了将该语料库作为机器学习模型的训练数据，或者将其作为NLP系统评估中的*黄金标准*所需的准备工作。在这个过程中，数据首先通过应用一些手写规则进行标注。然后，生成的标注语料库通常会被人工标注者审查和修正，因为它可能包含错误。尽管语料库需要审查过程，但通过基于规则的系统进行初步标注会比从零开始进行人工标注节省时间。
- en: The application needs to find named entities from a fixed, known set.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序需要从一个固定的已知集合中找到命名实体。
- en: The results have to be very precise – for example, grammar checking, proofreading,
    language learning, and authorship studies.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果必须非常精确——例如，语法检查、校对、语言学习和作者研究。
- en: A quick prototype is needed in order to test downstream processing, and the
    data collection and model training stages needed for machine learning would take
    too much time.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要一个快速的原型来测试下游处理，而机器学习所需的数据收集和模型训练阶段则需要太多时间。
- en: We will start by looking at regular expressions, a very common technique for
    analyzing text that contains well-understood patterns.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从正则表达式开始，它是一种非常常见的技术，用于分析包含已知模式的文本。
- en: Exploring regular expressions
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索正则表达式
- en: '**Regular expressions** are a widely used rule-based technique that is often
    used for recognizing fixed expressions. By **fixed expressions**, we mean words
    and phrases that are formed according to their own internal rules, which are largely
    different from the normal patterns of the language.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则表达式**是一种广泛使用的基于规则的技术，通常用于识别固定表达式。我们所说的**固定表达式**是指根据其自身规则形成的单词和短语，这些规则与语言的正常模式有很大的不同。'
- en: One type of fixed expression is *monetary amounts*. There are only a few variations
    in formats for monetary amounts – the number of decimal places, the symbol for
    the type of currency, and whether the numbers are separated by commas or periods.
    The application might only have a requirement to recognize specific currencies,
    which would simplify the rules further. Other common fixed expressions include
    *dates*, *times*, *telephone numbers*, *addresses*, *email addresses*, *measurements*,
    and *numbers*. Regular expressions in NLP are most frequently used in preprocessing
    text that will be further analyzed with other techniques.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一种固定表达式是*货币金额*。货币金额的格式变化很少——小数点位数、货币类型的符号，以及数字是否用逗号或句点分隔。应用程序可能只需要识别特定的货币，这样可以进一步简化规则。其他常见的固定表达式包括*日期*、*时间*、*电话号码*、*地址*、*电子邮件地址*、*度量单位*和*数字*。在NLP中，正则表达式最常用于对文本进行预处理，然后使用其他技术进一步分析。
- en: Different programming languages have slightly different formats for regular
    expressions. We will be using the Python formats defined at [https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html)
    and available in the Python `re` library. We will not define regular expression
    syntax here because there are many resources on the web that describe regular
    expression syntax, including the Python documentation, and we don’t need to duplicate
    that. You might find the information at [https://www.h2kinfosys.com/blog/nltk-regular-expressions/](https://www.h2kinfosys.com/blog/nltk-regular-expressions/)
    and [https://python.gotrained.com/nltk-regex/](https://python.gotrained.com/nltk-regex/)
    useful for getting into the details of regular expressions in NLTK.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的编程语言在正则表达式的格式上略有不同。我们将使用Python格式，具体格式可以参见[https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html)以及Python
    `re`库中的定义。我们这里不会定义正则表达式语法，因为网上有很多资源描述了正则表达式语法，包括Python文档，我们不需要重复这些内容。你可能会发现[https://www.h2kinfosys.com/blog/nltk-regular-expressions/](https://www.h2kinfosys.com/blog/nltk-regular-expressions/)和[https://python.gotrained.com/nltk-regex/](https://python.gotrained.com/nltk-regex/)上的信息有助于深入了解NLTK中的正则表达式。
- en: We will start by going over the basics of operating on strings with regular
    expressions, followed by some tips for making it easier to apply and debug regular
    expressions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍如何使用正则表达式操作字符串的基础知识，然后提供一些技巧，帮助简化正则表达式的应用和调试。
- en: Recognizing, parsing, and replacing strings with regular expressions
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用正则表达式识别、解析和替换字符串
- en: 'The simplest use of regular expressions is to simply note that a match occurred.
    What we want to do after a fixed expression has been matched depends on the goal
    of the application. In some applications, all we want to do is recognize that
    a fixed expression has occurred or did not occur. This can be useful, for example,
    in validating user input in web forms, so that users can correct invalid address
    formats. The following code shows how to recognize a US address with regular expressions:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式的最简单用法是仅仅标记是否匹配发生。固定表达式匹配后，我们要做什么取决于应用的目标。在某些应用中，我们只需要识别某个固定表达式是否发生过或没有发生过。例如，在验证网页表单中的用户输入时，这会很有用，这样用户就能纠正无效的地址格式。以下代码展示了如何使用正则表达式识别美国地址：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In other cases, we might want to parse the expression and assign meanings to
    the components – for example, in a date, it could be useful to recognize a month,
    a day, and a year. In still other cases, we might want to replace the expression
    with another expression, delete it, or normalize it so that all occurrences of
    the expression are in the same form. We could even want to do a combination of
    these operations. For example, if the application is classification, it is likely
    that we only need to know whether or not the regular expression occurred; that
    is, we don’t need its content. In that case, we can replace the expression with
    a `class` token such as `DATE`, so that a sentence such as *We received the package
    on August 2, 2022* becomes *We received the package on DATE*. Replacing the whole
    expression with a `class` token can also be used for redacting sensitive text
    such as social security numbers.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，我们可能希望解析表达式并为其组件赋予含义——例如，在日期中，识别月份、日期和年份可能会很有用。在某些情况下，我们可能希望用另一个表达式替换该表达式，删除它，或者规范化它，使得所有该表达式的出现形式都一致。我们甚至可能需要做这些操作的组合。例如，如果应用是分类，那么我们可能只需要知道正则表达式是否出现过；也就是说，我们不需要关心它的内容。在这种情况下，我们可以用`class`标记（如`DATE`）替换该表达式，这样像*我们在2022年8月2日收到了包裹*这样的句子就变成了*我们在DATE收到了包裹*。将整个表达式替换为`class`标记还可以用于编辑敏感文本，例如社会保障号码。
- en: 'The preceding code shows how to use regular expressions to match patterns in
    text and shows the code to confirm a match. However, this example just shows how
    to confirm that a match exists. We might want to do other things, such as replace
    the address with a class label, or label the matched portion of a string. The
    following code shows how to use the regular expression `sub` method to substitute
    a class label for an address:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码展示了如何使用正则表达式匹配文本中的模式，并展示了确认匹配的代码。然而，这个例子只是展示了如何确认匹配是否存在。我们可能还需要做其他事情，比如用类标签替换地址，或者标记字符串中的匹配部分。以下代码展示了如何使用正则表达式的`sub`方法替换地址为类标签：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Another useful operation is to label the whole expression with a semantic label
    such as `address`, as shown in the following code. This code shows how to add
    a label to the address. This enables us to identify US addresses in text and do
    tasks such as counting them or extracting all the addresses from texts:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的操作是为整个表达式添加语义标签，例如`address`，如下代码所示。这段代码展示了如何为地址添加标签。这样，我们就能够在文本中识别美国地址，并进行诸如计数或提取所有地址之类的任务：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The result of running the preceding code is as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码的结果如下：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Finally, regular expressions are useful if we want to remove the whole match
    from the text – for example, to remove HTML markup.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果我们想要从文本中删除整个匹配项，正则表达式非常有用——例如，删除HTML标记。
- en: General tips for using regular expressions
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用正则表达式的通用技巧
- en: Regular expressions can easily become very complex and difficult to modify and
    debug. They can also easily fail to recognize some examples of what they’re supposed
    to recognize and falsely recognize what they’re not supposed to recognize. While
    it is tempting to try to match the regular expression so that it recognizes exactly
    what it is supposed to recognize and nothing else, this can make the expression
    so complicated that it is difficult to understand. Sometimes, it can be better
    to miss a few edge cases to keep the expression simple.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式很容易变得非常复杂，且难以修改和调试。它们也很容易无法识别一些应该识别的示例，或者错误地识别了不该识别的内容。虽然很诱人去尝试将正则表达式匹配得恰好只识别它应该识别的内容，且不识别其他任何东西，但这样可能会使表达式变得过于复杂，难以理解。有时候，错过一些边缘情况可能比让表达式保持简单更好。
- en: 'If we find that an existing regular expression is failing to find some expressions
    that we want to capture, or incorrectly finding expressions that we don’t want
    to capture, it can sometimes be difficult to revise the existing expression without
    breaking some things that used to work. Here are a few tips that can make regular
    expressions easier to work with:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们发现现有的正则表达式未能识别一些我们希望捕获的表达式，或错误地识别了不该捕获的表达式，有时修改现有表达式而不破坏原有的功能可能会很困难。以下是一些能让正则表达式更容易使用的技巧：
- en: Write down what you want the regular expression to match first (such as *any
    two consecutive uppercase alphabetic characters*). This will be helpful in both
    clarifying what you’re trying to do as well as in helping catch any cases that
    you might have overlooked.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先写下你希望正则表达式匹配的内容（例如*任何两个连续的大写字母*）。这不仅有助于明确你想要做的事情，还能帮助你发现可能忽略的情况。
- en: Break complex expressions into components and test each component independently
    before putting them together. Besides helping with debugging, the component expressions
    can potentially be reused in other complex expressions. We saw this in the first
    code block in the previous section with components such as `street_name_re`.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将复杂的表达式拆分为多个组件，并在将它们组合之前独立测试每个组件。除了有助于调试外，这些组件表达式还可以在其他复杂表达式中重复使用。我们在上一节的第一个代码块中看到了这一点，像`street_name_re`这样的组件。
- en: Use existing tested regular expressions for common expressions, for example,
    the Python `datetime` package (see [https://docs.python.org/3/library/datetime.html](https://docs.python.org/3/library/datetime.html)),
    before trying to write your own regular expressions. They have been well tested
    over many years by many developers.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在尝试编写自己的正则表达式之前，先使用已有的经过测试的正则表达式来处理常见表达式，例如Python的`datetime`包（参见 [https://docs.python.org/3/library/datetime.html](https://docs.python.org/3/library/datetime.html)）。这些正则表达式经过多年多位开发者的充分测试。
- en: 'The next two sections cover specific ways to analyze two of the most important
    aspects of natural language: words and sentences.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两节将讨论分析自然语言中两个最重要方面的具体方法：单词和句子。
- en: The next section will start this topic by talking about analyzing individual
    words.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将通过分析单个单词来开始这个话题。
- en: Word-level analysis
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词级别的分析
- en: This section will discuss two approaches to analyzing words. The first one,
    lemmatization, involves breaking words down into their components in order reduce
    the variation in texts. The second one discusses some ideas for making use of
    hierarchically organized semantic information about the meanings of words in the
    form of ontologies.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将讨论两种分析单词的方法。第一种是词形还原，它通过将单词拆解成组成部分来减少文本中的变化。第二种方法讨论了一些关于如何利用关于单词意义的层次化语义信息（如本体论）进行分析的思路。
- en: Lemmatization
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词形还原
- en: In our earlier discussion of preprocessing text in [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107),
    we went over the task of **lemmatization** (and the related task of stemming)
    as a tool for regularizing text documents so that there is less variation in the
    documents we are analyzing. As we discussed, the process of lemmatization converts
    each word in the text to its root word, discarding information such as plural
    endings like *-s* in English. Lemmatization also requires a dictionary, because
    the dictionary supplies the root words for the words being lemmatized. We used
    Princeton University’s **WordNet** ([https://wordnet.princeton.edu/](https://wordnet.princeton.edu/))
    as a dictionary when we covered lemmatization in [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前讨论的 [*第 5 章*](B19005_05.xhtml#_idTextAnchor107) 文本预处理部分中，我们讲解了 **词形还原**（以及与之相关的词干提取）任务，作为一种规范化文本文档的方法，从而减少我们分析文档中的变异性。正如我们所讨论的，词形还原过程将文本中的每个单词转换为其词根，去除像英语中复数形式的
    *-s* 这样的信息。词形还原还需要一个字典，因为字典提供了被还原词汇的词根。当我们在 [*第 5 章*](B19005_05.xhtml#_idTextAnchor107)
    中讲解词形还原时，我们使用了普林斯顿大学的 **WordNet**（[https://wordnet.princeton.edu/](https://wordnet.princeton.edu/)）作为字典。
- en: We will use WordNet’s semantic information about the relationships among words
    in the next section, where we discuss ontologies and their applications.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用 WordNet 的语义信息，讨论本体及其应用。
- en: Ontologies
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本体
- en: '`entity`.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`entity`。'
- en: The ontology in *Figure 3**.2* is part of the **WordNet** ontology for English
    and a number of other languages. These hierarchical relationships are sometimes
    called **is a** relationships. For example, *an airplane is a vehicle*. In this
    example, we say that *vehicle* is a **superordinate** term and *airplane* is a
    **subordinate** term. WordNet uses some of its own terminology. In WordNet terminology,
    **hypernym** is the same thing as a superordinate term, and **hyponym** is the
    same thing as a subordinate term.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3.2* 中的本体是 **WordNet** 英语及其他语言的一个部分。这些层级关系有时被称为 **is a** 关系。例如，*an airplane
    is a vehicle*（一架飞机是一个交通工具）。在这个例子中，我们说 *vehicle* 是一个 **上位词**，*airplane* 是一个 **下位词**。WordNet
    使用自己的一些术语。在 WordNet 的术语中，**hypernym** 和上位词是一样的，**hyponym** 和下位词是一样的。'
- en: WordNet also includes many other semantic relationships, such as synonymy and
    *part-whole*. For example, we can find out that a wing is part of an airplane
    from WordNet. In addition, WordNet also includes part-of-speech information, and
    you will recall that we used this part-of-speech information in [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107)
    for part-of-speech tagging texts in preparation for lemmatization.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: WordNet 还包含许多其他语义关系，如同义词和 *部分—整体* 关系。例如，我们可以从 WordNet 中了解到，机翼是飞机的一部分。此外，WordNet
    还包括词性信息，你会记得我们在 [*第 5 章*](B19005_05.xhtml#_idTextAnchor107) 中使用了这些词性信息，用于对文本进行词性标注，为词形还原做准备。
- en: There are other ontologies besides WordNet, and you can even construct your
    own ontologies with tools such as Stanford University’s Protégé ([https://protege.stanford.edu/](https://protege.stanford.edu/)).
    However, WordNet is a good way to get started.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 WordNet，还有其他本体，你甚至可以使用斯坦福大学的 Protégé 工具（[https://protege.stanford.edu/](https://protege.stanford.edu/)）来构建你自己的本体。然而，WordNet
    是一个很好的入门方法。
- en: 'How can we make use of ontologies such as WordNet in NLP applications? Here
    are a few ideas:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何在自然语言处理（NLP）应用中利用像 WordNet 这样的本体呢？以下是一些想法：
- en: Develop a writing tool that helps authors find synonyms, antonyms, and definitions
    of words they would like to use.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发一个写作工具，帮助作者查找同义词、反义词以及他们想要使用的词语的定义。
- en: Count the number of mentions of different categories of words in texts. For
    example, you might be interested in finding all mentions of vehicles. Even if
    the text actually says *car* or *boat*, you could tell that the text mentions
    a vehicle by looking for words with *vehicle* as their hypernym.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计文本中不同类别的词汇出现次数。例如，你可能对查找所有提到“车辆”的地方感兴趣。即使文本中实际写的是 *car* 或 *boat*，你仍然可以通过查找与
    *vehicle* 相关的上位词来判断文本中提到了车辆。
- en: Generate additional examples of training data for machine learning by substituting
    different words with the same superordinate term in different sentence patterns.
    For example, suppose we have a chatbot that provides advice about cooking. It
    would probably get questions such as *How can I tell whether a pepper is ripe?*,
    or *Can I freeze tomatoes?* There are hundreds of types of food that could be
    substituted for *pepper* and *tomatoes* in those questions. It would be very tedious
    to create training examples for all of them. To avoid this, you could find all
    of the different types of vegetables in WordNet and generate training data by
    putting them into sentence templates to create new sentences.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在不同句型中用相同的上位词替换不同的单词，为机器学习生成额外的训练数据。例如，假设我们有一个关于烹饪的聊天机器人，它可能会收到类似*How can
    I tell whether a pepper is ripe?* 或*Can I freeze tomatoes?*的问题。这些问题中，*pepper*和*tomatoes*可以被成百上千种不同类型的食物替代。如果要为所有这些类型创建训练示例，将会非常繁琐。为了避免这种情况，你可以从WordNet中找到所有不同类型的蔬菜，并通过将它们放入句子模板中生成训练数据，从而创造新的句子。
- en: Let’s see an example of the previous strategy.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个之前策略的例子。
- en: 'You probably recall from earlier mentions of WordNet that it is included in
    NLTK, so we can import it and ask for the list of senses of *vegetable* (**synsets**)
    as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能记得之前提到过WordNet，它包含在NLTK中，因此我们可以导入它并查询*vegetable*的感知列表（**synsets**），如下所示：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We’ll then see that there are two *senses*, or meanings, of *vegetable*, and
    we can ask for their definitions in the following code:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将看到*vegetable*有两个*感知*，或意思，我们可以使用以下代码查询它们的定义：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The format of the sense names such as `vegetable.n.01` should be interpreted
    as `word` and `part-of-speech` (*n* means *noun* here), followed by the word’s
    order in the list of senses. We print the definitions of each of the two senses
    so that we can see what the WordNet senses mean. The resulting definitions are
    as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 感知名称的格式，例如`vegetable.n.01`，应该解释为`word`和`词性`（这里的*n*表示*名词*），后面跟着该单词在感知列表中的顺序。我们打印出每个感知的定义，以便查看WordNet中的感知含义。结果定义如下：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The first sense refers to the part that we eat, and the second sense refers
    to the plants whose parts we eat. If we are interested in cooking, we probably
    want the first sense of *vegetable* as *food*. Let’s get the list of all the vegetables
    in the first sense, using the following code:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个感知指的是我们吃的部分，第二个感知指的是我们吃的植物部分。如果我们对烹饪感兴趣，可能更想要把*vegetable*的第一个感知作为*食物*。我们可以使用以下代码获取第一个感知中的所有蔬菜列表：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The code goes through the following steps:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 代码执行以下步骤：
- en: Collect all the different types of the first sense of *vegetable* (the hyponyms)
    and store them in the `word_list` variable.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集所有第一个感知的蔬菜类型（下位词），并将它们存储在`word_list`变量中。
- en: Iterate through the list of words, collect the lemma for each word, and store
    the lemmas in the `simple_names` variable.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历单词列表，为每个单词收集其词根，并将词根存储在`simple_names`变量中。
- en: Print the words.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出单词。
- en: 'We can then generate some sample data by filling in a text template with each
    word, as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过将每个单词填充到文本模板中来生成一些示例数据，如下所示：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The preceding code shows the first few sentences that we can generate from the
    text frame and the list of vegetables. Of course, in a real application, we would
    want to have multiple text frames to get a good variety of sentences.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码展示了从文本框架和蔬菜列表生成的前几个句子。当然，在实际应用中，我们希望有多个文本框架，以便获得更丰富的句子种类。
- en: At the beginning of this section, we listed a few ways to apply ontologies in
    NLP applications; you can probably come up with more if you think of different
    ways that you could make use of the meanings of words to solve problems in natural
    language applications.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节开始时，我们列出了几种在自然语言处理应用中应用本体的方法；如果你想到其他不同的方式，利用单词的含义来解决自然语言应用中的问题，你可能还能想出更多方法。
- en: However, words don’t occur in isolation; they are combined with other words
    to create sentences with richer and more complex meanings. The next section will
    move on from analyzing words to the analysis of entire sentences, which we will
    analyze both syntactically and semantically.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，单词并不是孤立出现的；它们与其他单词组合在一起，构成具有更丰富、更复杂含义的句子。下一节将从单词分析转向整个句子的分析，我们将从句法和语义两个方面分析句子。
- en: Sentence-level analysis
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 句子级别分析
- en: Sentences can be analyzed in terms of their **syntax** (the structural relationships
    among parts of the sentence) or their **semantics** (the relationships among the
    meanings of the parts of the sentence). We’ll look at both of these types of analysis
    next. Recognizing syntactic relationships is useful on its own for applications
    such as grammar checking (does the subject of the sentence agree with the verb?
    Is the correct form of the verb being used?), while recognizing semantic relationships
    on their own is useful for applications such as finding the components of a request
    in chatbots. Recognizing both syntactic and semantic relationships together is
    an alternative to statistical methods in almost any NLP application.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 句子可以从**句法**（句子各部分之间的结构关系）或**语义**（句子各部分的意义关系）方面进行分析。接下来我们将讨论这两种分析类型。识别句法关系对于一些应用非常有用，例如语法检查（句子的主语是否与动词一致？动词的形式是否正确？），而识别语义关系则对于一些应用，如在聊天机器人中查找请求的组成部分，也非常有用。将句法和语义关系一起识别是几乎所有NLP应用中统计方法的替代方案。
- en: Syntactic analysis
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 句法分析
- en: The syntax of sentences and phrases can be analyzed in a process called `parse`
    package, which includes a variety of parsing algorithms that you can explore.
    For our examples in this section, we will use the chart parser in the `nltk.parse.ChartParser`
    class, which is a common and basic approach.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 句子和短语的句法可以通过一个叫做`parse`包的过程来分析，包中包含了多种解析算法，你可以进一步探索。在本节的例子中，我们将使用`nltk.parse.ChartParser`类中的图表解析器，这是一个常见且基础的方法。
- en: Context-free grammars and parsing
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上下文无关文法与解析
- en: A very common way to define the rules for syntactic parsing is **context-free
    grammars** (**CFGs**). CFGs can be used in chart parsing as well as many other
    parsing algorithms. You may be familiar with this format because it is widely
    used in computer science for defining formal languages, such as programming languages.
    CFGs consist of a set of *rules*. Each rule consists of a **left-hand side** (**LHS**)
    and a **right-hand side** (**RHS**), typically separated by a symbol, such as
    an arrow. The rule is interpreted to mean that the single symbol on the LHS is
    made up of the components of the RHS.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 定义句法分析规则的一个非常常见的方法是**上下文无关文法**（**CFGs**）。CFG可以用于图表解析以及许多其他解析算法。你可能对这种格式比较熟悉，因为它在计算机科学中被广泛应用于定义形式语言，如编程语言。CFG由一组*规则*组成。每个规则由**左侧**（**LHS**）和**右侧**（**RHS**）组成，通常通过一个符号（如箭头）来分隔。规则的解释是，LHS上的单一符号由RHS上的各个成分组成。
- en: For example, the context-free rule `S -> NP VP` states that a sentence (`S`)
    consists of a noun phrase (**NP**), followed by a **verb phrase** (**VP**). An
    NP can consist of a **determiner** (**Det**) such as *an*, *my*, or *the*, followed
    by one or two **nouns** (**Ns**) such as *elephant*, possibly followed by a **prepositional
    phrase** (**PP**), or just a **pronoun** (**Pro**), and so on. Every rule must
    be in turn defined with another rule, until the rules end in words (or, more generally,
    *terminal symbols*), which do not appear on the LHS of any rule.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，句法无关规则`S -> NP VP`表明一个句子（`S`）由一个名词短语（**NP**）和一个动词短语（**VP**）组成。一个NP可以由一个**限定词**（**Det**），如*an*、*my*或*the*，后面跟着一个或两个**名词**（**Ns**），如*elephant*，可能接着一个**介词短语**（**PP**），或者仅仅是一个**代词**（**Pro**），等等。每个规则必须通过另一个规则来定义，直到规则最终以单词（或更广泛地说，*终结符号*）结束，这些符号不会出现在任何规则的左侧（LHS）。
- en: 'The following shows the code for creating a CFG for a few rules of English.
    These are *constituency rules*, which show how the parts of the sentence are related
    to each other. There is another commonly used format, *dependencies*, which shows
    how the words are related to each other, which we will not explore in this book
    because the constituency rules are sufficient to illustrate the basic concepts
    of syntactic grammar and syntactic parsing:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了创建一些英语规则的CFG的代码。这些是*成分规则*，它们展示了句子各部分之间的关系。还有另一种常用的格式，即*依赖关系*，它展示了单词之间的关系，但我们在本书中不会探讨这种格式，因为成分规则足以说明句法语法和句法分析的基本概念：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This grammar is only able to parse a few sentences, such as *the children watched
    the movie in the family room*. For example, it would not be able to parse a sentence
    *the children slept* because, in this grammar, the VP has to include an object
    or prepositional phrase in addition to the verb. A full CFG for English would
    be much larger and more complex than the one in the preceding code. It’s also
    worth pointing out that the NLTK rules can be annotated with probabilities that
    indicate the likelihood of each alternative on the RHS.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这个语法只能解析少数几句句子，如*孩子们在家庭房间观看电影*。例如，它无法解析句子*孩子们睡觉*，因为在这个语法中，VP 除了动词外，必须包括宾语或介词短语。完整的英语
    CFG 会比前面的代码更大、更复杂。还值得指出的是，NLTK 规则可以附带概率标注，表示 RHS 上每个替代项的可能性。
- en: 'For example, *rule 4* (`Pro ![](img/011.png) ''I'' |''you''|''we''`) in the
    preceding code could have probabilities for the relative likelihoods of *I*, *you*,
    and *we*. In practice, this will result in more accurate parses, but it does not
    affect the examples we’ll show in this chapter. *Table 8.1* summarizes some CFG
    terminology:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，前面代码中的*规则 4*（`Pro ![](img/011.png) 'I' |'you'|'we'`）可能具有*I*、*you* 和 *we*
    的相对概率。在实践中，这将导致更准确的解析，但它不影响我们将在本章中展示的例子。*表 8.1* 总结了 CFG 术语的一些定义：
- en: '| **Symbol** | **Meaning** | **Example** |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| **符号** | **含义** | **示例** |'
- en: '| S | Sentence | The children watched the movie |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| S | 句子 | 孩子们观看了电影 |'
- en: '| NP | Noun phrase | The children |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| NP | 名词短语 | 孩子们 |'
- en: '| VP | Verb phrase | Watched the movie |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| VP | 动词短语 | 观看电影 |'
- en: '| PP | Prepositional phrase | In the family room |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| PP | 介词短语 | 在家庭房间 |'
- en: '| Pro | Pronoun | I, we, you, they, he, she, it |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Pro | 代词 | I, we, you, they, he, she, it |'
- en: '| Det | Determiner or article | The, a |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Det | 限定词或冠词 | the, a |'
- en: '| V | Verb | Watched, saw |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| V | 动词 | 看, 观看 |'
- en: '| N | Noun | Children, movie, elephant, family, room |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| N | 名词 | 孩子们，电影，象，家庭，房间 |'
- en: Table 8.1 – Meanings of CFG terms for the grammar in the CFG code block
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.1 – CFG 代码块中语法术语的含义
- en: '*Table 8.2* summarizes some syntactic conventions used in NLTK CFGs:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 8.2* 总结了在 NLTK CFG 中使用的一些句法约定：'
- en: '| **Symbol** | **Meaning** |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| **符号** | **含义** |'
- en: '| -> | Separates the LHS from the RHS |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| -> | 分隔左侧和右侧部分 |'
- en: '| &#124; | Separates alternate possibilities for RHSs that expand the LHS |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| &#124; | 分隔扩展左侧的右侧部分（RHS）的替代可能性 |'
- en: '| Single quotes | Indicates a word; that is, a terminal symbol |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 单引号 | 表示一个单词；即终结符号 |'
- en: '| Initial capitalization | Indicates a syntactic category; that is, a non-terminal
    symbol, which is expected to be defined by additional rules |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 首字母大写 | 表示一个句法类别；即非终结符号，预计通过其他规则进行定义 |'
- en: Table 8.2 – CFG syntax
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.2 – CFG 语法
- en: 'We can parse and visualize the sentence *the children watched the movie in
    the family room* with the grammar in the previous code block using the following
    code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用前面代码块中的语法，通过以下代码来解析并可视化句子*孩子们在家庭房间观看电影*：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can view the results of parsing in different ways – for example, as a bracketed
    text format, as in the following code:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以不同方式查看解析结果——例如，像下面的代码那样作为括号文本格式：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Notice that the parse directly reflects the grammar: the overall result is
    called *S*, because it came from the first rule in the grammar, `S -> NP VP`.
    Similarly, *NP* and *VP* are connected directly to *S*, and their child nodes
    are listed in parentheses after them.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，解析直接反映了语法：整体结果称为*S*，因为它来自语法中的第一个规则，`S -> NP VP`。同样，*NP* 和 *VP* 直接连接到 *S*，它们的子节点在它们后面以括号形式列出。
- en: 'The preceding format is useful for possible later stages of processing that
    may need to be computer-readable; however, it is a little bit difficult to read.
    *Figure 8**.1* shows a conventional tree diagram of this parse, which is easier
    to view. As in the case of the preceding text parse, you can see that it aligns
    directly with the grammar. The words, or terminal symbols, all appear at the bottom,
    or *leaves*, of the tree:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 上述格式对于后续的处理阶段非常有用，可能需要机器可读；然而，它有点难以阅读。*图 8.1* 显示了这个解析的常规树形图，更容易查看。与前面的文本解析相似，你可以看到它与语法直接对齐。单词或终结符号都出现在树的底部，或称为*叶子*：
- en: '![Figure 8.1 – Constituency tree for “the children watched the movie in the
    family room”](img/B19005_08_01.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – “孩子们在家庭房间观看电影”的句法树](img/B19005_08_01.jpg)'
- en: Figure 8.1 – Constituency tree for “the children watched the movie in the family
    room”
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – “孩子们在家庭房间观看电影”的句法树
- en: You can try parsing other sentences with this grammar, and you can also try
    modifying the grammar. For example, try adding a grammar rule that will enable
    the grammar to parse sentences with verbs that are not followed by NPs or PPs,
    such as *the* *children slept*.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试用这个语法解析其他句子，还可以尝试修改语法。例如，尝试添加一个语法规则，使得语法能够解析没有跟NP或PP的动词的句子，比如*孩子们睡觉了*。
- en: Semantic analysis and slot filling
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义分析和槽填充
- en: 'The previous sections on regular expressions and syntactic analysis dealt only
    with the structure of sentences, not their meaning. A syntactic grammar like the
    one shown in the preceding section can parse nonsense sentences such as *the movie
    watched the children in the room room* as long as they match the grammar. We can
    see this in *Figure 8**.2*:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 前面关于正则表达式和句法分析的章节仅仅讨论了句子的结构，而没有涉及它们的意义。像前面章节展示的这种句法语法，可以解析像*电影在房间里看孩子*这样的无意义句子，只要它们符合语法规则。我们可以在*图
    8.2*中看到这一点：
- en: '![Figure 8.2 – Constituency tree for “the movie watched the children in the
    room room”](img/B19005_08_02.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2 – “电影在房间里看孩子”句子的句法树](img/B19005_08_02.jpg)'
- en: Figure 8.2 – Constituency tree for “the movie watched the children in the room
    room”
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – “电影在房间里看孩子”句子的句法树
- en: In most applications, however, we don’t only want to find the syntactic structure
    of sentences; we also want to extract part or all of their meanings. The process
    of extracting meaning is called **semantic analysis**. The particular sense of
    *meaning* will vary depending on the application. For example, in many of the
    applications we’ve worked on so far in this book, the only meaning that needed
    to be derived from a document was its overall classification. This was the case
    in the movie review data – the only meaning that we wanted to get from the document
    was the positive or negative sentiment. The statistical methods that we’ve looked
    at in previous chapters are very good at doing this kind of coarse-grained processing.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在大多数应用程序中，我们不仅仅想找出句子的句法结构；我们还想提取它们的部分或全部含义。提取含义的过程叫做**语义分析**。*含义*的具体意义会根据应用程序的不同而变化。例如，在我们本书中到目前为止讨论的许多应用中，唯一需要从文档中推导出来的含义就是它的总体分类。这在电影评论数据中就是如此——我们希望从文档中获取的唯一含义就是积极或消极的情感。我们在前几章中讨论的统计方法非常擅长进行这种粗粒度的处理。
- en: However, there are other applications in which it’s necessary to get more detailed
    information about the relationships between items in the sentence. While there
    are machine learning techniques for getting fine-grained information (and we will
    be discussing them in *Chapters 9*, *10*, and *11*), they work best with large
    amounts of data. If there is less data available, sometimes rule-based processing
    can be more effective.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也有一些应用需要获得句子中项目之间关系的更详细信息。虽然有一些机器学习技术可以获取细粒度的信息（我们将在*第9章*、*第10章*和*第11章*中讨论它们），但它们在大量数据下表现最佳。如果数据较少，有时基于规则的处理会更加有效。
- en: Basic slot filling
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基本的槽填充
- en: For the next examples, we will be looking in detail at a technique often used
    in interactive applications, **slot filling**. This is a common technique used
    in voicebots and chatbots, although it is also used in non-interactive applications
    such as *information extraction*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于接下来的例子，我们将详细讨论一种在互动应用程序中经常使用的技术——**槽填充**。这是一种常用于语音机器人和聊天机器人的技术，尽管它也用于非互动应用程序，如*信息提取*。
- en: As an example, consider a chatbot application that helps a user find a restaurant.
    The application is designed to expect the user to offer some search criteria,
    such as the type of cuisine, the atmosphere, and the location. These criteria
    are the *slots* in the application. For example, the user might say, *I’d like
    to find a nice Italian restaurant near here*. The overall user goal, **Restaurant
    Search**, is the *intent*. We will be focusing on identifying slots in this chapter
    but will discuss intents in more detail in later chapters.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，考虑一个帮助用户寻找餐馆的聊天机器人应用程序。该应用程序被设计为期待用户提供一些搜索标准，如菜系类型、氛围和位置。这些标准就是应用程序中的*槽*。例如，用户可能会说，*我想找一家离这里近的意大利餐馆*。整体用户目标，**餐馆搜索**，就是*意图*。在这一章中，我们将重点关注识别槽，但会在后续章节中更详细地讨论意图。
- en: 'The design for this application is shown in *Figure 8**.3*:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用程序的设计如*图 8.3*所示：
- en: '![Figure 8.3 – Slots for a restaurant search application](img/B19005_08_03.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3 – 餐馆搜索应用程序的槽](img/B19005_08_03.jpg)'
- en: Figure 8.3 – Slots for a restaurant search application
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 餐厅搜索应用的槽位
- en: In processing the user’s utterance, the system has to identify what slots the
    user has specified, and it has to extract their values. This is all the information
    the system needs to help the user find a restaurant, so anything else in the sentence
    is normally ignored. This can lead to errors if the part of the sentence that
    is ignored is actually relevant, but in most cases, the approach works. This leads
    to a useful processing strategy in many applications, where the system only looks
    for information that is relevant to its task. This is in contrast to the syntactic
    parsing process that we reviewed previously, which required the system to analyze
    the entire sentence.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理用户的发言时，系统必须识别出用户指定了哪些槽位，并提取其值。这就是系统帮助用户找到餐厅所需要的所有信息，因此句子中的其他部分通常会被忽略。如果被忽略的部分实际上是相关的，这可能会导致错误，但在大多数情况下，这种方法是有效的。这也为许多应用程序提供了一种有用的处理策略，即系统只查找与其任务相关的信息。这与我们之前回顾的句法分析过程不同，后者要求系统分析整个句子。
- en: 'We can use the rule-based matcher in spaCy to create an application that analyzes
    a user’s utterance to find values for these slots. The basic approach is to define
    patterns for the system to find words that specify a slot and to define corresponding
    tags that label the values with their slot names. The following code shows how
    to find some of the slots shown in *Figure 8**.3* in sentences (we won’t show
    the code for all of the slots in order to keep the example short):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 spaCy 中的基于规则的匹配器来创建一个应用程序，分析用户的发言以找出这些槽位的值。基本方法是为系统定义模式，以找到指定槽位的词，并定义相应的标签来标记这些值及其槽位名称。以下代码展示了如何在句子中找到
    *图 8.3* 中显示的一些槽位（我们不会展示所有槽位的代码，以保持示例简短）：
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The preceding code starts by importing spaCy and the information that we need
    for processing text in the English language. The rule processor is called `ruler`
    and is added as a stage in the NLP pipeline. We then define three cuisines (a
    real application would likely have many more) and label them `CUISINE`. Similarly,
    we define patterns for recognizing price ranges, atmospheres, and locations. These
    rules state that if the user’s sentence contains a specific word or phrase, such
    as `near here`, the `LOCATION` slot should be filled with that word or phrase.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码首先导入了 spaCy 和我们在英语文本处理中需要的信息。规则处理器被称为 `ruler`，并作为 NLP 流程中的一个阶段添加。然后我们定义了三种菜系（实际应用中可能有更多种），并将它们标记为
    `CUISINE`。类似地，我们定义了用于识别价格范围、氛围和地点的模式。这些规则规定，如果用户的句子包含特定的词或短语，例如 `near here`，则应将
    `LOCATION` 槽位填充为该词或短语。
- en: 'The next step is to add the patterns to the rule processor (`ruler`) and then
    run the NLP processor on a sample sentence, *can you recommend a casual italian
    restaurant within walking distance?*. This process applies the rules to the document,
    which results in a set of labeled slots (which are called `doc.ents`. By printing
    the slots and values, we can see that the processor found three slots, `ATMOSPHERE`,
    `CUISINE`, and `LOCATION`, with values of `casual`, `Italian`, and `walking distance`,
    respectively. By trying other sentences, we can confirm the following important
    characteristics of this approach to slot filling:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将模式添加到规则处理器（`ruler`）中，然后在一个示例句子上运行 NLP 处理器，*你能推荐一个步行可达的休闲意大利餐厅吗？*。这个过程将规则应用于文档，结果是得到一组标记的槽位（称为`doc.ents`）。通过打印槽位和值，我们可以看到处理器找到了三个槽位，`ATMOSPHERE`、`CUISINE`
    和 `LOCATION`，对应的值分别是 `casual`、`Italian` 和 `walking distance`。通过尝试其他句子，我们可以确认这个槽位填充方法的以下几个重要特性：
- en: Parts of the sentence that don’t match a pattern, such as *can you recommend*,
    are ignored. This also means that the non-matching parts of the sentence can be
    nonsense, or they could actually be important to the meaning, but when they’re
    ignored, the system can potentially make a mistake. For example, if the utterance
    were *can you recommend a casual non-Italian restaurant within walking distance*,
    the system would incorrectly think that the user wanted to find an Italian restaurant
    by using these rules. Additional rules can be written to take these kinds of cases
    into account, but in many applications, we just want to accept some inaccuracies
    as the price of keeping the application simple. This has to be considered on an
    application-by-application basis.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子中不匹配模式的部分，如*你能推荐吗*，会被忽略。这也意味着，句子中不匹配的部分可能是无意义的，或者实际上对含义至关重要，但被忽略后，系统可能会出错。例如，如果用户说*你能推荐一个步行可达的非意大利餐馆吗*，系统会错误地认为用户想找一个意大利餐馆，依据这些规则。可以编写额外的规则来考虑这类情况，但在许多应用中，我们只希望接受一些不准确性，作为保持应用简洁的代价。这个问题需要根据具体应用来考虑。
- en: Slots and values will be recognized wherever they occur in the sentences; they
    don’t have to occur in any particular order.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 插槽和对应的值会在句子中出现的任何位置被识别；它们不需要出现在特定的顺序中。
- en: If a particular slot doesn’t occur in the sentence, that doesn’t cause any problems.
    It will just be left out of the resulting list of entities.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果特定插槽没有出现在句子中，通常不会导致问题。它只会从结果实体列表中被省略。
- en: If a slot occurs more than once, all occurrences will be recognized.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个插槽出现多次，所有出现的实例都会被识别。
- en: The names of the slot labels are up to the developer; they don’t have to be
    specific values. For example, we could have said `TYPE_OF_FOOD` instead of `CUISINE`,
    and the processing would be the same.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 插槽标签的名称由开发者自行决定；它们不需要是特定的值。例如，我们可以使用`TYPE_OF_FOOD`代替`CUISINE`，处理方式是一样的。
- en: 'We can use the spaCy visualizer, `displacy`, to get a clearer visualization
    of the result using the following code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用spaCy的可视化工具`displacy`，通过以下代码来获得结果的更清晰可视化：
- en: '[PRE13]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can see the result in *Figure 8**.4*, where the text and its slots and values
    are highlighted:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图8.4*中看到结果，其中文本及其插槽和对应的值已被高亮显示：
- en: '![Figure 8.4 – Slot visualization with displacy](img/B19005_08_04.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 – 使用displacy进行插槽可视化](img/B19005_08_04.jpg)'
- en: Figure 8.4 – Slot visualization with displacy
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 使用displacy进行插槽可视化
- en: Because our slots are custom (that is, not built into spaCy), to show colored
    slots, we have to define colors for the different slots (or `ents`) and assign
    the colors to the slots. We can then visualize the slots and values using a different
    color for each slot. The colors are defined in the `colors` variable in the preceding
    code. We can assign any colors to the slots that we find useful. The colors don’t
    need to be different, but it is normally most helpful if they are, and if they
    are fairly distinctive. The values of the colors in this example are hexadecimal
    codes, which have standard color interpretations. [https://www.color-hex.com/](https://www.color-hex.com/)
    is a useful website that shows the hexadecimal values for many colors.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的插槽是自定义的（即不是内建在spaCy中的），为了显示彩色的插槽，我们必须为不同的插槽（或`ents`）定义颜色，并将这些颜色分配给插槽。然后，我们可以使用不同的颜色可视化每个插槽及其对应的值。颜色在前面的代码中的`colors`变量中定义。我们可以为插槽分配任何认为有用的颜色。这些颜色不一定要不同，但通常来说，如果它们不同并且具有较高的区分度会更有帮助。此示例中的颜色值是十六进制代码，具有标准的颜色解释。[https://www.color-hex.com/](https://www.color-hex.com/)
    是一个显示许多颜色的十六进制值的实用网站。
- en: Using the spaCy id attribute
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用spaCy的id属性
- en: 'You have probably noticed that some of the slot values we have defined in this
    example mean the same thing – for example, `close by` and `near here`. If the
    slots and values are passed on to a later stage in processing such as database
    lookup, that later stage will have to have code for handling both `close by` and
    `near here`, even though the database lookup will be the same. This will complicate
    the application, so we would like to avoid it. spaCy provides another attribute
    of `ent`, `ent_id_`, for this purpose. This `id` attribute can be assigned in
    the patterns that find the slots, along with the label and pattern. This is accomplished
    by specifying an `id` attribute in the pattern declarations, which is a modification
    of the location patterns in the following code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在这个例子中我们定义的一些插槽值是相同的——例如，`close by` 和 `near here`。如果这些插槽和值被传递到后续的处理阶段，比如数据库查询，那么后续阶段必须有处理
    `close by` 和 `near here` 两者的代码，尽管数据库查询是相同的。这会使应用程序变得复杂，因此我们希望避免这种情况。spaCy 提供了
    `ent` 的另一个属性，`ent_id_`，用于这个目的。这个 `id` 属性可以在查找插槽的模式中分配，并且可以与标签和模式一起使用。通过在模式声明中指定
    `id` 属性来完成这一操作，这是以下代码中位置模式的修改：
- en: '[PRE14]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If we print the slots, values, and IDs that result from *can you recommend
    a casual italian restaurant close by*, the result is as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打印出来自 *can you recommend a casual italian restaurant close by* 的插槽、值和 ID，结果如下：
- en: '[PRE15]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we can see that the ID of `close by` is `nearby`, based on the `close`
    `by` pattern.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到 `close by` 的 ID 是 `nearby`，这是基于 `close` `by` 的模式。
- en: In the preceding code, we can see that the first three location patterns, which
    have similar meanings, have all been assigned the ID `nearby`. With this ID, the
    next stage in processing only needs to receive the `ent_id_` value, so it only
    has to handle `nearby`, and it doesn’t have to have additional cases for `close
    by` and `near me`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们可以看到前三个位置模式，这些模式具有相似的含义，都被分配了 `nearby` 的 ID。通过这个 ID，处理的下一阶段只需要接收 `ent_id_`
    值，因此只需要处理 `nearby`，而不需要为 `close by` 和 `near me` 添加额外的情况。
- en: Note that in this example, the results for the `CUISINE` and `ATMOSPHERE` slots
    have empty IDs, because these were not defined in the `CUISINE` and `ATMOSPHERE`
    patterns. It is nevertheless good practice to define IDs for all patterns, if
    there are any IDs, in order to keep the results uniform.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这个例子中，`CUISINE` 和 `ATMOSPHERE` 插槽的结果没有 ID，因为这些模式中没有定义 `CUISINE` 和 `ATMOSPHERE`。然而，最好为所有模式定义
    ID（如果有 ID），以保持结果的一致性。
- en: Also note that these patterns reflect some design decisions about what phrases
    are synonymous and, therefore, should have the same ID, and which phrases are
    not synonymous and should have different IDs.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 还请注意，这些模式反映了一些关于哪些短语是同义的设计决策，因此应具有相同的 ID，哪些短语不是同义的，应具有不同的 ID。
- en: In the preceding code, we can see that `short walk` doesn’t have the same ID
    as `near me`, for example. The design decision that was made here was to consider
    `short walk` and `near me` to have different meanings, and therefore to require
    different handling in the later stages of the application. Making decisions about
    which values are and are not synonymous will depend on the application and how
    rich the information available in the backend application is.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们可以看到 `short walk` 并没有与 `near me` 具有相同的 ID。例如，在这里做出的设计决策是将 `short walk`
    和 `near me` 视为具有不同的含义，因此在应用程序的后续阶段需要不同的处理。关于哪些值是同义词，哪些不是同义词的决策将取决于应用程序以及后端应用程序中可用的信息的丰富程度。
- en: 'We have described several useful rule-based approaches to NLP. *Table 8.3*
    summarizes these rule-based techniques by listing three important properties of
    rule-based techniques:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经描述了几种有用的基于规则的自然语言处理方法。*表 8.3* 总结了这些基于规则的技术，列出了规则技术的三个重要特性：
- en: The format of the rules
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规则的格式
- en: The types of processing that apply the rules to text
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用规则到文本的处理类型
- en: How the results are represented
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果如何表示
- en: '![](img/B19005_08_Table_01.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19005_08_Table_01.jpg)'
- en: Table 8.3 – Formats, processing, and results for rule-based techniques
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.3 – 基于规则的技术的格式、处理和结果
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we’ve learned several important skills that make use of rules
    for processing natural language.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了几种重要的技能，利用规则处理自然语言。
- en: We’ve learned how to apply regular expressions to identify fixed-format expressions
    such as numbers, dates, and addresses. We’ve also learned about the uses of rule-based
    Python tools such as the NLTK syntactic parsing libraries for analyzing the syntactic
    structure of sentences and how to apply them. Finally, we’ve learned about rule-based
    tools for semantics analysis such as spaCy’s `entity_ruler` for analyzing the
    slot-value semantics of sentences.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学会了如何应用正则表达式来识别固定格式的表达式，如数字、日期和地址。我们还了解了基于规则的Python工具的使用，例如NLTK语法分析库，用于分析句子的语法结构，并学会了如何应用它们。最后，我们学习了用于语义分析的基于规则的工具，例如spaCy的`entity_ruler`，用于分析句子的槽-值语义。
- en: The next chapter, [*Chapter 9*](B19005_09.xhtml#_idTextAnchor173), will begin
    the discussion on machine learning by introducing statistical techniques such
    as classification with Naïve Bayes and **term frequency-inverse document frequency**
    (**TF-IDF**), **support vector machines** (**SVMs**), and conditional random fields.
    In contrast to the rule-based approaches we have discussed in this chapter, statistical
    approaches are based on models that are learned from training data and then applied
    to new, previously unseen data. Unlike the all-or-none rule-based systems, statistical
    systems are based on probabilities.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章，[*第9章*](B19005_09.xhtml#_idTextAnchor173)将通过介绍统计技术，如使用朴素贝叶斯的分类方法、**词频-逆文档频率**（**TF-IDF**）、**支持向量机**（**SVMs**）和条件随机场，开始讨论机器学习。与我们在本章讨论的基于规则的方法不同，统计方法基于从训练数据中学习到的模型，然后应用到新的、未见过的数据上。与全或无的基于规则的系统不同，统计系统是基于概率的。
- en: As we explore these techniques, we’ll also consider how they can be combined
    with the rule-based techniques discussed in this chapter to create even more powerful
    and effective systems.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索这些技术时，我们还将考虑如何将它们与本章讨论的基于规则的技术结合起来，以创建更强大和有效的系统。
