- en: '22'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '22'
- en: Multi-Agent RL
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多智能体强化学习
- en: In the last chapter, we discussed discrete optimization problems. In this final
    chapter, we will introduce multi-agent reinforcement learning (sometimes abbreviated
    to MARL), a relatively new direction of reinforcement learning (RL) and deep RL,
    which is related to situations when multiple agents communicate in an environment.
    In real life, such problems appear in auctions, broadband communication networks,
    Internet of Things, and other scenarios.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了离散优化问题。在本章中，我们将介绍多智能体强化学习（有时缩写为MARL），这是一种相对较新的强化学习（RL）和深度强化学习（Deep
    RL）方向，涉及多个智能体在环境中进行交互的情况。现实生活中，这类问题出现在拍卖、宽带通信网络、物联网等场景中。
- en: In this chapter, we will just take a quick glance at MARL and experiment a bit
    with simple environments; but, of course, if you find it interesting, there are
    lots of things you can experiment with. In our experiments, we will use a straightforward
    approach, with agents sharing the policy that we are optimizing, but the observation
    will be given from the agent’s standpoint and include information about the other
    agent’s location. With that simplification, our RL methods will stay the same,
    and just the environment will require preprocessing and must handle the presence
    of multiple agents.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将快速浏览一下MARL，并在简单环境中进行一些实验；但当然，如果你对此感兴趣，仍有很多事情可以尝试。在我们的实验中，我们将采用一种直接的方式，智能体共享我们正在优化的策略，但观察结果将基于智能体的视角，并包括关于其他智能体位置的信息。通过这种简化，我们的RL方法将保持不变，唯一需要预处理的是环境，并且必须处理多个智能体的存在。
- en: 'More specifically, we will:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体来说，我们将：
- en: Start with an overview of the similarities and differences between the classical
    single-agent RL problem and MARL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从经典单一智能体强化学习问题与多智能体强化学习（MARL）之间的相似性和差异性概述开始
- en: Explore the MAgent environment, which was implemented and open sourced by the
    Geek.AI UK/China research group and later adopted by The Farama Foundation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索由Geek.AI英中研究小组实现并开源，后来被Farama基金会采纳的MAgent环境
- en: Use MAgent to train models in different environments with several groups of
    agents
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MAgent在不同的环境中训练多个智能体群体的模型
- en: What is multi-agent RL?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是多智能体强化学习？
- en: 'The multi-agent setup is a natural extension of the familiar RL model that
    we covered in Chapter [1](ch005.xhtml#x1-190001). In the classical RL setup, we
    have one agent communicating with the environment using observations, rewards,
    and actions. But in some problems that often arise in real life, we have several
    agents involved in the environment interaction. To give some concrete examples:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 多智能体设置是我们在第[1](ch005.xhtml#x1-190001)章中讨论的熟悉RL模型的自然扩展。在经典RL设置中，我们有一个智能体通过观察、奖励和行动与环境进行交互。但在一些经常出现在现实生活中的问题中，我们有多个智能体参与到环境交互中。为了举一些具体的例子：
- en: A chess game, when our program tries to beat the opponent
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一场象棋比赛，当我们的程序试图击败对手时
- en: A market simulation, like product advertisements or price changes, when our
    actions might lead to counter-actions from other participants
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个市场模拟，如产品广告或价格变化，当我们的行动可能引发其他参与者的反应时
- en: Multiplayer games, like Dota 2 or StarCraft II, when the agent needs to control
    several units competing with other players’ units (in this scenario, several units
    controlled by a single player might also cooperate to reach the goal)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多人游戏，如《Dota 2》或《StarCraft II》，当智能体需要控制多个单位与其他玩家的单位竞争时（在这种情况下，单一玩家控制的多个单位也可能会合作以实现目标）
- en: If other agents are outside of our control, we can treat them as part of the
    environment and still stick to the normal RL model with the single agent. As you
    saw in Chapter [20](ch024.xhtml#x1-36400020), training via self-play is a very
    powerful technique, which might lead to good policies without much sophistication
    on the environment side. But in some situations, that’s too limited and not exactly
    what we want.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果其他智能体不在我们的控制范围内，我们可以将其视为环境的一部分，并继续坚持使用单智能体的常规RL模型。如你在第[20](ch024.xhtml#x1-36400020)章所见，通过自对弈训练是一种非常强大的技术，可以在环境方面不需过多复杂化的情况下获得良好的策略。但在某些情况下，这种方法过于局限，并不是我们所需要的。
- en: In addition, as research shows, a group of simple agents might demonstrate collaborative
    behavior that is way more complex than expected. Some examples are the OpenAI
    blog post at [https://openai.com/blog/emergent-tool-use/](https://openai.com/blog/emergent-tool-use/)
    and the paper Emergent tool use from multi-agent autocurricula by Baker et al.
    [[Bak+20](#)] about the “hide-and-seek” game, where a group of agents collaborate
    and develop more and more sophisticated strategies and counter-strategies to win
    against another group of agents, for example, “build a fence from available objects”
    and “use a trampoline to catch agents behind the fence.”
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，研究表明，一组简单的智能体可能表现出远比预期更为复杂的协作行为。一些例子包括 OpenAI 博客文章 [https://openai.com/blog/emergent-tool-use/](https://openai.com/blog/emergent-tool-use/)
    和由 Baker 等人撰写的论文《多智能体自动课程中的突现工具使用》[[Bak+20](#)]，其中讨论了“捉迷藏”游戏，在这个游戏中，一组智能体合作并逐渐发展出越来越复杂的策略和反策略，以战胜另一组智能体。例如，“用现有物品建造栅栏”和“使用蹦床抓住栅栏后面的智能体”。
- en: 'In terms of different ways that agents might communicate, they can be separated
    into two groups:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在智能体可能进行交流的不同方式方面，可以将其分为两类：
- en: 'Competitive: When two or more agents try to beat each other in order to maximize
    their reward. The simplest setup is a two-player game, like chess, backgammon,
    or Atari Pong.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 竞争性：当两个或更多的智能体试图互相击败以最大化自己的奖励时。最简单的设置是双人游戏，如象棋、双陆棋或 Atari Pong。
- en: 'Collaborative: When a group of agents needs to use joint efforts to reach some
    goal.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协作性：当一组智能体需要共同努力以达成某个目标时。
- en: There are lots of examples that fall into one of these groups, but the most
    interesting and close to real-life scenarios are normally a mixture of both behaviors.
    There are tons of examples, starting from some board games that allow you to form
    allies and going up to modern corporations, where 100% collaboration is assumed,
    but real life is normally much more complicated than that.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多例子属于这些群体中的一种，但最有趣且最接近现实生活的场景通常是两种行为的混合。例子数不胜数，从一些允许你结盟的棋盘游戏到现代公司，在这些公司中，假设
    100% 的合作是理所当然的，但现实生活通常比这复杂得多。
- en: From a theoretical point of view, game theory has quite a developed foundation
    for both communication forms, but for the sake of brevity, we’re not going to
    deep dive into the field, which is large and has different terminology. If you’re
    curious, you can find lots of books and courses that explore it in great depth.
    To give an example, the minimax algorithm is a well-known result of game theory,
    and you saw it used in Chapter [20](ch024.xhtml#x1-36400020).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论角度来看，博弈论为这两种交流形式提供了相当成熟的基础，但为了简洁起见，我们不会深入探讨这一领域，它庞大且有不同的术语。如果你感兴趣，可以找到大量的书籍和课程，深入探讨这个话题。举个例子，最小最大算法是博弈论中的一个著名结果，你在第
    [20](ch024.xhtml#x1-36400020) 章看到过它的应用。
- en: MARL is a relatively young field, but activity has been growing over time; so,
    it might be interesting to keep an eye on it.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: MARL 是一个相对较年轻的领域，但随着时间的推移，活动逐渐增多；因此，关注这个领域可能会很有趣。
- en: Getting started with the environment
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用环境
- en: Before we jump into our first MARL example, let’s look at the environment we
    can use. If you want to play with MARL, your choice is a bit limited. All the
    environments that come with Gym support only one agent. There are some patches
    for Atari Pong, to switch it into two-player mode, but they are not standard and
    are an exception rather than the rule.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始第一个 MARL 示例之前，先来看看我们可以使用的环境。如果你想玩 MARL，选择有些有限。Gym 提供的所有环境只支持一个智能体。虽然有一些针对
    Atari Pong 的补丁，可以将其切换为双人模式，但它们并不是标准的，而是例外而非常规。
- en: DeepMind, together with Blizzard, has made StarCraft II publicly available (
    [https://github.com/deepmind/pysc2](https://github.com/deepmind/pysc2)) and makes
    for a very interesting and challenging environment for experimentation. However,
    for somebody who is taking their first steps in MARL, it might be too complex.
    In that regard, I have found the MAgent environment, originally developed by Geek.AI,
    to be perfectly suitable; it is simple and fast and has minimal dependency, but
    it still allows you to simulate different multi-agent scenarios for experimentation.
    It doesn’t provide a Gym-compatible API, but we will implement it on our own.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind与Blizzard一起使《星际争霸II》公开可用（[https://github.com/deepmind/pysc2](https://github.com/deepmind/pysc2)），为实验提供了一个非常有趣且具有挑战性的环境。然而，对于刚接触MARL的人来说，这可能过于复杂。在这方面，我发现由Geek.AI最初开发的MAgent环境非常合适；它简单、快速，并且依赖最小，但仍然允许您模拟不同的多智能体场景进行实验。它没有提供与Gym兼容的API，但我们将自行实现。
- en: 'If you’re interested in MARL, you might also check out the PettingZoo package
    from The Farama Foundation: [https://pettingzoo.farama.org](https://pettingzoo.farama.org).
    It includes more environments and a unified API for communication, but in this
    chapter, we’re focusing only on the MAgent environment.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对MARL感兴趣，你还可以查看Farama Foundation提供的PettingZoo包：[https://pettingzoo.farama.org](https://pettingzoo.farama.org)。它包含更多的环境和统一的API用于通信，但在本章中，我们只关注MAgent环境。
- en: An overview of MAgent
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MAgent概览
- en: Let’s look at MAgent at a high level. It provides the simulation of a grid world
    that 2D agents inhabit. These agents can observe things around them (according
    to their perception length), move to some distance from their location, and attack
    other agents around them.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从高层次来看MAgent。它提供了一个二维智能体栖息的网格世界的模拟。这些智能体可以观察周围的事物（根据它们的感知范围），移动到离当前位置一定距离的地方，并攻击周围的其他智能体。
- en: There might be different groups of agents with various characteristics and interaction
    parameters. For example, the first environment that we will consider is a predator-prey
    model, where “tigers” hunt “deer” and obtain a reward for that. In the environment
    configuration, you can specify lots of aspects of the group, like perception,
    movement, attack distance, the initial health of every agent in the group, how
    much health they spend on movement and attack, and so on. Aside from the agents,
    the environment might contain walls that are not crossable by the agents.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会有不同的智能体群体，具有不同的特征和交互参数。例如，我们将考虑的第一个环境是一个捕食者-猎物模型，其中“老虎”捕猎“鹿”并为此获得奖励。在环境配置中，您可以指定群体的许多方面，如感知、移动、攻击距离、群体中每个智能体的初始健康、它们在移动和攻击时消耗多少健康等等。除了智能体，环境中可能还包含墙壁，智能体无法穿越这些墙壁。
- en: The nice thing about MAgent is that it is very scalable, as it is implemented
    in C++ internally, just exposing the Python interface. This means that the environment
    can have thousands of agents in the group, providing you with observations and
    processing the agents’ actions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: MAgent的优点在于它非常具有可扩展性，因为它内部是用C++实现的，只暴露Python接口。这意味着环境中可以有数千个智能体，提供观察并处理智能体的行为。
- en: Installing MAgent
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装MAgent
- en: 'As it often happens, the original version of MAgent hasn’t been maintained
    for some time. Luckily for us, The Farama Foundation forked the original repo
    and are currently maintaining it, providing most of the original functionality.
    Their version is called MAgent2 and the documentation can be found here: [https://magent2.farama.org/](https://magent2.farama.org/).
    The GitHub repository is available here: [https://github.com/Farama-Foundation/magent2](https://github.com/Farama-Foundation/magent2).
    To install MAgent2, you need to run the following command:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如常常发生的那样，MAgent的原始版本已经有一段时间没有维护了。幸运的是，Farama Foundation分叉了原始的代码库并且目前在维护它，提供了大部分原始功能。他们的版本叫做MAgent2，文档可以在这里找到：[https://magent2.farama.org/](https://magent2.farama.org/)。GitHub代码库在这里：[https://github.com/Farama-Foundation/magent2](https://github.com/Farama-Foundation/magent2)。要安装MAgent2，您需要运行以下命令：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Setting up a random environment
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置随机环境
- en: To quickly understand the MAgent API and logic, I’ve implemented a simple environment
    with “tiger” and “deer” agents, where both groups are driven by the random policy.
    It might not be very interesting from an RL perspective, but it will allow us
    to quickly learn enough about the API to implement the Gym environment wrapper.
    The example can be found in Chapter22/forest_random.py and we’ll walk through
    it here.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速理解 MAgent API 和逻辑，我实现了一个简单的环境，其中有“老虎”和“鹿”智能体，两个组都由随机策略驱动。从强化学习的角度来看，它可能不太有趣，但它将帮助我们快速了解足够的
    API 内容，以实现 Gym 环境包装器。示例可以在 Chapter22/forest_random.py 中找到，我们将在这里逐步讲解。
- en: 'We start with ForestEnv, defined in lib/data.py, which defines the environment.
    This class is inherited from magent_parallel_env (yes, the name of the class is
    in lowercase, in contravention of the Python style guide, but that’s how it has
    been defined in the library), the base class for MAgent environments:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 lib/data.py 中定义的 ForestEnv 开始，它定义了环境。这个类继承自 magent_parallel_env（是的，类名是小写的，违反了
    Python 风格指南，但它在库中就是这样定义的），这是 MAgent 环境的基类：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This class mimics the Gym API, but it is not 100% compatible, so we will need
    to deal with this in our code later.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类模拟了 Gym API，但它不是 100% 兼容的，因此我们稍后需要在代码中处理这个问题。
- en: 'In the constructor, we instantiate the GridWorld class, which works as a Python
    adapter around the low-level MAgent C++ library API:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在构造函数中，我们实例化了 GridWorld 类，它作为 Python 适配器围绕低级的 MAgent C++ 库 API 工作：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code, we instantiate the GridWorld class, which implements
    most of the logic of our environment.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们实例化了 GridWorld 类，它实现了我们环境的大部分逻辑。
- en: 'The GridWorld class is configured by the Config instance returned by the get_config
    function:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: GridWorld 类由 get_config 函数返回的 Config 实例配置：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This function uses the forest_config function, which is imported from the magent.builtin.config.forest
    package and tweaks the configuration, adding the reward for deer on every step.
    This will be important when we start training the deer model, so a reward of 1
    on every step will incentivize the agent to live longer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数使用了来自 magent.builtin.config.forest 包的 forest_config 函数，并调整配置，在每一步都为鹿添加奖励。当我们开始训练鹿模型时，这将非常重要，因此每一步奖励
    1 会激励智能体活得更久。
- en: 'The rest of the configuration hasn’t been included here as it is largely unchanged
    and defines lots of details about the environment, including the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的配置没有在这里包含，因为它大部分没有变化，定义了许多关于环境的细节，包括以下内容：
- en: 'How many groups of agents do we have in the environment? In our case, we have
    two groups: “deer” and “tigers.”'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境中有多少组智能体？在我们的例子中，有两组： “鹿” 和 “老虎”。
- en: What are the properties of each group – how far can they see from their location?
    An example of this could be that the deer can see as far as one cell, but the
    tigers have can see as far as four. Can they attack others and how far? What is
    the initial health of each agent? How fast can they recover from damage? There
    are lots of parameters you can specify.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每组的属性是什么——它们从当前位置能看到多远？举个例子，鹿可以看到一格远，而老虎可以看到四格。它们能攻击其他人吗，攻击范围多远？每个智能体的初始生命值是多少？它们从伤害中恢复的速度有多快？你可以指定很多参数。
- en: How can they attack other groups and what damage does it do? There is lots of
    flexibility – for example, you can model the scenario when predators hunt only
    in pairs (we’ll do this experiment later in the chapter). In our current setup,
    the situation is simple – any tiger can attack any deer without restrictions.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们如何攻击其他组，以及造成什么样的伤害？这有很多灵活性——例如，你可以模拟捕食者仅成对狩猎的场景（我们稍后在本章会做这个实验）。在我们当前的设置中，情况很简单——任何一只老虎都可以攻击任何一只鹿，没有限制。
- en: 'The last function in the ForestEnv class is generate_map, which places walls,
    deer, and tigers randomly on the map:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ForestEnv 类中的最后一个函数是 generate_map，它会将墙壁、鹿和老虎随机放置在地图上：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now let’s get to the forest_random.py source code. In the beginning, we import
    the lib.data package and the VideoRecorder class from Gymnasium:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下 forest_random.py 的源代码。一开始，我们导入 lib.data 包和来自 Gymnasium 的 VideoRecorder
    类：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In Chapter [2](ch006.xhtml#x1-380002), we used the RecordVideo wrapper to capture
    environment observations automatically, but in the case of MAgent environments,
    it is not possible, due to different return values (all methods are returning
    dictionaries for all agents at once instead of single values). To get around this,
    we’ll use the VideoRecorder class to capture videos and write into the RENDER_DIR
    directory.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[2](ch006.xhtml#x1-380002)章中，我们使用了RecordVideo封装器来自动捕获环境观察，但在MAgent环境中，由于返回值不同（所有方法一次返回所有代理的字典而不是单个值），这无法实现。为了解决这个问题，我们将使用VideoRecorder类来捕获视频并写入RENDER_DIR目录。
- en: 'First, we create a ForestEnv instance and video recorder. An environment object
    contains the property agents, which keeps string identifiers for all the agents
    in the environment. In our case, it will be a list of values like deer_12 or tiger_3\.
    With the default configuration, on the map 64 × 64, we have 204 deer agents and
    40 tigers, so the env.agents list has 244 items:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个ForestEnv实例和视频记录器。环境对象包含属性agents，保存所有代理的字符串标识符。在我们的案例中，它将是一个类似deer_12或tiger_3的值列表。使用默认配置，在64×64的地图上，我们有204只鹿代理和40只老虎，因此env.agents列表包含244项：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We reset the environment using the reset() method, but now it returns one value
    (instead of the two in the Gym API). The returned value is a dict with agent IDs
    as keys and observation tensors as values:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用reset()方法重置环境，但现在它返回一个值（而不是Gym API中的两个值）。返回的值是一个字典，键是代理的ID，值是观察张量：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding code produces the following output:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The action space contains five mutually exclusive actions for deer (four directions
    + a “do nothing” action). Tigers can do the same, but in addition can attack in
    four directions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 行为空间包含五个互斥的行为，适用于鹿（四个方向+一个“不作为”行为）。老虎也可以执行这些动作，但它们还可以在四个方向上进行攻击。
- en: 'In terms of observations, every tiger gets a 9 × 9 matrix with five different
    planes of information. Deer are more short-sighted, so their observation is just
    3 × 3\. The observation always contains the agent in the center, so it shows the
    grid around this specific agent. The five planes of information are:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 就观察而言，每只老虎会得到一个9×9的矩阵，其中包含五个不同的观察平面。鹿的视野较短，所以他们的观察范围仅为3×3。观察总是以代理为中心，因此它显示的是围绕该特定代理的网格。五个观察平面包括：
- en: 'Walls: 1 if this cell contains the wall and 0 otherwise'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 墙壁：如果该单元格包含墙壁，则为1，否则为0。
- en: 'Group 1 (the group that the agent belongs to): 1 if the cell contains agents
    from the agent’s group and 0 otherwise'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一组（代理所属组）：如果单元格中包含来自代理组的代理，则为1，否则为0。
- en: 'Group 1 health: The relative health of the agent in this cell'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一组健康：该单元格中代理的相对健康值。
- en: 'Group 2 (the group with enemy agents): 1 if there is an enemy in this cell
    and 0 otherwise'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二组（敌方代理所在组）：如果单元格中有敌人，则为1，否则为0。
- en: 'Group 2 health: The relative health of the enemy or 0 if nothing is there'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二组健康：敌人的相对健康值，如果没有敌人则为0。
- en: If more groups are configured, the observation will contain more planes in the
    observation tensor. In addition, MAgent has a “minimap” functionality that adds
    the “zoomed-out” location of agents of every group. This minimap feature is disabled
    in my examples, but you can experiment with it to check the effect on the training.
    Without this feature, every agent sees only a limited range of cells around itself,
    but minimap allows them to have a more global view of the environment.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果配置了更多的组，观察将包含更多的观察平面。此外，MAgent有一个“迷你地图”功能，可以显示每个组的代理的“缩小视图”位置。这个迷你地图功能在我的示例中被禁用，但你可以试着开启它，检查它对训练的影响。如果没有这个功能，每个代理只能看到周围有限范围内的单元格，但迷你地图允许他们更全面地了解环境。
- en: Groups 1 and 2 are relative to the agent’s group; so, in the second plane, deer
    have information about other deer and for tigers, this plane includes other tigers.
    This makes observations group-independent and allows us to train a single policy
    for both groups if needed.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 第1组和第2组是相对于代理组的；因此，在第二平面中，鹿获取其他鹿的信息，而老虎则获得其他老虎的信息。这使得观察独立于组，从而如果需要的话，我们可以为两个组训练一个单一的策略。
- en: Another optional part of the observation is the so-called “extra features,”
    which includes the agent’s ID, last action, last reward, and normalized position.
    Concrete details could be found in the MAgent source code, but we’re not going
    to use this functionality in our examples.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 观察的另一个可选部分是所谓的“额外特征”，包括代理的ID、上一个行为、上一个奖励和归一化位置。具体细节可以在MAgent源代码中找到，但我们在示例中不使用此功能。
- en: 'Let’s continue describing our code. We have a loop that is repeated until we
    have alive agents in the environment. On every iteration, we sample random actions
    for all the agents and execute them in the environment:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续描述我们的代码。我们有一个循环，直到环境中没有活着的代理。在每次迭代中，我们为所有代理随机采样动作，并在环境中执行它们：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'All values returned from the env.step() function are dictionaries with agent_id
    as the key. Another very important detail about the MAgent environment is that
    the set of agents is volatile: agents can disappear from the environment (when
    they die, for example). In our “forest” environment, tigers are losing 0.1 points
    of health every step, which could be increased after eating deer. Deer lose health
    only after an attack and gain it on every step (likely from eating grass).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从`env.step()`函数返回的所有值都是以`agent_id`为键的字典。关于MAgent环境的另一个非常重要的细节是，代理集是易变的：代理可以从环境中消失（例如，当它们死亡时）。在我们的“森林”环境中，老虎每步失去0.1点健康值，吃掉鹿后这一损失可能会增加。鹿只有在受到攻击后才会失去健康值，而在每步中会恢复健康（很可能是因为吃了草）。
- en: When the agent dies (a tiger from starvation or a deer from a tiger’s attack),
    the corresponding entry in the all_dones dict is set to True and on the next iteration,
    the agent disappears from all the dictionaries and the env.agents list. So, after
    the death of one agent, the whole episode continues and we need to take this into
    account during the training.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当代理死亡（如老虎因饥饿死亡或鹿因老虎的攻击死亡）时，`all_dones`字典中的相应条目被设置为True，并且在下一次迭代中，代理会从所有字典和`env.agents`列表中消失。因此，在一个代理死亡后，整个回合会继续，我们需要在训练过程中考虑到这一点。
- en: In the preceding example, the loop is executed until no more agents are alive.
    As both tigers and deer are behaving randomly (and tigers are losing health at
    every step), it is very likely that all tigers will die from starvation and the
    surviving deer will live happily infinitely long. But the environment is configured
    to automatically remove all deer when no more tigers are left, so our program
    ends after 30-40 steps.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，循环会一直执行，直到没有代理存活。由于老虎和鹿都在随机行动（而且老虎每步都在失去健康），因此所有老虎很可能会因饥饿而死亡，而幸存的鹿将无限期地快乐生活下去。但环境被配置为在没有老虎的情况下自动移除所有鹿，因此我们的程序在30-40步后结束。
- en: 'At the end of the loop, we sum up the reward obtained by agents and track the
    amount of steps for which they were alive:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环结束时，我们将代理获得的奖励加总，并跟踪它们存活的步数：
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After the loop, we show the top 20 agents sorted by their reward:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 循环结束后，我们展示按奖励排序的前20个代理：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output from this tool might look like this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此工具的输出可能如下所示：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In my simulation, one agent (tiger_5) was especially lucky and lived longer
    than others. At the end, the program saves a video of the episode. The result
    of my run is available here: [https://youtube.com/shorts/pH-Rz9Q4yrI](https://youtube.com/shorts/pH-Rz9Q4yrI).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的模拟中，有一个代理（tiger_5）特别幸运，活得比其他的都久。最后，程序保存了这一回合的视频。我运行的结果可以在这里查看：[https://youtube.com/shorts/pH-Rz9Q4yrI](https://youtube.com/shorts/pH-Rz9Q4yrI)。
- en: 'In Figure [22.1](#x1-416097r1), two different states are shown: at the beginning
    of the game and close to the end. Tigers are shown with blue dots (or the darker
    ones if you’re reading this in grayscale), the red dots are deer, and the gray
    dots are walls (you can refer to the digital version of the book to see the colors
    in the screenshot). The attack direction is shown with small black arrows.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[22.1](#x1-416097r1)中，显示了两种不同的状态：游戏开始时和接近结束时。老虎用蓝点表示（如果你是用灰度模式查看，则是较深的蓝点），红点表示鹿，灰点表示墙壁（你可以参考书的数字版查看截图中的颜色）。攻击方向用小黑箭头表示。
- en: '![PIC](img/B22150_22_01.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_22_01.png)'
- en: 'Figure 22.1: Two states of the forest environment: at the beginning of the
    episode (left) and close to the end (right)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.1：森林环境的两种状态：在回合开始时（左侧）和接近结束时（右侧）
- en: In this example, both groups of agents were behaving randomly, which is not
    very interesting. In the next section, we’ll apply a deep Q-network (DQN) to improve
    the tiger’s hunting skills.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，两个代理组的行为是随机的，这并不特别有趣。在接下来的章节中，我们将应用深度Q网络（DQN）来提高老虎的狩猎技能。
- en: Deep Q-network for tigers
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 老虎的深度Q网络
- en: Here, we will apply the DQN model to the tiger group of agents to check whether
    they can learn how to hunt better. All of the agents share the network, so their
    behavior will be the same. The deer group will keep random behavior in this example
    to keep things simple for now; we’ll train them later in the chapter.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将应用DQN模型到老虎代理组，检查它们是否能学会更好地狩猎。所有代理共享网络，因此它们的行为将是相同的。鹿群将在本示例中保持随机行为，以便简化处理；我们将在本章后面训练它们。
- en: The training code can be found in Chapter22/forest_tigers_dqn.py; it doesn’t
    differ much from the other DQN versions from the previous chapters.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 训练代码可以在Chapter22/forest_tigers_dqn.py中找到；它与前几章的其他DQN版本没有太大区别。
- en: Understanding the code
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解代码
- en: To make the MAgent environment work with our classes, a specialized version
    of ExperienceSourceFirstLast was implemented to handle the specifics of the environment.
    This class is called MAgentExperienceSourceFirstLast and can be found in lib/data.py.
    Let’s check it out to understand how it fits into the rest of the code.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使MAgent环境与我们的类兼容，实施了一个专门版本的ExperienceSourceFirstLast来处理环境的特定情况。这个类被称为MAgentExperienceSourceFirstLast，可以在lib/data.py中找到。我们来查看一下它，了解它如何融入代码的其他部分。
- en: 'The first class we define is the items produced by our ExperienceSource. As
    we discussed in Chapter [7](ch011.xhtml#x1-1070007), instances of the class ExperienceFirstLast
    contain the following fields:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义的第一个类是由我们的ExperienceSource生成的条目。正如我们在第[7章](ch011.xhtml#x1-1070007)中讨论的，ExperienceFirstLast类的实例包含以下字段：
- en: 'state: Observations from the environment at the current step'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'state: 当前步骤中来自环境的观测数据'
- en: 'action: The action we executed'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'action: 我们执行的动作'
- en: 'reward: The amount of reward we obtained'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'reward: 我们获得的奖励数量'
- en: 'last_state: Observation after executing the action'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'last_state: 执行动作后的观测数据'
- en: 'In a multi-agent setup, every agent is producing the same set of data, but
    we also have to be able to identify which group this agent belongs to (in this
    tiger-deer example, does this experience correspond to the tiger or the deer’s
    trajectory?). To retain this information, we define a subclass, ExperienceFirstLastMARL,
    with a new field keeping the group’s name:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在多代理设置中，每个代理都会生成相同的数据集，但我们还需要能够识别该代理属于哪个组（在这个老虎-鹿的例子中，这个经验是属于老虎还是鹿的轨迹？）。为了保留这一信息，我们定义了一个子类ExperienceFirstLastMARL，并添加了一个新字段来保存组名：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the constructor of MAgentExperienceSourceFirstLast, we pass the following
    arguments:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在MAgentExperienceSourceFirstLast的构造函数中，我们传递了以下参数：
- en: 'magent_parallel_env: The MAgent parallel environment (we experimented with
    this in the previous section).'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'magent_parallel_env: MAgent并行环境（我们在前一节中进行了实验）。'
- en: 'agents_by_group: The PTAN BaseAgent object for every group of agents. In our
    tiger DQN example, tigers will be controlled by a neural network (ptan.agent.DQNAgent),
    but deer behave randomly.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'agents_by_group: 每个代理组的PTAN BaseAgent对象。在我们的老虎DQN示例中，老虎将由神经网络（ptan.agent.DQNAgent）控制，而鹿则是随机行为。'
- en: 'track_reward_group: The parameter that specifies the group for which we’re
    tracking the episode’s reward.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'track_reward_group: 指定我们要追踪的组的参数，该组用于追踪回合的奖励。'
- en: 'filter_group: An optional filter for the group for which we want to generate
    an experience. In our current example, we need only observations from tigers (as
    we train only tigers), but in the next section, we’ll train a DQN for both tigers
    and deer, so the filter will be disabled.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'filter_group: 可选的组过滤器，用于生成经验的组。在我们当前的示例中，我们只需要来自老虎的观测数据（因为我们只训练老虎），但在下一节中，我们将训练一个同时适用于老虎和鹿的DQN，因此该过滤器将被禁用。'
- en: 'In the subsequent constructor code, we store arguments and create two useful
    mappings for agents: from the agent ID to the group name and back:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在随后的构造函数代码中，我们存储参数并为代理创建两个有用的映射：从代理ID到组名，再从组名回到代理ID：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We also define a utility method to strip the agent’s numerical ID to get the
    group name (in our case, it will be tiger or deer).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个实用方法来提取代理的数字ID，从而获得组名（在我们的例子中，它将是老虎或鹿）。
- en: 'Now comes the main method of the class: the iterator interface, which produces
    experience items from the environment:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是该类的主要方法：迭代器接口，用于从环境中生成经验条目：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we reset the environment in the beginning and create initial states for
    agents (in case our agents will keep some state, but in this chapter’s examples,
    they are stateless).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，我们在开始时重置环境，并为代理创建初始状态（以防我们的代理会保持某些状态，但在本章的示例中，它们是无状态的）。
- en: 'Then, we iterate the episode until we have living agents (the same way we did
    in the example in the previous section). In this loop, we fill actions in the
    dictionary, mapping the agent ID to the action. To do this, we use PTAN BaseAgent
    instances, which work with batches of observations, so actions will be produced
    very efficiently for the whole group of agents:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们迭代本集，直到有存活的代理（就像我们在上一节的示例中所做的那样）。在这个循环中，我们将动作填充到字典中，将代理ID映射到动作。为此，我们使用PTAN
    BaseAgent实例，它们可以处理一批观察数据，因此可以非常高效地为整个代理组生成动作：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once we have actions to be executed, we send them to the environment and obtain
    dictionaries with new observations, rewards, and done and truncation flags. Then,
    we generate experience items for every alive agent we currently have:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了要执行的动作，我们将其发送到环境中，并获得包含新观察值、奖励、完成标志和截断标志的字典。然后，我们为每个当前存活的代理生成经验项：
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'At the end of the episode, we remember the number of steps and the average
    reward obtained by the group:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的结尾，我们记录了小组的步数和获得的平均奖励：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'With this class at hand, our DQN training code stays almost the same as in
    the single-agent RL case. The full source code of this example can be found in
    forest_tigers_dqn.py. Here, I’m going to show only part of the code where PTAN
    agents and the experience source are created (to illustrate how MAgentExperienceSourceFirstLast
    is being used):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个类，我们的DQN训练代码几乎与单代理RL情况相同。这个示例的完整源代码可以在`forest_tigers_dqn.py`中找到。在这里，我将仅展示一部分代码，展示如何创建PTAN代理和经验源（以说明`MAgentExperienceSourceFirstLast`是如何使用的）：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As you can see, tigers are controlled by the neural network (which is a very
    simple two-layer convolution plus a two-layer fully connected net). A group of
    deer is controlled by a random number generator. The experience replay buffer
    will be populated only with the tiger experience because of the filter_group="tiger"
    argument.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，老虎是由神经网络控制的（这个网络非常简单，由一个两层卷积层和一个两层全连接层组成）。一群鹿是由随机数生成器控制的。经验回放缓冲区只会填充老虎的经验，这是因为使用了`filter_group="tiger"`的参数。
- en: Training and results
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练与结果
- en: To start the training, run ./forest_tigers_dqn.py -n run_name --dev cuda. In
    one hour of training, the tiger’s test reward has reached the best score of 82,
    which is a significant improvement over the random baseline. Acting randomly,
    most tigers die after 20 steps and only a few lucky ones can live longer.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始训练，请运行`./forest_tigers_dqn.py -n run_name --dev cuda`。在一个小时的训练中，老虎的测试奖励达到了最佳得分82，相较于随机基准，取得了显著的提升。随机行动下，大多数老虎在20步后死亡，只有少数幸运的老虎能活得更久。
- en: Let’s calculate how many deer were eaten to get this score. Initially, each
    tiger has a health of 10 and spends 0.5 of their health on each step. In total,
    we have 40 tigers and 204 deer on the map (you can change this amount with command-line
    arguments). For every eaten deer, the tigers obtain 8 health points, which allows
    them to survive for an extra 16 steps. For every step, each tiger obtains a reward
    of 1, so the “excess reward” from the deer eaten by 40 tigers is 82 ⋅ 40 − 20
    ⋅ 40 = 2480\. Every deer gives 8 health points, which is converted into an extra
    16 steps of a tiger’s life, so the number of deer eaten is 2480∕16 = 155\. So,
    almost 76% of deer were hunted by the best policy we’ve got. Not bad given that
    deer are randomly placed on the map and tigers need to get to them to attack.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来计算为了获得这个分数，吃掉了多少只鹿。最初，每只老虎的生命值为10，每一步消耗0.5的生命值。总共有40只老虎和204只鹿在地图上（你可以通过命令行参数来改变这个数量）。每吃掉一只鹿，老虎获得8点生命值，使它们可以多存活16步。每步每只老虎获得1点奖励，所以40只老虎吃掉鹿所带来的“超额奖励”是82
    ⋅ 40 − 20 ⋅ 40 = 2480。每只鹿提供8点生命值，这转化为老虎多活16步，因此吃掉的鹿的数量是2480∕16 = 155。所以，差不多76%的鹿是被我们获得的最佳策略猎杀的。考虑到鹿是随机放置在地图上的，而老虎需要找到它们并进行攻击，这个成绩不错。
- en: It’s quite likely that the policy stopped improving just because of the limited
    view of the tigers. If you are curious, you can enable the minimap in the environment
    settings and experiment. With more information about the food’s location, it’s
    likely that the policy could be improved even more.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能是由于老虎视野有限，策略停止了改进。如果你感兴趣，可以在环境设置中启用小地图并进行实验。通过获得更多关于食物位置的信息，策略有可能得到进一步的改善。
- en: 'In Figure [22.2](#x1-419002r2), the average reward and number of steps during
    the training is shown. From it, you can see that the main growth was during the
    first 300 episodes and later, the training progress was almost 0:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[22.2](#x1-419002r2)中，显示了训练期间的平均奖励和步数。从中可以看出，主要的增长发生在前300个回合，而后来的训练进展几乎为0：
- en: '![PIC](img/B22150_22_02.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_22_02.png)'
- en: 'Figure 22.2: Average reward (left) and count of steps (right) from training
    episodes'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.2：训练回合中的平均奖励（左）和步数（右）
- en: 'However, Figure [22.3](#x1-419003r3) shows the plots for the test reward and
    steps, which demonstrate that the policy continued to improve even after 300 episodes
    (≈ 0.4 hours of training):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，图[22.3](#x1-419003r3)显示了测试奖励和步数的图表，表明即使在300个回合后（大约0.4小时的训练），策略仍在持续改进：
- en: '![PIC](img/B22150_22_03.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_22_03.png)'
- en: 'Figure 22.3: Average reward (left) and count of steps (right) from test episodes'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.3：测试回合的平均奖励（左）和步数（右）
- en: The final pair of charts in Figure [22.4](#x1-419005r4) shows the training loss
    and epsilon during the training. Both plots are correlated, which indicates that
    most of the novelty during the training was obtained in the exploration phase
    (as the loss value is high, which means that new situations arise during training).
    This might be an indication that better exploration methods might be beneficial
    for the final policy.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图[22.4](#x1-419005r4)中的最后一对图表显示了训练过程中的训练损失和epsilon。两个图表是相关的，这表明训练过程中大部分的新颖性是在探索阶段获得的（因为损失值较高，意味着在训练中会出现新情况）。这可能表明更好的探索方法可能对最终策略有所帮助。
- en: '![PIC](img/B22150_22_04.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_22_04.png)'
- en: 'Figure 22.4: Average loss (left) and epsilon (right) during the training'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.4：训练期间的平均损失（左）和epsilon（右）
- en: 'As usual, besides the training, I implemented a tool to check the models in
    action. It’s called forest_tigers_play.py and it loads the trained model and uses
    it during the episode, producing a video recording of the observations. The video
    from the best model (with a test score of 82.89) is available here: [https://www.youtube.com/shorts/ZZf80AHk538](https://www.youtube.com/shorts/ZZf80AHk538).
    As you can see, tigers’ hunting skills are now significantly better than the random
    policy: at the end of the episode, just 53 deer were left from the initial 204.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，除了训练外，我还实现了一个工具来检查模型的实际表现。它叫做forest_tigers_play.py，加载经过训练的模型并在回合中使用，生成观察视频记录。最佳模型的视频（测试得分82.89）可以在此观看：[https://www.youtube.com/shorts/ZZf80AHk538](https://www.youtube.com/shorts/ZZf80AHk538)。如您所见，老虎的狩猎技巧现在明显优于随机策略：在回合结束时，初始的204只鹿中只剩下53只。
- en: Collaboration by the tigers
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 老虎之间的合作
- en: The second experiment that I implemented was designed to make the tigers’ lives
    more complicated and encourage collaboration between them. The training and play
    code are the same; the only difference is in the MAgent environment’s configuration.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我实施的第二个实验旨在使老虎的生活变得更加复杂，并鼓励它们之间的合作。训练和游戏代码是相同的；唯一的区别在于MAgent环境的配置。
- en: 'If you pass the argument --mode double_attack to the training utility, the
    environment data.DoubleAttackEnv will be used. The only difference is the configuration
    object, which sets additional constraints on tigers’ attacks. In the new setup,
    they can attack deer only in pairs and have to do this at the same time. A single
    tiger’s attack doesn’t have any effect. This definitely complicates the training
    and hunting, as obtaining the reward from eating the deer is now much harder for
    tigers. To start the training, you can run the same train utility, but with an
    extra command-line argument:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将参数--mode double_attack传递给训练工具，则将使用环境数据.DoubleAttackEnv。唯一的区别是配置对象，它为老虎的攻击设置了额外的约束。在新的设置中，它们只能成对攻击鹿，并且必须同时进行。单只老虎的攻击没有任何效果。这无疑使训练和狩猎变得更加复杂，因为现在老虎从吃鹿中获得奖励变得更加困难。要开始训练，可以运行相同的训练工具，但需要额外的命令行参数：
- en: '[PRE20]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Let’s take a look at the results.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下结果。
- en: In Figure [22.5](#x1-420002r5), the reward and step plots for the training episodes
    are shown. As you can see, even after 2 hours of training, the reward is still
    improving. At the same time, the count of steps in the episode never exceeds 300,
    which might be an indication that tigers just don’t have nearby deer to eat and
    die from starvation (it also might just be an internal limit of steps in the environment).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[22.5](#x1-420002r5)中，显示了训练回合中的奖励和步骤图。正如你所见，即使经过 2 小时的训练，奖励仍在提高。同时，回合中的步骤数从未超过
    300，这可能表明老虎没有附近的鹿可以吃，最终因饥饿而死（也有可能只是环境中步骤的内部限制）。
- en: '![PIC](img/B22150_22_05.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_22_05.png)'
- en: 'Figure 22.5: Average reward (left) and count of steps (right) from training
    episodes in double_attack mode'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22.5：双重攻击模式下训练回合的平均奖励（左）和步骤计数（右）
- en: 'In contrast to single-tiger hunting mode, the loss during the training is not
    decreasing (as shown in Figure [22.6](#x1-420003r6)), which might indicate that
    training hyperparameters could be improved:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 与单只老虎狩猎模式相比，训练过程中的损失没有减少（如图[22.6](#x1-420003r6)所示），这可能表明训练超参数可以改进：
- en: '![PIC](img/B22150_22_06.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_22_06.png)'
- en: 'Figure 22.6: Average loss during training'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22.6：训练过程中的平均损失
- en: 'To test the model in action, you can use the same utility as before; just pass
    it the --mode double_attack argument. A video recording of the best model I got
    is available here: [https://youtu.be/VjGbzP1r7HY](https://youtu.be/VjGbzP1r7HY).
    As you can see, tigers are now moving in pairs, attacking the deer together.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试模型的实际效果，你可以使用之前相同的工具；只需传入 --mode double_attack 参数即可。我获得的最佳模型的视频记录可以在这里查看：[https://youtu.be/VjGbzP1r7HY](https://youtu.be/VjGbzP1r7HY)。正如你所见，老虎现在成双成对地移动，一起攻击鹿。
- en: Training both tigers and deer
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练老虎和鹿
- en: The next example is the scenario when both tigers and deer are controlled by
    different DQN models being trained simultaneously. Tigers are rewarded for living
    longer, which stimulates them to eat more deer, as at every step in the simulation,
    they lose health points. Deer are also rewarded on every timestamp.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例是老虎和鹿由不同的 DQN 模型控制并同时训练的场景。老虎因生存时间长而获得奖励，这刺激它们吃更多的鹿，因为在模拟的每一步中，它们都会失去健康点。鹿在每个时间戳也会获得奖励。
- en: The code is in forest_both_dqn.py and it is an extension of the previous example.
    For both groups of agents, we have a separate DQNAgent class instance, which uses
    separate neural networks to convert observations into actions. The experience
    source is the same, but now we’re not filtering on a tiger’s group experience
    (with the parameter filter_group=None). Because of this, our replay buffer now
    contains observations from all the agents in the environment, not just from tigers
    as in the previous example. During the training, we sample a batch and split examples
    from deer and tigers into two separate batches to be used for training their networks.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 代码位于 forest_both_dqn.py 中，是前一个示例的扩展。对于两组智能体，我们分别有一个 DQNAgent 类实例，它使用独立的神经网络将观察转化为动作。经验源是相同的，但现在我们不再过滤老虎的组经验（即参数
    filter_group=None）。因此，我们的回放缓冲区现在包含来自环境中所有智能体的观察，而不仅仅是像前一个示例那样来自老虎的观察。在训练过程中，我们会抽取一个批次，并将老虎和鹿的例子分成两个独立的批次，用于训练它们的网络。
- en: 'I’m not going to include all the code here, as it differs from the previous
    example only in small details. If you are curious, you can take a look at the
    source code in the GitHub repository. Figure [22.7](#x1-421001r7) shows the training
    reward and steps for tigers. You can see that initially, tigers were able to consistently
    increase their reward, but later, the growth stopped:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我这里不会包含所有代码，因为与前一个示例相比，差别仅在一些细节上。如果你感兴趣，可以查看 GitHub 仓库中的源代码。图[22.7](#x1-421001r7)显示了老虎的训练奖励和步骤。你可以看到，最初，老虎能够稳定地提高它们的奖励，但后来增长停止了：
- en: '![PIC](img/B22150_22_07.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_22_07.png)'
- en: 'Figure 22.7: Average reward for tigers (left) and count of steps (right) from
    training episodes'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22.7：老虎的平均奖励（左）和训练回合中的步骤计数（右）
- en: 'In the next two plots in Figure [22.8](#x1-421002r8), the reward for tigers
    and deer during the testing is shown. There is no clear trend here; both groups
    are competing and trying to beat their opponent:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[22.8](#x1-421002r8)的接下来的两个图中，显示了老虎和鹿在测试中的奖励。这里没有明确的趋势；两组都在竞争，试图击败对方：
- en: '![PIC](img/B22150_22_08.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_22_08.png)'
- en: 'Figure 22.8: Test reward for tigers (left) and deer (right)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22.8：老虎（左）和鹿（右）的测试奖励
- en: As you can see from Figure [22.8](#x1-421002r8), deer are way more successful
    than tigers, which is not surprising, as the speed of both is the same, so the
    deer just need to move all the time and wait for the tigers to die from starvation.
    If you want, you can experiment with the environment settings by increasing either
    the tigers’ speed or the wall density.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从图[22.8](#x1-421002r8)中看到的，鹿比老虎更成功，这并不令人惊讶，因为两者的速度相同，鹿只需要一直移动，等待老虎因饥饿而死亡。如果你愿意，可以通过提高老虎的速度或墙体密度来实验环境设置。
- en: 'As before, it is possible to visualize the learned policies with the utility
    forest_both_play.py, but now you need to pass two model files. Here is a video
    comparing the best model for deer and the best model for tigers: [https://youtube.com/shorts/vuVL1e26KqY](https://youtube.com/shorts/vuVL1e26KqY).
    In the video, all deer are just moving to the left of the field. Most likely,
    tigers can exploit this simple policy for their benefit.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，你可以使用utility工具forest_both_play.py来可视化学习到的策略，但现在需要传递两个模型文件。这里有一个视频，比较了鹿的最佳模型和老虎的最佳模型：[https://youtube.com/shorts/vuVL1e26KqY](https://youtube.com/shorts/vuVL1e26KqY)。在视频中，所有鹿都只是朝场地的左边移动。很可能，老虎能够利用这一简单的策略来为自己谋取利益。
- en: The battle environment
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 战斗环境
- en: Besides the tiger-deer environment, MAgent contains several other predefined
    configurations you can find in the magent2.builtin.config and magent2.environment
    packages. As a final example in this chapter, we’ll take a look at the “battle”
    configuration, where two groups of agents are fighting each other (without eating,
    thank goodness). Both agents have health points of 10 and every attack takes 2
    health points, so 5 consecutive attacks are required to get the reward for the
    agent.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 除了老虎-鹿环境外，MAgent还包含了其他一些预定义的配置，你可以在magent2.builtin.config和magent2.environment包中找到它们。本章的最后一个示例，我们将看看“战斗”配置，其中两组智能体互相对抗（幸好没有吃掉对方）。两个智能体的健康值都是10，每次攻击造成2点健康损失，因此需要5次连续攻击才能获得奖励。
- en: 'You can find the code in battel_dqn.py. In this setup, one group is behaving
    randomly and another is using the DQN to improve the policy. Training took two
    hours and the DQN was able to find a decent policy, but at the end, the training
    process diverged. In Figure [22.9](#x1-422002r9), the training and test reward
    plots are shown:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在battel_dqn.py中找到代码。在这个设置中，一组是随机行为，另一组使用DQN来改进策略。训练持续了两小时，DQN成功找到了一种不错的策略，但最终，训练过程出现了发散。在图[22.9](#x1-422002r9)中，展示了训练和测试的奖励图：
- en: '![PIC](img/B22150_22_09.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_22_09.png)'
- en: 'Figure 22.9: Average reward during training (left) and test (right) in the
    battle scenario'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.9：在战斗场景中的训练期间（左）和测试期间（右）平均奖励
- en: 'The video recording (produced by the tool battle_play.py) is available here:
    [https://youtube.com/shorts/ayfCa8xGY2k](https://youtube.com/shorts/ayfCa8xGY2k).
    The blue team is random and the red is controlled by the DQN.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 视频记录（由工具battle_play.py生成）可以在这里找到：[https://youtube.com/shorts/ayfCa8xGY2k](https://youtube.com/shorts/ayfCa8xGY2k)。蓝队是随机的，红队由DQN控制。
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we just touched a bit on the very interesting and dynamic field
    of MARL, which has several practical applications in trading simulation, communication
    networks, and others. There are lots of things that you can try on your own using
    the MAgent environment or other environments (like PySC2).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们仅简单介绍了一个非常有趣且充满活力的领域——多智能体强化学习（MARL），该领域在交易仿真、通信网络等方面有着多个实际应用。你可以在MAgent环境或其他环境（如PySC2）中自行尝试许多实验。
- en: My congratulations on reaching the end of the book! I hope that the book was
    useful and you enjoyed reading it as much as I enjoyed gathering the material
    and writing all the chapters. As a final word, I would like to wish you good luck
    in this exciting and dynamic area of RL. The domain is developing very rapidly,
    but with an understanding of the basics, it will become much simpler for you to
    keep track of the new developments and research in this field.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你读完了这本书！我希望这本书对你有所帮助，并且你像我在收集材料和撰写章节时一样，享受阅读它。最后，我希望你在这个令人兴奋且充满活力的强化学习领域好运。这个领域正在快速发展，但只要掌握了基础，追踪该领域的新进展和研究将变得更加简单。
- en: There are many very interesting topics left uncovered, such as partially observable
    Markov decision processes (where environment observations don’t fulfill the Markov
    property) or recent approaches to exploration, such as the count-based methods.
    There has been a lot of recent activity around multi-agent methods, where many
    agents need to learn how to coordinate to solve a common problem. I also haven’t
    mentioned the memory-based RL approach, where your agent can maintain some sort
    of memory to keep its knowledge and experience. A great deal of effort is being
    put into increasing the RL sample efficiency, which will ideally be close to human
    learning performance one day, but this is still a far-off goal at the moment.
    Of course, it’s not possible to cover the full domain in just one book, because
    new ideas appear almost every day. However, the goal of this book was to give
    you a practical foundation in the field, simplifying your own learning of the
    common methods.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多非常有趣的话题没有覆盖，例如部分可观察的马尔可夫决策过程（环境观察不满足马尔可夫性质）或近期的探索方法，比如基于计数的方法。最近围绕多智能体方法也有很多活动，其中多个智能体需要学习如何协调合作解决共同的问题。我也没有提到基于记忆的强化学习方法，在这种方法中，智能体可以维持某种记忆以保存其知识和经验。为了提高强化学习的样本效率，付出了大量努力，理想情况下，未来有一天它将接近人类的学习表现，但目前这仍是一个遥远的目标。当然，仅仅一本书是不可能覆盖完整领域的，因为几乎每天都有新思想出现。然而，本书的目标是为您提供该领域的实践基础，简化您对常用方法的学习。
- en: 'I’d like to end by quoting Volodymyr Mnih’s words from his talk, Recent Advances,
    Frontiers and Future of Deep RL, from the Deep RL Bootcamp, Berkeley, 2017, which
    are still very relevant: “The field of deep RL is very new and everything is still
    exciting. Literally, nothing is solved yet!”'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我想引用Volodymyr Mnih在2017年伯克利深度强化学习训练营上发表的《深度强化学习的最新进展、前沿与未来》演讲中的话，这些话仍然非常相关：“深度强化学习领域非常新，所有的东西都令人兴奋。字面上说，什么都还没有解决！”
- en: Leave a Review!
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 留下评论！
- en: Thank you for purchasing this book from Packt Publishing—we hope you enjoyed
    it! Your feedback is invaluable and helps us improve and grow. Once you’ve completed
    reading it, please take a moment to leave an [Amazon review](https://packt.link/r/1835882714);
    it will only take a minute, but it makes a big difference for readers like you.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您从Packt出版公司购买本书——我们希望您喜欢这本书！您的反馈对于我们来说非常宝贵，它帮助我们改进和成长。在您读完本书后，请花一点时间留下一个[亚马逊评论](https://packt.link/r/1835882714)，这只需几分钟，但对像您这样的读者来说意义重大。
- en: Scan the QR code below to receive a free ebook of your choice.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 扫描下面的二维码，领取您选择的免费电子书。
- en: '![PIC](img/file3.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file3.png)'
- en: '*https://packt.link/NzOWQ*'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*https://packt.link/NzOWQ*'
