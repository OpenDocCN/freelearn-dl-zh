- en: Recurrent Neural Network — RNN
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归神经网络 — RNN
- en: In [Chapter 3](4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml), *Deep Learning with
    ConvNets*, we learned about **convolutional neural networks** (CNN) and saw how
    they exploit the spatial geometry of their input. For example, CNNs apply convolution
    and pooling operations in one dimension for audio and text data along the time
    dimension, in two dimensions for images along the (height x width) dimensions
    and in three dimensions, for videos along the (height x width x time) dimensions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 3 章](4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml)《*卷积神经网络深度学习*》中，我们学习了**卷积神经网络**（CNN），并了解了它们如何利用输入的空间几何信息。例如，CNN
    在音频和文本数据的时间维度上应用一维卷积和池化操作，在图像的（高 x 宽）维度上应用二维卷积操作，在视频的（高 x 宽 x 时间）维度上应用三维卷积操作。
- en: In this chapter, we will learn about **recurrent neural networks** (**RNN**),
    a class of neural networks that exploit the sequential nature of their input.
    Such inputs could be text, speech, time series, and anything else where the occurrence
    of an element in the sequence is dependent on the elements that appeared before
    it. For example, the next word in the sentence *the dog...* is more likely to
    be *barks* than *car*, therefore, given such a sequence, an RNN is more likely
    to predict *barks* than *car*.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习**递归神经网络**（**RNN**），这是一类利用输入序列特性的神经网络。这些输入可以是文本、语音、时间序列或其他任何元素的顺序依赖于前面元素出现的场景。例如，句子
    *the dog...* 中下一个词更可能是 *barks* 而不是 *car*，因此，在这样的序列中，RNN 更有可能预测 *barks* 而不是 *car*。
- en: An RNN can be thought of as a graph of RNN cells, where each cell performs the
    same operation on every element in the sequence. RNNs are very flexible and have
    been used to solve problems such as speech recognition, language modeling, machine
    translation, sentiment analysis, and image captioning, to name a few. RNNs can
    be adapted to different types of problems by rearranging the way the cells are
    arranged in the graph. We will see some examples of these configurations and how
    they are used to solve specific problems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 可以被看作是一个 RNN 单元的图，其中每个单元对序列中的每个元素执行相同的操作。RNN 非常灵活，已经被用于解决语音识别、语言建模、机器翻译、情感分析、图像描述等问题。通过重新排列图中单元的排列方式，RNN
    可以适应不同类型的问题。我们将看到这些配置的一些示例，以及它们如何用于解决特定问题。
- en: 'We will also learn about a major limitation of the SimpleRNN cell, and how
    two variants of the SimpleRNN cell—**long short term memory** (**LSTM**) and **gated
    recurrent unit** (**GRU**)—overcome this limitation. Both LSTM and GRU are drop-in
    replacements for the SimpleRNN cell, so just replacing the RNN cell with one of
    these variants can often result in a major performance improvement in your network.
    While LSTM and GRU are not the only variants, it has been shown empirically (for
    more information refer to the articles: *An Empirical Exploration of Recurrent
    Network Architectures*, by R. Jozefowicz, W. Zaremba, and I. Sutskever, JMLR, 2015
    and *LSTM: A Search Space Odyssey*, by K. Greff, arXiv:1503.04069, 2015) that
    they are the best choices for most sequence problems.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还将学习简单 RNN 单元的一个主要局限性，以及两种简单 RNN 单元的变体——**长短期记忆**（**LSTM**）和**门控递归单元**（**GRU**）——如何克服这个局限性。LSTM
    和 GRU 都是 SimpleRNN 单元的替代品，因此，只需将 RNN 单元替换为这两种变体之一，通常就能显著提高网络的性能。虽然 LSTM 和 GRU
    不是唯一的变体，但通过实验证明（更多信息请参见文章：*递归网络架构的实证探索*，R. Jozefowicz、W. Zaremba 和 I. Sutskever，JMLR，2015
    年，以及 *LSTM: A Search Space Odyssey*，K. Greff，arXiv:1503.04069，2015 年），它们是大多数序列问题的最佳选择。'
- en: Finally, we will also learn about some tips to improve the performance of our
    RNNs and when and how to apply them.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还将学习一些提升 RNN 性能的技巧，以及何时和如何应用这些技巧。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: SimpleRNN cell
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SimpleRNN 单元
- en: Basic RNN implementation in Keras in generating text
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Keras 中生成文本的基本 RNN 实现
- en: RNN topologies
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN 拓扑结构
- en: LSTM, GRU, and other RNN variants
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM、GRU 和其他 RNN 变体
- en: SimpleRNN cells
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SimpleRNN 单元
- en: Traditional multilayer perceptron neural networks make the assumption that all
    inputs are independent of each other. This assumption breaks down in the case
    of sequence data. You have already seen the example in the previous section where
    the first two words in the sentence affect the third. The same idea is true of
    speech—if we are having a conversation in a noisy room, I can make reasonable
    guesses about a word I may not have understood based on the words I have heard
    so far. Time series data, such as stock prices or weather, also exhibit a dependence
    on past data, called the secular trend.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的多层感知机神经网络假设所有输入相互独立。但在序列数据的情况下，这一假设是无效的。你已经在上一节看到过例子，其中句子的前两个单词会影响第三个单词。这个想法在语音中也适用——如果我们在一个嘈杂的房间里交谈，我可以根据到目前为止听到的单词，对我可能没有听懂的单词做出合理的猜测。时间序列数据，例如股价或天气，也表现出对过去数据的依赖，这种依赖称为长期趋势。
- en: 'RNN cells incorporate this dependence by having a hidden state, or memory,
    that holds the essence of what has been seen so far. The value of the hidden state
    at any point in time is a function of the value of the hidden state at the previous
    time step and the value of the input at the current time step, that is:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: RNN单元通过拥有一个隐藏状态或记忆，来结合这种依赖关系，隐藏状态保存了到目前为止所看到的内容的精髓。任意时刻的隐藏状态值是前一个时间步隐藏状态值和当前时间步输入值的函数，即：
- en: '![](img/rnn-eq1.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/rnn-eq1.png)'
- en: '*h[t]* and *h[t-1]* are the values of the hidden states at the time steps *t*
    and *t-1* respectively, and *x[t]* is the value of the input at time *t*. Notice
    that the equation is recursive, that is, *h[t-1]* can be represented in terms
    of *h[t-2]* and *x[t-1]*, and so on, until the beginning of the sequence. This
    is how RNNs encode and incorporate information from arbitrarily long sequences.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*h[t]*和*h[t-1]*分别是时间步*t*和*t-1*的隐藏状态值，*x[t]*是时间*t*时的输入值。请注意，这个方程是递归的，也就是说，*h[t-1]*可以通过*h[t-2]*和*x[t-1]*来表示，依此类推，直到序列的开始。这就是RNN如何编码并整合来自任意长序列的信息。'
- en: 'We can also represent the RNN cell graphically as shown in the following diagram
    on the left. At time *t*, the cell has an input *x[t]* and an output *y[t]*. Part
    of the output *y[t]* (the hidden state *h[t]*) is fed back into the cell for use
    at a later time step *t+1*. Just as a traditional neural network''s parameters
    are contained in its weight matrix, the RNN''s parameters are defined by three
    weight matrices *U*, *V*, and *W*, corresponding to the input, output, and hidden
    state respectively:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将RNN单元通过图示表示，如左侧的图所示。在时间*t*时，单元有一个输入*x[t]*和一个输出*y[t]*。部分输出*y[t]*（即隐藏状态*h[t]*）被反馈回单元，在稍后的时间步*t+1*时使用。就像传统神经网络的参数包含在其权重矩阵中一样，RNN的参数由三个权重矩阵*U*、*V*和*W*定义，分别对应输入、输出和隐藏状态：
- en: '![](img/basic-rnn-1.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/basic-rnn-1.png)'
- en: Another way to look at an RNN to *unroll* it, as shown in the preceding diagram
    on the right. Unrolling means that we draw the network out for the complete sequence.
    The network shown here is a three-layer RNN, suitable for processing three element
    sequences. Notice that the weight matrices *U*, *V*, and *W* are shared across
    the steps. This is because we are applying the same operation on different inputs
    at each time step. Being able to share these weight vectors across all the time
    steps greatly reduces the number of parameters that the RNN needs to learn.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种看待RNN的方法是将其*展开*，如右侧前面图示所示。展开意味着我们为完整的序列绘制网络。这里显示的网络是一个三层RNN，适合处理三元素序列。请注意，权重矩阵*U*、*V*和*W*在各个步骤之间是共享的。这是因为我们在每个时间步上对不同的输入应用相同的操作。能够在所有时间步共享这些权重向量，极大地减少了RNN需要学习的参数数量。
- en: We can also describe the computations within an RNN in terms of equations. The
    internal state of the RNN at a time *t* is given by the value of the hidden vector
    *h[t]*, which is the sum of the product of the weight matrix *W* and the hidden
    state *h[t-1]* at time *t-1* and the product of the weight matrix *U* and the
    input *x[t]* at time *t*, passed through the *tanh* nonlinearity. The choice of
    *tanh* over other nonlinearities has to do with its second derivative decaying
    very slowly to zero. This keeps the gradients in the linear region of the activation
    function and helps combat the vanishing gradient problem. We will learn more about
    the vanishing gradient problem later in this chapter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以用方程来描述RNN中的计算。在时间*t*时，RNN的内部状态由隐藏向量*h[t]*的值给出，它是权重矩阵*W*与时间*t-1*时的隐藏状态*h[t-1]*的乘积，以及权重矩阵*U*与时间*t*的输入*x[t]*的乘积之和，经过*tanh*非线性激活函数处理。选择*tanh*而非其他非线性函数的原因在于它的二阶导数非常缓慢地趋近于零。这使得梯度保持在激活函数的线性区域，有助于解决梯度消失问题。我们将在本章稍后了解梯度消失问题。
- en: 'The output vector *y[t]* at time *t* is the product of the weight matrix *V*
    and the hidden state *h[t]*, with *softmax* applied to the product so the resulting
    vector is a set of output probabilities:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间*t*时，输出向量*y[t]*是权重矩阵*V*与隐藏状态*h[t]*的乘积，并对乘积应用*softmax*，使得结果向量成为一组输出概率：
- en: '![](img/rnn-eq2.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/rnn-eq2.png)'
- en: 'Keras provides the SimpleRNN (for more information refer to: [https://keras.io/layers/recurrent/](https://keras.io/layers/recurrent/))
    recurrent layer that incorporates all the logic we have seen so far, as well as
    the more advanced variants such as LSTM and GRU that we will see later in this
    chapter, so it is not strictly necessary to understand how they work in order
    to start building with them. However, an understanding of the structure and equations
    is helpful when you need to compose your own RNN to solve a given problem.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供了SimpleRNN（更多信息请参考：[https://keras.io/layers/recurrent/](https://keras.io/layers/recurrent/)）递归层，它包含了我们到目前为止所见的所有逻辑，以及更高级的变种，如LSTM和GRU，后者将在本章稍后介绍，因此不严格要求理解它们的工作原理就能开始使用它们。然而，理解其结构和方程在你需要自己构建RNN来解决特定问题时是非常有帮助的。
- en: SimpleRNN with Keras — generating text
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras的SimpleRNN — 生成文本
- en: RNNs have been used extensively by the **natural language processing** (**NLP**)
    community for various applications. One such application is building language
    models. A language model allows us to predict the probability of a word in a text
    given the previous words. Language models are important for various higher level
    tasks such as machine translation, spelling correction, and so on.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: RNN已被**自然语言处理**（**NLP**）领域广泛应用于各种任务。其中一个应用是构建语言模型。语言模型使我们能够预测给定前一个单词的情况下，文本中某个单词的概率。语言模型在机器翻译、拼写纠正等各类高级任务中至关重要。
- en: A side effect of the ability to predict the next word given previous words is
    a generative model that allows us to generate text by sampling from the output
    probabilities. In language modeling, our input is typically a sequence of words
    and the output is a sequence of predicted words. The training data used is existing
    unlabeled text, where we set the label *y[t]* at time *t* to be the input *x[t+1]*
    at time *t+1*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 能够根据前一个单词预测下一个单词的能力，带来了一个生成模型，让我们通过从输出概率中采样来生成文本。在语言建模中，我们的输入通常是一个单词序列，输出是一个预测的单词序列。使用的训练数据是现有的未标注文本，我们将时间*t*时的标签*y[t]*设置为时间*t+1*时的输入*x[t+1]*。
- en: For our first example of using Keras for building RNNs, we will train a character
    based language model on the text of *Alice in Wonderland* to predict the next
    character given 10 previous characters. We have chosen to build a character-based
    model here because it has a smaller vocabulary and trains quicker. The idea is
    the same as using a word-based language model, except we use characters instead
    of words. We will then use the trained model to generate some text in the same
    style.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们使用Keras构建RNN的第一个例子中，我们将训练一个基于字符的语言模型，使用《爱丽丝梦游仙境》这本书的文本，根据前10个字符预测下一个字符。我们选择构建一个基于字符的模型，因为它的词汇量较小，训练速度较快。其思想与使用基于单词的语言模型相同，只不过我们使用的是字符而非单词。然后，我们将使用训练好的模型生成一些相同风格的文本。
- en: 'First we import the necessary modules:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的模块：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We read our input text from the text of *Alice in Wonderland* on the Project
    Gutenberg website ([http://www.gutenberg.org/files/11/11-0.txt](http://www.gutenberg.org/files/11/11-0.txt)).
    The file contains line breaks and non-ASCII characters, so we do some preliminary
    cleanup and write out the contents into a variable called `text`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从《爱丽丝梦游仙境》中的文本读取输入文本，该文本位于Project Gutenberg网站上（[http://www.gutenberg.org/files/11/11-0.txt](http://www.gutenberg.org/files/11/11-0.txt)）。该文件包含换行符和非ASCII字符，因此我们做了一些初步清理，并将内容写入一个名为`text`的变量中：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Since we are building a character-level RNN, our vocabulary is the set of characters
    that occur in the text. There are 42 of them in our case. Since we will be dealing
    with the indexes to these characters rather than the characters themselves, the
    following code snippet creates the necessary lookup tables:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们构建的是字符级RNN，因此我们的词汇表是文本中出现的字符集合。在我们的例子中有42个字符。由于我们将处理这些字符的索引而不是字符本身，以下代码片段创建了必要的查找表：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The next step is to create the input and label texts. We do this by stepping
    through the text by a number of characters given by the `STEP` variable (`1` in
    our case) and then extracting a span of text whose size is determined by the `SEQLEN`
    variable (`10` in our case). The next character after the span is our label character:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建输入文本和标签文本。我们通过由`STEP`变量（在我们的例子中是`1`）给定的字符数遍历文本，然后提取一个由`SEQLEN`变量（在我们的例子中是`10`）决定大小的文本片段。片段之后的下一个字符就是我们的标签字符：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Using the preceding code, the input and label texts for the text `it turned
    into a pig` would look like this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的代码，输入文本和标签文本对于文本`it turned into a pig`将如下所示：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The next step is to vectorize these input and label texts. Each row of the
    input to the RNN corresponds to one of the input texts shown previously. There
    are `SEQLEN` characters in this input, and since our vocabulary size is given
    by `nb_chars`, we represent each input character as a one-hot encoded vector of
    size (`nb_chars`). Thus each input row is a tensor of size (`SEQLEN` and `nb_chars`).
    Our output label is a single character, so similar to the way we represent each
    character of our input, it is represented as a one-hot vector of size (`nb_chars`).
    Thus, the shape of each label is `nb_chars`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将这些输入文本和标签文本向量化。RNN的每一行输入都对应前面展示的其中一行输入文本。输入中有`SEQLEN`个字符，并且由于我们的词汇表大小由`nb_chars`给定，我们将每个输入字符表示为大小为(`nb_chars`)的独热编码向量。因此，每一行输入是一个大小为(`SEQLEN`和`nb_chars`)的张量。我们的输出标签是一个单一字符，因此与我们表示输入中每个字符的方式类似，它表示为大小为(`nb_chars`)的独热向量。因此，每个标签的形状为`nb_chars`：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Finally, we are ready to build our model. We define the RNN's output dimension
    to have a size of 128\. This is a hyper-parameter that needs to be determined
    by experimentation. In general, if we choose too small a size, then the model
    does not have sufficient capacity for generating good text, and you will see long
    runs of repeating characters or runs of repeating word groups. On the other hand,
    if the value chosen is too large, the model has too many parameters and needs
    a lot more data to train effectively. We want to return a single character as
    output, not a sequence of characters, so `return_sequences=False`. We have already
    seen that the input to the RNN is of shape (`SEQLEN` and `nb_chars`). In addition,
    we set `unroll=True` because it improves performance on the TensorFlow backend.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备好构建模型。我们将RNN的输出维度定义为128。这是一个需要通过实验确定的超参数。一般来说，如果选择的大小过小，模型将没有足够的能力生成良好的文本，并且你将看到长时间重复的字符或重复的词组。另一方面，如果选择的值过大，模型将有过多的参数，并且需要更多的数据来有效地训练。我们希望返回一个单一的字符作为输出，而不是一串字符，因此设置`return_sequences=False`。我们已经看到RNN的输入形状是(`SEQLEN`和`nb_chars`)。此外，我们设置`unroll=True`，因为它可以提高TensorFlow后端的性能。
- en: 'The RNN is connected to a dense (fully connected) layer. The dense layer has
    (`nb_char`) units, which emits scores for each of the characters in the vocabulary.
    The activation on the dense layer is a softmax, which normalizes the scores to
    probabilities. The character with the highest probability is chosen as the prediction.
    We compile the model with the categorical cross-entropy loss function, a good
    loss function for categorical outputs, and the RMSprop optimizer:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: RNN连接到一个密集（全连接）层。密集层有(`nb_char`)个单元，输出词汇表中每个字符的分数。密集层的激活函数是softmax，它将分数归一化为概率。具有最高概率的字符被选为预测结果。我们使用类别交叉熵损失函数来编译模型，这是一种适用于分类输出的良好损失函数，并使用RMSprop优化器：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Our training approach is a little different from what we have seen so far. So
    far our approach has been to train a model for a fixed number of epochs, then
    evaluate it against a portion of held-out test data. Since we don't have any labeled
    data here, we train the model for an epoch (`NUM_EPOCHS_PER_ITERATION=1`) then
    test it. We continue training like this for 25 (`NUM_ITERATIONS=25`) iterations,
    stopping once we see intelligible output. So effectively, we are training for
    `NUM_ITERATIONS` epochs and testing the model after each epoch.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练方法与之前所见的有所不同。到目前为止，我们的方法是训练模型固定次数的迭代次数，然后在一部分保留的测试数据上进行评估。由于我们这里没有任何标注数据，我们训练模型进行一次迭代（`NUM_EPOCHS_PER_ITERATION=1`），然后进行测试。我们以这种方式继续训练
    25 次（`NUM_ITERATIONS=25`），直到看到可理解的输出为止。所以，实际上，我们是训练 `NUM_ITERATIONS` 次迭代，每次迭代后测试模型。
- en: 'Our test consists of generating a character from the model given a random input,
    then dropping the first character from the input and appending the predicted character
    from our previous run, and generating another character from the model. We continue
    this 100 times (`NUM_PREDS_PER_EPOCH=100`) and generate and print the resulting
    string. The string gives us an indication of the quality of the model:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试包括从模型中生成一个字符，给定一个随机输入，然后去掉输入中的第一个字符，并附加我们上一轮预测的字符，再从模型中生成另一个字符。我们重复这个过程
    100 次（`NUM_PREDS_PER_EPOCH=100`），并生成和打印出结果字符串。这个字符串能为我们提供模型质量的指示：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output of this run is shown as follows. As you can see, the model starts
    out predicting gibberish, but by the end of the 25th epoch, it has learned to
    spell reasonably well, although it has trouble expressing coherent thoughts. The
    amazing thing about this model is that it is character-based and has no knowledge
    of words, yet it learns to spell words that look like they might have come from
    the original text:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 运行的输出如下所示。如你所见，模型一开始预测的是胡言乱语，但在第 25 次迭代结束时，它已经学会了相对准确地拼写单词，尽管它在表达连贯的思想方面仍然存在问题。这个模型的惊人之处在于它是基于字符的，并且不理解单词的含义，然而它还是学会了拼写出看起来像是原始文本的单词：
- en: '![](img/ss-6-1.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ss-6-1.png)'
- en: 'Generating the next character or next word of text is not the only thing you
    can do with this sort of model. This kind of model has been successfully used
    to make stock predictions (for more information refer to the article: *Financial
    Market Time Series Prediction with Recurrent Neural Networks*, by A. Bernal, S.
    Fok, and R. Pidaparthi, 2012) and generate classical music (for more information
    refer to the article: *DeepBach: A Steerable Model for Bach Chorales Generation*,
    by G. Hadjeres and F. Pachet, arXiv:1612.01010, 2016), to name a few interesting
    applications. Andrej Karpathy covers a few other fun examples, such as generating
    fake Wikipedia pages, algebraic geometry proofs, and Linux source code in his
    blog post at: *The Unreasonable Effectiveness of Recurrent Neural Networks* at
    [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '生成下一个字符或单词并不是你可以用这种模型做的唯一事情。这种模型已经成功地用于股市预测（更多信息请参见文章：*Financial Market Time
    Series Prediction with Recurrent Neural Networks*，A. Bernal、S. Fok 和 R. Pidaparthi，2012），以及生成古典音乐（更多信息请参见文章：*DeepBach:
    A Steerable Model for Bach Chorales Generation*，G. Hadjeres 和 F. Pachet，arXiv:1612.01010，2016），这些只是一些有趣的应用实例。Andrej
    Karpathy 在他的博客文章中涵盖了一些其他有趣的示例，比如生成假维基百科页面、代数几何证明和 Linux 源代码，文章标题为：*The Unreasonable
    Effectiveness of Recurrent Neural Networks*，网址为：[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)。'
- en: The source code for this example is available in `alice_chargen_rnn.py` in the
    code download for the chapter. The data is available from Project Gutenberg.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的源代码可以在章节的代码下载中找到，文件名为`alice_chargen_rnn.py`。数据可以从古腾堡计划获取。
- en: RNN topologies
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 拓扑结构
- en: The APIs for MLP and CNN architectures are limited. Both architectures accept
    a fixed-size tensor as input and produce a fixed-size tensor as output; and they
    perform the transformation from input to output in a fixed number of steps given
    by the number of layers in the model. RNNs don't have this limitation—you can
    have sequences in the input, the output, or both. This means that RNNs can be
    arranged in many ways to solve specific problems.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: MLP 和 CNN 架构的 API 是有限的。两种架构都接受一个固定大小的张量作为输入，并输出一个固定大小的张量；它们将在模型的层数所决定的固定步骤数中执行从输入到输出的转换。RNN
    没有这个限制——你可以在输入、输出或两者中都有序列。这意味着 RNN 可以以多种方式排列，以解决特定问题。
- en: As we have learned, RNNs combine the input vector with the previous state vector
    to produce a new state vector. This can be thought of as similar to running a
    program with some inputs and some internal variables. Thus RNNs can be thought
    of as essentially describing computer programs. In fact, it has been shown that
    RNNs are turing complete (for more information refer to the article: *On the Computational
    Power of Neural Nets*, by H. T. Siegelmann and E. D. Sontag, proceedings of the
    fifth annual workshop on computational learning theory, ACM, 1992.) in the sense
    that given the proper weights, they can simulate arbitrary programs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所学，RNN将输入向量与前一状态向量结合，生成一个新的状态向量。这可以类比为运行一个带有输入和一些内部变量的程序。因此，RNN可以被看作是本质上描述计算机程序。事实上，已有研究表明，RNN是图灵完备的（更多信息请参考文章：*On
    the Computational Power of Neural Nets*，H. T. Siegelmann 和 E. D. Sontag，计算学习理论第五届年度研讨会论文集，ACM，1992），即在适当的权重下，它们能够模拟任意程序。
- en: 'This property of being able to work with sequences gives rise to a number of
    common topologies, some of which we''ll discuss, as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这种能够处理序列的特性产生了许多常见的拓扑结构，接下来我们将讨论其中的一些：
- en: '![](img/rnn-toplogies.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/rnn-toplogies.png)'
- en: All these different topologies derive from the same basic structure shown in
    the preceding diagram. In this basic topology, all input sequences are of the
    same length and an output is produced at each time step. We have already seen
    an example of this with our character level RNN for generating words in *Alice
    in Wonderland*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些不同的拓扑结构都来源于前面图示的相同基本结构。在这个基本拓扑结构中，所有的输入序列长度相同，并且每个时间步都会生成一个输出。我们已经通过字符级别的RNN生成《爱丽丝梦游仙境》中的单词，看到过这样的例子。
- en: Another example of a many to many RNN could be a machine translation network
    shown as **(b)**, part of a general family of networks called sequence-to-sequence
    (for more information refer to: *Grammar as a Foreign Language*, by O. Vinyals,
    Advances in Neural Information Processing Systems, 2015). These take in a sequence
    and produces another sequence. In the case of machine translation, the input could
    be a sequence of English words in a sentence and the output could be the words
    in a translated Spanish sentence. In the case of a model that uses sequence-to-sequence
    to do **part-of-speech** (**POS**) tagging, the input could be the words in a
    sentence and the output could be the corresponding POS tags. It differs from the
    previous topology in that at certain time steps there is no input and at others
    there is no output. We will see an example of such a network later in this chapter.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个多对多的RNN例子是如**(b)**所示的机器翻译网络，它是序列到序列（sequence-to-sequence）网络家族的一部分（更多信息请参考：*Grammar
    as a Foreign Language*，O. Vinyals，神经信息处理系统进展，2015）。这些网络接受一个序列并生成另一个序列。在机器翻译的情况下，输入可以是一个句子中的英文单词序列，输出则是翻译后的西班牙语句子中的单词。在使用序列到序列模型进行**词性标注**（**POS**）的情况下，输入可以是一个句子中的单词，输出则是相应的词性标签。与之前的拓扑结构不同的是，在某些时间步没有输入，在其他时间步没有输出。我们将在本章后面看到这种网络的例子。
- en: 'Other variants are the one-to-many network shown as **(c)**, an example of
    which could be an image captioning network (for more information refer to the
    article: *Deep Visual-Semantic Alignments for Generating Image Descriptions*,
    by A. Karpathy, and F. Li, Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, 2015.), where the input is an image and the output a
    sequence of words.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其他变体是如**(c)**所示的单对多网络，一个例子是图像描述生成网络（更多信息请参考文章：*Deep Visual-Semantic Alignments
    for Generating Image Descriptions*，A. Karpathy 和 F. Li，计算机视觉与模式识别IEEE会议论文集，2015），其中输入是图像，输出是一个单词序列。
- en: 'Similarly, an example of a many-to-one network as shown in **(d)** could be
    a network that does sentiment analysis of sentences, where the input is a sequence
    of words and the output is a positive or negative sentiment (for more information
    refer to the article: *Recursive Deep Models for Semantic Compositionality over
    a Sentiment Treebank*, by R. Socher, Proceedings of the Conference on Empirical
    Methods in Natural Language Processing (EMNLP). Vol. 1631, 2013). We will see
    an (much simplified compared to the cited model) example of this topology as well
    later in the chapter.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，图**(d)**所示的一个多对一网络示例可能是一个句子情感分析网络，其中输入是一个单词序列，输出是一个正面或负面的情感（欲了解更多信息，请参考文章：*递归深度模型用于情感树库上的语义组合性*，作者：R.
    Socher，《自然语言处理经验方法会议论文集（EMNLP）》第1631卷，2013年）。我们将在本章后面看到该拓扑结构的一个（相比于引用的模型，简化了许多）示例。
- en: Vanishing and exploding gradients
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度消失和爆炸
- en: 'Just like traditional neural networks, training the RNN also involves backpropagation.
    The difference in this case is that since the parameters are shared by all time
    steps, the gradient at each output depends not only on the current time step,
    but also on the previous ones. This process is called **backpropagation through
    time** (**BPTT**) (for more information refer to the article: *Learning Internal
    Representations by Backpropagating errors*, by G. E. Hinton, D. E. Rumelhart,
    and R. J. Williams, Parallel Distributed Processing: Explorations in the Microstructure
    of Cognition 1, 1985):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 就像传统神经网络一样，训练RNN也涉及反向传播。不同之处在于，由于所有时间步共享参数，因此每个输出的梯度不仅依赖于当前时间步，还依赖于之前的时间步。这个过程叫做**时间反向传播**（**BPTT**）（欲了解更多信息，请参考文章：*通过反向传播错误学习内部表示*，作者：G.
    E. Hinton、D. E. Rumelhart 和 R. J. Williams，《平行分布处理：认知微观结构的探索1》，1985年）：
- en: '![](img/rnn-bptt.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/rnn-bptt.png)'
- en: Consider the small three layer RNN shown in the preceding diagram. During the
    forward propagation (shown by the solid lines), the network produces predictions
    that are compared to the labels to compute a loss *L[t]* at each time step. During
    backpropagation (shown by dotted lines), the gradients of the loss with respect
    to the parameters *U*, *V*, and *W* are computed at each time step and the parameters
    are updated with the sum of the gradients.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑前面图示的小型三层RNN。在前向传播过程中（通过实线表示），网络产生的预测结果与标签进行比较，以计算每个时间步的损失*L[t]*。在反向传播过程中（通过虚线表示），计算损失相对于参数*U*、*V*和*W*的梯度，并通过梯度之和更新参数。
- en: 'The following equation shows the gradient of the loss with respect to *W*,
    the matrix that encodes weights for the long term dependencies. We focus on this
    part of the update because it is the cause of the vanishing and exploding gradient
    problem. The other two gradients of the loss with respect to the matrices *U*
    and *V* are also summed up across all time steps in a similar way:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方程显示了损失相对于*W*的梯度，这个矩阵编码了长期依赖的权重。我们专注于这一部分的更新，因为它是梯度消失和爆炸问题的根源。损失相对于矩阵*U*和*V*的另外两个梯度也以类似的方式在所有时间步中求和：
- en: '![](img/bptt-eq1.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bptt-eq1.png)'
- en: 'Let us now look at what happens to the gradient of the loss at the last time
    step (*t=3*). As you can see, this gradient can be decomposed to a product of
    three sub gradients using the chain rule. The gradient of the hidden state *h2*
    with respect to *W* can be further decomposed as the sum of the gradient of each
    hidden state with respect to the previous one. Finally, each gradient of the hidden
    state with respect to the previous one can be further decomposed as the product
    of gradients of the current hidden state against the previous one:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看在最后一个时间步（*t=3*）时损失的梯度会发生什么。正如你所看到的，这个梯度可以通过链式法则分解为三个子梯度的乘积。隐藏状态*h2*相对于*W*的梯度可以进一步分解为每个隐藏状态相对于前一个隐藏状态的梯度之和。最后，每个隐藏状态相对于前一个隐藏状态的梯度可以进一步分解为当前隐藏状态与前一个隐藏状态的梯度的乘积：
- en: '![](img/bptt-eq2.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bptt-eq2.png)'
- en: Similar calculations are done to compute the gradient of losses *L[1]* and *L[2]*
    (at time steps 1 and 2) with respect to *W* and to sum them into the gradient
    update for *W*. We will not explore the math further in this book. If you want
    to do so on your own, this WILDML blog post ([https://goo.gl/l06lbX](https://goo.gl/l06lbX))
    has a very good explanation of BPTT, including more detailed derivations of the
    mathematics behind the process.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的计算也用于计算损失*L[1]*和*L[2]*（在时间步1和2时）相对于*W*的梯度，并将它们汇总为*W*的梯度更新。我们在本书中不会进一步探讨数学内容。如果你想自己研究，WILDML博客文章（[https://goo.gl/l06lbX](https://goo.gl/l06lbX)）对BPTT有很好的解释，包括更详细的数学推导过程。
- en: For our purposes, the final form of the gradient in the equation above tells
    us why RNNs have the problem of vanishing and exploding gradients. Consider the
    case where the individual gradients of a hidden state with respect to the previous
    one is less than one. As we backpropagate across multiple time steps, the product
    of gradients get smaller and smaller, leading to the problem of vanishing gradients.
    Similarly, if the gradients are larger than one, the products get larger and larger,
    leading to the problem of exploding gradients.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，上面方程中梯度的最终形式告诉我们为什么RNN会有消失梯度和爆炸梯度的问题。考虑一下隐藏状态相对于前一个隐藏状态的单独梯度小于1的情况。当我们在多个时间步进行反向传播时，梯度的乘积变得越来越小，导致消失梯度问题。类似地，如果梯度大于1，乘积会变得越来越大，导致爆炸梯度问题。
- en: The effect of vanishing gradients is that the gradients from steps that are
    far away do not contribute anything to the learning process, so the RNN ends up
    not learning long range dependencies. Vanishing gradients can happen for traditional
    neural networks as well, it is just more visible in case of RNNs, since RNNs tend
    to have many more layers (time steps) over which back propagation must occur.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 消失梯度的影响是，远离的步骤产生的梯度对学习过程没有任何贡献，因此RNN最终无法学习长距离的依赖关系。消失梯度问题也会发生在传统神经网络中，只是由于RNN通常有更多的层（时间步），反向传播必须在这些层之间发生，所以这个问题在RNN中更加明显。
- en: Exploding gradients are more easily detectable, the gradients will become very
    large and then turn into **not a number** (**NaN**) and the training process will
    crash. Exploding gradients can be controlled by clipping them at a predefined
    threshold as discussed in the paper: *On the Difficulty of Training Recurrent
    Neural Networks*, by R. Pascanu, T. Mikolov, and Y. Bengio, ICML, Pp 1310-1318,
    2013.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 爆炸梯度更容易被检测到，梯度会变得非常大，然后变成**非数值**（**NaN**），训练过程会崩溃。爆炸梯度问题可以通过将梯度裁剪到预定义的阈值来控制，正如论文《*On
    the Difficulty of Training Recurrent Neural Networks*》中所讨论的，作者为R. Pascanu、T. Mikolov和Y.
    Bengio，ICML，Pp 1310-1318，2013年。
- en: While there are a few approaches to minimize the problem of vanishing gradients,
    such as proper initialization of the *W* matrix, using a ReLU instead of *tanh*
    layers, and pre-training the layers using unsupervised methods, the most popular
    solution is to use the LSTM or GRU architectures. These architectures have been
    designed to deal with the vanishing gradient problem and learn long term dependencies
    more effectively. We will learn more about LSTM and GRU architectures later in
    this chapter.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有几种方法可以最小化消失梯度问题，比如适当初始化*W*矩阵，使用ReLU代替*tanh*层，以及使用无监督方法预训练层，最流行的解决方案是使用LSTM或GRU架构。这些架构被设计用来处理消失梯度问题，并更有效地学习长期依赖关系。我们将在本章后面深入了解LSTM和GRU架构。
- en: Long short term memory — LSTM
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期记忆（Long short term memory — LSTM）
- en: The LSTM is a variant of RNN that is capable of learning long term dependencies.
    LSTMs were first proposed by Hochreiter and Schmidhuber and refined by many other
    researchers. They work well on a large variety of problems and are the most widely
    used type of RNN.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM是RNN的一种变种，能够学习长期依赖关系。LSTM最早由Hochreiter和Schmidhuber提出，并经过许多其他研究者的改进。它们在许多问题中表现良好，是最广泛使用的RNN类型。
- en: 'We have seen how the SimpleRNN uses the hidden state from the previous time
    step and the current input in a *tanh* layer to implement recurrence. LSTMs also
    implement recurrence in a similar way, but instead of a single *tanh* layer, there
    are four layers interacting in a very specific way. The following diagram illustrates
    the transformations that are applied to the hidden state at time step *t*:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到SimpleRNN如何使用前一时间步的隐藏状态和当前输入通过*tanh*层来实现递归。LSTM也以类似的方式实现递归，但它们并非通过单一的*tanh*层，而是通过四个层在非常特定的方式下相互作用。以下图示说明了在时间步*t*对隐藏状态所应用的变换：
- en: '![](img/lstm-cell.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/lstm-cell.png)'
- en: The diagram looks complicated, but let us look at it component by component.
    The line across the top of the diagram is the cell state *c*, and represents the
    internal memory of the unit. The line across the bottom is the hidden state, and
    the *i*, *f*, *o*, and *g* gates are the mechanism by which the LSTM works around
    the vanishing gradient problem. During training, the LSTM learns the parameters
    for these gates.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图表看起来很复杂，但我们可以逐个组件地查看它。图表上方的线代表细胞状态 *c*，表示单元的内部记忆。底部的线是隐藏状态，*i*、*f*、*o* 和 *g*
    门是LSTM绕过梯度消失问题的机制。在训练过程中，LSTM会学习这些门的参数。
- en: 'In order to gain a deeper understanding of how these gates modulate the LSTM''s
    hidden state, let us consider the equations that show how it calculates the hidden
    state *h[t]* at time *t* from the hidden state *h[t-1]* at the previous time step:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入理解这些门如何调节LSTM的隐藏状态，我们可以考虑以下方程式，展示如何根据上一时间步的隐藏状态 *h[t-1]* 计算当前时间步 *t* 的隐藏状态
    *h[t]*：
- en: '![](img/lstm-eq1.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/lstm-eq1.png)'
- en: Here *i*, *f*, and *o* are the input, forget, and output gates. They are computed
    using the same equations but with different parameter matrices. The sigmoid function
    modulates the output of these gates between zero and one, so the output vector
    produced can be multiplied element-wise with another vector to define how much
    of the second vector can pass through the first one.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 *i*、*f* 和 *o* 分别是输入门、遗忘门和输出门。它们使用相同的方程计算，但有不同的参数矩阵。sigmoid函数调节这些门的输出，使其在0到1之间，因此输出向量可以与另一个向量逐元素相乘，从而定义第二个向量可以通过第一个向量的程度。
- en: The forget gate defines how much of the previous state *h[t-1]* you want to
    allow to pass through. The input gate defines how much of the newly computed state
    for the current input *x[t]* you want to let through, and the output gate defines
    how much of the internal state you want to expose to the next layer. The internal
    hidden state *g* is computed based on the current input *x[t]* and the previous
    hidden state *h[t-1]*. Notice that the equation for *g* is identical to that for
    the SimpleRNN cell, but in this case we will modulate the output by the output
    of the input gate *i*.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 遗忘门定义了你希望通过多少前一状态 *h[t-1]*。输入门定义了你希望让多少当前输入 *x[t]* 的新计算状态通过，输出门定义了你希望暴露多少内部状态到下一层。内部隐藏状态
    *g* 是基于当前输入 *x[t]* 和前一隐藏状态 *h[t-1]* 计算的。注意，*g* 的方程与SimpleRNN单元的方程是相同的，但在这种情况下，我们将通过输入门
    *i* 的输出来调节输出。
- en: Given *i*, *f*, *o*, and *g*, we can now calculate the cell state *c[t]* at
    time *t* in terms of *c[t-1]* at time (*t-1*) multiplied by the forget gate and
    the state *g* multiplied by the input gate *i*. So this is basically a way to
    combine the previous memory and the new input—setting the forget gate to *0* ignores
    the old memory and setting the input gate to *0* ignores the newly computed state.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 *i*、*f*、*o* 和 *g*，我们现在可以计算当前时间步 *t* 的细胞状态 *c[t]*，其计算方式是 *c[t-1]*（上一时间步的状态）与遗忘门的乘积，再加上
    *g*（当前状态）与输入门 *i* 的乘积。因此，这基本上是将以前的记忆与新的输入结合的一种方式——将遗忘门设为 *0* 会忽略旧的记忆，而将输入门设为 *0*
    会忽略新计算的状态。
- en: Finally, the hidden state *h[t]* at time *t* is computed by multiplying the
    memory *c[t]* with the output gate.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当前时间步 *t* 的隐藏状态 *h[t]* 通过将记忆 *c[t]* 与输出门相乘来计算。
- en: One thing to realize is that an LSTM is a drop-in replacement for a SimpleRNN
    cell, the only difference is that LSTMs are resistant to the vanishing gradient
    problem. You can replace an RNN cell in a network with an LSTM without worrying
    about any side effects. You should generally see better results along with longer
    training times.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，LSTM是SimpleRNN单元的直接替代，唯一的区别是LSTM对梯度消失问题具有抵抗力。你可以将网络中的RNN单元替换为LSTM，而无需担心任何副作用。通常情况下，你会看到更好的结果，但训练时间更长。
- en: 'If you would like to know more, WILDML blog post has a very detailed explanation
    of these LSTM gates and how they work. For a more visual explanation, take a look
    at Christopher Olah''s blog post: *Understanding LSTMs* ([http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))
    where he walks you step by step through these computations, with illustrations
    at each step.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多内容，WILDML博客有一篇关于这些LSTM门及其工作原理的详细解释。想要更直观的讲解，可以看看Christopher Olah的博客文章：*理解LSTM*
    ([http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))，他一步一步地带你走过这些计算，并在每个步骤中附有插图。
- en: LSTM with Keras — sentiment analysis
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM与Keras—情感分析
- en: Keras provides an LSTM layer that we will use here to construct and train a
    many-to-one RNN. Our network takes in a sentence (a sequence of words) and outputs
    a sentiment value (positive or negative). Our training set is a dataset of about
    7,000 short sentences from UMICH SI650 sentiment classification competition on
    Kaggle ([https://inclass.kaggle.com/c/si650winter11](https://inclass.kaggle.com/c/si650winter11)).
    Each sentence is labeled *1* or *0* for positive or negative sentiment respectively,
    which our network will learn to predict.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供了一个 LSTM 层，我们将在这里使用它来构建和训练一个多对一的 RNN。我们的网络输入一个句子（一系列单词），输出一个情感值（正面或负面）。我们的训练集来自
    Kaggle 上 UMICH SI650 情感分类比赛的数据集，包含约 7,000 个短句子 ([https://inclass.kaggle.com/c/si650winter11](https://inclass.kaggle.com/c/si650winter11))。每个句子会被标记为
    *1* 或 *0*，分别表示正面或负面情感，网络将学习预测这些情感标签。
- en: 'We start with the imports, as usual:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像往常一样从导入库开始：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Before we start, we want to do a bit of exploratory analysis on the data. Specifically
    we need to know how many unique words there are in the corpus and how many words
    are there in each sentence:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们希望对数据进行一些探索性分析。具体来说，我们需要了解语料库中有多少个唯一单词，以及每个句子中有多少个单词：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Using this, we get the following estimates for our corpus:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些信息，我们得到如下关于语料库的估算：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Using the number of unique words `len(word_freqs)`, we set our vocabulary size
    to a fixed number and treat all the other words as **out of vocabulary** (**OOV**)
    words and replace them with the pseudo-word UNK (for unknown). At prediction time,
    this will allow us to handle previously unseen words as OOV words as well.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用唯一单词的数量 `len(word_freqs)`，我们将词汇表大小设置为一个固定的数字，并将所有其他单词视为 **词汇表外** (**OOV**)
    单词，并用伪词 UNK（表示未知）替换它们。在预测时，这将允许我们将以前未见过的单词处理为 OOV 单词。
- en: 'The number of words in the sentence (`maxlen`) allows us to set a fixed sequence
    length and zero pad shorter sentences and truncate longer sentences to that length
    as appropriate. Even though RNNs handle variable sequence length, this is usually
    achieved either by padding and truncating as above, or by grouping the inputs
    in different batches by sequence length. We will use the former approach here.
    For the latter approach, Keras recommends using batches of size one (for more
    information refer to: [https://github.com/fchollet/keras/issues/40](https://github.com/fchollet/keras/issues/40)).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 句子中的单词数量 (`maxlen`) 允许我们设置固定的序列长度，并将较短的句子进行零填充，将较长的句子截断为该长度。尽管 RNN 能处理可变长度的序列，通常的做法是通过像上述方法一样填充和截断，或者根据序列长度将输入分成不同的批次。我们将在这里使用前者。对于后者，Keras
    建议使用大小为 1 的批次（更多信息请参考：[https://github.com/fchollet/keras/issues/40](https://github.com/fchollet/keras/issues/40)）。
- en: 'Based on the preceding estimates, we set our `VOCABULARY_SIZE` to `2002`. This
    is 2,000 words from our vocabulary plus the UNK pseudo-word and the PAD pseudo
    word (used for padding sentences to a fixed number of words), in our case 40 given
    by `MAX_SENTENCE_LENGTH`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的估算，我们将 `VOCABULARY_SIZE` 设置为 `2002`。这包括我们词汇表中的 2,000 个单词，加上 UNK 伪单词和 PAD
    伪单词（用于将句子填充到固定的单词数量），在我们的情况下是由 `MAX_SENTENCE_LENGTH` 给定的 40。
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next we need a pair of lookup tables. Each row of input to the RNN is a sequence
    of word indices, where the indices are ordered by most frequent to least frequent
    word in the training set. The two lookup tables allow us to lookup an index given
    the word and the word given the index. This includes the `PAD` and `UNK` pseudo-words
    as well:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一对查找表。每一行输入到 RNN 的都是一个单词索引序列，索引按训练集中单词的出现频率从高到低排序。两个查找表使我们能够根据单词查找索引，或根据索引查找单词。这也包括
    `PAD` 和 `UNK` 伪单词：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we convert our input sentences to word index sequences, pad them to the
    `MAX_SENTENCE_LENGTH` words. Since our output label in this case is binary (positive
    or negative sentiment), we don''t need to process the labels:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将输入的句子转换为单词索引序列，并将它们填充到 `MAX_SENTENCE_LENGTH` 个单词。由于我们的输出标签是二元的（正面或负面情感），我们不需要处理标签：
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we split the training set into a 80-20 training test split:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将训练集划分为 80-20 的训练集和测试集：
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following diagram shows the structure of our RNN:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了我们 RNN 的结构：
- en: '![](img/sentiment-lstm.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/sentiment-lstm.png)'
- en: The input for each row is a sequence of word indices. The sequence length is
    given by `MAX_SENTENCE_LENGTH`. The first dimension of the tensor is set to `None`
    to indicate that the batch size (the number of records fed to the network each
    time) is currently unknown at definition time; it is specified during run time
    using the `batch_size` parameter. So assuming an as-yet undetermined batch size,
    the shape of the input tensor is `(None, MAX_SENTENCE_LENGTH, 1)`. These tensors
    are fed into an embedding layer of size `EMBEDDING_SIZE` whose weights are initialized
    with small random values and learned during training. This layer will transform
    the tensor to a shape `(None,MAX_SENTENCE_LENGTH, EMBEDDING_SIZE)`. The output
    of the embedding layer is fed into an LSTM with sequence length `MAX_SENTENCE_LENGTH`
    and output layer size `HIDDEN_LAYER_SIZE`, so the output of the LSTM is a tensor
    of shape `(None, HIDDEN_LAYER_SIZE, MAX_SENTENCE_LENGTH)`. By default, the LSTM
    will output a single tensor of shape `(None, HIDDEN_LAYER_SIZE)` at its last sequence
    (`return_sequences=False`). This is fed to a dense layer with output size of `1`
    with a sigmoid activation function, so it will output either `0` (negative review)
    or `1` (positive review).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行的输入是一个单词索引序列。序列的长度由`MAX_SENTENCE_LENGTH`给定。张量的第一维被设置为`None`，表示批次大小（每次输入到网络中的记录数）在定义时尚不可知；它将在运行时通过`batch_size`参数指定。因此，假设批次大小尚未确定，输入张量的形状是`(None,
    MAX_SENTENCE_LENGTH, 1)`。这些张量被输入到一个嵌入层，大小为`EMBEDDING_SIZE`，其权重以小的随机值初始化，并在训练期间学习。这个层会将张量转换为形状`(None,
    MAX_SENTENCE_LENGTH, EMBEDDING_SIZE)`。嵌入层的输出被输入到一个LSTM中，序列长度为`MAX_SENTENCE_LENGTH`，输出层大小为`HIDDEN_LAYER_SIZE`，因此LSTM的输出是一个形状为`(None,
    HIDDEN_LAYER_SIZE, MAX_SENTENCE_LENGTH)`的张量。默认情况下，LSTM将在最后一个序列中输出一个形状为`(None,
    HIDDEN_LAYER_SIZE)`的张量（`return_sequences=False`）。这个输出被输入到一个输出大小为`1`的全连接层，并使用Sigmoid激活函数，因此它将输出`0`（负面评价）或`1`（正面评价）。
- en: 'We compile the model using the binary cross-entropy loss function since it
    predicts a binary value, and the Adam optimizer, a good general purpose optimizer.
    Note that the hyperparameters `EMBEDDING_SIZE`, `HIDDEN_LAYER_SIZE`, `BATCH_SIZE`
    and `NUM_EPOCHS` (set as constants as follows) were tuned experimentally over
    several runs:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用二进制交叉熵损失函数来编译模型，因为它预测的是二进制值，并且使用Adam优化器，这是一种良好的通用优化器。请注意，超参数`EMBEDDING_SIZE`、`HIDDEN_LAYER_SIZE`、`BATCH_SIZE`和`NUM_EPOCHS`（如下所示设置为常量）是在多次运行中通过实验调优的：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We then train the network for `10` epochs (`NUM_EPOCHS`) and batch size of
    `32` (`BATCH_SIZE`). At each epoch we validate the model using the test data:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将网络训练`10`个周期（`NUM_EPOCHS`），每个批次大小为`32`（`BATCH_SIZE`）。在每个周期，我们使用测试数据验证模型：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output of this step shows how the loss decreases and accuracy increases
    over multiple epochs:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步的输出显示了在多个周期中损失值如何下降，准确率如何上升：
- en: '![](img/ss-6-2.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ss-6-2.png)'
- en: 'We can also plot the loss and accuracy values over time using the following
    code:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用以下代码绘制损失值和准确率值随时间的变化：
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output of the preceding example is as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例的输出如下：
- en: '![](img/umich-lossplot.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/umich-lossplot.png)'
- en: 'Finally, we evaluate our model against the full test set and print the score
    and accuracy. We also pick a few random sentences from our test set and print
    the RNN''s prediction, the label and the actual sentence:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们对模型进行全测试集评估，并打印分数和准确率。我们还从测试集中随机挑选了几个句子，并打印RNN的预测结果、标签和实际句子：
- en: '[PRE18]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As you can see from the results, we get back close to 99% accuracy. The predictions
    the model makes for this particular set match exactly with the labels, although
    this is not the case for all predictions:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中可以看出，我们得到了接近99%的准确率。模型对这一特定数据集的预测完全与标签匹配，尽管并非所有预测都如此：
- en: '![](img/ss-6-3.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ss-6-3.png)'
- en: If you would like to run this code locally, you need to get the data from the
    Kaggle website.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在本地运行此代码，你需要从Kaggle网站获取数据。
- en: The source code for this example is available in the file `umich_sentiment_lstm.py`
    in the code download for this chapter.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的源代码可以在本章的代码下载中找到文件`umich_sentiment_lstm.py`。
- en: Gated recurrent unit — GRU
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 门控递归单元 — GRU
- en: 'The GRU is a variant of the LSTM and was introduced by K. Cho (for more information
    refer to: *Learning Phrase Representations using RNN Encoder-Decoder for Statistical
    Machine Translation*, by K. Cho, arXiv:1406.1078, 2014). It retains the LSTM''s
    resistance to the vanishing gradient problem, but its internal structure is simpler,
    and therefore is faster to train, since fewer computations are needed to make
    updates to its hidden state. The gates for a GRU cell are illustrated in the following
    diagram:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: GRU 是 LSTM 的一种变体，由 K. Cho 提出（更多信息请参考：*使用 RNN 编码器-解码器进行统计机器翻译的短语表示学习*，K. Cho，arXiv:1406.1078，2014）。它保留了
    LSTM 对梯度消失问题的抗性，但其内部结构更为简单，因此训练速度更快，因为更新其隐藏状态所需的计算较少。GRU 单元的门控机制如下面的图所示：
- en: '![](img/gru-cell.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/gru-cell.png)'
- en: Instead of the input, forget, and output gates in the LSTM cell, the GRU cell
    has two gates,
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 与 LSTM 单元的输入、遗忘和输出门不同，GRU 单元只有两个门，
- en: 'an update gate *z*, and a reset gate r. The update gate defines how much previous
    memory to keep around and the reset gate defines how to combine the new input
    with the previous memory. There is no persistent cell state distinct from the
    hidden state as in LSTM. The following equations define the gating mechanism in
    a GRU:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更新门 *z* 和一个重置门 r。更新门定义了保留多少前一段记忆，而重置门定义了如何将新输入与前一段记忆结合。与 LSTM 不同，这里没有与隐藏状态区分开的持久性单元状态。以下方程定义了
    GRU 中的门控机制：
- en: '![](img/gru-eq1.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/gru-eq1.png)'
- en: 'According to several empirical evaluations (for more information refer to the
    articles: *An Empirical Exploration of Recurrent Network Architectures*, by R.
    Jozefowicz, W. Zaremba, and I. Sutskever, JMLR, 2015 and *Empirical Evaluation
    of Gated Recurrent Neural Networks on Sequence Modeling*, by J. Chung, arXiv:1412.3555\.
    2014), GRU and LSTM have comparable performance and there is no simple way to
    recommend one or the other for a specific task. While GRUs are faster to train
    and need less data to generalize, in situations where there is enough data, an
    LSTM''s greater expressive power may lead to better results. Like LSTMs, GRUs
    are drop-in replacements for the SimpleRNN cell.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 根据几项实证评估（更多信息请参考文章：*递归网络架构的实证探索*，R. Jozefowicz，W. Zaremba，I. Sutskever，JMLR，2015
    和 *门控递归神经网络在序列建模中的实证评估*，J. Chung，arXiv:1412.3555，2014），GRU 和 LSTM 的性能相当，并且没有简单的方法来为特定任务推荐其中之一。尽管
    GRU 的训练速度更快且需要更少的数据来进行泛化，但在数据充足的情况下，LSTM 更强的表达能力可能会导致更好的结果。像 LSTM 一样，GRU 可以作为
    SimpleRNN 单元的替代品。
- en: Keras provides built in implementations of both `LSTM` and `GRU`, as well as
    the `SimpleRNN` class we saw earlier.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供了 `LSTM` 和 `GRU` 的内置实现，以及我们之前看到的 `SimpleRNN` 类。
- en: GRU with Keras — POS tagging
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras 中的 GRU — 词性标注
- en: Keras provides a GRU implementation, that we will use here to build a network
    that does POS tagging. A POS is a grammatical category of words that are used
    in the same way across multiple sentences. Examples of POS are nouns, verbs, adjectives,
    and so on. For example, nouns are typically used to identify things, verbs are
    typically used to identify what they do, and adjectives to describe some attribute
    of these things. POS tagging used to be done manually, but nowadays this is done
    automatically using statistical models. In recent years, deep learning has been
    applied to this problem as well (for more information refer to the article: *Natural
    Language Processing (almost) from Scratch*, by R. Collobert, Journal of Machine
    Learning Research, Pp. 2493-2537, 2011).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供了 GRU 的实现，我们将在这里使用它来构建一个进行词性标注（POS tagging）的网络。POS 是在多个句子中以相同方式使用的单词的语法类别。POS
    的例子有名词、动词、形容词等等。例如，名词通常用于标识事物，动词通常用于标识它们的动作，形容词则用于描述这些事物的某些属性。词性标注曾经是手动完成的，但现在通常使用统计模型自动完成。近年来，深度学习也被应用于这一问题（更多信息请参考文章：*几乎从零开始的自然语言处理*，R.
    Collobert，机器学习研究期刊，第 2493-2537 页，2011）。
- en: For our training data, we will need sentences tagged with part of speech tags.
    The Penn Treebank ([https://catalog.ldc.upenn.edu/ldc99t42](https://catalog.ldc.upenn.edu/ldc99t42))
    is one such dataset, it is a human annotated corpus of about 4.5 million words
    of American English. However, it is a non-free resource. A 10% sample of the Penn
    Treebank is freely available as part of the NLTK ([http://www.nltk.org/](http://www.nltk.org/)),
    which we will use to train our network.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的训练数据，我们需要带有词性标签的句子。Penn Treebank（[https://catalog.ldc.upenn.edu/ldc99t42](https://catalog.ldc.upenn.edu/ldc99t42)）就是一个这样的数据集，它是一个人工注释的约450万词的美式英语语料库。然而，它是一个非免费的资源。Penn
    Treebank的10%样本可以作为NLTK的一部分免费获取（[http://www.nltk.org/](http://www.nltk.org/)），我们将使用它来训练我们的网络。
- en: Our model will take in a sequence of words in a sentence and output the corresponding
    POS tags for each word. Thus for an input sequence consisting of the words [*The*,
    *cat*, *sat*, *on*, *the*, *mat*, *.*], the output sequence emitted would be the
    POS symbols [*DT*, *NN*, *VB*, *IN*, *DT*, *NN*].
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型将接收一个句子的单词序列，并输出每个单词的相应POS标签。因此，对于输入序列[*The*, *cat*, *sat*, *on*, *the*,
    *mat*, *.*]，输出的序列将是POS符号[*DT*, *NN*, *VB*, *IN*, *DT*, *NN*]。
- en: 'We start with the imports:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从导入开始：
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We then download the data from NLTK in a format suitable for our downstream
    code. Specifically, the data is available in parsed form as part of the NLTK Treebank
    corpus. We use the following Python code to download this data into two parallel
    files, one for the words in the sentences and one for the POS tags:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们从NLTK下载适合我们下游代码的数据格式。具体来说，这些数据作为NLTK Treebank语料库的一部分以解析的形式提供。我们使用以下Python代码将这些数据下载到两个并行文件中，一个存储句子中的单词，另一个存储POS标签：
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Once again, we want to explore the data a little to find out what vocabulary
    size to set. This time, we have to consider two different vocabularies, the source
    vocabulary for the words and the target vocabulary for the POS tags. We need to
    find the number of unique words in each vocabulary. We also need to find the maximum
    number of words in a sentence in our training corpus and the number of records.
    Because of the one-to-one nature of POS tagging, the last two values are identical
    for both vocabularies:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们想稍微探索一下数据，找出应该设置的词汇表大小。这次，我们需要考虑两个不同的词汇表，一个是单词的源词汇表，另一个是POS标签的目标词汇表。我们需要找出每个词汇表中唯一单词的数量。我们还需要找出训练语料库中每个句子的最大单词数和记录的数量。由于POS标注的独特一对一性质，最后两个值对于两个词汇表是相同的：
- en: '[PRE21]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Running this code tells us that there are 10,947 unique words and 45 unique
    POS tags. The maximum sentence size is 249, and the number of sentences in the
    10% set is 3,914\. Using this information, we decide to consider only the top
    5,000 words for our source vocabulary. Our target vocabulary has 45 unique POS
    tags, we want to be able to predict all of them, so we will consider all of them
    in our vocabulary. Finally, we set 250 to be our maximum sequence length:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码告诉我们，有10,947个独特的单词和45个独特的POS标签。最大句子长度是249，而10%数据集中的句子数量为3,914。根据这些信息，我们决定仅考虑源词汇表中的前5,000个单词。我们的目标词汇表有45个独特的POS标签，我们希望能够预测所有这些标签，因此我们会将所有这些标签考虑进词汇表。最后，我们将250设置为最大序列长度：
- en: '[PRE22]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Just like our sentiment analysis example, each row of the input will be represented
    as a sequence of word indices. The corresponding output will be a sequence of
    POS tag indices. So we need to build lookup tables to translate between the words/POS
    tags and their corresponding indices. Here is the code to do that. On the source
    side, we build a vocabulary index with two extra slots to hold the `PAD` and `UNK`
    pseudo-words. On the target side, we don''t drop any words so there is no need
    for the `UNK` pseudo-word:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们的情感分析示例一样，输入的每一行将表示为单词索引的序列。对应的输出将是POS标签索引的序列。因此，我们需要构建查找表，以便在单词/POS标签与其相应的索引之间进行转换。以下是实现该功能的代码。在源端，我们构建了一个词汇索引，并增加了两个额外的槽位来存储`PAD`和`UNK`伪单词。在目标端，我们不会丢弃任何单词，因此不需要`UNK`伪单词：
- en: '[PRE23]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The next step is to build our datasets to feed into our network. We will use
    these lookup tables to convert our input sentences into a word ID sequence of
    length `MAX_SEQLEN` (`250`). The labels need to be structured as a sequence of
    one-hot vectors of size `T_MAX_FEATURES` + 1 (`46`), also of length `MAX_SEQLEN`
    (`250`). The `build_tensor` function reads the data from the two files and converts
    them to the input and output tensors. Additional default parameters are passed
    in to build the output tensor. This triggers the call to `np_utils.to_categorical()`
    to convert the output sequence of POS tag IDs to one-hot vector representation:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是构建我们的数据集，将其输入到我们的网络中。我们将使用这些查找表将输入的句子转换为长度为`MAX_SEQLEN`（`250`）的词ID序列。标签需要被构建为一个大小为`T_MAX_FEATURES`
    + 1（`46`）的独热向量序列，长度也为`MAX_SEQLEN`（`250`）。`build_tensor`函数从两个文件读取数据并将其转换为输入和输出张量。额外的默认参数被传递进来构建输出张量。这会触发对`np_utils.to_categorical()`的调用，将输出序列的POS标签ID转换为独热向量表示：
- en: '[PRE24]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can then split the dataset into a 80-20 train-test split:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以将数据集分割为80-20的训练集和测试集：
- en: '[PRE25]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following figure shows the schematic of our network. It looks complicated,
    so let us deconstruct it:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了我们网络的示意图。它看起来很复杂，让我们来逐步解析：
- en: '![](img/postag-gru.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/postag-gru.png)'
- en: As previously, assuming that the batch size is as yet undetermined, the input
    to the network is a tensor of word IDs of shape `(None, MAX_SEQLEN, 1)`. This
    is sent through an embedding layer, which converts each word into a dense vector
    of shape (`EMBED_SIZE`), so the output tensor from this layer has the shape `(None,
    MAX_SEQLEN, EMBED_SIZE)`. This tensor is fed to the encoder GRU with an output
    size of `HIDDEN_SIZE`. The GRU is set to return a single context vector (`return_sequences=False`)
    after seeing a sequence of size `MAX_SEQLEN`, so the output tensor from the GRU
    layer has shape `(None, HIDDEN_SIZE)`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，假设批处理大小尚未确定，网络的输入是一个形状为`(None, MAX_SEQLEN, 1)`的词ID张量。该张量通过一个嵌入层，该层将每个词转换为形状为`(EMBED_SIZE)`的稠密向量，因此该层输出的张量形状为`(None,
    MAX_SEQLEN, EMBED_SIZE)`。该张量被送入编码器GRU，其输出大小为`HIDDEN_SIZE`。GRU被设置为在看到大小为`MAX_SEQLEN`的序列后返回一个单一的上下文向量（`return_sequences=False`），因此GRU层输出的张量形状为`(None,
    HIDDEN_SIZE)`。
- en: This context vector is then replicated using the RepeatVector layer into a tensor
    of shape `(None, MAX_SEQLEN, HIDDEN_SIZE)` and fed into the decoder GRU layer.
    This is then fed into a dense layer which produces an output tensor of shape `(None,
    MAX_SEQLEN, t_vocab_size)`. The activation function on the dense layer is a softmax.
    The argmax of each column of this tensor is the index of the predicted POS tag
    for the word at that position.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这个上下文向量通过RepeatVector层复制成形状为`(None, MAX_SEQLEN, HIDDEN_SIZE)`的张量，并送入解码器GRU层。接着该张量进入一个稠密层，输出形状为`(None,
    MAX_SEQLEN, t_vocab_size)`。稠密层的激活函数是softmax。该张量每一列的argmax值就是该位置单词预测的POS标签的索引。
- en: 'The model definition is shown as follows: `EMBED_SIZE`, `HIDDEN_SIZE`, `BATCH_SIZE`,
    and `NUM_EPOCHS` are hyperparameters which have been assigned these values after
    experimenting with multiple different values. The model is compiled with the `categorical_crossentropy`
    loss function since we have multiple categories of labels, and the optimizer used
    is the popular `adam` optimizer:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 模型定义如下所示：`EMBED_SIZE`、`HIDDEN_SIZE`、`BATCH_SIZE`和`NUM_EPOCHS`是超参数，这些值是在尝试多个不同值后确定的。由于我们有多个类别的标签，模型使用`categorical_crossentropy`作为损失函数，并且使用流行的`adam`优化器：
- en: '[PRE26]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We train this model for a single epoch. The model is very rich, with many parameters,
    and begins to overfit after the first epoch of training. When fed the same data
    multiple times in the next epochs, the model begins to overfit to the training
    data and does worse on the validation data:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练这个模型进行一个周期。该模型非常复杂，包含许多参数，并且在第一次训练后开始出现过拟合。在接下来的训练周期中，如果将相同的数据输入模型多次，模型开始对训练数据过拟合，并且在验证数据上的表现变差：
- en: '[PRE27]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output of the training and the evaluation is shown as follows. As you can
    see, the model does quite well after the first epoch of training:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和评估的输出如下所示。如你所见，模型在第一次训练后表现得相当不错：
- en: '![](img/ss-6-4.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ss-6-4.png)'
- en: 'Similar to actual RNNs, the three recurrent classes in Keras (`SimpleRNN`,
    `LSTM`, and `GRU`) are interchangeable. To demonstrate, we simply replace all
    occurrences of `GRU` in the previous program with `LSTM` and rerun the program.
    The model definition and the import statements are the only things that change:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 与实际的 RNN 类似，Keras 中的三种递归类（`SimpleRNN`、`LSTM` 和 `GRU`）是可以互换的。为了演示，我们只需将之前程序中的所有
    `GRU` 替换为 `LSTM`，然后重新运行程序。模型定义和导入语句是唯一改变的部分：
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: As you can see from the output, the results of the GRU-based network are quite
    comparable to our previous LSTM-based network.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出结果可以看出，基于 GRU 的网络结果与我们之前的基于 LSTM 的网络相当接近。
- en: 'Sequence-to-sequence models are a very powerful class of model. Its most canonical
    application is machine translation, but there are many others such as the previous
    example. Indeed, a lot of NLP tasks further up in the hierarchy, such as named
    entity recognition (for more information refer to the article: *Named Entity Recognition
    with Long Short Term Memory*, by J. Hammerton, Proceedings of the Seventh Conference
    on Natural Language Learning at HLT-NAACL, Association for Computational Linguistics,
    2003) and sentence parsing (for more information refer to the article: *Grammar
    as a Foreign Language*, by O. Vinyals, Advances in Neural Information Processing
    Systems, 2015), as well as more complex networks such as those for image captioning
    (for more information refer to the article: *Deep Visual-Semantic Alignments for
    Generating Image Descriptions*, by A. Karpathy, and F. Li, Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, 2015.), are examples of
    the sequence-to-sequence compositional model.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列模型是一类非常强大的模型。其最典型的应用是机器翻译，但还有许多其他应用，比如之前的例子。实际上，许多 NLP 任务在更高的层次上，诸如命名实体识别（更多信息请参考文章：*命名实体识别与长短期记忆*，J.
    Hammerton 著，发表于《第七届自然语言学习会议》，HLT-NAACL，计算语言学会，2003年）和句子解析（更多信息请参考文章：*语法作为外语*，O.
    Vinyals 著，发表于《神经信息处理系统进展》，2015年），以及更复杂的网络，如图像描述生成（更多信息请参考文章：*生成图像描述的深度视觉-语义对齐*，A.
    Karpathy 和 F. Li 著，发表于《IEEE计算机视觉与模式识别会议论文集》，2015年），都可以视为序列到序列组合模型的例子。
- en: The full code for this example can be found in the file `pos_tagging_gru.py`
    in the the code download for this chapter.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的完整代码可以在本章节的代码下载文件 `pos_tagging_gru.py` 中找到。
- en: Bidirectional RNNs
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双向 RNN
- en: At a given time step *t*, the output of the RNN is dependent on the outputs
    at all previous time steps. However, it is entirely possible that the output is
    also dependent on the future outputs as well. This is especially true for applications
    such as NLP, where the attributes of the word or phrase we are trying to predict
    may be dependent on the context given by the entire enclosing sentence, not just
    the words that came before it. Bidirectional RNNs also help a network architecture
    place equal emphasis on the beginning and end of the sequence, and increase the
    data available for training.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的时间步 *t*，RNN 的输出依赖于所有之前时间步的输出。然而，输出也完全可能依赖于未来的输出。这对于 NLP 等应用尤其重要，在这些应用中，我们尝试预测的单词或短语的属性可能依赖于整个封闭句子的上下文，而不仅仅是其前面的单词。双向
    RNN 还帮助网络架构对序列的开头和结尾给予同等的重视，并增加了可用于训练的数据量。
- en: Bidirectional RNNs are two RNNs stacked on top of each other, reading the input
    in opposite directions. So in our example, one RNN will read the words left to
    right and the other RNN will read the words right to left. The output at each
    time step will be based on the hidden state of both RNNs.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 双向 RNN 是两个 RNN 堆叠在一起，分别以相反的方向读取输入。因此，在我们的例子中，一个 RNN 将从左到右读取单词，另一个 RNN 将从右到左读取单词。每个时间步的输出将基于两个
    RNN 的隐藏状态。
- en: 'Keras provides support for bidirectional RNNs through a bidirectional wrapper
    layer. For example, for our POS tagging example, we could make our LSTMs bidirectional
    simply by wrapping them with this Bidirectional wrapper, as shown in the model
    definition code as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 通过双向包装层提供对双向 RNN 的支持。例如，在我们的词性标注示例中，我们只需使用这个双向包装层将 LSTM 转换为双向 RNN，就像以下的模型定义代码所示：
- en: '[PRE29]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This gives us performance comparable to the unidirectional LSTM example shown
    as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们得到了与下面展示的单向 LSTM 示例相媲美的性能：
- en: '![](img/ss-6-6.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ss-6-6.png)'
- en: Stateful RNNs
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有状态的 RNN
- en: RNNs can be stateful, which means that they can maintain state across batches
    during training. That is, the hidden state computed for a batch of training data
    will be used as the initial hidden state for the next batch of training data.
    However, this needs to be explicitly set, since Keras RNNs are stateless by default
    and resets the state after each batch. Setting an RNN to be stateful means that
    it can build a state across its training sequence and even maintain that state
    when doing predictions.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: RNN（循环神经网络）可以是有状态的，这意味着它们可以在训练过程中跨批次保持状态。也就是说，对于一批训练数据计算出的隐藏状态将作为下一批训练数据的初始隐藏状态。然而，这需要显式设置，因为Keras的RNN默认是无状态的，并且会在每个批次之后重置状态。将RNN设置为有状态意味着它可以在其训练序列中建立状态，甚至在进行预测时保持该状态。
- en: The benefits of using stateful RNNs are smaller network sizes and/or lower training
    times. The disadvantage is that we are now responsible for training the network
    with a batch size that reflects the periodicity of the data, and resetting the
    state after each epoch. In addition, data should not be shuffled while training
    the network, since the order in which the data is presented is relevant for stateful
    networks.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使用有状态RNN的好处是网络大小较小和/或训练时间较短。缺点是我们现在需要负责使用反映数据周期性的批次大小来训练网络，并在每个周期后重置状态。此外，在训练网络时数据不应被打乱，因为数据呈现的顺序对于有状态网络是相关的。
- en: Stateful LSTM with Keras — predicting electricity consumption
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras的有状态LSTM — 预测电力消耗
- en: In this example, we predict electricity consumption for a consumer using a stateful
    and stateless LSTM network and compare their behaviors. As you will recall, RNNs
    in Keras are stateless by default. In case of stateful models, the internal states
    computed after processing a batch of input is reused as initial states for the
    next batch. In other words, the state computed from element *i* in a batch will
    be used as initial state for for the element *i* in the next batch.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用有状态和无状态LSTM网络预测某个消费者的电力消耗并比较它们的表现。如你所记得，Keras中的RNN默认是无状态的。在有状态模型中，处理一批输入后的内部状态将作为下一批的初始状态重新使用。换句话说，从一批中的第*i*个元素计算出的状态将作为下一批中第*i*个元素的初始状态。
- en: The dataset we will use is the electricity load diagram dataset from the UCI
    Machine Learning Repository ([https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014)),
    and contains consumption information about 370 customers, taken at 15 minute intervals
    over a four year period from 2011 to 2014\. We randomly choose customer number
    250 for our example.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据集是来自UCI机器学习库的电力负荷图数据集（[https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014)），其中包含370个客户的消费信息，数据以15分钟为间隔，时间跨度为2011年至2014年的四年周期。我们随机选择了第250号客户作为示例。
- en: 'One thing to remember is that most problems can be solved with stateless RNNs,
    so if you do use a stateful RNN, make sure you need it. Typically, you would need
    it when the data has a periodic component. If you think a bit, you will realize
    that electricity consumption is periodic. Consumption tends to be higher during
    the day than at night. Let us extract the consumption data for customer number
    250 and plot the first 10 days of data. Finally we also save it to a binary NumPy
    file for our next step:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一点是，大多数问题都可以使用无状态RNN来解决，因此如果你使用有状态RNN，请确保你确实需要它。通常，当数据具有周期性成分时，你需要使用它。如果你稍微思考一下，你会意识到电力消耗是有周期性的。白天的消耗往往高于夜晚。让我们提取第250号客户的消费数据，并绘制前10天的数据。最后，我们还将其保存为二进制的NumPy文件，供下一步使用：
- en: '[PRE30]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output of the preceding example is as follow:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例输出如下：
- en: '![](img/econs_plot.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/econs_plot.png)'
- en: As you can see, there is clearly a daily periodic trend. So the problem is a
    good candidate for a stateful model. Also, based on our observation, a `BATCH_SIZE`
    of `96` (number of 15 minute readings over 24 hours) seems appropriate.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，数据明显存在日周期性趋势。因此，这个问题非常适合使用有状态模型。此外，根据我们的观察，`BATCH_SIZE`设置为`96`（24小时内每15分钟一次的读取数）似乎合适。
- en: We will show the code for the stateless version of the model simultaneously
    with the one for the stateful version. Most of the code is identical for both
    versions, so we will look at both versions simultaneously. I will point out the
    differences in the code as they arise.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将同时展示无状态版本和有状态版本的模型代码。两者的大部分代码是相同的，因此我们将同时查看这两个版本。我将在代码中出现差异时指出。
- en: 'First, as usual, we import the necessary libraries and classes:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，像往常一样，我们导入必要的库和类：
- en: '[PRE31]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next we load the data for customer 250 into a long array of size (`140256`)
    from the saved NumPy binary file and rescale it to the range *(0, 1)*. Finally,
    we reshape the input to three dimensions as needed by our network:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将客户250的数据从保存的NumPy二进制文件中加载到一个大小为`140256`的长数组中，并将其重新缩放到范围*(0, 1)*。最后，我们将输入调整为网络所需的三维形状：
- en: '[PRE32]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Within each batch, the model will take a sequence of 15 minute readings and
    predict the next one. The length of the input sequence is given by the `NUM_TIMESTEPS`
    variable in the code. Based on some experimentation, we get a value of `NUM_TIMESTEPS`
    as `20`, that is, each input row will be a sequence of length `20`, and the output
    will have length `1`. The next step rearranges the input array into `X` and `Y`
    tensors of shapes `(None, 4)` and `(None, 1)`. Finally, we reshape the input tensor
    `X` to three dimensions as required by the network:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个批次中，模型将处理一组15分钟的读数，并预测下一个读数。输入序列的长度由代码中的`NUM_TIMESTEPS`变量给出。根据一些实验，我们得到了`NUM_TIMESTEPS`的值为`20`，也就是说，每个输入行将是一个长度为`20`的序列，而输出的长度为`1`。下一步将输入数组重排列成`X`和`Y`张量，其形状分别为`(None,
    4)`和`(None, 1)`。最后，我们将输入张量`X`调整为网络所需的三维形状：
- en: '[PRE33]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We then split our `X` and `Y` tensors into a 70-30 training test split. Since
    we are working with time series, we just choose a split point and cut the data
    into two parts, rather than using the `train_test_split` function, which also
    shuffles the data:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将`X`和`Y`张量分割成70-30的训练测试集。由于我们处理的是时间序列数据，我们只需选择一个分割点并将数据切成两部分，而不是使用`train_test_split`函数，后者会打乱数据：
- en: '[PRE34]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'First we define our stateless model. We also set the values of `BATCH_SIZE`
    and `NUM_TIMESTEPS`, as we discussed previously. Our LSTM output size is given
    by `HIDDEN_SIZE`, another hyperparameter that is usually arrived at through experimentation.
    Here, we just set it to `10` since our objective is to compare two networks:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们定义我们的无状态模型。我们还设置了`BATCH_SIZE`和`NUM_TIMESTEPS`的值，正如我们之前讨论的那样。我们的LSTM输出大小由`HIDDEN_SIZE`给出，这是另一个通常通过实验得出的超参数。在这里，我们将其设置为`10`，因为我们的目标是比较两个网络：
- en: '[PRE35]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The corresponding definition for the stateful model is very similar, as you
    can see as follows. In the LSTM constructor, you need to set `stateful=True`,
    and instead of `input_shape` where the batch size is determined at runtime, you
    need to set `batch_input_shape` explicitly with the batch size. You also need
    to ensure that your training and test data sizes are perfect multiples of your
    batch size. We will see how to do that later when we look at the training code:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的有状态模型定义非常相似，如下所示。在LSTM构造函数中，你需要设置`stateful=True`，并且不再使用`input_shape`来确定批次大小，而是需要显式地使用`batch_input_shape`来设置批次大小。你还需要确保训练和测试数据的大小是批次大小的整数倍。我们稍后会在查看训练代码时看到如何做到这一点：
- en: '[PRE36]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next we compile the model, which is the same for both stateless and stateful
    RNNs. Notice that our metric here is mean squared error instead of our usual accuracy.
    This is because this is really a regression problem; we are interested in knowing
    how far off our predictions are with respect to the labels rather than knowing
    whether our prediction matched the label. You can find a full list of Keras built-in
    metrics on the Keras metrics page:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们编译模型，对于无状态和有状态的RNN模型是相同的。请注意，这里的评估指标是均方误差，而不是我们通常使用的准确率。因为这是一个回归问题；我们关注的是预测与标签之间的偏差，而不是预测是否与标签匹配。你可以在Keras的评估指标页面找到Keras内置指标的完整列表：
- en: '[PRE37]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'To train the stateless model, we can use the one liner that we have probably
    become very familiar with by now:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练无状态模型，我们可以使用我们现在可能已经非常熟悉的一行代码：
- en: '[PRE38]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The corresponding code for the stateful model is shown as follows. There are
    three things to be aware of here.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的有状态模型的代码如下所示。这里有三点需要注意：
- en: First, you should select a batch size that reflects the periodicity of your
    data. This is because stateful RNNs align the states from each batch to the next,
    so selecting the right batch size allows the network to learn faster.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你应该选择一个反映数据周期性的批次大小。这是因为有状态的RNN会将每个批次的状态与下一个批次对齐，所以选择正确的批次大小可以让网络学习得更快。
- en: Once you set the batch size, the size of your training and test sets needs to
    be exact multiples of your batch size. We have ensured this below by truncating
    the last few records from both our training and test sets.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置了批次大小，训练和测试集的大小需要是批次大小的精确倍数。我们通过截断训练和测试集中的最后几条记录来确保这一点：
- en: The second thing is that you need to fit the model manually, training the model
    in a loop for the required number of epochs. Each iteration trains the model for
    one epoch, and the state is retained across multiple batches. After each epoch,
    the state of the model needs to be reset manually.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 第二点是你需要手动拟合模型，即在需要的轮数内对模型进行训练。每次迭代训练模型一个轮次，并且状态在多个批次间保持。每个轮次结束后，模型的状态需要手动重置。
- en: 'The third thing is that the data should be fed in sequence. By default, Keras
    will shuffle the rows within each batch, which will destroy the alignment we need
    for the stateful RNN to learn effectively. This is done by setting `shuffle=False`
    in the call to `model.fit()`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 第三点是数据应该按顺序输入。默认情况下，Keras 会在每个批次内打乱行，这会破坏我们对状态 RNN 有效学习所需的对齐。通过在调用 `model.fit()`
    时设置 `shuffle=False` 可以避免这种情况：
- en: '[PRE39]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Finally, we evaluate the model against the test data and print out the scores:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们对模型进行评估并打印出得分：
- en: '[PRE40]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output for the stateless model, run over five epochs, is as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无状态模型，运行五轮后的输出如下所示：
- en: '![](img/ss-6-7.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ss-6-7.png)'
- en: 'The corresponding output for the stateful model, also run in a loop five times
    for one epoch each time, is as follows. Notice the result of the truncating operation
    in the second line:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对于带状态模型，运行五次，每次一个轮次的相应输出如下所示。注意第二行中截断操作的结果：
- en: '![](img/ss-6-8.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ss-6-8.png)'
- en: As you can see, the stateful model produces results that are slightly better
    than the stateless model. In absolute terms, since we have scaled our data to
    the *(0, 1)* range, this means that the stateless model has about 6.2% error rate
    and the stateful model has a 5.9% error rate, or conversely, they are about 93.8%
    and 94.1% accurate respectively. In relative terms, therefore, our stateful model
    outperforms the stateless model by a slight margin.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，带状态模型产生的结果略好于无状态模型。从绝对值来看，由于我们将数据缩放到 *(0, 1)* 范围内，这意味着无状态模型的错误率约为 6.2%，而带状态模型的错误率为
    5.9%；换句话说，它们的准确率分别约为 93.8% 和 94.1%。因此，从相对角度来看，我们的带状态模型比无状态模型略微更优。
- en: The source code for this example is provided in the files `econs_data.py` that
    parses the dataset, and `econs_stateful.py` that defines and trains the stateless
    and stateful models, available from the code download for this chapter.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的源代码包括 `econs_data.py` 文件（解析数据集）和 `econs_stateful.py` 文件（定义和训练无状态及带状态模型），可以从本章的代码下载中获取。
- en: Other RNN variants
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他 RNN 变体
- en: We will round up this chapter by looking at some more variants of the RNN cell.
    RNN is an area of active research and many researchers have suggested variants
    for specific purposes.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过查看一些 RNN 单元的变体来结束本章。RNN 是一个活跃的研究领域，许多研究者为特定目的提出了不同的变体。
- en: 'One popular LSTM variant is adding *peephole connections*, which means that
    the gate layers are allowed to peek at the cell state. This was introduced by
    Gers and Schmidhuber (for more information refer to the article: *Learning Precise
    Timing with LSTM Recurrent Networks*, by F. A. Gers, N. N. Schraudolph, and J.
    Schmidhuber, Journal of Machine Learning Research, pp. 115-43) in 2002.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的 LSTM 变体是添加 *窥视连接*，意味着门层可以查看单元状态。这是 Gers 和 Schmidhuber 在 2002 年提出的（有关更多信息，请参阅文章：*Learning
    Precise Timing with LSTM Recurrent Networks*，作者 F. A. Gers、N. N. Schraudolph 和
    J. Schmidhuber，《机器学习研究杂志》，第 115-143 页）。
- en: Another LSTM variant, that ultimately led to the GRU, is to use coupled forget
    and output gates. Decisions about what information to forget and what to acquire
    are made together, and the new information replaces the forgotten information.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种 LSTM 变体，最终导致了 GRU 的出现，是使用耦合的遗忘门和输出门。关于遗忘哪些信息以及获取哪些信息的决策是一起做出的，而新信息则替代了被遗忘的信息。
- en: 'Keras provides only the three basic variants, namely the SimpleRNN, LSTM, and
    GRU layers. However, that isn''t necessarily a problem. Gref conducted an experimental
    survey (for more information refer to the article: *LSTM: A Search Space Odyssey*,
    by K. Greff, arXiv:1503.04069, 2015) of many LSTM variants, and concluded that
    none of the variants improved significantly over the standard LSTM architecture.
    So the components provided in Keras are usually sufficient to solve most problems.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 'Keras 仅提供了三种基本变体，即 SimpleRNN、LSTM 和 GRU 层。然而，这不一定是问题。Gref 进行了一项实验性调查（有关更多信息，请参阅文章：*LSTM:
    A Search Space Odyssey*，作者 K. Greff，arXiv:1503.04069，2015），并得出结论，任何变体都没有显著改善标准
    LSTM 架构。因此，Keras 提供的组件通常足以解决大多数问题。'
- en: In case you do need the capability to construct your own layer, you can build
    custom Keras layers. We will look at how to build a custom layer in the next chapter.
    There is also an open source framework called recurrent shop ([https://github.com/datalogai/recurrentshop](https://github.com/datalogai/recurrentshop))
    that allows you to build complex recurrent neural networks with Keras.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你确实需要构建自己的层，Keras 允许你构建自定义层。我们将在下一章中学习如何构建自定义层。还有一个开源框架叫做 recurrent shop ([https://github.com/datalogai/recurrentshop](https://github.com/datalogai/recurrentshop))，它允许你使用
    Keras 构建复杂的循环神经网络。
- en: Summary
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at the basic architecture of recurrent neural networks
    and how they work better than traditional neural networks over sequence data.
    We saw how RNNs can be used to learn an author's writing style and generate text
    using the learned model. We also saw how this example can be extended to predicting
    stock prices or other time series, speech from noisy audio, and so on, as well
    as generate music that was composed by a learned model.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了循环神经网络的基本架构，以及它们如何在序列数据上比传统神经网络表现得更好。我们看到了如何使用 RNN 学习作者的写作风格，并生成基于所学模型的文本。我们还看到了如何将这个示例扩展到预测股价或其他时间序列、从嘈杂的音频中识别语音等任务，还可以生成由学习模型作曲的音乐。
- en: We looked at different ways to compose our RNN units and these topologies can
    be used to model and solve specific problems such as sentiment analysis, machine
    translation, image captioning, and classification, and so on.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了不同的 RNN 单元组合方式，这些拓扑结构可以用于建模和解决特定问题，如情感分析、机器翻译、图像标题生成、分类等。
- en: We then looked at one of the biggest drawbacks of the SimpleRNN architecture,
    that of vanishing and exploding gradients. We saw how the vanishing gradient problem
    is handled using the LSTM (and GRU) architectures. We also looked at the LSTM
    and GRU architectures in some detail. We also saw two examples of predicting sentiment
    using an LSTM-based model, and predicting POS tags using a GRU-based sequence-to-sequence
    architecture.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们研究了 SimpleRNN 架构的最大缺点之一——梯度消失和梯度爆炸问题。我们看到了如何通过 LSTM（和 GRU）架构来处理梯度消失问题。我们还详细了解了
    LSTM 和 GRU 架构。我们还看到了两个示例，分别使用基于 LSTM 的模型预测情感，以及使用基于 GRU 的序列到序列架构预测词性标记（POS）。
- en: We then learned about stateful RNNs and how they can be used in Keras. We also
    saw an example of learning a stateful RNN to predict CO levels in the atmosphere.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们学习了状态感知型 RNN（stateful RNN），以及如何在 Keras 中使用它们。我们还看到了一个使用状态感知型 RNN 来预测大气中二氧化碳水平的示例。
- en: Finally, we learned about some RNN variants that are not available in Keras,
    and briefly explored how to build them.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们学习了一些在 Keras 中不可用的 RNN 变体，并简要探讨了如何构建它们。
- en: In the next chapter, we will look at models that don't quite fit into the basic
    molds we have looked at so far. We will also look at composing these basic models
    larger and more complex ones using the Keras functional API, as well as look at
    some examples of customizing Keras to our needs.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨一些不完全符合我们目前所学基本模型的模型。我们还将学习如何使用 Keras 的函数式 API 将这些基本模型组合成更大更复杂的模型，并查看一些定制
    Keras 以满足我们需求的示例。
