- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Probabilistic TensorFlow
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率 TensorFlow
- en: 'Uncertainty is a fact of life; whether you are doing a classification task
    or a regression task, it is important to know how confident your model is in its
    prediction. Till now, we have covered the traditional deep learning models, and
    while they are great at many tasks, they are not able to handle uncertainty. Instead,
    they are deterministic in nature. In this chapter, you will learn how to leverage
    TensorFlow Probability to build models that can handle uncertainty, specifically
    probabilistic deep learning models and Bayesian networks. The chapter will include:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性是生活中的一部分；无论你是在进行分类任务还是回归任务，了解你的模型在预测中的信心度非常重要。到目前为止，我们已经介绍了传统的深度学习模型，虽然它们在许多任务中表现出色，但它们无法处理不确定性。相反，它们本质上是确定性的。在本章中，你将学习如何利用
    TensorFlow Probability 构建能够处理不确定性的模型，特别是概率深度学习模型和贝叶斯网络。本章内容包括：
- en: TensorFlow Probability
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Probability
- en: Distributions, events, and shapes in TensorFlow Probability
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Probability 中的分布、事件和形状
- en: Bayesian networks using TensorFlow Probability
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Probability 构建贝叶斯网络
- en: Understand uncertainty in machine learning models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解机器学习模型中的不确定性
- en: Model aleatory and epistemic uncertainty using TensorFlow Probability
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Probability 模拟随即性和认知不确定性
- en: All the code files for this chapter can be found at [https://packt.link/dltfchp12](https://packt.link/dltfchp12)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码文件可以在 [https://packt.link/dltfchp12](https://packt.link/dltfchp12) 找到
- en: Let’s start with first understanding TensorFlow Probability.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从理解 TensorFlow Probability 开始。
- en: TensorFlow Probability
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow Probability
- en: '**TensorFlow Probability** (**TFP**), a part of the TensorFlow ecosystem, is
    a library that provides tools for developing probabilistic models. It can be used
    to perform probabilistic reasoning and statistical analysis. It is built over
    TensorFlow and provides the same computational advantage.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow Probability** (**TFP**)，是 TensorFlow 生态系统的一部分，是一个为开发概率模型提供工具的库。它可以用于进行概率推理和统计分析。它建立在
    TensorFlow 之上，提供相同的计算优势。'
- en: '*Figure 12.1* shows the major components constituting TensorFlow Probability:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.1* 显示了构成 TensorFlow Probability 的主要组件：'
- en: '![Graphical user interface, application  Description automatically generated](img/B18331_12_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序 描述自动生成](img/B18331_12_01.png)'
- en: 'Figure 12.1: Different components of TensorFlow Probability'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1：TensorFlow Probability 的不同组件
- en: At the root, we have all numerical operations supported by TensorFlow, specifically
    the `LinearOperator` class (part of `tf.linalg`) – it contains all the methods
    that can be performed on a matrix, without the need to actually materialize the
    matrix. This provides computationally efficient matrix-free computations. TFP
    includes a large collection of probability distributions and their related statistical
    computations. It also has `tfp.bijectors`, which offers a wide range of transformed
    distributions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在根本上，我们有 TensorFlow 支持的所有数值运算，特别是 `LinearOperator` 类（属于 `tf.linalg`）——它包含了对矩阵执行的所有方法，而无需实际构造矩阵。这提供了计算上高效的矩阵自由计算。TFP
    包含大量概率分布及其相关的统计计算。它还包括 `tfp.bijectors`，提供了广泛的变换分布。
- en: Bijectors encapsulate the change of variables for probability density. That
    is, when one transforms one variable from space A to space B, we need a way to
    map the probability distributions of the variables as well. Bijectors provide
    us with all the tools needed to do so.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Bijectors 封装了概率密度的变量变换。也就是说，当一个变量从空间 A 变换到空间 B 时，我们需要一种方法来映射变量的概率分布。Bijectors
    为我们提供了完成这一任务所需的所有工具。
- en: 'TensorFlow Probability also provides `JointDistribution`, which allows the
    user to draw a joint sample and compute a joint log-density (log probability density
    function). The standard TFP distributions work on tensors, but `JointDistribution`
    works on the structure of tensors. `tfp.layers` provides neural network layers
    that can be used to extend the standard TensorFlow layers and add uncertainty
    to them. And finally, it provides a wide range of tools for probabilistic inference.
    In this chapter, we will go through some of these functions and classes; let us
    first start with installation. To install TFP in your working environment, just
    run:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow概率还提供了`JointDistribution`，它允许用户抽取联合样本并计算联合对数密度（对数概率密度函数）。标准的TFP分布作用于张量，但`JointDistribution`作用于张量的结构。`tfp.layers`提供了神经网络层，可用于扩展标准TensorFlow层并为其添加不确定性。最后，它还提供了广泛的概率推理工具。在本章中，我们将通过一些这些函数和类；我们首先从安装开始。要在你的工作环境中安装TFP，只需运行：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let us have some fun with TFP. To use TFP, we will need to import it. Additionally,
    we are going to do some plots. So, we import some additional modules:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们玩一下TFP。要使用TFP，我们需要导入它。此外，我们将进行一些绘图。因此，我们导入一些额外的模块：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we explore the different classes of distributions available in `tfp.distributions`:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们探索`tfp.distributions`中可用的不同分布类别：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here is the output:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can see that a rich range of distributions is available in TFP. Let us
    now try one of the distributions:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，TFP中有丰富的分布可供选择。现在让我们尝试其中一种分布：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can see that as `N` increases, the plot follows a nice normal distribution:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，随着`N`的增加，图形遵循一个很好的正态分布：
- en: '| **N=100** | ![Chart, histogram  Description automatically generated](img/B18331_12_02_1.png)
    |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| **N=100** | ![Chart, histogram  Description automatically generated](img/B18331_12_02_1.png)
    |'
- en: '| **N=1000** | ![Chart, histogram  Description automatically generated](img/B18331_12_02_2.png)
    |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| **N=1000** | ![Chart, histogram  Description automatically generated](img/B18331_12_02_2.png)
    |'
- en: '| **N=10000** | ![A picture containing histogram  Description automatically
    generated](img/B18331_12_02_3.png) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **N=10000** | ![A picture containing histogram  Description automatically
    generated](img/B18331_12_02_3.png) |'
- en: 'Figure 12.2: Normal distribution from randomly generated samples of sizes 100,
    1,000, and 10,000\. The distribution has a mean of zero and a standard deviation
    of one'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2：从随机生成的样本中生成的正态分布，样本大小为100、1,000和10,000。该分布的均值为零，标准差为一。
- en: Let us now explore the different distributions available with TFP.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探索TFP中可用的不同分布。
- en: TensorFlow Probability distributions
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow概率分布
- en: 'Every distribution in TFP has a shape, batch, and event size associated with
    it. The shape is the sample size; it represents independent and identically distributed
    draws or observations. Consider the normal distribution that we defined in the
    previous section:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: TFP中的每个分布都有一个与之相关的形状、批次和事件大小。形状是样本大小；它代表独立同分布的抽样或观测。考虑我们在前一节中定义的正态分布：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This defines a single normal distribution, with mean zero and standard deviation
    one. When we use the `sample` function, we do a random draw from this distribution.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这定义了一个单一的正态分布，均值为零，标准差为一。当我们使用`sample`函数时，我们从这个分布中进行随机抽样。
- en: 'Notice the details regarding `batch_shape` and `event_shape` if you print the
    object `normal`:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果打印对象`normal`，请注意`batch_shape`和`event_shape`的细节：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let us try and define a second `normal` object, but this time, `loc` and `scale`
    are lists:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试定义第二个`normal`对象，不过这次，`loc`和`scale`是列表：
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Did you notice the change in `batch_shape`? Now, if we draw a single sample
    from it, we will draw from two normal distributions, one with a mean of zero and
    standard deviation of one, and the other with a mean of zero and standard deviation
    of three. Thus, the batch shape determines the number of observations from the
    same distribution family. The two normal distributions are independent; thus,
    it is a batch of distributions of the same family.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你注意到`batch_shape`的变化了吗？现在，如果我们从中抽取一个样本，我们将从两个正态分布中抽取，一个均值为零，标准差为一，另一个均值为零，标准差为三。因此，批次形状决定了来自同一分布族的观测数。这两个正态分布是独立的；因此，它是一个同一分布族的分布批次。
- en: You can have batches of the same type of distribution family, like in the preceding
    example of having two normal distributions. You cannot create a batch of, say,
    a normal and a Gaussian distribution.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以有同一类型的分布族的批次，就像前面例子中有两个正态分布一样。你不能创建一个批次，例如，一个正态分布和一个高斯分布。
- en: 'What if we need a single normal distribution that is dependent on two variables,
    each with a different mean? This is made possible using `MultivariateNormalDiag`,
    and this influences the event shape – it is the atomic shape of a single draw
    or observation from this distribution:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要一个依赖于两个变量且每个变量具有不同均值的单一正态分布，该如何操作？这可以通过 `MultivariateNormalDiag` 来实现，并且这会影响事件形状——它是从该分布中抽取单个样本或观测值的原子形状：
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We can see that in the above output the `event_shape` has changed.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在上面的输出中，`event_shape` 已发生变化。
- en: Using TFP distributions
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TFP 分布
- en: 'Once you have defined a distribution, you can do a lot more. TFP provides a
    good range of functions to perform various operations. We have already used the
    `Normal` distribution and `sample` method. The section above also demonstrated
    how we can use TFP for creating univariate, multivariate, or independent distribution/s.
    TFP provides many important methods to interact with the created distributions.
    Some of the important ones include:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了分布，你可以做很多其他操作。TFP 提供了丰富的函数来执行各种操作。我们已经使用了 `Normal` 分布和 `sample` 方法。上面的部分也展示了如何使用
    TFP 创建单变量、多变量或独立分布。TFP 提供了许多重要方法，用于与创建的分布进行交互。以下是一些重要的方法：
- en: '`sample(n)`: It samples `n` observations from the distribution.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample(n)`：它从分布中抽取 `n` 个观测值。'
- en: '`prob(value)`: It provides probability (discrete) or probability density (continuous)
    for the value.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prob(value)`：它为该值提供概率（离散）或概率密度（连续）。'
- en: '`log_prob(values)`: Provides log probability or log-likelihood for the values.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_prob(values)`：为值提供对数概率或对数似然。'
- en: '`mean()`: It gives the mean of the distribution.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mean()`：它提供分布的均值。'
- en: '`stddev()`: It provides the standard deviation of the distribution.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stddev()`：它提供分布的标准差。'
- en: Coin Flip Example
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬币投掷示例
- en: 'Let us now use some of the features of TFP to describe data by looking at an
    example: the standard coin-flipping example we are familiar with from our school
    days. We know that if we flip a coin, there are only two possibilities – we can
    have either a head or a tail. Such a distribution, where we have only two discrete
    values, is called a **Bernoulli** distribution. So let us consider different scenarios:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 TFP 的一些功能来描述数据，以下是一个例子：我们在学校时就熟悉的标准硬币投掷例子。我们知道如果我们投掷一枚硬币，只有两种可能性——要么是正面，要么是反面。这样的分布，只有两个离散值，称为
    **伯努利** 分布。让我们考虑不同的场景：
- en: Scenario 1
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 场景 1
- en: A fair coin with a `0.5` probability of heads and `0.5` probability of tails.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个公平的硬币，正面概率为 `0.5`，反面概率为 `0.5`。
- en: 'Let us create the distribution:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建分布：
- en: '[PRE13]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now get some samples:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在获取一些样本：
- en: '[PRE14]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let us visualize the samples:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化这些样本：
- en: '[PRE15]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![Shape  Description automatically generated](img/B18331_12_03.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的形状描述](img/B18331_12_03.png)'
- en: 'Figure 12.3: Distribution of heads and tails from 2,000 observations'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3：来自 2,000 次观测的正面和反面的分布
- en: 'You can see that we have both heads and tails in equal numbers; after all,
    it is a fair coin. The probability of heads and tails as `0.5`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们正反面出现的次数是相等的；毕竟，它是一个公平的硬币。正面和反面的概率都是 `0.5`：
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Scenario 2
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 场景 2
- en: A biased coin with a 0.8 probability of heads and 0.2 probability of tails.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一个偏向正面的硬币，正面概率为 `0.8`，反面概率为 `0.2`。
- en: 'Now, since the coin is biased, with the probability of heads being `0.8`, the
    distribution would be created using:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于硬币是有偏的，正面概率为 `0.8`，我们将使用以下方法创建分布：
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now get some samples:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在获取一些样本：
- en: '[PRE19]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let us visualize the samples:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化这些样本：
- en: '[PRE20]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![Shape  Description automatically generated with medium confidence](img/B18331_12_04.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的形状描述，具有中等置信度](img/B18331_12_04.png)'
- en: 'Figure 12.4: Distribution of heads and tails from 2,000 coin flips of a biased
    coin'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4：来自 2,000 次偏向正面的硬币投掷的正面和反面分布
- en: 'We can see that now heads are much larger in number than tails. Thus, the probability
    of tails is no longer `0.5`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，现在正面的次数远大于反面。因此，反面的概率不再是 `0.5`：
- en: '[PRE21]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You will probably get a number close to `0.2`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会得到一个接近 `0.2` 的数字。
- en: Scenario 3
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 场景 3
- en: Two coins with one biased toward heads with a `0.8` probability, and the other
    biased toward heads with a `0.6` probability.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 两个硬币，一个偏向正面，正面概率为 `0.8`，另一个偏向正面，正面概率为 `0.6`。
- en: 'Now, we have two independent coins. Since the coins are biased, with the probabilities
    of heads being `0.8` and `0.6` respectively, we create a distribution using:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有两个独立的硬币。由于硬币有偏差，正面概率分别为 `0.8` 和 `0.6`，我们使用以下方法创建分布：
- en: '[PRE23]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now get some samples:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在获取一些样本：
- en: '[PRE24]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let us visualize the samples:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化这些样本：
- en: '[PRE25]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![Graphical user interface  Description automatically generated](img/B18331_12_05.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面  描述自动生成](img/B18331_12_05.png)'
- en: 'Figure 12.5: Distribution of heads and tails from 2,000 flips for two independent
    coins'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5：2000次独立投掷中，两个硬币的正反面分布
- en: The bar in blue corresponds to Coin 1, and the bar in orange corresponds to
    Coin 2\. The brown part of the graphs is the area where the results of the two
    coins overlap. You can see that for Coin 1, the number of heads is much larger
    as compared to Coin 2, as expected.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝色的柱子对应于硬币1，橙色的柱子对应于硬币2。图表中的棕色部分是两个硬币结果重叠的区域。可以看到，硬币1的正面数量明显大于硬币2，正如预期的那样。
- en: Normal distribution
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正态分布
- en: 'We can use the Bernoulli distribution where the data can have only two possible
    discrete values: heads and tails, good and bad, spam and ham, and so on. However,
    a large amount of data in our daily lives is continuous in range, with the normal
    distribution being very common. So let us also explore different normal distributions.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用伯努利分布，其中数据只有两个可能的离散值：正面和反面，好与坏，垃圾邮件和火腿，等等。然而，日常生活中的大量数据是连续范围的，正态分布是非常常见的。所以，让我们也来探讨不同的正态分布。
- en: 'Mathematically, the probability density function of a normal distribution can
    be expressed as:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，正态分布的概率密度函数可以表示为：
- en: '![](img/B18331_12_001.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_12_001.png)'
- en: where ![](img/B18331_08_023.png) is the mean of the distribution, and ![](img/B18331_07_010.png)
    is the standard deviation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B18331_08_023.png) 是分布的均值，![](img/B18331_07_010.png) 是标准差。
- en: In TFP, the parameter `loc` represents the mean and the parameter `scale` represents
    the standard deviation. Now, to illustrate the use of how we can use distribution,
    let us consider that we want to represent the weather data of a location for a
    particular season, say summer in Delhi, India.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TFP 中，参数 `loc` 表示均值，参数 `scale` 表示标准差。现在，为了说明我们如何使用分布，我们假设想要表示某个地点的天气数据，例如印度德里的夏季天气。
- en: Univariate normal
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单变量正态分布
- en: We can think that weather depends only on temperature. So, by having a sample
    of temperature in the summer months over many years, we can get a good representation
    of data. That is, we can have a univariate normal distribution.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以认为天气只依赖于温度。所以，通过收集多年来夏季的温度样本，我们可以获得数据的良好表示。也就是说，我们可以得到一个单变量正态分布。
- en: 'Now, based on weather data, the average high temperature in the month of June
    in Delhi is 35 degrees Celsius, with a standard deviation of 4 degrees Celsius.
    So, we can create a normal distribution using:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，基于天气数据，德里6月的平均最高温度为35摄氏度，标准差为4摄氏度。所以，我们可以通过以下方式创建正态分布：
- en: '[PRE26]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Get some observation samples from it:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 从中获取一些观测样本：
- en: '[PRE27]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'And let us now visualize it:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来可视化它：
- en: '[PRE28]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![Chart, histogram  Description automatically generated](img/B18331_12_06.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图表，直方图  描述自动生成](img/B18331_12_06.png)'
- en: 'Figure 12.6: Probability density function for the temperature of Delhi in the
    month of June'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.6：德里6月温度的概率密度函数
- en: It would be good to verify if the mean and standard deviation of our sample
    data is close to the values we described.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 检查我们的样本数据的均值和标准差是否接近我们描述的值是很有帮助的。
- en: 'Using the distribution, we can find the mean and standard deviation using:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用该分布，我们可以通过以下方式找到均值和标准差：
- en: '[PRE29]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'And from the sampled data, we can verify using:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从采样数据中，我们可以通过以下方式进行验证：
- en: '[PRE33]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Thus, the sampled data is following the same mean and standard deviation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，采样数据遵循相同的均值和标准差。
- en: Multivariate distribution
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多变量分布
- en: 'All is good so far. I show my distribution to a friend working in meteorology,
    and he says that using only temperature is not sufficient; the humidity is also
    important. So now, each weather point depends on two parameters – the temperature
    of the day and the humidity of the day. This type of data distribution can be
    obtained using the `MultivariateNormalDiag` distribution class, as defined in
    TFP:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止一切正常。我把我的分布展示给一位从事气象学的朋友看，他说仅使用温度是不够的，湿度也很重要。因此，现在每个天气点依赖于两个参数——当天的温度和湿度。这种数据分布可以通过
    TFP 中定义的 `MultivariateNormalDiag` 分布类来获得：
- en: '[PRE37]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '*Figure 12.7*, shows the multivariate normal distribution of two variables,
    temperature and humidity, generated using TFP:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12.7* 显示了使用 TFP 生成的两个变量（温度和湿度）的多变量正态分布：'
- en: '![Chart, scatter chart  Description automatically generated](img/B18331_12_07.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  描述自动生成](img/B18331_12_07.png)'
- en: 'Figure 12.7: Multivariate normal distribution with the x-axis representing
    temperature and the y-axis humidity'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7：多元正态分布，其中x轴代表温度，y轴代表湿度
- en: Using the different distributions and bijectors available in TFP, we can generate
    synthetic data that follows the same joint distribution as real data to train
    the model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TFP中提供的不同分布和双射函数，我们可以生成遵循与真实数据相同联合分布的合成数据来训练模型。
- en: Bayesian networks
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯网络
- en: '**Bayesian Networks** (**BNs**) make use of the concepts from graph theory,
    probability, and statistics to encapsulate complex causal relationships. Here,
    we build a **Directed Acyclic Graph** (**DAG**), where nodes, called factors (random
    variables), are connected by the arrows representing cause-effect relationships.
    Each node represents a variable with an associated probability (also called a
    **Conditional Probability Table** (**CPT**)). The links tell us about the dependence
    of one node over another. Though they were first proposed by Pearl in 1988, they
    have regained attention in recent years. The main cause of this renowned interest
    in BNs is that standard deep learning models are not able to represent the cause-effect
    relationship.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝叶斯网络**（**BNs**）利用图论、概率论和统计学的概念来封装复杂的因果关系。在这里，我们构建一个**有向无环图**（**DAG**），其中节点（称为因素或随机变量）通过箭头连接，表示因果关系。每个节点代表一个具有相关概率的变量（也称为**条件概率表**（**CPT**））。这些连接表示一个节点对另一个节点的依赖关系。尽管它们最早由Pearl于1988年提出，但近年来它们重新引起了关注。贝叶斯网络之所以受到广泛关注，主要原因是标准的深度学习模型无法表示因果关系。'
- en: 'Their strength lies in the fact that they can be used to model uncertainties
    combined with expert knowledge and data. They have been employed in diverse fields
    for their power to do probabilistic and causal reasoning. At the heart of the
    Bayesian network is Bayes’ rule:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的优势在于可以结合专家知识和数据来建模不确定性。由于其在做概率和因果推理方面的强大能力，贝叶斯网络已经在许多领域得到了应用。贝叶斯网络的核心是贝叶斯定理：
- en: '![](img/B18331_12_004.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_12_004.png)'
- en: Bayes’ rule is used to determine the joint probability of an event given certain
    conditions. The simplest way to understand the BN is that the BN can determine
    the causal relationship between the hypothesis and evidence. There is some unknown
    hypothesis H, about which we want to assess the uncertainty and make some decisions.
    We start with some prior belief about hypothesis H, and then based on evidence
    E, we update our belief about H.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理用于根据某些条件来确定事件的联合概率。理解贝叶斯网络最简单的方法是，它可以确定假设与证据之间的因果关系。假设有一个未知的假设H，我们想要评估它的不确定性并做出一些决策。我们从关于假设H的一些先验信念开始，然后根据证据E更新我们对H的信念。
- en: 'Let us try to understand it by example. We consider a very standard example:
    a garden with grass and a sprinkler. Now, using common sense, we know that if
    the sprinkler is on, the grass is wet. Let us now reverse the logic: what if you
    come back home and find that the grass is wet, what is the probability that the
    sprinkler is on, and what is the probability that it actually rained? Interesting,
    right? Let us add further evidence – you find that the sky is cloudy. Now, what
    do you think is the reason for the grass being wet?'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解它。我们考虑一个非常标准的例子：一个有草和喷头的花园。现在，凭常识我们知道，如果喷头开着，草地就会湿。现在让我们反过来说：如果你回家后发现草地湿了，那么喷头开着的概率是多少？而实际上下雨的概率又是多少？有意思吧？让我们进一步增加证据——你发现天空多云。那么，你认为草地湿的原因是什么？
- en: This sort of reasoning based on evidence is encompassed by BNs in the form of
    DAGs, also called causal graphs – because they provide an insight into the cause-effect
    relationship.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基于证据的推理通过贝叶斯网络（BNs）以有向无环图（DAG）的形式呈现，也叫因果图——因为它们提供了因果关系的洞察。
- en: 'To model the problem, we make use of the `JointDistributionCoroutine` distribution
    class. This distribution allows both the sampling of data and computation of the
    joint probability from a single model specification. Let us make some assumptions
    to build the model:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建模这个问题，我们使用了`JointDistributionCoroutine`分布类。这个分布类允许从单一的模型规范中同时进行数据采样和联合概率计算。让我们做出一些假设来建立模型：
- en: The probability that it is cloudy is `0.2`
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天空多云的概率是`0.2`
- en: The probability that it is cloudy and it rains is `0.8`, and the probability
    that it is not cloudy but it rains is `0.1`
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天空多云并且下雨的概率是`0.8`，而天空不多云但下雨的概率是`0.1`
- en: The probability that it is cloudy and the sprinkler is on is `0.1`, and the
    probability that it is not cloudy and the sprinkler is on is `0.5`
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 草地湿且喷洒器开启的情况下，云层存在的概率为`0.1`，而云层不存在且喷洒器开启的概率为`0.5`
- en: 'Now, for the grass, we have four possibilities:'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，对于草地，我们有四种可能性：
- en: '| **Sprinkler** | **Rain** | **Grass Wet** |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| **喷洒器** | **降雨** | **草地湿** |'
- en: '| F | F | 0 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| F | F | 0 |'
- en: '| F | T | 0.8 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| F | T | 0.8 |'
- en: '| T | F | 0.9 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| T | F | 0.9 |'
- en: '| T | T | 0.99 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| T | T | 0.99 |'
- en: 'Table 12.1: The conditional probability table for the Sprinkler-Rain-Grass
    scenario'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12.1：喷洒器-降雨-草地情境的条件概率表
- en: '*Figure 12.8* shows the corresponding BN DAG:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12.8* 显示了对应的BN有向无环图（DAG）：'
- en: '![Diagram  Description automatically generated](img/B18331_12_08.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图示描述自动生成](img/B18331_12_08.png)'
- en: 'Figure 12.8: Bayesian Network for our toy problem'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.8：我们玩具问题的贝叶斯网络
- en: 'This information can be represented by the following model:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这个信息可以通过以下模型表示：
- en: '[PRE38]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The above model will function like a data generator. The `Root` function is
    used to tell the node in the graph without any parent. We define a few utility
    functions, `broadcast` and `stack`:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 上述模型将像一个数据生成器一样工作。`Root`函数用来告诉图中的节点没有父节点。我们定义了几个实用函数，`broadcast`和`stack`：
- en: '[PRE39]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'To do inferences, we make use of the `MarginalizableJointDistributionCoroutine`
    class, as this allows us to compute marginalized probabilities:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行推理，我们使用了`MarginalizableJointDistributionCoroutine`类，因为它可以帮助我们计算边际化的概率：
- en: '[PRE40]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now, based on our observations, we can obtain the probability of other factors.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，基于我们的观察，我们可以获取其他因素的概率。
- en: 'Case 1:'
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 案例 1：
- en: 'We observe that the grass is wet (the observation corresponding to this is
    1 – if the grass was dry, we would set it to 0), we have no idea about the state
    of the clouds or the state of the sprinkler (the observation corresponding to
    an unknown state is set to “marginalize”), and we want to know the probability
    of rain (the observation corresponding to the probability we want to find is set
    to “tabulate”). Converting this into observations:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到草地是湿的（对应的观察值为1——如果草地是干的，我们会将其设为0），我们对于云层或喷洒器的状态一无所知（对应未知状态的观察值设置为“边际化”），并且我们想知道降雨的概率（对应我们想找到的概率的观察值设置为“列举”）。将其转化为观察值：
- en: '[PRE41]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now we get the probability of rain using:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们通过以下方式得到降雨的概率：
- en: '[PRE42]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The result is `array([0.27761015, 0.72238994], dtype=float32)`, that is, there
    is a 0.722 probability that it rained.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是`array([0.27761015, 0.72238994], dtype=float32)`，即有0.722的概率表示下雨了。
- en: 'Case 2:'
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 案例 2：
- en: 'We observe that the grass is wet, we have no idea about the state of the clouds
    or rain, and we want to know the probability of whether the sprinkler is on. Converting
    this into observations:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到草地是湿的，对于云层或降雨的状态我们一无所知，我们想要知道喷洒器是否开启的概率。将其转化为观察值：
- en: '[PRE43]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This results in probabilities `array([0.61783344, 0.38216656], dtype=float32)`,
    that is, there is a `0.382` probability that the sprinkler is on.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这得到了概率`array([0.61783344, 0.38216656], dtype=float32)`，即有`0.382`的概率表示喷洒器开启。
- en: 'Case 3:'
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 案例 3：
- en: 'What if we observe that there is no rain, and the sprinkler is off? What do
    you think is the state of the grass? Logic says the grass should not be wet. Let
    us confirm this from the model by sending it the observations:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们观察到没有下雨，且喷洒器关闭，你认为草地的状态会是什么？逻辑上，草地不应该是湿的。让我们通过将观察值传递给模型来确认这一点：
- en: '[PRE44]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: This results in the probabilities `array([1., 0], dtype=float32)`, that is,
    there is a 100% probability that the grass is dry, just the way we expected.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这得到了概率`array([1., 0], dtype=float32)`，即有100%的概率表示草地是干的，正如我们预期的那样。
- en: As you can see, once we know the state of the parents, we do not need to know
    the state of the parent’s parents – that is, the BN follows the local Markov property.
    In the example that we covered here, we started with the structure, and we had
    the conditional probabilities available to us. We demonstrate how we can do inference
    based on the model, and how despite the same model and CPDs, the evidence changes
    the **posterior probabilities**.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，一旦我们知道了父节点的状态，就不需要知道父节点的父节点的状态——也就是说，BN遵循局部马尔可夫性质。在我们这里讨论的例子中，我们从结构开始，且有条件概率可以使用。我们演示了如何基于模型进行推理，并且尽管使用的是相同的模型和条件概率分布（CPD），证据仍然会改变**后验概率**。
- en: In Bayesian networks, the structure (the nodes and how they are interconnected)
    and the parameters (the conditional probabilities of each node) are learned from
    the data. They are referred to as structured learning and parameter learning respectively.
    Covering the algorithms for structured learning and parameter learning are beyond
    the scope of this chapter.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯网络中，结构（节点及其相互连接方式）和参数（每个节点的条件概率）是从数据中学习得出的。它们分别被称为结构学习和参数学习。涉及结构学习和参数学习的算法超出了本章的范围。
- en: Handling uncertainty in predictions using TensorFlow Probability
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Probability 处理预测中的不确定性
- en: At the beginning of this chapter, we talked about the uncertainties in prediction
    by deep learning models and how the existing deep learning architectures are not
    able to account for those uncertainties. In this chapter, we will use the layers
    provided by TFP to model uncertainty.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开始时，我们讨论了深度学习模型中的预测不确定性，以及现有的深度学习架构无法解释这些不确定性。在本章中，我们将使用 TFP 提供的层来建模不确定性。
- en: Before adding the TFP layers, let us first understand the uncertainties a bit.
    There are two classes of uncertainty.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加 TFP 层之前，让我们先理解一下不确定性。我们可以将不确定性分为两类。
- en: Aleatory uncertainty
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机不确定性
- en: This exists because of the random nature of the natural processes. It is inherent
    uncertainty, present due to the probabilistic variability. For example, when tossing
    a coin, there will always be a certain degree of uncertainty in predicting whether
    the next toss will be heads or tails. There is no way to remove this uncertainty.
    In essence, every time you repeat the experiment, the results will have certain
    variations.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不确定性存在于自然过程的随机性中。它是固有的不确定性，由于概率的变化性而存在。例如，在投掷硬币时，总会有一定程度的不确定性，无法准确预测下一次投掷是正面还是反面。无法消除这种不确定性。本质上，每次重复实验时，结果都会有一定的变化。
- en: Epistemic uncertainty
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 认知不确定性
- en: This uncertainty comes from a lack of knowledge. There can be various reasons
    for this lack of knowledge, for example, an inadequate understanding of the underlying
    processes, an incomplete knowledge of the phenomena, and so on. This type of uncertainty
    can be reduced by understanding the reason, for example, to get more data, we
    conduct more experiments.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不确定性源于知识的缺乏。知识缺乏的原因可能有很多，比如对底层过程的理解不足、对现象的知识不完整等。这种类型的不确定性可以通过理解原因来减少，例如，通过获取更多数据，进行更多实验。
- en: The presence of these uncertainties increases risk. We require a way to quantify
    these uncertainties and, hence, quantify the risk.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不确定性的存在增加了风险。我们需要一种方法来量化这些不确定性，从而量化风险。
- en: Creating a synthetic dataset
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建合成数据集
- en: 'In this section, we will learn how to modify the standard deep neural networks
    to quantify uncertainties. Let us start with creating a synthetic dataset. To
    create the dataset, we consider that output prediction y depends on input x linearly,
    as given by the following expression:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何修改标准的深度神经网络以量化不确定性。我们从创建一个合成数据集开始。为了创建数据集，我们假设输出预测 y 与输入 x 之间是线性关系，如下所示：
- en: '![](img/B18331_12_005.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_12_005.png)'
- en: 'Here, ![](img/B18331_12_006.png) follows a normal distribution with mean zero
    and standard deviation 1 around x. The function below will generate this synthetic
    data for us. Do observe that to generate this data, we made use of the `Uniform`
    distribution and `Normal` distributions available as part of TFP distributions:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B18331_12_006.png) 服从均值为零，标准差为1的正态分布，围绕 x 变化。下面的函数将为我们生成这些合成数据。请注意，为了生成这些数据，我们使用了作为
    TFP 分布一部分的 `Uniform` 分布和 `Normal` 分布：
- en: '[PRE45]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '`y_true` is the value without including the normal distributed noise ![](img/B18331_10_003.png).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`y_true` 是不包含正态分布噪声的真实值 ![](img/B18331_10_003.png)。'
- en: 'Now we use it to create a training dataset and a validation dataset:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们用它来创建训练数据集和验证数据集：
- en: '[PRE46]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This will give us 2,000 datapoints for training and 500 datapoints for validation.
    *Figure 12.9* shows the plots of the two datasets, with ground truth (the value
    of *y* in the absence of any noise) in the background:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们提供 2,000 个用于训练的数据点和 500 个用于验证的数据点。*图 12.9* 显示了这两个数据集的图形，背景为真实值（在没有任何噪声的情况下的
    *y* 值）：
- en: '![Chart, scatter chart  Description automatically generated](img/B18331_12_09.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图 描述自动生成](img/B18331_12_09.png)'
- en: 'Figure 12.9: Plot of the synthetic dataset'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.9：合成数据集的图示
- en: Building a regression model using TensorFlow
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 构建回归模型
- en: 'We can build a simple Keras model to perform the task of regression on the
    synthetic dataset created in the preceding section:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以构建一个简单的Keras模型，执行对前一部分创建的合成数据集的回归任务：
- en: '[PRE47]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Let us see how good the fitted model works on the test dataset:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看拟合模型在测试数据集上的表现如何：
- en: '![Chart, scatter chart  Description automatically generated](img/B18331_12_10.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图 描述自动生成](img/B18331_12_10.png)'
- en: 'Figure 12.10: Ground truth and fitted regression line'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.10：真实值与拟合回归线
- en: It was a simple problem, and we can see that the fitted regression line almost
    overlaps the ground truth. However, there is no way to tell the uncertainty of
    predictions.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的问题，我们可以看到拟合的回归线几乎与真实值重合。然而，无法判断预测的不确定性。
- en: Probabilistic neural networks for aleatory uncertainty
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于随机不确定性的概率神经网络
- en: 'What if instead of linear regression, we build a model that can fit the distribution?
    In our synthetic dataset, the source of aleatory uncertainty is the noise, and
    we know that our noise follows a normal distribution, which is characterized by
    two parameters: the mean and standard deviation. So, we can modify our model to
    predict the mean and standard deviation distributions instead of actual *y* values.
    We can accomplish this using either the `IndependentNormal` TFP layer or the `DistributionLambda`
    TFP layer. The following code defines the modified model architecture:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不使用线性回归，而是构建一个能够拟合分布的模型，会怎样呢？在我们的合成数据集中，随机不确定性的来源是噪声，我们知道我们的噪声遵循正态分布，这种分布由两个参数来描述：均值和标准差。因此，我们可以修改我们的模型，预测均值和标准差的分布，而不是实际的*y*值。我们可以通过使用`IndependentNormal`
    TFP层或`DistributionLambda` TFP层来实现这一点。以下代码定义了修改后的模型架构：
- en: '[PRE48]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We will need to make one more change. Earlier, we predicted the *y* value;
    therefore, the mean square error loss was a good choice. Now, we are predicting
    the distribution; therefore, a better choice is the negative log-likelihood as
    the loss function:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要再做一次修改。之前，我们预测的是*y*值；因此，均方误差损失是一个不错的选择。现在，我们预测的是分布；因此，更好的选择是负对数似然作为损失函数：
- en: '[PRE49]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Let us now train this new model:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们训练这个新模型：
- en: '[PRE50]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Since now our model returns a distribution, we require the statistics mean
    and standard deviation for the test dataset:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 由于现在我们的模型返回的是一个分布，我们需要测试数据集的统计信息，包括均值和标准差：
- en: '[PRE51]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Note that the predicted mean now corresponds to the fitted line in the first
    case. Let us now see the plots:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，现在预测的均值对应于第一种情况中的拟合线。现在让我们来看一下图表：
- en: '[PRE52]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The following curve shows the fitted line, along with the aleatory uncertainty:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 以下曲线显示了拟合线以及随机不确定性：
- en: '![Line chart  Description automatically generated](img/B18331_12_11.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![折线图 描述自动生成](img/B18331_12_11.png)'
- en: 'Figure 12.11: Modelling aleatory uncertainty using TFP layers'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.11：使用TFP层建模随机不确定性
- en: You can see that our model shows less uncertainty near the origin, but as we
    move further away, the uncertainty increases.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我们的模型在原点附近的不确定性较小，但随着距离增大，不确定性增加。
- en: Accounting for the epistemic uncertainty
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 考虑到认识论不确定性
- en: In conventional neural networks, each weight is represented by a single number,
    and it is updated such that the loss of the model with respect to its weight is
    minimized. We assume that weights so learned are the optimum weights. But are
    they? To answer this question, we replace each weight with a distribution, and
    instead of learning a single value, we will now make our model learn a set of
    parameters for each weight distribution. This is accomplished by replacing the
    Keras `Dense` layer with the `DenseVariational` layer. The `DenseVariational`
    layer uses a variational posterior over the weights to represent the uncertainty
    in their values. It tries to regularize the posterior to be close to the prior
    distribution. Hence, to use the `DenseVariational` layer, we will need to define
    two functions, one prior generating function and another posterior generating
    function. We use the posterior and prior functions defined at [https://www.tensorflow.org/probability/examples/Probabilistic_Layers_Regression](https://www.tensorflow.org/probability/examples/Probabilistic_Layers_Regression).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的神经网络中，每个权重都由一个数字表示，并且该数字会被更新，以使得模型相对于其权重的损失最小化。我们假设这样学习到的权重就是最优权重。但真的是这样吗？为了回答这个问题，我们将每个权重替换为一个分布，而不是学习一个单一的值，我们现在让模型为每个权重分布学习一组参数。这是通过将
    Keras 的 `Dense` 层替换为 `DenseVariational` 层来实现的。`DenseVariational` 层通过对权重使用变分后验分布来表示其值的不确定性。它试图将后验分布正则化，使其接近先验分布。因此，为了使用
    `DenseVariational` 层，我们需要定义两个函数，一个是先验生成函数，另一个是后验生成函数。我们使用在 [https://www.tensorflow.org/probability/examples/Probabilistic_Layers_Regression](https://www.tensorflow.org/probability/examples/Probabilistic_Layers_Regression)
    上定义的后验和先验函数。
- en: 'Our model now has two layers, a `DenseVariational` layer followed by a `DistributionLambda`
    layer:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的模型有两层，一个是 `DenseVariational` 层，后面跟着一个 `DistributionLambda` 层：
- en: '[PRE53]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Again, as we are looking for distributions, the loss function that we use is
    the negative log-likelihood function:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，由于我们要处理的是分布，我们使用的损失函数是负对数似然函数：
- en: '[PRE54]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We continue with the same synthetic data that we created earlier and train
    the model:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续使用之前创建的相同的合成数据并训练模型：
- en: '[PRE55]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now that the model has been trained, we make the prediction, and to understand
    the concept of uncertainty, we make multiple predictions for the same input ranges.
    We can see the difference in variance in the result in the following graphs:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经训练完毕，我们进行预测，为了理解不确定性的概念，我们对相同的输入范围进行了多次预测。我们可以在以下图表中看到结果的方差差异：
- en: '| ![Chart, scatter chart  Description automatically generated](img/B18331_12_12_1.png)
    | ![Chart, scatter chart  Description automatically generated](img/B18331_12_12_2.png)
    |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| ![图表，散点图 自动生成的描述](img/B18331_12_12_1.png) | ![图表，散点图 自动生成的描述](img/B18331_12_12_2.png)
    |'
- en: 'Figure 12.12: Epistemic uncertainty'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.12：认识不确定性
- en: '*Figure 12.12* shows two graphs, one when only 200 training data points were
    used to build the model, and the second when 2,000 data points were used to train
    the model. We can see that when there is more data, the variance and, hence, the
    epistemic uncertainty reduces. Here, *overall mean* refers to the mean of all
    the predictions (100 in number), and in the case of *ensemble mean*, we considered
    only the first 15 predictions. All machine learning models suffer from some level
    of uncertainty in predicting outcomes. Getting an estimate or quantifiable range
    of uncertainty in the prediction will help AI users build more confidence in their
    AI predictions and will boost overall AI adoption.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12.12* 显示了两个图表，一个是使用仅 200 个训练数据点构建模型时的图表，另一个是使用 2000 个数据点训练模型时的图表。我们可以看到，当数据量增加时，方差以及因此的认识不确定性减少。这里的
    *总体均值* 指的是所有预测（共 100 个）的均值，而 *集成均值* 指的是我们只考虑了前 15 个预测值。所有机器学习模型在预测结果时都会受到一定程度的不确定性影响。获得一个估计值或可量化的不确定性范围，能帮助
    AI 用户在 AI 预测中建立更多信心，并推动 AI 的广泛应用。'
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter introduced TensorFlow Probability, the library built over TensorFlow
    to perform probabilistic reasoning and statistical analysis. The chapter started
    with the need for probabilistic reasoning – the uncertainties both due to the
    inherent nature of data and due to a lack of knowledge. We demonstrated how to
    use TensorFlow Probability distributions to generate different data distributions.
    We learned how to build a Bayesian network and perform inference. Then, we built
    Bayesian neural networks using TFP layers to take into account aleatory uncertainty.
    Finally, we learned how to account for epistemic uncertainty with the help of
    the `DenseVariational` TFP layer.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 TensorFlow Probability，这是一个建立在 TensorFlow 之上的库，用于进行概率推理和统计分析。本章从对概率推理的需求开始——数据固有的不确定性和由于缺乏知识而产生的不确定性。我们演示了如何使用
    TensorFlow Probability 分布生成不同的数据分布。我们学习了如何构建贝叶斯网络并执行推理。接着，我们使用 TFP 层构建了贝叶斯神经网络，以考虑到偶然性不确定性。最后，我们学会了如何借助
    `DenseVariational` TFP 层处理认知不确定性。
- en: In the next chapter, we will learn about TensorFlow AutoML frameworks.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习 TensorFlow AutoML 框架。
- en: References
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Dillon, J. V., Langmore, I., Tran, D., Brevdo, E., Vasudevan, S., Moore, D.,
    Patton, B., Alemi, A., Hoffman, M., and Saurous, R. A. (2017). *TensorFlow distributions*.
    arXiv preprint arXiv:1711.10604.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dillon, J. V., Langmore, I., Tran, D., Brevdo, E., Vasudevan, S., Moore, D.,
    Patton, B., Alemi, A., Hoffman, M., 和 Saurous, R. A. (2017). *TensorFlow 分布*.
    arXiv 预印本 arXiv:1711.10604.
- en: Piponi, D., Moore, D., and Dillon, J. V. (2020). *Joint distributions for TensorFlow
    probability*. arXiv preprint arXiv:2001.11819.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Piponi, D., Moore, D., 和 Dillon, J. V. (2020). *TensorFlow 概率的联合分布*. arXiv 预印本
    arXiv:2001.11819.
- en: 'Fox, C. R. and Ülkümen, G. (2011). *Distinguishing Two Dimensions of Uncertainty*,
    in Essays in Judgment and Decision Making, Brun, W., Kirkebøen, G. and Montgomery,
    H., eds. Oslo: Universitetsforlaget.'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Fox, C. R. 和 Ülkümen, G. (2011). *区分不确定性的两个维度*，载于《判断与决策的论文集》，Brun, W., Kirkebøen,
    G. 和 Montgomery, H. 编。奥斯陆：Universitetsforlaget。
- en: 'Hüllermeier, E. and Waegeman, W. (2021). *Aleatoric and epistemic uncertainty
    in machine learning: An introduction to concepts and methods*. Machine Learning
    110, no. 3: 457–506.'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hüllermeier, E. 和 Waegeman, W. (2021). *机器学习中的偶然性和认知不确定性：概念与方法简介*. 《机器学习》110卷，第3期：457–506。
- en: Join our book’s Discord space
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人交流，并与超过 2000 名成员一起学习： [https://packt.link/keras](https://packt.link/keras)
- en: '![](img/QR_Code1831217224278819687.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1831217224278819687.png)'
