- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: 'Applying Pretrained Generative Models: From Prototype to Production'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用预训练生成模型：从原型到生产
- en: In the preceding chapters, we explored the fundamentals of generative AI, explored
    various generative models, such as **generative adversarial networks** (**GANs**),
    diffusers, and transformers, and learned about the transformative impact of **natural
    language processing** (**NLP**). As we transition into the practical aspects of
    applying generative AI, we should ground our exploration in a practical example.
    This approach will provide a concrete context, making the technical aspects more
    relatable and the learning experience more engaging.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们探讨了生成式人工智能的基础，探讨了各种生成模型，例如**生成对抗网络**（GANs）、扩散器和转换器，并了解了**自然语言处理**（NLP）的变革性影响。随着我们转向应用生成式人工智能的实践方面，我们应该将我们的探索建立在实际例子之上。这种方法将提供一个具体的环境，使技术方面更加贴近实际，学习体验更加吸引人。
- en: We will introduce “StyleSprint,” a clothing shop looking to enhance its online
    presence. One way to achieve this is by crafting unique and engaging product descriptions
    for its various products. However, manually creating captivating descriptions
    for a large inventory is challenging. This situation is prime opportunity for
    the application of generative AI. By leveraging a pretrained generative model,
    StyleSprint can automate the crafting of compelling product descriptions, saving
    considerable time and enriching the online shopping experience for its customers.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍“StyleSprint”，这是一家希望增强其在线存在的服装店。实现这一目标的一种方式是为其各种产品编写独特且引人入胜的产品描述。然而，手动为大量库存创建吸引人的描述是具有挑战性的。这种情况是应用生成式人工智能的理想机会。通过利用预训练的生成模型，StyleSprint可以自动化制作引人入胜的产品描述，节省大量时间并丰富其客户的在线购物体验。
- en: As we step into the practical application of a pretrained generative **large
    language models** (**LLM**), the first order of business is to set up a Python
    environment conducive to prototyping with generative models. This setup is vital
    for transitioning the project from a prototype to a production-ready state, setting
    the stage for StyleSprint to realize its goal of automated content generation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们步入预训练生成**大型语言模型**（LLM）的实际应用时，首要任务是建立一个有利于使用生成模型进行原型的Python环境。这个设置对于将项目从原型过渡到生产就绪状态至关重要，为StyleSprint实现其自动化内容生成目标奠定基础。
- en: In *Chapters 2* and *3*, we used Google Colab for prototyping due to its ease
    of use and accessible GPU resources. It served as a great platform to test ideas
    quickly. However, as we shift our focus toward deploying our generative model
    in a real-world setting, it is essential to understand the transition from a prototyping
    environment such as Google Colab to a more robust, production-ready setup. This
    transition will ensure our solution is scalable, reliable, and well-optimized
    for handling real-world traffic. In this chapter, we will walk through the steps
    in setting up a production-ready Python environment, underscoring the crucial
    considerations for a smooth transition from prototype to production.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2章和第3章中，我们使用Google Colab进行原型设计，因为它易于使用且可访问的GPU资源丰富。它是一个快速测试想法的优秀平台。然而，当我们把重点转向在现实世界环境中部署我们的生成模型时，了解从原型环境（如Google
    Colab）到更稳健、生产就绪的设置的过渡至关重要。这种过渡将确保我们的解决方案可扩展、可靠，并且针对处理现实世界流量进行了优化。在本章中，我们将介绍设置生产就绪Python环境的步骤，强调从原型到生产的平稳过渡的关键考虑因素。
- en: By the end of this chapter, we will understand the process of taking a generative
    application from a prototyping environment to a production-ready setup. We will
    define a reliable and repeatable strategy for evaluating, monitoring, and deploying
    models to production.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将了解将生成应用程序从原型环境过渡到生产就绪设置的流程。我们将定义一个可靠且可重复的策略，用于评估、监控和部署模型到生产环境。
- en: Prototyping environments
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原型环境
- en: 'Jupyter notebooks provide an interactive computing environment to combine code
    execution, text, mathematics, plots, and rich media into a single document. They
    are ideal for prototyping and interactive development, making them a popular choice
    among data scientists, researchers, and engineers. Here is what they offer:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter笔记本提供了一个交互式计算环境，可以将代码执行、文本、数学、图表和丰富媒体结合成一个单一的文档。它们非常适合原型设计和交互式开发，因此在数据科学家、研究人员和工程师中非常受欢迎。以下是它们提供的内容：
- en: '**Kernel**: At the heart of a Jupyter notebook is a kernel, a computational
    engine that executes the code contained in the notebook. For Python, this is typically
    an IPython kernel. This kernel remains active and maintains the state of your
    notebook’s computations while the notebook is open.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内核**：Jupyter笔记本的核心是一个内核，它是一个计算引擎，负责执行笔记本中的代码。对于Python，这通常是IPython内核。当笔记本打开时，这个内核保持活跃并维护笔记本计算的状态。'
- en: '**Interactive execution**: Code cells allow you to write and execute code interactively,
    inspecting the results and tweaking the code as necessary.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交互式执行**：代码单元格允许你交互式地编写和执行代码，检查结果并根据需要调整代码。'
- en: '`pip` or `conda` commands.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pip`或`conda`命令。'
- en: '**Visualization**: You can embed plots, graphs, and other visualizations to
    explore data and results interactively.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可视化**：你可以嵌入图表、图形和其他可视化内容，以交互式地探索数据和结果。'
- en: '**Documentation**: Combining Markdown cells with code cells allows for well-documented,
    self-contained notebooks that explain the code and its output.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档**：将Markdown单元格与代码单元格结合使用，可以创建文档齐全、自包含的笔记本，解释代码及其输出。'
- en: 'A drawback to Jupyter notebooks is that they typically rely on the computational
    resources of your personal computer. Most personal laptops and desktops are not
    optimized or equipped to handle computationally intensive processes. Having adequate
    computational resources is crucial for managing the computational complexity of
    experimenting with an LLM. Fortunately, we can extend the capabilities of a Jupyter
    notebook with cloud-based platforms that offer computational accelerators such
    as **graphics processing units** (**GPUs**) and **tensor processing units** (**TPUs**).
    For example, Google Colab instantly enhances Jupyter notebooks, making them conducive
    to computationally intensive experimentation. Here are some of the key features
    of a cloud-based notebook environment such as Google Colab:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter笔记本的一个缺点是它们通常依赖于个人电脑的计算资源。大多数个人笔记本电脑和台式机都没有优化或配备处理计算密集型过程的硬件。拥有足够的计算资源对于管理实验LLM的计算复杂性至关重要。幸运的是，我们可以通过提供计算加速器（如**图形处理单元**（**GPU**）和**张量处理单元**（**TPU**））的云平台来扩展Jupyter笔记本的功能。例如，Google
    Colab可以立即增强Jupyter笔记本，使其有利于计算密集型实验。以下是云笔记本环境（如Google Colab）的一些关键特性：
- en: '**GPU/TPU access**: Provides free or affordable access to GPU and TPU resources
    for accelerated computation, which is crucial when working with demanding machine
    learning models'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPU/TPU访问**：提供免费或负担得起的GPU和TPU资源访问，这对于处理要求高的机器学习模型至关重要。'
- en: '**Collaboration**: Permits easy sharing and real-time collaboration, similar
    to Google Docs'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协作**：允许轻松共享和实时协作，类似于Google Docs。'
- en: '**Integration**: Allows for easy storage and access to notebooks and data'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成**：便于存储和访问笔记本和数据。'
- en: 'Let’s consider our StyleSprint scenario. We will want to explore a few different
    models to generate product descriptions before deciding on one that best fits
    StyleSprint’s goals. We can set up a minimal working prototype in Google Colab
    to compare models. Again, cloud-based platforms provide an optimal and accessible
    environment for initial testing, experimentation, and even some lightweight training
    of models. Here is how we might initially set up a generative model to start experimenting
    with automated product description generation for StyleSprint:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑我们的StyleSprint场景。在决定最适合StyleSprint目标的一个模型之前，我们可能想要探索几个不同的模型来生成产品描述。我们可以在Google
    Colab中设置一个最小的工作原型来比较模型。再次强调，云平台提供了一个最优且易于访问的环境，用于初始测试、实验，甚至是一些轻量级的模型训练。以下是我们可以如何初步设置一个生成模型，开始为StyleSprint进行自动化产品描述生成的实验：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Output:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：'
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this simple setup, we’re installing the `transformers` library, which offers
    a convenient interface to various pretrained models. We then initialize a text
    generation pipeline with an open source version of GPT-Neo, capable of generating
    coherent and contextually relevant text. This setup serves as a starting point
    for StyleSprint to experiment with generating creative product descriptions on
    a small scale.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的设置中，我们正在安装`transformers`库，它提供了一个方便的接口来访问各种预训练模型。然后，我们使用开源版本的GPT-Neo初始化一个文本生成管道，能够生成连贯且上下文相关的文本。这个设置作为StyleSprint在小型规模上实验生成创意产品描述的起点。
- en: Later in this chapter, we will expand our experiment to evaluate and compare
    multiple pretrained generative models to determine which best meets our needs.
    However, before advancing further in our experimentation and prototyping, it is
    crucial to strategically pause and project forward. This deliberate forethought
    allows us to consider the necessary steps for effectively transitioning our experiment
    into a production environment. By doing so, we ensure a comprehensive view of
    the project from end to end, to align with long-term operational goals.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面部分，我们将扩展我们的实验，评估和比较多个预训练的生成模型，以确定哪个最能满足我们的需求。然而，在我们进一步进行实验和原型设计之前，战略性地暂停并展望未来是至关重要的。这种深思熟虑的预见使我们能够考虑将实验有效过渡到生产环境所需的必要步骤。通过这样做，我们确保从端到端全面了解项目，以符合长期运营目标。
- en: '![Figure 4.1: Moving from prototyping to production—the stages](img/B21773_04_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1：从原型到生产的阶段](img/B21773_04_01.jpg)'
- en: 'Figure 4.1: Moving from prototyping to production—the stages'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：从原型到生产的阶段
- en: Transitioning to production
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转向生产
- en: 'As we plan for a production setup, we should first understand the intrinsic
    benefits and features of the prototyping environment we will want to carry forward
    to a production setting. Many of the features of prototyping environments such
    as Google Colab are deeply integrated and can easily go unnoticed, so it is important
    to dissect and catalog the features we will need in production. For example, the
    following features are inherent in Google Colab and will be critical in production:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们计划生产设置时，我们首先应该了解我们将要将其带入生产环境的原型环境的内在优势和功能。原型环境（如Google Colab）的许多功能都深度集成，并且可能很容易被忽视，因此重要的是要剖析和编制我们将需要的生产环境中的功能清单。例如，以下功能是Google
    Colab固有的，在生产环境中将至关重要：
- en: '`!pip install library_name`. In production, we will have to preinstall libraries
    or make sure we can install them as needed. We must also ensure that project-specific
    libraries do not interfere with other projects.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`!pip install library_name`。在生产环境中，我们必须预先安装库或确保我们可以在需要时安装它们。我们还必须确保特定于项目的库不会干扰其他项目。'
- en: '**Dependency isolation**: Google Colab automatically facilitates isolated dependencies,
    ensuring package installations and updates do not interfere with other projects.
    In production, we may also want to deploy various projects using the same infrastructure.
    Dependency isolation will be critical to prevent one project’s dependency updates
    from impacting other projects.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**依赖隔离**：Google Colab自动简化了依赖项的隔离，确保包安装和更新不会干扰其他项目。在生产环境中，我们也可能希望使用相同的架构部署各种项目。依赖隔离对于防止一个项目的依赖项更新影响其他项目至关重要。'
- en: '**Interactive code execution**: The interactive execution of code cells helps
    in testing individual code snippets, visualizing results, and debugging in real
    time. This convenience is not necessary in production but could be helpful for
    quick debugging.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交互式代码执行**：代码单元格的交互式执行有助于测试单个代码片段、可视化结果和实时调试。这种便利性在生产环境中并非必需，但对于快速调试可能有所帮助。'
- en: '**Resource accessibility**: With Colab, access to GPUs and TPUs is simplified,
    which is crucial for running computation-intensive tasks. For production, we will
    want to examine our dynamic computational needs and provision the appropriate
    infrastructure.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源可访问性**：使用Colab，访问GPU和TPU变得简单，这对于运行计算密集型任务至关重要。对于生产环境，我们将想要检查我们的动态计算需求，并配置适当的架构。'
- en: '**Data integration**: Colab offers simple connectivity to data sources for
    analysis and modeling. In production, we can either bootstrap our environment
    with data (i.e., deploy data directly into the environment) or ensure connectivity
    to remote data sources as needed.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集成**：Colab为分析和建模提供了简单的数据源连接。在生产环境中，我们可以通过将数据直接部署到环境中来启动我们的环境，或者根据需要确保与远程数据源的连接。'
- en: '**Versioning and collaboration**: Tracking versions of your project code with
    Google Colab can easily be accomplished using notebooks. Additionally, Colab is
    preconfigured to interact with Git. Git is a distributed version control system
    that is widely used for tracking changes in source code during software development.
    In production, we will also want to integrate Git to manage our code and synchronize
    it with a remote code repository such as GitHub or Bitbucket. Remote versioning
    ensures that our production environment always reflects the latest changes and
    enables ongoing collaboration.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**版本控制和协作**：使用 Google Colab 跟踪项目代码的版本很容易通过笔记本完成。此外，Colab 预配置为与 Git 交互。Git 是一个广泛用于在软件开发过程中跟踪源代码更改的分布式版本控制系统。在生产中，我们还将希望集成
    Git 来管理我们的代码并将其与远程代码仓库（如 GitHub 或 Bitbucket）同步。远程版本控制确保我们的生产环境始终反映最新的更改，并允许持续的协作。'
- en: '**Error handling and debugging**: In Colab, we have direct access to the Python
    runtime and can typically see error messages and tracebacks in real time to help
    identify and resolve issues. We will want the same level of visibility in production
    via adequate logging of system errors. In total, we want to carry over the convenience
    and simplicity of our Google Colab prototyping environment but provide the robustness
    and scalability required for production. To do so, we will map each of the key
    characteristics we laid out to a corresponding production solution. These key
    features should ensure a smooth transition for deploying StyleSprint’s generative
    model for automated product description generation.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误处理和调试**：在 Colab 中，我们直接访问 Python 运行时，通常可以实时看到错误信息和回溯，以帮助识别和解决问题。我们希望在生产环境中通过适当的系统错误记录达到相同级别的可见性。总的来说，我们希望保留我们
    Google Colab 原型环境中的便利性和简单性，但提供生产所需的鲁棒性和可扩展性。为此，我们将我们将我们提出的每个关键特性映射到相应的生产解决方案。这些关键特性应确保
    StyleSprint 自动化产品描述生成模型的部署过程顺利。'
- en: Mapping features to production setup
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将特性映射到生产设置
- en: 'To ensure we can seamlessly transition our prototyping environment to production,
    we can leverage Docker, a leading containerization tool. **Containerization**
    tools package applications with their dependencies for consistent performance
    across different systems. A containerized approach will help us replicate Google
    Colab’s isolated, uniform environments, ensuring reliability and reducing potential
    compatibility issues in production. The table that follows describes how we can
    map each of the benefits of our prototyping environment to a production analog:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们可以无缝地将我们的原型环境过渡到生产环境，我们可以利用 Docker，这是一个领先的容器化工具。**容器化**工具将应用程序及其依赖项打包，以确保在不同系统上的一致性能。容器化方法将帮助我们复制
    Google Colab 的隔离、统一环境，确保可靠性并减少生产中的潜在兼容性问题。下表描述了如何将我们原型环境的每个好处映射到生产类似物：
- en: '| **Feature** | **Environment** |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **特性** | **环境** |'
- en: '| --- | --- |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|  | **Prototyping** | **Production** |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | **原型设计** | **生产** |'
- en: '| --- | --- | --- |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Package management | Inherent through preinstalled package managers | Docker
    streamlines application deployment and consistency across environments including
    package managers. |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 软件包管理 | 通过预安装的软件包管理器固有的 | Docker 简化了应用程序的部署，并在包括软件包管理器在内的环境中保持一致性。|'
- en: '| Dependency isolation | Inherent through notebooks | Docker can also ensure
    projects are cleanly isolated. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 依赖项隔离 | 通过笔记本固有的 | Docker 还可以确保项目被干净地隔离。|'
- en: '| Interactive code execution | Inherent through notebooks | Docker helps to
    maintain versions of Python that provide interactive code execution by default.
    However, we may want to connect an **integrated development environment** (**IDE**)
    to our production environment to interact with code remotely as needed. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 交互式代码执行 | 通过笔记本固有的 | Docker 通过默认提供交互式代码执行的 Python 版本来帮助维护版本。然而，我们可能希望将**集成开发环境（IDE**）连接到我们的生产环境，以便在需要时远程与代码交互。|'
- en: '| Resource accessibility | Inherent for cloud-based notebooks | GPU-enabled
    Docker containers enhance production by enabling structured GPU utilization, allowing
    scalable, efficient model performance. |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 资源可访问性 | 对于基于云的笔记本固有 | 具有GPU功能的 Docker 容器通过允许结构化 GPU 利用，增强了生产，允许可扩展、高效的模型性能。|'
- en: '| Data integration | Not inherent, and requires code-based integration | Integrating
    Docker with a remote data source, such as AWS S3 or Google Cloud Storage, provides
    secure and scalable solutions for importing and exporting data. |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 数据集成 | 不是固有的，需要基于代码的集成 | 将 Docker 与远程数据源（如 AWS S3 或 Google Cloud Storage）集成，为导入和导出数据提供安全且可扩展的解决方案。|'
- en: '| Versioning and collaboration | Inherent through notebooks and preconfigured
    for Git | Integrating Docker with platforms such as GitHub or GitLab enables code
    collaboration and documentation. |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 版本控制和协作 | 通过笔记本和预配置的 Git 实现 | 将 Docker 与 GitHub 或 GitLab 等平台集成，可以促进代码协作和文档编制。|'
- en: '| Error handling and debugging | Inherent through direct interactive access
    to runtime | We can embed Python libraries such as `logging` or `Loguru` in Docker
    deployments for enhanced error tracking in production. |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 错误处理和调试 | 通过直接交互式访问运行时实现 | 我们可以在 Docker 部署中嵌入 Python 库，如 `logging` 或 `Loguru`，以在生产中增强错误跟踪。|'
- en: 'Table 4.1: Transitioning features from Colab to production via Docker'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1：通过 Docker 从 Colab 到生产的过渡功能
- en: Having mapped out the features of our prototyping environment to corresponding
    tools and practices for a production setup, we are now better prepared to implement
    a generative model for StyleSprint in a production-ready environment. The transition
    entails setting up a stable, scalable, and reproducible Python environment, a
    crucial step for deploying our generative model to automate the generation of
    product descriptions in a real-world setting. As discussed, we can leverage Docker
    in tandem with GitHub and its **continuous integration/continuous deployment**
    (**CI/CD**) capabilities, providing a robust framework for this production deployment.
    A CI pipeline automates the integration of code changes from multiple contributors
    into a shared repository. We pair CI with CD to automate the deployment of our
    code to a production environment.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在将我们的原型环境的功能映射到生产设置中相应的工具和实践之后，我们现在更好地准备在可生产环境中实施 StyleSprint 的生成模型。这一过渡包括设置一个稳定、可扩展和可重复的
    Python 环境，这是将我们的生成模型部署到实际环境中自动化生成产品描述的关键步骤。正如所讨论的，我们可以利用 Docker 与 GitHub 及其 **持续集成/持续部署**（**CI/CD**）功能相结合，为这种生产部署提供一个强大的框架。CI
    管道自动化了来自多个贡献者的代码更改的集成到一个共享仓库中。我们将 CI 与 CD 配对，以自动化将我们的代码部署到生产环境。
- en: Setting up a production-ready environment
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置生产环境
- en: So far, we have discussed how to bridge the gap between prototyping and production
    environments. Cloud-based environments such as Google Colab provide a wealth of
    features that are not inherently available in production. Now that we have a better
    understanding of those characteristics, the next step is to implement a robust
    production setup to ensure that our application can handle real-world traffic,
    scale as needed, and remain stable over time.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了如何弥合原型和生产环境之间的差距。基于云的环境，如 Google Colab，提供了在生产环境中本身不可用的丰富功能。现在，我们对这些特性有了更好的理解，下一步是实现一个健壮的生产设置，以确保我们的应用程序能够处理现实世界的流量，按需扩展，并保持稳定。
- en: The tools and practices in a production environment differ significantly from
    those in a prototyping environment. In production, scalability, reliability, resource
    management, and security become paramount, whereas, in a prototyping environment,
    the models are only relied upon by a few users for experimentation. In production,
    we could expect large-scale consumption from divisions throughout the organization.
    For example, in the StyleSprint scenario, there may be multiple departments or
    sub-brands hoping to automate their product descriptions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 生产环境中的工具和实践与原型环境中的工具和实践有很大不同。在生产中，可扩展性、可靠性、资源管理和安全性变得至关重要，而在原型环境中，模型仅由少数用户用于实验。在生产中，我们可能期望来自组织各个部门的广泛消费。例如，在
    StyleSprint 场景中，可能有多个部门或子品牌希望自动化他们的产品描述。
- en: In the early stages of our StyleSprint project, we can use free and open source
    tools such as Docker and GitHub for tasks such as containerization, version control,
    and CI. These tools are offered and managed by a community of users, giving us
    a cost-effective solution. As StyleSprint expands, we might consider upgrading
    to paid or enterprise editions that offer advanced features and professional support.
    For the moment, our focus is on leveraging the capabilities of the open source
    versions. Next, we will walk through the practical implementation of these tools
    step by step. By the end, we will be ready to deploy a production-ready **model-as-a-service**
    (**MaaS**) for automatic product descriptions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 StyleSprint 项目的早期阶段，我们可以使用 Docker 和 GitHub 等免费和开源工具来执行容器化、版本控制和 CI 等任务。这些工具由用户社区提供和管理，为我们提供了一个成本效益高的解决方案。随着
    StyleSprint 的发展，我们可能会考虑升级到提供高级功能和专业支持的付费或企业版。目前，我们的重点是利用开源版本的功能。接下来，我们将逐步介绍这些工具的实际应用。到那时，我们将准备好部署一个适用于自动产品描述的生产就绪的
    **模型即服务**（**MaaS**）。
- en: Local development setup
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地开发设置
- en: We begin by making sure we can connect to a production environment remotely.
    We can leverage an IDE, which is software that enables us to easily organize code
    and remotely connect to the production environment.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先确保我们可以远程连接到生产环境。我们可以利用一个集成开发环境（IDE），这是一种软件，使我们能够轻松组织代码并远程连接到生产环境。
- en: Visual Studio Code
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Visual Studio Code
- en: Begin by installing **Visual Studio Code** (**VS Code**), a free code editor
    by Microsoft. It is preferred for its integrated Git control, terminal, and marketplace
    for extensions that enhance its functionality. It provides a conducive environment
    for writing, testing, and debugging code.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先安装 **Visual Studio Code**（**VS Code**），这是微软提供的一款免费代码编辑器。由于其集成的 Git 控制、终端和扩展市场，它被广泛使用。它提供了一个有利于编写、测试和调试代码的环境。
- en: Project initialization
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目初始化
- en: Next, we set up a structured project directory to keep the code modular and
    organized. We will also initialize our working directory with Git, which enables
    us to synchronize code with a remote repository. As mentioned, we leverage Git
    to keep track of code changes and collaborate with others more seamlessly. Using
    the terminal window in Visual Studio, we can initialize the project using three
    simple commands. We use `mkdir` to create or “make” a directory. We use the `cd`
    command to change directories. Finally, we use `git init` to initialize our project
    with Git. Keep in mind that this assumes Git is installed. Instructions to install
    Git are made available on its website ([https://git-scm.com/](https://git-scm.com/)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置一个结构化的项目目录以保持代码模块化和有序。我们还将使用 Git 初始化我们的工作目录，这使得我们能够与远程仓库同步代码。正如之前提到的，我们利用
    Git 来跟踪代码变更并与他人更顺畅地协作。在 Visual Studio 的终端窗口中，我们可以使用三个简单的命令来初始化项目。我们使用 `mkdir`
    来创建或“制作”一个目录。我们使用 `cd` 命令来更改目录。最后，我们使用 `git init` 来使用 Git 初始化我们的项目。请注意，这假设 Git
    已经安装。有关安装 Git 的说明可在其网站上找到（[https://git-scm.com/](https://git-scm.com/))。
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Docker setup
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker 设置
- en: 'We’ll now move on to setting up a Docker container. A Docker container is an
    isolated environment that encapsulates an application and its dependencies, ensuring
    consistent operation across different systems. For clarity, we can briefly describe
    the key aspects of Docker as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将继续设置 Docker 容器。Docker 容器是一个隔离的环境，封装了应用程序及其依赖项，确保在不同系统上的一致性操作。为了清晰起见，我们可以简要描述
    Docker 的关键方面如下：
- en: '**Containers**: These are portable units comprising the application and its
    dependencies.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器**：这些是包含应用程序及其依赖的可移植单元。'
- en: '**Host operating system’s kernel**: When a Docker container is run on a host
    machine, it utilizes the kernel of the host’s operating system and resources to
    operate, but it does so in a way that is isolated from both the host system and
    other containers.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主机操作系统的内核**：当 Docker 容器在主机机器上运行时，它利用主机操作系统的内核和资源来操作，但它以隔离的方式操作，既不与主机系统也不与其他容器冲突。'
- en: '**Dockerfiles**: These are scripts used to create container images. They serve
    as a blueprint containing everything needed to run the application. This isolation
    and packaging method prevents application conflicts and promotes efficient resource
    use, streamlining development and deployment.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dockerfiles**：这些是用于创建容器镜像的脚本。它们作为蓝图，包含运行应用程序所需的一切。这种隔离和打包方法防止应用程序冲突并促进资源的高效使用，简化了开发和部署。'
- en: A containerized approach will help ensure consistency and portability. For example,
    assume StyleSprint finds a cloud-based hosting provider that is more cost-effective;
    moving to the new provider is as simple as migrating a few configuration files.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 容器化方法将有助于确保一致性和可移植性。例如，假设StyleSprint找到一个基于云的托管提供商，成本效益更高；迁移到新提供商就像迁移几个配置文件一样简单。
- en: We can install Docker from the official website. Docker provides easy-to-follow
    installation guides including support for various programming languages.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从官方网站安装Docker。Docker提供了易于遵循的安装指南，包括对各种编程语言的支持。
- en: Once Docker is installed, we can create a Dockerfile in the project directory
    to specify the environment setup. For GPU support, we will want to start from
    an NVIDIA CUDA base image. Docker, like many other virtualized systems, operates
    using a concept called **images**. Images are a snapshot of a preconfigured environment
    that can be used as a starting point for a new project. In our case, we will want
    to start with a snapshot that integrates GPU support using the CUDA library, which
    is a parallel processing library provided by NVIDIA. This library will enable
    the virtualized environment (or container) to leverage any GPUs installed on the
    host machine. Leveraging GPUs will accelerate model inferencing.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了Docker，我们就可以在项目目录中创建一个Dockerfile来指定环境设置。对于GPU支持，我们希望从一个NVIDIA CUDA基础镜像开始。Docker，就像许多其他虚拟化系统一样，使用一个称为**镜像**的概念来运行。镜像是一个预配置环境的快照，可以用作新项目的起点。在我们的情况下，我们希望从一个集成了CUDA库的快照开始，CUDA是NVIDIA提供的并行处理库。这个库将使虚拟化环境（或容器）能够利用主机机器上安装的任何GPU。利用GPU将加速模型推理。
- en: 'Now we can go ahead and create a Dockerfile with the specifications for our
    application:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以继续创建一个具有我们应用程序规格的Dockerfile：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This Dockerfile serves as a blueprint that Docker follows to build our container.
    We initiate the process from an official NVIDIA CUDA base image to ensure GPU
    support. The working directory in the container is set to `/app`, where we then
    copy the contents of our project. Following that, we install the necessary packages
    listed in the `requirements.txt` file. `Port 80` is exposed for external access
    to our application. Lastly, we specify the command to launch our application,
    which is running `app.py` using the Python interpreter. This setup encapsulates
    all the necessary components, including GPU support, to ensure our generative
    model operates efficiently in a production-like environment.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此Dockerfile作为Docker构建我们的容器的蓝图。我们从官方的NVIDIA CUDA基础镜像开始，以确保GPU支持。容器中的工作目录设置为`/app`，然后我们复制项目的所有内容。之后，我们安装`requirements.txt`文件中列出的必要包。`端口80`对外部访问我们的应用程序开放。最后，我们指定启动应用程序的命令，该应用程序使用Python解释器运行`app.py`。这种设置封装了所有必要的组件，包括GPU支持，以确保我们的生成模型在类似生产的环境中高效运行。
- en: Requirements file
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 需求文件
- en: 'We also need a method for keeping track of our Python-specific dependencies.
    The container will include Python but will not have any indication as to what
    requirements our Python application has. We can specify those dependencies explicitly
    by defining a `requirements.txt` file in our project directory to list all the
    necessary Python packages:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个方法来跟踪我们的Python特定依赖。容器将包含Python，但不会显示我们的Python应用程序有什么需求。我们可以在项目目录中定义一个`requirements.txt`文件，明确列出所有必要的Python包来指定这些依赖：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Application code
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用程序代码
- en: Now we can create an `app.py` file for our application code. This is where we
    will write the code for our generative model, leveraging libraries such as PyTorch
    and Transformers. To expose our model as a service, we will use *FastAPI*, a modern,
    high-performance framework for building web APIs. A web API is a protocol that
    enables different software applications to communicate and exchange data over
    the internet, allowing them to use each other’s functions and services.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以为我们的应用程序代码创建一个`app.py`文件。这是我们编写生成模型代码的地方，利用PyTorch和Transformers等库。为了将我们的模型作为服务公开，我们将使用*FastAPI*，这是一个用于构建Web
    API的现代、高性能框架。Web API是一种协议，它使不同的软件应用程序能够通过互联网进行通信和交换数据，允许它们使用彼此的功能和服务。
- en: 'The following snippet creates a minimal API that will serve the model responses
    whenever another application or software requests the `/generate/` endpoint. This
    will enable StyleSprint to host its model as a web service. This means that other
    applications (e.g., mobile apps, batch processes) can access the model using a
    simple URL. We can also add exception handling to provide an informative error
    message should the model produce any kind of error:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段创建了一个最小化的 API，该 API 将在另一个应用程序或软件请求 `/generate/` 端点时提供模型响应。这将使 StyleSprint
    能够将其模型作为 Web 服务托管。这意味着其他应用程序（例如，移动应用程序、批处理过程）可以使用简单的 URL 访问模型。我们还可以添加异常处理以提供有关模型产生任何类型错误的
    informative 错误消息：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that we have a Docker setup, the next step is to deploy the application
    to the host server. We can streamline this process with a CI/CD pipeline. The
    goal is to fully automate all deployment steps, including a suite of tests to
    ensure that any code changes do not introduce any errors. We then leverage GitHub
    Actions to create a workflow that is directly integrated with a code repository.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了 Docker 环境，下一步是将应用程序部署到主机服务器。我们可以通过 CI/CD 管道来简化此过程。目标是完全自动化所有部署步骤，包括一系列测试以确保任何代码更改不会引入任何错误。然后我们利用
    GitHub Actions 创建一个与代码仓库直接集成的工作流程。
- en: Creating a code repository
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建代码仓库
- en: 'Before we can leverage the automation capabilities of GitHub, we will need
    a repository. Creating a GitHub repository is straightforward, following these
    steps:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够利用 GitHub 的自动化功能之前，我们需要一个仓库。创建 GitHub 仓库非常简单，按照以下步骤操作：
- en: '**Sign up/log in to GitHub**: If you don’t have a GitHub account, sign up at
    [github.com](http://github.com). If you already have an account, just log in.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**注册/登录 GitHub**：如果您还没有 GitHub 账户，请在 [github.com](http://github.com) 上注册。如果您已经有了账户，只需登录即可。'
- en: '**Go to the repository creation page**: Click the **+** icon in the top-right
    corner of the GitHub home page and select **New repository**.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**转到仓库创建页面**：点击 GitHub 主页右上角的 **+** 图标并选择 **新建仓库**。'
- en: '**Fill in the** **repository details**:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**填写仓库详情**：'
- en: '**Repository Name**: Choose a name for your repository'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仓库名称**：为您的仓库选择一个名称'
- en: '**Description** (optional): Add a brief description of your repository'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述**（可选）：添加您仓库的简要描述'
- en: '**Visibility**: Select either **Public** (anyone can see this repository) or
    **Private** (only you and the collaborators you invite can see it)'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可见性**：选择 **公开**（任何人都可以看到此仓库）或 **私有**（只有您和您邀请的协作者可以看到）'
- en: '`.gitignore` file or choose a license. A `gitignore` file allows us to add
    paths or file types that should not be uploaded to the repository. For example,
    Python creates temporary files that are not critical to the application. Adding
    `` `__pycache__/` `` to the `gitignore` file will automatically ignore all contents
    of that directory.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.gitignore` 文件或选择一个许可证。一个 `gitignore` 文件允许我们添加不应上传到仓库的路径或文件类型。例如，Python 会创建对应用程序不关键的临时文件。将
    `` `__pycache__/` `` 添加到 `gitignore` 文件中将会自动忽略该目录下的所有内容。'
- en: '**Create repository**: Click the **Create** **repository** button.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建仓库**：点击 **创建仓库** 按钮。'
- en: With our repository setup complete, we can move on to defining our CI/CD pipeline
    to automate our deployments.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的仓库设置完成后，我们可以继续定义我们的 CI/CD 管道来自动化我们的部署。
- en: CI/CD setup
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CI/CD 配置
- en: 'To create a pipeline, we will need a configuration file that outlines the stages
    of deployment and instructs the automation server to build and deploy our Docker
    container. Let’s look at the steps:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个管道，我们需要一个配置文件，该文件概述了部署阶段并指导自动化服务器构建和部署我们的 Docker 容器。让我们看看步骤：
- en: In our GitHub repository, we can create a new file in the `.github/workflows`
    directory named `ci-cd.yml`. GitHub will automatically find any files in this
    directory to trigger deployments.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的 GitHub 仓库中，我们可以在 `.github/workflows` 目录下创建一个名为 `ci-cd.yml` 的新文件。GitHub
    将自动查找此目录中的任何文件以触发部署。
- en: 'Open `ci-cd.yml` and define the following workflow:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 `ci-cd.yml` 并定义以下工作流程：
- en: '[PRE6]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In this setup, our workflow consists of two primary jobs: build-and-test and
    deploy. The build-and-test job is responsible for checking out the code from the
    repository, building the Docker image, and executing any tests. On the other hand,
    the deploy job, which relies on completing build-and-test, handles *DockerHub*
    login and pushes the Docker image there. DockerHub, similar to GitHub, is a repository
    specifically for Docker images.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设置中，我们的工作流程由两个主要任务组成：构建和测试以及部署。构建和测试任务负责从代码库检出代码，构建Docker镜像，并执行任何测试。另一方面，依赖于构建和测试完成的部署任务，处理*DockerHub*登录并将Docker镜像推送到那里。DockerHub，类似于GitHub，是一个专门用于Docker镜像的仓库。
- en: For authenticating with DockerHub, it is advised to securely store your DockerHub
    credentials in your GitHub repository. This can be done by navigating to your
    repository on GitHub, clicking on `DOCKER_USERNAME` and `DOCKER_PASSWORD` as new
    repository secrets.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于与DockerHub进行身份验证，建议将您的DockerHub凭据安全地存储在您的GitHub仓库中。这可以通过在GitHub上导航到您的仓库，点击`DOCKER_USERNAME`和`DOCKER_PASSWORD`作为新的仓库密钥来完成。
- en: Notice that we did not have to perform any additional steps to execute the pipeline.
    The workflow is designed to trigger automatically upon a push (or upload) to the
    main branch. Recall that the entire process relies on the Git pattern where new
    changes are registered through a `commit` or check-in of code and a `push` or
    upload of code changes. Whenever changes are pushed, we can directly observe the
    entire pipeline in action within the **Actions** tab of the GitHub repository.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不需要执行任何额外的步骤来执行管道。工作流程设计为在向主分支推送（或上传）时自动触发。回想一下，整个过程依赖于Git模式，其中新更改通过代码的`commit`或检查-入以及代码更改的`push`或上传来注册。每当有更改被推送时，我们都可以直接在GitHub仓库的**操作**选项卡中观察到整个管道的运行情况。
- en: We have now walked through all of the steps necessary to deploy our model to
    production. With all of this critical setup behind us, we can now return to choosing
    the best model for our project. The goal is to find a model that can effectively
    generate captivating product descriptions for StyleSprint. However, the variety
    of generative models available requires a thoughtful choice based on our project’s
    needs and constraints.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经走过了部署我们的模型到生产所需的全部步骤。在所有这些关键的设置都完成之后，我们现在可以回到选择我们项目最佳模型的任务上。目标是找到一个能够有效地为StyleSprint生成吸引人的产品描述的模型。然而，可用的生成模型种类繁多，需要根据我们项目的需求和限制进行深思熟虑的选择。
- en: Moreover, we want to choose the right evaluation metrics and discuss other considerations
    that will guide us in making an informed decision for our project. This exploration
    will equip us with the knowledge needed to select a model that not only performs
    well but also aligns with our project objectives and the technical infrastructure
    we have established.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们希望选择合适的评估指标，并讨论其他将指导我们为项目做出明智决策的考虑因素。这次探索将使我们具备选择一个不仅表现良好而且与我们的项目目标和已建立的技术基础设施相一致的模式所需的知识。
- en: Model selection – choosing the right pretrained generative model
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型选择——选择合适的预训练生成模型
- en: Having established a minimal production environment in the previous section,
    we now focus on a pivotal aspect of our project – selecting the right generative
    model for generating engaging product descriptions. The choice of model is crucial
    as it significantly influences the effectiveness and efficiency of our solution.
    The objective is to automate the generation of compelling and accurate product
    descriptions for StyleSprint’s diverse range of retail products. By doing so,
    we aim to enrich the online shopping experience for customers while alleviating
    the manual workload of crafting unique product descriptions.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中建立了一个最小化生产环境之后，我们现在关注我们项目的关键方面——选择合适的生成模型来生成吸引人的产品描述。模型的选择至关重要，因为它显著影响我们解决方案的有效性和效率。目标是自动化生成StyleSprint各种零售产品的引人入胜且准确的产品描述。通过这样做，我们旨在丰富客户的在线购物体验，同时减轻手动编写独特产品描述的工作量。
- en: Our objective is to select a generative model that can adeptly handle nuanced
    and sophisticated text generation to significantly expedite the process of creating
    unique, engaging product descriptions, saving time and resources for StyleSprint.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目的是选择一个能够熟练处理细微和复杂的文本生成，从而显著加快创建独特、吸引人的产品描述的过程的生成模型，为StyleSprint节省时间和资源。
- en: In selecting our model, it is important to thoroughly evaluate various factors
    influencing its performance and suitability for the project.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择我们的模型时，重要的是要彻底评估影响其性能和适合项目的各种因素。
- en: Meeting project objectives
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 达成项目目标
- en: 'Before we can select and apply evaluation methods to our model selection process,
    we should first make sure we understand the project objectives. This involves
    defining the business problem, identifying any technical constraints, identifying
    any risk associated with the model, including interpretation of model outcomes,
    and ascertaining considerations for any potential disparate treatment or bias:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够选择并应用评估方法到我们的模型选择过程中之前，我们首先应该确保我们理解了项目目标。这包括定义业务问题，识别任何技术限制，识别与模型相关的任何风险，包括模型结果的解释，以及确定任何潜在的不平等对待或偏见的考虑因素：
- en: '**Problem definition**: In our scenario, the goal is to create accurate and
    engaging descriptions for a wide range of retail clothing. As StyleSprint’s product
    range may expand, the system should scale seamlessly to accommodate a larger inventory
    without significantly increasing operational costs. Performance expectations include
    compelling descriptions to attract potential customers, accuracy to avoid misrepresentation,
    and prompt generation to maintain an up-to-date online catalog. Additionally,
    StyleSprint may apply personalized content descriptions based on a user’s shopping
    history. This implies that the model may have to provide product descriptions
    in near-real-time.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题定义**：在我们的场景中，目标是创建一系列零售服装的准确和吸引人的描述。随着StyleSprint的产品范围可能扩大，系统应无缝扩展以适应更大的库存，而不会显著增加运营成本。性能预期包括吸引潜在客户的引人入胜的描述，准确性以避免误代表，以及快速生成以保持在线目录的更新。此外，StyleSprint可能会根据用户的购物历史应用个性化的内容描述。这意味着模型可能需要几乎实时地提供产品描述。'
- en: '**Technical constraints**: To maximize efficiency, there should not be any
    noticeable delay (latency) in responses from the model API. The system should
    be capable of real-time updates to the online catalog (as needed), and the hardware
    should support quick text generation without compromising quality while remaining
    cost-effective, especially as the product range expands.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**技术限制**：为了最大化效率，模型API的响应不应有任何明显的延迟（延迟）。系统应能够实时更新在线目录（如有需要），并且硬件应支持快速文本生成，同时不降低质量且保持成本效益，尤其是在产品范围扩大的情况下。'
- en: '**Transparency and openness**: Generally, pretrained models from developers
    who disclose architectures and training data sources are preferred, as this level
    of transparency allows StyleSprint to have a clear understanding of any risks
    or legal implications associated with model use. Additionally, any usage restrictions
    imposed by using models provided as APIs, such as request or token limitations,
    should be understood as they could hinder scalability for a growing catalog.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明度和开放性**：通常，来自披露架构和训练数据源的开发商的预训练模型更受欢迎，因为这种透明度使StyleSprint能够清楚地了解与模型使用相关的任何风险或法律影响。此外，使用作为API提供的模型时施加的任何使用限制，如请求或令牌限制，应被理解，因为它们可能会阻碍不断增长的目录的可扩展性。'
- en: '**Bias and fairness**: Identifying and mitigating biases in model outputs to
    ensure fair and neutral representations is crucial, especially given StyleSprint’s
    diverse target audience. Ensuring that the generated descriptions are culturally
    sensitive is of paramount importance. Fair representation ensures that the descriptions
    accurately and fairly represent the products to all potential customers, irrespective
    of their individual characteristics or social backgrounds.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏见和公平性**：识别和减轻模型输出中的偏见，以确保公平和中立的表现至关重要，尤其是在StyleSprint具有多样化的目标受众的情况下。确保生成的描述具有文化敏感性至关重要。公平的表现确保描述准确且公平地代表产品，无论潜在客户的个人特征或社会背景如何。'
- en: '**Suitability of pretraining**: The underlying pretraining of generative models
    plays a significant role in their ability to generate meaningful and relevant
    text. Investigating the domains and data on which the models were pretrained or
    fine-tuned is important. A model pretrained on a broad dataset may be versatile
    but could lack domain-specific nuances. For StyleSprint, a model that is fine-tuned
    on fashion-related data or that has the ability to be fine-tuned on such data
    would be ideal to ensure the generated descriptions are relevant and appealing.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预训练的适用性**：生成模型的基本预训练在其生成有意义和相关的文本的能力中起着重要作用。研究模型预训练或微调的领域和数据非常重要。在广泛数据集上预训练的模型可能具有多功能性，但可能缺乏特定领域的细微差别。对于StyleSprint来说，一个在时尚相关数据上微调或能够进行此类微调的模型将是最理想的，以确保生成的描述既相关又吸引人。'
- en: '**Quantitative metrics**: Evaluating the quality of generated product descriptions
    for StyleSprint necessitates a combination of lexical and semantic metrics. Lexical
    overlap metrics measure the lexical similarity between generated and reference
    texts. Specifically, **Bilingual Evaluation Understudy** (**BLEU**) emphasizes
    n-gram precision, **Recall-Oriented Understudy for Gisting Evaluation** (**ROUGE**)
    focuses on n-gram recall, and **Metric for Evaluation of Translation with Explicit
    Ordering** (**METEOR**) aims for a more balanced evaluation by considering synonyms
    and stemming. For contextual and semantic evaluation, we use similarity metrics
    to assess the semantic coherence and relevance of the generated descriptions,
    often utilizing embeddings to represent text in a way that captures its meaning.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定量指标**：评估StyleSprint生成的产品描述的质量需要结合词汇和语义指标。词汇重叠指标衡量生成文本与参考文本之间的词汇相似度。具体来说，**双语评估助手**（**BLEU**）强调n-gram精确度，**基于主旨的评估辅助工具**（**ROUGE**）侧重于n-gram召回率，而**具有显式排序的翻译评估指标**（**METEOR**）旨在通过考虑同义词和词干实现更平衡的评估。对于上下文和语义评估，我们使用相似性指标来评估生成描述的语义一致性和相关性，通常利用嵌入来表示文本，以捕捉其含义。'
- en: 'We can further refine our assessment of the alignment between generated descriptions
    and product images using models such as **Contrastive Language-Image Pretraining**
    (**CLIP**). Recall that we used CLIP in [*Chapter 2*](B21773_02.xhtml#_idTextAnchor045)
    to score the compatibility between captions and a synthesized image. In this case,
    we can apply CLIP to measure whether our generated descriptions accurately reflect
    the visual aspects of the products. Collectively, these evaluation techniques
    provide objective methods for assessing the performance of the generative model
    in creating effective product descriptions for StyleSprint:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用诸如**对比语言-图像预训练**（**CLIP**）等模型进一步细化我们对生成描述与产品图像之间对齐的评估。回想一下，我们在[*第二章*](B21773_02.xhtml#_idTextAnchor045)中使用了CLIP来评估字幕与合成图像之间的兼容性。在这种情况下，我们可以应用CLIP来衡量我们的生成描述是否准确地反映了产品的视觉方面。这些评估技术共同为StyleSprint创建有效的产品描述提供了客观的方法：
- en: '**Qualitative metrics**: We introduce qualitative evaluation to measure nuances
    such as the engaging and creative nature of descriptions. We also want to ensure
    we consider equity and inclusivity in the generated content, which is critical
    to avoid biases or language that could alienate or offend certain groups. Methods
    for engagement assessment could include customer surveys or A/B testing, a systematic
    method for testing two competing solutions. Additionally, having a diverse group
    reviewing the content for equity and inclusivity could provide valuable insights.
    These steps help StyleSprint create captivating, respectful, and inclusive product
    descriptions, fostering a welcoming environment for all customers.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定性指标**：我们引入定性评估来衡量描述的吸引力和创造性等细微差别。我们还希望确保在生成内容中考虑公平性和包容性，这对于避免可能使某些群体感到疏远或冒犯的语言或偏见至关重要。参与度评估的方法可能包括客户调查或A/B测试，这是一种测试两种竞争解决方案的系统方法。此外，让一个多元化的团队审查内容以确保公平性和包容性可以提供宝贵的见解。这些步骤有助于StyleSprint创建吸引人、尊重和包容的产品描述，为所有客户提供欢迎的环境。'
- en: '**Scalability**: The computational resources required to run a model and the
    model’s ability to scale with increasing data are vital considerations. Models
    that demand extensive computational power may not be practical for real-time generation
    of product descriptions, especially as the product range expands. A balance between
    computational efficiency and output quality is essential to ensure cost-effectiveness
    and scalability for StyleSprint.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：运行模型所需的计算资源以及模型随着数据增加而扩展的能力是至关重要的考虑因素。需要大量计算能力的模型可能不适合实时生成产品描述，尤其是在产品系列扩大的情况下。在计算效率和输出质量之间取得平衡对于确保StyleSprint的成本效益和可扩展性至关重要。'
- en: '**Customization and fine-tuning capabilities**: The ability to fine-tune or
    customize the model on domain-specific data is crucial for better aligning with
    brand-specific requirements. Exploring the availability and ease of fine-tuning
    can significantly impact the relevance and quality of generated descriptions,
    ensuring that they resonate well with the brand identity and product range of
    StyleSprint. In practice, some models are too large to fine-tune without considerable
    resources, even when efficient methods are applied. We will explore fine-tuning
    considerations in detail in the next chapter.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定制和微调能力**：在特定领域数据上微调或定制模型的能力对于更好地满足品牌特定要求至关重要。探索微调的可用性和易用性可以显著影响生成描述的相关性和质量，确保它们与StyleSprint的品牌身份和产品系列产生共鸣。在实践中，即使应用了高效的方法，一些模型也可能因为需要大量资源而无法进行微调。我们将在下一章中详细探讨微调的考虑因素。'
- en: 'Now that we have carefully considered how we might align the model to the project’s
    goals, we are almost ready to evaluate our initial model selection against a few
    others to ensure we make the right choice. However, before benchmarking, we should
    dedicate time to understanding one vital aspect of the model selection process:
    model size and computational complexity.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经仔细考虑了如何将模型与项目目标对齐，我们几乎准备好评估我们的初始模型选择与其他几个模型，以确保我们做出正确的选择。然而，在基准测试之前，我们应该花时间理解模型选择过程中的一个重要方面：模型大小和计算复杂度。
- en: Model size and computational complexity
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型大小和计算复杂度
- en: The size of a generative model is often described by the number of parameters
    it has. Parameters in a model are the internal variables that are fine-tuned during
    the training process based on the training data. In the context of neural networks
    used in generative models, parameters typically refer to the weights and biases
    adjusted through training to minimize the discrepancy between predicted outputs
    and actual targets.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型的大小通常由其参数数量来描述。模型中的参数是在训练过程中根据训练数据微调的内部变量。在生成模型中使用的神经网络背景下，参数通常指的是通过训练调整的权重和偏差，以最小化预测输出与实际目标之间的差异。
- en: 'Moreover, a model with more parameters can capture more complex patterns in
    the data, often leading to better performance on the task at hand. While larger
    models often perform better in terms of the quality of the generated text, there’s
    a point of diminishing returns beyond which increasing model size yields marginal
    improvements. Moreover, the increased size comes with its own set of challenges:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，具有更多参数的模型可以捕捉数据中的更复杂模式，通常在手头任务上表现更好。虽然较大的模型在生成文本的质量方面通常表现更好，但存在一个收益递减的点，超过这个点，增加模型大小只会带来微小的改进。此外，增加的大小也带来了一系列挑战：
- en: '**Computational complexity**: Larger models require more computational power
    and memory, during both training and inference (the phase where the model is used
    to make predictions or generate new data based on the learned parameters). This
    can significantly increase the costs and the time required to train and use the
    model, making it less suitable for real-time applications or resource-constrained
    environments.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算复杂度**：更大的模型在训练和推理阶段（模型用于根据学习到的参数进行预测或生成新数据）都需要更多的计算能力和内存。这可能会显著增加训练和使用模型的成本和时间，使其不太适合实时应用或资源受限的环境。'
- en: 'The number of parameters significantly impacts the computational complexity
    of a model. Each parameter in a model is a variable that must be stored in memory
    during computation, during both training and inference. Here are some specific
    considerations for computational requirements:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型中的参数数量显著影响模型的计算复杂度。模型中的每个参数都是一个变量，在计算过程中必须存储在内存中，无论是训练还是推理阶段。以下是关于计算需求的一些具体考虑因素：
- en: '**Memory and storage**: The total size of the model in memory is the product
    of the number of parameters and the size of each parameter (typically a 32-bit
    or 64-bit float). For instance, a model with 100 million parameters, each represented
    by a 32-bit float, would require approximately 400 MB of memory (100 million *
    32 bits = 400 million bits = 400 MB). Now consider a larger model, say with 10
    billion parameters; the memory requirement jumps to 40 GB (10 billion * 32 bits
    = 40 billion bits = 40 GB). This requirement is just for the parameters and does
    not account for other data and overheads the model needs for its operations.'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存和存储**：模型在内存中的总大小是参数数量和每个参数大小（通常是32位或64位浮点数）的乘积。例如，一个拥有1亿个参数的模型，每个参数由32位浮点数表示，将需要大约400MB的内存（1亿
    * 32位 = 4亿位 = 400MB）。现在考虑一个更大的模型，比如说有100亿个参数；内存需求跃升至40GB（100亿 * 32位 = 400亿位 =
    40GB）。这个需求仅针对参数，并不包括模型操作所需的其他数据和开销。'
- en: '**Loading into memory**: When a model is used for inference, its parameters
    must be loaded into the RAM of the machine it’s running on. For a large model
    with 10 billion parameters, you would need a machine with enough RAM to accommodate
    the entire model, along with additional memory for the operational overhead, the
    input data, and the generated output. Suppose the model is too large to fit in
    memory. In that case, it may need to be **sharded** or distributed across multiple
    machines or loaded in parts, which can significantly complicate the deployment
    and operation of the model and also increase the latency of generating outputs.'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加载到内存中**：当模型用于推理时，其参数必须加载到运行该模型的机器的RAM中。对于一个拥有100亿个参数的大型模型，你需要一台具有足够RAM的机器来容纳整个模型，以及额外的内存来处理操作开销、输入数据和生成的输出。假设模型太大而无法装入内存。在这种情况下，可能需要将其**分片**或分布到多台机器上，或者分部分加载，这可能会显著增加模型的部署和操作复杂性，并增加生成输出的延迟。'
- en: '**Specialized hardware requirements**: Larger models require specialized hardware,
    such as powerful GPUs or TPUs, which could increase the project costs. As discussed,
    models with a large number of parameters require powerful computational resources
    for both training and inference. Hardware accelerators such as GPUs and TPUs are
    often employed to meet these demands. These hardware accelerators are designed
    to handle the parallel computation capabilities needed for the matrix multiplications
    and other operations inherent in neural network computations, speeding up the
    processing significantly compared to traditional **central processing** **units**
    (**CPUs**).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专用硬件需求**：较大的模型需要专用硬件，例如强大的GPU或TPU，这可能会增加项目成本。正如讨论的那样，具有大量参数的模型需要强大的计算资源来进行训练和推理。GPU和TPU等硬件加速器通常被用于满足这些需求。这些硬件加速器旨在处理神经网络计算中固有的矩阵乘法和其他操作所需的并行计算能力，与传统**中央处理单元**（**CPU**）相比，可以显著加快处理速度。'
- en: Cloud-based infrastructure can alleviate the complexity of setup but often has
    usage-based pricing. Understanding infrastructure costs on a granular level is
    vital to ensuring that StyleSprint stays within its budget.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于云的基础设施可以减轻设置复杂性，但通常具有基于使用量的定价。在细粒度上理解基础设施成本对于确保StyleSprint保持在预算内至关重要。
- en: '**Latency**: We’ve briefly discussed latency, but it is important to reiterate
    that larger models typically have higher latency, which could be a problem for
    applications that require real-time responses. In our case, we can process the
    descriptions as batches asynchronously. However, StyleSprint may have projects
    that require fast turnarounds, requiring batches to be completed in hours and
    not days.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟**：我们简要讨论了延迟，但重要的是要重申，较大的模型通常具有更高的延迟，这可能会成为需要实时响应的应用程序的问题。在我们的案例中，我们可以异步处理描述作为批次。然而，StyleSprint可能有需要快速完成的项目，要求批次在数小时内而不是数天内完成。'
- en: In the case of StyleSprint, the trade-off between model performance and size
    must be carefully evaluated to ensure the final model meets the project’s performance
    requirements while staying within budget and hardware constraints. StyleSprint
    was hoping to have near-real-time responses to provide personalized descriptions,
    which typically translates to a smaller model with less computational complexity.
    However, it was also important that the model remains highly accurate and aligns
    with branding standards for tone and voice, which may require a larger model trained
    or fine-tuned on a larger dataset. In practice, we can evaluate the performance
    of models relative to size and complexity through benchmarking.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在StyleSprint的情况下，必须仔细评估模型性能和大小之间的权衡，以确保最终模型满足项目的性能要求，同时保持在预算和硬件约束范围内。StyleSprint希望提供近乎实时的响应来提供个性化的描述，这通常意味着一个更小的模型，具有更低的计算复杂度。然而，模型保持高度准确并与品牌的声音和语气标准保持一致也很重要，这可能需要在一个更大的数据集上训练或微调更大的模型。在实践中，我们可以通过基准测试来评估模型相对于大小和复杂度的性能。
- en: Benchmarking
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准测试
- en: Benchmarking is a systematic process used to evaluate the performance of different
    generative models against predefined criteria. This process involves comparing
    the models on various metrics to understand their strengths, weaknesses, and suitability
    for the project. It is an empirical method (based on observation) to obtain data
    on how the models perform under similar conditions, providing insights that can
    inform the decision-making process for model selection.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试是一种系统性的过程，用于评估不同生成模型相对于预定义标准的表现。这个过程涉及在多个指标上比较模型，以了解它们的优点、缺点以及适合项目的程度。这是一种基于观察的经验方法，用于获取模型在相似条件下表现的数据，提供可以指导模型选择决策过程的信息。
- en: In the StyleSprint scenario, benchmarking can be an invaluable exercise to navigate
    the trade-offs between model size, computational complexity, and the accuracy
    and creativity of generated descriptions.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在StyleSprint场景中，基准测试可以是一项非常有价值的练习，帮助我们权衡模型大小、计算复杂度以及生成描述的准确性和创造力之间的取舍。
- en: For our benchmarking exercise, we can return to our Google Colab prototyping
    environment to quickly load various generative models and run them through tests
    designed to evaluate their performance based on the considerations outlined in
    the previous sections, such as computational efficiency and text generation quality.
    Once we have completed our evaluation and comparison, we can make a few simple
    changes to our production application code and it will automatically redeploy.
    Benchmarking will be instrumental in measuring the quality of the descriptions
    relative to the model size and complexity. Recall that we will measure quality
    and overall model performance along several dimensions, including lexical and
    semantic similarity to a “gold standard” of human-written descriptions, and a
    qualitative assessment performed by a diverse group of reviewers.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的基准测试练习中，我们可以回到Google Colab原型环境，快速加载各种生成模型，并通过测试来评估它们的性能，这些测试是根据前几节中概述的考虑因素设计的，例如计算效率和文本生成质量。一旦我们完成了评估和比较，我们可以在我们的生产应用程序代码中进行一些简单的更改，它将自动重新部署。基准测试将在衡量描述质量相对于模型大小和复杂度的质量方面发挥关键作用。请记住，我们将从多个维度衡量质量和整体模型性能，包括与人类编写的“黄金标准”描述的词汇和语义相似性，以及由不同群体评审员进行的定性评估。
- en: The next step is to revisit and adapt our original prototyping code to include
    a few challenger models and apply evaluation metrics.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是回顾并调整我们的原始原型代码，以包括一些挑战者模型并应用评估指标。
- en: Updating the prototyping environment
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新原型环境
- en: For our evaluation steps, there are a few key changes to our original experimentation
    setup in Google Colab. First, we will want to make sure we leverage performance
    acceleration. Google Colab offers acceleration via GPU or TPU environments. For
    this experiment, we will leverage GPU. We will also want to transition from the
    Transformers library to a slightly more versatile library such as Langchain, which
    allows us to test both open source models such as GPT-Neo and commercial models
    such as GPT-3.5.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的评估步骤，我们对原始的Google Colab实验设置进行了一些关键性的调整。首先，我们希望确保利用性能加速。Google Colab通过GPU或TPU环境提供加速。对于这次实验，我们将利用GPU。我们还将从Transformers库过渡到一个稍微更通用的库，例如Langchain，这样我们可以测试开源模型如GPT-Neo和商业模型如GPT-3.5。
- en: GPU configuration
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPU配置
- en: 'Ensure you have a GPU enabled for better performance. Returning to Google Colab,
    we can follow these steps to enable GPU acceleration:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您启用了 GPU 以获得更好的性能。回到 Google Colab，我们可以按照以下步骤启用 GPU 加速：
- en: 'Click on **Runtime** in the top menu (see *Figure 4**.2*):'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在顶部菜单中点击**运行时**（见*图 4.2*）：
- en: '![Figure 4.2: Runtime drop-down menu](img/B21773_04_011.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2：运行时下拉菜单](img/B21773_04_011.jpg)'
- en: 'Figure 4.2: Runtime drop-down menu'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：运行时下拉菜单
- en: Select **Change runtime type** from the drop-down menu, as shown in the preceding
    screenshot.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从下拉菜单中选择**更改运行时类型**，如图中所示。
- en: 'In the pop-up window, select **GPU** from the **Hardware accelerator** drop-down
    menu (see *Figure 4**.3*):'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在弹出窗口中，从**硬件加速器**下拉菜单中选择**GPU**（见*图 4.3*）：
- en: '![Figure 4.3: Select GPU and click on Save](img/B21773_04_03.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3：选择 GPU 并点击保存](img/B21773_04_03.jpg)'
- en: 'Figure 4.3: Select GPU and click on Save'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：选择 GPU 并点击保存
- en: Click on **Save**.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**保存**。
- en: 'Now your notebook is set up to use a GPU to significantly speed up the computations
    needed for the benchmarking process. You can verify the GPU availability using
    the following code snippet:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您的笔记本已设置好使用 GPU，以显著加快基准测试过程所需的计算。您可以使用以下代码片段来验证 GPU 的可用性：
- en: '[PRE35]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This code snippet will return `True` if a GPU is available and `False` otherwise.
    This setup ensures that you have the necessary computational resources to benchmark
    various generative models. The utilization of a GPU will be crucial when it comes
    to handling large models and extensive computations.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段将返回 `True` 如果有 GPU 可用，否则返回 `False`。此设置确保您拥有进行各种生成模型基准测试所需的计算资源。在处理大型模型和大量计算时，使用
    GPU 将至关重要。
- en: Loading pretrained models with LangChain
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 LangChain 加载预训练模型
- en: 'In our first simple experiment, we relied on the Transformers library to load
    an open source version of GPT. However, for our benchmarking exercise, we want
    to evaluate the retail version of GPT-3 alongside open source models. We can leverage
    LangChain, a versatile library that provides a streamlined interface, to access
    both open source models from providers such as Hugging Face and closed source
    models such as OpenAI’s GPT-3.5\. LangChain offers a unified API that simplifies
    benchmarking and comparison through standardization. Here are the steps to do
    it:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一个简单实验中，我们依赖于 Transformers 库来加载 GPT 的开源版本。然而，对于我们的基准测试练习，我们希望评估 GPT 的零售版本以及开源模型。我们可以利用
    LangChain，这是一个功能丰富的库，它提供了一个简化的接口，可以访问来自 Hugging Face 等提供商的开源模型以及 OpenAI 的 GPT-3.5
    等封闭源模型。LangChain 提供了一个统一的 API，通过标准化简化了基准测试和比较。以下是完成此操作的步骤：
- en: '**Install necessary libraries**: We begin by installing the required libraries
    in our Colab environment. LangChain simplifies the interaction with models hosted
    on OpenAI and Hugging Face.'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**安装必要的库**：我们首先在我们的 Colab 环境中安装所需的库。LangChain 简化了与 OpenAI 和 Hugging Face 上托管的模型交互。'
- en: '[PRE36]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '**Set up credentials**: We obtain the credentials from OpenAI for accessing
    GPT-3, GPT-4, or whichever closed source model we select. We also provide credentials
    for the Hugging Face Hub, which hosts over 350,000 open source models. We must
    store these credentials securely to prevent any unauthorized access, especially
    in the case where model usage has an associated cost.'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置凭据**：我们从 OpenAI 获取访问 GPT-3、GPT-4 或我们选择的任何封闭源模型所需的凭据。我们还提供 Hugging Face
    Hub 的凭据，该 Hub 托管着超过 350,000 个开源模型。我们必须安全地存储这些凭据，以防止任何未经授权的访问，尤其是在模型使用涉及成本的情况下。'
- en: '[PRE37]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '**Load models**: With LangChain, we can quickly load models and generate responses.
    The following example demonstrates how to load GPT-3 and GPT-Neo from Hugging
    Face:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加载模型**：使用 LangChain，我们可以快速加载模型并生成响应。以下示例演示了如何从 Hugging Face 加载 GPT-3 和 GPT-Neo：'
- en: '[PRE41]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Notice that we have loaded two models that are significantly different in size.
    As the model signature suggests, GPT-Neo was trained on 2.7 billion parameters.
    Meanwhile, according to information available from OpenAI, Davinci was trained
    on 175 billion parameters. As discussed, a model that is significantly larger
    is expected to have captured much more complex patterns and will likely outperform
    a smaller model. However, these very large models are typically hosted by major
    providers and have higher usage costs. We will revisit cost considerations later.
    For now, we can continue to the next step, which is to prepare our testing data.
    Our test data should provide a baseline for model performance that will inform
    the cost versus performance trade-off.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们已经加载了两个在大小上显著不同的模型。正如模型签名所暗示的，GPT-Neo是在27亿个参数上训练的。同时，根据OpenAI提供的信息，Davinci是在1750亿个参数上训练的。正如讨论的那样，一个显著更大的模型预计会捕捉到更复杂的模式，并且可能会优于较小的模型。然而，这些非常大的模型通常由主要提供商托管，并且具有更高的使用成本。我们将在稍后重新审视成本考虑因素。现在，我们可以继续到下一步，即准备我们的测试数据。我们的测试数据应该为模型性能提供一个基准，这将告知成本与性能之间的权衡。
- en: Setting up testing data
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置测试数据
- en: In this context, testing data should comprise product attributes from the StyleSprint
    website (e.g., available colors, sizes, materials, etc.) and existing product
    descriptions written by the StyleSprint team. The human-written descriptions serve
    as the “ground truth,” or the standard against which to compare the models’ generated
    descriptions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，测试数据应包括来自StyleSprint网站的产品属性（例如，可用颜色、尺寸、材料等）以及StyleSprint团队编写的现有产品描述。人工编写的描述作为“真实情况”，或比较模型生成的描述的标准。
- en: We can gather product data from existing datasets by scraping data from e-commerce
    websites or using a pre-collected dataset from StyleSprint’s database. We should
    also ensure a varied collection of products to test a model’s capability across
    different categories and styles. The process of dividing data into distinct groups
    or segments based on shared characteristics is typically referred to as segmentation.
    Understanding a model’s behavior across segments should give us an indication
    of how well it can perform across the entire family of products. For the purposes
    of this example, product data is made available in the GitHub companion to this
    book ([https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python](https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python)).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过从电子商务网站抓取数据或使用StyleSprint数据库中的预收集数据集来收集产品数据。我们还应该确保收集到多样化的产品，以测试模型在不同类别和风格上的能力。根据共享特征将数据划分为不同的组或段的过程通常被称为分段。了解模型在各个段上的行为应该会给我们一个指示，即它在整个产品系列中表现如何。为了本例的目的，产品数据在本书的GitHub配套资源中提供（[https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python](https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python))。
- en: 'Let’s see how we can extract relevant information for further processing:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何提取相关信息以进行进一步处理：
- en: '[PRE51]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We must also format the product data in a way that makes it ready to be input
    into the models for description generation. This could be just the product title
    or a combination of product attributes:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须以使产品数据准备好输入到描述生成模型中的方式对其进行格式化。这可能是仅产品标题或产品属性的组合：
- en: '[PRE52]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Finally, we will ask the model to generate a batch of product descriptions using
    each model.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将要求模型使用每个模型生成一批产品描述。
- en: '[PRE53]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Now, with the testing data set up, we have a structured dataset of product information,
    reference descriptions, and images ready for use in the evaluation steps.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，随着测试数据集的建立，我们有了结构化的产品信息、参考描述和图像数据集，这些数据集已准备好用于评估步骤。
- en: Quantitative metrics evaluation
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定量指标评估
- en: Now that we have leveraged Langchain to load multiple models and prepared testing
    data, we are ready to begin applying evaluation metrics. These metrics capture
    accuracy and alignment with product images and will help us assess how well the
    models generate product descriptions compared to humans. As discussed, we focused
    on two categories of metrics, lexical and semantic similarity, which provide a
    measure of how many of the same words were used and how much semantic information
    is common to both the human and AI-generated product descriptions.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经利用Langchain加载了多个模型并准备好了测试数据，我们准备开始应用评估指标。这些指标捕捉了准确度和与产品图片的匹配度，并将帮助我们评估模型生成产品描述与人类相比的效果如何。正如讨论的那样，我们专注于两个类别的指标，即词汇和语义相似度，这些指标提供了使用相同单词的数量以及人类和AI生成的产品描述中共同语义信息的度量。
- en: In the following code block, we apply `BLEU`, `ROUGE`, and `METEOR` to evaluate
    the lexical similarity between the generated text and the reference text. Each
    of these has a reference-based assumption. This means that each metric assumes
    we are comparing against a human reference. We have already set aside our reference
    descriptions (or gold standard) for a diverse set of products to compare side-by-side
    with the generated descriptions.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们应用`BLEU`、`ROUGE`和`METEOR`来评估生成文本与参考文本之间的词汇相似度。每个指标都有基于参考的假设。这意味着每个指标都假设我们是在与人类参考进行比较。我们已经为各种产品预留了参考描述（或黄金标准），以便与生成描述进行并排比较。
- en: '[PRE54]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We can evaluate the semantic coherence and relevance of the generated descriptions
    using sentence embeddings:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用句子嵌入来评估生成描述的语义连贯性和相关性：
- en: '[PRE55]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Alignment with CLIP
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与CLIP的匹配
- en: We again leverage the CLIP model to evaluate the alignment between generated
    product descriptions and corresponding images, similar to our approach in [*Chapter
    2*](B21773_02.xhtml#_idTextAnchor045). The CLIP model, adept at correlating visual
    and textual content, scores the congruence between each product image and its
    associated generated and reference descriptions. The reference description serves
    as a human baseline for accuracy. These scores provide a quantitative measure
    of our generative model’s effectiveness at producing descriptions that correspond
    well to the product image. The following is a snippet from a component that processes
    the generated descriptions combined with corresponding images to generate a CLIP
    score. The full component code (including image pre-processing) is available in
    the `chapter 4` folder of this book’s GitHub repository at [https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python](https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python)).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次利用CLIP模型来评估生成产品描述与相应图像之间的匹配度，类似于我们在[*第二章*](B21773_02.xhtml#_idTextAnchor045)中的方法。CLIP模型擅长关联视觉和文本内容，对每个产品图像及其关联的生成和参考描述进行一致性评分。参考描述作为准确性的基准。这些分数为我们生成模型在生成与产品图像匹配良好的描述方面的有效性提供了定量度量。以下是从一个处理生成描述及其对应图像以生成CLIP分数的组件中摘录的一段代码。完整的组件代码（包括图像预处理）可在本书GitHub仓库的`第4章`文件夹中找到，网址为[https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python](https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python))。
- en: '[PRE56]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: In evaluating product descriptions using the CLIP model, the alignment scores
    generated for each image-description pair are computed relative to other descriptions
    in the batch. Essentially, CLIP assesses how well a specific description (either
    generated or reference) aligns with a given image compared to other descriptions
    within the same batch. For example, a score of 33.79 indicates that the description
    aligns with the image 33.79% better than the other descriptions in the batch align
    with that image. In comparing against the reference, we expect that the scores
    based on the generated descriptions should align closely with the scores based
    on the reference descriptions.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用CLIP模型评估产品描述时，为每个图像-描述对生成的匹配分数是相对于批次中的其他描述来计算的。本质上，CLIP评估特定描述（无论是生成的还是参考的）与给定图像相比，与其他批次内描述的匹配程度如何。例如，分数为33.79表示该描述与图像的匹配度比批次中其他描述与该图像的匹配度高出33.79%。在对比参考时，我们期望基于生成描述的分数应与基于参考描述的分数紧密匹配。
- en: Now that we have calculated lexical and semantic similarity to the reference
    scores, and alignment between images and generated descriptions relative to reference
    descriptions, we can evaluate our models holistically and interpret the outcome
    of our quantitative evaluation.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算了与参考分数的词汇和语义相似性，以及图像与生成的描述相对于参考描述的对齐程度，我们可以全面评估我们的模型并解释定量评估的结果。
- en: Interpreting outcomes
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释结果
- en: 'We begin with lexical similarity, which gives us an indication of similarity
    in phrasing and keywords between the reference and generated descriptions:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从词汇相似性开始，这为我们提供了参考描述和生成的描述在措辞和关键词之间的相似性指示：
- en: '|  | **BLEU** | **ROUGE** | **METEOR** |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  | **BLEU** | **ROUGE** | **METEOR** |'
- en: '| GPT-3.5 | 0.147 | 0.094 | 0.261 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | 0.147 | 0.094 | 0.261 |'
- en: '| GPT-Neo | 0.132 | 0.05 | 0.059 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| GPT-Neo | 0.132 | 0.05 | 0.059 |'
- en: 'Table 4.2: Lexical similarity'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.2：词汇相似性
- en: 'In evaluating text generated by GPT-3.5 and GPT-Neo models, we use several
    lexical similarity metrics: BLEU, ROUGE, and METEOR. BLEU scores, which assess
    the precision of matching phrases, show GPT-3.5 (0.147) slightly outperforming
    GPT-Neo (0.132). ROUGE scores, focusing on the recall of content, indicate that
    GPT-3.5 (0.094) better captures reference content than GPT-Neo (0.05). METEOR
    scores, combining both precision and recall with synonym matching, reveal a significant
    lead for GPT-3.5 (0.261) over GPT-Neo (0.059). Overall, these metrics suggest
    that GPT-3.5’s generated text aligns more closely with reference standards, both
    in word choice and content coverage, compared to that of GPT-Neo.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估GPT-3.5和GPT-Neo模型生成的文本时，我们使用了几个词汇相似性指标：BLEU、ROUGE和METEOR。BLEU分数，评估匹配短语的精确度，显示GPT-3.5（0.147）略优于GPT-Neo（0.132）。ROUGE分数，侧重于内容的召回率，表明GPT-3.5（0.094）比GPT-Neo（0.05）更好地捕捉了参考内容。METEOR分数，结合了精确度和召回率以及同义词匹配，显示GPT-3.5（0.261）在GPT-Neo（0.059）之上具有显著优势。总体而言，这些指标表明，与GPT-Neo相比，GPT-3.5生成的文本在词汇选择和内容覆盖方面与参考标准更为接近。
- en: 'Next, we evaluate semantic similarity, which measures how closely the meanings
    of the generated text align with the reference text. This assessment goes beyond
    mere word-to-word matching and considers the context and overall intent of the
    sentences. Semantic similarity evaluates the extent to which the generated text
    captures the nuances, concepts, and themes present in the reference text, providing
    insight into the model’s ability to understand and replicate deeper semantic meanings:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们评估语义相似性，这衡量了生成的文本的意义与参考文本的意义之间的接近程度。这种评估不仅超越了单纯的词对词匹配，还考虑了句子的上下文和整体意图。语义相似性评估了生成的文本在多大程度上捕捉到了参考文本中存在的细微差别、概念和主题，从而提供了对模型理解和复制更深层次语义意义的洞察：
- en: '| **Model** | **Mean** **cosine similarity** |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **平均** **余弦相似度** |'
- en: '| GPT-3.5 | 0.8192 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | 0.8192 |'
- en: '| GPT-Neo | 0.2289 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| GPT-Neo | 0.2289 |'
- en: 'Table 4.3: Semantic similarity'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.3：语义相似性
- en: The mean cosine similarity scores reveal a stark contrast between the two models’
    performance in semantic similarity. GPT-3.5 shows a high degree of semantic alignment
    with the reference text. GPT-Neo’s significantly lower score suggests a relatively
    poor performance, indicating that the generated descriptions were fundamentally
    dissimilar to descriptions written by humans.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 平均余弦相似度分数揭示了两个模型在语义相似性方面的显著差异。GPT-3.5显示出与参考文本的高度语义一致性。GPT-Neo的显著较低分数表明相对较差的性能，这表明生成的描述与人类编写的描述在本质上存在差异。
- en: 'Finally, we review the CLIP scores, which tell us how well the generated descriptions
    align visually with the corresponding images. These scores, derived from a model
    trained to understand and correlate visual and textual data, provide a measure
    of the relevance and accuracy of the text in representing the visual content.
    High CLIP scores indicate a strong correlation between the text and the image,
    suggesting that the generated descriptions are not only textually coherent but
    also contextually appropriate and visually descriptive:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们回顾CLIP分数，这些分数告诉我们生成的描述与相应图像在视觉上的对齐程度。这些分数来自一个训练有素以理解和关联视觉和文本数据的模型，提供了文本在代表视觉内容的相关性和准确性的度量。高CLIP分数表明文本与图像之间存在强烈的关联，这表明生成的描述不仅在文本上连贯，而且在上下文中适当且视觉描述性强：
- en: '| **Model** | **Mean CLIP** | **Reference delta** |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **平均CLIP** | **参考delta** |'
- en: '| GPT-3.5 | 26.195 | 2.815 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | 26.195 | 2.815 |'
- en: '| GPT-Neo | 22.647 | 6.363 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| GPT-Neo | 22.647 | 6.363 |'
- en: 'Table 4.4: Comparative CLIP score analysis for GPT-3.5 and GPT-Neo models'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.4：GPT-3.5 和 GPT-Neo 模型的比较 CLIP 分数分析
- en: We calculated the CLIP scores from the reference descriptions, which represent
    the average alignment score between a set of benchmark descriptions and the corresponding
    images. We then calculated CLIP scores for each model and analyzed the delta.
    In concert with our other metrics, GPT-3.5 has a clear advantage over GPT-Neo,
    aligning more closely with the reference.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从参考描述中计算了 CLIP 分数，这些分数代表了一组基准描述与相应图像之间的平均对齐分数。然后我们为每个模型计算了 CLIP 分数并分析了差异。与我们的其他指标一起，GPT-3.5
    在与 GPT-Neo 对齐方面具有明显优势，更接近参考描述。
- en: Overall, GPT-3.5 appears to significantly outperform GPT-Neo across all quantitative
    measures. However, it is worth noting that GPT-3.5 incurs a higher cost and generally
    has a higher latency than GPT-Neo. In this case, the StyleSprint team would conduct
    a qualitative analysis to accurately determine whether the GPT-Neo descriptions
    do not align with brand guidelines and expectations, therefore making the cost
    of using the better model worthwhile. As discussed, the trade-off here is not
    clear-cut. StyleSprint must carefully consider that although using a commodity
    such as GPT-3.5 does not incur computational costs directly, on-demand costs could
    increase significantly as model usage rises.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，GPT-3.5 在所有定量指标上似乎都显著优于 GPT-Neo。然而，值得注意的是，GPT-3.5 的成本更高，通常比 GPT-Neo 的延迟更高。在这种情况下，StyleSprint
    团队将进行定性分析，以准确确定 GPT-Neo 描述是否与品牌指南和期望不一致，从而使使用更好模型的成本变得值得。正如所讨论的，这里的权衡并不明显。StyleSprint
    必须仔细考虑，尽管使用像 GPT-3.5 这样的商品不会直接产生计算成本，但随着模型使用量的增加，按需成本可能会显著增加。
- en: The contrasting strengths of the two models pose a decision-making challenge.
    While one clearly excels in performance metrics and alignment with CLIP, implying
    higher accuracy and semantic correctness, the other is significantly more resource-efficient
    and scalable, which is crucial for cost-effectiveness. At this stage, it becomes
    critical to assess model outcomes qualitatively and to engage stakeholders to
    help understand organizational priorities.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 两个模型的对比优势提出了一个决策挑战。虽然一个在性能指标和 CLIP 对齐方面明显优于另一个，这暗示着更高的准确性和语义正确性，而另一个在资源效率和可扩展性方面显著更高，这对于成本效益至关重要。在这个阶段，评估模型结果并进行利益相关者参与以帮助理解组织优先级变得至关重要。
- en: With these considerations in mind, we’ll revisit qualitative considerations
    such as transparency, bias, and fairness and how they play into the broader picture
    of deploying a responsible and effective AI system.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些因素，我们将重新审视定性考虑因素，如透明度、偏见和公平性，以及它们如何在部署负责任和有效的 AI 系统的更广泛画面中发挥作用。
- en: Responsible AI considerations
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负责任的 AI 考虑因素
- en: Addressing implicit or covert societal biases in AI systems is crucial to ensure
    responsible AI deployment. Although it may not seem obvious how a simple product
    description could introduce bias, the language used can inadvertently reinforce
    stereotypes or exclude certain groups. For instance, descriptions that consistently
    associate certain body types or skin tones with certain products or that unnecessarily
    default to gendered language can unintentionally perpetuate societal biases. However,
    with a structured mitigation approach, including algorithmic audits, increased
    model transparency, and stakeholder engagement, StyleSprint can make sure its
    brand promotes equity and inclusion.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 解决 AI 系统中的隐含或隐蔽的社会偏见对于确保负责任的 AI 部署至关重要。尽管一个简单的产品描述如何引入偏见可能并不明显，但使用的语言可能会无意中强化刻板印象或排除某些群体。例如，那些持续将某些体型或肤色与某些产品关联的描述，或是不必要地使用性别语言，可能会无意中延续社会偏见。然而，通过结构化的缓解方法，包括算法审计、增加模型透明度和利益相关者参与，StyleSprint
    可以确保其品牌促进公平和包容。
- en: Addressing and mitigating biases
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决和缓解偏见
- en: 'We present several considerations, as suggested by Costanza-Chock et al. in
    *Who Audits the Auditors? Recommendations from a field scan of the algorithmic*
    *auditing ecosystem*:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了几个考虑因素，如 Costanza-Chock 等人在 *谁审计审计员？算法审计生态系统现场扫描的建议* 中所建议的：
- en: '**Professional** **environment examination**: Creating a supportive professional
    environment is crucial for addressing algorithmic fairness. Implementing whistleblower
    protections facilitates the safe reporting of biases and unfair practices while
    establishing processes for individuals to report harms to ensure these concerns
    are addressed proactively.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专业环境考察**：创建一个支持性的专业环境对于解决算法公平性至关重要。实施举报人保护措施可以促进安全地报告偏见和不公平行为，同时建立个人报告伤害的过程，以确保这些担忧得到积极解决。'
- en: '**Custom versus standardized audit frameworks**: While custom audit frameworks
    are expected, considering standardized methods may enhance rigor and transparency
    in bias mitigation efforts. Engaging with external auditing entities could offer
    unbiased evaluations of StyleSprint’s AI systems, aligning with the observations
    by Costanza-Chock et al. (2022).'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定制与标准化审计框架**：虽然定制审计框架是预期的，但考虑标准化方法可能会增强偏见缓解工作的严谨性和透明度。与外部审计实体合作可能提供对StyleSprint的AI系统的无偏见评估，与Costanza-Chock等人（2022年）的观察结果相一致。'
- en: '**Focusing on equity, not just equality**: Equity notions acknowledge differing
    needs, essential for a comprehensive approach to fairness. Performing intersectional
    and small population analyses could help you to understand and address biases
    beyond legally protected classes.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关注公平而非仅仅平等**：公平观念承认不同的需求，这对于公平的全面方法至关重要。进行交叉和少数群体分析可以帮助你理解和解决超出法律保护类别之外的偏见。'
- en: '**Disclosure and transparency**: Disclosing audit methods and outcomes can
    foster a culture of transparency and continuous improvement. Officially released
    audits could help you establish best practices and gain stakeholder trust.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**披露和透明度**：披露审计方法和结果可以培养透明度和持续改进的文化。官方发布的审计可以帮助你建立最佳实践并获得利益相关者的信任。'
- en: '**Mixed methods analyses**: As presented, a mix of technical and qualitative
    analyses could provide a holistic view of the system’s fairness. Engaging non-technical
    stakeholders could emphasize qualitative analyses.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合方法分析**：正如所展示的，技术和定性分析的结合可以提供对系统公平性的全面视角。与非技术利益相关者合作可以强调定性分析。'
- en: '**Community and stakeholder engagement**: Again, involving diverse groups and
    domain experts in audits could ensure diverse perspectives are considered in bias
    mitigation efforts. Establishing feedback loops with stakeholders could facilitate
    continuous improvement.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社区和利益相关者参与**：再次强调，在审计中涉及多样化的群体和领域专家可以确保在偏见缓解工作中考虑多样化的观点。与利益相关者建立反馈循环可以促进持续改进。'
- en: '**Continuous learning and improvement**: Staying updated on emerging standards
    and best practices regarding AI fairness is crucial for continuous improvement.
    Fostering a culture of learning could help in adapting to evolving fairness challenges
    and regulatory landscapes, thus ensuring StyleSprint’s AI systems remain fair
    and responsible over time.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续学习和改进**：关注关于AI公平性的新兴标准和最佳实践对于持续改进至关重要。培养学习文化有助于适应不断变化的公平挑战和监管环境，从而确保StyleSprint的AI系统在一段时间内保持公平和负责任。'
- en: Transparency and explainability
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 透明度和可解释性
- en: Generally, explainability in machine learning refers to the ability to understand
    the internal mechanics of a model, elucidating how it makes decisions or predictions
    based on given inputs. However, achieving explainability in generative models
    can be much more complex. As discussed, unlike discriminative machine learning
    models, generative models do not have the objective of learning a decision boundary,
    nor do they reflect a clear notion of features or a direct mapping between input
    features and predictions. This absence of feature-based decision-making makes
    traditional explainability techniques ineffective for generative foundational
    models such as GPT-4.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，机器学习中的可解释性指的是理解模型内部机制的能力，阐明它是如何根据给定的输入做出决策或预测的。然而，在生成模型中实现可解释性可能要复杂得多。正如所讨论的，与判别性机器学习模型不同，生成模型没有学习决策边界的目标，它们也不反映特征或输入特征与预测之间的直接映射的明确概念。这种基于特征的决策缺失使得传统的可解释性技术对于GPT-4等生成基础模型无效。
- en: Alternatively, we can adopt some pragmatic transparency practices, such as clear
    documentation made accessible to all relevant stakeholders, to foster a shared
    understanding and expectations regarding the model’s capabilities and usage.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以采用一些务实的透明度实践，例如制作清晰且对所有相关利益相关者可访问的文档，以促进对模型能力和使用的共同理解和期望。
- en: The topic of explainability is a critical space to watch, especially as generative
    models become more complex and their outcomes become increasingly more difficult
    to rationalize, which may present unknown risk implications.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性的主题是一个需要关注的重点领域，尤其是在生成模型变得更加复杂，其结果越来越难以合理化时，这可能会带来未知的风险影响。
- en: Promising research from Anthropic, OpenAI, and others suggests that sparse autoencoders—neural
    networks that activate only a few neurons at a time—could facilitate the identification
    of abstract and understandable patterns. This method could help explain the network's
    behavior by highlighting features that align with human concepts.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Anthropic、OpenAI和其他机构的令人有希望的研究表明，稀疏自动编码器——一次只激活几个神经元的神经网络——可以促进识别抽象和可理解的模式。这种方法可以通过突出与人类概念一致的特征来帮助解释网络的行为。
- en: Final deployment
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终部署
- en: 'Assuming we have carefully gathered quantitative and qualitative feedback regarding
    the best model for the job, we can select our model and update our production
    environment to deploy and serve it. We will continue to use FastAPI for creating
    a web server to serve our model, and Docker to containerize our application. However,
    now that we have been introduced to the simplicity of LangChain, we will continue
    to leverage its simplified interface. Our existing CI/CD pipeline will ensure
    streamlined automatic deployment and continuous application monitoring. This means
    that deploying our model is as simple as checking-in our latest code. We begin
    with updating our dependencies list:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经仔细收集了关于最适合这项工作的模型的数量和质量反馈，我们可以选择我们的模型并更新我们的生产环境以部署和提供服务。我们将继续使用FastAPI来创建一个网络服务器以提供服务，并使用Docker来容器化我们的应用程序。然而，现在我们已经了解了LangChain的简单性，我们将继续利用其简化的接口。我们现有的CI/CD管道将确保流畅的自动部署和持续的应用程序监控。这意味着部署我们的模型就像检查我们的最新代码一样简单。我们首先更新我们的依赖项列表：
- en: '`requirements.txt` file in your project to include the necessary libraries:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的项目中的`requirements.txt`文件中包含必要的库：
- en: '[PRE57]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '**Update the Dockerfile**: Modify your Dockerfile to ensure it installs the
    updated requirements and properly sets up the environment for running LangChain
    with FastAPI:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更新Dockerfile**：修改您的Dockerfile以确保安装更新的要求，并正确设置运行LangChain与FastAPI的环境：'
- en: '[PRE61]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '**Update the FastAPI application**: Modify your FastAPI application to utilize
    Langchain for interacting with GPT-3.5\. Ensure your OpenAI API key is securely
    stored and accessible to your application:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更新FastAPI应用程序**：修改您的FastAPI应用程序以利用Langchain与GPT-3.5交互。确保您的OpenAI API密钥安全存储且可供应用程序访问：'
- en: '[PRE75]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: Testing and monitoring
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试和监控
- en: Once the model is deployed, perform necessary tests to ensure the setup works
    as expected. Continue to monitor the system’s performance, errors, and other critical
    metrics to ensure reliable operation.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署后，进行必要的测试以确保设置按预期工作。继续监控系统性能、错误和其他关键指标，以确保可靠运行。
- en: By this point, we have updated our production environment to deploy and serve
    GPT-3.5, facilitating the generation of text based on the prompts received via
    the FastAPI application. This setup ensures a scalable, maintainable, and secure
    deployment of our new generative model. However, we should also explore some best
    practices regarding application reliability.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经更新了我们的生产环境以部署和提供GPT-3.5，这有助于根据通过FastAPI应用程序接收到的提示生成文本。这种设置确保了我们的新生成模型的可扩展性、可维护性和安全性部署。然而，我们也应该探索一些关于应用程序可靠性的最佳实践。
- en: Maintenance and reliability
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维护和可靠性
- en: 'Maintaining reliability in our StyleSprint deployment is critical. As we employ
    Langchain with FastAPI, Docker, and CI/CD, it’s essential to set up monitoring,
    alerting, automatic remediation, and failover mechanisms. This section outlines
    a possible approach to ensure continuous operation and robustness in our production
    environment:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在StyleSprint部署中保持可靠性至关重要。随着我们使用Langchain、FastAPI、Docker和CI/CD，设置监控、警报、自动修复和故障转移机制是必不可少的。本节概述了一种可能的方案，以确保生产环境中的持续运行和稳健性：
- en: '**Monitoring tools**: Integrate monitoring tools within the CI/CD pipeline
    to continuously track system performance and model metrics. This step is fundamental
    for identifying and rectifying issues proactively.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控工具**：在CI/CD管道中集成监控工具，以持续跟踪系统性能和模型指标。这一步对于主动识别和纠正问题至关重要。'
- en: '**Alerting mechanisms**: Establish alerting mechanisms to notify the maintenance
    team whenever anomalies or issues are detected. Tuning the alerting thresholds
    accurately is crucial to catch issues early and minimize false alarms.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**警报机制**：建立警报机制，以便在检测到异常或问题时通知维护团队。准确调整警报阈值对于早期发现问题并最小化误报至关重要。'
- en: '**Automatic remediation**: Utilize Kubernetes’ self-healing features and custom
    scripts triggered by certain alerts for automatic remediation. This setup aims
    to resolve common issues autonomously, reducing the need for human intervention.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动修复**：利用Kubernetes的自愈功能和由某些警报触发的自定义脚本进行自动修复。这种设置旨在自主解决常见问题，减少对人工干预的需求。'
- en: '**Failover mechanisms**: Implement a failover mechanism by setting up secondary
    servers and databases. In case of primary server failure, these secondary setups
    take over to ensure continuous service availability.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**故障转移机制**：通过设置辅助服务器和数据库来实现故障转移机制。在主服务器故障的情况下，这些辅助设置接管以确保服务连续可用。'
- en: '**Regular updates via CI/CD**: Employ the CI/CD pipeline for managing, testing,
    and deploying updates to LangChain, FastAPI, or other components of the stack.
    This process keeps the deployment updated and secure, reducing the maintenance
    burden significantly.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过CI/CD进行定期更新**：使用CI/CD管道来管理、测试和部署LangChain、FastAPI或其他堆栈组件的更新。此过程保持部署更新和安全，显著减少维护负担。'
- en: By meticulously addressing each of these areas, you’ll be laying down a solid
    foundation for a reliable and maintainable StyleSprint deployment.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仔细处理这些领域的每一个，你将为可靠的StyleSprint部署打下坚实的基础。
- en: Summary
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter outlined the process of transitioning the StyleSprint generative
    AI prototype to a production-ready deployment for creating engaging product descriptions
    on an e-commerce platform. It started with setting up a robust Python environment
    using Docker, GitHub, and CI/CD pipelines for efficient dependency management,
    testing, and deployment. The focus then shifted to selecting a suitable pretrained
    model, emphasizing alignment with project goals, computational considerations,
    and responsible AI practices. This selection relied on both quantitative benchmarking
    and qualitative evaluation. We then outlined the deployment of the selected model
    using FastAPI and LangChain, ensuring a scalable and reliable production environment.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 本章概述了将StyleSprint生成式AI原型过渡到为电子商务平台创建引人入胜的产品描述的生产就绪部署的过程。它从使用Docker、GitHub和CI/CD管道设置一个健壮的Python环境开始，以实现高效的依赖管理、测试和部署。然后，重点转向选择合适的预训练模型，强调与项目目标的一致性、计算考虑因素和负责任的AI实践。此选择依赖于定量基准测试和定性评估。然后，我们概述了使用FastAPI和LangChain部署所选模型的过程，确保可扩展和可靠的生产环境。
- en: Following the strategies outlined in this chapter will equip teams with the
    necessary insights and steps to successfully transition their generative AI prototype
    into a maintainable and value-adding production system. In the next chapter, we
    will explore fine-tuning and its importance in LLMs. We will also weigh in on
    the decision-making process, addressing when it is more beneficial to fine-tune
    versus zero or few-shot prompting.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 按照本章概述的策略，将为团队提供必要的见解和步骤，以成功地将他们的生成式AI原型过渡到可维护且增值的生产系统。在下一章中，我们将探讨微调及其在LLMs中的重要性。我们还将权衡决策过程，讨论在何种情况下微调比零样本或少量样本提示更有益。
- en: 'Part 2: Practical Applications of Generative AI'
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：生成式AI的实际应用
- en: This part focuses on the practical applications of generative AI, including
    fine-tuning models for specific tasks, understanding domain adaptation, mastering
    prompt engineering, and addressing ethical considerations. It aims to provide
    hands-on insights and methodologies for effectively implementing and leveraging
    generative AI in various contexts with a focus on responsible adoption.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分重点介绍生成式AI的实际应用，包括针对特定任务微调模型、理解领域自适应、掌握提示工程以及处理伦理考量。它旨在提供实际见解和方法，以有效地在各种环境中实施和利用生成式AI，并重点关注负责任的采用。
- en: 'This part contains the following chapters:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 5*](B21773_05.xhtml#_idTextAnchor180), *Fine-Tuning Generative Models
    for Specific Tasks*'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第5章*](B21773_05.xhtml#_idTextAnchor180), *针对特定任务微调生成模型*'
- en: '[*Chapter 6*](B21773_06.xhtml#_idTextAnchor211), *Understanding Domain Adaptation
    for Large Language Models*'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第6章*](B21773_06.xhtml#_idTextAnchor211), *理解大型语言模型中的领域自适应*'
- en: '[*Chapter 7*](B21773_07.xhtml#_idTextAnchor225), *Mastering the Fundamentals
    of Prompt Engineering*'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B21773_07.xhtml#_idTextAnchor225), *掌握提示工程的基本原理*'
- en: '[*Chapter 8*](B21773_08.xhtml#_idTextAnchor251), *Addressing Ethical Considerations
    and Charting a Path toward Trustworthy Generative AI*'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B21773_08.xhtml#_idTextAnchor251), *处理伦理考量并规划通往可信生成式AI的道路*'
