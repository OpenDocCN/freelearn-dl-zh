- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Introduction to Natural Language Processing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理简介
- en: Today, we are faced with a staggering amount of text data coming at us from
    all directions, from social media platforms to email communications, and from
    text messages to online reviews. This exponential growth in text data has led
    to a rapid growth in the development of text-based applications powered by advanced
    deep learning techniques, used to unlock insights from text data. We find ourselves
    in the dawn of a transformative era, powered by tech giants such as Google and
    Microsoft and revolutionary start-ups such as OpenAI and Anthropic. These visionaries
    are leading the charge in building powerful solutions capable of solving a myriad
    of text-based challenges, such as summarizing large volumes of documents, extracting
    sentiments from online platforms, and generating text for blog posts – the list
    of uses is endless.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们面临着来自各个方向的大量文本数据，无论是社交媒体平台上的内容、电子邮件通讯、短信，还是在线评论。文本数据的指数级增长，推动了基于文本的应用程序的快速发展，这些应用程序采用先进的深度学习技术，从文本数据中提取有价值的见解。我们正处在一个变革时代的黎明，这一进程得到了像谷歌、微软这样的科技巨头，以及OpenAI、Anthropic这样的革命性初创公司的推动。这些先驱者正在带领我们构建强大的解决方案，能够解决各种基于文本的挑战，例如总结大量文档、提取社交平台上的情感信息以及为博客文章生成文本——这类应用的清单几乎是无穷无尽的。
- en: Real-world text data can be messy; it could be riddled with unwanted information
    such as punctuation marks, special characters, and common words that may not contribute
    significantly to the text’s meaning. Hence, we will kick off this chapter by looking
    at some basic text preprocessing steps to help transform text data into a more
    digestible form in preparation for modeling. Again, you may wonder, how do these
    machines learn to understand text? How do they make sense of words and sentences,
    or even grasp their semantic meaning or the context in which words are used? In
    this chapter, we will journey through the fundamentals of **natural language processing**
    (**NLP**). We will explore concepts such as tokenization, which deals with how
    we segment text into individual words or terms (tokens). We will also explore
    the concept of word embeddings – here, we will see how they enable models to capture
    the meaning, context, and relationship between words.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界中的文本数据可能会很混乱；它可能充斥着不必要的信息，例如标点符号、特殊字符和一些常见词汇，这些词汇可能对文本的实际含义没有显著贡献。因此，我们将从一些基本的文本预处理步骤开始，帮助将文本数据转化为更易处理的形式，为建模做好准备。再次提问，你可能会想，这些机器是如何学习理解文本的？它们是如何理解单词和句子，甚至是掌握单词的语义含义或使用的上下文？在本章中，我们将探讨**自然语言处理**（**NLP**）的基础知识。我们将探讨一些概念，比如分词，它是处理如何将文本分割成单个单词或术语（词元）。我们还将探讨词嵌入的概念——在这里，我们将看到它们如何帮助模型捕捉单词之间的含义、上下文和关系。
- en: Then, we will put together all we have learned in this chapter to build a sentiment
    analysis model, using the Yelp Polarity dataset to classify customer reviews.
    As an interactive activity, we will examine how to visualize word embeddings in
    TensorFlow; this can be useful in gaining a snapshot of how our model understands
    and represents different words. We will also explore various techniques to improve
    the performance of our sentiment analysis classifier. By the end of this chapter,
    you will have a good foundational understanding of how to preprocess and model
    text data, as well as the skills required to tackle real-world NLP problems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将把本章中学到的所有知识结合起来，构建一个情感分析模型，使用Yelp Polarity数据集对客户评论进行分类。作为一项互动活动，我们将探讨如何在TensorFlow中可视化词嵌入；这有助于快速了解模型如何理解和表示不同的单词。我们还将探讨各种技术来提高情感分析分类器的性能。通过本章的学习，你将对如何预处理和建模文本数据有一个良好的基础理解，并掌握解决实际NLP问题所需的技能。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主题：
- en: Text preprocessing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本预处理
- en: Building a sentiment classifier
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建情感分类器
- en: Embedding visualization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入可视化
- en: Model improvement
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型改进
- en: Text preprocessing
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本预处理
- en: NLP is an exciting and evolving field that lies at the intersection of computer
    science and linguistics. It empowers computers with the ability to understand,
    analyze, interpret, and generate text data. However, working with text data presents
    a unique set of challenges, one that differs from the tabular and image data we
    worked with in the earlier sections of this book. *Figure 10**.1* gives us a high-level
    overview of some of the inherent challenges that text data presents. Let’s drill
    into them and see what and how they present issues to us when building deep learning
    models with text data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是一个激动人心且不断发展的领域，位于计算机科学和语言学的交汇点。它赋予计算机理解、分析、解释和生成文本数据的能力。然而，处理文本数据提出了独特的挑战，这与我们在本书早期章节中处理的表格数据和图像数据有所不同。*图10.1*为我们提供了文本数据所面临的一些固有挑战的高层次概览。让我们深入探讨这些问题，看看它们在构建文本数据的深度学习模型时如何成为问题。
- en: '![Figure 10.1 – The challenges presented by text data](img/B18118_10_001.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1 – 文本数据所面临的挑战](img/B18118_10_001.jpg)'
- en: Figure 10.1 – The challenges presented by text data
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – 文本数据所面临的挑战
- en: Text data in its natural form is unstructured, and this is just the beginning
    of the uniqueness of this interesting type of data we will work with in this chapter.
    Let's illustrate some of the issues by looking at these two sentences – “*The
    house next to ours is beautiful*” versus “*Our neighbor’s house is one everyone
    in this area admires*.” Both phrases have a similar sentiment, yet they have different
    structures and varying lengths. To humans, this lack of structure and varying
    length is not a challenge, but when we work with deep learning models, this could
    pose a challenge. To address these challenges, we can consider ideas such as tokenization,
    which refers to the splitting of text data into smaller units called tokens. These
    tokens could be used to represent words, sub-words, or individual characters.
    To handle the varying length of text data in preparation for modeling, we will
    reuse an old trick that we applied when working with image data with CNNs – padding.
    By padding the sentences, we can ensure that our data (such as sentences or paragraphs)
    is of the same length. This uniformity makes our data more digestible for our
    models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据以其自然形态是非结构化的，这只是我们将在本章中处理的这一有趣数据类型独特性的开始。通过对比这两句话——“*我们家旁边的房子很美丽*”和“*我们邻居的房子是这个地区每个人都赞赏的*”，我们可以说明一些问题。两者传达的情感相似，但它们的结构不同，长度也各异。对于人类来说，这种缺乏结构和长度变化并不成问题，但当我们使用深度学习模型时，这可能会成为一个挑战。为了解决这些挑战，我们可以考虑像标记化这样的思想，它指的是将文本数据拆分成更小的单元，称为标记。我们可以用这些标记表示单词、子单词或个别字符。为了处理文本数据长度的变化以便建模，我们将重用一个在处理图像数据时使用的老方法——填充。通过填充句子，我们可以确保数据（如句子或段落）的长度一致。这种一致性使得数据对我们的模型更易于消化。
- en: Again, we may come across words with multiple meanings and decoding the meaning
    of these words largely depends on the context in which they are used. For example,
    if we have a sentence reading “*I will be at the bank*,” without additional context,
    it is difficult to tell whether *bank* refers to a financial bank or a riverbank.
    Words such as this add an extra layer of complexity when modeling text data. To
    handle this issue, we need to apply techniques that capture the essence of words
    and their surrounding words. A good example of such a technique is word embeddings.
    Word embeddings are powerful vector representations that can be used to capture
    the semantic meaning of words, by enabling words with a similar meaning or context
    to have similar representations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可能会遇到具有多重含义的单词，解码这些单词的含义在很大程度上取决于它们使用的上下文。例如，如果我们看到一句话是“*我会在银行待着*”，没有额外的上下文，很难判断*银行*是指金融银行还是河岸。像这样的词语增加了在建模文本数据时的复杂性。为了解决这个问题，我们需要应用能够捕捉单词及其周围单词本质的技术。一个很好的例子就是词嵌入。词嵌入是强大的向量表示，用于捕捉单词的语义含义，使得具有相似意义或上下文的单词具有相似的表示形式。
- en: Other issues we could face when working with text data are typos, spelling variations,
    and noise. To tackle these issues, we can use noise-filtering techniques to filter
    out URLs, special characters, and other irrelevant entities when collecting data
    online. Let’s say we have a sample sentence – “*Max loves to play chess at the
    London country club, and he is the best golfer on our street*.” When we examine
    this sentence, we see that it contains common words such as *and*, *is*, and *the*.
    Although these words are needed for linguistic coherence, in some instances, they
    may not add semantic value. If this is the case, then we may want to remove these
    words to reduce the dimensionality of our data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在处理文本数据时可能会遇到的问题包括拼写错误、拼写变异和噪音。为了解决这些问题，我们可以使用噪音过滤技术，在收集在线数据时过滤掉网址、特殊字符和其他无关的实体。假设我们有一个示例句子——“*Max喜欢在伦敦乡村俱乐部打棋，而且他是我们街上最棒的高尔夫球手*。”当我们检查这个句子时，会发现它包含了一些常见的词语，如*and*、*is*和*the*。尽管这些词语对语言的连贯性是必要的，但在某些情况下，它们可能并不增加语义价值。如果是这种情况，我们可能会希望去除这些词语，以降低数据的维度。
- en: 'Now that we’ve covered some foundational ideas around the challenges of text
    data, let’s see how to preprocess text data by looking at how we can extract and
    clean text data on machine learning from Wikipedia. Here, we will see how to apply
    TensorFlow to perform techniques such as tokenization, padding, and using word
    embeddings to extract meaning from text. To access the sample data, use this link:
    [https://en.wikipedia.org/wiki/Machine_learning](https://en.wikipedia.org/wiki/Machine_learning).
    Let’s begin:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了一些关于文本数据挑战的基础知识，让我们来看看如何预处理文本数据，了解如何从维基百科中提取和清洗机器学习的文本数据。在这里，我们将看到如何应用TensorFlow执行诸如分词、填充以及使用词嵌入等技术，从文本中提取意义。要访问示例数据，可以使用这个链接：[https://en.wikipedia.org/wiki/Machine_learning](https://en.wikipedia.org/wiki/Machine_learning)。让我们开始吧：
- en: 'We will start by importing the necessary libraries:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先导入必要的库：
- en: '[PRE0]'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We use these libraries to effectively fetch, preprocess, and tokenize web data,
    preparing it for modeling with neural networks. When we want to access data from
    the internet, the `requests` library can prove to be a useful tool, enabling us
    to streamline the process of retrieving information from web pages by making requests
    to web servers and fetching web data. The collected data is often in the HTML
    format, which isn’t in the best shape for us to feed into our models. This is
    where `BeautifulSoup` (an intuitive tool for parsing HTML and XML) comes into
    the picture, enabling us to easily navigate and access the content we need. To
    perform string manipulation, text cleaning, or extracting patterns, we can use
    the `re` module. We also import the `Tokenizer` class from TensorFlow’s Keras
    API, which enables us to perform tokenization, thus converting our data into a
    model-friendly format.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用这些库来有效地获取、预处理和分词网络数据，为神经网络建模做准备。当我们想要访问互联网上的数据时，`requests`库可以证明是一个有用的工具，它使我们能够通过向网络服务器发出请求并获取网页数据，简化从网页获取信息的过程。收集到的数据通常是HTML格式的，这种格式并不适合我们直接输入到模型中。这时，`BeautifulSoup`（一个直观的HTML和XML解析工具）就派上用场了，它帮助我们轻松浏览和访问所需的内容。为了执行字符串操作、文本清洗或提取模式，我们可以使用`re`模块。我们还从TensorFlow的Keras
    API中导入了`Tokenizer`类，它使我们能够执行分词操作，从而将数据转换为适合模型的格式。
- en: 'Then, we assign our variable to the web page we want to scrape; in this case,
    we are interested in scraping data from the Wikipedia page on machine learning:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将我们的变量分配给我们想要抓取的网页；在这种情况下，我们感兴趣的是抓取维基百科关于机器学习的页面数据：
- en: '[PRE4]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We use the `GET` method to retrieve data from the web server. The web server
    replies with the status code that tells us whether the request was a success or
    failure. It also returns other metadata along with the HTML content of the web
    page – in our case, the Wikipedia page. We save the server’s response to the `GET`
    request in the `response` object.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用`GET`方法从网络服务器获取数据。服务器回复一个状态码，告诉我们请求是成功还是失败。它还会返回其他元数据以及网页的HTML内容——在我们这个例子中，是维基百科页面。我们将服务器对`GET`请求的响应保存在`response`对象中。
- en: 'We use the `BeautifulSoup` class to do the heavy lifting of parsing the HTML
    content, which we access by using `response.content`:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`BeautifulSoup`类来完成解析HTML内容的繁重工作，我们通过`response.content`来访问这些内容：
- en: '[PRE8]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, we convert the raw HTML contents contained in `response.content` into
    a digestible format by specifying `html.parser` and storing the result in the
    `soup` variable.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们将`response.content`中包含的原始HTML内容转换为可消化的格式，指定`html.parser`，并将结果存储在`soup`变量中。
- en: 'Now, let’s extract all the text contents in a paragraph:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们提取段落中的所有文本内容：
- en: '[PRE10]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We use `soup.find_all('p')` to extract all the paragraphs stored in the `soup`
    variable. Then, we apply the `join` method to combine them into a body of text
    in which each paragraph is repeated by a space, and then we store this text in
    the `passage` variable.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用 `soup.find_all('p')` 提取存储在 `soup` 变量中的所有段落。然后，我们应用 `join` 方法将它们合并成一段文本，每个段落之间用空格分隔，最后将这段文本存储在
    `passage` 变量中。
- en: 'The next step is the removal of stopwords from our data. Stopwords are common
    words that may not contain useful information in a certain use case. Hence, we
    may want to remove them to help reduce the dimensionality of our data, especially
    for tasks where these high-frequency words offer little importance, such as information
    retrieval or document clustering. Here, it may be wise to remove stopwords to
    enable faster convergence and produce better categorization. Examples of stopwords
    are words such as “and,” “the,” “in,” and “is”:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是从数据中删除停用词。停用词是指在某些使用场景下可能不包含有用信息的常见单词。因此，我们可能希望删除它们，以帮助减少数据的维度，特别是在这些高频词对任务贡献较小的情况下，比如信息检索或文档聚类。在这里，删除停用词有助于加速收敛并产生更好的分类结果。停用词的例子包括“and”、“the”、“in”和“is”：
- en: '[PRE13]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We have defined a list of stopwords. This way, we have the flexibility of adding
    words of our choice to this list. This approach can be useful when working on
    domain-specific projects in which you may want to extend your stopword list.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们已经定义了一个停用词列表。这样，我们就可以灵活地将自己选择的单词添加到这个列表中。当处理领域特定的项目时，这种方法非常有用，因为你可能需要扩展停用词列表。
- en: Note
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This step might not always be beneficial. In some NLP tasks, stopwords might
    contain useful information. For example, in text generation or machine translation,
    a model needs to generate/translate stopwords to produce coherent sentences.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步骤可能并不总是有益的。在一些自然语言处理任务中，停用词可能包含有用的信息。例如，在文本生成或机器翻译中，模型需要生成/翻译停用词，以便产生连贯的句子。
- en: 'Let’s convert the entire passage into lowercase. We do this to ensure words
    with the same semantic meaning are not represented differently – for example,
    “DOG” and “dog.” By ensuring all our data is in lowercase, we introduce uniformity
    to our dataset, removing the possibility of duplicate representation of the same
    word. To convert our text to lowercase, we use the following code:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将整段文字转换为小写字母。这样做是为了确保语义相同的单词不会有不同的表示——例如，“DOG”和“dog”。通过确保我们的数据都是小写字母，我们为数据集引入了一致性，避免了同一单词重复表示的可能性。为了将我们的文本转换为小写字母，我们使用以下代码：
- en: '[PRE40]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: When we run the code, it converts all our text data to lowercase.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们运行代码时，它会将所有的文本数据转换为小写字母。
- en: Note
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Converting a body of text to lowercase isn’t always the best solution. In fact,
    in some use cases, such as sentiment analysis, converting to lowercase may lead
    to information loss because capital letters are usually used to express strong
    emotions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 将一段文字转换为小写字母并不总是最好的解决方案。实际上，在一些使用场景中，例如情感分析，将文字转换为小写字母可能会导致信息丢失，因为大写字母通常用来表达强烈的情感。
- en: 'Next, let’s remove the HTML tags, special characters, and stopwords from the
    passage we gathered from Wikipedia. To do this, we‘ll use the following code:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们从我们从 Wikipedia 收集到的段落中删除 HTML 标签、特殊字符和停用词。为此，我们将使用以下代码：
- en: '[PRE41]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In the first line of code, we remove the HTML tags, after which we remove unwanted
    special characters in our data. Then, we pass the passage through a stopword filter
    to check and remove words that are in the stopword list, after which we combine
    the remaining words into a passage, separated by spaces between them. We print
    the first 500 characters to get an idea of what our processed text looks like.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在第一行代码中，我们删除了 HTML 标签，接着删除数据中不需要的特殊字符。然后，我们通过停用词过滤器检查并删除停用词列表中的单词，之后将剩余的单词组合成一段文字，单词之间用空格隔开。我们打印前
    500 个字符，以便了解我们处理过的文本是什么样的。
- en: 'Let''s print the first 500 characters and compare that with the web page shown
    in *Figure 10**.2*:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打印前 500 个字符，并与*图 10.2*中的网页进行比较：
- en: '[PRE49]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: When we compare the output with the web page, we can see that our text is all
    in lowercase and all the stopwords, special characters, and HTML tags have been
    removed.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们将输出与网页进行比较时，我们可以看到我们的文本都是小写字母，并且所有的停用词、特殊字符和 HTML 标签都已被删除。
- en: '![Figure 10.2 – A screenshot of the Wikipedia page on machine learning](img/B18118_10_002.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.2 – 机器学习 Wikipedia 页面截图](img/B18118_10_002.jpg)'
- en: Figure 10.2 – A screenshot of the Wikipedia page on machine learning
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 – 机器学习的维基百科页面截图
- en: Here, we explored some simple steps in preparing text data for modeling with
    neural networks. Now, let’s extend our learning by exploring tokenization.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们探索了使用神经网络建模时准备文本数据的一些简单步骤。现在，让我们通过探索分词来扩展我们的学习。
- en: Tokenization
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词
- en: We have looked at some important ideas about how to preprocess real-world text
    data. Our next step is to map out a strategy to prepare our text model. To do
    this, let’s begin by examining the concept of tokenization, which entails breaking
    sentences into smaller units called tokens. Tokenization can be implemented at
    character, sub-word, word, or even sentence level. It is common to see a lot of
    word-level tokenizers; however, the choice of tokenizer to use largely depends
    on the use case.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了一些关于如何预处理现实世界文本数据的重要概念。接下来的步骤是制定准备文本模型的策略。为此，让我们从分词的概念入手，分词意味着将句子拆分成更小的单位，称为“词元（tokens）”。分词可以在字符、子词、词语，甚至是句子层面进行。常见的是使用基于词的分词器；然而，选择哪种分词器主要取决于具体的使用场景。
- en: Let’s see how we can apply tokenization to a sentence with a sample. Let’s say
    we have this sentence – “*I like playing chess in my leisure time*.” Applying
    word-level tokenization will give us output such as `["i", "like", "playing",
    "chess", "in", "my", "leisure", "time"]`, while if we decide to use character
    level tokenization, we will have output such as `['I', ' ', 'l', 'i', 'k', 'e',
    ' ', 'p', 'l', 'a', 'y', 'i', 'n', 'g', ' ', 'c', 'h', 'e', 's', 's', ' ', 'i',
    'n', ' ', 'm', 'y', ' ', 'l', 'e', 'i', 's', 'u', 'r', 'e', ' ', 't', 'i', 'm',
    'e']`. In word-level tokenization, we split across each word, while in character-level
    tokenization, we split on each character. You can also see that wide spaces within
    the sentence are included when we use character-level tokenization. For subword
    and sentence-level tokenization, we split into subwords and sentences respectively.
    Now, let’s use TensorFlow to implement word-level and character-level tokenization.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何应用分词到句子上。假设我们有这样一句话 – “*I like playing chess in my leisure time*。”应用基于词的分词会得到如下输出：`["i",
    "like", "playing", "chess", "in", "my", "leisure", "time"]`，而如果我们决定使用字符级分词，输出会是：`['I',
    ' ', 'l', 'i', 'k', 'e', ' ', 'p', 'l', 'a', 'y', 'i', 'n', 'g', ' ', 'c', 'h',
    'e', 's', 's', ' ', 'i', 'n', ' ', 'm', 'y', ' ', 'l', 'e', 'i', 's', 'u', 'r',
    'e', ' ', 't', 'i', 'm', 'e']`。在基于词的分词中，我们按单词拆分，而在字符级分词中，我们按字符拆分。你还可以看到，在字符级分词中，句子中的空格也被包括在内。对于子词和句子级的分词，我们分别按子词和句子进行拆分。现在，让我们用TensorFlow来实现基于词和字符的分词。
- en: Word-level tokenization
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于词的分词
- en: 'Let’s see how to perform word-level tokenization with TensorFlow:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用TensorFlow执行基于词的分词：
- en: 'We will begin by importing the `Tokenizer` class:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从导入`Tokenizer`类开始：
- en: '[PRE50]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Then, we create a variable called text to hold our sample sentence (`"Machine
    learning is fascinating. It is a field full of challenges!"`). We create an instance
    of the `Tokenizer` class to handle the tokenization of our sample sentence:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们创建一个变量`text`来存储我们的示例句子（`"Machine learning is fascinating. It is a field
    full of challenges!"`）。我们创建`Tokenizer`类的实例来处理我们示例句子的分词：
- en: '[PRE51]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We can pass several parameters into the `tokenizer` class, depending on our
    use case. For instance, we can set the maximum number of words we want to keep
    by using `num_words`. Also, we may want to convert our entire text into lowercase;
    we can do this with the `tokenizer` class. However, if we don’t specify these
    parameters, TensorFlow will apply the default parameters. Then, we use the `fit_on_text`
    method to fit the tokenizer on our text data. The `fit_on_text` method goes through
    the input text and creates a vocabulary made up of unique words. It also counts
    the number of occurrences of each word in our input text data.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以根据使用场景向`tokenizer`类传递多个参数。例如，我们可以使用`num_words`来设置希望保留的最大词数。此外，我们可能希望将整个文本转换为小写字母；这可以通过`tokenizer`类来实现。然而，如果不指定这些参数，TensorFlow会应用默认参数。接着，我们使用`fit_on_text`方法来拟合我们的文本数据。`fit_on_text`方法会遍历输入文本，并创建一个由唯一单词组成的词汇表，同时计算每个单词在输入文本中的出现次数。
- en: 'To view the mapping of words to integer values, we use the `word_index` property
    of our `tokenizer` object:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看单词到整数值的映射，我们使用`tokenizer`对象的`word_index`属性：
- en: '[PRE55]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'When we print out the result, we can see that `word_index` returns a dictionary
    of key-value pairs, where each key-value pair corresponds to a unique word and
    its assigned integer index in the tokenizer’s vocabulary:'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们打印结果时，我们可以看到 `word_index` 返回一个键值对字典，其中每个键值对对应一个唯一单词及其在标记器词汇表中的整数索引：
- en: '[PRE57]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: You can see that the exclamation mark in our sample sentence is gone and the
    word `'is'` is listed only once. Also, you can see that our indexing begins at
    `1` and not `0`, because `0` is reserved as a special token, which we will encounter
    shortly. Now, let’s also examine how to perform character-level tokenization.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们示例句子中的感叹号已经消失，且单词 `'is'` 只列出了一次。此外，你可以看到我们的索引是从 `1` 开始的，而不是从 `0`，因为 `0`
    被保留为一个特殊标记，我们稍后会遇到它。现在，让我们来看看如何进行字符级别的标记化。
- en: Character-level tokenization
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 字符级别标记化
- en: 'In character-level tokenization, we split the text on each character within
    our sample text. To do this with TensorFlow, we slightly modify the code we used
    for word-level tokenization:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在字符级别标记化中，我们根据每个字符拆分示例文本。为了使用 TensorFlow 完成此操作，我们稍微修改了用于单词级别标记化的代码：
- en: '[PRE58]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Here, we set the `char_level` argument to `True` when we create our `tokenizer`
    instance. When we do this, we can see that only unique characters in our text
    will be treated as separate tokens:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，当我们创建 `tokenizer` 实例时，我们将 `char_level` 参数设置为 `True`。这样，我们可以看到文本中的唯一字符将被作为独立标记处理：
- en: '[PRE59]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Note that every unique character is represented in this scenario, including
    whitespaces (`' '`) with a token value of `1`, full stops (`'.'`) with a token
    value of `15`, and exclamations (`'!'`) with a token value of `19`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这种情况下，每个唯一字符都会被表示，包括空格（`' '`），它的标记值是 `1`，句号（`'.'`）的标记值是 `15`，以及感叹号（`'!'`）的标记值是
    `19`。
- en: Another type of tokenization we talked about is subwords. Subwords involve breaking
    down words into commonly occurring groups of characters – for example, “unhappiness”
    might be tokenized into [“un”, “happiness”]. Once the text is tokenized, each
    token can be transformed into a numerical representation, using one of the encoding
    methods that we will discuss in this chapter. Now, let’s look at another concept
    called sequencing.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的另一种标记化方式是子词标记化。子词标记化涉及将单词分解为常见字符组合，例如，“unhappiness” 可能被标记为 [“un”, “happiness”]。一旦文本被标记化，每个标记可以使用我们将在本章讨论的编码方法之一转换为数值表示。现在，让我们看看另一个叫做序列化的概念。
- en: Sequencing
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列化
- en: 'The order in which words are used in a sentence is crucial to understanding
    the meaning they convey; sequencing is the process of converting sentences or
    a group of words or tokens into their numerical representations, preserving the
    sequential order of words when building NLP applications using neural networks.
    In TensorFlow, we can use the `texts_to_sequences` function to convert our tokenized
    text into a sequence of integers. From the output of our word-level tokenization
    step, we now know that our sample sentence (`"Machine learning is fascinating.
    It is a field full of challenges!"`) can be represented by a list of tokens:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 单词在句子中的使用顺序对于理解它们所传达的意义至关重要；序列化是将句子或一组单词或标记转换为它们的数值表示的过程，在使用神经网络构建自然语言处理应用时，能够保留单词的顺序。在
    TensorFlow 中，我们可以使用 `texts_to_sequences` 函数将我们标记化的文本转换为整数序列。从我们单词级别标记化步骤的输出中，我们现在知道我们的示例句子（`"机器学习很迷人。它是一个充满挑战的领域！"）可以通过一系列标记来表示：
- en: '[PRE60]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'By converting text into sequences, we translate human-readable text into a
    machine-readable format while preserving the order in which words occur. When
    we print the result, we get the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将文本转换为序列，我们将人类可读的文本转换为机器可读的格式，同时保留单词出现的顺序。当我们打印结果时，我们会得到如下输出：
- en: '[PRE61]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The output printed is the sequence of integers that represent the original text.
    In many real-world scenarios, we will have to handle sentences of varying lengths.
    While it is not a problem for humans to understand sentences irrespective of their
    length, neural networks require us to put our data in a defined type of input
    format. In the image classification section of this book, we used a fixed width
    and height when passing image data as input; with text data, we have to ensure
    that all our sentences are of the same length. To do this, let’s return to a concept
    we discussed in [*Chapter 7*](B18118_07.xhtml#_idTextAnchor146)*,* *Image Classification
    with Convolutional Neural Networks,* padding, and see how we can leverage it to
    resolve the issue of varying sentence lengths.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出的结果是表示原始文本的整数序列。在许多现实世界的场景中，我们需要处理不同长度的句子。虽然人类可以理解不同长度的句子，但神经网络要求我们将数据以特定类型的输入格式提供。在本书的图像分类部分，我们在传递图像数据作为输入时使用了固定的宽度和高度；对于文本数据，我们必须确保所有的句子长度相同。为了实现这一点，让我们回到我们在[*第七章*](B18118_07.xhtml#_idTextAnchor146)，《卷积神经网络的图像分类》中讨论过的一个概念——填充，并看看如何利用它来解决句子长度不一致的问题。
- en: Padding
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 填充
- en: 'In [*Chapter 7*](B18118_07.xhtml#_idTextAnchor146), *Image Classification with
    Convolutional Neural Networks,* we introduced the concept of padding when we discussed
    CNNs. In the context of NLP, padding is the process of adding elements to a sequence
    to ensure that it attains a desired length. To do this in TensorFlow, we use the
    `pad_sequences` function from the `keras` preprocessing module. Let’s use an example
    to explain the application of padding to text data:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第七章*](B18118_07.xhtml#_idTextAnchor146)，《卷积神经网络的图像分类》中，我们在讨论CNN时介绍了填充的概念。在NLP的背景下，填充是向序列中添加元素以确保它达到所需长度的过程。为了在TensorFlow中实现这一点，我们使用`keras`预处理模块中的`pad_sequences`函数。让我们通过一个示例来解释如何将填充应用于文本数据：
- en: 'Let’s say we have the following four sentences:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们有以下四个句子：
- en: '[PRE62]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'When we perform word-level tokenization and sequencing, the output will look
    like this:'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们执行词级标记化和序列化时，输出将如下所示：
- en: '[PRE68]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: We can see that the length of our returned sequences varies with the second
    sentence longer than the other sentences. Let’s resolve this issue using padding
    next.
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，返回的序列长度各不相同，第二个句子比其他句子长。接下来，让我们通过填充来解决这个问题。
- en: 'We import `pad_sequences` from the TensorFlow Keras preprocessing module:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从TensorFlow Keras预处理模块中导入`pad_sequences`：
- en: '[PRE70]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: The `pad_sequences` function takes various parameters – here, we will discuss
    a few important ones, such as `sequences`, `maxlen`, `truncating`, and `padding`.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`pad_sequences`函数接受多个参数——在这里，我们将讨论一些重要的参数，如`sequences`、`maxlen`、`truncating`和`padding`。'
- en: 'Let’s start passing sequences as the only parameter and observe what the result
    looks like:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们开始仅传递序列作为参数，并观察结果是什么样的：
- en: '[PRE71]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'When we use the `pad_sequence` function, it ensures all the sentences are the
    same length as our longest sentence. To achieve this, a special token (`0`) is
    used to pad the shorter sentences until they are of the same length as the longest
    sentence. The special token (`0`) does not carry any meaning, and models are built
    to ignore them during training and inference:'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们使用`pad_sequences`函数时，它会确保所有的句子与我们最长的句子长度相同。为了实现这一点，使用一个特殊的符号（`0`）来填充较短的句子，直到它们与最长句子的长度相同。这个特殊符号（`0`）没有任何意义，模型会在训练和推理过程中忽略它们：
- en: '[PRE73]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'From the output, we see that every other sentence has zeros added to it until
    its length is the same length as our longest sentence (sentence two), which has
    the longest sequence. Note that all the zeros are added at the beginning of each
    sentence. This scenario is known as `padding=post` parameter:'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从输出中，我们看到每隔一个句子就会在其前面添加零，直到它的长度与我们最长的句子（第二个句子）相同，而第二个句子有最长的序列。请注意，所有的零都是在每个句子的开头添加的。这个场景被称为`padding=post`参数：
- en: '[PRE77]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'When we print the result, we get the following:'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们打印结果时，我们得到以下输出：
- en: '[PRE81]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: In this case, we can see that the zeros are added at the end of shorter sentences.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以看到零被添加到较短句子的末尾。
- en: 'Another useful parameter is `maxlen`. It is used to specify the maximum length
    for all sequences we want to keep. In this case, any sequence greater than the
    specified `maxlen` will be truncated. To see how `maxlen` works, let’s add another
    sentence to our list of sentences:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个有用的参数是`maxlen`。它用于指定我们希望保留的所有序列的最大长度。在这种情况下，任何超过指定`maxlen`的序列都会被截断。为了看看`maxlen`如何工作，让我们给我们的句子列表添加另一个句子：
- en: '[PRE85]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'We take our new list of sentences and perform tokenization and sequencing on
    them. Then, we pad the numerical representations to ensure that our input data
    is of the same length, and to ensure that our special (`0`) tokens are added at
    the end of a sentence, we set `padding` to `post`. When we implement these steps,
    our output looks like this:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们拿到新的句子列表，并对它们进行标记化和序列化处理。然后，我们填充数值表示，确保输入数据的长度一致，并确保我们的特殊标记（`0`）出现在句子的末尾，我们将`padding`设置为`post`。当我们实施这些步骤时，输出结果如下：
- en: '[PRE93]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: From the output, we can see that one sentence is quite long, and the other sentences
    are largely made up of numerical representations with many zeros. In this case,
    it’s clear that our longest sentence is an outlier, since all the other sentences
    are much smaller. This can skew our model’s learning process and also increase
    the computation resource required to model our data, especially when we work with
    large datasets or limited computation resources. To fix this situation, we apply
    the `maxlen` parameter. It is important to use a good `max_length` value; otherwise,
    we could lose important information in our data due to truncation. It is a good
    idea to make the maximum length long enough to capture useful information without
    adding much noise.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从输出结果可以看到，其中一句话相当长，而其他句子大多由许多零组成的数字表示。在这种情况下，很明显我们的最长句子是一个异常值，因为其他句子都要短得多。这可能会扭曲我们模型的学习过程，并且还会增加对计算资源的需求，尤其是在处理大数据集或有限计算资源时。为了解决这个问题，我们应用了`maxlen`参数。使用合适的`max_length`值非常重要，否则，我们可能会因截断而丢失数据中的重要信息。最好将最大长度设置得足够长，以捕捉有用信息，同时不增加太多噪声。
- en: 'Let’s see how to apply `maxlen` in our example. We start by setting the `max_length`
    variable to `10`. This means it will take only a maximum of 10 tokens. We pass
    the `maxlen` parameter and print our padded sequence:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看如何在示例中应用`maxlen`。我们首先将`max_length`变量设置为`10`。这意味着它最多只会使用10个标记。我们传递`maxlen`参数并打印我们的填充序列：
- en: '[PRE98]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Our result produces a much shorter sequence:'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的结果生成了一个更短的序列：
- en: '[PRE104]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Note that our longest sentence has been truncated at the beginning of the sequence.
    What if we want to truncate the sentence at the end? How do we achieve this? To
    do this, we introduce another parameter called `truncating` and set it to `post`:'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们的最长句子已经在序列的开头被截断。如果我们想将句子截断在结尾呢？我们该如何实现呢？为此，我们引入了另一个参数，叫做`truncating`，并将其设置为`post`：
- en: '[PRE109]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'Our result will look like this:'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的结果将如下所示：
- en: '[PRE113]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: We now have all our sequences padded and truncating done at the end of the sentence.
    Now, what if we train our model to classify text using these five sentences, and
    we want to make a prediction on a new sentence (“*I love playing chess*”)? Remember
    from our training sentences that our model will have tokens to represent “I” and
    “love.” However, it has no way of knowing or representing “playing” and “Chess.”
    This presents us with another problem. Let’s look at how to solve this.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经完成了所有序列的填充和在句子末尾进行的截断。那么，如果我们训练模型来使用这五个句子进行文本分类，并且我们想对一个新的句子（“*I love
    playing chess*”）进行预测呢？记住，在训练句子中，我们的模型将有标记表示“I”和“love”。然而，它没有办法知道或表示“playing”和“Chess”。这给我们带来了另一个问题。让我们看看如何解决这个问题。
- en: Out of vocabulary
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词汇表外
- en: 'So far, we have seen how to prepare our data, moving from a sequence of text
    data that makes up sentences to numerical representations to train our models.
    Now, let’s say we build a text classification model, which we train using the
    five sentences in our sentence list. Of course, this is a hypothetical situation,
    which would hold true even when we train with a massive amount of text data, as
    we will eventually come across words that our model has not seen before in training,
    such as in this scenario with our sample test sentence (“*I love playing chess*”).
    This means we must prepare our model to handle words that are not present in our
    predefined vocabulary. To fix this issue, we use `oov_token="<OOV>"` argument
    during the tokenization process:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到如何准备数据，从构成句子的文本数据序列转换为数值表示，来训练我们的模型。现在，假设我们构建一个文本分类模型，使用我们句子列表中的五个句子进行训练。当然，这只是一个假设的情况，即使我们用大量文本数据进行训练，最终也会遇到模型在训练时未见过的单词，正如我们在这个示例测试句子（“*I
    love playing chess*”）中看到的那样。这意味着我们必须准备好让模型处理那些不在我们预定义词汇表中的单词。为了解决这个问题，我们在标记化过程中使用了`oov_token="<OOV>"`参数：
- en: '[PRE118]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'After this, we fit the tokenizer on the training sentences, converting the
    sentences to sequences of integers, and then we print out the word index:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这之后，我们在训练句子上拟合分词器，将句子转换为整数序列，然后我们打印出单词索引：
- en: '[PRE119]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'Now, we can see that the `"<OOV>"` string is chosen to represent these OOV
    words and has a value of `1`. This token will take care of any unknown words that
    the model comes across. Let’s see this in action with our sample test sentence:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到`"<OOV>"`字符串被选中来表示这些OOV单词，并且它的值为`1`。这个标记将处理模型遇到的任何未知单词。让我们用我们的示例测试句子来看看实际效果：
- en: '[PRE120]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'We pass in our test sentence (`"I love playing chess"`), which contains words
    our model has not seen before, and then use the `texts_to_sequences` method to
    convert the test sentence into a sequence. Because we fit the tokenizer on the
    training sentences, it will replace each word in the test sentence with its corresponding
    numerical representation from the word index. However, the words “playing” and
    “chess,” which were not present in our training sentences, will be replaced with
    the index of the special OOV token; hence, the `print` statement returns the following:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们传入我们的测试句子（`"I love playing chess"`），其中包含了模型之前未见过的单词，然后使用`texts_to_sequences`方法将测试句子转换为一个序列。由于我们在训练句子上拟合了分词器，它会用单词索引中相应的数字表示来替换测试句子中的每个单词。然而，像“playing”和“chess”这样在我们的训练句子中未出现过的单词，将会被替换为特殊OOV标记的索引；因此，`print`语句返回如下内容：
- en: '[PRE121]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: Here, the token value of `1` is used for the words `playing` and `chess`. Using
    the OOV token is a common practice in NLP to handle words that are not present
    in the training data but may appear in the test or real-world data.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`1`的标记值被用于“playing”和“chess”这两个单词。使用OOV标记是自然语言处理（NLP）中处理训练数据中未出现但可能出现在测试数据或现实世界数据中的单词的常见做法。
- en: Now, we have our text data as a numerical representation. We have also preserved
    the sequence in which words occur; however, we need to find a way to capture the
    semantic meaning of the words and their relationships with each other. To do this,
    we use word embeddings. Let’s discuss word embeddings next.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的文本数据已经转化为数值表示。同时，我们也保留了单词出现的顺序；然而，我们需要找到一种方法来捕捉单词的语义意义及其相互之间的关系。为此，我们使用词嵌入。接下来我们将讨论词嵌入。
- en: Word embeddings
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词嵌入
- en: A significant landmark in the field of NLP is the use of word embeddings. With
    word embeddings, we are able to solve many complex modern-day text-based problems.
    **Word embeddings** are a type of word representation that allows words with similar
    meanings to have a similar representation, with the ability to capture the context
    in which a word is used. Along with the context, word embedding is also able to
    capture the semantic and syntactic similarity between words and how a word relates
    to other words. This allows ML models to generalize better when using word embedding
    in comparison to instances where words are used as standalone input.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP领域，一个重要的里程碑就是词嵌入的使用。通过词嵌入，我们能够解决许多复杂的现代文本问题。**词嵌入**是一种单词表示方式，使得具有相似意义的单词能够有相似的表示，并且能够捕捉到单词使用的上下文。除了上下文，词嵌入还能够捕捉单词之间的语义和句法相似性，以及单词与其他单词之间的关系。这使得机器学习模型在使用词嵌入时，能够比将单词作为独立输入时更好地进行泛化。
- en: Strategies such as one-hot encoding prove to be inefficient, as it builds a
    sparse representation of words largely made up of zeros. This happens because
    the more words we have in our vocabulary, the greater the number of zeros we will
    have in our resulting vector when we apply one-hot encoding. Conversely, word
    embedding is a dense vector representation in a continuous space that can capture
    the meaning, context, and relationship between words using dense and low-dimensional
    vectors.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 像独热编码（one-hot encoding）这样的策略被证明是低效的，因为它会构建一个稀疏的单词表示，主要由零组成。这是因为我们词汇量越大，应用独热编码时生成的向量中零的数量就会越多。相反，词嵌入是一种密集的向量表示，它处在一个连续的空间中，能够使用稠密且低维的向量捕捉单词的意义、上下文及其之间的关系。
- en: 'Let’s examine the following sample sentences and see how word embedding works:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下以下几个示例句子，看看词嵌入是如何工作的：
- en: She enjoys reading books.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 她喜欢阅读书籍。
- en: He likes reading newspapers.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 他喜欢阅读报纸。
- en: They are eating grapes.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 他们在吃葡萄。
- en: 'We start by tokenizing each sentence and apply sequencing to transform each
    sentence into a sequence of integers:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先对每个句子进行分词，并应用序列化，将每个句子转换成一个整数序列：
- en: '[1, 2, 3, 4]'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1, 2, 3, 4]'
- en: '[5, 6, 3, 7]'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5, 6, 3, 7]'
- en: '[8, 9, 10, 11]'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8, 9, 10, 11]'
- en: Observe that with our returned sequence, we have successfully captured the order
    in which the words that make up each sentence occur. However, this approach fails
    to take into consideration the meaning of words or the relationship between words.
    For example, the words “enjoy” and “likes” both portray positive sentiments in
    sentence 1 and sentence 2, while both sentences have “reading” as their common
    action. When we design deep learning models, we want them to be aware that “books”
    and “newspapers” are more closely related and differ from words such as “grapes”
    and “eating,” as shown in *Figure 10**.3*.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通过我们返回的序列，我们成功地捕捉到了构成每个句子的单词出现的顺序。然而，这种方法未能考虑单词的含义或单词之间的关系。例如，“enjoy”和“likes”这两个词在句子1和句子2中都表现出积极的情感，同时两个句子都有“reading”作为共同的动作。当我们设计深度学习模型时，我们希望它们能意识到“books”和“newspapers”之间的关系更为紧密，而与“grapes”和“eating”等词有很大不同，如*图
    10.3*所示。
- en: '![Figure 10.3 – Word embedding](img/B18118_10_003.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.3 – 词嵌入](img/B18118_10_003.jpg)'
- en: Figure 10.3 – Word embedding
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 – 词嵌入
- en: Word embeddings empower our models to capture relationships between words, thus
    enhancing our ability to build better-performing models. We have explored some
    foundational ideas around text preprocessing and data preparation, taking our
    text data from words to numerical representations, capturing both sequencing and
    the underlying relationships between words used in language. Let’s now put together
    everything we have learned and build a sentiment analysis model, using the Yelp
    Polarity dataset. We will start by training our own word embedding from scratch,
    after which we will apply pretrained word embeddings to our use case.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入使我们的模型能够捕捉单词之间的关系，从而增强了我们构建高性能模型的能力。我们已经探讨了一些关于文本预处理和数据准备的基础概念，将文本数据从单词转换为数值表示，同时捕捉语言中单词的顺序和潜在关系。现在，让我们将所学的知识整合起来，使用
    Yelp 极性数据集构建一个情感分析模型。我们将从头开始训练自己的词嵌入，之后再将预训练的词嵌入应用到我们的应用案例中。
- en: The Yelp Polarity dataset
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Yelp 极性数据集
- en: 'In this experiment, we will work with the Yelp Polarity dataset. This dataset
    is made up of a training size of 560,000 reviews and 38,000 reviews for testing,
    with each entry consisting of a text-based review and a label (positive – 1 and
    negative – 0). The data was drawn from customer reviews of restaurants, hair salons,
    locksmiths, and so on. This dataset presents some real challenges – for example,
    the reviews are made up of text with varying lengths, from short reviews to very
    long reviews. Also, the data contains the use of slang and different dialects.
    The dataset is available at this link: [https://www.tensorflow.org/datasets/catalog/yelp_polarity_reviews](https://www.tensorflow.org/datasets/catalog/yelp_polarity_reviews).'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们将使用 Yelp 极性数据集。该数据集由 56 万条训练评论和 3.8 万条测试评论组成，每条评论包含一个基于文本的评论和一个标签（正面
    – 1 和负面 – 0）。这些数据来自于顾客对餐厅、美发店、锁匠等的评价。该数据集面临一些实际挑战——例如，评论的文本长度各异，从简短评论到非常长的评论都有。此外，数据中还包含了俚语和不同的方言。该数据集可以通过以下链接获取：[https://www.tensorflow.org/datasets/catalog/yelp_polarity_reviews](https://www.tensorflow.org/datasets/catalog/yelp_polarity_reviews)。
- en: 'Let’s start building our model:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建我们的模型：
- en: 'We will begin by loading the required libraries:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从加载所需的库开始：
- en: '[PRE122]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: We import the necessary libraries to load, split, preprocess, and visualize
    word embeddings, and model our dataset with TensorFlow for our sentiment analysis
    use case.
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们导入必要的库来加载、拆分、预处理和可视化词嵌入，并使用 TensorFlow 对数据集进行建模，以便进行情感分析的应用。
- en: 'Then, we load the dataset:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们加载数据集：
- en: '[PRE129]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: We use the `tf.load` function to fetch datasets from TensorFlow datasets. Here,
    we specify our dataset, which is the Yelp Polarity reviews dataset. We also split
    our data into training and testing sets. We shuffle our data by setting shuffle
    to `True`, and we set `with_info=True` to ensure we can retrieve metadata of the
    dataset, which can be accessed using the `dataset_info` variable. We also set
    `as_supervised=True`; when we do this, it returns a tuple made up of the input
    and target rather than a dictionary. This way we can directly use the dataset
    with the `fit` method to train our models. We now have our training dataset as
    `train_dataset` and our testing set as `test_dataset`; both datasets are `tf.data.Dataset`
    objects. Let’s proceed with some quick data exploration before we build our sentiment
    analysis models on our training data and evaluate them on the testing data.
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用`tf.load`函数从TensorFlow数据集中获取数据集。在这里，我们指定了我们的数据集，即Yelp极性评论数据集。我们还将数据拆分为训练集和测试集。通过将shuffle设置为`True`，我们对数据进行了洗牌，并设置`with_info=True`以确保能够检索数据集的元数据，这些信息可以通过`dataset_info`变量访问。我们还设置了`as_supervised=True`；当我们这样做时，它返回由输入和目标组成的元组，而不是字典。这样，我们就可以直接使用数据集与`fit`方法来训练我们的模型。现在，我们的训练数据集是`train_dataset`，测试集是`test_dataset`；这两个数据集都是`tf.data.Dataset`对象。在构建情感分析模型并在测试数据上评估之前，让我们先进行一些快速的数据探索。
- en: 'Let’s write some functions to enable us to explore our dataset:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们编写一些函数，以便探索我们的数据集：
- en: '[PRE134]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '[PRE136]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE136]'
- en: '[PRE137]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: '[PRE139]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '[PRE140]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE140]'
- en: '[PRE141]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '[PRE143]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE143]'
- en: '[PRE144]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE144]'
- en: '[PRE145]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '[PRE146]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '[PRE147]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '[PRE148]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE148]'
- en: '[PRE149]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE150]'
- en: '[PRE151]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '[PRE152]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE152]'
- en: '[PRE153]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '[PRE155]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE155]'
- en: '[PRE156]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE156]'
- en: '[PRE157]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE157]'
- en: '[PRE158]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '[PRE159]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE160]'
- en: '[PRE161]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE162]'
- en: '[PRE163]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE163]'
- en: '[PRE164]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE164]'
- en: '[PRE165]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE165]'
- en: '[PRE166]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE166]'
- en: '[PRE167]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE167]'
- en: '[PRE168]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE168]'
- en: '[PRE169]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE169]'
- en: '[PRE170]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE170]'
- en: We use the `get_reviews` function to examine reviews from either the training
    or testing sets. This function displays the specified number of reviews and their
    corresponding labels; by default, it displays the first five reviews. However,
    we can set this parameter to any number we want. The second function is the `dataset_insight`
    function – this function performs several analyses, such as extracting the shortest,
    longest, and average length of reviews. It also generates the total count of positive
    and negative reviews in the dataset. Because we are working with a large dataset,
    we set `dataset_insight` to explore the first 2,000 samples. If you increase the
    number of samples, it will take a long time to analyze the data. We pass the total
    positive and negative count of reviews into the `plot_reviews` function to give
    us a graphical distribution of the data.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用`get_reviews`函数来查看训练集或测试集中的评论。此函数显示指定数量的评论及其相应的标签；默认情况下，它会显示前五个评论。然而，我们可以将该参数设置为任何我们想要的数字。第二个函数是`dataset_insight`函数——该函数执行多个分析操作，比如提取评论的最短、最长和平均长度。它还生成数据集中正面和负面评论的总数。由于我们处理的是一个大数据集，我们将`dataset_insight`设置为探索前2,000个样本。如果你增加样本数量，分析数据将需要更长的时间。我们将评论的正面和负面总数传递给`plot_reviews`函数，以便生成数据的图形分布。
- en: 'Let’s check out the first seven reviews in our training data:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们查看训练数据中的前七条评论：
- en: '[PRE171]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '[PRE172]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE172]'
- en: '[PRE173]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE173]'
- en: '[PRE174]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE174]'
- en: '[PRE175]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE175]'
- en: 'When we run the code, it returns the top seven reviews. Also, we only return
    the first 100 characters of each review for brevity:'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们运行代码时，它返回前七条评论。此外，为了简洁起见，我们只返回每条评论的前100个字符：
- en: '[PRE176]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE176]'
- en: '[PRE177]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE177]'
- en: '[PRE178]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE178]'
- en: '[PRE179]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE179]'
- en: '[PRE180]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE180]'
- en: '[PRE181]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE181]'
- en: '[PRE182]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE182]'
- en: '[PRE183]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE183]'
- en: 'Let’s examine some important statistics about our training data:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查一下关于训练数据的一些重要统计信息：
- en: '[PRE184]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE184]'
- en: '[PRE185]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE185]'
- en: '[PRE186]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE186]'
- en: '[PRE187]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE187]'
- en: '[PRE188]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE188]'
- en: '[PRE189]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE189]'
- en: '[PRE190]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE190]'
- en: '[PRE191]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE191]'
- en: 'When we run the code, it returns the following:'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们运行代码时，它返回以下内容：
- en: '[PRE192]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE192]'
- en: '[PRE193]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE193]'
- en: '[PRE194]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE194]'
- en: '[PRE195]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE195]'
- en: '[PRE196]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE196]'
- en: 'Let’s plot the distributions of our sampled training data:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们绘制采样训练数据的分布图：
- en: '[PRE197]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE197]'
- en: Here, we call the `plot_reviews` function and pass in the total number of positive
    and negative reviews from our sampled training data. When we run the code, we
    get the plot shown in *Figure 10**.4*.
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们调用`plot_reviews`函数，并传入我们采样的训练数据中的正面和负面评论的总数。当我们运行代码时，我们会得到图中所示的*图 10.4*。
- en: '![Figure 10.4 – A distribution of reviews from our sampled training data](img/B18118_10_004.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.4 – 我们采样的训练数据中的评论分布](img/B18118_10_004.jpg)'
- en: Figure 10.4 – A distribution of reviews from our sampled training data
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 – 我们采样的训练数据中的评论分布
- en: From the sampled training dataset, we see that our reviews are finely balanced.
    Therefore, we can proceed to train our model on this dataset. Let’s do that now.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 从采样的训练数据集中，我们可以看到评论分布非常均衡。因此，我们可以继续在此数据集上训练我们的模型。现在就让我们开始吧。
- en: 'We define the key parameters for the tokenization, sequencing, and training
    processes:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了标记化、序列化和训练过程中的关键参数：
- en: '[PRE198]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE198]'
- en: '[PRE199]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE199]'
- en: '[PRE200]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE200]'
- en: '[PRE201]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE201]'
- en: '[PRE202]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE202]'
- en: '[PRE203]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE203]'
- en: '[PRE204]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE204]'
- en: '[PRE205]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE205]'
- en: '[PRE206]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE206]'
- en: '[PRE207]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE207]'
- en: '[PRE208]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE208]'
- en: We set our vocabulary size to 10,000\. This means the tokenizer will focus on
    the top 10,000 words in our dataset. When selecting the vocabulary size, there
    is a need to strike a balance between computational efficiency and capturing word
    diversity present within the dataset under consideration. If we increase the vocab
    size, we are likely to capture more nuances that can enrich our model’s understanding,
    but this will require more computational resources for training. Also, if we reduce
    the vocab size, training will be much faster; however, we will only capture a
    small portion of the linguistic variations present in our dataset.
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将词汇表的大小设置为 10,000。这意味着分词器将集中处理数据集中排名前 10,000 的单词。在选择词汇表大小时，需要在计算效率和捕捉数据集中存在的单词多样性之间找到平衡。如果增加词汇表的大小，我们可能会捕捉到更多细微的语言差异，从而丰富模型的理解，但这将需要更多的计算资源进行训练。此外，如果减少词汇表大小，训练将会更快；然而，我们将只能捕捉到数据集中语言变化的一小部分。
- en: Next, we set the embedding dimension to 16\. This means each word will be represented
    by a 16-dimensional vector. The choice of embedding dimension is usually based
    on empirical testing. Here, our choice of embedding dimension was based on computational
    efficiency. If we use higher dimensions such as 64 or 128, we are likely to capture
    more nuanced relationships between words; however, we will need more computational
    resources for training. When working with large datasets, you may wish to use
    higher dimensions for better performance.
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们将嵌入维度设置为 16。这意味着每个单词将由一个 16 维的向量表示。嵌入维度的选择通常基于经验测试。在这里，我们选择 16 作为嵌入维度，主要是考虑到计算效率。如果我们使用更高的维度，比如
    64 或 128，我们可能会捕捉到单词之间更加细微的关系；然而，这也需要更多的计算资源进行训练。在处理大规模数据集时，您可能希望使用更高的维度来提高性能。
- en: We set our max length to 132 words; we use this based on the average word length
    we got during our exploration of the first 2,000 reviews in our data. Reviews
    longer than 132 words will be truncated after the first 132 words are selected.
    Our choice of maximum length is to ensure we strike a decent compromise between
    computational efficiency and capturing the most important aspects of most of the
    reviews in our dataset. We set truncation and `padding` to `post`; this ensures
    that longer sentences are cut off at the end of a sequence and shorter sentences
    are padded with zeros at the end of the sequence. The key assumption here is that
    most of the important information we will find in a customer’s review is likely
    to be found at the beginning part of the review.
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将最大长度设置为 132 个单词；这个长度是基于我们在探索数据集中前 2,000 条评论时得到的平均单词长度来选择的。超过 132 个单词的评论将被截断，只保留前
    132 个单词。我们选择这个最大长度，是为了在计算效率和捕捉数据集中大部分评论的最重要内容之间找到一个合理的平衡。我们将截断和 `padding` 设置为
    `post`；这确保了较长的句子会在序列的末尾被截断，而较短的句子则会在序列的末尾用零进行填充。这里的关键假设是，大多数客户评论中的重要信息可能会出现在评论的开头部分。
- en: Next, we set the OOV token to cater to OOV words that may occur in the test
    set but which the model did not see during training. Setting this parameter prevents
    our model from running into errors when handling unseen words. We also set the
    number of epochs that our model will train for to 10\. Although we use 10 to test
    out our model, you may wish to train for longer and perhaps use callbacks to monitor
    the model’s performance during training on a validation set.
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们将 OOV（Out Of Vocabulary）标记设置为处理在测试集上可能出现但在训练期间模型未见过的 OOV 单词。设置这个参数可以防止模型在处理未见过的单词时发生错误。我们还将模型训练的
    epoch 数量设置为 10。虽然我们使用 10 进行模型测试，但你可能希望训练更多的轮次，并且可以使用回调来在训练过程中监控模型在验证集上的表现。
- en: With all our parameters defined, we can now instantiate our `Tokenizer` class,
    passing in `num_words` and `oov_token` as arguments.
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在定义了所有参数之后，我们现在可以实例化我们的 `Tokenizer` 类，并将 `num_words` 和 `oov_token` 作为参数传递进去。
- en: 'To reduce the processing time, we will make use of the 20,000 samples for training:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了减少处理时间，我们将使用 20,000 个样本进行训练：
- en: '[PRE209]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE209]'
- en: '[PRE210]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE210]'
- en: '[PRE211]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE211]'
- en: '[PRE212]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE212]'
- en: '[PRE213]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE213]'
- en: '[PRE214]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE214]'
- en: '[PRE215]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE215]'
- en: '[PRE216]'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE216]'
- en: '[PRE217]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE217]'
- en: '[PRE218]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE218]'
- en: '[PRE219]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE219]'
- en: '[PRE220]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE220]'
- en: '[PRE221]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE221]'
- en: '[PRE222]'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE222]'
- en: '[PRE223]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE223]'
- en: '[PRE224]'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE224]'
- en: Here, we train our model with the first 20,000 samples from the Yelp Polarity
    training dataset. We collect these reviews and their corresponding labels, and
    since the data is in the form of bytes, we use UTF-8 encoding to decode the string,
    after which we append the text and their labels to their respective lists. We
    convert the list of labels for easy manipulation using NumPy. After this, we fit
    the tokenizer on our selected training data and convert the text to a sequence.
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们用 Yelp Polarity 训练数据集中的前 20,000 个样本来训练模型。我们收集这些评论及其对应的标签，并且由于数据是以字节的形式存在，我们使用
    UTF-8 编码来解码字符串，然后将文本及其标签分别添加到相应的列表中。我们使用 NumPy 将标签列表转换为便于操作的形式。之后，我们对所选训练数据进行分词，并将文本转换为序列。
- en: 'For testing purposes, we take 8,000 samples. The set of steps we carry out
    here is quite similar to those on the training set; however, we do not fit on
    text on the test set. This step is only for training purposes to help the neural
    network learn the word-to-index mapping in the training set:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了测试，我们选择 8,000 个样本。我们在这里执行的步骤与训练集上的步骤非常相似；然而，我们并不对测试集中的文本进行拟合。此步骤仅用于训练目的，帮助神经网络学习训练集中的词到索引的映射：
- en: '[PRE225]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE225]'
- en: '[PRE226]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE226]'
- en: '[PRE227]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE227]'
- en: '[PRE228]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE228]'
- en: '[PRE229]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE229]'
- en: '[PRE230]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE230]'
- en: '[PRE231]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE231]'
- en: '[PRE232]'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE232]'
- en: '[PRE233]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE233]'
- en: '[PRE234]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE234]'
- en: '[PRE235]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE235]'
- en: '[PRE236]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE236]'
- en: We take the first 8,000 samples from our test dataset. It is important to use
    the same tokenizer that was used to fit our training data here. This ensures that
    the word index mapping learned by the tokenizer during training is applied to
    the test set, and words not learned in the training set are replaced with the
    OOV token.
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们从测试数据集中取出前 8,000 个样本。重要的是要使用与训练数据拟合时相同的分词器。这确保了分词器在训练过程中学到的词汇索引映射被应用到测试集上，并且训练集中未学习到的单词会被替换为
    OOV（Out-Of-Vocabulary）标记。
- en: 'The next step is to pad and truncate the sequences of integers representing
    the texts in the training and testing sets, ensuring that they all have the same
    length:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是对表示训练集和测试集中文本的整数序列进行填充和截断，确保它们具有相同的长度：
- en: '[PRE237]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE237]'
- en: '[PRE238]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE238]'
- en: '[PRE239]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE239]'
- en: '[PRE240]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE240]'
- en: '[PRE241]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE241]'
- en: '[PRE242]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE242]'
- en: '[PRE243]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE243]'
- en: The output returned, `train_padded` and `test_padded`, is NumPy arrays of shape
    (`num_sequences` and `maxlen`). Now, every sequence that makes up these arrays
    is of the same length.
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 返回的输出 `train_padded` 和 `test_padded` 是形状为（`num_sequences` 和 `maxlen`）的 NumPy
    数组。现在，这些数组中的每个序列都具有相同的长度。
- en: 'We want to set up a validation set, which will help us track how our modeling
    process is going. To do this, we can use the `train_test_split` function from
    scikit-learn:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们想要设置一个验证集，这将帮助我们跟踪模型训练过程的进展。为此，我们可以使用来自 scikit-learn 的 `train_test_split`
    函数：
- en: '[PRE244]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE244]'
- en: '[PRE245]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE245]'
- en: '[PRE246]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE246]'
- en: '[PRE247]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE247]'
- en: Here, we split the data into training and validation sets, with 20 percent set
    as the validation set.
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们将数据划分为训练集和验证集，其中 20% 用作验证集。
- en: 'Let’s proceed to build our sentiment analysis model:'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续构建我们的情感分析模型：
- en: '[PRE248]'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE248]'
- en: '[PRE249]'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE249]'
- en: '[PRE250]'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE250]'
- en: '[PRE251]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE251]'
- en: '[PRE252]'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE252]'
- en: '[PRE253]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE253]'
- en: '[PRE254]'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE254]'
- en: '[PRE255]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE255]'
- en: '[PRE256]'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE256]'
- en: We build our model with TensorFlow’s Keras API. Note that we have a new layer
    called the embedding layer, which is used to represent words in a dense vector
    space. This layer takes in the vocabulary size, the embedding dimension, and the
    max length as its parameters. In this experiment, we are training our word embeddings
    as a part of our model. We can also train this layer independently for the purpose
    of learning word embeddings. This can come in handy when we intend to use the
    same word embeddings across multiple models. In [*Chapter 11*](B18118_11.xhtml#_idTextAnchor267),
    *NLP with TensorFlow,* we will see how to apply a pretrained embedding layer from
    TensorFlow Hub.
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用 TensorFlow 的 Keras API 构建我们的模型。请注意，我们有一个新的层——嵌入层，它用于在密集的向量空间中表示单词。此层接受词汇大小、嵌入维度和最大长度作为参数。在这个实验中，我们将词嵌入作为模型的一部分进行训练。我们也可以独立训练这一层，用于学习词嵌入。这在我们打算在多个模型之间共享相同词嵌入时非常有用。在
    [*第 11 章*](B18118_11.xhtml#_idTextAnchor267) *使用 TensorFlow 进行自然语言处理* 中，我们将看到如何应用来自
    TensorFlow Hub 的预训练嵌入层。
- en: When we pass in a two-dimensional tensor of shape (`batch_size`, `input_length`),
    where each sample is an integer sequence, the embedding layer returns a three-dimensional
    tensor of shape (`batch_size`, `input_length`, `embedding_dim`). At the start
    of training, embedding vectors are randomly initialized. As we train the model,
    these vectors are adjusted, ensuring words with a similar context are clustered
    closely together within the embedding space. Instead of using discrete values,
    word embeddings make use of continuous values that our model can use to discern
    patterns and model intricate relationships with the data.
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们传入一个形状为(`batch_size`, `input_length`)的二维张量，其中每个样本是一个整数序列时，嵌入层会返回一个形状为(`batch_size`,
    `input_length`, `embedding_dim`)的三维张量。在训练开始时，嵌入向量是随机初始化的。随着模型的训练，这些向量会不断调整，确保具有相似上下文的单词在嵌入空间中被紧密地聚集在一起。与使用离散值不同，词嵌入使用的是连续值，我们的模型可以利用这些值来辨别模式并模拟数据之间复杂的关系。
- en: The `GlobalAveragePooling1D` layer is applied to reduce the dimensionality of
    our data; it applies an average pooling operation. For example, if we apply `GlobalAveragePooling1D`
    to a sequence of words, it will return a summarized, single vector as output that
    can be fed into our fully connected layers for classification. Because we are
    performing binary classification, we use one neuron in our output layer and a
    sigmoid as our activation function.
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`GlobalAveragePooling1D`层用于降低数据的维度；它执行平均池化操作。例如，如果我们对一个单词序列应用`GlobalAveragePooling1D`，它将返回一个总结后的单一向量，该向量可以输入到我们的全连接层进行分类。由于我们正在执行二分类任务，我们的输出层使用一个神经元，并将sigmoid作为激活函数。'
- en: 'Now, we compile and fit our model. We pass in the loss as `binary_crossentropy`
    for the compilation step. We use Adam as our optimizer, and for our classification
    metric, we use accuracy:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将编译并训练我们的模型。在编译步骤中，我们传入`binary_crossentropy`作为损失函数。我们使用Adam优化器，并将准确率作为我们的分类评估指标：
- en: '[PRE257]'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE257]'
- en: '[PRE258]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE258]'
- en: '[PRE259]'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE259]'
- en: 'We fit the model for 10 epochs using our training data (`train_padded`) and
    labels (`train_labels`) and use the validation data to track our experiment:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用训练数据（`train_padded`）和标签（`train_labels`）对模型进行了10个epoch的训练，并使用验证数据来跟踪实验进展：
- en: '[PRE260]'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE260]'
- en: '[PRE261]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE261]'
- en: '[PRE262]'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE262]'
- en: '[PRE263]'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE263]'
- en: 'We report the results from the last 5 epochs:'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们报告最后5个epoch的结果：
- en: '[PRE264]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE264]'
- en: '[PRE265]'
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE265]'
- en: '[PRE266]'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE266]'
- en: '[PRE267]'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE267]'
- en: '[PRE268]'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE268]'
- en: '[PRE269]'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE269]'
- en: '[PRE270]'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE270]'
- en: '[PRE271]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE271]'
- en: '[PRE272]'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE272]'
- en: '[PRE273]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE273]'
- en: The model reaches an accuracy of 0.9786 on training and a validation accuracy
    of 0.8783\. This tells us there is an element of overfitting. Let’s see how our
    model will do on unseen data. To do this, let’s evaluate the model with our test
    data.
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在训练集上的准确率达到了0.9786，验证集上的准确率为0.8783。这告诉我们模型存在过拟合现象。接下来，我们看看模型在未见过的数据上的表现。为此，我们将使用测试数据评估模型。
- en: 'We use the `evaluate` function to evaluate the trained model:'
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`evaluate`函数来评估训练好的模型：
- en: '[PRE274]'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE274]'
- en: '[PRE275]'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE275]'
- en: '[PRE276]'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE276]'
- en: '[PRE277]'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE277]'
- en: We pass in the test data (`test_padded`) and test labels (`test_labels`) and
    print out the loss and accuracy. The model reached an accuracy of 0.8783 on the
    test set.
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们传入测试数据（`test_padded`）和测试标签（`test_labels`），并输出损失和准确率。模型在测试集上的准确率为0.8783。
- en: 'It is good practice to plot the loss and accuracy curves during training and
    validation, as it provides us with valuable insights into the learning process
    of the model and its performance. To do this, let’s construct a function called
    `plot_history`:'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练和验证过程中绘制损失和准确率曲线是一个好习惯，因为它能为我们提供关于模型学习过程和性能的宝贵见解。为了实现这一点，我们将构建一个名为`plot_history`的函数：
- en: '[PRE278]'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE278]'
- en: '[PRE279]'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE279]'
- en: '[PRE280]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE280]'
- en: '[PRE281]'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE281]'
- en: '[PRE282]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE282]'
- en: '[PRE283]'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE283]'
- en: '[PRE284]'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE284]'
- en: '[PRE285]'
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE285]'
- en: '[PRE286]'
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE286]'
- en: '[PRE287]'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE287]'
- en: '[PRE288]'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE288]'
- en: '[PRE289]'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE289]'
- en: '[PRE290]'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE290]'
- en: '[PRE291]'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE291]'
- en: '[PRE292]'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE292]'
- en: '[PRE293]'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE293]'
- en: '[PRE294]'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE294]'
- en: '[PRE295]'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE295]'
- en: '[PRE296]'
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE296]'
- en: '[PRE297]'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE297]'
- en: '[PRE298]'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE298]'
- en: '[PRE299]'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE299]'
- en: This function takes in the `history` object and returns to us both the loss
    and accuracy curves. The `plot_history` function will create a figure with two
    subplots – the subplot on the left shows the training and validation accuracy
    per epoch, and the subplot on the right shows the training and validation loss
    per epoch.
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个函数接收`history`对象，并返回损失和准确率曲线。`plot_history`函数将创建一个包含两个子图的图形——左侧子图显示每个 epoch
    的训练和验证准确率，右侧子图显示每个 epoch 的训练和验证损失。
- en: '![Figure 10.5 – The loss and accuracy curves](img/B18118_10_005.jpg)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.5 – 损失和准确率曲线](img/B18118_10_005.jpg)'
- en: Figure 10.5 – The loss and accuracy curves
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 – 损失和准确率曲线
- en: From the plots in *Figure 10**.5*, we can see that the model’s accuracy on training
    increases per epoch; however, the validation accuracy begins to fall slightly
    around the end of the first epoch. The training loss also falls steadily per epoch,
    while the validation loss rises steadily over each epoch, thus indicating overfitting.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 10.5*中的图表可以看出，模型在训练中的准确率每个 epoch 都在提高；然而，验证准确率在第一个 epoch 结束时开始略微下降。训练损失也在每个
    epoch 中稳步下降，而验证损失则在每个 epoch 中稳步上升，这表明模型存在过拟合现象。
- en: 'Before we explore ways to fix overfitting in our case study, let’s try four
    new sentences and see how our model fares on them:'
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们探索如何解决过拟合问题之前，让我们先尝试四个新的句子，看看我们的模型如何处理它们：
- en: '[PRE300]'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE300]'
- en: '[PRE301]'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE301]'
- en: '[PRE302]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE302]'
- en: '[PRE303]'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE303]'
- en: '[PRE304]'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE304]'
- en: 'In this example, we have the sentiments of each sentence indicated for reference.
    Let’s see how our trained model will perform on each of these new sentences:'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此示例中，我们给出了每个句子的情感供参考。让我们看看我们训练的模型在这些新句子上的表现如何：
- en: '[PRE305]'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE305]'
- en: '[PRE306]'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE306]'
- en: '[PRE307]'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE307]'
- en: '[PRE308]'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE308]'
- en: '[PRE309]'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE309]'
- en: '[PRE310]'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE310]'
- en: '[PRE311]'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE311]'
- en: '[PRE312]'
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE312]'
- en: '[PRE313]'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE313]'
- en: '[PRE314]'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE314]'
- en: '[PRE315]'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE315]'
- en: '[PRE316]'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE316]'
- en: '[PRE317]'
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE317]'
- en: '[PRE318]'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE318]'
- en: '[PRE319]'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE319]'
- en: '[PRE320]'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE320]'
- en: '[PRE321]'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE321]'
- en: '[PRE322]'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE322]'
- en: 'Let’s print out the sequence corresponding to each sentence, along with the
    sentiment the model predicted:'
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打印出每个句子对应的序列，并显示模型预测的情感：
- en: '[PRE323]'
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE323]'
- en: '[PRE324]'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE324]'
- en: '[PRE325]'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE325]'
- en: '[PRE326]'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE326]'
- en: '[PRE327]'
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE327]'
- en: '[PRE328]'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE328]'
- en: '[PRE329]'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE329]'
- en: '[PRE330]'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE330]'
- en: '[PRE331]'
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE331]'
- en: '[PRE332]'
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE332]'
- en: '[PRE333]'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE333]'
- en: '[PRE334]'
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE334]'
- en: '[PRE335]'
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE335]'
- en: Our sentiment analysis model was able to effectively predict the results correctly.
    What about if we want to visualize embedd[ings? TensorFlow has an embeddin](https://projector.tensorflow.org)g
    projector, which can be accessed at [https://projector.tensorflow.org](https://projector.tensorflow.org).
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的情感分析模型能够有效地预测正确的结果。如果我们想要可视化嵌入呢？TensorFlow提供了一个嵌入投影仪，可以通过[https://projector.tensorflow.org](https://projector.tensorflow.org)访问。
- en: Embedding visualization
  id: totrans-490
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入可视化
- en: 'If we wish to visualize word embeddings from our trained model, we will need
    to extract the learned embeddings from the embedding layer and load them into
    the embedding projector provided by TensorBoard. Let’s examine how we can do this:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望可视化来自训练模型的词嵌入，我们需要从嵌入层提取学习到的嵌入，并将其加载到TensorBoard提供的嵌入投影仪中。让我们看看如何操作：
- en: 'Extract the embedding layer weights:'
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取嵌入层权重：
- en: '[PRE336]'
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE336]'
- en: '[PRE337]'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE337]'
- en: '[PRE338]'
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE338]'
- en: '[PRE339]'
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE339]'
- en: '[PRE340]'
  id: totrans-497
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE340]'
- en: The first step is to retrieve the learned weights from our embedding layer after
    training. Next, we obtain the word index mapping that was generated during the
    tokenization process. If we apply a `print` statement, we can see the vocabulary
    size and the embedding dimension.
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一步是从嵌入层中提取训练后的学习权重。接下来，我们获取在分词过程中生成的词汇映射。如果我们应用`print`语句，我们可以看到词汇大小和嵌入维度。
- en: 'Then, save the weights and vocabulary to disk. The TensorFlow Projector reads
    these file types and uses them to plot the vectors in 3D space, so we can visualize
    them:'
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将权重和词汇保存到磁盘。TensorFlow投影仪读取这些文件类型，并使用它们在3D空间中绘制向量，从而使我们能够可视化它们：
- en: '[PRE341]'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE341]'
- en: '[PRE342]'
  id: totrans-501
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE342]'
- en: '[PRE343]'
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE343]'
- en: '[PRE344]'
  id: totrans-503
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE344]'
- en: '[PRE345]'
  id: totrans-504
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE345]'
- en: '[PRE346]'
  id: totrans-505
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE346]'
- en: '[PRE347]'
  id: totrans-506
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE347]'
- en: '[PRE348]'
  id: totrans-507
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE348]'
- en: '[PRE349]'
  id: totrans-508
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE349]'
- en: The next step is to save the embedding vectors and the vocabulary (words) as
    two separate `vecs.tsv` and `meta.tsv`, respectively. When we run this code block,
    we see that we have two new files in our Google Colab notebook, as shown in *Figure
    10**.6*.
  id: totrans-509
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是将嵌入向量和词汇（单词）分别保存为两个独立的`vecs.tsv`和`meta.tsv`文件。当我们运行此代码块时，我们会看到在Google Colab笔记本中有两个新文件，如*图10.6*所示。
- en: '![Figure 10.6 – A screenshot showing the meta and vecs files](img/B18118_10_006.jpg)'
  id: totrans-510
  prefs: []
  type: TYPE_IMG
  zh: '![图10.6 – 显示元数据和向量文件的截图](img/B18118_10_006.jpg)'
- en: Figure 10.6 – A screenshot showing the meta and vecs files
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6 – 显示元数据和向量文件的截图
- en: 'Download the files locally:'
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件下载到本地：
- en: '[PRE350]'
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE350]'
- en: '[PRE351]'
  id: totrans-514
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE351]'
- en: '[PRE352]'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE352]'
- en: '[PRE353]'
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE353]'
- en: '[PRE354]'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE354]'
- en: '[PRE355]'
  id: totrans-518
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE355]'
- en: To download the required files from Google Colab to our local machine, run this
    code block. Note that you need to move these files from your server to your local
    machine if you work in a cloud environment.
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要将所需文件从Google Colab下载到本地计算机，请运行此代码块。请注意，如果您在云环境中工作，需要将这些文件从服务器转移到本地计算机。
- en: '[Visualize the embeddings. Open](https://projector.tensorflow.org/) the embedding
    projector using this link: [https://projector.tensorflow.org/](https://projector.tensorflow.org/).
    Then, you will have to click on the load button to load the `vectors.tsv` and
    `metadata.tsv` files you downloaded to your local machine. Once you successfully
    upload the files, the word embeddings will appear in 3D, as illustrated in *Figure
    10**.7*.'
  id: totrans-520
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[可视化嵌入。打开](https://projector.tensorflow.org/)嵌入投影仪，使用此链接：[https://projector.tensorflow.org/](https://projector.tensorflow.org/)。然后，您需要点击加载按钮，将下载到本地计算机的`vectors.tsv`和`metadata.tsv`文件加载进去。一旦成功上传文件，词嵌入将以3D形式显示，如*图10.7*所示。'
- en: '![Figure 10.7 – A snapshot of word embeddings](img/B18118_10_007.jpg)'
  id: totrans-521
  prefs: []
  type: TYPE_IMG
  zh: '![图10.7 – 词嵌入的快照](img/B18118_10_007.jpg)'
- en: Figure 10.7 – A snapshot of word embe[ddings](https://www.tensorflow.org/text/guide/word_embeddings?hl=en)
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7 – 词嵌入的快照[查看详细](https://www.tensorflow.org/text/guide/word_embeddings?hl=en)
- en: '[To learn more about embedding visualization, see th](https://www.tensorflow.org/text/guide/word_embeddings?hl=en)e
    documentation: [https://www.tensorflow.org/text/guide/word_embeddings?hl=en](https://www.tensorflow.org/text/guide/word_embeddings?hl=en).
    We have now seen how to visualize word embeddings. Now, let’s try to improve the
    performance of our model.'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '[要了解更多关于嵌入可视化的信息，请参阅文档](https://www.tensorflow.org/text/guide/word_embeddings?hl=en)：[https://www.tensorflow.org/text/guide/word_embeddings?hl=en](https://www.tensorflow.org/text/guide/word_embeddings?hl=en)。我们现在已经看到了如何可视化词嵌入。接下来，让我们尝试提高模型的性能。'
- en: Improving the performance of the model
  id: totrans-524
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高模型性能
- en: Earlier, we discussed some factors that we should consider as we designed our
    baseline architecture for sentiment analysis in this chapter. Also, in [*Chapter
    8*](B18118_08.xhtml#_idTextAnchor186), *Handling Overfitting,* we explored some
    foundational concepts to mitigate against overfitting. There, we looked at ideas
    such as early stopping and dropout regularization. To curb overfitting, let’s
    begin by tuning some of our model’s hyperparameters. To do this, let’s construct
    a function called `sentiment_model`. This function takes in three parameters –
    `vocab_size`, `embedding_dim`, and the size of the training set.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们讨论了在本章中设计情感分析基线架构时需要考虑的一些因素。此外，在[*第8章*](B18118_08.xhtml#_idTextAnchor186)中，*处理过拟合*部分，我们探讨了一些基本概念，以减轻过拟合问题。在那里，我们看到了早停（early
    stopping）和丢弃正则化（dropout regularization）等思想。为了遏制过拟合，让我们从调整模型的一些超参数开始。为此，我们将构建一个名为`sentiment_model`的函数。这个函数需要三个参数——`vocab_size`、`embedding_dim`和训练集的大小。
- en: Increasing the size of the vocabulary
  id: totrans-526
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增加词汇表的大小
- en: 'One hyperparameter we may consider changing is the size of the vocabulary.
    Increasing the vocabulary size empowers the model to learn more unique words from
    our dataset. Let’s see how this will impact the performance of our base model.
    Here, we adjust `vocab_size` from `10000` to `20000`, while keeping the other
    hyperparameters constant:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能考虑调整的一个超参数是词汇表的大小。增加词汇表的大小使得模型能够从我们的数据集中学习到更多独特的单词。让我们看看这会如何影响基线模型的性能。在这里，我们将`vocab_size`从`10000`调整为`20000`，同时保持其他超参数不变：
- en: '[PRE356]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE356]'
- en: The model reaches a test accuracy of 0.8749 compared to 0.8783, which was achieved
    by our base model. Here, increasing `vocab_size` had no positive impact on the
    performance of our model.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的测试准确率为0.8749，而基线模型的准确率为0.8783。在这里，增加`vocab_size`对模型性能没有带来正面的影响。
- en: When we use a larger vocabulary size, our model will learn more unique words,
    which could be a good idea, depending on the dataset and the use case. On the
    downside, more parameters and computational resource is required for us to efficiently
    train our model. Again, there is a greater risk of overfitting.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用更大的词汇表时，模型将学习到更多独特的单词，这可能是个好主意，具体取决于数据集和使用场景。但另一方面，更多的参数和计算资源要求意味着我们需要更高效地训练模型。同时，过拟合的风险也会更大。
- en: In light of these issues, it is important to strike the right balance by ensuring
    we have a large enough `vocab_size` to capture the nuances in our data, without
    introducing the risk of overfitting at the same time. One strategy is to set a
    minimum frequency threshold, such that rare words that may lead to overfitting
    are excluded from our vocabulary. Another idea we can try is to adjust the dimensions
    of the embedding. Let’s discuss that next.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些问题，确保我们拥有足够大的`vocab_size`以捕捉数据中的细微差别，同时避免引入过拟合的风险，是至关重要的。一个策略是设置最小频率阈值，从而排除可能导致过拟合的稀有单词。我们还可以尝试调整嵌入维度。接下来我们来讨论这个。
- en: Adjusting the embedding dimension
  id: totrans-532
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整嵌入维度
- en: 'The embedding dimension refers to the size of the vector space in which words
    are represented. A high-dimension embedding has the ability to capture more nuanced
    relationships between words. However, it also increases the model complexity and
    may lead to overfitting, especially when working with small datasets. Let’s adjust
    `embedding_dim` from `16` to `32` while keeping other parameters constant and
    see what the impact will be on our experiment:'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入维度是指表示单词的向量空间的大小。高维度的嵌入能够捕捉到单词之间更为细致的关系。然而，它也增加了模型的复杂度，可能导致过拟合，尤其是在处理小数据集时。让我们将`embedding_dim`从`16`调整为`32`，同时保持其他参数不变，看看这对我们的实验有什么影响：
- en: '[PRE357]'
  id: totrans-534
  prefs: []
  type: TYPE_PRE
  zh: '[PRE357]'
- en: In 10 epochs, our new model with a larger embedding dimension reached an accuracy
    of 0.8720 on the test set. This falls short of our baseline model. Here, we see
    firsthand that an increase in the embedding dimension doesn’t always guarantee
    a better-performing model. When the embedding dimension is too small, it may fail
    to capture important relationships in our data. Conversely, an oversized embedding
    will lead to increased computation requirements and a greater risk of overfitting.
    It is important to note that a small embedding dimension suffices for simpler
    tasks or smaller datasets, while a larger embedding is an excellent choice for
    a large dataset. A pragmatic approach is to begin with a smaller embedding and
    gradually increase its size, while keeping an eye on the model’s performance during
    each iteration. Usually, the performance will improve, and at some point, diminishing
    returns will set in. When this happens, we stop training. Now, we can collect
    more data, increase the number of samples, and see what our results look like.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 经过10轮训练，使用较大嵌入维度的新模型在测试集上达到了0.8720的准确率。这个结果低于我们基准模型的表现。在这里，我们亲眼看到了嵌入维度的增加并不总是能保证模型性能的提升。当嵌入维度过小时，可能无法捕捉到数据中的重要关系。相反，嵌入维度过大会导致计算需求增加，并增加过拟合的风险。值得注意的是，对于简单任务或小型数据集，小的嵌入维度就足够了，而对于大型数据集，更大的嵌入维度则是一个更好的选择。一种务实的方法是从较小的嵌入维度开始，并在每次迭代时逐渐增加其大小，同时密切关注模型的表现。通常，性能会有所提升，但在某个点之后，收益递减现象会显现出来。到时，我们就停止训练。现在，我们可以收集更多数据，增加样本量，看看结果如何。
- en: Collecting more data
  id: totrans-536
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集更多数据
- en: 'In [*Chapter 8*](B18118_08.xhtml#_idTextAnchor186), *Handling Overfitting,*
    we explored this option when handling overfitting. Collecting more data samples
    enables us to have a more diverse set of examples that our model can learn from.
    However, this process can be time-consuming. Also, more data may not help in cases
    where it is noisy or irrelevant. For our case study, let’s increase the training
    data size from 20,000 samples to 40,000 samples:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第8章*](B18118_08.xhtml#_idTextAnchor186)《处理过拟合》中，我们探讨了在处理过拟合时的这一选项。收集更多的数据样本能够为我们提供更多样化的例子，供我们的模型学习。然而，这个过程可能会非常耗时。而且，当数据噪声大或不相关时，更多的数据可能并不会有所帮助。对于我们的案例研究，让我们将训练数据量从20,000个样本增加到40,000个样本：
- en: '[PRE358]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE358]'
- en: After 10 epochs of training, we see that our model’s performance improves to
    0.8726 on our test set. Collecting more data can be a good strategy, as it may
    provide our model with a more diverse dataset; however, it didn’t work in this
    instance. So, let’s move on to other ideas; this time, let’s try dropout regularization.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 经过10轮训练后，我们看到模型在测试集上的性能提升至0.8726。收集更多的数据可能是一个不错的策略，因为它可以为我们的模型提供更多样化的数据集；然而，这次并未奏效。那么，让我们尝试其他的想法；这次，我们来尝试dropout正则化。
- en: Dropout regularization
  id: totrans-540
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dropout正则化
- en: 'In [*Chapter 8*](B18118_08.xhtml#_idTextAnchor186), *Handling Overfitting,*
    we discussed dropout regularization, where we randomly dropped out a percentage
    of neurons from our model during training to break co-dependence among neurons
    in our model. Since we are dealing with a case of overfitting, let’s try out this
    technique. To implement dropout in our model, we can add a dropout layer, as shown
    here:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第8章*](B18118_08.xhtml#_idTextAnchor186)《处理过拟合》中，我们讨论了dropout正则化方法，在训练过程中我们随机丢弃一部分神经元，以打破神经元之间的共依赖关系。由于我们正在处理过拟合的情况，试试这个技巧吧。为了在我们的模型中实现dropout，我们可以添加一个dropout层，示例如下：
- en: '[PRE359]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE359]'
- en: Here, we set our dropout rate to 50 percent, meaning we turn off half of the
    neurons during training. Our new model achieves an accuracy of 0.8830, which is
    marginally better than our baseline model. Dropout can help to enhance the robustness
    of our model by preventing co-dependence between neurons in it. However, we must
    apply dropout with caution. If we drop out too many neurons, our model becomes
    too simple and begins to underfit because it is unable to capture the underlying
    patterns in our data. Also, if we apply a low dropout value to our model, we may
    not achieve the desired regularization effect we hope for. It is a good idea to
    experiment with different dropout values to find the best balance between model
    complexity and generalization. Now, let’s try out different optimizers.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将我们的丢弃率设定为50％，这意味着在训练期间关闭了一半的神经元。我们的新模型达到了0.8830的准确率，略优于基线模型。丢弃可以通过防止神经元之间的相互依赖来增强模型的稳健性。然而，我们必须谨慎应用丢弃。如果我们丢弃了太多的神经元，我们的模型会变得过于简单，开始欠拟合，因为它无法捕捉数据中的潜在模式。此外，如果我们将低丢弃值应用于模型，可能无法实现我们希望的正则化效果。尝试不同的丢弃值以找到模型复杂性和泛化之间的最佳平衡是一个好主意。现在，让我们尝试不同的优化器。
- en: Trying a different optimizer
  id: totrans-544
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 尝试不同的优化器
- en: 'While Adam is a good general-purpose optimizer, you might find that a different
    optimizer, such as SGD or RMSprop, works better for your specific task. Different
    optimizers might work better, depending on the task at hand. Let’s try out RMSprop
    for our use case and see how it fares:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Adam是一个很好的通用优化器，但您可能会发现，对于您的特定任务，不同的优化器（如SGD或RMSprop）效果更好。根据手头的任务，不同的优化器可能效果更好。让我们尝试使用RMSprop来处理我们的用例，看看它的表现如何：
- en: '[PRE360]'
  id: totrans-546
  prefs: []
  type: TYPE_PRE
  zh: '[PRE360]'
- en: We achieved a test accuracy of 0.8920, beating our baseline model, using RMSprop
    as our optimizer. When choosing an appropriate optimizer, it is important to assess
    the key properties of your use case. For example, when working with large datasets,
    SGD is a more suitable choice than batch gradient descent, as SGD’s use of mini-batches
    reduces the computational cost. This attribute is useful when working with limited
    computation resources. It’s worth noting that if we have too many small batches
    while using SGD, it could lead to noisy updates; on the flip side, very large
    batch sizes could increase the computational cost.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用RMSprop作为优化器，在超过基线模型的情况下，实现了0.8920的测试准确率。在选择适当的优化器时，评估您的用例的关键属性至关重要。例如，在处理大型数据集时，SGD比批量梯度下降更合适，因为SGD使用小批量减少了计算成本。在使用有限计算资源时，这一特性非常有用。值得注意的是，如果我们在使用SGD时有太多小批量，可能会导致嘈杂的更新；另一方面，非常大的批量大小可能会增加计算成本。
- en: Adam is an excellent default optimizer for many deep learning use cases, as
    it combines the advantages of RMSprop and Momentum; however, when we deal with
    simple convex problems such as linear regression, SGD proves to be a better choice,
    due to Adam’s overcompensation in these scenarios. With this, we draw the curtain
    on this chapter.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: Adam是许多深度学习用例的优秀默认优化器，因为它结合了RMSprop和动量的优点；然而，当处理简单的凸问题（如线性回归）时，SGD在这些情景中表现更佳，因为Adam在这些情况下会过度补偿。至此，我们结束了本章。
- en: Summary
  id: totrans-549
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter, we explored the foundations of NLP. We began by looking at
    how to handle real-world text data, and we explored some preprocessing ideas,
    using tools such as Beautiful Soup, requests, and regular expressions. Then, we
    unpacked various ideas, such as tokenization, sequencing, and the use of word
    embedding to transform text data into vector representations, which not only preserved
    the sequential order of text data but also captured the relationships between
    words. We took a step further by building a sentiment analysis classifier using
    the Yelp Polarity dataset from the TensorFlow dataset. Finally, we performed a
    series of experiments with different hyperparameters in a bid to improve our base
    model’s performance and overcome overfitting.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了NLP的基础知识。我们首先看了如何处理现实世界的文本数据，探索了一些预处理思路，使用了Beautiful Soup、requests和正则表达式等工具。然后，我们展开了各种想法，如分词、序列化和使用词嵌入将文本数据转换为向量表示，这不仅保留了文本数据的顺序，还捕捉了单词之间的关系。我们进一步迈出了一步，利用来自TensorFlow数据集的Yelp极性数据集构建了情感分析分类器。最后，我们进行了一系列实验，使用不同的超参数来改进我们基础模型的性能并克服过拟合问题。
- en: In the next chapter, we will introduce **Recurrent Neural Networks** (**RNNs**)
    and see how they do things differently from the DNN we used in this chapter. We
    will put RNNs to the test as we will build a new classifier with them. We will
    also take things a step further by experimenting with pretrained embeddings, and
    finally, we will round off the NLP section by generating text in a fun exercise,
    using a dataset of children's stories. See you there.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍**递归神经网络**（**RNNs**），并看看它们与本章中使用的DNN有何不同。我们将通过构建一个新的分类器来测试RNNs。我们还将通过实验使用预训练的嵌入，将事情推进到一个新层次，最后，我们将在一个有趣的练习中生成文本，使用的是儿童故事数据集。到时见。
- en: Questions
  id: totrans-552
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Let’s test what we have learned in this chapter.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试一下本章所学的内容。
- en: Using the test notebook, load the IMDB dataset from TFDS.
  id: totrans-554
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用测试笔记本，从TFDS加载IMDB数据集。
- en: Use a different embedding dimension and evaluate the model on the test set.
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用不同的嵌入维度，并在测试集上评估模型。
- en: Use a different vocabulary size and evaluate the model on the test set.
  id: totrans-556
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用不同的词汇大小，并在测试集上评估模型。
- en: Add more layers and evaluate the model on the test set.
  id: totrans-557
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加更多层并在测试集上评估模型。
- en: Use your best model to make predictions on the sample sentences given.
  id: totrans-558
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你最好的模型对给定的样本句子进行预测。
- en: Further reading
  id: totrans-559
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: 'To learn more, you can check out the following resources:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多信息，可以查看以下资源：
- en: Kapoor, A., Gulli, A. and Pal, S. (2020) *Deep Learning with TensorFlow and
    Keras, Third Edition*. Packt Publishing Ltd.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kapoor, A., Gulli, A. 和 Pal, S.（2020年） *《使用TensorFlow和Keras的深度学习（第三版）》*，Packt
    Publishing Ltd.
- en: '*Twitter Sentiment Classification using Distant Supervision* by Go et al. (2009)'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Twitter情感分类使用远程监督*，由Go等人（2009年）提出。'
- en: '*Embedding Projector: Interactive Visualization and Interpretation of Embeddings*
    by Smilkov et al. (2016)'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*嵌入投影器：嵌入的互动可视化和解释*，由Smilkov等人（2016年）提出。'
- en: '*A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural
    Networks for Sentence Classification* by Zhang et al. (2016)'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*卷积神经网络在句子分类中的敏感性分析（及实务指南）*，由Zhang等人（2016年）提出。'
