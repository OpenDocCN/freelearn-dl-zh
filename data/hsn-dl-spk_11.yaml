- en: NLP Basics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理基础
- en: 'In the previous chapter, several topics were covered concerning the undertaking
    of DL distributed training in a Spark cluster. The concepts presented there are
    common to any network model. Starting from this chapter, specific use cases for
    RNNs or LSTMs will be looked at first, and then CNNs will be covered. This chapter
    starts by introducing the following core concepts of **Natural Language Processing**
    (**NLP**):'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，涉及了在Spark集群中进行深度学习分布式训练的多个主题。那里介绍的概念适用于任何网络模型。从本章开始，将首先讨论RNN或LSTM的具体应用场景，接着介绍CNN的应用。本章开始时，将介绍以下**自然语言处理**（**NLP**）的核心概念：
- en: Tokenizers
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词器
- en: Sentence segmentation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子分割
- en: Part-of-speech tagging
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词性标注
- en: Named entity extraction
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体提取
- en: Chunking
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词块分析
- en: Parsing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语法分析
- en: The theory behind the concepts in the preceding list will be detailed before
    finally presenting two complete Scala examples of NLP, one using Apache Spark
    and the Stanford core NLP library, and the other using the Spark core and the
    `Spark-nlp` library (which is built on top of Apache Spark MLLib). The goal of
    the chapter is to make readers familiar with NLP, before moving to implementations
    based on DL (RNNs) through DL4J and/or Keras/Tensorflow in combination with Spark,
    which will be the core topic of the next chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 上述概念背后的理论将被详细讲解，最后将呈现两个完整的Scala例子，一个使用Apache Spark和斯坦福核心NLP库，另一个使用Spark核心和`Spark-nlp`库（该库构建在Apache
    Spark MLLib之上）。本章的目标是让读者熟悉NLP，然后进入基于深度学习（RNN）的实现，使用DL4J和/或Keras/Tensorflow结合Spark，这将是下一章的核心内容。
- en: NLP
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）
- en: NLP is the field of using computer science and AI to process and analyze natural
    language data and then make machines able to interpret it as humans do. During
    the 1980s, when this concept started to get hyped, language processing systems
    were designed by hand coding a set of rules. Later, following increases in calculation
    power, a different approach, mostly based on statistical models, replaced the
    original one. A later ML approach (supervised learning first, also semi-supervised
    or unsupervised at present time) brought advances in this field, such as voice
    recognition software and human language translation, and will probably lead to
    more complex scenarios, such as natural language understanding and generation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是利用计算机科学和人工智能处理与分析自然语言数据，使机器能够像人类一样理解这些数据的领域。在1980年代，当这个概念开始受到关注时，语言处理系统是通过手动编写规则来设计的。后来，随着计算能力的增加，一种主要基于统计模型的方法取代了原来的方法。随后的机器学习（ML）方法（最初是监督学习，目前也有半监督或无监督学习）在这一领域取得了进展，例如语音识别软件和人类语言翻译，并且可能会引领更复杂的场景，例如自然语言理解和生成。
- en: 'Here is how NLP works. The first task, called the speech-to-text process, is
    to understand the natural language received. A built-in model performs speech
    recognition, which does the conversion from natural to programming language. This
    happens by breaking down speech into very small units and then comparing them
    to previous units coming speech that has been input previously. The output determines
    the words and sentences that most probably have been said. The next task, called
    **part-of-speech** (**POS**) tagging (or word-category disambiguation in some
    literature), identifies words as their grammatical forms (nouns, adjectives, verbs,
    and so on) using a set of lexicon rules. At the end of these two phases, a machine
    should understand the meaning of the input speech. A possible third task of an
    NLP process is text-to-speech conversion: at the end, the programming language
    is converted into a textual or audible format understandable by humans. That''s
    the ultimate goal of NLP: to build software that analyzes, understands, and can
    generate human languages in a natural way, making computers communicate as if
    they were humans.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是NLP的工作原理。第一个任务，称为语音转文本过程，是理解接收到的自然语言。一个内置模型执行语音识别，将自然语言转换为编程语言。这个过程通过将语音分解为非常小的单元，然后与之前输入的语音单元进行比较来实现。输出结果确定最可能被说出的单词和句子。接下来的任务，称为**词性标注**（**POS**）（在一些文献中也称为词类消歧），使用一组词汇规则识别单词的语法形式（名词、形容词、动词等）。完成这两个阶段后，机器应该能够理解输入语音的含义。NLP过程的第三个任务可能是文本转语音转换：最终，编程语言被转换为人类可以理解的文本或语音格式。这就是NLP的最终目标：构建能够分析、理解并自然生成语言的软件，使计算机能够像人类一样进行交流。
- en: 'Given a piece of text, there are three things that need to be considered and
    understood when implementing NLP:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一段文本，在实现 NLP 时，有三件事需要考虑和理解：
- en: '**Semantic information**: The specific meaning of a single word. Consider,
    for example, the word *pole*, which could have different meanings (one end of
    a magnet, a long stick, and others). In a sentence like *extreme right and extreme
    left are the two poles of the political system,* in order to understand the correct
    meaning, it is important to know the relevant definition of pole. A reader would
    easily infer which one it is, but a machine can''t without ML or DL.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义信息**：单个词的具体含义。例如，考虑单词*pole*，它可能有不同的含义（磁铁的一端、一根长棍等）。在句子*极右和极左是政治系统的两个极端*中，为了理解正确的含义，了解极端的相关定义非常重要。读者可以很容易地推断出它指的是哪种含义，但机器在没有机器学习（ML）或深度学习（DL）的情况下无法做到这一点。'
- en: '**Syntax information**: The phrase structure. Consider the sentence *William
    joined the football team already with long international experience*. Depending
    on how it is read, it has different meanings (it could be William or the football
    team that has long international experience).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语法信息**：短语结构。考虑句子*William 加入了拥有丰富国际经验的足球队*。根据如何解读，它有不同的含义（可能是威廉拥有丰富的国际经验，也可能是足球队拥有丰富的国际经验）。'
- en: '**Context information**: The context where a word or a phrase appears. Consider,
    for example, the adjective *low*. It is often positive when part of a context
    of convenience (for example, *This mobile phone has a low price*), but it is almost
    always negative when talking about supplies (for example, *Supplies of drinkable
    water are running low*).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文信息**：词语或短语出现的上下文。例如，考虑形容词*low*。它在便利的上下文中通常是积极的（例如，*这款手机价格很低*），但在谈到供应时几乎总是消极的（例如，*饮用水供应不足*）。'
- en: The following subsections will explain the main concepts of NLP supervised learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节将解释 NLP 监督学习的主要概念。
- en: Tokenizers
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词器
- en: 'Tokenization means defining what a word is in NLP ML algorithms. Given a text,
    tokenization is the task of cutting it down into pieces, called **tokens**, while
    at the same time also removing particular characters (such as punctuation or delimiters).
    For example, given this input sentence in the English language:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 分词意味着在 NLP 机器学习算法中定义一个词是什么。给定一段文本，分词任务是将其切分成片段，称为**tokens**，同时去除特定字符（如标点符号或分隔符）。例如，给定以下英语输入句子：
- en: '`To be, or not to be, that is the question`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`To be, or not to be, that is the question`'
- en: 'The result of tokenization would produce the following 11 tokens:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 分词的结果将产生以下11个tokens：
- en: '`To be or or not to be that is the question`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`To be or or not to be that is the question`'
- en: 'One big challenge with tokenization is about what the correct tokens to use
    are. In the previous example, it was easy to decide: we cut down on white spaces
    and removed all the punctuation characters. But what if the input text isn''t
    in English? For some other languages, such as Chinese for example, where there
    are no white spaces, the preceding rules don''t work. So, any ML/DL model training
    for NLP should consider the specific rules for a language.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 分词的一个大挑战是如何确定正确的tokens。在前一个示例中，决定是很容易的：我们去除了空格和所有标点符号字符。但如果输入文本不是英语呢？例如中文等其他语言，没有空格，前述规则就不起作用了。因此，任何针对
    NLP 的机器学习或深度学习模型训练都应考虑到特定语言的规则。
- en: 'But even when limited to a single language, let''s say English, there could
    be tricky cases. Consider the following example sentence:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但即使仅限于单一语言，比如英语，也可能出现棘手的情况。考虑以下示例句子：
- en: '`David Anthony O''Leary is an Irish football manager and former player`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`David Anthony O''Leary 是一位爱尔兰足球经理和前球员`'
- en: 'How do you manage the apostrophe? There are five possible tokenizations in
    this case for `O''Leary`. These are as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如何处理撇号？在这种情况下，`O'Leary`有五种可能的分词方式，分别如下：
- en: '`leary`'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`leary`'
- en: '`oleary`'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`oleary`'
- en: '`o''leary`'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`o''leary`'
- en: '`o'' leary`'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`o'' leary`'
- en: '`o leary`'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`o leary`'
- en: But which one is the desired one? A simple strategy that comes quickly to mind
    could be to just split out all the non-alphanumeric characters in sentences. So,
    getting the `o` and `leary` tokens would be acceptable, because doing a Boolean
    query search with those tokens would match three cases out of five. But what about
    this following sentence?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，哪个是期望的结果呢？一个快速想到的简单策略可能是把句子中的所有非字母数字字符去掉。因此，获取`o`和`leary`这些tokens是可以接受的，因为用这些tokens进行布尔查询搜索会匹配五个案例中的三个。但以下这个句子呢？
- en: '`Michael O''Leary has slammed striking cabin crew at Aer Lingus saying “they
    aren''t being treated like Siberian salt miners”.`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`Michael O''Leary批评了在爱尔兰航空罢工的机组人员，称“他们没有被像西伯利亚盐矿工一样对待”。`'
- en: 'For *aren''t, *there are four possible tokenizations, which are as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*aren't*，有四种可能的词元拆分方式，如下：
- en: '`aren''t`'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`aren''t`'
- en: '`arent`'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`arent`'
- en: '`are n''t`'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`are n''t`'
- en: '`aren t`'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`aren t`'
- en: Again, while the `o` and `leary` split looks fine, what about the `aren` and
    `t` split? This last one doesn't look good; a Boolean query search with those
    tokens would match two cases only out of four.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，虽然`o`和`leary`的拆分看起来没问题，但`aren`和`t`的拆分怎么样呢？最后这个拆分看起来不太好；用这些词元做布尔查询搜索，只有四种情况中的两种能匹配。
- en: Challenges and issues with tokenization are language-specific. A deep knowledge
    of the language of the input documents is required in this context.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 词元化的挑战和问题是语言特定的。在这种情况下，需要深入了解输入文档的语言。
- en: Sentence segmentation
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 句子分割
- en: 'Sentence segmentation is the process of splitting up a text into sentences.
    From its definition, it seems a straightforward process, but several difficulties
    can occur with it, for example, the presence of punctuation marks that can be
    used to indicate different things:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 句子分割是将文本拆分成句子的过程。从定义来看，这似乎是一个简单的过程，但也可能会遇到一些困难，例如，存在可能表示不同含义的标点符号：
- en: '`Streamsets Inc. has released the new Data Collector 3.5.0\. One of the new
    features is the MongoDB lookup processor.`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`Streamsets公司发布了新的Data Collector 3.5.0版本。新功能之一是MongoDB查找处理器。`'
- en: 'Looking at the preceding text, you can see that the same punctuation mark (`.`)
    is used for three different things, not just as a sentence separator. Some languages,
    such as Chinese for example, come with unambiguous sentence-ending markers, while
    others don''t. So a strategy needs to be set. The quickest and dirtiest approach
    to locate the end of a sentence in a case like that in the previous example is
    the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下前面的文本，你会发现同一个标点符号（`.`）被用来表示三种不同的意思，而不仅仅是作为句子的分隔符。某些语言，比如中文，拥有明确的句尾标记，而其他语言则没有。因此，必须制定一个策略。在像前面例子中这种情况下，找到句子结束的位置的最快且最粗暴的方法是：
- en: If it is a full stop, then it ends a sentence
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是句号，那么它表示一句话的结束。
- en: If the token preceding a full stop is present in a hand-precompiled list of
    abbreviations, then the full stop doesn't end the sentence
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果紧接句号的词元出现在预先编译的缩写词列表中，那么句号不表示句子的结束。
- en: If the next token after a full stop is capitalized, then the full stop ends
    the sentence
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果句号后的下一个词元是大写字母开头的，那么句号表示一句话的结束。
- en: This gets more than 90% of sentences correct, but something smarter can be done,
    such as rule-based boundary disambiguation techniques (automatically learn a set
    of rules from input documents where the sentence breaks have been pre-marked),
    or better still, use a neural network (this can achieve more than 98% accuracy).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法能正确处理超过90%的句子，但可以做得更智能一些，比如使用基于规则的边界消歧技术（自动从标记过句子断句的输入文档中学习一组规则），或者更好的是，使用神经网络（这可以达到超过98%的准确率）。
- en: POS tagging
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词性标注
- en: 'POS tagging in NLP is the process of marking a word, depending on its definition
    and context as well, inside a text as corresponding to a particular POS. Speech
    has nine main parts—nouns, verbs, adjectives, articles, pronouns, adverbs, conjunctions,
    prepositions, and interjections. Each of them is divided into sub-classes. This
    process is more complex than tokenization and sentence segmentation. POS tagging
    can''t be generic because, depending on the context, the same word could have
    a different POS tag in sentences belonging to the same text, for example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注是自然语言处理中根据单词的定义和上下文标记文本中每个单词为相应词性的过程。语言有九大类词性——名词、动词、形容词、冠词、代词、副词、连词、介词和感叹词。每一类都有子类。这个过程比词元化和句子分割更复杂。词性标注不能是通用的，因为根据上下文，相同的单词在同一文本中的句子中可能具有不同的词性标签，例如：
- en: '`Please lock the door and don''t forget the key in the lock.`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`请锁好门，并且不要忘记把钥匙留在锁里。`'
- en: Here, the word `lock` is used twice with two different meanings (as a verb and
    as a noun) in the same sentence. Differences across languages should be considered
    as well. So this is a process that can't be handled manually, but it should be
    machine-based. The algorithms used can be rule-based or stochastic. Rule-based
    algorithms, in order to assign tags to unknown (or at least ambiguous) words,
    make use of contextual information. Disambiguation is achieved by analyzing different
    linguistic features of a word, such as the preceding and following words. A rule-based
    model is trained from a starting set of rules and data, and attempts to infer
    execution instructions for POS tagging. Stochastic taggers relate to different
    approaches; basically, any model that includes probability or frequency can be
    labeled this way. A simple stochastic tagger can disambiguate words based only
    on the probability a word occurs with a particular tag. More complex stochastic
    taggers are more efficient, of course. One of the most popular is the hidden Markov
    model ([https://en.wikipedia.org/wiki/Hidden_Markov_model](https://en.wikipedia.org/wiki/Hidden_Markov_model)),
    a statistical model in which the system being modeled is assumed to be a Markov
    process ([https://en.wikipedia.org/wiki/Markov_chain](https://en.wikipedia.org/wiki/Markov_chain))
    with hidden states.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，单词 `lock` 在同一句话中被两次使用，且含义不同（作为动词和名词）。不同语言之间的差异也应该考虑在内。因此，这是一个无法手动处理的过程，应该由机器来完成。使用的算法可以是基于规则的，也可以是基于随机的。基于规则的算法，为了给未知（或至少模糊的）单词分配标签，利用上下文信息。通过分析单词的不同语言特征，如前后的单词，可以实现歧义消解。基于规则的模型从一组初始规则和数据开始训练，尝试推断出
    POS 标注的执行指令。随机标注器涉及不同的方法；基本上，任何包含概率或频率的模型都可以这样标记。一种简单的随机标注器可以仅通过单词与特定标签发生的概率来消除歧义。当然，更复杂的随机标注器效率更高。最流行的之一是隐藏马尔可夫模型（[https://en.wikipedia.org/wiki/Hidden_Markov_model](https://en.wikipedia.org/wiki/Hidden_Markov_model)），这是一种统计模型，其中被建模的系统被假设为具有隐藏状态的马尔可夫过程（[https://en.wikipedia.org/wiki/Markov_chain](https://en.wikipedia.org/wiki/Markov_chain)）。
- en: Named entity extraction (NER)
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）
- en: 'NER is the sub-task of NLP, the goal of which is to locate and classify named
    entities in a text into predefined categories. Let''s give an example. We have
    the following sentence:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: NER 是 NLP 的一个子任务，其目标是在文本中定位和分类命名实体，并将其划分为预定义的类别。让我们举个例子。我们有以下句子：
- en: '`Guglielmo is writing a book for Packt Publishing in 2018.`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`Guglielmo 正在为 Packt Publishing 写一本书，出版时间为 2018 年。`'
- en: 'An NER process on it will produce the following annotated text:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对其进行 NER 处理后，得到以下注释文本：
- en: '`[Guglielmo][Person] is writing a book for [Packt Publishing][Organization]
    in [2018][Time] .`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`[Guglielmo][人名] 正在为 [Packt Publishing][组织] 写一本书，出版时间为 [2018][时间] 。`'
- en: Three entities have been detected, a person, `Guglielmo`, a two-token organization, `Packt
    Publishing`, and a temporal expression, `2018`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 已经检测到三个实体，一个人，`Guglielmo`，一个由两个标记组成的组织，`Packt Publishing`，以及一个时间表达，`2018`。
- en: Traditionally, NER has been applied to structured text, but lately the number
    of use cases for unstructured text has grown.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，NER 应用于结构化文本，但最近，非结构化文本的使用案例数量有所增加。
- en: Challenges with automating this process implementation have been case sensitivity
    (earlier algorithms often failed to recognize, for example, that Guglielmo Iozzia
    and GUGLIELMO IOZZIA are the same entity), different uses of punctuation marks,
    and missing separation characters. Implementations of NER systems use linguistic
    grammar-based techniques or statistical models and ML. Grammar-based systems can
    give better precision, but have a huge cost in terms of months of work by experienced
    linguists, plus they have low recall. ML-based systems have a high recall, but
    require a large amount of manually annotated data for their training. Unsupervised
    approaches are coming through order to drastically reduce the effort of data annotation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化实现此过程的挑战包括大小写敏感性（早期算法经常无法识别例如 Guglielmo Iozzia 和 GUGLIELMO IOZZIA 是同一个实体）、标点符号的不同使用以及缺失的分隔符。NER
    系统的实现使用了基于语言学语法的技术、统计模型和机器学习。基于语法的系统可以提供更高的精度，但在经验丰富的语言学家工作数月的成本上有很大开销，而且召回率较低。基于机器学习的系统具有较高的召回率，但需要大量手动标注的数据来进行训练。无监督方法正在崭露头角，旨在大幅减少数据标注的工作量。
- en: Another challenge for this process is the context domain—several studies have
    demonstrated that NER systems developed for one domain (reaching high performance
    on it) typically don't perform well the same way in other domains. An NER system
    that has been trained in a Twitter content, for example, can't be applied, expecting
    the same performance and accuracy, to medical records. This applies for both rule-based
    and statistical/ML systems; a considerable effort is needed when tuning NER systems
    in a new domain in order for them to reach the same level of performance they
    had in the original domain where they were successfully trained.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的另一个挑战是上下文领域——一些研究表明，为一个领域开发的命名实体识别（NER）系统（在该领域达到较高的性能）通常在其他领域表现不佳。例如，一个已经在Twitter内容上训练过的NER系统，不能简单地应用到医疗记录中，并期望它能达到同样的性能和准确性。这适用于基于规则和统计/机器学习的系统；在新的领域中调整NER系统以达到在原始领域成功训练时的同样性能，需要付出相当大的努力。
- en: Chunking
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分块
- en: Chunking in NLP is the process of extracting phrases from text. It is used because
    simple tokens may not represent the real meaning of the text under examination.
    As an example, consider the phrase *Great Britain*; while the two separate words
    make sense, it is more advisable to use *Great Britain* as a single word. Chunking
    works on top of POS tagging; typically POS tags are input, while chunks are the
    output from it. This process is very similar to the way the human brain chunks
    information together to make it easier to process and understand. Think about
    the way you memorize, for example, sequences of numbers (such as debit card pins,
    telephone numbers, and others); you don't tend to memorize them as individual
    numbers, but try to group them together in a way that makes them easier to remember.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理中的分块（Chunking）是从文本中提取短语的过程。使用分块的原因是，因为简单的词语可能无法代表所分析文本的真实含义。举个例子，考虑短语*Great
    Britain*；虽然这两个单独的词语有意义，但更好的做法是将*Great Britain*作为一个整体来使用。分块通常建立在词性标注（POS tagging）的基础上；通常，词性标注是输入，而分块则是它的输出。这个过程与人类大脑将信息分块以便于处理和理解的方式非常相似。想一想你记忆数字序列（例如借记卡密码、电话号码等）的方式；你通常不会把它们当作单独的数字来记，而是试图将它们分组，以便更容易记住。
- en: 'Chunking can be up or down. Chunking up tends more to abstraction; chunking
    down tends to look for more specific details. As an example, consider the following
    scenario in a call with a ticket sales and distribution company. The operator
    asks the question "*which kind of tickets would you like to purchase?*" The customer''s
    answer, "*Concert tickets*", is chunking up, because it moves towards a higher
    level of abstraction. Then, the operator asks more questions, such as "*which
    genre*," "*which artist or group*," "*for which dates and locations*,", "*for
    how many people*,", "*which sector*,", and so on, in order to get more details
    and fulfill the customer''s needs (this is chunking down). At the end, you can
    think about chunking as a hierarchy of sets. For a given context, there is always
    a higher-level set, which has subsets, and each subset can have other subsets.
    For example, consider a programming language as a higher-level subset; you can
    then have the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 分块可以向上或向下进行。向上分块更倾向于抽象化；向下分块则更倾向于寻找更具体的细节。举个例子，考虑在一个票务销售和分发公司的电话中发生的场景。接线员问：“*您想购买哪种类型的票？*”顾客的回答是：“*音乐会票*”，这属于向上分块，因为它更倾向于一个更高层次的抽象。然后，接线员提出更多问题，如：“*哪种类型*”，“*哪位艺术家或团体*”，“*哪个日期和地点*”，“*多少人*”，“*哪个区域*”等等，以获得更多细节并满足顾客的需求（这就是向下分块）。最终，你可以将分块视为一种集合的层次结构。对于一个特定的上下文，总是有一个更高层次的集合，它有子集，每个子集又可以有其他子集。例如，可以考虑编程语言作为一个更高层次的子集；然后你可以得到以下情况：
- en: '`Programming Language`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`编程语言`'
- en: '`Scala (Subset of Programming Language)`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`Scala（编程语言的子集）`'
- en: '`Scala 2.11 (Subset of Scala)`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`Scala 2.11（Scala的子集）`'
- en: '`Trait (a specific concept of Scala)`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`特性（Scala的特定概念之一）`'
- en: '`Iterator (a core Scala trait)`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`迭代器（Scala的核心特性之一）`'
- en: Parsing
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解析
- en: 'Parsing in NLP is the process of determining the syntactic structure of a text.
    It works by analyzing the text constituent words and it bases itself on the underlying
    grammar of the specific language in which the text has been written. The outcome
    of parsing is a parse tree of each sentence part of the input text. A parse tree
    is an ordered, rooted tree that represents the syntactic structure of a sentence
    according to some context-free grammar (a set of rules that describe all the possible
    strings in a given formal language). Let''s make an example. Consider the English
    language and the following example grammar:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 中的解析是确定文本句法结构的过程。它通过分析文本的组成词汇进行工作，并基于文本所在语言的基础语法。解析的结果是输入文本每个句子的解析树。解析树是一个有序的、带根的树，表示句子的句法结构，依据的是某种上下文无关语法（描述给定形式语言中所有可能字符串的规则集）。我们来举个例子。考虑英语语言和以下的语法示例：
- en: '`sentence -> noun-phrase, verb-phrase`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`sentence -> 名词短语，动词短语`'
- en: '`noun-phrase -> proper-noun`'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`noun-phrase -> 专有名词`'
- en: '`noun-phrase -> determiner, noun`'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`noun-phrase -> 决定词，名词`'
- en: '`verb-phrase -> verb, noun-phrase`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`verb-phrase -> 动词，名词短语`'
- en: 'Consider the phrase `Guglielmo wrote a book` and apply the parsing process
    to it. The output would be a parse tree like this:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑短语 `Guglielmo wrote a book`，并对其应用解析过程。输出将是这样的解析树：
- en: '![](img/22e7c67c-532a-4dd6-839b-9ffe6f8fb299.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22e7c67c-532a-4dd6-839b-9ffe6f8fb299.png)'
- en: Currently, the approaches to automated machine-based parsing are statistical,
    probabilistic, or ML.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，自动化机器解析的方法主要是统计的、概率的或机器学习（ML）方法。
- en: Hands-on NLP with Spark
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spark 进行 NLP 实践
- en: In this section, some examples of the implementation of NLP (and its core concepts
    as described in the previous sections) with Apache Spark are going to be detailed.
    These examples don't include DL4J or other DL frameworks, as NLP with multilayer
    neural networks will be the main topic of the next chapter.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，将详细介绍在 Apache Spark 中实现 NLP（以及前述核心概念）的几个示例。这些示例不包括 DL4J 或其他深度学习框架，因为多层神经网络的
    NLP 将是下一章的主要内容。
- en: While one of the core components of Spark, MLLib, is an ML library, it doesn't
    provide any facility for NLP. So, you need to use some other NLP library or framework
    on top of Spark.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Spark 的核心组件之一，MLLib，是一个机器学习库，但它并未提供 NLP 的相关功能。因此，您需要在 Spark 上使用其他 NLP 库或框架。
- en: Hands-on NLP with Spark and Stanford core NLP
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spark 和 Stanford Core NLP 进行 NLP 实践
- en: The first example covered in this chapter involves a Scala Spark wrapper of
    the Stanford core NLP ([https://github.com/stanfordnlp/CoreNLP](https://github.com/stanfordnlp/CoreNLP))
    library, which is open source and released with the GNU general public licence
    v3 ([https://www.gnu.org/licenses/gpl-3.0.en.html](https://www.gnu.org/licenses/gpl-3.0.en.html)).
    It is a Java library that provides a set of natural language analysis tools. Its
    basic distribution provides model files for the analysis of English, but the engine
    is compatible with models for other languages as well. It is stable and ready
    for production, and widely used across different areas of academic and industry.
    Spark CoreNLP ([https://github.com/databricks/spark-corenlp](https://github.com/databricks/spark-corenlp))
    is a wrapper of the Stanford Core NLP Java library for Apache Spark. It has been
    implemented in Scala. The Stanford Core NLP annotators have been wrapped as Spark
    DataFrames.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第一个示例涉及使用 Scala 包装的 Stanford Core NLP ([https://github.com/stanfordnlp/CoreNLP](https://github.com/stanfordnlp/CoreNLP))
    库，它是开源的，并以 GNU 通用公共许可证 v3 发布 ([https://www.gnu.org/licenses/gpl-3.0.en.html](https://www.gnu.org/licenses/gpl-3.0.en.html))。它是一个
    Java 库，提供一套自然语言分析工具。其基本分发版提供了用于分析英语的模型文件，但该引擎也兼容其他语言的模型。它稳定且适用于生产环境，广泛应用于学术和工业领域。Spark
    CoreNLP ([https://github.com/databricks/spark-corenlp](https://github.com/databricks/spark-corenlp))
    是 Stanford Core NLP Java 库的 Apache Spark 封装。它已用 Scala 实现。Stanford Core NLP 注释器已作为
    Spark DataFrame 封装。
- en: 'The spark-corenlp library''s current release contains a single Scala class
    function, which provides all of the high-level wrapper methods, as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: spark-corenlp 库的当前版本包含一个 Scala 类函数，提供了所有高级封装方法，如下所示：
- en: '`cleanXml`: Takes as input an XML document and removes all the XML tags.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cleanXml`：输入一个 XML 文档，并移除所有 XML 标签。'
- en: '`tokenize`: Tokenizes the input sentence into words.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenize`：将输入句子分割成单词。'
- en: '`ssplit`: Splits its input document into sentences.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ssplit`：将输入文档分割成句子。'
- en: '`pos`: Generates the POS tags of its input sentence.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pos`：生成输入句子的词性标签。'
- en: '`lemma`: Generates the word lemmas of its input sentence.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lemma`：生成输入句子的词形还原。'
- en: '`ner`: Generates the named entity tags of its input sentence.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ner`：生成输入句子的命名实体标签。'
- en: '`depparse`: Generates the semantic dependencies of its input sentence.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`depparse`：生成输入句子的语义依赖关系。'
- en: '`coref`: Generates the `coref` chains of its input document.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`coref`：生成输入文档的 `coref` 链。'
- en: '`natlog`: Generates the natural logic notion of polarity for each token in
    its input sentence. The possible returned values are up, down, or flat.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`natlog`：生成输入句子中每个词元的自然逻辑极性。可能的返回值有：up（上升）、down（下降）或 flat（平稳）。'
- en: '`openie`: Generates a list of open IE triples as flat quadruples.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openie`：生成一组开放的 IE 三元组，表示为扁平的四元组。'
- en: '`sentiment`: Measures the sentiment of its input sentence. The scale goes from
    zero (strongly negative) to four (strongly positive).'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sentiment`：测量输入句子的情感。情感评分的范围是从零（强烈负面）到四（强烈正面）。'
- en: 'The first thing to do is to set up the dependencies for this example. It has
    dependencies on Spark SQL and the Stanford core NLP 3.8.0 (the importing of the
    models needs to be explicitly specified through the `Models` classifier), as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的是设置此示例的依赖项。它依赖于 Spark SQL 和 Stanford core NLP 3.8.0（需要通过 `Models` 分类器显式指定模型的导入），如下所示：
- en: '[PRE0]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When you need to work with a single language, let''s say Spanish, you can also
    choose to import the models for that language only, through its specific classifier,
    as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当您只需要处理某一种语言时，例如西班牙语，您也可以选择仅导入该语言的模型，通过其特定的分类器，方式如下：
- en: '[PRE1]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: No library is available for `spark-corenlp` on Maven central. So, you have to
    build its JAR file starting from the GitHub source code, and then add it to the
    classpath of your NLP application or, if your applications rely on an artifact
    repository, such as JFrog Artifactory ([https://jfrog.com/artifactory/](https://jfrog.com/artifactory/)),
    Apache Archiva ([https://archiva.apache.org/index.cgi](https://archiva.apache.org/index.cgi)),
    or Sonatype Nexus OSS ([https://www.sonatype.com/nexus-repository-oss](https://www.sonatype.com/nexus-repository-oss)),
    store the JAR file there and then add the dependency to it in your project build
    file the same way as for any other dependency available in Maven central.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Maven 中央库中没有可用的 `spark-corenlp` 库。因此，您必须从 GitHub 源代码构建其 JAR 文件，然后将其添加到您的 NLP
    应用程序的类路径中，或者如果您的应用程序依赖于某个工件库（例如 JFrog Artifactory（[https://jfrog.com/artifactory/](https://jfrog.com/artifactory/)）、Apache
    Archiva（[https://archiva.apache.org/index.cgi](https://archiva.apache.org/index.cgi)）或
    Sonatype Nexus OSS（[https://www.sonatype.com/nexus-repository-oss](https://www.sonatype.com/nexus-repository-oss)）），请将
    JAR 文件存储在那里，并按照与 Maven 中央库中任何其他依赖项相同的方式，将其依赖关系添加到您的项目构建文件中。
- en: 'I previously mentioned that `spark-corenlp` wraps Stanford core NLP annotators
    as DataFrames. So, the first thing to do in the source code is to create a `SparkSession`,
    as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前提到过，`spark-corenlp` 将 Stanford core NLP 注释器封装为 DataFrame。因此，源代码中需要做的第一件事是创建一个
    `SparkSession`，如下所示：
- en: '[PRE2]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create now a `Sequence` ([https://www.scala-lang.org/api/current/scala/collection/Seq.html](https://www.scala-lang.org/api/current/scala/collection/Seq.html))
    for the input text content (in XML format) and then transform it into a DataFrame,
    as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，创建一个 `Sequence`（[https://www.scala-lang.org/api/current/scala/collection/Seq.html](https://www.scala-lang.org/api/current/scala/collection/Seq.html)）来表示输入文本内容（XML
    格式），然后将其转换为 DataFrame，如下所示：
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Given this input, we could do different NLP operations using the `functions`
    available methods, such as cleaning from the tags the input XML included in the
    `text` field of the `input` DataFrame, splitting each sentence into single words,
    generating the named entity tags for each sentence, and measuring the sentiment
    of each sentence, for example:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这个输入，我们可以使用 `functions` 可用的方法执行不同的 NLP 操作，例如从标签中清理输入的 XML（包含在 `input` DataFrame
    的 `text` 字段中）、将每个句子拆分成单词、生成每个句子的命名实体标签，并测量每个句子的情感等操作：
- en: '[PRE4]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'At the end, we print the output of those operations (`output` itself is a DataFrame),
    as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们打印这些操作的输出（`output` 本身是一个 DataFrame），如下所示：
- en: '[PRE5]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we need to stop and destroy the `SparkSession`, as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要停止并销毁 `SparkSession`，如下所示：
- en: '[PRE6]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Executing this example, the output is as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此示例时，输出如下：
- en: '![](img/e3407710-8b0e-48b1-8834-ea8e69520571.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3407710-8b0e-48b1-8834-ea8e69520571.png)'
- en: The XML content has been cleared from the tags, the sentences have been split
    into their single words as expected, and for some words (`Birmingham`, `Mumbai`)
    a named entity tag (`LOCATION`) has been generated. And, the sentiment is positive
    (`3`) for both input sentences!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: XML 内容已经从标签中清除，句子已按照预期拆分成单个单词，对于某些单词（如`Birmingham`、`Mumbai`），已生成命名实体标签（`LOCATION`）。并且，对于输入的两句话，情感分析结果为积极（`3`）！
- en: This approach is a good way to start with NLP in Scala and Spark; the API provided
    by this library is simple and high level, and gives time to people to assimilate
    the core NLP concepts quickly, while leveraging the Spark DataFrames capabilities.
    But there are downsides with it. Where there is a need to implement more complex
    and custom NLP solutions, the available API is too simple to tackle them. Also
    a licensing problem could occur if your final system isn't for internal purposes
    only, but your company plan is to sell and distribute your solution to customers;
    the Stanford core NLP library and `spark-corenlp` models depend on, and are released
    under, the full GNU GPL v3 license, which forbids redistributing it as part of
    proprietary software. The next section presents a more viable alternative for
    Scala and Spark.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法是开始使用Scala和Spark进行NLP的好方式；该库提供的API简单且高层次，能够帮助人们快速理解NLP的核心概念，同时利用Spark DataFrames的强大功能。但它也有缺点。当需要实现更复杂和定制化的NLP解决方案时，现有的API过于简单，难以应对。此外，如果你的最终系统不仅仅是内部使用，而是计划销售并分发给客户，则可能会出现许可问题；斯坦福核心NLP库和`spark-corenlp`模型依赖于并且在完整的GNU
    GPL v3许可下发布，禁止将其作为专有软件的一部分重新分发。下一节介绍了一个更可行的Scala和Spark替代方案。
- en: Hands-on NLP with Spark NLP
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark NLP进行实践操作
- en: Another alternative library to integrate with Apache Spark in order to do NLP
    is `spark-nlp` ([https://nlp.johnsnowlabs.com/](https://nlp.johnsnowlabs.com/))
    by the John Snow labs ([https://www.johnsnowlabs.com/](https://www.johnsnowlabs.com/)).
    It is open source and released under Apache License 2.0, so is different from
    `spark-corenlp` because its licensing model makes it possible to redistribute
    it as part of a commercial solution. It has been implemented in Scala on top of
    the Apache Spark ML module and it is available in Maven central. It provides NLP
    annotations for machine learning pipelines that at the same time are easy to understand
    and use, have great performance, and can scale easily in a distributed environment.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可与Apache Spark集成以进行NLP的替代库是John Snow Labs的`spark-nlp`（[https://nlp.johnsnowlabs.com/](https://nlp.johnsnowlabs.com/)）（[https://www.johnsnowlabs.com/](https://www.johnsnowlabs.com/)）。它是开源的，并且在Apache许可证2.0下发布，因此与`spark-corenlp`不同，它的许可模式使得可以将其作为商业解决方案的一部分重新分发。它是在Scala上实现的，基于Apache
    Spark ML模块，并且可以在Maven中央仓库中找到。它为机器学习流水线提供了NLP注释，这些注释既易于理解和使用，又具有出色的性能，并且能够在分布式环境中轻松扩展。
- en: The release I am referring to in this section is 1.6.3 (the latest at the time
    this book was written).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我在本节中提到的版本是1.6.3（本书写作时的最新版本）。
- en: 'The core concept of `spark-nlp` is the Spark ML pipeline ([https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/ml/Pipeline.html](https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/ml/Pipeline.html)). A
    pipeline consists of a sequence of stages. Each stage could be a transformer ([https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/ml/Transformer.html](https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/ml/Transformer.html))
    or an estimator ([https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/ml/Estimator.html](https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/ml/Estimator.html)).
    Transformers transform an input dataset into another, while estimators fit a model
    to data. When the fit method of a pipeline is invoked, its stages are then executed
    in order. Three types of pre-trained pipelines are available: the basic, advanced,
    and sentiment. The library provides also several pre-trained models for NLP and
    several annotators. But, let''s start with a simple first example in order to
    clarify the details of the main concepts of `spark-nlp`. Let''s try to implement
    a basic pipeline for ML-based named entity tag extraction.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark-nlp`的核心概念是Spark ML流水线（[https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/ml/Pipeline.html](https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/ml/Pipeline.html)）。一个流水线由一系列阶段组成。每个阶段可以是一个变换器（[https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/ml/Transformer.html](https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/ml/Transformer.html)）或一个估算器（[https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/ml/Estimator.html](https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/ml/Estimator.html)）。变换器将输入数据集转换为另一个数据集，而估算器则将模型拟合到数据上。当流水线的拟合方法被调用时，其各个阶段会按顺序执行。现有三种类型的预训练流水线：基础型、进阶型和情感型。该库还提供了多个预训练的NLP模型和多个标注器。但为了澄清`
    spark-nlp`的核心概念，让我们从一个简单的示例开始。我们尝试实现一个基于ML的命名实体标签提取的基本流水线。'
- en: 'The following example depends on the Spark SQL and MLLib components and the
    `spark-nlp` library:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例依赖于Spark SQL和MLLib组件以及`spark-nlp`库：
- en: '[PRE7]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We need to start a `SparkSession` before anything else, as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要首先启动一个`SparkSession`，如下所示：
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Before creating the pipeline, we need to implement its single stages. The first
    one is `com.johnsnowlabs.nlp.DocumentAssembler`, to specify the column of the
    application input to parse and the output column name (which will be the input
    column for the next stage), as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建管道之前，我们需要实现其各个阶段。第一个阶段是`com.johnsnowlabs.nlp.DocumentAssembler`，用于指定应用程序输入的列以进行解析，并指定输出列名称（该列将作为下一个阶段的输入列），如下所示：
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The next stage is a `Tokenizer` (`com.johnsnowlabs.nlp.annotators.Tokenizer`),
    as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个阶段是`Tokenizer`（`com.johnsnowlabs.nlp.annotators.Tokenizer`），如下所示：
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After this stage, any input sentences should have been split into single words.
    We need to clean up those tokens, so the next stage is a `normalizer` (`com.johnsnowlabs.nlp.annotators.Normalizer`),
    as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段之后，任何输入的句子应该已经被拆分成单个词语。我们需要清理这些词语，因此下一个阶段是`normalizer`（`com.johnsnowlabs.nlp.annotators.Normalizer`），如下所示：
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can now use one of the pre-trained models of the `spark-nlp` library to
    generate the named entity tags, as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用`spaek-nlp`库中的一个预训练模型来生成命名实体标签，如下所示：
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here we are using the `NerDLModel` class (`com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel`),
    which behind the scenes uses a TensorFlow pre-trained model. The named entity
    tags generated by that model are in IOB format ([https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))),
    so we need to make them in a more human readable format. We can achieve this using
    the `NerConverter` class (`com.johnsnowlabs.nlp.annotators.ner.NerConverter`),
    as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们使用了`NerDLModel`类（`com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel`），它背后使用的是一个TensorFlow预训练模型。该模型生成的命名实体标签采用IOB格式（[https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)))，因此我们需要将它们转换为更易读的格式。我们可以使用`NerConverter`类（`com.johnsnowlabs.nlp.annotators.ner.NerConverter`）来实现这一点，如下所示：
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The last stage is to finalize the output of the pipeline, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个阶段是最终化管道的输出，如下所示：
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: For this, we use the `Finisher` transformer (`com.johnsnowlabs.nlp.Finisher`).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们使用了`Finisher`转换器（`com.johnsnowlabs.nlp.Finisher`）。
- en: 'We can now create the pipeline using the stages created so far, as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用目前创建的阶段来构建管道，如下所示：
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You have already probably noticed that each stage's output column is the input
    for the next stage's input column. That's because the stages for a pipeline are
    executed in the exact order they are listed in the input `Array` for the `setStages`
    method.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，每个阶段的输出列是下一个阶段输入列的输入。这是因为管道的各个阶段会按它们在`setStages`方法的输入`Array`中列出的顺序依次执行。
- en: 'Let''s now feed the application with some sentences, as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们给应用程序输入一些句子，如下所示：
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The same as for the `spark-corenlp` example in the previous section, we have
    created a `Sequence` for the input text content and then transformed it into a
    Spark DataFrame.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一节中`spaek-corenlp`的示例相同，我们为输入文本内容创建了一个`Sequence`，然后将其转换为Spark DataFrame。
- en: 'By invoking the `fit` method of the `pipeline`, we can execute all of its stages,
    as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用`pipeline`的`fit`方法，我们可以执行所有阶段，如下所示：
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And, we get the resulting DataFrame output, as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到如下的结果DataFrame输出：
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This will give the following output:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/b167214a-e7d3-4fe5-9c23-5ea43298325b.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b167214a-e7d3-4fe5-9c23-5ea43298325b.png)'
- en: 'When we take a closer look it is seen as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们仔细观察时，情况如下所示：
- en: '![](img/2a93289e-b733-44a7-af1d-657fb05d34e1.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a93289e-b733-44a7-af1d-657fb05d34e1.png)'
- en: An `ORGANIZATION` named entity tag has been generated for the word `Packt` and
    a `PERSON` named entity tag has been generated for the word `Guglielmo`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 已为单词`Packt`生成了一个`ORGANIZATION`命名实体标签，为单词`Guglielmo`生成了一个`PERSON`命名实体标签。
- en: '`spark-nlp` also provides a class, `com.johnsnowlabs.util.Benchmark`, to perform
    the benchmarking of pipeline execution, for example:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark-nlp`还提供了一个类，`com.johnsnowlabs.util.Benchmark`，用于执行管道执行的基准测试，例如：'
- en: '[PRE19]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, we stop the `SparkSession` at the end of the pipeline execution, as
    follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在管道执行结束时停止`SparkSession`，如下所示：
- en: '[PRE20]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Let's now do something more complex. The pipeline in this second example does
    tokenization with n-grams ([https://en.wikipedia.org/wiki/N-gram](https://en.wikipedia.org/wiki/N-gram)),
    sequences of *n* tokens (typically words) from a given text or speech. The dependencies
    for this example are the same as for the previous one presented in this section—Spark
    SQL, Spark MLLib, and `spark-nlp`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们做一些更复杂的事情。这个第二个示例中的管道使用n-grams进行分词（[https://en.wikipedia.org/wiki/N-gram](https://en.wikipedia.org/wiki/N-gram)），它是从给定的文本或语音中提取的*n*个标记（通常是单词）的序列。此示例的依赖项与本节前面展示的示例相同——Spark
    SQL、Spark MLLib和`spark-nlp`。
- en: 'Create a `SparkSession` and configure some Spark properties, as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`SparkSession`并配置一些Spark属性，如下所示：
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The first three stages for the pipeline are the same as for the previous example,
    as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 管道的前三个阶段与之前的示例相同，如下所示：
- en: '[PRE22]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Put a `finisher` stage before using the n-gram stage, as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用n-gram阶段之前添加一个`finisher`阶段，如下所示：
- en: '[PRE23]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The n-gram stage uses the `NGram` class ([https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.ml.feature.NGram](https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.ml.feature.NGram))
    of Spark MLLib, as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram阶段使用了Spark MLLib中的`NGram`类（[https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.ml.feature.NGram](https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.ml.feature.NGram)），如下所示：
- en: '[PRE24]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`NGram` is a feature transformer that converts an input array of strings into
    an array of n-grams. The chosen value for *n* in this example is `3`. We need
    now a further `DocumentAssembler` stage for the n-gram results, as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`NGram`是一个特征变换器，它将输入的字符串数组转换为n-grams数组。在这个示例中，选择的*n*值是`3`。现在，我们需要一个额外的`DocumentAssembler`阶段来处理n-gram的结果，如下所示：'
- en: '[PRE25]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let''s implement the pipeline, as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现管道，如下所示：
- en: '[PRE26]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now feed the application with the same input sentences as for the previous
    example:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，用与之前示例相同的输入句子来运行应用程序：
- en: '[PRE27]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'And execute the stages of the pipeline, as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 然后执行管道的各个阶段，如下所示：
- en: '[PRE28]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Print the results to the screen:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 将结果打印到屏幕上：
- en: '[PRE29]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This produces the following output:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成以下输出：
- en: '![](img/0ab368f7-ea37-4f79-9492-4665a7b935c9.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ab368f7-ea37-4f79-9492-4665a7b935c9.png)'
- en: 'Finally, we stop the `SparkSession`, as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们停止`SparkSession`，如下所示：
- en: '[PRE30]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The final example is an ML sentiment analysis using the Vivek Narayanan ([https://github.com/vivekn](https://github.com/vivekn))
    model. Sentiment analysis, which is a practical application of NLP, is the process
    of identifying and categorizing, through a computer, the opinion expressed in
    a text, in order to determine whether its writer's/speaker's attitude towards
    a product or topic is positive, negative, or neutral. In particular, for this
    example, we are going to train and validate the model on movie reviews. The dependencies
    for this example are the usual ones—Spark SQL, Spark MLLib, and `spark-nlp`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的示例是使用Vivek Narayanan（[https://github.com/vivekn](https://github.com/vivekn)）模型进行的机器学习情感分析。情感分析是自然语言处理的一个实际应用，它是通过计算机识别和分类文本中表达的意见，以确定其作者/讲述者对某一产品或话题的态度是积极的、消极的，还是中立的。特别地，在这个示例中，我们将训练并验证电影评论的模型。此示例的依赖项与往常一样——Spark
    SQL、Spark MLLib和`spark-nlp`。
- en: 'As usual, create a `SparkSession` (while also configuring some Spark properties),
    as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，创建一个`SparkSession`（同时配置一些Spark属性），如下所示：
- en: '[PRE31]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We then need two datasets, one for training and one for testing. For simplicity,
    we define the training dataset as a `Sequence` and then transform it into a DataFrame,
    where the columns are the review text and the associated sentiment, as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要两个数据集，一个用于训练，一个用于测试。为了简便起见，我们将训练数据集定义为一个`Sequence`，然后将其转换为DataFrame，其中列为评论文本和相关情感，如下所示：
- en: '[PRE32]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can now define the stages for the pipeline. The first three stages are exactly
    the same as for the previous example pipelines (`DocumentAssembler`, `Tokenizer`, and
    `Normalizer`), as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以定义管道的各个阶段。前面三个阶段与之前示例管道中的完全相同（`DocumentAssembler`、`Tokenizer`和`Normalizer`），如下所示：
- en: '[PRE33]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We can now use the `com.johnsnowlabs.nlp.annotators.sda.vivekn.ViveknSentimentApproach`
    annotator, as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用`com.johnsnowlabs.nlp.annotators.sda.vivekn.ViveknSentimentApproach`注解器，如下所示：
- en: '[PRE34]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Create the pipeline using the stages previously defined:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前定义的各个阶段创建管道：
- en: '[PRE35]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And then start the training, as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然后开始训练，如下所示：
- en: '[PRE36]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Once the training has completed, we can use the following testing dataset to
    test it:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们可以使用以下测试数据集进行测试：
- en: '[PRE37]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output will be as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/7332a1d8-0bb5-490a-b091-870b2c270648.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7332a1d8-0bb5-490a-b091-870b2c270648.png)'
- en: Two sentences that are part of the testing dataset have been properly marked
    as negative.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据集中的两个句子已经被正确标记为负面。
- en: 'It is of course possible to do a benchmark for sentiment analysis, too, through
    the `spark-nlp` `Benchmark` class, as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，也可以通过`spark-nlp`的`Benchmark`类来进行情感分析的基准测试，如下所示：
- en: '[PRE38]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: At the end of this section, we can state that `spark-nlp` provides more features
    than `spark-corenlp`*,* is well integrated with Spark MLLib, and, thanks to its
    licensing model, doesn't present the same issues concerning the distribution of
    the application/system in which it is used and integrated. It is a stable library
    and ready for production in Spark environments. Unfortunately, most of its documentation
    is missing and the existing documentation is minimal and not well maintained,
    despite project development being active.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节结束时，我们可以说明`spak-nlp`提供了比`spark-corenlp`更多的功能，且与Spark MLLib集成良好，并且得益于其许可模型，在应用程序/系统的分发上不会出现相同的问题。它是一个稳定的库，适用于Spark环境中的生产环境。不幸的是，它的大部分文档缺失，现有的文档非常简略且维护不善，尽管项目仍在积极开发中。
- en: In order to understand how a single feature works and how to combine them together,
    you have to go through the source code in GitHub. The library also uses existing
    ML models implemented through Python frameworks and provides a Scala class to
    represent them, hiding the underlying model implementation details from developers.
    This will work in several use case scenarios, but in order to build more robust
    and efficient models, you would probably have to move to implementing your own
    neural network model. Only DL4J will give you that level of freedom in development
    and training with Scala.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解某个功能如何工作以及如何将它们结合在一起，你必须浏览GitHub中的源代码。该库还使用通过Python框架实现的现有ML模型，并提供了一个Scala类来表示它们，将底层的模型实现细节隐藏起来，避免开发人员接触。这在多个使用场景中都能有效，但为了构建更强大和高效的模型，你可能需要自己实现神经网络模型。只有DL4J才能在Scala中为开发和训练提供那种自由度。
- en: Summary
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we became familiar with the main concepts of NLP and started
    to get hands-on with Spark, exploring two potentially useful libraries, `spark-corenlp` and
    `spark-nlp`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们了解了NLP的主要概念，并开始动手使用Spark，探索了两个潜在有用的库，`spark-corenlp`和`spark-nlp`。
- en: In the next chapter, we will see how it is possible to achieve the same or better
    results by implementing complex NLP scenarios in Spark though DL (mostly RNN-based).
    We will explore different implementations by using DL4J, TensorFlow, Keras, the
    TensorFlow backend, and DL4J + Keras model imports.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将看到如何通过实现复杂的NLP场景在Spark中实现相同或更好的结果，主要是基于RNN的深度学习。我们将通过使用DL4J、TensorFlow、Keras、TensorFlow后端以及DL4J
    + Keras模型导入来探索不同的实现方式。
