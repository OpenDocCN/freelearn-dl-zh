- en: Image Classification Using Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积神经网络进行图像分类
- en: It is not an exaggeration to say that the huge growth of interest in deep learning
    can be mostly attributed to convolutional neural networks. **Convolutional neural
    networks** (**CNNs**) are the main building blocks of image classification models
    in deep learning, and have replaced most techniques that were previously used
    by specialists in the field. Deep learning models are now the de facto method
    to perform all large-scale image tasks, including image classification, object
    detection, detecting artificially generated images, and even attributing text
    descriptions to images. In this chapter, we will look at some of these techniques.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不夸张地说，深度学习领域对卷积神经网络的巨大兴趣增长，可以说主要归功于卷积神经网络。**卷积神经网络**（**CNN**）是深度学习中图像分类模型的主要构建块，并且已经取代了以前该领域专家使用的大多数技术。深度学习模型现在是执行所有大规模图像任务的事实标准方法，包括图像分类、目标检测、检测人工生成的图像，甚至为图像添加文本描述。本章中，我们将探讨其中一些技术。
- en: Why are CNNs so important? To explain why, we can look at the history of the
    ImageNet competition. The **ImageNet** competition is an open large-scale image
    classification challenge that has one thousand categories. It can be considered
    as the unofficial world championship for image classification. Teams, mostly fielded
    by academics and researchers, compete from around the world. In 2011, an error
    rate of around 25% was the benchmark. In 2012, a team led by Alex Krizhevsky and
    advised by Geoffrey Hinton achieved a huge leap by winning the competition with
    an error rate of 16%. Their solution consisted of 60 million parameters and 650,000
    neurons, five convolutional layers, some of which are followed by max-pooling
    layers, and three fully-connected layers with a final 1,000-way softmax layer
    to do the final classification.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么CNN如此重要？为了说明这一点，我们可以回顾一下ImageNet竞赛的历史。**ImageNet**竞赛是一个开放的大规模图像分类挑战，共有一千个类别。它可以视为图像分类的非正式世界锦标赛。参赛队伍大多由学者和研究人员组成，来自世界各地。2011年，约25%的错误率是基准。2012年，由Alex
    Krizhevsky领导、Geoffrey Hinton指导的团队通过取得16%的错误率，赢得了比赛，取得了巨大进步。他们的解决方案包括6000万个参数和65万个神经元，五个卷积层，部分卷积层后接最大池化层，以及三个全连接层，最终通过一个1000分类的Softmax层进行最终分类。
- en: Other researchers built on their techniques in subsequent years, with the result
    that the original ImageNet competition is essentially considered *solved.* In
    2017, almost all teams achieved an error rate of less than 5%. Most people consider
    that the 2012 ImageNet victory heralded the dawn of the new deep learning revolution.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 其他研究人员在随后的几年里在他们的技术基础上做了改进，最终使得原始的ImageNet竞赛被基本视为*解决*。到2017年，几乎所有的队伍都达到了低于5%的错误率。大多数人认为，2012年ImageNet的胜利标志着新一轮深度学习革命的开始。
- en: In this chapter, we will look at image classification using CNN. We are going
    to start with the `MNIST` dataset, which is considered as the *Hello World* of
    deep learning tasks. The `MNIST` dataset consists of grayscale images of size
    28 x 28 of 10 classes, the numbers 0-9\. This is a much easier task than the ImageNet
    competition; there are 10 categories rather than 1,000, the images are in grayscale
    rather than color and most importantly, there are no backgrounds in the MNIST
    images that can potentially confuse the model. Nevertheless, the MNIST task is
    an important one in its own right; for example, most countries use postal codes
    containing digits. Every country uses automatic address routing solutions that
    are more complex variations of this task.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨使用CNN进行图像分类。我们将从`MNIST`数据集开始，`MNIST`被认为是深度学习任务的*Hello World*。`MNIST`数据集包含10个类别的灰度图像，尺寸为28
    x 28，类别为数字0-9。这比ImageNet竞赛要容易得多；它有10个类别而不是1000个，图像是灰度的而不是彩色的，最重要的是，MNIST图像中没有可能混淆模型的背景。然而，MNIST任务本身也很重要；例如，大多数国家使用包含数字的邮政编码，每个国家都有更复杂变体的自动地址路由解决方案。
- en: We will use the MXNet library from Amazon for this task. The MXNet library is
    an excellent library introduction to deep learning as it allows us to code at
    a higher level than other libraries such as TensorFlow, which we cover later on
    in this book.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本任务中我们将使用Amazon的MXNet库。MXNet库是深度学习的一个优秀入门库，它允许我们以比其他库（如后面会介绍的TensorFlow）更高的抽象级别进行编码。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: What are CNNs?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是CNN？
- en: Convolutional layers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层
- en: Pooling layers
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层
- en: Softmax
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Softmax
- en: Deep learning architectures
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习架构
- en: Using MXNet for image classification
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MXNet进行图像分类
- en: CNNs
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）
- en: CNNs are the cornerstone of image classification in deep learning. This section
    gives an introduction to them, explains the history of CNNs, and will explain
    why they are so powerful.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）是深度学习中图像分类的基石。本节将介绍它们，讲解CNN的历史，并解释它们为何如此强大。
- en: 'Before we begin, we will look at a simple deep learning architecture. Deep
    learning models are difficult to train, so using an existing architecture is often
    the best place to start. An architecture is an existing deep learning model that
    was state-of-the-art when initially released. Some examples are AlexNet, VGGNet,
    GoogleNet, and so on. The architecture we will look at is the original LeNet architecture
    for digit classification from Yann LeCun and others from the mid 1990s. This architecture
    was used for the `MNIST` dataset. This dataset is comprised of grayscale images
    of 28 x 28 size that contain the digits 0 to 9\. The following diagram shows the
    LeNet architecture:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我们将先看看一个简单的深度学习架构。深度学习模型难以训练，因此使用现有的架构通常是最好的起点。架构是一个现有的深度学习模型，在最初发布时是最先进的。一些例子包括AlexNet、VGGNet、GoogleNet等。我们将要介绍的是Yann
    LeCun及其团队于1990年代中期提出的用于数字分类的原始LeNet架构。这个架构被用来处理`MNIST`数据集。该数据集由28 x 28尺寸的灰度图像组成，包含数字0到9。以下图示展示了LeNet架构：
- en: '![](img/f59bb2d3-51a3-4092-8010-a129b9248d7c.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f59bb2d3-51a3-4092-8010-a129b9248d7c.png)'
- en: 'Figure 5.1: The LeNet architecture'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1：LeNet架构
- en: The original images are 28 x 28 in size. We have a series of hidden layers which
    are convolution and pooling layers (here, they are labeled *subsampling*). Each
    convolutional layer changes structure; for example, when we apply the convolutions
    in the first hidden layer, our output size is three dimensional. Our final layer
    is of size 10 x 1, which is the same size as the number of categories. We can
    apply a `softmax` function here to convert the values in this layer to probabilities
    for each category. The category with the highest probability would be the category
    prediction for each image.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像大小为28 x 28。我们有一系列的隐藏层，包括卷积层和池化层（在这里，它们被标记为*子采样*）。每个卷积层改变结构；例如，当我们在第一隐藏层应用卷积时，输出的大小是三维的。我们的最终层大小是10
    x 1，这与类别的数量相同。我们可以在这里应用一个`softmax`函数，将这一层的值转换为每个类别的概率。具有最高概率的类别将是每个图像的类别预测。
- en: Convolutional layers
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层
- en: This section shows how convolutional layers work in greater depth. At a basic
    level, convolutional layers are nothing more than a set of filters. When you look
    at images while wearing glasses with a red tint, everything appears to have a
    red hue. Now, imagine if these glasses consisted of different tints embedded within
    them, maybe a red tint with one or more horizontal green tints. If you had such
    a pair of glasses, the effect would be to highlight certain aspects of the scene
    in front of you. Any part of the scene that had a green horizontal line would
    become more focused.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将更深入地展示卷积层的工作原理。从基本层面来看，卷积层不过是一组滤波器。当你戴着带有红色镜片的眼镜看图像时，所有的事物似乎都带有红色的色调。现在，想象这些眼镜由不同的色调组成，也许是带有红色色调的镜片，里面还嵌入了一些水平绿色色调。如果你拥有这样一副眼镜，效果将是突出前方场景的某些方面。任何有绿色水平线的地方都会更加突出。
- en: 'Convolutional layers apply a selection of patches (or convolutions) over the
    previous layer’s output. For example, for a face recognition task, the first layer’s
    patches identify basic features in the image, for example, an edge or a diagonal
    line. The patches are moved across the image to match different parts of the image.
    Here is an example of a 3 x 3 convolutional block applied across a 6 x 6 image:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层会在前一层的输出上应用一系列补丁（或卷积）。例如，在人脸识别任务中，第一层的补丁会识别图像中的基本特征，例如边缘或对角线。这些补丁会在图像上移动，匹配图像的不同部分。下面是一个3
    x 3卷积块在6 x 6图像上应用的示例：
- en: '![](img/9e8858f5-faf5-4c81-80d8-23b1caa232f8.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e8858f5-faf5-4c81-80d8-23b1caa232f8.png)'
- en: 'Figure 5.2: An example of a single convolution applied across an image'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：应用于图像的单一卷积示例
- en: 'The values in the convolutional block are multiplied element by element (that
    is, not matrix multiplication), and the values are added to give a single value.
    Here is an example:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积块中的值是逐元素相乘（即非矩阵乘法），然后将这些值加起来得到一个单一值。下面是一个示例：
- en: '![](img/3701b84b-891e-4c43-bba3-2dc35c04c6f1.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3701b84b-891e-4c43-bba3-2dc35c04c6f1.png)'
- en: 'Figure 5.3: An example of a convolution block applied to two parts of an input
    layer'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3：应用于输入层两个部分的卷积块示例
- en: In this example, our convolutional block is a diagonal pattern. The first block
    in the image (*A1:C3*) is also a diagonal pattern, so when we multiply the elements
    and sum them, we get a relatively large value of **6.3**. In comparison, the second
    block in the image (*D4:F6*) is a horizontal line pattern, so we get a much smaller
    value.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们的卷积块呈对角线模式。图像中的第一个块（*A1:C3*）也是对角线模式，因此当我们进行元素乘法并求和时，得到一个相对较大的值**6.3**。相比之下，图像中的第二个块（*D4:F6*）是水平线模式，因此我们得到一个较小的值。
- en: 'It can be difficult to visualize how convolutional layers work across the entire
    image, so the following R Shiny application will show it more clearly. This application
    is included in the code for this book in the `Chapter5/server.R` file. Open this
    file in **RStudio** and select **Run app**. Once the application is loaded, select
    **Convolutional Layers** from the left menu bar. The application loads the first
    100 images from the `MNIST` dataset, which we will use later for our first deep
    learning image classification task. The images are grayscale images of size 28
    x 28 of handwritten digits 0 to 9\. Here is a screenshot of the application with
    the fourth image selected, which is a four:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化卷积层如何作用于整个图像可能很困难，因此下面的R Shiny应用将更加清晰地展示这一过程。该应用包含在本书的`Chapter5/server.R`文件中。请在**RStudio**中打开该文件并选择**Run
    app**。应用加载后，选择左侧菜单栏中的**Convolutional Layers**。应用将加载`MNIST`数据集中的前100张图像，这些图像稍后将用于我们的第一次深度学习图像分类任务。图像为28
    x 28大小的灰度手写数字0到9的图像。下面是应用程序的截图，选择的是第四张图像，显示的是数字四：
- en: '![](img/4719d54a-cdbd-4ef9-a302-fc9a7b1c9bf2.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4719d54a-cdbd-4ef9-a302-fc9a7b1c9bf2.png)'
- en: 'Figure 5.4: The Shiny application showing a horizontal convolutional filter'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4：显示水平卷积滤波器的Shiny应用
- en: 'Once loaded, you can use the slider to browse through the images. In the top-right
    corner, there are four choices of convolutional layers to apply to the image.
    In the previous screenshot, a horizontal line convolutional layer is selected
    and we can see what this looks like in the text box in the top right corner. When
    we apply the convolutional filter to the input image on the left, we can see that
    the resulting image on the right is almost entirely grey, except for where the
    horizontal line was in the original image. Our convolutional filter has matched
    the parts in the image that have a horizontal line. If we change the convolutional
    filter to a vertical line, we get the following result:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 加载后，你可以使用滑块浏览图像。在右上角，有四个可选择的卷积层可应用于图像。在前面的截图中，选择了一个水平线卷积层，我们可以在右上角的文本框中看到它的样子。当我们将卷积滤波器应用到左侧的输入图像时，我们可以看到右侧的结果图像几乎完全是灰色的，只有在原始图像中的水平线位置才会突出显示。我们的卷积滤波器已经匹配了图像中包含水平线的部分。如果我们将卷积滤波器更改为垂直线，结果将如下所示：
- en: '![](img/684ef9f1-9e06-476a-91ce-a339e28b065b.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/684ef9f1-9e06-476a-91ce-a339e28b065b.png)'
- en: 'Figure 5.5: The Shiny application showing a vertical convolutional filter'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5：显示垂直卷积滤波器的Shiny应用
- en: Now, we can see that, after the convolution is applied, the vertical lines in
    the original image are highlighted in the resultant image on the right. In effect,
    applying these filters is a type of feature extraction. I encourage you to use
    the application and browse through images and see how different convolutions apply
    to images of the different categories.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到，在应用卷积后，原始图像中的垂直线在右侧的结果图像中被突出显示。实际上，应用这些滤波器是一种特征提取的方式。我鼓励你使用该应用浏览图像，看看不同的卷积是如何应用于不同类别的图像的。
- en: 'This is the basis of convolutional filters, and while it is a simple concept,
    it becomes powerful when you start doing two things:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是卷积滤波器的基础，虽然它是一个简单的概念，但当你开始做两件事时，它会变得非常强大：
- en: Combining many convolutional filters to create convolutional layers
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将许多卷积滤波器组合成卷积层
- en: Applying another set of convolutional filters (that is, a convolutional layer)
    to the output of a previous convolutional layer
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将另一组卷积滤波器（即卷积层）应用于先前卷积层的输出
- en: This may take some time to get your head around. If I apply a filter to an image
    and then apply a filter to that output, what do I get? And if I then apply that
    a third time, that is, apply a filter to an image and then apply a filter to that
    output, and then apply a filter to that output, what do I get? The answer is that
    each subsequent layer combines identified features from the previous layers to
    find even more complicated patterns, for example, corners, arcs, and so on. Later
    layers find even richer features such as a circle with an arc over it, indicating
    the eye of a person.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要一些时间才能理解。如果我对一张图像应用了一个滤波器，然后对该输出再次应用一个滤波器，我得到的结果是什么？如果我再应用第三次，也就是先对一张图像应用滤波器，再对该输出应用滤波器，然后对该输出再应用滤波器，我得到的是什么？答案是，每一层后续的卷积层都会结合前一层识别到的特征，找到更为复杂的模式，例如角落、弧线等等。后续的层会发现更丰富的特征，比如一个带有弧形的圆圈，表示人的眼睛。
- en: 'There are two parameters that are used to control the movement of the convolution:
    padding and strides. In the following diagram, we can see that the original image
    is of size 6 x 6, while there are 4 x 4 subgraphs. We have therefore reduced the
    data representation from a 6 x 6 matrix to a 4 x 4 matrix. When we apply a convolution
    of size *c1*, *c2* to data of size n, m, the output will be *n-c1+1*, *m-c2+1*.
    If we want our output to be the same size as our input, we can pad the input by
    adding zeros to borders of the images. For the previous example, we add a 1-pixel
    border around the entire image. The following diagram shows how the first 3 x
    3 convolution would be applied to the image with padding:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个参数用于控制卷积的移动：填充（padding）和步幅（strides）。在下图中，我们可以看到原始图像的大小是6 x 6，而有4 x 4的子图。我们因此将数据表示从6
    x 6矩阵减少到了4 x 4矩阵。当我们将大小为*c1*，*c2*的卷积应用于大小为n，m的数据时，输出将是*n-c1+1*，*m-c2+1*。如果我们希望输出与输入大小相同，可以通过在图像边缘添加零来填充输入。对于之前的例子，我们在整个图像周围添加一个1像素的边框。下图展示了如何将第一个3
    x 3卷积应用于具有填充的图像：
- en: '![](img/328c6a41-1e2f-4474-bd01-479977cc7e23.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/328c6a41-1e2f-4474-bd01-479977cc7e23.png)'
- en: Figure 5.6 Padding applied before a convolution
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 在卷积前应用的填充
- en: The second parameter we can apply to convolutions is strides, which control
    the movement of the convolution. The default is 1, which means the convolution
    moves by 1 each time, first to the right and then down. In practice, this value
    is rarely changed, so we will not consider it further.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以应用于卷积的第二个参数是步幅（strides），它控制卷积的移动。默认值为1，这意味着卷积每次移动1个单位，首先向右移动，然后向下移动。在实际应用中，这个值很少被改变，因此我们将不再进一步讨论它。
- en: We now know that convolutions act like small feature generators, that they are
    applied across an input layer (which is image data for the first layer), and that
    subsequent convolution layers find even more complicated features. But how are
    they calculated? Do we need to carefully craft a set of convolutions manually
    to apply them to our model? The answer is no; these convolutions are automatically
    calculated for us through the magic of the gradient descent algorithm. The best
    patterns are found after many iterations through the training dataset.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道，卷积像小型特征生成器一样工作，它们应用于输入层（对于第一层来说是图像数据），而后续的卷积层则发现更复杂的特征。那么它们是如何计算的呢？我们是否需要精心手动设计一组卷积来应用到我们的模型中？答案是否定的；这些卷积是通过梯度下降算法的魔力自动计算出来的。最佳的特征模式是在经过多次训练数据集迭代后找到的。
- en: So, how do convolutions work once we get beyond 2-3 levels of layers? The answer
    is that it is difficult for anyone to understand the exact mathematics of how
    convolutional layers work. Even the original designers of these architects may
    not fully understand what is happening in the hidden layers in a series of CNNs.
    If this worries you, then recall that the solution that won the ImageNet competition
    in 2012 had 60 million parameters. With the advance in computing power, deep learning
    architectures may have hundreds of millions of parameters. It is simply not possible
    for any person to fully understand what is happening in such a complicated model.
    This is why they are often called **black-box** models.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，一旦我们超越了2-3层卷积层，卷积是如何工作的呢？答案是，任何人都很难理解卷积层具体的数学原理。即使是这些网络设计的原始设计者，也可能并不完全理解一系列卷积神经网络中的隐藏层在做什么。如果这让你感到担忧，请记住，2012年赢得ImageNet竞赛的解决方案有6000万个参数。随着计算能力的进步，深度学习架构可能会有数亿个参数。对于任何人来说，完全理解如此复杂的模型中的每一个细节是不可能的。这就是为什么这些模型常常被称为**黑箱**模型的原因。
- en: It might surprise you at first. How can deep learning achieve human-level performance
    in image classification and how can we build deep learning models if we do not
    fully understand how they work? This question has divided the deep learning community,
    largely along the demarcation between industry and academia. Many (but not all)
    researchers believe that we should get a more fundamental understanding of how
    deep learning models work. Some researchers also believe that we can only develop
    the next generation of artificial intelligence applications by getting a better
    understanding of how current architectures work. At a recent NIPS conference (one
    of the oldest and most notable conferences for deep learning), deep learning was
    unfavorably compared to alchemy. Meanwhile, practitioners in the industry are
    not concerned with how deep learning works. They are more focused on building
    ever more complex deep learning architectures to maximize accuracy or performance.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能一开始让你感到惊讶。深度学习如何在图像分类中实现人类水平的表现？如果我们不能完全理解它们是如何工作的，我们又如何构建深度学习模型呢？这个问题已经将深度学习社区分裂，主要是行业与学术界之间的分歧。许多（但不是所有）研究人员认为，我们应该更深入地理解深度学习模型的工作原理。一些研究人员还认为，只有通过更好地理解当前架构的工作原理，我们才能开发出下一代人工智能应用。在最近的NIPS会议上（这是深度学习领域最古老和最重要的会议之一），深度学习被不利地与炼金术相提并论。与此同时，业界的从业者并不关心深度学习是如何工作的。他们更关注构建更加复杂的深度学习架构，以最大化准确性或性能。
- en: Of course, this is a crude representation of the state of the industry; not
    all academics are inward looking and not all practitioners are just tweaking models
    to get small improvements. Deep learning is still relatively new (although the
    foundation blocks of neural networks have been known about for decades). But this
    tension does exist and has been around for awhile – for example, a popular deep
    learning architecture introduced *Inception* modules, which were named after the
    *Inception* movie. In the film, Leonardo DiCaprio leads a team that alter people’s
    thoughts and opinions by embedding themselves within people’s dreams. Initially,
    they go one layer deep, but then go deeper, in effect going to dreams within dreams.
    As they go deeper, the worlds get more complicated and the outcomes less certain.
    We will not go into detail here about what *Inception modules* are, but they combine
    convolutional and max pooling layers in parallel. The authors of the paper acknowledged
    the memory and computational cost of the model within the paper, but by naming
    the key component as an *Inception module,* they were subtly suggesting which
    side of the argument they were on.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这只是业界现状的粗略描述；并不是所有学者都是向内看的，也不是所有从业者都仅仅是在调整模型以获得小幅改进。深度学习仍然相对较新（尽管神经网络的基础块已经存在了数十年）。但这种紧张关系确实存在，并且已经持续了一段时间——例如，一种流行的深度学习架构引入了*Inception*模块，命名灵感来源于电影《盗梦空间》中的*Inception*。在这部电影中，莱昂纳多·迪卡普里奥带领一个团队，通过进入人们的梦境来改变他们的思想和观点。最初，他们只进入一个层次的梦境，但随后深入下去，实际上进入了梦中的梦境。随着他们深入，梦境变得越来越复杂，结果也变得不确定。我们在这里不会详细讨论*Inception模块*的具体内容，但它们将卷积层和最大池化层并行结合在一起。论文的作者在论文中承认了该模型的内存和计算成本，但通过将关键组件命名为*Inception模块*，他们巧妙地暗示了自己站在哪一方。
- en: 'After the breakthrough performance of the winner of the 2012 ImageNet competition,
    two researchers were unsatisfied that there was no insight into how the model
    worked. They decided to reverse-engineer the algorithm, attempting to show the
    input pattern that caused a given activation in the feature maps. This was a non-trivial
    task, as some layers used in the original model (for example, pooling layers)
    discarded information. Their paper showed the top 9 activations for each layer.
    Here is the feature visualization for the first layer:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在2012年ImageNet竞赛获胜者的突破性表现之后，两个研究人员对模型的工作原理缺乏洞见感到不满。他们决定逆向工程该算法，尝试展示导致特征图中某一激活的输入模式。这是一项非平凡的任务，因为原始模型中使用的某些层（例如池化层）会丢弃信息。他们的论文展示了每一层的前9个激活。以下是第一层的特征可视化：
- en: '![](img/798437be-2e43-4581-99e1-db373bd99ef0.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/798437be-2e43-4581-99e1-db373bd99ef0.png)'
- en: 'Figure 5.7: Feature visualization for the first layer in a CNN Source: https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7：CNN第一层的特征可视化 来源：https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf
- en: 'The image is in two parts; on the left we can see the convolution (the paper
    only highlights 9 convolutions for each layer). On the right, we can see examples
    of patterns within images that match that convolution. For example, the convolution
    in the top-left corner is a diagonal edge detector. Here is the feature visualization
    for the second layer:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片分为两部分；左边我们可以看到卷积（论文只突出了每层的9个卷积）。右边，我们可以看到与该卷积匹配的图像中的模式示例。例如，左上角的卷积是一个对角线边缘检测器。以下是第二层的特征可视化：
- en: '![](img/8995cb80-262e-4472-81dd-a0201ee82632.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8995cb80-262e-4472-81dd-a0201ee82632.png)'
- en: 'Figure 5.8: Feature visualization for the second layer in a CNN Source: https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8：CNN第二层的特征可视化 来源：https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf
- en: 'Again, the image on the left is an interpretation of the convolution, while
    the image on the right shows examples of image patches that activate for that
    convolution. Here, we are starting to see some combinatorial patterns. For example,
    in the top-left, we can see patterns with stripes. Even more interesting is the
    example in the second row and second column. Here, we see circle shapes, which
    can indicate an eyeball in a person or an animal. Now, let''s move on to feature
    visualization for the third layer:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，左侧的图像是卷积的解释，而右侧的图像展示了激活该卷积的图像补丁示例。在这里，我们开始看到一些组合模式。例如，在左上角，我们可以看到有条纹的图案。更有趣的是第二行第二列的例子。在这里，我们看到圆形图形，这可能表示一个人的或动物的眼球。现在，让我们继续来看第三层的特征可视化：
- en: '![](img/44322ff1-5a85-4c01-af92-00d8d8716b79.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/44322ff1-5a85-4c01-af92-00d8d8716b79.png)'
- en: 'Figure 5.9: Feature visualization for the third layer in a CNN Source: https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9：CNN第三层的特征可视化 来源：https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf
- en: In the third layer, we are seeing some really interesting patterns. In the second
    row and second column, we have identified parts of a car (wheels). In the third
    row and third column, we have begun to identify peoples' faces. In the second
    row and fourth column, we are identifying text within the images.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三层中，我们可以看到一些非常有趣的模式。在第二行第二列，我们已经识别出车轮的部分。在第三行第三列，我们已经开始识别人脸。在第二行第四列，我们识别出了图像中的文本。
- en: In the paper, the authors show examples for more layers. I encourage you to
    read the paper to get further insight into how convolutional layers work.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中，作者展示了更多层的示例。我鼓励你阅读论文，进一步了解卷积层的工作原理。
- en: It is important to note that, while deep learning models can achieve human-level
    performance on image classification, they do not interpret images as humans do.
    They have no concept of what a cat is or what a dog is. They can only match the
    patterns given. In the paper, the authors highlight an example where the matched
    patterns have little in common; the model is matching features in the background
    (grass) instead of foreground objects.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，尽管深度学习模型在图像分类任务中能够达到与人类相当的表现，但它们并不像人类那样解读图像。它们没有“猫”或“狗”的概念，它们只能匹配给定的模式。在论文中，作者强调了一个例子，在这个例子中，匹配的模式几乎没有任何相似之处；模型匹配的是背景中的特征（如草地），而不是前景中的物体。
- en: In another image classification task, the model failed to work in practice.
    The task was to classify wolves versus dogs. The model failed in practice because
    the model was trained with data which had wolves in their natural habitat, that
    is, snow. Therefore, the model assumed its task was to differentiate between *snow* and
    *dog*. Any image of a wolf in another setting was wrongly classified.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个图像分类任务中，模型在实际操作中失败了。任务是区分狼和狗。模型在实际应用中失败是因为训练数据包含了处于自然栖息地的狼——即雪地。因此，模型误以为任务是区分*雪*和*狗*。任何在其他环境下的狼的图像都会被错误分类。
- en: The lesson from this is that your training data should be varied and closely
    related to the data that the model will be expected to predict against. This may
    sound obvious in theory, but it is not always easy to do so in practice. We will
    discuss this further in the next chapter.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从中得到的教训是，训练数据应该是多样化的，并且与模型预期要预测的实际数据密切相关。理论上这可能听起来很显而易见，但在实践中往往并不容易做到。我们将在下一章进一步讨论这一点。
- en: Pooling layers
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化层
- en: 'Pooling layers are used in CNNs to reduce the number of parameters in the model
    and therefore they reduce overfitting. They can be thought of as a type of dimensionality
    reduction. Similar to convolutional layers, a pooling layer moves over the previous
    layer but the operation and return value are different. It returns a single value
    and the operation is usually the maximum value of the cells in that patch, hence
    the name max-pooling. You can also perform other operations, for example, average
    pooling, but this is less common. Here is an example of max-pooling using a 2
    x 2 block. The first block has the values 7, 0, 6, 6 and the maximum value of
    these is 7, so the output is 7\. Note that padding is not normally used with max-pooling
    and that it usually applies a stride parameter to move the block. Here, the stride
    is 2, so once we get the max of the first block, we move across, 2 cells to the
    right:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层在卷积神经网络（CNN）中用于减少模型中的参数数量，因此可以减少过拟合。它们可以被看作是一种降维方式。类似于卷积层，池化层在上一层上滑动，但操作和返回值不同。它返回一个单一的值，通常是该区域内各个单元格的最大值，因此称之为最大池化。你也可以执行其他操作，例如平均池化，但这种方式较少使用。这里是一个使用
    2 x 2 块进行最大池化的例子。第一个块的值为 7、0、6、6，其中最大值为 7，因此输出为 7。注意，最大池化通常不使用填充（padding），并且它通常会应用步幅参数来移动块。这里的步幅是
    2，因此在获取第一个块的最大值后，我们会向右移动 2 个单元：
- en: '![](img/4fdf19a0-cf55-4d13-bccd-91b5b63c0283.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4fdf19a0-cf55-4d13-bccd-91b5b63c0283.png)'
- en: 'Figure 5.10: Max-Pooling applied to a matrix'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10：最大池化应用于矩阵
- en: 'We can see that max-pooling reduces the output by a factor of 4; the input
    was 6 x 6 and the output is 3 x 3\. If you have not seen this before, your first
    reaction is probably disbelief. Why are we throwing away data? Why do we use max-pooling
    at all? There are three parts to this answer:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，最大池化将输出减少了四倍；输入是 6 x 6，输出是 3 x 3。如果你之前没有见过这种情况，你的第一反应可能是不相信。为什么我们要丢弃数据？为什么要使用最大池化？这个问题的答案有三个部分：
- en: '**Pooling**: It is normally applied after a convolutional layer, so instead
    of executing over pixels, we execute over matched patterns. Downsizing after convolutional
    layers does not discard 75% of the input data; there is still enough signal there
    to find the pattern if it exists.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**池化**：它通常在卷积层之后应用，因此我们不是在像素上操作，而是在匹配的模式上操作。卷积层之后的降维并不会丢弃 75% 的输入数据；如果存在模式，数据中仍然有足够的信号来识别它。'
- en: '**Regularization**: If you have studied machine learning, you will know that
    many models have problems with correlated features and that you are generally
    advised to remove correlated features. In image data, features are highly correlated
    with the spatial pattern around them. Applying max-pooling reduces the data while
    maintaining the features.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化**：如果你学习过机器学习，你会知道许多模型在处理相关特征时会遇到问题，而且通常建议去除相关特征。在图像数据中，特征与它们周围的空间模式高度相关。应用最大池化可以在保持特征的同时减少数据。'
- en: '**Execution speed**: When we consider the two earlier reasons, we can see that
    max-pooling greatly reduces the size of the network without removing too much
    of the signal. This makes training the model much quicker.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行速度**：当我们考虑前面提到的两个原因时，我们可以看到，最大池化大大减少了网络的大小，而没有去除过多的信号。这使得模型训练变得更快。'
- en: It is important to note the difference in the parameters used in the convolutional
    layer compared to the pooling layer. In general, a convolutional block is bigger
    (3 x 3) than the pooling block (2 x 2) and they should not overlap. For example,
    do not use a 4 x 4 convolutional block and a 2 x 2 pooling block. If they did
    overlap, the pooling block would just operate over the same convolutional blocks
    and the model would not train correctly.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，卷积层和池化层使用的参数是不同的。通常，卷积块的尺寸比池化块大（例如 3 x 3 的卷积块和 2 x 2 的池化块），而且它们不应该重叠。例如，不能同时使用
    4 x 4 的卷积块和 2 x 2 的池化块。如果它们重叠，池化块将仅仅在相同的卷积块上操作，模型将无法正确训练。
- en: Dropout
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout
- en: '**Dropout** is a form of regularization which aims to prevent a model from
    overfitting. Overfitting is when the model is memorizing parts of the training
    dataset, but is not as accurate on unseen test data. When you build a model, you
    can check if overfitting is a problem by looking at the gap between the accuracy
    on the training set against the accuracy on the test set. If performance is much
    better on the training dataset, then the model is overfitting. Dropout refers
    to removing nodes randomly from a network temporarily during training. It is usually
    only applied to hidden layers, and not input layers. Here is an example of dropout
    applied to a neural network:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**Dropout**是一种正则化方法，旨在防止模型过拟合。过拟合是指模型记住了训练数据集中的一部分内容，但在未见过的测试数据上表现不佳。当你构建模型时，可以通过查看训练集的准确度与测试集的准确度之间的差距来检查是否存在过拟合问题。如果训练集上的表现远好于测试集，那么模型就是过拟合的。Dropout指的是在训练过程中临时随机移除网络中的一些节点。它通常只应用于隐藏层，而不应用于输入层。下面是应用dropout的神经网络示例：'
- en: '![](img/25613efe-f5b3-4596-8cca-80e3f0bc38e2.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/25613efe-f5b3-4596-8cca-80e3f0bc38e2.jpg)'
- en: 'Figure 5.11: An example of dropout in a deep learning model'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11：深度学习模型中dropout的示例
- en: For each forward pass, a different set of nodes is removed, and therefore the
    network is different each time. In the original paper, dropout is compared to
    ensemble techniques, and in a way it is. There are some similarities to how dropout
    works and how random forest selects a random selection of features for each tree.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 每次前向传播时，会移除一组不同的节点，因此每次网络的结构都会不同。在原始论文中，dropout被与集成技术进行了比较，从某种程度上来说，它确实有相似之处。dropout的工作方式与随机森林为每棵树随机选择特征的方式有些相似。
- en: Another way to look at dropout is that each node in a layer must learn to work
    with all the nodes in that layer and the inputs it gets from the previous layer.
    It prevents one or a small number of nodes in a layer from getting large weights
    and dominating the outputs from that layer. This means that each node in a layer
    will work as a group and prevent some nodes from being too lazy and other nodes
    from being too dominant.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种看待dropout的方式是，每个层中的节点必须学会与该层中的所有节点以及它从上一层获得的输入一起工作。这可以防止某些节点在层内占据主导地位并获得过大的权重，从而影响该层的输出。这意味着每个层中的节点将作为一个整体工作，防止某些节点过于懒惰，而其他节点过于支配。
- en: Flatten layers, dense layers, and softmax
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Flatten层、密集层和softmax
- en: After applying multiple convolutional layers, the resulting data structure is
    a multi-dimensional matrix (or tensor). We must transform this into a matrix that
    is in the shape of the required output. For example, if our classification task
    has 10 classes (for example, 10 for the `MNIST` example), we need the output of
    the model to be a 1 x 10 matrix. We do this by taking the results of our convolutional
    and max-pooling layers and using a Flatten layer to reshape the data. The last
    layer should have the same number of nodes as the number of classes we wish to
    predict for. If our task is binary classification, the `activation` function in
    our last layer will be sigmoid. If our task is binary classification, the `activation`
    function in our last layer will be softmax.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 应用多个卷积层后，得到的数据结构是一个多维矩阵（或张量）。我们必须将其转换为所需输出形状的矩阵。例如，如果我们的分类任务有10个类别（例如，`MNIST`示例中的10个类别），则需要将模型的输出设置为一个1
    x 10的矩阵。我们通过将卷积层和最大池化层的结果进行处理，使用Flatten层重新塑造数据来实现这一点。最后一层的节点数应与我们要预测的类别数相同。如果我们的任务是二分类任务，则最后一层的`activation`函数将是sigmoid。如果我们的任务是多分类任务，则最后一层的`activation`函数将是softmax。
- en: Before applying the softmax/sigmoid activation, we may optionally apply a number
    of dense layers. A dense layer is just a normal hidden layer, as we saw in [Chapter
    1](00c01383-1886-46d0-9435-29dfb3e08055.xhtml),* Getting Started with Deep Learning*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用softmax/sigmoid激活函数之前，我们可以选择性地应用多个密集层。密集层就是我们在[第1章](00c01383-1886-46d0-9435-29dfb3e08055.xhtml)《深度学习入门》中看到的普通隐藏层。
- en: 'We need a softmax layer because the values in the last layer are numeric but
    range from -infinity to + infinity. We must convert these series of input values
    into a series of probabilities that says how likely the instance is for each category.
    The function to transform these numeric values to a series of probabilities must
    have the following characteristics:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个softmax层，因为最后一层的值是数字，但范围从负无穷到正无穷。我们必须将这些输入值转换为一系列概率，表示该实例属于每个类别的可能性。将这些数值转换为概率的函数必须具有以下特点：
- en: Each output value must be between 0.0 to 1.0
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个输出值必须在 0.0 到 1.0 之间。
- en: The sum of the output values should be exactly 1.0
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出值的总和应为 1.0。
- en: 'One way to do this is to just rescale the values by dividing each input value
    by the sum of the absolute input values. That approach has two problems:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是通过将每个输入值除以绝对输入值的总和来重新缩放这些值。这个方法有两个问题：
- en: It does not handle negative values correctly
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它无法正确处理负值。
- en: Rescaling the input values may give us probabilities that are too close to each
    other
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新缩放输入值可能会导致概率值过于接近。
- en: 'These two issues can be solved by first applying **e^x** (where e is 2.71828)
    to each input value and then rescaling those values. This transforms any negative
    number to a small positive number, and it also causes the probabilities to be
    more polarized. This can be demonstrated with an example; here, we can see the
    result from our dense layers. The values for categories 5 and 6 are quite close
    at 17.2 and 15.8, respectively. However, when we apply the `softmax` function,
    the probability value for category 5 is 4 times the probability value for category
    6\. The `softmax` function tends to result in probabilities that emphasize one
    category over all others, which is exactly what we want:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个问题可以通过首先对每个输入值应用 **e^x**（其中 e 为 2.71828）然后重新缩放这些值来解决。这将把任何负数转换为一个小的正数，同时也使得概率更加极化。可以通过一个示例来演示这一点；在这里，我们可以看到来自稠密层的结果。类别
    5 和 6 的值分别为 17.2 和 15.8，相当接近。然而，当我们应用 `softmax` 函数时，类别 5 的概率值是类别 6 的 4 倍。`softmax`
    函数倾向于使某个类别的概率远远大于其他类别，这正是我们所希望的：
- en: '![](img/4cbe0564-d081-4f69-9b77-5dc0e94402be.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4cbe0564-d081-4f69-9b77-5dc0e94402be.png)'
- en: Figure 5.12 Example of the softmax function
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 Softmax 函数的示例。
- en: Image classification using the MXNet library
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 MXNet 库进行图像分类。
- en: The MXNet package was introduced in [Chapter 1](00c01383-1886-46d0-9435-29dfb3e08055.xhtml),
    *Getting Started with Deep Learning*, so go back to that chapter for instructions
    on how to install the package if you have not already done so. We will demonstrate
    how to get almost 100% accuracy on a classification task for image data. We will
    use the `MNIST` dataset that we introduced in [Chapter 2](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml),
    *Image Classification Using Convolutional Neural Networks*. This dataset contains
    images of handwritten digits (0-9), and all images are of size 28 x 28\. It is
    the *Hello World!* equivalent in deep learning. There’s a long-term competition
    on Kaggle that uses this dataset. The script `Chapter5/explore.Rmd` is an R markdown
    file that explores this dataset.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet 包在[第 1 章](00c01383-1886-46d0-9435-29dfb3e08055.xhtml)中介绍过，*深度学习入门*，如果你还没有安装这个包，可以回到该章查看安装说明。我们将演示如何在图像数据分类任务中获得接近
    100% 的准确率。我们将使用在[第 2 章](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml)中介绍的 `MNIST`
    数据集，*使用卷积神经网络进行图像分类*。该数据集包含手写数字（0-9）的图像，所有图像大小为 28 x 28。它是深度学习中的 *Hello World!*。Kaggle
    上有一个长期的竞赛使用这个数据集。脚本 `Chapter5/explore.Rmd` 是一个 R markdown 文件，用于探索这个数据集。
- en: 'First, we will check if the data has already been downloaded, and if it has
    not, we will download it. If the data is not available at this link, see the code
    in `Chapter2/chapter2.R` for an alternative way to get the data:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将检查数据是否已经下载，如果没有，我们将下载它。如果该链接无法获取数据，请参阅 `Chapter2/chapter2.R` 中的代码，获取数据的替代方法：
- en: '[PRE0]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next we read the data into R and check it:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将数据读取到 R 中并进行检查：
- en: '[PRE1]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We have `20` rows and `785` columns. Here, we will look at the rows at the
    tail of the dataset and look at the first 6 columns and the last 6 columns:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有 `20` 行和 `785` 列。在这里，我们将查看数据集末尾的行，并查看前 6 列和最后 6 列：
- en: '[PRE2]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We have `785` columns. The first column is the data label, and then we have
    784 columns named `pixel0`, …, `pixel783` with the pixel values. Our images are
    *28 x 28 = 784*, so everything looks OK.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有 `785` 列。第一列是数据标签，然后是 784 列，命名为 `pixel0`、…、`pixel783`，其中包含像素值。我们的图像是 *28
    x 28 = 784*，因此一切看起来正常。
- en: Before we start building models, it is always a good idea to ensure that your
    data is in the correct format and that your features and labels are aligned correctly.
    Let's plot the first 9 instances with their data labels.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始构建模型之前，确保数据格式正确、特征与标签对齐总是一个好主意。让我们绘制前 9 个实例及其数据标签。
- en: 'To do this, we will create a `helper` function called `plotInstance` that takes
    in the pixel values and outputs the image with an optional header:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为此，我们将创建一个名为 `plotInstance` 的 `helper` 函数，该函数接受像素值并输出图像，带有可选的标题：
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output of this code shows the first 9 images and their classification:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码的输出显示了前9张图像及其分类：
- en: '![](img/ca8e8a1a-ba42-4536-8103-7e5ac5b98ee7.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca8e8a1a-ba42-4536-8103-7e5ac5b98ee7.png)'
- en: 'Figure 5.13: The first 9 images in the MNIST dataset'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13：MNIST数据集中的前9张图像
- en: This completes our data exploration. Now, we can move on to creating some deep
    learning models using the MXNet library. We will create two models—the first is
    a standard neural network which we will use as a baseline. The second deep learning
    model is based on an architecture called **LeNet**. This is an old architecture,
    but is suitable in this case because our images are low resolution and do not
    contain backgrounds. Another advantage of LeNet is that it is possible to train
    quickly, even on CPUs, because it does not have many layers.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们的数据探索。现在，我们可以继续使用MXNet库创建一些深度学习模型。我们将创建两个模型——第一个是标准的神经网络模型，我们将其作为基线。第二个深度学习模型基于一个名为**LeNet**的架构。这是一个较旧的架构，但由于我们的图像分辨率较低且不包含背景，因此在这种情况下适用。LeNet的另一个优点是，它的训练速度很快，即使是在CPU上也能高效训练，因为它的层数不多。
- en: 'The code for this section is in  `Chapter5/mnist.Rmd`. We must read data into
    R and convert it into matrices. We will split the training data into a train set
    and test set to get an unbiased estimate of accuracy. Because we have a large
    number of rows, we can use a split ratio of 90/10:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 本节代码位于`Chapter5/mnist.Rmd`。我们必须将数据读入R并转换为矩阵。我们将训练数据分割成训练集和测试集，以便获得不偏的准确性估计。由于数据行数较多，我们可以使用90/10的分割比例：
- en: '[PRE4]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Each image is represented as row of 784 (28 x 28) pixel values. The value of
    each pixel is in the range 0-255, we linearly transform it into 0-1 by dividing
    by 255\. We also transpose the input matrix to because column major format in
    order to use it in  `mxnet`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图像表示为784个（28 x 28）像素值的一行。每个像素的值范围为0-255，我们通过除以255将其线性转换为0-1。我们还对输入矩阵进行转置，因为`mxnet`使用的是列主序格式。
- en: '[PRE5]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Before creating a model, we should check that our dataset is balanced, i.e.
    the number of instances for each digit is reasonably even:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建模型之前，我们应该检查我们的数据集是否平衡，即每个数字的实例数是否合理均衡：
- en: '[PRE6]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This looks ok, we can now move on to creating some deep learning models.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来没问题，我们现在可以继续创建一些深度学习模型了。
- en: Base model (no convolutional layers)
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础模型（无卷积层）
- en: 'Now that we have explored the data and we are satisfied that it looks OK, the
    next step is to create our first deep learning model. This is similar to the example
    we saw in the previous chapter. The code for this is in `Chapter5/mnist.Rmd`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了数据，并且确认它看起来没问题，下一步是创建我们的第一个深度学习模型。这与我们在上一章看到的示例类似。该代码位于`Chapter5/mnist.Rmd`：
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s look at this code in detail:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细查看这段代码：
- en: In `mxnet`, we use its own data type symbol to configure the network.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`mxnet`中，我们使用其自有的数据类型符号来配置网络。
- en: We create the first hidden layer (`fullconnect1 <- ....`). This parameters are
    the data as input, the layer's name and the number of neurons in the layer.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了第一个隐藏层（`fullconnect1 <- ....`）。这些参数是输入数据、层的名称以及该层的神经元数。
- en: We apply an activation function to the `fullconnect`  layer (`activation1 <-
    ....`). The `mx.symbol.Activation` function takes the output from the first hidden
    layer, `fullconnect1`.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对`fullconnect`层应用激活函数（`activation1 <- ....`）。`mx.symbol.Activation`函数接收来自第一个隐藏层`fullconnect1`的输出。
- en: The second hidden layer (`fullconnect1 <- ....`) takes `activation1` as the
    input.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个隐藏层（`fullconnect1 <- ....`）将`activation1`作为输入。
- en: The second activation is similar to  `activation1`.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个激活函数类似于`activation1`。
- en: The  `fullconnect3`  is the output layer. This layer has 10 neurons because
    this is a multi-classification problem and there are 10 classes.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`fullconnect3`是输出层。该层有10个神经元，因为这是一个多分类问题，共有10个类别。'
- en: Finally, we use a softmax activation to get a probabilistic prediction for each
    class.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用softmax激活函数来为每个类别获得一个概率预测。
- en: 'Now, let''s train the base model. I have a GPU installed, so I can use that.
    You may need to change the line to `devices <- mx.cpu()`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们训练基础模型。我安装了GPU，因此可以使用它。你可能需要将这一行改为`devices <- mx.cpu()`：
- en: '[PRE8]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To make a prediction, we will call the `predict` function. We can then create
    a confusion matrix and calculate our accuracy level on test data:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行预测，我们将调用`predict`函数。然后我们可以创建混淆矩阵，并计算测试数据的准确率：
- en: '[PRE9]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The accuracy of our base model is `0.971`. Not bad, but let's see if we can
    improve on it.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的基础模型的准确率为`0.971`。还不错，但让我们看看能否有所改进。
- en: LeNet
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LeNet
- en: 'Now, we can create a model based on the LeNet architecture. This is a very
    simple model; we have two sets of convolutional and pooling layers and then a
    Flatten layer, and finally two dense layers. The code for this is in `Chapter5/mnist.Rmd`.
    First let''s define the model:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以基于 LeNet 架构创建一个模型。这是一个非常简单的模型；我们有两组卷积层和池化层，然后是一个 Flatten 层，最后是两个全连接层。相关代码在
    `Chapter5/mnist.Rmd` 中。首先，我们来定义这个模型：
- en: '[PRE10]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, let''s reshape the data so that it can be used in MXNet:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们重新调整数据的形状，以便它可以在 MXNet 中使用：
- en: '[PRE11]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, we can build the model:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以构建模型：
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, let''s evaluate the model:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们评估模型：
- en: '[PRE13]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The accuracy of our CNN model is `0.9835714`, which is quite an improvement
    over the accuracy of our base model, which was `0.971`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 CNN 模型的准确率是 `0.9835714`，相比我们基准模型的 `0.971`，有了相当大的提升。
- en: 'Finally, we can visualize our model in R:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以在 R 中可视化我们的模型：
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This produces the following plot, which shows the architecture of the deep
    learning model:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成以下图表，展示深度学习模型的架构：
- en: '![](img/44ca3de2-5605-49eb-9042-80099ff1b36f.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/44ca3de2-5605-49eb-9042-80099ff1b36f.png)'
- en: 'Figure 5.14: Convolutional deep learning model (LeNet)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14：卷积深度学习模型（LeNet）
- en: Congratulations! You have built a deep learning model that is over 98% accurate!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你！你已经构建了一个准确率超过 98% 的深度学习模型！
- en: We saw the architecture of LeNet in *Figure 5.1* and we have programmed it using
    the MXNet library. Let's analyze the LeNet architecture in more detail. Essentially,
    we have two convolutional groups and two fully connected layers. Our convolutional
    groups have a convolutional layer, followed by an `activation` function and then
    a pooling layer. This combination of layers is very common across many deep learning
    image classification tasks. The first convolution layer has 64 blocks of 5 x 5
    size with no padding. This will possibly miss some features at the edges of the
    images, but if we look back at our sample images in *Figure 5.15*, we can see
    that most images do not have any data around the borders. We use pooling layers
    with `pool_type=max`. Other types are possible; average pooling was commonly used
    but has fallen out of favor recently. It is another hyper-parameter to try. We
    calculate our pools in 2 x 2 and then stride `(“jump”)` by 2\. Therefore, each
    input value is only used once in the max pool layer.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *图 5.1* 中看到了 LeNet 的架构，并且已经使用 MXNet 库进行了编程。接下来，我们更详细地分析 LeNet 架构。本质上，我们有两组卷积层和两层全连接层。我们的卷积组包含一个卷积层，接着是一个
    `activation` 函数，然后是一个池化层。这种层的组合在许多深度学习图像分类任务中非常常见。第一层卷积层有 64 个 5 x 5 大小的卷积块，没有填充。这可能会错过图像边缘的一些特征，但如果我们回顾
    *图 5.15* 中的样本图像，我们可以看到大多数图像的边缘并没有数据。我们使用 `pool_type=max` 的池化层。其他类型也是可能的；平均池化曾经常用，但最近已经不太流行了。这也是一个可以尝试的超参数。我们计算
    2 x 2 的池化区域，然后步长为 2（即“跳跃”）。因此，每个输入值在最大池化层中只使用一次。
- en: We use `tanh` as an `activation` function for our first convolutional block,
    and then use  `relu`  for subsequent layers. If you wish, you can try to change
    these and see what effect they have. Once we have executed our convolutional layers,
    we can use Flatten to restructure the data into a format that can be used by a
    fully connected layer. A fully connected layer is just a collection of nodes in
    a layer, that is, similar to the layers in the base model in the previous code.
    We have two layers, one with 512 nodes and the other with 10 nodes. We select
    10 nodes in our last layer as this is the number of categories in our problem.
    Finally, we use a softmax to convert the numeric quantities in this layer into
    a set of probabilities for each category. We have achieved 98.35% accuracy, which
    is quite an improvement on a *normal* deep learning model, but there is still
    room for improvement. Some models can get 99.5% accuracy on this dataset, that
    is, 5 wrongly classified records in 1,000\. Next, we will look at a different
    dataset that, while similar to MNIST, is harder than MNIST. This is the Fashion
    `MNIST` dataset, which has grayscale images of the same size as MNIST and also
    has 10 categories.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为第一个卷积块使用`Tanh`作为激活函数，然后为后续层使用`ReLU`。如果你愿意，可以尝试更改这些并查看它们的效果。执行卷积层后，我们可以使用Flatten将数据重构为全连接层可以使用的格式。全连接层就是一组节点集合，也就是类似于前面代码中基本模型的层。我们有两层，一层包含512个节点，另一层包含10个节点。我们选择在最后一层中使用10个节点，因为这是我们问题中的类别数量。最后，我们使用Softmax将该层中的数值转化为每个类别的概率集。我们已经达到了98.35%的准确率，这比*普通*的深度学习模型有了显著的提升，但仍然有改进空间。一些模型在该数据集上的准确率可达到99.5%，也就是每1000个记录中有5个错误分类。接下来，我们将查看一个不同的数据集，虽然它与MNIST相似，但比MNIST要更具挑战性。这就是Fashion
    `MNIST`数据集，具有与MNIST相同大小的灰度图像，并且也有10个类别。
- en: Classification using the fashion MNIST dataset
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Fashion MNIST数据集进行分类
- en: 'This dataset is in the same structure as `MNIST`, so we can just change our
    dataset and use the existing boilerplate code we have for loading the data. The
    script `Chapter5/explore_Fashion.Rmd` is an R markdown file that explores this
    dataset; it is almost identical to the `explore.Rmd` that we used for the `MNIST`
    dataset, so we will not repeat it. The only change to the `explore.Rmd` is to
    output the labels. We will look at 16 examples because this is a new dataset.
    Here are some sample images from this dataset that are created using the same
    boilerplate code we used to create the example for the `MNIST` dataset:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集与`MNIST`的结构相同，因此我们只需要更换数据集，并使用我们现有的加载数据的样板代码。脚本`Chapter5/explore_Fashion.Rmd`是一个R
    markdown文件，用来探索这个数据集；它与我们之前用于`MNIST`数据集的`explore.Rmd`几乎完全相同，因此我们不再重复。唯一的不同是在`explore.Rmd`中增加了输出标签。我们将查看16个示例，因为这是一个新数据集。以下是使用我们为`MNIST`数据集创建示例时所用的相同样板代码，生成的一些来自该数据集的示例图像：
- en: '![](img/367f26fc-344b-499f-a1de-5e190590764f.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/367f26fc-344b-499f-a1de-5e190590764f.png)'
- en: 'Figure 5.15: Some images from the Fashion MNIST dataset'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15：来自Fashion MNIST数据集的部分图像
- en: An interesting fact about this dataset is that the company that released it
    also created a GitHub repository where they tested machine learning libraries
    against this dataset. The benchmarks are available at [http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/).
    If we look through these results, none of the machine libraries they tried achieved
    over 90% accuracy (they did not try deep learning). This is the target we want
    to beat with a deep learning classifier. The deep learning model code is in `Chapter5/fmnist.R`
    and achieves over 91% accuracy on this dataset. There are some small, but significant
    differences to the model architecture above. Try to spot them without peeking
    at the explanation.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集的一个有趣的事实是，发布该数据集的公司还创建了一个GitHub仓库，在那里他们对比了多种机器学习库在该数据集上的表现。基准测试可以在[http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/)查看。如果我们查看这些结果，会发现他们尝试的所有机器学习库都没有达到90%的准确率（他们没有尝试深度学习）。这是我们希望通过深度学习分类器超越的目标。深度学习模型的代码在`Chapter5/fmnist.R`中，能够在该数据集上实现超过91%的准确率。与上面模型架构相比，有一些小的但重要的差异。试着在不查看解释的情况下找到它们。
- en: First, let's define the model architecture.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义模型架构。
- en: '[PRE15]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now let''s train the model:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来训练模型：
- en: '[PRE16]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The first change is that we switch to use `relu` as an `Activation` function
    for all layers. Another change was that we use padding for the convolutional layers,
    which we did to capture the features at the borders of the image. We increased
    the number of nodes in each layer to add depth to the model. We also added a dropout
    layer to prevent the model from overfitting. We also added logging to our model,
    which outputs the train and validation metrics for each epoch. We used these to
    check how our model performs and to decide if it is overfitting.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个变化是我们将所有层的激活函数切换为使用`relu`。另一个变化是我们为卷积层使用了填充，以便捕捉图像边缘的特征。我们增加了每一层的节点数，给模型增加了深度。我们还添加了一个dropout层，以防止模型过拟合。我们还在模型中加入了日志记录功能，输出每个epoch的训练和验证指标。我们利用这些数据来检查模型表现，并决定它是否过拟合。
- en: 'Here are the accuracy results and the diagnostic plot for this model:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该模型的准确率结果及其诊断图：
- en: '[PRE17]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: One thing to note is that we are using the same validation/test set for showing
    metrics during training and to evaluate the final model. This is not a good practice,
    but it is acceptable here because we are not using validation metrics to tune
    the hyperparameters of the model. The accuracy of our CNN model is `0.9106667`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，我们在训练过程中展示度量指标和评估最终模型时使用的是相同的验证/测试集。这并不是一个好做法，但在这里是可以接受的，因为我们并没有使用验证指标来调整模型的超参数。我们CNN模型的准确率是`0.9106667`。
- en: 'Let''s plot the accuracy of the train and validation sets as the model is trained. The
    deep learning model code has a `callback` function which saves the metrics as
    the model is trained. We can use this to plot the training and validation metrics
    for each epoch:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制训练集和验证集准确率随模型训练进展的变化图。深度学习模型代码中有一个`callback`函数，可以在模型训练时保存指标。我们可以利用它绘制每个epoch的训练和验证指标图：
- en: '[PRE18]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This shows us how our model is performing after each epoch (or training run). This
    produces the following screenshot:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这向我们展示了模型在每个epoch（或训练轮次）后的表现。以下是生成的截图：
- en: '![](img/b92b126f-4ded-40c8-b316-1fbae5700357.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b92b126f-4ded-40c8-b316-1fbae5700357.png)'
- en: 'Figure 5.16: Training and validation accuracy by epoch'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.16：每个epoch的训练和验证准确率
- en: 'The two main points to be taken from this graph are that:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 从这张图中可以得出两个主要结论：
- en: The model is overfitting. We can see a clear gap between performance on the
    training set at **0.95xxx** and the validation set at **0.91xxx**.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型出现了过拟合。我们可以看到训练集的性能为**0.95xxx**，而验证集的性能为**0.91xxx**，二者之间存在明显的差距。
- en: We could have probably stopped the model training after 8 epochs, as performance
    did not improve after this point.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能在第8个epoch后就可以停止模型训练，因为此后性能没有再提升。
- en: As we have discussed in previous chapters, deep learning models will almost
    always overfit by default, but there are methods to negate this. The second issue
    is related to *early stopping*, and it is vital that you know how to do this so
    that you do not waste hours in continuing to train a model which is no longer
    improving. This is especially relevant if you are building the model using cloud
    resources. We will look at these and more issues related to building deep learning
    models in the next chapter.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前几章讨论的那样，深度学习模型默认几乎总是会出现过拟合，但有方法可以避免这一点。第二个问题与*早停*相关，了解如何做到这一点至关重要，这样你就不会浪费数小时继续训练一个不再改进的模型。如果你是使用云资源来构建模型，这一点尤其重要。我们将在下一章讨论这些以及与构建深度学习模型相关的更多问题。
- en: References/further reading
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献/进一步阅读
- en: These papers are classical deep learning papers in this domain. Some of them
    document winning approaches to ImageNet competitions. I encourage you to download
    and read all of them. You may not understand them at first, but their importance
    will become more evident as you continue on your journey in deep learning.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这些论文是该领域经典的深度学习论文，其中一些记录了赢得ImageNet竞赛的方案。我鼓励你下载并阅读所有这些论文。你可能一开始无法理解它们，但随着你在深度学习领域的进展，它们的重要性将变得更加显而易见。
- en: Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. *ImageNet Classification
    with Deep Convolutional Neural Networks*. Advances in neural information processing
    systems. 2012.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky, Alex, Ilya Sutskever 和 Geoffrey E. Hinton. *使用深度卷积神经网络进行ImageNet分类*.
    神经信息处理系统进展. 2012.
- en: Szegedy, Christian, et al. *Going Deeper with Convolutions*. Cvpr, 2015.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy, Christian 等人. *通过卷积深入探索*. Cvpr, 2015.
- en: 'LeCun, Yann, et al. *Learning Algorithms for Classification: A Comparison on
    Handwritten Digit Recognition*. Neural networks: the statistical mechanics perspective
    261 (1995): 276.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun, Yann等人。*分类学习算法：手写数字识别比较研究*。神经网络：统计力学视角261（1995）：276。
- en: Zeiler, Matthew D., and Rob Fergus. *Visualizing and Understanding Convolutional
    Networks*. European conference on computer vision. Springer, Cham, 2014.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeiler, Matthew D.和Rob Fergus。*可视化和理解卷积网络*。欧洲计算机视觉会议。斯普林格，香槟，2014。
- en: 'Srivastava, Nitish, et al. *Dropout: A Simple Way to Prevent Neural Networks
    from Overfitting*. The Journal of Machine Learning Research 15.1 (2014): 1929-1958.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava, Nitish等人。*Dropout：防止神经网络过拟合的简单方法*。机器学习研究杂志15.1（2014）：1929-1958。
- en: Summary
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we used deep learning for image classification. We discussed
    the different layer types that are used in image classification: convolutional
    layers, pooling layers, dropout, dense layers, and the softmax activation function.
    We saw an R-Shiny application that shows how convolutional layers perform feature
    engineering on image data.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用深度学习进行图像分类。我们讨论了用于图像分类的不同层类型：卷积层，池化层，Dropout，全连接层以及Softmax激活函数。我们看到了一个R-Shiny应用程序，展示了卷积层如何在图像数据上进行特征工程。
- en: We used the MXNet deep learning library in R to create a base deep learning
    model which got 97.1% accuracy. We then developed a CNN deep learning model based
    on the LeNet architecture, which achieved over 98.3% accuracy on test data. We
    also used a slightly harder dataset (`Fashion MNIST`) and created a new model
    that achieved over 91% accuracy. This accuracy score was better than all of the
    other scores that used non-deep learning algorithms. In the next chapter, we will
    build on what we have covered and show you how we can take advantage of pre-trained
    models for classification and as building blocks for new deep learning models.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用MXNet深度学习库在R中创建了一个基础深度学习模型，其准确率达到了97.1%。然后，我们基于LeNet架构开发了一个CNN深度学习模型，在测试数据上实现了超过98.3%的准确率。我们还使用了一个稍难的数据集（`Fashion
    MNIST`），并创建了一个新模型，其准确率超过了91%。这个准确率比使用非深度学习算法的所有分数都要好。在下一章中，我们将建立在我们所讨论的基础上，并展示如何利用预训练模型进行分类，以及作为新深度学习模型的构建块。
- en: In the next chapter, we are going to discuss important topics in deep learning
    concerning tuning and optimizing your models. This includes how to use the limited
    data you may have, data pre-processing, data augmentation, and hyperparameter
    selection.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论深度学习中关于调优和优化模型的重要主题。这包括如何利用可能有的有限数据，数据预处理，数据增强以及超参数选择。
