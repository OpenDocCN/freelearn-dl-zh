- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Multi-Modal Networks and Image Captioning with ResNets and Transformer Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态网络与图像描述生成，使用 ResNet 和 Transformer 网络
- en: '"A picture is worth a thousand words" is a famous adage. In this chapter, we''ll
    put this adage to the test and generate captions for an image. In doing so, we''ll
    work with **multi-modal** networks. Thus far, we have operated on text as input.
    Humans can handle multiple sensory inputs together to make sense of the environment
    around them. We can watch a video with subtitles and combine the information provided
    to understand the scene. We can use facial expressions and lip movement along
    with sounds to understand speech. We can recognize text in an image, and we can
    answer natural language questions about images. In other words, we have the ability
    to process information from different modalities at the same time, and then put
    them together to understand the world around us. The future of artificial intelligence
    and deep learning is in building multi-modal networks as they closely mimic human
    cognitive functions.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “一图胜千言”是一句著名的谚语。在本章中，我们将验证这句谚语，并为图像生成描述。在此过程中，我们将使用**多模态**网络。到目前为止，我们的输入是文本。人类可以将多种感官输入结合起来，理解周围的环境。我们可以带字幕观看视频并结合所提供的信息来理解场景。我们可以通过面部表情和唇部动作与声音一起理解语言。我们可以在图像中识别文本，并能回答有关图像的自然语言问题。换句话说，我们能够同时处理来自不同模态的信息，并将它们整合在一起理解我们周围的世界。人工智能和深度学习的未来在于构建多模态网络，因为它们能
    closely 模拟人类的认知功能。
- en: Recent advances in image, speech, and text processing lay a solid foundation
    for multi-modal networks. This chapter transitions you from the world of NLP to
    the world of multi-modal learning, where we will combine visual and textual features
    using the familiar Transformer architecture.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图像、语音和文本处理的最新进展为多模态网络奠定了坚实的基础。本章将引导你从自然语言处理（NLP）领域过渡到多模态学习领域，我们将使用熟悉的 Transformer
    架构结合视觉和文本特征。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Overview of multi-modal deep learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多模态深度学习概述
- en: Vision and language tasks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉与语言任务
- en: Detailed overview of the Image Captioning task and the MS-COCO dataset
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像描述任务和 MS-COCO 数据集的详细概述
- en: Architecture of a residual network, specifically ResNet
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 残差网络架构，特别是 ResNet
- en: Extracting features from images using pre-trained ResNet50
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练的 ResNet50 提取图像特征
- en: Building a full Transformer model from scratch
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零构建完整的 Transformer 模型
- en: Ideas for improving the performance of image captioning
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升图像描述生成性能的思路
- en: Our journey starts with an overview of the various tasks in the visual understanding
    domain, with a focus on tasks that combine language and images.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的旅程从视觉理解领域的各种任务概述开始，重点介绍结合语言和图像的任务。
- en: Multi-modal deep learning
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态深度学习
- en: The dictionary definition of "modality" states that it is "a particular mode
    in which something exists or is experienced or expressed." Sensory modalities,
    like touch, taste, smell, vision, and sound, allow humans to experience the world
    around them. Suppose you are out at the farm picking strawberries, and your friend
    tells you to pick ripe and red strawberries. The instruction, *ripe and red strawberries*,
    is processed and converted into a visual and haptic criterion. As you see strawberries
    and feel them, you know instinctively if they match the criteria of *ripe and
    red*. This task is an example of multiple modalities working together for a task.
    As you can imagine, these capabilities are essential for robotics.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: “模态”一词的词典定义是“某物存在、体验或表达的特定方式。”感官模态，如触觉、味觉、嗅觉、视觉和听觉，使人类能够体验周围的世界。假设你在农场采摘草莓，朋友告诉你挑选成熟且红的草莓。指令
    *成熟且红的草莓* 会被处理并转化为视觉和触觉标准。当你看到草莓并触摸它们时，你会直觉地知道它们是否符合 *成熟且红的* 标准。这项任务就是多个模态协同工作来完成一个任务的例子。正如你能想象的，这些能力对机器人学至关重要。
- en: As a direct application of the preceding example, consider a harvesting robot
    that needs to pick ripe and ready fruit. In December 1976, Harry McGurk and John
    MacDonald published a piece of research titled *Hearing lips and seeing voices*
    ([https://www.nature.com/articles/264746a0](https://www.nature.com/articles/264746a0))
    in the reputed journal, Nature. They recorded a video of a young woman talking,
    where utterances of the syllable *ba* had been dubbed onto the lip movement of
    the syllable *ga*. When this video was played back to adults, people repeated
    hearing the syllable *da*. When the audio track was played without the video,
    the right syllable was reported. This research paper highlighted the role of vision
    in speech recognition. Speech recognition models using lip-reading information
    were developed in the field of **Audio-Visual Speech Recognition** (**AVSR**).
    There are several exciting applications of multi-modal deep learning models in
    medical devices and diagnosis, learning technology, and other **Artificial Intelligence**
    (**AI**) areas.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为前面示例的直接应用，考虑一个需要采摘成熟果实的收割机器人。1976年12月，Harry McGurk和John MacDonald在著名期刊《自然》上发表了一篇题为*听嘴唇，看声音*的研究论文（[https://www.nature.com/articles/264746a0](https://www.nature.com/articles/264746a0)）。他们录制了一段年轻女性说话的视频，其中*ba*音节的发音被配上了*ga*音节的口型。当这个视频播放给成年人时，人们听到的音节是*da*。而当没有视频只播放音频时，正确的音节被报告了出来。这篇研究论文强调了视觉在语音识别中的作用。使用唇读信息的语音识别模型在**视听语音识别**（**AVSR**）领域得到了开发。多模态深度学习模型在医疗设备和诊断、学习技术及其他**人工智能**（**AI**）领域中有许多令人兴奋的应用。
- en: Let's drill down into the specific interaction of vision and language and the
    various tasks we can perform.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨视觉与语言的具体互动以及我们可以执行的各种任务。
- en: Vision and language tasks
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉与语言任务
- en: 'A combination of **Computer Vision** (**CV**) and **Natural Language Processing**
    (**NLP**) allows us to build smart AI systems that can see and talk. CV and NLP
    together produce interesting tasks for model development. Taking an image and
    generating a caption for it is a well-known task. A practical application of this
    task is generating alt-text tags for images on web pages. Visually impaired readers
    use screen readers, which can read these tags while reading the page, improving
    the accessibility of web pages. Other topics in this area include video captioning
    and storytelling – composing a story from a sequence of images. The following
    image shows some examples of images and captions. Our primary focus in this chapter
    is on image captioning:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算机视觉**（**CV**）和**自然语言处理**（**NLP**）的结合使我们能够构建能够“看”和“说”的智能AI系统。CV和NLP的结合为模型开发提供了有趣的任务。给定一张图像并为其生成描述是一个广为人知的任务。该任务的一个实际应用是为网页上的图像生成替代文本标签。视觉障碍读者使用屏幕阅读器来读取这些标签，从而在浏览网页时提高网页的可访问性。该领域的其他话题包括视频描述和讲故事——从一系列图像中编写故事。下图展示了图像和描述的一些示例。本章的主要关注点是图像描述：'
- en: '![A picture containing photo, room, bunch, many  Description automatically
    generated](img/B16252_07_01.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含照片、房间、大量物品的图片  描述自动生成](img/B16252_07_01.png)'
- en: 'Figure 7.1: Example images with captions'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：带有描述的示例图像
- en: '**Visual Question Answering** (**VQA**) is the challenging task of answering
    questions about objects in the image. The following image shows some examples
    from the VQA dataset. Compared to image captioning, where prominent objects are
    reflected in the caption, VQA is a more complex task. Answering the question may
    also require some reasoning.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**视觉问答**（**VQA**）是一个具有挑战性的任务，旨在回答关于图像中物体的问题。下图展示了来自VQA数据集的一些示例。与图像描述不同，图像描述会在描述中体现显著物体，而VQA是一个更为复杂的任务。回答问题可能还需要一定的推理。'
- en: 'Consider the bottom-right panel in the following image. Answering the question,
    "Does this person have 20/20 vision?" requires reasoning. Datasets for VQA are
    available at [visualqa.org](http://visualqa.org):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 请看下图右下方的面板。回答问题“这个人视力是20/20吗？”需要推理。VQA的数据集可以在[visualqa.org](http://visualqa.org)获取：
- en: '![A person posing for a photo  Description automatically generated](img/B16252_07_02.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![一人摆姿势拍照  描述自动生成](img/B16252_07_02.png)'
- en: 'Figure 7.2: Examples from the VQA Dataset (Source: VQA: Visual Question Answering
    by Agrawal et al.)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：来自VQA数据集的示例（来源：VQA：视觉问答，Agrawal等人）
- en: 'Reasoning leads to another challenging but fascinating task – **Visual Commonsense
    Reasoning** (**VCR**). When we look at an image, we can guess emotions, actions,
    and frame a hypothesis of what is happening. Such a task is quite easy for people
    and may even happen without conscious effort. The aim of the VCR task is to build
    models that can perform such a task. These models should also be able to explain
    or choose an appropriate reason for the logical inference that''s been made. The
    following image shows an example from the VCR dataset. More details on the VCR
    dataset can be found at [visualcommonsense.com](http://visualcommonsense.com):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 推理引出了另一个具有挑战性但又令人着迷的任务——**视觉常识推理**（**VCR**）。当我们查看一张图像时，我们可以猜测情绪、动作，并推测正在发生的事情。这个任务对人类来说相当简单，甚至可能不需要有意识地努力。VCR
    任务的目标是构建能够执行此类任务的模型。这些模型还应能够解释或选择一个适当的理由，来说明已作出的逻辑推理。以下图像展示了 VCR 数据集中的一个示例。有关
    VCR 数据集的更多细节，请访问 [visualcommonsense.com](http://visualcommonsense.com)：
- en: '![A screenshot of a social media post  Description automatically generated](img/B16252_07_03.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![社交媒体帖子截图 说明自动生成](img/B16252_07_03.png)'
- en: 'Figure 7.3: VCR example (Source: From Recognition to Cognition: Visual Commonsense
    Reasoning by Zellers et al.)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3：VCR 示例（来源：《从识别到认知：视觉常识推理》，Zellers 等人著）
- en: Thus far, we have gone from images to text. The reverse is also possible and
    is an active area of research. In this task, images or videos are generated from
    text using GANs and other generative architectures. Imagine being able to generate
    an illustrative comic book from the text of a story! This particular task is at
    the forefront of research currently.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经从图像转向了文本。反过来也可以实现，并且是一个活跃的研究领域。在这个任务中，图像或视频是通过使用生成对抗网络（GANs）和其他生成架构从文本生成的。想象一下，能够根据故事的文本生成一本插画漫画书！这个特定任务目前处于研究的前沿。
- en: A critical concept in this area is **visual grounding**. Grounding enables tying
    concepts in language to the real world. Simply put, it matches words to objects
    in a picture. By combining vision and language, we can ground concepts from languages
    to parts of an image. For example, mapping the word "basketball" to something
    that looks like one in an image is called visual grounding. There can be more
    abstract concepts that can be grounded. For example, a short elephant and a short
    person have different measurements. Grounding provides us with a way to see what
    models are learning and helps us guide them in the right direction.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域的一个关键概念是**视觉基础**。基础使得将语言中的概念与现实世界相连接。简单来说，就是将词语与图片中的物体相匹配。通过结合视觉和语言，我们可以将语言中的概念与图像中的部分进行对接。例如，将“篮球”这个词与图像中看起来像篮球的物体匹配，这就是视觉基础。也可以有更抽象的概念进行基础化。例如，一只矮小的大象和一个矮小的人具有不同的测量值。基础为我们提供了一种方式来查看模型正在学习的内容，并帮助我们引导它们朝着正确的方向前进。
- en: Now that we have a proper perspective on vision and language tasks, let's dive
    deep into an image captioning task.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对视觉和语言任务有了一个正确的视角，让我们深入探讨图像描述任务。
- en: Image captioning
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像描述
- en: Image captioning is all about describing the contents of an image in a sentence.
    Captions can help in content-based image retrieval and visual search. We already
    discussed how captions could improve the accessibility of websites by making it
    easier for screen readers to summarize the content of an image. A caption can
    be considered a summary of the image. Once we frame the problem as an image summarization
    problem, we can adapt the seq2seq model from the previous chapter to solve this
    problem. In text summarization, the input is a sequence of the long-form article,
    and the output is a short sequence summarizing the content. In image captioning,
    the output is similar in format to summarization. However, it may not be obvious
    how to structure an image that consists of pixels as a sequence of embeddings
    to be fed into the Encoder.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图像描述就是用一句话描述图像的内容。描述有助于基于内容的图像检索和视觉搜索。我们已经讨论过，描述如何通过使屏幕阅读器更容易总结图像内容来提高网站的可访问性。描述可以视为图像的总结。一旦我们将问题框定为图像摘要问题，我们可以借用上一章中的
    seq2seq 模型来解决这个问题。在文本摘要中，输入是长篇文章的序列，输出是总结内容的简短序列。在图像描述中，输出格式与摘要类似。然而，如何将由像素组成的图像结构化为一系列嵌入，以便输入到编码器中，这可能并不显而易见。
- en: Secondly, the summarization architecture used **Bi-directional Long Short-Term
    Memory networks** (**BiLSTMs**), with the underlying principle that words that
    are closer together to each other are similar to each other in meaning. BiLSTMs
    exploited this property by looking at the input sequence from both sides and generated
    encoded representations. Generating a representation for an image that works for
    the Encoder requires some thought.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，摘要架构使用了**双向长短期记忆网络**（**BiLSTMs**），其基本原理是相互之间靠得更近的单词在意义上也较为相似。BiLSTMs通过从两侧查看输入序列并生成编码表示来利用这一特性。为图像生成适合编码器的表示需要一些思考。
- en: 'A naïve solution for representing images as a sequence could be expressing
    them as a list of pixels. So, an image of size 28x28 pixels becomes a sequence
    of 784 tokens. When the tokens represent text, an Embedding layer learns the representation
    of each token. If this Embedding layer had a dimension of 64, then each token
    would be represented by a 64-dimensional vector. This embedding vector was learned
    during training. Extending our analogy of using a pixel as a token, a straightforward
    solution is to use the value of the Red/Green/Blue channels of the pixel in an
    image to generate a three-dimensional embedding. However, training these three
    dimensions does not sound like a logical approach. More importantly, pixels are
    laid out in a 2D representation, while the text is laid out in a 1D representation.
    This concept is illustrated in the following image. Words are related to words
    next to each other. When pixels are laid out in a sequence, the **data locality**
    of these pixels is broken since the content of a pixel is related to the pixels
    all around it, not just to the left and right of it. This idea is shown by the
    following super zoomed in image of a tulip:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个表示图像为序列的简单解决方案是将其表示为像素列表。因此，一个28x28像素的图像就变成了784个标记的序列。当这些标记代表文本时，嵌入层学习每个标记的表示。如果这个嵌入层的维度为64，那么每个标记将通过一个64维的向量来表示。这个嵌入向量是在训练过程中学习到的。继续延伸我们使用像素作为标记的类比，一种直接的解决方案是使用图像中每个像素的红/绿/蓝通道值来生成三维嵌入。然而，训练这三个维度似乎并不是一种合乎逻辑的方法。更重要的是，像素在2D表示中排布，而文本则是在1D表示中排布。这个概念在以下图像中得到了说明。单词与其旁边的单词相关。当像素以序列的形式排列时，这些像素的**数据局部性**被打破，因为像素的内容与其周围的所有像素相关，而不仅仅是与其左右相邻的像素相关。这个想法通过下面的图像展示出来：
- en: '![](img/B16252_07_04.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_07_04.png)'
- en: 'Figure 7.4: Data locality in text versus images'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4：文本与图像中的数据局部性
- en: 'Data locality and translation invariance are two critical properties of images.
    Translation invariance is the idea that an object can appear in various spots
    in an image. In a fully connected model, the model would try to learn the position
    of the object, which would prevent the model from generalizing. The specialized
    architecture of **Convolutional Neural Networks** (**CNNs**) can be used to exploit
    these properties and extract signals from the image. At a high level, we use CNNs,
    specifically the **ResNet50** architecture, to convert the image into a tensor
    that can be fed to a seq2seq architecture. Our model will combine the best of
    CNNs and RNNs to handle the image and text parts under the seq2seq model. The
    following diagram shows our architecture at a very high level:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 数据局部性和平移不变性是图像的两个关键特性。平移不变性是指一个物体可以出现在图像的不同位置。在一个全连接模型中，模型会试图学习物体的位置，这会阻止模型的泛化。**卷积神经网络**（**CNNs**）的专门架构可以用来利用这些特性并从图像中提取信号。总的来说，我们使用CNNs，特别是**ResNet50**架构，将图像转换为可以输入到seq2seq架构的张量。我们的模型将在seq2seq模型下结合CNNs和RNNs的优势来处理图像和文本部分。以下图示展示了我们架构的高层概述：
- en: '![](img/B16252_07_05.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_07_05.png)'
- en: 'Figure 7.5: High-level image captioning model architecture'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5：高级图像标注模型架构
- en: While a comprehensive explanation of CNNs is beyond the scope of this book,
    we will review the key concepts in short. Since we will be using a pre-trained
    CNN model, we won't have to go into much depth about CNNs. *Python Machine Learning,
    Third Edition*, published by Packt, is an excellent resource for reading up on
    CNNs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对CNNs的全面解释超出了本书的范围，但我们将简要回顾关键概念。由于我们将使用一个预训练的CNN模型，因此无需深入探讨CNNs的细节。*Python机器学习（第三版）*（由Packt出版）是一本阅读CNNs的优秀资源。
- en: In the previous chapter on text summarization, we built a seq2seq model with
    attention. In this chapter, we will build a Transformer model. Transformer models
    are currently state of the art in NLP. The Encoder part of the Transformer is
    the core of the **Bidirectional Encoder Representations from Transformers** (**BERT**)
    architecture. The Decoder part of the Transformer is the core of the **Generative
    Pre-trained Transformer** (**GPT**) family of architectures. There is a specific
    advantage of the Transformer architecture that is relevant to the image captioning
    problem. In the seq2seq architecture, we used BiLSTMS, which tries to learn relationships
    via co-occurrence. In the Transformer architecture, there is no recurrence. Instead,
    positional encodings and self-attention model relationships are made between inputs.
    This change enables us to feed in processed image patches as input and hope that
    the relationships between the image patches will be learned.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章的文本摘要中，我们构建了一个带有注意力机制的seq2seq模型。在这一章，我们将构建一个Transformer模型。Transformer模型目前是自然语言处理领域的最前沿技术。Transformer的编码器部分是**双向编码器表示（Bidirectional
    Encoder Representations from Transformers）**（**BERT**）架构的核心。Transformer的解码器部分是**生成式预训练Transformer**（**GPT**）系列架构的核心。Transformer架构有一个在图像字幕生成问题中尤为重要的特定优势。在seq2seq架构中，我们使用了BiLSTM，它尝试通过共现来学习关系。在Transformer架构中，没有递归。相反，使用位置编码和自注意力机制来建模输入之间的关系。这一变化使我们能够将处理后的图像补丁作为输入，并希望学习到图像补丁之间的关系。
- en: Implementing the image captioning model requires a large amount of code as we
    will implement several pieces, like pre-processing images, with ResNet50 and a
    complete implementation of Transformer architecture from scratch. This chapter
    contains much more code than the other chapters. We will rely on code fragments
    to highlight the most important aspects of the code rather than going over every
    line of code in detail, as we have been doing so far.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 实现图像字幕生成模型需要大量代码，因为我们将实现多个部分，例如使用ResNet50进行图像预处理，并从零开始完整实现Transformer架构。本章的代码量远远超过其他章节。我们将依赖代码片段来突出代码中的最重要部分，而不是像以前那样逐行详细讲解代码。
- en: 'The main steps of building our model are summarized here:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型的主要步骤总结如下：
- en: '**Downloading the data**: Given the large size of the dataset, this is a time-consuming
    activity.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**下载数据**：由于数据集的庞大体积，这是一个耗时的活动。'
- en: '**Pre-processing captions**: Since the captions are in JSON format, they are
    flattened into a CSV for easier processing.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预处理字幕**：由于字幕是JSON格式的，它们被平坦化为CSV格式，以便更轻松地处理。'
- en: '**Feature extraction**: We pass the image files through ResNet50 to extract
    features and save them to speed up training.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征提取**：我们通过ResNet50将图像文件传递来提取特征，并将其保存，以加速训练。'
- en: '**Transformer training**: A full Transformer model with positional encoding,
    multi-head attention, an Encoder, and a Decoder is trained on the processed data.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Transformer训练**：一个完整的Transformer模型，包括位置编码、多头注意力机制、编码器和解码器，在处理后的数据上进行训练。'
- en: '**Inference**: Use the trained model to caption some images!'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**推理**：使用训练好的模型为一些图像生成字幕！'
- en: '**Evaluating performance**: **Bilingual Evaluation Understudy** (**BLEU**)
    scores are used to compare the trained models with ground truth data.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估性能**：使用**双语评估替代法**（**Bilingual Evaluation Understudy**，简称**BLEU**）分数来比较训练模型与真实数据。'
- en: Let's start with the dataset first.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先从数据集开始。
- en: MS-COCO dataset for image captioning
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MS-COCO数据集用于图像字幕生成
- en: Microsoft published the **Common Objects in Context** or **COCO** dataset in
    2014\. All the versions of the dataset can be found at [cocodataset.org](http://cocodataset.org).
    The COCO dataset is a big dataset that's used for object detection, segmentation,
    and captioning, among other annotations. Our focus will be on the 2014 training
    and validation images, where five captions per image are available. There are
    roughly 83K images in the training set and 41K images in the validation set. The
    training and validation images and captions need to be downloaded from the COCO
    website.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 微软在2014年发布了**上下文中的常见物体**（**Common Objects in Context**，简称**COCO**）数据集。所有版本的数据集可以在[cocodataset.org](http://cocodataset.org)上找到。COCO数据集是一个大型数据集，广泛用于物体检测、分割和字幕生成等任务。我们的重点将放在2014年的训练和验证图像上，每个图像都有五个字幕。训练集大约有83K张图像，验证集有41K张图像。训练和验证图像及字幕需要从COCO网站下载。
- en: '**Large download warning**: The training image dataset is approximately 13
    GB, while the validation dataset is over 6 GB. The annotations for the image files,
    which include captions, are about 214 MB in size. Please be careful of your internet
    bandwidth usage and potential costs as you download this dataset.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**大文件下载警告**：训练集图像数据集大约为 13 GB，而验证集数据集超过 6 GB。图像文件的注释，包括标题，大小约为 214 MB。下载该数据集时，请小心你的网络带宽使用和潜在费用。'
- en: Google has also published a new Conceptual Captions dataset at [https://ai.google.com/research/ConceptualCaptions](https://ai.google.com/research/ConceptualCaptions).
    It contains over 3M images. Having a large dataset allows deep models to train
    better. There is a corresponding competition where you can submit your models
    and see how they compete with others.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Google 还发布了一个新的 Conceptual Captions 数据集，地址为 [https://ai.google.com/research/ConceptualCaptions](https://ai.google.com/research/ConceptualCaptions)。它包含超过
    300 万张图像。拥有一个大型数据集可以让深度模型更好地训练。还有一个相应的比赛，你可以提交你的模型，看看它与其他模型的表现如何。
- en: 'Given that these are large downloads, you may wish to use the download that''s
    the most comfortable to you. If `wget` is available on your environment, you could
    use it to download the files, like so:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些是大文件下载，你可能希望使用最适合你的下载方式。如果你的环境中有 `wget`，你可以使用它来下载文件，方法如下：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note that the annotations for the training and validation sets are in one compressed
    archive. Once the files have been downloaded, they need to be unzipped. Each of
    these compressed files creates its own folder and puts the contents in there.
    We will create a folder called `data` and move all the expanded contents inside
    it:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，训练集和验证集的注释文件是一个压缩包。下载文件后，需要解压。每个压缩文件都会创建一个文件夹，并将内容放入其中。我们将创建一个名为 `data`
    的文件夹，并将所有解压后的内容移动到其中：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'All the images are either in the `train2014` or `val2014` folder. The code
    for the initial pre-processing of the data is in the `data-download-preprocess.py`
    file. Captions for the training and validation images can be found in the `captions_train2014.json`
    or `captions_val2014.json` JSON file inside the `annotations` subfolder. Both
    of these files are in a similar format. The files have four main keys – info,
    image, license, and annotation. The image key contains a record per image, along
    with information about the size, URL, name, and a unique ID that is used to refer
    to that image in the dataset. Captions are stored as a tuple of the image ID and
    caption text, along with a unique ID for the caption. We use the Python `json`
    module to read and process these files:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图像都在 `train2014` 或 `val2014` 文件夹中。数据的初步预处理代码位于 `data-download-preprocess.py`
    文件中。训练和验证图像的标题可以在 `annotations` 子文件夹中的 `captions_train2014.json` 或 `captions_val2014.json`
    JSON 文件中找到。这两个文件的格式相似。文件中有四个主要键——info、image、license 和 annotation。image 键包含每个图像的记录，以及关于图像的大小、URL、名称和用于引用该图像的唯一
    ID。标题以图像 ID 和标题文本的元组形式存储，并带有一个用于标题的唯一 ID。我们使用 Python 的 `json` 模块来读取和处理这些文件：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Our objective is to produce a single simple file with two columns – one for
    the image file name and another containing the caption for that file. Note that
    the validation set contains half the number of images of the training set. In
    a seminal paper on captioning titled *Deep Visual-Semantic Alignment for Generating
    Image Descriptions*, Andrej Karpathy and Fei-Fei Li proposed training on all the
    training and validation images after reserving 5,000 images from the validation
    set for testing. We will follow this approach by processing the image names and
    IDs into a dictionary:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是生成一个包含两列的简单文件——一列是图像文件名，另一列是该文件的标题。请注意，验证集包含的图像数量是训练集的一半。在一篇关于图像标题生成的开创性论文《*深度视觉-语义对齐用于生成图像描述*》中，Andrej
    Karpathy 和 Fei-Fei Li 提出了在保留 5,000 张验证集图像用于测试后，训练所有的训练集和验证集图像。我们将通过将图像名称和 ID 处理成字典来遵循这种方法：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Since each image has five captions, the validation set cannot be split based
    on captions. Otherwise, there will be leakage of data from the training set into
    the validation/test set. In the preceding code, we reserved the last 5K images
    for the validation set.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个图像都有五个标题，验证集不能根据标题进行拆分。否则，会出现训练集数据泄漏到验证集/测试集的情况。在前面的代码中，我们保留了最后 5K 张图像用于验证集。
- en: 'Now, let''s go over the captions for the training and validation images and
    create a combined list. We will create empty lists to store the tuples of image
    paths and captions:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们查看训练集和验证集图像的标题，并创建一个合并的列表。我们将创建空列表来存储图像路径和标题的元组：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we will process all the training captions:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将处理所有的训练标签：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For the validation captions, the logic is similar, but we need to ensure that
    no captions are included for the images that have been reserved:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于验证标签，逻辑类似，但我们需要确保不为已预留的图像添加标签：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Hopefully, there should not be any errors. If you encounter errors, this could
    be due to corrupted downloads or errors while unzipping the files. The training
    dataset is shuffled to aid in training. Finally, two CSV files are persisted with
    the training and testing data:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 希望没有任何错误。如果遇到错误，可能是由于下载文件损坏或解压时出错。训练数据集会进行洗牌，以帮助训练。最后，会持久化保存两个CSV文件，分别包含训练数据和测试数据：
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: At this point, the data download and pre-processing phases are complete. The
    next step is to pre-process all the images using ResNet50 to extract features.
    Before we write the code for that, we will take a short detour and look at CNNs
    and the ResNet architecture. If you are already comfortable with CNNs, you may
    skip ahead to the code part.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，数据下载和预处理阶段已经完成。下一步是使用ResNet50对所有图像进行预处理，以提取特征。在我们编写相关代码之前，我们将稍作绕行，了解一下CNN和ResNet架构。如果你已经熟悉CNN，可以跳过这部分，直接进入代码部分。
- en: Image processing with CNNs and ResNet50
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CNN和ResNet50进行图像处理
- en: In the world of deep learning, specific architectures have been developed to
    handle specific modalities. CNNs have been incredibly successful in processing
    images and are the standard architecture for CV tasks. A good mental model for
    using a pre-trained model for extracting features from images is that of using
    pre-trained word embeddings like GloVe for text. In this particular case, we use
    a specific architecture called ResNet50\. While a comprehensive treatment of CNNs
    is outside the scope of this book, a brief overview of CNNs and ResNet will be
    provided in this section. If you are already comfortable with these concepts,
    you may skip ahead to the section titled *Image feature extraction with ResNet50*.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的世界里，已经开发出特定的架构来处理特定的模态。CNN在处理图像方面取得了巨大的成功，并且是计算机视觉任务的标准架构。使用预训练模型提取图像特征的一个很好的思维模型是，将其类比为使用预训练的词向量（如GloVe）来处理文本。在这个特定的案例中，我们使用一种叫做ResNet50的架构。虽然本书并不深入讲解CNN的所有细节，但这一节将简要概述CNN和ResNet。如果你已经熟悉这些概念，可以跳到*使用ResNet50进行图像特征提取*这一部分。
- en: CNNs
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNN（卷积神经网络）
- en: 'CNNs are an architecture designed to learn from the following key properties,
    which are relevant to image recognition:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: CNN（卷积神经网络）是一种旨在从以下关键特性中学习的架构，这些特性与图像识别相关：
- en: '**Data locality**: The pixels in an image are highly correlated to the pixels
    around them.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据局部性**：图像中的像素与周围像素高度相关。'
- en: '**Translation invariance**: An object of interest, for example, a bird, may
    appear at different places in an image. The model should be able to identify the
    object, irrespective of the object''s position in the image.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平移不变性**：一个感兴趣的物体，例如鸟，可能出现在图像的不同位置。模型应该能够识别该物体，而不管它在图像中的位置如何。'
- en: '**Scale invariance**: An object of interest may have a smaller or large size,
    depending on the zoom. Ideally, the model should be able to identify objects of
    interest in an image, irrespective of their size.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**尺度不变性**：感兴趣的物体可能会根据缩放显示为较小或较大的尺寸。理想情况下，模型应该能够识别图像中的物体，而不管它们的尺寸如何。'
- en: Convolution and pooling layers are key components that aid CNNs in extracting
    features from images.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层和池化层是帮助CNN从图像中提取特征的关键组件。
- en: Convolutions
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积操作
- en: 'A convolution is a mathematical operation that is performed on patches taken
    from an image with a filter. A filter is a matrix, usually square and with 3x3,
    5x5, and 7x7 as common dimensions. The following image shows an example of a 3x3
    convolution matrix applied to a 5x5 image. The image patches are taken from left
    to right and then top to bottom. The number of pixels this patch shifts by every
    step is called the **stride length**. A stride length of 1 in a horizontal and
    vertical direction reduces a 5x5 image to a 3x3 image, as shown here:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是一种数学运算，它在从图像中提取的区域上执行，使用的是过滤器。过滤器是一个矩阵，通常是方形的，常见的尺寸为3x3、5x5和7x7。下图展示了一个3x3卷积矩阵应用于5x5图像的例子。图像区域从左到右、从上到下提取。每次步进的像素数被称为**步幅**。在水平和垂直方向上，步幅为1时，会将一个5x5的图像缩减为3x3的图像，如下所示：
- en: '![A close up of a green screen  Description automatically generated](img/B16252_07_06.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![一张绿色屏幕的特写图像 自动生成的描述](img/B16252_07_06.png)'
- en: 'Figure 7.6: Example of a convolution operation'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6：卷积操作示例
- en: 'The specific filter that was applied here is an edge detection filter. Prior
    to CNNs, CV relied heavily on handcrafted filters. Sobel filters are an example
    of a special filter for the purpose of edge detection. The `convolution-example.ipynb`
    notebook provides an example of detecting edges using the Sobel filter. The code
    is quite straightforward. After the imports, the image file is loaded and converted
    into a grayscale image:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里应用的特定滤波器是边缘检测滤波器。在CNN出现之前，计算机视觉（CV）在很大程度上依赖于手工制作的滤波器。Sobel滤波器是用于边缘检测的特殊滤波器之一。`convolution-example.ipynb`笔记本提供了使用Sobel滤波器检测边缘的示例。代码非常简单。在导入模块后，图像文件被加载并转换为灰度图像：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we define and apply the Sobel filters to the image:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义并将Sobel滤波器应用到图像中：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The original image, along with the intermediate versions, are shown in the
    following image:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像及其中间版本如下图所示：
- en: '![A screen shot of a computer  Description automatically generated](img/B16252_07_07.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图 描述自动生成](img/B16252_07_07.png)'
- en: 'Figure 7.7: Edge detection using Sobel filters'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7：使用Sobel滤波器进行边缘检测
- en: Constructing such filters is very tedious. However, CNNs can learn many such
    filters by treating the filter matrices as learnable parameters. CNNs often pass
    an image through hundreds or thousands of such filters, referred to as channels,
    and stack them together. You can think of each filter as detecting some features,
    like vertical lines, horizontal lines, arcs, circles, trapezoids, and so on. However,
    the magic happens when multiple such layers are put together. Stacking multiple
    layers leads to learning hierarchical representations. An easy way to understand
    this concept is by imagining that earlier layers are learning simple shapes like
    lines and arcs, middle layers are learning shapes like circles and hexagons, and
    the top layers are learning complex objects like stop signs and steering wheels.
    The convolution operation is the key innovation that exploits data locality and
    extracts features that enable translation invariance.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 构建这样的滤波器是非常繁琐的。然而，CNN（卷积神经网络）可以通过将滤波器矩阵视为可学习的参数来学习许多这样的滤波器。CNN通常会通过数百或数千个这样的滤波器（称为通道）处理一张图像，并将它们堆叠在一起。你可以将每个滤波器视为检测某些特征，如竖直线、水平线、弧形、圆形、梯形等。然而，当多个这样的层组合在一起时，魔法就发生了。堆叠多个层导致了分层表示的学习。理解这一概念的一个简单方法是，想象早期的层学习的是简单的形状，如线条和弧形；中间层学习的是圆形和六边形等形状；顶层学习的是复杂的物体，如停车标志和方向盘。卷积操作是关键创新，它利用数据的局部性并提取出特征，从而实现平移不变性。
- en: A consequence of this layering is the amount of data flowing through the model
    increasing. Pooling is an operation that helps reduce the dimensions of the data
    flowing through and further highlights these features.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分层的结果是模型中流动的数据量增加。池化是一种帮助减少通过数据的维度并进一步突出这些特征的操作。
- en: Pooling
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 池化
- en: Once the values from the convolution operation have been computed, a pooling
    operation can be applied to patches to further concentrate the signal in the image.
    The most common form of pooling is called **Max pooling** and is demonstrated
    in the following diagram. It is as simple as taking the maximum value in a patch.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦卷积操作的值被计算出来，就可以对图像中的补丁应用池化操作，以进一步集中图像中的信号。最常见的池化形式称为**最大池化**，并在以下图示中展示。这就像是在一个补丁中取最大值一样简单。
- en: 'The following diagram shows max pooling on non-overlapping 2x2 patches:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示显示了在不重叠的2x2补丁上进行最大池化：
- en: '![A close up of a colorful background  Description automatically generated](img/B16252_07_08.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![色彩丰富的背景特写 描述自动生成](img/B16252_07_08.png)'
- en: 'Figure 7.8: Max pooling operation'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8：最大池化操作
- en: Another way to pool is by averaging the values. While pooling reduces the complexity
    and computation load, it also helps modestly with scale invariance. However, there
    is a chance that such a model overfits and does not generalize well. Dropout is
    a technique that helps with regularization and enables such models to generalize
    better.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种池化方法是通过对值进行平均。虽然池化降低了复杂性和计算负担，但它也在一定程度上帮助了尺度不变性。然而，这样的模型有可能会出现过拟合，并且无法很好地泛化。Dropout（丢弃法）是一种有助于正则化的技术，它使得此类模型能够更好地泛化。
- en: Regularization with dropout
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用dropout进行正则化
- en: 'You may recall that we used dropout settings in previous chapters with the
    LSTM and BiLSTM settings. The core idea behind dropout is shown in the following
    diagram:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，在之前的章节中，我们在LSTM和BiLSTM设置中使用了dropout设置。dropout的核心思想如下图所示：
- en: '![A close up of a logo  Description automatically generated](img/B16252_07_09.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![Logo的特写，描述自动生成](img/B16252_07_09.png)'
- en: 'Figure 7.9: Dropout'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9：丢弃法
- en: Rather than connecting every unit from a lower layer to every unit in the next
    higher layer of the model, some of the connections are randomly dropped during
    training time. Inputs are dropped only during training time. Since dropping inputs
    reduces the total input reaching a node compared to test/inference time, inputs
    are upscaled in the proportion of dropout to ensure the relative magnitudes are
    preserved. Dropping some of the inputs during training forces the model to learn
    more from each of the inputs. This is because it cannot rely on the presence of
    a specific input. This helps the network build resilience to missing inputs and
    consequently helps generalize the models.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 与其将低层的每个单元连接到模型中每个更高层的单元，不如在训练期间随机丢弃一些连接。输入仅在训练期间被丢弃。由于丢弃输入会减少到达节点的总输入，相对于测试/推理时间，需要按丢弃率上调输入，以确保相对大小保持一致。训练期间丢弃一些输入迫使模型从每个输入中学习更多内容，因为它不能依赖于特定输入的存在。这有助于网络对缺失输入的鲁棒性，从而帮助模型的泛化。
- en: A combination of these techniques helped build deeper and deeper networks. A challenge
    that showed up as networks got deeper was that the signal from the inputs became
    quite small in the higher layers. Residual connections is a technique that helps
    deal with this problem.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术的结合帮助构建了越来越深的网络。随着网络变得越来越深，出现的一个挑战是输入信号在更高层变得非常小。残差连接是一种帮助解决这个问题的技术。
- en: Residual connections and ResNets
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 残差连接与ResNet
- en: Intuition suggests that adding more layers should make performance better. A deeper
    network has more model capacity, so it should be able to model more complex distributions
    compared to shallower networks. As deeper and deeper models were built, a degradation
    in accuracy was observed. Since the reduction happened even on the training data,
    overfitting can be ruled out as a probable cause. As inputs pass through more
    and more layers, the optimizers have a harder time adjusting the gradients to
    the point where learning is impaired in the model. Kaiming He and his collaborators
    published the ResNet architecture in their seminal paper titled *Deep Residual
    Learning for Image Recognition*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 直觉上，增加更多层应该能提高性能。更深的网络拥有更多的模型容量，因此它应该能够比浅层网络更好地建模复杂的分布。然而，随着更深的模型的建立，精度出现了下降。由于这种下降甚至出现在训练数据上，因此可以排除过拟合作为可能的原因。随着输入通过越来越多的层，优化器在调整梯度时变得越来越困难，以至于学习在模型中受到抑制。Kaiming
    He及其合作者在他们的开创性论文《*深度残差学习用于图像识别*》中发布了ResNet架构。
- en: We must understand residual connections before understanding ResNets. The core
    concept of the residual connection is shown in the following diagram. In a regular
    dense layer, the input is first multiplied by the weights. Then, biases are added
    in, which is a linear operation. The output is passed through an activation function,
    like ReLU, which introduces non-linearity in the layer. The output from the activation
    function is the final output of the layer.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解ResNet之前，我们必须理解残差连接。残差连接的核心概念如下图所示。在常规的密集层中，输入首先与权重相乘。然后，添加偏置，这是一个线性操作。输出通过激活函数（如ReLU），这为层引入了非线性。激活函数的输出是该层的最终输出。
- en: 'However, residual connections introduce a summation in-between the linear computation
    and the activation function, as shown on the right-hand side of the following
    diagram:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，残差连接在线性计算和激活函数之间引入了求和，如下图右侧所示：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_07_10.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![手机屏幕截图，描述自动生成](img/B16252_07_10.png)'
- en: 'Figure 7.10: A conceptual residual connection'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10：一个概念性的残差连接
- en: 'Note that the preceding diagram is only for illustrating the core concept behind
    residual connections. In ResNets, the residual connection is made between multiple
    blocks. The following diagram shows the basic building blocks of ResNet50, also
    referred to as the bottleneck design. This design is called the bottleneck design
    because the 1x1 convolution blocks reduce the dimensions of the inputs before
    passing them to the 3x3 convolution. The last 1x1 block scales the inputs out
    again for the next layer:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面的图示仅用于说明残差连接背后的核心概念。在 ResNet 中，残差连接发生在多个模块之间。下图展示了 ResNet50 的基本构建模块，也称为瓶颈设计。之所以称之为瓶颈设计，是因为
    1x1 卷积块在将输入传递到 3x3 卷积之前会减少输入的维度。最后一个 1x1 块再次将输入的维度扩大，以便传递到下一层：
- en: '![A close up of a keyboard  Description automatically generated](img/B16252_07_11.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![键盘的特写  描述自动生成](img/B16252_07_11.png)'
- en: 'Figure 7.11: ResNet50 bottleneck building block'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11：ResNet50 瓶颈构建模块
- en: 'ResNet50 is composed of several such blocks stacked on top of each other. There
    are four groups, each consisting of three to seven such blocks. **BatchNorm**
    or batch normalization was proposed by Sergey Ioffe and Christian Szegedy in their
    paper titled *Batch Normalization: Accelerating Deep Network Training By Reducing
    Internal Covariate Shift* in 2015\. Batch normalization aims to reduce the variance
    of the outputs coming from one layer being fed into the next layer. By reducing
    this variance, BatchNorm acts like L2 regularization, which attempts to do the
    same thing by adding the penalties of the magnitude of the weights to the cost
    function. The main motivation of BatchNorm is to efficiently backpropagate gradient
    updates through a large number of layers, while minimizing the risk that this
    update could result in divergence. In stochastic gradient descent, gradients are
    used to update the weights of all the layers at the same time, assuming that the
    output of one layer doesn''t impact any other layers. However, this is not a completely
    valid assumption. For an *n*-layer network, computing this would need *n*th order
    gradients, which is intractable. Instead, batch-norm is used, which works on one
    mini-batch at a time and the constraints of the updates to reduce this unwanted
    shift in the distribution of weights. It does this by normalizing the outputs
    before they are fed into the next layer.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 'ResNet50 由几个这样的模块堆叠而成，共有四个组，每组包含三个到七个模块。**BatchNorm**（批量归一化）是 Sergey Ioffe
    和 Christian Szegedy 在 2015 年发表的论文 *Batch Normalization: Accelerating Deep Network
    Training By Reducing Internal Covariate Shift* 中提出的。批量归一化旨在减少从一层输出到下一层输入的输出方差。通过减少这种方差，BatchNorm
    起到了类似 L2 正则化的作用，后者通过向损失函数中添加权重大小的惩罚来实现类似的效果。BatchNorm 的主要动机是有效地通过大量层反向传播梯度更新，同时最小化这种更新导致发散的风险。在随机梯度下降中，梯度用于同时更新所有层的权重，假设一层的输出不会影响其他任何层。然而，这并不是一个完全有效的假设。对于一个
    *n* 层网络，计算这个将需要 *n* 阶梯度，这在计算上是不可行的。相反，使用了批量归一化，它一次处理一个小批量，并通过限制更新来减少权重分布中的这种不必要的变化。它通过在将输出传递到下一层之前进行归一化来实现这一点。'
- en: The last two layers of ResNet50 are dense layers that classify the outputs from
    the last block into an object category. Covering ResNets comprehensively is a
    tough ask, but hopefully, this crash course on CNNs and ResNets has given you
    enough background on how they work. You are encouraged to read the referenced
    papers and *Deep Learning with TensorFlow 2 and Keras, Second Edition*, published
    by Packt, for a detailed treatment of this topic. Fortunately for us, TensorFlow
    provides a pre-trained ResNet50 model that is ready for use. In the next section,
    we'll use this pre-trained ResNet50 model for extracting image features.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet50 的最后两层是全连接层，将最后一个模块的输出分类为一个物体类别。全面了解 ResNet 是一项艰巨的任务，但希望通过本次关于卷积神经网络（CNN）和
    ResNet 的速成课程，您已经获得了足够的背景知识来理解它们的工作原理。鼓励您阅读参考文献和 *《深度学习与 TensorFlow 2 和 Keras 第二版》*，该书由
    Packt 出版，提供了更为详细的内容。幸运的是，TensorFlow 提供了一个预训练的 ResNet50 模型，可以直接使用。在接下来的部分中，我们将使用这个预训练的
    ResNet50 模型提取图像特征。
- en: Image feature extraction with ResNet50
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ResNet50 进行图像特征提取
- en: ResNet50 models are trained on the ImageNet dataset. This dataset contains millions
    of images in over 20,000 categories. The large-scale visual recognition challenge,
    ILSVRC, focuses on the top 1,000 categories for models to compete on recognizing
    images. Consequently, the top layers of the ResNet50 that perform classification
    have a dimension of 1,000\. The idea behind using a pre-trained ResNet50 model
    is that it is already able to parse out objects that may be useful in image captioning.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet50 模型是在 ImageNet 数据集上训练的。该数据集包含超过 20,000 个类别的数百万张图片。大规模视觉识别挑战赛 ILSVRC
    主要聚焦于 1,000 个类别，模型们在这些类别上进行图像识别竞赛。因此，执行分类的 ResNet50 顶层具有 1,000 的维度。使用预训练的 ResNet50
    模型的想法是，它已经能够解析出在图像描述中可能有用的对象。
- en: The `tensorflow.keras.applications` package provides pre-trained models like
    ResNet50\. At the time of writing, all the pre-trained models provided are related
    to CV. Loading up the pre-trained model is quite easy. All the code for this section
    is in the `feature-extraction.py` file in this chapter's folder on GitHub. The
    main reason for using a separate file is that it gives us the ability to run feature
    extraction as a script.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`tensorflow.keras.applications` 包提供了像 ResNet50 这样的预训练模型。写本文时，所有提供的预训练模型都与计算机视觉（CV）相关。加载预训练模型非常简单。此部分的所有代码都位于本章
    GitHub 文件夹中的 `feature-extraction.py` 文件中。使用单独文件的主要原因是，它让我们能够以脚本的方式运行特征提取。'
- en: 'Given that we will be processing over 100,000 images, this process may take
    a while. CNNs benefit greatly from a GPU in computation. Let''s get into the code
    now. First, we must set up the paths for the CSV file we created from the JSON
    annotations in the previous chapter:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们将处理超过 100,000 张图片，这个过程可能需要一段时间。卷积神经网络（CNN）在计算上受益于 GPU。现在让我们进入代码部分。首先，我们必须设置从上一章的
    JSON 注释中创建的 CSV 文件的路径：
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'ResNet50 expects each image to be 224x224 pixels with three channels. The input
    images from the COCO set have different sizes. Hence, we must convert the input
    files into the standard that ResNet was trained on:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet50 期望每张图像为 224x224 像素，并具有三个颜色通道。来自 COCO 数据集的输入图像具有不同的尺寸。因此，我们必须将输入文件转换为
    ResNet 训练时使用的标准：
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The highlighted code shows a special pre-processing function provided by the
    ResNet50 package. The pixels in the input image are loaded into an array via the
    `decode_jpeg()` function. Each pixel has a value between 0 and 255 for each color
    channel. The `preprocess_input()` function normalizes the pixel values so that
    their mean is 0\. Since each input image has five captions, we should only process
    the unique images in the dataset:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 高亮显示的代码展示了 ResNet50 包提供的一个特殊预处理函数。输入图像中的像素通过 `decode_jpeg()` 函数加载到数组中。每个像素在每个颜色通道的值介于
    0 和 255 之间。`preprocess_input()` 函数将像素值归一化，使其均值为 0。由于每张输入图像都有五个描述，我们只应处理数据集中独特的图像：
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we must convert the dataset into a `tf.dat.Dataset`, which makes it easier
    to batch and process the input files using the convenience function defined previously:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须将数据集转换为 `tf.dat.Dataset`，这样可以更方便地批量处理和使用之前定义的便捷函数处理输入文件：
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For efficiently processing and generating features, we must process 16 image
    files at a time. The next step is loading a pre-trained ResNet50 model:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了高效地处理和生成特征，我们必须一次处理 16 张图片。下一步是加载一个预训练的 ResNet50 模型：
- en: '[PRE15]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The preceding output has been abbreviated for brevity. The model contains over
    23 million trainable parameters. We don't need the top classification layer as
    we are using the model for feature extraction. We defined a new model with the
    input and output layer. Here, we took the output from the last layer. We could
    take output from different parts of ResNet by changing the definition of the `hidden_layer`
    variable. In fact, this variable can be a list of layers, in which case the output
    of the `features_extract` model will be the output from each of the layers in
    the list.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出已简化以节省篇幅。该模型包含超过 2300 万个可训练参数。我们不需要顶层的分类层，因为我们使用该模型进行特征提取。我们定义了一个新的模型，包括输入和输出层。在这里，我们取了最后一层的输出。我们也可以通过改变
    `hidden_layer` 变量的定义来从 ResNet 的不同部分获取输出。实际上，这个变量可以是一个层的列表，在这种情况下，`features_extract`
    模型的输出将是列表中每一层的输出。
- en: 'Next, a directory must be set up to store the extracted features:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，必须设置一个目录来存储提取的特征：
- en: '[PRE17]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The feature extraction model can work on batches of images and predict the
    output. The output is 2,048 patches of 7x7 pixels for each image. If a batch of
    16 images is supplied, then the output from the model will be a tensor of dimensions
    [16, 7, 7, 2048]. We store the features of each image file as a separate file
    while flattening the dimensions to [49, 2048]. Each image has now been converted
    into a sequence of 49 pixels, with an embedding size of 2,048\. The following
    code performs this action:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取模型可以处理图像批次并预测输出。每张图像的输出为2,048个7x7像素的补丁。如果输入的是16张图像的批次，那么模型的输出将是一个[16, 7,
    7, 2048]维度的张量。我们将每张图像的特征存储为单独的文件，同时将维度展平为[49, 2048]。每张图像现在已经被转换成一个包含49个像素的序列，嵌入大小为2,048。以下代码执行此操作：
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This could be a time-consuming operation, depending on your computing environment.
    On my Ubuntu Linux box with an RTX 2070 GPU, this took ~23 minutes.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是一个耗时的操作，具体取决于你的计算环境。在我的Ubuntu Linux系统和RTX 2070 GPU的配置下，这个过程花费了大约23分钟。
- en: 'The last step in data pre-processing is to train the Subword Encoder. This
    part should be quite familiar to you as it is identical to what we''ve done in
    previous chapters:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理的最后一步是训练子词编码器。你应该对这部分非常熟悉，因为它与我们在前几章中做的完全相同：
- en: '[PRE19]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that we included two special tokens to signal the start and end of the
    sequences. You may recall this technique from *Chapter 5*, *Generating Text with
    RNNs and GPT-2*. Here, we used a slightly different way of accomplishing the same
    technique to show how you can accomplish the same objective in different ways.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们包含了两个特殊的标记来表示序列的开始和结束。你可能会回忆起在*第五章*中，*使用RNN和GPT-2生成文本*时提到过这种技术。在这里，我们采用了稍微不同的方式来实现相同的技术，目的是展示如何用不同的方式实现相同的目标。
- en: With that, pre-processing and feature extraction is complete. The next step
    is defining the Transformer model. Then, we will be ready to train the model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，预处理和特征提取已经完成。接下来的步骤是定义Transformer模型。然后，我们就可以开始训练模型了。
- en: The Transformer model
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer模型
- en: The Transformer model was discussed in *Chapter 4*, *Transfer Learning with
    BERT*. It was inspired by the seq2seq model and has an Encoder and a Decoder part.
    Since the Transformer model does not rely on RNNs, input sequences need to be
    annotated with positional encodings, which allow the model to learn about the
    relationships between inputs. Removing recurrence improves the speed of the model
    vastly while reducing the memory footprint. This innovation of the Transformer
    model has made very large-sized models such as BERT and GPT-3 possible. The Encoder
    part of the Transformer model was shown in the aforementioned chapter. The full
    Transformer model was shown in *Chapter 5*, *Generating Text with RNNs and GPT-2*.
    We will start with a modified version of the full Transformer. Specifically, we
    will modify the Encoder part of the Transformer to create a visual Encoder, which
    takes image data as input instead of text sequences. There are some other small
    modifications to be made to accommodate images as input to the Encoder. The Transformer
    model we are going to build is shown in the following diagram. The main difference
    here is how the input sequence is encoded. In the case of text, we will tokenize
    the text using a Subword Encoder and pass it through an Embedding layer, which
    is trainable.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型在*第四章*中有讨论，*使用BERT进行迁移学习*。它的灵感来源于seq2seq模型，并包含了编码器（Encoder）和解码器（Decoder）部分。由于Transformer模型不依赖于RNN，输入序列需要使用位置编码进行标注，这使得模型能够学习输入之间的关系。移除循环结构显著提高了模型的速度，并减少了内存占用。Transformer模型的这一创新使得像BERT和GPT-3这样的大型模型成为可能。Transformer模型的编码器部分已在前述章节中展示。完整的Transformer模型已在*第五章*，*使用RNN和GPT-2生成文本*中展示。我们将从一个修改版的完整Transformer模型开始。具体来说，我们将修改Transformer的编码器部分，创建一个视觉编码器，该编码器以图像数据作为输入，而不是文本序列。为了适应图像作为输入，编码器需要做一些其他小的修改。我们将要构建的Transformer模型在下图中展示。这里的主要区别在于输入序列的编码方式。在文本的情况下，我们将使用子词编码器对文本进行分词，并将其通过一个可训练的嵌入层。
- en: 'As training proceeds, the embeddings of the tokens are also learned. In the
    case of image captioning, we will pre-process the images into a sequence of 49
    pixels, each with an "embedding" size of 2,048\. This actually simplifies padding
    the inputs. All the images are pre-processed so that they''re the same length.
    Consequently, padding and masking the inputs is not required:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练的进行，标记的嵌入（embeddings）也在不断学习。在图像描述的情况下，我们将图像预处理为一个由 49 个像素组成的序列，每个像素的“嵌入”大小为
    2,048。这样实际上简化了输入的填充处理。所有图像都经过预处理，使其具有相同的长度。因此，不需要填充和掩码输入：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_07_12.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![一张手机的截图，自动生成的描述](img/B16252_07_12.png)'
- en: 'Figure 7.12: Transformer model with a visual Encoder'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12：带有视觉编码器的 Transformer 模型
- en: 'The following pieces of code need to be implemented to build the Transformer
    model:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码段需要实现才能构建 Transformer 模型：
- en: Positional encoding of the inputs, along with input and output masks. Our inputs
    are of a fixed length, but the output and captions are of a variable length.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入的位置信息编码，以及输入和输出的掩码。我们的输入是固定长度的，但输出和描述是可变长度的。
- en: Scaled dot-product attention and multi-head attention to enable the Encoders
    and Decoders to focus on specific aspects of the data.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放点积注意力和多头注意力，使编码器和解码器能够专注于数据的特定方面。
- en: An Encoder that consists of multiple repeating blocks.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个由多个重复块组成的编码器。
- en: A Decoder that uses the outputs from the Encoder through its repeating blocks.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个使用编码器输出的解码器，通过其重复块进行操作。
- en: The code for the Transformer has been taken from the TensorFlow tutorial titled
    *Transformer model for language understanding*. We will be using this code as
    the base and adapting it for the image captioning use case. One of the beautiful
    things about the Transformer architecture is that if we can cast a problem as
    a sequence-to-sequence problem, then we can apply the Transformer model. As we
    describe the implementation, the main points of the code will be highlighted.
    Note that the code for this section is in the `visual_transformer.py` file.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 的代码来自于 TensorFlow 教程，标题为 *Transformer 模型用于语言理解*。我们将使用这段代码作为基础，并将其适配于图像描述的应用场景。Transformer
    架构的一个优点是，如果我们能将问题转化为序列到序列的问题，就可以应用 Transformer 模型。在描述实现过程时，代码的主要要点将被突出展示。请注意，本节的代码位于
    `visual_transformer.py` 文件中。
- en: Implementing the full Transformer model does take a little bit of code. If you
    are already familiar with the Transformer model or want to only know where our
    model differs from the standard Transformer model, please focus on the next section
    and the *VisualEncoder* section. You can read the rest of the sections at your
    leisure.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 实现完整的 Transformer 模型确实需要一些代码。如果你已经熟悉 Transformer 模型，或者只是想了解我们的模型与标准 Transformer
    模型的不同之处，请专注于下一节和 *VisualEncoder* 部分。其余部分可以在你有空时阅读。
- en: Positional encoding and masks
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置编码和掩码
- en: Transformer models don't use RNNs. This allows them to compute all the outputs
    in one step, leading to significant improvements in speed and also the ability
    to learn dependencies across long inputs. However, it comes at the cost of the
    model not knowing anything about the relationship between neighboring words or
    tokens. A positional encoding vector, with values for the odd and even positions
    of the tokens to help the model learn relationships between the positions of inputs,
    helps compensate for the lack of information about the ordering of tokens.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型不使用 RNN。这使得它们能够在一步计算中得出所有输出，从而显著提高了速度，并且能够学习长输入之间的依赖关系。然而，这也意味着模型无法了解相邻单词或标记之间的关系。为了弥补缺乏关于标记顺序的信息，使用了位置编码向量，其中包含奇数和偶数位置的值，帮助模型学习输入位置之间的关系。
- en: Embeddings help place tokens that are similar in meaning close to each other
    in the embedding space. Positional encodings put tokens closer to each other based
    on their position in the sentence. Put together, the two are quite powerful.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入帮助将含义相似的标记放置在嵌入空间中彼此靠近的位置。位置编码根据标记在句子中的位置将标记相互靠近。将两者结合使用非常强大。
- en: 'In image captioning, this is important for captions. Technically, we don''t
    need to provide these positional encodings for the image inputs as ResNet50 should
    have produced appropriate patches. Positional encoding can, however, still be
    used for the inputs as well. Positional encoding uses a *sin* function for even
    positions and a *cos* function for odd positions. The formula for computing the
    encodings for a position is:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像描述中，这对描述是非常重要的。从技术上讲，对于图像输入，我们不需要提供这些位置编码，因为ResNet50应该已经生成了适当的补丁。然而，位置编码仍然可以用于输入。位置编码对于偶数位置使用*sin*函数，对于奇数位置使用*cos*函数。计算某个位置编码的公式如下：
- en: '![](img/B16252_07_001.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_07_001.png)'
- en: 'Here, *w*[i] is defined as:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*w*[i]定义为：
- en: '![](img/B16252_07_002.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_07_002.png)'
- en: In the preceding formula, *pos* refers to the position of a given token, *d*[model]
    refers to the dimensions of the embeddings, and *i* is the specific dimension
    being computed. The positional encoding process produces a vector with the same
    dimensions as the embedding for each token. You may be wondering why this complex
    formulation is used for computing these positional encodings. Wouldn't numbering
    the tokens from one side to the other suffice? It turns out that the positional
    encoding algorithm must have a few characteristics. First, the values must generalize
    easily to sequences of a variable length. Using a straight-up numbering scheme
    would prevent inputs that have sequences longer than those in the training data.
    The output should be unique for each token's position. Furthermore, the distance
    between any two positions should be consistent across different lengths of input
    sequences. This formulation is relatively simple to implement. The code for this
    is in the Positional Encoder section of the file.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，*pos*表示给定标记的位置，*d*[model]表示嵌入的维度，*i*是正在计算的特定维度。位置编码过程为每个标记生成与嵌入相同维度的向量。你可能会想，为什么要使用这么复杂的公式来计算这些位置编码？难道只是从一端到另一端编号标记不就行了吗？事实证明，位置编码算法必须具备一些特性。首先，数值必须能够轻松地推广到长度可变的序列。使用简单的编号方案会导致输入的序列长度超过训练数据时无法处理。此外，输出应该对每个标记的位置是唯一的。而且，不同长度的输入序列中，任何两个位置之间的距离应该保持一致。这种公式相对简单易实现。相关的代码在文件中的位置编码器部分。
- en: 'First, we must compute the *angle*, as shown in the preceding *w*[i] formula,
    like so:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须计算*角度*，如前面的*w*[i]公式所示，如下所示：
- en: '[PRE20]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, we must compute the vector of positional encodings:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须计算位置编码的向量：
- en: '[PRE21]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The next step is to compute the masks for input and output. Let''s focus on
    the Decoder for a second. Since we are not using an RNN, the entire output is
    fed to the Decoder at once. However, we don''t want the Decoder to look at data
    from future timesteps. So, the outputs must be masked. In terms of the Encoder,
    masks are needed if the input is padded to a fixed length. However, in our case,
    the inputs are always exactly a length of 49\. So, the mask is a fixed vector
    of ones:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是计算输入和输出的掩码。让我们暂时关注解码器。由于我们没有使用RNN，整个输出一次性传入解码器。然而，我们不希望解码器看到未来时间步的数据。所以，输出必须被掩蔽。就编码器而言，如果输入被填充到固定长度，则需要掩码。然而，在我们的情况下，输入总是正好是49的长度。所以，掩码是一个固定的全1向量：
- en: '[PRE22]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The first method is used to mask inputs if they are padded. This method has
    been included for the sake of completeness, but you will see later that we pass
    it a sequence of ones. So, all this method does is reshape the masks. The second
    mask function is used for masking Decoder inputs so that it can only see the positions
    it has generated.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方法用于掩盖填充的输入。这个方法是为了完整性而包括的，但稍后你会看到我们传递给它的是一个由1组成的序列。所以，这个方法的作用只是重新调整掩码。第二个掩码函数用于掩蔽解码器的输入，使其只能看到自己生成的位置。
- en: The layers of the transfer Encoder and Decoder use a specific form of attention.
    This is a fundamental building block of the architecture and will be implemented
    next.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器的编码器和解码器层使用特定形式的注意力机制。这是架构的基本构建块，接下来将进行实现。
- en: Scaled dot-product and multi-head attention
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩放点积和多头注意力
- en: The purpose of the attention function is to match a query to a set of key-value
    pairs. The output is a sum of the values, weighted by the correspondence between
    the query and the key. multi-head attention learns multiple ways to compute the
    scaled dot-product attention and combines it.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力函数的目的是将查询与一组键值对进行匹配。输出是值的加权和，权重由查询和键之间的匹配度决定。多头注意力学习多种计算缩放点积注意力的方式，并将它们结合起来。
- en: Scaled dot-product attention is computed by multiplying the query vector by
    the key vector. This product is scaled by the square root of the dimensions of
    the query and key. Note that this formulation assumes that the key and query vectors
    have the same dimensions. Practically, the dimensions of the query, key, and value
    vectors are all set to the size of the embedding.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放点积注意力通过将查询向量与键向量相乘来计算。这个乘积通过查询和键的维度的平方根进行缩放。注意，这个公式假设键和查询向量具有相同的维度。实际上，查询、键和值向量的维度都设置为嵌入的大小。
- en: 'This was referred to as *d*[model] in the position encoding. After computing
    the scaled product of the key and query vector, a softmax is applied, and the
    result of the softmax is multiplied by the value vector. A mask is used to mask
    the product of the query and keys:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在位置编码中，这被称为*d*[model]。在计算键和值向量的缩放点积之后，应用softmax，softmax的结果再与值向量相乘。使用掩码来遮盖查询和键的乘积。
- en: '[PRE23]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Multi-ahead attention concatenates outputs from multiple scaled dot-product
    attention units and passes them through a linear layer. The dimensions of the
    embedding inputs are divided by the number of heads to compute the dimensions
    of the key and value vectors. Multi-head attention is implemented as a custom
    layer. First, we must create the constructor:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力将来自多个缩放点积注意力单元的输出进行拼接，并通过一个线性层传递。嵌入输入的维度会被头数除以，用于计算键和值向量的维度。多头注意力实现为自定义层。首先，我们必须创建构造函数：
- en: '[PRE24]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Note the `assert` statement that is highlighted. When the Transformer model
    is instantiated, it is vital to choose some parameters so that the number of heads
    divides the model size or embedding dimensions completely. The main computation
    of this layer is in the `call()` function:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，突出显示的`assert`语句。当实例化Transformer模型时，选择某些参数至关重要，以确保头数能够完全除尽模型大小或嵌入维度。此层的主要计算在`call()`函数中：
- en: '[PRE25]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The three highlighted rows show splitting the vectors into multiple heads.
    `split_heads()` is defined like so:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这三行突出显示了如何将向量分割成多个头。`split_heads()`定义如下：
- en: '[PRE26]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This completes the multi-head attention implementation. This is the key part
    of the Transformer model. There is a small detail surrounding a Dense layer, which
    is used to aggregate the outputs from multi-head attention. It is quite simple:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了多头注意力的实现。这是Transformer模型的关键部分。这里有一个关于Dense层的小细节，Dense层用于聚合来自多头注意力的输出。它非常简单：
- en: '[PRE27]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Thus far, we have looked at the following parameters for specifying a Transformer
    mode:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经查看了指定Transformer模型的以下参数：
- en: '*d*[model] is used for the size of the embeddings and primary flow of inputs'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*d*[model]用于嵌入的大小和输入的主要流动'
- en: '*d*[ff] is the size of the output from the intermediate Dense layer in the
    FeedForward part'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*d*[ff]是前馈部分中中间Dense层输出的大小'
- en: '*h* specifies the number of heads for multi-head attention'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*h*指定了多头注意力的头数'
- en: Next, we will implement a visual Encoder, which has been modified to accommodate
    images as input.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现一个视觉编码器，该编码器已经被修改为支持图像作为输入。
- en: VisualEncoder
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VisualEncoder
- en: 'The diagram shown in the *The Transformer model* section shows the Encoder''s
    structure. The Encoder processes the inputs with positional encodings and masks,
    and then passes them through stacks of multi-head attention and feed-forward blocks.
    The implementation deviates from the TensorFlow tutorial as the input in the tutorial
    is text. In our case, we are passing 49x2,048 vectors that were generated by passing
    images through ResNet50\. The main difference is in how the inputs are handled.
    `VisualEncoder` is built as a layer to allow composition into the eventual Transform
    model:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Transformer模型*部分中展示的图表显示了编码器的结构。编码器通过位置编码和掩码处理输入，然后将其通过多头注意力和前馈层的堆栈进行传递。这个实现偏离了TensorFlow教程，因为教程中的输入是文本。在我们的案例中，我们传递的是49x2,048的向量，这些向量是通过将图像传入ResNet50生成的。主要的区别在于如何处理输入。`VisualEncoder`被构建为一个层，以便最终组成Transformer模型：
- en: '[PRE28]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The constructor is shown next. A new parameter that states the number of layers
    is introduced. The original paper used 6 layers, 512 as *d*[model], 8 multi-attention
    heads, and 2,048 as the size of the intermediate feed-forward output. Note the
    highlighted lines in the preceding code. The dimensions of the pre-processed images
    can vary depending on the layer of ResNet50 from which output is pulled. We pass
    the input through a dense layer, `fc`, to the size inputs according to the model.
    This allows us to experiment with different models to pre-process images such
    as VGG19 or Inception without changing the architecture. Also, note that the maximum
    position encoding is hardcoded to 49, since that is the dimension of the output
    of the ResNet50 model. Lastly, we add a flag that can switch positional encoding
    on or off in the Visual Encoder. You should experiment with training models with
    and without positional encodings in the input to see if this helps or hinders
    learning.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数如下所示。引入了一个新的参数，用于指示层数。原始论文使用了6层，512作为*d*[模型]，8个多头注意力头，以及2,048作为中间前馈输出的大小。请注意前面代码中标记的行。预处理过的图像的尺寸可能会有所不同，具体取决于从ResNet50中提取输出的层。我们将输入通过一个稠密层`fc`，调整为模型所需的输入大小。这使得我们可以在不改变架构的情况下，尝试使用不同的模型来预处理图像，比如VGG19或Inception。还要注意，最大位置编码被硬编码为49，因为那是ResNet50模型输出的维度。最后，我们添加了一个标志位，可以在视觉编码器中开启或关闭位置编码。你应该尝试训练包含或不包含位置编码的模型，看看这是否有助于或阻碍学习。
- en: '`VisualEncoder` is composed of multiple multi-head attention and feed-forward
    blocks. We can utilize a convenience class, `EncoderLayer`, to define one such
    block. A stack of these blocks is created based on the input parameters. We will
    examine the internals of `EncoderLayer` momentarily. First, let''s see how inputs
    pass through `VisualEncoder`. The `call()` function is used to produce the outputs
    for the given inputs:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`VisualEncoder`由多个多头注意力和前馈块组成。我们可以利用一个方便的类`EncoderLayer`来定义这样一个块。根据输入参数创建这些块的堆叠。我们稍后会检查`EncoderLayer`的内部实现。首先，让我们看看输入如何通过`VisualEncoder`传递。`call()`函数用于生成给定输入的输出：'
- en: '[PRE29]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This code is fairly simple due to the abstractions defined previously. Note
    the use of the training flag to turn dropout on or off. Now, let''s see how `EncoderLayer`
    is defined. Each Encoder building is composed of two sub-blocks. The first sub-block
    passes inputs through multi-head attention, while the second sub-block passes
    the output of the first sub-block through the 2-layer feed-forward layer:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 由于之前定义的抽象，这段代码相当简单。请注意使用训练标志来开启或关闭dropout。现在，让我们看看`EncoderLayer`是如何定义的。每个Encoder构建体由两个子块组成。第一个子块将输入传递通过多头注意力，而第二个子块则将第一个子块的输出通过2层前馈层：
- en: '[PRE30]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Each layer first computes the output from multi-head attention and passes it
    through dropout. A residual connection passes the sum of the output and input
    through LayerNorm. The second part of this block passes the output of the first
    LayerNorm through the feed-forward layer and another dropout layer.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层首先通过多头注意力计算输出，并将其传递通过dropout。一个残差连接将输出和输入的和通过LayerNorm。该块的第二部分将第一层LayerNorm的输出通过前馈层和另一个dropout层。
- en: Again, a residual connection combines the output and input to the feed-forward
    part before passing it through LayerNorm. Note the use of dropout and residual
    connections, which were developed for CV in the Transformer architecture.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，一个残差连接将输出和输入合并，并传递给前馈部分，再通过LayerNorm。请注意，dropout和残差连接的使用，这些都是在Transformer架构中为计算机视觉（CV）任务开发的。
- en: '**Layer normalization or LayerNorm**'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**层归一化或LayerNorm**'
- en: LayerNorm was proposed in 2016 in a paper by the same name as an alternative
    to BatchNorm for RNNs. BatchNorm, as described in the *CNNs* section, normalizes
    the outputs across the entire batch. But sequences can be of variable length in
    the case of RNNs. A different formulation is required for normalization that can
    handle variable sequence lengths. LayerNorm normalizes across all the hidden units
    in a given layer. It is independent of the batch size, and the normalization is
    the same for all the units in a given layer. LayerNorm results in a significant
    speedup of training and convergence of seq2seq style models.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: LayerNorm于2016年在一篇同名论文中提出，作为RNN的BatchNorm替代方案。如*CNNs*部分所述，BatchNorm会在整个批次中对输出进行归一化。但是在RNN的情况下，序列长度是可变的。因此，需要一种不同的归一化公式来处理可变长度的序列。LayerNorm会在给定层的所有隐藏单元之间进行归一化。它不依赖于批次大小，并且对给定层中的所有单元进行相同的归一化。LayerNorm显著加快了训练和seq2seq模型的收敛速度。
- en: With `VisualEncoder` in place, we are ready to implement the Decoder before
    we put this all together into the full Transformer.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 有了`VisualEncoder`，我们就准备好实现解码器，然后将这一切组合成完整的Transformer。
- en: Decoder
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器
- en: 'The Decoder is also composed of blocks, just like the Encoder. Each block of
    the Decoder, however, contains three sub-blocks, as shown in the diagram in the
    *The Transformer model* section. There is a masked multi-head attention sub-block,
    followed by a multi-head attention block, and finally a feed-forward sub-block.
    The feed-forward sub-block is identical to the Encoder sub-block. We must define
    a Decoder layer that can be stacked to construct the Decoder. The constructor
    for this is shown here:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器也由块组成，和编码器一样。然而，解码器的每个块包含三个子模块，如*Transformer模型*部分中的图所示。首先是掩蔽多头注意力子模块，接着是多头注意力块，最后是前馈子模块。前馈子模块与编码器子模块相同。我们必须定义一个可以堆叠的解码器层来构建解码器。其构造器如下所示：
- en: '[PRE31]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Three sub-blocks should be quite evident based on the preceding variables.
    Input passes through this layer and is converted into output, as defined by the
    computations in the `call()` function:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面的变量，三个子模块应该是相当明显的。输入通过这一层，并根据`call()`函数中的计算转换为输出：
- en: '[PRE32]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The first sub-block, also referred to as the masked multi-head attention block,
    uses the output tokens, masked to the current position being generated. The outputs,
    in our case, are the tokens that make up the caption. The look-ahead mask masks
    tokens that haven't been generated yet.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个子模块，也称为掩蔽多头注意力模块，使用输出令牌，并对当前生成的位置进行掩蔽。在我们的例子中，输出是组成标题的令牌。前瞻掩蔽将未生成的令牌进行掩蔽。
- en: Note that this sub-block does not use the output of the Encoder. It is trying
    to predict the relationship of the next token to the previous token that was generated.
    The second sub-block uses the output of the Encoder, along with the output of
    the previous sub-block, to generate the outputs. Finally, the feed-forward network
    generates the final output by operating on the output of the second sub-block.
    Both the multi-head attention sub-blocks have their own attention weights.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个子模块不使用编码器的输出。它试图预测下一个令牌与之前生成的令牌之间的关系。第二个子模块使用编码器的输出以及前一个子模块的输出生成输出。最后，前馈网络通过对第二个子模块的输出进行处理来生成最终输出。两个多头注意力子模块都有自己的注意力权重。
- en: 'We define the Decoder as a custom layer that is composed of multiple `DecoderLayer`
    blocks. The structure of the Transformer is symmetrical. The number of Encoder
    and Decoder blocks is the same. The constructor is defined first:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将解码器定义为一个由多个`DecoderLayer`块组成的自定义层。Transformer的结构是对称的。编码器和解码器的块数是相同的。构造器首先被定义：
- en: '[PRE33]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output of the Decoder is computed by the `call()` function:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的输出是通过`call()`函数计算的：
- en: '[PRE34]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Whew, that was a fair amount of code. The structure of the Transformer model
    is so elegant. The beauty of the model allows us to stack more Encoder and Decoder
    layers to create more powerful models, as demonstrated by GPT-3 recently. Let's
    put the Encoder and Decoder together to create a full Transformer.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，这真是一大段代码。Transformer模型的结构如此优雅。该模型的美丽之处在于，允许我们堆叠更多的编码器和解码器层，创建更强大的模型，正如最近的GPT-3所展示的那样。让我们将编码器和解码器结合起来，构建一个完整的Transformer。
- en: Transformer
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer
- en: 'The Transformer is composed of the Encoder, the Decoder, and the final Dense
    layer for generating output token distributions across the subword vocabulary:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer由编码器、解码器和最终的密集层组成，用于生成跨子词词汇的输出令牌分布：
- en: '[PRE35]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: That was a whirlwind tour of the full Transformer code. Ideally, Keras in TensorFlow
    will provide a higher-level API for defining a Transformer model without you having
    to write the code out. If this was too much to absorb, then focus on the masks
    and VisualEncoder as they are the only deviations from the standard Transformer
    architecture.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是完整Transformer代码的快速概览。理想情况下，TensorFlow中的Keras将提供一个更高层次的API来定义Transformer模型，而不需要你编写代码。如果这对你来说有点复杂，那就专注于掩码和VisualEncoder，因为它们是与标准Transformer架构的唯一偏离之处。
- en: We are now ready to train the model. We'll take a very similar approach to the
    one we adopted in the previous chapter, by setting up learning rate annealing
    and checkpointing.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备开始训练模型。我们将采用与上一章相似的方法，通过设置学习率衰减和检查点进行训练。
- en: Training the Transformer model with VisualEncoder
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用VisualEncoder训练Transformer模型
- en: Training the Transformer model can take hours as we want to train for around
    20 epochs. It is best to put the training code into a file so that it can be run
    from the command line. Note that the model will be able to show some results even
    after 4 epochs of training. The training code is in the `caption-training.py`
    file. At a high level, the following steps need to be performed before starting
    training. First, the CSV file with captions and image names is loaded in, and
    the corresponding paths for the files with extracted image features are appended.
    The Subword Encoder is also loaded in. A `tf.data.Dataset` is created with the
    encoded captions and image features for easy batching and feeding them into the
    model for training. A loss function, an optimizer with a learning rate schedule,
    is created for use in training. A custom training loop is used to train the Transformer
    model. Let's go over these steps in detail.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 训练Transformer模型可能需要几个小时，因为我们希望训练大约20个epoch。最好将训练代码放到一个文件中，这样可以从命令行运行。请注意，即使训练只进行4个epoch，模型也能显示一些结果。训练代码位于`caption-training.py`文件中。总体来说，在开始训练之前需要执行以下步骤。首先，加载包含标题和图像名称的CSV文件，并附加包含提取图像特征文件的相应路径。还需要加载Subword
    Encoder。创建一个`tf.data.Dataset`，包含编码后的标题和图像特征，便于批处理并将它们输入到模型中进行训练。为训练创建一个损失函数、一个带有学习率计划的优化器。使用自定义训练循环来训练Transformer模型。让我们详细了解这些步骤。
- en: Loading training data
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载训练数据
- en: 'The following code loads the CSV file we generated in the pre-processing step:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码加载我们在预处理步骤中生成的CSV文件：
- en: '[PRE36]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The captions in the data are tokenized using the Subword Encoder we generated
    and persisted to disk earlier:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中的标题使用我们之前生成并保存在磁盘上的Subword Encoder进行分词：
- en: '[PRE37]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The maximum length of the captions is generated to accommodate 99% of the caption
    lengths. All the captions are truncated or padded to this maximum length:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的标题长度是为了适应99%的标题长度而生成的。所有的标题都会被截断或填充到这个最大长度：
- en: '[PRE38]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Image features are persisted to disk. When training begins, those features
    need to be read from the disk and fed in, along with the encoded captions. The
    name of the file containing the image features is then added to the dataset:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图像特征被保存在磁盘上。当训练开始时，这些特征需要从磁盘读取并与编码后的标题一起输入。然后，包含图像特征的文件名被添加到数据集中：
- en: '[PRE39]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'A `tf.data.Dataset` is created and a map function that reads image features
    while enumerating batches is set up:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个`tf.data.Dataset`，并设置一个映射函数，在枚举批次时读取图像特征：
- en: '[PRE40]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now that the dataset has been prepared, we are ready to instantiate the Transformer
    model.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据集已经准备好，我们可以实例化Transformer模型了。
- en: Instantiating the Transformer model
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实例化Transformer模型
- en: 'We will instantiate a small model in terms of the number of layers, attention
    heads, embedding dimensions, and feed-forward units:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实例化一个较小的模型，具体来说是层数、注意力头数、嵌入维度和前馈单元的数量：
- en: '[PRE41]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'For comparison, the BERT base model contains the following parameters:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较，BERT基础模型包含以下参数：
- en: '[PRE42]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'These settings are available in the file but commented out. Using these settings
    slows down training and requires a large amount of GPU memory. A couple of other
    parameters need to be set up and the Transformer instantiated:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设置在文件中可用，但被注释掉了。使用这些设置会减慢训练速度，并需要大量的GPU内存。还需要设置其他一些参数，并实例化Transformer：
- en: '[PRE43]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This model contains over 4 million trainable parameters. It is a smaller model
    than we have seen previously:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型包含超过400万个可训练参数。它比我们之前看到的模型要小：
- en: '[PRE44]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: However, the model summary is not available since the input dimensions have
    not yet been supplied. The summary will be available once we've run a training
    example through the model.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于输入维度尚未提供，因此模型摘要不可用。一旦我们通过模型运行一个训练示例，摘要将可用。
- en: A custom learning rate schedule is created for training the model. A custom
    learning rate schedule anneals or reduces the learning rate as the model improves
    its accuracy, resulting in better accuracy. This process is called learning rate
    decay or learning rate annealing and was discussed in detail in *Chapter 5*, *Generating
    Text with RNNs and GPT-2*.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 为训练模型创建一个自定义学习率调度。自定义学习率调度会随着模型准确性的提高而逐渐降低学习率，从而提高准确性。这个过程被称为学习率衰减或学习率退火，在*第5章*《使用RNN和GPT-2生成文本》中有详细讨论。
- en: Custom learning rate schedule
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义学习率调度
- en: 'This rate schedule is identical to the one proposed in the *Attention Is All
    You Need* paper:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这个学习率调度与*Attention Is All You Need* 论文中提出的完全相同：
- en: '[PRE45]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The following graph shows the learning schedule:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了学习计划：
- en: '![A close up of a person  Description automatically generated](img/B16252_07_13.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![一个人的特写 描述自动生成](img/B16252_07_13.png)'
- en: 'Figure 7.13: Custom learning rate schedule'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13：自定义学习率调度
- en: When training starts, a higher learning rate is used as the loss is high. As
    the model learns more and more, the loss starts decreasing, which requires a lower
    learning rate. Using the preceding learning rate schedule significantly speeds
    up training and convergence. We also need a loss function to optimize.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练开始时，由于损失较高，因此使用较高的学习率。随着模型不断学习，损失开始下降，这时需要使用较低的学习率。采用上述学习率调度可以显著加快训练和**收敛**。我们还需要一个损失函数来进行优化。
- en: Loss and metrics
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失和指标
- en: 'The loss function is based on categorical cross-entropy. It is a common loss
    function that we have used in previous chapters. In addition to the loss, an accuracy
    metric is also defined to track how the model is doing on the training set:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数基于类别交叉熵。这是一个常见的损失函数，我们在前几章中也使用过。除了损失函数外，还定义了准确度指标，以跟踪模型在训练集上的表现：
- en: '[PRE46]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This formulation has been used in previous chapters as well. We are almost ready
    to start training. There are two more steps we must follow before we get into
    the custom training function. We need to set up checkpoints to save progress in
    case of failures, and we also need to mask inputs for the Encoder and Decoder.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式在前几章中也有使用。我们已经差不多可以开始训练了。在进入自定义训练函数之前，我们还需要完成两个步骤。我们需要设置检查点，以便在发生故障时保存进度，同时还需要为
    Encoder 和 Decoder 屏蔽输入。
- en: Checkpoints and masks
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查点和屏蔽
- en: 'We need to specify a checkpoint directory for TensorFlow to save progress.
    We will use a `CheckpointManager` here, which automatically manages the checkpoints
    and stores a limited number of them. A checkpoint can be quite large. Five checkpoints
    for the small model would take up approximately 243 MB of space. Larger models
    would take up more space:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要指定一个检查点目录，以便 TensorFlow 保存进度。这里我们将使用 `CheckpointManager`，它会自动管理检查点并存储有限数量的检查点。一个检查点可能非常大。对于小模型，五个检查点大约占用
    243 MB 的空间。更大的模型会占用更多的空间：
- en: '[PRE47]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Next, a method that will create masks for the input images and captions must
    be defined:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，必须定义一个方法来为输入图像和字幕创建屏蔽：
- en: '[PRE48]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Inputs are always a constant length, so the input sequence is set as ones. Only
    the captions, which are used by the Decoder, are masked. There are two types of
    masks for the Decoder. The first mask is the padding mask. Since the captions
    are set to the maximum length to handle 99% of the captions, which works out at
    about 22 tokens, any captions that are smaller than this number of tokens have
    padding appended to the end of them. The padding mask helps separate caption tokens
    from padding tokens. The second mask is the look-ahead mask. It prevents the Decoder
    from seeing tokens from the future or tokens it has not generated yet. Now, we
    are ready to train the model.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 输入始终是固定长度，因此输入序列被设置为全 1。只有 Decoder 使用的字幕被屏蔽。Decoder 有两种类型的屏蔽。第一种是填充屏蔽。由于字幕被设置为最大长度，以处理99%的字幕，大约是
    22 个标记，任何少于这个标记数的字幕都会在末尾添加填充。填充屏蔽有助于将字幕标记与填充标记分开。第二种是前瞻屏蔽。它防止 Decoder 看到未来的标记或尚未生成的标记。现在，我们准备好开始训练模型。
- en: Custom training
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义训练
- en: 'Similar to the summarization model, teacher forcing will be used for training.
    Consequently, a custom training function will be used. First, we must define a function
    that will train on one batch of data:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 与摘要模型类似，训练时将使用教师强制（teacher forcing）。因此，将使用一个定制的训练函数。首先，我们必须定义一个函数来对一批数据进行训练：
- en: '[PRE49]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This method is very similar to the summarization training code. All we need
    to do now is define the number of epochs and batch size and start training:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法与摘要训练代码非常相似。现在我们需要做的就是定义训练的轮数（epochs）和批次大小（batch size），然后开始训练：
- en: '[PRE50]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Training can be started from the command line:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 训练可以从命令行启动：
- en: '[PRE51]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This training may take some time. An epoch of training takes about 11 minutes
    on my GPU-enabled machine. If you contrast this to the summarization model, this
    model is training extremely fast. Compared to the summarization model, which contains
    13 million parameters, it is much smaller and trains very fast. This speed boost
    is due to the lack of recurrence.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这次训练可能需要一些时间。一个训练周期在我启用GPU的机器上大约需要11分钟。如果与摘要模型对比，这个模型的训练速度非常快。与包含1300万个参数的摘要模型相比，它要小得多，训练也非常快。这个速度提升归功于缺乏递归结构。
- en: The state-of-the-art summarization models use the Transformer architecture along
    with subword encoding. Given that you have all the pieces of the Transformer,
    a good exercise to test your understanding would be editing the VisualEncoder
    to process text and rebuild the summarization model as a Transformer. You will
    then be able to experience these speedup and accuracy improvements.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的摘要模型使用Transformer架构，并结合子词编码。考虑到你已经掌握了Transformer的所有构件，一个很好的练习来测试你的理解就是编辑VisualEncoder来处理文本，并将摘要模型重建为Transformer。这样你就能体验到这些加速和精度的提升。
- en: A longer training time allows the model to learn better. However, this model
    can give reasonable results in as few as 5-10 epochs of training. Once training
    is complete, we can try the model on some images.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 更长的训练时间能让模型学习得更好。然而，这个模型在仅经过5-10个训练周期后就能给出合理的结果。训练完成后，我们可以尝试将模型应用到一些图像上。
- en: Generating captions
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成字幕
- en: First, you need to be congratulated! You made it through a whirlwind implementation
    of the Transformer. I am sure you must have noticed a number of common building
    blocks that were used in previous chapters. Since the Transformer model is complex,
    we left it for this chapter to look at other techniques like Bahdanau attention,
    custom layers, custom rate schedules, custom training using teacher forcing, and
    checkpointing so that we could cover a lot of ground quickly in this chapter.
    You should consider all these building blocks an important part of your toolkit
    when you try and solve an NLP problem.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要祝贺自己！你已经通过了一个快速实现Transformer的过程。我相信你一定注意到前几章中使用过的一些常见构建块。由于Transformer模型非常复杂，我们把它放到这一章，探讨像巴赫达努（Bahdanau）注意力机制、定制层、定制学习率计划、使用教师强制（teacher
    forcing）进行的定制训练以及检查点（checkpoint）等技术，这样我们就可以快速覆盖大量内容。在你尝试解决NLP问题时，应该把所有这些构建块视为你工具包中的重要组成部分。
- en: Without further ado, let's try and caption some images. Again, we will use a
    Jupyter notebook for inference so that we can quickly try out different images.
    All the code for inference is in the `image-captioning-inference.ipynb` file.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 不再多说，让我们尝试为一些图像生成字幕。我们将再次使用Jupyter notebook进行推理，以便快速尝试不同的图像。所有推理代码都在`image-captioning-inference.ipynb`文件中。
- en: The inference code needs to load the Subword Encoder, set up masking, instantiate
    a ResNet50 model to extract features from test images, and generate captions a
    token at a time until the end of the sequence or a maximum sequence length is
    reached. Let's go over these steps one at a time.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 推理代码需要加载子词编码器，设置遮掩（masking），实例化一个ResNet50模型来从测试图像中提取特征，并一遍遍地生成字幕，直到序列结束或达到最大序列长度。让我们一一走过这些步骤。
- en: 'Once we''ve done the appropriate imports and optionally initialized the GPU,
    we can load the Subword Encoder that was saved when we pre-processed the data:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成了适当的导入并可选地初始化了GPU，就可以加载在数据预处理时保存的子词编码器（Subword Encoder）：
- en: '[PRE52]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We must now instantiate the Transformer model. This is an important step to
    ensure the parameters are the same as the checkpoint ones:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须实例化Transformer模型。这是一个重要步骤，确保参数与检查点中的参数相同：
- en: '[PRE53]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Restoring the model from the checkpoint requires the optimizer, even though
    we are not training the model. So, we will reuse the custom scheduler from the
    training code. As this code was provided previously, it has been omitted here.
    For the checkpoint, I used a model that was trained for 40 epochs, but without
    positional encoding in the Encoder:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 从检查点恢复模型时需要优化器，即使我们并没有训练模型。因此，我们将重用训练代码中的自定义调度器。由于该代码之前已经提供，这里省略了它。对于检查点，我使用了一个训练了40个周期的模型，但在编码器中没有使用位置编码：
- en: '[PRE54]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Finally, we must set up the masking function for the generated captions. Note
    that the look ahead masks don''t really help during inference as future tokens
    have not been generated yet:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们必须为生成的字幕设置遮罩功能。请注意，前瞻遮罩在推理过程中并没有真正的帮助，因为未来的标记尚未生成：
- en: '[PRE55]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The main code for inference is in an `evaluate()` function. This method takes
    in the image features generated by ResNet50 as input and seeds the output caption
    sequence with the start token. Then, it runs in a loop to generate a token at
    a time while updating the masks, until an end of sequence token is encountered
    or the maximum length of the caption is reached:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 推理的主要代码在`evaluate()`函数中。此方法将ResNet50生成的图像特征作为输入，并用起始标记来初始化输出字幕序列。然后，它进入循环，每次生成一个标记，同时更新遮罩，直到遇到序列结束标记或达到字幕的最大长度：
- en: '[PRE56]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'A wrapper method is used to call the evaluation method and print out the caption:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个包装方法来调用评估方法并输出字幕：
- en: '[PRE57]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The only thing remaining now is instantiating a ResNet50 model to extract features
    from image files on the fly:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在唯一剩下的就是实例化一个ResNet50模型，以便从图像文件中动态提取特征：
- en: '[PRE58]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'It''s the moment of truth, finally! Let''s try out the model on an image. We
    will load the image, pre-process it for ResNet50, and extract the features from
    it:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 终于到了关键时刻！让我们在一张图像上尝试一下模型。我们将加载图像，对其进行ResNet50预处理，并从中提取特征：
- en: '[PRE59]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The following is the example image and its caption:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是示例图像及其字幕：
- en: '![A person riding a wave on a surfboard in the ocean  Description automatically
    generated](img/B16252_07_14.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![一个人正在海洋中的冲浪板上骑着波浪  自动生成的描述](img/B16252_07_14.png)'
- en: 'Figure 7.14: Generated caption - A man is riding a surfboard on a wave'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14：生成的字幕 - 一名男子在波浪上骑着冲浪板
- en: This looks like an amazing caption for the given image! However, the overall
    accuracy of the model is in the low 30s. There is a lot of scope for improvement
    in the model. The next section talks about the state-of-the-art techniques for
    image captioning and also proposes some simpler ideas that you can try and play around
    with.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来是给定图像的一个精彩字幕！然而，模型的整体准确度只有30%左右。模型还有很大的改进空间。下一节将讨论图像字幕生成的最先进技术，并提出一些你可以尝试和玩弄的更简单的思路。
- en: Note that you may see slightly different results. The reviewer for this book
    got the result *A man in a black shirt is riding a surfboard* while running this
    code. This is expected as slight differences in the probabilities and the exact
    place where the model stops training in the loss surface is not exact. We are
    operating in the probabilistic realm here, so there may be slight differences.
    You may have experienced similar differences in the text generation and summarization
    code in the previous chapters as well.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你可能会看到略微不同的结果。这本书的审阅者在运行这段代码时得到的结果是*一个穿黑衬衫的男人正在骑冲浪板*。这是预期的结果，因为概率上的微小差异以及模型在损失曲面中停止训练的精确位置并不完全相同。我们在这里是操作于概率领域，因此可能会有一些微小的差异。你可能也在前几章的文本生成和摘要代码中经历过类似的差异。
- en: 'The following image shows some more examples of images and their captions.
    The notebook contains several good, as well as some atrocious, examples of the
    generated labels:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了更多图像及其字幕的示例。该笔记本包含了几个不错的示例，也有一些糟糕的生成标签示例：
- en: '![A picture containing photo, different, various, group  Description automatically
    generated](img/B16252_07_15.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含照片、不同、各种、群体的图片  自动生成的描述](img/B16252_07_15.png)'
- en: 'Figure 7.15: Examples of images and their generated captions'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15：图像及其生成的字幕示例
- en: None of these images were in the training set. The caption quality goes down
    from top to bottom. Our model understands close up, cake, groups of people, sandy
    beaches, streets, and luggage, among other things. However, the bottom two examples
    are concerning. They hint at some **bias** in the model. In both of the bottom
    two images, the model is misinterpreting gender.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像都不在训练集中。从上到下，字幕质量下降。我们的模型理解特写、蛋糕、人群、沙滩、街道和行李等等。然而，最后两个示例令人担忧。它们暗示模型中存在某种**偏见**。在这两张底部的图像中，模型误解了性别。
- en: The images were deliberately chosen to show a woman in a business suit and women
    playing basketball. In both cases, the model proposes men in the captions. When
    the model was tried with a female tennis player's image, it guessed the right
    gender, but it changed genders in an image from a women's soccer game. Bias in
    models is a very important concern. In cases such as image captioning, this bias
    is immediately apparent. In fact, over 600,000 images were removed from the ImageNet
    database ([https://bit.ly/3qk4FgN](https://bit.ly/3qk4FgN)) in 2019 after bias
    was found in how it classifies and tags people in its pictures. ResNet50 is pre-trained
    on ImageNet. However, in other models, the bias may be harder to detect. Building
    fair deep learning models and reducing bias in models are active areas of research
    in the ML community.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像是有意选择的，展示了一位穿着商务套装的女性和打篮球的女性。在这两种情况下，模型在字幕中建议的都是男性。当模型尝试使用女子网球运动员的图像时，它猜对了性别，但在女子足球比赛的图像中却改变了性别。模型中的偏见在图像字幕等情况下是立即显现的。事实上，2019年在发现它如何分类和标记图像中存在偏见后，ImageNet数据库中已移除了超过60万张图像（[https://bit.ly/3qk4FgN](https://bit.ly/3qk4FgN)）。ResNet50是在ImageNet上预训练的。然而，在其他模型中，偏见可能更难以检测。建立公平的深度学习模型和减少模型偏见是机器学习社区的活跃研究领域。
- en: You may have noticed that we skipped running the model on an evaluation set
    and on the test set. This was done for brevity, and also because those techniques
    were covered previously.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，我们跳过了在评估集和测试集上运行模型的步骤。这是为了简洁起见，也因为这些技术之前已经涵盖过了。
- en: 'A quick note on metrics for evaluating the quality of captions. We saw ROUGE
    metrics in the previous chapters. ROUGE-L is still applicable in the case of image
    captioning. You can use a mental model of the caption as a summary of an image,
    as opposed to the summary of a paragraph in text summarization. There can be more than
    one way to express the summary, and ROUGE-L tries to capture the intent. There
    are two other commonly reported metrics:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 关于评估字幕质量的度量标准的简短说明。在前几章节中我们看到了ROUGE度量标准。在图像字幕中，ROUGE-L仍然适用。您可以将字幕的心理模型视为图像的摘要，而不是文本摘要中段落的摘要。有多种表达摘要的方式，而ROUGE-L试图捕捉意图。还有两个常报告的度量标准：
- en: '**BLEU**: This stands for **Bilingual Evaluation Understudy** and is the most
    popular metric in machine translation. We can cast the image captioning problem
    as a machine translation problem as well. It relies on n-grams for computing the
    overlap of the predicted text with a number of reference texts and combines the
    results into one score.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BLEU**：这代表**双语评估助手**，是机器翻译中最流行的度量标准。我们也可以把图像字幕问题视为机器翻译问题。它依赖于n-gram来计算预测文本与多个参考文本的重叠，并将结果合并为一个分数。'
- en: '**CIDEr**: This stands for **Consensus-Based Image Description Evaluation**
    and was proposed in a paper by the same name in 2015\. It tries to deal with the
    difficulty of automatic evaluation when multiple captions could be reasonable
    by combining TF-IDF and n-grams. The metric tries to compare the captions generated
    by the model against multiple captions by human annotators and tries to score
    them based on consensus.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CIDEr**：这代表**基于一致性的图像描述评估**，并在2015年的同名论文中提出。它试图处理自动评估的困难，当多个字幕可能都合理时，通过结合TF-IDF和n-gram来比较模型生成的字幕与多个人类注释者的字幕，并根据共识进行评分。'
- en: Before wrapping up this chapter, let's spend a little time discussing ways to
    improve performance and state-of-the-art models.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束之前，让我们花点时间讨论提升性能和最先进模型的方法。
- en: Improving performance and state-of-the-art models
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升性能和最先进模型
- en: Let's first talk through some simple experiments you can try to improve performance
    before talking about the latest models. Recall our discussion on positional encodings
    for inputs in the Encoder. Adding or removing positional encodings helps or hinders
    performance. In the previous chapter, we implemented the beam search algorithm
    for generating summaries. You can adapt the beam search code and see an improvement
    in the results with beam search. Another avenue of exploration is the ResNet50\.
    We used a pre-trained network and did not fine-tune it further. It is possible
    to build an architecture where ResNet is part of the architecture and not a pre-processing
    step. Image files are loaded in, and features are extracted from ResNet50 as part
    of the VisualEncoder. ResNet50 layers can be trained from the get-go, or only
    in the last few iterations. This idea is implemented in the `resnet-finetuning.py`
    file for you to try. Another line of thinking is using a different object detection
    model than ResNet50 or using the output from a different layer. You can try a
    more complex version of ResNet like ResNet152, or a different object detection
    model like Detectron from Facebook or other models. It should be quite easy to
    use a different model in our code as it is quite modular.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论最新的模型之前，让我们先讨论一些你可以尝试的简单实验来提高性能。回想一下我们在编码器中对输入位置编码的讨论。添加或移除位置编码会有助于或妨碍性能。在上一章中，我们实现了用于生成摘要的束搜索算法。你可以调整束搜索代码，并在结果中看到束搜索的改善。另一个探索方向是ResNet50。我们使用了一个预训练的网络，但没有进一步微调。也可以构建一个架构，其中ResNet是架构的一部分，而不是一个预处理步骤。图像文件被加载，并且特征从ResNet50中提取作为视觉编码器的一部分。ResNet50的层可以从一开始就进行训练，或者仅在最后几次迭代中训练。这一想法在`resnet-finetuning.py`文件中实现，你可以尝试。另一种思路是使用与ResNet50不同的物体检测模型，或者使用来自不同层的输出。你可以尝试使用更复杂的ResNet版本，如ResNet152，或使用来自Facebook的Detectron等其他物体检测模型。由于我们的代码非常模块化，因此使用不同的模型应该是非常简单的。
- en: When you use a different model for extracting image features, the key will be
    to make sure tensor dimensions are flowing properly through the Encoder. The Decoder
    should not require any changes. Depending on the complexity of the model, you
    can either pre-process and store the image features or compute them on the fly.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用不同的模型来提取图像特征时，关键是确保张量维度能够在编码器中正确流动。解码器不应需要任何更改。根据模型的复杂性，你可以选择预处理并存储图像特征，或者实时计算它们。
- en: Recall that we just used the pixels from the image directly. This was based
    on a paper published recently at CVPR titled *Pixel-BERT*. Most models use region
    proposals extracted from images instead of the pixels directly. Object detection
    in an image involves drawing a boundary around that object in the image. Another
    way to perform the same task is to classify each pixel into an object or background.
    These region proposals can be in the form of bounding boxes in an image. State-of-the-art
    models use bounding boxes or region proposals as input.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，我们直接使用了图像中的像素。这是基于最近在CVPR上发表的一篇名为*Pixel-BERT*的论文。大多数模型使用从图像中提取的区域提议，而不是直接使用像素。图像中的物体检测涉及在图像中的物体周围绘制边界。另一种执行相同任务的方法是将每个像素分类为物体或背景。这些区域提议可以以图像中的边界框形式存在。最先进的模型将边界框或区域提议作为输入。
- en: The second-biggest gain in image captioning comes from pre-training. Recall
    that BERT and GPT are pre-trained on specific pre-training objectives. Models
    differ based on whether the Encoder is pre-trained or both the Encoder and Decoder
    are pre-trained. A common pre-training objective is a version of the BERT MLM
    task. Recall that BERT inputs are structured as `[CLS] I1 I2 … In [SEP] J1 J2
    … Jk [SEP]`, where some of the tokens from the input sequence are masked. This
    is adapted for image captioning, where the image features and caption tokens in
    the input are concatenated. Caption tokens are masked similar to how they are
    in the BERT model, and the pre-training objective is for the model to predict
    the masked token. After pre-training, the output of the CLS token can be used
    for classification or fed to the Decoder to generate the caption. Care must be
    exercised to not pre-train on the same dataset, like that for evaluation. An example
    of the setup could be using the Visual Genome and Flickr30k datasets for pre-training
    and COCO for fine-tuning.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图像描述的第二大提升来自于预训练。回想一下，BERT和GPT都是在特定的预训练目标下进行预训练的。模型的区别在于，是否仅对编码器（Encoder）进行预训练，或者编码器和解码器（Decoder）都进行了预训练。一种常见的预训练目标是BERT的掩码语言模型（MLM）任务版本。回想一下，BERT的输入结构为`[CLS]
    I1 I2 … In [SEP] J1 J2 … Jk [SEP]`，其中输入序列中的部分标记被遮掩。这个过程被适用于图像描述，其中输入中的图像特征和描述标记被拼接在一起。描述标记被像BERT模型中的遮掩一样进行遮掩，预训练目标是让模型预测被遮掩的标记。预训练后，CLS标记的输出可以用于分类，或者送入解码器生成描述。需要小心的是，不能在相同的数据集上进行预训练，例如评估时使用的数据集。设置的一个示例是使用Visual
    Genome和Flickr30k数据集进行预训练，并使用COCO进行微调。
- en: Image captioning is an active area of research. The research is just getting
    started on multi-modal networks in general. Now, let's recap everything we've
    learned in this chapter.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图像描述是一个活跃的研究领域。关于多模态网络的研究才刚刚起步。现在，让我们回顾一下本章所学的内容。
- en: Summary
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In the world of deep learning, specific architectures have been developed to
    handle specific modalities. **Convolutional Neural Networks** (**CNNs**) have
    been incredibly effective in processing images and is the standard architecture
    for CV tasks. However, the world of research is moving toward the world of multi-modal
    networks, which can take multiple types of inputs, like sounds, images, text,
    and so on and perform cognition like humans. After reviewing multi-modal networks,
    we dived into vision and language tasks as a specific focus. There are a number
    of problems in this particular area, including image captioning, visual question
    answering, VCR, and text-to-image, among others.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的领域中，已经开发出特定的架构来处理特定的模态。**卷积神经网络**（**CNNs**）在处理图像方面非常有效，是计算机视觉（CV）任务的标准架构。然而，研究领域正在向多模态网络的方向发展，这些网络能够处理多种类型的输入，如声音、图像、文本等，并且能够像人类一样进行认知。在回顾多模态网络后，我们将重点深入研究视觉和语言任务。这个领域中存在许多问题，包括图像描述、视觉问答、视觉-常识推理（VCR）和文本生成图像等。
- en: Building on our learnings from previous chapters on seq2seq architectures, custom
    TensorFlow layers and models, custom learning schedules, and custom training loops,
    we implemented a Transformer model from scratch. Transformers are state of the
    art at the time of writing. We took a quick look at the basic concepts of CNNs
    to help with the image side of things. We were able to build a model that may
    not be able to generate a thousand words for a picture but is definitely able
    to generate a human-readable caption. Its performance still needs improvement,
    and we discussed a number of possibilities so that we can try to do so, including
    the latest techniques.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们在前几章中学习的seq2seq架构、自定义TensorFlow层和模型、自定义学习计划以及自定义训练循环，我们从零开始实现了一个Transformer模型。Transformer模型是目前写作时的最先进技术。我们快速回顾了CNN的基本概念，以帮助理解图像相关部分。我们成功构建了一个模型，虽然它可能无法为一张图片生成千言万语，但它绝对能够生成一条人类可读的描述。该模型的表现仍需改进，我们讨论了若干可能性，以便进行改进，包括最新的技术。
- en: It is apparent that deep models perform very well when they contain a lot of
    data. The BERT and GPT models have shown the value of pre-training on massive
    amounts of data. It is still very hard to get good quality labeled data for use
    in pre-training or fine-tuning. In the world of NLP, we have a lot of text data,
    but not enough labeled data. The next chapter focuses on weak supervision to build
    classification models that can label data for pre-training or even fine-tuning
    tasks.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，当深度模型包含大量数据时，它们的表现非常出色。BERT和GPT模型已经展示了在海量数据上进行预训练的价值。对于预训练或微调，获得高质量的标注数据仍然非常困难。在自然语言处理领域，我们有大量的文本数据，但标注数据却远远不够。下一章将重点介绍弱监督学习，构建能够为预训练甚至微调任务标注数据的分类模型。
