- en: '*Chapter 6*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第六章*'
- en: Gated Recurrent Units (GRUs)
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 门控递归单元（GRU）
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你将能够：
- en: Assess the drawback of simple Recurrent Neural Networks (RNNs)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估简单递归神经网络（RNNs）的缺点
- en: Describe the architecture of Gated Recurrent Units (GRUs)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述门控递归单元（GRU）的架构
- en: Perform sentiment analysis using GRUs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GRU进行情感分析
- en: Apply GRUs for text generation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用GRU进行文本生成
- en: The chapter aims to provide a solution to the existing drawbacks of the current
    architecture of RNNs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在为现有RNN架构的缺点提供解决方案。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: In previous chapters, we studied text processing techniques such as word embedding,
    tokenization, and Term Frequency Inverse Document Frequency (TFIDF). We also learned
    about a specific network architecture called a Recurrent Neural Network (RNN)
    that has the drawback of vanishing gradients.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了文本处理技术，如词嵌入、分词和词频-逆文档频率（TFIDF）。我们还了解了一种特定的网络架构——递归神经网络（RNN），其存在消失梯度的问题。
- en: In this chapter, we are going to study a mechanism that deals with vanishing
    gradients by using a methodical approach of adding memory to the network. Essentially,
    the gates that are used in GRUs are vectors that decide what information should
    be passed onto the next stage of the network. This, in turn, helps the network
    to generate output accordingly.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究一种通过在网络中添加记忆的有序方法来处理消失梯度的机制。本质上，GRU中使用的门控是决定哪些信息应该传递到网络下一个阶段的向量。反过来，这有助于网络相应地生成输出。
- en: 'A basic RNN generally consists of an input layer, output layer, and several
    interconnected hidden layers. The following diagram displays the basic architecture
    of an RNN:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一个基本的RNN通常由输入层、输出层和几个相互连接的隐藏层组成。下图展示了RNN的基本架构：
- en: '![Figure 6.1: A basic RNN'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.1：一个基本的RNN'
- en: '](img/C13783_06_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_01.jpg)'
- en: 'Figure 6.1: A basic RNN'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.1：一个基本的RNN
- en: RNNs, in their simplest form, suffer from a drawback, that is, their inability
    to retain long-term relationships in the sequence. To rectify this flaw, a special
    layer called Gated Recurrent Unit (GRU) needs to be added to the simple RNN network.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: RNN在其最简单的形式中存在一个缺点，即无法保留序列中的长期关系。为了纠正这个缺陷，需要向简单的RNN网络中添加一个特殊的层，叫做门控递归单元（GRU）。
- en: In this chapter, we will first explore the reason behind the inability of Simple
    RNNs to retain long term dependencies, followed by the introduction of the GRU
    layer and how it attempts to solve this specific issue. We will then go on to
    build a network with the GRU layer included.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先探讨简单RNN无法保持长期依赖关系的原因，然后介绍GRU层及其如何尝试解决这个特定问题。接着，我们将构建一个包含GRU层的网络。
- en: The Drawback of Simple RNNs
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单RNN的缺点
- en: Let's take a look at a simple example in order to revisit the concept of vanishing
    gradients.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来回顾一下消失梯度的概念。
- en: 'Essentially, you wish to generate an English poem using an RNN. Here, you set
    up a simple RNN to do your bidding and it ends up producing the following sentence:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，你希望使用RNN生成一首英文诗歌。在这里，你设置了一个简单的RNN来完成任务，结果它生成了以下句子：
- en: '"The flowers, despite it being autumn, blooms like a star".'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '"这些花，尽管已是秋天，却像星星一样盛开。"'
- en: One can easily spot the grammatical error here. The word 'blooms' should be
    'bloom' since at the beginning of the sentence, the word 'flowers' indicates that
    you should be using the plural form of the word 'bloom' to bring about the subject-verb
    agreement in the sentence. A simple RNN fails at this job because it is incapable
    of retaining any information about a dependency between the word 'flowers' that
    occurs early in the sentence and the word 'blooms,' which occurs much later (theoretically,
    it should be able to!).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易发现这里的语法错误。单词'blooms'应该是'bloom'，因为句子开头的单词'flowers'表示应该使用'bloom'的复数形式，以使主谓一致。简单的RNN无法完成这一任务，因为它无法保留句子开头出现的单词'flowers'和后面出现的单词'blooms'之间的依赖关系（理论上，它应该能够做到！）。
- en: A **GRU** helps to solve this issue by eliminating the 'vanishing gradient'
    problem that hinders the learning ability of the network where long-term relationships
    within the text are not preserved by the network. In the following sections, we'll
    focus our attention on understanding the vanishing gradient problem and discuss
    how a GRU resolves the issue in more detail
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**GRU**（门控循环单元）通过消除“消失梯度”问题来帮助解决这一问题，该问题妨碍了网络的学习能力，在长时间的文本关系中，网络未能保留这些关系。在接下来的部分中，我们将专注于理解消失梯度问题，并详细讨论
    GRU 如何解决这个问题。'
- en: 'Let''s now recall how a neural network learns. In the training phase, the inputs
    get propagated, layer by layer, up to the output layer. Since we know the exact
    value that the output should be producing for a given input during training, we
    calculate the error between the expected output and the output obtained. This
    error is then fed into a cost function (which varies depending on the problem
    and the creativity of the network developer). Now, the next step is to calculate
    the gradient of this cost function with respect to every parameter of the network,
    starting from the layer nearest to the output layer right down to the bottom layer
    where the input layer is present:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回顾一下神经网络是如何学习的。在训练阶段，输入逐层传播，直到输出层。由于我们知道在训练过程中，对于给定输入，输出应该产生的确切值，我们计算预期输出和实际输出之间的误差。然后，这个误差被输入到一个成本函数中（这个成本函数会根据问题和网络开发者的创意而有所不同）。接下来的步骤是计算该成本函数相对于网络每个参数的梯度，从离输出层最近的层开始，一直到最底层的输入层：
- en: '![Figure 6.2: A simple neural network'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.2：一个简单的神经网络'
- en: '](img/C13783_06_02.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_02.jpg)'
- en: 'Figure 6.2: A simple neural network'
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.2：一个简单的神经网络
- en: Consider a very simple neural network with only four layers and only one connection
    between each layer and one single output, as shown in the preceding diagram. Note
    that you will never use such a network in practice; it is presented here only
    for demonstrating the concept of vanishing gradients.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个非常简单的神经网络，只有四层，并且每一层之间只有一个连接，并且只有一个单一的输出，如前图所示。请注意，实际应用中你不会使用这样的网络；它这里只是用来演示消失梯度问题的概念。
- en: 'Now, to calculate the gradient of the cost function with respect to the bias
    term of the first hidden layer (b[1]), the following calculation needs to be carried
    out:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了计算成本函数相对于第一隐藏层偏置项 b[1] 的梯度，需要进行以下计算：
- en: '![Figure 6.3: Gradient calculation using chain rule'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.3：使用链式法则计算梯度'
- en: '](img/C13783_06_03.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_03.jpg)'
- en: 'Figure 6.3: Gradient calculation using chain rule'
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.3：使用链式法则计算梯度
- en: 'Here, each element can be explained as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，每个元素的解释如下：
- en: grad(x, y) = the gradient of x with respect to y
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: grad(x, y) = x 关于 y 的梯度
- en: d(var) = the derivative of 'sigmoid' of the 'var' variable
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: d(var) = 'var' 变量的 'sigmoid' 导数
- en: w[i] = the weight of the 'i' layer
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: w[i] = 第 'i' 层的权重
- en: b[i] = the bias term in the 'i' layer
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: b[i] = 第 'i' 层的偏置项
- en: a[i] = the activation function of the 'i' layer
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: a[i] = 第 'i' 层的激活函数
- en: z[j] = w[j]*a[j-1] + b[j]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: z[j] = w[j]*a[j-1] + b[j]
- en: The preceding expression can be attributed to the chain rule of differentiation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 上述表达式可以归因于微分链式法则。
- en: 'The preceding equation involves the multiplication of several terms. If the
    values of most of these terms are a fraction between -1 and 1, the multiplication
    of such fractions will yield a term with a very small value at the end. In the
    preceding example, the value of grad(C,b[1]) will a very small fraction. The problem
    here is, this gradient is the term that will be used to update the value of b[1]
    for the next iteration:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程涉及多个项的乘法。如果这些项中的大多数值是在 -1 和 1 之间的分数，那么这些分数的乘积最终会得到一个非常小的值。在上述例子中，grad(C,b[1])
    的值将是一个非常小的分数。问题在于，这个梯度是将在下一次迭代中用来更新 b[1] 值的项：
- en: '![Figure 6.4: Updating value of b[1] using the gradient'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.4：使用梯度更新 b[1] 的值'
- en: '](img/C13783_06_04.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_04.jpg)'
- en: 'Figure 6.4: Updating value of b[1] using the gradient'
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.4：使用梯度更新 b[1] 的值
- en: Note
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: There could be several ways of performing an update using different optimizers,
    but the concept remains essentially the same.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同优化器执行更新可能有几种方式，但概念本质上是相同的。
- en: The consequence of this action is that the value of b[1] hardly changes from
    the last iteration, which leads to a very slow learning progress. In a real-world
    network, which might be several layers deep, this update will be still smaller.
    Hence, the deeper the network, the more severe the problem with gradients. Another
    observation made here is that the layers that are closer to the output layer learn
    quicker than those that are closer to the input layer since there are fewer multiplication
    terms. This also leads to an asymmetry in learning, leading to the instability
    of the gradients.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况的结果是，b[1]的值几乎没有从上一次迭代中改变，这导致了非常缓慢的学习进展。在一个现实世界的网络中，可能有很多层，这种更新会变得更加微小。因此，网络越深，梯度问题就越严重。这里还观察到，靠近输出层的层学习得比靠近输入层的层更快，因为前者的乘法项更少。这也导致了学习的不对称性，进而引发了梯度的不稳定性。
- en: So, what bearing does this issue have on simple RNNs? Recall the structure of
    RNNs;. it is essentially an unfolding of layers in time with as many layers as
    there are words (for a modelling problem). The learning proceeds through Backpropagation
    Through Time (BPTT), which is exactly the same as the regime that was described
    previously. The only difference is that the same parameters are updated in every
    layer. The later layers correspond to the words that appear later in the sentence,
    while the earlier layers are those that correspond to the words appearing earlier
    in the sentence. With vanishing gradients, the earlier layers do not change much
    from their initial values and, hence, they fail to have much effect on the later
    layers. The more far-back-in-time a layer is from a given layer at time, 't',
    the less influential it is for determining the output of the layer at 't'. Hence,
    in our example sentence, the network struggles to learn that the word 'flowers'
    is plural, which results in the wrong form of the word 'bloom' being used.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这个问题对简单RNN有什么影响呢？回顾一下RNN的结构；它本质上是一个随着时间展开的层次结构，层数与单词数量相同（对于建模问题）。学习过程通过时间反向传播（BPTT）进行，这与之前描述的机制完全相同。唯一的区别是，每一层的相同参数都会被更新。后面的层对应的是句子中后出现的单词，而前面的层对应的是句子中先出现的单词。由于梯度消失，前面的层与初始值的变化很小，因此它们对后续层的影响也很小。距离当前时间点't'越远的层，对该时间点层输出的影响就越小。因此，在我们的示例句子中，网络很难学习到“flowers”是复数形式，这导致了错误的“bloom”形式被使用。
- en: The Exploding Gradient Problem
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度爆炸问题
- en: As it turns out, gradients not only vanish but they can explode as well – that
    is, early layers can learn too quickly with a large deviation in values from one
    training iteration to the next, while the gradients of the later layers don't
    change very quickly. How can this happen? Well, revisiting our equation, if the
    value of individual terms is much larger than the magnitude of 1, a multiplicative
    effect results in the gradients becoming huge. This leads to a destabilization
    of the gradients and causes issues with learning.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，梯度不仅会消失，还可能会爆炸——也就是说，早期层可能学习得过快，训练迭代之间的值偏差较大，而后期层的梯度变化则不那么迅速。这是如何发生的呢？好吧，回顾一下我们的方程式，如果单个项的值远大于1的数量级，乘法效应就会导致梯度变得非常大。这会导致梯度的不稳定并引发学习问题。
- en: Ultimately, the problem is one of unstable gradients. In practice, the vanishing
    gradients problem is much more common and harder to solve than the exploding gradients
    problem.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这个问题是梯度不稳定的问题。实际上，梯度消失问题比梯度爆炸问题更常见，也更难解决。
- en: 'Fortunately, the exploding gradient problem has a robust solution: clipping.
    Clipping simply refers to stopping the value of gradients from growing beyond
    a predefined value. If the value is not clipped, you will begin seeing NaNs (Not
    a Number) for the gradients and weights of the network due to the representational
    overflow of computers. Providing a ceiling for the value will help to avoid this
    issue. Note that clipping only curbs the magnitude of the gradient, but not its
    direction. So, the learning still proceeds in the correct direction. A simple
    visualization of the effect of gradient clipping can be seen in the following
    diagram:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，梯度爆炸问题有一个强有力的解决方案：裁剪（clipping）。裁剪仅仅是指停止梯度值的增长，防止其超过预设的值。如果梯度值没有被裁剪，你将开始看到梯度和网络权重出现NaN（不是一个数字），这是由于计算机的表示溢出。为梯度值设定上限可以帮助避免这个问题。请注意，裁剪只限制梯度的大小，而不限制其方向。因此，学习过程仍然沿着正确的方向进行。梯度裁剪效果的简单可视化可以在下图中看到：
- en: '![Figure 6.5: Clipping gradients to combat the explosion of gradients'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.5：裁剪梯度以应对梯度爆炸'
- en: '](img/C13783_06_05.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_05.jpg)'
- en: 'Figure 6.5: Clipping gradients to combat the explosion of gradients'
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.5：裁剪梯度以应对梯度爆炸
- en: Gated Recurrent Units (GRUs)
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 门控循环单元（GRUs）
- en: GRUs help the network to remember long-term dependencies in an explicit manner.
    This is achieved by introducing more variables in the structure of a simple RNN.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: GRUs帮助网络以显式的方式记住长期依赖关系。这是通过在简单的RNN结构中引入更多的变量实现的。
- en: 'So, what will help us to get rid of the vanishing gradients problem? Intuitively
    speaking, if we allow the network to transfer most of the knowledge from the activation
    function of the previous timesteps, then an error can be backpropagated more faithfully
    than a simple RNN case. If you are familiar with residual networks for image classification,
    then you will recognize this function as being similar to that of a skip connection.
    Allowing the gradient to backpropagate without vanishing enables the network to
    learn more uniformly across layers and, hence, eliminates the issue of gradient
    instability:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，什么可以帮助我们解决梯度消失问题呢？直观地说，如果我们允许网络从前一个时间步的激活函数中转移大部分知识，那么误差可以比简单的RNN情况更忠实地反向传播。如果你熟悉用于图像分类的残差网络，你会发现这个函数与跳跃连接（skip
    connection）非常相似。允许梯度反向传播而不消失，能够使网络在各层之间更加均匀地学习，从而消除了梯度不稳定的问题：
- en: '![Figure 6.6: The full GRU structure'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.6：完整的GRU结构'
- en: '](img/C13783_06_06.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_06.jpg)'
- en: 'Figure 6.6: The full GRU structure'
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.6：完整的GRU结构
- en: 'The different signs in the preceding diagram are as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 上图中不同符号的含义如下：
- en: '![Figure 6.7: The meanings of the different signs in the GRU diagram'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.7：GRU图中不同符号的含义'
- en: '](img/C13783_06_07.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_07.jpg)'
- en: 'Figure 6.7: The meanings of the different signs in the GRU diagram'
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.7：GRU图中不同符号的含义
- en: Note
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The Hadamard product operation is an elementwise matrix multiplication.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 哈达玛积运算是逐元素矩阵乘法。
- en: The preceding diagram has all its components exploited by a GRU. You can observe
    the activation functions, h, represented at different timesteps (**h[t]**, **h[t-1]**).
    The **r[t]** term refers to the reset gate and **z[t]** term refers to the update
    gate. The **h'[t]** term refers to a candidate function, which we'll represent
    using the **h_candidate[t]** variable in the equation for the purpose of being
    explicit. The GRU layer uses the update gate to decide on the amount of previous
    information that can be passed onto the next activation, while it uses the reset
    gate to determine the amount of previous information to forget. In this section,
    we shall examine each of these terms in detail and explore how they help the network
    to remember long-term relations in the text structure.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了GRU所利用的所有组件。你可以观察到不同时间步（**h[t]**、**h[t-1]**）下的激活函数h。**r[t]**项表示重置门，**z[t]**项表示更新门。**h'[t]**项表示候选函数，为了明确表达，我们将在方程中用**h_candidate[t]**变量表示它。GRU层使用更新门来决定可以传递给下一个激活的先前信息量，同时使用重置门来决定需要忘记的先前信息量。在本节中，我们将详细检查这些术语，并探讨它们如何帮助网络记住文本结构中的长期关系。
- en: 'The expression for the activation function (hidden layer) for the next layer
    is as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下一层的激活函数（隐藏层）的表达式如下：
- en: '![Figure 6.8: The expression for the activation function for the next layer
    in terms of the candidate activation function](img/C13783_06_08.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8：下一层激活函数的表达式，以候选激活函数为基础](img/C13783_06_08.jpg)'
- en: 'Figure 6.8: The expression for the activation function for the next layer in
    terms of the candidate activation function'
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.8：下一层激活函数的表达式，以候选激活函数为基础
- en: The activation function is, therefore, a weighing of the activation from the
    previous timestep and a candidate activation function for this timestep. The **z[t]**
    function is a sigmoid function and, hence, it takes a value between 0 and 1\.
    In most practical cases, the value is closer to 0 or 1\. Before going into the
    preceding expression in more depth, let's take a moment to observe the effect
    of the introduction of a weighted summing scheme for updating the activation function.
    If the value of **z[t]** remains 1 for several timesteps, then that means the
    value of the activation function at a very early timestep can still be propagated
    to a much later timestep. This, in turn, provides the network with a memory.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，激活函数是上一时间步的激活与当前时间步的候选激活函数的加权和。**z[t]** 函数是一个 sigmoid 函数，因此其取值范围在 0 和 1 之间。在大多数实际情况下，值更接近
    0 或 1。在深入探讨前述表达式之前，让我们稍微观察一下引入加权求和方案更新激活函数的效果。如果**z[t]** 在多个时间步中保持为 1，则表示非常早期时间步的激活函数的值仍然可以传递到更晚的时间步。这反过来为网络提供了记忆能力。
- en: Additionally, observe how this is different to a simple RNN, where the value
    of the activation function is overwritten at every timestep without an explicit
    weighing of the previous timestep activation (the contribution of the previous
    activation in a simple RNN is present within the nonlinearity and, hence, is implicit).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请观察它与简单 RNN 的不同之处，在简单 RNN 中，激活函数的值在每个时间步都会被覆盖，而不会显式地加权前一时间步的激活（在简单 RNN 中，前一激活的贡献是通过非线性操作隐含存在的）。
- en: Types of Gates
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 门类型
- en: Let's now expand on the previous equation for the activation update in the following
    sections.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在接下来的部分中展开讨论前述的激活更新方程。
- en: The Update Gate
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更新门
- en: 'The update gate is represented by the following diagram. As you can see from
    the full GRU diagram, only the relevant parts are highlighted. The purpose of
    the update gate is to determine the amount of information that needs to be passed
    on from the previous timesteps to the next step activation. To understand the
    diagram and the function of the update gate, consider the following expression
    for calculating the update gate:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 更新门由下图表示。正如你从完整的 GRU 图中看到的，只有相关部分被突出显示。更新门的目的是确定从前一时间步传递到下一步激活的必要信息量。为了理解该图和更新门的功能，请考虑以下计算更新门的表达式：
- en: '![Figure 6.9: The expression for calculating the update gate'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.9：计算更新门的表达式'
- en: '](img/C13783_06_09.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_09.jpg)'
- en: 'Figure 6.9: The expression for calculating the update gate'
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.9：计算更新门的表达式
- en: 'The following figure shows a graphical representation of the update gate:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了更新门的图形表示：
- en: '![Figure 6.10: The update gate in a full GRU diagram'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.10：完整 GRU 图中的更新门'
- en: '](img/C13783_06_10.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_10.jpg)'
- en: 'Figure 6.10: The update gate in a full GRU diagram'
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.10：完整 GRU 图中的更新门
- en: The number of hidden states is **n_h** (the dimensionality of **h**), while
    the number of input dimensions is n_x. The input at timestep t (**x[t]**), is
    multiplied by a new set of weights, **W_z**, using the dimensions (**n_h, n_x**).
    The activation function from the previous timestep, (**h[t-1]**), is multiplied
    by another new set of weights, **U_z**, using the dimensions (**n_h, n_h**).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏状态的数量是**n_h**（**h**的维度），而输入维度的数量是n_x。时间步 t 的输入（**x[t]**）将与一组新权重 **W_z** 相乘，维度为（**n_h,
    n_x**）。上一时间步的激活函数（**h[t-1]**）将与另一组新权重 **U_z** 相乘，维度为（**n_h, n_h**）。
- en: 'Note that the multiplications here are matrix multiplications. These two terms
    are then added together and passed through a sigmoid function to squish the output,
    **z[t]**, within a range of [0,1]. The **z[t]** output has the same dimensions
    as the activation function, that is, (**n_h, 1**). The **W_z** and **U_z** parameters
    also need to be learned using BPTT. Let''s write a simple Python snippet to aid
    in our understanding of the update gate:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里的乘法是矩阵乘法。然后将这两个项相加，并通过 sigmoid 函数将输出 **z[t]** 压缩到 **[0,1]** 范围内。**z[t]**
    输出具有与激活函数相同的维度，即（**n_h, 1**）。**W_z** 和 **U_z** 参数也需要通过 BPTT 进行学习。让我们编写一个简单的 Python
    代码片段来帮助我们理解更新门：
- en: '[PRE0]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](img/C13783_06_11.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C13783_06_11.jpg)'
- en: 'Figure 6.11: A screenshot displaying the weights and activation functions'
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.11：显示权重和激活函数的截图
- en: 'Following is the code snippet for update gate expression:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是更新门表达式的代码片段：
- en: '[PRE1]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the previous code snippet, we initialised the random values for **x[t]**,
    **W_z**, **U_z**, and **h_prev** in order to demonstrate the calculation of **z[t]**
    . In a real network, these variables will have more relevant values.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们初始化了 **x[t]**、**W_z**、**U_z** 和 **h_prev** 的随机值，以演示 **z[t]** 的计算。在真实的网络中，这些变量将具有更相关的值。
- en: The Reset Gate
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重置门
- en: 'The reset gate is represented by the following diagram. As you can see from
    the full GRU diagram, only the relevant parts are highlighted. The purpose of
    the reset gate is to determine the amount of information from the previous timestep
    that should be forgotten in order to calculate the next step activation. In order
    to understand the diagram and the function of the reset gate, consider the following
    expression for calculating the reset gate:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 重置门由以下图示表示。从完整的 GRU 图示中可以看到，只有相关部分被突出显示。重置门的目的是确定在计算下一个步骤的激活时，应该遗忘多少来自前一个时间步的信息。为了理解图示和重置门的功能，考虑以下用于计算重置门的表达式：
- en: '![Figure 6.12: The expression for calculating the reset gate'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.12：计算重置门的表达式'
- en: '](img/C13783_06_12.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_12.jpg)'
- en: 'Figure 6.12: The expression for calculating the reset gate'
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.12：计算重置门的表达式
- en: 'The following figure shows a graphical representation of the reset gate:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了重置门的图形表示：
- en: '![Figure 6.13: The reset gate'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.13：重置门'
- en: '](img/C13783_06_13.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_13.jpg)'
- en: 'Figure 6.13: The reset gate'
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.13：重置门
- en: The input at timestep, **t**, is multiplied by the weights, **W_r**, using the
    dimensions (**n_h, n_x**). The activation function from the previous timestep,
    (**h[t-1]**), is then multiplied by another new set of weights, **U_r**, using
    the dimensions (**n_h, n_h**). Note that the multiplications here are matrix multiplications.
    These two terms are then added together and passed through a sigmoid function
    to squish the r[t] output within a range of **[0,1]**. The **r[t]** output has
    the same dimensions as the activation function, that is, (**n_h, 1**).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间步 **t** 处的输入与权重 **W_r** 相乘，使用维度（**n_h, n_x**）。然后，将前一个时间步的激活函数（**h[t-1]**）与另一组新权重
    **U_r** 相乘，使用维度（**n_h, n_h**）。请注意，这里的乘法是矩阵乘法。然后将这两个项相加，并通过 sigmoid 函数将 **r[t]**
    输出压缩到 **[0,1]** 范围内。**r[t]** 输出具有与激活函数相同的维度，即（**n_h, 1**）。
- en: 'The **W_r** and **U_r** parameters also need to be learned using BPTT. Let''s
    take a look at how to calculate the reset gate expression in Python:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**W_r** 和 **U_r** 参数也需要通过 BPTT 进行学习。让我们来看一下如何在 Python 中计算重置门的表达式：'
- en: '[PRE2]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the preceding snippet, the values of the **x_t**, **h_prev**, **n_h**, and
    **n_x** variables have been used from the update gate code snippet. Note that
    the values of **r_t** may not be particularly close to either 0 or 1 since it
    is an example. In a well-trained network, the values are expected to be close
    to 0 or 1:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，已经使用了来自更新门代码片段的 **x_t**、**h_prev**、**n_h** 和 **n_x** 变量的值。请注意，**r_t**
    的值可能不会特别接近 0 或 1，因为这只是一个示例。在经过良好训练的网络中，值应该接近 0 或 1：
- en: '![Figure 6.14: A screenshot displaying the values of the weights'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.14：显示权重值的截图'
- en: '](img/C13783_06_14.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_14.jpg)'
- en: 'Figure 6.14: A screenshot displaying the values of the weights'
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.14：显示权重值的截图
- en: '![Figure 6.15: A screenshot displaying the r_t output'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.15：显示 `r_t` 输出的截图'
- en: '](img/C13783_06_15.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_15.jpg)'
- en: 'Figure 6.15: A screenshot displaying the r_t output'
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.15：显示 `r_t` 输出的截图
- en: The Candidate Activation Function
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 候选激活函数
- en: 'A candidate activation function for replacing the previous timestep activation
    function is also calculated at every timestep. As the name suggests, the candidate
    activation function represents an alternative value that the next timestep activation
    function should take. Take a look at the expression for calculating the candidate
    activation function, as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步长还计算了一个候选激活函数，用于替换前一个时间步长的激活函数。顾名思义，候选激活函数表示下一个时间步长激活函数应该采取的替代值。看看以下计算候选激活函数的表达式：
- en: '![Figure 6.16: The expression for calculating the candidate activation function'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.16: 计算候选激活函数表达式'
- en: '](img/C13783_06_16.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_16.jpg)'
- en: 'Figure 6.16: The expression for calculating the candidate activation function'
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 6.16: 计算候选激活函数表达式'
- en: 'The following figure shows a graphical representation of the candidate activation
    function:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了候选激活函数的图形表示：
- en: '![Figure 6.17: The candidate activation function'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.17: 候选激活函数'
- en: '](img/C13783_06_17.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_17.jpg)'
- en: 'Figure 6.17: The candidate activation function'
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 6.17: 候选激活函数'
- en: 'The input at timestep, t, is multiplied by the weights, W, using the dimensions
    (**n_h**, **n_x**). The W matrix serves the same purpose as the matrix that is
    used in a simple RNN. Then, an element-wise multiplication is carried out between
    the reset gate and the activation function from the previous timestep, (**h[t-1]**).
    This operation is referred to as ''hadamard multiplication''. The result of this
    multiplication is matrix-multiplied by U using the dimensions (**n_h, n_h**).
    The U matrix is the same matrix that is used with a simple RNN. These two terms
    are then added together and passed through a hyperbolic tan function to squish
    the output **h_candidate[t]** within a range of [-1,1]. The **h_candidate[t]**
    output has the same dimensions as the activation function, that is, (**n_h, 1**):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间步 t 处的输入与权重 W 进行了维度 (**n_h**, **n_x**) 的乘法。W 矩阵的作用与简单 RNN 中使用的矩阵相同。然后，重置门与上一个时间步长的激活函数
    (**h[t-1]**) 进行逐元素乘法。这个操作被称为 'Hadamard 乘法'。乘法的结果通过维度为 (**n_h, n_h**) 的 U 矩阵进行矩阵乘法。U
    矩阵与简单 RNN 中使用的矩阵相同。然后将这两项加在一起，并通过双曲正切函数进行处理，以使输出 **h_candidate[t]** 被压缩到 [-1,1]
    的范围内。**h_candidate[t]** 输出的维度与激活函数相同，即 (**n_h, 1**)：
- en: '[PRE3]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Again, the same values for the variables have been used as in the calculation
    of the update and reset gate. Note that the Hadamard matrix multiplication has
    been implemented using the NumPy function, ''multiply'':'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用与更新门和重置门计算中相同的变量值。请注意，Hadamard 矩阵乘法是使用 NumPy 函数 'multiply' 实现的：
- en: '![Figure 6.18: A screenshot displaying how the W and U weights are defined'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.18: 展示了 W 和 U 权重如何定义的截图'
- en: '](img/C13783_06_18.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_18.jpg)'
- en: 'Figure 6.18: A screenshot displaying how the W and U weights are defined'
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 6.18: 展示了 W 和 U 权重如何定义的截图'
- en: 'The following figure shows a graphical representation of the **h_candidate**
    function:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了 **h_candidate** 函数的图形表示：
- en: '![Figure 6.19: A screenshot displaying the value of h_candidate'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.19: 展示了 h_candidate 值的截图'
- en: '](img/C13783_06_19.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_19.jpg)'
- en: 'Figure 6.19: A screenshot displaying the value of h_candidate'
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 6.19: 展示了 h_candidate 值的截图'
- en: 'Now, since the values of the update gate, the reset gate, and the candidate
    activation function have been calculated, we can code up the expression for the
    current activation function that will be passed onto the next layer:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于已经计算出更新门、重置门和候选激活函数的值，我们可以编写用于传递到下一层的当前激活函数表达式：
- en: '[PRE4]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Figure 6.20: A screenshot displaying the value of the current activation
    function'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.20: 展示了当前激活函数的值的截图'
- en: '](img/C13783_06_20.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_20.jpg)'
- en: 'Figure 6.20: A screenshot displaying the value of the current activation function'
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 6.20: 展示了当前激活函数的值的截图'
- en: 'Mathematically speaking, the update gate serves the purpose of selecting a
    weighting between the previous activation function and the candidate activation
    function. Hence, it is responsible for the final update of the activation function
    for the current timestep and in determining how much of the previous activation
    function and candidate activation function will pass onto the next layer. The
    reset gate acts as a way to select or unselect the parts of the previous activation
    function. This is why an element-wise multiplication is carried out between the
    previous activation function and the reset gate vector. Consider our previous
    example of the poem generation sentence:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度讲，更新门的作用是选择前一个激活函数和候选激活函数之间的加权。因此，它负责当前时间步的激活函数的最终更新，并决定多少前一个激活函数和候选激活函数将传递到下一层。复位门的作用是选择或取消选择前一个激活函数的部分。这就是为什么要对前一个激活函数和复位门向量进行逐元素乘法的原因。考虑我们之前提到的诗歌生成句子的例子：
- en: '"The flowers, despite it being autumn, blooms like a star."'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: “尽管是秋天，花朵像星星一样绽放。”
- en: A reset gate will serve to remember that the word 'flowers' affect the plurality
    of the word 'bloom,' which occurs toward the end of the sentence. Hence, the particular
    value in the reset gate vector that is responsible for remembering the plurality
    or singularity of the word will hold a value that is closer to the values of 0
    or 1\. If a 0 value denotes that the word is singular, then, in our case, the
    reset gate will hold the value of 1 in order to remember that the word 'bloom'
    should now hold the plural form. Different values in the reset gate vector will
    remember different relations within the complex structure of the sentence.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一个复位门将用于记住单词“flowers”对单词“bloom”复数形式的影响，这种影响发生在句子的后半部分。因此，复位门向量中特定的值负责记住单词的复数或单数形式，该值将接近0或1。如果0表示单数，那么在我们的例子中，复位门将保持值为1，以记住单词“bloom”应采用复数形式。复位门向量中的不同值将记住句子复杂结构中的不同关系。
- en: 'As another example, consider the following sentence:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个例子，请考虑以下句子：
- en: '"The food from France was delicious, but French people were also very accommodating."'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: “来自法国的食物很美味，但法国人也非常热情好客。”
- en: 'Examining the structure of the sentence, we can see that there are several
    complex relations that need to be kept in mind:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析句子结构时，我们可以看到有几个复杂的关系需要记住：
- en: The word 'food' corresponds with the word 'delicious' (here, 'delicious' can
    only be used in the context of 'food').
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词“food”与单词“delicious”相关（这里，“delicious”只能在“food”的语境中使用）。
- en: The word 'France' corresponds with 'French' people.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词“France”与“French”人相关。
- en: The word 'people' and 'were' are related to each other; that is, the use of
    the word 'people' dictates that the correct form of 'was' is used.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词“people”和“were”是相互关联的；也就是说，单词“people”的使用决定了“was”的正确形式。
- en: In a well-trained network, the reset gate will have an entry in its vector for
    all such relations. The value of these entries will be suitably turned 'off' or
    'on' depending on which relationship needs to be remembered from the previous
    activations and which needs to be forgotten. In practice, it is difficult to ascribe
    an entry of the reset gate or hidden state to a particular function. The interpretability
    of deep learning networks is, hence, a hot research topic.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个训练良好的网络中，复位门的向量中会包含所有这些关系的条目。这些条目的值将根据需要记住哪些来自先前激活的关系，哪些需要遗忘，适当地被设置为“关闭”或“开启”。在实践中，很难将复位门或隐藏状态的条目归因于某个特定功能。因此，深度学习网络的可解释性仍然是一个热门的研究话题。
- en: GRU Variations
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GRU的变种
- en: The form of GRU just described form of a GRU is the full GRU. Several independent
    researchers have utilized different forms of GRU, such as by removing the reset
    gate entirely or by using activation functions. The full GRU is, however, still
    the most used approach.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 上面描述的GRU的形式是完整的GRU。许多独立的研究者已使用不同形式的GRU，比如完全移除复位门或使用激活函数。然而，完整的GRU仍然是最常用的方法。
- en: Sentiment Analysis with GRU
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于GRU的情感分析
- en: 'Sentiment analysis is a popular use case for applying natural language processing
    techniques. The aim of sentiment analysis is to determine whether a given piece
    of text can be considered as conveying a ''positive'' sentiment or a ''negative''
    sentiment. For example, consider the following text reviewing a book:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是应用自然语言处理技术的一个热门用例。情感分析的目的是判断给定的文本是否可以被视为传达“正面”情绪或“负面”情绪。例如，考虑以下文本对一本书的评论：
- en: '"The book had its moments of glory, but seemed to be missing the point quite
    frequently. An author of such calibre certainly had more in him than what was
    delivered through this particular work."'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '"这本书有过辉煌的时刻，但似乎经常偏离了要点。这样水准的作者，肯定比这本书所呈现的要更多。"'
- en: To a human reader, it is perfectly clear that the mentioned book review conveys
    a negative sentiment. So, how would you go about building a machine learning model
    for the classification of sentiments? As always, for using a supervised learning
    approach, a text corpus containing several samples is needed. Each piece of text
    in this corpus should have a label indicating whether the text can be mapped to
    a positive or a negative sentiment. The next step will be to build a machine learning
    model using this data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人类读者来说，显然提到的这本书评传达了一个负面情绪。那么，如何构建一个机器学习模型来进行情感分类呢？和往常一样，使用监督学习方法时，需要一个包含多个样本的文本语料库。语料库中的每一篇文本应该有一个标签，指示该文本是可以映射到正面情绪还是负面情绪。下一步是使用这些数据构建一个机器学习模型。
- en: 'Observing the example sentence, you can already see that such a task could
    be challenging for a machine learning model to solve. If a simple tokenization
    or TFIDF approach is used, the words such as ''glory'' and ''calibre'' would be
    easily misunderstood by the classifier as conveying a positive sentiment. To make
    matters worse, there is no word in the text that can be directly interpreted as
    negative. This observation also brings about the need to connect different parts
    of the text structure in order to derive a meaning out of the sentence. For instance,
    the first sentence can be broken into two parts:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 观察这个例子，你已经可以看到这个任务对于机器学习模型来说可能是具有挑战性的。如果使用简单的分词或 TFIDF 方法，像‘辉煌’和‘水准’这样的词语可能会被分类器误认为是传达正面情绪。更糟糕的是，文本中没有任何词语可以直接解读为负面情绪。这一观察也揭示了需要连接文本中不同部分结构的必要性，以便从句子中提取出真正的意义。例如，第一句话可以被分解为两部分：
- en: '"The book had its moments of glory"'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '"这本书有过辉煌的时刻"'
- en: '",but seemed to be missing the point quite frequently."'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '"，但似乎经常偏离了要点。"'
- en: Looking at just the first part of the sentence can lead you to conclude that
    the remark is a positive one. It is only when the second sentence is taken into
    consideration that the meaning of the sentence can be truly understood as depicting
    negative feelings. Hence, there is a need to retain long term dependency here.
    A simple RNN is, therefore, not good enough for the task. Let's now apply a GRU
    to a sentiment classification task and see how it performs.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅看句子的第一部分，可能会让你得出评论是积极的结论。只有在考虑到第二句话时，句子的含义才真正能被理解为表达负面情感。因此，这里需要保留长期依赖关系。简单的
    RNN 模型显然无法胜任这项任务。那么我们来尝试在情感分类任务中应用 GRU，并看看它的表现。
- en: 'Exercise 25: Calculating the Model Validation Accuracy and Loss for Sentiment
    Classification'
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 25：计算情感分类模型的验证准确率和损失
- en: 'In this exercise, will we code up a simple sentiment classification system
    using the imdb dataset. The imdb dataset consists of 25,000 train text sequences
    and 25,000 test text sequences – each containing a review for a movie. The output
    variable is a binary variable having a value of 0 if the review is negative, and
    a value of 1 if the review is positive:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用 imdb 数据集编写一个简单的情感分类系统。imdb 数据集包含 25,000 个训练文本序列和 25,000 个测试文本序列——每个序列都包含一篇电影评论。输出变量是一个二元变量，如果评论为负面，则值为
    0；如果评论为正面，则值为 1：
- en: Note
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: All exercises and activities should be run in a Jupyter notebook. The requirements.txt
    file for creating the Python environment for running this notebook is as h5py==2.9.0,
    keras==2.2.4, numpy==1.16.1, tensorflow==1.12.0.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 所有练习和活动都应该在 Jupyter notebook 中运行。用于创建 Python 环境以运行此 notebook 的 `requirements.txt`
    文件内容如下：h5py==2.9.0，keras==2.2.4，numpy==1.16.1，tensorflow==1.12.0。
- en: '**Solution:**'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案：**'
- en: 'We begin by loading the dataset, as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载数据集，如下所示：
- en: '[PRE5]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s also define the maximum number of topmost frequent words to consider
    when generating the sequence for training as 10,000\. We will also restrict the
    sequence length to 500:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将定义生成训练序列时要考虑的最大频率最高的单词数为10,000。我们还将限制序列长度为500：
- en: '[PRE6]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s now load the data as follows:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们按如下方式加载数据：
- en: '[PRE7]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Figure 6.21: A screenshot showing the train and test sequences'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.21：显示训练和测试序列的截图'
- en: '](img/C13783_06_21.jpg)'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13783_06_21.jpg)'
- en: 'Figure 6.21: A screenshot showing the train and test sequences'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.21：显示训练和测试序列的截图
- en: 'There could be sequences having a length that is shorter than 500; therefore,
    we need to pad them out to have a length of exactly 500\. We can use a Keras function
    for this purpose:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能存在长度小于500的序列，因此我们需要对其进行填充，使其长度恰好为500。我们可以使用Keras函数来实现这一目的：
- en: '[PRE8]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s examine the shapes of the train and test data, as follows:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查训练和测试数据的形状，如下所示：
- en: '[PRE9]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Verify that the shape of both the arrays is (25,000, 500).
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 验证两个数组的形状是否为(25,000, 500)。
- en: 'Let''s now build an RNN with a GRU unit. First, we need to import the necessary
    packages, as follows:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们构建一个带有GRU单元的RNN。首先，我们需要导入必要的包，如下所示：
- en: '[PRE10]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Since we'll use the sequential API of Keras to build the model, we need to import
    the sequential model API from the Keras model. The embedding layer essentially
    turns input vectors into a fixed size, which can then be fed to the next layer
    of the network. If used, it must be added as the first layer to the network. We
    also import a Dense layer, since it is this layer that ultimately gives a distribution
    over the target variable (0 or 1).
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们将使用Keras的顺序API来构建模型，因此需要从Keras模型中导入顺序模型API。嵌入层本质上将输入向量转换为固定大小，然后可以将其馈送到网络的下一层。如果使用，它必须作为网络的第一层添加。我们还导入了一个Dense层，因为最终正是这个层提供了目标变量（0或1）的分布。
- en: 'Finally, we import the GRU unit; let''s initialize the sequential model API
    and add the embedding layer, as follows:'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们导入GRU单元；让我们初始化顺序模型API并添加嵌入层，如下所示：
- en: '[PRE11]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The embedding layer takes max_features as input, which is defined by us to be
    10,000\. The 32 value is set here as the next GRU layer expects 32 inputs from
    the embedding layer.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 嵌入层接受max_features作为输入，我们定义它为10,000。32的值在此处设置，因为下一个GRU层期望从嵌入层获取32个输入。
- en: 'Next, we''ll add the GRU and the dense layer, as follows:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将添加GRU和Dense层，如下所示：
- en: '[PRE12]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The 32 value is arbitrarily chosen and can function as one of the hyperparameters
    to tune when designing the network. It represents the dimensionality of the activation
    functions. The dense layer only gives out the 1 value, which is a probability
    of the review (that is, our target variable) to be 1\. We choose sigmoid as the
    activation function here.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 32的值是随意选择的，并可以作为设计网络时调整的超参数之一。它表示激活函数的维度。Dense层只输出1个值，即评论（即我们的目标变量）为1的概率。我们选择sigmoid作为激活函数。
- en: 'Next, we compile the model with the binary cross-entropy loss and the rmsprop
    optimizer:'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们使用二元交叉熵损失和rmsprop优化器来编译模型：
- en: '[PRE13]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We choose to track the accuracy (train and validation) as the metric. Next,
    we fit the model on our sequence data. Note that we also assign 20% of the sample
    from the training data as the validation dataset. We also set the number of epochs
    to be 10 and the batch_size to be 128 – that is, in a single forward-backward
    pass, we choose to pass 128 sequences in a single batch:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们选择将准确率（训练和验证）作为度量指标。接下来，我们将模型拟合到我们的序列数据上。请注意，我们还将从训练数据中分配20%的样本作为验证数据集。我们还设置了10个周期和128的批次大小——即在一次前向反向传递中，我们选择在一个批次中传递128个序列：
- en: '[PRE14]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Figure 6.22: A screenshot displaying the variable history output of the training
    model'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.22：显示训练模型的变量历史输出的截图'
- en: '](img/C13783_06_22.jpg)'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13783_06_22.jpg)'
- en: 'Figure 6.22: A screenshot displaying the variable history output of the training
    model'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.22：显示训练模型的变量历史输出的截图
- en: The variable history can be used to keep track of the training progress. The
    previous function will trigger a training session, which, on a local CPU, should
    take a couple of minutes to train.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 变量历史可以用来跟踪训练进度。上一个函数将触发一个训练会话，在本地CPU上训练需要几分钟时间。
- en: 'Next, let''s take a look at how exactly the training progressed by plotting
    the losses and accuracy. For this, we''ll define a plotting function as follows:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们通过绘制损失和准确率来查看训练进展的具体情况。为此，我们将定义一个绘图函数，如下所示：
- en: '[PRE15]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s call our function on the history variable that us obtained as an output
    of the ''fit'' function:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在作为 'fit' 函数输出的 history 变量上调用我们的函数：
- en: '[PRE16]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'When run by author, the output of the preceding code looks like the following
    diagram:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当作者运行时，前面的代码输出如下面的图示所示：
- en: '**Expected Output:**'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**期望输出：**'
- en: '![Figure 6.23: The training and validation accuracy for the sentiment classification
    task'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.23：情感分类任务的训练和验证准确率](img/C13783_06_23.jpg)'
- en: '](img/C13783_06_23.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_23.jpg)'
- en: 'Figure 6.23: The training and validation accuracy for the sentiment classification
    task'
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.23：情感分类任务的训练和验证准确率
- en: 'The following diagram demonstrates the training and validation loss:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了训练和验证的损失：
- en: '![Figure 6.24: The training and validation loss for the sentiment classification
    task'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.24：情感分类任务的训练和验证损失](img/C13783_06_24.jpg)'
- en: '](img/C13783_06_24.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_24.jpg)'
- en: 'Figure 6.24: The training and validation loss for the sentiment classification
    task'
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.24：情感分类任务的训练和验证损失
- en: Note
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The validation accuracy is pretty high in the best epoch (~87%).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在最佳时期，验证准确率相当高（约87%）。
- en: 'Activity 7: Developing a Sentiment Classification Model Using a Simple RNN'
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 7：使用简单 RNN 开发情感分类模型
- en: In this activity, we aim to generate a model for sentiment classification using
    a simple RNN. This is done to judge the effectiveness of GRUs over simple RNNs.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们的目标是使用一个简单的 RNN 生成一个情感分类模型。这样做是为了判断 GRU 相较于简单 RNN 的效果。
- en: Load the dataset.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集。
- en: Pad the sequences out so that each sequence has the same number of characters.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填充序列，使每个序列具有相同数量的字符。
- en: Define and compile the model using a simple RNN with 32 hidden units.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义并编译模型，使用带有 32 个隐藏单元的简单 RNN。
- en: Plot the validation and training accuracy and losses.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制验证和训练准确率与损失。
- en: Note
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for the activity can be found on page 317.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第 317 页找到。
- en: Text Generation with GRUs
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 GRU 进行文本生成
- en: The problem of text generation requires an algorithm in order to come up with
    new text based on a training corpus. For example, if you feed the poems of Shakespeare
    into a learning algorithm, then the algorithm should be able to generate new text
    (character by character or word by word) in the style of Shakespeare. We will
    now see how to approach this problem with what we have learned in this chapter.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成问题需要一个算法来根据训练语料库生成新文本。例如，如果你将莎士比亚的诗歌输入学习算法，那么该算法应该能够以莎士比亚的风格生成新文本（逐字符或逐词生成）。接下来，我们将展示如何使用本章所学的内容来处理这个问题。
- en: 'Exercise 26: Generating Text Using GRUs'
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 26：使用 GRU 生成文本
- en: 'So, let''s revisit the problem that we introduced in the previous section of
    this chapter. That is, you wish to use a deep learning method to generate a poem.
    Let''s go about solving this problem using a GRU. We will be using The Sonnets
    written by Shakespeare to train our model so that our output poem is in the style
    of Shakespeare:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们回顾一下本章前面部分提出的问题。也就是说，你希望使用深度学习方法生成一首诗。我们将使用 GRU 来解决这个问题。我们将使用莎士比亚的十四行诗来训练我们的模型，以便我们的输出诗歌呈现莎士比亚风格：
- en: 'Let''s begin by importing the required Python packages, as follows:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先导入所需的 Python 包，如下所示：
- en: '[PRE17]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The use of each package will become clear in the code snippets that follow.
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个包的使用将在接下来的代码片段中变得清晰。
- en: 'Next, we define a function that reads from the file that contains the Shakespearean
    sonnets and prints out the first 200 characters:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数，从包含莎士比亚十四行诗的文件中读取内容并打印出前 200 个字符：
- en: '[PRE18]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Figure 6.25: A screenshot of THE SONNETS](img/C13783_06_25.jpg)'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 6.25：莎士比亚十四行诗截图](img/C13783_06_25.jpg)'
- en: 'Figure 6.25: A screenshot of THE SONNETS'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.25：莎士比亚十四行诗截图
- en: 'Next, we''ll perform certain data preparation steps. First, we will get a list
    of the distinct characters from the file that was read in. We will then make a
    dictionary that maps each character to an integer index. Finally, we will create
    another dictionary that maps an integer index to the characters:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将进行一些数据准备步骤。首先，我们将从读取的文件中获取所有不同字符的列表。然后，我们将创建一个字典，将每个字符映射到一个整数索引。最后，我们将创建另一个字典，将整数索引映射到字符：
- en: '[PRE19]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, we will generate the sequences for the training data from the text. We
    will feed a fixed length of 40 characters per sequence for the model. The sequences
    will be made such that there is a sliding window of three steps with each sequence.
    Consider the following part of the poem:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将从文本中生成训练数据的序列。我们会为模型输入每个固定长度为40个字符的序列。这些序列将按滑动窗口的方式生成，每个序列滑动三步。考虑诗歌中的以下部分：
- en: '"From fairest creatures we desire increase,'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '"从最美的生物中，我们渴望增长，'
- en: That thereby beauty's rose might never die,"
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这样美丽的玫瑰就永远不会凋谢，"
- en: 'We aim to achieve the following result from the preceding snippet of text:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是从前面的文本片段中得到以下结果：
- en: '![Figure 6.26: A screenshot of the training sequences'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.26：训练序列的屏幕截图'
- en: '](img/C13783_06_26.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_06_26.jpg)'
- en: 'Figure 6.26: A screenshot of the training sequences'
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.26：训练序列的屏幕截图
- en: These are sequences with a length of 40 characters each. Each subsequent string
    is shifted by three steps to the right of the previous string. This arrangement
    is so that we end up with enough sequences (but not too many, which would be the
    case with a step of 1). In general, we could have more sequences, but since this
    example is a demonstration and, hence, will run on a local CPU, feeding in too
    many sequences will make the training process much longer than desired.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这些序列的长度均为40个字符。每个后续字符串都比前一个字符串向右滑动了三步。这样安排是为了确保我们得到足够的序列（而不会像步长为1时那样得到太多序列）。通常，我们可以有更多的序列，但由于这个例子是演示用的，因此将运行在本地CPU上，输入过多的序列会使训练过程变得比预期要长得多。
- en: 'Additionally, for each of these sequences, we need to have one output character
    that is the next character in the text. Essentially, we are teaching the model
    to observe 40 characters and then learn what the next most likely character will
    be. To understand what the output character might be, consider the following sequence:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于这些序列中的每一个，我们需要有一个输出字符，作为文本中的下一个字符。本质上，我们是在教模型观察40个字符，然后学习下一个最可能的字符是什么。为了理解输出字符是什么，可以考虑以下序列：
- en: That thereby beauty's rose might never d
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这样美丽的玫瑰就永远不会凋谢
- en: 'The output character for this sequence will be the i character. This is because
    in the text, i is the next character. The following code snippet achieves the
    same:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 该序列的输出字符将是i字符。这是因为在文本中，i是下一个字符。以下代码片段实现了相同的功能：
- en: '[PRE20]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We now have the sequences that we wish to train on and the corresponding character
    output for the same. We will now need to obtain a training matrix for the samples
    and another matrix for the output characters, which can be fed to the model to
    train:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了希望训练的序列及其对应的字符输出。接下来，我们需要为样本获得一个训练矩阵，并为输出字符获得另一个矩阵，这些矩阵将被输入到模型中进行训练：
- en: '[PRE21]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here, x is the matrix that holds our input training samples. The shape of the
    x array is the number of sequences, the maximum number of characters, and the
    number of distinct characters. Therefore, x is a three-dimensional matrix. So,
    for each sequence, that is, for every timestep (= maximum number of characters),
    we have a one-hot-coded vector with the same length as the number of distinct
    characters in the text. This vector has a value of 1, where the character at the
    given step is present, and all the other entries are 0\. y is a two-dimensional
    matrix with the shape of the number of sequences and the number of distinct characters).
    Thus, for every sequence, we have a one-hot-coded vector with the same length
    as the number of distinct characters. This vector has all the entries as 0 except
    for the one that corresponds to the current output character. The one-hot-encoding
    is accomplished using the dictionary mappings that we created in the earlier step.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，x是存储输入训练样本的矩阵。x数组的形状是序列的数量、最大字符数和不同字符的数量。因此，x是一个三维矩阵。所以，对于每个序列，也就是对于每个时间步（=最大字符数），我们都有一个与文本中不同字符数量相同长度的独热编码向量。这个向量在给定时间步的字符位置上为1，其他所有位置为0。y是一个二维矩阵，形状为序列数和不同字符数。因此，对于每个序列，我们都有一个与不同字符数量相同长度的独热编码向量。除了与当前输出字符对应的位置为1，其他所有位置都是0。独热编码是通过我们在之前步骤中创建的字典映射完成的。
- en: 'We are now ready to define our model, as follows:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在准备好定义我们的模型，如下所示：
- en: '[PRE22]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We make use of the sequential API, add a GRU layer with 128 hidden parameters,
    and then add a dense layer.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用顺序API，添加一个具有128个隐藏参数的GRU层，然后添加一个全连接层。
- en: Note
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The dense layer has the same number of outputs as the number of distinct characters.
    This is because we're essentially learning a distribution of the possible characters
    in our vocabulary. In this sense, this is essentially a multiclass classification
    problem, which also explains our choice of categorical cross-entropy for the cost
    function.
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Dense层的输出数量与不同字符的数量相同。这是因为我们本质上是在学习我们词汇表中可能字符的分布。从这个意义上讲，这本质上是一个多类分类问题，这也解释了我们为何选择分类交叉熵作为代价函数。
- en: 'We will now go ahead and fit our model to the data, as follows:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在继续将模型拟合到数据上，如下所示：
- en: '[PRE23]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here, we have selected a batch size of 128 sequences and training for 10 epochs.
    We will also save the model in hdf5 format file for later use:'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们选择了128个序列的批量大小，并训练了10个周期。我们还将模型保存在hdf5格式文件中，以便以后使用：
- en: '![Figure 6.27: A screenshot displaying epochs'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.27：显示训练周期的截图'
- en: '](img/C13783_06_27.jpg)'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13783_06_27.jpg)'
- en: 'Figure 6.27: A screenshot displaying epochs'
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.27：显示训练周期的截图
- en: Note
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You should increase the number of the GRUs and epochs. The higher the value
    for these, the more time it will take to train the model and better results can
    be expected.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该增加GRU的数量和训练周期。它们的值越高，训练模型所需的时间就越长，但可以期待更好的结果。
- en: 'Next, we need to be able to use the model to actually generate some text, as
    follows:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要能够使用模型实际生成一些文本，如下所示：
- en: '[PRE24]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We also define a sampling function that selects a candidate character given
    a probability distribution over the number of characters:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还定义了一个采样函数，根据字符的概率分布选择一个候选字符：
- en: '[PRE25]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We are sampling using a multinomial distribution; the temperature parameter
    helps to add bias to the probability distribution such that the less likely words
    can have more or less representation. You can also simply try to return an argument
    argmax over the preds variable, but this will likely result in a repetition of
    words:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用多项分布进行采样；温度参数有助于为概率分布添加偏差，使得较不可能的词可以有更多或更少的表示。你也可以简单地尝试对preds变量使用argmax参数，但这可能会导致单词的重复：
- en: '[PRE26]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We pass the loaded model and the number of characters that we wish to generate.
    We then pass a seed text for the model to use as the input (remember, we taught
    the model to predict the next character given a sequence length of 40 characters).
    This is being done before the for loop kicks in. In the first pass of the loop,
    we pass our seed text to the model, generate the output character, and append
    the output character in the ''generated'' variable. In the next pass, we shift
    our newly updated sequence (with 41 characters after first pass) to the right
    by one character, so that the model can now take this 40 character input with
    the last character being the new character that we just generated. The function
    can now be called as follows:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们传入加载的模型和希望生成的字符数。然后，我们传入一个种子文本供模型作为输入（记住，我们教模型在给定40个字符的序列长度的情况下预测下一个字符）。这一过程发生在for循环开始之前。在循环的第一次迭代中，我们将种子文本传入模型，生成输出字符，并将输出字符附加到‘generated’变量中。在下一次迭代中，我们将刚更新的序列（在第一次迭代后有41个字符）右移一个字符，这样模型就可以将这个包含40个字符的新输入作为输入，其中最后一个字符是我们刚刚生成的新字符。现在，可以按如下方式调用函数：
- en: '[PRE27]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'And voila! You have a poem written in Shakespearean style. An example output
    is shown as follows:'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 看！你已经写出了一首莎士比亚风格的诗歌。一个示例输出如下所示：
- en: '![](img/C13783_06_28.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C13783_06_28.jpg)'
- en: 'Figure 6.28: A screenshot displaying the output of the generated poem sequence'
  id: totrans-266
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.28：显示生成的诗歌序列输出的截图
- en: 'You will immediately notice that the poem does not really make sense. This
    can be attributed to two reasons:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 你会立刻注意到诗歌并没有真正有意义。这可以归因于两个原因：
- en: The preceding output was generated with a very small amount of data or sequences.
    Therefore, the model was unable to learn much. In practice, you would use a much
    larger dataset, make many more sequences out if it, and train the model using
    GPUs for a practical training time (we will learn about training on the cloud
    GPU in the last chapter 9- 'A practical flow NLP project workflow in an organization').
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上述输出是在使用非常少量的数据或序列时生成的。因此，模型无法学到太多。在实践中，你会使用一个更大的数据集，从中生成更多的序列，并使用GPU进行训练以确保合理的训练时间（我们将在最后一章9-‘组织中的实际NLP项目工作流’中学习如何在云GPU上训练）。
- en: Even if trained with a massive amount of data, there will always be some errors
    since a model can only learn so much.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使使用大量数据进行训练，也总会有一些错误，因为模型的学习能力是有限的。
- en: We can still, however, see that even with this basic setup there are words that
    make sense despite our model being a character generation model. There are phrases
    such as 'I have liking' that are valid as standalone phrases.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们仍然可以看到，即使在这个基本设置下，模型是一个字符生成模型，仍然有些词语是有意义的。像“我有喜好”这样的短语，作为独立短语是有效的。
- en: Note
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: White space, newline characters, and more are also being learned by the model.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 空白字符、换行符等也被模型学习。
- en: 'Activity 8: Train Your Own Character Generation Model Using a Dataset of Your
    Choice'
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 8：使用你选择的数据集训练你自己的字符生成模型
- en: We just used some of Shakespeare's work to generate our own poem. You don't
    need to restrict yourself to poem generation but you can use any piece of text
    to start generating your own piece of writing. The basic steps and setup remains
    same as discussed in the previous example.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚使用了一些莎士比亚的作品来生成我们自己的诗歌。你不必局限于诗歌生成，也可以使用任何一段文本来开始生成自己的写作。基本的步骤和设置与之前的示例相同。
- en: Note
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Create a conda environment using the requirements file and activate it. Then,
    run the code in a Jupyter notebook. Don't forget to input a text file containing
    the text from an author in whose style you wish to generate new text.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 requirements 文件创建一个 Conda 环境并激活它。然后，在 Jupyter notebook 中运行代码。别忘了输入一个包含你希望生成新文本的作者风格的文本文件。
- en: Load the text file.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载文本文件。
- en: Create dictionaries mapping the characters to indices and vice versa.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建字典，将字符映射到索引，反之亦然。
- en: Create sequences from the text.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从文本中创建序列。
- en: Make input and output arrays to feed to the model.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建输入和输出数组以供模型使用。
- en: Build and train the model using GRU.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 GRU 构建并训练模型。
- en: Save the model.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存模型。
- en: Define the sampling and generation functions.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义采样和生成函数。
- en: Generate the text.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成文本。
- en: Note
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for the activity can be found on page 320.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 活动的解决方案可以在第320页找到。
- en: Summary
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A GRU is an extension of a simple RNN, which helps to combat the vanishing gradient
    problem by allowing the model to learn long-term dependencies in the text structure.
    A variety of use cases can benefit from this architectural unit. We discussed
    a sentiment classification problem and learned how GRUs perform better than simple
    RNNs. We then saw how text can be generated using GRUs.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: GRU 是简单 RNN 的扩展，它通过让模型学习文本结构中的长期依赖关系来帮助解决梯度消失问题。各种用例可以从这个架构单元中受益。我们讨论了情感分类问题，并了解了
    GRU 如何优于简单 RNN。接着我们看到如何使用 GRU 来生成文本。
- en: In the next chapter, we talk about another advancement over a simple RNN – Long
    Short-Term Memory (LSTM) networks, and explore what advantages they bring with
    their new architecture.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论简单 RNN 的另一项进展——长短期记忆（LSTM）网络，并探讨它们通过新架构带来的优势。
