- en: '*Chapter 3*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第3章*'
- en: Fundamentals of Natural Language Processing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理基础
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你将能够：
- en: Classify different areas of natural language processing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类自然语言处理的不同领域
- en: Analyze basic natural language processing libraries in Python
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析Python中的基本自然语言处理库
- en: Predict the topics in a set of texts
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测一组文本中的主题
- en: Develop a simple language model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发一个简单的语言模型
- en: This chapter covers different fundamentals and areas of natural language processing,
    along with its libraries in Python.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了自然语言处理的不同基础知识和领域，并介绍了Python中的相关库。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: '**Natural Language Processing** (**NLP**) is an area of **Artificial Intelligence**
    (**AI**) with the goal of enabling computers to understand and manipulate human
    language in order to perform useful tasks. Within this area, there are two sections:
    **Natural Language Understanding** (**NLU**) and **Natural Language Generation**
    (**NLG**).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）是**人工智能**（**AI**）的一个领域，旨在使计算机能够理解并处理人类语言，以执行有用的任务。在这个领域中，有两个部分：**自然语言理解**（**NLU**）和**自然语言生成**（**NLG**）。'
- en: In recent years, AI has changed the way machines interact with humans. AI helps
    people solve complex equations by performing tasks such as recommending a movie
    according to your tastes (recommender systems). Thanks to the high performance
    of GPUs and the huge amount of data available, it's possible to create intelligent
    systems that are capable of learning and behaving like humans.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，人工智能改变了机器与人类的互动方式。人工智能通过执行一些任务帮助人们解决复杂的方程式，比如根据你的口味推荐电影（推荐系统）。得益于GPU的高性能和海量数据的可用性，已经可以创建能够学习并表现得像人类一样的智能系统。
- en: There are many libraries that aim to help with the creation of these systems.
    In this chapter, we will review the most famous Python libraries to extract and
    clean information from raw text. You may consider this task complex, but a complete
    understanding and interpretation of the language is a difficult task in itself.
    For example, the sentence "Cristiano Ronaldo scores three goals" would be hard
    for a machine to understand because it would not know who Cristiano Ronaldo is
    or what is meant by the number of goals.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多库旨在帮助创建这些系统。在本章中，我们将回顾一些最著名的Python库，用于从原始文本中提取和清理信息。你可能会认为这项任务很复杂，但语言的完整理解和解释本身就是一项艰巨的任务。例如，“Cristiano
    Ronaldo进了三个球”这句话对于机器来说很难理解，因为它不知道Cristiano Ronaldo是谁，也不知道“进了三个球”是什么意思。
- en: One of the most popular topics in NLP is **Question Answering** (**QA**). This
    discipline also consists of **Information Retrieva**l (**IR**). These systems
    construct answers by querying a database for knowledge or information, but they
    are capable of extracting answers from a collection of natural language documents.
    That is how a search engine such as Google works.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）中最受欢迎的主题之一是**问答系统**（**QA**）。这个领域还包含**信息检索**（**IR**）。这些系统通过查询数据库中的知识或信息来构建答案，但它们也能够从一组自然语言文档中提取答案。这就是像谷歌这样的搜索引擎的工作原理。
- en: In the industry today, NLP is becoming more and more popular. The latest NLP
    trends are online advertisement matching, sentiment analysis, automated translation,
    and chatbots.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 目前在行业中，自然语言处理正变得越来越流行。最新的NLP趋势包括在线广告匹配、情感分析、自动翻译和聊天机器人。
- en: Conversational agents, popularly known as chatbots, are the next challenge for
    NLP. They can hold real conversation and many companies use them to get feedback
    about their products or to create a new advertising campaign, by analyzing the
    behavior and opinions of clients through the chatbot. Virtual assistants are a
    great example of NLP and they have already been introduced to the market. The
    most famous are Siri, Amazon's Alexa, and Google Home. In this book, we will create
    a chatbot to control a virtual robot that is able to understand what we want the
    robot to do.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对话代理，通常被称为聊天机器人，是自然语言处理的下一个挑战。它们能够进行真实的对话，许多公司使用它们来获取产品反馈或创建新的广告活动，通过分析客户在聊天机器人中的行为和意见。虚拟助手是自然语言处理的一个极好例子，它们已经进入市场。最著名的包括Siri、亚马逊的Alexa和Google
    Home。在本书中，我们将创建一个聊天机器人来控制一个虚拟机器人，使其能够理解我们希望机器人做什么。
- en: Natural Language Processing
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: 'As mentioned before, NLP is an AI field that takes care of understanding and
    processing human language. NLP is located at the intersection between AI, computer
    science, and linguistics. The main aim of this area is to make computers understand
    statements or words written in human languages:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，NLP 是一个涉及理解和处理人类语言的人工智能领域。NLP 位于人工智能、计算机科学和语言学的交汇点。这个领域的主要目标是让计算机理解用人类语言写成的陈述或词语：
- en: '![Figure 3.1: Representation of NLP within AI, linguistics, and computer science](img/C13550_03_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1：人工智能、语言学和计算机科学中的自然语言处理表示](img/C13550_03_01.jpg)'
- en: 'Figure 3.1: Representation of NLP within AI, linguistics, and computer science'
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.1：人工智能、语言学和计算机科学中的自然语言处理表示
- en: Linguistic science focuses on the study of human language, trying to characterize
    and explain the different approaches of language.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 语言学科学专注于研究人类语言，试图描述和解释语言的不同方法。
- en: A language can be defined as a set of rules and a set of symbols. Symbols are
    combined and used to broadcast information and are structured by rules. Human
    language is special. We cannot simply picture it as naturally formed symbols and
    rules; depending on the context, the meaning of words can change.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 语言可以被定义为一组规则和符号的集合。符号通过规则组合使用，以传达信息。人类语言是特别的。我们不能简单地把它看作是自然形成的符号和规则；根据上下文，词汇的意义可能会发生变化。
- en: NLP is becoming more popular and can solve many difficult problems. The amount
    of text data available is very large, and it is impossible for a human to process
    all that data. In Wikipedia, the average number of new articles per day is 547,
    and in total, there are more than 5,000,000 articles. As you can imagine, a human
    cannot read all that information.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理正变得越来越流行，并且可以解决许多困难的问题。可用的文本数据量非常大，人类无法处理所有这些数据。在维基百科中，每天平均有 547 篇新文章，总共有超过
    500 万篇文章。可以想象，人类无法阅读所有这些信息。
- en: There are three challenges faced by NLP. The first challenge is collecting all
    the data, the second is classifying it, and the final one is extracting the relevant
    information.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 面临三个挑战。第一个挑战是收集所有数据，第二个是对数据进行分类，最后一个是提取相关信息。
- en: NLP solves many tedious tasks, such as spam detection in emails, **part-of-speech**
    (**POS**) tagging, and named entity recognition. With deep learning, NLP can also
    solve voice-to-text problems. Although NLP shows a lot of power, there are some
    cases such as working without having a good solution from the dialog between a
    human and a machine, QA systems summarization and machine translation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理解决了许多繁琐的任务，如电子邮件中的垃圾邮件检测、**词性**（**POS**）标注和命名实体识别。通过深度学习，NLP 还可以解决语音转文本问题。尽管
    NLP 展示了很强的能力，但也存在一些情况，例如在人机对话中没有得到良好的解决方案、问答系统的摘要和机器翻译等。
- en: Parts of NLP
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自然语言处理的部分
- en: 'As mentioned before, NLP can be divided into two groups: NLU and NLG.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，自然语言处理（NLP）可以分为两个部分：自然语言理解（NLU）和自然语言生成（NLG）。
- en: '**Natural Language Understanding**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言理解**'
- en: This section of NLP relates to the understanding and analysis of human language.
    It focusses on the comprehension of text data, and processing it to extract relevant
    information. NLU provides direct human-computer interaction and performs tasks
    related to the comprehension of language.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的自然语言处理涉及对人类语言的理解和分析。它侧重于对文本数据的理解，并处理这些数据以提取相关信息。NLU 提供直接的人机交互，并执行与语言理解相关的任务。
- en: NLU covers the hardest of AI challenges, and that is the interpretation of text.
    The main challenge of NLU is understanding dialog.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: NLU 涉及人工智能最具挑战性的任务之一，那就是文本的理解。NLU 的主要挑战是理解对话。
- en: Note
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: NLP uses a set of methods for generating, processing, and understanding language.
    NLU uses functions to understand the meaning of a text.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理使用一套方法来生成、处理和理解语言。自然语言理解通过功能来理解文本的意义。
- en: 'Previously, a conversation was represented as a tree, but this approach cannot
    cover many dialog cases. To cover more cases, more trees would be required, one
    for each context of the conversation, leading to the repeating of many sentences:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，对话被表示为一棵树，但这种方法无法涵盖许多对话情况。为了覆盖更多的情况，需要更多的树，每个对话上下文对应一棵树，这就导致了许多句子的重复：
- en: '![Figure 3.2: Representation of a dialogue using trees](img/C13550_03_02.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2：使用树表示对话](img/C13550_03_02.jpg)'
- en: 'Figure 3.2: Representation of a dialogue using trees'
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.2：使用树表示对话
- en: 'This approach is outdated and inefficient because is based on fixed rules;
    it''s essentially an if-else structure. But now, NLU has contributed another approach.
    A conversation can be represented as a Venn diagram where each set is a context
    of the conversation:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法已经过时且低效，因为它基于固定的规则；本质上是一个if-else结构。但现在，NLU（自然语言理解）贡献了另一种方法。对话可以表示为一个维恩图，其中每个集合代表对话的一个上下文：
- en: '![Figure 3.3: Representation of a conversation using a Venn diagram](img/C13550_03_03.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3：使用维恩图表示对话](img/C13550_03_03.jpg)'
- en: 'Figure 3.3: Representation of a conversation using a Venn diagram'
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.3：使用维恩图表示对话
- en: As you can see in the previous figures, the NLU approach improves the structure
    of understanding a conversation, because it is not a fixed structure that contains
    if-else conditions. The main goal of NLU is to interpret the meaning of human
    language and deal with the contexts of a conversation, solving ambiguities and
    managing data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在之前的图中所见，NLU方法通过改进对话理解的结构，提升了效果，因为它不是一个包含if-else条件的固定结构。NLU的主要目标是解释人类语言的意义，处理对话的上下文，解决歧义并管理数据。
- en: '**Natural Language Generation**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言生成**'
- en: NLG is the process of producing phrases, sentences, and paragraphs with meaning
    and structure. It is an area of NLP that does not deal with understanding text.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: NLG是生成具有意义和结构的短语、句子和段落的过程。它是NLP的一个领域，不涉及理解文本。
- en: To generate natural language, NLG methods need relevant data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成自然语言，NLG方法需要相关数据。
- en: 'NLG has three components:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: NLG有三个组成部分：
- en: '**Generator**: Responsible for including the text within an intent to have
    it related with the context of the situation'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成器**：负责将文本包含在意图中，使其与情境的上下文相关联。'
- en: '**Components and levels of representations**: Gives structure to the generated
    text'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组成部分和表示层次**：为生成的文本提供结构'
- en: '**Application**: Saves relevant data from the conversation to follow a logical
    thread'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用**：保存对话中的相关数据，以跟随逻辑脉络'
- en: Generated text must be in a human-readable format. The advantages of NLG are
    that you can make your data accessible and you can create summaries of reports
    rapidly.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本必须是人类可读的格式。NLG的优点在于你可以使数据变得易于访问，并且可以迅速创建报告摘要。
- en: Levels of NLP
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自然语言处理的层次
- en: Human language has different levels of representation. Each representation level
    is more complex than the previous level. As we ascend through the levels, it gets
    more difficult to understand the language.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 人类语言有不同的表示层次。每个表示层次都比前一个层次更复杂。随着层次的上升，理解语言变得越来越困难。
- en: 'The two first levels depend on the data type (audio or text), in which we have
    the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个层次依赖于数据类型（音频或文本），具体分为以下几类：
- en: '**Phonological analysis**: If the data is speech, first, we need to analyze
    the audio to have sentences.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音学分析**：如果数据是语音，首先我们需要分析音频以获得句子。'
- en: '**OCR/tokenization**: If we have text, we need to recognize the characters
    and form words using computer vision (OCR). If not, we will need to tokenize the
    text (that is, split the sentence into units of text).'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OCR/分词**：如果我们有文本，需要通过计算机视觉（OCR）识别字符并构成单词。如果没有，我们需要对文本进行分词（即将句子拆分成文本单元）。'
- en: Note
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The OCR process is the identification of characters in an image. Once it generates
    words, they are processed as raw text.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: OCR过程是识别图像中的字符。一旦它生成了单词，这些单词就会作为原始文本进行处理。
- en: '**Morphological analysis**: Focused on the words of a sentence and analyzing
    its morphemes.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**形态学分析**：专注于句子的单词，并分析其语素。'
- en: '**Syntactic analysis**: This level focuses on the grammatical structure of
    a sentence. That means understanding different parts of a sentence, such as the
    subject or the predicate.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**句法分析**：这一层次专注于句子的语法结构。即理解句子中的不同部分，比如主语或谓语。'
- en: '**Semantic representation**: A program does not understand a single word; it
    can know the meaning of a word by knowing how the word is used in a sentence.
    For example, "cat" and "dog" could mean the same for an algorithm because they
    can be used in the same way. Understanding sentences in this way is called word-level
    meaning.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义表示**：程序并不理解单个词汇；它可以通过知道单词在句子中的使用方式来理解词汇的意义。例如，“猫”和“狗”对于算法来说可能意味着相同，因为它们可以以相同的方式使用。通过这种方式理解句子被称为词汇级别的意义。'
- en: '**Discourse processing**: Analyzing and identifying connected sentences in
    a text and their relationships. By doing this, an algorithm could understand what
    the topic of the text is.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**话语处理**：分析和识别文本中连接的句子及其关系。通过这样做，算法可以理解文本的主题是什么。'
- en: NLP shows great potential in today's industry, but there are some exceptions.
    Using deep learning concepts, we can work with some of these exceptions to get
    better results. Some of these problems will be reviewed in *Chapter 4*, *Neural
    Networks with NLP*. The advantage of text processing techniques and the improvement
    of recurrent neural networks are the reasons why NLP is becoming increasingly
    important.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）在今天的行业中展现出巨大的潜力，但也存在一些例外。通过使用深度学习的概念，我们可以处理这些例外问题，从而获得更好的结果。这些问题将在*第
    4 章*，*神经网络与 NLP*中进行回顾。文本处理技术的优势和递归神经网络的改进是 NLP 变得越来越重要的原因。
- en: NLP in Python
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python 中的 NLP
- en: Python has become very popular in recent years, by combining the power of general-purpose
    programming languages with the use of specific domain languages, such as MATLAB
    and R (designed for mathematics and statistics). It has different libraries for
    data loading, visualization, NLP, image processing, statistics, and more. Python
    has the most powerful libraries for text processing and machine learning algorithms.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，Python 变得非常流行，它将通用编程语言的强大功能与特定领域语言的使用相结合，如 MATLAB 和 R（用于数学和统计）。它有不同的库用于数据加载、可视化、NLP、图像处理、统计等。Python
    拥有最强大的文本处理和机器学习算法库。
- en: Natural Language Toolkit (NLTK)
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自然语言工具包（NLTK）
- en: NLTK is the most common kit of tools for working with human language data in
    Python. It includes a set of libraries and programs for processing natural language
    and statistics. NLTK is commonly used as a learning tool and for carrying out
    research.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 是 Python 中用于处理人类语言数据的最常见工具包。它包括一套用于处理自然语言和统计数据的库和程序。NLTK 通常作为学习工具和进行研究时使用。
- en: This library provides interfaces and methods for over 50 corpora and lexical
    resources. NLTK is capable of classifying text and performing other functions,
    such as tokenization, stemming (extracting the stem of a word), tagging (identifying
    the tag of a word, such as person, city…), and parsing (syntax analysis).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 该库提供了超过 50 个语料库和词汇资源的接口和方法。NLTK 能够对文本进行分类并执行其他功能，如分词、词干提取（提取单词的词干）、标注（识别单词的标签，如人名、城市等）和句法分析。
- en: 'Exercise 10: Introduction to NLTK'
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 10：NLTK 入门
- en: 'In this exercise, we will review the most basic concepts about the NLTK library.
    As we said before, this library is one of the most widely used tools for NLP.
    It can be used to analyze and study text, disregarding useless information. These
    techniques can be applied to any text data, for example, to extract the most important
    keywords from a set of tweets or to analyze an article in a newspaper:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将回顾关于 NLTK 库的最基本概念。正如我们之前所说，这个库是自然语言处理（NLP）领域中最广泛使用的工具之一。它可以用来分析和研究文本，忽略无关的信息。这些技术可以应用于任何文本数据，例如，从一组推文中提取最重要的关键词，或分析一篇报纸文章：
- en: Note
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: All the exercises in this chapter will be executed in Google Colab.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有练习将在 Google Colab 中执行。
- en: Open up your Google Colab interface.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你的 Google Colab 界面。
- en: Create a folder for the book.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为书籍创建一个文件夹。
- en: 'Here, we are going to process a sentence with basic methods of the NLTK library.
    First of all, let''s import the necessary methods (`stopwords`, `word_tokenize`,
    and `sent_tokenize`):'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们将使用 NLTK 库的基本方法处理一个句子。首先，让我们导入必要的方法（`stopwords`、`word_tokenize` 和 `sent_tokenize`）：
- en: '[PRE0]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we create a sentence and apply the methods:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们创建一个句子并应用这些方法：
- en: '[PRE1]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](img/C13550_03_04.jpg)'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C13550_03_04.jpg)'
- en: 'Figure 3.4: Sentence divided into a sub-sentence'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.4：将句子划分为子句
- en: '[PRE2]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Fig 3.5: Tokenizing a sentence into words](img/C13550_03_05.jpg)'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.5：将句子分解成单词](img/C13550_03_05.jpg)'
- en: 'Figure 3.5: Tokenizing a sentence into words'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.5：将句子分解成单词
- en: Note
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '`Sent_tokenize` returns a list of different sentences. One of the disadvantages
    of NLTK is that `sent_tokenize` does not analyze the semantic structure of the
    whole text; it just splits the text by the dots.'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Sent_tokenize` 返回一个包含不同句子的列表。NLTK 的一个缺点是 `sent_tokenize` 并没有分析整个文本的语义结构；它只是根据句号将文本分割。'
- en: 'With the sentence tokenized sentence by words, let''s subtract the stop words.
    The stop words are a set of words without relevant information about the text.
    Before using `stopwords`, we will need to download it:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过单词分词后的句子，我们来去除停用词。停用词是一组没有关于文本相关信息的词语。在使用`停用词`之前，我们需要下载它：
- en: '[PRE3]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we set the language of our `stopwords` as English:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将`停用词`的语言设置为英语：
- en: '[PRE4]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output is as follows:'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 3.6: Stopwords set as English](img/C13550_03_06.jpg)'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.6: 停用词设置为英语](img/C13550_03_06.jpg)'
- en: 'Figure 3.6: Stopwords set as English'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 3.6: 停用词设置为英语'
- en: 'Process the sentence, deleting `stopwords`:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理句子，删除`停用词`：
- en: '[PRE5]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 3.7: Sentence without stop words](img/C13550_03_07.jpg)'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.7: 去除停用词后的句子](img/C13550_03_07.jpg)'
- en: 'Figure 3.7: Sentence without stop words'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 3.7: 去除停用词后的句子'
- en: 'We can now modify the set of `stopwords` and check the output:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以修改`停用词`的集合并检查输出：
- en: '[PRE6]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Figure 3.8: Setting stop words](img/C13550_03_08.jpg)'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.8: 设置停用词](img/C13550_03_08.jpg)'
- en: 'Figure 3.8: Setting stop words'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 3.8: 设置停用词'
- en: 'Stemmers remove morphological affixes from words. Let''s define a stemmer and
    process our sentence. `Porter stemmer` is an algorithm for performing this task:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词干提取器去除单词的形态学词缀。让我们定义一个词干提取器并处理我们的句子。`Porter词干提取器`是一种执行此任务的算法：
- en: '[PRE7]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output is as follows:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 3.9: Setting stop words](img/C13550_03_09.jpg)'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.9: 设置停用词](img/C13550_03_09.jpg)'
- en: 'Figure 3.9: Setting stop words'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 3.9: 设置停用词'
- en: 'Finally, let''s classify each word by its type. To do this, we will use a POS
    tagger:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们按类型对每个单词进行分类。为此，我们将使用一个词性标注器：
- en: '[PRE8]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 3.10: POS tagger](img/C13550_03_10.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.10: 词性标注器](img/C13550_03_10.jpg)'
- en: 'Figure 3.10: POS tagger'
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 3.10: 词性标注器'
- en: Note
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The averaged perceptron tagger is an algorithm trained to predict the category
    of a word.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 平均感知机标注器是一种算法，用于预测单词的类别。
- en: As you may have noticed in this exercise, NLTK can easily process a sentence.
    Also, it can analyze a huge set of text documents without any problem. It supports
    many languages and the tokenization process is faster than that for similar libraries,
    and it has many methods for each NLP problem.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在这次练习中可能已经注意到的，NLTK能够轻松处理一个句子。它还可以分析大量文本文档，毫无问题。它支持多种语言，且分词过程比类似的库要快，并且每个NLP问题都有许多方法可供使用。
- en: spaCy
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: spaCy
- en: spaCy is another library for NLP in Python. It does look similar to NLTK, but
    you will see some differences in the way it works.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy是Python中的另一个自然语言处理库。它看起来与NLTK相似，但你会发现它的工作方式有所不同。
- en: spaCy was developed by Matt Honnibal and is designed for data scientists to
    clean and normalize text easily. It's the quickest library in terms of preparing
    text data for a machine learning model. It includes built-in word vectors and
    some methods for comparing the similarity between two or more texts (these methods
    are trained with neural networks).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy由Matt Honnibal开发，旨在帮助数据科学家轻松清理和标准化文本。它是准备机器学习模型文本数据的最快库。它包含内置的词向量和一些方法，用于比较两个或多个文本之间的相似性（这些方法是通过神经网络训练的）。
- en: Its API is easy to use and more intuitive than NLTK. Often, in NLP, spaCy is
    compared to NumPy. It provides methods and functions for performing tokenization,
    lemmatization, POS tagging, NER, dependency parsing, sentence and document similarity,
    text classification, and more.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 它的API易于使用，比NLTK更直观。通常，在自然语言处理（NLP）中，spaCy与NumPy进行比较。它提供了执行分词、词形还原、词性标注、命名实体识别（NER）、依赖解析、句子和文档相似性分析、文本分类等任务的方法和功能。
- en: As well as having linguistic features, it also has statistical models. This
    means you can predict some linguistic annotations, such as whether a word is a
    verb or a noun. Depending on the language you want to make predictions in, you
    will need to change a module. Within this section are Word2Vec models, which we
    will discuss in *Chapter 4*, *Neural Networks with NLP.*
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 它不仅具有语言学特征，还拥有统计模型。这意味着你可以预测一些语言注释，例如判断一个词是动词还是名词。根据你希望进行预测的语言，你需要更改一个模块。在这一部分中有Word2Vec模型，我们将在*第4章*中讨论，*神经网络与自然语言处理*。
- en: spaCy has many advantages, as we said before, but there are some cons too; for
    instance, it supports only 8 languages (NLTK supports 17 languages), the tokenization
    process is slow (and this time-consuming process could be critical on a long corpus),
    and overall, it is not flexible (that is, it just provides API methods without
    the possibility of modifying any parameters).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所说，spaCy有许多优点，但也有一些缺点；例如，它仅支持8种语言（NLTK支持17种语言），分词过程较慢（这个耗时的过程在长文本中可能会很关键），而且总的来说，它不够灵活（也就是说，它只提供了API方法，无法修改任何参数）。
- en: Before starting with the exercise, let's review the architecture of spaCy. The
    most important data structures of spaCy are the Doc and the Vocab.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始练习之前，我们先回顾一下spaCy的架构。spaCy最重要的数据结构是Doc和Vocab。
- en: The Doc structure is the text you are loading; it is not a string. It is composed
    of a sequence of tokens and their annotations. The Vocab structure is a set of
    lookup tables, but what are lookup tables and why is the structure important?
    Well, a lookup table in computation is an array indexing an operation that replaces
    a runtime. spaCy centralizes information that is available across documents. This
    means that it is more efficient, as this saves memory. Without these structures,
    the computational speed of spaCy would be slower.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Doc结构是你正在加载的文本；它不是一个字符串。它由一系列标记及其注释组成。Vocab结构是一组查找表，那么什么是查找表，它为什么重要呢？查找表是计算中的一个数组索引操作，它替代了运行时操作。spaCy将跨文档可用的信息集中化。这意味着它更加高效，因为这样节省了内存。没有这些结构，spaCy的计算速度将会更慢。
- en: However, the structure of Doc is different to Vocab because Doc is a container
    of data. A Doc object owns the data and is composed of a sequence of tokens or
    spans. There are also a few lexemes, which are related to the Vocab structure
    because they do not have context (unlike the token container).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Doc的结构与Vocab不同，因为Doc是数据的容器。一个Doc对象拥有数据，并由一系列标记或跨度组成。还有一些词素（lexemes），它们与Vocab结构有关，因为它们没有上下文（与标记容器不同）。
- en: Note
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: A lexeme is a unit of lexical meaning without having inflectional endings. The
    area of study for this is morphological analysis.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 词素是没有屈折词尾的词汇意义单位。研究这一领域的是形态学分析。
- en: The figure 3.11 shows us the spaCy architecture.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 显示了spaCy架构。
- en: '![Figure 3.11: spaCy architecture](img/C13550_03_11.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.11: spaCy架构](img/C13550_03_11.jpg)'
- en: 'Figure 3.11: spaCy architecture'
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.11：spaCy架构
- en: Depending on the language model you are loading, you will have a different pipeline
    and a different Vocab.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你加载的语言模型不同，你将拥有不同的处理流程和Vocab。
- en: 'Exercise 11: Introduction to spaCy'
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 11：spaCy 简介
- en: 'In this exercise, we will do the same transformations that we performed in
    *Exercise 10*, *Introduction to NLTK*, and to the same sentence as in that exercise
    but with the spaCy API. This exercise will help you to understand and learn about
    the differences between these libraries:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将进行与*练习 10*、*NLTK简介*中相同的转换，使用spaCy API对该练习中的同一个句子进行操作。这个练习将帮助你理解和学习这些库之间的差异：
- en: Open up your Google Colab interface.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你的Google Colab界面。
- en: Create a folder for the book.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为这本书创建一个文件夹。
- en: 'Then, import the package to use all its features:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，导入包以使用它的所有功能：
- en: '[PRE9]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now we are going to initialize our `nlp` object. This object is a part of the
    spaCy methods. By executing this line of code, we are loading the model inside
    the parenthesis:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将初始化我们的`nlp`对象。这个对象是spaCy方法的一部分。通过执行这行代码，我们正在加载括号内的模型：
- en: '[PRE10]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s take the same sentence as in *Exercise 10*, *Introduction to NLTK,*
    and create the Doc container:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用与*练习 10*、*NLTK简介*中相同的句子，并创建Doc容器：
- en: '[PRE11]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, print `doc1`, its format, the 5th and 11th token, and a span between the
    5th and the 11th token. You will see this:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，打印`doc1`、它的格式，第5个和第11个标记，以及第5个和第11个标记之间的跨度。你将看到如下结果：
- en: '[PRE12]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 3.12: Output of a spaCy document](img/C13550_03_12.jpg)'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.12: spaCy 文档输出](img/C13550_03_12.jpg)'
- en: 'Figure 3.12: Output of a spaCy document'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.12：spaCy 文档输出
- en: As we saw in Figure 3.5, documents are composed of tokens and spans. First,
    we are going to see the spans of `doc1`, and then its tokens.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们在图 3.5 中看到的，文档由标记（tokens）和跨度（spans）组成。首先，我们将看到`doc1`的跨度，然后是它的标记。
- en: 'Print the spans:'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 打印跨度：
- en: '[PRE13]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 3.13: Printing the spans of doc1](img/C13550_03_13.jpg)'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.13: 打印 doc1 的跨度](img/C13550_03_13.jpg)'
- en: 'Figure 3.13: Printing the spans of doc1'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.13：打印 doc1 的跨度
- en: 'Print the tokens:'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 打印令牌：
- en: '[PRE14]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/C13550_03_14.jpg)'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C13550_03_14.jpg)'
- en: 'Figure 3.14: Printing the tokens of doc1'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.14：打印 doc1 的令牌
- en: Once we have the document divided into tokens, the stop words can be removed.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们将文档划分为令牌，停用词就可以被去除。
- en: 'First, we need to import them:'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们需要导入它们：
- en: '[PRE15]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 3.15: 10 stop words in spaCy](img/C13550_03_15.jpg)'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.15：spaCy中的10个停用词](img/C13550_03_15.jpg)'
- en: 'Figure 3.15: 10 stop words in spaCy'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.15：spaCy中的10个停用词
- en: 'But the token container has the `is_stop` attribute:'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 但令牌容器有 `is_stop` 属性：
- en: '[PRE16]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 3.16: The is_stop attribute of tokens'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.16：令牌的 `is_stop` 属性](img/C13550_03_16.jpg)'
- en: '](img/C13550_03_16.jpg)'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13550_03_16.jpg)'
- en: 'Figure 3.16: The is_stop attribute of tokens'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.16：令牌的 `is_stop` 属性
- en: 'To add new stop words, we must modify the `vocab` container:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要添加新的停用词，我们必须修改 `vocab` 容器：
- en: '[PRE17]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output here would be as follows:'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的输出结果如下：
- en: 'True'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真
- en: 'To perform speech tagging, we initialize the token container:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要执行词性标注，我们初始化令牌容器：
- en: '[PRE18]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 3.17: The .pos_ attribute of tokens](img/C13550_03_17.jpg)'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.17：令牌的 `.pos_` 属性](img/C13550_03_17.jpg)'
- en: 'Figure 3.17: The .pos_ attribute of tokens'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.17：令牌的 `.pos_` 属性
- en: 'The document container has the `ents` attribute, with the entity of the tokens.
    To have more entities in our document, let''s declare a new one:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文档容器具有 `ents` 属性，包含令牌的实体。为了在文档中包含更多实体，我们可以声明一个新的实体：
- en: '[PRE19]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output is as follows:'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 3.18: The .label_ attribute of tokens](img/C13550_03_18.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.18：令牌的 `.label_` 属性](img/C13550_03_18.jpg)'
- en: 'Figure 3.18: The .label_ attribute of tokens'
  id: totrans-176
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.18：令牌的 `.label_` 属性
- en: Note
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: As you can see in this exercise, spaCy is much easier to use than NLTK, but
    NLTK provides more methods to perform different operations on text. spaCy is perfect
    for production. That means, in the least amount of time, you can perform basic
    processes on text.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在本练习中所见，spaCy比NLTK更易于使用，但NLTK提供了更多方法来执行不同的文本操作。spaCy非常适合用于生产环境。这意味着，在最短的时间内，你就能对文本进行基本处理。
- en: The exercise has ended! You can now pre-process a text using NLTK or spaCy.
    Depending on the task you want to perform, you will be able to choose one of these
    libraries to clean your data.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 练习已结束！现在你可以使用NLTK或spaCy对文本进行预处理。根据你要执行的任务，你将能够选择其中一个库来清理数据。
- en: Topic Modeling
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主题建模
- en: Within NLU, which is a part of NLP, one of the many tasks that can be performed
    is extracting the meaning of a sentence, a paragraph, or a whole document. One
    approach to understanding a document is through its topics. For example, if a
    set of documents is from a newspaper, the topics might be politics or sports.
    With topic modeling techniques, we can obtain a bunch of words representing various
    topics. Depending on your set of documents, you will then have different topics
    represented by different words. The goal of these techniques is to know the different
    types of documents in your corpus.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言理解（NLU）中，作为自然语言处理（NLP）的一部分，许多任务之一是提取句子、段落或整个文档的含义。理解文档的一种方法是通过其主题。例如，如果一组文档来自一份报纸，那么这些主题可能是政治或体育。通过主题建模技术，我们可以获得一组代表不同主题的词语。根据你的文档集，你将拥有由不同词语代表的不同主题。这些技术的目标是了解语料库中不同类型的文档。
- en: Term Frequency – Inverse Document Frequency (TF-IDF)
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 术语频率 – 逆文档频率（TF-IDF）
- en: '**TF-IDF** is a commonly used NLP model for extracting the most important words
    from a document. To perform this classification, the algorithm will assign a weight
    to each word. The idea of this method is to ignore words without relevance to
    the meaning of a global concept, (which means the overall topic of a text), so
    those terms will be down-weighted (which means that they will be ignored). Down-weighing
    them will allow us to find the keywords of that document (the words with the greatest
    weights).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**TF-IDF** 是一种常用的NLP模型，用于从文档中提取最重要的词汇。为了进行这种分类，算法会为每个单词分配一个权重。这种方法的思想是忽略那些与全球概念（即文本的整体主题）无关的词语，因此这些词语的权重会被降低（意味着它们将被忽略）。降低它们的权重将帮助我们找到该文档的关键字（即权重最大的词语）。'
- en: 'Mathematically, the algorithm to find the weight of a term in a document is
    as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，算法用来查找文档中术语权重的方法如下：
- en: '![Figure 3.19: TF-IDF formula](img/C13550_03_19.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.19：TF-IDF 公式](img/C13550_03_19.jpg)'
- en: 'Figure 3.19: TF-IDF formula'
  id: totrans-186
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.19：TF-IDF 公式
- en: '*Wi,j*: Weight of the term, i, in the document, j'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Wi,j*：术语 i 在文档 j 中的权重'
- en: '*tf,j*: Number of occurrences of i in j'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*tf,j*: i在j中的出现次数'
- en: '*df,j*: Number of documents containing i'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*df,j*: 包含i的文档数量'
- en: '*N*: Total number of documents'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N*: 文档的总数'
- en: The result is the number of times a term appears in that document, multiplied
    by the log of the total number of documents, divided by the number of documents
    that contain the term.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是术语在该文档中出现的次数，乘以总文档数的对数，再除以包含该术语的文档数量。
- en: Latent Semantic Analysis (LSA)
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 潜在语义分析（LSA）
- en: LSA is one of the foundational techniques of topic modeling. It analyzes the
    relationship between a set of documents and their terms, and produces a set of
    concepts related to them.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: LSA是主题建模的基础技术之一。它分析文档集与其术语之间的关系，并生成与之相关的一组概念。
- en: LSA is a step ahead when compared to TF-IDF. In a large set of documents, the
    TF-IDF matrix has very noisy information and many redundant dimensions, so the
    LSA algorithm performs dimensionality reduction.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 与TF-IDF相比，LSA更具前瞻性。在大规模文档集中，TF-IDF矩阵包含大量噪声信息和冗余维度，因此LSA算法执行了降维处理。
- en: 'This reduction is performed with Singular Value Decomposition (SVD). SVD factorizes
    a matrix, M, into the product of three separate matrices:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这一降维处理通过奇异值分解（SVD）进行。SVD将矩阵M分解为三个独立矩阵的乘积：
- en: '![Figure 3.20: Singular Value Decomposition](img/C13550_03_20.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图3.20：奇异值分解](img/C13550_03_20.jpg)'
- en: 'Figure 3.20: Singular Value Decomposition'
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.20：奇异值分解
- en: '*A*: This is the input data matrix.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*A*: 这是输入数据矩阵。'
- en: '*m*: This is the number of documents.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*m*: 这是文档的数量。'
- en: '*n*: This is the number of terms.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n*: 这是术语的数量。'
- en: '*U*: Left singular vectors. Our document-topic matrix.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*U*: 左奇异向量。我们的文档-主题矩阵。'
- en: '*S*: Singular values. Represents the strength of each concept. This is a diagonal
    matrix.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S*: 奇异值。表示每个概念的强度。这是一个对角矩阵。'
- en: '*V*: Right singular vectors. Represents terms'' vectors in terms of topics.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*V*: 右奇异向量。表示术语在主题中的向量。'
- en: Note
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This method is more efficient on a large set of documents, but there are better
    algorithms to perform this task such as LDA or PLSA.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该方法在大规模文档集上更为高效，但还有更好的算法可以执行此任务，比如LDA或PLSA。
- en: 'Exercise 12: Topic Modeling in Python'
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习12：Python中的主题建模
- en: 'In this exercise, TF-IDF and LSA will be coded in Python using a specific library.
    By the end of this exercise, you will be able to perform these techniques to extract
    the weights of a term in a document:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，将使用特定的库在Python中编写TF-IDF和LSA代码。完成本练习后，你将能够执行这些技术来提取文档中术语的权重：
- en: Open up your Google Colab interface.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你的Google Colab界面。
- en: Create a folder for the book.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为书籍创建一个文件夹。
- en: 'To generate the TF-IDF matrix, we could code the formula in Figure 3.19, but
    we are going to use one of the most famous libraries for machine learning algorithms
    in Python, scikit-learn:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了生成TF-IDF矩阵，我们可以编写图3.19中的公式，但我们将使用Python中最著名的机器学习库之一——scikit-learn：
- en: '[PRE20]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The corpus we are going to use for this exercise will be simple, with just
    four sentences:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在本练习中使用的语料库非常简单，只有四个句子：
- en: '[PRE21]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'With the `TfidfVectorizer` method, we can convert the collection of documents
    in our corpus to a matrix of TF-IDF features:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`TfidfVectorizer`方法，我们可以将语料库中的文档集合转换为TF-IDF特征矩阵：
- en: '[PRE22]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `get_feature_names()` method shows the extracted features.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`get_feature_names()`方法显示提取的特征。'
- en: Note
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE23]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is as follows:'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 3.21: Feature names of the corpus](img/C13550_03_21.jpg)'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图3.21：语料库的特征名称](img/C13550_03_21.jpg)'
- en: 'Figure 3.21: Feature names of the corpus'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.21：语料库的特征名称
- en: 'X is a sparse matrix. To see its content, we can use the `todense()` function:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: X是一个稀疏矩阵。要查看其内容，我们可以使用`todense()`函数：
- en: '[PRE24]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output is as follows:'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 3.22: TF-IDF matrix of the corpus](img/C13550_03_22.jpg)'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图3.22：语料库的TF-IDF矩阵](img/C13550_03_22.jpg)'
- en: 'Figure 3.22: TF-IDF matrix of the corpus'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.22：语料库的TF-IDF矩阵
- en: 'Now let''s perform dimensionality reduction with LSA. The `TruncatedSVD` method
    uses SVD to transform the input matrix. In this exercise, we''ll use `n_components=10`.
    From now on, you have to use `n_components=100` (it has better results in larger
    corpuses):'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们使用LSA进行降维。`TruncatedSVD`方法使用SVD对输入矩阵进行变换。在本练习中，我们将使用`n_components=10`。从现在开始，你需要使用`n_components=100`（它在较大的语料库中有更好的效果）：
- en: '[PRE25]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output is as follows:'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 23: Dimensionality reduction with LSA](img/C13550_03_23.jpg)'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图23：使用LSA进行降维](img/C13550_03_23.jpg)'
- en: 'Figure 23: Dimensionality reduction with LSA'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图23：使用LSA进行降维
- en: '`attribute .components_` shows the weight of each `vectorizer.get_feature_names()`.
    Notice that the LSA matrix has a range of 4x16, we have 4 documents in our corpus
    (concepts), and the vectorizer has 16 features (terms):'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`attribute .components_` 显示每个 `vectorizer.get_feature_names()` 的权重。注意，LSA 矩阵的范围为
    4x16，我们的语料库中有 4 个文档（概念），而矢量化器有 16 个特征（术语）：'
- en: '[PRE26]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output is as follows:'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 3.24: The desired TF-IDF matrix output'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.24：期望的 TF-IDF 矩阵输出](img/C13550_03_24.jpg)'
- en: '](img/C13550_03_24.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13550_03_24.jpg)'
- en: 'Figure 3.24: The desired TF-IDF matrix output'
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.24：期望的 TF-IDF 矩阵输出
- en: The exercise has ended successfully! This was a preparatory exercise for *Activity
    3*, *Process a Corpus*. Do check the seventh step of the exercise – it will give
    you the key to complete the activity ahead. I encourage you to read the scikit-learn
    documentation and learn how to see the potential of these two methods. Now you
    know how to create the TF-IDF matrix. This matrix could be huge, so to manage
    the data better, the LSA algorithm performs dimensionality reduction on the weight
    of each term in the document.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 练习已经成功结束！这是*活动 3*的预备练习，*处理语料库*。请务必查看练习的第七步——它将为你提供完成后续活动的关键。我鼓励你阅读 scikit-learn
    文档，学习如何发现这两种方法的潜力。现在你已经知道如何创建 TF-IDF 矩阵。这个矩阵可能会非常庞大，因此，为了更好地管理数据，LSA 算法对文档中每个术语的权重进行了降维处理。
- en: 'Activity 3: Process a Corpus'
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 3：处理语料库
- en: In this activity, we will process a really small corpus to clean the data and
    extract the keywords and concepts using LSA.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将处理一个非常小的语料库，通过 LSA 清理数据并提取关键词和概念。
- en: 'Imagine this scenario: the newspaper vendor in your town has published a competition.
    It consists of predicting the category of an article. This newspaper does not
    have a structural database, which means it has only raw data. They provide a small
    set of documents, and they need to know whether the article is political, scientific,
    or sports-related:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下这个场景：你所在城镇的报摊举办了一场比赛。比赛的内容是预测一篇文章的类别。该报纸没有结构化数据库，这意味着它只有原始数据。他们提供了一小组文档，需要知道这篇文章是政治类、科学类还是体育类：
- en: Note
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You can choose between spaCy and the NLTK library to do the activity. Both solutions
    will be valid if the keywords are related at the end of the LSA algorithm.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 spaCy 和 NLTK 库之间选择进行活动。如果在 LSA 算法结束时，关键词相关性得以保留，那么两种解决方案都有效。
- en: Load the corpus documents and store them in a list.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载语料库文档并将其存储在列表中。
- en: Note
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The corpus documents can be found on GitHub, [https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/tree/master/Lesson03/Activity03/dataset](https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/tree/master/Lesson03/Activity03/dataset)
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语料库文档可以在 GitHub 上找到，[https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/tree/master/Lesson03/Activity03/dataset](https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/tree/master/Lesson03/Activity03/dataset)
- en: Pre-process the text with spaCy or NLTK.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 spaCy 或 NLTK 预处理文本。
- en: Apply the LSA algorithm.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用 LSA 算法。
- en: 'Show the first five keywords related to each concept:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示与每个概念相关的前五个关键词：
- en: 'Keywords: moon, apollo, earth, space, nasa'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关键词：moon, apollo, earth, space, nasa
- en: 'Keywords: yard, touchdown, cowboys, prescott, left'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关键词：yard, touchdown, cowboys, prescott, left
- en: 'Keywords: facebook, privacy, tech, consumer, data'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关键词：facebook, privacy, tech, consumer, data
- en: Note
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The output keywords probably will not be the same as yours. If your keywords
    are not related then check the solution.
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的关键词可能与你的不一样。如果你的关键词不相关，请检查解决方案。
- en: 'The output is as follows:'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 3.25: Output example of the most relevant words in a concept (f1)](img/C13550_03_25.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.25：概念中最相关词语的输出示例（f1）](img/C13550_03_25.jpg)'
- en: 'Figure 3.25: Output example of the most relevant words in a concept (f1)'
  id: totrans-257
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.25：概念中最相关词语的输出示例（f1）
- en: Note
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity is available on page 306.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第 306 页找到。
- en: Language Modeling
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言建模
- en: So far, we have reviewed the most basic techniques for pre-processing text data.
    Now we are going to dive deep into the structure of natural language – language
    models. We can consider this topic an introduction to machine learning in NLP.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经回顾了文本数据预处理的最基本技术。现在，我们将深入探讨自然语言的结构——语言模型。我们可以将此话题视为自然语言处理（NLP）中机器学习的入门。
- en: Introduction to Language Models
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语言模型简介
- en: A statistical **Language Model** (**LM**) is the probability distribution of
    a sequence of words, which means, to assign a probability to a particular sentence.
    For example, LMs could be used to calculate the probability of an upcoming word
    in a sentence. This involves making some assumptions about the structure of the
    LM and how it will be formed. An LM is never totally correct with its output,
    but using one is often necessary.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 一个统计**语言模型**（**LM**）是一个单词序列的概率分布，这意味着，它为一个特定的句子分配一个概率。例如，语言模型可以用来计算句子中即将到来的单词的概率。这涉及到对语言模型结构以及如何形成它做出一些假设。一个语言模型的输出从来不是完全正确的，但使用它通常是必要的。
- en: LMs are used in many more NLP tasks. For example, in machine translation, it
    is important to know what sentence precedes the next. LMs are also used for speech
    recognition, to avoid ambiguity, for spelling corrections, and for summarization.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型（LM）在许多NLP任务中都有应用。例如，在机器翻译中，了解下一句前面的句子非常重要。语言模型还用于语音识别，以避免歧义，拼写纠错以及摘要生成等。
- en: 'Let''s see how an LM is mathematically represented:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看语言模型是如何在数学上表示的：
- en: P(W) = P(w1, w2,w3,w4,…wn)
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(W) = P(w1, w2, w3, w4, … wn)
- en: '*P(W)* is our LM and *wi* are the words included in *W*, and as we mentioned
    before, we can use it to compute the probability of an upcoming word in this way:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(W)* 是我们的语言模型（LM），*wi* 是包含在 *W* 中的单词，正如我们之前提到的，我们可以用它来计算即将到来的单词的概率，方式如下：'
- en: P(w5|w1,w2,w3,w4)
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(w5|w1, w2, w3, w4)
- en: This (w1, w2, w3, w4) states what the probability of *w5* (the upcoming word)
    could be in a given sequence of words.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这个（w1, w2, w3, w4）表示在给定的单词序列中，*w5*（即将到来的单词）的概率可能是多少。
- en: 'Looking at this example, P (w5|w1, w2, w3, w4), we can assume this:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 看这个例子，P(w5|w1, w2, w3, w4)，我们可以做出这样的假设：
- en: P(actual word | previous words)
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P（实际单词 | 前一个单词）
- en: Depending on the number of previous words we are looking at to obtain the probability
    of the actual word, there are different models we can use. So, now we are going
    to introduce some important concepts regarding such models.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们查看的前几个单词的数量来获取实际单词的概率，我们可以使用不同的模型。那么，现在我们将介绍一些关于这些模型的重要概念。
- en: The Bigram Model
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Bigram模型
- en: 'The bigram model is a sequence of two consecutive words. For example, in the
    sentence "My cat is white," there are these bigrams:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: bigram模型是由两个连续的单词组成的序列。例如，在句子“我的猫是白色的”中，有以下这些bigram：
- en: My cat
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我的猫
- en: Cat is
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 猫是
- en: Is white
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 是白色的
- en: 'Mathematically, a bigram has this form:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，bigram模型有以下形式：
- en: 'Bigram model: P(wi|wi-1)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bigram模型：P(wi|wi-1)
- en: N-gram Model
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: N-gram模型
- en: If we change the length of the previous word, we obtain the N-gram model. It
    works just like the bigram model but considers more words than the previous set.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们改变前一个单词的长度，就得到了N-gram模型。它的工作原理与bigram模型相似，但考虑的单词比前一个集合更多。
- en: 'Using the previous example of "My cat is white," this is what we can obtain:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前的例子“我的猫是白色的”，我们可以得到以下结果：
- en: '**Trigram**'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Trigram**'
- en: My cat is
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我的猫是
- en: Cat is white
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 猫是白色的
- en: '**4-gram**'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**4-gram**'
- en: My cat is white
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我的猫是白色的
- en: '**N-Gram Problem**'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '**N-Gram问题**'
- en: 'At this point, you could think the n-gram model is more accurate than the bigram
    model because the n-gram model has access to additional "previous knowledge."
    However, n-gram models are limited to a certain extent, because of long-distance
    dependencies. An example would be, "After thinking about it a lot, I bought a
    television," which we compute as:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你可能会认为n-gram模型比bigram模型更准确，因为n-gram模型可以访问更多的“先前知识”。然而，由于长距离依赖，n-gram模型也存在一定的局限性。一个例子是，“经过深思熟虑，我买了一台电视”，我们将其计算为：
- en: P(television| after thinking about it a lot, I bought a)
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P（电视 | 经过深思熟虑，我买了一台）
- en: 'The sentence "After thinking about it a lot, I bought a television" is probably
    the only sequence of words with this structure in our corpus. If we change the
    word "television" for another word, for example "computer," the sentence "After
    thinking about it a lot, I bought a computer" is also valid, but in our model,
    the following would be the case:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 句子“经过深思熟虑，我买了一台电视”可能是我们语料库中唯一具有这种结构的单词序列。如果我们将“电视”这个词换成另一个词，例如“电脑”，句子“经过深思熟虑，我买了一台电脑”也是有效的，但在我们的模型中，以下情况将会发生：
- en: P(computer| after thinking about it a lot, I bought a) = 0
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P（电脑 | 经过深思熟虑，我买了一台） = 0
- en: This sentence is valid, but our model is not accurate, so we need to be careful
    with the use of n-gram models.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 这个句子是有效的，但我们的模型不够准确，所以我们在使用n-gram模型时需要小心。
- en: Calculating Probabilities
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算概率
- en: '**Unigram Probability**'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '**Unigram概率**'
- en: 'The unigram is the simplest case for calculating probabilities. It counts the
    number of times a word appears in a set of documents. Here is the formula for
    this:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 单语是计算概率的最简单情况。它计算一个词在一组文档中出现的次数。它的公式如下：
- en: '![Figure 3.26: Unigram probability estimation](img/C13550_03_26.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.26：单语概率估计](img/C13550_03_26.jpg)'
- en: 'Figure 3.27: Unigram probability estimation'
  id: totrans-298
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.27：单语概率估计
- en: '*c(wi)* is the number of times'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c(wi)* 是出现次数'
- en: '*wi* appears in the whole corpus. The size of the corpus is just how many tokens
    are in it.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*wi* 在整个语料库中出现。语料库的大小就是它包含的词项数量。'
- en: '**Bigram Probability**'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '**双语概率**'
- en: 'To estimate bigram probability, we are going to use maximum likelihood estimation:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计双语概率，我们将使用最大似然估计：
- en: '![Figure 3.27: Bigram probability estimation](img/C13550_03_27.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.27：双语概率估计](img/C13550_03_27.jpg)'
- en: 'Figure 3.27: Bigram probability estimation'
  id: totrans-304
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.27：双语概率估计
- en: To understand this formula better, let's look at an example.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个公式，我们来看一个例子。
- en: 'Imagine our corpus is composed of these three sentences:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的语料库由这三句话组成：
- en: My name is Charles.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我的名字是查尔斯。
- en: Charles is my name.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 查尔斯是我的名字。
- en: My dog plays with the ball.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我的狗在玩球。
- en: 'The size of the corpus is 14 words, and now we are going to estimate the probability
    of the sequence "my name":'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库的大小是14个词，现在我们要估计 "my name" 这一序列的概率。
- en: '![Figure 3.28: Example of bigram estimation](img/C13550_03_28.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.28：双语估计的例子](img/C13550_03_28.jpg)'
- en: 'Figure 3.28: Example of bigram estimation'
  id: totrans-312
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.28：双语估计的例子
- en: '**The Chain Rule**'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '**链式法则**'
- en: Now we know the concepts of bigrams and n-grams, we need to know how we can
    obtain those probabilities.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了双语和n-gram的概念，我们需要了解如何获得这些概率。
- en: 'If you have basic statistics knowledge, you might think the best option is
    to apply the chain rule and join each probability. For example, in the sentence
    "My cat is white," the probability is as follows:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有基本的统计学知识，你可能会认为最好的选择是应用链式法则，将每个概率连接起来。例如，在句子 "My cat is white" 中，概率如下：
- en: P(my cat is white) = p(white|my cat is) p(is|my cat) p(cat|my) p(my)
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(my cat is white) = p(white|my cat is) p(is|my cat) p(cat|my) p(my)
- en: It seems to be possible with this sentence, but if we had a much longer sentence,
    long-distance dependency problems would appear and the result of the n-gram model
    could be incorrect.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎在这个句子中是可行的，但如果我们有一个更长的句子，长距离依赖问题就会出现，n-gram 模型的结果可能会不正确。
- en: '**Smoothing**'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '**平滑**'
- en: So far, we have a probabilistic model, and if we want to estimate the parameters
    of our model, we can use the maximum likelihood of estimation.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们有了一个概率模型，如果我们想估计模型的参数，可以使用最大似然估计法。
- en: One of the big problems of LMs is insufficient data. Our data is limited, so
    there will be many unknown events. What does this mean? It means we'll end up
    with an LM that gives a probability of 0 to unseen words.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型（LM）面临的一个大问题是数据不足。我们的数据是有限的，因此会有许多未知事件。这意味着什么？这意味着我们最终得到的语言模型会对未见过的词汇给出0的概率。
- en: 'To solve this problem, we are going to use a smoothing method. With this smoothing
    method, every probability estimation result will be greater than zero. The method
    we are going to use is add-one smoothing:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们将使用平滑方法。通过这个平滑方法，每个概率估计结果都会大于零。我们将使用的方法是加一平滑：
- en: '![Figure 3.29: Add-one smoothing in bigram estimation](img/C13550_03_29.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.29：双语估计中的加一平滑](img/C13550_03_29.jpg)'
- en: 'Figure 3.29: Add-one smoothing in bigram estimation'
  id: totrans-323
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.29：双语估计中的加一平滑
- en: '*V* is the number of distinct tokens in our corpus.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '*V* 是我们语料库中不同词项的数量。'
- en: Note
  id: totrans-325
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: There are more smoothing methods with better performance; this is the most basic
    method.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更多表现更好的平滑方法；这只是最基本的方法。
- en: '**Markov Assumption**'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '**马尔可夫假设**'
- en: 'Markov assumption is very useful for estimating the probabilities of a long
    sentence. With this method, we can solve the problem of long-distance dependencies.
    Markov assumption simplifies the chain rule to estimate long sequences of words.
    Each estimation only depends on the previous step:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫假设对于估计长句子的概率非常有用。通过这种方法，我们可以解决长距离依赖问题。马尔可夫假设简化了链式法则，用于估计长序列的词汇。每次估计只依赖于前一步：
- en: '![Figure 3.30: Markov assumption](img/C13550_03_30.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.30：马尔可夫假设](img/C13550_03_30.jpg)'
- en: 'Figure 3.30: Markov assumption'
  id: totrans-330
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.30：马尔可夫假设
- en: 'We can also have a second-order Markov assumption, which depends on two previous
    terms, but we are going to use first-order Markov assumption:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用二阶马尔可夫假设，它依赖于前两个词项，但我们将使用一阶马尔可夫假设：
- en: '![Figure 3.31: Example of Markov](img/C13550_03_31.jpg)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.31：马尔科夫示例](img/C13550_03_31.jpg)'
- en: 'Figure 3.31: Example of Markov'
  id: totrans-333
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.31：马尔科夫示例
- en: 'If we apply this to the whole sentence, we get this:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将其应用于整个句子，结果如下：
- en: '![Figure 3.32: Example of Markov for a whole sentence](img/C13550_03_32.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.32：整个句子的马尔科夫示例](img/C13550_03_32.jpg)'
- en: 'Figure 3.32: Example of Markov for a whole sentence'
  id: totrans-336
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.32：整个句子的马尔科夫示例
- en: Decomposing the sequence of words in the aforementioned way will output the
    probabilities more accurately.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 按照上述方式分解单词序列将更加准确地输出概率。
- en: 'Exercise 13: Create a Bigram Model'
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 13：创建二元模型
- en: 'In this exercise, we are going to create a simple LM with unigrams and bigrams.
    Also, we will compare the results of creating the LM both without add-one smoothing
    and with it. One application of the n-gram is, for example, in keyboard apps.
    They can predict your next word. That prediction could be done with a bigram model:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将创建一个简单的语言模型（LM），使用 unigram 和 bigram。同样，我们将比较在没有加一平滑和加一平滑的情况下创建语言模型的结果。n-gram
    的一个应用示例是键盘应用。它们可以预测你下一个单词。这个预测可以通过一个二元模型来实现：
- en: Open up your Google Colab interface.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你的 Google Colab 界面。
- en: Create a folder for the book.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建书籍文件夹。
- en: 'Declare a small, easy training corpus:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 声明一个小的、易于训练的语料库：
- en: '[PRE27]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Import the required libraries and load the model:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库并加载模型：
- en: '[PRE28]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Tokenize it with spaCy. To be faster in doing the smoothing and the bigrams,
    we are going to create three lists:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 spaCy 对其进行分词。为了加快平滑处理和二元模型的速度，我们将创建三个列表：
- en: '`Tokens`: All tokens of the corpus'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Tokens`：语料库中的所有标记'
- en: '`Tokens_doc`: List of lists with the tokens of each corpus'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Tokens_doc`：包含每个语料库标记的列表的列表'
- en: '`Distinc_tokens`: All tokens removing duplicates:'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Distinc_tokens`：去重后的所有标记：'
- en: '[PRE29]'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s create a first loop to iterate over the sentences in our corpus. The
    `doc` variable will contain a sequence of the sentences'' tokens:'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们先创建一个循环，遍历语料库中的句子。`doc` 变量将包含句子的标记序列：
- en: '[PRE30]'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now we are going to create a second loop to iterate through the tokens to push
    them into the corresponding list. The `t` variable will be each token of the sentence:'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们将创建第二个循环，遍历标记并将其推入相应的列表中。`t` 变量将是句子的每个标记：
- en: '[PRE31]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Create the unigram model and test it:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 unigram 模型并进行测试：
- en: '[PRE32]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Result = 0.1388888888888889
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果 = 0.1388888888888889
- en: 'Add the smoothing and test it with the same word:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加平滑并使用相同的单词进行测试：
- en: '[PRE33]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Result = 0.1111111111111111
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果 = 0.1111111111111111
- en: Note
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The problem with this smoothing method is that every unseen word has the same
    probability.
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种平滑方法的问题在于每个未见过的单词都有相同的概率。
- en: 'Create the bigram model:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建二元模型（bigram）：
- en: '[PRE34]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We need to iterate through all of the tokens in the documents to try to find
    the number of times that `word1` and `word2` appear together:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要遍历文档中的所有标记，尝试找到 `word1` 和 `word2` 一起出现的次数：
- en: '[PRE35]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output is as follows:'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 3.33: Output showing the times word1 and word2 appear together in
    the document](img/C13550_03_33.jpg)'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.33：输出显示 word1 和 word2 在文档中一起出现的次数](img/C13550_03_33.jpg)'
- en: 'Figure 3.33: Output showing the times word1 and word2 appear together in the
    document'
  id: totrans-369
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.33：输出显示 word1 和 word2 在文档中一起出现的次数
- en: 'Add the smoothing to the bigram model:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为二元模型添加平滑：
- en: '[PRE36]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output is as follows:'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 3.34: Output after adding smoothing to the model](img/C13550_03_34.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.34：为模型添加平滑后输出结果](img/C13550_03_34.jpg)'
- en: 'Figure 3.34: Output after adding smoothing to the model'
  id: totrans-374
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.34：为模型添加平滑后输出结果
- en: Congratulations! You have completed the last exercise of this chapter. In the
    next chapter, you will see that this LM approach is a fundamental deep NLP approach.
    You can now take a huge corpus and create your own LM.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经完成了本章的最后一个练习。在下一章，你将看到这种语言模型（LM）方法是一种基础的深度自然语言处理（NLP）方法。现在你可以利用庞大的语料库，创建属于你自己的语言模型（LM）。
- en: Note
  id: totrans-376
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Applying the Markov assumption, the final probability will round the 0\. I recommend
    using log() and adding each component. Also, check the precision bits of your
    code (float16 < float32 < float64).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 应用马尔科夫假设，最终的概率将四舍五入为0。我建议使用log()并逐个添加各个组件。此外，检查代码的精度位数（float16 < float32 < float64）。
- en: Summary
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'NLP is becoming more and more important in AI. Industries analyze huge quantities
    of raw text data, which is unstructured. To understand this data, we use many
    libraries to process it. NLP is divided into two groups of methods and functions:
    NLG to generate natural language, and NLU to understand it.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）在人工智能中变得越来越重要。各个行业分析大量未经结构化的原始文本数据。为了理解这些数据，我们使用许多库进行处理。NLP分为两大类方法和功能：NLG用于生成自然语言，NLU用于理解自然语言。
- en: Firstly, it is important to clean text data, since there will be a lot of useless,
    irrelevant information. Once the data is ready to be processed, through a mathematical
    algorithm such as TF-IDF or LSA, a huge set of documents can be understood. Libraries
    such as NLTK and spaCy are useful for doing this task. They provide methods to
    remove the noise in data. A document can be represented as a matrix. First, TF-IDF
    can give a global representation of a document, but when a corpus is big, the
    better option is to perform dimensionality reduction with LSA and SVD. scikit-learn
    provides algorithms for processing documents, but if documents are not pre-processed,
    the result will not be accurate. Finally, the use of language models could be
    necessary, but they need to be formed of a valid training set of documents. If
    the set of documents is good, the language model should be able to generate language.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，清理文本数据非常重要，因为其中会有很多无用的、不相关的信息。一旦数据准备好进行处理，通过诸如TF-IDF或LSA等数学算法，就能理解大量文档。像NLTK和spaCy这样的库在完成这项任务时非常有用，它们提供了去除数据噪音的方法。文档可以被表示为一个矩阵。首先，TF-IDF能够给出文档的全局表示，但当语料库较大时，更好的选择是通过LSA和SVD进行降维处理。scikit-learn提供了处理文档的算法，但如果文档没有经过预处理，结果将不准确。最后，可能需要使用语言模型，但它们需要由有效的训练集文档构成。如果文档集质量良好，语言模型应该能够生成语言。
- en: In the next chapter, we will introduce **Recurrent Neural Networks** (**RNNs**).
    We will be looking at some advanced models of these RNNs and will accordingly
    be one step ahead in building our robot.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍**递归神经网络**（**RNNs**）。我们将探讨这些RNN的一些高级模型，并因此在构建我们的机器人时走在前列。
