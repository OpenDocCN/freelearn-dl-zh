- en: Chapter 6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章
- en: Using the Standard Toolbox for Bayesian Deep Learning
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标准工具箱进行贝叶斯深度学习
- en: As we saw in previous chapters, vanilla NNs often produce poor uncertainty estimates
    and tend to make overconfident predictions, and some aren’t capable of producing
    uncertainty estimates at all. By contrast, probabilistic architectures offer principled
    means to obtain high-quality uncertainty estimates; however, they have a number
    of limitations when it comes to scaling and adaptability.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中看到的，普通的神经网络往往产生较差的不确定性估计，并且往往会做出过于自信的预测，而有些甚至根本无法生成不确定性估计。相比之下，概率架构提供了获得高质量不确定性估计的原则性方法；然而，在扩展性和适应性方面，它们有一些局限性。
- en: While both PBP and BBB can be implemented with popular ML frameworks (as shown
    in our previous TensorFlow examples), they are very complex. As we saw in the
    last chapter, implementing even a simple network isn’t straightforward. This means
    that adapting them to new architectures is awkward and time-consuming (particularly
    for PBP, although it is possible – see *Fully Bayesian Recurrent Neural Networks
    for Safe Reinforcement* *Learning*). For simple tasks, such as the examples from
    *Chapter 5, Principled* *Approaches for Bayesian Deep Learning*, this isn’t an
    issue. But in many real-world tasks, such as
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管PBP和BBB都可以通过流行的机器学习框架来实现（正如我们在之前的TensorFlow示例中所展示的），但它们非常复杂。正如我们在上一章中看到的，实现一个简单的网络也并非易事。这意味着将它们适应到新架构中是一个笨拙且耗时的过程（特别是PBP，尽管是可能的——参见*完全贝叶斯递归神经网络用于安全强化*
    *学习*）。对于一些简单任务，例如*第5章，贝叶斯深度学习的原则性方法*中的示例，这并不是问题。但在许多现实世界的任务中，例如
- en: machine translation or object recognition, far more sophisticated network architectures
    are necessary.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译或物体识别等任务，需要更为复杂的网络架构。
- en: While some academic institutions or large research organizations may have the
    time and resources required to adapt these complex probabilistic methods to a
    variety of sophisticated architectures, in many cases this simply is not viable.
    Additionally, more and more industry researchers and engineers are turning to
    transfer learning-based methods, using pre-trained networks as the backbone of
    their models. In these cases, it’s impossible to simply add probabilistic machinery
    to predefined architectures.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一些学术机构或大型研究组织可能具备足够的时间和资源来将这些复杂的概率方法适应到各种复杂的架构中，但在许多情况下，这并不可行。此外，越来越多的行业研究人员和工程师正在转向基于迁移学习的方法，使用预训练的网络作为模型的骨干。在这些情况下，简单地将概率机制添加到预定义架构中是不可行的。
- en: To address this, in this chapter, we will explore how common paradigms in deep
    learning can be harnessed to develop probabilistic models. The methods introduced
    here show that, with relatively minor tweaks, you can easily adapt large, sophisticated
    architectures to produce high-quality uncertainty estimates. We’ll even introduce
    techniques that will enable you to get uncertainty estimates from networks you’ve
    already trained!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，本章将探讨如何利用深度学习中的常见范式来开发概率模型。这里介绍的方法表明，通过相对较小的调整，您可以轻松地将大型复杂架构适应到高质量的不确定性估计中。我们甚至会介绍一些技术，使您能够从已训练的网络中获取不确定性估计！
- en: The chapter will cover three key approaches for facilitating model uncertainty
    estimation easily with common deep learning frameworks. First, we will look at
    **Monte Carlo Dropout** (**MC dropout**), a method that induces variance across
    predictions by utilizing dropout at inference time. Second, we will introduce
    deep ensembles, whereby multiple neural networks are combined to facilitate both
    uncertainty estimation and improved model performance. Finally, we will explore
    various methods for adding a Bayesian layer to our model, allowing any model to
    produce uncertainty estimates.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖三种关键方法，以便在常见的深度学习框架中轻松进行模型不确定性估计。首先，我们将介绍**蒙特卡洛 Dropout**（**MC dropout**），一种通过在推理时使用Dropout来引入预测方差的方法。其次，我们将介绍深度集成方法，即通过结合多个神经网络来促进不确定性估计和提高模型性能。最后，我们将探索将贝叶斯层添加到模型中的各种方法，使任何模型都能产生不确定性估计。
- en: 'These topics will be covered in the following sections:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下内容将在接下来的章节中讨论：
- en: Introducing approximate Bayesian inference via dropout
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Dropout引入近似贝叶斯推断
- en: Using ensembles for model uncertainty estimates
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用集成方法进行模型不确定性估计
- en: Exploring neural network augmentation with Bayesian last-layer methods
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索通过贝叶斯最后一层方法增强神经网络
- en: 6.1 Technical requirements
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 技术要求
- en: 'To complete the practical tasks in this chapter, you will need a Python 3.8
    environment with the SciPy stack and the following additional Python packages
    installed:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章的实际任务，您需要一个 Python 3.8 环境，并安装 SciPy 堆栈以及以下附加的 Python 包：
- en: TensorFlow 2.0
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 2.0
- en: TensorFlow Probability
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 概率
- en: 'All of the code for this book can be found on the GitHub repository for the
    book: [https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference](https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的所有代码可以在本书的 GitHub 仓库找到：[https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference](https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference)。
- en: 6.2 Introducing approximate Bayesian inference via dropout
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 通过 dropout 引入近似贝叶斯推断
- en: '**Dropout** is traditionally used to prevent overfitting an NN. First introduced
    in 2012, it is now used in many common NN architectures and is one of the easiest
    and most widely used regularization methods. The idea of dropout is to randomly
    turn off (or drop) certain units of a neural network during training. Because
    of this, the model cannot solely rely on a particular small subset of neurons
    to solve the task it was given. Instead, the model is forced to find different
    ways to solve its task. This improves the robustness of the model and makes it
    less likely to overfit.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**Dropout** 传统上用于防止神经网络的过拟合。它最早在 2012 年提出，现在被广泛应用于许多常见的神经网络架构，并且是最简单且最常用的正则化方法之一。Dropout
    的核心思想是在训练过程中随机关闭（或丢弃）神经网络的某些单元。因此，模型不能仅依赖某一小部分神经元来解决任务。相反，模型被迫找到不同的方式来完成任务。这提高了模型的鲁棒性，并使其不太可能过拟合。'
- en: 'If we simplify a network to *y* = *Wx*, where *y* is the output of our network,
    *x* the input, and *W* our model weights, we can think of dropout as:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们简化一个网络为 *y* = *Wx*，其中 *y* 是我们网络的输出，*x* 是输入，*W* 是我们的模型权重，我们可以将 dropout 理解为：
- en: '![ ( { wj, p wˆj = ( 0, otherwise ](img/file139.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![ ( { wj, p wˆj = ( 0, otherwise ](img/file139.jpg)'
- en: where *w*[*j*] is the new weights after applying dropout, *w*[*j*] is our weights
    before applying dropout, and *p* is our probability of *not* applying dropout.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *w*[*j*] 是应用 dropout 后的新权重，*w*[*j*] 是应用 dropout 前的权重，*p* 是我们不应用 dropout 的概率。
- en: 'The original dropout paper recommends randomly dropping 50% of the units in
    a network and applying dropout to all layers. Input layers should not have the
    same dropout probability because this would mean that we throw away 50% of the
    input information for our network, which makes it more difficult for the model
    to converge. In practice, you can experiment with different dropout probabilities
    to find the dropout rate that works well for your specific dataset and model;
    that is another hyperparameter you can optimize. Dropout is typically available
    as a standalone layer in all standard neural network libraries you can find online.
    You typically add it after your activation function:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 dropout 论文建议随机丢弃网络中 50% 的单元，并对所有层应用 dropout。输入层的 dropout 概率不应相同，因为这意味着我们丢弃了
    50% 的输入信息，这会使模型更难收敛。实际上，您可以尝试不同的 dropout 概率，找到最适合您的特定数据集和模型的丢弃率；这是另一个您可以优化的超参数。Dropout
    通常作为一个独立的层，在所有标准的神经网络库中都可以找到。您通常在激活函数之后添加它：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that we’ve been reminded of the vanilla application of dropout, let’s look
    at how we can use it for Bayesian inference.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了 dropout 的基本应用，让我们看看如何将其用于贝叶斯推断。
- en: 6.2.1 Using dropout for approximate Bayesian inference
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 使用 dropout 进行近似贝叶斯推断
- en: 'Traditional dropout methods make the prediction of dropout networks deterministic
    at test time by turning off dropout during inference. However, we can also use
    the stochasticity of dropout to our advantage. This is called **Monte** **Carlo
    (MC)** dropout, and the idea is as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的 dropout 方法使得在测试时 dropout 网络的预测是确定性的，因为在推理过程中关闭了 dropout。然而，我们也可以利用 dropout
    的随机性来为我们带来优势。这就是所谓的 **蒙特卡罗 (MC)** dropout，其思想如下：
- en: We use dropout during test time.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在测试时使用 dropout。
- en: Instead of running inference once, we run it many times (for example, 30-100
    times).
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不是只运行一次推理，而是运行多次（例如，30-100 次）。
- en: We then average the predictions to get our uncertainty estimates.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们对预测结果取平均，以获得我们的不确定性估计。
- en: Why is this beneficial? As we said before, using dropout forces the model to
    learn different ways to solve its task. So, when we keep dropout enabled during
    inference, we use slightly different networks that all process the input via a
    slightly different path through the model. This diversity is helpful when we want
    a calibrated uncertainty score, as we will see in the next section, where we will
    discuss the concept of deep ensembles. Instead of predicting a point estimate
    (a single value) for each input, our network now produces a distribution of values
    (made up of multiple forward passes). We can use this distribution to compute
    a mean and variance associated with each input data point, as shown in *Figure*
    [*6.1*](#x1-85011r1).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这有益？正如我们之前所说，使用dropout可以迫使模型学习解决任务的不同方法。因此，当我们在推理过程中保持启用dropout时，我们使用的是稍微不同的网络，这些网络通过模型的不同路径处理输入数据。这种多样性在我们希望获得校准的**不确定性评分**时非常有用，正如我们在下一节中所看到的，我们将讨论深度集成的概念。我们现在不再为每个输入预测一个点估计（一个单一值），而是让网络生成一组值的分布（由多个前向传递组成）。我们可以使用这个分布来计算每个输入数据点的均值和方差，如*图*
    [*6.1*](#x1-85011r1)所示。
- en: '![PIC](img/file140.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file140.png)'
- en: 'Figure 6.1: Example of MC dropout'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1：MC dropout 示例
- en: 'We can also interpret MC dropout in a Bayesian way. Using these slightly different
    networks with dropout can be seen as sampling from a distribution of all possible
    models: the posterior distribution over all of the parameters (or weights) of
    our network:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以用贝叶斯的方式来解释MC dropout。使用这些稍微不同的网络进行dropout可以看作是从所有可能模型的分布中进行采样：网络所有参数（或权重）上的后验分布：
- en: '![𝜃t ∼ P (𝜃|D ) ](img/file141.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![𝜃t ∼ P (𝜃|D ) ](img/file141.jpg)'
- en: Here, *𝜃*[*t*] is a dropout configuration and ∼ a single sample from our posterior
    distribution *P*(*𝜃*|*D*). This way, MC dropout is equivalent to a form of approximate
    Bayesian inference, similar to the methods we saw in [*Chapter 5*](CH5.xhtml#x1-600005),
    [*Principled Approaches for Bayesian Deep Learning*](CH5.xhtml#x1-600005).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*𝜃*[*t*]是一个dropout配置，∼表示从我们的后验分布*P*(*𝜃*|*D*)中抽取的单个样本。这样，MC dropout就相当于一种近似贝叶斯推断的方法，类似于我们在[*第5章*](CH5.xhtml#x1-600005)中看到的方法，[*贝叶斯深度学习的原则性方法*](CH5.xhtml#x1-600005)。
- en: Now that we have an idea of how MC dropout works, let’s implement it in TensorFlow.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对MC dropout的工作原理有所了解，让我们在TensorFlow中实现它。
- en: 6.2.2 Implementing MC dropout
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 实现MC dropout
- en: 'Let’s assume we have trained a model with the convolutional architecture described
    in this chapter’s first hands-on exercise. We can now use dropout at inference
    by setting `training=True`:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经训练了本章第一个实践练习中描述的卷积架构的模型。现在，我们可以通过将`training=True`来在推理过程中使用dropout：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This allows us to get our mean and variance for every prediction of our model.
    Each row of our `Predictions` variable contains the predictions associated with
    each input, obtained from consecutive forward passes. From these predictions,
    we can compute the means and variances, as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得我们能够为模型的每次预测计算均值和方差。我们的`Predictions`变量的每一行都包含与每个输入相关的预测结果，这些预测是通过连续的前向传递获得的。从这些预测中，我们可以计算均值和方差，如下所示：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As with all neural networks, Bayesian neural networks require some degree of
    fine-tuning via hyperparameters. The following three hyperparameters are particularly
    important for MC dropout:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有神经网络一样，贝叶斯神经网络需要通过超参数进行一定程度的微调。以下三个超参数对于MC dropout尤为重要：
- en: '**Number of dropout layers**: How many layers (in our `Sequential` object)
    will use dropout, and which layers these will be.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout层数**：在我们的`Sequential`对象中，使用dropout的层数是多少，具体是哪些层。'
- en: '**Dropout rate**: The likelihood that nodes will be dropped.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout率**：节点被丢弃的概率。'
- en: '**The number of MC dropout samples**: A new hyperparameter specific to MC dropout.
    Shown here as `nb_inference` , this defines the number of times to sample from
    the MC dropout network at inference time.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MC dropout样本数量**：这是MC dropout特有的一个新超参数。这里表示为`nb_inference`，它定义了在推理时从MC dropout网络中采样的次数。'
- en: We’ve now seen how MC dropout can be used in a new way, giving us an easy and
    intuitive method to compute Bayesian uncertainties using familiar tools. But this
    isn’t the only method we have available to us. In the next section, we’ll see
    how ensembling can be applied to neural networks; providing another straightforward
    approach for approximating BNNs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经看到MC dropout可以以一种新的方式使用，提供了一种简单直观的方法来利用熟悉的工具计算贝叶斯不确定性。但这并不是我们唯一可以使用的方法。在下一节中，我们将看到如何将集成方法应用于神经网络；这为我们提供了另一种逼近BNN的直接方法。
- en: 6.3 Using ensembles for model uncertainty estimates
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 使用集成方法进行模型不确定性估计
- en: 'This section will introduce you to deep ensembles: a popular method for obtaining
    Bayesian uncertainty estimates using an ensemble of deep networks.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍深度集成方法：这是一种通过深度网络集成来获得贝叶斯不确定性估计的流行方法。
- en: 6.3.1 Introducing ensembling methods
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 介绍集成方法
- en: 'A common strategy in machine learning is to combine several single models into
    a committee of models. The process of learning such a combination of models is
    called **ensemble learning**, and the resulting committee of models is called
    an ensemble. Ensemble learning involves two main components: first, the different
    single models need to be trained. There are various strategies to obtain different
    models from the same training data: the models can be trained on different subsets
    of data, we can train different model types or models with different architectures,
    or we can initialize the same model types with different hyperparameters. Second,
    the outputs of the different single models need to be combined. Common strategies
    for combining the predictions of single models are simply taking their average
    or taking a majority vote among all members of the ensemble. More advanced strategies
    are taking a weighted average or, if more training data is available, learning
    an additional model to combine the different predictions of the ensemble members.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中一个常见的策略是将多个单一模型组合成一个模型委员会。学习这种模型组合的过程称为**集成学习**，而得到的模型委员会则称为集成模型。集成学习包含两个主要部分：首先，多个单一模型需要被训练。有多种策略可以从相同的训练数据中获得不同的模型：可以在不同的数据子集上训练模型，或者训练不同类型的模型或具有不同架构的模型，亦或是使用不同超参数初始化相同类型的模型。其次，需要将不同单一模型的输出进行组合。常见的组合单一模型预测的策略是直接取其平均值，或者对集成模型中的所有成员进行多数投票。更高级的策略包括取加权平均值，或者如果有更多的训练数据，则可以学习一个额外的模型来结合集成成员的不同预测结果。
- en: Ensembles are very popular in machine learning because they tend to improve
    predictive performance by minimizing the risk of accidentally picking a model
    with poor performance. In fact, ensembles are guaranteed to perform at least as
    well as any single model. Furthermore, ensembles will perform better than single
    models if there is enough diversity among the predictions of ensemble members.
    Diversity here means that different ensemble members make different mistakes on
    a given data sample. If, for example, some ensemble members misclassify the image
    of a dog as “cat,” but the majority of ensemble members make the correct prediction
    (“dog”), then the combined ensemble output will still be correct (“dog”). More
    generally, as long as every single model has an accuracy greater than 50% and
    the models make independent mistakes, then the predictive performance of the ensemble
    will approach 100% accuracy as we add more and more ensemble members.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法在机器学习中非常流行，因为它们通常通过最小化意外选择性能较差模型的风险来提高预测性能。事实上，集成模型至少能够与任何单一模型一样好地执行。更重要的是，如果集成成员的预测存在足够的多样性，集成方法的表现将优于单一模型。这里的多样性意味着不同的集成成员在给定的数据样本上会犯不同的错误。例如，如果一些集成成员将一只狗的图像误分类为“猫”，但大多数集成成员做出了正确的预测（“狗”），那么集成模型的最终输出仍然是正确的（“狗”）。更一般来说，只要每个单一模型的准确率超过50%，并且模型的错误是独立的，那么随着集成成员数量的增加，集成的预测性能将接近100%的准确度。
- en: In addition to improving predictive performance, we can leverage the degree
    of agreement (or disagreement) among ensemble members to obtain an uncertainty
    estimate along with the prediction of the ensemble. In the context of image classification,
    for example, if almost all ensemble members predict that the image shows a dog,
    then we can say that the ensemble predicted “dog” with high confidence (or low
    uncertainty). Conversely, if there is significant disagreement among the predictions
    of different ensemble members, then we will observe high uncertainty in the form
    of significant variance across the outputs from the ensemble members, telling
    us that the prediction has low confidence.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 除了提高预测性能外，我们还可以利用集成成员之间的一致性（或不一致性）来获得不确定性估计，并与集成的预测结果一起使用。例如，在图像分类的情况下，如果几乎所有集成成员都预测图像显示的是一只狗，那么我们可以说集成模型以高置信度（或低不确定性）预测为“狗”。相反，如果不同集成成员的预测存在显著的不一致，那么我们将观察到高不确定性，即集成成员输出之间的方差较大，这表明预测的置信度较低。
- en: Now that we are equipped with a basic understanding of ensembles, it is worth
    highlighting that MC dropout, which we explored in the previous section, may also
    be seen as an ensemble method. When we enable dropout during inference, we effectively
    run inference with a slightly different (sub-)network every time. The combination
    of these different sub-networks can be considered as a committee of different
    models, and therefore an ensemble. This observation led a team at Google to look
    into alternative ways of creating ensembles from DNNs, which led to the discovery
    of deep ensembles (Lakshminarayan et al, 2016) , which are introduced in the following
    section.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经具备了对集成方法的基本理解，值得指出的是，我们在前一节中探讨的MC Dropout也可以看作一种集成方法。当我们在推理过程中启用Dropout时，我们实际上每次都在运行一个略有不同的（子）网络。这些不同子网络的组合可以看作是多个模型的委员会，因此也是一种集成方法。这一观察促使谷歌团队研究从深度神经网络（DNN）创建集成的替代方法，最终发现了深度集成（Lakshminarayan等，2016），这一方法将在接下来的章节中介绍。
- en: 6.3.2 Introducing deep ensembles
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 引入深度集成
- en: 'The main idea behind deep ensembles is straightforward: train several different
    DNN models, then combine their predictions via averaging to improve model performance
    and leverage the agreement among the predictions of these models to obtain an
    estimate of predictive uncertainty.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 深度集成的主要思想很简单：训练多个不同的深度神经网络（DNN）模型，然后通过平均它们的预测结果来提高模型性能，并利用这些模型预测结果的一致性来估计预测的不确定性。
- en: More formally, assume that we have some training data **X**, where *X* ∈ℝ^(*D*),
    and corresponding target labels **y**. For example, in image classification the
    training data would be images, and the target labels would be integers that denote
    which object class is shown in the corresponding image, so *y* ∈{1*,...,K*} where
    *K* is the total number of classes. Training a single neural network means that
    we model the probabilistic predictive distribution *p*[*𝜃*](*y*|*x*) over the
    labels and optimize *𝜃*, the parameters of the NN. For deep ensembles, we train
    **M** neural networks whose parameters can be described as {*𝜃*[*m*]}[*m*=1]^(*M*),
    where each *𝜃*[*m*] is optimized independently using **X** and **y** (meaning
    that we train each NN independently on the same data). The predictions of the
    deep ensemble members are combined via averaging, using *p*(*y*|*x*) = *M*^(−1)
    ∑ [*m*=1]^(*M*)*p*[*𝜃*[m]](*y*|*x,𝜃*[*m*]).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，假设我们有一些训练数据**X**，其中 *X* ∈ℝ^(*D*)，以及相应的目标标签**y**。例如，在图像分类中，训练数据是图像，目标标签是表示图像中显示的是哪一类物体的整数，所以
    *y* ∈{1*,...,K*}，其中 *K* 是类别的总数。训练一个单一的神经网络意味着我们对标签建模概率预测分布 *p*[*𝜃*](*y*|*x*)，并优化
    *𝜃*，即神经网络的参数。对于深度集成，我们训练**M**个神经网络，它们的参数可以表示为{*𝜃*[*m*]}[*m*=1]^(*M*)，其中每个 *𝜃*[*m*]
    都是使用**X**和**y**独立优化的（这意味着我们在相同的数据上独立训练每个神经网络）。深度集成成员的预测通过平均值进行结合，使用 *p*(*y*|*x*)
    = *M*^(−1) ∑ [*m*=1]^(*M*)*p*[*𝜃*[m]](*y*|*x,𝜃*[*m*])。
- en: '*Figure* [*6.2*](#x1-89003r2) illustrates the idea behind deep ensembles. Here,
    we have trained *M* = 3 different feed-forward NNs. Notice that each network has
    its own unique set of network weights, as illustrated by the varying thickness
    of the edges connecting the network notes. Each of the three networks will output
    its own prediction score, as illustrated by the green nodes, and we combine these
    scores via averaging.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*图* [*6.2*](#x1-89003r2) 说明了深度集成的思想。在这里，我们训练了 *M* = 3 个不同的前馈神经网络。请注意，每个网络都有自己独特的网络权重集，正如通过连接网络节点的边缘厚度不同所示。三个网络中的每一个都会输出自己的预测分数，如绿色节点所示，我们通过平均这些分数来进行结合。'
- en: '![PIC](img/file142.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file142.png)'
- en: 'Figure 6.2: Example of a deep ensemble. Note that the three networks differ
    in their weights, as illustrated by edges with different thicknesses'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：深度集成示例。请注意，三个网络在权重上有所不同，正如通过不同厚度的边缘所示。
- en: 'How can we train several different neural network models if only one dataset
    is available for training? The strategy proposed in the original paper (and still
    the most commonly used strategy) is to start every training with a random initialization
    of the network’s weights. If every training starts with a different set of weights,
    the different training runs are likely to produce networks with different function
    approximations of the training data. This is because NNs tend to have many more
    weight parameters than there are samples in the training dataset. Therefore, the
    same observations in the training dataset can be approximated by many different
    weight parameter combinations. During training, the different NN models will each
    converge to their own parameter combination and will occupy different local optima
    on the loss landscape. Because of this, the different NNs will also often have
    differing perspectives on a given data sample, for example, the image of a dog.
    This also means that the different NNs will make different mistakes, for example,
    when classifying the data sample. The degree of consensus between the different
    networks in an ensemble provides information about how certain an ensemble is
    in its predictions for a given data point: the more the networks agree, the more
    confident we can be in the prediction.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果只有一个数据集可供训练，我们如何训练多个不同的神经网络模型？原始论文提出的策略（也是目前最常用的策略）是每次训练都从网络权重的随机初始化开始。如果每次训练都从不同的权重集合开始，那么不同的训练运行可能会产生不同的网络，其训练数据的函数逼近方式也会有所不同。这是因为神经网络往往拥有比训练数据集中的样本数量更多的权重参数。因此，训练数据集中的相同观测值可以通过许多不同的权重参数组合来逼近。在训练过程中，不同的神经网络模型将各自收敛到自己的参数组合，并在损失函数的局部最优点上占据不同位置。因此，不同的神经网络通常会对给定的数据样本（例如，一只狗的图像）有不同的看法。这也意味着不同的神经网络在分类数据样本时可能会犯不同的错误。集成中不同网络之间的共识程度提供了关于集成模型对某一数据点预测的置信度信息：网络越一致，我们对预测的信心就越强。
- en: 'Alternative ways to train different NN models with the same training data set
    are: to use a random ordering of mini-batches during training, to use different
    hyperparameters for every training run, or to use different network architecture
    for every model altogether. These strategies can also be combined, and understanding
    exactly which combination of strategies leads to the best outcomes, in terms predictive
    performance and predictive uncertainty, is an active field of research.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同训练数据集训练不同神经网络模型的替代方法包括：在训练过程中使用迷你批次的随机排序、为每次训练运行使用不同的超参数，或为每个模型使用不同的网络架构。这些策略也可以结合使用，精确理解哪些策略组合能带来最佳结果（无论是预测性能还是预测不确定性）仍然是一个活跃的研究领域。
- en: 6.3.3 Implementing a deep ensemble
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 实现深度集成
- en: The following code example illustrates how to train a deep ensemble using the
    strategy of random weight initialization to obtain differing ensemble members.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例展示了如何使用随机权重初始化策略训练深度集成模型，以获得不同的集成成员。
- en: 'Step 1: Importing libraries'
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 1：导入库
- en: 'We start by importing the relevant packages and setting the number of ensembles
    to `3` for this code example:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入相关的包，并将集成数量设置为`3`，用于本代码示例：
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Step 2: Obtaining data'
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 2：获取数据
- en: 'We then download the `MNIST`` Fashion` dataset, which is a dataset that contains
    images of ten different clothing items:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们下载`MNIST`` Fashion`数据集，这是一个包含十种不同服装项目图像的数据集：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Step 3: Constructing our ensemble model'
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 3：构建集成模型
- en: 'Next, we create a helper function that defines our model. As you can see, we
    use a simple image classifier structure that consists of two convolutional layers,
    each followed by a max-pooling operation, and several fully connected layers:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个辅助函数来定义我们的模型。如你所见，我们使用一个简单的图像分类器结构，包含两个卷积层，每个卷积层后跟一个最大池化操作，以及若干全连接层：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We also create another helper function that compiles the model for us, using
    `Adam` as our optimizer and a categorical cross-entropy loss:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还创建了另一个辅助函数，使用`Adam`作为优化器，并采用类别交叉熵损失来编译模型：
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Step 4: Training'
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 4：训练
- en: 'We then train three different networks on the same dataset. Since the network
    weights are initialized at random, this will result in three different models.
    You will see that the training accuracy varies slightly between models:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在相同的数据集上训练三个不同的网络。由于网络权重是随机初始化的，这将导致三个不同的模型。你会看到不同模型的训练准确度略有差异：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Step 5: Inference'
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 5：推理
- en: 'We can then perform inference and obtain the Predictions for each of the models
    for all images in the test split. We can also take the mean across the predictions
    of the three models, which will give us one prediction vector per image:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以执行推理并获得测试集中的每个模型对所有图像的预测结果。我们还可以对三个模型的预测结果取平均值，这样每个图像就会有一个预测向量：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: That’s it. We have trained an ensemble of networks and performed inference.
    Given that we have several predictions per image now, we can also look at images
    where the three models disagree.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。我们已经训练了一个网络集成并进行了推理。由于现在每个图像都有多个预测，我们还可以查看三个模型预测结果不一致的图像。
- en: 'Let’s, for example, find the image with the highest disagreement and visualize
    it:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，我们可以找到预测结果不一致最多的图像并将其可视化：
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Looking at the image in *Figure* [*6.3*](#x1-95107r3), even for a human it
    is hard to tell whether there is a t-shirt, shirt, or bag in the image:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 看看*图* [*6.3*](#x1-95107r3)中的图像，甚至对于人类来说，也很难判断图像中是T恤、衬衫还是包：
- en: '![PIC](img/file143.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file143.png)'
- en: 'Figure 6.3: Image with highest variance among ensemble predictions. The correct
    ground truth label is ”t-shirt,” but it is hard to tell, even for a human'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：集成预测中方差最大的图像。正确的真实标签是"t-shirt"，但即使是人类也很难判断。
- en: While we’ve seen that deep ensembles have several favorable qualities, they
    are not without limitations. In the next section, we’ll explore what kinds of
    things we may wish to bear in mind when considering deep ensembles.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经看到深度集成有几个有利的特性，但它们也不是没有局限性。在下一节中，我们将探讨在考虑深度集成时可能需要注意的事项。
- en: 6.3.4 Practical limitations of deep ensembles
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.4 深度集成的实际局限性
- en: Some practical limitations of ensembles become obvious when taking them from
    the research environment to production at scale. We know that, in theory, the
    predictive performance and the uncertainty estimate of an ensemble is expected
    to improve as we add more ensemble members. However, there is a cost of adding
    more ensemble members as the memory footprint and inference cost of ensembles
    increases linearly with the number of ensemble members. This can make deploying
    ensembles in a production setting a costly choice. For every NN that we add to
    the ensemble, we will need to store an extra set of network weights, which significantly
    increases memory requirements. Equally, for every network, we will also need to
    run an additional forward pass during inference. Even though the inferences of
    different networks can be run in parallel, and the impact on inference time can
    therefore be mitigated, such an approach will still require more compute resources
    than single models. As more compute resources tend to translate to higher costs,
    the decision of using an ensemble versus a single model will need to trade off
    the benefits of better performance and uncertainty estimation with the increase
    in cost.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从研究环境到大规模生产环境中应用集成模型时，一些实际局限性变得显而易见。我们知道，理论上，随着我们增加更多的集成成员，集成模型的预测性能和不确定性估计会有所提升。然而，增加更多集成成员是有代价的，因为集成模型的内存占用和推理成本会随着集成成员数量的增加而线性增长。这可能使得在生产环境中部署集成模型成为一个高成本的选择。对于我们添加到集成中的每一个神经网络，我们都需要存储一组额外的网络权重，这会显著增加内存需求。同样，对于每个网络，我们还需要在推理过程中进行额外的前向传递。尽管不同网络的推理可以并行进行，因此推理时间的影响可以得到缓解，但这种方法仍然需要比单一模型更多的计算资源。由于更多的计算资源往往意味着更高的成本，使用集成模型与单一模型之间的决策需要在更好的性能和不确定性估计的好处与成本增加之间进行权衡。
- en: 'Recent research has tried to address or mitigate these practical limitations.
    In an approach called BatchEnsembles ([**?**]), for example, all ensemble members
    share one underlying weight matrix. The final weight matrix for each ensemble
    member is obtained by element-wise multiplication of this shared weight matrix
    with a rank-one matrix that is unique to each ensemble member. This reduces the
    number of parameters that need to be stored for each additional ensemble member
    and thus reduces memory footprint. The ensemble’s computational cost is also reduced
    because the BatchEnsembles can exploit vectorization, and the output for all ensemble
    members can be computed in a single forward pass. In a different approach, called
    **multi-input/multi-output** **processing** (**MIMO**; [**?**]), a single network
    is encouraged to learn several independent sub-networks. During training, multiple
    inputs are passed along with multiple, correspondingly labeled outputs. The network
    will, for example, be presented with three images: of a dog, a cat and a chicken.
    Corresponding output labels are passed and the network will need to learn to predict
    ”dog” on its first output node, ”cat” on its second output node, and ”chicken”
    on its third. During inference, one single image will be repeated three times
    and the MIMO ensemble will produce three different predictions (one on each output
    node). As a result, the memory footprint and computational cost of the MIMO approach
    is almost as little as that of a single neural network, while still providing
    all the benefits of an ensemble.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究尝试解决或减轻这些实际限制。例如，在一种叫做BatchEnsembles（[**?**]）的方法中，所有集成成员共享一个基础权重矩阵。每个集成成员的最终权重矩阵是通过将该共享权重矩阵与一个唯一的秩一矩阵按元素相乘得到的，这个秩一矩阵对每个集成成员都是唯一的。这减少了每增加一个集成成员需要存储的参数数量，从而减小了内存占用。BatchEnsembles的计算成本也得到了降低，因为它们可以利用向量化，并且所有集成成员的输出可以在一次前向传递中计算出来。在另一种方法中，称为**多输入/多输出**处理（**MIMO**；[**?**]），单个网络被鼓励学习多个独立的子网络。在训练过程中，多个输入与多个相应标注的输出一起传递。例如，网络会被呈现三张图片：一张狗的、一张猫的和一张鸡的。相应的输出标签也会传递，网络需要学习在第一个输出节点上预测“狗”，在第二个输出节点上预测“猫”，在第三个输出节点上预测“鸡”。在推理过程中，一张单独的图片会被重复三次，MIMO集成会产生三个不同的预测（每个输出节点一个）。因此，MIMO方法的内存占用和计算成本几乎与单一神经网络相当，同时仍能提供集成方法的所有优势。
- en: 6.4 Exploring neural network augmentation with Bayesian last-layer methods
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 探索贝叶斯最后一层方法在神经网络增强中的应用
- en: 'Through the course of [*Chapter 5*](CH5.xhtml#x1-600005), [*Principled Approaches
    for Bayesian Deep* *Learning*](CH5.xhtml#x1-600005) and [*Chapter 6*](#x1-820006),
    [*Using the Standard Toolbox for Bayesian Deep* *Learning*](#x1-820006), we’ve
    explored a variety of methods for Bayesian inference with DNNs. These methods
    have incorporated some form of uncertainty information at every layer, whether
    through the use of explicitly probabilistic means or via ensemble-based or dropout-based
    approximations. These methods have certain advantages. Their consistent Bayesian
    (or, more accurately, approximately Bayesian) mechanics mean that they are consistent:
    the same principles are applied at each layer, both in terms of network architecture
    and update rules. This makes them easier to justify from a theoretical standpoint,
    as we know that any theoretical guarantees apply at each layer. In addition to
    this, it means that we have the benefit of being able to access uncertainties
    at every level: we can exploit embeddings in these networks just as we exploit
    embeddings in standard deep learning models, and we’ll have access to uncertainties
    along with those embeddings.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通过[*第5章*](CH5.xhtml#x1-600005)、《[*贝叶斯深度学习的原则方法*](CH5.xhtml#x1-600005)》和[*第6章*](#x1-820006)、《[*使用标准工具箱进行贝叶斯深度学习*](#x1-820006)》，我们探索了多种用于深度神经网络（DNN）的贝叶斯推理方法。这些方法在每一层中都引入了某种形式的不确定性信息，无论是通过显式的概率方法，还是通过基于集成或丢弃法的近似。这些方法有其独特的优势。它们一致的贝叶斯（或者更准确地说，近似贝叶斯）机制意味着它们是一致的：相同的原理在每一层都得到应用，无论是在网络架构还是更新规则方面。这使得从理论角度解释它们变得更容易，因为我们知道任何理论上的保证都适用于每一层。除此之外，这还意味着我们能够在每一层访问不确定性：我们可以像在标准深度学习模型中利用嵌入一样，利用这些网络中的嵌入，并且我们将能够同时访问这些嵌入的不确定性。
- en: However, these networks also come with some drawbacks. As we’ve seen, methods
    such as PBP and BBB have more complicated mechanics, which makes them more difficult
    to apply to more sophisticated neural network architectures. The topics earlier
    in this chapter demonstrate that we can get around this by using MC dropout or
    deep ensembles, but they increase our overheads in terms of computation and/or
    memory footprint. This is where **Bayesian Last-Layer** (**BLL**) methods (see
    *Figure* [*6.4*](#x1-97005r4)) come in. This class of methods gives us both the
    flexibility of using any NN architecture, while also being more computationally
    and memory efficient than MC dropout or deep ensembles.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些网络也有一些缺点。正如我们所看到的，像 PBP 和 BBB 这样的算法具有更复杂的机制，这使得它们更难应用于更复杂的神经网络架构。本章前面讨论的内容表明，我们可以通过使用
    MC dropout 或深度集成来绕过这些问题，但它们会增加我们的计算和/或内存开销。此时，**贝叶斯最后一层**（**BLL**）方法（参见*图* [*6.4*](#x1-97005r4)）便派上用场。这类方法既能让我们灵活地使用任何神经网络架构，同时比
    MC dropout 或深度集成方法在计算和内存上更为高效。
- en: '![PIC](img/file144.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file144.png)'
- en: 'Figure 6.4: Vanilla NN compared to a BLL network'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6.4: Vanilla NN 与 BLL 网络的比较'
- en: 'As you’ve probably guessed, the fundamental principle behind BLL methods is
    to estimate uncertainties only at the last-layer. But what you may not have guessed
    is why this is possible. Deep learning’s success is due to the non-linear nature
    of NNs: the successive layers of non-linear transformations enable them to learn
    rich lower-dimensional representations of high-dimensional data. However, this
    non-linearity makes model uncertainty estimation difficult. Closed-form solutions
    for model uncertainty estimation are available for a variety of linear models,
    but unfortunately, this isn’t the case for our highly non-linear DNNs. So, what
    can we do?'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经猜到的，BLL 方法背后的基本原理是仅在最后一层估计不确定性。但是你可能没有猜到的是，为什么这会成为可能。深度学习的成功归因于神经网络的非线性特性：连续的非线性变换使其能够学习高维数据的丰富低维表示。然而，这种非线性使得模型不确定性估计变得困难。线性模型的模型不确定性估计有现成的封闭形式解，但不幸的是，对于我们高度非线性的
    DNN 来说，情况并非如此。那么，我们能做什么呢？
- en: 'Well, fortunately for us, the representations learned by the DNNs can also
    serve as inputs to simpler linear models. In this way, we let the DNN do the heavy
    lifting: condensing our high-dimensional input space down to a task-specific low-dimensional
    representation. Because of this, the penultimate layer in the NN is far easier
    to deal with; after all, in most cases our output is simply some linear transformation
    of this layer. This means we can apply a linear model to this layer, which in
    turn means we can apply closed-form solutions for model uncertainty estimation.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，DNN 学到的表示也可以作为更简单线性模型的输入。通过这种方式，我们让 DNN 来承担繁重的工作：将高维输入空间压缩为特定任务的低维表示。因此，神经网络中的倒数第二层要处理起来容易得多；毕竟，在大多数情况下，我们的输出仅仅是该层的某种线性变换。这意味着我们可以将线性模型应用于该层，这也意味着我们可以应用封闭形式解来进行模型不确定性估计。
- en: We can make use of other last-layer approaches too; recent work has demonstrated
    that MC dropout is effective when applied only at the last layer. While this still
    requires multiple forward passes, these forward passes only need to be done for
    a single layer, making them much more computationally efficient, particularly
    for larger models.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以利用其他最后一层方法；最近的研究表明，当仅在最后一层应用时，MC dropout 也很有效。尽管这仍然需要多次前向传播，但这些前向传播只需在单一层中完成，因此在计算上更加高效，尤其是对于较大的模型。
- en: 6.4.1 Last-layer methods for Bayesian inference
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 贝叶斯推理的最后一层方法
- en: The method proposed by Jasper Snoek et al. in their 2015 paper, *Scalable* *Bayesian
    Optimization Using Deep Neural Networks*, introduces the concept of using a post-hoc
    Bayesian linear regressor to obtain model uncertainties for DNNs. This method
    was devised as a way of achieving Gaussian Process-like high-quality uncertainties
    with improved scalability.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Jasper Snoek 等人在他们2015年的论文《*可扩展* *贝叶斯优化使用深度神经网络*》中提出的方法，引入了使用事后贝叶斯线性回归器来获得 DNN
    模型不确定性的概念。该方法被设计为一种实现类似高斯过程的高质量不确定性估计的方式，并且具有更好的可扩展性。
- en: The method first involves training a NN on some data *X* and targets **y**.
    This training phase trains a linear output layer, **z**[*i*], resulting in a network
    that produces point estimates (typical of a standard DNN). We then take the penultimate
    layer (or the last hidden layer), **z**[*i*−1], as our set of basis functions.
    From here, it’s simply a case of replacing our final layer with a Bayesian linear
    regressor. Now, instead of our point estimates, our network will produce a predictive
    mean and variance. For further details on this method and adaptive basis regression,
    we point the reader to Jasper Snoek et al.’s paper, and to Christopher Bishop’s
    *Pattern Recognition and Machine* *Learning*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法首先涉及在一些数据*X*和目标**y**上训练一个神经网络（NN）。这个训练阶段训练一个线性输出层，**z**[*i*]，结果是一个生成点估计的网络（这在标准的深度神经网络中是典型的）。然后，我们将倒数第二层（或最后一层隐藏层）**z**[*i*−1]作为我们的基础函数集。从这里开始，只需要将最后一层替换为贝叶斯线性回归器。现在，我们的网络将生成预测的均值和方差，而不是点估计。关于该方法和自适应基础回归的更多细节，请参阅Jasper
    Snoek等人的论文，以及Christopher Bishop的*模式识别与机器学习*。
- en: Now, let’s see how we achieve this in code.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何通过代码实现这一过程。
- en: 'Step 1: Creating and training our base model'
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 1：创建和训练我们的基础模型
- en: 'First, we set up and train our network:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们设置并训练我们的网络：
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Step 2: Using a neural network layer as a basis function'
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 2：使用神经网络层作为基础函数
- en: 'Now that we have our base network, we just need to access the penultimate layer
    so that we can feed this as our basis function to our Bayesian regressor. This
    is easily done using TensorFlow’s high-level API, for example:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了基础网络，我们只需要访问倒数第二层，这样我们就可以将其作为基础函数传递给我们的贝叶斯回归器。这可以通过使用TensorFlow的高级API轻松完成，例如：
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will build a model that will allow us to obtain the output of the second
    hidden layer by simply calling its `predict` method:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这将构建一个模型，允许我们通过简单地调用其`predict`方法来获得第二个隐藏层的输出：
- en: '[PRE12]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This is all that’s needed to prepare our basis function for passing to our Bayesian
    linear regressor.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们为传递给贝叶斯线性回归器准备基础函数所需要做的一切。
- en: 'Step 3: Preparing our variables for Bayesian linear regression'
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 3：为贝叶斯线性回归准备我们的变量
- en: 'For the Bayesian regressor, we assume that our outputs, *y*[*i*] ∈ **y**, are
    conditionally normally distributed according to a linear relationship with our
    inputs, **x**[*i*] ∈ *X*:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于贝叶斯回归器，我们假设我们的输出，*y*[*i*] ∈ **y**，根据与输入**x**[*i*] ∈ *X*的线性关系条件地服从正态分布：
- en: '![yi = 𝒩 (α + x⊺iβ, σ2) ](img/file145.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![yi = 𝒩 (α + x⊺iβ, σ²) ](img/file145.jpg)'
- en: 'Here, *α* is our bias term, *β* are our model coefficients, and *σ*² is the
    variance associated with our predictions. We’ll also make some prior assumptions
    about these parameters, namely:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*α*是我们的偏置项，*β*是我们的模型系数，*σ*²是与我们的预测相关的方差。我们还将对这些参数做出一些先验假设，即：
- en: '![α ≈ 𝒩 (0,1) ](img/file146.jpg)![β ≈ 𝒩 (0,1) ](img/file147.jpg)![σ2 ≈ |𝒩 (0,1)|
    ](img/file148.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![α ≈ 𝒩 (0,1) ](img/file146.jpg)![β ≈ 𝒩 (0,1) ](img/file147.jpg)![σ² ≈ |𝒩 (0,1)|
    ](img/file148.jpg)'
- en: 'Note that equation 6.6 denotes the half-normal of a Gaussian distribution.
    To wrap up the Bayesian regressor in such a way that it’s easy (and practical)
    to integrate it with our Keras model, we’ll create a `BayesianLastLayer` class.
    This class will use the TensorFlow Probability library to allow us to implement
    the probability distributions and sampling functions we’ll need for our Bayesian
    regressor. Let’s walk through the various components of our class:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，公式6.6表示的是高斯分布的半正态分布。为了将贝叶斯回归器包装成易于（且实用地）与我们的Keras模型集成的形式，我们将创建一个`BayesianLastLayer`类。这个类将使用TensorFlow
    Probability库，使我们能够实现贝叶斯回归器所需的概率分布和采样函数。让我们逐步了解我们类的各个组件：
- en: '[PRE13]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As we see here, our class requires at least two arguments at instantiation:
    `model`, which is our Keras model, and `basis``_layer`, which is the layer output
    we wanted to feed to our Bayesian regressor. The following arguments are all parameters
    for the **Hamiltonian Monte-Carlo** (**HMC**) sampling for which we define some
    default values. These values may need to be changed depending on the input. For
    example, for a higher dimensional input (for instance, if you’re using `layer``_1`),
    you may want to further reduce the step size and increase both the number of burn-in
    steps and the overall number of samples.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们的类在实例化时至少需要两个参数：`model`，即我们的Keras模型；和`basis``_layer`，即我们希望馈送给贝叶斯回归器的层输出。接下来的参数都是**哈密顿蒙特卡罗**（**HMC**）采样的参数，我们为其定义了一些默认值。根据输入的不同，这些值可能需要调整。例如，对于更高维度的输入（例如，如果你使用的是`layer``_1`），你可能希望进一步减小步长并增加燃烧期步骤的数量以及总体样本数。
- en: 'Step 4: Connecting our basis function model'
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第4步：连接我们的基础函数模型
- en: 'Next, we simply define a few functions for creating our basis function model
    and for obtaining its outputs:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们简单定义几个函数，用于创建我们的基础函数模型并获取其输出：
- en: '[PRE14]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Step 5: Creating a method to fit our Bayesian linear regression parameters'
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第5步：创建适配贝叶斯线性回归参数的方法
- en: Now things get a little more complicated. We need to define the `fit()` method,
    which will use HMC sampling to find our model parameters *α*, *β*, and *σ*². We’ll
    provide an overview of what the code is doing here, but for more (hands-on) information
    on sampling, we direct the reader to *Bayesian Analysis with Python* by Osvaldo
    Martin.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在事情变得有些复杂。我们需要定义`fit()`方法，它将使用HMC采样来找到我们的模型参数*α*、*β*和*σ*²。我们将在这里提供代码做了什么的概述，但关于采样的更多（实践）信息，我们推荐读者参考Osvaldo
    Martin的《Python贝叶斯分析》。
- en: 'Firstly, we define a joint distribution using the priors described in equations
    4.3-4.5\. Thanks to TensorFlow Probability’s `distributions` module, this is pretty
    straightforward:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用方程4.3-4.5中描述的先验定义一个联合分布。得益于TensorFlow Probability的`distributions`模块，这非常简单：
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We then go on to set up our sampler using TensorFlow Probability’s `HamiltonianMonteCarlo`
    sampler class. To do this, we’ll need to define our target log probability function.
    The `distributions` module makes this fairly trivial, but we still need to define
    a function to feed our model parameters to the distribution object’s `log``_prob()`
    method (line 28). We can then pass this to the instantiation of `hmc``_kernel`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用TensorFlow Probability的`HamiltonianMonteCarlo`采样器类来设置我们的采样器。为此，我们需要定义目标对数概率函数。`distributions`模块使得这一过程相当简单，但我们仍然需要定义一个函数，将我们的模型参数传递给分布对象的`log``_prob()`方法（第28行）。然后我们将其传递给`hmc``_kernel`的实例化：
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now that things are set up, we’re ready to run our sampler. To do this, we
    call the `mcmc.sample``_chain()` function, passing in our HMC parameters, an initial
    state for our model parameters, and our HMC sampler. We then run our sampling,
    which returns `states`, which comprises our parameter samples, and `kernel``_results`,
    which contains some information about the sampling process. The information we
    care about here is to do with the proportion of accepted samples. If our sampler
    has run successfully, then we’ll have a good proportion of accepted samples (indicating
    a high acceptance rate). If it hasn’t been successful, then our acceptance rate
    will be low (perhaps even 0%!) and we may need to tune our sampler parameters.
    We print this to the console so that we can keep an eye on the acceptance rate
    (we wrap the call to `sample``_chain()` in a `run``_chain()` function so that
    it can be extended to sampling with multiple chains):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切已经设置好，我们准备运行采样器了。为此，我们调用`mcmc.sample``_chain()`函数，传入我们的HMC参数、模型参数的初始状态和我们的HMC采样器。然后我们运行采样，它会返回`states`，其中包含我们的参数样本，以及`kernel``_results`，其中包含一些关于采样过程的信息。我们关心的信息是关于接受样本的比例。如果采样器运行成功，我们将有一个较高比例的接受样本（表示接受率很高）。如果采样器没有成功，接受率会很低（甚至可能是0%！），这时我们可能需要调整采样器的参数。我们会将这个信息打印到控制台，以便随时监控接受率（我们将对`sample``_chain()`的调用封装在`run``_chain()`函数中，这样它可以扩展为多链采样）：
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Once we’ve run our sampler, we can fetch our model parameters. We take them
    from the post-burn-in samples and assign them to class variables for later use
    in inference:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行了采样器，我们就可以获取我们的模型参数。我们从后燃烧样本中提取它们，并将其分配给类变量，以便后续推断使用：
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Step 6: Inference'
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第6步：推断
- en: 'The last thing we need to do is implement a function to make predictions using
    the learned parameters of our joint distribution. To do this, we’ll define two
    functions: `get``_divd``_dist()`, which will obtain the posterior predictive distribution
    given our input, and `predict()`, which will call `get``_divd``_dist()` and compute
    our mean (*μ*) and standard deviation (*σ*) from our posterior distribution:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的最后一件事是实现一个函数，利用我们联合分布的学习到的参数来进行预测。为此，我们将定义两个函数：`get``_divd``_dist()`，它将根据我们的输入获取后验预测分布；以及`predict()`，它将调用`get``_divd``_dist()`并计算我们后验分布的均值（*μ*）和标准差（*σ*）：
- en: '[PRE19]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And that’s it! We have our BLL implementation! With this class, we have a powerful
    and principled means of obtaining Bayesian uncertainty estimates by using penultimate
    NN layers as basis functions for Bayesian regression. Making use of it is as simple
    as passing our model and defining which layer we want to use as our basis function:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们实现了BLL！通过这个类，我们可以通过使用倒数第二层神经网络作为贝叶斯回归的基函数，获得强大而有原则的贝叶斯不确定性估计。使用它的方法非常简单，只需传入我们的模型并定义我们希望使用哪个层作为基函数：
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'While this is a powerful tool, it’s not always suited for the task at hand.
    You can experiment with this yourself: try creating a model with a larger embedding
    layer. As the size of the layer increases, you should start to see that the acceptance
    rate of the sampler drops. Once it’s large enough, the acceptance rate may even
    fall to 0%. So, we’ll need to modify the parameters of our sampler: reducing the
    step size, increasing the number of samples, and increasing the number of burn-in
    samples. As the dimensionality of the embedding grows, it becomes more and more
    difficult to obtain a representative set of samples for the distribution.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是一个强大的工具，但并不总是适合当前任务。你可以自己进行实验：尝试创建一个更大的嵌入层。随着层的大小增加，你应该会看到采样器的接受率下降。一旦它变得足够大，接受率甚至可能下降到0%。因此，我们需要修改采样器的参数：减少步长，增加样本数，并增加烧入样本数。随着嵌入维度的增加，获取一个代表性样本集来描述分布变得越来越困难。
- en: 'For some applications, this isn’t an issue, but when dealing with complex,
    high-dimensional data, this can quickly become problematic. Applications in domains
    such as computer vision, speech processing, and molecular modeling all rely on
    high-dimensional embeddings. One solution here is to reduce these embeddings further,
    for example, via dimensionality reduction. But doing so can have an unpredictable
    effect on these encodings: in fact, by reducing the dimensionality, you could
    be unintentionally removing sources of uncertainty, resulting in poorer quality
    uncertainty estimates.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些应用来说，这不是问题，但在处理复杂的高维数据时，这可能很快成为一个问题。计算机视觉、语音处理和分子建模等领域的应用都依赖于高维嵌入。这里的一个解决方案是进一步降低这些嵌入的维度，例如通过降维。但这样做可能会对这些编码产生不可预测的影响：事实上，通过降低维度，你可能会无意中去除一些不确定性的来源，从而导致更差的质量的不确定性估计。
- en: So, what can we do instead? Fortunately, there are a few other last-layer options
    we can employ. Next, we’ll see how we can use last-layer dropout to approximate
    the Bayesian linear regression approach introduced here.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们能做些什么呢？幸运的是，我们可以使用一些其他的最后一层选项。接下来，我们将看看如何使用最后一层的丢弃法（dropout）来逼近这里介绍的贝叶斯线性回归方法。
- en: 6.4.2 Last-layer MC dropout
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 最后一层MC丢弃
- en: 'Earlier in the chapter, we saw how we can use dropout at test time to obtain
    a distribution over our model predictions. Here, we’ll combine that concept with
    the concept of last-layer uncertainties: adding an MC dropout layer, but only
    as a single layer that we add to a pre-trained network.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章早些时候，我们看到如何在测试时使用丢弃法获取模型预测的分布。在这里，我们将这个概念与最后一层不确定性概念结合：添加一个MC丢弃层，但仅作为我们添加到预训练网络中的一个单一层。
- en: 'Step 1: Connecting to our base model'
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 1：连接到我们的基础模型
- en: 'Similarly to the Bayesian last-layer method, we first need to obtain the output
    from our model’s penultimate layer:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 与贝叶斯最后一层方法类似，我们首先需要从模型的倒数第二层获取输出：
- en: '[PRE21]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Step 2: Adding an MC dropout layer'
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 2：添加MC丢弃层
- en: 'Now, instead of implementing a Bayesian regressor, we’ll simply instantiate
    a new output layer, which applies dropout to the penultimate layer:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们不再实现一个贝叶斯回归器，而是简单地实例化一个新的输出层，应用丢弃法（dropout）到倒数第二层：
- en: '[PRE22]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Step 3: Training the MC dropout last-layer'
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 3：训练MC丢弃的最后一层
- en: 'Because we’ve now added a new final layer, we need to run an additional step
    of training so that it can learn the mapping from our penultimate layer to the
    new output; but because our original model is doing all of the heavy lifting,
    this training is both computationally cheap and quick to run:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们现在增加了一个新的最终层，我们需要进行额外的训练步骤，让它能够学习从倒数第二层到新输出的映射；但由于我们原始模型已经完成了大部分工作，这个训练过程既计算成本低，又运行快速：
- en: '[PRE23]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Step 4: Obtaining uncertainties'
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第4步：获取不确定性
- en: 'Now that our last layer is trained, we can implement a function to obtain the
    mean and standard deviation for our predictions using multiple forward passes
    of our MC dropout layer; line 3 onwards should be familiar from earlier in the
    chapter, and line 2 simply obtains the output from our original model’s penultimate
    layer:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的最后一层已经训练完成，我们可以实现一个函数，通过对MC dropout层进行多次前向传递来获取预测的均值和标准差；从第3行开始应该和本章前面的内容相似，第2行只是获取我们原始模型倒数第二层的输出：
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Step 5: Inference'
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第5步：推理
- en: 'All that’s left is to call this function and obtain our new model outputs,
    complete with uncertainty estimates:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的就是调用这个函数，获取我们的新模型输出，并附带不确定性估计：
- en: '[PRE25]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Last-layer MC dropout is by far one of the easiest ways to obtain uncertainty
    estimates from pre-trained networks. Unlike standard MC dropout, it doesn’t require
    training a model from scratch, so you can apply this post-hoc to networks you’ve
    already trained. Additionally, unlike the other last-layer methods, it can be
    implemented in just a few straightforward steps that never stray from TensorFlow’s
    standard API.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层MC dropout迄今为止是从预训练网络中获得不确定性估计的最简单方法。与标准的MC dropout不同，它不需要从头开始训练模型，因此你可以将其应用于你已经训练好的网络。此外，与其他最后一层方法不同，它只需要几个简单的步骤即可实现，并且始终遵循TensorFlow的标准API。
- en: 6.4.3 Recap of last-layer methods
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.3 最后一层方法回顾
- en: Last-layer methods are an excellent tool for when you need to obtain uncertainty
    estimates from a pre-trained network. Given how expensive and time-consuming neural
    network training can be, it’s nice not to have to start from scratch just because
    you need some predictive uncertainties. Additionally, given that more and more
    machine learning practitioners are relying on state-of-the-art pre-trained models,
    these kinds of techniques are a practical way to incorporate model uncertainties
    after the fact.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层方法是当你需要从预训练网络中获取不确定性估计时的一个极好的工具。考虑到神经网络训练的高昂成本和耗时，能够在不从头开始的情况下仅因为需要预测不确定性而避免重新训练，实在是非常便利。此外，随着越来越多的机器学习从业者依赖于最先进的预训练模型，这些技术在事后结合模型不确定性是一个非常实用的方法。
- en: 'But there are drawbacks to last-layer methods too. Unlike other methods, we’re
    relying on a fairly limited source of variance: the penultimate layer of our model.
    This limits how much stochasticity we can induce over our model outputs, meaning
    we’re at risk of over-confident predictions. Bear this in mind when using last-layer
    methods and, if you see the hallmark signs of over-confidence, consider using
    a more comprehensive method to obtain your predictive uncertainties.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，最后一层方法也有其缺点。与其他方法不同，我们依赖的是一个相对有限的方差来源：我们模型的倒数第二层。这限制了我们能够在模型输出上引入的随机性，因此我们有可能会面临过于自信的预测。在使用最后一层方法时请记住这一点，如果你看到过度自信的典型迹象，考虑使用更全面的方法来获取预测的不确定性。
- en: 6.5 Summary
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 小结
- en: 'In this chapter, we’ve seen how familiar machine learning and deep learning
    concepts can be used to develop models with predictive uncertainties. We’ve also
    seen how, with relatively minor modifications, we can add uncertain estimates
    to pre-trained models. This means we can go beyond the point-estimate approach
    of standard NNs: using uncertainties to gain valuable insights into the performance
    of our models, and allowing us to develop more robust applications.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们看到了如何利用熟悉的机器学习和深度学习概念开发带有预测不确定性的模型。我们还看到了，通过相对少量的修改，我们可以将不确定性估计添加到预训练模型中。这意味着我们可以超越标准神经网络的点估计方法：利用不确定性获得关于模型性能的宝贵见解，从而使我们能够开发更稳健的应用。
- en: However, as with the methods introduced in [*Chapter 5*](CH5.xhtml#x1-600005),
    [*Principled Approaches* *for Bayesian Deep Learning*](CH5.xhtml#x1-600005), all
    techniques have advantages and disadvantages. For example, last-layer methods
    may give us the flexibility to add uncertainties to any model, but they’re limited
    by the representation that the model has already learned. This could result in
    very low variance outputs, resulting in an overconfident model. Similarly, while
    ensemble methods allow us to capture variance across every layer of the network,
    they come at significant computational cost, requiring that we have multiple networks,
    rather than just a single network.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，就像[*第五章*](CH5.xhtml#x1-600005)，[*贝叶斯深度学习的原则方法*](CH5.xhtml#x1-600005)中介绍的方法一样，所有技术都有其优点和缺点。例如，最后一层方法可能使我们能够向任何模型添加不确定性，但它们受到模型已经学习到的表示的限制。这可能导致输出的方差非常低，从而产生过于自信的模型。同样，集成方法虽然允许我们捕获网络每一层的方差，但它们需要显著的计算成本，需要我们有多个网络，而不仅仅是单个网络。
- en: In the next chapter, we will examine the advantages and disadvantages in more
    detail, and learn how we can address some of the shortcomings of these methods.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将更详细地探讨优缺点，并学习如何解决这些方法的一些缺点。
