- en: Annotating Images with Object Detection API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用物体检测 API 注释图像
- en: 'Computer vision has made great leaps forward in recent years because of deep
    learning, thus granting computers a higher grade in understanding visual scenes.
    The potentialities of deep learning in vision tasks are great: allowing a computer
    to visually perceive and understand its surroundings is a capability that opens
    the door to new artificial intelligence applications in both mobility (for instance,
    self-driving cars can detect if an appearing obstacle is a pedestrian, an animal
    or another vehicle from the camera mounted on the car and decide the correct course
    of action) and human-machine interaction in everyday-life contexts (for instance,
    allowing a robot to perceive surrounding objects and successfully interact with
    them).'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，计算机视觉因深度学习取得了重大进展，从而赋予计算机更高的理解视觉场景的能力。深度学习在视觉任务中的潜力巨大：让计算机能够视觉感知和理解其周围环境是开启新型人工智能应用的大门，这些应用包括移动领域（例如，自动驾驶汽车能够从车载摄像头检测出障碍物是行人、动物还是其他车辆，并决定正确的行动路线）以及日常生活中的人机交互（例如，让机器人感知周围物体并成功与之互动）。
- en: After presenting ConvNets and how they operate in the first chapter, we now
    intend to create a quick, easy project that will help you to use a computer to
    understand images taken from cameras and mobile phones, using images collected
    from the Internet or directly from your computer's webcam. The goal of the project
    is to find the exact location and the type of the objects in an image.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章介绍了卷积神经网络（ConvNets）及其操作原理后，我们现在打算创建一个快速、简单的项目，帮助您使用计算机理解来自相机和手机拍摄的图像，图像可以来自互联网或直接来自计算机的摄像头。该项目的目标是找出图像中物体的准确位置和类型。
- en: In order to achieve such classification and localization, we will leverage the
    new TensorFlow object detection API, a Google project that is part of the larger
    TensorFlow models project which makes a series of pre-trained neural networks
    available off-the-shelf for you to wrap up in your own custom applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这种分类和定位，我们将利用新的 TensorFlow 物体检测 API，这是一个谷歌项目，属于更大的 TensorFlow 模型项目的一部分，该项目为您提供一系列预训练的神经网络，您可以将其直接用于您的自定义应用程序中。
- en: 'In this chapter, we are going to illustrate the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将展示以下内容：
- en: The advantages of using the right data for your project
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正确数据对您的项目的优势
- en: A brief presentation of the TensorFlow object detection API
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 物体检测 API 简介
- en: How to annotate stored images for further use
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何注释存储的图像以供进一步使用
- en: How to visually annotate a video using `moviepy`
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 `moviepy` 对视频进行视觉注释
- en: How to go real-time by annotating images from a webcam
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过注释来自网络摄像头的图像实现实时操作
- en: The Microsoft common objects in context
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微软常见物体语境数据集
- en: Advances in application of deep learning  in computer vision are often highly
    focalized on the kind of classification problems that can be summarized by challenges
    such as ImageNet (but also, for instance, PASCAL VOC - [http://host.robots.ox.ac.uk/pascal/VOC/voc2012/](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/))
    and the ConvNets suitable to crack it (Xception, VGG16, VGG19, ResNet50, InceptionV3,
    and MobileNet, just to quote the ones available in the well-known package `Keras`: [https://keras.io/applications/](https://keras.io/applications/)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在计算机视觉中的应用进展通常高度集中在可以通过诸如 ImageNet（但也包括 PASCAL VOC - [http://host.robots.ox.ac.uk/pascal/VOC/voc2012/](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/)）等挑战进行总结的分类问题上，以及适合解决这些问题的卷积神经网络（如
    Xception、VGG16、VGG19、ResNet50、InceptionV3 和 MobileNet，仅举出在知名包 `Keras` 中提供的几种：
    [https://keras.io/applications/](https://keras.io/applications/)）。
- en: Though deep learning networks based on ImageNet data are the actual state of
    the art,  such networks can experience difficulties when faced with real-world
    applications. In fact, in practical applications, we have to process images that
    are quite different from the examples provided by ImageNet. In ImageNet the elements
    to be classified are clearly the only clear element present in the image, ideally
    set in an unobstructed way near the center of a neatly composed photo. In the
    reality of images taken from the field, objects are randomly scattered around,
    in often large number.  All these objects are also quite different from each other,
    creating sometimes confusing settings. In addition, often objects of interest
    cannot be clearly and directly perceived because they are visually obstructed
    by other potentially interesting objects.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于ImageNet数据的深度学习网络是当前的技术前沿，但这些网络在面对实际应用时可能会遇到困难。实际上，在实际应用中，我们必须处理的图像与ImageNet提供的示例有很大不同。在ImageNet中，待分类的元素通常是图像中唯一清晰可见的元素，理想情况下位于照片的中心，且无遮挡。然而，在实际拍摄的图像中，物体往往分散在不同位置，并且数量众多。所有这些物体之间的差异也很大，导致有时会出现混乱的场景。此外，物体往往会被其他潜在有趣的物体遮挡，使得它们无法被清晰直接地感知。
- en: 'Please refer to the figure from the following mentioned reference:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下提到的参考文献中的图示：
- en: 'Figure 1: A sample of images from ImageNet: they are arranged in a hierarchical
    structure, allowing working with both general or more specific classes.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：来自ImageNet的图像示例：它们按层次结构排列，既可以处理一般类别，也可以处理更具体的类别。
- en: 'SOURCE: DENG, Jia, et al. Imagenet: A large-scale hierarchical image database.
    In: Computer Vision and Pattern Recognition, 2009\. CVPR 2009\. IEEE Conference
    on. IEEE, 2009\. p. 248-255.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：DENG, Jia等. ImageNet：一个大规模层次化图像数据库。载于：计算机视觉与模式识别，2009。CVPR 2009。IEEE会议。IEEE，2009年，248-255页。
- en: Realistic images contain multiple objects that sometimes can hardly be distinguished
    from a noisy background. Often you really cannot create interesting projects just
    by labeling an image with a tag simply telling you the object was recognized with
    the highest confidence.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 真实图像包含多个物体，这些物体有时很难与嘈杂的背景区分开。通常，单纯通过标记图像并用一个标签告诉你物体被识别出的置信度最高，你实际上无法创造出有趣的项目。
- en: 'In a real-world application, you really need to be able to do the following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，你真的需要能够做到以下几点：
- en: Object classification of single and multiple instances when recognizing various
    objects, often of the same class
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个和多个实例的物体分类，通常是同一类别的不同物体
- en: Image localization, that is understanding where the objects are in the image
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像定位，即理解物体在图像中的位置
- en: 'Image segmentation,  by marking each pixel in the images with a label: the
    type of object or background in order to be able to cut off interesting parts
    from the background.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分割，通过为图像中的每个像素打上标签：标明物体类型或背景，以便能够从背景中切割出有趣的部分。
- en: 'The necessity to train a ConvNet to be able to achieve some or all of the preceding
    mentioned objectives led to the creation of the **Microsoft common objects in
    context** (**MS COCO**) dataset, as described in the paper: LIN, Tsung-Yi, et
    al. Microsoft coco: common objects in context. In: *European conference on computer
    vision*. Springer, Cham, 2014\. p. 740-755. (You can read the original paper at
    the following link: [https://arxiv.org/abs/1405.0312](https://arxiv.org/abs/1405.0312).)
    This dataset is made up of 91 common object categories, hierarchically ordered,
    with 82 of them having more than 5,000 labeled instances. The dataset totals 2,500,000
    labeled objects distributed in 328,000 images.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现前述目标之一或全部目标，需要训练一个ConvNet，这促成了**微软常见物体上下文（MS COCO）**数据集的创建，具体描述见论文：LIN,
    Tsung-Yi等。Microsoft coco：常见物体的上下文。载于：*欧洲计算机视觉会议*。Springer，Cham，2014年，740-755页。（你可以通过以下链接阅读原文：[https://arxiv.org/abs/1405.0312](https://arxiv.org/abs/1405.0312)。）该数据集由91个常见物体类别组成，按层次结构排列，其中82个类别具有超过5,000个标记实例。该数据集总计包含2,500,000个标记物体，分布在328,000张图像中。
- en: 'Here are the classes that can be recognized in the MS COCO dataset:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是MS COCO数据集中可以识别的类别：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Though the `ImageNet` dataset can present 1,000 object classes (as described
    at [https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a))
    distributed in 14,197,122 images,  MS COCO offers the peculiar feature of multiple
    objects distributed in a minor number of images (the dataset has been gathered
    using Amazon Mechanical Turk, a somehow more costly approach but shared by ImageNet,
    too). Given such premises, the MS COCO images can be considered very good examples
    of *contextual relationships and non-iconic object views*, since objects are arranged
    in realistic positions and settings. This can be verified from this comparative
    example taken from the MS COCO paper previously mentioned:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`ImageNet`数据集可以展示1,000个物体类别（如[https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)所述），并且分布在14,197,122张图像中，MS
    COCO则提供了一个独特的特点：多个物体分布在较少的图像中（该数据集是通过亚马逊Mechanical Turk收集的，这种方法相对更为昂贵，但也被ImageNet采用）。基于这一前提，MS
    COCO的图像可以被视为*上下文关系和非图标物体视图*的极好示例，因为物体被安排在现实的场景和位置中。从下文中可以验证这一点，这是从上述提到的MS COCO论文中提取的比较示例：
- en: '![](img/e385e147-49ee-4177-a540-cd7715d1a81d.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e385e147-49ee-4177-a540-cd7715d1a81d.png)'
- en: 'Figure 2: Examples of iconic and non-iconic images. SOURCE: <q>LIN, Tsung-Yi,
    et al. Microsoft coco: common objects in context. In: European conference on computer
    vision. Springer, Cham, 2014\. p. 740-755.</q>'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '图2：图标和非图标图像的示例。来源：<q>LIN, Tsung-Yi, 等. Microsoft coco: common objects in context.
    在：欧洲计算机视觉会议。Springer, Cham, 2014。p. 740-755。</q>'
- en: In addition, the image annotation of MS COCO is particularly rich, offering
    the coordinates of the contours of the objects present in the images. The contours
    can be easily translated into bounding boxes, boxes that delimit the part of the
    image where the object is located. This is a rougher way to locate objects than
    the original one used for training MS COCO itself, based on pixel segmentation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，MS COCO的图像注释特别丰富，提供了图像中物体轮廓的坐标。这些轮廓可以轻松转换为边界框，即限定物体所在图像部分的框。这是一种比用于训练MS COCO本身的原始方法（基于像素分割）更粗略的物体定位方式。
- en: In the following figure, a crowded row has been carefully segmented by defining
    notable areas in an image and creating a textual description of those areas. In
    machine learning terms, this translates to assigning a label to every pixel in
    the image and trying to predict the segmentation class (corresponding to the textual
    description). Historically this has been done with image processing until ImageNet
    2012 when deep learning proved a much more efficient solution.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，一排拥挤的物体被仔细分割，通过定义图像中的显著区域并创建这些区域的文本描述。在机器学习的术语中，这相当于为图像中的每个像素分配一个标签，并尝试预测分割类别（对应于文本描述）。历史上，这一任务一直通过图像处理完成，直到2012年ImageNet的出现，深度学习证明是一种更高效的解决方案。
- en: '2012 marked a milestone in computer vision because for the first time a deep
    learning solution provided many superior results than any technique used before:
    <q>KRIZHEVSKY, Alex; SUTSKEVER, Ilya; HINTON, Geoffrey E. Imagenet classification
    with deep convolutional neural networks. In: Advances in neural information processing
    systems. 2012\. p. 1097-1105</q> ( [https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年是计算机视觉的一个里程碑，因为深度学习解决方案首次提供了比任何之前使用的技术都要优越的结果：<q>KRIZHEVSKY, Alex; SUTSKEVER,
    Ilya; HINTON, Geoffrey E. 使用深度卷积神经网络进行ImageNet分类。 在：神经信息处理系统的进展。2012年。p. 1097-1105</q>
    ([https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf))。
- en: 'Image segmentation is particularly useful for various tasks, such as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割对于多种任务特别有用，例如：
- en: Highlighting the important objects in an image, for instance in medical applications
    detecting areas with illness
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图像中突出显示重要物体，例如在医学应用中检测患病区域
- en: Locating objects in an image so that a robot can pick them up or manipulate
    them
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图像中定位物体，以便机器人能够拾取或操作它们
- en: Helping with road scene understanding for self-driving cars or drones to navigate
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帮助自动驾驶汽车或无人机理解道路场景以进行导航
- en: Editing images by automatically extracting portions of an image or removing
    a background
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过自动提取图像的一部分或去除背景来编辑图像
- en: 'This kind of annotation is very expensive (hence the reduced number of examples
    in MS COCO) because it has to be done completely by hand and it requires attention
    and precision. There are some tools to help with annotating by segmenting an image.
    You can find a comprehensive list at [https://stackoverflow.com/questions/8317787/image-labelling-and-annotation-tool](https://stackoverflow.com/questions/8317787/image-labelling-and-annotation-tool).
    However, we can suggest the following two tools, if you want to annotate by segmentation
    images by yourself:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种标注非常昂贵（因此MS COCO中的样本数量较少），因为它必须完全手动完成，且需要细心和精确。有一些工具可以帮助通过图像分割来进行标注。你可以在[https://stackoverflow.com/questions/8317787/image-labelling-and-annotation-tool](https://stackoverflow.com/questions/8317787/image-labelling-and-annotation-tool)找到一个完整的工具列表。然而，如果你想自己通过分割图像进行标注，我们可以推荐以下两款工具：
- en: LabelImg [https://github.com/tzutalin/labelImg](https://github.com/tzutalin/labelImg)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LabelImg [https://github.com/tzutalin/labelImg](https://github.com/tzutalin/labelImg)
- en: FastAnnotationTool [https://github.com/christopher5106/FastAnnotationTool](https://github.com/christopher5106/FastAnnotationTool)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FastAnnotationTool [https://github.com/christopher5106/FastAnnotationTool](https://github.com/christopher5106/FastAnnotationTool)
- en: 'All these tools can also be used for the much simpler annotation by bounding
    boxes, and they really can come in handy if you want to retrain a model from MS
    COCO using a class of your own. (We will mention this again at the end of the
    chapter):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些工具也可以用于通过边界框进行更简单的标注，如果你想使用自己定义的类别重新训练一个基于MS COCO的模型，它们确实会派上用场。（我们将在本章最后再次提到这一点）：
- en: '![](img/c1fdcbaf-6a54-4ee1-8097-2ceb57ac2a70.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c1fdcbaf-6a54-4ee1-8097-2ceb57ac2a70.png)'
- en: A pixel segmentation of an image used in MS COCO training phase
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在MS COCO训练阶段使用的图像像素分割
- en: The TensorFlow object detection API
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow目标检测API
- en: As a way of boosting the capabilities of the research community, Google research
    scientists and software engineers often develop state-of-the-art models and make
    them available to the public instead of keeping them proprietary. As described
    in the Google research blog post, [https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html](https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html)
    , on October 2016, Google's in-house object detection system placed first in the
    COCO detection challenge, which is focused on finding objects in images (estimating
    the chance that an object is in this position) and their bounding boxes (you can
    read the technical details of their solution at [https://arxiv.org/abs/1611.10012](https://arxiv.org/abs/1611.10012)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提升研究社区能力的一种方式，Google的研究科学家和软件工程师通常会开发最先进的模型，并将其公开，而不是保留为专有技术。正如Google研究博客中所描述的，
    [https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html](https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html)，在2016年10月，Google内部的目标检测系统在COCO检测挑战赛中获得第一名，该挑战赛专注于在图像中找到物体（估算物体在该位置的可能性）及其边界框（你可以在[https://arxiv.org/abs/1611.10012](https://arxiv.org/abs/1611.10012)阅读他们解决方案的技术细节）。
- en: The Google solution has not only contributed to quite a few papers and been
    put to work in some Google products (Nest Cam - [https://nest.com/cameras/nest-aware/](https://nest.com/cameras/nest-aware/),
    Image Search - [https://www.blog.google/products/search/now-image-search-can-jump-start-your-search-style/](https://www.blog.google/products/search/now-image-search-can-jump-start-your-search-style/),
    and Street View - [https://research.googleblog.com/2017/05/updating-google-maps-with-deep-learning.html](https://research.googleblog.com/2017/05/updating-google-maps-with-deep-learning.html)),
    but has also been released to the larger public as an open source framework built
    on top of TensorFlow.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Google的解决方案不仅为很多论文做出了贡献，并且已被应用到一些Google产品中（Nest Cam - [https://nest.com/cameras/nest-aware/](https://nest.com/cameras/nest-aware/)，图片搜索
    - [https://www.blog.google/products/search/now-image-search-can-jump-start-your-search-style/](https://www.blog.google/products/search/now-image-search-can-jump-start-your-search-style/)，街景
    - [https://research.googleblog.com/2017/05/updating-google-maps-with-deep-learning.html](https://research.googleblog.com/2017/05/updating-google-maps-with-deep-learning.html)），同时也作为一个开源框架发布给更广泛的公众，建立在TensorFlow之上。
- en: 'The framework offers some useful functions and  these five pre-trained different
    models (constituting the so-called pre-trained Model Zoo):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架提供了一些有用的功能以及这五个预训练的不同模型（构成了所谓的预训练模型库）：
- en: Single Shot Multibox Detector (SSD) with MobileNets
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于MobileNets的单次检测多框（SSD）
- en: SSD with Inception V2
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于Inception V2的SSD
- en: Region-Based Fully Convolutional Networks (R-FCN) with Resnet 101
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于Resnet 101的区域卷积神经网络（R-FCN）
- en: Faster R-CNN with Resnet 101
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于Resnet 101的Faster R-CNN
- en: Faster R-CNN with Inception Resnet v2
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Inception Resnet v2 的 Faster R-CNN
- en: 'The models are in growing order of precision in detection and slower speed
    of execution of the detection process. MobileNets, Inception and Resnet refer
    to different types of CNN network architectures (MobileNets, as the name suggests,
    it is the architecture optimized for mobile phones, smaller in size and faster
    in execution). We have discussed CNN architecture in the previous chapter, so
    you can refer there for more insight on such architectures. If you need a refresher,
    this blog post by Joice Xu can help you revise the topic in an easy way: [https://towardsdatascience.com/an-intuitive-guide-to-deep-network-architectures-65fdc477db41](https://towardsdatascience.com/an-intuitive-guide-to-deep-network-architectures-65fdc477db41).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型按照检测精度的逐渐提高以及检测过程执行速度的逐渐减慢排序。MobileNets、Inception 和 Resnet 指的是不同类型的卷积神经网络（CNN）架构（MobileNets，顾名思义，是为手机优化的架构，体积小、执行速度快）。我们在上一章已经讨论过
    CNN 架构，您可以参考那里以获取更多关于这些架构的深入信息。如果您需要复习，Joice Xu 撰写的这篇博客文章可以帮助您以一种简便的方式复习该主题：[https://towardsdatascience.com/an-intuitive-guide-to-deep-network-architectures-65fdc477db41](https://towardsdatascience.com/an-intuitive-guide-to-deep-network-architectures-65fdc477db41)。
- en: '**Single Shot Multibox Detector** (**SSD**),  **Region-Based Fully convolutional
    networks** (**R-FCN**)  and **Faster Region-based convolutional neural networks**
    (**Faster R-CNN**) are instead the different models to detect multiple objects
    in images. In the next paragraph, we are going to explain something about how
    they effectively work.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**单次多框检测器**（**SSD**）、**基于区域的全卷积网络**（**R-FCN**）和 **更快的基于区域的卷积神经网络**（**Faster
    R-CNN**）是用于检测图像中多个物体的不同模型。在接下来的段落中，我们将解释这些模型如何有效工作。'
- en: Depending on your application, you can decide on the most suitable model for
    you (you have to experiment a bit), or aggregate results from multiple models
    in order to get better results (as done by the researchers at Google in order
    to win the COCO competition).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的应用，你可以选择最适合你的模型（你需要进行一些实验），或者将多个模型的结果汇总，以获得更好的效果（正如 Google 的研究人员在 COCO 竞赛中所做的那样）。
- en: Grasping the basics of R-CNN, R-FCN and  SSD models
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 掌握 R-CNN、R-FCN 和 SSD 模型的基础知识
- en: Even if you have clear in mind how a  CNN can manage to classify an image, it
    could be less obvious for you how a neural network can localize multiple objects
    into an image by defining its bounding box (a rectangular perimeter bounding the
    object itself). The first and easiest solution that you may imagine could be to
    have a sliding window and apply the CNN on each window, but that could be really
    computationally expensive for most real-world applications (if you are powering
    the vision of a self-driving car, you do want it to recognize the obstacle and
    stop before hitting it).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你已经清楚地知道 CNN 如何管理图像分类，理解神经网络如何通过定义物体的边界框（围绕物体的矩形边界）来定位图像中的多个物体，可能仍然不那么显而易见。你可能想象的第一个也是最简单的解决方案是使用滑动窗口，并在每个窗口上应用
    CNN，但对于大多数现实世界的应用来说，这可能非常耗费计算资源（如果你为自动驾驶汽车提供视觉系统，你肯定希望它在撞到障碍物之前就能识别并停下来）。
- en: 'You can find more about the sliding windows approach for object detection in
    this blog post by Adrian Rosebrock: [https://www.pyimagesearch.com/2015/03/23/sliding-windows-for-object-detection-with-python-and-opencv/](https://www.pyimagesearch.com/2015/03/23/sliding-windows-for-object-detection-with-python-and-opencv/)
    that makes an effective example by pairing it with image pyramid.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 Adrian Rosebrock 撰写的这篇博客文章中找到更多关于物体检测的滑动窗口方法：[https://www.pyimagesearch.com/2015/03/23/sliding-windows-for-object-detection-with-python-and-opencv/](https://www.pyimagesearch.com/2015/03/23/sliding-windows-for-object-detection-with-python-and-opencv/)，该文章通过将其与图像金字塔配对，提供了一个有效的示例。
- en: 'Though reasonably intuitive, because of its complexity and being computationally
    cumbersome (exhaustive and working at different image scales), the sliding window
    has quite a few limits, and an alternative preferred solution has immediately
    been found in the *region proposal* algorithms. Such algorithms use image segmentation
    (segmenting, that is dividing the image into areas based on the main color differences
    between areas themselves) in order to create a tentative enumeration of possible
    bounding boxes in an image. You can find a detailed explanation of how the algorithm
    works in this post by Satya Mallik: [https://www.learnopencv.com/selective-search-for-object-detection-cpp-python/](https://www.learnopencv.com/selective-search-for-object-detection-cpp-python/).
    The point is that region proposal algorithms suggest a limited number of boxes
    to be evaluated, a much smaller one than the one proposed by an exhaustive sliding
    windows algorithm. That allowed them to be applied in the first R-CNN, Region-based
    convolutional neural networks, which worked by:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管滑动窗口方法在一定程度上直观，但由于其复杂性和计算上的笨重（需要进行大量计算并处理不同图像尺度），滑动窗口有许多局限性，因此很快就找到了一个优选的解决方案——*区域提议*算法。这些算法使用图像分割（即基于区域之间主要颜色差异将图像划分为多个区域）来创建图像中可能的边界框的初步枚举。你可以在Satya
    Mallik的这篇文章中找到详细的算法工作原理：[https://www.learnopencv.com/selective-search-for-object-detection-cpp-python/](https://www.learnopencv.com/selective-search-for-object-detection-cpp-python/)。关键是，区域提议算法建议一个有限数量的框进行评估，远远少于滑动窗口算法提议的数量。这使得它们能够应用于第一个R-CNN——基于区域的卷积神经网络，工作原理为：
- en: finding a few hundreds or thousands of regions of interest in the image, thanks
    to a region proposal algorithm
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 得益于区域提议算法，找到图像中的几百个或几千个感兴趣区域。
- en: Process by a CNN each region of interest, in order to create features of each
    area
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过CNN处理每个感兴趣区域，以便为每个区域创建特征。
- en: Use the features to classify the region by a support vector machine and a linear
    regression to compute bounding boxes that are more precise.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用特征通过支持向量机分类区域，并通过线性回归计算更精确的边界框。
- en: 'The immediate evolution of R-CNN was Fast R-CNN which made things even speedier
    because:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN的直接演化是Fast R-CNN，它使得处理速度更快，因为：
- en: it processed all the image at once with CNN, transformed it and applied the
    region proposal on the transformation. This cut down the CNN processing from a
    few thousand calls to a single one.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它一次性处理整张图像，通过CNN进行转换，并将区域提议应用于该转换。这将CNN处理的次数从几千次减少到一次。
- en: Instead of using an SVM for classification, it used a soft-max layer and a linear
    classifier, thus simply extending the CNN instead of passing the data to a different
    model.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它不是使用SVM进行分类，而是使用soft-max层和线性分类器，从而简单地扩展了CNN，而不是将数据传递给不同的模型。
- en: In essence, by using a Fast R-CNN we had again a single classification network
    characterized by a special filtering and selecting layer, the region proposal
    layer, based on a non-neural network algorithm. Faster R-CNN even changed that
    layer, by replacing it with a region proposal neural network. That made the model
    even more complicated but most effective and faster than any previous method.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，通过使用Fast R-CNN，我们再次得到了一个单一的分类网络，该网络具有一个基于非神经网络算法的特殊过滤和选择层，即区域提议层。Faster
    R-CNN甚至改变了这个层，将其替换为区域提议神经网络。这使得模型变得更加复杂，但比任何以前的方法更有效且更快速。
- en: 'R-FCN, anyway, are even faster than Faster R-CNN, because they are fully convolutional
    networks, that don’t use any fully connected layer after their convolutional layers.
    They are end-to-end networks: from input by convolutions to output. That simply
    makes them even faster (they have a much lesser number of weights than CNN with
    a fully connect layer at their end). But their speed comes at a price, they have
    not been characterized anymore by image invariance (CNN can figure out the class
    of an object, no matter how the object is rotated). Faster R-CNN supplements this
    weakness by a position-sensitive score map, that is a way to check if parts of
    the original image processed by the FCN correspond to parts of the class to be
    classified. In easy words, they don’t compare to classes, but to part of classes.
    For instance, they don’t classify a dog, but a dog-upper-left part, a dog-lower-right-part
    and so on. This approach allows to figure out if there is a dog in a part of the
    image, no matter how it is orientated. Clearly, this speedier approach comes at
    the cost of less precision, because position-sensitive score maps cannot supplement
    all the original CNN characteristics.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，R-FCN比Faster R-CNN还要快，因为它们是完全卷积网络，在卷积层之后不使用任何全连接层。它们是端到端的网络：从输入到输出，都是通过卷积完成。这使得它们更加快速（它们的权重数量远少于使用全连接层的CNN）。但它们的速度是有代价的，它们不再具备图像不变性（CNN能够识别物体的类别，无论物体如何旋转）。Faster
    R-CNN通过位置敏感的得分图来弥补这一缺陷，这是一种检查FCN处理的原始图像的部分是否与待分类的类别部分对应的方法。简而言之，它们并不直接对比类别，而是对比类别的部分。例如，它们不会识别一只狗，而是识别狗的左上部分、右下部分，依此类推。这种方法能够帮助识别图像的某部分是否包含狗，不管它的方向如何。显然，这种更快速的方法以牺牲精度为代价，因为位置敏感得分图无法完全补充CNN的所有特征。
- en: 'Finally, we have SSD (Single Shot Detector). Here the speed is even greater
    because the network simultaneously predicts the bounding box location and its
    class as it processes the image. SSD computes a large number of bounding boxes,
    by simply skipping the region proposal phase. It just reduces highly-overlapping
    boxes, but still, it processes the largest number of bounding boxes compared to
    all the model we mentioned up-so-far. Its speed is because as it delimits each
    bounding box it also classifies it: by doing everything in one shot, it has the
    fastest speed, though performs in a quite comparable way.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来谈谈SSD（单次检测器）。这里的速度更快，因为网络在处理图像的同时同时预测边界框的位置及其类别。SSD通过直接跳过区域提议阶段来计算大量的边界框。它只是减少了高度重叠的框，但相比之前提到的所有模型，它处理的边界框数量最大。它的速度是因为每次限定一个边界框时，它也会对其进行分类：通过一举完成所有任务，它拥有最快的速度，尽管其表现与其他模型相当。
- en: 'Another short article by Joice Xu can provide you with more details on the
    detection models we discussed up so far: [https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9](https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Joice Xu的另一篇简短文章可以为你提供更多我们之前讨论的检测模型的细节：[https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9](https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9)
- en: Summing up all the discussion, in order to choose the network you have to consider
    that you are combining different CNN architectures in classification power and
    network complexity and different detection models. It is their combined effect
    to determinate the capability of the network to spot objects, to correctly classify
    them, and to do all that in a timely fashion.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 总结所有讨论内容，在选择网络时，你需要考虑的是，你正在将不同的CNN架构与分类能力和网络复杂性结合在一起，并与不同的检测模型结合。正是它们的综合效应决定了网络在识别物体、正确分类物体以及及时完成这些任务的能力。
- en: 'If you desire to have more reference in regard to the speed and precision of
    the models we have briefly explained, you can consult: *Speed/accuracy trade-offs
    for modern convolutional object detectors*. Huang J, Rathod V, Sun C, Zhu M, Korattikara
    A, Fathi A, Fischer I, Wojna Z, Song Y, Guadarrama S, Murphy K, CVPR 2017: [http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_SpeedAccuracy_Trade-Offs_for_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_SpeedAccuracy_Trade-Offs_for_CVPR_2017_paper.pdf) Yet,
    we cannot but advise to just test them in practice for your application, evaluating
    is they are good enough for the task and if they execute in a reasonable time.
    Then it is just a matter of a trade-off you have to best decide for your application.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望获得更多关于我们简要说明的模型的速度和精度参考，可以参考：*现代卷积物体检测器的速度/精度权衡*。黄俊，Rathod V，孙成，朱敏，Korattikara
    A，Fathi A，Fischer I，Wojna Z，宋阳，Guadarrama S，Murphy K，CVPR 2017：[http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_SpeedAccuracy_Trade-Offs_for_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_SpeedAccuracy_Trade-Offs_for_CVPR_2017_paper.pdf)
    但我们还是建议您在实践中测试这些模型，以评估它们是否足够适合您的任务，并且是否能在合理的时间内执行。然后，您只需根据您的应用做出最佳的速度/精度权衡。
- en: Presenting our project plan
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 展示我们的项目计划
- en: 'Given such a powerful tool made available by TensorFlow, our plan is to leverage
    its API by creating a class you can use for annotating images both visually and
    in an external file. By annotating, we mean the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TensorFlow提供了如此强大的工具，我们的计划是通过创建一个类来利用它的API，您可以使用该类对图像进行视觉和外部文件标注。这里所说的标注包括以下内容：
- en: Pointing out the objects in an image (as recognized by a model trained on MS
    COCO)
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指出图像中的物体（由在MS COCO上训练的模型识别）
- en: Reporting the level of confidence in the object recognition (we will consider
    only objects above a minimum probability threshold, which is set to 0.25, based
    on the *speed/accuracy trade-offs for modern convolutional object detector*s discussed
    in the paper previously mentioned)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 报告物体识别的置信度（我们只考虑置信度超过最低概率阈值的物体，该阈值设为0.25，基于之前提到的*现代卷积物体检测器的速度/精度权衡*论文中的讨论）
- en: Outputting the coordinates of two opposite vertices of the bounding box for
    each image
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出每张图像的边界框两个对角顶点的坐标
- en: Saving all such information in a text file in JSON format
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有这些信息以JSON格式保存在文本文件中
- en: Visually representing the bounding box on the original image, if required
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如有需要，在原始图像上可视化表示边界框
- en: 'In order to achieve such objectives, we need to:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这些目标，我们需要：
- en: Download one of the pre-trained models (available in `.pb` format - [protobuf](https://developers.google.com/protocol-buffers/))
    and make it available in-memory as a TensorFlow session.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载一个预训练模型（以`.pb`格式提供 - [protobuf](https://developers.google.com/protocol-buffers/)），并将其作为TensorFlow会话加载到内存中。
- en: Reformulate the helper code provided by TensorFlow in order to make it easier
    to load labels, categories, and visualization tools by a class that can be easily
    imported into your scripts.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新构建TensorFlow提供的辅助代码，使得通过一个可以轻松导入脚本的类，加载标签、类别和可视化工具变得更加容易。
- en: Prepare a simple script to demonstrate its usage with single images, videos,
    and videos captured from a webcam.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备一个简单的脚本，演示如何使用单张图像、视频和从网络摄像头捕获的视频。
- en: We start by setting up an environment suitable for the project.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过设置适合项目的环境来开始。
- en: Setting up an environment suitable for the project
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为项目设置适当的环境
- en: 'You don''t need any specialized environment in order to run the project, though
    we warmly suggest installing Anaconda `conda` and creating a separated environment
    for the project. The instructions to run if `conda` is available on your system
    are as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 您不需要任何专业环境来运行该项目，尽管我们强烈建议安装Anaconda `conda`并为项目创建一个独立环境。如果您的系统中已有`conda`，则按照以下说明运行：
- en: '[PRE1]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After activating the environment, you can install some other packages that
    require a `pip install` command or a `conda install` command pointing to another
    repository (`menpo`, `conda-forge`):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 启动环境后，您可以安装其他一些包，使用`pip install`命令或`conda install`命令指向其他仓库（如`menpo`，`conda-forge`）：
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In case you prefer another way of running this project, just consider that you
    need `numpy`, `pillow`, `TensorFlow`, `opencv`, `imageio`, `tqdm`, and `moviepy`
    in order to run it successfully.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您倾向于另一种运行该项目的方式，请确保您已安装`numpy`，`pillow`，`TensorFlow`，`opencv`，`imageio`，`tqdm`和`moviepy`，以便顺利运行。
- en: For everything to run smoothly, you also need to create a directory for your
    project and to save in it the `object_detection` directory of the TensorFlow object
    detection API project ([https://github.com/tensorflow/models/tree/master/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection)).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保一切顺利运行，你还需要为你的项目创建一个目录，并将 TensorFlow 目标检测 API 项目的`object_detection`目录保存到其中（[https://github.com/tensorflow/models/tree/master/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection)）。
- en: 'You can simply obtain that by using the `git` command on the entire TensorFlow
    models'' project and selectively pulling only that directory. This is possible
    if your Git version is 1.7.0 (February 2012) or above:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在整个 TensorFlow 模型项目中使用`git`命令并选择性地拉取该目录来轻松获取。这在你的 Git 版本为 1.7.0（2012年2月）或以上时是可行的：
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'These commands will fetch all the objects in the TensorFlow models project,
    but it won''t check them out. By following those previous commands by:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令将获取 TensorFlow 模型项目中的所有对象，但它不会检出它们。通过执行这些前面的命令后：
- en: '[PRE4]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You will now have only the `object_detection` directory and its contents as
    *checked out* on your filesystem and no other directories or files present.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你只会在文件系统中看到`object_detection`目录及其内容，并且没有其他目录或文件。
- en: Just keep in mind that the project will need to access the `object_detection`
    directory, thus you will have to keep the project script in the very same directory
    of `object_detection` directory. In order to use the script outside of its directory,
    you will need to access it using a full path.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 只需记住，该项目需要访问`object_detection`目录，因此你必须将项目脚本保存在与`object_detection`目录相同的目录中。如果要在该目录外使用脚本，你需要使用完整路径访问它。
- en: Protobuf compilation
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Protobuf 编译
- en: The TensorFlow object detection API uses *protobufs*, protocol buffers -- Google's
    data interchange format ([https://github.com/google/protobuf](https://github.com/google/protobuf)),
    to configure the models and their training parameters. Before the framework can
    be used, the protobuf libraries must be compiled, and that requires different
    steps if you are in a Unix (Linux or Mac) or Windows OS environment.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 目标检测 API 使用 *protobufs*（协议缓冲区）——Google 的数据交换格式（[https://github.com/google/protobuf](https://github.com/google/protobuf)），用于配置模型及其训练参数。在框架使用之前，必须编译
    protobuf 库，如果你使用的是 Unix（Linux 或 Mac）或 Windows 操作系统环境，步骤会有所不同。
- en: Windows installation
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Windows 安装
- en: First, unpack the [protoc-3.2.0-win32.zip](https://github.com/google/protobuf/releases/download/v3.2.0/protoc-3.2.0-win32.zip)
    that can be found at [https://github.com/google/protobuf/releases](https://github.com/google/protobuf/releases)
    into the project folder. Now you should have a new `protoc-3.4.0-win32` directory,
    containing a `readme.txt` and two directories, `bin`, and `include`. The folders
    contain a precompiled binary version of the protocol buffer compiler (*protoc*).
    All you have to do is add the `protoc-3.4.0-win32` directory to the system path.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，解压[protoc-3.2.0-win32.zip](https://github.com/google/protobuf/releases/download/v3.2.0/protoc-3.2.0-win32.zip)，它可以在[https://github.com/google/protobuf/releases](https://github.com/google/protobuf/releases)找到，将其解压到项目文件夹中。现在你应该会看到一个新的`protoc-3.4.0-win32`目录，其中包含一个`readme.txt`文件和两个目录：`bin`和`include`。这些文件夹包含了协议缓冲区编译器（*protoc*）的预编译二进制版本。你需要做的就是将`protoc-3.4.0-win32`目录添加到系统路径中。
- en: 'After adding it to the system path, you can execute the following command:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将其添加到系统路径后，你可以执行以下命令：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: That should be enough to allow the TensorFlow object detection API to work on
    your computer.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该足够让 TensorFlow 目标检测 API 在你的计算机上运行。
- en: Unix installation
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Unix 安装
- en: For Unix environments, the installation procedure can be done using shell commands,
    just follow the instructions available at [https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Unix 环境，安装过程可以通过 shell 命令完成，只需按照[https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md)上的说明进行操作。
- en: Provisioning of the project code
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目代码的配置
- en: 'We start scripting our project in the file `tensorflow_detection.py` by loading
    the necessary packages:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始在文件`tensorflow_detection.py`中编写脚本，通过加载必要的包：
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In order to be able to process videos, apart from OpenCV 3, we also need the
    `moviepy` package. The package `moviepy` is a project that can be found at [http://zulko.github.io/moviepy/](http://zulko.github.io/moviepy/)
    and freely used since it is distributed with an MIT license. As described on its
    home page, `moviepy` is a tool for video editing (that is cuts, concatenations,
    title insertions), video compositing (non-linear editing), video processing, or
    to create advanced effects.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够处理视频，除了OpenCV 3，我们还需要`moviepy`包。`moviepy`包是一个可以在[http://zulko.github.io/moviepy/](http://zulko.github.io/moviepy/)找到并自由使用的项目，因为它是以MIT许可证分发的。正如其主页所描述的，`moviepy`是一个用于视频编辑（如剪辑、拼接、标题插入）、视频合成（非线性编辑）、视频处理或创建高级效果的工具。
- en: 'The package operates with the most common video formats, including the GIF
    format. It needs the `FFmpeg` converter ([https://www.ffmpeg.org/](https://www.ffmpeg.org/))
    in order to properly operate, therefore at its first usage it will fail to start
    and will download `FFmpeg` as a plugin using `imageio`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 该包支持最常见的视频格式，包括GIF格式。为了正确操作，它需要`FFmpeg`转换器（[https://www.ffmpeg.org/](https://www.ffmpeg.org/)），因此在首次使用时，它将无法启动并会通过`imageio`插件下载`FFmpeg`：
- en: '[PRE7]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, we require two useful functions available in the `object_detection`
    directory from the TensorFlow API project:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要TensorFlow API项目中`object_detection`目录下的两个有用函数：
- en: '[PRE8]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We define the `DetectionObj` class and its `init` procedure. The initialization
    expects only a parameter and the model name (which is initially set to the less
    well performing, but faster and more lightweight model, the SSD MobileNet), but
    a few internal parameters can be changed to suit your use of the class:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了`DetectionObj`类及其`init`过程。初始化只需要一个参数和模型名称（默认设置为性能较差，但速度较快且更轻量级的模型，SSD MobileNet），但一些内部参数可以根据你的使用情况进行更改：
- en: '`self.TARGET_PATH` pointing out the directory where you want the processed
    annotations to be saved.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.TARGET_PATH`指向你希望保存处理后注释的目录。'
- en: '`self.THRESHOLD` fixing the probability threshold to be noticed by the annotation
    process. In fact, any model of the suit will output many low probability detections
    in every image. Objects with too low probabilities are usually false alarms, for
    such reasons you fix a threshold and ignore such highly unlikely detection. As
    a rule of thumb, 0.25 is a good threshold in order to spot uncertain objects due
    to almost total occlusion or visual clutter.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.THRESHOLD`设置了注释过程要注意的概率阈值。事实上，套件中的任何模型都会在每张图片中输出许多低概率的检测。概率太低的对象通常是误报，因此你需要设置阈值，忽略那些极不可能的检测。根据经验，0.25是一个不错的阈值，用于识别由于几乎完全遮挡或视觉杂乱而不确定的对象。'
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As a convenient variable to have access to, you have the `self.LABELS` containing
    a dictionary relating a class numerical code to its textual representation. Moreover,
    the `init` procedure will have the `TensorFlow` session loaded, open, and ready
    to be used at `self.TF_SESSION`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个方便的变量，你可以访问`self.LABELS`，它包含一个将类别数字代码与文本表示相对应的字典。此外，`init`过程将加载、打开并准备好使用`TensorFlow`会话，位于`self.TF_SESSION`。
- en: 'The functions `load_frozen_model` and `download_frozen_model` will help the
    `init` procedure to load the chosen frozen model from disk and, if not available,
    will help to download it as a TAR file from the internet and unzip it in the proper
    directory (which is `object_detection`):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`load_frozen_model`和`download_frozen_model`函数将帮助`init`过程从磁盘加载所选的冻结模型，如果模型不可用，它们将帮助从互联网下载该模型作为TAR文件，并将其解压到正确的目录（即`object_detection`）：'
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The function `download_frozen_model` leverages the `tqdm` package in order
    to visualize its progress as it downloads the new models from the internet. Some
    models are quite large (over 600 MB) and it may take a long time. Providing visual
    feedback on the progress and estimated time of completion will allow the user
    to be more confident about the progression of the operations:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`download_frozen_model`函数利用`tqdm`包来可视化其下载新模型时的进度。某些模型相当大（超过600MB），可能需要很长时间。提供进度和预计完成时间的可视化反馈将使用户对操作的进展更加有信心：'
- en: '[PRE11]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following two functions, `load_image_from_disk` and `load_image_into_numpy_array`,
    are necessary in order to pick an image from disk and transform it into a Numpy
    array suitable for being processed by any of the TensorFlow models available in
    this project:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两个函数，`load_image_from_disk`和`load_image_into_numpy_array`，是从磁盘中选择图像并将其转换为适合本项目中任何TensorFlow模型处理的Numpy数组所必需的：
- en: '[PRE12]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `detect` function, instead, is the core of the classification functionality
    of the class. The function just expects lists of images to be processed. A Boolean
    flag, `annotate_on_image`, just tells the script to visualize the bounding box
    and the annotation directly on the provided images.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`detect`函数则是该类分类功能的核心。该函数仅期望处理图像列表。一个布尔标志`annotate_on_image`仅告诉脚本在提供的图像上直接可视化边界框和注释。'
- en: 'Such a function is able to process images of different sizes, one after the
    other, but it necessitates processing each one singularly. Therefore, it takes
    each image and expands the dimension of the array, adding a further dimension. This
    is necessary because the model expects an array of size: number of images * height
    * width * depth.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的函数能够依次处理不同尺寸的图像，但需要逐个图像进行处理。因此，它会将每个图像扩展数组的维度，增加一个额外的维度。这是必要的，因为模型期望数组的尺寸为：图像数量
    * 高度 * 宽度 * 深度。
- en: Note, we could pack all the batch images to be predicted into a single matrix.
    That would work fine, and it would be faster if all the images were of the same
    height and width, which is an assumption that our project does not make, hence
    the single image processing.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们可以将所有批次的图像打包成一个矩阵进行预测。这是可行的，并且如果所有图像的高度和宽度相同，它会更快，但我们的项目并未做出这一假设，因此采用了逐个图像处理的方法。
- en: We then take a few tensors in the model by name (`detection_boxes`, `detection_scores`,
    `detection_classes`, `num_detections`), which are exactly the outputs we expect
    from the model, and we feed everything to the input tensor, `image_tensor`, which
    will normalize the image in a suitable form for the layers of the model to process.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过名称从模型中提取几个张量（`detection_boxes`、`detection_scores`、`detection_classes`、`num_detections`），这些正是我们期望从模型中得到的输出，我们将所有内容输入到`image_tensor`张量中，它会将图像标准化成适合模型层处理的形式。
- en: 'The results are gathered into a list and the images are processed with the
    detection boxes and represented if required:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 结果被汇总成一个列表，图像经过检测框处理并在需要时呈现：
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The function `detection_on_image` just processes the results from the `detect`
    function and returns a new image enriched by bounding boxes which will be represented
    on screen by the function `visualize_image` (You can adjust the latency parameter,
    which corresponds to the seconds the image will stay on screen before the script
    passes to process another image).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`detection_on_image`函数仅处理来自`detect`函数的结果，并返回一张新图像，这张图像通过边界框进行丰富，边界框将在屏幕上通过`visualize_image`函数呈现（你可以调整延迟参数，该参数定义了图像在屏幕上停留的秒数，脚本会在此后处理下一个图像）。'
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The function `visualize_image` offers a few parameters that could be modified
    in order to suit your needs in this project. First of all, `image_size` provides
    the desired size of the image to be represented on screen. Larger or shorter images
    are therefore modified in order to partially resemble this prescribed size. The
    `latency` parameter, instead, will define the time in seconds that each image
    will be represented on the screen, thus locking the object detection procedure,
    before moving to the next one. Finally, the `bluish_correction` is just a correction
    to be applied when images are offered in the **BGR** format (in this format the
    color channels are arranged in the order: **blue-green-red** and it is the standard
    for the OpenCV library: [https://stackoverflow.com/questions/14556545/why-opencv-using-bgr-colour-space-instead-of-rgb](https://stackoverflow.com/questions/14556545/why-opencv-using-bgr-colour-space-instead-of-rgb))
    , instead of the **RGB** (**red-green-blue**), which is the image format the model
    is expecting:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`visualize_image`函数提供了一些可以修改的参数，以满足你在本项目中的需求。首先，`image_size`提供了图像在屏幕上显示的期望大小。因此，较大或较小的图像会被修改，以部分符合这个规定的尺寸。`latency`参数则定义了每张图像在屏幕上显示的时间（秒），即在转向处理下一张图像之前，锁定目标检测过程。最后，`bluish_correction`是当图像以**BGR**格式提供时应用的修正（在这种格式中，颜色通道按：**蓝色-绿色-红色**的顺序排列，这是OpenCV库的标准：[https://stackoverflow.com/questions/14556545/why-opencv-using-bgr-colour-space-instead-of-rgb](https://stackoverflow.com/questions/14556545/why-opencv-using-bgr-colour-space-instead-of-rgb)），而不是模型预期的**RGB**（**红色-绿色-蓝色**）图像格式。'
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Annotations are prepared and written to disk by the `serialize_annotations`
    function, which will create single JSON files containing, for each image, the
    data regarding the detected classes, the vertices of the bounding boxes, and the
    detection confidence. For instance, this is the result from a detection on a dog''s
    photo:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注释由`serialize_annotations`函数准备并写入磁盘，该函数会为每个图像创建单独的 JSON 文件，包含关于检测到的类别、边界框的顶点以及检测置信度的数据。例如，这是在狗的照片上进行检测的结果：
- en: '[PRE16]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The JSON points out the detected class, a single dog, the level of confidence
    (about 0.91 confidence), and the vertices of the bounding box, and expresses as
    percentages the height and width of the image (they are therefore relative, not
    absolute pixel points):'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: JSON文件指出检测到的类别（单一的狗）、置信度（大约0.91的置信度），以及边界框的顶点，并以百分比表示图像的高度和宽度（因此它们是相对的，而不是绝对像素点）：
- en: '[PRE17]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The function `get_time` conveniently transforms the actual time into a string
    that can be used in a filename:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_time`函数方便地将当前时间转换为可以用于文件名的字符串：'
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we prepare three detection pipelines, for images, videos, and webcam.
    The pipeline for images loads each image into a list. The pipeline for videos
    lets the `VideoFileClip` module from `moviepy` do all the heavy lifting after
    simply passing the `detect` function appropriately wrapped in the `annotate_photogram`
    function. Finally, the pipeline for webcam capture relies on a simple `capture_webcam`
    function that, based on OpenCV''s VideoCapture, records a number of snapshots
    from the webcam returning just the last (the operation takes into account the
    time necessary for the webcam before adjusting to the light levels of the environment):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备了三条检测管道，分别用于图像、视频和网络摄像头。图像的管道将每个图像加载到一个列表中。视频的管道允许`moviepy`的`VideoFileClip`模块进行所有繁重的处理，只需将`detect`函数适当地包装在`annotate_photogram`函数中。最后，网络摄像头捕获的管道依赖于一个简单的`capture_webcam`函数，它基于
    OpenCV 的 VideoCapture，从网络摄像头记录多个快照，只返回最后一张（该操作考虑了网络摄像头在适应环境光照水平前所需的时间）：
- en: '[PRE19]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `capture_webcam` function will acquire an image from your webcam using
    the `cv2.VideoCapture` functionality ([http://docs.opencv.org/3.0-beta/modules/videoio/doc/reading_and_writing_video.html](http://docs.opencv.org/3.0-beta/modules/videoio/doc/reading_and_writing_video.html))
    . As webcams have first to adjusts to the light conditions present in the environment
    where the picture is taken, the procedure discards a number of initial shots,
    before taking the shot that will be used in the object detection procedure. In
    this way, the webcam has all the time to adjust its light settings, :'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`capture_webcam`函数将使用`cv2.VideoCapture`功能从你的网络摄像头获取图像（[http://docs.opencv.org/3.0-beta/modules/videoio/doc/reading_and_writing_video.html](http://docs.opencv.org/3.0-beta/modules/videoio/doc/reading_and_writing_video.html)）。由于网络摄像头首先需要调整光照条件以适应拍照环境，因此该过程会丢弃一些初始快照，之后再拍摄用于目标检测的图像。通过这种方式，网络摄像头有足够时间调整光照设置：'
- en: '[PRE20]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `file_pipeline` comprises all the steps necessary to load images from storage
    and visualize/annotate them:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`file_pipeline`包括加载图像并可视化/注释它们所需的所有步骤：'
- en: Loading images from disk.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从磁盘加载图像。
- en: Applying object detection on the loaded images.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对加载的图像应用目标检测。
- en: Writing the annotations for each image in a JSON file.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个图像的注释写入一个 JSON 文件。
- en: 'If required by the Boolean parameter `visualize`, represent the images with
    its bounding boxes on the computer''s screen:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果布尔参数`visualize`需要，代表带有边界框的图像并显示在计算机屏幕上：
- en: '[PRE21]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `video_pipeline` simply arranges all the steps necessary to annotate a
    video with bounding boxes and, after completing the operation, saves it to disk:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`video_pipeline`简单地安排了注释视频所需的所有步骤，并在完成操作后将其保存到磁盘：'
- en: '[PRE22]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `webcam_pipeline` is the function that arranges all the steps when you
    want to annotate an image acquired from your webcam:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`webcam_pipeline`是当你想要注释来自网络摄像头的图像时，安排所有步骤的函数：'
- en: Captures an image from the webcam.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从网络摄像头捕获图像。
- en: 'Saves the captured image to disk (using `cv2.imwrite` which has the advantage
    of writing different image formats based on the target filename, see at: [http://docs.opencv.org/3.0-beta/modules/imgcodecs/doc/reading_and_writing_images.html](http://docs.opencv.org/3.0-beta/modules/imgcodecs/doc/reading_and_writing_images.html)'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将捕获的图像保存到磁盘（使用`cv2.imwrite`，其优势是根据目标文件名写入不同的图像格式，详见：[http://docs.opencv.org/3.0-beta/modules/imgcodecs/doc/reading_and_writing_images.html](http://docs.opencv.org/3.0-beta/modules/imgcodecs/doc/reading_and_writing_images.html)）
- en: Applies object detection on the image.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对图像应用目标检测。
- en: Saves the annotation JSON file.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存注释的JSON文件。
- en: 'Represents visually the image with bounding boxes:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直观表示带有边界框的图像：
- en: '[PRE23]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Some simple applications
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些简单的应用
- en: 'As a concluding paragraph of the code provisioning, we demonstrate just three
    simple scripts leveraging the three different sources used by our project: files,
    videos, webcam.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 作为代码提供部分的结尾，我们展示了三个简单的脚本，利用了我们项目中的三种不同数据来源：文件、视频和摄像头。
- en: Our first testing script aims at annotating and visualizing three images after
    importing the class `DetectionObj` from the local directory (In cases where you
    operate from another directory, the import won't work unless you add the project
    directory to the Python path).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个测试脚本旨在注释和可视化三张图像，前提是从本地目录导入类`DetectionObj`（如果你在其他目录下操作，除非你将项目目录添加到Python路径中，否则导入将无法工作）。
- en: 'In order to add a directory to the Python path in your script, you just have
    to put `sys.path.insert` command before the part of the script that needs access
    to that directory:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在脚本中将目录添加到Python路径中，你只需在需要访问该目录的脚本部分之前放置`sys.path.insert`命令：
- en: '`import sys`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`import sys`'
- en: '`sys.path.insert(0,''/path/to/directory'')`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`sys.path.insert(0,''/path/to/directory'')`'
- en: 'Then we activate the class, declaring it using the SSD MobileNet v1 model.
    After that, we have to put the path to every single image into a list and feed
    it to the method `file_pipeline`:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们激活该类，使用SSD MobileNet v1模型进行声明。接着，我们需要将每个图像的路径放入一个列表中，并传递给`file_pipeline`方法：
- en: '[PRE24]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output that we receive after our detection class has been placed on the
    intersection image and will return us another image enriched with bounding boxes
    around objects recognized with enough confidence:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在检测类应用到交叉口图像后收到的输出，将返回另一个图像，图像中包含了足够置信度的边界框，框住了识别出的物体：
- en: '![](img/1ec56e41-57e4-43c2-b6ac-b96c25af2b86.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1ec56e41-57e4-43c2-b6ac-b96c25af2b86.png)'
- en: Object detection by SSD MobileNet v1 on a photo of an intersection
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉口照片上进行SSD MobileNet v1物体检测
- en: After running the script, all three images will be represented with their annotations
    on the screen (each one for three seconds) and a new JSON file will be written
    on disk (in the target directory, which corresponds to the local directory if
    you have not otherwise stated it by modifying the class variable `TARGET_CLASS`).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本后，所有三张图像将带有注释在屏幕上显示（每张显示三秒钟），并且一个新的JSON文件将被写入磁盘（写入目标目录，如果你没有通过修改类变量`TARGET_CLASS`来特别指定它，该目录将默认为本地目录）。
- en: In the visualization, you will see all the bounding boxes relative to objects
    whose prediction confidence is above 0.5\. Anyway, you will notice that, in this
    case of an annotated image of an intersection (depicted in the preceding figure),
    not all cars and pedestrians have been spotted by the model.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在可视化中，你将看到所有与物体相关的边界框，前提是预测置信度高于0.5。无论如何，你会注意到，在这种交叉口图像的注释案例中（如前图所示），并不是所有的汽车和行人都被模型识别到。
- en: By looking at the JSON file, you will discover that many other cars and pedestrians
    have been located by the model, though with lesser confidence. In the file, you
    will find all the objects detected with at least 0.25 confidence, a threshold
    which represents a common standard in many studies on object detection (but you
    can change it by modifying the class variable `THRESHOLD`).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 查看JSON文件后，你会发现模型还定位了许多其他汽车和行人，尽管置信度较低。在该文件中，你会找到所有置信度至少为0.25的物体，这一阈值在许多物体检测研究中是常见的标准（但你可以通过修改类变量`THRESHOLD`来更改它）。
- en: 'Here you can see the scores generated in the JSON file. Only eight detected
    objects are above the visualization threshold of 0.5, whereas 16 other objects
    have lesser scores:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到JSON文件中生成的分数。只有八个检测到的物体的分数高于可视化阈值0.5，而其他16个物体的分数较低：
- en: '[PRE25]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'And here you can find the relative class of the detected objects. Many cars
    have been spotted with lesser confidence. They actually may be cars in the image
    or errors. In accordance with your application of the Detection API, you may want
    to adjust your threshold or use another model and estimate an object only if it
    has been repeatedly detected by different models above a threshold:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里你可以找到被检测物体的相关类别。许多汽车被以较低的置信度检测到。它们可能确实是图像中的汽车，也可能是错误的识别。根据你对检测API的应用，你可能想要调整阈值，或者使用另一种模型，只有当物体被不同模型重复检测，并且置信度超过阈值时，才估计它是一个有效的物体：
- en: '[PRE26]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Applying detection to videos uses the same scripting approach. This time you
    just point to the appropriate method, `video_pipeline`, the path to the video,
    and set whether the resulting video should have audio or not (by default audio
    will be filtered out). The script will do everything by itself, saving, on the
    same directory path as the original video, a modified and annotated video (you
    can spot it because it has the same filename but with the addition of `annotated_`
    before it):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 将检测应用于视频使用相同的脚本方法。这一次，您只需指向适当的方法`video_pipeline`，提供视频路径，并设置结果视频是否应包含音频（默认情况下音频会被过滤掉）。脚本将自动完成所有操作，在与原始视频相同的目录路径下保存一个经过修改和注释的视频（您可以通过文件名看到它，因为它与原始视频文件名相同，只是在前面加上`annotated_`）：
- en: '[PRE27]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, you can also leverage the exact same approach for images acquired
    by a webcam. This time you will be using the method `webcam_pipeline`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您还可以采用完全相同的方法来处理摄像头获取的图像。这次您将使用方法`webcam_pipeline`：
- en: '[PRE28]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The script will activate the webcam, adjust the light, pick a snapshot, save
    the resulting snapshot and its annotation JSON file in the current directory,
    and finally represent the snapshot on your screen with bounding boxes on detected
    objects.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本将激活摄像头，调整光线，拍摄快照，将生成的快照及其注释 JSON 文件保存在当前目录，并最终在您的屏幕上展示带有检测物体边界框的快照。
- en: Real-time webcam detection
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时摄像头检测
- en: The previous `webcam_pipeline` is not a real-time detection system because it
    just takes snapshots and applies detection to the single taken image. This is
    a necessary limitation because dealing with webcam streaming requires intensive
    I/O data exchange. In particular, the problem is the queue of images arriving
    from the webcam to the Python interpreter that locks down Python until the transfer
    is completed. Adrian Rosebrock on his website pyimagesearch proposes a simple
    solution based on threads that you can read about at this Web address: [http://www.pyimagesearch.com/2015/12/21/increasing-webcam-fps-with-python-and-opencv/](http://www.pyimagesearch.com/2015/12/21/increasing-webcam-fps-with-python-and-opencv/).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的`webcam_pipeline`并不是一个实时检测系统，因为它只拍摄快照并对单个图像进行检测。这是一个必要的限制，因为处理摄像头流需要大量的 I/O
    数据交换。特别地，问题出在来自摄像头的图像队列，它会锁住 Python，直到传输完成。Adrian Rosebrock 在他的网站 pyimagesearch
    上提出了一种基于线程的简单解决方案，您可以在此网址查看：[http://www.pyimagesearch.com/2015/12/21/increasing-webcam-fps-with-python-and-opencv/](http://www.pyimagesearch.com/2015/12/21/increasing-webcam-fps-with-python-and-opencv/)。
- en: The idea is very simple. In Python,  because of the **global interpreter lock**
    (**GIL**), only one thread can execute at a time. If there is some I/O operation
    that blocks the thread (such as downloading a file or getting an image from the
    webcam), all the remaining commands are just delayed for it to complete causing
    a very slow execution of the program itself. It is then a good solution to move
    the blocking I/O operation to another thread. Since threads share the same memory,
    the program thread can proceed with its instructions and inquiry from time to
    time the I/O thread in order to check if it has completed its operations.  Therefore,
    if moving images from the webcam to the memory of the program is a blocking operation,
    letting another thread dealing with I/O could be the solution. The main program
    will just inquiry the I/O thread, pick the image from a buffer containing only
    the latest received image and plot it on the screen.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法非常简单。在 Python 中，由于**全局解释器锁**（**GIL**），一次只能执行一个线程。如果某个 I/O 操作阻塞了线程（例如下载文件或从摄像头获取图像），那么所有剩余的命令都会被延迟，直到该操作完成，从而导致程序执行非常缓慢。此时，将阻塞的
    I/O 操作移到另一个线程是一个好的解决方案。由于线程共享相同的内存，程序线程可以继续执行指令，并不时地向 I/O 线程询问是否完成了操作。因此，如果将图像从摄像头移动到程序内存是一个阻塞操作，让另一个线程处理
    I/O 操作可能就是解决方案。主程序将只是查询 I/O 线程，从缓冲区中提取最新接收到的图像并在屏幕上绘制。
- en: '[PRE29]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The above code implements this solution using a `webcamStream` class that instantiates
    a thread for the webcam I/O, allowing the main Python program to always have at
    hand the latest received image, processed by the TensorFlow API (using `ssd_mobilenet_v1_coco_11_06_2017`).
    The processed image is fluidly plotted on the screen using an `OpenCV` function,
    listening to the space bar keystroke in order to terminate the program.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码使用了一个`webcamStream`类来实现这个解决方案，该类实例化一个用于网络摄像头 I/O 的线程，使主 Python 程序始终拥有最新接收到的图像，并通过
    TensorFlow API（使用`ssd_mobilenet_v1_coco_11_06_2017`）对其进行处理。处理后的图像通过 OpenCV 函数流畅地绘制在屏幕上，监听空格键以终止程序。
- en: Acknowledgements
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: 'Everything related to this project started from the following paper: *Speed/accuracy
    trade-offs for modern convolutional object detectors(*[https://arxiv.org/abs/1611.10012](https://arxiv.org/abs/1611.10012)*)*
    by Huang J, Rathod V, Sun C, Zhu M, Korattikara A, Fathi A, Fischer I, Wojna Z,
    Song Y, Guadarrama S, Murphy K, CVPR 2017\. Concluding this chapter, we have to
    thank all the contributors of the TensorFlow object detection API for their great
    job programming the API and making it open-source and thus free and accessible
    to anyone: Jonathan Huang, Vivek Rathod, Derek Chow, Chen Sun, Menglong Zhu, Matthew
    Tang, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song,
    Sergio Guadarrama, Jasper Uijlings, Viacheslav Kovalevskyi, Kevin Murphy. We also
    cannot forget to thank Dat Tran for his inspirational posts on medium of two MIT
    licensed projects on how to use the TensorFlow object detection API for real-time recognition
    even on custom ([https://towardsdatascience.com/building-a-real-time-object-recognition-app-with-tensorflow-and-opencv-b7a2b4ebdc32](https://towardsdatascience.com/building-a-real-time-object-recognition-app-with-tensorflow-and-opencv-b7a2b4ebdc32)
    and [https://towardsdatascience.com/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9](https://towardsdatascience.com/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9))'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 与该项目相关的一切都始于以下论文：*Speed/accuracy trade-offs for modern convolutional object
    detectors*（[https://arxiv.org/abs/1611.10012](https://arxiv.org/abs/1611.10012)）由黄健、拉索德维克、孙晨、朱梦龙、科拉提卡拉阿努普、法蒂阿利瑞扎、费舍尔伊恩、沃纳兹比格涅夫、宋阳、瓜达拉玛谢尔吉奥、墨菲凯文、CVPR
    2017\. 结束这一章，我们必须感谢所有贡献者 TensorFlow 目标检测 API 的伟大工作，他们编程了这个 API 并使其开源，因此任何人都可以免费访问：乔纳森·黄、维韦克·拉索德、德里克·周、陈孙、孟龙·朱、马修·唐、阿努普·科拉提卡拉、阿利瑞扎·法蒂、伊恩·费舍尔、兹比格涅夫·沃纳、杨嵩、塞尔吉奥·瓜达拉玛、贾斯珀·乌吉林斯、维亚切斯拉夫·科瓦列夫斯基、凯文·墨菲。我们也不能忘记感谢
    Dat Tran，在他的 Medium 上发布了两个 MIT 许可证项目的启发性文章，介绍如何实时使用 TensorFlow 目标检测 API 进行识别，甚至是在自定义情况下（[https://towardsdatascience.com/building-a-real-time-object-recognition-app-with-tensorflow-and-opencv-b7a2b4ebdc32](https://towardsdatascience.com/building-a-real-time-object-recognition-app-with-tensorflow-and-opencv-b7a2b4ebdc32)
    和 [https://towardsdatascience.com/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9](https://towardsdatascience.com/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9)）
- en: Summary
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This project has helped you to start immediately classifying objects in images
    with confidence without much hassle.  It helps you to see what a ConvNet could
    do for your problem, focusing more on the wrap up (possibly a larger application)
    you have in mind,and annotating many images for training more ConvNets with fresh
    images of a selected class.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目帮助您立即开始自信地对图像中的对象进行分类，而无需太多麻烦。它帮助您了解 ConvNet 对您的问题可以做些什么，更专注于您心中的总结（可能是一个更大的应用），并注释许多图像以训练更多使用选定类别新图像的
    ConvNets。
- en: During the project, you have learned quite a few useful technicalities you can
    reuse in many projects dealing with images. First of all, you now know how to
    process different kinds of visual inputs from images, videos, and webcam captures.
    You also know how to load a frozen model and put it to work, and also how to use
    a class to access a TensorFlow model.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目期间，您学到了许多有用的技术细节，可以在处理图像的许多项目中重复使用。首先，您现在知道如何处理来自图像、视频和网络摄像头捕获的不同类型的视觉输入。您还知道如何加载冻结模型并使其运行，以及如何使用类来访问
    TensorFlow 模型。
- en: 'On the other hand, clearly, the project has some limitations that you may encounter
    sooner or later, and that may spark the idea to try to integrate your code and
    make it shine even more. First of all, the models we have discussed will soon
    be surpassed by newer and more efficient ones (you can check here for newly available
    models: [https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md](https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md)),
    and you will need to incorporate new ones or create your own architecture ([https://github.com/tensorflow/models/blob/master/object_detection/g3doc/defining_your_own_model.md](https://github.com/tensorflow/models/blob/master/object_detection/g3doc/defining_your_own_model.md)).
    Then you may need to combine the model to reach the accuracy you need in your
    project (the paper *Speed/accuracy trade-offs for modern convolutional object
    detectors* reveals how researchers at Google have done it). Finally,  you may
    need to tune a ConvNet to recognize a new class (you can read how to do that here,
    but beware, it is a long process and a project by itself: [https://github.com/tensorflow/models/blob/master/object_detection/g3doc/using_your_own_dataset.md](https://github.com/tensorflow/models/blob/master/object_detection/g3doc/using_your_own_dataset.md)).'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，很明显，这个项目存在一些局限性，你迟早会遇到这些问题，这也许会激发你将代码整合起来，使它更加出色。首先，我们讨论的模型很快会被更新的、更高效的模型所超越（你可以在这里查看新发布的模型：[https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md](https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md)），你需要将它们纳入到你的项目中，或者创建你自己的架构（[https://github.com/tensorflow/models/blob/master/object_detection/g3doc/defining_your_own_model.md](https://github.com/tensorflow/models/blob/master/object_detection/g3doc/defining_your_own_model.md)）。接着，你可能需要将不同的模型进行结合，以实现项目所需的准确度（论文*Speed/accuracy
    trade-offs for modern convolutional object detectors*揭示了谷歌研究人员是如何做到的）。最后，你可能需要调整一个卷积神经网络（ConvNet）来识别新的类别（你可以在这里阅读如何做，但请注意，这是一个漫长的过程，且本身就是一个项目：[https://github.com/tensorflow/models/blob/master/object_detection/g3doc/using_your_own_dataset.md](https://github.com/tensorflow/models/blob/master/object_detection/g3doc/using_your_own_dataset.md)）。
- en: In the next chapter, we will look at state-of-the-art object detection in images,
    devising a project that will lead you to produce complete discursive captions
    describing submitted images, not just simple labels and bounding boxes.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究图像中的最先进的目标检测技术，并设计一个项目，帮助你生成完整的描述性标题，描述提交的图像，而不仅仅是简单的标签和边界框。
