- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Generative AI and Large Language Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式AI和大型语言模型
- en: In this chapter, we will explore recipes that use the generative aspect of the
    transformer models to generate text. As we touched upon the same in [*Chapter
    8*](B18411_08.xhtml#_idTextAnchor205), *Transformers and Their Applications*,
    the generative aspect of the transformer models uses the decoder component of
    the transformer network. The decoder component is responsible for generating text
    based on the provided context.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索使用transformer模型的生成特性来生成文本的配方。正如我们在[*第8章*](B18411_08.xhtml#_idTextAnchor205)“变压器及其应用”中提到的，transformer模型的生成特性使用transformer网络的解码器组件。解码器组件负责根据提供的上下文生成文本。
- en: With the advent of the **General Purpose Transformers** (**GPT**) family of
    **Large Language Models** (**LLMs**), these have only grown in size and capability
    with each new version. LLMs such as GPT-4 have been trained on large corpora of
    text and can match or beat their state-of-the-art counterparts in many NLP tasks.
    These LLMs have also built upon their generational capability and they can be
    instructed to generate text based on human prompting.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 随着**通用变压器**（**GPT**）家族的**大型语言模型**（**LLM**）的出现，这些模型随着每个新版本的发布，其规模和能力都在不断增长。例如，GPT-4已经在大量文本语料库上进行了训练，并在许多NLP任务中与最先进的模型相匹配或超越。这些LLM还基于其生成能力，可以接受人类的提示来生成文本。
- en: We will use generative models based on the transformer architecture for our
    recipes.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用基于transformer架构的生成模型作为我们的配方。
- en: 'This chapter contains the following recipes:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含以下配方：
- en: Running an LLM locally
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地运行LLM
- en: Running an LLM to follow instructions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行LLM以遵循指令
- en: Augmenting an LLM with external data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用外部数据增强LLM
- en: Augmenting the LLM with external content
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用外部内容增强LLM
- en: Creating a chatbot using an LLM
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLM创建聊天机器人
- en: Generating code using an LLM
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLM生成代码
- en: Generating a SQL query using human-defined requirements
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用人类定义的要求生成SQL查询
- en: Agents – making an LLM to reason and act
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理 – 使LLM进行推理和行动
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code for this chapter is in a folder named `Chapter10` in the GitHub repository
    of the book ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter10](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter10)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于书籍的GitHub仓库中名为`Chapter10`的文件夹中([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter10](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter10))。
- en: As in previous chapters, the packages required for this chapter are part of
    the `poetry`/`pip` requirements configuration file that is present in the repository.
    We recommend that the reader set up the environment beforehand.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，本章所需的包是存储库中`poetry`/`pip`要求配置文件的一部分。我们建议读者事先设置环境。
- en: Model access
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型访问
- en: In this chapter, we will use models from Hugging Face and OpenAI. The following
    are the instructions to enable model access for the various models that will be
    used for the recipes in this chapter.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用来自Hugging Face和OpenAI的模型。以下是为本章中使用的各种模型启用模型访问的说明。
- en: '**Hugging Face Mistral model**: Create the necessary credentials on the Hugging
    Face site to ensure that the model is available to be used or downloaded via the
    code. Please visit the Mistral model details at [https://huggingface.co/mistralai/Mistral-7B-v0.3](https://huggingface.co/mistralai/Mistral-7B-v0.3).
    You will need to request access to the model on the site before running the recipe
    that uses this model.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**Hugging Face Mistral模型**：在Hugging Face网站上创建必要的凭据，以确保模型可以通过代码使用或下载。请访问Mistral模型详细信息[https://huggingface.co/mistralai/Mistral-7B-v0.3](https://huggingface.co/mistralai/Mistral-7B-v0.3)。在运行使用此模型的配方之前，您需要在网站上请求对模型的访问。'
- en: '**Hugging Face Llama model**: Create the necessary credentials on the Hugging
    Face site to ensure that the model is available to be used or downloaded via the
    code. Please visit the Llama 3.1 model details at [https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct).
    You will have to request for the model access on the site before you run the recipe
    that uses this model.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**Hugging Face Llama模型**：在Hugging Face网站上创建必要的凭据，以确保模型可以通过代码使用或下载。请访问Llama
    3.1模型详细信息[https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)。在运行使用此模型的配方之前，您必须在该网站上请求对模型的访问。'
- en: In the code snippets, we are using Jupyter as an environment for execution.
    If you are using the same, you will something like the screenshot shown here.
    You can enter the token in the text field and let the recipe make progress. The
    recipe will wait for the token to be entered the first time. Subsequent runs of
    the recipe will use the cached token that the Hugging Face library creates for
    the user locally.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码片段中，我们使用 Jupyter 作为执行环境。如果你使用的是相同的，你将看到类似于这里显示的截图。你可以在文本字段中输入令牌，并让食谱继续进行。食谱将等待第一次输入令牌。食谱的后续运行将使用
    Hugging Face 库为用户在本地创建的缓存令牌。
- en: '![](img/B18411_10_1.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18411_10_1.jpg)'
- en: Figure 10.1 – Copying a token from Hugging Face
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – 从 Hugging Face 复制令牌
- en: '`api-token`. In the code snippets, we are using Jupyter as an environment for
    execution. If you are using the same, you will see a text box where you will need
    to enter the `api-token`. You can enter the token in the text field and let the
    recipe make progress. The recipe will wait for the token to be entered.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`api-token`。在代码片段中，我们使用 Jupyter 作为执行环境。如果你使用的是相同的，你将看到一个文本框，你需要在其中输入 `api-token`。你可以在文本字段中输入令牌，并让食谱继续进行。食谱将等待输入令牌。'
- en: Running an LLM locally
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在本地运行 LLM
- en: In this recipe, we will learn how to load an LLM locally using the CPU or GPU
    and generate text from it after giving it a starting text as seed input. An LLM
    running locally can be instructed to generate text based on prompting. This new
    paradigm of generation of text via instruction prompting has brought the LLM to
    recent prominence. Learning to do this allows for control over hardware resources
    and environment setup, optimizing performance and enabling rapid experimentation
    or prototyping with text generation from seed inputs. This enhances data privacy
    and security, along with a reduced reliance on cloud services, and facilitates
    cost-effective deployment for educational and practical applications. As we run
    an LLM locally as part of the recipe, we will use instruction prompting to make
    it generate text based on a simple instruction.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将学习如何使用 CPU 或 GPU 在本地加载 LLM，并在给出起始文本作为种子输入后从它生成文本。本地的 LLM 可以被指示根据提示生成文本。这种通过指令提示生成文本的新范式使
    LLM 最近备受瞩目。学习这样做可以控制硬件资源和环境设置，优化性能，并允许从种子输入中进行快速实验或原型设计。这增强了数据隐私和安全，减少了对外部云服务的依赖，并促进了教育和实践应用的成本效益部署。由于我们在食谱中作为一部分运行
    LLM，我们将使用指令提示来让它根据简单的指令生成文本。
- en: Getting ready
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We recommend that you use a system with at least 16 GB of RAM or a system with
    a GPU that has at least 8 GB of VRAM. These examples were created on a system
    with 8 GB of RAM and an nVidia RTX 2070 GPU with 8 GB of VRAM. These examples
    will work without a GPU as long as there is 16 GB of system RAM. In this recipe,
    we will load the **Mistral-7B** model using the Hugging Face ([https://huggingface.co/docs](https://huggingface.co/docs))
    libraries. This model has a smaller size compared to other language models in
    its class but can outperform them on several NLP tasks. The Mistral-7B model with
    7 billion network parameters can outperform the **Llama2** model, which has over
    13 billion parameters.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议你使用至少有 16 GB RAM 的系统或至少有 8 GB VRAM 的 GPU 系统。这些示例是在一个有 8 GB RAM 和一个 8 GB
    VRAM 的 nVidia RTX 2070 GPU 的系统上创建的。只要系统有 16 GB 的 RAM，这些示例在没有 GPU 的情况下也能工作。在这个食谱中，我们将使用
    Hugging Face ([https://huggingface.co/docs](https://huggingface.co/docs)) 库加载
    **Mistral-7B** 模型。与同类其他语言模型相比，该模型尺寸更小，但在多个 NLP 任务上可以超越它们。拥有 70 亿网络参数的 Mistral-7B
    模型可以超越拥有超过 130 亿参数的 **Llama2** 模型。
- en: It is required that the user create the necessary credentials on the Hugging
    Face site to ensure that the model is available to be used or downloaded via the
    code. Please refer to *Model access* under the *Technical requirements* section
    to complete the step to access the Mistral model. Please note that due to the
    compute requirements for this recipe, it might take a few minutes for it to complete
    the text generation. If the required compute capacity is unavailable, we recommend
    that the reader refer to the *Using OpenAI models instead of local ones* section
    at the end of this chapter and use the method described there to use an OpenAI
    model for this recipe.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 用户需要在 Hugging Face 网站上创建必要的凭证，以确保模型可以通过代码使用或下载。请参考“技术要求”部分下的“模型访问”以完成访问 Mistral
    模型的步骤。请注意，由于本食谱的计算需求，生成文本可能需要几分钟才能完成。如果所需的计算能力不可用，我们建议读者参考本章末尾的“使用 OpenAI 模型而不是本地模型”部分，并使用那里描述的方法使用
    OpenAI 模型为本食谱提供服务。
- en: How to do it…
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Do the necessary imports:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入：
- en: '[PRE0]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In this step, we set up the login for Hugging Face. Though we can set the token
    directly in the code, we recommend setting the token in an environment variable
    and then reading from it in the notebook. Calling the **login** method with the
    token authorizes the call to Hugging Face and allows the code to download the
    model locally and use it:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们设置 Hugging Face 的登录。虽然我们可以在代码中直接设置令牌，但我们建议在环境变量中设置令牌，然后在笔记本中读取它。使用带有令牌的
    **login** 方法授权对 Hugging Face 的调用，并允许代码本地下载模型并使用它：
- en: '[PRE1]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In this step, we initialize the device, the **mistralai/Mistral-7B-v0.3** model,
    and the tokenizer, respectively. We set the **device_map** parameter to **auto**,
    which lets the pipeline pick the available device to use. We set the **load_in_4bit**
    parameter to **True**. This lets us load the quantized model for the inference
    (or generation) step. Using a quantized model consumes less memory and lets us
    load the model locally on systems with limited memory. The loading of the quantized
    model is handled by the **AutoModelForCausalLM** module, and it downloads a model
    from the Hugging Face hub that has been quantized to the bit size specified in
    the parameter:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们分别初始化设备、**mistralai/Mistral-7B-v0.3** 模型和分词器。我们将 **device_map** 参数设置为
    **auto**，这允许管道选择可用的设备来使用。我们将 **load_in_4bit** 参数设置为 **True**。这使我们能够加载用于推理（或生成）步骤的量化模型。使用量化模型消耗更少的内存，并允许我们在内存有限的系统上本地加载模型。量化模型的加载由
    **AutoModelForCausalLM** 模块处理，并从 Hugging Face 网络下载一个已量化到参数中指定比特大小的模型：
- en: '[PRE2]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this step, we initialize a generation config. This generation config is
    passed to the model, instructing it on how to generate the text. We set the **num_beams**
    parameter to **4**. This parameter results in the generated text being more coherent
    and grammatically correct as the number of beams is increased. However, a greater
    number of beams also results in decoding (or text-generation) time. We set the
    **early_stopping** parameter to **True** as the generation of the next word is
    concluded as soon as the number of beams reaches the value specified in the **num_beams**
    parameter. The **eos_token_id** (e.g., **50256** for GPT models) and **pad_token_id**
    (e.g., **0** for GPT models) are defaulted to use the model’s token IDs. These
    token IDs are used to specify the end-of-sentence and padding tokens that will
    be used by the model. The **max_new_tokens** parameter specifies the maximum number
    of tokens that will be generated. There are more parameters that can be specified
    for generating the text and we encourage you to play around with different values
    of the previously specified parameters, as well as any additional parameters for
    customizing the text generation. For more information, please refer to the transformer
    documentation on the **GenerationConfig** class at [https://github.com/huggingface/transformers/blob/main/src/transformers/generation/configuration_utils.py](https://github.com/huggingface/transformers/blob/main/src/transformers/generation/configuration_utils.py):'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们初始化一个生成配置。这个生成配置被传递给模型，指导它如何生成文本。我们将**num_beams**参数设置为**4**。随着光束数量的增加，生成的文本将更加连贯和语法正确。然而，光束数量越多，解码（或文本生成）的时间也会更长。我们将**early_stopping**参数设置为**True**，因为一旦光束数量达到**num_beams**参数中指定的值，下一个单词的生成就会结束。**eos_token_id**（例如，GPT模型的**50256**）和**pad_token_id**（例如，GPT模型的**0**）默认使用模型的标记ID。这些标记ID用于指定模型将使用的句子结束和填充标记。**max_new_tokens**参数指定将生成的最大标记数。还有更多参数可以用于生成文本，我们鼓励您尝试调整之前指定的参数值，以及任何其他用于自定义文本生成的附加参数。有关更多信息，请参阅[https://github.com/huggingface/transformers/blob/main/src/transformers/generation/configuration_utils.py](https://github.com/huggingface/transformers/blob/main/src/transformers/generation/configuration_utils.py)上的转换器文档中的**GenerationConfig**类：
- en: '[PRE3]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this step, we initialize a seed sentence. This seed sentence acts as a prompt
    to the model asking it to generate a step-by-step way to make an apple pie:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们初始化一个种子句子。这个种子句子作为对模型的提示，要求它生成制作苹果派的逐步方法：
- en: '[PRE4]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In this step, we tokenize the seed sentence to transform the text into the
    corresponding embedded representation and pass it to the model to generate the
    text. We also pass the **generation_config** instance to it. The model generates
    the token IDs as part of its generation:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将种子句子进行标记化，将文本转换为相应的嵌入表示，并将其传递给模型以生成文本。我们还传递了**generation_config**实例给它。模型在其生成过程中生成标记ID：
- en: '[PRE5]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this step, we decode the token IDs that were generated from the previous
    step. The transformer model uses special tokens such as **CLS** or **MASK** and
    to generate the text as part of the training. We set the value of **skip_special_tokens**
    to **True**. This allows us to omit these special tokens and generate pure text
    as part of our output. We print the decoded (or generated) text.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们解码前一步生成的标记ID。转换器模型使用特殊的标记，如**CLS**或**MASK**，并将它们作为训练的一部分生成文本。我们将**skip_special_tokens**的值设置为**True**。这允许我们省略这些特殊标记，并将纯文本作为输出的一部分生成。我们打印解码（或生成的）文本。
- en: '[PRE6]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output would look like the following. We have shortened the output for
    brevity. You might see a longer result:'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容。为了简洁，我们缩短了输出。您可能会看到一个更长的结果：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Running an LLM to follow instructions
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行一个大型语言模型来遵循指令
- en: In this recipe, we will learn how to get an LLM to follow instructions via prompting.
    An LLM can be provided some context and asked to generate text based on that context.
    This is a very novel feature of an LLM. The LLM can be specifically instructed
    to generate text based on explicit user requirements. Using this feature expands
    the breadth of use cases and applications that can be developed. The context and
    the question to be answered can be generated dynamically and used in various use
    cases ranging from answering simple math problems to sophisticated data extraction
    from knowledge bases.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们将学习如何通过提示让LLM遵循指令。LLM可以提供一些上下文，并要求根据该上下文生成文本。这是LLM的一个非常新颖的功能。LLM可以被特别指示根据明确的用户要求生成文本。使用此功能可以扩展可以开发的使用案例和应用的范围。上下文和要回答的问题可以动态生成，并用于各种用例，从回答简单的数学问题到从知识库中提取复杂的数据。
- en: We will use the `meta-llama/Meta-Llama-3.1-8B-Instruct` model for this recipe.
    This model is built on top of the `meta-llama/Meta-Llama-3.1-8B` model and has
    been tuned to follow instructions via prompts.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`meta-llama/Meta-Llama-3.1-8B-Instruct`模型进行此配方。此模型建立在`meta-llama/Meta-Llama-3.1-8B`模型之上，并经过调整以通过提示遵循指令。
- en: Getting ready
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: It is required that the user create the necessary credentials on the Hugging
    Face site to ensure that the model is available to be used or downloaded via the
    code. Please refer to *Model access* under the *Technical requirements* section
    to complete the step to access the Llama model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 用户需要在Hugging Face网站上创建必要的凭据，以确保模型可以通过代码使用或下载。请参考*技术要求*部分下的*模型访问*以完成访问Llama模型的步骤。
- en: You can use the **10.2_instruct_llm.ipynb** notebook from the code site if you
    want to work from an existing notebook. Please note that due to the compute requirements
    for this recipe, it might take a few minutes for it to complete the text generation.
    If the required compute capacity is unavailable, we recommend that the reader
    refer to the *Using OpenAI models instead of local ones* section at the end of
    this chapter and use the method described there to use an OpenAI model for this
    recipe.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想从一个现有的笔记本开始工作，可以使用代码网站上的**10.2_instruct_llm.ipynb**笔记本。请注意，由于本配方对计算能力的要求，生成文本可能需要几分钟才能完成。如果所需的计算能力不可用，我们建议读者参考本章末尾的*使用OpenAI模型而不是本地模型*部分，并使用那里描述的方法使用OpenAI模型进行此配方。
- en: How to do it…
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'The recipe does the following things:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 配方执行以下操作：
- en: It initializes an LLM model to be loaded into memory.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它初始化一个LLM模型，以便将其加载到内存中。
- en: It initializes a prompt to instruct the LLM to perform a task. This task is
    that of answering a question.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它初始化一个提示，指示LLM执行一个任务。这个任务是回答问题。
- en: It sends the prompt to the LLM and asks it to generate an answer.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将提示发送到LLM，并要求它生成一个答案。
- en: 'The steps for the recipe are as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 配方的步骤如下：
- en: 'Do the necessary imports:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入：
- en: '[PRE8]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Set up the login for Hugging Face. Set the **HuggingFace** token in an environment
    variable and read from it into a local variable. Calling the **login** method
    with the token authorizes the call to **HuggingFace** and allows the code to download
    the model locally and use it. You will see a similar login window as the one shown
    in the *Running an LLM locally* recipe in this chapter:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Hugging Face的登录。将**HuggingFace**令牌设置在环境变量中，并从中读取到本地变量。使用令牌调用**login**方法授权对**HuggingFace**的调用，并允许代码在本地下载模型并使用它。您将看到类似于本章中*在本地运行LLM*配方中所示的一个类似的登录窗口：
- en: '[PRE9]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this step, we specify the model name. We also define the quantization configuration.
    Quantization is a technique to reduce the size of the internal LLM network weights
    to a lower precision. This allows us to load the model on systems with limited
    CPU or GPU memory. Loading an LLM with its default precision requires a large
    amount of CPU/GPU memory. In this case, we load the network weights in four bits
    using the **load_in_4bit** parameter of the **BitsAndBytesConfig** class. The
    other parameters used are described as follows:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此步骤中，我们指定模型名称。我们还定义了量化配置。量化是一种技术，可以将内部LLM网络权重的大小降低到更低的精度。这允许我们在有限的CPU或GPU内存的系统上加载模型。使用默认精度加载LLM需要大量的CPU/GPU内存。在这种情况下，我们使用**BitsAndBytesConfig**类的**load_in_4bit**参数以四位加载网络权重。其他使用的参数描述如下：
- en: '**bnb_4bit_compute_dtype**: This parameter specifies the data type that is
    used during the computation. Though the network weights are stored in four bits,
    the computation still happens in 16 or 32 bits as defined by this parameter. Setting
    this to **torch.float16** results in speed improvements in certain scenarios.'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**bnb_4bit_compute_dtype**：此参数指定在计算期间使用的数据类型。尽管网络权重以四位存储，但计算仍然按照此参数定义的16或32位进行。将此设置为**torch.float16**在某些情况下会导致速度提升。'
- en: '**bnb_4bit_use_double_quant**: This parameter specifies that nested quantization
    should be used. This means that a second quantization is performed which saves
    an additional 0.4 bits per parameter in the network. This helps us save the memory
    needed for the model.'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**bnb_4bit_use_double_quant**：此参数指定应使用嵌套量化。这意味着执行第二次量化，这可以在网络中为每个参数节省额外的0.4位。这有助于我们节省模型所需的内存。'
- en: '**bnb_4bit_quant_type**: This **nf4** parameter value initializes the weights
    of the network using a normal distribution, which is useful during the training
    of the model. However, it does not have any impact on inference, such as for this
    recipe. We will still be setting this to **nf4** to keep it consistent with the
    model weights.'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**bnb_4bit_quant_type**：此**nf4**参数值使用正态分布初始化网络的权重，这在模型训练期间很有用。然而，它对推理没有影响，例如对于这个食谱。我们仍然将其设置为**nf4**，以保持与模型权重的统一。'
- en: 'For quantization concepts, we recommend referring to the blog post at [https://huggingface.co/blog/4bit-transformers-bitsandbytes](https://huggingface.co/blog/4bit-transformers-bitsandbytes),
    where this is explained in greater detail. Please note that in order to load the
    model in 4-bit, it is required that a GPU is used:'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于量化概念，我们建议参考[https://huggingface.co/blog/4bit-transformers-bitsandbytes](https://huggingface.co/blog/4bit-transformers-bitsandbytes)上的博客文章，其中对此有更详细的解释。请注意，为了以4位加载模型，需要使用GPU：
- en: '[PRE10]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In this step, we load the **meta-llama/Meta-Llama-3.1-8B-Instruct** model and
    the corresponding tokenizer:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们加载**meta-llama/Meta-Llama-3.1-8B-Instruct**模型和相应的分词器：
- en: '[PRE11]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In this step, we initialize a pipeline that weaves the model and tokenizer
    together with some additional parameters. We covered the description of these
    parameters in the *Running an LLM locally* recipe in this chapter. We recommend
    referring to that recipe for more details on these parameters. We are adding an
    additional parameter named **repetition_penalty** here. This ensures that the
    LLM does not go into a state where it starts repeating itself or parts of the
    text that were generated before:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们初始化一个将模型和分词器结合在一起，并带有一些附加参数的管道。我们在本章的*在本地运行LLM*食谱中介绍了这些参数的描述。我们建议您参考该食谱以获取有关这些参数的更多详细信息。我们在这里添加了一个名为**repetition_penalty**的附加参数。这确保LLM不会进入开始重复自身或之前生成的文本部分的状态：
- en: '[PRE12]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this step, we create a prompt that sets up an instruction context that can
    be passed to the LLM. The LLM acts as per the instructions set up in the prompt.
    In this case, we start our instruction with a conversation between the user and
    the agent. The conversation starts with the question **What is your favourite
    country?**. This question is followed by the model answer in the form of **Well,
    I am quite fascinated with Peru.**. We then follow it up with another instruction
    by asking the question **What can you tell me about Peru?**. This methodology
    serves as a template for the LLM to learn our intent and generate an answer for
    the follow-up question based on the pattern we specified in our instruction prompt:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们创建一个提示，设置一个可以传递给LLM的指令上下文。LLM根据提示中设置的指令行事。在这种情况下，我们以用户和代理之间的对话开始我们的指令。对话以问题**你最喜欢的国家是哪里？**开始。这个问题后面跟着模型的回答，形式为**嗯，我对秘鲁很着迷**。然后我们通过提出问题**你能告诉我关于秘鲁的什么？**来继续另一个指令。这种方法为LLM提供了一个模板，以便学习我们的意图，并根据我们在指令提示中指定的模式生成后续问题的答案：
- en: '[PRE13]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In this step, we execute the pipeline with the prompt and execute it. We additionally
    specify the maximum number of tokens that should be generated as part of the output.
    This explicitly instructs the LLM to stop generation once the specific length
    is reached:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们使用提示执行管道并执行它。我们还指定了作为输出应生成的最大令牌数。这明确指示LLM在达到特定长度时停止生成：
- en: '[PRE14]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will result in the following output:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: There’s more…
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Now that we have seen a way to instruct a model to generate text, we can just
    change the prompt and get the model to generate text for a completely different
    kind of question. Let us change the prompt text to the following and use the same
    recipe to generate text based on the updated prompt:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了指导模型生成文本的方法，我们只需更改提示，就可以让模型为完全不同的问题生成文本。让我们将提示文本更改为以下内容，并使用相同的菜谱根据更新的提示生成文本：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This results in the following output:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As we can see from the preceding output, the model is able to understand the
    instructions quite clearly. It is able to reason well and answer the question
    correctly. This recipe only used the context that was stored within the LLM. More
    specifically, the LLM used its internal knowledge to answer this question. LLMs
    are trained on huge corpora of text and can generate answers based on that large
    corpus. In the next recipe, we will learn how to augment the knowledge of an LLM.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述输出所示，模型能够相当清楚地理解指令。它能够很好地推理并正确回答问题。这个菜谱只使用了存储在LLM中的上下文。更具体地说，LLM使用其内部知识来回答这个问题。LLMs是在大量文本语料库上训练的，可以根据这个大型语料库生成答案。在下一个菜谱中，我们将学习如何增强LLM的知识。
- en: Augmenting an LLM with external data
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用外部数据增强LLM
- en: In the following recipes, we will learn how to get an LLM to answer questions
    on which it has not been trained. These could include information that was created
    after the LLM was trained. New content keeps getting added to the World Wide Web
    daily. There is no one LLM that can be trained on that context every day. The
    **Retriever Augmented Generation** (**RAG**) frameworks allow us to augment the
    LLM with additional content that can be sent as input to it for generating content
    for downstream tasks. This allows us to save on costs too since we do not have
    to spend time and compute costs on retraining a model based on updated content.
    As a basic introduction to RAG, we will augment an LLM with some content from
    a few web pages and ask some questions pertaining to the content contained in
    those pages. For this recipe, we will first load the LLM and ask it a few questions
    without providing it any context. We will then augment this LLM with additional
    context and ask the same questions. We will compare the answers, which will demonstrate
    the power of the LLM when coupled with augmented content.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下菜谱中，我们将学习如何让LLM回答它未训练过的问题。这可能包括在LLM训练后创建的信息。新内容每天都在互联网上不断添加。没有哪个LLM可以每天在这个上下文中进行训练。**检索增强生成**（**RAG**）框架允许我们通过可以发送给LLM作为输入的额外内容来增强LLM。这使我们也能节省成本，因为我们不必花费时间和计算成本来根据更新的内容重新训练模型。作为RAG的基本介绍，我们将使用来自几个网页的一些内容来增强LLM，并就这些页面中的内容提出一些问题。对于这个菜谱，我们首先加载LLM并就一些问题提出问题，而不提供任何上下文。然后我们将额外的上下文添加到这个LLM中，并再次提出相同的问题。我们将比较答案，这将展示LLM与增强内容结合时的强大功能。
- en: Executing a simple prompt-to-LLM chain
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行简单的提示到LLM链
- en: In this recipe, we will create a simple prompt that can be used to instruct
    an LLM. A prompt is a template with placeholder values that can be populated at
    runtime. The LangChain framework allows us to weave a prompt and an LLM together,
    along with other components in the mix, to generate text. We will explore these
    techniques in this and some of the recipes that follow.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将创建一个简单的提示，可以用来指导一个LLM。提示是一个带有占位符值的模板，这些值可以在运行时填充。LangChain框架允许我们将提示和LLM结合起来，以及其他混合中的组件，以生成文本。我们将在这篇文档和随后的某些菜谱中探讨这些技术。
- en: Getting ready
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: We must create the necessary credentials on the Hugging Face site to ensure
    that the model is available to be used or downloaded via the code. Please refer
    to *Model access* under the *Technical requirements* section to complete the step
    to access the Llama model.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须在Hugging Face网站上创建必要的凭据，以确保模型可以通过代码使用或下载。请参阅“技术要求”部分下的“模型访问”，以完成访问Llama模型的步骤。
- en: In this recipe, we will use the LangChain framework ([https://www.langchain.com/](https://www.langchain.com/))
    to demonstrate the LangChain framework and its capabilities with an example based
    on **LangChain Expression Language** (**LCEL**). Let us start with a simple recipe
    based on the LangChain framework and extend it in the recipes that follow from
    there on. The first part of this recipe is very similar to the previous one. The
    only difference is the use of the LangChain framework.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在此食谱中，我们将使用LangChain框架([https://www.langchain.com/](https://www.langchain.com/))通过基于**LangChain表达式语言**（**LCEL**）的示例来展示LangChain框架及其功能。让我们从一个基于LangChain框架的简单食谱开始，并在随后的食谱中从那里扩展。此食谱的第一部分与上一个食谱非常相似。唯一的区别是使用了LangChain框架。
- en: You can use the `10.3_langchain_prompt_with_llm.ipynb` notebook from the code
    site if you want to work from an existing notebook. Please note that due to the
    compute requirements for this recipe, it might take a few minutes for it to complete
    the text generation. If the required compute capacity is unavailable, we recommend
    that you refer to the *Using OpenAI models instead of local ones* section at the
    end of this chapter and use the method described there to use an OpenAI model
    for this recipe.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想从一个现有的笔记本开始工作，可以使用代码网站上的`10.3_langchain_prompt_with_llm.ipynb`笔记本。请注意，由于此食谱的计算需求，生成文本可能需要几分钟。如果所需的计算能力不可用，我们建议您参考本章末尾的*使用OpenAI模型而不是本地模型*部分，并使用那里描述的方法使用OpenAI模型来完成此食谱。
- en: How to do it…
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The recipe does the following things:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 该食谱执行以下操作：
- en: It initializes an LLM model to be loaded into memory.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它初始化一个LLM模型以加载到内存中。
- en: It initializes a prompt to instruct the LLM perform a task. This task is that
    of answering a question.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它初始化一个提示来指导LLM执行任务。这个任务是回答问题。
- en: It sends the prompt to the LLM and asks it to generate an answer. This is all
    done via the LangChain framework.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将提示发送给LLM并要求它生成答案。所有这些操作都是通过LangChain框架完成的。
- en: 'The steps for the recipe are as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 该食谱的步骤如下：
- en: 'Start with doing the necessary imports:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先进行必要的导入：
- en: '[PRE18]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In this step, we initialize the model name and the quantization configuration.
    We have expanded upon quantization in the *Running an LLM to follow instructions*
    recipe; please check there for more details. We will use the **meta-llama/Meta-Llama-3.1-8B-Instruct**
    model that was released by Meta in July of 2024\. It has outperformed models of
    bigger size on many NLP tasks:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们初始化模型名称和量化配置。我们已经在*运行LLM以遵循指令*食谱中扩展了量化；请查阅那里以获取更多详细信息。我们将使用Meta在2024年7月发布的**meta-llama/Meta-Llama-3.1-8B-Instruct**模型。它在许多NLP任务上优于更大规模的模型：
- en: '[PRE19]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In this step, we initialize the model. We have elaborated on the methodology
    for loading the model and the tokenizer using the **Transformers** library in
    detail in [*Chapter 8*](B18411_08.xhtml#_idTextAnchor205). To avoid repeating
    the same information here, please refer to that chapter for more details:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们初始化模型。我们已经在[*第8章*](B18411_08.xhtml#_idTextAnchor205)中详细阐述了使用**Transformers**库加载模型和分词器的方法。为了避免在此重复相同的信息，请参阅该章节以获取更多详细信息：
- en: '[PRE20]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In this step, we initialize the pipeline. We have elaborated on the pipeline
    construct from the transformers library in detail in [*Chapter 8*](B18411_08.xhtml#_idTextAnchor205).
    To avoid repeating the same information here, please refer to that chapter for
    more details:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们初始化管道。我们已经在[*第8章*](B18411_08.xhtml#_idTextAnchor205)中详细阐述了从transformers库中获取的管道结构。为了避免在此重复相同的信息，请参阅该章节以获取更多详细信息：
- en: '[PRE21]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In this step, we initialize a chat prompt template, which is of the defined
    **ChatPromptTemplate** type. The **from_messages** method takes a series of (**message
    type**, **template**) tuples. The second tuple in the messages array has the **{input}**
    template. This signifies that this value will be passed later:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们初始化一个聊天提示模板，它属于定义的**ChatPromptTemplate**类型。**from_messages**方法接受一系列（**消息类型**，**模板**）元组。消息数组中的第二个元组包含**{input}**模板。这表示该值将在稍后传递：
- en: '[PRE22]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In this step, we initialize an output parser that is of the **StrOutputParser**
    type. It converts a chat message returned by an LLM instance to a string:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们初始化一个输出解析器，它属于**StrOutputParser**类型。它将LLM实例返回的聊天消息转换为字符串：
- en: '[PRE23]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We initialize an instance of a chain next. The chain pipes the output of one
    component to the next. In this instance, the prompt is sent to the LLM and it
    operates on the prompt instance. The output of this operation is a chat message.
    The chat message is then sent to the **output_parser**, which converts it into
    a string. In this step, we only set up the various components of the chain:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们初始化链的一个实例。链将一个组件的输出传递给下一个组件。在这个实例中，提示被发送到LLM，并对其实例进行操作。这个操作的输出是一个聊天消息。然后，聊天消息被发送到**output_parser**，它将其转换为字符串。在这个步骤中，我们只设置了链的各个组件：
- en: '[PRE24]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In this step, we invoke the chain and print the results. We pass the input
    argument in a dictionary. We set up the prompt template as a message that had
    the **{input}** placeholder defined there. As part of the chain invocation, the
    input argument is passed through to the template. The chain invokes the command.
    The chain is instructing the LLM to generate the answer to the question it asked
    via the prompt that we set up previously. As we can see from the output, the advice
    presented in this example is good. We have clipped the output for brevity and
    you might see a longer output:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们调用链并打印结果。我们通过字典传递输入参数。我们将提示模板设置为一个包含**{input}**占位符的消息。作为链调用的一部分，输入参数被传递到模板中。链调用命令。链指示LLM通过我们之前设置的提示来生成它所询问的问题的答案。正如我们可以从输出中看到的那样，这个例子中提出的建议是好的。为了简洁起见，我们已剪辑输出，您可能会看到更长的输出：
- en: '[PRE25]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In this step, we change the prompt a bit and make it answer a simple question
    about the 2024 Paris Olympics:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们稍微改变提示，使其回答一个关于2024年巴黎奥运会的简单问题：
- en: '[PRE27]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The following output is generated for the question. We can see that the answer
    to the question of the number of volunteers is inaccurate by comparing the answer
    to the Wikipedia source. We have omitted a large part of the text that was returned
    in the result. However, to show an example, the Llama 3.1 model generated more
    text than we asked it to and started answering more questions that it was never
    asked. In the next recipe, we will provide a web page source to an LLM and compare
    the returned results with this one for the same question:'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是对问题的输出。我们可以通过将答案与维基百科源进行比较，看到关于志愿者数量的答案不准确。我们省略了结果中返回的大量文本。然而，为了展示一个例子，Llama
    3.1模型生成的文本比我们要求的要多，并开始回答它从未被问过的问题。在下一个菜谱中，我们将提供一个网页源到LLM，并将返回的结果与这个问题进行比较：
- en: '[PRE28]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Augmenting the LLM with external content
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过外部内容增强LLM
- en: In this recipe, we will expand upon the previous example and build a chain that
    passes external content to the LLM and helps it answer questions based on that
    augmented content. The technique learned as part of this recipe will help us understand
    a simple framework for how to extract content from a source and store that in
    a medium that is conducive to fast semantic searches based on context. Once we
    learn how to store the content in a searchable format, we can use that store to
    extract answers to questions that are in open form. This approach can be scaled
    for production as well using the right tools and approaches. Our goal here is
    to demonstrate the basic framework to extract an answer to a question, given a
    content source.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将扩展之前的示例，构建一个链，将外部内容传递给LLM，并帮助它根据增强内容回答问题。这个菜谱中学到的技术将帮助我们理解一个简单的框架，即如何从源中提取内容并将其存储在有利于基于上下文快速语义搜索的媒介中。一旦我们学会了如何以可搜索的格式存储内容，我们就可以使用这个存储库来提取开放形式的问题的答案。使用适当的工具和方法，这种方法也可以扩展到生产环境中。我们的目标是展示提取问题答案的基本框架，给定内容源。
- en: Getting ready
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备中
- en: We will use a model from OpenAI in this recipe. Please refer to *Model access*
    under the *Technical requirements* section to complete the step to access the
    OpenAI model. You can use the `10.4_rag_with_llm.ipynb` notebook from the code
    site if you want to work off an existing notebook.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用OpenAI的模型。请参考“技术要求”部分下的“模型访问”，以完成访问OpenAI模型的步骤。如果您想从一个现有的笔记本开始工作，可以使用代码网站上的`10.4_rag_with_llm.ipynb`笔记本。
- en: How to do it…
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'The recipe does the following things:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这个菜谱做了以下事情：
- en: It initializes the ChatGPT LLM
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化ChatGPT LLM
- en: It scrapes content from a webpage and breaks it into chunks.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它从网页上抓取内容并将其分解成块。
- en: The text in the document chunks is vectorized and stored in a vector store
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档块中的文本被矢量化并存储在矢量存储中
- en: A chain is created that wires the LLM, the vector store, and a prompt with a
    question to answer questions based on the content present on the web page
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个链，将LLM、向量存储和带有问题的提示连接起来，以根据网页上的内容回答问题
- en: 'The steps for the recipe are as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 配方的步骤如下：
- en: 'Do the necessary imports:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入：
- en: '[PRE29]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In this step, we initialize the **gpt-4o-mini** model from OpenAI using the
    ChatOpenAI initializer:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们使用ChatOpenAI初始化器初始化OpenAI的**gpt-4o-mini**模型：
- en: '[PRE30]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In this step, we load the Wikipedia entry on the 2024 Summer Olympics. We initialize
    a **WebBaseLoader** object and pass it the Wikipedia URL for the 2024 Summer Olympics.
    It extracts the HTML content and the main content on each HTML page that is parsed.
    The **load** method on the loader instance triggers the extraction of the content
    from the URLs:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们加载关于2024年夏季奥运会的维基百科条目。我们初始化一个**WebBaseLoader**对象，并将2024年夏季奥运会的维基百科URL传递给它。它提取每个HTML页面的HTML内容和主内容。加载实例上的**load**方法触发从URL提取内容：
- en: '[PRE31]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In this step, we initialize the text splitter instance and call the **split_documents**
    method on it. This splitting of the document is a needed step as an LLM can only
    operate on a context of a limited length. For some large documents, the length
    of the document exceeds the maximum context length supported by the LLM. Breaking
    a document into chunks and using those to match the query text allows us to retrieve
    more relevant parts from the document. The **RecursiveCharacterTextSplitter**
    splits the document based on newline, spaces, and double-newline characters:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们初始化文本拆分实例，并在其上调用**split_documents**方法。这种文档拆分是一个必要的步骤，因为LLM只能在一个有限长度的上下文中操作。对于一些大型文档，文档的长度超过了LLM支持的最大的上下文长度。将文档拆分成块并使用这些块来匹配查询文本，使我们能够从文档中检索更多相关的部分。**RecursiveCharacterTextSplitter**根据换行符、空格和双换行符拆分文档：
- en: '[PRE32]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In this step, we initialize a vector store. We initialize the vector store
    with the document chunks and the embedding provider. The vector store creates
    embeddings of the document chunks and stores them along with the document metadata.
    For production-grade applications, we recommend visiting the following URL: [https://python.langchain.com/docs/integrations/vectorstores/](https://python.langchain.com/docs/integrations/vectorstores/)'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这一步，我们初始化一个向量存储。我们使用文档块和嵌入提供者初始化向量存储。向量存储为文档块创建嵌入并将它们与文档元数据一起存储。对于生产级应用，我们建议访问以下URL：[https://python.langchain.com/docs/integrations/vectorstores/](https://python.langchain.com/docs/integrations/vectorstores/)
- en: There, you can select a vector store based on your requirements. The LangChain
    framework is versatile and works with a host of prominent vector stores.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在那里，你可以根据自己的需求选择一个向量存储。LangChain框架非常灵活，可以与许多著名的向量存储一起工作。
- en: Next, we initialize a retriever by making a call to the `as_retriever` method
    of the vector-store instance. The retriever returned by the method is used to
    retrieve the content from the vector store. The `as_retriever` method is passed
    a `search_type` argument with the `similarity` value, which is also the default
    option. This means that the vector store will be searched against the question
    text based on similarity. The other options supported are `mmr`, which penalizes
    search results of the same type and returns diverse results, and `similarity_score_threshold`,
    which operates in the same way as the `similarity` search type, but can filter
    out the results based on a threshold. These options also support an accompanying
    dictionary argument that can be used to tweak the search parameters. We recommend
    that the readers refer to the LangChain documentation and tweak the parameters
    based on their requirements and empirical findings
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们通过调用向量存储实例的`as_retriever`方法来初始化一个检索器。该方法返回的检索器用于从向量存储中检索内容。`as_retriever`方法传递一个带有`similarity`值的`search_type`参数，这也是默认选项。这意味着向量存储将根据相似度对问题文本进行搜索。其他支持选项包括`mmr`，它惩罚相同类型的搜索结果并返回多样化的结果，以及`similarity_score_threshold`，它以与`similarity`搜索类型相同的方式操作，但可以根据阈值过滤结果。这些选项还支持一个伴随的字典参数，可以用来调整搜索参数。我们建议读者参考LangChain文档并根据他们的需求和经验发现调整参数。
- en: 'We also define a helper method, `format_docs`, that appends the content of
    all the repository docs separated by two newline characters:'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还定义了一个辅助方法`format_docs`，它将所有存储库文档的内容追加，每个文档之间用两个换行符分隔：
- en: '[PRE33]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In this step, we define a chat template and create an instance of **ChatPromptTemplate**
    from it. This prompt template instructs the LLM to answer the question for the
    given context. This context is provided by the augmentation step via the vector
    store search results:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们定义一个聊天模板，并从它创建一个**ChatPromptTemplate**实例。此提示模板指示LLM为给定上下文回答问题。此上下文由增强步骤通过向量存储搜索结果提供：
- en: '[PRE34]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In this step, we set up the chain. The chain sequence sets up the retriever
    as a context provider. The **question** argument is assumed to be passed later
    by the chain. The next component is the prompt, which supplies the context value.
    The populated prompt is sent to the LLM. The LLM pipes or forwards the results
    to the **StrOutputParser()** string, which is designed to return the string contained
    in the output of the LLM. There is no execution in this step. We are only setting
    up the chain:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们设置链。链序列设置检索器作为上下文提供者。**问题**参数假设稍后由链传递。下一个组件是提示，它提供上下文值。填充的提示被发送到LLM。LLM将结果管道或转发到**StrOutputParser()**字符串，该字符串设计为返回LLM输出中的字符串。在这个步骤中没有执行操作。我们只是在设置链：
- en: '[PRE35]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In this step, we invoke the chain and print the results. For each invocation,
    the question text is matched by similarity against the vector store. Then, the
    relevant document chunks are returned, followed by the LLM using these document
    chunks as context and using that context to answer the respective questions. As
    we can see in this case, the answers returned by the chain are accurate:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们调用链并打印结果。对于每次调用，问题文本将与向量存储进行相似度匹配。然后，返回相关的文档片段，接着LLM使用这些文档片段作为上下文，并使用该上下文来回答相应的问题。正如我们在这个例子中可以看到的，链返回的答案是准确的：
- en: '[PRE36]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Invoke the chain with another question and print the results. As we can see
    in this case, the answers returned by the chain are accurate, though I am skeptical
    about whether **Breaking** is indeed a sport, as returned in the results:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用另一个问题调用链并打印结果。正如我们在这个例子中可以看到的，链返回的答案是准确的，尽管我对结果中返回的**Breaking**是否真的是一项运动表示怀疑：
- en: '[PRE38]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Invoke the chain with another question and print the results. As we can see
    in this case, the answers returned by the chain are accurate:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用另一个问题调用链并打印结果。正如我们在这个例子中可以看到的，链返回的答案是准确的：
- en: '[PRE40]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: If we compare these results with the last step of the previous recipe, we can
    see that the LLM returned accurate information as per the content on the Wikipedia
    page. This is an effective use case for RAG where the LLM uses the context to
    answer the question, instead of making up information as it did in the previous
    recipe.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这些结果与上一个菜谱的最后一步进行比较，我们可以看到LLM根据维基百科页面上的内容返回了准确的信息。这是一个有效的RAG用例，其中LLM使用上下文来回答问题，而不是像上一个菜谱中那样编造信息。
- en: Creating a chatbot using an LLM
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LLM创建聊天机器人
- en: In this recipe, we will create a chatbot using the LangChain framework. In the
    previous recipe, we learned how to ask questions to an LLM based on a piece of
    content. Though the LLM was able to answer questions accurately, the interaction
    with the LLM was completely stateless. The LLM looks at each question in isolation
    and ignores any previous interactions or questions that it was asked. In this
    recipe, we will use an LLM to create a chat interaction, wherein the LLM will
    be aware of the previous conversations and use the context from them to answer
    subsequent questions. Applications of such a framework would be to converse with
    document sources and get to the right answer by asking a series of questions.
    These document sources could be of a wide variety of types, from internal company
    knowledge bases to customer contact center troubleshooting guides. Our goal here
    is to present a basic step-by-step framework to demonstrate the essential components
    working together to achieve the end goal.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用LangChain框架创建一个聊天机器人。在之前的菜谱中，我们学习了如何根据一段内容向LLM提问。尽管LLM能够准确回答问题，但与LLM的交互是完全无状态的。LLM会单独查看每个问题，并忽略任何之前的交互或被问过的问题。在这个菜谱中，我们将使用LLM创建一个聊天交互，其中LLM将了解之前的对话，并使用这些对话的上下文来回答后续的问题。此类框架的应用包括与文档源进行对话，通过一系列问题得到正确答案。这些文档源可以是各种类型，从公司内部知识库到客户联系中心故障排除指南。我们的目标是展示一个基本的逐步框架，以演示基本组件如何协同工作以实现最终目标。
- en: Getting ready
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use a model from OpenAI in this recipe. Please refer to *Model access*
    under the *Technical requirements* section to complete the step to access the
    OpenAI model. You can use the `10.5_chatbot_with_llm.ipynb` notebook from the
    code site if you want to work from an existing notebook.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用OpenAI的一个模型。请参考“技术要求”部分下的*模型访问*以完成访问OpenAI模型的步骤。如果你想从一个现有的笔记本开始工作，可以使用代码网站上的`10.5_chatbot_with_llm.ipynb`笔记本。
- en: How to do it…
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The recipe does the following things:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 该菜谱执行以下操作：
- en: It initializes the ChatGPT LLM and an embedding provider. The embedding provider
    is used to vectorize the document content so that a vector-based similarity search
    can be performed.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它初始化ChatGPT LLM和一个嵌入提供者。嵌入提供者用于将文档内容向量化，以便执行基于向量的相似度搜索。
- en: It scrapes content from a webpage and breaks it into chunks.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它从网页上抓取内容并将其分解成块。
- en: The text in the document chunks is vectorized and stored in a vector store.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档块中的文本被向量化并存储在向量存储中。
- en: A conversation is started with the LLM via some curated prompts and a follow-up
    question is asked based on the answer provided by the LLM in the previous context.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过一些精心挑选的提示和一个基于LLM在先前上下文中提供的答案的后续问题，开始与LLM进行对话。
- en: 'Let’s get started:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧：
- en: 'Do the necessary imports:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入：
- en: '[PRE42]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In this step, we initialize the **gpt-4o-mini** model from OpenAI using the
    ChatOpenAI initializer:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们使用ChatOpenAI初始化器初始化OpenAI的**gpt-4o-mini**模型：
- en: '[PRE43]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'In this step, we load the embedding provider. The content from the webpage
    is vectorized via the embedding provider. We use the pre-trained **sentence-transformers/all-mpnet-base-v2**
    model using the call to the **HuggingFaceEmbeddings** constructor call. This model
    is a good one for encoding short sentences or a paragraph. The encoded vector
    representation captures the semantic context well. Please refer to the model card
    at [https://huggingface.co/sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)
    for more details:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们加载嵌入提供者。网页内容通过嵌入提供者进行向量化。我们使用通过调用**HuggingFaceEmbeddings**构造函数的预训练模型**sentence-transformers/all-mpnet-base-v2**。这是一个编码短句子或段落的好模型。编码的向量表示很好地捕捉了语义上下文。请参阅[https://huggingface.co/sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)上的模型卡片以获取更多详细信息：
- en: '[PRE44]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'In this step, we will load a web page that has content based on which we want
    to ask questions. You are free to choose any webpage of your choice. We initialize
    a **WebBaseLoader** object and pass it the URL. We call the **load** method for
    the loader instance. Feel free to change the link to any other webpage that you
    might want to use as the chat knowledge base:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将加载一个包含我们想要提问内容的网页。你可以自由选择任何你喜欢的网页。我们初始化一个**WebBaseLoader**对象，并传入URL。我们调用加载实例的**load**方法。你可以随意更改链接到任何你可能想要用作聊天知识库的网页：
- en: '[PRE45]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Initialize the text splitter instance of the **RecursiveCharacterTextSplitter**
    type. Use the text splitter instance to split the documents into chunks:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化**RecursiveCharacterTextSplitter**类型的文本拆分实例。使用文本拆分实例将文档拆分成块：
- en: '[PRE46]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We initialize the vector or embedding store from the document chunks that we
    created in the previous step. We pass it the document chunks and the embedding
    provider. We also initialize the vector store retriever and the output parser.
    The retriever will provide the augmented content to the chain via the vector store.
    We provided more details in the *Augmenting the LLM with external content* recipe
    from this chapter. To avoid repetition, we recommend referring to that recipe:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从上一步创建的文档块初始化向量或嵌入存储。我们传入文档块和嵌入提供者。我们还初始化向量存储检索器和输出解析器。检索器将通过向量存储向链提供增强内容。我们在这章的*使用外部内容增强LLM*菜谱中提供了更多细节。为了避免重复，我们建议参考那个菜谱：
- en: '[PRE47]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'In this step, we initialize a contextualized system prompt. A system prompt
    defines the persona and the instruction that is to be followed by the LLM. In
    this case, we use the system prompt to contain the instruction that the LLM has
    to use the chat history to formulate a standalone question. We initialize the
    prompt instance with the system prompt definition and set it up with the expectation
    that it will have access to the **chat_history** variable that will be passed
    to it at run time. We also set it up with the question template that will also
    be passed at run time:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们初始化了一个上下文化的系统提示。系统提示定义了角色和LLM需要遵循的指令。在这种情况下，我们使用系统提示包含LLM必须使用聊天历史来制定独立问题的指令。我们使用系统提示定义初始化提示实例，并设置期望它将能够访问在运行时传递给它的
    `chat_history` 变量。我们还设置了在运行时也将传递的提问模板：
- en: '[PRE48]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'In this step, we initialize the contextualized chain. As you can see in the
    previous code snippet, we are setting up the prompt with the context and the chat
    history. This chain uses the chat history and a given follow-up question from
    the user and sets up the context for it as part of the prompt. The populated prompt
    template is sent to the LLM. The idea here is that the subsequent question will
    not provide any context and ask the question based on the chat history generated
    so far:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们初始化了上下文化的链。正如您在前面的代码片段中看到的，我们正在使用上下文和聊天历史设置提示。这个链使用聊天历史和用户给出的后续问题，并将其作为提示的一部分设置上下文。填充的提示模板被发送到LLM。这里的想法是后续问题不会提供任何上下文，而是基于迄今为止生成的聊天历史来提问：
- en: '[PRE49]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'In this step, we initialize a system prompt, much like in the previous recipe,
    based on RAG. This prompt just sets up a prompt template. However, we pass this
    prompt a contextualized question as the chat history grows. This prompt always
    answers a contextualized question, barring the first one:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们初始化了一个系统提示，类似于之前的食谱，基于RAG。这个提示只是设置了一个提示模板。然而，随着聊天历史的增长，我们向这个提示传递一个上下文化的问题。这个提示总是回答一个上下文化的问题，除了第一个问题之外：
- en: '[PRE50]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We initialize two helper methods. The **contextualized_question** method returns
    the contextualized chain if a chat history exists; otherwise, it returns the input
    question. This is the typical scenario for the first question. Once the **chat_history**
    is present, it returns the contextualized chain. The **format_docs** method concatenates
    the page content for each document separated by two newline characters:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们初始化了两个辅助方法。`contextualized_question` 方法在存在聊天历史的情况下返回上下文化的链，否则返回输入问题。这是第一个问题的典型场景。一旦存在
    `chat_history`，它将返回上下文化的链。`format_docs` 方法将每个文档的页面内容连接起来，由两个换行符分隔：
- en: '[PRE51]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'In this step, we set up a chain. We use the **RunnablePassthrough** class to
    set up the context. The **RunnablePassthrough** class allows us to pass the input
    or add additional data to the input via dictionary values. The **assign** method
    will take a key and will assign the value to this key. In this case, the key is
    **context** and the assigned value for it is the result of the chained evaluation
    of the contextualized question, the retriever, and the **format_docs**. Putting
    that into the context of the entire recipe, for the first question, the context
    will use the set of matched records for the question. For the second question,
    the context will use the contextualized question from the chat history, retrieve
    a set of matching records, and pass that as the context. The LangChain framework
    uses a deferred execution model here. We set up the chain here with the necessary
    constructs such as **context**, **qa_prompt**, and the LLM. This is just setting
    the expectation with the chain that all these components will pipe their input
    to the next component when the chain is invoked. Any placeholder arguments that
    were set as part of the prompts will be populated and used during invocation:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们设置了一个链。我们使用 `RunnablePassthrough` 类来设置上下文。`RunnablePassthrough` 类允许我们通过字典值传递输入或向输入添加额外的数据。`assign`
    方法将接受一个键并将值分配给该键。在这种情况下，键是 `context`，为其分配的值是上下文化问题的链式评估结果、检索器和 `format_docs`。将这一点放入整个食谱的上下文中，对于第一个问题，上下文将使用匹配的记录集。对于第二个问题，上下文将使用聊天历史中的上下文化问题，检索一组匹配的记录，并将其作为上下文传递。LangChain框架在这里使用延迟执行模型。我们在这里设置链，使用必要的结构，如
    `context`、`qa_prompt` 和LLM。这只是在设置链的期望，即所有这些组件将在链被调用时将它们的输入传递给下一个组件。任何作为提示部分设置的占位符参数将在调用期间填充和使用：
- en: '[PRE52]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'In this step, we initialize a chat history array. We ask a simple question
    to the chain by invoking it. What happens internally is the question is essentially
    just the first question since there is no chat history present at this point.
    The **rag_chain** just answers the question simply and prints the answer. We also
    extend the **chat_history** with the returned message:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们初始化一个聊天历史数组。我们通过调用链向链提出一个简单的问题。内部发生的情况是，由于此时没有聊天历史，所以问题本质上就是第一个问题。**rag_chain**简单地回答问题并打印答案。我们还通过返回的消息扩展了**chat_history**：
- en: '[PRE53]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This results in the following output:'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE54]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'In this step, we invoke the chain again with a subsequent question, without
    providing many contextual cues. We provide the chain with the chat history and
    print the answer to the second question. Internally, the **rag_chain** and the
    **contextualize_q_chain** work in tandem to answer this question. The **contextualize_q_chain**
    uses the chat history to add more context to the follow-up question, retrieves
    matched records, and sends that as context to the **rag_chain**. The **rag_chain**
    used the context and the contextualized question to answer the subsequent question.
    As we observe from the output, the LLM was able to decipher what **it** means
    in this context:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们再次调用链，提出后续问题，而不提供许多上下文线索。我们向链提供聊天历史并打印第二个问题的答案。内部，**rag_chain**和**contextualize_q_chain**协同工作来回答这个问题。**contextualize_q_chain**使用聊天历史为后续问题添加更多上下文，检索匹配的记录，并将这些作为上下文发送给**rag_chain**。**rag_chain**使用上下文和上下文化的问题来回答后续问题。正如我们从输出中观察到的，LLM能够理解在这个上下文中**它**的含义：
- en: '[PRE55]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'This results in the following output:'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE56]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Note:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：
- en: We provided a basic workflow for how to execute RAG-based flows. We recommend
    referring to the LangChain documentation and using the necessary components to
    run solutions in production. Some of these would include evaluating other vector
    DB stores, using concrete types such as **BaseChatMessageHistory** and **RunnableWithMessageHistory**
    to better manage chat histories. Also, use LangServe to expose endpoints to serve
    requests.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了一个基本的RAG（Retrieval-Augmented Generation）流程执行工作流程。我们建议参考LangChain文档，并使用必要的组件在生产环境中运行解决方案。其中一些包括评估其他向量数据库存储，使用如**BaseChatMessageHistory**和**RunnableWithMessageHistory**等具体类型来更好地管理聊天历史。此外，使用LangServe来公开端点以服务请求。
- en: Generating code using an LLM
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LLM生成代码
- en: In this recipe, we will explore how an LLM can be used to generate code. We
    will use two separate examples to check the breadth of coverage for the generation.
    We will also compare the output from two LLMs to observe how the generation varies
    across two different models. Applications of such methods are already incorporated
    in popular **Integrated Development Environments** (**IDEs**). Our goal here is
    to demonstrate a basic framework for how to use a pre-trained LLM to generate
    code snipped based on simple human-defined requirements.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将探讨如何使用LLM生成代码。我们将使用两个独立的示例来检查生成的覆盖范围。我们还将比较两个不同模型的输出，以观察生成在两个不同模型之间的差异。此类方法的运用已经融入了流行的**集成开发环境**（**IDEs**）。我们的目标是展示如何使用预训练的LLM根据简单的人类定义要求生成代码片段的基本框架。
- en: Getting ready
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use a model from Hugging Face as well as OpenAI in this recipe. Please
    refer to *Model access* under the *Technical requirements* section to complete
    the step to access the Llama and OpenAI models. You can use the `10.6_code_generation_with_llm.ipynb`
    notebook from the code site if you want to work from an existing notebook. Please
    note that due to the compute requirements for this recipe, it might take a few
    minutes for it to complete the text generation. If the required compute capacity
    is unavailable, we recommend referring to the *Using OpenAI models instead of
    local ones section* at the end of this chapter and using the method described
    there to use an OpenAI model for this recipe.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将使用Hugging Face和OpenAI的模型。请参考“技术要求”部分下的“模型访问”，以完成访问Llama和OpenAI模型的步骤。如果您想从一个现有的笔记本开始工作，可以使用代码网站上的`10.6_code_generation_with_llm.ipynb`笔记本。请注意，由于这个配方对计算资源的要求，生成文本可能需要几分钟时间。如果所需的计算能力不可用，我们建议参考本章末尾的“使用本地模型而不是OpenAI模型”部分，并使用那里描述的方法来使用OpenAI模型进行此配方。
- en: How to do it…
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The recipe does the following things:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 该配方执行以下操作：
- en: It initializes a prompt template that instructs the LLM to generate code for
    a given problem statement
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它初始化了一个提示模板，指导LLM（大型语言模型）为给定的问题陈述生成代码
- en: It initializes an LLM model and a tokenizer and wires them together in a pipeline
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它初始化一个LLM模型和一个分词器，并将它们在管道中连接起来
- en: It creates a chain that connects the prompt, LLM and string post-processor to
    generate a code snippet based on a given instruction
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它创建了一个链，将提示、LLM和字符串后处理器连接起来，根据给定的指令生成代码片段
- en: We additionally show the result of the same instructions when executed via an
    OpenAI model
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还展示了通过OpenAI模型执行相同指令的结果
- en: 'The steps for the recipe are as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 配方的步骤如下：
- en: 'Do the necessary imports:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入：
- en: '[PRE57]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: In this step, we define a template. This template defines the instruction or
    the system prompt that is sent to the model as the task description. In this case,
    the template defines an instruction to generate Python code based on users’ requirements.
    We use this template to initialize a prompt object. The initialized object is
    of the **ChatPromptTemplate** type. This object lets us send requirements to the
    model in an interactive way. We can converse with the model based on our instructions
    to generate several code snippets without having to load the model each time.
    Note the **{input}** placeholder in the prompt. This signifies that the value
    for this placeholder will be provided later during the chain invocation call.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们定义了一个模板。这个模板定义了作为任务描述发送给模型的指令或系统提示。在这种情况下，模板定义了一个根据用户要求生成Python代码的指令。我们使用这个模板来初始化一个提示对象。初始化的对象是**ChatPromptTemplate**类型。这个对象允许我们以交互式的方式向模型发送要求。我们可以根据我们的指令与模型进行对话，生成多个代码片段，而无需每次都加载模型。注意提示中的**{input}**占位符。这表示这个占位符的值将在链调用期间稍后提供。
- en: '[PRE58]python'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE58]python'
- en: '....'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '....'
- en: '[PRE59]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Set up the parameters for the model. *Steps 3-5* have been explained in more
    detail in the *Executing a simple prompt-to-LLM chain* recipe earlier in this
    chapter. Please refer to that recipe for more details. We also initialize a configuration
    for quantization. This has been described in more detail in the *Running an LLM
    to follow instructions* recipe in this chapter. To avoid repetition, we recommend
    referring to *step 3* of that recipe:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置模型的参数。*步骤3-5*在本书前面的*执行简单的提示到LLM链*配方中已有更详细的解释。请参阅该配方以获取更多详细信息。我们还初始化了一个量化配置。这在本书的*运行LLM以遵循指令*配方中已有更详细的描述。为了避免重复，我们建议您参考该配方的*步骤3*：
- en: '[PRE60]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Initialize the model. In this instance, as we are working to generate code,
    we use the **Meta-Llama-3.1-8B-Instruct** model. This model also has the ability
    to generate code. For a model of this size, it has demonstrated very good performance
    for code generation:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化模型。在这个例子中，由于我们正在生成代码，我们使用了**Meta-Llama-3.1-8B-Instruct**模型。此模型还具有生成代码的能力。对于这个规模的模型，它在代码生成方面已经展示了非常好的性能：
- en: '[PRE61]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We initialize the pipeline with the model and the tokenizer:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用模型和分词器初始化管道：
- en: '[PRE62]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We initialize the chain with the prompt and the model:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用提示和模型初始化链：
- en: '[PRE63]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We invoke the chain and print the result. As we can see from the output, the
    generated code is reasonably good, with the Node **class** having a constructor
    along with the **inorder_traversal** helper method. It also prints out the instructions
    to use the class. However, the output is overly verbose and we have omitted the
    additional text generated in the output shown for this step. The output contains
    code for preorder traversal too, which we did not instruct the LLM to generate:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们调用链并打印结果。从输出中我们可以看到，生成的代码相当不错，Node **类**具有构造函数以及**inorder_traversal**辅助方法。它还打印出使用类的说明。然而，输出过于冗长，我们省略了此步骤输出中显示的附加文本。输出还包含前序遍历的代码，而我们并没有指示LLM生成：
- en: '[PRE64]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'This generates the following output:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了以下输出：
- en: '[PRE65]python'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE65]python'
- en: '....'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '....'
- en: '[PRE66]python'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE66]python'
- en: 'class Node:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 'class Node:'
- en: 'def __init__(self, value):'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 'def __init__(self, value):'
- en: self.value = value
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: self.value = value
- en: self.left = None
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: self.left = None
- en: self.right = None
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: self.right = None
- en: 'class BinaryTree:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 'class BinaryTree:'
- en: 'def __init__(self):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 'def __init__(self):'
- en: self.root = None
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: self.root = None
- en: 'def insert(self, value):'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 'def insert(self, value):'
- en: 'if self.root is None:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 'if self.root is None:'
- en: self.root = Node(value)
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: self.root = Node(value)
- en: 'else:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: self._insert(self.root, value)
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: self._insert(self.root, value)
- en: 'def _insert(self, node, value):'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 'def _insert(self, node, value):'
- en: 'if value < node.value:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 'if value < node.value:'
- en: 'if node.left is None:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 'if node.left is None:'
- en: node.left = Node(value)
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: node.left = Node(value)
- en: 'else:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: self._insert(node.left, value)
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: self._insert(node.left, value)
- en: 'else:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: 'if node.right is None:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 'if node.right is None:'
- en: node.right = Node(value)
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: node.right = Node(value)
- en: 'else:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: self._insert(node.right, value)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: self._insert(node.right, value)
- en: 'def inorder(self):'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 'def inorder(self):'
- en: result = []
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: result = []
- en: self._inorder(self.root, result)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: self._inorder(self.root, result)
- en: return result
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: return result
- en: 'def _inorder(self, node, result):'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 'def _inorder(self, node, result):'
- en: 'if node is not None:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 'if node is not None:'
- en: self._inorder(node.left, result)
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: self._inorder(node.left, result)
- en: result.append(node.value)
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: result.append(node.value)
- en: self._inorder(node.right, result)
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: self._inorder(node.right, result)
- en: tree = BinaryTree()
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: tree = BinaryTree()
- en: tree.insert(8)
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: tree.insert(8)
- en: tree.insert(3)
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: tree.insert(3)
- en: tree.insert(10)
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: tree.insert(10)
- en: tree.insert(1)
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: tree.insert(1)
- en: tree.insert(6)
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: tree.insert(6)
- en: tree.insert(14)
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: tree.insert(14)
- en: tree.insert(4)
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: tree.insert(4)
- en: tree.insert(7)
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: tree.insert(7)
- en: tree.insert(13)
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: tree.insert(13)
- en: 'print(tree.inorder())  # Output: [1, 3, 4, 6, 7, 8, 10, 13, 14]'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 'print(tree.inorder())  # 输出: [1, 3, 4, 6, 7, 8, 10, 13, 14]'
- en: '[PRE67]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'result = chain.invoke({"input": "write a program to generate a 512-bit SHA3
    hash"})'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'result = chain.invoke({"input": "write a program to generate a 512-bit SHA3
    hash"})'
- en: print(result)
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: print(result)
- en: '[PRE68]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'System: Write some python code to solve the user''s problem. Keep the answer
    as brief as possible.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 'System: Write some python code to solve the user''s problem. Keep the answer
    as brief as possible.'
- en: 'Return only python code in Markdown format, e.g.:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 仅返回Markdown格式的Python代码，例如：
- en: '[PRE69]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Human: write a program to generate a 512-bit SHA3 hash'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 'Human: write a program to generate a 512-bit SHA3 hash'
- en: '[PRE70]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: from langchain_openai import ChatOpenAI
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: from langchain_openai import ChatOpenAI
- en: '[PRE72]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: api_key = os.environ.get('OPENAI_API_KEY')
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: api_key = os.environ.get('OPENAI_API_KEY')
- en: llm = ChatOpenAI(openai_api_key=api_key)
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: llm = ChatOpenAI(openai_api_key=api_key)
- en: '[PRE73]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'result = chain.invoke({"input": " write a program to generate a 512-bit SHA3
    hash"})'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'result = chain.invoke({"input": " write a program to generate a 512-bit SHA3
    hash"})'
- en: print(result)
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: print(result)
- en: '[PRE74]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'result = chain.invoke({"input": "write a program to generate a 512-bit AES
    hash"})'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'result = chain.invoke({"input": "write a program to generate a 512-bit AES
    hash"})'
- en: print(result)
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: print(result)
- en: '[PRE77]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: from langchain_core.prompts import ChatPromptTemplate
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: from langchain_core.prompts import ChatPromptTemplate
- en: from langchain_core.output_parsers import StrOutputParser
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: from langchain_core.output_parsers import StrOutputParser
- en: from langchain_core.runnables import RunnablePassthrough
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: from langchain_core.runnables import RunnablePassthrough
- en: from langchain_community.utilities import SQLDatabase
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: from langchain_community.utilities import SQLDatabase
- en: from langchain_openai import ChatOpenAI
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: from langchain_openai import ChatOpenAI
- en: import os
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: import os
- en: '[PRE80]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'template = """You are a SQL expert. Based on the table schema below, write
    just the SQL query without the results that would answer the user''s question.:'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: template = """您是一位SQL专家。基于以下表结构，仅编写SQL查询，无需包含结果，以回答用户的问题：
- en: '{schema}'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '{schema}'
- en: 'Question: {question}'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Question: {question}'
- en: SQL Query:"""
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SQL Query:"""
- en: prompt = ChatPromptTemplate.from_template(template)
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: prompt = ChatPromptTemplate.from_template(template)
- en: '[PRE81]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: db = SQLDatabase.from_uri(
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: db = SQLDatabase.from_uri(
- en: '"sqlite:///db/northwind-SQLite3/dist/northwind.db")'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"sqlite:///db/northwind-SQLite3/dist/northwind.db")'
- en: '[PRE82]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'def get_schema(_):'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def get_schema(_):'
- en: return db.get_table_info()
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return db.get_table_info()
- en: '[PRE83]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'def run_query(query):'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def run_query(query):'
- en: return db.run(query)
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return db.run(query)
- en: '[PRE84]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: api_key = os.environ.get('OPENAI_API_KEY')
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: api_key = os.environ.get('OPENAI_API_KEY')
- en: model = ChatOpenAI(openai_api_key=api_key)
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: model = ChatOpenAI(openai_api_key=api_key)
- en: '[PRE85]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: sql_response = (
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: sql_response = (
- en: RunnablePassthrough.assign(schema=get_schema)
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RunnablePassthrough.assign(schema=get_schema)
- en: '| prompt'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '| prompt'
- en: '| model.bind(stop=["\nSQLResult:"])'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '| model.bind(stop=["\nSQLResult:"])'
- en: '| StrOutputParser()'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '| StrOutputParser()'
- en: )
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: )
- en: '[PRE86]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'sql_response.invoke({"question": "How many employees are there?"})'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'sql_response.invoke({"question": "How many employees are there?"})'
- en: '[PRE87]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '''SELECT COUNT(*) FROM Employees'''
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '''SELECT COUNT(*) FROM Employees'''
- en: '[PRE88]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'sql_response.invoke({"question": "How many employees have been tenured for
    more than 11 years?"})'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'sql_response.invoke({"question": "How many employees have been tenured for
    more than 11 years?"})'
- en: '[PRE89]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '"SELECT COUNT(*) \nFROM Employees \nWHERE HireDate <= DATE(''now'', ''-5 years'')"'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '"SELECT COUNT(*) \nFROM Employees \nWHERE HireDate <= DATE(''now'', ''-5 years'')"'
- en: '[PRE90]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'template = """Based on the table schema below, question, sql query, and sql
    response, write a natural language response:'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: template = """基于以下表结构，问题，SQL查询和SQL响应，编写一个自然语言响应：
- en: '{schema}'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '{schema}'
- en: 'Question: {question}'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Question: {question}'
- en: 'SQL Query: {query}'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'SQL Query: {query}'
- en: 'SQL Response: {response}"""'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'SQL Response: {response}"""'
- en: prompt_response = ChatPromptTemplate.from_template(template)
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: prompt_response = ChatPromptTemplate.from_template(template)
- en: '[PRE91]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: full_chain = (
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: full_chain = (
- en: RunnablePassthrough.assign(query=sql_response).assign(
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RunnablePassthrough.assign(query=sql_response).assign(
- en: schema=get_schema,
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: schema=get_schema,
- en: 'response=lambda x: run_query(x["query"]),'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'response=lambda x: run_query(x["query"]),'
- en: )
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: )
- en: '| prompt_response'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '| prompt_response'
- en: '| model'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '| model'
- en: )
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: )
- en: '[PRE92]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'result = full_chain.invoke({"question": "How many employees are there?"})'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'result = full_chain.invoke({"question": "How many employees are there?"})'
- en: print(result)
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: print(result)
- en: '[PRE93]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: content='There are 9 employees in the database.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: content='There are 9 employees in the database.'
- en: '[PRE94]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'result = full_chain.invoke({"question": "Give me the name of employees who
    have been tenured for more than 11 years?"})'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'result = full_chain.invoke({"question": "Give me the name of employees who
    have been tenured for more than 11 years?"})'
- en: print(result)
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: print(result)
- en: '[PRE95]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: content='The employees who have been tenured for more than 11 years are Nancy
    Davolio, Andrew Fuller, and Janet Leverling.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: content='The employees who have been tenured for more than 11 years are Nancy
    Davolio, Andrew Fuller, and Janet Leverling.'
- en: '[PRE96]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: from langchain.agents import AgentType, initialize_agent, load_tools
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从 langchain.agents 导入 AgentType, initialize_agent, load_tools
- en: from langchain.agents.tools import Tool
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从 langchain.agents.tools 导入 Tool
- en: from langchain.chains import LLMMathChain
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从 langchain.chains 导入 LLMMathChain
- en: from langchain_experimental.plan_and_execute import (
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从 langchain_experimental.plan_and_execute 导入 (
- en: PlanAndExecute, load_agent_executor, load_chat_planner)
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: PlanAndExecute, load_agent_executor, load_chat_planner)
- en: from langchain.utilities import SerpAPIWrapper
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从 langchain.utilities 导入 SerpAPIWrapper
- en: from langchain_openai import OpenAI
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从 langchain_openai 导入 OpenAI
- en: '[PRE97]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'api_key = ''OPEN_API_KEY'' # set your OPENAI API key'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'api_key = ''OPEN_API_KEY'' # set your OPENAI API key'
- en: 'serp_api_key=''SERP API KEY'' # set your SERPAPI key'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'serp_api_key=''SERP API KEY'' # set your SERPAPI key'
- en: llm = OpenAI(api_key=api_key, temperature=0)
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: llm = OpenAI(api_key=api_key, temperature=0)
- en: '[PRE98]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: search_helper = SerpAPIWrapper(serpapi_api_key=serp_api_key)
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: search_helper = SerpAPIWrapper(serpapi_api_key=serp_api_key)
- en: math_helper = LLMMathChain.from_llm(llm=llm, verbose=True)
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: math_helper = LLMMathChain.from_llm(llm=llm, verbose=True)
- en: '[PRE99]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: search_tool = Tool(name='Search', func=search_helper.run,
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: search_tool = Tool(name='Search', func=search_helper.run,
- en: description="use this tool to search for information")
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: description="use this tool to search for information")
- en: math_tool = Tool(name='Calculator', func=math_helper.run,
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: math_tool = Tool(name='Calculator', func=math_helper.run,
- en: description="use this tool for mathematical calculations")
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: description="use this tool for mathematical calculations")
- en: '[PRE100]'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: tools = [search_tool, math_tool]
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: tools = [search_tool, math_tool]
- en: '[PRE101]'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: action_planner = load_chat_planner(llm)
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: action_planner = load_chat_planner(llm)
- en: '[PRE102]'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: agent_executor = load_agent_executor(llm, tools, verbose=True)
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: agent_executor = load_agent_executor(llm, tools, verbose=True)
- en: '[PRE103]'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: agent = PlanAndExecute(planner=action_planner,
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: agent = PlanAndExecute(planner=action_planner,
- en: executor=agent_executor, verbose=True)
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: executor=agent_executor, verbose=True)
- en: '[PRE104]'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: agent.invoke("How many more FIFA world cup wins does Brazil have compared to
    France?")
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: agent.invoke("How many more FIFA world cup wins does Brazil have compared to
    France?")
- en: '[PRE105]'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: Entering new PlanAndExecute chain...
  id: totrans-398
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 正在进入新的 PlanAndExecute 链...
- en: steps=[Step(value='Gather data on the number of FIFA world cup wins for Brazil
    and France.'), Step(value='Calculate the difference between the two numbers.'),
    Step(value='Output the difference as the answer.\n')]
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: steps=[Step(value='Gather data on the number of FIFA world cup wins for Brazil
    and France.'), Step(value='Calculate the difference between the two numbers.'),
    Step(value='Output the difference as the answer.\n')]
- en: Entering new AgentExecutor chain...
  id: totrans-400
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 正在进入新的 AgentExecutor 链...
- en: 'Action:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 'Action:'
- en: '{'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: '"action": "Search",'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '"action": "Search",'
- en: '"action_input": "Number of FIFA world cup wins for Brazil and France"'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '"action_input": "Number of FIFA world cup wins for Brazil and France"'
- en: '}'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: Finished chain.
  id: totrans-406
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 链接完成。
- en: '*****'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '*****'
- en: 'Step: Gather data on the number of FIFA world cup wins for Brazil and France.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 'Step: Gather data on the number of FIFA world cup wins for Brazil and France.'
- en: 'Response: Action:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 'Response: Action:'
- en: '{'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: '"action": "Search",'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '"action": "Search",'
- en: '"action_input": "Number of FIFA world cup wins for Brazil and France"'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '"action_input": "Number of FIFA world cup wins for Brazil and France"'
- en: '}'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: Entering new AgentExecutor chain...
  id: totrans-414
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 正在进入新的 AgentExecutor 链...
- en: 'Thought: I can use the Calculator tool to subtract the number of wins for France
    from the number of wins for Brazil.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 思考：我可以使用计算器工具从巴西的胜利次数中减去法国的胜利次数。
- en: 'Action:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 'Action:'
- en: '[PRE106]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: Entering new LLMMathChain chain...
  id: totrans-418
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 正在进入新的 LLMMathChain 链...
- en: 5 - 2[PRE107]
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 5 - 2[PRE107]
- en: '...numexpr.evaluate("5 - 2")...'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '...numexpr.evaluate("5 - 2")...'
- en: 'Answer: 3'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：3
- en: Finished chain.
  id: totrans-422
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 链接完成。
- en: 'Observation: Answer: 3'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 观察结果：答案：3
- en: 'Thought: I have the final answer now.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 思考：我现在有了最终答案。
- en: 'Action:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 'Action:'
- en: '[PRE108]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: Finished chain.
  id: totrans-427
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 链接完成。
- en: '*****'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '*****'
- en: 'Step: Calculate the difference between the two numbers.'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 'Step: Calculate the difference between the two numbers.'
- en: 'Response: The difference between the number of FIFA world cup wins for Brazil
    and France is 3.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 'Response: The difference between the number of FIFA world cup wins for Brazil
    and France is 3.'
- en: Entering new AgentExecutor chain...
  id: totrans-431
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 正在进入新的 AgentExecutor 链...
- en: 'Action:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 'Action:'
- en: '{'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: '"action": "Final Answer",'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '"action": "Final Answer",'
- en: '"action_input": "The difference between the number of FIFA world cup wins for
    Brazil and France is 3."'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '"action_input": "The difference between the number of FIFA world cup wins for
    Brazil and France is 3."'
- en: '}'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: Finished chain.
  id: totrans-437
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 链接完成。
- en: '*****'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '*****'
- en: 'Step: Output the difference as the answer.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 'Step: Output the difference as the answer.'
- en: 'Response: Action:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 'Response: Action:'
- en: '{'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: '"action": "Final Answer",'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '"action": "Final Answer",'
- en: '"action_input": "The difference between the number of FIFA world cup wins for
    Brazil and France is 3."'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '"action_input": "The difference between the number of FIFA world cup wins for
    Brazil and France is 3."'
- en: '}'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: Finished chain.
  id: totrans-445
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 链接完成。
- en: '{''input'': ''How many more FIFA world cup wins does Brazil have compared to
    France?'','
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '{''input'': ''How many more FIFA world cup wins does Brazil have compared to
    France?'','
- en: '''output'': ''Action:\n{\n  "action": "Final Answer",\n  "action_input": "The
    difference between the number of FIFA world cup wins for Brazil and France is
    3."\n}\n\n''}'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '''输出'': ''操作：\n{\n  "操作": "最终答案"，\n  "操作输入": "巴西和法国获得FIFA世界杯胜利次数的差是3。"\n}\n\n''}'
- en: '[PRE109]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: import getpass
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: import getpass
- en: from langchain_openai import ChatOpenAI
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: from langchain_openai import ChatOpenAI
- en: os.environ["OPENAI_API_KEY"] = getpass.getpass()
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: os.environ["OPENAI_API_KEY"] = getpass.getpass()
- en: llm = ChatOpenAI(model="gpt-4o-mini")
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: llm = ChatOpenAI(model="gpt-4o-mini")
- en: '```'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '```'
- en: This completes our chapter on generative AI and LLMs. We have just scratched
    the surface of what is possible via generative AI; we hope that the examples presented
    in this chapter help illuminate the capabilities of LLMs and their relation to
    generative AI. We recommend exploring the LangChain site for updates and new tools
    and agents for their use cases and applying them in production scenarios following
    the established best practices. New models are frequently added on the Hugging
    Face site and we recommend staying up to date with the latest model updates and
    their related use cases. This makes it easier to become effective NLP practitioners.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们对生成式AI和LLMs的章节。我们刚刚触及了通过生成式AI可能实现的一小部分；我们希望本章中提供的示例有助于阐明LLMs的能力及其与生成式AI的关系。我们建议探索LangChain网站以获取更新和新工具以及它们的应用场景，并在生产环境中遵循既定的最佳实践来应用它们。Hugging
    Face网站上经常添加新的模型，我们建议关注最新的模型更新及其相关用例。这使得成为有效的NLP从业者变得更加容易。
