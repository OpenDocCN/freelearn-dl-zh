- en: Encoding Inputs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码输入
- en: 'In this chapter, we will be covering the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Need for encoding
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码的需求
- en: Encoding an image
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码图像
- en: Encoding for recommender systems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于推荐系统的编码
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: A typical image is comprised thousands of pixels; text is also comprised thousands
    of unique words, and the number of distinct customers of a company could be in
    the millions. Given this, all three—user, text, and images—would have to be represented
    as a vector in thousands of dimensional planes. The drawback of representing a
    vector in such a high dimensional space is that we will not able to calculate
    the similarity of vectors efficiently.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一幅典型的图像由数千个像素组成；文本也由数千个独特单词组成，而公司的独特客户数量可能达到百万级。考虑到这一点，用户、文本和图像三者都必须表示为数千个维度平面中的向量。在这样一个高维空间中表示向量的缺点在于，我们将无法有效计算向量之间的相似性。
- en: Representing an image, text, or user in a lower dimension helps us in grouping
    entities that are very similar. Encoding is a way to perform unsupervised learning
    to represent an input in a lower dimension with minimal loss of information while
    retaining the information about images that are similar.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 表示图像、文本或用户在较低维度中有助于我们将非常相似的实体分组。编码是执行无监督学习的一种方法，以最小信息损失的方式将输入表示为较低维度，同时保留与相似图像有关的信息。
- en: 'In this chapter, we will be learning about the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下内容：
- en: Encoding an image to a much a lower dimension
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像编码到更低维度
- en: Vanilla autoencoder
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 香草自编码器
- en: Multilayer autoencoder
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多层自编码器
- en: Convolutional autoencoder
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积自编码器
- en: Visualizing encodings
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化编码
- en: Encoding users and items in recommender systems
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推荐系统中编码用户和项目
- en: Calculating the similarity between encoded entities
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算编码实体之间的相似性
- en: Need for encoding
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码的需求
- en: Encoding is typically used where the number of dimensions in a vector is huge.
    Encoding helps turn a large vector into a vector that has far fewer dimensions
    without losing much information from the original vector. In the following sections,
    let's explore the need for encoding images, text, and recommender systems.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 编码通常用于向量维度巨大的情况。编码有助于将大向量转换为具有较少维度的向量，同时不会从原始向量中丢失太多信息。在接下来的几节中，让我们探讨编码图像、文本和推荐系统的需求。
- en: Need for encoding in text analysis
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分析中的编码需求
- en: 'To understand the need for encoding in text analysis, let''s consider the following
    scenario. Let''s go through the following two sentences:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解文本分析中编码的必要性，让我们考虑以下情景。让我们看看以下两个句子：
- en: '![](img/295eeeca-8cb4-4c90-b9a8-c303c76c4fdf.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/295eeeca-8cb4-4c90-b9a8-c303c76c4fdf.png)'
- en: 'In traditional text analysis, the preceding two sentences are one-hot encoded,
    as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的文本分析中，前两个句子被独热编码如下：
- en: '![](img/26bdf572-bf24-4ad1-9c8d-30c1cc2c3814.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26bdf572-bf24-4ad1-9c8d-30c1cc2c3814.png)'
- en: Note that there are five unique words in the two sentences.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这两个句子中有五个唯一单词。
- en: 'The preceding one-hot encoded versions of the words result in an encoded version
    of sentences as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 单词的独热编码版本导致句子的编码版本如下：
- en: '![](img/87b77716-a1e5-4266-b56f-37c4292347a9.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87b77716-a1e5-4266-b56f-37c4292347a9.png)'
- en: In the preceding scenario, we can see that the Euclidian distance between the
    two sentences is greater than zero, as the encodings of **like** and **enjoy**
    are different. However, intuitively, we know that the words enjoy and like are
    very similar to each other. Further, the distance between (**I**, **Chess**) is
    the same as (**like**, **enjoy**).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述情景中，我们可以看到两个句子之间的欧几里德距离大于零，因为**like**和**enjoy**的编码是不同的。然而，直观上，我们知道like和enjoy这两个词非常相似。此外，**I**和**Chess**之间的距离与**like**和**enjoy**之间的距离相同。
- en: Note that, given that there are five unique words across the two sentences,
    we represent each word in a five-dimensional space. In an encoded version, we
    represent a word in a lower dimension (let's say, three-dimensions) in such a
    way that words that are similar will have less distance between them when compared
    to words that are not similar.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，鉴于这两个句子中有五个唯一单词，我们将每个单词表示为五维空间中的一个单词。在编码版本中，我们以较低维度（比如三维）表示一个单词，以使相似的单词之间的距离较小，而不是相似的单词之间的距离较大。
- en: Need for encoding in image analysis
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分析中编码的需求
- en: 'To understand the need for encoding in image analysis, let''s consider the
    scenario where we group images; however, the labels of images are not present.
    For further clarification, let''s consider the following images of the same label
    in the MNIST dataset:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解图像分析中对编码的需求，我们来考虑一个场景：我们对图像进行分组，但图像的标签并不存在。为了进一步澄清，我们来看一下MNIST数据集中相同标签的以下图像：
- en: '![](img/a63ab262-0216-4f6e-82c4-5a3d9254d6f0.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a63ab262-0216-4f6e-82c4-5a3d9254d6f0.png)'
- en: Intuitively, we know that both the preceding images correspond to the same label.
    However, when we take the Euclidian distance between the preceding two images,
    the distance is greater than zero, as different pixels are highlighted in the
    preceding two images.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，我们知道前面这两张图片对应的是相同的标签。然而，当我们计算这两张图片之间的欧几里得距离时，距离大于零，因为这两张图片中突出显示的像素不同。
- en: 'You should notice the following issue in storing the information of an image:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该注意到在存储图像信息时存在以下问题：
- en: While the image comprises a total of 28 x 28 = 784 pixels, the majority of the
    columns are black and thus no information is composed in them, resulting in them
    occupying more space while storing information than is needed.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管图像由总共28 x 28 = 784个像素组成，但大部分列是黑色的，因此这些列没有包含信息，导致它们在存储信息时占用了比实际需要更多的空间。
- en: Using autoencoders, we represent the preceding two images in a lower dimension
    in such a way that the distance between the two encoded versions is now much smaller
    and at the same time ensuring that the encoded version does not lose much information
    from the original image.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自动编码器，我们将前面的两张图片表示为较低维度，这样编码版本之间的距离会变得更小，同时确保编码版本不会丢失太多原始图像的信息。
- en: Need for encoding in recommender systems
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐系统中对编码的需求
- en: To understand the need for encoding in recommender systems, let's consider the
    scenario of movie recommendations for customers. Similar to text analysis, if
    we were to one-hot encode each movie/customer, we would end up with multiple thousand-dimensional
    vectors for each movie (as there are thousands of movies). Encoding users in a
    much lower dimension based on the viewing habits of customers, which results in
    grouping movies based on the similarity of movies, could help us map movies that
    a user is more likely to watch.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解推荐系统中对编码的需求，我们来考虑顾客电影推荐的场景。类似于文本分析，如果我们对每部电影/顾客进行独热编码，我们将为每部电影（因为电影数量成千上万）得到多个千维的向量。基于顾客的观看习惯，将用户编码到更低的维度，并根据电影的相似性对电影进行分组，这样可以帮助我们推荐顾客更可能观看的电影。
- en: A similar concept can also be applied to e-commerce recommendation engines,
    as well as recommending products to a customer in a supermarket.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的概念也可以应用于电子商务推荐引擎，以及在超市中向顾客推荐商品。
- en: Encoding an image
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码一张图片
- en: Image encoding can be performed in multiple ways. In the following sections,
    we will contrast the performance of vanilla autoencoders, multilayer autoencoders,
    and convolutional autoencoders. The term auto-encoding refers to encoding in such
    a way that the original input is recreated with a far fewer number of dimensions
    in an image.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图像编码可以通过多种方式进行。在接下来的章节中，我们将对比普通自动编码器、多层自动编码器和卷积自动编码器的性能。自动编码一词指的是以一种方式进行编码，使得原始输入可以在图像中用更少的维度重建。
- en: An autoencoder takes an image as input and encodes the input image into a lower
    dimension in such a way that we can reconstruct the original image by using only
    the encoded version of the input image. Essentially, you can think of the encoded
    version of similar images as having similar encoded values.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器将图像作为输入，并将输入图像编码为较低的维度，这样我们就可以仅使用输入图像的编码版本来重建原始图像。本质上，你可以认为相似图像的编码版本具有相似的编码值。
- en: Getting ready
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Before we define our strategy, let''s get a feel for how autoencoders work:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们定义策略之前，让我们先了解一下自动编码器是如何工作的：
- en: We'll define a toy dataset that has one vector with 11 values
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将定义一个包含11个值的玩具数据集
- en: 'We''ll represent the 11 values in a lower dimension (two-dimensions):'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将把这11个值表示为较低维度（二维）：
- en: The information present in input data is preserved as much as possible while
    lowering the dimensions
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在降低维度的同时，尽可能保留输入数据中存在的信息
- en: The vector in low dimensional space is called an **embedding**/**encoded** **vector**,
    **bottleneck** **feature**/**vector**, or a **compressed representation**
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低维空间中的向量称为**嵌入**/**编码** **向量**、**瓶颈** **特征**/**向量**，或者是**压缩表示**
- en: The 11 values are converted into two values by performing a matrix multiplication
    of input values with a random weight matrix that is 11 x 2 in dimensions
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将输入值与一个维度为11 x 2的随机权重矩阵进行矩阵乘法，11个值被转换为两个值。
- en: The lower dimension vector represents bottleneck features. Bottleneck features
    are features that are required to reconstruct the original image
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较低维度的向量表示瓶颈特征。瓶颈特征是重建原始图像所需的特征。
- en: 'We''ll reconstruct the lower dimension bottleneck feature vector to obtain
    the output vector:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将重建较低维度的瓶颈特征向量，以获得输出向量：
- en: The two-dimension feature vector is multiplied by a matrix that is 2 x 11 in
    shape to obtain an output that is 1 x 11 in shape. Matrix multiplication of 1
    x 2 with 2 x 11 vectors gives an output that is 1 x 11 in shape.
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二维特征向量与一个形状为2 x 11的矩阵相乘，得到一个形状为1 x 11的输出。1 x 2与2 x 11向量的矩阵乘法将得到一个1 x 11形状的输出。
- en: We'll calculate the sum of squared difference between the input vector and the
    output vector
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将计算输入向量和输出向量之间的平方差之和：
- en: We vary the randomly initialized weight vectors to minimize the sum of squared
    difference between the input and output vectors
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过调整随机初始化的权重向量来最小化输入和输出向量之间的平方差之和。
- en: The resulting encoded vector would be a lower dimensional vector that represents
    an 11-dimensional vector in two-dimensional space
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果编码向量将是一个低维度的向量，表示二维空间中的11维向量。
- en: While leveraging neural networks, you can consider the encoded vector as a hidden
    layer that connects the input and output layer.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在利用神经网络时，您可以将编码向量视为连接输入层和输出层的隐藏层。
- en: Additionally, for the neural network, the input and output layer values are
    exactly the same and the hidden layer has a lower dimension than the input layer.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于神经网络，输入层和输出层的值是完全相同的，隐藏层的维度低于输入层。
- en: 'In this recipe, we''ll learn about multiple autoencoders:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将了解多种自编码器：
- en: Vanilla autoencoder
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vanilla自编码器
- en: Multilayer autoencoder
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多层自编码器
- en: Convolutional autoencoder
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积自编码器
- en: How to do it...
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: In the following sections, we will implement multiple variations of autoencoders
    in Python (the code file is available as `Auto_encoder.ipynb` in GitHub).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将实现多种自编码器的变种（代码文件可在GitHub上的`Auto_encoder.ipynb`中找到）。
- en: Vanilla autoencoder
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Vanilla自编码器
- en: 'A vanilla autoencoder looks as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Vanilla自编码器长得如下：
- en: '![](img/d67047e4-6d9e-4cd9-bccf-fdd5f74f2ea5.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d67047e4-6d9e-4cd9-bccf-fdd5f74f2ea5.png)'
- en: As displayed in the preceding diagram, a Vanilla autoencoder reconstructs the
    input with a minimal number of hidden layers and hidden units in its network.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，Vanilla自编码器使用最少的隐藏层和隐藏单元来重建输入数据。
- en: 'To understand how a vanilla autoencoder works, let''s go through the following
    recipe, where we reconstruct MNIST images using a lower-dimensional encoded version
    of the original image (the code file is available as `Auto_encoder.ipynb` in GitHub):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解Vanilla自编码器如何工作，让我们按照以下步骤操作，其中我们使用原始图像的低维编码版本来重建MNIST图像（代码文件可在GitHub上的`Auto_encoder.ipynb`中找到）：
- en: 'Import the relevant packages:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包：
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the dataset:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入数据集：
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Reshape and scale the dataset:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重塑并缩放数据集：
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Construct the network architecture:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建网络架构：
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A summary of model is as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的摘要如下：
- en: '![](img/20ebaedb-619c-4ea0-a329-6281c3dc9837.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20ebaedb-619c-4ea0-a329-6281c3dc9837.png)'
- en: In the preceding code, we are representing a 784-dimensional input in a 32-dimensional
    encoded version.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将一个784维的输入表示为一个32维的编码版本。
- en: 'Compile and fit the model:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译并拟合模型：
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that we are using the mean squared error loss function, as the pixel values
    are continuous. Additionally, the input and output arrays are just the same—`X_train`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用均方误差损失函数，因为像素值是连续的。此外，输入和输出数组是相同的——`X_train`。
- en: 'Print a reconstruction of the first four input images:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印前四个输入图像的重建结果：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The reconstructured MNIST digits are as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 重建后的MNIST数字如下：
- en: '![](img/5f536303-198e-479f-a4d6-b3d57a3b67a4.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f536303-198e-479f-a4d6-b3d57a3b67a4.png)'
- en: 'To understand how well the autoencoder worked, let''s compare the preceding
    predictions with the original input images:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解自编码器的效果如何，我们来比较一下之前的预测和原始输入图像：
- en: 'The original MNIST digits are as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的MNIST数字如下：
- en: '![](img/17100fd3-9b8d-42bd-8eab-da2d28d86814.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17100fd3-9b8d-42bd-8eab-da2d28d86814.png)'
- en: From the preceding images, we can see that the reconstructed images are blurred
    when compared to the original input image.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图像中，我们可以看到，重建的图像与原始输入图像相比有些模糊。
- en: To get around the issue of blurring, let's build multilayer autoencoders that
    are deep (thereby resulting in more parameters) and thus potentially a better
    representation of the original image.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免模糊问题，我们来构建更深的多层自编码器（从而产生更多的参数），这样可以更好地表示原始图像。
- en: Multilayer autoencoder
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层自编码器
- en: 'A multilayer autoencoder looks as follows, where there are more number of hidden
    layers connecting the input layer to output layer:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 多层自编码器如下所示，其中有更多的隐藏层将输入层与输出层连接起来：
- en: '![](img/1f3ee687-42fa-453a-b0a0-93c0bf13c546.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f3ee687-42fa-453a-b0a0-93c0bf13c546.png)'
- en: Essentially, a multilayer autoencoder reconstructs the input with more hidden
    layers in its network.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，多层自编码器通过更多的隐藏层来重建输入。
- en: 'To build a multilayer autoencoder, we will repeat the same steps that we had
    in the previous section, up until *step 3*. However, *step 4*, where the network
    architecture is defined, will be modified to include multilayers, as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建多层自编码器，我们将重复前一节中的相同步骤，直到*步骤 3*。然而，*步骤 4*，即定义网络架构的部分，将被修改为包含多层，如下所示：
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'A summary of model is as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的摘要如下：
- en: '![](img/2314656c-b2fc-4c71-a031-206d5f870de8.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2314656c-b2fc-4c71-a031-206d5f870de8.png)'
- en: In the preceding network, our first hidden layer has 100 units, the second hidden
    layer (which is the embedded version of the image) is 32-dimensional, and the
    third hidden layer is 100-dimensional in shape.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述网络中，我们的第一个隐藏层有100个单元，第二个隐藏层（即图像的嵌入版本）是32维的，第三个隐藏层是100维的。
- en: 'Once the network architecture is defined, we compile and run it, as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦网络架构定义完成，我们就可以编译并运行它，步骤如下：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The predictions of the preceding model are as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 上述模型的预测结果如下：
- en: '![](img/ff4c5081-bf2f-4346-9439-33d59b9a2bd4.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff4c5081-bf2f-4346-9439-33d59b9a2bd4.png)'
- en: Note that the preceding predictions are still a little blurred compared to the
    original images.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与原始图像相比，之前的预测结果仍然有些模糊。
- en: Convolutional autoencoder
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积自编码器
- en: So far, we have explored vanilla and multilayer autoencoders. In this section,
    we will see how convolutional autoencoders work in reconstructing the original
    images from a lower-dimensional vector.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了传统和多层自编码器。在本节中，我们将看到卷积自编码器如何从低维向量中重建原始图像。
- en: 'Convolutional autoencoders look as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积自编码器如下所示：
- en: '![](img/3cd826e0-e037-416a-992e-7ab29296eb34.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3cd826e0-e037-416a-992e-7ab29296eb34.png)'
- en: Essentially, a convolutional autoencoder reconstructs the input with more hidden
    layers in its network where the hidden layers consist of convolution, pooling,
    and upsampling the downsampled image.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，卷积自编码器通过更多的隐藏层来重建输入，其中隐藏层包括卷积、池化以及对下采样图像的上采样。
- en: Similar to a multilayer autoencoder, a convolutional autoencoder differs from
    other types of autoencoder in its model architecture. In the following code, we
    will define the model architecture for the convolutional autoencoder while every
    other step remains similar to the vanilla autoencoder up until *step 3*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于多层自编码器，卷积自编码器与其他类型的自编码器在模型架构上有所不同。在以下代码中，我们将定义卷积自编码器的模型架构，而其他步骤与传统自编码器保持一致，直到*步骤
    3*。
- en: 'The only differences between the `X_train` and `X_test` shapes are defined
    as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`X_train` 和 `X_test` 形状之间唯一的区别如下所示：'
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Note that, in the preceding step, we are reshaping the image so that it can
    be passed to a `conv2D` method:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的步骤中，我们正在重塑图像，以便将其传递给 `conv2D` 方法：
- en: 'Define the model architecture:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型架构：
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the preceding code, we have defined a convolutional architecture where we
    reshaped the input image so that it has a 32-dimensional embedded version in the
    middle of its architecture and finally upsample it so that we are able to reconstruct
    it.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们定义了一个卷积架构，将输入图像重塑为具有32维嵌入版本的图像，该嵌入版本位于架构的中间，最后进行上采样，从而使我们能够重建该图像。
- en: 'A summary of the model is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的摘要如下：
- en: '![](img/008c615f-9a99-4eb9-9a39-141aa90aeda8.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/008c615f-9a99-4eb9-9a39-141aa90aeda8.jpg)'
- en: Compile and fit the model
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译并拟合模型
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once we make predictions on the first four test data points, the reconstructed
    images look as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对前四个测试数据点进行预测，重建后的图像如下所示：
- en: '![](img/ede7d605-daf3-4166-a7ae-c42a38c8a464.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ede7d605-daf3-4166-a7ae-c42a38c8a464.png)'
- en: Note that the reconstruction is now slightly better than the previous two reconstructions
    (using Vanilla and multilayer autoencoders) of the test images.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当前的重构效果略优于之前使用Vanilla和多层自编码器对测试图像进行的两个重构。
- en: Grouping similar images
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将相似的图像分组
- en: In the previous sections, we represented each image in a much lower dimension
    with the intuition that images that are similar will have similar embeddings and
    images that are not similar will have dissimilar embeddings. However, we have
    not yet looked at the similarity measure or examined embeddings in detail.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们将每个图像表示为较低维度的向量，直觉是相似的图像会有相似的嵌入，而不相似的图像则会有不同的嵌入。然而，我们还没有考察相似度度量，也没有详细检查嵌入。
- en: 'In this section, we will try and plot embeddings in a 2D space. We can reduce
    the 32-dimensional vector to a two-dimensional space by using a technique called
    **t-SNE**. (More about t-SNE can be found here: [http://www.jmlr.org/papers/v9/vandermaaten08a.html](http://www.jmlr.org/papers/v9/vandermaaten08a.html).)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将尝试在二维空间中绘制嵌入。我们可以使用一种叫做**t-SNE**的技术，将32维向量降维到二维空间。（更多关于t-SNE的内容可以参考这里：[http://www.jmlr.org/papers/v9/vandermaaten08a.html](http://www.jmlr.org/papers/v9/vandermaaten08a.html)。）
- en: This way, our feeling that similar images will have similar embeddings can be
    proved, as similar images should be clustered together in the two-dimensional
    plane.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们的直觉——相似的图像会有相似的嵌入——可以得到验证，因为相似的图像应该聚类在二维平面上。
- en: 'In the following code, we will represent embeddings of all the test images
    in a two-dimensional plane:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们将所有测试图像的嵌入表示为二维平面：
- en: 'Extract the 32-dimensional vector of each of the 10,000 images in the test:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取测试集中每个10,000张图像的32维向量：
- en: '[PRE11]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Perform t-SNE to generate a two-dimensional vector:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行t-SNE生成二维向量：
- en: '[PRE12]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Plot the visualization of the t-SNE dimensions for the test image embeddings:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制测试图像嵌入的t-SNE维度可视化：
- en: '[PRE13]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'A visualization of embeddings in two dimensional space is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 二维空间中嵌入的可视化如下所示：
- en: '![](img/9f76bf7a-b13e-4f60-85bd-a035f6b14dd5.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f76bf7a-b13e-4f60-85bd-a035f6b14dd5.png)'
- en: Note that, in the preceding chart, we see that, more often than not, clusters
    are formed among images that correspond to the same label.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在上面的图表中，我们可以看到，相同标签的图像通常会形成簇。
- en: Encoding for recommender systems
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐系统的编码
- en: So far, in the previous sections, we have encoded an image. In this section,
    we will encode users and movies in a movie-related dataset. The reason for this
    is that there could be millions of users as customers and thousands of movies
    in a catalog. Thus, we are not in a position to one-hot encode such data straight
    away. Encoding comes in handy in such a scenario. One of the most popular techniques
    that's used in encoding for recommender systems is matrix factorization. In the
    next section, we'll understand how it works and generate embeddings for users
    and movies.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在前面的章节中，我们对图像进行了编码。在本节中，我们将对电影相关数据集中的用户和电影进行编码。原因在于，可能会有数百万个用户和成千上万部电影在目录中。因此，我们不能直接对这些数据进行独热编码。在这种情况下，编码就显得尤为重要。矩阵分解是推荐系统中常用的编码技术之一。在下一节中，我们将理解它的工作原理，并为用户和电影生成嵌入。
- en: Getting ready
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The thinking behind encoding users and movies is as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 编码用户和电影的思路如下：
- en: If two users are similar in terms of liking certain movies, the vectors that
    represent the two users should be similar. In the same manner, if two movies are
    similar (potentially, they belong to the same genre or have the same cast), they
    should have similar vectors.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个用户在喜欢某些电影方面相似，那么表示这两个用户的向量应该相似。类似地，如果两部电影相似（可能属于同一类型或有相同的演员阵容），它们的向量应该相似。
- en: 'The strategy that we''ll adopt to encode movies, so that we recommend a new
    set of movies based on the historical set of movies watched by a user, is as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用的电影编码策略，目的是根据用户观看过的历史电影推荐一组新的电影，具体如下：
- en: Import the dataset that contains information of the users and the rating they
    gave to different movies that they watched
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入包含用户信息及他们给不同电影打分的数据集
- en: Assign IDs to both users and movies
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为用户和电影分配ID
- en: Convert users and movies into 32-dimensional vectors
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将用户和电影转换为32维向量
- en: 'Use the functional API in Keras to perform the dot product of the 32-dimensional
    vectors of movies and users:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Keras的功能API执行电影和用户的32维向量的点积：
- en: If there are 100,000 users and 1,000 movies, the movie matrix will be 1,000
    x 32 dimensions and the user matrix will be 100,000 x 32 dimensions
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果有100,000个用户和1,000部电影，则电影矩阵将是1,000 x 32维，而用户矩阵将是100,000 x 32维。
- en: The dot product of the two will be 100,000 x 1,000 in dimension
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两者的点积将是100,000 x 1,000的维度。
- en: Flatten the output and pass it through a dense layer, before connecting to the
    output layer, which has a linear activation and has output values ranging from
    1 to 5
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出展平并通过一个密集层，然后连接到输出层，输出层具有线性激活，输出值的范围从1到5。
- en: Fit the model
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Extract the embedding weights of movies
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取电影的嵌入权重
- en: Extract the embedding weights of users
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取用户的嵌入权重
- en: Movies that are similar to a given movie of interest can be found by calculating
    the pairwise similarity of the movie of interest with every other movie in the
    dataset
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过计算目标电影与数据集中其他所有电影的成对相似度，找到与给定电影相似的电影。
- en: How to do it...
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In the following code, we will come up with with a vector for a user and a
    movie in a typical recommender system (The code file is available as `Recommender_systems.ipynb` in
    GitHub):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们将为一个用户和一部电影设计一个向量，这在典型的推荐系统中使用（代码文件可以在GitHub的`Recommender_systems.ipynb`中找到）：
- en: Import the dataset. The recommended dataset is available in code in GitHub.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入数据集。推荐的数据集可以在GitHub上的代码中找到。
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](img/922112db-3673-419c-ae17-295a587c0051.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/922112db-3673-419c-ae17-295a587c0051.jpg)'
- en: 'Convert the user and movies into a categorical variable. In the following code,
    we create two new variables—`User2` and `Movies2`—which are categorical:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将用户和电影转换为分类变量。在下面的代码中，我们创建了两个新的变量——`User2` 和 `Movies2`——它们是分类变量：
- en: '[PRE15]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Assign a unique ID to each user and movie:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个用户和电影分配一个唯一的ID：
- en: '[PRE16]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Add the unique IDs as new columns to our original table:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将唯一的ID作为新列添加到原始表中：
- en: '[PRE17]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Define embeddings for each user ID and unique ID:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个用户ID和唯一ID定义嵌入：
- en: '[PRE18]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the preceding code, we are extracting the total number of unique users and
    unique movies in the dataset:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们正在提取数据集中唯一用户和唯一电影的总数：
- en: '[PRE19]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In the preceding code, we are defining a function that takes an ID as input
    and converts it into an embedding vector that is `n_out` in dimensions for the
    total of `n_in` values:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们定义了一个函数，输入一个ID，将其转换为一个嵌入向量，该向量的维度为`n_out`，总共有`n_in`个值：
- en: '[PRE20]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the preceding code, we are extracting 100 dimensions for each unique user
    and also for each unique movie.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们正在为每个唯一用户以及每个唯一电影提取100个维度。
- en: 'Define the model:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型：
- en: '[PRE21]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'A summary of model is as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 模型摘要如下：
- en: '![](img/f135011c-a121-4c89-868a-a4d1f89a3baa.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f135011c-a121-4c89-868a-a4d1f89a3baa.jpg)'
- en: 'Fit the model:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE22]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Extract the vectors of each user or movie:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取每个用户或电影的向量：
- en: '[PRE23]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As we thought earlier, movies that are similar should have similar vectors.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所想，类似的电影应该有相似的向量。
- en: Typically, while identifying the similarity between embeddings, we use a measure
    named cosine similarity (there's more information on how cosine similarity is
    calculated in the next chapter).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在识别嵌入之间的相似性时，我们使用一种称为余弦相似度的度量（如何计算余弦相似度的更多信息将在下一章中介绍）。
- en: 'For a randomly selected movie that is located in the 574^(th) position, cosine
    similarity is calculated as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个随机选中的电影，其位于第574^(个)位置，余弦相似度计算如下：
- en: '[PRE24]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: From the preceding code, we can calculate the ID that is most similar to the
    movie located in the 574^(th) location of the categorical movie column.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以计算出与位于分类电影列中第574^(个)位置的电影最相似的ID。
- en: Once we look into the movie ID list, we should see that the most similar movies
    to the given movie, indeed happen to be similar, intuitively.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们查看电影ID列表，我们应该会看到与给定电影最相似的电影，直观上它们确实是相似的。
