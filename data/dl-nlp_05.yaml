- en: '*Chapter 5*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第五章*'
- en: Recurrent Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将能够：
- en: Describe classical feedforward networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述经典的前馈神经网络
- en: Differentiate between feedforward neural networks and recurrent neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分前馈神经网络和循环神经网络
- en: Evaluate the application of backpropagation through time for recurrent neural
    networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估通过时间反向传播应用于循环神经网络的效果
- en: Describe the drawbacks of recurrent neural networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述循环神经网络的缺点
- en: Use recurrent neural networks with keras to solve the author attribution problem
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keras 中的循环神经网络解决作者归属问题
- en: This chapter aims to introduce you to recurrent neural networks and their applications,
    as well as their drawbacks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在介绍循环神经网络及其应用，并讨论它们的缺点。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: We encounter different kinds of data in our day-to-day lives, and some of this
    data has temporal dependencies (dependencies over time) while some does not. For
    example, an image by itself contains the information it wants to convey. However,
    data forms such as audio and video have dependencies over time. They cannot convey
    information if a fixed point in time is taken into consideration. Based on the
    problem statement, the input that's needed in order to solve the problem can differ.
    If we have a model to detect a particular person in a frame, a single image can
    be used as input. However, if we need to detect their actions, we need a stream
    of images, contiguous in time, as the input. We can understand the person's actions
    by analyzing these images together, but not independently.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在日常生活中会遇到不同种类的数据，其中一些数据具有时间依赖性（随时间变化的依赖关系），而另一些则没有。例如，一张图片本身就包含了它想要传达的信息。然而，音频和视频等数据形式则具有时间依赖性。如果只考虑一个固定的时间点，它们无法传递信息。根据问题的描述，解决问题所需的输入可能会有所不同。如果我们有一个模型来检测某个特定人物在一帧中的出现，那么可以使用单张图片作为输入。然而，如果我们需要检测他们的动作，我们需要一系列连续的图像作为输入。我们可以通过分析这些图像来理解一个人的动作，但不能仅仅通过单独的图像来分析。
- en: While watching a movie, a particular scene makes sense because its context is
    known, and we remember all the information gathered before in the movie to understand
    the current scene. This is very important, and we, as humans, can do this because
    our brains can store memory, analyze past data, and retrieve useful information
    to understand the current scene.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在观看电影时，某个特定场景之所以能够理解，是因为已知其上下文，并且我们记住了电影中之前收集的所有信息，来理解当前场景。这一点非常重要，而我们作为人类之所以能够做到这一点，是因为我们的脑袋能够存储记忆，分析过去的数据，并提取有用信息以理解当前的场景。
- en: Networks such as multi-layered perceptron and convolutional neural networks
    lack this capability. Every input given to these networks is treated independently,
    and they don't store any information from past inputs to analyze the current inputs
    because they lack memory in their architecture. That being the case, maybe there
    is a way we can enable neural networks to have memory. We can try and make them
    store useful information from the past and make them retrieve information from
    the past that helps them to analyze the current input. This is indeed possible,
    and the architecture for it is called the **Recurrent** **Neural** **Network**
    (**RNN**).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 像多层感知机和卷积神经网络这样的网络缺乏这种能力。它们对每个输入都视为独立处理，并且不会存储任何来自过去输入的信息来分析当前输入，因为它们在架构上缺乏记忆功能。在这种情况下，也许我们可以让神经网络具有记忆功能。我们可以尝试让它们存储过去的有用信息，并从过去获取有助于分析当前输入的信息。这是完全可能的，其架构被称为**循环神经网络**（**RNN**）。
- en: 'Before we delve deep into the theory of RNNs, let''s take a look at their applications.
    Currently, RNNs are widely used. Some of the applications are as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解 RNN 的理论之前，我们先来看一下它们的应用。目前，RNN 被广泛应用。以下是一些应用：
- en: '*Speech recognition*: Whether it''s Amazon''s Alexa, Apple''s Siri, Google''s
    voice assistant, or Microsoft''s Cortana, all their speech recognition systems
    use RNNs.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语音识别*：无论是亚马逊的 Alexa，苹果的 Siri，谷歌的语音助手，还是微软的 Cortana，它们的所有语音识别系统都使用 RNN。'
- en: '*Time series predictions*: Any application with time series data, such as stock
    market data, website traffic, call center traffic, movie recommendations, Google
    Maps routes, and so on, uses RNNs to predict future data, the optimal path, optimal
    resource allocations, and so on.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间序列预测*：任何具有时间序列数据的应用程序，如股市数据、网站流量、呼叫中心流量、电影推荐、Google Maps 路线等等，都使用 RNN 来预测未来数据、最佳路径、最佳资源分配等。'
- en: '*Natural language processing*: Applications such as machine translation (for
    Google Translate, for instance), chatbots (such as those for Slack and Google),
    and question answering all use RNNs to model dependencies.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自然语言处理*：机器翻译（例如Google Translate）、聊天机器人（如Slack和Google的聊天机器人）以及问答系统等应用都使用RNN来建模依赖关系。'
- en: Previous Versions of Neural Networks
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络的早期版本
- en: Around 40 years ago, it became clear that **Feed** **Forward** **Neural** **Networks**
    (**FFNNs**) could not capture time-variable dependencies, which are essential
    for capturing the time-variable properties of a signal. Modeling time-variable
    dependencies is very important in many applications involving real-world data,
    such as speech and video, in which data has time-variable properties. Also, human
    biological neural networks have a recurrent relationship, so it is the most obvious
    direction to take. How could this recurrent relationship be added to existing
    feedforward networks?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大约40年前，人们发现**前馈神经网络**（**FFNNs**）无法捕捉时间变化的依赖关系，而这对于捕捉信号的时间变化特性至关重要。建模时间变化的依赖关系在许多涉及现实世界数据的应用中非常重要，例如语音和视频，这些数据具有时间变化的特性。此外，人类生物神经网络具有递归关系，因此这是最明显的发展方向。如何将这种递归关系添加到现有的前馈网络中呢？
- en: One of the first attempts to achieve this was done by adding delay elements,
    and the network was called the **Time-Delay** **Neural** **Network**, or **TDNN**
    for short.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一目标的首次尝试之一是通过添加延迟元素，网络被称为**时延神经网络**，简称**TDNN**。
- en: In this network, as the following figure shows, the delay elements are added
    to the network and the past inputs are given to the network along with the current
    timestep as the input to the network. This definitely has an advantage over the
    traditional feed forward networks but has the disadvantage of having only so many
    inputs from the past as the window allows. If the window is too large, the network
    grows with increasing parameters and computational complexities.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个网络中，正如下图所示，延迟元素被添加到网络中，过去的输入与当前时刻一起作为网络的输入。与传统的前馈网络相比，这种方法无疑具有优势，但也有一个缺点，即只能接收来自过去的有限输入，这取决于窗口的大小。如果窗口太大，网络随着参数的增加而增长，计算复杂度也随之增加。
- en: '![Figure 5.1: TDNN structure'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.1：TDNN结构'
- en: '](img/C13783_5_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_01.jpg)'
- en: 'Figure 5.1: TDNN structure'
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.1：TDNN结构
- en: Then came Elman networks, or simple RNNs. Elman networks are very similar to
    feedforward networks, except that the hidden layer of output is stored and used
    for the next input. This way, information from the previous timesteps can be captured
    in these hidden states.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 随后出现了Elman网络，或称为简单RNN（**Simple RNN**）。Elman网络与前馈网络非常相似，不同之处在于其输出的隐藏层会被存储并用于下一个输入。这样，前一个时刻的信息可以在这些隐藏状态中被捕获。
- en: One way of looking at Elman networks is that at each input, we append the previous
    hidden layers' outputs along with the inputs and send them all as the inputs to
    the network. So, if the input size is **m** and the hidden layer size is **n**,
    the effective input layer size becomes **m+n**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 观察Elman网络的一种方式是，在每个输入时，我们将前一个隐藏层的输出与当前输入一起附加，并将它们作为网络的输入。因此，如果输入大小是**m**，隐藏层大小是**n**，则有效的输入层大小变为**m+n**。
- en: The following figure shows a simple three-layer network, where the previous
    state is fed back to the network to store the context, and therefore it is called
    **SimpleRNN**. There are other variations to this architecture, such as Jordan
    networks, which we will not study in this chapter. For those are interested in
    the early history of RNNs, reading more on Elman networks and Jordan networks
    might be the best place to start.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了一个简单的三层网络，其中之前的状态被反馈到网络中以存储上下文，因此称之为**SimpleRNN**。这种架构有其他变种，例如Jordan网络，我们在本章中不会学习这些变种。对于那些对RNN早期历史感兴趣的人来说，阅读更多关于Elman网络和Jordan网络的资料可能是一个很好的起点。
- en: '![Figure 5.2: SimpleRNN structure'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.2：SimpleRNN结构'
- en: '](img/C13783_5_02.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_02.jpg)'
- en: 'Figure 5.2: SimpleRNN structure'
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.2：SimpleRNN结构
- en: And then came the **RNN**, which is the topic of this chapter. We will look
    into RNNs in detail in the coming sections It is important to note that in recurrent
    networks, since there are memory units and weights associated to these units,
    they need to be learned during backpropagation. Since these gradients are also
    backpropagated through time, we call it **Back** **Propagation** **Through** **Time**,
    or **BPTT**. We will discuss BPTT in detail in the upcoming sections. However,
    TDNN, Elman networks, and RNNs have a major drawback due to BPTT, and it is called
    vanishing gradients. Vanishing gradients is a problem where gradients get smaller
    and smaller as they backpropagate, and in these networks, as timesteps increase,
    back-propagated gradients get smaller and smaller, resulting in vanishing gradients.
    It's almost impossible to capture time dependencies greater than 20 timesteps.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后就出现了 **RNN**，这是本章的主题。我们将在接下来的章节中详细探讨 RNN。需要注意的是，在递归网络中，由于存在与这些单位相关的记忆单元和权重，这些内容需要在反向传播中学习。由于这些梯度也通过时间进行反向传播，因此我们称之为
    **通过时间的反向传播**（**BPTT**）。我们将在后续章节中详细讨论 BPTT。然而，由于 BPTT，TDNN、Elman 网络和 RNN 存在一个主要缺陷，这个问题被称为梯度消失。梯度消失是指梯度在反向传播时越来越小，在这些网络中，随着时间步长的增加，反向传播的梯度越来越小，最终导致梯度消失。捕捉超过
    20 个时间步长的时间依赖性几乎是不可能的。
- en: To address this issue, an architecture called the **Long** **Short-Term** **Memory**
    (**LSTM**) architecture was introduced. The key idea here is to hold some cell
    states constant and introduce them as needed in future timesteps. These decisions
    are made by gates, including forget gates and output gates. Another commonly used
    variant of the LSTM is called the **Gated** **Recurrent** **Unit**, or **GRU**
    for short. Don't worry much if you didn't understand this completely. There are
    two chapters following that are dedicated to making these concepts clear.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，引入了一种名为 **长短期记忆**（**LSTM**）的架构。这里的关键思想是保持一些细胞状态不变，并根据需要将其引入到未来的时间步长中。这些决策由门控机制完成，包括遗忘门和输出门。LSTM
    的另一种常见变体叫做 **门控递归单元**（**GRU**），简称 **GRU**。如果你还没有完全理解这些概念，不用太担心。接下来有两章内容专门讲解这些概念。
- en: RNNs
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN
- en: Recurrent often means occurring repeatedly. The recurrent part of RNNs simply
    means that the same task is done over all the inputs in the input sequence (for
    RNNs, we give a sequence of timesteps as the input sequence). One main difference
    between feed forward networks and RNNs is that RNNs have memory elements called
    states that capture the information from the previous inputs. So, in this architecture,
    the current output not only depends on the current input, but also on the current
    state, which takes into account past inputs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '"递归"通常意味着反复发生。RNN 的递归部分简单来说就是在输入序列中的所有输入上执行相同的任务（对于 RNN，我们将时间步长序列作为输入序列）。前馈网络和
    RNN 之间的一个主要区别是，RNN 拥有称为状态的记忆单元，用于捕获来自前一个输入的信息。因此，在这个架构中，当前的输出不仅依赖于当前输入，还依赖于当前的状态，而该状态则考虑了过去的输入。'
- en: RNNs are trained by sequences of inputs rather than a single input; similarly,
    we can consider each input to an RNN as a sequence of timesteps. The state elements
    in RNNs contain information about past inputs to process the current input sequence.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 通过输入序列而不是单一输入进行训练；同样，我们也可以将 RNN 的每个输入视为时间步长的序列。RNN 中的状态单元包含有关过去输入的信息，以处理当前的输入序列。
- en: '![Figure 5.3: RNN structure'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.3：RNN 结构'
- en: '](img/C13783_5_03.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_03.jpg)'
- en: 'Figure 5.3: RNN structure'
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.3：RNN 结构
- en: For each input in the input sequence, the RNN gets a state, calculates its output,
    and sends its state to the next input in the sequence. The same set of tasks is
    repeated for all the elements in the sequence.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入序列中的每个输入，RNN 获取一个状态，计算其输出，并将其状态传递给序列中的下一个输入。对于序列中的所有元素，都会重复执行相同的任务。
- en: It's easy to understand RNNs and their operations by comparing them to feedforward
    networks. Let's do that now.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 RNN 与前馈网络进行比较，我们可以更容易理解 RNN 及其运作方式。现在就让我们来进行这样的比较。
- en: By now, it's very clear that the inputs are independent of each other in feedforward
    neural networks, so we train the network by randomly drawing pairs of inputs and
    outputs. There is no significance to the sequence. At any given time, the output
    is a function of input and weights.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，很明显，在前馈神经网络中，输入彼此之间是独立的，因此我们通过随机抽取输入和输出的配对来训练网络。序列本身没有任何重要性。在任何给定的时刻，输出只是输入和权重的函数。
- en: '![Figure 5.4: Expression for the output of an RNN'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.4：RNN 输出的表达式'
- en: '](img/C13783_05_04.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_05_04.jpg)'
- en: 'Figure 5.4: Expression for the output of an RNN'
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.4：RNN 的输出表达式
- en: 'In RNNs, our output at time **t** depends not only on the current input and
    the weight, but also on previous inputs. In this case, the output at time **t**
    will be defined as shown:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RNN 中，时间 **t** 的输出不仅依赖于当前输入和权重，还依赖于之前的输入。在这种情况下，时间 **t** 的输出定义如下：
- en: '![Figure 5.5: Expression for the output of an RNN at time t'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.5：RNN 在时间 t 的输出表达式'
- en: '](img/C13783_05_05.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_05_05.jpg)'
- en: 'Figure 5.5: Expression for the output of an RNN at time t'
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.5：RNN 在时间 t 的输出表达式
- en: Let's look at a simple structure of an RNN that is called a folded model. In
    the following figure, the **S****t** state vector is fed back into the network
    from the previous timestep. One important takeaway from this representation is
    that RNNs share the same weight matrices across timesteps. By increasing the timesteps,
    we are not learning more parameters, but we are looking at a bigger sequence.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个简单的 RNN 结构，称为折叠模型。在下图中，**S**<sub>t</sub> 状态向量从前一个时间步反馈到网络。这个表示法的一个重要启示是，RNN
    在各个时间步之间共享相同的权重矩阵。通过增加时间步，我们并不是在学习更多的参数，而是在查看更大的序列。
- en: '![Figure 5.6: Folded model of an RNN'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.6：RNN 的折叠模型'
- en: '](img/C13783_5_06.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_06.jpg)'
- en: 'Figure 5.6: Folded model of an RNN'
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.6：RNN 的折叠模型
- en: 'This is a folded model of an RNN:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 RNN 的折叠模型：
- en: '**Xt** : Current input vector in the input sequence'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**Xt**：输入序列中的当前输入向量'
- en: '**Yt**: Current output vector in the output sequence'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**Yt**：输出序列中的当前输出向量'
- en: '**St**: Current state vector'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**St**：当前状态向量'
- en: '**Wx**: Weight matrix connecting the input vector to the state vector'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**Wx**：连接输入向量到状态向量的权重矩阵'
- en: '**Wy**: Weight matrix connecting the state vector to the output vector'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**Wy**：连接状态向量到输出向量的权重矩阵'
- en: '**Ws**: Weight matrix connecting the state vector of previous timestep to the
    next one'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**Ws**：连接前一时间步的状态向量到下一时间步的权重矩阵'
- en: Since the input, **x** is a sequence of timesteps and we perform the same task
    for elements in this sequence, we can unfold this model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于输入 **x** 是一个时间步序列，并且我们对该序列中的每个元素执行相同的任务，因此我们可以展开该模型。
- en: '![Figure 5.7: Unfolding of an RNN'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.7：RNN 的展开'
- en: '](img/C13783_5_07.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_07.jpg)'
- en: 'Figure 5.7: Unfolding of an RNN'
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.7：RNN 的展开
- en: For example, the output at time **t+1**,**y****t+1** depends on input at time
    **t+1**, weight matrices, and all the inputs before it.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，时间 **t+1** 的输出，**y**<sub>t+1</sub>，依赖于时间 **t+1** 的输入、权重矩阵以及之前的所有输入。
- en: '![Figure 5.8: Unfolded RNN'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.8：展开的 RNN'
- en: '](img/C13783_5_08.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_08.jpg)'
- en: 'Figure 5.8: Unfolded RNN'
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.8：展开的 RNN
- en: Since RNNs are extensions of FFNNs, it's best to understand the differences
    between these architectures.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 RNN 是 FFNN 的扩展，理解这两种架构之间的差异非常重要。
- en: '![Figure 5.9: Differences between FFNNs and RNNs'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.9：FFNN 和 RNN 的差异'
- en: '](img/C13783_5_09.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_09.jpg)'
- en: 'Figure 5.9: Differences between FFNNs and RNNs'
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.9：FFNN 和 RNN 的差异
- en: 'The output expressions for FFNNs and RNNs are as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: FFNN 和 RNN 的输出表达式如下：
- en: '![Figure 5.10: Output expressions for FFNNs and RNNs'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.10：FFNN 和 RNN 的输出表达式'
- en: '](img/C13783_5_10.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_10.jpg)'
- en: 'Figure 5.10: Output expressions for FFNNs and RNNs'
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.10：FFNN 和 RNN 的输出表达式
- en: From the previous figure and equations, it is very evident that there are a
    lot of similarities between these two architectures. In fact, they are the same
    if **Ws=0**. This is obviously the case since **Ws** is the weight associated
    with the state that is fed back to the network. Without **Ws**, there is no feedback,
    which is the basis of the RNN.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图和公式可以明显看出，这两种架构之间有很多相似之处。事实上，如果 **Ws=0**，它们是相同的。显然是这种情况，因为 **Ws** 是与反馈到网络的状态相关的权重。没有
    **Ws** 就没有反馈，这是 RNN 的基础。
- en: In FFNNs, the output at **t** depends on the input at **t** and weight matrices.
    In RNNs, the output at **t** depends on input at **t**, **t-1**, **t-2**, and
    so on, as well as the weight matrices. This is explained with the further calculation
    of hidden vector **h** in the case of an FFNN and **s** in the case of an RNN.
    At first glance, it might look like the state at **t** depends on the input at
    **t**, the state at **t-1**, and the weight matrices; and the state at **t-1**
    depends on the input at **t-1**, the state at **t-2**, and so on; creating a chain
    that goes back all the way to the first timestep considered. The output calculations
    of both FFNNs and RNNs are same, though.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在 FFNN（前馈神经网络）中，输出依赖于**t**时刻的输入和权重矩阵。在 RNN 中，输出不仅依赖于**t**时刻的输入，还依赖于**t-1**、**t-2**等时刻的输入，以及权重矩阵。这可以通过进一步计算隐藏向量**h**（对于
    FFNN）和**s**（对于 RNN）来解释。乍一看，似乎**t**时刻的状态依赖于**t**时刻的输入、**t-1**时刻的状态和权重矩阵；而**t-1**时刻的状态依赖于**t-1**时刻的输入、**t-2**时刻的状态，依此类推，形成一个从第一时刻开始回溯的链条。不过，FFNN
    和 RNN 的输出计算是相同的。
- en: RNN Architectures
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RNN 架构
- en: RNNs can come in many forms, and the appropriate architecture needs to be chosen
    depending on the problem we are solving.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: RNN（循环神经网络）可以有多种形式，具体使用哪种架构需要根据我们要解决的问题来选择。
- en: '![Figure 5.11 Different architectures of RNNs'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.11 不同架构的 RNN'
- en: '](img/C13783_05_11.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_05_11.jpg)'
- en: Figure 5.11 Different architectures of RNNs
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.11 不同架构的 RNN
- en: '*One to many*: In this architecture, a single input is given, and the output
    is a sequence. An example of this is image captioning, where the input is a single
    image, and the output is a sequence of words explaining the image.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*一对多*：在这种架构中，给定一个单一的输入，输出是一个序列。一个例子是图像描述，其中输入是单一的图像，输出是一系列描述图像的单词。'
- en: '*Many to one*: In this architecture, a sequence of inputs is given, but a single
    output is expected. An example is any time series prediction where the next timestep
    in the sequence needs to be predicted, given the previous timesteps.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*多对一*：在这种架构中，给定一个输入序列，但期望一个单一的输出。一个例子是时间序列预测，其中需要预测下一个时刻的值，基于之前的时刻。'
- en: '*Many to many*: In this architecture, an input sequence is given to the network,
    and the network outputs a sequence. In this case, the sequence can be either synced
    or not synced. For example, in machine translation, the whole sentence needs to
    be fed in before the networks starts to translate it. Sometimes, the input and
    output are not in sync; for example, in the case of speech enhancement, where
    an audio frame is given as input and a cleaner version of the input frame is the
    output expected. In such cases, the input and output are in sync.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*多对多*：在这种架构中，输入序列被提供给网络，网络输出一个序列。在这种情况下，序列可以是同步的，也可以是不同步的。例如，在机器翻译中，整个句子需要先输入网络，然后网络才开始进行翻译。有时，输入和输出不是同步的；例如，在语音增强中，输入是一个音频帧，而输出是该音频帧的清晰版本。在这种情况下，输入和输出是同步的。'
- en: RNNs can also be stacked on top of each other. It is important to note that
    each RNN in the stack has its own weight matrices. So, the weight matrices are
    shared on the horizontal axis (the time axis) and not on the vertical axis (the
    number of RNNs).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 也可以堆叠在一起。需要注意的是，每个堆叠中的 RNN 都有自己的权重矩阵。因此，权重矩阵在横向（时间轴）上是共享的，而不是在纵向（RNN 数量轴）上共享的。
- en: '![Figure 5.12: Stacked RNNs'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.12: 堆叠的 RNN'
- en: '](img/C13783_5_11.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_11.jpg)'
- en: 'Figure 5.12: Stacked RNNs'
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.12: 堆叠的 RNN'
- en: BPTT
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BPTT
- en: RNNs can deal with varying sequence lengths, can be used in different forms,
    and can be stacked on top of each other. Previously, you have come across the
    back propagation technique to backpropagate loss values to adjust weights. In
    the case of RNNs, something similar can be done, with a bit of a twist, which
    is a gate loss through time. It's called **BPTT**.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 可以处理不同长度的序列，能以不同形式使用，且可以堆叠在一起。之前，你已经遇到过反向传播技术，用于反向传播损失值以调整权重。对于 RNN，也可以进行类似的操作，不过稍有不同，那就是通过时间传递的门控损失。它被称为**BPTT**（反向传播通过时间）。
- en: 'From the basic theory of back propagation, we know the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 根据反向传播的基本理论，我们知道以下内容：
- en: '![Figure 5.13: Expression for weight update'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.13: 权重更新的表达式'
- en: '](img/C13783_05_13.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_05_13.jpg)'
- en: 'Figure 5.13: Expression for weight update'
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.13: 权重更新的表达式'
- en: 'The update value is calculated through gradient calculations using the chain
    rule:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 更新值是通过链式法则的梯度计算得出的：
- en: '![Figure 5.14 Partial derivative of error with regards to weight'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.14 权重的误差偏导数'
- en: '](img/C13783_05_14.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_05_14.jpg)'
- en: Figure 5.14 Partial derivative of error with regards to weight
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.14 误差相对于权重的偏导数
- en: Here, **α** is the learning rate. The partial derivative of **Error** (**loss**)
    with respect to the weight matrix is the main calculation. Once this new matrix
    is obtained, adjusting the weight matrices is simply adding this new matrix, scaled
    by a learning factor, to itself.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，**α** 是学习率。误差（**损失**）相对于权重矩阵的偏导数是主要的计算。获得新的矩阵后，调整权重矩阵就是将这个新矩阵按学习因子缩放后加到原矩阵上。
- en: When calculating the update values for RNNs, we will use BPTT.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算 RNN 的更新值时，我们将使用 BPTT。
- en: 'Let''s look at an example to understand this better. Consider a loss function,
    such as the mean squared error (which is commonly used for regression problems):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来更好地理解这一点。考虑一个损失函数，例如均方误差（常用于回归问题）：
- en: '![Figure 5.15: Loss function'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.15：损失函数'
- en: '](img/C13783_5_15.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_15.jpg)'
- en: 'Figure 5.15: Loss function'
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.15：损失函数
- en: 'At timestep **t = 3**, the loss calculated is as shown:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间步 **t = 3** 时，计算得到的损失如图所示：
- en: '![Figure 5.16 Loss at time t=3'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.16 时间 t=3 时的损失'
- en: '](img/C13783_05_16.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_05_16.jpg)'
- en: Figure 5.16 Loss at time t=3
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.16 时间 t=3 时的损失
- en: This loss needs to be backpropagated, and the **Wy**, **Wx**, and **Ws** weights
    need to be updated.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失需要进行反向传播，**Wy**、**Wx** 和 **Ws** 权重需要更新。
- en: As seen previously, we need to calculate the update value to adjust these weights,
    and this update value can be calculated using partial derivatives and the chain
    rule.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们需要计算更新值来调整这些权重，这个更新值可以通过偏导数和链式法则来计算。
- en: 'There are three parts to doing this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此操作有三个部分：
- en: Update Weight **Wy** by calculating the partial derivative of the error with
    respect to **Wy**
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过计算误差相对于 **Wy** 的偏导数来更新权重 **Wy**
- en: Update Weight **Ws** by calculating the partial derivative of the error with
    respect to **Ws**
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过计算误差相对于 **Ws** 的偏导数来更新权重 **Ws**
- en: Update Weight **Wx** by calculating the partial derivative of the error with
    respect to **Wx**
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过计算误差相对于 **Wx** 的偏导数来更新权重 **Wx**
- en: Before we look at these updates, let's unroll the model and keep the part of
    the network that's actually relevant for our calculations.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看这些更新之前，先将模型展开，并保留对我们计算有实际意义的网络部分。
- en: '![Figure 5.17 Unfolded RNN with loss at time t=3'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.17 展开后的 RNN，时间 t=3 时的损失'
- en: '](img/C13783_5_18.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_18.jpg)'
- en: Figure 5.17 Unfolded RNN with loss at time t=3
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.17 展开后的 RNN，时间 t=3 时的损失
- en: Since we are looking at how loss at **t=3** affects the weight matrices, the
    loss values at and previous to **t=2** are not relevant. Now, we need to understand
    how to backpropagate this loss through the network.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们关注的是时间 **t=3** 时的损失如何影响权重矩阵，时间 **t=2** 及之前的损失值不再相关。现在，我们需要理解如何将损失反向传播通过网络。
- en: Let's look at each of these updates and show the gradient flow for each of the
    updates shown in the preceding figure.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来逐个查看这些更新，并展示前图中每个更新的梯度流动。
- en: Updates and Gradient Flow
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新与梯度流
- en: 'The updates can be listed as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 更新可以列出如下：
- en: Adjusting weight matrix **Wy**
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整权重矩阵 **Wy**
- en: Adjusting weight matrix **Ws**
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整权重矩阵 **Ws**
- en: For updating **Wx**
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新 **Wx** 的过程
- en: Adjusting Weight Matrix **Wy**
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调整权重矩阵 **Wy**
- en: 'The model can be visualized as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型可以通过如下方式进行可视化：
- en: '![Figure 5.18: Back propagation of loss through weight matrix Wy'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.18：通过权重矩阵 **Wy** 对损失进行反向传播'
- en: '](img/C13783_5_19.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_19.jpg)'
- en: 'Figure 5.18: Back propagation of loss through weight matrix Wy'
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.18：通过权重矩阵 **Wy** 对损失进行反向传播
- en: 'For Wy, the update is very simple since there are no additional paths or variables
    between Wy and the error. The matrix can be realized as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 **Wy**，更新非常简单，因为 **Wy** 和误差之间没有其他路径或变量。该矩阵可以按以下方式表示：
- en: '![Figure 5.19: Expression for weight matrix Wy'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.19：权重矩阵 **Wy** 的表达式'
- en: '](img/C13783_05_19.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_05_19.jpg)'
- en: 'Figure 5.19: Expression for weight matrix Wy'
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.19：权重矩阵 **Wy** 的表达式
- en: Adjusting Weight Matrix **Ws**
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调整权重矩阵 **Ws**
- en: '![Figure 5.20: Back propagation of loss through weight matrix Ws with respect
    to S3'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.20：通过权重矩阵 **Ws** 对损失进行反向传播，关于 S3'
- en: '](img/C13783_5_20.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_20.jpg)'
- en: 'Figure 5.20: Back propagation of loss through weight matrix Ws with respect
    to S3'
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.20：通过权重矩阵 **Ws** 对损失进行反向传播，关于 S3
- en: 'We can calculate the partial derivate of error with respect to **Ws** using
    the chain rule, as shown in the previous figure. It looks like that is what is
    needed, but it''s important to remember that **S****t**is dependent on **S****t-1**,
    and therefore **S****3** is dependent on **S****2**, so we need to consider **S****2**
    also, as shown here:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用链式法则计算误差关于**Ws**的偏导数，如前面的图所示。看起来这就是所需的，但重要的是要记住，**S****t**依赖于**S****t-1**，因此**S****3**依赖于**S****2**，所以我们还需要考虑**S****2**，如图所示：
- en: '![Figure 5.21: Back propagation of loss through weight matrix Ws with respect
    to S2'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.21：通过权重矩阵 Ws 对 S2 进行损失的反向传播'
- en: '](img/C13783_5_21.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_21.jpg)'
- en: 'Figure 5.21: Back propagation of loss through weight matrix Ws with respect
    to S2'
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.21：通过权重矩阵 Ws 对 S2 进行损失的反向传播
- en: 'Again, **S****2** in turn depends on **S****1**, and therefore **S****1** needs
    to be considered, too, as shown here:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，**S****2**依赖于**S****1**，因此也需要考虑**S****1**，如图所示：
- en: '![Figure 5.22: Back propagation of loss through weight matrix Ws with respect
    to S1'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.22：通过权重矩阵 Ws 对 S1 进行损失的反向传播'
- en: '](img/C13783_5_22.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_22.jpg)'
- en: 'Figure 5.22: Back propagation of loss through weight matrix Ws with respect
    to S1'
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.22：通过权重矩阵 Ws 对 S1 进行损失的反向传播
- en: 'At **t=3**, we must consider the contribution of state **S****3** to the error,
    the contribution of state **S****2** to the error, and the contribution of state
    **S****1** to the error, **E****3**. The final value looks like this:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在**t=3**时，我们必须考虑状态**S****3**对误差的贡献，状态**S****2**对误差的贡献，以及状态**S****1**对误差的贡献，**E****3**。最终的值如下所示：
- en: '![Figure 5.23: Sum of all derivatives of error with respect to Ws at t=3'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.23：t=3 时关于 Ws 的所有误差导数之和'
- en: '](img/C13783_5_23.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_23.jpg)'
- en: 'Figure 5.23: Sum of all derivatives of error with respect to Ws at t=3'
  id: totrans-151
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.23：t=3 时关于 Ws 的所有误差导数之和
- en: 'In general, for timestep **N**, all the contributions of the previous timesteps
    need to be considered. So, the general formula looks like this:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，对于时间步**N**，需要考虑之前时间步的所有贡献。因此，一般公式如下所示：
- en: '![Figure 5.24: General expression for the derivative of error with respect
    to Ws'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.24：关于 Ws 的误差导数的一般表达式'
- en: '](img/C13783_5_24.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_24.jpg)'
- en: 'Figure 5.24: General expression for the derivative of error with respect to
    Ws'
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.24：关于 Ws 的误差导数的一般表达式
- en: For Updating **Wx**
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于更新**Wx**
- en: We can calculate the partial derivate of error with respect to **Wx** using
    the chain rule, as shown in the next few figures. With the same reasoning that
    **S****t** is dependent on **S****t-1**, the calculation of partial derivative
    of error with respect to **Wx** can be divided into three stages at **t=3**.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用链式法则计算误差关于**Wx**的偏导数，如接下来的几张图所示。基于**S****t**依赖于**S****t-1**的相同推理，误差关于**Wx**的偏导数计算可以分为三个阶段，在**t=3**时进行。
- en: '![Figure 5.25: Back propagation of loss through weight matrix Wx with respect
    to S2'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.25：通过权重矩阵 Wx 对 S2 进行损失的反向传播'
- en: '](img/C13783_5_25.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_25.jpg)'
- en: 'Figure 5.25: Back propagation of loss through weight matrix Wx with respect
    to S2'
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.25：通过权重矩阵 Wx 对 S2 进行损失的反向传播
- en: 'Back propagation of loss through weight matrix Wx with respect to S2:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 通过权重矩阵 Wx 对 S2 进行损失的反向传播：
- en: '![Figure 5.26: Back propagation of loss through weight matrix Wx with respect
    to S2'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.26：通过权重矩阵 Wx 对 S2 进行损失的反向传播'
- en: '](img/C13783_5_26.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_26.jpg)'
- en: 'Figure 5.26: Back propagation of loss through weight matrix Wx with respect
    to S2'
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.26：通过权重矩阵 Wx 对 S2 进行损失的反向传播
- en: 'Back propagation of loss through weight matrix Wx with respect to S1:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 通过权重矩阵 Wx 对 S1 进行损失的反向传播：
- en: '![Figure 5.27: Back propagation of loss through weight matrix Wx with respect
    to S1'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.27：通过权重矩阵 Wx 对 S1 进行损失的反向传播'
- en: '](img/C13783_5_27.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_27.jpg)'
- en: 'Figure 5.27: Back propagation of loss through weight matrix Wx with respect
    to S1'
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.27：通过权重矩阵 Wx 对 S1 进行损失的反向传播
- en: 'Similar to the previous discussion, at **t=3**, we must consider the contribution
    of state **S****3** to the error, the contribution of state **S****2** to the
    error, and the contribution of state **S****1** to the error, **E****3**. The
    final value looks like this:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于前面的讨论，在**t=3**时，我们必须考虑状态**S****3**对误差的贡献，状态**S****2**对误差的贡献，以及状态**S****1**对误差的贡献，**E****3**。最终的值如下所示：
- en: '![Figure 5.28: Sum of all derivatives of error with respect to Wx at t=3'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.28：t=3 时关于 Wx 的所有误差导数之和'
- en: '](img/C13783_5_28.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_28.jpg)'
- en: 'Figure 5.28: Sum of all derivatives of error with respect to Wx at t=3'
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.28: 在 t=3 时关于 Wx 的所有误差导数之和'
- en: 'In general, for timestep N, all the contributions of the previous timesteps
    need to be considered. So, the general formula looks as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，对于时间步 N，需要考虑前面所有时间步的贡献。因此，通用公式如下所示：
- en: '![Figure 5.29: General expression of derivative of error with respect to Wx'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.29: 关于 Wx 的误差导数的通用表达式'
- en: '](img/C13783_5_29.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_29.jpg)'
- en: 'Figure 5.29: General expression of derivative of error with respect to Wx'
  id: totrans-176
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.29: 关于 Wx 的误差导数的通用表达式'
- en: Since the chain of derivatives already has 5 multiplicative terms at **t=3**,
    this number grows to 22 multiplicative terms for timestep 20\. It's possible that
    each of these derivatives could be either greater than 0 or less than 0\. Due
    to consecutive multiplications with longer timesteps, the total derivative gets
    smaller or larger. This problem is either vanishing gradients or exploding gradients.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于链式导数在**t=3**时已经有 5 个相乘项，到第 20 时间步时，这个数量增长到了 22 个相乘项。每一个导数可能大于 0 或小于 0。由于连续乘法和更长的时间步，总导数会变得更小或更大。这个问题即为消失梯度或爆炸梯度。
- en: Gradients
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度
- en: 'The two types of gradients that have been identified are:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 已识别的两种梯度类型是：
- en: Exploding gradients
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爆炸梯度
- en: Vanishing gradients
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消失梯度
- en: Exploding Gradients
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 爆炸梯度
- en: As the name indicates, this happens when gradients explode to much bigger values.
    This could be one of the problems that RNN architectures could encounter with
    larger timesteps. This could happen when each of the partial derivatives is larger
    than **1**, and multiplication of these partial derivatives leads to an even larger
    value. These larger gradient values cause a dramatic shift in the weight values
    each time they are adjusted using back propagation, leading to a network that
    doesn't learn well.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名称所示，当梯度爆炸到更大的值时，就会发生这种情况。这可能是 RNN 架构在较大时间步时遇到的问题之一。当每个偏导数大于 **1** 时，乘法会导致一个更大的值。这些更大的梯度值每次通过反向传播调整权重时，会导致权重发生剧烈变化，从而使网络无法很好地学习。
- en: There are some techniques used to mitigate this issue, such as gradient clipping,
    wherein the gradient is normalized once it exceeds a set threshold.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些技术可以缓解这个问题，比如梯度裁剪，当梯度超过设定的阈值时会进行归一化处理。
- en: Vanishing Gradients
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 消失梯度
- en: Whether it is RNNs or CNNs, vanishing gradients could be a problem if calculated
    loss has to travel back a lot. In CNNs, this problem could occur when there are
    a lot of layers with activations such as sigmoid or tanh. The loss has to travel
    all the way back to the initial layers, and these activations generally dilute
    them by the time they reach the initial layers, which means there are almost no
    weight updates for the initial layers, resulting in underfitting. This is even
    common in RNNs, since even if a network has one RNN layer but a large number of
    timesteps, the loss has to travel all the way through the timesteps due to backpropagation
    through time. Since the gradients are multiplicative, as seen in the generalized
    derivative expressions earlier, these values tend to become low, and weights are
    not updated after a certain timestep. This means that even if more timesteps are
    shown to a network, the network can't benefit because the gradients cannot travel
    all the way back. This limitation in RNNs is due to vanishing gradients.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是 RNN 还是 CNN，如果计算出的损失需要反向传播很长时间，消失梯度可能会成为问题。在 CNN 中，当有很多层并且激活函数是 sigmoid 或
    tanh 时，这个问题可能会出现。损失需要反向传播到最初的层，而这些激活函数通常会在损失到达最初的层时将其稀释，意味着初始层几乎没有权重更新，导致欠拟合。即使是在
    RNN 中也很常见，因为即使网络只有一个 RNN 层但时间步长较多，损失也需要通过时间反向传播穿越所有的时间步。由于梯度是相乘的，如前面所见的广义导数表达式，这些值往往变得较小，且在某个时间步后权重不会被更新。这意味着即使给网络显示更多的时间步，网络也无法受益，因为梯度无法完全反向传播。这种
    RNN 的限制是由消失梯度引起的。
- en: As the name indicates, this happens when the gradients become too small. This
    could happen when each of partial derivatives is smaller than 1 and multiplication
    of these partial derivatives leads to a much smaller value. With this geometric
    decay of information, the network cannot learn properly. There are almost no changes
    in the weight values, which leads to underfitting.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名称所示，当梯度变得过小时，就会发生这种情况。当每个偏导数小于1时，这种情况可能发生，并且这些偏导数的乘积会导致一个更小的值。由于信息的几何衰减，网络无法正确学习。权重值几乎没有变化，这会导致欠拟合。
- en: There must be a better mechanism to use to know what parts of the previous timesteps
    to remember, what to forget, and so on. To address this issue, architectures such
    as LSTM networks and GRUs were created.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 必须有一种更好的机制来知道应记住前面时刻的哪些部分，哪些部分该忘记，等等。为了解决这个问题，像 LSTM 网络和 GRU 这样的架构应运而生。
- en: RNNs with Keras
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Keras 构建 RNN
- en: So far, we have discussed the theory behind RNNs, but there are a lot of frameworks
    available that can abstract away the implementation details. As long as we know
    how to use these frameworks, we can successfully get our projects working. **TensorFlow**,
    **Theano**, **Keras**, **PyTorch**, and **CNTK** are some of these frameworks.
    In this chapter, let's take a closer look at the most commonly used framework,
    called **Keras**. It uses either Tensorflow or Theano as the backend, indicating
    that it creates an even higher level of abstraction than other frameworks. It
    is a tool best suited for beginners. Once comfortable with Keras, tools such as
    TensorFlow give much more power in implementing custom functions.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了RNN的理论背景，但有许多可用的框架可以抽象出实现的细节。只要我们知道如何使用这些框架，我们就能成功地让项目运行。**TensorFlow**、**Theano**、**Keras**、**PyTorch**
    和 **CNTK** 都是这些框架中的一部分。在这一章中，让我们更详细地了解最常用的框架——**Keras**。它使用 Tensorflow 或 Theano
    作为后端，这意味着它创建了比其他框架更高的抽象级别。它是最适合初学者的工具。一旦熟悉了 Keras，像 TensorFlow 这样的工具可以在实现自定义函数时提供更大的能力。
- en: 'There are many variants of RNNs that you will study in the next few chapters,
    but all of them use the same base class, called RNN:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在接下来的几章中学习到许多 RNN 的变种，但它们都使用相同的基类，称为 RNN：
- en: '[PRE0]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In this chapter, we have discussed the simple form of the RNN, which is called
    **SimpleRNN** in Keras:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们讨论了 RNN 的简单形式，它在 Keras 中称为 **SimpleRNN**：
- en: '[PRE1]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As you can see from the arguments here, there are two kinds: one for regular
    kernels, used to compute the outputs of a layer, and the other for recurrent kernels
    used to compute states. Don''t worry too much about constraints, regularizers,
    initializers, and dropout. You can find more about them at https://keras.io/layers/recurrent/.
    They are mostly used to avoid overfitting. The role of activation here is the
    same as the role of activation with any other layer.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从这里的参数所看到的，有两种类型：一种是常规的卷积核，用于计算层的输出，另一种是用于计算状态的循环卷积核。不要太担心约束、正则化器、初始化器和丢弃法。你可以在
    [https://keras.io/layers/recurrent/](https://keras.io/layers/recurrent/) 找到更多信息。它们主要用于避免过拟合。激活函数在这里的作用与任何其他层的激活函数作用相同。
- en: The units are the number of recurrent units in a particular layer. The greater
    the number of units, the more parameters there are that need to be learned.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 单元数是指特定层中递归单元的数量。单元数量越多，需要学习的参数就越多。
- en: '`return_sequences` is the argument that specifies whether the RNN layer should
    return the whole sequence or just the last timestep. If `return_sequences` is
    false, the output of the RNN layer is just the last timestep, so we cannot stack
    this with another RNN layer. In other words, if an RNN layer needs to be stacked
    by another RNN layer, `return_sequences` need to be true. If an RNN layer is connected
    to the Dense layer, this can argument can be either true or false, depending on
    the application.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`return_sequences` 是指定 RNN 层是否应返回整个序列还是仅返回最后一个时刻的参数。如果 `return_sequences` 为
    False，则 RNN 层的输出仅为最后一个时刻，因此无法将其与另一个 RNN 层堆叠。换句话说，如果一个 RNN 层需要堆叠到另一个 RNN 层，则 `return_sequences`
    需要为 True。如果 RNN 层连接到 Dense 层，这个参数可以是 True 或 False，具体取决于应用需求。'
- en: The `return_state` argument specifies whether the last state of the RNN needs
    to be returned along with the output. This can be set to either True or False,
    depending on the application.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`return_state` 参数指定是否需要返回 RNN 的最后状态以及输出结果。根据应用需求，可以将其设置为 True 或 False。'
- en: '`go_backwards` can be used if, for any reason, the input sequence needs to
    be processed backward. Keep a note that if this is set to True, even the returned
    sequence is reversed.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`go_backwards` 可以用来处理输入序列的反向处理。如果由于某种原因需要反向处理输入序列，设置为 True 即可。请注意，如果将其设置为 True，返回的序列也会被反转。'
- en: '`stateful` is an argument that can be set to true if a state needs to be passed
    between batches. If this argument is set to true, the data needs to be handled
    carefully; we have a topic covering this in detail.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`stateful` 是一个参数，如果需要在批次之间传递状态，可以将其设置为 true。如果将此参数设置为 true，数据需要谨慎处理；我们有一个话题会详细讲解这个问题。'
- en: '`unroll` is an argument that leads to the network being unrolled if set to
    true, which can speed up operations but can be very memory extensive depending
    on the timesteps. Generally, this argument is set to true for short sequences.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`unroll` 是一个参数，若设置为 true，会使网络展开，这样可以加速操作，但根据时间步长的不同，可能会非常消耗内存。通常，对于短序列，这个参数会设置为
    true。'
- en: 'The number of timesteps is not an argument for a particular layer since it
    stays the same for the whole network, which is represented in the input shape.
    This brings us to the important point of the shape of the network when using RNNs:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 时间步长不是某一层的参数，因为它对整个网络保持一致，这在输入形状中有所表示。这引出了使用 RNN 时网络形状的一个重要点：
- en: '[PRE2]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注
- en: If you start building a network with an RNN layer, `input_shape` must be specified.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你开始构建一个包含 RNN 层的网络，必须指定`input_shape`。
- en: After a model is built, `model.summary()` can be used to see the shapes of each
    layer and the total number of parameters.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型后，可以使用`model.summary()`查看每一层的形状以及总参数数量。
- en: 'Exercise 23: Building an RNN Model to Show the Stability of Parameters over
    Time'
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 23：构建一个 RNN 模型，以展示参数随时间的稳定性
- en: Let's build a simple RNN model to show that the parameters do not change with
    timesteps. Note that while mentioning the `input_shape` argument, `batch_size`
    need not be mentioned unless needed. It is needed for a stateful network, which
    we will discuss next. `batch_size` is mentioned while training the model with
    the fit() or `fit_generator()` functions.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个简单的 RNN 模型，展示参数在时间步长上保持不变。注意，提到`input_shape`参数时，除非需要，否则无需提及`batch_size`。在状态保持网络中需要`batch_size`，我们接下来将讨论这个问题。训练模型时，`batch_size`会在使用fit()或`fit_generator()`函数时提及。
- en: 'The following steps will help you with the solution:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你解决这个问题：
- en: Import the necessary Python packages. We will be using Sequential, SimpleRNN,
    and Dense.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的 Python 包。我们将使用 Sequential、SimpleRNN 和 Dense。
- en: '[PRE3]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we define the model and its layers:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义模型及其各层：
- en: '[PRE4]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can check the summary of the model:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以查看模型的摘要：
- en: '[PRE5]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`model.summary()` gives the following output:'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`model.summary()` 会给出以下输出：'
- en: '![](img/C13783_05_30.jpg)'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C13783_05_30.jpg)'
- en: 'Figure 5.30: Model summary for model layers'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.30：模型层的模型摘要
- en: In this case, `batch_size` parameter, which will be provided by the `fit()`
    function. The output of the RNN layer is **(None, 64)** since it is not returning
    the sequence.
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，`batch_size` 参数将由`fit()`函数提供。由于不返回序列，RNN 层的输出形状是 **(None, 64)**。
- en: 'Let''s look at the model that returns sequence:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看一下返回序列的模型：
- en: '[PRE6]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The summary of the model that returns sequence looks like this:'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 返回序列的模型摘要如下所示：
- en: '![](img/C13783_05_31.jpg)'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C13783_05_31.jpg)'
- en: 'Figure 5.31: Model summary of sequence-returning model'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.31：返回序列模型的模型摘要
- en: Now the RNN layer is returning a sequence, and therefore its output shape is
    3D instead of 2D, as seen earlier. Also, note that the **Dense** layer is automatically
    adjusted to this change in its input. The **Dense** layer with the current Keras
    version has the capability of adjusting to time_steps from a previous RNN layer.
    In the previous versions of Keras, **TimeDistributed**(**Dense**) was used to
    achieve this.
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，RNN 层返回的是一个序列，因此它的输出形状是 3D，而不是之前看到的 2D。此外，请注意，**Dense** 层会自动适应其输入的变化。当前版本的
    Keras 中，**Dense** 层能够自动适应来自之前 RNN 层的时间步长。在之前的 Keras 版本中，**TimeDistributed**(**Dense**)
    用于实现这一点。
- en: 'We have previously discussed how the RNN shares its parameters over timesteps.
    Let''s see that in action and change the timesteps of the previous model from
    10 to 1,000:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们之前讨论过 RNN 如何在时间步长上共享参数。让我们实际看一下，并将之前模型的时间步长从 10 改为 1,000：
- en: '[PRE7]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Figure 5.32: Model summary for timesteps'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.32：时间步长的模型摘要'
- en: '](img/C13783_05_32.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_05_32.jpg)'
- en: 'Figure 5.32: Model summary for timesteps'
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.32：时间步长的模型摘要
- en: Clearly, the output shapes of the network changed to this new time_steps. However,
    there is no change in the parameters between the two models.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，网络的输出形状已经改变为这个新的time_steps。然而，两个模型之间的参数没有变化。
- en: This indicates that the parameters are shared over time and are not impacted
    by changing the number of timesteps. Note that the same is applicable to the **Dense**
    layer when operating on more than one timestep.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明参数是随时间共享的，并且不会受到时间步数变化的影响。请注意，当在多个时间步上操作时，同样适用于**Dense**层。
- en: Stateful versus Stateless
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有状态与无状态
- en: 'There are two modes of operation available with RNNs considering the states:
    the stateless and stateful modes. If the **argument stateful=True**, you are working
    with stateful mode, and **False** signifies stateless mode.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: RNN有两种考虑状态的操作模式：无状态模式和有状态模式。如果**参数stateful=True**，则您正在使用有状态模式，**False**表示无状态模式。
- en: Stateless mode is basically saying that one example in a batch is not related
    to any example in the next batch; that is, every example is independent in the
    given case. The state is reset after every example. Each example has a certain
    number of timesteps depending on the model architecture. For example, the last
    model we saw had 1,000 timesteps, and between these 1000 timesteps, the state
    vector was calculated and passed from one timestep to the next. However, at the
    end of the example or the beginning of the next example, there was no state passed.
    Each example was independent and therefore there was no consideration needed regarding
    the way the data was shuffled.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态模式基本上是指批次中的一个示例与下一个批次中的任何示例无关；也就是说，每个示例在给定情况下是独立的。每个示例后的状态会被重置。每个示例具有根据模型架构确定的时间步数。例如，我们看到的上一个模型有1,000个时间步，在这1,000个时间步之间，状态向量会被计算并从一个时间步传递到下一个时间步。然而，在示例的结尾或下一个示例的开始时，没有状态被传递。每个示例是独立的，因此无需考虑数据如何洗牌。
- en: In stateful mode, the state from example **i** of **batch 1** is passed to the
    **i+1** example of **batch 2**. This means that the state is passed from one example
    to the next among batches. For this reason, the examples must be contiguous across
    batches and cannot be random. The following figure explains this situation. The
    examples **i**, **i+1**, **i+2**, and so on are contiguous, and so are **j**,
    **j+1**, **j+2**, and so on, and **k**, **k+1**, **k+2**, and so on.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在有状态模式下，**批次1**的示例**i**的状态会传递到**批次2**的**i+1**示例。这意味着状态会在批次之间的示例中传递。因此，示例必须在批次之间是连续的，而不能是随机的。下图解释了这一情况。示例**i**、**i+1**、**i+2**等是连续的，**j**、**j+1**、**j+2**等也是连续的，**k**、**k+1**、**k+2**等也是如此。
- en: '![Figure 5.33 Batch formations for stateful RNN'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.33 状态RNN的批量格式'
- en: '](img/C13783_5_33.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_5_33.jpg)'
- en: Figure 5.33 Batch formations for stateful RNN
  id: totrans-239
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.33 状态RNN的批量格式
- en: 'Exercise 24: Turning a Stateless Network into a Stateful Network by Only Changing
    Arguments'
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习24：通过仅更改参数将无状态网络转变为有状态网络
- en: In order to turn a network from stateless to stateful by changing the arguments,
    the following steps should be taken.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过更改参数将网络从无状态转变为有状态，应该采取以下步骤。
- en: 'First, we would need to import the required Python packages:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要导入所需的Python包：
- en: '[PRE8]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, build the model using `Sequential` and define the layers:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`Sequential`构建模型并定义层：
- en: '[PRE9]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Set the optimizer to `Adam`, set `categorical` `crosstropy` as the loss parameter,
    and set the metrics to fit the model. Compile the model and fit the model over
    100 epochs:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将优化器设置为`Adam`，将`categorical` `crossentropy`设置为损失函数参数，并设置指标来拟合模型。编译模型并在100个周期内拟合模型：
- en: '[PRE10]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Assume that `X` and `Y` are training data as contiguous examples. Turn this
    model into a stateful one:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设`X`和`Y`是作为连续示例的训练数据。将此模型转为有状态模型：
- en: '[PRE11]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Set the optimizer to `Adam`, set `categorical` `crossentropy` as the loss parameter,
    and set the metrics to fit the model. Compile the model and fit the model over
    100 epochs:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将优化器设置为`Adam`，将`categorical` `crossentropy`设置为损失函数参数，并设置指标来拟合模型。编译模型并在100个周期内拟合模型：
- en: '[PRE12]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can use a box and whisker plot to visualize the output.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用箱型图来可视化输出。
- en: '[PRE13]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Expected output:**'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 5.34: Box and whisker plot for stateful vs stateless'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.34：有状态与无状态的箱型图'
- en: '](img/C13783_05_34.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_05_34.jpg)'
- en: 'Figure 5.34: Box and whisker plot for stateful vs stateless'
  id: totrans-257
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.34：有状态与无状态的箱型图
- en: Note
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The output may vary depending on the data used.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 输出可能会根据使用的数据而有所不同。
- en: From the concept of stateful models, we understand that the data fed in batches
    need to be contiguous, so turn randomization **OFF**. However, even with **batch_size
    >1**, the data across batches will not be contiguous, so make **batch_size=1**.
    By turning the network to **stateful=True** and fitting it with the mentioned
    parameters, we are essentially training the model correctly in a stateful manner.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 从有状态模型的概念中，我们了解到，按批次输入的数据需要是连续的，因此需要关闭随机化**OFF**。然而，即使**batch_size >1**，批次之间的数据也不会是连续的，因此需要设置**batch_size=1**。通过将网络设置为**stateful=True**并使用上述参数进行拟合，实际上我们是在以有状态的方式正确地训练模型。
- en: However, we are not using the concept of mini batch gradient descent, and nor
    are we shuffling the data. So, a generator needs to be implemented that can carefully
    train a stateful network, which is outside the scope of this chapter.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们没有使用小批量梯度下降的概念，也没有对数据进行打乱。因此，需要实现一个生成器，以便小心地训练有状态的网络，但这超出了本章的范围。
- en: '`model.compile` is a function where an optimizer and a loss function are assigned
    to the network, along with the metrics that we care about.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.compile`是一个函数，用于为网络分配优化器、损失函数和我们关心的评估指标。'
- en: '`model.fit()` is a function that is used to train a model by specifying its
    training data, validation data, the number of epochs, the batch size, the mode
    of shuffling, and more.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.fit()`是一个用于训练模型的函数，通过指定训练数据、验证数据、训练轮数、批次大小、打乱模式等来进行训练。'
- en: 'Activity 6: Solving a Problem with an RNN – Author Attribution'
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动6：使用RNN解决问题——作者身份归属。
- en: Author attribution is a classic text classification problem that comes under
    the umbrella of natural language processing (NLP). Authorship attribution is a
    well-studied problem that led to the field of **stylometry**.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 作者身份归属是一个经典的文本分类问题，属于自然语言处理（NLP）范畴。作者身份归属是一个研究深入的问题，它催生了**风格计量学**领域。
- en: In this problem, we are given a set of documents from certain authors. We need
    to train a model to understand the authors' styles and use the model to identify
    the authors of the unknown documents. As with many other NLP problems, it has
    benefited greatly from the increase in available computer power, data, and advanced
    machine learning techniques. This makes authorship attribution a natural candidate
    for the use of **deep learning (DL**). In particular, we can benefit from DL's
    ability to automatically extract the relevant features for a specific problem.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题中，我们给定了一组来自特定作者的文档。我们需要训练一个模型来理解作者的写作风格，并使用该模型来识别未知文档的作者。与许多其他自然语言处理（NLP）问题一样，这个问题得益于计算能力、数据和先进机器学习技术的提升。这使得作者身份归属成为使用**深度学习（DL）**的一个自然选择。特别是，我们可以利用深度学习自动提取与特定问题相关的特征的能力。
- en: 'In this activity, we will focus on the following:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动中，我们将专注于以下内容：
- en: Extracting character-level features from the text of each author (to get each
    author's style)
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从每个作者的文本中提取字符级别的特征（以获取每个作者的写作风格）。
- en: Using those features to build a classification model for authorship attribution
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些特征构建一个分类模型来进行作者身份归属。
- en: Applying the model for identifying the author of a set of unknown documents
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型应用于识别一组未知文档的作者。
- en: Note
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You can find the required data for the activity at https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2005.
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2005找到本次活动所需的数据。
- en: The following steps will help you with the solution.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你解决问题。
- en: Import the necessary Python packages.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的Python包。
- en: Upload the text document to be used. Then, pre-process the text file by converting
    all text into lowercase, converting all newlines and multiple whitespaces into
    single whitespaces, and removing any mention of the authors' names, otherwise
    we risk data leakage.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上传要使用的文本文件。然后，通过将所有文本转换为小写、将所有换行符和多个空格转换为单个空格，并去除任何提到作者姓名的部分来预处理文本文件，否则我们可能会导致数据泄露。
- en: To break the long texts into smaller sequences, we use the `Tokenizer` class
    from the Keras framework.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将长文本分割成较小的序列，我们使用Keras框架中的`Tokenizer`类。
- en: Proceed to create the training and validation sets.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来创建训练集和验证集。
- en: We construct the model graph and perform the training procedure.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们构建模型图并执行训练过程。
- en: Apply the model to the unknown papers. Do this for all the papers in the **Unknown**
    folder.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型应用于未知的文档。对**Unknown**文件夹中的所有文档进行此操作。
- en: '**Expected output:**'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 5.35: Output for author attribution'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.35：作者身份归属输出'
- en: '](img/C13783_05_35.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_05_35.jpg)'
- en: 'Figure 5.35: Output for author attribution'
  id: totrans-283
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.35：作者归属的输出
- en: Note
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for the activity can be found on page 309.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 活动的解答可以在第309页找到。
- en: Summary
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we were introduced to RNNs and covered the major differences
    between the architectures of RNNs and FFNNs. We looked at BPTT and how weight
    matrices are updated. We learned how to use RNNs using Keras and solved a problem
    of author attribution using RNNs in Keras. We looked at the shortcomings of RNNs
    by looking at vanishing gradients and exploding gradients. In the next chapters,
    we will look into architectures that will address these issues.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了循环神经网络（RNN），并讨论了RNN与前馈神经网络（FFNN）架构之间的主要区别。我们学习了反向传播通过时间（BPTT）以及权重矩阵的更新方法。我们通过Keras学习了如何使用RNN，并通过Keras解决了一个作者归属的问题。我们通过观察梯度消失和梯度爆炸问题，分析了RNN的不足之处。在接下来的章节中，我们将深入研究解决这些问题的架构。
