- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Diffusion Models and AI Art
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩散模型与AI艺术
- en: 'In prior chapters, we’ve looked at examples of how generative models can be
    used to create novel images; we’ve also seen how language models can be used to
    author answers to questions or create entirely new creative text like poems. In
    this chapter, we bring together these two concepts by showing how user prompts
    can be translated into images, allowing you to author “AI art” using natural language.
    In addition to creating novel images, we can perform some useful functions like
    extending an image beyond its current boundaries (“outfilling”) and defining features
    for safety screening in our results. We’ll also look at one of the foundational
    ideas underlying this image generation methodology, the *diffusion model*, which
    uses the concept of heat transfer to represent how an input of random numbers
    is “decoded” into an image. To illustrate these ideas, we’ll primarily work with
    *Stable Diffusion*, an open-source generative model, but similar concepts apply
    to closed-source models such as *Midjourney* and *DALL-E.* Topics we’ll cover
    include:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们已经看到生成模型如何用于创建新颖的图像的示例；我们还看到语言模型如何用于回答问题或创建像诗歌这样完全新颖的文本。在本章中，我们通过展示如何将用户提示转换为图像，将这两个概念结合起来，允许您使用自然语言创作“AI艺术”。除了创建新颖的图像外，我们还可以执行一些有用的功能，如扩展图像超出其当前边界（“outfilling”）并定义安全筛选功能在我们的结果中。我们还将探讨支持这种图像生成方法的基本思想之一，即*扩散模型*，它使用热传递的概念来表示随机数输入如何被“解码”为图像。为了说明这些想法，我们主要将使用*稳定扩散*，这是一个开源生成模型，但类似的概念也适用于闭源模型，如*Midjourney*和*DALL-E*。我们将涵盖的主题包括：
- en: How diffusion models relate to other kinds of image-generating models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩散模型与其他类型的图像生成模型的关系
- en: How the Stable Diffusion model combines **Variational Autoencoders** (**VAEs**)
    and diffusion models to create extremely efficient image sampling
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稳定扩散模型如何结合**变分自编码器**（**VAEs**）和扩散模型，以创建极其高效的图像采样
- en: 'Some examples of using the Stable Diffusion model in the Hugging Face pipelines
    library, where we:'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Hugging Face流水线库中使用稳定扩散模型的一些示例，我们：
- en: Evaluate key parameters that impact the output of the image generation task
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估影响图像生成任务输出的关键参数
- en: 'Walk through how the pieces of the Hugging Face pipeline implement each step
    of the image generation task to create a picture from a user prompt:'
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演示Hugging Face流水线的各个步骤如何实现图像生成任务，从用户提示创建图片：
- en: Tokenizing the user prompt as a byte string
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将用户提示标记为字节字符串
- en: Encoding the byte string prompt as a vector
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将字节字符串提示编码为向量
- en: Generating random number vectors
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成随机数向量
- en: Using the encoded prompt and random input to run multiple denoising steps to
    generate a compressed form of the new image
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用编码的提示和随机输入运行多个去噪步骤，生成新图像的压缩形式
- en: Uncompressing the new image using the decoder arm of a VAE
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用VAE的解码器部分解压新图像
- en: 'A walk through image generation: Why we need diffusion models'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像生成的详细步骤：为什么我们需要扩散模型
- en: 'Diffusion models are among the latest and most popular methods for image generation,
    particularly based on user-provided natural language prompts. The conceptual challenge
    of this class of image generation model is to create a method that is:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型是最新和最流行的图像生成方法之一，特别是基于用户提供的自然语言提示。这一类图像生成模型的概念挑战在于创建一种方法，即：
- en: Scalable to train and execute
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展进行训练和执行
- en: Able to generate a diversity of images, including with user-guided prompts
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够生成多样化的图像，包括使用用户引导的提示
- en: Able to generate natural-looking images
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够生成看起来自然的图像
- en: Has stable training behavior that is possible to replicate easily
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有稳定的训练行为，易于复制
- en: One approach to this problem is “autoregressive” models, where the image is
    generated pixel by pixel, using the prior-generated pixels as successive inputs1\.
    The inputs to these models could be both a set of image pixels and natural language
    instructions from the user that are encoded into an embedding vector. This approach
    is slow, as it makes each pixel dependent upon prior steps in the model output.
    As we’ve seen in prior chapters, **Generative Adversarial Networks** (**GANs**)
    can also be used to synthesize images, but they have unstable training behavior
    that is tricky to replicate and have a tendency to get stuck in local “modes,”
    rather than generating a broader distribution of natural images². As we saw with
    VAEs in [*Chapter 11*](Chapter_11.xhtml), the objective function based on pixel-wise
    approximation may not create the most realistic images. Recently, *diffusion models*
    have arisen as a promising alternative. What are they, and how do they solve some
    of the challenges we’ve mentioned?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是“自回归”模型，其中图像是逐像素生成的，使用先前生成的像素作为连续输入1。 这些模型的输入可以是图像像素集合，也可以是用户提供的自然语言指令，这些指令被编码为嵌入向量。
    这种方法比较慢，因为它使得每个像素依赖于模型输出中的前几个步骤。 正如我们在前几章中所看到的，**生成对抗网络**（**GANs**）也可以用来合成图像，但它们的训练行为不稳定，难以复制，并且容易陷入局部“模式”，而不是生成更广泛的自然图像分布²。
    正如我们在[*第11章*](Chapter_11.xhtml)中看到的那样，基于逐像素近似的目标函数可能无法创建最逼真的图像。 最近，*扩散模型*作为一种有前景的替代方法崭露头角。
    它们是什么？它们如何解决我们提到的一些挑战？
- en: 'Pictures from noise: Using diffusion to model natural image variability'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从噪声到图像：使用扩散模型来模拟自然图像的变化性
- en: The core idea of diffusion models is that we can represent images as a set of
    pixels, which are like a cloud in high-dimensional space. That cloud is highly
    structured, representing colored patches and objects. If we add noise – such as
    random normal noise – to that structure, it becomes a spherical cloud. However,
    if we had a recipe for how to reverse that “blurring” of the image, we could create
    new images from a set of random points.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型的核心思想是，我们可以将图像表示为一组像素，这些像素就像高维空间中的一个云团。 这个云团是高度结构化的，代表了颜色斑块和物体。 如果我们添加噪声——比如随机正态噪声——到这个结构中，它将变成一个球形云团。
    然而，如果我们有一种方法可以逆转这种图像的“模糊”，我们就可以从一组随机点中生成新图像。
- en: 'Let’s look at how to write this out mathematically. We start with our “forward
    process,” which takes input data, such as an image, ![](img/B22333_15_001.png),
    and applies stepwise noise to turn it into a vector of random normals. We will
    label this forward “blurring” process ![](img/B22333_11_022.png)*,* and we can
    represent it as a *Markov* process where each step depends only on the prior step:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下如何将其用数学形式写出来。我们从“前向过程”开始，它接受输入数据，例如一张图片，![](img/B22333_15_001.png)，并应用逐步噪声将其转化为一个随机正态向量。我们将这个前向“模糊”过程标记为
    ![](img/B22333_11_022.png)*,* 并且可以将其表示为一个*马尔可夫*过程，其中每一步仅依赖于前一步：
- en: '![](img/B22333_15_003.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22333_15_003.png)'
- en: In other words, the image at the end composed of random pixels is created by
    repetitively applying a function ![](img/B22333_11_022.png)to step ![](img/B22333_15_005.png),
    dependent on the prior value of ![](img/B22333_11_001.png). This function ![](img/B22333_11_022.png)
    defines a transition process that follows a Gaussian distribution parameterized
    by ![](img/B22333_15_008.png), which controls the variance³. The value of ![](img/B22333_15_008.png)
    determines the level of noise applied at each step – smaller values (low ![](img/B22333_15_008.png))
    result in a gradual increase in noise, while larger values (high ![](img/B22333_15_008.png))
    accelerate the transition, causing the image to degrade into a noisy set of random
    pixels more quickly. Once we’ve applied this “blurring” transformation enough
    times, the data will be in a distribution such as a random normal.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，最终由随机像素组成的图像是通过反复应用一个函数 ![](img/B22333_11_022.png) 到步骤 ![](img/B22333_15_005.png)，依赖于
    ![](img/B22333_11_001.png) 的先前值创建的。 这个函数 ![](img/B22333_11_022.png) 定义了一个转移过程，它遵循一个由
    ![](img/B22333_15_008.png) 参数化的高斯分布，该参数控制方差³。 ![](img/B22333_15_008.png) 的值决定了在每一步应用的噪声水平——较小的值（低
    ![](img/B22333_15_008.png)）导致噪声逐渐增加，而较大的值（高 ![](img/B22333_15_008.png)）则加速转变，使图像更快地退化成一组随机像素。
    一旦我们应用足够多次这个“模糊”变换，数据将处于如随机正态分布的状态。
- en: 'What if we now want to recover an image from this blurred cloud? We just apply
    a “reverse” transformation, ![](img/B22333_11_021.png), using a similar formula:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果我们现在想从这个模糊的云团中恢复一张图像怎么办？我们只需要应用“反向”变换 ![](img/B22333_11_021.png)，使用类似的公式：
- en: '![](img/B22333_15_013.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22333_15_013.png)'
- en: We can see that ![](img/B22333_11_021.png) and ![](img/B22333_11_022.png) are
    reverses of each other, but that ![](img/B22333_11_021.png) also represents a
    recipe for taking random data and generating images from it.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到！[](img/B22333_11_021.png) 和！[](img/B22333_11_022.png) 是彼此的反向过程，但！[](img/B22333_11_021.png)
    还代表了一个从随机数据生成图像的过程。
- en: 'This process is illustrated below:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程如下所示：
- en: '![Figure 15.1: The diffusion process for noising and denoising images4](img/B22333_15_01.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.1：加噪和去噪图像的扩散过程4](img/B22333_15_01.png)'
- en: 'Figure 15.1: The diffusion process for noising and denoising images⁴'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.1：加噪和去噪图像的扩散过程⁴
- en: This design seems promising conceptually, but it’s not clear how we would guarantee
    that ![](img/B22333_11_021.png) and ![](img/B22333_11_022.png) are sufficiently
    close that they would result in high-quality samples when we apply them. In other
    words, we need a method to optimize the parameters of ![](img/B22333_11_021.png)
    and ![](img/B22333_11_022.png) so that they are tuned to generate high-quality
    reconstructions of an input image once it has been blurred and recovered through
    ![](img/B22333_11_021.png). It’s perhaps not surprising, given the familiar ![](img/B22333_11_021.png)
    and ![](img/B22333_11_022.png) distributions from our discussion of VAEs in [*Chapter
    11*](Chapter_11.xhtml), that this problem can be solved through variational inference⁴.
    Let’s see how.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计在概念上似乎很有前景，但我们不清楚如何保证！[](img/B22333_11_021.png) 和！[](img/B22333_11_022.png)
    足够接近，以便在应用时能够生成高质量的样本。换句话说，我们需要一种方法来优化！[](img/B22333_11_021.png) 和！[](img/B22333_11_022.png)
    的参数，使它们调整到生成输入图像的高质量重构，当图像经过模糊并通过！[](img/B22333_11_021.png) 恢复后。这可能并不令人惊讶，因为在我们讨论变分自编码器（VAE）时，熟悉的！[](img/B22333_11_021.png)
    和！[](img/B22333_11_022.png) 分布出现在 [*第 11 章*](Chapter_11.xhtml) 中，因此这个问题可以通过变分推断来解决⁴。让我们看看如何做到。
- en: Using variational inference to generate high-quality diffusion models
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用变分推断生成高质量的扩散模型
- en: 'Recall that the **Evidence Lower Bound** (**ELBO**) gives an expression for
    the log-likelihood of a difficult-to-calculate distribution p in terms of an approximating,
    easy-to-calculate distribution ![](img/B22333_11_022.png):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，**证据下界**（**ELBO**）给出了一个表达式，用于表示一个难以计算的分布 p 的对数似然，它通过一个易于计算的近似分布来表示！[](img/B22333_11_022.png)：
- en: '![](img/B22333_15_025.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22333_15_025.png)'
- en: 'Instead of directly maximizing the likelihood of ![](img/B22333_11_021.png),
    we can maximize the right-hand side, which is a lower bound on the likelihood
    of ![](img/B22333_11_021.png), in terms of the divergence with an approximating
    distribution ![](img/B22333_11_022.png). For convenience purposes, we often minimize
    the negative log-likelihood (as many computational packages take the minima of
    a function), which gives the following equation for the diffusion model:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是直接最大化！[](img/B22333_11_021.png) 的似然性，而是最大化右边的部分，即！[](img/B22333_11_021.png)
    似然性的下界，该下界与一个近似分布！[](img/B22333_11_022.png) 的散度相关。为了方便起见，我们通常最小化负对数似然（因为许多计算包采用最小化函数），这就得到了以下扩散模型的方程：
- en: '![](img/B22333_15_029.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22333_15_029.png)'
- en: Note that this equation can be evaluated at multiple steps t in the noising/denoising
    process. We can write this out more explicitly as a loss function with beginning,
    intermediate, and final values.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个方程可以在加噪/去噪过程中的多个步骤 t 上进行评估。我们可以将其更明确地写成一个包含起始、中间和最终值的损失函数。
- en: '![](img/B22333_15_030.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22333_15_030.png)'
- en: 'Here, DKL is the Kullback–Leibler divergence, as we saw in [*Chapter 11*](Chapter_11.xhtml).
    Recall that the forward noising process *q* has a variance ![](img/B22333_15_031.png).
    We could try to learn this value, but as it’s often small, we can treat it as
    fixed. Thus, the last term in this equation, at time *T*, drops out since it is
    a constant. What about the values from *t*=1 to *T-*1? We already described that
    *q* doesn’t have learnable parameters, so we are interested in learning the parameters
    of *p*, the reverse process that converts random noise into an image. In the expression:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，DKL 是 Kullback–Leibler 散度，正如我们在 [*第 11 章*](Chapter_11.xhtml) 中所看到的。记住，前向加噪过程
    *q* 的方差为！[](img/B22333_15_031.png)。我们可以尝试学习这个值，但由于它通常较小，我们可以将其视为固定值。因此，这个方程中的最后一项，在时间
    *T* 时会消失，因为它是一个常数。那么，从 *t*=1 到 *T*-1 的值如何呢？我们已经描述过 *q* 没有可学习的参数，因此我们关心的是学习 *p*
    的参数，即将随机噪声转化为图像的反向过程。在这个表达式中：
- en: '![](img/B22333_15_032.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22333_15_032.png)'
- en: 'We will typically keep the variance as a fixed value, so we just need to learn
    a function to predict the mean – and that function could be a neural network that
    takes the input pixels at a given step and outputs a slightly less noisy image.
    However, we can reparameterize this equation to make it easier to optimize. Using
    the normal distribution, we can write this intermediate likelihood ![](img/B22333_15_033.png)
    as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将方差保持为固定值，因此我们只需要学习一个预测均值的函数——这个函数可以是一个神经网络，接受给定步长的输入像素，并输出稍微减少噪声的图像。然而，我们可以重新参数化这个方程，以使其更易于优化。使用正态分布，我们可以将这个中间似然度
    ![](img/B22333_15_033.png) 写为：
- en: '![](img/B22333_15_034.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22333_15_034.png)'
- en: 'C is a constant and falls out of the minimization. We can calculate the value
    of the mean at a given point in time using the average variance per timestep.
    Let:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: C 是一个常数，并且在最小化过程中消失。我们可以使用每步的平均方差来计算给定时间点的均值。设：
- en: '![](img/B22333_15_035.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22333_15_035.png)'
- en: 'And:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 并且：
- en: '![](img/B22333_15_036.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22333_15_036.png)'
- en: 'Then, at each timestep, *x* can be represented as:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在每个时间步，*x* 可以表示为：
- en: '![](img/B22333_15_037.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22333_15_037.png)'
- en: 'And we’ll optimize:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将优化：
- en: '![](img/B22333_15_038.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22333_15_038.png)'
- en: 'This expression shows how the function predicting the mean of *x* can be represented
    as an equation in which the unknown is a function predicting the noise *e* as
    a function of *t*:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式展示了如何将预测 *x* 均值的函数表示为一个方程，其中未知数是预测噪声 *e* 作为 *t* 的函数：
- en: '![](img/B22333_15_039.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22333_15_039.png)'
- en: 'This finally leads us to the following expression:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这将导致我们得到以下表达式：
- en: '![](img/B22333_15_040.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22333_15_040.png)'
- en: Given fixed values for ![](img/B22333_15_041.png), ![](img/B22333_15_042.png),
    and ![](img/B22333_15_043.png), and input data ![](img/B22333_11_037.png), we
    are optimizing a function to predict the noise we should subtract at each step
    of the reverse process ![](img/B22333_11_032.png) to obtain an image ![](img/B22333_11_032.png)
    from a sample of random noise. Like ![](img/B22333_15_047.png), that ![](img/B22333_15_048.png)
    will be a neural network, and that is what we will see implemented in the Stable
    Diffusion model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 给定固定的值 ![](img/B22333_15_041.png)，![](img/B22333_15_042.png) 和 ![](img/B22333_15_043.png)，以及输入数据
    ![](img/B22333_11_037.png)，我们正在优化一个函数，用于预测我们在每一步反向过程 ![](img/B22333_11_032.png)
    中应该减去的噪声，以便从一个随机噪声样本获得图像 ![](img/B22333_11_032.png)。就像 ![](img/B22333_15_047.png)
    一样，那个 ![](img/B22333_15_048.png) 将是一个神经网络，这也是我们在 Stable Diffusion 模型中看到的实现方式。
- en: 'For the term L[o] in the diffusion equation on the previous page (i.e., ![](img/B22333_15_049.png)),
    in practice, it has not been found to be needed to train a probabilistic diffusion
    function, so it is dropped. We can make one more improvement; if the sample already
    has low noise (after we’ve run the reverse process for many steps), we can down-weight
    subsequent samples when we subtract the model-predicted noise. We do this by incorporating
    the simulation step t explicitly as a term in our noise-predicting neural network
    *e*, and drop the multipliers:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前一页扩散方程中的项 L[o]（即 ![](img/B22333_15_049.png)），实际上，训练一个概率扩散函数时发现不需要它，因此它被去掉了。我们可以再做一次改进；如果样本已经具有较低的噪声（在我们进行了多步反向过程之后），我们可以在减去模型预测的噪声时对后续样本进行降权。我们通过将模拟步骤
    t 显式地作为噪声预测神经网络 *e* 中的一项来做到这一点，并去掉乘法因子：
- en: '![](img/B22333_15_050.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22333_15_050.png)'
- en: As we’ll see later, how we execute e at each step of the simulation to remove
    noise successively from a random image is an important design choice in diffusion
    models, known as the scheduler.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们稍后所见，在每一步模拟中如何执行 e，以逐步去除随机图像中的噪声，是扩散模型中的一个重要设计选择，称为调度器（scheduler）。
- en: 'However, we have one last challenge to resolve; we can optimize the likelihood
    function above efficiently, but the actual generation step will be costly since
    we could be working with large images, and the size of *x* remains fixed throughout
    the simulation steps. This is where Stable Diffusion comes in: it leverages the
    VAE models we saw in [*Chapter 11*](Chapter_11.xhtml)to perform the forward and
    reverse processes we describe above in a latent space that is much smaller than
    the original image, meaning it is considerably faster for training and inference.
    Let’s take a look.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还有一个最后的挑战需要解决；我们可以高效地优化上面的似然函数，但实际生成步骤将是昂贵的，因为我们可能处理的是大图像，并且 *x* 的大小在整个模拟步骤中保持固定。这就是
    Stable Diffusion 的作用：它利用我们在 [*第 11 章*](Chapter_11.xhtml) 中看到的 VAE 模型，在一个比原始图像小得多的潜在空间中执行我们上面描述的正向和反向过程，这意味着它在训练和推理时速度更快。我们来看看。
- en: 'Stable Diffusion: Generating images in latent space'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Stable Diffusion：在潜在空间中生成图像
- en: As we described, the major insight for the Stable Diffusion model was that instead
    of performing the forward process *q* and reverse process *p* that we’ve trained
    through variation inference in the image space, we do so using a VAE to compress
    the images, making the calculation much faster than the slower diffusion calculation
    that can be executed in the original pixel space; this process is shown in *Figure
    15**.2*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所描述的，Stable Diffusion模型的一个重要见解是，我们并不是通过变分推断在图像空间中执行前向过程*q*和反向过程*p*，而是使用VAE来压缩图像，这使得计算比在原始像素空间中执行的较慢扩散计算要快得多；这个过程在*图15.2*中展示。
- en: '![Figure 15.2: The Stable Diffusion model5](img/B22333_15_02.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图15.2：Stable Diffusion模型5](img/B22333_15_02.png)'
- en: 'Figure 15.2: The Stable Diffusion model⁵'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2：Stable Diffusion模型⁵
- en: Let’s walk through some of the elements of this workflow. On the far right,
    the input image *x* is “blurred” using a VAE into a latent vector *z*. Thus, the
    forward step *q* is executed through one pass through the VAE! Then, we can incorporate
    “conditioning information” (such as a textual prompt from the user) using an embedding
    method on the far right. Now, to run the “reverse process,” *p*, we execute a
    time-varying U-Net⁶ to predict the noise, e, that we should remove from a random
    image at each time step. The U-Net (*Figure 15**.2*) is made up of a number of
    transformer layers, which compress the latent vector z generated by the VAE (or
    sampled randomly) into a smaller length before expanding it, in order to enhance
    the salient features of the latent vector. The “U” in the name comes from the
    fact that if the layers are arranged visually with the largest, outermost layers
    at the top and the innermost, narrowest layers at the bottom of a graph of the
    network, it resembles the letter U. Due to this architecture, the U-Net is well
    suited to extract features/details in images (through the forward, encoding path)
    that are then labeled/highlighted at a pixel level (through the reverse, decoding
    path that expands the image to its original dimensions). In our example, where
    we use latent vectors instead of the original image, each pass of the latent vector
    through this U-Net represents one “step” of the denoising process ![](img/B22333_11_021.png).
    You’ll also notice we’ve added residual connections in this U-Net to enable the
    efficient flow of information through the network. We then decode the “denoised”
    latent vector with the VAE in reverse.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步了解一下这个工作流的一些要素。在最右侧，输入图像*x*通过VAE被“模糊化”为潜在向量*z*。因此，前向步骤*q*通过一次VAE的传递执行！接下来，我们可以通过最右侧的嵌入方法，结合“条件信息”（如用户的文本提示）。现在，为了执行“反向过程”*p*，我们运行一个随时间变化的U-Net⁶来预测我们应该从每个时间步骤的随机图像中去除的噪声e。U-Net（*图15.2*）由多个变换器层组成，这些层将VAE生成的潜在向量z（或随机采样的）压缩成更小的长度，再扩展它，以增强潜在向量的显著特征。名字中的“U”来自于这个事实：如果将各层按视觉方式排列，最大的最外层在顶部，最内层的最窄层在网络图的底部，它看起来像字母U。由于这种架构，U-Net非常适合提取图像中的特征/细节（通过前向编码路径），这些特征随后在像素级别进行标记/突出显示（通过反向解码路径将图像扩展到其原始尺寸）。在我们的示例中，我们使用潜在向量代替原始图像，每次通过U-Net传递潜在向量代表去噪过程的一个“步骤”
    ![](img/B22333_11_021.png)。你还会注意到，我们在这个U-Net中加入了残差连接，以便信息能高效地在网络中流动。然后，我们通过VAE反向解码“去噪”的潜在向量。
- en: In the training phase of this model, we would take pairs of images and prompts
    describing them, embed them in the model, and optimize the lower bound given above.
    If we are not training the model, we don’t even need to run the VAE forward to
    create random vectors; we just sample them. Now that we’ve seen how Stable Diffusion
    is set up, and the details of how it evolved from earlier ideas for image generation,
    let’s see how to put it into practice.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在该模型的训练阶段，我们将成对的图像和描述它们的提示输入模型，并优化上述的下界。如果我们不训练模型，则无需运行VAE前向传递来创建随机向量；我们只需对其进行采样。现在我们已经了解了Stable
    Diffusion的设置，以及它是如何从早期的图像生成思想演变而来的，让我们看看如何将其付诸实践。
- en: Running Stable Diffusion in the cloud
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在云端运行Stable Diffusion
- en: To start, let’s quickly set up our own instance of the Stable Diffusion model
    in Python code and run an example. For this purpose, we’ll be using Google Colab
    ([https://colab.research.google.com/](https://colab.research.google.com/)), a
    cloud environment that allows you to utilize high-performance **Graphics Processing
    Unit** (**GPU**) computing and large memory resources from your laptop. Colab
    is free, but you can also pay for higher-availability resources if you desire.
    The interface resembles the Python Jupyter notebooks ([https://jupyter.org/](https://jupyter.org/))
    that you’ve likely used in the past.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们快速在 Python 代码中设置自己的 Stable Diffusion 模型实例并运行示例。为此，我们将使用 Google Colab（[https://colab.research.google.com/](https://colab.research.google.com/)），一个允许你利用来自笔记本的高性能**图形处理单元**（**GPU**）计算和大内存资源的云环境。Colab
    是免费的，但如果需要，你也可以支付更高可用性的资源费用。该界面类似于你过去可能使用过的 Python Jupyter 笔记本（[https://jupyter.org/](https://jupyter.org/)）。
- en: Installing dependencies and running an example
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装依赖并运行示例
- en: Once you’ve set up your Colab account, you just need to install the diffusers
    package and a few dependencies. Diffusers is a library created by the company
    Hugging Face ([https://huggingface.co/docs/diffusers/index](https://huggingface.co/docs/diffusers/index))
    that provides easy access to a set of state-of-the-art diffusion models (including
    Stable Diffusion). It utilizes the pipelines API, also developed by Hugging Face,
    which abstracts many of the complexities of these models to a simple interface.
    *Figure 15**.3* demonstrates the commands you would provide in a Colab notebook
    to install diffusers and its dependencies.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 设置好 Colab 帐户后，你只需安装 diffusers 包及其一些依赖项即可。Diffusers 是由 Hugging Face 公司创建的一个库（[https://huggingface.co/docs/diffusers/index](https://huggingface.co/docs/diffusers/index)），提供了对一组最先进的扩散模型（包括
    Stable Diffusion）的简便访问。它利用 Hugging Face 开发的管道 API，该 API 将这些模型的许多复杂性抽象为一个简单的接口。*图
    15.3* 展示了你在 Colab 笔记本中提供的安装 diffusers 及其依赖项的命令。
- en: '![Figure 15.3: Dependencies for diffusers](img/B22333_15_03.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.3：Diffusers 的依赖项](img/B22333_15_03.png)'
- en: 'Figure 15.3: Dependencies for diffusers'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.3：Diffusers 的依赖项
- en: For this example, you’ll want to make sure you have a GPU-enabled runtime, which
    you can choose by going to *Runtime* and then *Change runtime type* on the top
    ribbon on the notebook.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，你需要确保你的运行时启用了 GPU，这可以通过进入*运行时*，然后在笔记本顶部的功能区选择*更改运行时类型*来完成。
- en: '![Figure 15.4: Runtime for the diffusers example](img/B22333_15_04.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.4：Diffusers 示例的运行时](img/B22333_15_04.png)'
- en: 'Figure 15.4: Runtime for the diffusers example'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.4：Diffusers 示例的运行时
- en: From there, we’ll initialize the Stable Diffusion 1.4 model using a series of
    simple commands. First, we’ll load the model, then initialize a pipeline on the
    GPU on our runtime. Then we merely need to supply a text prompt to the pipeline;
    the model will be run interactively and we can display the result directly in
    the notebook.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们将使用一系列简单的命令初始化 Stable Diffusion 1.4 模型。首先，我们加载模型，然后在运行时的 GPU 上初始化管道。接着，我们只需要向管道提供文本提示；模型将以交互方式运行，并且我们可以直接在笔记本中显示结果。
- en: 'To start with, we’ll use an example from the Stable Diffusion paper⁵. The user
    prompt is “a zombie in the style of Picasso,” and the Stable Diffusion model translates
    this prompt into an image representing an undead monster in the abstract, cubist
    style of the famous 20th-century artist Pablo Picasso:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，我们将使用 Stable Diffusion 论文中的一个示例。用户提示是“以毕加索风格的僵尸”，Stable Diffusion 模型将这个提示转换为一幅表现出僵尸怪物的图像，图像采用著名
    20 世纪艺术家巴勃罗·毕加索的抽象立体主义风格：
- en: '![Figure 15.5: An example output using the Picasso zombie prompt](img/B22333_15_05.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.5：使用 Picasso 僵尸提示的示例输出](img/B22333_15_05.png)'
- en: 'Figure 15.5: An example output using the Picasso zombie prompt'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.5：使用 Picasso 僵尸提示的示例输出
- en: However, remember that this is not a deterministic output like a typical machine
    learning prediction, where we get the same output for a given input. Rather, we’re
    sampling from possible model outputs from a distribution, so we’re not limited
    to generating a single output. Indeed, if we modify the `num_images_per_prompt`
    parameter, we can generate a set of images all from the same prompt by printing
    each element of the `images` list.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请记住，这并不是像典型机器学习预测那样的确定性输出，即对于给定输入我们会得到相同的输出。相反，我们是从可能的模型输出中进行采样，因此我们不限于生成单一的输出。事实上，如果我们修改`num_images_per_prompt`参数，我们可以通过打印
    `images` 列表的每个元素，生成一组来自相同提示的图像。
- en: '![Figure 15.6: Generating alternative images from the zombie prompt](img/B22333_15_06.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.6：从僵尸提示生成替代图像](img/B22333_15_06.png)'
- en: 'Figure 15.6: Generating alternative images from the zombie prompt'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.6：从僵尸提示生成替代图像
- en: Now that we’ve looked at a basic example, let’s modify some of the parameters
    to see how they impact the output.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看了一个基本示例，让我们修改一些参数，看看它们如何影响输出。
- en: Key parameters for Stable Diffusion text-to-image generation
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稳定扩散文本到图像生成的关键参数
- en: 'Besides generating multiple images, what other parameters could we modify in
    this example? One would be to remove the prompt (provide a blank prompt) and see
    what output we would get:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 除了生成多张图像，我们还可以在这个示例中修改哪些其他参数？其中一个是去除提示词（提供空白提示），看看我们会得到什么输出：
- en: '![Figure 15.7: Running Stable Diffusion with a blank prompt](img/B22333_15_07.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.7：使用空白提示运行稳定扩散模型](img/B22333_15_07.png)'
- en: 'Figure 15.7: Running Stable Diffusion with a blank prompt'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.7：使用空白提示运行稳定扩散模型
- en: Interestingly, as you can see in *Figure 15**.7,* the result is a set of seemingly
    random images, but not blank images or completely random noise. The reason for
    this can be explained by one of the components of the pipeline, the VAE we covered
    in [*Chapter 11*](Chapter_11.xhtml)and the data used to develop it, as we’ll see
    later in this chapter.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，正如在*图 15.7*中所见，结果是一组看似随机的图像，但不是空白图像或完全随机的噪声。这可以通过管道中的一个组件——我们在[*第 11 章*](Chapter_11.xhtml)中讲过的
    VAE，以及用于开发它的数据来解释，正如我们将在本章后面看到的那样。
- en: 'We can also modify how much importance we give to the prompt in generating
    images, using the *guidance scale* parameter. As we saw in our overview of the
    Stable Diffusion model, we can think of the image generation step as modeling
    the pixels in the image as particles that drift in a multi-dimensional space.
    The motion of those particles can either be pushed in a particular direction in
    correlation with the input prompt from the user or move randomly according to
    their initial configuration. The default for the guidance scale is 7.5 – let’s
    see what happens if we change it to alternative values between 0 and 10:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过修改在生成图像时给提示词的重要性，使用*引导尺度*参数。如我们在概述稳定扩散模型时所见，我们可以将图像生成步骤看作是将图像中的像素建模为在多维空间中漂移的粒子。这些粒子的运动可以根据用户输入的提示词朝特定方向推动，或者根据其初始配置随机移动。引导尺度的默认值为
    7.5——让我们看看将其改为 0 到 10 之间的其他值会发生什么：
- en: "![Figu\uFEFFre 15.8: Modifying the guidance scale from 0 to 10](img/B22333_15_08.png)"
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.8：将引导尺度从 0 修改为 10](img/B22333_15_08.png)'
- en: 'Figure 15.8: Modifying the guidance scale from 0 to 10'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.8：将引导尺度从 0 修改为 10
- en: You can see that as the guidance scale increases from 0 to 10, the generated
    image more clearly resembles the prompt. The image at 0 looks very much like the
    output from the blank prompt examples in *Figure 15**.6* – indeed, under this
    setting, the model is using a blank input. At 0, the model will pay no attention
    to the prompt, as we’ll see later in this chapter.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，随着引导尺度从 0 增加到 10，生成的图像越来越明显地与提示词相符。在 0 的图像看起来非常像*图 15.6*中的空白提示示例——事实上，在这个设置下，模型使用的是空白输入。在
    0 时，模型不会关注提示词，正如我们将在本章后面看到的那样。
- en: 'The impact of this parameter is perhaps more notable when using a more complex
    prompt, such as the one we used in the last chapter:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用更复杂的提示时，这个参数的影响可能更为显著，正如我们在上一章中使用的提示：
- en: A zombie in the style of Monet. The zombie is dressed in a farmer’s outfit and
    holds a paintbrush and canvas. The sun is setting, and there are mountains in
    the distance. The hay in the field in which the zombie is standing comes up to
    its waist. There are red flowers in the field
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一只以莫奈风格绘制的僵尸。僵尸穿着农民服装，手持画笔和画布。太阳正在落山，远处有群山。僵尸站立的田地里的干草高到它的腰部。田野中有红色的花朵。
- en: '*Figure 15**.9* shows a comparison of applying a guidance scale of 0 to 10;
    you can see in the final image that the zombie figure begins to appear.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15.9* 显示了应用引导尺度从 0 到 10 的比较；你可以在最终的图像中看到僵尸形象开始显现。'
- en: '![Figure 15.9: Image generated with guidance scales 0, 7.5, and 10 using a
    complex prompt](img/B22333_15_09.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.9：使用引导尺度 0、7.5 和 10，通过复杂提示生成的图像](img/B22333_15_09.png)'
- en: 'Figure 15.9: Image generated with guidance scales 0, 7.5, and 10 using a complex
    prompt'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.9：使用引导尺度 0、7.5 和 10，通过复杂提示生成的图像
- en: 'In addition to the guidance scale, the number of diffusion steps in the image
    generation process also impacts how crisp the output is. As we’ve seen, the image
    generation by the model can be represented by pixels behaving as particles moving
    in space. The longer they are able to move, the farther they can transition from
    an initial, random arrangement to a new configuration that resembles an image.
    The default in this pipeline is 50 steps: let’s see what happens if we modify
    that to `1`, `3`, and `10` in *Figure 15**.10*:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 除了引导尺度外，图像生成过程中扩散步骤的数量也会影响输出的清晰度。正如我们所看到的，模型生成的图像可以通过像粒子在空间中移动的像素来表示。它们能够移动的时间越长，它们就能从初始的随机排列过渡到一个新的配置，最终形成一幅类似图像的效果。该管道中的默认设置为
    50 步：让我们看看如果我们将其修改为 `1`、`3` 和 `10` 时会发生什么，如*图 15.10*所示：
- en: "![Figure 15.\uFEFF10: Images generated with 1, 3, and 10 simulation steps](img/B22333_15_10.png)"
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.10：通过 1、3 和 10 步模拟生成的图像](img/B22333_15_10.png)'
- en: 'Figure 15.10: Images generated with 1, 3, and 10 simulation steps'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.10：通过 1、3 和 10 步模拟生成的图像
- en: As the number of simulation steps increases, the generated image goes from blurry
    to resembling our initial examples – at 3 steps, we see output that resembles
    our prompt but without the simplified cubist lines that become clearer with more
    simulation steps. We’ll see later in this chapter how each simulation attempts
    to subtract “noise” from the image, and thus makes it more clear as more steps
    are run.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模拟步骤数量的增加，生成的图像从模糊变得越来越像我们的初始示例——在 3 步时，我们看到的输出与提示相似，但没有简化的立体派线条，随着更多模拟步骤的进行，这些线条变得更加清晰。稍后在本章中，我们将看到每个模拟如何尝试从图像中减去“噪声”，从而使图像随着更多步骤的进行变得更加清晰。
- en: 'Another way we can modify the input is by introducing “negative” prompts, which
    cancel out part of the initial prompt. Let’s see how this works by providing `zombie`,
    `Picasso`, or `Cubist` as the negative prompt in *Figure 15**.11*:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们修改输入的另一种方式是通过引入“负面”提示，这会取消初始提示的一部分。让我们通过在*图 15.11*中提供 `zombie`、`Picasso` 或
    `Cubist` 作为负面提示来看一下这如何起作用：
- en: "![Figure 15.11: \uFEFFImage generated with negative prompts](img/B22333_15_11.png)"
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.11：通过负面提示生成的图像](img/B22333_15_11.png)'
- en: 'Figure 15.11: Image generated with negative prompts'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.11：通过负面提示生成的图像
- en: You can see that if we provide elements of the prompt (`zombie` or `Picasso`),
    we can cancel out either the subject matter or the style of the image. We don’t
    even need to use the exact words; as you can see using `Cubist` (a term closely
    associated with the art style of Picasso) produces a similar output to a negative
    prompt using the artist’s name explicitly. This is because of how the prompts
    are encoded as numerical vectors when they are passed to the model, which allows
    the model to compare the similarity of terms, as we’ll see later when we discuss
    the embedding step.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，如果我们提供提示词的元素（`zombie` 或 `Picasso`），我们可以取消图像的主题或风格。我们甚至不需要使用精确的词语；正如你所看到的，使用
    `Cubist`（一个与毕加索艺术风格紧密相关的术语）生成的结果与使用艺术家名字明确作为负面提示的输出非常相似。这是因为当提示词被传递给模型时，它们被编码为数值向量，从而使得模型能够比较词语之间的相似性，正如我们稍后在讨论嵌入步骤时会看到的那样。
- en: In addition to modifying the content of the image, we can also easily change
    its size, as you can see in *Figure 15**.12.*
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 除了修改图像的内容外，我们还可以轻松改变其大小，正如你在*图 15.12*中看到的那样。
- en: "![Figure 15.12: Imag\uFEFFe generated with varying dimensions](img/B22333_15_12.png)"
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.12：生成具有不同尺寸的图像](img/B22333_15_12.png)'
- en: 'Figure 15.12: Image generated with varying dimensions'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.12：生成具有不同尺寸的图像
- en: The size of the resulting image is easily changed by modifying the size of the
    ultimate decoder layer in the last step of the pipeline, as we’ll see later.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过修改管道最后步骤中最终解码层的大小，可以轻松改变生成图像的尺寸，稍后我们将看到这一点。
- en: 'One of the risks of generating images in an application is that the output
    could be offensive; fortunately, the pipeline in this example has a built-in feature,
    a safety checker, to screen such potentially inappropriate content. We can see
    the effect of this by modifying the prompt (*Figure 15**.13*):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用中生成图像的风险之一是输出可能会冒犯他人；幸运的是，这个示例中的管道具有内建的安全检查器，能够筛查此类潜在的不当内容。我们可以通过修改提示来看出这个效果（*图
    15.13*）：
- en: "![Figure 15.13: Image\uFEFF generated with a toxic/offensive prompt](img/B22333_15_13.png)"
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.13：生成的有毒/冒犯性提示的图像](img/B22333_15_13.png)'
- en: 'Figure 15.13: Image generated with a toxic/offensive prompt'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.13：通过有毒/冒犯性提示生成的图像
- en: The safety checker is a model that classifies features of the produced image
    as **Not Safe for Work** (**NSFW**) and blocks them. The features it uses to produce
    this classification are quite similar to the embeddings used to feed the prompt
    into the model to generate the image.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 安全检查器是一个模型，它将生成的图像特征分类为**不适宜工作**（**NSFW**）并加以阻止。它用于进行这种分类的特征与用来将提示词输入模型以生成图像的嵌入特征非常相似。
- en: Now that we’ve seen numerous ways that we can tweak the output of the model
    through various parameters, let’s explore how each of these parameters appears
    step by step as we walk through each of the components underlying the pipeline.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到许多可以通过各种参数调整模型输出的方法，让我们一步步探索这些参数是如何出现在流水线的每个组件中的。
- en: Deep dive into the text-to-image pipeline
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨文本到图像的流水线
- en: In the previous section, we produced all the examples by providing the prompt
    and various arguments to the pipeline directly. The pipeline consists of several
    components that act in sequence to produce images from your prompt. These components
    are contained in a Python dictionary that is part of the `Pipeline` class, and
    so, like any Python dictionary, you can print the key names of the fields to inspect
    the components (*Figure 15**.14*).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们通过直接向流水线提供提示词和各种参数生成了所有示例。流水线由几个按顺序执行的组件组成，这些组件从提示词中生成图像。这些组件包含在`Pipeline`类的Python字典中，因此，像任何Python字典一样，你可以打印字段的键名称来检查组件（*图
    15.14*）。
- en: "![Figure 15.14: Co\uFEFFmponents of the Stable Diffusion pipeline](img/B22333_15_14.png)"
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.14：稳定扩散流水线的组件](img/B22333_15_14.png)'
- en: 'Figure 15.14: Components of the Stable Diffusion pipeline'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.14：稳定扩散流水线的组件
- en: 'We’ve seen each of these in action in the prior examples, as will become clearer
    as we walk through the execution of each:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的示例中已经看到过这些组件的实际应用，随着我们逐步执行每个组件，相关细节将变得更加清晰：
- en: The tokenizer takes our prompt and turns it into a byte representation
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词器将我们的提示词转换为字节表示
- en: The text encoder takes that byte representation and turns it into a numerical
    vector
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本编码器将字节表示转换为数值向量
- en: The U-Net, which takes a vector of random numbers and the encoded prompt and
    merges them
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: U-Net，它接受一个随机数向量和编码后的提示词，并将其合并
- en: The scheduler, which runs diffusion steps to “denoise” this merged vector
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度器，运行扩散步骤以对合并后的向量进行“去噪”
- en: The VAE, which converts the merged vector into one or more generated images
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VAE（变分自编码器），它将合并后的向量转换为一个或多个生成的图像
- en: The feature extractor, which extracts elements from the generated image that
    might be labeled as offensive
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取器，它从生成的图像中提取可能被标记为不适宜的元素
- en: The safety checker, which scores those extracted elements to see whether the
    image might be censored
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全检查器，它对提取的元素进行评分，以判断图像是否可能被审查
- en: Let’s step through each component and see how the parameters we looked at earlier
    come into play in the execution.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步解析每个组件，看看我们之前提到的参数如何在执行过程中发挥作用。
- en: The tokenizer
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词器
- en: 'The first step in this pipeline is to convert the prompt into a set of *tokens*,
    or individual elements to be passed into the textual embedding step. You can access
    a lot of information about the tokenizer by printing this pipeline component to
    the notebook:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 该流水线的第一步是将提示词转换为一组*token*，或者是传递给文本嵌入步骤的单个元素。你可以通过在笔记本中打印该流水线组件来访问分词器的很多信息：
- en: "![Figure 15.15: T\uFEFFhe tokenizer properties](img/B22333_15_15.png)"
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.15：分词器属性](img/B22333_15_15.png)'
- en: 'Figure 15.15: The tokenizer properties'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.15：分词器属性
- en: Stable Diffusion uses a **Contrastive Language Image Processing** (**CLIP**)
    model to compute embeddings, which is trained on a joint dataset of images and
    their captions². The tokenizer provides the raw input to compute the textual vectors
    used in the image generation process. You may have encountered tokenization in
    the past in one-hot encoding for natural language processing, in which a word
    (or character) is indexed by a number (for example, each letter in the English
    alphabet can be indexed with the number 0 to 25). Stable Diffusion and similar
    state-of-the-art models use a more efficient embedding than simply mapping each
    word to an index – instead, they map the text to bytes (using an encoding such
    as UTF-8) and represent commonly occurring byte pairs as a single byte, a technique
    called **Byte Pair Encoding** (**BPE**)⁸.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散使用**对比语言图像处理**（**CLIP**）模型来计算嵌入，该模型在图像及其描述的联合数据集上进行训练²。标记器提供了原始输入，用以计算在图像生成过程中使用的文本向量。你可能曾在过去的自然语言处理中的独热编码中遇到过标记化，在这种方法中，一个单词（或字符）通过一个数字进行索引（例如，英文字母的每个字母可以通过数字
    0 到 25 来索引）。稳定扩散和类似的最先进模型使用比简单将每个单词映射到一个索引更高效的嵌入方式——它们将文本映射为字节（使用如 UTF-8 等编码），并将常见的字节对表示为一个字节，这种技术称为**字节对编码**（**BPE**）⁸。
- en: 'BPE is based on the idea that we can compress strings by looking for common
    recurring patterns. Let’s take an example:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: BPE 的基本思想是，通过寻找常见的重复模式来压缩字符串。我们来举个例子：
- en: abcabcabde
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: abcabcabde
- en: 'In the first pass, we notice that the most commonly occurring pair of characters
    is *ab*; we can convert this to a new character, *f*:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次处理时，我们注意到最常出现的字符对是*ab*；我们可以将其转换为一个新字符，*f*：
- en: fcfcfde
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: fcfcfde
- en: 'Now, *fc* is the most commonly occurring pair. Convert this to *g*:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，*fc*是最常出现的字符对。将其转换为*g*：
- en: ggfde
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ggfde
- en: 'Finally, convert *gg* to *h*:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将*gg*转换为*h*：
- en: hfde
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: hfde
- en: We’ve now compressed the input string from 10 characters to 4, which is much
    more efficient to work with computationally. If we need to recover the original
    string, we just to store a lookup table of the pairs and their corresponding character
    to reverse this operation, which we can run recursively.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将输入字符串从 10 个字符压缩成了 4 个字符，这在计算上更为高效。如果我们需要恢复原始字符串，只需存储一个包含字符对及其对应字符的查找表，便可逆转此操作，我们可以递归地执行这一操作。
- en: One additional detail is that while this example used characters, in practice
    we use bytes. This is because special characters like emojis would break a fixed-vocabulary
    character pair compressor since the special characters might not be in the lookup
    table, but all text can be represented uniformly as bytes, making it more robust.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个细节是，尽管这个示例使用了字符，但实际上我们使用的是字节。这是因为像表情符号这样的特殊字符会破坏固定词汇表字符对压缩器，因为这些特殊字符可能不在查找表中，而所有文本都可以统一表示为字节，这使得它更加健壮。
- en: So, to summarize, the tokenizer converts the words in the prompt into bytes
    and uses a pre-computed lookup table of frequently occurring byte pairs to index
    those bytes with a set of IDs. You can see this in action by running just the
    `tokenizer` on the input prompt, as shown in *Figure 15**.16:*
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，标记器将提示中的单词转换为字节，并使用预先计算好的常见字节对查找表，将这些字节索引为一组 ID。你可以通过运行标记器处理输入提示来看到这一过程，正如在*图
    15.16*中所示：
- en: "![Figure 15.16:\uFEFF Converting the prompt to byte token IDs](img/B22333_15_16.png)"
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.16：将提示转换为字节令牌 ID](img/B22333_15_16.png)'
- en: 'Figure 15.16: Converting the prompt to byte token IDs'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.16：将提示转换为字节令牌 ID
- en: You can access the encoding map that Stable Diffusion’s encoder uses through
    the `encoder` property and verify that “320” corresponds to the pair of bytes
    for the letter “a” and whitespace. Similarly, “49406” is a placeholder character
    representing the start of a sentence.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过`encoder`属性访问稳定扩散编码器使用的编码映射，并验证“320”对应字母“a”和空格的字节对。同样，“49406”是一个占位符字符，表示句子的开始。
- en: "![Figure 15.17: T\uFEFFhe tokenizer encoding map](img/B22333_15_17_A.png)"
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.17：标记器编码映射](img/B22333_15_17_A.png)'
- en: 'Figure 15.17: The tokenizer encoding map'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.17：标记器编码映射
- en: Generating text embedding
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成文本嵌入
- en: 'The next step in the pipeline is to transfer the byte-indexed prompt into numerical
    vectors that can be used as inputs to the image generation step of the model.
    This embedding is performed by the CLIP neural network, whose properties you can
    examine in the notebook, as shown in *Figure 15**.18*:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线的下一步是将字节索引的提示转换为数值向量，这些向量可作为模型生成图像步骤的输入。这一嵌入操作由 CLIP 神经网络执行，你可以在笔记本中检查其属性，正如在*图
    15.18*中所示：
- en: "![Figure 15.18: T\uFEFFhe embedding model](img/B22333_15_18.png)"
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.18：嵌入模型](img/B22333_15_18.png)'
- en: 'Figure 15.18: The embedding model'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.18：嵌入模型
- en: Unlike the tokenizer, which was a lookup table, this component is a neural network
    that produces embedding vectors of size 768\. You can see that the layers in this
    network are a stack of 12 transformer modules, followed by a final layer of normalization.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 与查找表形式的分词器不同，这个组件是一个神经网络，它生成大小为768的嵌入向量。你可以看到，这个网络的层次是由12个transformer模块堆叠而成，最后是一层归一化层。
- en: 'If we execute this model on the output from our prior step (cast as a tensor,
    the input type needed for the embedding model, and sent to the GPU with the `to`
    command), we’ll get an output of size 768 (for each token) representing the embedded
    prompt:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在前一步的输出上执行此模型（将其转换为张量，这是嵌入模型所需的输入类型，并通过`to`命令发送到GPU），我们将得到一个大小为768的输出（每个令牌对应一个值），表示嵌入的提示：
- en: "![Figure 15.19: G\uFEFFenerating the embedding from the prompt](img/B22333_15_19.png)"
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.19：从提示生成嵌入](img/B22333_15_19.png)'
- en: 'Figure 15.19: Generating the embedding from the prompt'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.19：从提示生成嵌入
- en: Let’s dissect what is happening in the code block in *Figure 15**.19.* The prompt
    (`"a zombie in the style of Picasso"`) is first passed to the tokenizer in the
    pipeline, with a maximum length of 77 (the maximum number of embeddable tokens).
    As we saw above, this function will return a byte-pair-encoded representation
    of the prompt. These tokens are then mapped to a numerical vector of length 768
    each, which you can verify by examining the shape of the model output.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们剖析*图 15.19*中代码块发生的事情。提示（`"a zombie in the style of Picasso"`）首先传递到管道中的分词器，最大长度为77（可嵌入令牌的最大数量）。正如我们上面所看到的，这个函数将返回一个字节对编码的提示表示。这些令牌然后被映射到一个长度为768的数值向量，你可以通过检查模型输出的形状来验证这一点。
- en: In addition to encoding the prompt itself as a numerical vector, we also encode
    a blank prompt ( ““). This is because when we later pass the embedded prompt to
    the image generation step, we want to control how much importance we assign to
    the prompt in generating the image (using the *guidance scale* parameter we’ll
    see later). To provide a reference, we need to also provide the embedding using
    no prompt at all, and the difference between the two will provide information
    to the image generation model on how to modify the generated image at each step
    of the process.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将提示本身编码为数值向量外，我们还会编码一个空白提示（“”）。这是因为当我们稍后将嵌入的提示传递到图像生成步骤时，我们希望控制在生成图像时分配给提示的权重（使用我们稍后会看到的*引导尺度*参数）。为了提供一个参考，我们还需要提供一个没有任何提示的嵌入，这两者之间的差异将为图像生成模型提供信息，告诉模型在每个步骤中如何修改生成的图像。
- en: Generating the latent image using the VAE decoder
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 VAE 解码器生成潜在图像
- en: To create an image based on your prompt, Stable Diffusion starts with a matrix
    of normally distributed random numbers. This is because, as we mentioned earlier,
    the model was developed using the random vectors (*latent* vectors) generated
    by VAE that we saw in [*Chapter 11*](Chapter_11.xhtml), which consists of an *encoder*
    and a *decoder*. As a reminder, the encoder is a neural network that takes as
    input an image and as output generates a (usually lower dimensional) vector or
    matrix of random numbers. This random number matrix is a kind of “barcode” for
    the image, which allows the important information to be compressed into a lower-dimensional
    space that takes up less memory on your computer – the fact that these vectors
    are smaller than the original image is one of the key optimizations that make
    the Stable Diffusion algorithm work so well. The decoder is a second neural network
    that is used to reverse this compression, turning a set of random numbers into
    an image.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了根据你的提示创建图像，Stable Diffusion 从一组服从正态分布的随机数矩阵开始。这是因为，正如我们之前提到的，该模型是通过 VAE 生成的随机向量（*潜在*向量）开发的，VAE
    在我们在[*第11章*](Chapter_11.xhtml)中看到过，它由一个*编码器*和一个*解码器*组成。作为提醒，编码器是一个神经网络，它将图像作为输入，并输出生成一个（通常是低维的）随机数向量或矩阵。这个随机数矩阵就像图像的“条形码”，它允许将重要信息压缩到一个低维空间，从而减少计算机上的内存占用——这些向量比原始图像要小，这是使
    Stable Diffusion 算法能够如此有效的关键优化之一。解码器是第二个神经网络，用来逆转这一压缩过程，将一组随机数转回为图像。
- en: To see how this works, you can input an image into the `vae` component of the
    Stable Diffusion pipeline, as shown in *Figure 15**.20\.* First, you need to convert
    an input image into a tensor using the `torchvision to_tensor` function, then
    pass it through the encoder to create a 4 x 64 x 64 output – the `half()` command
    is to convert the input to float16\. In this example, you can see we have compressed
    a 512-by-512 RGB image into a 4-by-64-by-64 vector.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个过程，你可以将一张图像输入到 Stable Diffusion 流水线中的 `vae` 组件，如*图 15.20*所示。首先，你需要使用 `torchvision
    to_tensor` 函数将输入图像转换为张量，然后通过编码器将其传递，生成一个 4 x 64 x 64 的输出——`half()` 命令是将输入转换为 float16。在这个例子中，你可以看到我们已经将一张
    512 x 512 的 RGB 图像压缩成了一个 4 x 64 x 64 的向量。
- en: '![Figure 15.20: Generating the latent vector using the VAE](img/B22333_15_20.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.20：使用 VAE 生成潜在向量](img/B22333_15_20.png)'
- en: 'Figure 15.20: Generating the latent vector using the VAE'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.20：使用 VAE 生成潜在向量
- en: Now you can run the decoder to verify that you can turn this latent vector back
    into an image (which is the final step of the Stable Diffusion algorithm you’ll
    see in a bit), as shown in *Figure 15**.21.*
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以运行解码器来验证是否能将这个潜在向量转换回图像（这是你稍后会看到的 Stable Diffusion 算法的最后一步），如*图 15.21*所示。
- en: '![Figure 15.21: Decoding the latent vector](img/B22333_15_21.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.21：解码潜在向量](img/B22333_15_21.png)'
- en: 'Figure 15.21: Decoding the latent vector'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.21：解码潜在向量
- en: Now that we are able to generate samples from a latent vector and encode our
    prompt, we’re ready to generate images using the U-Net, the final network in the
    Stable Diffusion pipeline.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经能够从潜在向量生成样本并编码我们的提示，准备好使用 U-Net 来生成图像，这是 Stable Diffusion 流水线中的最终网络。
- en: The U-Net
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: U-Net
- en: The last element of the Stable Diffusion pipeline is U-Net, which takes the
    encoded prompt and a vector of random noise that is the same shape as an encoded
    image from the VAE (*Figure 15**.2)*. The U-Net, similar to the VAE, performs
    an encoding operation through a set of neural network layers and then decodes
    that output into a vector the same size as the random input. Each time we pass
    the latent vector through the U-Net, we are predicting how much noise, *e*, to
    subtract from the latent vector in the last step. Running this operation multiple
    times constitutes the “reverse” process for the Stable Diffusion model.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion 流水线的最后一个元素是 U-Net，它接受编码的提示和一个与 VAE 编码图像形状相同的随机噪声向量（*图 15.2*）。U-Net
    类似于 VAE，通过一组神经网络层执行编码操作，然后将该输出解码成一个与随机输入相同大小的向量。每次我们将潜在向量通过 U-Net 时，我们都在预测在最后一步需要从潜在向量中减去多少噪声，*e*。多次运行这个操作构成了
    Stable Diffusion 模型的“反向”过程。
- en: Since there was no original image – we supplied a random vector – the encoded
    prompt provides the model with the context of what image to generate.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有原始图像——我们提供了一个随机向量——编码的提示为模型提供了生成图像的上下文。
- en: "![Figure 15.\uFEFF22: The U-Net image generation process](img/B22333_15_22.png)"
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: "![图 15.\uFEFF22：U-Net 图像生成过程](img/B22333_15_22.png)"
- en: 'Figure 15.22: The U-Net image generation process'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.22：U-Net 图像生成过程
- en: Let’s walk through the steps of generating an image. Our first step is to generate
    a random input of the same dimension as the VAE output, using `torch.randn`. We
    set a fixed seed (manual seed) so that we can make this process repeatable by
    generating the same random vector each time we call the code – this will make
    it easy to debug.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步走过生成图像的过程。我们的第一步是使用 `torch.randn` 生成一个与 VAE 输出相同维度的随机输入。我们设置一个固定种子（手动种子），这样每次调用代码时都能生成相同的随机向量——这将方便我们进行调试。
- en: The component of the pipeline that will run the diffusion process – moving a
    random vector to a generated image – is called the *scheduler*. It specifies a
    number of timesteps to run this diffusion process and what properties each of
    those timesteps has. For the Stable Diffusion pipeline we are using, the default
    scheduler is the *PNDMScheduler*⁹. It specifies a set of differential equations
    to use to update the noise prediction at each step of the simulation; the amount
    of noise is determined by a parameter (`init_noise_sigma`) to scale our simple
    random input. Some schedulers apply different scaling/noise at each step of the
    simulation, but the PNDM scheduler does not, so we do not have to call the `scale_model_input`
    function of the scheduler at each step.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 执行扩散过程——将随机向量转换为生成图像——的管道组件被称为*调度器*。它指定了运行此扩散过程所需的时间步数，以及每个时间步的属性。对于我们使用的稳定扩散管道，默认的调度器是*PNDMScheduler*⁹。它指定了一组微分方程，用于更新每步模拟中的噪声预测；噪声的量由一个参数（`init_noise_sigma`）来确定，用于缩放我们的简单随机输入。有些调度器会在每个模拟步骤中应用不同的缩放/噪声，但PNDM调度器不会，因此我们不需要在每步调用调度器的`scale_model_input`函数。
- en: You’ll notice we also concatenate the blank embedding and prompt; this is more
    efficient than processing them sequentially and comparing the output and allows
    us to perform those calculations in parallel. Finally, we set the *guidance scale*
    parameter, which defaults to 7.5\. Lower values assign less importance to the
    input prompt, and will lead to an image that less resembles the prompt. Greater
    values will place more importance on the prompt.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，我们还将空白嵌入和提示拼接在一起；这种方法比顺序处理并比较输出更高效，并且允许我们并行执行这些计算。最后，我们设置了*指导尺度*参数，默认值为7.5。较小的值会减少输入提示的重要性，从而生成的图像与提示的相似度较低。较大的值则会更加重视提示。
- en: At each step of the diffusion process, we duplicate the latent vector so that
    it can be compared with the blank embedding and the prompt. We then pass the textual
    embedding and the latent image vector to the U-Net, which returns a prediction
    of what the latent vector would be without noise. We split this output into two
    parts; one where that output has been conditioned using the embedded prompt and
    one that receives the blank embedding.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步的扩散过程中，我们都会复制潜在向量，以便将其与空白嵌入和提示进行比较。然后，我们将文本嵌入和潜在图像向量传递给U-Net，U-Net返回一个没有噪声的潜在向量预测。我们将该输出分成两部分：一部分是使用嵌入的提示条件化的输出，另一部分接收空白嵌入。
- en: We then create the final U-Net output, `noise_pred`, at each step of the diffusion
    process by adding in a weighted difference between the prompt-conditioned and
    unconditional outputs, with the importance of that difference provided by the
    `guidance_scale`. Then we run the scheduler diffusion equation to generate the
    input for the next pass.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在扩散过程的每一步创建最终的U-Net输出`noise_pred`，方法是将提示条件化输出和无条件输出之间的加权差异相加，这个差异的权重由`guidance_scale`提供。接着，我们运行调度器扩散方程，以生成下一轮输入。
- en: After several rounds (here, 50) of passing the random vector through the U-Net,
    we decode it with the VAE to get the final output. The code in *Figure 15**.23*
    shows how this happens.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 经过多轮（这里是50轮）将随机向量通过U-Net，我们使用VAE进行解码以获得最终输出。*图15.23*中的代码展示了这一过程是如何发生的。
- en: "![Figure\uFEFF 15.23: Decoding the U-Net output with the VAE](img/B22333_15_23.png)"
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图15.23：使用VAE解码U-Net输出](img/B22333_15_23.png)'
- en: 'Figure 15.23: Decoding the U-Net output with the VAE'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.23：使用VAE解码U-Net输出
- en: We need to undo the noise scaling we applied at the beginning of the scheduler
    (`init_sigma_noise`) by dividing by the *random* variable we had used as a multiplier
    earlier when we began the diffusion process, then use the decoder arm of the VAE
    to obtain the image from the latent vector. We recenter the output and then bind
    it between 0 and 1 so that the colors will show up correctly in the notebook.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要通过除以在调度器开始时应用的*随机*变量（`init_sigma_noise`），以撤销我们在调度器开始时进行的噪声缩放操作。然后，使用VAE的解码器臂从潜在向量中获取图像。我们重新调整输出的中心位置，并将其绑定在0和1之间，以确保颜色在笔记本中正确显示。
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at how the Stable Diffusion algorithm was developed
    and how it is implemented through the Hugging Face pipeline API. In the process,
    we saw how a diffusion model addresses conceptual problems with autoregressive
    transformer and GAN models by modeling the distribution of natural pixels. We
    also saw how this generative diffusion process can be represented as a reversible
    Markov process, and how we can train the parameters of a diffusion model using
    a variational bound, similar to a VAE.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了稳定扩散算法是如何开发的，以及它是如何通过 Hugging Face pipeline API 实现的。在这个过程中，我们看到扩散模型是如何通过建模自然像素的分布来解决自回归变换器和
    GAN 模型的概念性问题的。我们还看到，这一生成性扩散过程可以被表示为一个可逆的马尔可夫过程，以及我们如何通过变分界限训练扩散模型的参数，这与 VAE 类似。
- en: Furthermore, we saw how the efficiency of a diffusion model is improved by executing
    the forward and reverse process in latent space in the Stable Diffusion model.
    We also illustrated how natural language user prompts are represented as byte
    encodings and transformed into numerical vectors. Finally, we looked at the role
    of the VAE in generating compressed image vectors, and how the U-Net of Stable
    Diffusion uses the embedded user prompt and a vector of random numbers to generate
    images by predicting the amount of noise that should be removed in each step of
    the reverse process.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还看到，在稳定扩散模型中，通过在潜在空间中执行正向和反向过程，扩散模型的效率得到了提高。我们还说明了如何将自然语言用户提示表示为字节编码，并将其转换为数值向量。最后，我们探讨了
    VAE 在生成压缩图像向量中的作用，以及稳定扩散的 U-Net 如何使用嵌入的用户提示和随机数向量，通过预测每一步反向过程中应去除的噪声量来生成图像。
- en: References
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Ramesh, Aditya et al. “*Zero-Shot Text-to-Image Generation.*” *ArXiv* abs/2102.12092
    (2021).
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ramesh, Aditya 等人. “*零-shot 文本到图像生成.*” *ArXiv* abs/2102.12092 (2021).
- en: Brock, Andrew; Donahue, Jeff; and Simonyan, Karen. “*Large scale GAN training
    for high fidelity natural image synthesis.*” *arXiv preprint arXiv:1809.11096*
    (2018).
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Brock, Andrew; Donahue, Jeff; 和 Simonyan, Karen. “*大规模 GAN 训练用于高保真自然图像合成.*”
    *arXiv 预印本 arXiv:1809.11096* (2018).
- en: 'Sohl-Dickstein, Jascha; Weiss, Eric; Maheswaranathan, Niru; and Ganguli, Surya
    (2015-06-01). `"Deep Unsupervised Learning using Nonequilibrium Thermodynamics"`
    (PDF). *Proceedings of the 32nd International Conference on Machine Learning*.
    37\. PMLR: 2256–2265.'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Sohl-Dickstein, Jascha; Weiss, Eric; Maheswaranathan, Niru; 和 Ganguli, Surya
    (2015-06-01). `"深度无监督学习与非平衡热力学"`（PDF）。*第32届国际机器学习大会论文集*。37\. PMLR: 2256–2265.'
- en: 'Ho, Jonathan; Jain, Ajay; and Abbeel, Pieter. “*Denoising diffusion probabilistic
    models.*” *Advances in neural information processing systems* 33 (2020): 6840-6851.'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Ho, Jonathan; Jain, Ajay; 和 Abbeel, Pieter. “*去噪扩散概率模型.*” *神经信息处理系统进展* 33 (2020):
    6840-6851.'
- en: 'Rombach, Robin et al. “*High-Resolution Image Synthesis with Latent Diffusion
    Models.*” *2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)* (2021): 10674-10685.'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Rombach, Robin 等人. “*基于潜在扩散模型的高分辨率图像合成.*” *2022 IEEE/CVF 计算机视觉与模式识别会议（CVPR）*
    (2021): 10674-10685.'
- en: 'Ronneberger, Olaf; Fischer, Philipp; and Brox, Thomas. Unet: Convolutional
    networks for biomedical image segmentation. In MICCAI (3), volume 9351 of Lecture
    Notes in Computer Science, pages 234–241\. Springer, 2015.'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Ronneberger, Olaf; Fischer, Philipp; 和 Brox, Thomas. Unet: 用于生物医学图像分割的卷积网络.
    在 MICCAI (3) 中，计算机科学讲义第 9351 卷，页 234–241\. Springer，2015。'
- en: Radford, Alec et al. “*Learning transferable visual models from natural language
    supervision.*” *International conference on machine learning*. PmLR, 2021.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Radford, Alec 等人. “*从自然语言监督中学习可迁移的视觉模型.*” *国际机器学习大会*。PmLR，2021。
- en: '[http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)'
- en: '[https://arxiv.org/pdf/2202.09778.pdf](https://arxiv.org/pdf/2202.09778.pdf)
    Liu, Luping et al. “*Pseudo numerical methods for diffusion models on manifolds.*”
    *arXiv preprint arXiv:2202.09778* (2022).'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/2202.09778.pdf](https://arxiv.org/pdf/2202.09778.pdf)
    Liu, Luping 等人. “*流形上的扩散模型伪数值方法.*” *arXiv 预印本 arXiv:2202.09778* (2022).'
- en: Join our communities on Discord and Reddit
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们在 Discord 和 Reddit 上的社区
- en: Have questions about the book or want to contribute to discussions on Generative
    AI and LLMs? Join our Discord server at [https://packt.link/I1tSU](https://packt.link/I1tSU)
    and our Reddit channel at [https://packt.link/rmYYs](https://packt.link/rmYYs)
    to connect, share, and collaborate with like-minded AI professionals.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对本书有疑问或想参与生成式AI和LLM的讨论？加入我们的Discord服务器 [https://packt.link/I1tSU](https://packt.link/I1tSU)
    和Reddit频道 [https://packt.link/rmYYs](https://packt.link/rmYYs)，与志同道合的AI专业人士连接、分享和合作。
- en: '| **Discord QR** | **Reddit QR** |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| **Discord二维码** | **Reddit二维码** |'
- en: '| ![](img/Discord_Babcock_1.png) | ![](img/Reddit_Babcock.png) |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/Discord_Babcock_1.png) | ![](img/Reddit_Babcock.png) |'
- en: '![](img/New_Packt_Logo1.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/New_Packt_Logo1.png)'
- en: '[www.packtpub.com](https://www.packtpub.com)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.packtpub.com](https://www.packtpub.com)'
- en: Subscribe to our online digital library for full access to over 7,000 books
    and videos, as well as industry leading tools to help you plan your personal development
    and advance your career. For more information, please visit our website.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 订阅我们的在线数字图书馆，完全访问超过7,000本书籍和视频，以及行业领先的工具，帮助你规划个人发展并推进职业生涯。更多信息，请访问我们的网站。
- en: Why subscribe?
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要订阅？
- en: Spend less time learning and more time coding with practical eBooks and Videos
    from over 4,000 industry professionals
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过来自4000多名行业专家的实用电子书和视频，减少学习时间，增加编码时间
- en: Improve your learning with Skill Plans built especially for you
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过为你量身定制的技能计划提高学习效果
- en: Get a free eBook or video every month
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每月获得一本免费的电子书或视频
- en: Fully searchable for easy access to vital information
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全可搜索，轻松访问重要信息
- en: Copy and paste, print, and bookmark content
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制粘贴、打印和书签内容
- en: At [www.packt.com](https://www.packt.com), you can also read a collection of
    free technical articles, sign up for a range of free newsletters, and receive
    exclusive discounts and offers on Packt books and eBooks.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [www.packt.com](https://www.packt.com)，你还可以阅读一系列免费的技术文章，注册各种免费的新闻通讯，并获得Packt书籍和电子书的独家折扣和优惠。
- en: Other Books You May Enjoy
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你可能会喜欢的其他书籍
- en: 'If you enjoyed this book, you may be interested in these other books by Packt:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这本书，可能会对Packt的其他书籍感兴趣：
- en: '[![](img/9781836200079.jpg)](https://www.packtpub.com/en-us/product/llm-engineers-handbook-9781836200062)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/9781836200079.jpg)](https://www.packtpub.com/en-us/product/llm-engineers-handbook-9781836200062)'
- en: '**LLM Engineer’s Handbook**'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLM工程师手册**'
- en: Paul Iusztin, Maxime Labonne
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Paul Iusztin，Maxime Labonne
- en: 'ISBN: 978-1-83620-007-9'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 'ISBN: 978-1-83620-007-9'
- en: Implement robust data pipelines and manage LLM training cycles
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施强大的数据管道并管理LLM训练周期
- en: Create your own LLM and refine it with the help of hands-on examples
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建你自己的LLM，并通过实际案例的帮助来优化它
- en: Get started with LLMOps by diving into core MLOps principles such as orchestrators
    and prompt monitoring
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过深入了解核心MLOps原则（如协调器和提示监控）开始学习LLMOps
- en: Perform supervised fine-tuning and LLM evaluation
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行监督式微调和LLM评估
- en: Deploy end-to-end LLM solutions using AWS and other tools
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AWS和其他工具部署端到端的LLM解决方案
- en: Design scalable and modularLLM systems
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计可扩展和模块化的LLM系统
- en: Learn about RAG applications by building a feature and inference pipeline
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过构建特性和推理管道来了解RAG应用
- en: '[![](img/9781803247281.jpg)](https://www.packtpub.com/en-us/product/generative-ai-with-amazon-bedrock-9781804618585)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/9781803247281.jpg)](https://www.packtpub.com/en-us/product/generative-ai-with-amazon-bedrock-9781804618585)'
- en: '**Generative AI with Amazon Bedrock**'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过Amazon Bedrock生成式AI**'
- en: Shikhar Kwatra, Bunny Kaushik
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Shikhar Kwatra，Bunny Kaushik
- en: 'ISBN: 978-1-80324-728-1'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 'ISBN: 978-1-80324-728-1'
- en: Explore the generative AI landscape and foundation models in Amazon Bedrock
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索Amazon Bedrock中的生成式AI领域和基础模型
- en: Fine-tune generative models to improve their performance
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调生成式模型以提高其性能
- en: Explore several architecture patterns for different business use cases
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索不同商业用例的多个架构模式
- en: Gain insights into ethical AI practices, model governance, and risk mitigation
    strategies
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取有关道德AI实践、模型治理和风险缓解策略的见解
- en: Enhance your skills in employing agents to develop intelligence and orchestrate
    tasks
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强你使用代理开发智能和协调任务的技能
- en: Monitor and understand metrics and Amazon Bedrock model response
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控和理解Amazon Bedrock模型响应的指标
- en: Explore various industrial use cases and architectures to solve real-world business
    problems using RAG
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索各种工业用例和架构，利用RAG解决现实世界的商业问题
- en: Stay on top of architectural best practices and industry standards
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 紧跟架构最佳实践和行业标准
- en: Packt is searching for authors like you
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Packt正在寻找像你这样的作者
- en: If you’re interested in becoming an author for Packt, please visit [authors.packtpub.com](https://authors.packtpub.com)
    and apply today. We have worked with thousands of developers and tech professionals,
    just like you, to help them share their insight with the global tech community.
    You can make a general application, apply for a specific hot topic that we are
    recruiting an author for, or submit your own idea.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣成为 Packt 的作者，请访问 [authors.packtpub.com](https://authors.packtpub.com)
    并今天就申请。我们已经与成千上万的开发者和技术专业人士合作，像你一样，帮助他们与全球技术社区分享他们的见解。你可以提交一般申请，申请我们正在招聘作者的特定热门话题，或者提交你自己的创意。
- en: Share your thoughts
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分享你的想法
- en: Now you’ve finished *Generative AI with Python and PyTorch*, *Second Edition*,
    we’d love to hear your thoughts! If you purchased the book from Amazon, please
    [click here to go straight to the Amazon review page](https://packt.link/r/1835884458)
    for this book and share your feedback or leave a review on the site that you purchased
    it from.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经完成了 *《Python 和 PyTorch 的生成式 AI》*（*第二版*），我们很想听听你的想法！如果你是从 Amazon 购买的本书，请
    [点击这里直接进入 Amazon 书评页面](https://packt.link/r/1835884458)，分享你的反馈或在你购买的站点上留下评论。
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你的评论对我们和技术社区来说非常重要，将帮助我们确保提供卓越的内容质量。
