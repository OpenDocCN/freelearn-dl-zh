- en: Constructing an LSTM Network for Time Series
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建用于时间序列的LSTM网络
- en: In this chapter, we will discuss how to construct a **long short-term memory**
    (**LSTM**) neural network to solve a medical time series problem. We will be using
    data from 4,000 **i****ntensive care unit** (**ICU**) patients. Our goal is to
    predict the mortality of patients using a given set of generic and sequential
    features. We have six generic features, such as age, gender, and weight. Also,
    we have 37 sequential features, such as cholesterol level, temperature, pH, and
    glucose level. Each patient has multiple measurements recorded against these sequential
    features. The number of measurements taken from each patient differs. Furthermore,
    the time between measurements also differs among patients.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论如何构建**长短期记忆**（**LSTM**）神经网络来解决医学时间序列问题。我们将使用来自4,000名**重症监护病房**（**ICU**）患者的数据。我们的目标是通过给定的一组通用和序列特征来预测患者的死亡率。我们有六个通用特征，如年龄、性别和体重。此外，我们还有37个序列特征，如胆固醇水平、体温、pH值和葡萄糖水平。每个患者都有多个针对这些序列特征的测量记录。每个患者的测量次数不同。此外，不同患者之间测量的时间间隔也有所不同。
- en: LSTM is well-suited to this type of problem due to the sequential nature of
    the data. We could also solve it using a regular **recurrent neural network**
    (**RNN**), but the purpose of LSTM is to avoid vanishing and exploding gradients.
    LSTM is capable of capturing long-term dependencies because of its cell state.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据的序列性质，LSTM非常适合此类问题。我们也可以使用普通的**递归神经网络**（**RNN**）来解决，但LSTM的目的是避免梯度消失和梯度爆炸。LSTM能够捕捉长期依赖关系，因为它具有单元状态。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下配方：
- en: Extracting and reading clinical data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取和读取临床数据
- en: Loading and transforming data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载和转换数据
- en: Constructing input layers for a network
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建网络的输入层
- en: Constructing output layers for a network
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建网络的输出层
- en: Training time series data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练时间序列数据
- en: Evaluating the LSTM network's efficiency
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估LSTM网络的效率
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: A concrete implementation of the use case discussed in this chapter can be found
    here: [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/06_Constructing_LSTM_Network_for_time_series/sourceCode/cookbookapp-lstm-time-series/src/main/java/LstmTimeSeriesExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/06_Constructing_LSTM_Network_for_time_series/sourceCode/cookbookapp-lstm-time-series/src/main/java/LstmTimeSeriesExample.java).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的用例的具体实现可以在这里找到：[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/06_Constructing_LSTM_Network_for_time_series/sourceCode/cookbookapp-lstm-time-series/src/main/java/LstmTimeSeriesExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/06_Constructing_LSTM_Network_for_time_series/sourceCode/cookbookapp-lstm-time-series/src/main/java/LstmTimeSeriesExample.java)。
- en: After cloning the GitHub repository, navigate to the `Java-Deep-Learning-Cookbook/06_Constructing_LSTM_Network_for_time_series/sourceCode`
    directory. Then, import the `cookbookapp-lstm-time-series` project as a Maven
    project by importing `pom.xml`.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 克隆GitHub仓库后，进入`Java-Deep-Learning-Cookbook/06_Constructing_LSTM_Network_for_time_series/sourceCode`目录。然后，通过导入`pom.xml`，将`cookbookapp-lstm-time-series`项目作为Maven项目导入。
- en: Download the clinical time series data from here: [https://skymindacademy.blob.core.windows.net/physionet2012/physionet2012.tar.gz](https://skymindacademy.blob.core.windows.net/physionet2012/physionet2012.tar.gz).
    The dataset is from the PhysioNet Cardiology Challenge 2012.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里下载临床时间序列数据：[https://skymindacademy.blob.core.windows.net/physionet2012/physionet2012.tar.gz](https://skymindacademy.blob.core.windows.net/physionet2012/physionet2012.tar.gz)。该数据集来自PhysioNet心脏病挑战2012。
- en: 'Unzip the package after the download. You should see the following directory
    structure:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 下载后解压文件。你应该会看到以下目录结构：
- en: '![](img/008ce812-7845-46e1-bb46-a61d4f155a8c.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/008ce812-7845-46e1-bb46-a61d4f155a8c.png)'
- en: The features are contained in a directory called `sequence` and the labels are
    contained in a directory called `mortality`. Ignore the other directories for
    now. You need to update file paths to features/labels in the source code to run
    the example.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储在名为`sequence`的目录中，标签存储在名为`mortality`的目录中。暂时忽略其他目录。你需要在源代码中更新特征/标签的文件路径，以便运行示例。
- en: Extracting and reading clinical data
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取和读取临床数据
- en: '**ETL** (short for **Extract, Transform, and Load**) is the most important
    step in any deep learning problem. We''re focusing on data extraction in this
    recipe, where we will discuss how to extract and process clinical time series
    data. We have learned about regular data types, such as normal CSV/text data and
    images, in previous chapters. Now, let''s discuss how to deal with time series
    data. We will use clinical time series data to predict the mortality of patients.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**ETL**（提取、转换、加载的缩写）是任何深度学习问题中最重要的一步。在本方案中，我们将重点讨论数据提取，其中我们将讨论如何提取和处理临床时间序列数据。我们在前几章中了解了常规数据类型，例如普通的CSV/文本数据和图像。现在，让我们讨论如何处理时间序列数据。我们将使用临床时间序列数据来预测患者的死亡率。'
- en: How to do it...
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create an instance of `NumberedFileInputSplit`  to club all feature files together:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`NumberedFileInputSplit`实例，将所有特征文件合并在一起：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create an instance of `NumberedFileInputSplit`  to club all label files together:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`NumberedFileInputSplit`实例，将所有标签文件合并在一起：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create record readers for features/labels:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为特征/标签创建记录读取器：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How it works...
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Time series data is three-dimensional. Each sample is represented by its own
    file. Feature values in columns are measured on different time steps denoted by
    rows. For instance, in step 1, we saw the following snapshot, where time series
    data is displayed:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据是三维的。每个样本由它自己的文件表示。列中的特征值是在不同的时间步骤上测量的，这些时间步骤由行表示。例如，在第1步中，我们看到了下面的快照，其中显示了时间序列数据：
- en: '![](img/aaad59fe-f05f-47be-9ca0-94fa113e7b3a.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aaad59fe-f05f-47be-9ca0-94fa113e7b3a.png)'
- en: 'Each file represents a different sequence. When you open the file, you will
    see the observations (features) recorded on different time steps, as shown here:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文件代表一个不同的序列。当你打开文件时，你会看到在不同时间步骤上记录的观察值（特征），如下所示：
- en: '![](img/ff49eeb2-399f-4e88-8194-03612a69d5f1.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff49eeb2-399f-4e88-8194-03612a69d5f1.png)'
- en: The labels are contained in a single CSV file, which contains a value of `0`,
    indicating death, or a value of `1`, indicating survival. For example, for the
    features in `1.csv`, the output labels are in `1.csv` under the mortality directory. Note
    that we have a total of 4,000 samples. We divide the entire dataset into train/test
    sets so that our training data has 3,200 examples and the testing data has 800
    examples.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 标签包含在一个CSV文件中，其中包含值`0`表示死亡，值`1`表示生存。例如，对于`1.csv`中的特征，输出标签位于死亡目录下的`1.csv`中。请注意，我们共有4000个样本。我们将整个数据集分为训练集和测试集，使得训练数据包含3200个样本，测试数据包含800个样本。
- en: In step 3, we used `NumberedFileInputSplit`to read and club all the files (features/labels)
    with a numbered format.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3步中，我们使用了`NumberedFileInputSplit`来读取并将所有文件（特征/标签）以编号格式合并在一起。
- en: '`CSVSequenceRecordReader` is to read sequences of data in CSV format, where each
    sequence is defined in its own file.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`CSVSequenceRecordReader`用于读取CSV格式的数据序列，其中每个序列都定义在自己的文件中。'
- en: As you can see in the preceding screenshots, the first row is just meant for
    feature labels and needs to be bypassed.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，第一行仅用于特征标签，需要跳过。
- en: 'Hence, we have created the following CSV sequence reader:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们创建了以下CSV序列读取器：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Loading and transforming data
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载和转换数据
- en: After the data extraction phase, we need to transform the data before loading
    it into a neural network. During data transformation, it is very important to
    ensure that any non-numeric fields in the dataset are transformed into numeric
    fields. The role of data transformation doesn't end there. We can also remove
    any noise in the data and adjust the values. In this recipe, we load the data
    into a dataset iterator and transform the data as required.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据提取阶段之后，我们需要在将数据加载到神经网络之前进行数据转换。在数据转换过程中，确保数据集中的任何非数字字段都被转换为数字字段是非常重要的。数据转换的作用不仅仅是这样。我们还可以去除数据中的噪声并调整数值。在此方案中，我们将数据加载到数据集迭代器中，并按需要转换数据。
- en: We extracted the time series data into record reader instances in the previous
    recipe. Now, let's create train/test iterators from them. We will also analyze
    the data and transform it if needed.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个方案中，我们将时间序列数据提取到记录读取器实例中。现在，让我们从这些实例中创建训练/测试迭代器。我们还将分析数据并在需要时进行转换。
- en: Getting ready
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Before we proceed, refer to the dataset in the following screenshot to understand
    how every sequence of the data looks:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，请参考下面的截图中的数据集，以了解每个数据序列的样子：
- en: '![](img/509bb04f-9227-4878-87d5-d1211bd1d1dc.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/509bb04f-9227-4878-87d5-d1211bd1d1dc.png)'
- en: Firstly, we need to check for the existence of any non-numeric features in the
    data. We need to load the data into the neural network for training, and it should
    be in a format that the neural network can understand. We have a sequenced dataset
    and it appears that non-numeric values are not present. All 37 features are numeric.
    If you look at the range of feature data, it is close to a normalized format.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要检查数据中是否存在任何非数值特征。我们需要将数据加载到神经网络中进行训练，并且它应该是神经网络能够理解的格式。我们有一个顺序数据集，并且看起来没有非数值值。所有
    37 个特征都是数值型的。如果查看特征数据的范围，它接近于标准化格式。
- en: How to do it...
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何做的...
- en: 'Create the training iterator using `SequenceRecordReaderDataSetIterator`:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `SequenceRecordReaderDataSetIterator` 创建训练迭代器：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Create the test iterator using `SequenceRecordReaderDataSetIterator`:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `SequenceRecordReaderDataSetIterator` 创建测试迭代器：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How it works...
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In steps 1 and 2, we used `AlignmentMode` while creating the iterators for
    the training and test datasets. The `AlignmentMode` deals with input/labels of
    varying lengths (for example, one-to-many and many-to-one situations). Here are
    some types of alignment modes:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 1 和 2 中，我们在创建训练和测试数据集的迭代器时使用了`AlignmentMode`。`AlignmentMode` 处理不同长度的输入/标签（例如，一对多和多对一的情况）。以下是一些对齐模式的类型：
- en: '`ALIGN_END`: This is intended to align labels or input at the last time step.
    Basically, it adds zero padding at the end of either the input or the labels.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ALIGN_END`：这是用于在最后一个时间步对齐标签或输入。基本上，它在输入或标签的末尾添加零填充。'
- en: '`ALIGN_START`: This is intended to align labels or input at the first time
    step. Basically, it adds zero padding at the end of the input or the labels.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ALIGN_START`：这是用于在第一个时间步对齐标签或输入。基本上，它在输入或标签的末尾添加零填充。'
- en: '`EQUAL_LENGTH`: This assumes that the input time series and label are of the
    same length, and all examples are the same length.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EQUAL_LENGTH`：这假设输入时间序列和标签具有相同的长度，并且所有示例的长度都相同。'
- en: '`SequenceRecordReaderDataSetIterator`: This helps to generate a time series
    dataset from the record reader passed in. The record reader should be based on
    sequence data and is optimal for time series data. Check out the attributes passed
    to the constructor:'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SequenceRecordReaderDataSetIterator`：这个工具帮助从传入的记录读取器生成时间序列数据集。记录读取器应基于序列数据，最适合用于时间序列数据。查看传递给构造函数的属性：'
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`testFeaturesReader` and `testLabelsReader` are record reader objects for input
    data (features) and labels (for evaluation), respectively. The Boolean attribute
    (`false`) refers to whether we have regression samples. Since we are talking about
    time series classification, this is going to be false. For regression data, this
    has to be set to `true`.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`testFeaturesReader` 和 `testLabelsReader` 分别是输入数据（特征）和标签（用于评估）的记录读取器对象。布尔属性（`false`）表示我们是否有回归样本。由于我们在讨论时间序列分类问题，这里为
    `false`。对于回归数据，必须将其设置为 `true`。'
- en: Constructing input layers for the network
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建网络的输入层
- en: LSTM layers will have gated cells that are capable of capturing long-term dependencies,
    unlike regular RNN. Let's discuss how we can add a special LSTM layer in our network
    configuration. We can use a multilayer network or computation graph to create
    the model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 层将具有门控单元，能够捕捉长期依赖关系，不同于常规 RNN。让我们讨论一下如何在网络配置中添加一个特殊的 LSTM 层。我们可以使用多层网络或计算图来创建模型。
- en: In this recipe, we will discuss how to create input layers for our LSTM neural
    network. In the following example, we will construct a computation graph and add
    custom layers to it.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将讨论如何为我们的 LSTM 神经网络创建输入层。在以下示例中，我们将构建一个计算图，并向其中添加自定义层。
- en: How to do it...
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何做的...
- en: 'Configure the neural network using `ComputationGraph`, as shown here:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `ComputationGraph` 配置神经网络，如下所示：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Configure the LSTM layer:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置 LSTM 层：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Add the LSTM layer to the `ComputationGraph` configuration:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 LSTM 层添加到 `ComputationGraph` 配置中：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: How it works...
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In step 1, we defined a graph vertex input as the following after calling the
    `graphBuilder()` method:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 1 中，我们在调用 `graphBuilder()` 方法后定义了一个图顶点输入，如下所示：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: By calling `graphBuilder()`, we are actually constructing a graph builder to
    create a computation graph configuration.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用`graphBuilder()`，我们实际上是在构建一个图构建器，以创建计算图配置。
- en: 'Once the LSTM layers are added into the `ComputationGraph` configuration in
    step 3, they will act as input layers in the `ComputationGraph` configuration. We
    pass the previously mentioned graph vertex input (`trainFeatures`) to our LSTM
    layer, as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦LSTM层在步骤3中被添加到`ComputationGraph`配置中，它们将作为输入层存在于`ComputationGraph`配置中。我们将前面提到的图顶点输入（`trainFeatures`）传递给我们的LSTM层，如下所示：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The last attribute, `trainFeatures`, refers to the graph vertex input. Here,
    we're specifying that the `L1` layer is the input layer.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的属性`trainFeatures`指的是图的顶点输入。在这里，我们指定`L1`层为输入层。
- en: The main purpose of the LSTM neural network is to capture the long-term dependencies
    in the data. The derivatives of a `tanh` function can sustain for a long range
    before reaching the zero value. Hence, we use `Activation.TANH` as the activation
    function for the LSTM layer.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM神经网络的主要目的是捕获数据中的长期依赖关系。`tanh`函数的导数在达到零值之前可以持续很长一段时间。因此，我们使用`Activation.TANH`作为LSTM层的激活函数。
- en: The `forgetGateBiasInit()`set forgets gate bias initialization. Values in the
    range of `1` to `5` could potentially help with learning or long-term dependencies.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`forgetGateBiasInit()`设置忘记门的偏置初始化。`1`到`5`之间的值可能有助于学习或长期依赖关系的捕获。'
- en: We use the `Builder`strategy to define the LSTM layers along with the required
    attributes, such as `nIn` and `nOut`.These are input/output neurons, as we saw
    in [Chapters 3](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml), *Building Deep Neural
    Networks for Binary Classification*, and [Chapter 4](4a688ef9-2dd8-47de-abaf-456fa88bcfc2.xhtml),
    *Building Convolutional Neural Networks*. We add LSTM layers using the `addLayer` method.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`Builder`策略来定义LSTM层及其所需的属性，例如`nIn`和`nOut`。这些是输入/输出神经元，正如我们在[第3章](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml)，*构建二分类深度神经网络*和[第4章](4a688ef9-2dd8-47de-abaf-456fa88bcfc2.xhtml)，*构建卷积神经网络*中所看到的那样。我们通过`addLayer`方法添加LSTM层。
- en: Constructing output layers for the network
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建网络的输出层
- en: The output layer design is the last step in configuring the neural network layer.
    Our aim is to implement a time series prediction model. We need to develop a time
    series classifier to predict patient mortality. The output layer design should
    reflect this purpose. In this recipe, we will discuss how to construct the output
    layer for our use case.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层设计是配置神经网络层的最后一步。我们的目标是实现一个时间序列预测模型。我们需要开发一个时间序列分类器来预测患者的死亡率。输出层的设计应该反映这一目标。在本教程中，我们将讨论如何为我们的用例构建输出层。
- en: How to do it...
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Design the output layer using `RnnOutputLayer`:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`RnnOutputLayer`设计输出层：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Use the `addLayer()` method to add an output layer to the network configuration:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`addLayer()`方法将输出层添加到网络配置中：
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: How it works...
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何工作...
- en: While constructing the output layer, make note of the `nOut` value of the preceding
    LSTM input layer. This will be taken as `nIn` for the output layer. `nIn` should
    be the same as `nOut` of the preceding LSTM input layer.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建输出层时，注意前一个LSTM输入层的`nOut`值。这个值将作为输出层的`nIn`。`nIn`应该与前一个LSTM输入层的`nOut`值相同。
- en: In steps 1 and step 2, we are essentially creating an LSTM neural network, an
    extended version of a regular RNN. We used gated cells to have some sort of internal
    memory to hold long-term dependencies. For a predictive model to make predictions
    (patient mortality), we need to have probability produced by the output layer.
    In step 2, we see that `SOFTMAX` is used at the output layer of a neural network.
    This activation function is very helpful for computing the probability for the
    specific label. `MCXENT` is the ND4J implementation for the negative loss likelihood
    error function. Since we use the negative loss likelihood loss function, it will
    push the results when the probability value is found to be high for a label on
    a particular iteration.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤1和步骤2中，我们实际上是在创建一个LSTM神经网络，它是常规RNN的扩展版本。我们使用了门控单元来实现某种内部记忆，以保持长期依赖关系。为了使预测模型能够进行预测（如患者死亡率），我们需要通过输出层生成概率。在步骤2中，我们看到`SOFTMAX`被用作神经网络输出层的激活函数。这个激活函数在计算特定标签的概率时非常有用。`MCXENT`是ND4J中负对数似然误差函数的实现。由于我们使用的是负对数似然损失函数，它将在某次迭代中，当某个标签的概率值较高时，推动结果的输出。
- en: '`RnnOutputLayer`is more like an extended version of regular output layers found
    in feed-forward networks. We can also use `RnnOutputLayer` for one-dimensional
    CNN layers. There is also another output layer, named `RnnLossLayer`,where the
    input and output activations are the same. In the case of `RnnLossLayer`, we have
    three dimensions with the `[miniBatchSize,nIn,timeSeriesLength]` and `[miniBatchSize,nOut,timeSeriesLength]` shape,
    respectively.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`RnnOutputLayer`更像是常规前馈网络中的扩展版本输出层。我们还可以将`RnnOutputLayer`用于一维的CNN层。还有另一个输出层，叫做`RnnLossLayer`，其输入和输出激活相同。在`RnnLossLayer`的情况下，我们有三个维度，分别是`[miniBatchSize,
    nIn, timeSeriesLength]`和`[miniBatchSize, nOut, timeSeriesLength]`的形状。'
- en: 'Note that we''ll have to specify the input layer that is to be connected to
    the output layer. Take a look at this code again:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们必须指定要连接到输出层的输入层。再看看这段代码：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We mentioned that the `L1` layer is the input layer to the output layer.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到过，`L1`层是从输入层到输出层的。
- en: Training time series data
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练时间序列数据
- en: So far, we have constructed network layers and parameters to define the model
    configuration. Now it's time to train the model and see the results. We can then
    check whether any of the previously-defined model configuration can be altered
    to obtain optimal results. Be sure to run the training instance multiple times
    before making any conclusions from the very first training session. We need to
    observe a consistent output to ensure stable performance.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经构建了网络层和参数来定义模型配置。现在是时候训练模型并查看结果了。然后，我们可以检查是否可以修改任何先前定义的模型配置，以获得最佳结果。在得出第一个训练会话的结论之前，务必多次运行训练实例。我们需要观察稳定的输出，以确保性能稳定。
- en: In this recipe, we train our LSTM neural network against the loaded time series
    data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将训练LSTM神经网络来处理加载的时间序列数据。
- en: How to do it...
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Create the `ComputationGraph` model from the previously-created model configuration:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从之前创建的模型配置中创建`ComputationGraph`模型：
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Load the iterator and train the model using the `fit()` method:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载迭代器并使用`fit()`方法训练模型：
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You can use the following approach as well:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用以下方法：
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We can then avoid using a `for` loop by directly specifying the `epochs` parameter
    in the `fit()` method.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过直接在`fit()`方法中指定`epochs`参数来避免使用`for`循环。
- en: How it works...
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In step 2, we pass both the dataset iterator and epoch count to start the training
    session. We use a very large time series dataset, hence a large epoch value will
    result in more training time. Also, a large epoch may not always guarantee good
    results, and may end up overfitting. So, we need to run the training experiment
    multiple times to arrive at an optimal value for epochs and other important hyperparameters.
    An optimal value would be the bound where you observe the maximum performance
    for the neural network.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2步中，我们将数据集迭代器和训练轮数传递给训练会话。我们使用了一个非常大的时间序列数据集，因此较大的轮数将导致更长的训练时间。此外，较大的轮数并不总是能保证良好的结果，甚至可能导致过拟合。所以，我们需要多次运行训练实验，以找到轮数和其他重要超参数的最佳值。最佳值是指你观察到神经网络性能最大化的界限。
- en: Effectively, we are optimizing our training process using memory-gated cells
    in layers. As we discussed earlier, in the *Constructing input layers for the
    network* recipe, LSTMs are good for holding long-term dependencies in datasets.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们在使用内存门控单元优化训练过程。正如我们之前在*构建网络的输入层*这一部分所讨论的，LSTM非常适合在数据集中保持长期依赖关系。
- en: Evaluating the LSTM network's efficiency
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估LSTM网络的效率
- en: After each training iteration, the network's efficiency is measured by evaluating
    the model against a set of evaluation metrics. We optimize the model further on
    upcoming training iterations based on the evaluation metrics. We use the test
    dataset for evaluation. Note that we are performing binary classification for
    the given use case. We predict the chances of that patient surviving. For classification
    problems, we can plot a **Receiver Operating Characteristics** (**ROC**) curve
    and calculate the **Area Under The Curve** (**AUC**) score to evaluate the model's
    performance. The AUC score ranges from 0 to 1\. An AUC score of 0 represents 100%
    failed predictions and 1 represents 100% successful predictions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次训练迭代后，通过评估模型并与一组评估指标进行比较，来衡量网络的效率。我们根据评估指标进一步优化模型，并在接下来的训练迭代中进行调整。我们使用测试数据集进行评估。请注意，我们在这个用例中执行的是二分类任务。我们预测的是患者存活的概率。对于分类问题，我们可以绘制**接收者操作特征**（**ROC**）曲线，并计算**曲线下面积**（**AUC**）分数来评估模型的表现。AUC
    分数的范围是从 0 到 1。AUC 分数为 0 表示 100% 的预测失败，而 1 表示 100% 的预测成功。
- en: How to do it...
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Use ROC for the model evaluation:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 ROC 进行模型评估：
- en: '[PRE18]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Generate output from features in the test data:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从测试数据的特征生成输出：
- en: '[PRE19]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Use the ROC evaluation instance to perform the evaluation by calling `evalTimeseries()`:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 ROC 评估实例，通过调用 `evalTimeseries()` 执行评估：
- en: '[PRE20]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Display the AUC score (evaluation metrics) by calling `calculateAUC()`:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 `calculateAUC()` 来显示 AUC 分数（评估指标）：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: How it works...
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In step 3, `actuals` are the actual output for the test input, and `predictions`
    are the observed output for the test input.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 3 中，`actuals` 是测试输入的实际输出，而 `predictions` 是测试输入的观察输出。
- en: The evaluation metrics are based on the difference between `actuals` and `predictions`.
    We used ROC evaluation metrics to find this difference. An ROC evaluation is ideal
    for binary classification problems with datasets that have a uniform distribution
    of the output classes. Predicting patient mortality is just another binary classification
    puzzle.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标基于 `actuals` 和 `predictions` 之间的差异。我们使用 ROC 评估指标来找出这个差异。ROC 评估适用于具有输出类别均匀分布的数据集的二分类问题。预测患者死亡率只是另一个二分类难题。
- en: '`thresholdSteps` in the parameterized constructor of `ROC` is the number of
    threshold steps to be used for the ROC calculation. When we decrease the threshold,
    we get more positive values. It increases the sensitivity and means that the neural
    network will be less confident in uniquely classifying an item under a class.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`ROC` 的参数化构造函数中的 `thresholdSteps` 是用于 ROC 计算的阈值步数。当我们减少阈值时，会得到更多的正值。这提高了敏感度，意味着神经网络在对某个项进行分类时将对其类别的唯一分类信心较低。'
- en: 'In step 4, we printed the ROC evaluation metrics by calling `calculateAUC()`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 4 中，我们通过调用 `calculateAUC()` 打印了 ROC 评估指标：
- en: '[PRE22]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `calculateAUC()` method will calculate the area under the ROC curve plotted
    from the test data. If you print the results, you should see a probability value
    between `0` and `1`. We can also call the `stats()` method to display the whole
    ROC evaluation metrics, as shown here:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`calculateAUC()` 方法将计算从测试数据绘制的 ROC 曲线下的面积。如果你打印结果，你应该看到一个介于 `0` 和 `1` 之间的概率值。我们还可以调用
    `stats()` 方法显示整个 ROC 评估指标，如下所示：'
- en: '![](img/94b354c8-da4b-4ceb-b51a-cc76d468a60a.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94b354c8-da4b-4ceb-b51a-cc76d468a60a.png)'
- en: The `stats()` method will display the AUC score along with the **AUPRC** (short
    for **Area Under Precision/Recall Curve**) metrics. AUPRC is another performance
    metric where the curve represents the trade-off between precision and recall values.
    For a model with a good AUPRC score, positive samples can be found with fewer
    false positive results.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`stats()` 方法将显示 AUC 分数以及 **AUPRC**（**精准率/召回率曲线下面积**）指标。AUPRC 是另一种性能评估指标，其中曲线表示精准率和召回率之间的权衡。对于一个具有良好
    AUPRC 分数的模型，能够在较少的假阳性结果下找到正样本。'
