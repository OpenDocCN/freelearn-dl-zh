- en: '*Chapter 11*: Streamlining Network Implementation with AutoML'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第11章*：使用AutoML简化网络实现'
- en: Computer vision, particularly when combined with deep learning, is a field that's
    not suitable for the faint of heart! While in traditional computer programming,
    we have a limited set of options for debugging and experimentation, this is not
    the case in machine learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉，特别是与深度学习结合时，是一个不适合胆小者的领域！在传统的计算机编程中，我们有有限的调试和实验选项，而在机器学习中，情况则不同。
- en: Of course, the stochastic nature of machine learning itself plays a role in
    making the process of creating a good enough solution difficult, but so do the
    myriad of parameters, variables, knobs, and settings we need to get right to unlock
    the true power of a neural network for a particular problem.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，机器学习本身的随机性在使得创建足够好的解决方案变得困难的过程中起着一定作用，但我们需要调整的无数参数、变量、控制和设置同样是挑战所在，只有将它们调整正确，才能释放神经网络在特定问题上的真正力量。
- en: Selecting a proper architecture is just the beginning because we also need to
    consider preprocessing techniques, learning rates, optimizers, loss functions,
    and data splits, among a multiplicity of other factors.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的架构只是开始，因为我们还需要考虑预处理技术、学习率、优化器、损失函数、数据拆分等众多因素。
- en: My point is that deep learning is hard! Where do you start? Wouldn't it be great
    if we had a way to ease the burden of searching through such an ample spectrum
    of combinations?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我的观点是，深度学习很难！从哪里开始呢？如果我们能有一种方法来减轻在如此多的组合中寻找的负担，那该多好！
- en: Well, it exists! It's called **Automatic Machine Learning** (**AutoML**), and
    in this chapter, we'll learn how to leverage one of the most promising tools in
    this field, built on top of TensorFlow, known as **AutoKeras**.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，它确实存在！它被称为**自动机器学习**（**AutoML**），在本章中，我们将学习如何利用这一领域最有前景的工具之一，它是建立在TensorFlow之上的，叫做**AutoKeras**。
- en: 'In this chapter, we are going to cover the following recipes:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下配方：
- en: Creating a simple image classifier with AutoKeras
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AutoKeras创建一个简单的图像分类器
- en: Creating a simple image regressor with AutoKeras
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AutoKeras创建一个简单的图像回归器
- en: Exporting and importing a model in AutoKeras
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在AutoKeras中导出和导入模型
- en: Controlling architecture generation with AutoKeras' AutoModel
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AutoKeras的AutoModel控制架构生成
- en: Predicting age and gender with AutoKeras
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AutoKeras预测年龄和性别
- en: Let's get started!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'One of the first things you''ll notice is that **AutoML** is very resource-intensive,
    so accessing a **GPU** is a must if you want to replicate and extend the recipes
    we''ll discuss in this chapter. Also, because we''ll be using **AutoKeras** in
    all the examples provided, install it as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你会首先注意到的是，**AutoML**非常消耗资源，因此如果你想复制并扩展我们将在本章中讨论的配方，访问**GPU**是必须的。此外，由于我们将在所有提供的示例中使用**AutoKeras**，请按以下方式安装它：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The **AutoKeras** version we'll be using in this chapter only works with TensorFlow
    2.3, so ensure you have it installed as well (if you prefer, you can create a
    different environment altogether). In the *Getting ready* section of each recipe,
    you'll find any preparatory information needed. As usual, the code shown in this
    chapter is available at [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch11](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch11).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将使用的**AutoKeras**版本仅支持TensorFlow 2.3，因此请确保已安装该版本（如果愿意，你也可以创建一个全新的环境）。在每个配方的*准备工作*部分，你会找到任何需要的准备信息。和往常一样，本章中的代码可以在[https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch11](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch11)获取。
- en: 'Check out the following link to see the Code in Action video:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接，观看Code in Action视频：
- en: '[https://bit.ly/2Na6XRz](https://bit.ly/2Na6XRz).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bit.ly/2Na6XRz](https://bit.ly/2Na6XRz)'
- en: Creating a simple image classifier with AutoKeras
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AutoKeras创建一个简单的图像分类器
- en: Image classification must be the de facto application of neural networks for
    computer vision. However, as we know, depending on the complexity of the dataset,
    the availability of information, and countless other factors, the process of creating
    a proper image classifier can be quite cumbersome at times.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类无疑是神经网络在计算机视觉中的事实性应用。然而，正如我们所知道的，根据数据集的复杂性、信息的可用性以及无数其他因素，创建一个合适的图像分类器的过程有时可能相当繁琐。
- en: In this recipe, we'll implement an image classifier effortlessly thanks to the
    magic of **AutoML**. Don't believe me? Let's begin and see for ourselves!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将借助**AutoML**的魔力轻松实现一个图像分类器。不信吗？那就开始吧，一起看看！
- en: How to do it…
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'By the end of this recipe, you''ll have implemented an image classifier in
    a dozen lines of code or less! Let''s get started:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程结束时，你将能用不超过十几行代码实现一个图像分类器！让我们开始吧：
- en: 'Import all the required modules:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有需要的模块：
- en: '[PRE1]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For the sake of simplicity, we'll use the well-known `Fashion-MNIST` dataset,
    a more challenging version of the famous `MNIST`.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了简化，我们将使用著名的`Fashion-MNIST`数据集，它是著名的`MNIST`的一个更具挑战性的版本。
- en: 'Load the train and test data:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载训练和测试数据：
- en: '[PRE2]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Normalize the images to the range [0, 1]:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像归一化到[0, 1]的范围：
- en: '[PRE3]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Define the number of epochs we''ll allow each possible network (known as a
    trial) to train:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义我们允许每个可能的网络（称为一次试验）训练的轮次数：
- en: '[PRE4]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here''s where the magic happens. Define an instance of `ImageClassifier()`:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这就是魔法发生的地方。定义一个`ImageClassifier()`实例：
- en: '[PRE5]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Notice that we are seeding the classifier with 9 and allowing it to find a suitable
    network 10 times. We're doing this so that the **Neural Architecture Search**
    (**NAS**) process terminates in a reasonable amount of time (to learn more about
    **NAS**, please refer to the *See also* section).
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，我们将分类器的种子设为9，并允许它找到一个合适的网络10次。我们这样做是为了让**神经架构搜索**（**NAS**）过程在合理的时间内终止（要了解更多关于**NAS**的信息，请参考*参见*部分）。
- en: 'Fit the classifier on the test data over 10 epochs (per trial):'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上对分类器进行10个轮次的训练（每次试验）：
- en: '[PRE6]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Lastly, evaluate the best classifier on the test set and print out the accuracy:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在测试集上评估最佳分类器并打印准确率：
- en: '[PRE7]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: After a while (let's not forget the library is training 10 models with varying
    complexity), we should obtain an accuracy of 93%, give or take. That's not bad,
    considering we didn't even write 10 lines of code!
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 过一段时间后（别忘了库正在训练10个具有不同复杂度的模型），我们应该能得到大约93%的准确率。考虑到我们甚至没有写10行代码，这个结果还不错！
- en: We'll discuss what we've done a bit more in the *How it works…* section.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*工作原理…*部分进一步讨论我们所做的工作。
- en: How it works…
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: In this recipe, we created the most effortless image classifier ever! We delegated
    all major decisions to the **AutoML** tool, **AutoKeras**. From selecting an architecture,
    to which optimizer to use, all such decisions were made by the framework.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们创建了最轻松的图像分类器！我们将所有主要决策都交给了**AutoML**工具——**AutoKeras**。从选择架构到选择使用哪种优化器，所有这些决策都由框架做出。
- en: You might have noticed that we limited the search space by specifying a maximum
    of 10 trials and 10 epochs per trial. We did this so that the program terminates
    in a reasonable amount of time, but as you might suspect, these parameters can
    also be trusted to **AutoKeras**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，我们通过指定最多10次试验和每次试验最多10个轮次来限制搜索空间。我们这样做是为了让程序在合理的时间内终止，但正如你可能猜到的，这些参数也可以交给**AutoKeras**来处理。
- en: Despite all the autonomy **AutoML** has, we can guide the framework if we wish.
    What **AutoML** offers is, as its name suggests, a way to automate the search
    for a good enough combination for a particular problem. However, this doesn't
    mean that human expertise and prior knowledge is not necessary. In fact, it is
    often the case that a well-crafted network, typically the product of thoroughly
    studying the data, often performs better than one found by **AutoML** with no
    prior information whatsoever.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管**AutoML**具有很高的自主性，我们仍然可以根据需要指导框架。正如其名所示，**AutoML**提供了一种自动化寻找针对特定问题足够好组合的方法。然而，这并不意味着不需要人类的专业知识和先前的经验。事实上，通常情况下，一个经过精心设计的网络（通常是通过深入研究数据所得）往往比**AutoML**在没有任何先前信息的情况下找到的网络效果更好。
- en: In the end, **AutoML** is a tool, and as such, it should be used to enhance
    our mastery of deep learning, not to replace it – because it can't.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，**AutoML**是一个工具，应该用来增强我们对深度学习的掌握，而不是取而代之——因为它做不到这一点。
- en: See also
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'You can learn more about **NAS** here: [https://en.wikipedia.org/wiki/Neural_architecture_search](https://en.wikipedia.org/wiki/Neural_architecture_search).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里了解更多关于**NAS**的信息：[https://en.wikipedia.org/wiki/Neural_architecture_search](https://en.wikipedia.org/wiki/Neural_architecture_search)。
- en: Creating a simple image regressor with AutoKeras
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AutoKeras创建一个简单的图像回归器
- en: The power and usefulness of **AutoKeras** is not limited to image classification.
    Although not as popular, image regression is a similar problem where we want to
    predict a continuous quantity based on the spatial information in an image.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**AutoKeras**的强大功能不仅限于图像分类。尽管不如图像分类流行，图像回归是一个类似的问题，我们希望根据图像中的空间信息预测一个连续的量。'
- en: In this recipe, we'll train an image regressor to predict people's ages while
    using **AutoML**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们将训练一个图像回归器，预测人们的年龄，同时使用**AutoML**。
- en: Let's begin.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Getting ready
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We'll be using `APPA-REAL` dataset in this recipe, which contains 7,591 images
    labeled with the real and apparent ages for a wide range of subjects. You can
    read more about the dataset and download it from [http://chalearnlap.cvc.uab.es/dataset/26/description/#](http://chalearnlap.cvc.uab.es/dataset/26/description/#).
    Decompress the data in a directory of your preference. For the purposes of this
    recipe, we'll assume the dataset is located within the `~/.keras/datasets/appa-real-release`
    folder.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们将使用`APPA-REAL`数据集，该数据集包含7,591张图像，标注了广泛对象的真实年龄和表观年龄。您可以在[http://chalearnlap.cvc.uab.es/dataset/26/description/#](http://chalearnlap.cvc.uab.es/dataset/26/description/#)查看更多有关该数据集的信息并下载它。将数据解压到您选择的目录中。为了配方的目的，我们假设数据集位于`~/.keras/datasets/appa-real-release`文件夹中。
- en: 'Here are some sample images:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些示例图像：
- en: '![Figure 11.1 – Sample images from the APPA-REAL dataset'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.1 – 来自APPA-REAL数据集的示例图像](img/B14768_11_001.jpg)'
- en: '](img/B14768_11_001.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_11_001.jpg)'
- en: Figure 11.1 – Sample images from the APPA-REAL dataset
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – 来自APPA-REAL数据集的示例图像
- en: Let's implement this recipe!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现这个配方！
- en: How to do it…
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Follow these steps to complete this recipe:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤完成此配方：
- en: 'Import the modules we will be using:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入我们将要使用的模块：
- en: '[PRE8]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Each subset (train, test, and validation) of the dataset is defined in a CSV
    file. There, among many other columns, we have the path to the image and the real
    age of the person depicted in a photo. In this step, we will define the `load_mapping()`
    function, which will create a map from the image paths to the labels that we''ll
    use to load the actual data in memory:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集的每个子集（训练集、测试集和验证集）都在一个CSV文件中定义。在这个文件中，除了许多其他列外，我们还有图像路径和照片中人物的实际年龄。在此步骤中，我们将定义`load_mapping()`函数，该函数将从图像路径创建一个映射，用于加载实际数据到内存中：
- en: '[PRE9]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Define the `get_image_and_labels()` function, which takes the mapping produced
    by the `load_mapping()` function and returns an array of images (normalized to
    the range [-1, 1]) and an array of the corresponding ages:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`get_image_and_labels()`函数，该函数接收`load_mapping()`函数生成的映射，并返回一个图像数组（归一化到[-1,
    1]范围内）和一个相应年龄的数组：
- en: '[PRE10]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Notice that each image has been resized so that its dimensions are 64x64x3\.
    This is necessary because the images in the dataset don't have homogeneous dimensions.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，每张图像都已调整大小，确保其尺寸为64x64x3。这是必要的，因为数据集中的图像尺寸不统一。
- en: 'Define the paths to the CSV files to create the data mappings for each subset:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义CSV文件的路径，以创建每个子集的数据映射：
- en: '[PRE11]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Define the paths to the directories where the images for each subset live:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义每个子集的图像所在目录的路径：
- en: '[PRE12]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create the mappings for each subset:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个子集创建映射：
- en: '[PRE13]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Get the images and labels for each subset:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取每个子集的图像和标签：
- en: '[PRE14]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We''ll train each network in a trial for a maximum of 15 epochs:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在每次试验中训练每个网络，最多训练15个epoch：
- en: '[PRE15]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We instantiate an `ImageRegressor()` object, which encapsulates the `adam`
    as the optimizer:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们实例化一个`ImageRegressor()`对象，它封装了`adam`优化器：
- en: '[PRE16]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Fit the regressor. Notice that we are passing our own validation set. If we
    don''t do this, **AutoKeras** takes 20% of the training data to validate its experiments
    by default:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合回归器。请注意，我们传递了自己的验证集。如果我们不这样做，**AutoKeras**默认会取20%的训练数据来验证它的实验：
- en: '[PRE17]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we must evaluate the best regressor on the test data and print its
    performance metric:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们必须在测试数据上评估最佳回归器并打印其性能指标：
- en: '[PRE18]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: After a while, we should obtain a test loss of 241.248, which is not bad if
    we take into account that the bulk of our work consisted of loading the dataset.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一段时间后，我们应该获得241.248的测试损失，考虑到我们的工作主要是加载数据集，这个结果还不错。
- en: Let's move on to the *How it works…* section.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入*它是如何工作的……*部分。
- en: How it works…
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In this recipe, we delegated the creation of a model to an **AutoML** framework,
    similar to what we did in the *Creating a simple image classifier with AutoKeras*
    recipe. However, this time, our goal was to solve a regression problem, namely
    predicting the age of a person based on a photo of their face, instead of a classification
    one.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将模型的创建委托给了**AutoML**框架，类似于我们在*使用AutoKeras创建简单图像分类器*食谱中的做法。不过，这次，我们的目标是解决回归问题，即根据人脸照片预测一个人的年龄，而不是分类问题。
- en: This time, because we used a real-world dataset, we had to implement several
    helper functions to load the data and make it the proper shape for **AutoKeras**
    to use it. However, after doing this, we let the framework take the wheel, leveraging
    its built-in **NAS** algorithm to find the best possible model in a span of 15
    iterations.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，因为我们使用了一个真实世界的数据集，我们不得不实现几个辅助函数来加载数据，并将其转化为**AutoKeras**可以使用的正确形状。不过，在做完这些之后，我们让框架接管，利用它内建的**NAS**算法，在15次迭代中找到最佳模型。
- en: We obtained a respectable 241.248 loss on the test set. Predicting the age of
    a person is not an easy task, even though it might appear that it is at first.
    I invite you to take a closer look at the *APPA-REAL* CSV files so that you can
    see the deviation in the human estimates of people's ages!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在测试集上获得了一个相当不错的241.248的损失值。预测一个人的年龄并不是一件简单的任务，尽管乍一看它似乎很简单。我邀请你仔细查看*APPA-REAL*的CSV文件，这样你就能看到人类对年龄估算的偏差！
- en: See also
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'You can learn more about **NAS** here: [https://en.wikipedia.org/wiki/Neural_architecture_search](https://en.wikipedia.org/wiki/Neural_architecture_search).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里了解更多关于**NAS**的信息：[https://en.wikipedia.org/wiki/Neural_architecture_search](https://en.wikipedia.org/wiki/Neural_architecture_search)。
- en: Exporting and importing a model in AutoKeras
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在AutoKeras中导出和导入模型
- en: One worry we might have when working with **AutoML** is the black-box nature
    of the tools available. Do we have control over the produced models? Can we extend
    them? Understand them? Reuse them?
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用**AutoML**时，我们可能会担心其黑盒性质。我们能控制生成的模型吗？我们可以扩展它们吗？理解它们吗？重新使用它们吗？
- en: Of course we can! The good thing about **AutoKeras** is that it is built on
    top of TensorFlow, so despite its sophistication, under the hood, the models being
    trained are just TensorFlow graphs that we can export and tweak and tune later
    if we need to.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当然可以！**AutoKeras**的一个优点是它构建在TensorFlow之上，因此尽管它非常复杂，但在底层，训练的模型只是TensorFlow图，我们可以在以后导出、调整和优化这些模型（如果需要的话）。
- en: In this recipe, we'll learn how to export a model trained on **AutoKeras**,
    and then import it as a plain old TensorFlow network.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将学习如何导出一个在**AutoKeras**上训练的模型，然后将其作为一个普通的TensorFlow网络导入。
- en: Are you ready? Let's begin.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你准备好了吗？让我们开始吧。
- en: How to do it…
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到…
- en: 'Follow these steps to complete this recipe:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤完成本食谱：
- en: 'Import the necessary dependencies:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的依赖项：
- en: '[PRE19]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Load the train and test splits of the `Fashion-MNIST` dataset:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`Fashion-MNIST`数据集的训练和测试集：
- en: '[PRE20]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Normalize the data to the [0, 1] interval:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据归一化到[0, 1]区间：
- en: '[PRE21]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Define the number of epochs we''ll train each network for:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义我们将为每个网络训练的周期数：
- en: '[PRE22]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Create an `ImageClassifier()` that''ll try to find to best possible classifier,
    over 20 trials, with each one trained for 10 epochs. We will instruct `adam` as
    the optimizer and seed `ImageClassifier()` for the sake of reproducibility:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`ImageClassifier()`，它将尝试在20次试验中找到最佳分类器，每次训练10个周期。我们将指定`adam`作为优化器，并为可重复性设置`ImageClassifier()`的种子：
- en: '[PRE23]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Fit the classifier. We''ll allow **AutoKeras** to automatically pick 20% of
    the training data for validation:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练分类器。我们将允许**AutoKeras**自动选择20%的训练数据作为验证集：
- en: '[PRE24]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Export the best model and save it to disk:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导出最佳模型并将其保存到磁盘：
- en: '[PRE25]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Load the model back into memory:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型重新加载到内存中：
- en: '[PRE26]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Evaluate the training model on the test set:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估训练模型：
- en: '[PRE27]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Print a text summary of the best model:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印最佳模型的文本摘要：
- en: '[PRE28]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Lastly, generate a graph of the architecture of the best model found by **AutoKeras**:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，生成**AutoKeras**找到的最佳模型的架构图：
- en: '[PRE29]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'After 20 trials, the best model that was created by **AutoKeras** achieves
    91.5% accuracy on the test set. The following screenshot shows the model''s summary:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在20次试验之后，**AutoKeras**创建的最佳模型在测试集上的准确率达到了91.5%。以下截图展示了模型的摘要：
- en: '![Figure 11.2 – AutoKeras'' best model summary'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.2 – **AutoKeras**最佳模型摘要'
- en: '](img/B14768_11_002.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_11_002.jpg)'
- en: Figure 11.2 – AutoKeras' best model summary
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – **AutoKeras**最佳模型摘要
- en: 'The following diagram shows the model''s architecture:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了模型的架构：
- en: '![Figure 11.3 – AutoKeras'' best model architecture'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.3 – AutoKeras的最佳模型架构'
- en: '](img/B14768_11_003.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_11_003.jpg)'
- en: Figure 11.3 – AutoKeras' best model architecture
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 – AutoKeras的最佳模型架构
- en: In *Figure 11.2*, we can see the network **AutoKeras** deemed the most suitable
    for **Fashion-MNIST**, at least within the bounds we established. You can take
    a closer look at the full architecture in the companion GitHub repository.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图11.2*中，我们可以看到**AutoKeras**被认为是最适合**Fashion-MNIST**的网络，至少在我们设定的范围内是这样。你可以在配套的GitHub仓库中更详细地查看完整架构。
- en: Let's move on to the next section.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续到下一节。
- en: How it works…
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In this recipe, we demonstrated that **AutoML** can work as a great starting
    point when we're tackling a new computer vision problem. How? We can use it to
    produce well-performing models out of the gate, which we can then extend based
    on our domain knowledge of the dataset at hand.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们展示了**AutoML**如何作为我们解决新计算机视觉问题时的一个很好的起点。如何做？我们可以利用它快速生成表现良好的模型，然后基于我们对当前数据集的领域知识进行扩展。
- en: 'The formula to do this is straightforward: let **AutoML** do the grunt work
    for a while; then, export the best network and import it into the confines of
    TensorFlow so that you can build your solution on top of it.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一点的公式很简单：让**AutoML**做一段时间的繁重工作，然后导出最佳网络并将其导入TensorFlow的框架中，这样你就可以在其上构建你的解决方案。
- en: This not only showcases the usability of tools such as **AutoKeras**, but allows
    us to peak behind the curtain, understanding the building blocks of the models
    engendered by **NAS**.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅展示了像**AutoKeras**这样的工具的可用性，还让我们得以窥见幕后，理解由**NAS**生成的模型的构建块。
- en: See also
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'The basis of **AutoKeras** is **NAS**. You can read more about it here (it''s
    pretty interesting!): [https://en.wikipedia.org/wiki/Neural_architecture_search](https://en.wikipedia.org/wiki/Neural_architecture_search).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**AutoKeras**的基础是**NAS**。你可以在这里阅读更多相关内容（非常有趣！）：[https://en.wikipedia.org/wiki/Neural_architecture_search](https://en.wikipedia.org/wiki/Neural_architecture_search)。'
- en: Controlling architecture generation with AutoKeras' AutoModel
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AutoKeras的AutoModel控制架构生成
- en: Letting **AutoKeras** automagically figure out what architecture works best
    is great, but it can be time-consuming – unacceptably so at times.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让**AutoKeras**自动决定哪个架构最适合是很好的，但这可能会非常耗时——有时是不可接受的。
- en: Can we exert more control? Can we hint at which options work best for our particular
    problem? Can we meet **AutoML** halfway by providing a set of guidelines it must
    follow according to our prior knowledge or preference, but still give it enough
    leeway to experiment?
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否施加更多控制？我们能否提示哪些选项最适合我们的特定问题？我们能否通过提供一组必须遵循的指导方针，使**AutoML**在我们事先的知识或偏好基础上进行实验，同时又给它足够的自由度进行试探？
- en: Yes, we can, and in this recipe, you'll learn how by utilizing a special feature
    in **AutoKeras** known as AutoModel!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我们可以，在这个配方中，你将通过利用**AutoKeras**中的一个特别功能——AutoModel来学习如何操作！
- en: How to do it…
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Follow these steps to learn how to customize the search space of the `AutoModel`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这些步骤学习如何自定义`AutoModel`的搜索空间：
- en: 'The first thing we need to do is import all the necessary dependencies:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是导入所有必需的依赖项：
- en: '[PRE30]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Because we''ll be training our customized model on `Fashion-MNIST`, we must
    load the train and test splits, respectively:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因为我们将在`Fashion-MNIST`上训练我们的自定义模型，所以我们必须分别加载训练和测试拆分数据：
- en: '[PRE31]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To avoid numerical instability issues, let''s normalize the images of both
    splits so that they''re in the range [0, 1]:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了避免数值不稳定问题，让我们将两个拆分的图像归一化到[0, 1]范围内：
- en: '[PRE32]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Define the `create_automodel()` function, which defines the custom search space
    of the underlying `Block` is in charge of a defined task, such as image augmentation,
    normalization, image processing, or classification. First, we must define the
    input block, which will be normalized and augmented through the `Normalization()`
    and `ImageAugmentation()` blocks, respectively:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`create_automodel()`函数，该函数定义了底层`Block`的自定义搜索空间，负责定义的任务，如图像增强、归一化、图像处理或分类。首先，我们必须定义输入块，它将通过`Normalization()`和`ImageAugmentation()`块分别进行归一化和增强：
- en: '[PRE33]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Notice that we disabled horizontal and vertical flipping in the `ImageAugmentation()`
    block. This is because these operations alter the class of images in `Fashion-MNIST`.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，我们在`ImageAugmentation()`块中禁用了水平和垂直翻转。这是因为这些操作会改变`Fashion-MNIST`中图像的类别。
- en: 'Now, we''ll bifurcate the graph. The left branch searches for vanilla convolutional
    layers, thanks to `ConvBlock()`. On the right branch, we''ll explore more sophisticated
    Xception-like architectures (for more information about the **Xception** architecture,
    refer to the *See also* section):'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将图表分叉。左侧分支使用`ConvBlock()`搜索普通卷积层。右侧分支，我们将探索更复杂的类似Xception的架构（有关**Xception**架构的更多信息，请参阅*另见*部分）：
- en: '[PRE34]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In the previous snippet, we instructed **AutoKeras** to only explore **Xception**
    architectures pre-trained on ImageNet.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们指示**AutoKeras**只探索在ImageNet上预训练的**Xception**架构。
- en: 'We''ll merge the left and right branches, flatten them, and pass the result
    through a `DenseBlock()`, which, as its name suggests, searches for fully connected
    combinations of layers:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将合并左右两个分支，展开它们，然后通过`DenseBlock()`传递结果，正如它的名字所示，它会搜索完全连接的层组合：
- en: '[PRE35]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output of this graph will be a `ClassificationHead()`. This is because
    we''re dealing with a classification problem. Notice that we don''t specify the
    number of classes. This is because **AutoKeras** infers this information from
    the data:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个图表的输出将是一个`ClassificationHead()`。这是因为我们处理的是一个分类问题。请注意，我们没有指定类别的数量。这是因为**AutoKeras**会从数据中推断出这些信息：
- en: '[PRE36]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can close `create_automodel()` by building and returning an `AutoModel()`
    instance. We must specify the inputs and outputs, as well as the maximum number
    of trials to perform:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过构建并返回一个`AutoModel()`实例来结束`create_automodel()`。我们必须指定输入和输出，以及要执行的最大尝试次数：
- en: '[PRE37]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let''s train each trial model for 10 epochs:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们训练每个试验模型10个周期：
- en: '[PRE38]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Create the `AutoModel` and fit it:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`AutoModel`并进行拟合：
- en: '[PRE39]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let''s export the best model:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们导出最佳模型：
- en: '[PRE40]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Evaluate the model on the test set:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估模型：
- en: '[PRE41]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Plot the architecture of the best model:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制最佳模型的架构：
- en: '[PRE42]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The final architecture I obtained achieved 90% accuracy on the test set, although
    your results may vary. What''s even more interesting is the structure of the generated
    model:'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我最终得到的架构在测试集上的准确率达到了90%，尽管你的结果可能有所不同。更有趣的是生成的模型的结构：
- en: '![Figure 11.4 – AutoKeras'' best model architecture'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.4 – AutoKeras的最佳模型架构](img/B14768_11_004.jpg)'
- en: '](img/B14768_11_004.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.4 – AutoKeras的最佳模型架构](img/B14768_11_004.jpg)'
- en: Figure 11.4 – AutoKeras' best model architecture
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 – AutoKeras的最佳模型架构
- en: The preceding diagram reveals that `AutoModel` produced a network according
    to the blueprint we laid out in `create_automodel()`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图表揭示了`AutoModel`根据我们在`create_automodel()`中设计的蓝图生成了一个网络。
- en: Now, let's move on to the *How it works…* section.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入*它是如何工作的……*部分。
- en: How it works…
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In this recipe, we took advantage of `AutoModel` module to trim down the search
    space. This is a very useful feature when we have an idea of what our final model
    should look like. This leads to huge time gains because we don't allow **AutoKeras**
    to waste time trying out unfruitful, useless combinations. One example of such
    bad combinations can be seen in *Step 4*, where we told **AutoKeras** not to try
    to flip images as part of its image augmentation scheme. This is because, due
    to the characteristics of our problem, this operation changes the classes of the
    numbers in **Fashion-MNIST**.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们利用了`AutoModel`模块来缩小搜索空间。当我们大致知道最终模型应该是什么样时，这个特性非常有用。因为我们不会让**AutoKeras**浪费时间尝试无效、无用的组合，所以可以节省大量时间。一个这样的无效组合的例子可以在*第4步*中看到，我们告诉**AutoKeras**不要将图像翻转作为图像增强的一部分。因为根据我们问题的特点，这个操作会改变**Fashion-MNIST**中数字的类别。
- en: Proof that we steered `create_automodel()` function.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 证明我们引导了`create_automodel()`函数。
- en: Impressive, right?
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 很令人印象深刻，对吧？
- en: See also
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'One thing we didn''t do here is implement our own `Block`, which is possible
    in **AutoKeras**. Why don''t you give it a try? You can start by reading the docs
    here: [https://autokeras.com/tutorial/customized/](https://autokeras.com/tutorial/customized/).
    For a list of all available blocks, go to [https://autokeras.com/block/](https://autokeras.com/block/).
    In this recipe, we used Xception-like layers. To find out more about Xception,
    you can read the original paper: [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们没有做的一件事是实现我们自己的`Block`，这在**AutoKeras**中是可能的。为什么不尝试一下呢？你可以从阅读这里的文档开始：[https://autokeras.com/tutorial/customized/](https://autokeras.com/tutorial/customized/)。要查看所有可用的模块，可以访问[https://autokeras.com/block/](https://autokeras.com/block/)。在这个示例中，我们使用了类似Xception的层。要了解更多关于Xception的信息，可以阅读原始论文：[https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)。
- en: Predicting age and gender with AutoKeras
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AutoKeras预测年龄和性别
- en: In this recipe, we'll study a practical application of AutoML that can be used
    as a template to create prototypes, MVPs, or just to tackle real-world applications
    with the help of AutoML.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，我们将学习 AutoML 的一个实际应用，可以作为模板来创建原型、MVP，或者仅仅借助 AutoML 来解决现实世界中的问题。
- en: 'More concretely, we''ll create an age and gender classification program with
    a twist: the architecture of both the gender and age classifiers will be the responsibility
    of **AutoKeras**. We''ll be in charge of getting and shaping the data, as well
    as creating the framework to test the solution on our own images.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们将创建一个年龄和性别分类程序，其中有一个特别的地方：性别和年龄分类器的架构将由**AutoKeras**负责。我们将负责获取和整理数据，并创建框架来在我们自己的图像上测试解决方案。
- en: I hope you're ready because we are about to begin!
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你准备好了，因为我们马上开始！
- en: Getting ready
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备好了吗
- en: 'We need a couple of external libraries, such as OpenCV, `scikit-learn`, and
    `imutils`. All these dependencies can be installed at once, as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要几个外部库，比如 OpenCV、`scikit-learn` 和 `imutils`。所有这些依赖项可以一次性安装，方法如下：
- en: '[PRE43]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: On the data side, we'll use the **Adience** dataset, which contains 26,580 images
    of 2,284 subjects, along with their gender and age. To download the data, go to
    [https://talhassner.github.io/home/projects/Adience/Adience-data.html](https://talhassner.github.io/home/projects/Adience/Adience-data.html).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据方面，我们将使用**Adience**数据集，其中包含 26,580 张 2,284 个主体的图像，并附有其性别和年龄。要下载数据，请访问 [https://talhassner.github.io/home/projects/Adience/Adience-data.html](https://talhassner.github.io/home/projects/Adience/Adience-data.html)。
- en: 'Next, you''ll need to navigate to the **Download** section and enter your name
    and email, as shown in the following screenshot:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要进入**下载**部分并输入你的姓名和电子邮件，如下图所示：
- en: '![Figure 11.5 – Enter your information to receive the credentials of the FTP
    server where the data is'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.5 – 输入你的信息以接收存储数据的 FTP 服务器凭证'
- en: '](img/B14768_11_005.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_11_005.jpg)'
- en: Figure 11.5 – Enter your information to receive the credentials of the FTP server
    where the data is
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 – 输入你的信息以接收存储数据的 FTP 服务器凭证
- en: 'Once you hit the **Submit** button, you''ll get the credentials required for
    the FTP server where the data is located. You can access this here: [http://www.cslab.openu.ac.il/download/](http://www.cslab.openu.ac.il/download/).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你点击**提交**按钮，你将获得访问 FTP 服务器所需的凭证，该服务器存储着数据。你可以在这里访问：[http://www.cslab.openu.ac.il/download/](http://www.cslab.openu.ac.il/download/)。
- en: 'Make sure that you click on the first link, labeled **Adience OUI Unfiltered
    faces for gender and age classification**:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 确保点击第一个链接，标签为**Adience OUI 未滤镜的人脸用于性别和年龄分类**：
- en: '![Figure 11.6 – Going to the highlighted link'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.6 – 进入高亮链接'
- en: '](img/B14768_11_006.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_11_006.jpg)'
- en: Figure 11.6 – Going to the highlighted link
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6 – 进入高亮链接
- en: 'Enter the credentials you received previously and access the second link, named
    **AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification**:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 输入你之前收到的凭证并访问第二个链接，名称为**AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification**：
- en: '![Figure 11.7 – Clicking the highlighted link'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.7 – 点击高亮链接'
- en: '](img/B14768_11_007.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_11_007.jpg)'
- en: Figure 11.7 – Clicking the highlighted link
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 – 点击高亮链接
- en: 'Finally, download `aligned.tar.gz`, `fold_frontal_0_data.txt`, `fold_frontal_1_data.txt`,
    `fold_frontal_2_data.txt`, `fold_frontal_3_data.txt`, `and fold_frontal_4_data.txt`:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，下载 `aligned.tar.gz`、`fold_frontal_0_data.txt`、`fold_frontal_1_data.txt`、`fold_frontal_2_data.txt`、`fold_frontal_3_data.txt`
    和 `fold_frontal_4_data.txt`：
- en: '![Figure 11.8 – Downloading aligned.tar.gz and all the fold_frontal_*_data.txt
    files'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.8 – 下载 aligned.tar.gz 和所有 fold_frontal_*_data.txt 文件'
- en: '](img/B14768_11_008.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_11_008.jpg)'
- en: Figure 11.8 – Downloading aligned.tar.gz and all the fold_frontal_*_data.txt
    files
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 – 下载 aligned.tar.gz 和所有 fold_frontal_*_data.txt 文件
- en: Unzip `aligned.tar.gz` into a directory of your preference as `adience`. Inside
    that directory, create a subdirectory named `folds`, and move all the `fold_frontal_*_data.txt`
    files inside it. For the purposes of this recipe, we'll assume the dataset is
    located within `~/.keras/datasets/adience`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 解压 `aligned.tar.gz` 到你选择的目录中，命名为 `adience`。在该目录内，创建一个名为 `folds` 的子目录，并将所有 `fold_frontal_*_data.txt`
    文件移动到其中。为了本方案的方便，我们假设数据集位于 `~/.keras/datasets/adience`。
- en: 'Here are some sample images:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些示例图像：
- en: '![Figure 11.9 – Sample images from the Adience dataset'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.9 – 来自 Adience 数据集的示例图像'
- en: '](img/B14768_11_009.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_11_009.jpg)'
- en: Figure 11.9 – Sample images from the Adience dataset
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9 – 来自 Adience 数据集的示例图像
- en: Let's implement this recipe!
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现这个方案吧！
- en: How to do it…
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Complete these steps to implement an age and gender classifier using **AutoML**:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 完成以下步骤以实现一个年龄和性别分类器，使用**AutoML**：
- en: 'The first thing we need to do is import all the necessary dependencies:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是导入所有必要的依赖：
- en: '[PRE44]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Define the base path to the `Adience` dataset, as well as the folds (which
    contain the relationships between the images and the ages and genders of their
    subjects, in CSV format):'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`Adience`数据集的基本路径，以及包含图像与其受试者年龄和性别关系的折叠（CSV格式）：
- en: '[PRE45]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The ages in `Adience` are expressed as intervals, groups, or brackets. Here,
    we will define an array that we will use to map the reported age in the folds
    to the correct bracket:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Adience`中的年龄以区间、组别或括号的形式表示。在这里，我们将定义一个数组，用于将折叠中的报告年龄映射到正确的区间：'
- en: '[PRE46]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Define the `age_to_bin()` function, which takes an input as it appears in a
    fold CSV row and maps it to the corresponding bin. For instance, if the input
    is `(27, 29)`, the output will be `25_32`:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`age_to_bin()`函数，该函数接收一个输入（如折叠CSV行中的值），并将其映射到相应的区间。例如，如果输入是`(27, 29)`，输出将是`25_32`：
- en: '[PRE47]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Define a function that will compute the area of a rectangle. We''ll use this
    later to get the largest face detection possible:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来计算矩形的面积。我们稍后将用它来获取最大的面部检测区域：
- en: '[PRE48]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We''ll also draw a bounding box around the detected face, captioned with the
    recognized age and gender:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将绘制一个边框框住检测到的人脸，并附上识别出的年龄和性别：
- en: '[PRE49]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Define the `predict()` function, which we''ll use to predict both the age and
    gender (depending on `model`) of a person whose face was passed into the `roi`
    parameter:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`predict()`函数，我们将用它来预测传入`roi`参数的人的年龄和性别（取决于`model`）：
- en: '[PRE50]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Define the lists where we''ll store all the images, ages, and genders of the
    dataset:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义存储数据集中所有图像、年龄和性别的列表：
- en: '[PRE51]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Iterate over each fold file. These will be in CSV format:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历每个折叠文件。这些文件将是CSV格式：
- en: '[PRE52]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'If the age or gender fields are not well-defined, skip the current line:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果年龄或性别字段不明确，跳过当前行：
- en: '[PRE53]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Map the age to a valid bin. If we get `None` from `age_to_bin()`, this means
    the age doesn''t correspond to any of our defined categories, so we must skip
    this record:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将年龄映射到一个有效的区间。如果从`age_to_bin()`返回`None`，这意味着年龄不对应我们定义的任何类别，因此必须跳过此记录：
- en: '[PRE54]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Load the image:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载图像：
- en: '[PRE55]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Append the image, age, and gender to the corresponding collections:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像、年龄和性别添加到相应的集合中：
- en: '[PRE56]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Create two copies of the images, one for each problem (age classification and
    gender prediction):'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个问题（年龄分类和性别预测）创建两份图像副本：
- en: '[PRE57]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Encode the age and genders:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码年龄和性别：
- en: '[PRE58]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Define the number of trials and epochs per trial. These parameters affect both
    models:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义每次试验的次数和每次试验的周期。这些参数会影响两个模型：
- en: '[PRE59]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'If there''s a trained version of the age classifier, load it; otherwise, train
    an `ImageClassifier()` from scratch and save it to disk:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有训练好的年龄分类器，加载它；否则，从头开始训练一个`ImageClassifier()`并保存到磁盘：
- en: '[PRE60]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'If there''s a trained version of the gender classifier, load it; otherwise,
    train an `ImageClassifier()` from scratch and save it to disk:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有训练好的性别分类器，加载它；否则，从头开始训练一个`ImageClassifier()`并保存到磁盘：
- en: '[PRE61]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Read a test image from disk:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从磁盘读取测试图像：
- en: '[PRE62]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Create a **Haar Cascades** face detector. (This is a topic outside the scope
    of this book. If you want to learn more about Haar Cascades, go to the *See also*
    section of this recipe.) Use the following code to do so:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个**Haar Cascades**人脸检测器。（这是本书范围之外的主题。如果你想了解更多关于Haar Cascades的内容，请参阅本配方的*另见*部分。）使用以下代码来完成：
- en: '[PRE63]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Resize the image so that it is 380 pixels wide. Thanks to the `imutils.resize()`
    function, we can rest assured that the result will preserve the aspect ratio.
    This is because the function computes the height automatically to guarantee this
    condition:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整图像大小，使其宽度为380像素。得益于`imutils.resize()`函数，我们可以放心结果会保持纵横比。因为该函数会自动计算高度以确保这一条件：
- en: '[PRE64]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Create a copy of the original image so that we can draw the detections on it:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建原始图像的副本，以便我们可以在其上绘制检测结果：
- en: '[PRE65]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Convert the image into grayscale and pass it through the face detector:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像转换为灰度，并通过人脸检测器：
- en: '[PRE66]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Verify whether there are detections and fetch the one with the largest area:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证是否有检测到的对象，并获取具有最大面积的一个：
- en: '[PRE67]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Extract the region of interest (`roi`) corresponding to the detected face and
    extract its age and gender:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取与检测到的人脸相对应的兴趣区域（`roi`），并提取其年龄和性别：
- en: '[PRE68]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Notice that we use each encoder to revert back to a human-readable label for
    both the predicted age and gender.
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，我们使用每个编码器来还原为人类可读的标签，用于预测的年龄和性别。
- en: 'Plot the predicted age and gender on the original image and show the result:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将预测的年龄和性别标注在原始图像上，并显示结果：
- en: '[PRE69]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Important note
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: The first time you execute this script, you'll have to wait a very long time
    – probably more than 24 hours (depending on your hardware). This is because each
    model is trained for a high number of trials and epochs. However, subsequent runs
    should be way faster because the program will load the trained classifiers.
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一次执行此脚本时，您需要等待很长时间——可能超过24小时（取决于您的硬件）。这是因为每个模型都经过大量的试验和轮次训练。然而，之后的运行会更快，因为程序将加载已经训练好的分类器。
- en: 'We can see an example of a successful prediction of both age and gender in
    the following screenshot:'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以在以下截图中看到一个成功预测年龄和性别的例子：
- en: '![Figure 11.10 – Our models state the person in the photo is female and is
    between 25 and 32 years of age. Seems about right, doesn''t it?](img/B14768_11_010.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.10 – 我们的模型判定照片中的人是女性，年龄在25至32岁之间。看起来差不多吧？](img/B14768_11_010.jpg)'
- en: Figure 11.10 – Our models state the person in the photo is female and is between
    25 and 32 years of age. Seems about right, doesn't it?
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10 – 我们的模型判定照片中的人是女性，年龄在25至32岁之间。看起来差不多吧？
- en: Isn't it truly amazing how the heavy lifting was done by **AutoKeras**? We're
    living in the future!
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 难道不是真正令人惊叹吗？所有的繁重工作都是由**AutoKeras**完成的！我们正在生活在未来！
- en: How it works…
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'In this recipe, we implemented a practical solution to a surprisingly challenging
    problem: age and gender prediction.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们实现了一个实际的解决方案，来应对一个令人意外的挑战性问题：年龄和性别预测。
- en: Why is this challenging? The apparent age of a person can vary, depending on
    multiple factors, such as ethnicity, gender, health, and other life conditions.
    We humans are not as great as we think we are at estimating the age of a man or
    a woman based solely on their physical features.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这很具挑战性？一个人的表面年龄可能会有所不同，取决于多种因素，如种族、性别、健康状况和其他生活条件。我们人类在仅凭外貌特征来估算一个人（男性或女性）的年龄时，并没有我们想象中的那么准确。
- en: For instance, a mostly healthy 25-year-old person will look vastly different
    than another 25-year-old that's a heavy drinker and smoker.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个大致健康的25岁的人与另一个25岁的重度饮酒和吸烟者看起来会有很大的不同。
- en: 'Either way, we trusted the power of **AutoML** to find two models: one for
    gender classification and another for age prediction. We must highlight that,
    in this case, we framed age prediction as a classification problem instead of
    a regression one. This is because it makes it a bit easier to select an age range
    instead of producing a precise quantity.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我们相信**AutoML**的力量，找到了两个模型：一个用于性别分类，另一个用于年龄预测。我们必须强调，在这种情况下，我们将年龄预测视为分类问题，而不是回归问题。这是因为这样更容易选择一个年龄范围，而不是给出一个精确的数值。
- en: After a long wait (we trained both models over 100 epochs per trial), we obtained
    two competent networks that we integrated into a framework that automatically
    detects a face in a photo, and using these models, tags them with the predicted
    age and gender.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 经过长时间的等待（我们对每个模型进行了超过100轮的训练），我们得到了两个合格的网络，并将它们整合到一个框架中，该框架可以自动识别照片中的人脸，并使用这些模型标注出预测的年龄和性别。
- en: As you may have noticed, we relied on `ImageClassifier()`, which means we gave
    100% control of the network creation process to `AutoModel` to narrow down the
    search space, therefore arriving at potentially better solutions at a fraction
    of the time. Why don't you give it a try?
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们依赖于`ImageClassifier()`，这意味着我们将网络创建过程的100%控制权交给了`AutoModel`，以缩小搜索空间，从而在更短的时间内获得潜在的更好解决方案。为什么不试试看呢？
- en: See also
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'Read the following paper to learn how the authors of the **Adience** dataset
    solve this problem: [https://talhassner.github.io/home/projects/cnn_agegender/CVPR2015_CNN_AgeGenderEstimation.pdf](https://talhassner.github.io/home/projects/cnn_agegender/CVPR2015_CNN_AgeGenderEstimation.pdf).
    To learn more about the Haar Cascade classifier we used previously, read this
    tutorial: [https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html](https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html).'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读以下论文，了解**Adience**数据集的作者如何解决这个问题：[https://talhassner.github.io/home/projects/cnn_agegender/CVPR2015_CNN_AgeGenderEstimation.pdf](https://talhassner.github.io/home/projects/cnn_agegender/CVPR2015_CNN_AgeGenderEstimation.pdf)。要了解更多关于我们之前使用的Haar
    Cascade分类器的信息，请阅读这个教程：[https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html](https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html)。
