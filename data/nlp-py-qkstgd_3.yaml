- en: Leveraging Linguistics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用语言学
- en: In this chapter, we are going to pick up a simple use case and see how we can
    solve it. Then, we repeat this task again, but on a slightly different text corpus.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将选择一个简单的用例，看看我们如何解决它。然后，我们再次执行这个任务，但是在一个略微不同的文本语料库上。
- en: This helps us learn about build intuition when using linguistics in NLP. I will
    be using spaCy here, but you are free to use NLTK or an equivalent. There are
    programmatic differences in their APIs and styles, but the underlying theme remains
    the same.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于我们了解在自然语言处理中使用语言学时的构建直觉。在这里，我将使用spaCy，但您可以使用NLTK或等效工具。它们的API和风格存在程序性差异，但基本主题保持不变。
- en: In the previous chapter, we had our first taste of handling free text. Specifically,
    we learned how to tokenize text into words and sentences, pattern match with regex,
    and make fast substitutions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们第一次尝试处理自由文本。具体来说，我们学习了如何将文本标记为单词和句子，使用正则表达式进行模式匹配，以及进行快速替换。
- en: By doing all of this, we operated with text on a *string* as the main representation.
    In this chapter, we will use *language* and *grammar* as the main representations.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 通过做所有这些，我们以文本的*字符串*作为主要表示形式。在本章中，我们将使用*语言*和*语法*作为主要表示形式。
- en: 'In this chapter, we will learn about the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: spaCy, the natural language library for industrial use
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: spaCy，工业用自然语言库
- en: The NLP pipeline, and a bit of English grammar
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理管道，以及一些英语语法
- en: Real-life examples regarding what we can do with linguistics
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于我们可以用语言学做什么的现实生活示例
- en: Linguistics and NLP
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言学与自然语言处理
- en: '**T**his section is dedicated to introducing you to the ideas and tools that
    have been around during several decades of linguistics. The most traditional way
    to introduce this is to take an idea, talk about it at length, and then put all
    of this together.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**T**his部分致力于向您介绍在语言学几十年的发展中一直存在的思想和工具。介绍这种思想的最传统方式是先提出一个想法，详细地讨论它，然后把这些都放在一起。'
- en: Here, I am going to do this the other way around. We will solve two problems
    and, in the process, look at the tools we will be using. Instead of talking to
    you about a number 8 spanner, I am giving you a car engine and the tools, and
    I will introduce the tools as I use them.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我将反过来这样做。我们将解决两个问题，在这个过程中，我们将查看我们将使用的工具。而不是向您介绍一个8号扳手，我给您提供了一个汽车引擎和工具，我将在使用它们时介绍这些工具。
- en: Most NLP tasks are solved in a sequential pipeline, with the results from one
    component feeding into the next.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数自然语言处理任务都是在顺序管道中解决的，一个组件的结果会输入到下一个组件。
- en: There is a wide variety of data structures that are used to store pipeline results
    and intermediate steps. Here, for simplicity, I am going to use only the data
    structures that are already in spaCy and the native Python ones like lists and
    dictionaries.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 存储管道结果和中间步骤的数据结构种类繁多。在这里，为了简单起见，我将只使用spaCy中已有的以及原生的Python数据结构，如列表和字典。
- en: 'Here, we will tackle the following real-life inspired challenges:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将解决以下源于现实生活的挑战：
- en: Redacting names from any document, for example, for GDPR compliance
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从任何文档中删除姓名，例如，为了符合GDPR规定
- en: Making quizzes from any text, for example, from a Wikipedia article
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从任何文本中制作测验，例如，从维基百科文章中
- en: Getting started
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用
- en: 'You can install spaCy via `conda` or `pip`. Since I am in a `conda` environment,
    I will use the `conda` installation, as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过`conda`或`pip`安装spaCy。由于我处于`conda`环境中，我将使用`conda`安装，如下所示：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let's download the English language model provided by spaCy. We are going to
    use `en_core_web_lg` (the `lg` at the end stands for *large*). This means that
    this is the most comprehensive and best performing model that spaCy has released
    for general-purpose use.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们下载spaCy提供的英语语言模型。我们将使用`en_core_web_lg`（末尾的`lg`代表*大型*）。这意味着这是spaCy为通用目的发布的最全面和性能最好的模型。
- en: 'You only need to do this once:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 您只需做一次：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you run into any errors when you download this, you can use the smaller model
    instead.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在下载时遇到任何错误，您可以使用较小的模型代替。
- en: For Windows Shell, you can use `python -m spacy download en` as the administrator.
    From a Linux Terminal, you can use `sudo python -m spacy download en`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Windows Shell，您可以使用管理员权限执行`python -m spacy download en`。从Linux终端，您可以使用`sudo
    python -m spacy download en`。
- en: 'Let''s get the imports out of the way:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先处理一下导入：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The version I am using here is `version 2.0.11` from conda, but you can use
    any version above 2.0.x.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里使用的是来自conda的`version 2.0.11`版本，但您可以使用任何高于2.0.x的版本。
- en: Introducing textacy
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍textacy
- en: 'Textacy is a very underappreciated set of tools that revolves around spaCy.
    Its tagline tells you exactly what it does: *NLP, before and after spaCy*. It
    implements tools that use spaCy under the hood, ranging from data-streaming utilities
    for production use to higher level text-clustering functions.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Textacy是一套非常被低估的工具集，它围绕spaCy构建。其标语清楚地告诉你它做什么：*NLP，spaCy前后*。它实现了使用spaCy底层工具的工具，从用于生产的数据流实用程序到高级文本聚类功能。
- en: 'You can install textacy via `pip` or `conda`. On `conda`, it''s available on
    the `conda-forge` channel instead of the main `conda` channel. We''ve done this
    by adding a `-c` flag and the channel name after that:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过`pip`或`conda`安装textacy。在`conda`中，它可在`conda-forge`频道而不是主`conda`频道中找到。我们通过添加`-c`标志和之后的频道名称来实现这一点：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have the set up and have installation out of our way, let's get
    ready to tackle our challenge in the following section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经完成了设置并且安装问题已经解决，那么让我们为下一节中的挑战做好准备。
- en: Redacting names with named entity recognition
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用命名实体识别进行姓名红字
- en: The challenge for this section is to replace all human names with [REDACTED]
    in free text.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本节挑战在于将自由文本中所有的人名替换为[REDACTED]。
- en: Let's imagine that you are a new engineer at the European Bank Co. In preparation
    for the **General Data Processing Regulation** (**GDPR**), the bank is scrubbing
    off names of their customers from all of their old records and special internal
    communications like email and memos. They ask you to do this.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是欧洲银行公司的初级工程师。为了准备**通用数据保护条例**（**GDPR**），银行正在从所有旧记录和特殊内部通讯（如电子邮件和备忘录）中清除客户的姓名。他们要求你这么做。
- en: The first way you can do this is to look up the names of your customers and
    match each of them against all of your emails. This can be painfully slow and
    error-prone. For example, let's say the bank has a customer named John D'Souza
    – you might simply refer to him as DSouza in an email, so an exact match for D'Souza
    will never be scrubbed from the system.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以采取的第一种方法是查找客户的姓名，并将每个姓名与所有电子邮件进行匹配。这可能会非常慢且容易出错。例如，假设银行有一个名叫John D'Souza的客户——你可能会在电子邮件中简单地称他为DSouza，因此D'Souza的精确匹配永远不会从系统中清除。
- en: Here, we will use an automatic NLP technique to assist us. We will parse all
    of our emails from spaCy and simply replace everyone's names with the token [REDACTED].
    This will be at least 5-10 times faster than matching millions of substrings against
    millions of substrings.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用自动NLP技术来帮助我们。我们将使用spaCy解析所有电子邮件，并将所有人的姓名简单地替换为标记[REDACTED]。这将至少比匹配数百万个子字符串快5-10倍。
- en: 'We will use a small excerpt from the *Harry Potter and Chamber of Secrets*,
    talking about flu as an example:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用《哈利·波特与密室》的一小段摘录作为例子，讨论流感：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s parse the text with spaCy. This runs the entire NLP pipeline:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用spaCy解析文本。这运行了整个NLP管道：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`doc` now contains a parsed version of the text. We can use it to do anything
    we want! For example, the following command will print out all the named entities
    that were detected:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`doc`现在包含文本的解析版本。我们可以用它来做任何我们想做的事情！例如，以下命令将打印出所有检测到的命名实体：'
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The spaCy object `doc` has an attribute called `ents` which stores all detected
    entities. To find this, spaCy has done a few things behind the scenes for us,
    for example:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy对象`doc`有一个名为`ents`的属性，用于存储所有检测到的实体。为了找到这个，spaCy为我们做了几件幕后工作，例如：
- en: '**Sentence segmentation**, to break the long text into smaller sentences'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**句子分割**，将长文本分割成更小的句子'
- en: '**Tokenization**, to break each sentence into individual words or tokens'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词**，将每个句子分割成单独的单词或标记'
- en: '**Removed stop words**, to remove words like *a, an, the,* and *of*'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移除停用词**，移除像*a, an, the,*和*of*这样的词'
- en: '**NER** for statistical techniques in order to find out which *entities* are
    there in the text and label them with the entity''s type'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命名实体识别**，用于统计技术，以找出文本中的哪些*实体*，并用实体类型进行标记'
- en: 'Let''s take a quick look at the `doc` object, too:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下`doc`对象：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `doc` object has a specific object called `ents`, which is short for entities.
    We can use these to look up all of the entities in our text. Additionally, each
    entity has a label:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`doc`对象有一个名为`ents`的特定对象，简称实体。我们可以使用这些来查找文本中的所有实体。此外，每个实体都有一个标签：'
- en: In spaCy, all information is stored by numeric hashing. Therefore, `entity.label`
    will be a numeric entry like 378, while `entity.label_` will be human-readable,
    for example, `PERSON`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 spaCy 中，所有信息都是通过数字哈希存储的。因此，`entity.label` 将是一个数字条目，例如 378，而 `entity.label_`
    将是可读的，例如，`PERSON`。
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In spaCy, all human-readable labels can also be explained using the simple
    `spacy.explain(label)` syntax:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在 spaCy 中，所有可读的标签也可以使用简单的 `spacy.explain(label)` 语法进行解释：
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Using spaCy''s NER, let''s write a simple function to replace each PERSON name
    with [REDACTED]:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 spaCy 的 NER，让我们编写一个简单的函数来将每个 PERSON 名称替换为 [已删除]：
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The function takes in text as a string and parses it in the `doc` object using
    the `nlp` object, which we loaded earlier. Then, it traverses each token in the
    document (remember tokenization?). Each token is added to a list. If the token
    has the entity type of a person, it is replaced with [REDACTED] instead.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 函数接收一个字符串作为文本输入，并使用我们之前加载的 `nlp` 对象在 `doc` 对象中解析它。然后，它遍历文档中的每个标记（记得标记化吗？）。每个标记都被添加到一个列表中。如果标记具有人的实体类型，则将其替换为[已删除]。
- en: 'At the end, we reconstruct the original sentence by converting this list back
    into a string:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过将这个列表转换回字符串来重建原始句子：
- en: As an exercise, try completing this challenge in-place by editing the original
    string itself instead of creating a new string.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，尝试直接编辑原始字符串来完成这个挑战，而不是创建一个新的字符串。
- en: '[PRE12]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The preceding output is still a leaky faucet if you are trying to make GDPR-compliant
    edits. By using two [REDACTED] blocks instead of one, we are disclosing the number
    of words in a name. This can be seriously harmful if we were to use this in some
    other context, for example, redacting locations or organization names.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您试图进行符合 GDPR 的编辑，前面的输出仍然是一个漏洞百出的水龙头。通过使用两个 [已删除] 块而不是一个，我们正在披露姓名中的单词数量。如果我们将此用于其他上下文，例如删除地点或组织名称，这可能会非常有害。
- en: 'Let''s fix this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修复这个问题：
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We do this by merging entities separately from the pipeline. Notice the two
    extra lines of code which call `ent.merge()` on all entities found. The `ent.merge()`
    function combines all of the tokens in each *entity* into one single token. This
    is why it needs to be called on each entity:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过从管道中单独合并实体来实现这一点。注意，这里有两行额外的代码，它们在所有找到的实体上调用 `ent.merge()`。`ent.merge()`
    函数将每个 *实体* 中的所有标记合并为一个单独的标记。这就是为什么它需要在每个实体上调用：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This output, in practice, can still be incomplete. You might want to remove
    the gender here, for example, *Madam*. Since we are already disclosing the designation,
    which is *nurse*, giving away the gender makes it easier to infer for people (or
    even machines) who are reading this document.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这个输出仍然可能是不完整的。您可能想在这里删除性别，例如，*女士*。由于我们已经在披露称号，即 *护士*，透露性别使得阅读此文档的人（甚至机器）更容易推断出来。
- en: 'Exercise: Remove any gender pronouns in reference to names.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 练习：删除对姓名的任何性别代词。
- en: 'Hint: Look up the co-reference resolution to help you with this.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：查找共指消解以帮助您完成此操作。
- en: Entity types
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实体类型
- en: 'spaCy supports the following entity types in the large language model that
    we loaded in the `nlp` object:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 支持我们在 `nlp` 对象中加载的大语言模型中的以下实体类型：
- en: '| Type | Description |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 描述 |'
- en: '| --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| PERSON | People, including fictional people |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 人 | 包括虚构人物 |'
- en: '| NORP | Nationalities or religious or political groups |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| NORP | 国籍或宗教或政治团体 |'
- en: '| FAC | Buildings, airports, highways, bridges, and so on |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| FAC | 建筑物、机场、高速公路、桥梁等 |'
- en: '| ORG | Companies, agencies, institutions, and so on |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 组织 | 公司、机构、机构等 |'
- en: '| GPE | Countries, cities, states |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| GPE | 国家、城市、州 |'
- en: '| LOC | Non-GPE locations, mountain ranges, bodies of water |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 地点 | 非GPE地点、山脉、水体 |'
- en: '| PRODUCT | Objects, vehicles, foods, and so on (not services) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 产品 | 物体、车辆、食物等（不包括服务） |'
- en: '| EVENT | Named hurricanes, battles, wars, sports events, and so on |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 事件 | 命名的飓风、战役、战争、体育赛事等 |'
- en: '| WORK_OF_ART | Titles of books, songs, and so on |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 艺术作品 | 书籍、歌曲等的标题 |'
- en: '| LAW | Named documents made into laws |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 法律 | 被制成法律的命名文件 |'
- en: '| LANGUAGE | Any named language |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 语言 | 任何命名语言 |'
- en: '| DATE | Absolute or relative dates or periods |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 日期 | 绝对或相对日期或时间段 |'
- en: '| TIME | Times smaller than a day |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 时间 | 小于一天的时间单位 |'
- en: '| PERCENT | Percentage, including *%* |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 百分比 | 包括 *%* |'
- en: '| MONEY | Monetary values, including unit |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 货币 | 包括单位的货币值 |'
- en: '| QUANTITY | Measurements, such as weight or distance |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 数量 | 如重量或距离的度量 |'
- en: '| ORDINAL | *First*, *second*, and so on |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 序数词 | *第一*、*第二*等 |'
- en: '| CARDINAL | Numerals that do not fall under another type |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 基数 | 不属于其他类型的数词 |'
- en: Let's look at some examples of the preceding entity types in real-world sentences.
    We will also use `spacy.explain()` on all of the entities to build a quick mental
    model of how these things work.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些现实世界句子中先前实体类型的例子。我们还将使用`spacy.explain()`来解释所有实体，以快速建立对这些事物如何工作的心理模型。
- en: 'Given how lazy I am, I will write a function that I can reuse again and again
    so that I can simply focus on learning and not debugging code for different examples:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我懒惰，我会写一个可以反复使用的函数，这样我就可以专注于学习，而不是为不同的例子调试代码：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s give it a spin with a few simple examples to begin with:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先用几个简单的例子试一试：
- en: '[PRE16]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let''s look at a slightly longer sentence and Eastern example:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个稍微长一点的句子和东方例子：
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Interesting – the model got `Taj Mahal` wrong. Taj Mahal is obviously a world-famous
    monument. However, the model has made a believable mistake, because `Taj Mahal`
    was also the stage name of a blues musician.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，模型将`泰姬陵`弄错了。泰姬陵显然是一座世界著名的纪念碑。然而，模型犯了一个可信的错误，因为`泰姬陵`也是一位蓝调音乐家的艺名。
- en: In most production use cases, we *fine-tune* the built-in spaCy models for specific
    languages using our own annotations. This will teach the model that Taj Mahal,
    for us, is almost always a monument and not a blues musician.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数生产用例中，我们使用自己的注释来对内置的spaCy模型进行特定语言的微调。这将教会模型，对我们来说，泰姬陵几乎总是指一座纪念碑，而不是一位蓝调音乐家。
- en: 'Let''s see if the model repeats these mistakes in other examples:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看模型在其他例子中是否会重复这些错误：
- en: '[PRE18]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s try a different sentence with a different meaning of Ashoka:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个不同含义的阿育王的不同句子：
- en: '[PRE19]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Here, spaCy is able to leverage the word `University` to infer that Ashoka is
    a name of an organization and not King Ashoka from Indian history.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，spaCy能够利用单词`大学`来推断Ashoka是一个组织的名字，而不是印度历史上的阿育王。
- en: It has also figured out that `Young India Fellowship` is one logical entity
    and has not tagged `India` as a location.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 它还确定`Young India Fellowship`是一个逻辑实体，并且没有将`India`标记为地点。
- en: It helps to see a few examples such as these ones to form a mental model regarding
    what the limits of what we can and cannot do are.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 看几个这样的例子有助于形成关于我们能做什么和不能做什么的心理模型。
- en: Automatic question generation
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动问题生成
- en: Can you automatically convert a sentence into a question?
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你能自动将一个句子转换成问题吗？
- en: For instance, *Martin Luther King Jr. was a civil rights activist and skilled
    orator.* to *Who was Martin Luther King Jr.?*
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，*马丁·路德·金是一位民权活动家和熟练的演说家*，变为*马丁·路德·金是谁？*
- en: Notice that when we convert a sentence into a question, the answer might not
    be in the original sentence anymore. To me, the answer to that question might
    be something different, and that's fine. We are not aiming for answers here.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当我们把一个句子转换成问题，答案可能不再在原始句子中。对我来说，那个问题的答案可能不同，这没关系。我们在这里不是追求答案。
- en: Part-of-speech tagging
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词性标注
- en: Sometimes, we want to pull out keywords or keyphrases from a larger body of
    text quickly. This helps us mentally paint a picture of what this text is about.
    This is particularly helpful in the analysis of texts, like long emails or essays.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们希望快速从大量文本中提取关键词或关键短语。这有助于我们在心理上描绘出文本的主题。这在分析文本，如长电子邮件或论文时尤其有用。
- en: 'As a quick hack, we can pull out all relevant *nouns*. This is because most
    keywords are in fact nouns of some form:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种快速的方法，我们可以提取所有相关的*名词*。这是因为大多数关键词实际上是以某种形式存在的名词：
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We need noun chunks. Noun chunks are noun phrases – not single words, but a
    short phrase which describes the noun. For example, *the blue skies* or *the world's
    largest conglomerate*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要名词块。名词块是名词短语——不是单个单词，而是一个短语，用来描述名词。例如，*蓝天*或*世界最大的企业集团*。
- en: 'To get the noun chunks from a document, simply iterate over `doc.noun_chunks`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要从文档中获取名词块，只需迭代`doc.noun_chunks`：
- en: '[PRE22]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Our example text has two sentences, and we can pull out noun phrase chunks from
    each sentence. We will pull out noun phrases instead of single words. This means
    that we are able to pull out *an Indian classical instrument* as one noun. This
    is quite useful, and we will see why in a moment.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例文本有两个句子，我们可以从每个句子中提取名词短语块。我们将提取名词短语而不是单个单词。这意味着我们能够提取*一个印度古典乐器*作为一个名词。这非常有用，我们将在稍后看到原因。
- en: 'Next, let''s take a quick look at all of the parts-of-speech tags in our example
    text. We will use verbs and adjectives to write some simple question-generating
    logic:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们快速浏览一下示例文本中所有的词性标签。我们将使用动词和形容词来编写一些简单的生成问题的逻辑：
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Notice that here, *instrument* is tagged as a NOUN, while *Indian* and *classical*
    are tagged as adjectives. This makes sense. Additionally, *Bansoori* and *Guitar*
    are tagged as PROPN, or proper nouns.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这里，*工具*被标记为名词，而*印度人*和*古典的*被标记为形容词。这很有道理。此外，*班苏里*和*吉他*被标记为专有名词，或称PROPN。
- en: '**Common nouns versus** **proper nouns:** Nouns name people, places, and things.
    Common nouns name general items such as waiter, jeans, and country. Proper nouns
    name specific things such as Roger, Levi''s, and India.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**普通名词与专有名词的区别**：名词用来命名人、地点和事物。普通名词用来命名一般项目，例如服务员、牛仔裤和国家。专有名词用来命名特定事物，例如罗杰、李维斯和印度。'
- en: Creating a ruleset
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个规则集
- en: 'Quite often when using linguistics, you will be writing custom rules. Here
    is one data structure suggestion to help you store these rules: a list of dictionaries.
    Each dictionary in turn can have elements ranging from simple string lists to
    lists of strings. Avoid nesting a list of dictionaries inside a dictionary:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用语言学时，你经常会编写自定义规则。这里有一个数据结构建议，可以帮助你存储这些规则：字典列表。每个字典可以包含从简单的字符串列表到字符串列表的各种元素。避免在字典内部嵌套字典列表：
- en: '[PRE24]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Here, I have written two rules. Each rule is simply a collection of part-of-speech
    tags that has been stored under the `req_tags` key. Each rule is comprised of
    all of the tags that I will look for in a particular sentence.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我编写了两个规则。每个规则只是存储在`req_tags`键下的词性标签集合。每个规则由我将要在特定句子中查找的所有标签组成。
- en: Depending on `id`, I will use a hardcoded question template to generate my questions.
    In practice, you can and should move the question template to your ruleset.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 根据`id`，我将使用硬编码的问题模板来生成我的问题。在实践中，你可以也应该将问题模板移动到你的规则集中。
- en: 'Next, I need a function to pull out all of the tokens that match a particular
    tag. We do this by simply iterating over the entire list of and matching each
    token against the target tag:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我需要一个函数来提取所有与特定标签匹配的标记。我们通过简单地遍历整个列表并匹配每个标记与目标标签来完成这项工作：
- en: '[PRE25]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'On runtime complexity:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 关于运行时复杂度：
- en: This is slow O(n). As an exercise, can you think of a way to reduce this to
    O(1)?
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这很慢，是O(n)。作为一个练习，你能想到一种方法将其减少到O(1)吗？
- en: 'Hint: You can precompute some results and store them, but at the cost of more
    memory consumption.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：你可以预先计算一些结果并将它们存储起来，但这会以更多的内存消耗为代价。
- en: Next, I am going to write a function to use the preceding ruleset, and also
    use a question template.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将编写一个函数来使用前面的规则集，并使用问题模板。
- en: 'Here is the broad outline that I will follow for each sentence:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我将为每个句子遵循的广泛概述：
- en: For each rule ID, check if all the required tags (`req_tags`) meet the conditions
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个规则ID，检查所有必需的标签（`req_tags`）是否满足条件
- en: Find the first rule ID that matches
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到第一个匹配的规则ID
- en: Find the words that match the required part of the speech tags
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到匹配所需词性标签的单词
- en: Fill in the corresponding question template and return the question string
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填写相应的问题模板并返回问题字符串
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Within each rule ID match, I do something more: I drop all but the first match
    for each part-of-speech tag that I receive. For instance, when I query for `NNP`,
    I later pick the first element with `NNP[0]`, convert it into a string, and drop
    all other matches.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个规则ID匹配中，我会做更多的事情：我会丢弃除了第一个匹配之外的所有匹配项，对于我收到的每个词性标签。例如，当我查询`NNP`时，我稍后会选择带有`NNP[0]`的第一个元素，将其转换为字符串，并丢弃所有其他匹配项。
- en: 'While this is a perfectly good approach for simple sentences, this breaks down
    when you have conditional statements or complex reasoning. Let''s run the preceding
    function for each sentence in the example text and see what questions we get:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这对于简单句子来说是一个非常好的方法，但当遇到条件语句或复杂推理时，这种方法就会失效。让我们运行前面函数中的每个句子，看看我们会得到什么问题：
- en: '[PRE27]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This is quite good. In practice, you will need a much larger set, maybe 10-15
    rulesets and corresponding templates just to have a decent coverage of *What?*
    questions.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当不错。在实践中，你可能需要一个更大的集合，可能需要10-15个规则集和相应的模板，才能对*什么？*问题有足够的覆盖。
- en: Another few rulesets might be needed to cover *When*, *Who*, and *Where* type
    questions. For instance, *Who plays Bansoori?* is also a valid question from the
    second sentence that we have in the preceding code.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 可能还需要几个规则集来涵盖*何时*、*谁*和*哪里*类型的问题。例如，*谁演奏班苏里？*也是从前面代码中的第二个句子中得出的有效问题。
- en: Question and answer generation using dependency parsing
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用依存句法进行问答生成
- en: This means PoS tagging and a rule-driven engine can have large coverage and
    reasonable precision with respect to the questions – but it will still be a little
    tedious to maintain, debug, and generalize this system.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着词性标注和基于规则的引擎在处理这些问题时可以具有很大的覆盖率和合理的精确度，但维护、调试和泛化这个系统仍然会有些繁琐。
- en: We need a set of better tools that is less reliant on the *state* of tokens
    and more on the relationship between them. This will allow you to change the relationship
    to form a question instead. This is where dependency parsing comes in.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个更好的工具集，它更少依赖于*标记*的状态，而更多地依赖于它们之间的关系。这将允许你改变关系来形成问题。这就是依存句法分析的作用所在。
- en: What is a dependency parser?
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是依存句法分析器？
- en: '"A dependency parser analyzes the grammatical structure of a sentence, establishing
    relationships between "head" words and words which modify those heads."'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '"依存句法分析器分析句子的语法结构，建立“头部”词与修饰这些头部词的词之间的关系。"'
- en: '- from [Stanford NNDEP Project](https://nlp.stanford.edu/software/nndep.html)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '- 来自 [斯坦福NNDEP项目](https://nlp.stanford.edu/software/nndep.html)'
- en: A dependency parser helps us understand the various ways in which parts of the
    sentence interact or depend on each other. For instance, how is a noun modified
    by adjectives?
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 依存句法分析器帮助我们理解句子各部分之间相互作用的多种方式。例如，名词是如何被形容词修饰的？
- en: '[PRE28]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Some of these terms are simple enough to guess, for example, `ROOT` is where
    the dependency tree might begin, `nsubj` is the noun or nominal subject, and `cc`
    is a conjunction. However, this is still incomplete. Luckily for us, spaCy includes
    the nifty `explain()` function to help us interpret these:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些术语足够简单，可以猜测，例如，`ROOT`是依存树可能开始的地方，`nsubj`是名词或名词主语，而`cc`是连词。然而，这仍然是不完整的。幸运的是，spaCy包括了巧妙的`explain()`函数来帮助我们解释这些：
- en: '[PRE29]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This gives us the following explainer text:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下解释性文本：
- en: '[PRE30]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This gives us a good starting point to Google away and pick up some linguistics-specific
    terms. For example, a *conjunct* is often used to connect two clauses, while an
    *attribute* is simply a way to highlight something which is a property of the
    nominal subject.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一个很好的起点，可以Google一些语言学特定的术语。例如，*conjunct*通常用于连接两个子句，而*attribute*只是强调名词主语属性的一种方式。
- en: Nominal subjects are usually nouns or pronouns, which, in turn, are actors (via
    verbs) or have properties (via attributes).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 名词主语通常是名词或代词，它们反过来又是行为者（通过动词）或具有属性（通过属性）。
- en: Visualizing the relationship
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化关系
- en: 'spaCy has a built-in tool called **displacy** for displaying simple, but clean
    and powerful visualizations. It offers two primary modes: named entity recognition
    and dependency parsing. Here, we will use the `dep`, or dependency mode:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy有一个内置工具称为**displacy**，用于显示简单但清晰且强大的可视化。它提供两种主要模式：命名实体识别和依存句法。在这里，我们将使用`dep`，或依存模式：
- en: '[PRE31]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let''s take the first sentence for a quick study: we can see that **instrument**
    is **amod**, or adjectively modified by **Indian classicial**. We pulled this
    phrase earlier as a noun chunk:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速研究第一句话：我们可以看到**instrument**是**amod**，即由**Indian classical**形容词修饰。我们之前已经把这个短语作为名词块提取出来：
- en: '![](img/550acbbf-98b4-49f1-bce5-933a2c814a19.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/550acbbf-98b4-49f1-bce5-933a2c814a19.png)'
- en: This means that when we pulled noun phrase chunks out of this sentence, spaCy
    must have finished dependency parsing already under the hood.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着当我们从这个句子中提取名词短语块时，spaCy已经在幕后完成了依存句法分析。
- en: Also, notice the direction of arrows while the NOUN (instrument) is being modified
    by ADJ. It is the `attr` of the ROOT VERB (is).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，注意箭头的方向，当NOUN（instrument）被ADJ（形容词）修饰时。它是ROOT动词（is）的`attr`属性。
- en: 'I leave the dependency visualization of the second sentence up to you to complete:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我把第二句话的依存可视化留给你来完成：
- en: '![](img/c77e59c7-1051-490d-ac47-b5899ee247c3.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c77e59c7-1051-490d-ac47-b5899ee247c3.png)'
- en: This logical tree structure of simple sentences is what we will exploit to simplify
    our question generation. To do this, we need two important pieces of information
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这简单句子的逻辑树结构是我们将利用来简化问题生成的。为此，我们需要两个重要的信息
- en: The main verb, also known as the ROOT
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要动词，也称为ROOT
- en: The subjects on which this ROOT verb is acting
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个ROOT动词所作用的主语
- en: Let's write some functions to extract these dependency entities in the spaCy
    token format, without converting them into strings.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一些函数来提取这些依存实体，以spaCy标记格式，而不将它们转换为字符串。
- en: Introducing textacy
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍textacy
- en: 'Alternatively, we can import them from textacy itself:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以从textacy本身导入它们：
- en: '[PRE32]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Inside a Jupyter Notebook, you can see the docstring AND function implementation
    by using the `??` syntax inside the Jupyter Notebook itself:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jupyter Notebook中，你可以通过在Jupyter Notebook本身中使用`??`语法来看到文档字符串和函数实现：
- en: '[PRE33]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Usually, when you ask somebody a question, they are often about a piece of information,
    for example, *What is the capital of India?* Sometimes, they are also about a
    certain action, for example, W*hat did you do on Sunday?*
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当你问某人一个问题，他们通常是在询问一些信息，例如，*印度的首都是什么？*有时，他们也可能是在询问某个特定的行为，例如，*你周日做了什么？*
- en: 'Answering *what* means that we need to find out what the verbs are acting on.
    This means that we need to find the subjects of the verb. Let''s take a more concrete
    but simple example to explore this:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 回答“什么”意味着我们需要找出动词所作用的对象。这意味着我们需要找到动词的主语。让我们用一个更具体但简单的例子来探讨这个问题：
- en: '[PRE34]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: What are the entities in this sentence?
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这句话中的实体是什么？
- en: '[PRE35]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](img/b7a4ae6b-1235-4487-b4ba-57abeacd1086.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b7a4ae6b-1235-4487-b4ba-57abeacd1086.png)'
- en: The preceding example might return ORG for the smaller en model. This is why
    using `en_core_web_lg` is important. It gives much better performance.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的例子可能会为较小的en模型返回ORG。这就是为什么使用`en_core_web_lg`很重要的原因。它提供了更好的性能。
- en: 'Let''s try the first few lines of Berlin''s Wikipedia entry:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试柏林维基百科条目的前几行：
- en: '[PRE36]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![](img/2004af84-7959-461e-ac29-4adffdaf01ac.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2004af84-7959-461e-ac29-4adffdaf01ac.png)'
- en: 'Let''s find out the main verb in this sentence:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们找出这个句子中的主要动词：
- en: '[PRE37]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: And what are the nominal subjects of this verb?
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这个动词的名词主语是什么？
- en: '[PRE38]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You will notice that this has a reasonable overlap with the noun phrases that
    we pulled from our part-of-speech tagging. However, some of them are different,
    too:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到这合理地重叠了我们从词性标注中提取的名词短语。然而，也有一些是不同的：
- en: '[PRE39]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As an exercise, extend this approach to at least add *Who*, *Where*, and *When*
    questions as a best practice.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，将这种方法扩展到至少添加*谁*、*哪里*和*何时*问题，这是一个最佳实践。
- en: Leveling up – question and answer
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升水平——提问和回答
- en: So far, we have been trying to generate questions. But if you were trying to
    make an automated quiz for students, you would also need to mine the right answer.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在尝试生成问题。但如果你试图为学生制作自动测验，你还需要挖掘正确的答案。
- en: The answer, in this case, will be simply the objects of a verb. What is an object
    of a verb?
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，答案将是动词的宾语。动词的宾语是什么？
- en: In the sentence, "Give the book to me", "book" is the direct object of the verb
    "give", and "me" is the indirect object.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在句子“Give the book to me”中，“book”是动词“give”的直接宾语，“me”是间接宾语。
- en: – from the Cambridge English Dictionary
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: – 来自剑桥英语词典
- en: 'Loosely, the object is the piece on which our verb acts. This is almost always
    the answer to our *what* question. Let''s write a question to find the objects
    of any verb – or, we can pull it from `textacy.spacier.utils.`:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 拉丁语中，宾语是动词作用的对象。这几乎总是我们“什么”问题的答案。让我们写一个问题来找到任何动词的宾语——或者，我们可以从`textacy.spacier.utils.`中提取它：
- en: '[PRE40]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let''s do this for all of the verbs:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对所有的动词都这样做：
- en: '[PRE41]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let''s look at the output of our functions from the example text. The first
    is the sentence itself, then the root verb, then the lemma form of that verb,
    followed by the subjects of the verb, and finally the objects:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的函数从示例文本中的输出。首先是句子本身，然后是根动词，然后是那个动词的词形，接着是动词的主语，最后是动词的宾语：
- en: '[PRE42]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let''s arrange the preceding pieces of information into a neat function that
    we can then reuse:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把前面的信息整理成一个整洁的函数，然后我们可以重用它：
- en: '[PRE43]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let''s run it on our example text and see where it goes:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的示例文本上运行它，看看它会去哪里：
- en: '[PRE44]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This seems right to me. Let''s run this on a larger sample of sentences. This
    sample has varying degrees of complexities and sentence structures:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我看来似乎是正确的。让我们在更大的句子样本上运行这个。这个样本具有不同复杂程度和句子结构：
- en: '[PRE45]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let''s run it on the whole large example text:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在整个大型示例文本上运行它：
- en: '[PRE46]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Putting it together and the end
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整合并结束
- en: 'Linguistics is incredibly powerful. I have given you only a taste of its immense
    utility here. We looked at two motivating use cases and a lot of powerful ideas.
    For each use case, I have listed the related idea here:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 语言学非常强大。我在这里只给你尝到了它巨大效用的一小部分。我们研究了两个激励用例和许多强大的想法。对于每个用例，我在这里列出了相关的想法：
- en: 'Redacting names:'
  id: totrans-218
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改姓名：
- en: Named entity recognition
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: 'Question and answer generation:'
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提问和回答生成：
- en: Part-of-speech tagging
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词性标注
- en: Lemmatization
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词形还原
- en: Dependency parsing
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依存句法分析
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We now have a way to generate questions and answers. What were you going to
    ask the user? Can you match our answers against the user's answers?
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了一种生成问题和答案的方法。你打算问用户什么问题？你能将我们的答案与用户的答案进行匹配吗？
- en: Sure, an exact match is perfect. But we should also be looking for *meaning*
    matches, for example, *cake* with *pastry*, or *honesty* with *truthfulness*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，精确匹配是完美的。但我们也应该寻找*意义*匹配，例如，*蛋糕*与*糕点*，或者*诚实*与*真诚*。
- en: We could use a synonym dictionary – but how do we extend this into sentences
    and documents?
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用同义词词典——但如何将其扩展到句子和文档中呢？
- en: In the next chapter, we will answer all of these questions using text representations.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用文本表示来回答所有这些问题。
