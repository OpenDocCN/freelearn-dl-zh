- en: '*Chapter 4*: Makings of the Markov Decision Process'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 4 章*：马尔可夫决策过程的构成'
- en: In the first chapter, we talked about many applications of **Reinforcement Learning**
    (**RL**), from robotics to finance. Before implementing any RL algorithms for
    these applications, we need to first model them mathematically. **Markov Decision
    Process** (**MDP**) is the framework we use to model these sequential decision-making
    problems. MDPs have some special characteristics that make it easier for us to
    theoretically analyze those problems. Building on that theory, **Dynamic Programming**
    (**DP**) is the field that proposes solution methods for MDPs. RL, in some sense,
    is a collection of approximate DP approaches that enable us to obtain good (but
    not necessarily optimal) solutions to very complex problems that are intractable
    to solve with exact DP methods.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们讨论了**强化学习**（**RL**）的许多应用，从机器人学到金融。在为这些应用实现任何 RL 算法之前，我们需要先对其进行数学建模。**马尔可夫决策过程**（**MDP**）是我们用来建模这些序贯决策问题的框架。MDP
    具有一些特殊的特性，使得我们可以更容易地从理论上分析这些问题。在此基础上，**动态规划**（**DP**）是提出 MDP 解决方法的领域。从某种意义上讲，RL
    是一组近似 DP 方法，能够让我们为非常复杂、无法通过精确的 DP 方法解决的问题找到较好的（但不一定是最优的）解决方案。
- en: 'In this chapter, we will step-by-step build an MDP, explain its characteristics,
    and lay down the mathematical foundation for the RL algorithms coming up in later
    chapters. In an MDP, the actions an agent takes have long-term consequences, which
    is what differentiates it from the **Multi-Armed Bandit** (**MAB**) problems we
    covered earlier. This chapter focuses on some key concepts that quantify this
    long-term impact. It involves a bit more theory than other chapters, but don''t
    worry, we will quickly dive into Python exercises to get a better grasp of the
    concepts. Specifically, we cover the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将一步一步地构建一个 MDP，解释其特性，并为后续章节中将介绍的 RL 算法奠定数学基础。在 MDP 中，智能体采取的行动具有长期后果，这也是它与我们之前讨论的**多臂老虎机**（**MAB**）问题的区别。本章重点讨论一些量化这一长期影响的关键概念。虽然涉及的理论比其他章节稍多，但别担心，我们将很快进入
    Python 实践环节，帮助更好地掌握这些概念。本章具体包括以下内容：
- en: Markov chains
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马尔可夫链
- en: Markov reward processes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马尔可夫奖励过程
- en: Markov decision processes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: Partially observable MDPs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部分可观测的 MDP
- en: Starting with Markov chains
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从马尔可夫链开始
- en: We start this chapter with Markov chains, which do not involve any decision-making.
    They only model a special type of stochastic processes that are governed by some
    internal transition dynamics. Therefore, we won't talk about an agent yet. Understanding
    how Markov chains work will allow us to lay the foundation for the MDPs that we
    will cover later.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从马尔可夫链开始，这不涉及任何决策过程。它们仅仅是建模某些内部转移动态驱动的特定类型的随机过程。因此，我们暂时不谈及智能体。理解马尔可夫链如何工作，将帮助我们为后续涉及的马尔可夫决策过程（MDP）奠定基础。
- en: Stochastic processes with the Markov property
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 具有马尔可夫性质的随机过程
- en: We already defined the **state** as the set information that completely describes
    the situation that an environment is in. If the next state that the environment
    will transition into only depends on the current state, not the past ones, we
    say that the process has the **Markov property**. This is named after the Russian
    mathematician Andrey Markov.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了**状态**，即完全描述环境所处情境的集合。如果环境将要转移到的下一个状态仅仅依赖于当前状态，而不依赖于过去的状态，我们称该过程具有**马尔可夫性质**。这一性质得名于俄国数学家安德烈·马尔可夫。
- en: 'Imagine a broken robot that randomly moves in a grid world. At any given step,
    the robot goes up, down, left, and right with 0.2, 0.3, 0.25, and 0.25 probability,
    respectively. This is depicted in *Figure 4.1*, as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个在网格世界中随机移动的坏掉的机器人。在任何给定的步骤中，机器人以 0.2、0.3、0.25 和 0.25 的概率分别向上、向下、向左和向右移动。如下图
    *图 4.1* 所示：
- en: '![Figure 4.1 – A broken robot in a grid world, currently at (1,2)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.1 – 一个坏掉的机器人，当前位于 (1,2) 的网格世界中'
- en: '](img/B14160_04_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_04_01.jpg)'
- en: Figure 4.1 – A broken robot in a grid world, currently at (1,2)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 一个坏掉的机器人，当前位于 (1,2) 的网格世界中
- en: The robot is currently in state ![](img/Formula_04_001.png). It does not matter
    where it has come from; it will be in state ![](img/Formula_04_002.png) with a
    probability of ![](img/Formula_04_003.png), in ![](img/Formula_04_004.png) with
    a probability of ![](img/Formula_04_005.png), and so on. Since the probability
    of where it will transition next depends only on which state it is currently in,
    but not where it was before, the process has the Markov property.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人当前处于状态![](img/Formula_04_001.png)中。它从哪里来并不重要；它将以![](img/Formula_04_003.png)的概率进入状态![](img/Formula_04_002.png)，以![](img/Formula_04_005.png)的概率进入状态![](img/Formula_04_004.png)，依此类推。由于它下一步的转移概率仅取决于当前所处的状态，而与之前所处的状态无关，因此该过程具有马尔可夫性质。
- en: 'Let''s define this more formally. We denote the state at time ![](img/Formula_04_006.png)
    by ![](img/Formula_04_007.png). A process has the Markov property if the following
    holds for all states and times:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更正式地定义这一点。我们用![](img/Formula_04_007.png)表示时间![](img/Formula_04_006.png)时的状态。如果对于所有状态和时间，以下条件成立，则该过程具有马尔可夫性质：
- en: '![](img/Formula_04_008.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_008.jpg)'
- en: Such a stochastic process is called a **Markov chain**. Note that if the robot
    hits a wall, we assume that it bounces back and remains in the same state. So,
    while in state ![](img/Formula_04_009.png), for example, the robot will be still
    there in the next step with a probability of ![](img/Formula_04_010.png).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的随机过程称为**马尔可夫链**。请注意，如果机器人撞到墙壁，我们假设它会反弹回去并保持在原状态。因此，例如，在状态![](img/Formula_04_009.png)时，机器人将在下一步仍停留在那里，概率为![](img/Formula_04_010.png)。
- en: 'A Markov chain is usually depicted using a directed graph. The directed graph
    for the broken robot example in a ![](img/Formula_04_011.png) grid world would
    be as in *Figure 4.2*:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫链通常使用有向图来表示。在网格世界中，破损机器人示例的有向图如下所示，如*图 4.2*所示：
- en: '![Figure 4.2 – A Markov chain diagram for the robot example in a 2x2 grid world'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.2 – 2x2网格世界中机器人示例的马尔可夫链图'
- en: '](img/B14160_04_02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_04_02.jpg)'
- en: Figure 4.2 – A Markov chain diagram for the robot example in a 2x2 grid world
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 2x2网格世界中机器人示例的马尔可夫链图
- en: Tip
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Many systems can be made Markovian by including historical information in the
    state. Consider a modified robot example where the robot is more likely to continue
    in the direction it moved in the previous time step. Although such a system seemingly
    does not satisfy the Markov property, we can simply redefine the state to include
    the visited cells over the last two time steps, such as ![](img/Formula_04_012.png).
    The transition probabilities would be independent of the past states under this
    new state definition and the Markov property would be satisfied.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 许多系统可以通过将历史信息包含在状态中来变成马尔可夫过程。考虑一个修改后的机器人示例，其中机器人更可能继续沿着上一步的方向移动。虽然这样的系统表面上似乎不满足马尔可夫性质，但我们可以简单地重新定义状态，将过去两步中访问过的单元格包括在内，例如![](img/Formula_04_012.png)。在这个新状态定义下，转移概率将与过去的状态无关，马尔可夫性质将得到满足。
- en: Now that we have defined what a Markov chain is, let's go deeper. Next, we will
    look at how to classify states in a Markov chain as they might differ in terms
    of their transition behavior.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了马尔可夫链，接下来让我们深入探讨。接下来，我们将研究如何根据转移行为的不同来分类马尔可夫链中的状态。
- en: Classification of states in a Markov chain
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫链中状态的分类
- en: An environment that can go from any state to any other state after some number
    of transitions, as we have in our robot example, is a special kind of Markov chain.
    As you can imagine, a more realistic system would involve states with a richer
    set of characteristics, which we will introduce next.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一种环境可以在若干次过渡后从任何状态转移到任何其他状态，正如我们在机器人示例中所看到的那样，这是一种特殊类型的马尔可夫链。正如你所想，现实中更复杂的系统会涉及到具有更丰富特征集的状态，接下来我们将介绍这些特征。
- en: Reachable and communicating states
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可达状态与可通信状态
- en: If the environment can transition from state ![](img/Formula_04_013.png) to
    state ![](img/Formula_04_014.png) after some number of steps with a positive probability,
    we say ![](img/Formula_04_015.png) is **reachable** from ![](img/Formula_04_016.png).
    If ![](img/Formula_04_017.png) is also reachable from ![](img/Formula_04_018.png),
    those states are said to **communicate**. If all the states in a Markov chain
    communicate with each other, we say that the Markov chain is **irreducible**,
    which is what we had in our robot example.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果环境能够在一定步数后以正概率从状态 ![](img/Formula_04_013.png) 过渡到状态 ![](img/Formula_04_014.png)，我们称
    ![](img/Formula_04_015.png) 是从 ![](img/Formula_04_016.png) **可达** 的。如果 ![](img/Formula_04_017.png)
    也是从 ![](img/Formula_04_018.png) 可达的，那么这些状态被称为**通讯**。如果马尔可夫链中的所有状态都彼此通讯，我们称该马尔可夫链是**不可约**的，这正是我们机器人示例中的情况。
- en: Absorbing state
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 吸收态
- en: 'A state, ![](img/Formula_04_019.png), is an **absorbing state** if the only
    possible transition is to itself, which is ![](img/Formula_04_020.png). Imagine
    that the robot cannot move again if it crashes into a wall in the preceding example.
    This would be an example of an absorbing state since the robot can never leave
    it. The ![](img/Formula_04_021.png) version of our grid world with an absorbing
    state could be represented in a Markov chain diagram, as in *Figure 4.3*:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个状态 ![](img/Formula_04_019.png) 只能过渡到自身，即 ![](img/Formula_04_020.png)，那么这个状态被称为**吸收态**。想象一下，如果在前述例子中机器人撞到墙壁后无法再移动，那么这就是吸收态的一个例子，因为机器人永远无法离开这个状态。带有吸收态的网格世界版本可以用马尔可夫链图表示，如*图
    4.3*所示：
- en: '![Figure 4.3 – A Markov chain diagram with an absorbing state'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.3 – 具有吸收态的马尔可夫链图'
- en: '](img/B14160_04_03.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_04_03.jpg)'
- en: Figure 4.3 – A Markov chain diagram with an absorbing state
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – 具有吸收态的马尔可夫链图
- en: An absorbing state is equivalent to a terminal state that marks the end of an
    episode in the context of RL, which we defined in [*Chapter 1*](B14160_01_Final_SK_ePub.xhtml#_idTextAnchor016)*,
    Introduction to Reinforcement Learning*. In addition to terminal states, an episode
    can also terminate after a time limit T is reached.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RL 上下文中，吸收态等同于标记了一集结束的终端状态，我们在[*第 1 章*](B14160_01_Final_SK_ePub.xhtml#_idTextAnchor016)*
    强化学习导论*中定义过。除了终端状态外，一集还可以在达到时间限制 T 后结束。
- en: Transient and recurrent states
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 瞬态状态和经常态状态
- en: A state ![](img/Formula_04_022.png) is called a **transient state**, if there
    is another state ![](img/Formula_04_023.png), that is reachable from ![](img/Formula_04_024.png),
    but not vice versa. Provided enough time, an environment will eventually move
    away from transient states and never come back.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在另一个状态 ![](img/Formula_04_023.png)，可以从状态 ![](img/Formula_04_024.png) 到达它，但反之不成立，则状态
    ![](img/Formula_04_022.png) 被称为**瞬态状态**。在足够长的时间内，环境最终会远离瞬态状态并且不再返回。
- en: Consider a modified grid world with two sections; let's call them the light
    side and the dark side for fun. The possible transitions in this world are illustrated
    in *Figure 4.4*. Can you identify the transient state(s)?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个修改后的网格世界，分为两个部分；我们可以玩味地称它们为光面和暗面。这个世界中的可能过渡如 *图 4.4* 所示。你能识别出瞬态状态吗？
- en: '![Figure 4.4 – Grid world with a light and dark side'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.4 – 具有明暗面的网格世界'
- en: '](img/B14160_04_04.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_04_04.jpg)'
- en: Figure 4.4 – Grid world with a light and dark side
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – 具有明暗面的网格世界
- en: If your answer is ![](img/Formula_04_027.png) of the light side, think again.
    For each of the states on the light side, there is a way out to the states on
    the dark side without a way back. So, wherever the robot is on the light side,
    it will eventually transition into the dark side and won't be able to come back.
    Therefore, all the states on the light side are transient. Such a dystopian world!
    Similarly, in the modified grid world with a **crashed** state, all the states
    are transient except the **crashed** state.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为光面上的答案是 ![](img/Formula_04_027.png)，请再想一想。光面上的每个状态都有一条通向暗面的出路，但没有返回的可能性。因此，无论机器人位于光面的哪个位置，它最终都会过渡到暗面并且无法返回。因此，所有光面上的状态都是瞬态的。这是一个类似于反乌托邦的世界！同样，在带有**崩溃**状态的修改后网格世界中，所有状态都是瞬态的，除了**崩溃**状态。
- en: Finally, a state that is not transient is called a **recurrent state**. The
    states on the dark side are recurrent in this example.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，不是瞬态的状态被称为**经常态状态**。在这个例子中，暗面上的状态是经常态的。
- en: Periodic and aperiodic states
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 周期性和非周期性状态
- en: 'We call a state, ![](img/Formula_04_028.png), **periodic** if all of the paths
    leaving ![](img/Formula_04_029.png) come back after some multiple of ![](img/Formula_04_030.png)
    steps. Consider the example in *Figure 4.5*, where all the states have a period
    of ![](img/Formula_04_031.png):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称一个状态 ![](img/Formula_04_028.png) 为**周期性**，如果所有从 ![](img/Formula_04_029.png)
    离开的路径在经过某个多倍数的 ![](img/Formula_04_030.png) 步后会返回。考虑 *图 4.5* 中的例子，其中所有状态的周期为 ![](img/Formula_04_031.png)：
- en: '![Figure 4.5 – A Markov chain with periodic states, k=4'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.5 – 具有周期状态的马尔可夫链，k=4'
- en: '](img/B14160_04_05.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_04_05.jpg)'
- en: Figure 4.5 – A Markov chain with periodic states, k=4
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – 具有周期状态的马尔可夫链，k=4
- en: A recurrent state is called **aperiodic** if ![](img/Formula_04_032.png).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ![](img/Formula_04_032.png)，则称重现状态为**非周期性**。
- en: Ergodicity
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 遍历性
- en: 'We can finally define an important class of Markov chains. A Markov chain is
    called **ergodic** if all states exhibit the following properties:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终可以定义一个重要类别的马尔可夫链。如果所有状态都表现出以下特性，则称马尔可夫链为**遍历**：
- en: Communicate with each other (irreducible)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 彼此通信（不可约）
- en: Are recurrent
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是重现的
- en: Are aperiodic
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是非周期性的
- en: For ergodic Markov chains, we can calculate a single probability distribution
    that tells which state the system would be in, after a very long time from its
    initialization, with what probability. This is called the **steady state probability
    distribution**.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于遍历马尔可夫链，我们可以计算出一个单一的概率分布，告诉我们系统在经过很长时间从初始化开始后，会以什么概率处于某个状态。这被称为**稳态概率分布**。
- en: So far, so good, but what we have covered has also been a bit dense with all
    the sets of definitions. Before we go into practical examples, though, let's also
    define the math of how a Markov chain transitions between states.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利，但我们所涉及的内容也稍微有些密集，充满了定义的集合。在我们进入实际例子之前，让我们先定义一下马尔可夫链在状态之间转换的数学原理。
- en: Transitionary and steady state behavior
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转移和稳态行为
- en: We can mathematically calculate how a Markov chain behaves over time. To this
    end, we first need to know the **initial probability distribution** of the system.
    When we initialize a grid world, for example, in which state does the robot appear
    at the beginning? This is given by the initial probability distribution. Then,
    we define the **transition probability matrix**, whose entries give, well, the
    transition probabilities between all state pairs from one time step to the next.
    More formally, the entry at the ![](img/Formula_04_033.png) row and ![](img/Formula_04_034.png)
    column of this matrix gives ![](img/Formula_04_035.png), where ![](img/Formula_04_036.png)
    and ![](img/Formula_04_037.png) are the state indices (starting with 1 in our
    convention).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过数学方法计算马尔可夫链随时间的行为。为此，我们首先需要知道系统的**初始概率分布**。例如，当我们初始化一个网格世界时，机器人一开始会处于哪个状态？这就是由初始概率分布给出的。然后，我们定义**转移概率矩阵**，它的条目给出了从一个时间步到下一个时间步之间所有状态对的转移概率。更正式地，矩阵中位于
    ![](img/Formula_04_033.png) 行和 ![](img/Formula_04_034.png) 列的条目给出 ![](img/Formula_04_035.png)，其中
    ![](img/Formula_04_036.png) 和 ![](img/Formula_04_037.png) 是状态索引（按照我们的惯例，从1开始）。
- en: 'Now, to calculate the probability of the system being in state ![](img/Formula_04_038.png)
    after ![](img/Formula_04_039.png) steps, we use the following formula:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了计算系统在经过 ![](img/Formula_04_039.png) 步后处于状态 ![](img/Formula_04_038.png)
    的概率，我们使用以下公式：
- en: '![](img/Formula_04_040.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_040.jpg)'
- en: Here, ![](img/Formula_04_041.png) is the initial probability distribution and
    ![](img/Formula_04_042.png) is the transition probability matrix raised to the
    power ![](img/Formula_04_043.png). Note that ![](img/Formula_04_044.png) gives
    the probability of being in state ![](img/Formula_04_045.png) after ![](img/Formula_04_046.png)
    steps when started in state ![](img/Formula_04_047.png).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_04_041.png) 是初始概率分布，![](img/Formula_04_042.png) 是转移概率矩阵的幂
    ![](img/Formula_04_043.png)。请注意，![](img/Formula_04_044.png) 给出了在从状态 ![](img/Formula_04_047.png)
    开始后，经过 ![](img/Formula_04_046.png) 步后处于状态 ![](img/Formula_04_045.png) 的概率。
- en: Info
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: A Markov chain is completely characterized by the ![](img/Formula_04_048.png)
    tuple, where ![](img/Formula_04_049.png) is the set of all states and ![](img/Formula_04_050.png)
    is the transition probability matrix.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一个马尔可夫链完全由 ![](img/Formula_04_048.png) 元组来表征，其中 ![](img/Formula_04_049.png)
    是所有状态的集合，![](img/Formula_04_050.png) 是转移概率矩阵。
- en: Yes, we have covered a lot of definitions and theory so far. Now, it is a good
    time to finally look at a practical example.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，到目前为止我们已经介绍了很多定义和理论。现在，正是时候看一下实际的例子了。
- en: Example – n-step behavior in the grid world
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 – 网格世界中的n步行为
- en: In many RL algorithms, the core idea is to arrive at a consistency between our
    understanding of the environment in its current state and after ![](img/Formula_04_051.png)
    steps of transitions and to iterate until this consistency is ensured. Therefore,
    it is important to get a solid intuition of how an environment modeled as a Markov
    chain evolves over time. To this end, we will look into ![](img/Formula_04_052.png)-step
    behavior in the grid world example. Follow along!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多强化学习算法中，核心思想是使我们对环境当前状态的理解与经过![](img/Formula_04_051.png)步转移后的理解保持一致，并不断迭代，直到这种一致性得到确保。因此，了解作为马尔可夫链建模的环境随时间演化的直觉非常重要。为此，我们将研究![](img/Formula_04_052.png)步的网格世界行为。跟着一起走吧！
- en: 'Let''s start by creating a ![](img/Formula_04_053.png) grid world with our
    robot in it, similar to the one in *Figure 4.1*. For now, let''s always initialize
    the world with the robot being at the center. Moreover, we index the states/cells
    so that ![](img/Formula_04_054.png) So, the initial probability distribution,
    ![](img/Formula_04_055.png), is given by the following code:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从创建一个![](img/Formula_04_053.png)网格世界开始，机器人在其中，类似于*图 4.1*中的情况。现在，我们总是将机器人初始化在中心。此外，我们将状态/单元格索引为![](img/Formula_04_054.png)所以，初始概率分布![](img/Formula_04_055.png)由以下代码给出：
- en: '[PRE0]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, `q` is the initial probability distribution.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，`q` 是初始概率分布。
- en: 'We define a function that gives the ![](img/Formula_04_056.png) transition
    probability matrix:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义一个函数，给出![](img/Formula_04_056.png)的转移概率矩阵：
- en: '[PRE1]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The code may seem a bit long but what it does is pretty simple: it just fills
    an ![](img/Formula_04_057.png) transition probability matrix according to specified
    probabilities of going up, down, left, and right.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码看起来有点长，但它的功能非常简单：它根据指定的上、下、左、右移动概率，填充一个![](img/Formula_04_057.png) 转移概率矩阵。
- en: 'Get the transition probability matrix for the ![](img/Formula_04_058.png) grid
    world of ours:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取我们这个![](img/Formula_04_058.png)网格世界的转移概率矩阵：
- en: '[PRE2]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Calculate the ![](img/Formula_04_059.png)-step transition probabilities. For
    example, for ![](img/Formula_04_060.png), we have the following:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算![](img/Formula_04_059.png)步的转移概率。例如，对于![](img/Formula_04_060.png)，我们有以下内容：
- en: '[PRE3]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The result will look like the following:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE4]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Nothing surprising, right? The output just tells us that the robot starting
    at the center will be a cell above with a probability of ![](img/Formula_04_061.png),
    a cell down with a probability of ![](img/Formula_04_062.png), and so on. Let''s
    do this for 3, 10, and 100 steps. The results are shown in *Figure 4.6*:'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 没什么令人惊讶的，对吧？输出只是告诉我们，从中心出发的机器人，以![](img/Formula_04_061.png)的概率在上面一个单元，以![](img/Formula_04_062.png)的概率在下面一个单元，依此类推。让我们对3步、10步和100步进行测试。结果如*图
    4.6*所示：
- en: '![Figure 4.6 – n-step transition probabilities'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.6 – n步转移概率'
- en: '](img/B14160_04_06.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_04_06.jpg)'
- en: Figure 4.6 – n-step transition probabilities
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – n步转移概率
- en: You might notice that the probability distribution after 10 steps and 100 steps
    are very similar. This is because the system has almost reached a steady state
    after a few steps. So, the chance that we will find the robot in a specific state
    is almost the same after 10, 100, or 1,000 steps. Also, you should have noticed
    that we are more likely to find the robot at the bottom cells, simply because
    we have ![](img/Formula_04_063.png).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，10步和100步后的概率分布非常相似。这是因为系统在几步之后几乎达到了稳态。因此，我们在特定状态下找到机器人的机会在10步、100步或1000步后几乎是相同的。此外，你应该注意到，我们更有可能在底部单元格找到机器人，这仅仅是因为我们有![](img/Formula_04_063.png)。
- en: Before we wrap up our discussion about the transitionary and steady state behaviors,
    let's go back to ergodicity and look into a special property of ergodic Markov
    chains.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束关于转移状态和稳态行为的讨论之前，让我们回到遍历性，探讨遍历马尔可夫链的一个特殊性质。
- en: Example – a sample path in an ergodic Markov chain
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 – 在遍历马尔可夫链中的一个样本路径
- en: If the Markov chain is ergodic, we can simply simulate it for a long time once
    and estimate the steady state distribution of the states through the frequency
    of visits. This is especially useful if we don't have access to the transition
    probabilities of the system, but we can simulate it.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果马尔可夫链是遍历的，我们可以通过长时间模拟它，并通过访问频率估计状态的稳态分布。这对于我们无法访问系统转移概率的情况特别有用，但我们可以进行模拟。
- en: 'Let''s see this in an example:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来看看：
- en: 'First, let''s Import the SciPy library to count the number of visits. Set the
    number of steps to one million in the sample path, initialize a vector to keep
    track of the visits, and initialize the first state to `4`, which is ![](img/Formula_04_064.png):'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入 SciPy 库来统计访问次数。将样本路径的步数设为一百万，初始化一个向量来跟踪访问次数，并将第一个状态初始化为`4`，即 ![](img/Formula_04_064.png)：
- en: '[PRE5]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Simulate the environment for one million steps:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模拟环境一百万步：
- en: '[PRE6]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Count the number of visits to each state:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 统计每个状态的访问次数：
- en: '[PRE7]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You will see numbers similar to the following:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将看到类似于以下的数字：
- en: '[PRE8]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The results are indeed very much in line with the steady state probability distribution
    we calculated.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果确实与我们计算的稳态概率分布非常一致。
- en: Great job so far, as we've covered Markov chains in a fair amount of detail,
    worked on some examples, and gained a solid intuition! Before we close this section,
    let's briefly look into a more realistic type of Markov process.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你做得很棒，我们已经详细讨论了马尔可夫链，做了一些例子，并且获得了扎实的直觉！在我们结束这一部分之前，让我们简要地了解一种更现实的马尔可夫过程类型。
- en: Semi-Markov processes and continuous-time Markov chains
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 半马尔可夫过程和连续时间马尔可夫链
- en: All of the examples and formulas we have provided so far are related to discrete-time
    Markov chains, which are environments where transitions occur at discrete time
    steps, such as every minute or every 10 seconds. But in many real-world scenarios,
    when the next transition will happen is also random, which makes them a **semi-Markov
    process**. In those cases, we are usually interested in predicting the state after
    ![](img/Formula_04_065.png) amount of time (rather than after ![](img/Formula_04_066.png)
    steps).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们提供的所有示例和公式都与离散时间马尔可夫链相关，这是过渡在离散时间步长（如每分钟或每10秒）发生的环境。但在许多现实世界的场景中，下一个过渡发生的时间也是随机的，这使得它们成为**半马尔可夫过程**。在这种情况下，我们通常感兴趣的是预测经过
    ![](img/Formula_04_065.png) 时间后系统的状态（而不是经过 ![](img/Formula_04_066.png) 步骤后的状态）。
- en: One example of a scenario where a time component is important is queuing systems
    – for instance, the number of customers waiting in a customer service line. A
    customer could join the queue anytime and a representative could complete the
    service with a customer at any time – not just at discrete time steps. Another
    example is a work-in-process inventory waiting in front of an assembly station
    to be processed in a factory. In all these cases, analyzing the behavior of the
    system over time is very important to be able to improve the system and take action
    accordingly.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 时间成分重要的一个示例场景是排队系统——例如，顾客在客服队列中的等待人数。顾客可以随时加入队列，而客服人员也可以在任何时间为顾客提供服务——而不仅仅是在离散时间步长处。另一个例子是工厂中等待在组装站前处理的在制品库存。在这些情况下，随着时间推移分析系统的行为是非常重要的，这样才能改善系统并采取相应的行动。
- en: In semi-Markov processes, we would need to know the current state of the system,
    and also how long the system has been in it. This means the system depends on
    the past from the time perspective, but not from the perspective of the type of
    transition it will make – hence the name semi-Markov.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在半马尔可夫过程中，我们需要知道系统的当前状态，以及系统在该状态下的持续时间。这意味着从时间的角度来看，系统依赖于过去，但从过渡类型的角度来看并不依赖过去——因此称为半马尔可夫过程。
- en: 'Let''s look into several possible versions of how this can be of interest to
    us:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下几个可能的版本，看看这对我们来说有何意义：
- en: If we are only interested in the transitions themselves, not when they happen,
    we can simply ignore everything related to time and work with the **embedded Markov
    chain of the semi-Markov process**, which is essentially the same as working with
    a discrete-time Markov chain.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们只对过渡本身感兴趣，而不关心它们发生的时刻，我们可以简单地忽略所有与时间相关的内容，并使用**半马尔可夫过程的嵌入式马尔可夫链**，这实际上与使用离散时间马尔可夫链是一样的。
- en: In some processes, although the time between transitions is random, it is memoryless,
    which means exponentially distributed. Then, we have the Markov property fully
    satisfied, and the system is a **continuous-time Markov chain**. Queuing systems,
    for example, are often modeled in this category.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些过程中，虽然过渡之间的时间是随机的，但它是无记忆的，这意味着是指数分布的。那么，我们就完全满足马尔可夫性质，系统就是**连续时间马尔可夫链**。排队系统就是常常属于这一类的模型。
- en: If it is both that we are interested in, and the time component and the transition
    times are not memoryless, then we have a general semi-Markov process.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们既对时间成分感兴趣，又过渡时间不是无记忆的，那么我们就有一个一般的半马尔可夫过程。
- en: When it comes to working with these types of environments and solving them using
    RL, although not ideal, it is common to treat everything as discrete and use the
    same RL algorithms developed for discrete-time systems with some workarounds.
    For now, it is good for you to know and acknowledge the differences, but we will
    not go deeper into semi-Markov processes. Instead, you will see what these workarounds
    are when we solve continuous-time examples in later chapters.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用强化学习（RL）解决这些类型的环境时，尽管不是最理想的，通常将所有内容视为离散的，并使用为离散时间系统开发的相同RL算法，并通过一些变通方法解决。现在，你需要了解并承认这些差异，但我们不会深入讨论半马尔可夫过程。相反，当我们在后面的章节中解决连续时间示例时，你会看到这些变通方法是如何工作的。
- en: We have made great progress toward building our understanding of MDPs with Markov
    chains. The next step in this journey is to introduce a "reward" to the environment.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在通过马尔可夫链构建MDP（马尔可夫决策过程）理解方面取得了很大进展。接下来，我们的目标是为环境引入“奖励”。
- en: Introducing the reward – Markov reward process
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入奖励 – 马尔可夫奖励过程
- en: In our robot example so far, we have not really identified any situation/state
    that is "good" or "bad." In any system, though, there are desired states to be
    in and there are other states that are less desirable. In this section, we will
    attach rewards to states/transitions, which gives us a **Markov Reward Process**
    (**MRP**). We then assess the "value" of each state.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们目前的机器人示例中，我们并未真正识别出任何“好”或“坏”的情境/状态。然而，在任何系统中，都会有理想的状态和其他不太理想的状态。在本节中，我们将奖励附加到状态/转移上，这给我们带来了**马尔可夫奖励过程**（**MRP**）。然后我们评估每个状态的“价值”。
- en: Attaching rewards to the grid world example
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将奖励附加到网格世界示例
- en: 'Remember the version of the robot example where it could not bounce back to
    the cell it was in when it hits a wall but crashed in a way that it was not recoverable?
    From now on, we will work on that version, and attach rewards to the process.
    Now, let''s build this example:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 记得机器人示例的版本吗？当它撞到墙壁时，它无法反弹回原来的格子，而是以无法恢复的方式碰撞并崩溃？从现在开始，我们将使用这个版本，并为过程附加奖励。现在，让我们构建这个示例：
- en: 'Modify the transition probability matrix to assign self-transition probabilities
    to the "crashed" state that we add to the matrix:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改转移概率矩阵，将自转移概率分配给我们添加到矩阵中的“碰撞”状态：
- en: '[PRE9]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Assign rewards to transitions:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为转移分配奖励：
- en: '[PRE10]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: For every transition where the robot stays alive, it collects +1 reward. It
    collects 0 reward when it crashes. Since "crashed" is a terminal/absorbing state,
    we terminate the episode there. Simulate this model for different initializations,
    100K times for each initialization, and see how much reward is collected on average
    in each case.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每当机器人保持活着时，它就会获得+1的奖励。当它碰撞时，获得0奖励。由于“碰撞”是一个终止/吸收状态，我们将在此处终止该回合。模拟该模型，使用不同的初始化，每个初始化进行100K次模拟，并观察每种情况下的平均奖励。
- en: 'The results will look as in *Figure 4.7* (yours will be a bit different due
    to the randomness):'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如*图4.7*所示（由于随机性，你的结果会稍有不同）：
- en: '![Figure 4.7 – Average returns with respect to the initial state'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.7 – 相对于初始状态的平均回报'
- en: '](img/B14160_04_07.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_04_07.jpg)'
- en: Figure 4.7 – Average returns with respect to the initial state
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 – 相对于初始状态的平均回报
- en: In this example, if the initial state is ![](img/Formula_04_067.png), the average
    return is the highest. This makes it a "valuable" state to be in. Contrast this
    with state ![](img/Formula_04_068.png) with an average return of ![](img/Formula_04_069.png).
    Not surprisingly, it is not a great state to be in. This is because it is more
    likely for the robot to hit the wall earlier when it starts in the corner. Another
    thing that is not surprising is that the returns are vertically symmetrical (almost),
    since ![](img/Formula_04_070.png).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，如果初始状态是![](img/Formula_04_067.png)，那么平均回报是最高的。这使得它成为一个“有价值”的状态。与此对比的是状态![](img/Formula_04_068.png)，其平均回报为![](img/Formula_04_069.png)。不出所料，这不是一个理想的状态。这是因为当机器人从角落开始时，它更可能早早撞到墙壁。另一个不奇怪的现象是，回报在垂直方向上几乎对称，因为![](img/Formula_04_070.png)。
- en: Now that we have calculated the average reward with respect to each initialization,
    let's go deeper and see how they are related to each other.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算了每种初始化情况下的平均奖励，让我们深入探讨一下它们之间的关系。
- en: Relationships between average rewards with different initializations
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同初始化情况下平均奖励的关系
- en: 'The average returns that we have observed have quite a structural relationship
    between them. Think about it: assume the robot started at ![](img/Formula_04_071.png)
    and made a transition to ![](img/Formula_04_072.png). Since it is still alive,
    we collected a reward of +1\. If we knew the "value" of state ![](img/Formula_04_073.png),
    would we need to continue the simulation to figure out what return to expect?
    Not really! The value already gives us the expected return from that point on.
    Remember that this is a Markov process and what happens next does not depend on
    the past!'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到的平均回报之间有着相当的结构性关系。想一想：假设机器人从![](img/Formula_04_071.png)开始，并过渡到![](img/Formula_04_072.png)。由于它仍然存活，我们获得了+1的奖励。如果我们知道状态![](img/Formula_04_073.png)的“值”，我们还需要继续模拟以确定预期的回报吗？其实不需要！这个值已经告诉我们从那时起的预期回报。记住，这是一个马尔科夫过程，接下来的事情不依赖于过去的状态！
- en: 'We can extend this relationship to derive the value of a state from the other
    state values. But remember that the robot could have transitioned into some other
    state. Taking into account other possibilities and denoting the value of a state
    by ![](img/Formula_04_074.png), we obtain the following relationship:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以扩展这一关系，从其他状态值推导出某一状态的值。但请记住，机器人可能已经过渡到其他状态。考虑到其他可能性，并将状态的值表示为![](img/Formula_04_074.png)，我们得到以下关系：
- en: '![](img/Formula_04_075.jpg)![](img/Formula_04_076.jpg)![](img/Formula_04_077.jpg)![](img/Formula_04_078.jpg)![](img/Formula_04_079.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_075.jpg)![](img/Formula_04_076.jpg)![](img/Formula_04_077.jpg)![](img/Formula_04_078.jpg)![](img/Formula_04_079.jpg)'
- en: As you can see, some small inaccuracies in the estimations of the state values
    aside, the state values are consistent with each other.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，除了状态值估计中的一些小误差外，状态值彼此之间是一致的。
- en: Tip
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: This recursive relationship between state values is central to many RL algorithms
    and it will come up again and again. We will formalize this idea using the Bellman
    equation in the next section.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 状态值之间的这种递归关系是许多强化学习算法的核心，它将一再出现。我们将在下一节使用贝尔曼方程来正式化这个想法。
- en: Let's formalize all these concepts in the next section.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下一节正式化所有这些概念。
- en: Return, discount, and state values
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 返回、折扣和状态值
- en: 'We define the **return** in a Markov process after time step ![](img/Formula_04_080.png)
    as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将马尔科夫过程在时间步骤![](img/Formula_04_080.png)之后的**回报**定义如下：
- en: '![](img/Formula_04_081.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_081.jpg)'
- en: 'Here, ![](img/Formula_04_082.png) is the reward at time ![](img/Formula_04_083.png)
    and ![](img/Formula_04_084.png) is the terminal time step. This definition, however,
    could be potentially problematic. In an MRP that has no terminal state, the return
    could go up to infinity. To avoid this, we introduce a **discount rate**, ![](img/Formula_04_085.png),
    in this calculation and define a **discounted return**, as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_04_082.png)是时间![](img/Formula_04_083.png)的奖励，![](img/Formula_04_084.png)是终止时间步骤。然而，这个定义可能会有潜在的问题。在一个没有终止状态的马尔科夫过程（MRP）中，回报可能会无限大。为了解决这个问题，我们在此计算中引入了**折扣率**，![](img/Formula_04_085.png)，并定义了**折扣回报**，如下所示：
- en: '![](img/Formula_04_086.jpg)![](img/Formula_04_087.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_086.jpg)![](img/Formula_04_087.jpg)'
- en: 'For ![](img/Formula_04_088.png), this sum is guaranteed to be finite as far
    as the reward sequence is bounded. Here is how varying ![](img/Formula_04_089.png)
    affects the sum:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于![](img/Formula_04_088.png)，只要奖励序列是有限的，这个和就一定是有限的。下面是折扣率![](img/Formula_04_089.png)变化时如何影响和的示意：
- en: '![](img/Formula_04_090.png) values closer to 1 place almost equal emphasis
    on distant rewards as immediate rewards.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于接近1的![](img/Formula_04_090.png)值，远期奖励和即时奖励几乎被赋予相等的重要性。
- en: When ![](img/Formula_04_091.png), all the rewards, distant or immediate, are
    weighted equally.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当![](img/Formula_04_091.png)时，所有的奖励，无论是远期的还是即时的，都会被赋予相同的权重。
- en: For ![](img/Formula_04_092.png) values closer to 0, the sum is more myopic.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于接近0的![](img/Formula_04_092.png)值，和的结果更为短视。
- en: At ![](img/Formula_04_093.png), the return is equal to the immediate reward.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当![](img/Formula_04_093.png)时，回报等于即时奖励。
- en: 'Throughout the rest of the book, our goal will be to maximize the expected
    discounted return. So, it is important to understand the other benefits of using
    a discount in the return calculation:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们的目标是最大化预期的折扣回报。因此，理解使用折扣计算回报的其他好处是很重要的：
- en: The discount diminishes the weight placed on the rewards that will be obtained
    in the distant future. This is reasonable as our estimations about the distant
    future may not be very accurate when we bootstrap values estimations using other
    estimations (more on this later).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 折扣减少了我们对遥远未来所获得奖励的重视。这是合理的，因为我们对于遥远未来的估计可能并不准确，尤其是当我们利用其他估计来推算价值时（稍后会详细讨论）。
- en: Human (and animal) behavior prefers immediate rewards over future rewards.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类（以及动物）行为倾向于选择立即奖励而非未来奖励。
- en: For financial rewards, immediate rewards are more valuable due to the time value
    of money.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于金融奖励，立即奖励更具价值，因为金钱的时间价值。
- en: 'Now that we have defined the discounted return, the **value** of a state, ![](img/Formula_04_095.png),
    is defined as the expected discounted return when starting in ![](img/Formula_04_0951.png):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了折扣回报，**状态**的价值，![](img/Formula_04_095.png)，被定义为在![](img/Formula_04_0951.png)状态下开始时的期望折扣回报：
- en: '![](img/Formula_04_096.jpg)![](img/Formula_04_097.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_096.jpg)![](img/Formula_04_097.jpg)'
- en: 'Note that this definition allows us to use the recursive relationships that
    we figured out in the previous section:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这一定义允许我们使用我们在上一节中推导出的递归关系：
- en: '![](img/Formula_04_098.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_098.jpg)'
- en: This equation is called the **Bellman equation for MRP**. It is what we utilized
    in the preceding grid world example when we calculated the value of a state from
    the other state values. The Bellman equation is at the heart of many RL algorithms
    and is of crucial importance. We will give its full version after we introduce
    the MDP.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程被称为**Bellman方程（MRP）**。它就是我们在前面的网格世界示例中用来根据其他状态值计算一个状态的价值时所使用的方程。Bellman方程是许多强化学习算法的核心，具有至关重要的意义。我们将在介绍MDP之后给出它的完整版本。
- en: Let's close this section with a more formal definition of an MRP, which is detailed
    in the following info box.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以MRP的一个更正式的定义结束这一部分，详细内容请见下方的信息框。
- en: Info
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: An MRP is fully characterized by a ![](img/Formula_04_099.png) tuple, where
    ![](img/Formula_04_100.png) is a set of states, ![](img/Formula_04_101.png) is
    a transition probability matrix, ![](img/Formula_04_102.png) is a reward function,
    and ![](img/Formula_04_103.png) is a discount factor.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: MRP完全由一个![](img/Formula_04_099.png)元组来表征，其中![](img/Formula_04_100.png)是一个状态集合，![](img/Formula_04_101.png)是一个转移概率矩阵，![](img/Formula_04_102.png)是奖励函数，而![](img/Formula_04_103.png)是折扣因子。
- en: Next, we will look at how to calculate the state values analytically.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看如何分析地计算状态值。
- en: Analytically calculating the state values
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析地计算状态值
- en: The Bellman equation gives us the relationships between the state values, rewards,
    and transition probabilities. When the transition probabilities and the reward
    dynamics are known, we can use the Bellman equation to precisely calculate the
    state values. Of course, this is only feasible when the total number of states
    is small enough to make the calculations. Let's now see how we can do this.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Bellman方程为我们提供了状态值、奖励和转移概率之间的关系。当转移概率和奖励动态已知时，我们可以使用Bellman方程精确计算状态值。当然，只有在状态的总数足够小以便进行计算时，这才是可行的。现在让我们看看如何做到这一点。
- en: 'When we write the Bellman equation in matrix form, it looks as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将Bellman方程写成矩阵形式时，它如下所示：
- en: '![](img/Formula_04_104.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_104.jpg)'
- en: 'Here, ![](img/Formula_04_105.png) is a column vector where each entry is the
    value of the corresponding state, and ![](img/Formula_04_106.png) is another column
    vector where each entry corresponds to the reward obtained when transitioned into
    that state. Accordingly, we get the following expanded representation of the previous
    formula:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_04_105.png)是一个列向量，每个条目是相应状态的值，而![](img/Formula_04_106.png)是另一个列向量，每个条目对应转移到该状态时获得的奖励。因此，我们得到了前面公式的扩展表示：
- en: '![](img/Formula_04_107.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_107.jpg)'
- en: 'We can solve this system of linear equations as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过如下方式求解这个线性方程组：
- en: '![](img/Image88879.jpg)![](img/Formula_04_109.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Image88879.jpg)![](img/Formula_04_109.jpg)'
- en: 'Now it is time to implement this for our grid world example. Note that, in
    the ![](img/Formula_04_110.png) example, we have 10 states, where the 10th state
    represents the robot''s crash. Transitioning into any state results in +1 reward,
    except in the "crashed" state. Let''s get started:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候在我们的网格世界示例中实现这一点了。请注意，在![](img/Formula_04_110.png)示例中，我们有10个状态，其中第10个状态表示机器人的撞车。转移到任何状态都会得到+1奖励，除了“撞车”状态。让我们开始吧：
- en: 'Construct the ![](img/Formula_04_111.png) vector:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建![](img/Formula_04_111.png)向量：
- en: '[PRE11]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Set ![](img/Formula_04_112.png) (something very close to 1 that we actually
    have in the example) and calculate the state values:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设定 ![](img/Formula_04_112.png)（实际例子中接近 1 的值），并计算状态值：
- en: '[PRE12]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output will look like this:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将类似于这样：
- en: '[PRE13]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Remember that these are the true (theoretical, rather than estimated) state
    values (for the given discount rate), and they are aligned with what we estimated
    through simulation before in *Figure 4.7*!
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这些是真正的（理论上的，而非估算的）状态值（对于给定的折扣率），它们与我们之前通过模拟在*图 4.7*中估算的结果是一致的！
- en: If you are wondering why we did not simply set ![](img/Formula_04_113.png) to
    1, remember that we have now introduced a discount factor, which is necessary
    for things to converge mathematically. If you think about it, there is a chance
    that the robot will randomly move but stay alive infinitely long, collecting an
    infinite reward. Yes, this is extremely unlikely, and you will never see this
    in practice. So, you may think that we can set ![](img/Formula_04_114.png) here.
    However, this would lead to a singular matrix that we cannot take the inverse
    of. So, instead, we will choose ![](img/Formula_04_115.png). For practical purposes,
    this discount factor almost equally weighs the immediate and future rewards.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在想，为什么我们不直接将 ![](img/Formula_04_113.png) 设为 1，请记住，我们现在引入了折扣因子，这对于数学收敛是必要的。如果你仔细想想，机器人有可能随机移动，但能无限长时间存活，获得无限奖励。是的，这种情况极其不可能，而且你在实际中也永远不会遇到。所以，你可能会认为我们可以在这里设定
    ![](img/Formula_04_114.png)。然而，这会导致一个奇异矩阵，我们无法求其逆。所以，我们选择 ![](img/Formula_04_115.png)。在实际应用中，这个折扣因子几乎等权重地考虑了即时奖励和未来奖励。
- en: We can estimate the state values in other ways than simulation or matrix inversion.
    Let's look at an iterative approach next.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过其他方法来估算状态值，而不仅仅是通过模拟或矩阵求逆。接下来我们来看一种迭代方法。
- en: Estimating the state values iteratively
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迭代估算状态值
- en: One of the central ideas in RL is to use the value function definition to estimate
    the value functions iteratively. To achieve that, we arbitrarily initialize the
    state values and use its definition as an update rule. Since we estimate states
    based on other estimations, this is a **bootstrapping** method. We stop when the
    maximum update to the state value over all the states is below a set threshold.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的一个核心思想是使用价值函数定义来迭代估算价值函数。为了实现这一点，我们随便初始化状态值，并使用其定义作为更新规则。由于我们是基于其他估算值来估算状态，这是一种**自举**方法。我们会在所有状态的最大更新低于设定阈值时停止。
- en: 'Here is the code to estimate the state values in our robot example:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这是估算我们机器人例子中的状态值的代码：
- en: '[PRE14]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The result will closely resemble the estimations in *Figure 4.7*. Just run
    the following code:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将与*图 4.7*中的估计值非常相似。只需运行以下代码：
- en: '[PRE15]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You should get something similar to the following:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到类似以下的结果：
- en: '[PRE16]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This looks great! Again, remember the following:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错！再次提醒，记住以下几点：
- en: We had to iterate over all possible states. This is intractable when the state
    space is large.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须遍历所有可能的状态。当状态空间很大时，这是不可行的。
- en: We used the transition probabilities explicitly. In a realistic system, we don't
    know what these probabilities are.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们显式地使用了转移概率。在一个现实的系统中，我们并不知道这些概率。
- en: Modern RL algorithms tackle these drawbacks by using a function approximation
    to represent the states and sample the transitions from (a simulation of) the
    environment. We will visit those approaches in later chapters.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现代强化学习算法通过使用函数逼近来表示状态，并从（环境的模拟）中采样转移，从而解决了这些缺点。我们将在后续章节中探讨这些方法。
- en: 'So far, so good! Now, we will incorporate the last major piece into this picture:
    actions.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利！现在，我们将把最后一个主要部分加入到这个图景中：动作。
- en: Bringing the action in – MDP
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入动作 – MDP
- en: The MRP allowed us to model and study a Markov chain with rewards. Of course,
    our ultimate goal is to control such a system to achieve the maximum reward. Now,
    we will incorporate decisions into the MRP.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: MRP 使我们能够建模和研究带有奖励的马尔可夫链。当然，我们的最终目标是控制这样的系统，以实现最大奖励。现在，我们将把决策融入到 MRP 中。
- en: Definition
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义
- en: An MDP is simply an MRP with decisions affecting transition probabilities and
    potentially the rewards.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: MDP 只是一个 MRP，其中决策影响转移概率，并可能影响奖励。
- en: Info
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: An MDP is characterized by a ![](img/Formula_04_116.png) tuple, where we have
    a finite set of actions, ![](img/Formula_04_117.png), on top of the MRP.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: MDP 的特征是一个 ![](img/Formula_04_116.png) 元组，其中我们在 MRP 的基础上添加了一个有限的动作集合，![](img/Formula_04_117.png)。
- en: 'MDP is the mathematical framework behind RL. So, now is the time to recall
    the RL diagram that we introduced in [*Chapter 1*](B14160_01_Final_SK_ePub.xhtml#_idTextAnchor016),
    *Introduction to Reinforcement Learning*:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: MDP 是强化学习背后的数学框架。所以，现在是时候回顾我们在[*第1章*](B14160_01_Final_SK_ePub.xhtml#_idTextAnchor016)中介绍的强化学习图示
    *Introduction to Reinforcement Learning*：
- en: '![Figure 4.8 – MDP diagram'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.8 – MDP 图示'
- en: '](img/B14160_04_08.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_04_08.jpg)'
- en: Figure 4.8 – MDP diagram
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – MDP 图示
- en: 'Our goal in an MDP is to find a **policy** that maximizes the expected cumulative
    reward. A policy simply tells which action(s) to take for a given state. In other
    words, it is a mapping from states to actions. More formally, a policy is a distribution
    over actions given states, and is denoted by ![](img/Formula_04_118.png):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在MDP中的目标是找到一个**策略**，使得期望的累积奖励最大化。策略简单地告诉给定状态下应该采取哪些行动。换句话说，它是一个从状态到行动的映射。更正式地说，策略是给定状态下的行动分布，用
    ![](img/Formula_04_118.png) 表示：
- en: '![](img/Formula_04_119.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_119.jpg)'
- en: 'The policy of an agent potentially affects the transition probabilities, as
    well as the rewards, and it fully defines the agent''s behavior. It is also stationary
    and does not change over time. Therefore, the dynamics of the MDP are defined
    by the following transition probabilities:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的策略可能会影响转移概率和奖励，它完全定义了代理的行为。策略也是静态的，不会随时间改变。因此，MDP的动态由以下转移概率定义：
- en: '![](img/Formula_04_120.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_120.jpg)'
- en: These are for all states and actions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这些适用于所有状态和行动。
- en: Next, let's see how an MDP might look in the grid world example.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看MDP在网格世界示例中的表现。
- en: Grid world as an MDP
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格世界作为MDP
- en: 'Imagine that we can control the robot in our grid world, but only to some extent.
    In each step, we can take one of the following actions: up, down, left, and right.
    Then, the robot goes in the direction of the action with 70% chance and one of
    the other directions with 10% chance each. Given these dynamics, a sample policy
    could be as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们可以控制网格世界中的机器人，但仅限于某种程度。在每一步中，我们可以采取以下行动之一：上、下、左和右。然后，机器人以70%的概率朝着所选方向移动，并以10%的概率朝着其他方向移动。基于这些动态，可能的一个策略如下：
- en: Right, when in state ![](img/Formula_04_121.png)
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对了，当处于状态 ![](img/Formula_04_121.png) 时
- en: Up, when in state ­![](img/Formula_04_122.png)
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上，当处于状态 ­![](img/Formula_04_122.png) 时
- en: 'The policy also determines the transition probability matrix and the reward
    distribution. For example, we can write the transition probabilities for state
    ![](img/Formula_04_123.png) and given our policy, as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 策略还决定了转移概率矩阵和奖励分布。例如，我们可以将状态 ![](img/Formula_04_123.png) 下给定策略的转移概率写成如下：
- en: '![](img/Formula_04_124.png)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/Formula_04_124.png)'
- en: '![](img/Formula_04_125.png)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/Formula_04_125.png)'
- en: '![](img/Formula_04_126.png)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/Formula_04_126.png)'
- en: Tip
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示
- en: Once a policy is defined in an MDP, the state and reward sequence become an
    MRP.
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦在MDP中定义了策略，状态和奖励序列就变成了MRP。
- en: 'So far, so good. Now, let''s be a bit more rigorous about how we express the
    policy. Remember that a policy is actually a probability distribution over actions
    given the state. Therefore, saying that "the policy is to take the "right" action
    in state ![](img/Formula_04_127.png)" actually means "we take the "right" action
    with probability 1 when in state ![](img/Formula_04_128.png)." This can be expressed
    more formally as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利。现在，让我们更加严谨地表达策略。记住，策略实际上是给定状态下的一种行动的概率分布。因此，说“策略是在状态 ![](img/Formula_04_127.png)
    下采取‘右’行动”，实际上意味着“在状态 ![](img/Formula_04_128.png) 下，我们以概率1采取‘右’行动。”这可以更正式地表示如下：
- en: '![](img/Formula_04_129.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_129.jpg)'
- en: 'A perfectly legitimate policy is a probabilistic one. For example, we can choose
    to take the left or up action when in state ![](img/Formula_04_130.png) with equal
    probability, which is as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完全合法的策略是一个概率策略。例如，我们可以选择在状态 ![](img/Formula_04_130.png) 下以相等的概率采取左或上行动，表达式如下：
- en: '![](img/Formula_04_131.jpg)![](img/Formula_04_132.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_131.jpg)![](img/Formula_04_132.jpg)'
- en: Again, our goal in RL is to figure out an optimal policy for the environment
    and the problem at hand that maximizes the expected discounted return. Starting
    from the next chapter, we will go into the details of how to do that and solve
    detailed examples. For now, this example is enough to illustrate what an MDP looks
    like in a toy example.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们在强化学习中的目标是找出一个最优策略，使得环境和当前问题最大化期望的折扣回报。从下一章开始，我们将详细介绍如何做到这一点，并解决详细的示例。现在，这个例子足以说明在玩具示例中MDP是什么样子的。
- en: Next, we will define the value functions and related equations for MDPs as we
    did for MRPs.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将像定义MRP时那样，定义MDP的值函数和相关方程。
- en: State-value function
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 状态值函数
- en: 'We have already talked about the value of a state in the context of the MRP.
    The value of a state, which we now formally call the **state-value function**,
    is defined as the expected discounted return when starting in state ![](img/Formula_04_133.png).
    However, there is a crucial point here: *the state-value function in an MDP is
    defined for a policy*. After all, the transition probability matrix is determined
    by the policy. So, changing the policy is likely to lead to a different state-value
    function. This is formally defined as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在MRP的背景下讨论过状态的值。现在我们正式称之为**状态值函数**，它被定义为在状态 ![](img/Formula_04_133.png)
    开始时的期望折扣回报。然而，这里有一个关键点：*在MDP中，状态值函数是针对策略定义的*。毕竟，转移概率矩阵是由策略决定的。因此，改变策略很可能会导致不同的状态值函数。这个定义如下：
- en: '![](img/Formula_04_134.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_134.jpg)'
- en: Note the ![](img/Formula_04_135.png) subscript in the state-value function,
    as well as the expectation operator. Other than that, the idea is the same as
    what we defined with the MRP.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意状态值函数中的 ![](img/Formula_04_135.png) 下标，以及期望操作符。除此之外，思路与我们在MRP中定义的相同。
- en: 'Now, we can finally define the Bellman equation for ![](img/Formula_04_136.png):'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们终于可以为 ![](img/Formula_04_136.png) 定义贝尔曼方程：
- en: '![](img/Formula_04_137.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_137.jpg)'
- en: You already know that the values of the states are related to each other from
    our discussion on MRPs. The only difference here is that now the transition probabilities
    depend on the actions and the corresponding probabilities of taking them in a
    given state as per the policy. Imagine "no action" is one of the possible actions
    in our grid world. The state values in *Figure 4.7* would correspond to the policy
    of taking no action in any state.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经知道，状态的值彼此是相关的，这一点我们在讨论MRP时已经提到过。这里唯一不同的是，转移概率现在依赖于动作以及根据策略在给定状态下采取这些动作的相应概率。假设“无动作”是我们网格世界中的一种可能动作。*图4.7*中的状态值对应于在任何状态下都不采取动作的策略。
- en: Action-value function
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作值函数
- en: 'An interesting quantity we use a lot in RL is the action-value function. Now,
    assume that you have a policy, ![](img/Formula_04_138.png) (not necessarily an
    optimal one). The policy already tells you which actions to take for each state
    with the associated probabilities, and you will follow that policy. However, for
    the current time step, you ask "what would be the expected cumulative return if
    I take action ![](img/Formula_04_139.png) initially while in the current state,
    and follow ![](img/Formula_04_140.png) thereafter for all states?" The answer
    to this question is the **action-value function**. Formally, this is how we define
    it in various but equivalent ways:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，我们常用的一个有趣量是动作值函数。假设你有一个策略，![](img/Formula_04_138.png)（不一定是最优的）。该策略已经告诉你每个状态应该采取哪些动作以及相应的概率，并且你会遵循该策略。然而，对于当前时间步，你问：“如果我在当前状态下最初采取动作
    ![](img/Formula_04_139.png)，然后在所有状态下都遵循 ![](img/Formula_04_140.png)，那么预期的累计回报会是多少？”这个问题的答案就是**动作值函数**。正式地，我们可以以各种但等效的方式定义它：
- en: '![](img/Formula_04_141.jpg)![](img/Formula_04_142.jpg)![](img/Formula_04_143.jpg)![](img/Formula_04_144.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_141.jpg)![](img/Formula_04_142.jpg)![](img/Formula_04_143.jpg)![](img/Formula_04_144.jpg)'
- en: Now, you may ask what the point in defining this quantity is if we will follow
    policy ![](img/Formula_04_145.png) thereafter anyway. Well, it can be shown that
    we can improve our policy by choosing the action that gives the highest action
    value for state ![](img/Formula_04_146.png), represented by ![](img/Formula_04_147.png).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能会问，如果我们之后反正会遵循策略 ![](img/Formula_04_145.png)，那么定义这个量有什么意义呢？好吧，事实证明，我们可以通过选择在状态
    ![](img/Formula_04_146.png) 下给出最高动作值的动作来改进我们的策略，这个值由 ![](img/Formula_04_147.png)
    表示。
- en: We will come to how to improve and find the optimal policies later in the next
    chapter.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章讨论如何改进并找到最优策略。
- en: Optimal state-value and action-value functions
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最优状态值和动作值函数
- en: 'An optimal policy is one that gives the optimal state-value function:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 最优策略是给出最优状态值函数的策略：
- en: '![](img/Formula_04_148.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_148.jpg)'
- en: 'An optimal policy is denoted by ![](img/Formula_04_149.png). Note that more
    than one policy could be optimal. However, there is a single optimal state-value
    function. We can also define optimal action-value functions, as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 最优策略用![](img/Formula_04_149.png)表示。请注意，可能有多个策略是最优的。然而，只有一个最优状态值函数。我们也可以定义最优的行动值函数，如下所示：
- en: '![](img/Formula_04_150.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_150.jpg)'
- en: 'The relationship between the optimal state-value and action-value functions
    is the following:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 最优状态值函数和行动值函数之间的关系如下所示：
- en: '![](img/Formula_04_151.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_151.jpg)'
- en: Bellman optimality
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝尔曼最优性
- en: 'When we defined the Bellman equation earlier for ![](img/Formula_04_152.png),
    we needed to use ![](img/Formula_04_153.png) in the equation. This is because
    the state-value function is defined for a policy and we needed to calculate the
    expected reward and the value of the following state with respect to the action(s)
    suggested by the policy while in state ![](img/Formula_04_154.png) (together with
    the corresponding probabilities if multiple actions are suggested to be taken
    with positive probability). This equation was the following:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们之前定义贝尔曼方程时，针对![](img/Formula_04_152.png)，我们需要在方程中使用![](img/Formula_04_153.png)。这是因为状态值函数是针对策略定义的，我们需要计算期望的奖励以及根据策略在状态![](img/Formula_04_154.png)下建议的行动所得到的后续状态的值（如果多个行动被建议且有正概率被采取，还需要考虑相应的概率）。这个方程式如下所示：
- en: '![](img/Formula_04_155.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_155.jpg)'
- en: 'However, while dealing with the optimal state-value function, ![](img/Formula_04_156.png),
    we don''t really need to retrieve ![](img/Formula_04_157.png) from somewhere to
    plug into the equation. Why? Because the optimal policy should be suggesting an
    action that maximizes the consequent expression. After all, the state-value function
    represents the cumulative expected reward. If the optimal policy was not suggesting
    the action that maximizes the expectation term, it would not be an optimal policy.
    Therefore, for the optimal policy and the state-value function, we can write a
    special form of the Bellman equation. This is called the **Bellman optimality
    equation** and is defined as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在处理最优状态值函数![](img/Formula_04_156.png)时，我们实际上不需要从某个地方检索![](img/Formula_04_157.png)并将其代入方程式。为什么？因为最优策略应该建议一个能够最大化后续表达式的行动。毕竟，状态值函数代表的是累积的期望奖励。如果最优策略没有建议一个能够最大化期望项的行动，那它就不是最优策略。因此，对于最优策略和状态值函数，我们可以写出贝尔曼方程的特殊形式。这就是**贝尔曼最优性方程**，定义如下：
- en: '![](img/Formula_04_158.jpg)![](img/Formula_04_159.jpg)![](img/Formula_04_160.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_158.jpg)![](img/Formula_04_159.jpg)![](img/Formula_04_160.jpg)'
- en: 'We can write the Bellman optimality equation for the action-value function
    similarly:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以类似地为行动值函数写出贝尔曼最优性方程：
- en: '![](img/Formula_04_161.jpg)![](img/Formula_04_162.jpg)![](img/Formula_04_163.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_161.jpg)![](img/Formula_04_162.jpg)![](img/Formula_04_163.jpg)'
- en: The Bellman optimality equation is one of the most central ideas in RL, which
    will form the basis of many of the algorithms we will introduce in the next chapter.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼最优性方程是强化学习中的核心思想之一，它将构成我们在下一章中介绍的许多算法的基础。
- en: With that, we have now covered a great deal of the theory behind the RL algorithms.
    Before we actually go into using them to solve some RL problems, we will discuss
    an extension to MDPs next, called partially observable MDPs, which frequently
    occur in many real-world problems.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们已经覆盖了强化学习算法背后的大部分理论。在我们实际使用这些算法来解决一些强化学习问题之前，我们将讨论MDP的一个扩展，叫做部分可观测的MDP，这种情况在许多现实世界问题中经常出现。
- en: Partially observable MDPs
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部分可观测的MDP
- en: 'The definition of policy we have used in this chapter so far is that it is
    a mapping from the state of the environment to actions. Now, the question we should
    ask is *is the state really known to the agent in all types of environments*?
    Remember the definition of state: it describes everything in an environment related
    to the agent''s decision-making (in the grid world example, the color of the walls
    is not important, for instance, so it would not be part of the state).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们所使用的策略定义是，它是从环境状态到行动的映射。现在，我们应该问一个问题：*在所有类型的环境中，代理是否真的能完全知道状态？* 记住状态的定义：它描述了与代理决策相关的环境中的一切（例如，在网格世界的例子中，墙壁的颜色并不重要，因此它不会成为状态的一部分）。
- en: If you think about it, this is a very strong definition. Consider the situation
    when someone is driving a car. Does the driver know everything about the world
    around them while making their driving decisions? Of course not! To begin with,
    the cars would be blocking each other in the driver's sight more often than not.
    Not knowing the precise state of the world does not stop anyone from driving,
    though. In such cases, we base our decision on our **observations**, for example,
    what we see and hear during driving, rather than the state. Then, we say the environment
    is **partially observable**. If it is an MDP, we call it a **partially observable
    MDP**, or **POMDP**.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细想想，这是一个非常强的定义。考虑一下有人开车的情形。司机在做出驾驶决策时，是否了解周围世界的所有信息？当然不是！首先，汽车经常会挡住司机的视线。尽管不知道世界的精确状态，并不会阻止任何人驾驶。在这种情况下，我们会基于**观察**来做决策，例如在驾驶过程中我们看到和听到的内容，而不是基于状态。然后，我们就说环境是**部分可观察的**。如果这是一个MDP，我们就称之为**部分可观察的MDP**，或**POMDP**。
- en: In a POMDP, the probability of seeing a particular observation for an agent
    depends on the latest action and the current state. The function that describes
    this probability distribution is called the **observation function**.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在POMDP中，智能体看到特定观察的概率取决于最新的动作和当前状态。描述这一概率分布的函数称为**观察函数**。
- en: Info
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: 'A POMDP is characterized by a ![](img/Formula_04_164.png) tuple, where ![](img/Formula_04_165.png)
    is the set of possible observations and ![](img/Formula_04_166.png) is an observation
    function, as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 一个部分可观察的马尔可夫决策过程（POMDP）由一个 ![](img/Formula_04_164.png) 元组来描述，其中 ![](img/Formula_04_165.png)
    是可能观察的集合， ![](img/Formula_04_166.png) 是观察函数，具体如下：
- en: '![](img/Formula_04_167.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_167.png)'
- en: In practice, having a partially observable environment usually requires keeping
    a memory of observations to base the actions off of. In other words, a policy
    is formed based not only on the latest observation but also on the observations
    from the last ![](img/Formula_04_168.png) steps. To better understand why this
    works, think of how much information a self-driving car can get from a single,
    frozen scene obtained from its camera. This picture alone does not reveal some
    important information about the environment, such as the speed and the exact directions
    of other cars. To infer that, we need a sequence of scenes and then to see how
    the cars have moved between the scenes.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，部分可观察环境通常需要保留观察记忆，以便基于这些观察采取行动。换句话说，策略不仅是基于最新的观察，还基于过去几步的观察。为了更好地理解为什么这样有效，可以想象一辆自动驾驶汽车从它的摄像头获取到的一幅静态场景能获得多少信息。仅凭这一张图片无法揭示有关环境的一些重要信息，比如其他汽车的速度和确切行驶方向。为了推断这些信息，我们需要一系列场景，并观察汽车在这些场景之间是如何移动的。
- en: Tip
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: In partially observable environments, keeping a memory of observations makes
    it possible to uncover more information about the state of the environment. That
    is why many famous RL settings utilize **Long** **Short-Term** **Memory** (**LSTM**)
    networks to process the observations. We will look at this in more detail in later
    chapters.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在部分可观察环境中，保留观察记忆使得能够揭示更多关于环境状态的信息。这就是为什么许多著名的强化学习（RL）设置使用**长短期记忆**（**LSTM**）网络来处理观察信息。我们将在后续章节中更详细地讨论这个问题。
- en: With that, we conclude our discussion on MDPs. You are now set to dive into
    how to solve RL problems!
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，我们已结束关于MDP的讨论。现在你可以开始深入了解如何解决强化学习问题了！
- en: Summary
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: 'In this chapter, we covered the mathematical framework in which we model the
    sequential decision-making problems we face in real life: MDPs. To this end, we
    started with Markov chains, which do not involve any concept of reward or decision-making.
    Markov chains simply describe stochastic processes where the system transitions
    based on the current state, independent of the previously visited states. We then
    added the notion of reward and started discussing things such as which states
    are more advantageous to be in in terms of the expected future rewards. This created
    a concept of a "value" for a state. We finally brought in the concept of "decision/action"
    and defined the MDP. We then finalized the definitions of state-value functions
    and action-value functions. Lastly, we discussed partially observable environments
    and how they affect the decision-making of an agent.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们覆盖了建模现实生活中顺序决策问题的数学框架：马尔可夫决策过程（MDPs）。为此，我们从不涉及奖励或决策概念的马尔可夫链开始。马尔可夫链仅仅描述了基于当前状态转移的随机过程，转移与之前访问过的状态无关。接着，我们加入了奖励的概念，开始讨论哪些状态在预期未来奖励的角度更具优势。这样产生了“状态价值”的概念。最后，我们引入了“决策/行动”的概念，并定义了MDP。我们最终明确了状态值函数和行动值函数的定义。最后，我们讨论了部分可观察环境以及它们如何影响智能体的决策。
- en: The Bellman equation variations we introduced in this chapter are central to
    many of the RL algorithms today, which are called "value-based methods." Now that
    you are equipped with a solid understanding of what they are, starting from the
    next chapter, we will use these ideas to come up with optimal policies. In particular,
    we will first look at the exact solution algorithms to MDPs, which are dynamic
    programming methods. We will then go into methods such as Monte Carlo and temporal-difference
    learning, which provide approximate solutions but don't require knowing the precise
    dynamics of the environment, unlike dynamic programming methods.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍的贝尔曼方程变体是今天许多强化学习算法的核心，通常被称为“基于价值的方法”。现在你已经对它们有了扎实的理解，从下一章开始，我们将利用这些思想来得出最优策略。具体来说，我们将首先讨论马尔可夫决策过程（MDPs）的精确解法算法，即动态规划方法。接着，我们将讨论像蒙特卡罗和时序差分学习这样的算法，它们提供近似解法，但不像动态规划方法那样需要知道环境的精确动态。
- en: Stay tuned and see you in the next chapter!
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 敬请期待，下章再见！
- en: Exercises
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Calculate ![](img/Formula_04_169.png)-step transition probabilities for the
    robot using the Markov chain model we introduced with the state initialized at
    ![](img/Formula_04_170.png). You will notice that it will take a bit more time
    for the system to reach the steady state.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们介绍的马尔可夫链模型计算机器人在![](img/Formula_04_170.png)状态下的![](img/Formula_04_169.png)-步转移概率。你会注意到，系统到达稳态所需的时间稍长。
- en: Modify the Markov chain to include the absorbing state for the robot crashing
    into the wall. What does your ![](img/Formula_04_171.png) look like for a large
    ![](img/Formula_04_172.png)?
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改马尔可夫链，将机器人撞墙的吸收状态包括在内。对于一个较大的![](img/Formula_04_172.png)，你的![](img/Formula_04_171.png)看起来是什么样的？
- en: Using the state values in *Figure 4.7*, calculate the value of a corner state
    using the estimates for the neighboring state values.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*图4.7*中的状态值，计算一个角落状态的值，利用邻近状态值的估计。
- en: Iteratively estimate the state values in the grid world MRP using matrix forms
    and operations instead of a `for` loop.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用矩阵形式和运算代替`for`循环，迭代估计网格世界马尔可夫决策过程中的状态值。
- en: Calculate the ![](img/Formula_04_173.png) action value, where the π policy corresponds
    to taking no action in any state, using the values in *Figure 4.7*. Based on how
    ![](img/Formula_04_174.png) compares to ![](img/Formula_04_175.png), would you
    consider changing your policy to take the up action instead of no action in state
    ![](img/Formula_04_176.png)?
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算![](img/Formula_04_173.png)的行动值，其中π策略对应于在任何状态下不采取行动，使用*图4.7*中的数值。根据![](img/Formula_04_174.png)与![](img/Formula_04_175.png)的比较，你会考虑在状态![](img/Formula_04_176.png)中改变策略，采取向上的行动，而不是不行动吗？
- en: Further reading
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Silver, D. (2015). *Lecture 2: Markov Decision Processes*. Retrieved from a
    UCL course on RL: [https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf](https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver, D. (2015). *第2讲：马尔可夫决策过程*。来自UCL的强化学习课程：[https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf](https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf)
- en: 'Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*.
    A Bradford book'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton, R. S., & Barto, A. G. (2018). *强化学习：导论*。Bradford出版社
- en: Ross, S. M. (1996). *Stochastic Processes*. 2nd ed., Wiley
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ross, S. M. (1996). *随机过程*. 第2版，Wiley
