- en: 10\. Playing an Atari Game with Deep Recurrent Q-Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10\. 使用深度递归 Q 网络玩 Atari 游戏
- en: Introduction
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 引言
- en: In this chapter, we will be introduced to **Deep Recurrent Q Networks** (**DRQNs**)
    and their variants. You will train **Deep Q Network** (**DQN**) models with **Convolutional
    Neural Networks** (**CNNs**) and **Recurrent Neural Networks** (**RNNs**). You
    will acquire hands-on experience of using the OpenAI Gym package to train reinforcement
    learning agents to play an Atari game. You will also learn how to analyze long
    sequences of input and output data using attention mechanisms. By the end of this
    chapter, you will have a good understanding of what DRQNs are and how to implement
    them with TensorFlow.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍 **深度递归 Q 网络**（**DRQNs**）及其变种。你将使用 **卷积神经网络**（**CNNs**）和 **递归神经网络**（**RNNs**）训练
    **深度 Q 网络**（**DQN**）模型。你将获得使用 OpenAI Gym 包训练强化学习代理玩 Atari 游戏的实践经验。你还将学习如何使用注意力机制分析长时间序列的输入和输出数据。在本章结束时，你将对
    DRQNs 有一个清晰的理解，并能够使用 TensorFlow 实现它们。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In the previous chapter, we learned that DQNs achieved higher performance compared
    to traditional reinforcement learning techniques. Video games are a perfect example
    of where DQN models excel. Training an agent to play video games can be quite
    difficult for traditional reinforcement learning agents as there is a huge number
    of possible combinations of states, actions, and Q-values to be processed and
    analyzed during the training.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解到 DQNs 相比传统强化学习技术取得了更高的性能。视频游戏是 DQN 模型表现优异的典型例子。训练一个代理来玩视频游戏对于传统的强化学习代理来说非常困难，因为在训练过程中需要处理和分析大量可能的状态、动作和
    Q 值组合。
- en: Deep learning algorithms are renowned for handling high-dimensional tensors.
    Some researchers combined Q-learning techniques with deep learning models to overcome
    this limitation and came up with DQNs. A DQN model comprises a deep learning model
    that is used as a function approximation of Q-values. This technique constituted
    a major breakthrough in the reinforcement learning field as it helped to handle
    much larger state and action spaces than traditional models.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习算法以处理高维张量而闻名。一些研究人员将 Q 学习技术与深度学习模型相结合，克服了这一局限性，并提出了 DQNs。DQN 模型包含一个深度学习模型，作为
    Q 值的函数逼近。此技术在强化学习领域取得了重大突破，因为它有助于处理比传统模型更大的状态空间和动作空间。
- en: Since then, further research has been undertaken and different types of DQN
    models have been designed, such as DRQNs or **Deep Attention Recurrent Q Networks**
    (**DARQNs**). In this chapter, we will see how DQN models can benefit from CNN
    and RNN models, which have achieved amazing results in computer vision and natural
    language processing. We will look at how to train such models to play the famous
    Atari game Breakout in the next section.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 从那时起，进一步的研究已展开，设计了不同类型的 DQN 模型，如 DRQNs 或 **深度注意力递归 Q 网络**（**DARQNs**）。在本章中，我们将看到
    DQN 模型如何从 CNN 和 RNN 模型中受益，这些模型在计算机视觉和自然语言处理领域取得了惊人的成果。我们将在下一节中介绍如何训练这些模型来玩著名的
    Atari 游戏《打砖块》。
- en: Understanding the Breakout Environment
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解《打砖块》环境
- en: We will be training different deep reinforcement learning agents to play the
    game Breakout in this chapter. Before diving in, let's learn some more about the
    game.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将训练不同的深度强化学习代理来玩《打砖块》游戏。在深入之前，先了解一下这款游戏。
- en: Breakout is an arcade game designed and released in 1976 by Atari. Steve Wozniak,
    co-founder of Apple, was part of the design and development team. The game was
    extremely popular at that time and multiple versions were developed over the years.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 《打砖块》是一款由 Atari 在 1976 年设计并发布的街机游戏。苹果公司联合创始人 Steve Wozniak 是设计和开发团队的一员。这款游戏在当时非常受欢迎，随着时间的推移，多个版本被开发出来。
- en: 'The goal of the game is to break all the bricks located at the top of the screen
    with a ball (since the game was developed in 1974 with low screen definition,
    the ball is represented by pixels and so its shape can be seen as a rectangle
    in the following screenshot) without dropping it. The player can move a paddle
    horizontally at the bottom of the screen to hit the ball before it drops and bounce
    it back toward the bricks. Also, the ball will bounce back after hitting the side
    walls or the ceiling. The game ends when either the ball drops (in this case,
    the player loses) or when all the bricks have been broken and the player wins
    and can proceed to the next stage:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏的目标是用一个球打破位于屏幕顶部的所有砖块（由于该游戏于1974年开发，屏幕分辨率较低，因此球由像素表示，在以下截图中它的形状看起来像一个矩形），而不让球掉下来。玩家可以在屏幕底部水平移动一个挡板，在球掉下之前将其击打回来，并将球弹回砖块。同时，球在撞击侧墙或天花板后会反弹。游戏结束时，如果球掉落（此时玩家失败），或者当所有砖块都被打破，玩家获胜并可进入下一阶段：
- en: '![Figure 10.1: Screenshot of Breakout'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.1: 《打砖块》游戏截图'
- en: '](img/B16182_10_01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_10_01.jpg)'
- en: 'Figure 10.1: Screenshot of Breakout'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10.1: 《打砖块》游戏截图'
- en: The `gym` package from OpenAI provides an environment that emulates this game
    and allows deep reinforcement learning agents to train and play on it. The name
    of the environment that we will be using is `BreakoutDeterministic-v4`. Given
    below are some basic code implementations of this environment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的`gym`包提供了一个模拟此游戏的环境，允许深度强化学习智能体在其上进行训练和游戏。我们将使用的环境名称是`BreakoutDeterministic-v4`。下面是该环境的一些基本代码实现。
- en: 'You will need to load the Breakout environment from the `gym` package before
    being able to train an agent to play this game. To do so, we will use the following
    code snippet:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在能够训练智能体玩这个游戏之前，你需要从`gym`包中加载《打砖块》环境。为此，我们将使用以下代码片段：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is a deterministic game where the actions chosen by the agent will happen
    every time as intended and with a frame skipping rate of `4`. Frame skipping corresponds
    to the number of frames an action is repeated until a new action is performed.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个确定性游戏，智能体选择的动作每次都会按预期发生，并且具有`4`的帧跳跃率。帧跳跃指的是在执行新动作之前，一个动作会被重复多少帧。
- en: 'The game comprises four deterministic actions, as shown by the following code:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 该游戏包括四个确定性的动作，如以下代码所示：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following is the output of the code:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码的输出结果：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The observation space is a color image (a box of `3` channels) of size `210`
    by `160`:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 观察空间是一个大小为`210` x `160`的彩色图像（包含`3`个通道）：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following is the output of the code:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码的输出结果：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To initialize the game and get the first initial state, we need to call the
    `.reset()` method, as shown in the following code:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要初始化游戏并获取第一个初始状态，我们需要调用`.reset()`方法，代码如下所示：
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To sample an action (that is, taking a random action from all the possible
    actions) from the action space, we can use the `.sample()` method:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从动作空间中采样一个动作（即从所有可能的动作中随机选择一个），我们可以使用`.sample()`方法：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, to perform a single action and get its results from the environment,
    we need to call the `.step()` method:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，要执行一个动作并获取其从环境中返回的结果，我们需要调用`.step()`方法：
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following screenshot is a `new_state` result of the environment state after
    performing an action:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示的是执行一个动作后的环境状态的`new_state`结果：
- en: '![Figure 10.2: Result of the new state after performing an action'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.2: 执行动作后的新状态结果'
- en: '](img/B16182_10_02.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_10_02.jpg)'
- en: 'Figure 10.2: Result of the new state after performing an action'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10.2: 执行动作后的新状态结果'
- en: 'The `.step()` method returns four different objects:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`.step()`方法返回四个不同的对象：'
- en: The new environment state resulting from the previous action.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由前一个动作产生的新环境状态。
- en: The reward related to the previous action.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与前一个动作相关的奖励。
- en: A flag indicating whether the game has ended after the previous action (either
    a win or the game is over).
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个标志，指示在前一个动作之后游戏是否已经结束（无论是胜利还是游戏结束）。
- en: Some additional information from the environment. This information cannot be
    used to train the agent, as stated in the OpenAI instructions.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自环境的其他信息。正如OpenAI的说明所述，这些信息不能用于训练智能体。
- en: Having gone through some basic code implementation of Breakout in OpenAI, let's
    perform our first exercise where we will have our agent play this game.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成了一些关于OpenAI中《打砖块》游戏的基本代码实现后，接下来我们将进行第一次练习，让我们的智能体来玩这个游戏。
- en: 'Exercise 10.01: Playing Breakout with a Random Agent'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '练习 10.01: 使用随机智能体玩《打砖块》'
- en: 'In this exercise, we will be implementing some functions for playing the game
    Breakout that will be useful for the remainder of the chapter. We will also create
    an agent that takes random actions:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将实现一些用于玩 Breakout 游戏的函数，这些函数将在本章剩余部分中非常有用。我们还将创建一个随机动作的智能体：
- en: 'Open a new Jupyter Notebook file and import the `gym` library:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter Notebook 文件并导入`gym`库：
- en: '[PRE8]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Create a class called `RandomAgent` that takes a single input parameter named
    `env`, the game environment. This class will have a method called `get_action()`
    that will return a random action from the environment:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`RandomAgent`的类，该类接收一个名为`env`的输入参数，即游戏环境。该类将拥有一个名为`get_action()`的方法，该方法将从环境中返回一个随机动作：
- en: '[PRE9]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Create a function called `initialize_env()` that will return the initial state
    of the given input environment, a `False` value that corresponds to the initial
    value of a done flag, and `0` as the initial value of the reward:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`initialize_env()`的函数，该函数将返回给定输入环境的初始状态，一个对应于完成标志初始值的`False`值，以及作为初始奖励的`0`：
- en: '[PRE10]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Create a function called `play_game()` that takes an agent, a state, a done
    flag, and a list of rewards as inputs. This will return the total reward received.
    This `play_game()` function will iterate until the done flag equals `True`. At
    each iteration, it will perform the following actions: get an action from the
    agent, perform the action on the environment, accumulate the reward received,
    and prepare for the next state:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`play_game()`的函数，该函数接收一个智能体、一个状态、一个完成标志和一个奖励列表作为输入。该函数将返回收到的总奖励。`play_game()`函数将在完成标志为`True`之前进行迭代。在每次迭代中，它将执行以下操作：从智能体获取一个动作，在环境中执行该动作，累计收到的奖励，并为下一状态做准备：
- en: '[PRE11]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Create a function called `train_agent()` that takes as inputs an environment,
    a number of episodes, and an agent. This function will create a `deque` object
    from the collections package and iterate through the number of episodes provided.
    At each iteration, it will perform the following actions: initialize the environment
    with `initialize_env()`, play a game with `play_game()`, and append the received
    rewards to the `deque` object. Finally, it will print the average score of the
    games played:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`train_agent()`的函数，该函数接收一个环境、一个游戏轮数和一个智能体作为输入。该函数将从`collections`包中创建一个`deque`对象，并根据提供的轮数进行迭代。在每次迭代中，它将执行以下操作：使用`initialize_env()`初始化环境，使用`play_game()`玩游戏，并将收到的奖励追加到`deque`对象中。最后，它将打印游戏的平均得分：
- en: '[PRE12]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Instantiate a Breakout environment called `env` using the `gym.make()` function:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`gym.make()`函数实例化一个名为`env`的 Breakout 环境：
- en: '[PRE13]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Instantiate a `RandomAgent` object called `agent`:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个名为`agent`的`RandomAgent`对象：
- en: '[PRE14]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Create a variable called `episodes` that will take the value `10`:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`episodes`的变量，并将其值设置为`10`：
- en: '[PRE15]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Call the `train_agent` function by providing `env`, episodes, and the agent:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过提供`env`、轮次和智能体来调用`train_agent`函数：
- en: '[PRE16]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After training the agent, you will expect to achieve something approaching
    the following score (your score may be slightly different due to the randomness
    of the game):'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在训练完智能体后，你将期望达到以下分数（由于游戏的随机性，你的分数可能略有不同）：
- en: '[PRE17]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The random agent is achieving a low score after 10 episodes, that is, 0.6\.
    We will consider that the agent will have learned to play this game if it achieves
    a score above 10\. However, since we have use a low number of episodes, we have
    not yet reached a stage where we achieve a score above 10\. At this stage, however,
    we have created some functions for playing the game Breakout that we will reuse
    and update for the coming sections.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 随机智能体在 10 轮游戏后取得了较低的分数，即 0.6。我们会认为当智能体的得分超过 10 时，它已经学会了玩这个游戏。然而，由于我们使用的游戏轮数较少，我们还没有达到得分超过
    10 的阶段。然而，在这一阶段，我们已经创建了一些玩 Breakout 游戏的函数，接下来我们将重用并更新这些函数。
- en: Note
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/30CfVeH](https://packt.live/30CfVeH).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此部分的源代码，请参考[https://packt.live/30CfVeH](https://packt.live/30CfVeH)。
- en: You can also run this example online at [https://packt.live/3hi12nU](https://packt.live/3hi12nU).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行此示例，网址为[https://packt.live/3hi12nU](https://packt.live/3hi12nU)。
- en: In the next section, we will look at CNN models and how to build them in TensorFlow.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习 CNN 模型以及如何在 TensorFlow 中构建它们。
- en: CNNs in TensorFlow
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow中的 CNN
- en: CNNs are a type of deep learning architecture that achieved amazing results
    in computer vision tasks such as image classification, object detection, and image
    segmentation. Self-driving cars are an example of a real-life application of such technology.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: CNN是一种深度学习架构，在计算机视觉任务中取得了惊人的成果，如图像分类、目标检测和图像分割。自动驾驶汽车是这种技术的实际应用示例。
- en: 'The main element of CNNs is the convolutional operation, where a filter is
    applied to different parts of an image to detect specific patterns and generate
    a feature map. A feature map can be thought of as an image with the detected patterns
    highlighted, as shown in the following example:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的主要元素是卷积操作，其中通过将滤波器应用于图像的不同部分来检测特定的模式，并生成特征图。特征图可以看作是一个突出显示检测到的模式的图像，如下例所示：
- en: '![Figure 10.3: Example of a vertical edge feature map'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.3：垂直边缘特征图示例'
- en: '](img/B16182_10_03.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_10_03.jpg)'
- en: 'Figure 10.3: Example of a vertical edge feature map'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3：垂直边缘特征图示例
- en: 'A CNN is composed of several convolutional layers that apply the convolutional
    operation with different filters. The final layers of a CNN are usually one or
    several fully connected layers that are responsible for making the right predictions
    for a given dataset. For example, the final layer of a CNN trained to predict
    images of digits will be a fully connected layer of 10 neurons. Each neuron will
    be responsible for predicting the probability of occurrence of each digit (0 to
    9):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: CNN由多个卷积层组成，每个卷积层使用不同的滤波器进行卷积操作。CNN的最后几层通常是一个或多个全连接层，负责为给定数据集做出正确的预测。例如，训练用于预测数字图像的CNN的最后一层将是一个包含10个神经元的全连接层。每个神经元将负责预测每个数字（0到9）的发生概率：
- en: '![Figure 10.4: Example of a CNN architecture for classifying images of digits'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.4：用于分类数字图像的CNN架构示例'
- en: '](img/B16182_10_04.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_10_04.jpg)'
- en: 'Figure 10.4: Example of a CNN architecture for classifying images of digits'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4：用于分类数字图像的CNN架构示例
- en: 'Building CNN models is extremely easy with TensorFlow, thanks to the Keras
    API. To define a convolutional layer, we just need to use the `Conv2D()` class,
    as shown in the following code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorFlow构建CNN模型非常简单，这要归功于Keras API。要定义一个卷积层，我们只需要使用`Conv2D()`类，如以下代码所示：
- en: '[PRE18]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding example, we have created a convolutional layer with `128` filters
    (or kernels) of size `3` by `3`, and `relu` as the `activation` function.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们创建了一个具有`128`个`3x3`大小的滤波器（或内核）的卷积层，并使用`relu`作为激活函数。
- en: Note
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Throughout the course of this chapter, we'll be using the ReLU activation function
    for CNN models, as it is one of the most performant activation functions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用ReLU激活函数来构建CNN模型，因为它是最具性能的激活函数之一。
- en: 'To define a fully connected layer, we will use the `Dense()` class:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义一个全连接层，我们将使用`Dense()`类：
- en: '[PRE19]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In Keras, we can use the `Sequential()` class to create a multi-layer CNN:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，我们可以使用`Sequential()`类来创建一个多层CNN：
- en: '[PRE20]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Please note that you need to provide the dimensions of the input images for
    the first convolutional layer only. After defining the layers of your model, you
    will need to compile it by providing the loss function, the optimizer, and the
    metrics to be displayed:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您只需要为第一个卷积层提供输入图像的维度。定义完模型的各层后，您还需要通过提供损失函数、优化器和要显示的度量标准来编译模型：
- en: '[PRE21]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, the last step is to train the CNN with the training set on a specified
    number of `epochs`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最后一步是用训练集和指定数量的`epochs`来训练CNN：
- en: '[PRE22]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Another useful method from TensorFlow is `tf.image.rgb_to_grayscale()`, which
    is used to convert a color image to grayscale:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow中的另一个有用方法是`tf.image.rgb_to_grayscale()`，它用于将彩色图像转换为灰度图像：
- en: '[PRE23]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To resize an input image, we will use the `tf.image.resize()` method:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要调整输入图像的大小，我们将使用`tf.image.resize()`方法：
- en: '[PRE24]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now that we know how to build a CNN model, let's put this into practice in the
    following exercise.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何构建CNN模型，接下来让我们在以下练习中将其付诸实践。
- en: 'Exercise 10.02: Designing a CNN Model with TensorFlow'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习10.02：使用TensorFlow设计CNN模型
- en: 'In this exercise, we will be designing a CNN model with TensorFlow. This model
    will be used for our DQN agent in *Activity 10.01*, *Training a DQN with CNNs
    to Play Breakout*, where we will train this model to play the game Breakout. Perform
    the following steps to implement the exercise:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将使用TensorFlow设计一个CNN模型。该模型将用于我们在*活动10.01*中使用的DQN代理，*使用CNN训练DQN玩打砖块游戏*，我们将在其中训练这个模型玩打砖块游戏。执行以下步骤来实现这个练习：
- en: 'Open a new Jupyter Notebook file and import the `tensorflow` package:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter Notebook文件并导入`tensorflow`包：
- en: '[PRE25]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Import the `Sequential` class from `tensorflow.keras.models`:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`tensorflow.keras.models`导入`Sequential`类：
- en: '[PRE26]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Instantiate a sequential model and save it to a variable called `model`:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个顺序模型并将其保存到变量`model`中：
- en: '[PRE27]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Import the `Conv2D` class from `tensorflow.keras.layers`:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`tensorflow.keras.layers`导入`Conv2D`类：
- en: '[PRE28]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Instantiate a convolutional layer with `Conv2D` with `32` filters of size `8`,
    a stride of 4 by 4, relu as the activation function, and an input shape of (`84`,
    `84`, `1`). These dimensions are related to the size of the screen for the game
    Breakout. Save it to a variable called `conv1`:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`Conv2D`实例化一个卷积层，设置`32`个大小为`8`的滤波器，步幅为4x4，激活函数为relu，输入形状为（`84`，`84`，`1`）。这些维度与Breakout游戏屏幕的大小有关。将其保存到变量`conv1`中：
- en: '[PRE29]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Instantiate a second convolutional layer with `Conv2D` with `64` filters of
    size `4`, a stride of `2` by `2`, and `relu` as the activation function. Save
    it to a variable called `conv2`:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`Conv2D`实例化第二个卷积层，设置`64`个大小为`4`的滤波器，步幅为2x2，激活函数为`relu`。将其保存到变量`conv2`中：
- en: '[PRE30]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Instantiate a third convolutional layer with `Conv2D` with `64` filters of
    size `3`, a stride of `1` by `1`, and `relu` as the activation function. Save
    it to a variable called `conv3`:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`Conv2D`实例化第三个卷积层，设置`64`个大小为`3`的滤波器，步幅为1x1，激活函数为`relu`。将其保存到变量`conv3`中：
- en: '[PRE31]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Add the three convolutional layers to the model by means of the `add()` method:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`add()`方法将三个卷积层添加到模型中：
- en: '[PRE32]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Import the `Flatten` class from `tensorflow.keras.layers`. This class will
    resize the output of the convolutional layers to a one-dimension vector:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`tensorflow.keras.layers`导入`Flatten`类。这个类将调整卷积层输出的大小，转化为一维向量：
- en: '[PRE33]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Add an instantiated `Flatten` layer to the model by means of the `add()` method:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`add()`方法将一个实例化的`Flatten`层添加到模型中：
- en: '[PRE34]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Import the `Dense` class from `tensorflow.keras.layers`:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`tensorflow.keras.layers`导入`Dense`类：
- en: '[PRE35]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Instantiate a fully connected layer with `256` units and `relu` as the activation function:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`256`个单元实例化一个全连接层，并将激活函数设置为`relu`：
- en: '[PRE36]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Instantiate a fully connected layer with `4` units, which corresponds to the
    number of possible actions from the game Breakout:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`4`个单元实例化一个全连接层，这与Breakout游戏中可能的操作数量相对应：
- en: '[PRE37]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Add the two fully connected layers to the model by means of the `add()` method:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`add()`方法将两个全连接层添加到模型中：
- en: '[PRE38]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Import the `RMSprop` class from `tensorflow.keras.optimizers`:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`tensorflow.keras.optimizers`导入`RMSprop`类：
- en: '[PRE39]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Instantiate an `RMSprop` optimizer with `0.00025` as the learning rate:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`0.00025`作为学习率实例化一个`RMSprop`优化器：
- en: '[PRE40]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Compile the model by specifying `mse` as the loss function, `RMSprop` as `optimizer`,
    and `accuracy` as the metric to be displayed during training to the `compile`
    method:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在`compile`方法中指定`mse`作为损失函数，`RMSprop`作为优化器，`accuracy`作为训练期间显示的指标，来编译模型：
- en: '[PRE41]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Print a summary of the model using the `summary` method:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`summary`方法打印模型的摘要：
- en: '[PRE42]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Following is the output of the code:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是代码的输出：
- en: '![Figure 10.5: Summary of the CNN model'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图10.5：CNN模型摘要'
- en: '](img/B16182_10_05.jpg)'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_10_05.jpg)'
- en: 'Figure 10.5: Summary of the CNN model'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5：CNN模型摘要
- en: The output shows the architecture of the model we just built, together with
    the different layers and the number of parameters that will be used during the
    training of the model.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了我们刚刚构建的模型的架构，包括不同的层以及在模型训练过程中使用的参数数量。
- en: Note
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2YrqiiZ](https://packt.live/2YrqiiZ).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2YrqiiZ](https://packt.live/2YrqiiZ)。
- en: You can also run this example online at [https://packt.live/3fiNMxE](https://packt.live/3fiNMxE).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/3fiNMxE](https://packt.live/3fiNMxE)上在线运行这个示例。
- en: We have designed a CNN model with three convolutional layers. In the next section,
    we will see how we can use this model in relation to a DQN agent.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经设计了一个包含三个卷积层的CNN模型。在接下来的部分，我们将看到如何将这个模型与DQN代理结合使用。
- en: Combining a DQN with a CNN
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将DQN与CNN结合
- en: Humans play video games using their sight. They look at the screen, analyze
    the situation, and decide what the best action to be performed is. In video games,
    there can be a lot of things happening on the screen, so being able to see all
    these patterns can give a significant advantage in playing the game. Combining
    a DQN with a CNN can help a reinforcement learning agent to learn the right action
    to take given a particular situation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 人类通过视觉玩视频游戏。他们观察屏幕，分析情况，并决定最合适的行动。在视频游戏中，屏幕上可能会发生许多事情，因此能够看到所有这些模式可以在游戏中提供显著的优势。将DQN与CNN结合，可以帮助强化学习智能体根据特定情况学习采取正确的行动。
- en: 'Instead of just using fully connected layers, a DQN model can be extended with
    convolutional layers as inputs. The model will then be able to analyze the input
    image, find the relevant patterns, and feed them to the fully connected layers
    responsible for predicting the Q-values, as shown in the following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅仅使用全连接层，DQN模型还可以通过卷积层作为输入来扩展。模型将能够分析输入图像，找到相关模式，并将它们输入到负责预测Q值的全连接层，如下所示：
- en: '![Figure 10.6: Difference between a normal DQN and a DQN combined'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.6：普通DQN与结合卷积层的DQN之间的区别'
- en: with convolutional layers
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用卷积层
- en: '](img/B16182_10_06.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_10_06.jpg)'
- en: 'Figure 10.6: Difference between a normal DQN and a DQN combined with convolutional
    layers'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6：普通DQN与结合卷积层的DQN之间的区别
- en: Adding convolutional layers helps the agent to better understand the environment.
    The DQN agent that we will build in the coming activity will use the CNN model
    from *Exercise 10.02*, *Designing a CNN Model with TensorFlow*, to output the
    Q-values for a given state. But rather than using a single model, we will use
    two models instead. The models will share the exact same architecture.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 添加卷积层有助于智能体更好地理解环境。我们将在接下来的活动中构建的DQN智能体将使用*练习10.02*中的CNN模型，*使用TensorFlow设计CNN模型*，以输出给定状态的Q值。但我们将使用两个模型，而不是单一模型。这两个模型将共享完全相同的架构。
- en: The first model will be responsible for predicting the Q-values for playing
    the game, while the second one (referred to as the target model) will be responsible
    for learning what should be the optimal Q-values. This technique helps the target
    model to converge faster on the optimal solution.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个模型将负责预测玩游戏时的Q值，而第二个模型（称为目标模型）将负责学习应当是什么样的最优Q值。这种技术帮助目标模型更快地收敛到最优解。
- en: 'Activity 10.01: Training a DQN with CNNs to Play Breakout'
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动10.01：训练DQN与CNN一起玩Breakout
- en: In this activity, we will build a DQN with additional convolutional layers and
    train it to play the game Breakout with CNNs. We will add experience replay to
    the agent. We will need to preprocess the images in order to create a sequence
    of four images for our Breakout game.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将构建一个带有额外卷积层的DQN，并训练它使用CNN玩Breakout游戏。我们将为智能体添加经验回放。我们需要预处理图像，以便为Breakout游戏创建四张图像的序列。
- en: 'The following instructions will help you to complete this activity:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下指令将帮助你完成此任务：
- en: Import the relevant packages (`gym`, `tensorflow`, `numpy`).
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的包（`gym`、`tensorflow`、`numpy`）。
- en: Reshape the training and test sets.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对训练集和测试集进行重塑。
- en: Create a DQN class with the `build_model()` method, which will instantiate a
    CNN model composed of the `get_action()` method, which will apply the epsilon-greedy
    algorithm to choose the action to be played, the `add_experience()` method to
    store in memory the experience acquired by playing the game, the `replay()` method,
    which will perform experience replay by sampling experiences from the memory and
    train the DQN model, and the `update_epsilon()` method to gradually decrease the
    epsilon value for epsilon-greedy.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含`build_model()`方法的DQN类，该方法将实例化一个由`get_action()`方法组成的CNN模型，`get_action()`方法将应用epsilon-greedy算法选择要执行的动作，`add_experience()`方法将存储通过玩游戏获得的经验，`replay()`方法将执行经验回放，通过从记忆中抽样经验并训练DQN模型，`update_epsilon()`方法将逐渐减少epsilon值以适应epsilon-greedy算法。
- en: Use the `initialize_env()` function to initialize the environment by returning
    the initial state, `False` for the done flag, and `0` as the initial reward.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`initialize_env()`函数通过返回初始状态、`False`表示任务未完成标志、以及`0`作为初始奖励来初始化环境。
- en: 'Create a function called `preprocess_state()` that will perform the following
    preprocessing on an image: crop the image to remove unnecessary parts, convert
    to a grayscale image, and resize the image to a square shape.'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`preprocess_state()`的函数，该函数将对图像执行以下预处理：裁剪图像以去除不必要的部分，将图像转换为灰度图像，并将图像调整为正方形。
- en: Create a function called `play_game()` that will play a game until it is over,
    and then store the experience and the accumulated reward.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`play_game()`的函数，该函数将在游戏结束前持续进行游戏，然后存储经验和累积的奖励。
- en: Create a function called `train_agent()` that will iterate through a number
    of episodes where the agent will play a game and perform experience replay.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`train_agent()`的函数，该函数将通过多个回合进行迭代，代理将在每回合中玩游戏并进行经验回放。
- en: Instantiate a Breakout environment and train a DQN agent to play this game for
    `50` episodes. Please note that it might take longer for this step to execute
    as we are training large models.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个Breakout环境并训练一个DQN代理进行`50`个回合的游戏。请注意，由于我们正在训练较大的模型，这一步骤可能需要更长时间才能完成。
- en: 'The expected output will be close to the one shown here. You may have slightly
    different values on account of the randomness of the game and the randomness of
    the epsilon-greedy algorithm in choosing the action to be played:'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将接近这里显示的结果。由于游戏的随机性以及epsilon-greedy算法选择执行动作时的随机性，您可能会看到略有不同的值：
- en: '[PRE43]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 752.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本次活动的解答可以在第752页找到。
- en: 'In the next section, we will see how we can extend this model with another
    type of deep learning architecture: the RNN.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何通过另一种深度学习架构来扩展这个模型：RNN。
- en: RNNs in TensorFlow
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow中的RNN
- en: In the previous section, we saw how to integrate a CNN into a DQN model to improve
    the performance of a reinforcement learning agent. We added a few convolutional
    layers as inputs to the fully connected layers of the DQN model. These convolutional
    layers helped the model to analyze visual patterns from the game environment and
    make better decisions.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们展示了如何将卷积神经网络（CNN）集成到深度Q网络（DQN）模型中，以提高强化学习代理的性能。我们添加了一些卷积层，作为DQN模型的全连接层的输入。这些卷积层帮助模型分析游戏环境中的视觉模式，并做出更好的决策。
- en: 'There is a limitation, however, to using a traditional CNN approach. CNNs can
    only analyze a single image. While playing video games such as Breakout, analyzing
    a sequence of images is a much more powerful tool when it comes to understanding
    the movements of the ball. This is where RNNs come to the fore:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用传统的CNN方法有一个局限性。CNN只能分析单张图像。而在玩像Breakout这样的电子游戏时，分析图像序列要比分析单张图像更有力，因为它有助于理解球的运动轨迹。这就是RNN发挥作用的地方：
- en: '![Figure 10.7: Sequencing of RNNs'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.7：RNN的序列化'
- en: '](img/B16182_10_07.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_10_07.jpg)'
- en: 'Figure 10.7: Sequencing of RNNs'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7：RNN的序列化
- en: RNNs are a specific architecture of neural networks that take a sequence of
    inputs. They are very popular in natural language processing for treating corpora
    of texts for speech recognition, chatbots, or text translation. Texts can be defined
    as sequences of words that are correlated with one another. It is hard to determine
    the topic of a sentence or a paragraph just by looking at a single word. You have
    to look at a sequence of multiple words before being able to make a guess.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: RNN是神经网络的一种特定架构，它处理一系列输入。它们在自然语言处理领域非常流行，用于处理语料库中的文本，例如语音识别、聊天机器人或文本翻译。文本可以被定义为一系列相互关联的单词。仅凭单个单词很难判断一个句子或段落的主题。你必须查看多个单词的序列，才能做出猜测。
- en: There are different types of RNN models. The most popular ones are **Gated Recurrent
    Unit** (**GRU**) and **Long Short-Term Memory** (**LSTM**). Both of these models
    have a memory that keeps a record of the different inputs the model has already
    processed (for instance, the first five words of a sentence) and combines them
    with new inputs (such as the sixth word of a sentence).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同类型的RNN模型，其中最流行的是**门控循环单元**（**GRU**）和**长短期记忆**（**LSTM**）。这两种模型都有记忆功能，可以记录模型已经处理过的不同输入（例如，句子的前五个单词），并将它们与新的输入（如句子的第六个单词）结合起来。
- en: 'In TensorFlow, we can build an `LSTM` layer of `10` units as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中，我们可以按照如下方式构建一个包含`10`个单元的`LSTM`层：
- en: '[PRE44]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The sigmoid activation function is the most popular one used for RNN models.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid激活函数是RNN模型中最常用的激活函数。
- en: 'The syntax will be very similar for defining a `GRU` layer:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 定义`GRU`层的语法与此非常相似：
- en: '[PRE45]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'In Keras, we can use the `Sequential()` class to create a multi-layer LSTM:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，我们可以使用 `Sequential()` 类来创建一个多层 LSTM：
- en: '[PRE46]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Before fitting the model, you will need to compile it by providing the loss
    function, the optimizer, and the metrics to be displayed:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合模型之前，你需要通过提供损失函数、优化器和要显示的度量标准来编译它：
- en: '[PRE47]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We already saw how to define LSTM layers previously, but in order to combine
    them with a CNN model, we need to use a wrapper in TensorFlow called `TimeDistributed()`.
    This class is used to apply the same specified layer to each timestep of an input
    tensor, such as the following:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经看到如何定义 LSTM 层，但为了将其与 CNN 模型结合使用，我们需要在 TensorFlow 中使用一个名为 `TimeDistributed()`
    的封装类。该类用于将相同的指定层应用到输入张量的每个时间步，如下所示：
- en: '[PRE48]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In the preceding example, the same fully connected layer is applied to each
    of the timesteps received. In our case, we want to apply a convolutional layer
    to each image of a sequence before feeding an LSTM model. To build such a sequence,
    we will need to stack multiple images together to create a sequence that the RNN
    model will take as input. Let's now perform an exercise to design a combination
    of CNN and RNN models.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，完全连接的层被应用到接收到的每个时间步。在我们的案例中，我们希望在将图像序列输入到 LSTM 模型之前，先对每个图像应用卷积层。为了构建这样的序列，我们需要将多个图像堆叠在一起，以便
    RNN 模型可以将其作为输入。现在，让我们进行一个练习，设计一个 CNN 和 RNN 模型的组合。
- en: 'Exercise 10.03: Designing a Combination of CNN and RNN Models with TensorFlow'
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 10.03：设计一个结合 CNN 和 RNN 模型的 TensorFlow 组合
- en: 'In this exercise, we will be designing a combination of CNN and RNN models
    with TensorFlow. This model will be used by our DRQN agent in *Activity 10.02,
    Training a DRQN to Play Breakout*, to play the game Breakout:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将设计一个结合了 CNN 和 RNN 的模型，该模型将被我们的 DRQN 代理用于 *活动 10.02，训练 DRQN 玩 Breakout*，以玩
    Breakout 游戏：
- en: 'Open a new Jupyter Notebook and import the `tensorflow` package:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter Notebook 并导入 `tensorflow` 包：
- en: '[PRE49]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Import the `Sequential` class from `tensorflow.keras.models`:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `tensorflow.keras.models` 导入 `Sequential` 类：
- en: '[PRE50]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Instantiate a `sequential` model and save it to a variable called `model`:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个 `sequential` 模型，并将其保存到名为 `model` 的变量中：
- en: '[PRE51]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Import the `Conv2D` class from `tensorflow.keras.layers`:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `tensorflow.keras.layers` 导入 `Conv2D` 类：
- en: '[PRE52]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Instantiate a convolutional layer with `Conv2D` with `32` filters of size `8`,
    a stride of `4` by `4`, and `relu` as the activation function. Save it to a variable
    called `conv1`:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `Conv2D` 实例化一个卷积层，该层具有 `32` 个大小为 `8` 的滤波器，步长为 `4` x `4`，激活函数为 `relu`。并将其保存到名为
    `conv1` 的变量中：
- en: '[PRE53]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Instantiate a second convolutional layer with `Conv2D` with `64` filters of
    size `4`, a stride of `2` by `2`, and `relu` as the activation function. Save
    it to a variable called `conv2`:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `Conv2D` 实例化第二个卷积层，该层具有 `64` 个大小为 `4` 的滤波器，步长为 `2` x `2`，激活函数为 `relu`。并将其保存到名为
    `conv2` 的变量中：
- en: '[PRE54]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Instantiate a third convolutional layer with `Conv2D` with `64` filters of
    size `3`, a stride of `1` by `1`, and `relu` as the activation function. Save
    it to a variable called `conv3`:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `Conv2D` 实例化第三个卷积层，该层具有 `64` 个大小为 `3` 的滤波器，步长为 `1` x `1`，激活函数为 `relu`。并将其保存到名为
    `conv3` 的变量中：
- en: '[PRE55]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Import the `TimeDistributed` class from `tensorflow.keras.layers`:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `tensorflow.keras.layers` 导入 `TimeDistributed` 类：
- en: '[PRE56]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Instantiate a time-distributed layer that will take `conv1` as the input and
    (`4`, `84`, `84`, `1`) as the input shape. Save it to a variable called `time_conv1`:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个时间分布层，该层将 `conv1` 作为输入，输入形状为 (`4`, `84`, `84`, `1`)。并将其保存到一个名为 `time_conv1`
    的变量中：
- en: '[PRE57]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Instantiate a second time-distributed layer that will take `conv2` as the input.
    Save it to a variable called `time_conv2`:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化第二个时间分布层，该层将 `conv2` 作为输入，并将其保存到名为 `time_conv2` 的变量中：
- en: '[PRE58]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Instantiate a third time-distributed layer that will take `conv3` as the input.
    Save it to a variable called `time_conv3`:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化第三个时间分布层，该层将 `conv3` 作为输入，并将其保存到名为 `time_conv3` 的变量中：
- en: '[PRE59]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Add the three time-distributed layers to the model using the `add()` method:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `add()` 方法将三个时间分布层添加到模型中：
- en: '[PRE60]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Import the `Flatten` class from `tensorflow.keras.layers`:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `tensorflow.keras.layers` 导入 `Flatten` 类：
- en: '[PRE61]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Instantiate a time-distributed layer that will take a `Flatten()` layer as
    input. Save it to a variable called `time_flatten`:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个时间分布层，该层将 `Flatten()` 层作为输入，并将其保存到名为 `time_flatten` 的变量中：
- en: '[PRE62]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Add the `time_flatten` layer to the model with the `add()` method:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `add()` 方法将 `time_flatten` 层添加到模型中：
- en: '[PRE63]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Import the `LSTM` class from `tensorflow.keras.layers`:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `tensorflow.keras.layers` 导入 `LSTM` 类：
- en: '[PRE64]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Instantiate an LSTM layer with `512` units. Save it to a variable called `lstm`:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个具有 `512` 单元的 LSTM 层，并将其保存到名为 `lstm` 的变量中：
- en: '[PRE65]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Add the LSTM layer to the model with the `add()` method:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `add()` 方法将 LSTM 层添加到模型中：
- en: '[PRE66]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Import the `Dense` class from `tensorflow.keras.layers`:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `tensorflow.keras.layers` 导入 `Dense` 类：
- en: '[PRE67]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Instantiate a fully connected layer with `128` units and `relu` as the activation function:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个包含 `128` 个单元且激活函数为 `relu` 的全连接层：
- en: '[PRE68]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Instantiate a fully connected layer with `4` units:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `4` 个单元实例化一个全连接层：
- en: '[PRE69]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Add the two fully connected layers to the model with the `add()` method:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `add()` 方法将两个全连接层添加到模型中：
- en: '[PRE70]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Import the `RMSprop` class from `tensorflow.keras.optimizers`:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `tensorflow.keras.optimizers` 导入 `RMSprop` 类：
- en: '[PRE71]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Instantiate `RMSprop` with `0.00025` as the learning rate:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用学习率为 `0.00025` 的 `RMSprop` 实例：
- en: '[PRE72]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Compile the model by specifying `mse` as the loss function, `RMSprop` as the
    optimizer, and `accuracy` as the metric to be displayed during training to the
    `compile` method:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在 `compile` 方法中指定 `mse` 作为损失函数，`RMSprop` 作为优化器，以及 `accuracy` 作为在训练期间显示的度量，来编译模型：
- en: '[PRE73]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Print a summary of the model using the `summary` method:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `summary` 方法打印模型摘要：
- en: '[PRE74]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Following is the output of the code:'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是代码输出：
- en: '![Figure 10.8: Summary of the CNN+RNN model'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 10.8：CNN+RNN 模型摘要'
- en: '](img/B16182_10_08.jpg)'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_10_08.jpg)'
- en: 'Figure 10.8: Summary of the CNN+RNN model'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8：CNN+RNN 模型摘要
- en: We have successfully combined a CNN model with an RNN model. The preceding output
    shows the architecture of the model we just built with the different layers and
    the number of parameters that will be used during training. This model takes as
    input a sequence of four images and passes it to the RNN, which will analyze their
    relationship before feeding the results to the fully connected layers, which will
    be responsible for predicting the Q-values.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功地将 CNN 模型与 RNN 模型结合。前面的输出展示了我们刚刚构建的模型架构，其中包含不同的层和在训练过程中使用的参数数量。该模型以四张图像的序列作为输入，并将其传递给
    RNN，RNN 会分析它们之间的关系，然后将结果传递给全连接层，全连接层将负责预测 Q 值。
- en: Note
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2UDB3h4](https://packt.live/2UDB3h4).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看该部分的源代码，请访问 [https://packt.live/2UDB3h4](https://packt.live/2UDB3h4)。
- en: You can also run this example online at [https://packt.live/3dVrf9T](https://packt.live/3dVrf9T).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，访问 [https://packt.live/3dVrf9T](https://packt.live/3dVrf9T)。
- en: Now that we know how to build an RNN, we can combine this technique with a DQN
    model. This kind of model is called a DRQN, and this is what we are going to look
    at in the next section.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何构建一个 RNN，我们可以将这个技术与 DQN 模型结合。这样的模型被称为 DRQN，我们将在下一节中探讨这个模型。
- en: Building a DRQN
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 DRQN
- en: 'A DQN can benefit greatly from RNN models facilitating the processing of sequential
    images. Such an architecture is known as **Deep Recurrent Q Network** (**DRQN**).
    Combining a GRU or LSTM model with a CNN model will allow the reinforcement learning
    agent to understand the movement of the ball. To do so, we just need to add an
    LSTM (or GRU) layer between the convolutional and fully connected layers, as shown
    in the following figure:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 可以从 RNN 模型中受益，RNN 可以帮助处理序列图像。这种架构被称为 **深度递归 Q 网络** (**DRQN**)。将 GRU 或 LSTM
    模型与 CNN 模型结合，将使强化学习代理能够理解球的运动。为了实现这一点，我们只需在卷积层和全连接层之间添加一个 LSTM（或 GRU）层，如下图所示：
- en: '![Figure 10.9: DRQN architecture'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.9：DRQN 架构'
- en: '](img/B16182_10_09.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_10_09.jpg)'
- en: 'Figure 10.9: DRQN architecture'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9：DRQN 架构
- en: To feed the RNN model with a sequence of images, we need to stack several images
    together. For the Breakout game, after initializing the environment, we will need
    to take the first image and duplicate it several times in order to have the first
    initial sequence of images. Having done this, after each action, we can append
    the latest image to the sequence and remove the oldest one in order to maintain
    the exact same size of sequence (for instance, a sequence of a maximum of four
    images).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将图像序列输入到 RNN 模型中，我们需要将多张图像堆叠在一起。对于 Breakout 游戏，在初始化环境之后，我们需要获取第一张图像并将其复制多次，以形成第一组初始图像序列。完成后，在每次动作后，我们可以将最新的图像附加到序列中，并移除最旧的图像，从而保持序列大小不变（例如，最大四张图像的序列）。
- en: 'Activity 10.02: Training a DRQN to Play Breakout'
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 10.02：训练 DRQN 玩 Breakout 游戏
- en: 'In this activity, we will build a DRQN model by replacing the DQN model from
    *Activity 10.01*, *Training a DQN with CNNs to Play Breakout*. We will then train
    the DRQN model to play the Breakout game and analyze the performance of the agent.
    The following instructions will help you to complete this activity:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将通过替换*活动10.01*中的DQN模型来构建一个DRQN模型，*使用CNN训练DQN玩Breakout游戏*。然后，我们将训练DRQN模型来玩Breakout游戏，并分析智能体的性能。以下说明将帮助您完成本活动：
- en: Import the relevant packages (`gym`, `tensorflow`, `numpy`).
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的包（`gym`、`tensorflow`、`numpy`）。
- en: Reshape the training and test sets.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重塑训练集和测试集。
- en: 'Create the `DRQN` class with the following methods: the `build_model()` method
    to instantiate a CNN combined with an RNN model, the `get_action()` method to
    apply the epsilon-greedy algorithm to choose the action to be played, the `add_experience()`
    method to store in memory the experience acquired by playing the game, the `replay()`
    method, which will perform experience replay by sampling experiences from the
    memory and train the DRQN model with a callback to save the model every two episodes,
    and the `update_epsilon()` method to gradually decrease the epsilon value for
    epsilon-greedy.'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`DRQN`类，并包含以下方法：`build_model()`方法用于实例化一个结合CNN和RNN的模型，`get_action()`方法用于应用epsilon-greedy算法选择要执行的动作，`add_experience()`方法用于将游戏过程中获得的经验存储在记忆中，`replay()`方法通过从记忆中采样经验进行经验回放，并每两轮保存一次模型，`update_epsilon()`方法用于逐渐减少epsilon-greedy中的epsilon值。
- en: Use the `initialize_env()` function to train the agent, which will initialize
    the environment by returning the initial state, `False` for the done flag, and
    `0` as the initial reward.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`initialize_env()`函数来训练智能体，该函数通过返回初始状态、`False`的done标志和`0`作为初始奖励来初始化环境。
- en: 'Create a function called `preprocess_state()` that will perform the following
    preprocessing on an image: crop the image to remove unnecessary parts, convert
    to a grayscale image, and then resize the image to a square shape.'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`preprocess_state()`的函数，对图像进行以下预处理：裁剪图像以去除不必要的部分，将图像转换为灰度图像，然后将图像调整为方形。
- en: Create a function called `combine_images()` that will stack a sequence of images.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`combine_images()`的函数，用于堆叠一系列图像。
- en: Create a function called `play_game()` that will play a game until it is over,
    and then store the experience and the accumulated reward.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`play_game()`的函数，该函数将玩一局游戏直到结束，然后存储经验和累积奖励。
- en: Create a function called `train_agent()` that will iterate through a number
    of episodes where the agent will play a game and perform experience replay.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`train_agent()`的函数，该函数将通过多轮训练让智能体玩游戏并执行经验回放。
- en: Instantiate a Breakout environment and train a `DRQN` agent to play this game
    for `200` episodes.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个Breakout环境，并训练一个`DRQN`智能体进行`200`轮游戏。
- en: Note
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: We recommend training for 200 (or 400) episodes in order to train the models
    properly and achieve good performance, but this may take a few hours depending
    on the system configuration. Alternatively, you can reduce the number of episodes,
    which will reduce the training time but will impact the performance of the agent.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们建议训练200轮（或400轮），以便正确训练模型并获得良好的性能，但这可能需要几个小时，具体取决于系统配置。或者，您可以减少训练轮数，这会减少训练时间，但会影响智能体的性能。
- en: 'The expected output will be close to the one shown here. You may have slightly
    different values on account of the randomness of the game and the randomness of
    the epsilon-greedy algorithm in choosing the action to be played:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 预期的输出结果将接近此处显示的内容。由于游戏的随机性和epsilon-greedy算法在选择动作时的随机性，您可能会得到稍微不同的值：
- en: '[PRE75]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Note
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 756.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第756页找到。
- en: In the next section, we will see how we can improve the performance of our model
    by adding an attention mechanism to DRQN and building a DARQN model.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将看到如何通过将注意力机制添加到DRQN中来提高模型的性能，并构建DARQN模型。
- en: Introduction to the Attention Mechanism and DARQN
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**注意力机制和DARQN介绍**'
- en: In the previous section, we saw how adding an RNN model to a DQN helped to increase
    its performance. RNNs are known for handling sequential data such as temporal
    information. In our case, we used a combination of CNNs and RNNs to help our reinforcement
    learning agent to better understand sequences of images from the game.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分，我们看到将 RNN 模型添加到 DQN 中有助于提高其性能。RNN 因处理序列数据（如时间信息）而闻名。在我们的案例中，我们使用了 CNN
    和 RNN 的组合，帮助我们的强化学习智能体更好地理解来自游戏的图像序列。
- en: 'However, RNN models do have some limitations when it comes to analyzing long
    sequences of input or output data. To overcome this situation, researchers have
    come up with a technique called attention, which is the principal technique behind
    a **Deep Attention Recurrent Q-Network** (**DARQN**). The DARQN model is the same
    as the DRQN model, with just an attention mechanism added to it. To better understand
    this concept, we will go through an example of its application: neural translation.
    Neural translation is the field of translating text from one language to another,
    such as translating Shakespeare''s plays, which were written in English, into
    French.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，RNN 模型在分析长序列输入或输出数据时确实存在一些局限性。为了解决这一问题，研究人员提出了一种叫做注意力机制的技术，这也是**深度注意力递归 Q
    网络**（**DARQN**）的核心技术。DARQN 模型与 DRQN 模型相同，只是增加了一个注意力机制。为了更好地理解这个概念，我们将通过一个应用实例：神经翻译。神经翻译是将文本从一种语言翻译成另一种语言的领域，例如将莎士比亚的戏剧（原文为英语）翻译成法语。
- en: 'Sequence-to-sequence models are the best fit for such a task. They comprise
    two components: an encoder and a decoder. Both of them are RNN models, such as
    an LSTM or GRU model. The encoder is responsible for processing a sequence of
    words from the input data (in our previous example, this would be a sentence of
    English words) and generates an encoded version called the context vector. The
    decoder will take this context vector as input and will predict the relevant output
    sequence (a sentence of French words, in our example):'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列模型最适合此类任务。它们包括两个组件：编码器和解码器。它们都是 RNN 模型，如 LSTM 或 GRU 模型。编码器负责处理输入数据中的一系列词语（在我们之前的例子中，这将是一个英语单词的句子），并生成一个被称为上下文向量的编码版本。解码器将这个上下文向量作为输入，并预测相关的输出序列（在我们的例子中是法语单词的句子）：
- en: '![Figure 10.10: Sequence-to-sequence model'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.10: 序列到序列模型'
- en: '](img/B16182_10_10.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_10_10.jpg)'
- en: 'Figure 10.10: Sequence-to-sequence model'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10.10: 序列到序列模型'
- en: The size of the context vector is fixed. It is an encoded version of the input
    sequence with only the relevant information. You can think of it as a summary
    of the input data. However, the set size of this vector limits the model in terms
    of retaining sufficient relevant information from long sequences. It will tend
    to "forget" the earlier elements of a sequence. But in the case of translation,
    the beginning of a sentence usually contains very important information, such
    as its subject.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文向量的大小是固定的。它是输入序列的编码版本，只包含相关信息。你可以将它视为输入数据的总结。然而，这个向量的固定大小限制了模型从长序列中保留足够相关信息的能力。它往往会“遗忘”序列中的早期元素。但在翻译的情况下，句子的开头通常包含非常重要的信息，例如其主语。
- en: 'The attention mechanism not only provides the decoder with the context vector,
    but also the previous states of the encoder. This enables the decoder to find
    relevant relationships between previous states, the context vector, and the desired
    output. This will help in our example to understand the relationship between two
    elements that are far away from one another in the input sequence:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制不仅为解码器提供上下文向量，还提供编码器的前一个状态。这使得解码器能够找到前一个状态、上下文向量和所需输出之间的相关关系。在我们的例子中，这有助于理解输入序列中两个远离彼此的元素之间的关系：
- en: '![Figure 10.11: Sequence-to-sequence model with an attention mechanism'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.11: 带有注意力机制的序列到序列模型'
- en: '](img/B16182_10_11.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_10_11.jpg)'
- en: 'Figure 10.11: Sequence-to-sequence model with an attention mechanism'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10.11: 带有注意力机制的序列到序列模型'
- en: 'TensorFlow provides an `Attention` class. It takes as input a tensor of shape
    `[output, states]`. It is better to use it by using the functional API, where
    each layer acts as a function that takes inputs and provides outputs as results.
    In this case, we can simply extract the output and states from a GRU layer and
    provide them as inputs for the attention layer:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow提供了一个`Attention`类。它的输入是一个形状为`[output, states]`的张量。最好通过使用函数式API来使用它，其中每个层作为一个函数接受输入并提供输出结果。在这种情况下，我们可以简单地从GRU层提取输出和状态，并将它们作为输入提供给注意力层：
- en: '[PRE76]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: To build a DARQN model, we just need to add this attention mechanism to a DRQN model.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建DARQN模型，我们只需要将注意力机制添加到DRQN模型中。
- en: Let's add this attention mechanism to our previous DRQN agent (in *Activity
    10.02*, *Training a DRQN to Play Breakout*) and build a DARQN model in the next
    activity.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个注意力机制添加到我们之前的DRQN代理（在*活动10.02*，*训练DRQN玩Breakout*中），并在下一个活动中构建DARQN模型。
- en: 'Activity 10.03: Training a DARQN to Play Breakout'
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动10.03：训练DARQN玩Breakout
- en: 'In this activity, we will build a DARQN model by adding an attention mechanism
    to our previous DRQN from *Activity 10.02*, *Training a DRQN to Play Breakout*.
    We will then train the model to play the Breakout game and then analyze the performance
    of the agent. The following instructions will help you to complete this activity:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动中，我们将通过向之前的DRQN中添加一个注意力机制来构建DARQN模型（来自*活动10.02*，*训练DRQN玩Breakout*）。然后，我们将训练该模型来玩Breakout游戏，并分析代理的表现。以下说明将帮助你完成此活动：
- en: Import the relevant packages (`gym`, `tensorflow`, and `numpy`).
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的包（`gym`、`tensorflow`和`numpy`）。
- en: Reshape the training and test sets.
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重塑训练集和测试集。
- en: 'Create a `DARQN` class with the following methods: the `build_model()` method,
    which will instantiate a CNN combined with an RNN model (similar to *Exercise
    10.03*, *Designing a Combination of CNN and RNN Models with TensorFlow*); the
    `get_action()` method, which will apply the epsilon-greedy algorithm to choose
    the action to be played; the `add_experience()` method to store in memory the
    experience acquired by playing the game; the `replay()` method, which will perform
    experience replay by sampling experiences from the memory and train the DARQN
    model with a callback to save the model every two episodes; and the `update_epsilon()`
    method to gradually decrease the epsilon value for epsilon-greedy.'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`DARQN`类，包含以下方法：`build_model()`方法，它将实例化一个结合了CNN和RNN的模型（类似于*练习10.03*，*使用TensorFlow设计CNN和RNN模型的组合*）；`get_action()`方法，它将应用epsilon-greedy算法选择要执行的动作；`add_experience()`方法，用于将游戏中获得的经验存储到内存中；`replay()`方法，它将通过从内存中采样经验并训练DARQN模型来执行经验重放，并在每两次回合后保存模型；以及`update_epsilon()`方法，用于逐渐减少epsilon值以进行epsilon-greedy。
- en: Initialize the environment using the `initialize_env()` function by returning
    the initial state, `False` for the done flag, and `0` as the initial reward.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`initialize_env()`函数初始化环境，返回初始状态，`False`作为done标志，以及`0`作为初始奖励。
- en: 'Use the `preprocess_state()` function to perform the following preprocessing
    on an image: crop the image to remove unnecessary parts, convert to a grayscale
    image, and resize the image to a square shape.'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`preprocess_state()`函数对图像进行以下预处理：裁剪图像以去除不必要的部分，转换为灰度图像，并将图像调整为正方形。
- en: Create a function called `combine_images()` that will stack a sequence of images.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`combine_images()`的函数，用于堆叠一系列图像。
- en: Use the `play_game()` function to play a game until it is over, and then store
    the experience and the accumulated reward.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`play_game()`函数进行游戏直到结束，然后存储经验和累积奖励。
- en: Iterate through a number of episodes where the agent will play a game and perform
    experience replay using the `train_agent()` function.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过若干回合进行迭代，代理将进行游戏并使用`train_agent()`函数执行经验重放。
- en: Instantiate a Breakout environment and train a `DARQN` agent to play this game
    for `400` episodes.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个Breakout环境并训练一个`DARQN`代理玩这个游戏，共进行`400`个回合。
- en: Note
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: We recommend training for 400 episodes in order to properly train the model
    and achieve good performance, but this may take a few hours depending on the system
    configuration. Alternatively, you can reduce the number of episodes, which will
    reduce the training time but will impact the performance of the agent.
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们建议训练400个回合，以便正确训练模型并获得良好的性能，但这可能会根据系统配置花费几个小时。或者，你可以减少回合数，这将减少训练时间，但会影响代理的表现。
- en: 'The output will be close to what you see here. You may have slightly different
    values on account of the randomness of the game and the randomness of the epsilon-greedy
    algorithm in choosing the action to be played:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果将接近你看到的这里。由于游戏的随机性以及epsilon-greedy算法在选择行动时的随机性，你可能会看到略有不同的值：
- en: '[PRE77]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Note
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 761.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动的解答可以在第761页找到。
- en: Summary
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to combine deep learning techniques to a DQN
    model and train it to play the Atari game Breakout. We first looked at adding
    convolutional layers to the agent for processing screenshots from the game. This
    helped the agent to better understand the game environment.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何将深度学习技术与DQN模型相结合，并训练它来玩Atari游戏《Breakout》。我们首先探讨了如何为智能体添加卷积层，以处理来自游戏的截图。这帮助智能体更好地理解游戏环境。
- en: We then took things a step further and added an RNN to the outputs of the CNN
    model. We created a sequence of images and fed it to an LSTM layer. This sequential
    model provided the DQN agent with the ability to "visualize" the direction of
    the ball. This kind of model is called a DRQN.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步改进了模型，在CNN模型的输出上添加了一个RNN。我们创建了一系列图像并将其输入到LSTM层。这种顺序模型使得DQN智能体能够“可视化”球的方向。这种模型被称为DRQN。
- en: Finally, we used an attention mechanism and trained a DARQN model to play the
    Breakout game. This mechanism helped the model to better understand previous relevant
    states and improved its performance drastically. This field is still evolving
    as new deep learning techniques and models are designed, outperforming previous
    generations in the process.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用了注意力机制并训练了一个DARQN模型来玩《Breakout》游戏。该机制帮助模型更好地理解之前相关的状态，并显著提高了其表现。随着新的深度学习技术和模型的设计，该领域仍在不断发展，这些新技术在不断超越上一代模型的表现。
- en: In the next chapter, you will be introduced to policy-based methods and the
    actor-critic model, which consists of multiple models responsible for computing
    an action based on a state and calculating the Q-values.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，你将接触到基于策略的方法和演员-评论员模型，该模型由多个子模型组成，负责根据状态计算行动并计算Q值。
