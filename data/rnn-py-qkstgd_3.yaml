- en: Generating Your Own Book Chapter
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成你自己的书籍章节
- en: In this chapter, we will take a step further into exploring the TensorFlow library
    and how it can be leveraged to solve complex tasks. In particular, you will build
    a neural network that generates a new (non-existing) chapter of a book by learning
    patterns from the existing chapters. In addition, you will grasp more of the TensorFlow
    functionalities, such as saving/restoring a model, and so on.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将进一步探索TensorFlow库，并了解如何利用它来解决复杂的任务。特别是，你将构建一个神经网络，通过学习现有章节中的模式，生成一本书的新（不存在的）章节。此外，你还将掌握更多TensorFlow的功能，例如保存/恢复模型等。
- en: This chapter will also introduce a new and more powerful recurrent neural network
    model called the **gated recurrent unit** (**GRU**). You will learn how it works
    and why we are choosing it over the simple RNN.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还将介绍一个新的、更强大的递归神经网络模型——**门控递归单元（GRU）**。你将了解它的工作原理，以及为什么我们选择它而不是简单的RNN。
- en: 'In summary, the topics of the chapter include the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本章节的主题包括以下内容：
- en: Why use the GRU network? You will learn how the GRU network works, what problems
    it solves, and what its benefits are.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么使用GRU网络？你将了解GRU网络是如何工作的，它解决了哪些问题，以及它的优势是什么。
- en: Generating your book chapter—you will go step by step over the process of generating
    a book chapter. This includes collecting and formatting the training data, building
    the TensorFlow graph of the GRU model, training the network and, finally, generating
    the text word by word.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成书籍章节—你将一步一步地了解生成书籍章节的过程。这包括收集和格式化训练数据，构建GRU模型的TensorFlow图，训练网络，最后逐字生成文本。
- en: By the end of the chapter, you should have gained both a theoretical and a practical
    knowledge that will give you the freedom to experiment with any problems of medium
    difficulty.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到章节结束时，你应该已经获得了理论和实践知识，这将使你能够自由地实验解决中等难度的问题。
- en: Why use the GRU network?
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么使用GRU网络？
- en: In recent years, the recurrent neural network model has presented fascinating
    results which can even be seen in real-life applications like language translation,
    speech synthesis and more. A phenomenal application of GRUs happens to be text
    generation. With the current state-of-the-art models, we can see results which,
    a decade ago, were just a dream. If you want to truly appreciate these results,
    I strongly recommend you read Andrej Karpathy's article on *The* *Unreasonable
    Effectiveness of Recurrent Neural Networks* ([http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，递归神经网络模型呈现出令人着迷的成果，这些成果甚至可以在实际应用中看到，如语言翻译、语音合成等。GRU的一个非凡应用就是文本生成。通过当前最先进的模型，我们可以看到十年前只存在于梦想中的结果。如果你想真正欣赏这些成果，我强烈建议你阅读Andrej
    Karpathy的文章《*递归神经网络的非理性有效性*》([http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/))。
- en: Having said that, we can introduce the **Gated Recurrent Unit (GRU)** as a model
    which sits behind these exceptional outcomes. Another model of that kind is the
    **Long Short-Term Memory** (**LSTM**) which is slightly more advanced. Both architectures
    aim to solve the vanishing gradient problem—a major issue with the simple RNN
    model. If you recall from [Chapter 1](d6266376-9b8b-4d69-925b-a4e56307951b.xhtml), *Introducing
    Recurrent Neural Networks*, the problem represents the network's inability to
    learn long-distance dependencies and, thus, it cannot make accurate predictions
    on complex tasks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们可以介绍**门控递归单元（GRU）**，它是这些卓越成果背后的模型。另一个类似的模型是**长短期记忆网络**（**LSTM**），它稍微先进一些。这两种架构都旨在解决消失梯度问题——这是简单RNN模型的一个主要问题。如果你还记得[第1章](d6266376-9b8b-4d69-925b-a4e56307951b.xhtml)，*介绍递归神经网络*，这个问题代表着网络无法学习长距离的依赖关系，因此它无法对复杂任务做出准确预测。
- en: Both the GRU and LSTM deal with that problem using, so-called, gates. These
    gates decide what information to erase or propagate towards the prediction.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: GRU和LSTM都通过所谓的“门”来处理这个问题。这些门决定了哪些信息需要被抹去或传递到预测中。
- en: We will first focus on the GRU model since it is simpler and easier to understand
    and, then, you will have the chance to explore the LSTM model in the upcoming
    chapters.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先关注GRU模型，因为它更简单、更容易理解，之后，你将有机会在接下来的章节中探索LSTM模型。
- en: 'As mentioned above, the GRU''s main objective is to yield excellent results
    on long sequences. It achieves this by modifying the standard RNN cell with the
    introduction of update and reset gates. This network works the same way as a normal
    RNN model in terms of inputs, memory states and outputs. The key difference lies
    in the specifics of the cell at each time step. You will understand that better
    by using the following graph:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，GRU的主要目标是对长序列产生优异的结果。它通过引入更新门和重置门来修改标准RNN单元，以实现这一目标。就输入、记忆状态和输出而言，这个网络与普通RNN模型的工作方式相同。关键的不同在于每个时间步长内单元的具体细节。通过下图，你将更好地理解这一点：
- en: '![](img/34bf213e-0f04-4a82-8680-d019e77a8248.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/34bf213e-0f04-4a82-8680-d019e77a8248.png)'
- en: 'These are the notations for the preceding graph:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述图表的符号说明：
- en: '![](img/3367b79b-0983-499a-bd31-dcf67a4b1300.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3367b79b-0983-499a-bd31-dcf67a4b1300.png)'
- en: The illustration presents a single GRU cell. The cell accepts ![](img/376c7374-119e-4882-8e0e-e6edde065f0d.png) and ![](img/58fd635b-13c4-4f8c-8847-f8d078355acd.png) as
    inputs where ![](img/ea13db01-634b-4c38-8897-72dda90f8db7.png) is a vector representation
    of the input word at time step, ![](img/a633c7ef-850b-49e5-9f1e-c34302133b36.png) and ![](img/dea74485-61fb-4fe2-ab27-d5404c1f9936.png) is
    the memory state from the previous step *t-1*. Furthermore, the cell outputs the
    calculated memory state of the current step t. If you recall from before, the
    aim of this intermediate memory state is to pass information through all time
    steps and keep or discard knowledge. The preceding process should already be familiar
    to you from the RNN explanation in [Chapter 1](d6266376-9b8b-4d69-925b-a4e56307951b.xhtml),
    *Introducing Recurrent Neural Networks*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 插图展示了一个单一的GRU单元。该单元接收 ![](img/376c7374-119e-4882-8e0e-e6edde065f0d.png) 和 ![](img/58fd635b-13c4-4f8c-8847-f8d078355acd.png) 作为输入，其中 ![](img/ea13db01-634b-4c38-8897-72dda90f8db7.png) 是输入词汇在时间步长上的向量表示， ![](img/a633c7ef-850b-49e5-9f1e-c34302133b36.png) 和 ![](img/dea74485-61fb-4fe2-ab27-d5404c1f9936.png) 是来自上一步*
    t-1 *的记忆状态。此外，单元输出当前时间步长t的计算记忆状态。如果你还记得之前的内容，记忆状态的作用是通过所有时间步传递信息，并决定是否保留或丢弃知识。前述过程你应该已经在[第1章](d6266376-9b8b-4d69-925b-a4e56307951b.xhtml)《递归神经网络介绍》中有所了解。
- en: 'The new and interesting thing is what happens inside this GRU cell. The calculations
    aim to decide what information from ![](img/376c7374-119e-4882-8e0e-e6edde065f0d.png) and ![](img/58fd635b-13c4-4f8c-8847-f8d078355acd.png) should
    be passed forward or eliminated. That decision-making process is handled by the
    following set of equations:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 新颖和有趣的部分是GRU单元内部发生了什么。计算的目标是决定从 ![](img/376c7374-119e-4882-8e0e-e6edde065f0d.png) 和 ![](img/58fd635b-13c4-4f8c-8847-f8d078355acd.png) 中哪些信息应该被传递或删除。这个决策过程由以下一组方程式来处理：
- en: '![](img/574c4ece-0b28-4c0b-bb8d-8cdda14722ee.png)![](img/5732ef56-46ed-4801-bf9f-72e1625a47be.png)![](img/04af8703-339a-4251-b772-b6834530e305.png)![](img/f705c042-f50c-4db3-b991-888eb786689f.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/574c4ece-0b28-4c0b-bb8d-8cdda14722ee.png)![](img/5732ef56-46ed-4801-bf9f-72e1625a47be.png)![](img/04af8703-339a-4251-b772-b6834530e305.png)![](img/f705c042-f50c-4db3-b991-888eb786689f.png)'
- en: 'The first equation presents the update gate. Its purpose is to determine how
    much of the past information should be propagated in the future. To do that, first
    we multiple the input ![](img/727a3c6c-9f10-4487-8068-c6616e116751.png) with its
    own weight ![](img/5df3e75c-160a-493a-969b-a8e051e64588.png) and then sum the
    result with the other multiplication between the memory state from the last step
    ![](img/8343eb22-4e2c-4f1c-8afc-02d384a7635b.png) and its weight ![](img/300602e0-d0f3-47ab-9aa6-6cb1cd548489.png).
    The exact values of these weights are determined during training. This is shown
    in the following screenshot:'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个方程表示更新门。它的目的是决定过去的信息应当传递多少到未来。为此，首先我们将输入 ![](img/727a3c6c-9f10-4487-8068-c6616e116751.png) 与其自身的权重 ![](img/5df3e75c-160a-493a-969b-a8e051e64588.png) 相乘，然后将结果与上一步的记忆状态 ![](img/8343eb22-4e2c-4f1c-8afc-02d384a7635b.png) 和其权重 ![](img/300602e0-d0f3-47ab-9aa6-6cb1cd548489.png) 相乘的结果相加。此权重的具体值是在训练过程中确定的。如下截图所示：
- en: '![](img/4b067bd3-8ad5-4fbc-902f-37413a26239b.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b067bd3-8ad5-4fbc-902f-37413a26239b.png)'
- en: 'The second equation presents the reset gate. As the name states, this gate
    is used to decide how much of the past information should be omitted. Again, using 
     ![](img/be09906d-c9b4-49df-8dfe-4c378ca26248.png)  and  ![](img/ca4e3868-54fe-4ed0-8c05-d8d5df3b458e.png) 
    we calculate its value. The difference is that instead of using the same weights,
    our network learns a different set of weights—![](img/2bbec3e4-f7cb-4176-b093-8a9aa1ce2203.png) 
    and   ![](img/bb7dc14b-2f49-4a2c-a580-a37ec718e6b2.png). This is shown in the
    following screenshot:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个方程介绍了重置门。顾名思义，这个门用于决定应该丢弃多少过去的信息。同样，使用 ![](img/be09906d-c9b4-49df-8dfe-4c378ca26248.png) 和 ![](img/ca4e3868-54fe-4ed0-8c05-d8d5df3b458e.png) 来计算它的值。不同之处在于，我们的网络不是使用相同的权重，而是学习了一组不同的权重——![](img/2bbec3e4-f7cb-4176-b093-8a9aa1ce2203.png) 和 ![](img/bb7dc14b-2f49-4a2c-a580-a37ec718e6b2.png)。这在下面的截图中有展示：
- en: '![](img/f9b70fa5-9fe4-4e28-a940-3e5bdc7d1a57.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9b70fa5-9fe4-4e28-a940-3e5bdc7d1a57.png)'
- en: 'Both the update and reset gate us the sigmoid as a final step when producing
    the value. If you recall from [Chapter 1](d6266376-9b8b-4d69-925b-a4e56307951b.xhtml), *Introducing
    Recurrent Neural Networks*, the sigmoid ([https://www.youtube.com/watch?v=WcDtwxi7Ick&t=3s](https://www.youtube.com/watch?v=WcDtwxi7Ick&t=3s))
    is a type of activation function which squashes the input between `0` and `1`:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 更新门和重置门在生成值时，都会使用 sigmoid 作为最终步骤。如果你还记得[第一章](d6266376-9b8b-4d69-925b-a4e56307951b.xhtml)《介绍递归神经网络》中的内容，sigmoid
    激活函数（[https://www.youtube.com/watch?v=WcDtwxi7Ick&t=3s](https://www.youtube.com/watch?v=WcDtwxi7Ick&t=3s)）是一种激活函数，它将输入值压缩在
    `0` 和 `1` 之间：
- en: The third equation is a temporary internal memory state which uses the input ![](img/60f41a34-e946-43d1-94b4-82ee799159a1.png) and
    the reset gate ![](img/d5b59ad5-b909-4204-ba05-d16c4cddb85b.png) to store the
    relevant information from the past. Here we use a *tanh* activation function which
    is similar to a sigmoid, but instead squashes the output between `-1` and `1`.
    Here ([https://stats.stackexchange.com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function](https://stats.stackexchange.com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function))
    is a good explanation of the difference between both activations. As you can see,
    we use a different notation ![](img/9b13810b-264c-4404-b677-1eb4d81018d6.png) called
    element-wise or Hadamard multiplication ([https://www.youtube.com/watch?v=2GPZlRVhQWY](https://www.youtube.com/watch?v=2GPZlRVhQWY)).
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个方程是一个临时的内部记忆状态，它使用输入 ![](img/60f41a34-e946-43d1-94b4-82ee799159a1.png) 和重置门 ![](img/d5b59ad5-b909-4204-ba05-d16c4cddb85b.png) 来存储来自过去的相关信息。在这里我们使用的是
    *tanh* 激活函数，它类似于 sigmoid，但不同之处在于，它将输出压缩在 `-1` 和 `1` 之间。这里的 ([https://stats.stackexchange.com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function](https://stats.stackexchange.com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function))
    是对两者激活函数区别的一个很好解释。如你所见，我们使用了不同的符号 ![](img/9b13810b-264c-4404-b677-1eb4d81018d6.png)，这叫做逐元素（element-wise）或
    Hadamard 乘法 ([https://www.youtube.com/watch?v=2GPZlRVhQWY](https://www.youtube.com/watch?v=2GPZlRVhQWY))。
- en: 'If you have the vectors `[1, 2, 3]` and `[0, -1, 4]` the Hadamard product will
    be `[1*0, 2*(-1), 3*4] = [0, -2, 12]`. This is shown in the following screenshot:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有两个向量 `[1, 2, 3]` 和 `[0, -1, 4]`，那么 Hadamard 乘积就是 `[1*0, 2*(-1), 3*4] = [0,
    -2, 12]`。这在下面的截图中有展示：
- en: '![](img/135941cc-6c9a-490d-b046-afa3d9258010.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/135941cc-6c9a-490d-b046-afa3d9258010.png)'
- en: 'The final equation calculates memory state  ![](img/5784b329-ace9-438e-ac39-d9393cc0306b.png) at
    the current time step t. To do this, we use the temporary internal memory state ![](img/648f9972-eec8-444f-a618-5e19d4321085.png) ,
    the previous memory state  ![](img/d87bf251-5cbd-4639-8587-ecfb7db7213a.png) and
    the update gate ![](img/2ea4af13-4ae7-4f8a-8d65-b43dce7c5b3b.png). Again, we are
    using the element-wise multiplication which makes the update gate decide how much
    information to propagate forward. Let''s illustrate this with an example:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终方程计算当前时间步 t 的记忆状态 ![](img/5784b329-ace9-438e-ac39-d9393cc0306b.png)。为此，我们使用临时内部记忆状态 ![](img/648f9972-eec8-444f-a618-5e19d4321085.png)，前一时刻的记忆状态 ![](img/d87bf251-5cbd-4639-8587-ecfb7db7213a.png) 和更新门 ![](img/2ea4af13-4ae7-4f8a-8d65-b43dce7c5b3b.png)。同样，我们使用逐元素乘法来决定更新门要传播多少信息。我们通过一个例子来说明：
- en: 'Imagine you want to do sentiment analysis on a book review to determine how
    people feel about a certain book. Let''s say that the review starts like this:
    *The book was super exciting and I liked it a lot. It reveals the story of a young
    woman...*. Here we want to keep the first part of the review until the end, so
    that we make an accurate prediction. In that case, the network will learn to make ![](img/8797dd6d-8b8f-4a0d-934c-a577a9082175.png) close
    to 1, so that ![](img/be8b30c1-a821-4e1a-80ad-d5934acd0632.png) is close to 0\.
    This way all future memory states will hold mostly information about this first
    part (*The book was super exciting and I liked it a lot.*) and won''t take into
    account any irrelevant information that comes next.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想对一本书的评论进行情感分析，以确定人们对这本书的感受。假设评论开始是这样的：*这本书非常激动人心，我非常喜欢。它揭示了一个年轻女性的故事……*。在这里，我们希望保留评论的前半部分，直到结尾，以便做出准确的预测。在这种情况下，网络将学会将 ![](img/8797dd6d-8b8f-4a0d-934c-a577a9082175.png) 接近
    1，这样 ![](img/be8b30c1-a821-4e1a-80ad-d5934acd0632.png) 就会接近 0。这样，所有未来的记忆状态将主要保留关于这部分信息（*这本书非常激动人心，我非常喜欢。*），而不会考虑接下来的任何无关信息。
- en: Combining the above equations results in a powerful model, which can learn to
    keep full or partial information at any step, and enhance the final prediction.
    You can easily see how this solution solves the vanishing gradient problem by
    letting the network (based on the weights) decide what should influence the predictions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 将上述方程结合起来，结果是一个强大的模型，它能够学习在任何步骤保持完整或部分信息，从而增强最终的预测。你可以很容易地看到，这个解决方案是如何通过让网络（根据权重）决定什么应该影响预测，来解决梯度消失问题的。
- en: Generating your book chapter
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成你的书籍章节
- en: 'After going through the theoretical part of this chapter, we are ready to dive
    into coding. I hope you grasp the fundamental behind the GRU model and will feel
    comfortable seeing the notations in the TensorFlow program. It consists of five
    parts, most of which may be familiar to you from [Chapter 2](c57a4667-7e5a-49ad-9cab-ad32989a5878.xhtml),
    *Building Your First RNN with TensorFlow*:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成了本章的理论部分后，我们准备进入编码部分。我希望你能掌握 GRU 模型的基本原理，并且在看到 TensorFlow 程序中的符号时能感到轻松。它由五个部分组成，其中大部分内容你应该在[第2章](c57a4667-7e5a-49ad-9cab-ad32989a5878.xhtml)，*用
    TensorFlow 构建你的第一个 RNN*中有所接触：
- en: '**Obtaining the book text**: this one is really straightforward. Your task
    is to assure a lot of plain text is ready for training.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**获取书籍文本**：这一部分非常直接。你的任务是确保有大量的纯文本为训练做好准备。'
- en: '**Encoding the text**: this one can be challenging, since we need to accommodate
    the encoding with the proper dimensions. Sometimes, this operation can take more
    time than expected but it is a requirement for compiling the program flawlessly.
    There are different types of encoding algorithms and we will choose a fairly simple
    one so you fully understand its true essence.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码文本**：这一部分可能比较有挑战性，因为我们需要将编码与适当的维度进行对接。有时候，这个操作可能会比预期花费更多时间，但它是完美编译程序的必要条件。编码算法有很多种，我们将选择一个相对简单的算法，这样你就能完全理解它的真正含义。'
- en: '**Building the TensorFlow graph**: this operation should be familiar to you
    from [Chapter 2](c57a4667-7e5a-49ad-9cab-ad32989a5878.xhtml), *Building Your First
    RNN with TensorFlo*w. We will use similar steps with the difference that now the
    operational cell is a GRU instead of a normal RNN.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建 TensorFlow 图**：这一步你应该从[第2章](c57a4667-7e5a-49ad-9cab-ad32989a5878.xhtml)，*用
    TensorFlow 构建你的第一个 RNN*中有所了解。我们将使用类似的步骤，唯一的区别是现在操作单元是 GRU，而不是普通的 RNN。'
- en: '**Training the network**: this step should also be familiar to you from [Chapter
    2](c57a4667-7e5a-49ad-9cab-ad32989a5878.xhtml), *Building Your First RNN with
    TensorFlow*. We will again use batches to make our training faster and occupy
    less memory.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练网络**：这一步你应该从[第2章](c57a4667-7e5a-49ad-9cab-ad32989a5878.xhtml)，*用 TensorFlow
    构建你的第一个 RNN*中有所了解。我们将再次使用批处理来加速训练并减少内存占用。'
- en: '**Generating your new text**: this is the new and unique step in our program.
    We will use the already trained weights and biases to predict the sequences of
    words. Using appropriate hyperparameters with a large set of data can yield understandable
    paragraphs which one can easily assume are real.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成你的新文本**：这是我们程序中的新步骤，也是独特的步骤。我们将使用已经训练好的权重和偏置来预测词语序列。使用适当的超参数和大量数据集，可以生成可以理解的段落，读者很容易认为这些段落是真实的。'
- en: 'You will be writing the code in a new file called `ch3_task.py`. First, install
    the Python libraries using the following code:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在一个新的文件中编写代码，文件名为 `ch3_task.py`。首先，使用以下代码安装 Python 库：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, open `ch3_task.py` and import the preceding libraries, as shown in the
    following code:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，打开`ch3_task.py`并导入之前的库，如下所示：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now it is time to explore the steps.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是探索步骤的时候了。
- en: Obtaining the book text
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取书籍文本
- en: The first step in building any machine learning task is to obtain the data.
    In a professional environment you would divide it into training, validation and
    testing data. Normally the distribution is 60%, 20%, 20% People often confuse
    validation with test data or even omit using the former. The validation data is
    used to evaluate the model while tuning the hyperparameters. In contrast, the
    test data is used only to give an overall evaluation of the model. You SHOULD
    NOT use the test data to make changes on your model. Since the task is to generate
    text, our data will be used only for training. Then, we can leverage the model
    to guess words one by one.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 构建任何机器学习任务的第一步是获取数据。在专业环境中，通常会将数据划分为训练、验证和测试数据。通常的分配比例为 60%、20%、20%。人们常常将验证数据与测试数据混淆，甚至忽略使用前者。验证数据用于在调整超参数时评估模型。相比之下，测试数据仅用于对模型进行总体评估。你不应该使用测试数据来对模型进行调整。由于该任务是生成文本，我们的数据将仅用于训练。然后，我们可以利用该模型逐个猜测单词。
- en: Our aim is to yield a meaningful new chapter based on the *The Hunger Games*
    books. We should store the text in a new file called `the_hunger_games.txt`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是根据*饥饿游戏*书籍生成有意义的新章节。我们应该将文本存储在一个名为`the_hunger_games.txt`的新文件中。
- en: 'First, we need to build our dictionary using that file. This will happen using
    the two functions called `get_words(file_name)` and `build_dictionary(words)`as
    shown in the following example:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要使用该文件构建我们的字典。这将通过两个名为`get_words(file_name)`和`build_dictionary(words)`的函数完成，如下所示：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The previous function aims to create a list of all the words in `the_hunger_games.txt`.
    Now let''s build the actual dictionary using the following code:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个函数的目的是创建`the_hunger_games.txt`中所有单词的列表。现在，让我们使用以下代码构建实际的字典：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here we use the Python built-in library collections. It can easily create a
    list of tuples where each tuple is formed of a string (word) and time of occurrences
    of this word in the list `words`. Thus, `most_common_words` does not contain any
    duplicate elements.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们使用 Python 内置库 collections。它可以轻松创建一个元组列表，其中每个元组由一个字符串（单词）和该单词在列表`words`中出现的次数组成。因此，`most_common_words`
    不包含任何重复元素。
- en: The dictionaries `word2id` and `id2word` associate a number with each word which
    ensures a straightforward access to all words.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 字典`word2id`和`id2word`将每个单词与一个数字关联，从而确保可以直接访问所有单词。
- en: 'Finally, we execute the `get_words()` and `build_dictionary()` functions, so
    that the words and dictionaries can be accessed globally, as shown in the following
    example:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们执行`get_words()`和`build_dictionary()`函数，以便可以全局访问单词和字典，如下所示：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Encoding the text
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码文本
- en: This part shows how to encode our dataset using the popular one-hot encoding.
    The reason behind this operation lies in the fact that any neural network operates
    using some sort of numerical representation of strings.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分展示了如何使用流行的独热编码对我们的数据集进行编码。进行此操作的原因在于，任何神经网络都通过某种数字表示来处理字符串。
- en: First, we declare `section_length = 20` which represents the length of a single
    section in our encoded dataset. This dataset is a collection of sections where
    each section has 20 one-hot encoded words.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们声明`section_length = 20`，表示我们编码数据集中单个片段的长度。该数据集是一个由多个片段组成的集合，每个片段包含 20 个独热编码的单词。
- en: Then, we store the sections of 20 words in the `input_values` array. The 21st
    word is used as the output value for that particular section. This means that,
    during training, the network learns that the words *I love reading non-fiction...I
    can find these type of* (example sequence of 20 words extracted from the training
    set) are followed by *book*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将 20 个单词的片段存储在`input_values`数组中。第 21 个单词作为该特定片段的输出值。这意味着，在训练过程中，网络会学习到像*我喜欢阅读非小说类书籍……我能找到这些类型的*（从训练集中提取的
    20 个单词示例序列）之后会出现*书籍*。
- en: After that, comes the one-hot encoding which is also quite straightforward.
    We create two arrays of zeros with dimensions `(num_sections, section_length,
    most_common_words_length)`—for the inputs and `(num_sections, most_common_words_length)`—for
    the outputs. We iterate over the `input_values` and find the index of each word
    in each section. Using these indices, we replace the values in the one-hot arrays
    with `1`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是独热编码（one-hot encoding），这也相当简单。我们创建两个零数组，维度为`(num_sections, section_length,
    most_common_words_length)`——用于输入，`(num_sections, most_common_words_length)`——用于输出。我们遍历`input_values`，并找到每个词在每个部分中的索引。使用这些索引，我们用`1`替换独热数组中的值。
- en: 'The code for this is in the following example:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的代码在以下示例中：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we store the encoded words in two global variables (we also use the
    parameter `words` from the previous part of that chapter), as shown in the following
    example:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将编码后的词汇存储在两个全局变量中（我们还使用了上一部分中的`words`参数），如下所示：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Building the TensorFlow graph
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建TensorFlow图
- en: This step builds the most fundamental part of our program—the neural network
    graph.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步构建了我们程序中最基本的部分——神经网络图。
- en: 'First, we start by initializing the hyperparameters of the model, as shown
    in the following example:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们开始初始化模型的超参数，如下例所示：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'One often experiments with the above values until the model receives decent
    results:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 人们通常会反复尝试上述值，直到模型得到较好的结果：
- en: The `learning_rate` ([https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10](https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10))
    is used in backpropagation and should have a fairly small value.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate` ([https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10](https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10))
    用于反向传播，通常应设置为一个较小的值。'
- en: The `batch_size` determines how many elements each batch should have. The data
    is often divided into batches so that training is faster and requires less memory.
    You will see more about the usage of batches later.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`决定了每个批次应包含多少个元素。数据通常被划分成批次，以便加速训练并减少内存需求。稍后你会了解更多批次使用的内容。'
- en: '`number_of_iterations` is how many training steps we should take. A training
    step includes picking one batch from the data and performing forward and then
    backward propagation, which updates the weights and biases.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`number_of_iterations`表示我们应该进行多少训练步骤。一个训练步骤包括从数据中选择一个批次，并进行前向传播和后向传播，这会更新权重和偏差。'
- en: '`number_hidden_units` ([https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell](https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell))
    is the number of units used in any RNN cell. There is actually a pretty neat formula
    ([https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-network](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-network)[)](https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell)
    which calculates that number based on the input and output neurons of the network.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`number_hidden_units` ([https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell](https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell))
    是每个RNN单元中使用的单元数。实际上，有一个非常巧妙的公式 ([https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-network](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-network)[)](https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell)，它根据网络的输入和输出神经元来计算这个数值。'
- en: 'After we have defined the above parameters, it is time to specify our graph.
    This is demonstrated in the following snippets of code:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了上述参数后，接下来是指定我们的图结构。这在以下代码片段中演示：
- en: 'We start with the TensorFlow placeholder X which holds the training data at
    that current batch, and Y—which holds the predicted data at that current batch.
    This is shown in the following code:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从TensorFlow的占位符X开始，它存储当前批次的训练数据，Y存储当前批次的预测数据。这在以下代码中展示：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we initialize our weights and biases using a normal distribution. The
    dimensions of weights is `[number_hidden_units, most_common_words_length]` which
    assures correct multiplication in our prediction. The same logic goes for biases
    with dimensions `[most_common_words_length]`. This is shown in the following example:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们使用正态分布初始化权重和偏置。权重的维度是`[number_hidden_units, most_common_words_length]`，这确保了我们预测中的正确乘法。同样的逻辑也适用于维度为`[most_common_words_length]`的偏置。以下示例展示了这一过程：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we specify the GRU cell. All the complex logic learned in the first section
    of that chapter is hidden behind the previous line of code. [Chapter 2](c57a4667-7e5a-49ad-9cab-ad32989a5878.xhtml), *Building
    Your First RNN with TensorFlow*, explained why we then pass the parameter `num_units`,
    which is shown in the following example:'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们指定GRU单元。第一部分章节中学到的所有复杂逻辑都隐藏在前一行代码背后。[第二章](c57a4667-7e5a-49ad-9cab-ad32989a5878.xhtml)，*使用TensorFlow构建你的第一个RNN*，解释了为什么我们随后传递参数`num_units`，如下示例所示：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Then, we calculate the outputs using the GRU cell and the inputs X. An important
    step is to transpose those outputs with [1, 0, 2] permutations.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们使用GRU单元和输入X来计算输出。一个重要步骤是对这些输出进行[1, 0, 2]的转置操作。
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let''s review the following illustration to understand how this `last_output` is
    obtained:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下示意图来理解如何获得这个`last_output`：
- en: '![](img/cf3fa029-3080-498d-adfc-4747035c567f.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf3fa029-3080-498d-adfc-4747035c567f.png)'
- en: The illustration shows how one input example of `section_length` steps is plugged
    into the network. This operation should be done `batch_size` times for each individual
    example having `section_length` steps, but, for the sake of simplicity, we are
    showing only one example.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 示意图展示了如何将一个包含`section_length`步的输入例子插入到网络中。这个操作应该对每个包含`section_length`步的单独例子执行`batch_size`次，但为了简化起见，我们这里只展示了一个例子。
- en: After iteratively going through each time step, we produce a `section_length` number
    of outputs, each one having the dimensions `[most_common_words_length, 1]`. So,
    for one example of the `section_length` input time steps, we produce `section_length`
    output steps. Presenting all outputs mathematically results in a `[batch_size,
    section_length, most_common_words_length]` matrix. The height of the matrix is
    `batch_size` - the number of individual examples in a single batch. The width
    of the matrix is `section_length` - the number of time steps for each example.
    The depth of the matrix is `most_common_words_length` - the dimension of each
    element.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代地经过每个时间步后，我们会生成一个`section_length`数量的输出，每个输出的维度为`[most_common_words_length,
    1]`。因此，对于一个`section_length`输入时间步的例子，我们会产生`section_length`个输出步。将所有输出数学表示会得到一个`[batch_size,
    section_length, most_common_words_length]`矩阵。矩阵的高度是`batch_size`——单个批次中的例子数量。矩阵的宽度是`section_length`——每个例子的时间步数。矩阵的深度是`most_common_words_length`——每个元素的维度。
- en: To make a prediction, we are only concerned about the `output_last` at each
    example and, since the number of examples is `batch_size`, we only need the `batch_size`
    output values. As seen previously, we reshape the matrix `[batch_size, section_length,
    most_common_words_length]` into `[section_length, batch_size, most_common_words_length]`
    which will make it easier to get the `output_last` from each example. Then, we
    use `tf.gather` to obtain the `last_output` tensor.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行预测，我们只关注每个例子的`output_last`，并且由于例子的数量是`batch_size`，我们只需要`batch_size`个输出值。如前所示，我们将矩阵`[batch_size,
    section_length, most_common_words_length]`重塑为`[section_length, batch_size, most_common_words_length]`，这样可以更容易地从每个例子中获取`output_last`。接着，我们使用`tf.gather`来获取`last_output`张量。
- en: 'Below is a code implementation of the above explanation:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是上述解释的代码实现：
- en: 'As we now have the array with these final step values, we can make our prediction
    and use it (in a combination with the label values as seen on the fourth line
    of the previous example) to find the loss at this training step. Since the loss
    has the same dimensions as the labels (expected output values) and logits (predicted
    output values), we use `tf.reduce_mean` to produce a single `total_loss`. This
    is demonstrated in the following code:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在我们已经得到了包含这些最终步骤值的数组，我们可以进行预测，并将其与标签值（如前面示例第四行所示）结合使用，以计算此训练步骤的损失。由于损失的维度与标签（预期输出值）和logits（预测输出值）相同，我们使用`tf.reduce_mean`来生成一个单一的`total_loss`。以下代码演示了这一过程：
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, the `total_loss` is used during backpropagation, with the aim of improving
    the model''s performance by adjusting its weights and biases. This is done through `tf.train.AdamOptimizer` which
    is run during training, and that is detailed in the following section:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终，`total_loss` 在反向传播中使用，目的是通过调整模型的权重和偏置来提高模型的性能。这是通过在训练过程中运行的 `tf.train.AdamOptimizer`
    实现的，详细内容将在接下来的章节中介绍：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Training the network
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练网络
- en: 'Once the model is built, we need to train it using the pre-collected data.
    This operation follows the code snippets below:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型建立完成，我们需要使用预先收集的数据来训练它。这个操作遵循下面的代码片段：
- en: 'We start by initializing all the TensorFlow variables. Then, we have the `iter_offset` which
    makes sure the right batch is extracted from the data. This is shown in the following
    code:'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们首先初始化所有的 TensorFlow 变量。然后，我们使用 `iter_offset` 确保从数据中提取正确的批次。以下代码展示了这一过程：
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, the `tf.train.Saver()` creates a saver object which periodically saves
    the model locally. This helps us in case something happens and our training is
    interrupted. Also, it helps us during the prediction phase to look up the pre-trained
    parameters and so we do not have to run the training every time we want to make
    a prediction:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，`tf.train.Saver()` 创建了一个保存器对象，它定期将模型保存在本地。这有助于我们在训练过程中出现中断的情况下恢复模型。此外，它还可以帮助我们在预测阶段查找预训练的参数，从而不必每次进行预测时都重新训练模型：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, comes the time for the actual training. We loop through the training data
    while calculating the optimizer using the individual batches. These calculations
    will minimize the loss function and we can see it decreasing by printing its value,
    as shown in the snippets as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，真正的训练开始了。我们循环遍历训练数据，并使用每个批次计算优化器。这些计算将最小化损失函数，我们可以通过打印损失值看到它的减少，下面的代码片段展示了这一点：
- en: 'First, we need to divide the data into batches. This is done with the help
    of the `iter_offset` parameter. It keeps track of the lower bound of each batch
    so we always get the next batch from the training set, as shown in the following
    code:'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们需要将数据划分为批次。我们通过 `iter_offset` 参数来实现这一点。它跟踪每个批次的下限，确保我们总是从训练集获取下一个批次，以下代码展示了这一过程：
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we should perform the training by calculating the `optimizer` and `total_loss`.
    We can run a TensorFlow session with the current batch input and output. Finally,
    we should print the loss function, so we can keep track of our progress. If our
    network is training successfully, the value of the loss function should decrease
    at each step:'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们应该通过计算 `optimizer` 和 `total_loss` 来进行训练。我们可以运行一个 TensorFlow 会话，使用当前批次的输入和输出。最后，我们应该打印损失函数的值，以便跟踪我们的进展。如果网络训练成功，损失函数的值应该在每一步都减少：
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Normally, this training takes several hours to finish. You can speed up the
    process by increasing your computational power. We will discuss some techniques
    to do that in [Chapter 6](c57a4667-7e5a-49ad-9cab-ad32989a5878.xhtml), *Improve
    Your RNN Performance*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这个训练过程需要几个小时才能完成。你可以通过增加计算能力来加速这个过程。我们将在[第 6 章](c57a4667-7e5a-49ad-9cab-ad32989a5878.xhtml)中讨论一些加速的方法，*提升你的
    RNN 性能*。
- en: Generating your new text
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成你的新文本
- en: After you have successfully trained your model, it is time to generate your
    new *The Hunger Games* chapter.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功训练模型之后，接下来是生成你新的 *饥饿游戏* 章节的时候。
- en: 'The preceding code can be divided into two parts:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码可以分为两部分：
- en: Training the model using a custom input
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自定义输入训练模型
- en: Predicting the next 1,000 words in the sequence
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测序列中的下一个 1,000 个词
- en: 'Let''s explore the code snippets below:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索下面的代码片段：
- en: 'In the beginning, we initialized a custom input of 21 words (we used the (`section_length
    + 1`) in order to match the model''s dimensions). This input is used to give a
    starting point on our prediction. Next, we will train the existing network with
    it, so that the weight and biases are optimized for the upcoming predictions,
    as shown in the following example:'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一开始，我们初始化了一个包含 21 个词的自定义输入（我们使用了 (`section_length + 1`) 来匹配模型的维度）。这个输入用于提供我们预测的起始点。接下来，我们将用这个输入训练现有的网络，以便优化权重和偏置，以应对即将到来的预测，下面的示例展示了这一过程：
- en: '[PRE18]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we can restore the saved model from the `ckpt` folder in order to train
    it using the newest input, as demonstrated in the following code:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们可以从 `ckpt` 文件夹恢复已保存的模型，并使用最新的输入对其进行训练，如下面的代码所示：
- en: '[PRE19]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We should then encode that input in a one-hot vector with dimensions `[1, section_length,
    most_common_words_length]`. That array should be fed into our model so that the
    predicted words follow the sequence. You may notice that we omit the last word
    and add it later on to produce the `text_next_X` array (see the following code).
    We do that in order to give an unbiased head start to our text generation. This
    is demonstrated in the following example:'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们应该将该输入编码成一个维度为`[1, section_length, most_common_words_length]`的 one-hot
    向量。这个数组应该被输入到我们的模型中，以便生成的预测单词能够遵循顺序。你可能会注意到，我们省略了最后一个单词，稍后会添加它，以生成`text_next_X`数组（见以下代码）。我们这样做是为了给文本生成提供一个无偏的起点。下面的示例中进行了演示：
- en: '[PRE20]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, we should train the network using the encoded input sentence. To maintain
    an unbiased head start of the training, we need to add the last word from the
    sentence before starting the prediction shown in the following example. A slightly
    confusing part can be the `np.concatenate` method. We should first reshape the `text_X` to
    easily append the last section and then reshape the result to accommodate the prediction evaluation.
    This is shown in the following example:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们应该使用编码后的输入句子来训练网络。为了保持训练的无偏起点，我们需要在开始预测之前，添加句子中的最后一个单词，如下例所示。稍微有些令人困惑的部分可能是`np.concatenate`方法。我们首先应重塑`text_X`，以便轻松附加最后一部分，然后再重塑结果以适应预测评估。这在以下示例中展示：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The last part is actually generating the words. At each step (of 1,000 steps)
    we can calculate the prediction using the current `test_next_X`. Then, we can
    remove the first character from the current `test_next_X` and append the prediction.
    This way we constantly keep a set of 20 words where the last element is a fresh
    new prediction. This is shown in the following example:'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一步实际上是生成单词。在每一步（共1,000步）中，我们可以使用当前的`test_next_X`来计算预测。然后，我们可以从当前的`test_next_X`中移除第一个字符，并附加预测结果。通过这种方式，我们始终保持一个包含20个单词的集合，其中最后一个元素是一个全新的预测单词。这个过程在以下示例中进行了展示：
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The method `prediction_to_one_hot` encodes the prediction into a one hot encoding
    array. This is defined in the following example:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`prediction_to_one_hot`方法将预测结果编码成一个 one hot 编码数组。这个方法在以下示例中定义：'
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: After running the code, you should see the final chapter printed out in the
    console. If there is inconsistency among the words, you need to tweak some of
    the hyperparameters. I will explain in the last chapter how you can optimize your
    model and receive good performance. Train a network with the snippets above and
    keep me updated about your performance. I would be really happy to see your end
    results.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，你应该能在控制台看到最终的章节输出。如果单词之间存在不一致的情况，你需要调整一些超参数。我将在最后一章中解释如何优化你的模型并获得良好的性能。请使用上面的代码片段训练网络，并随时告诉我你的性能表现。我会非常高兴看到你的最终结果。
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you went through the process of building a book chapter generator
    using a Gated Recurrent Unit neural network. You understood what sits behind this
    powerful model and how you can put it into practice with a handful of lines of
    code using TensorFlow. In addition, you faced the challenge of preparing and clearing
    your data so that your model is trained correctly.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何使用门控循环单元（GRU）神经网络构建一个书籍章节生成器的过程。你了解了这个强大模型背后的原理，以及如何通过几行代码在 TensorFlow
    中将其付诸实践。此外，你还面临了准备和清理数据的挑战，以确保模型能够正确训练。
- en: In the next chapter, you will fortify your skills by implementing your first
    real-life practical application—a language translator. You have probably faced
    the online Google Translate software and were amazed by how well it worked. In
    the next chapter, you will understand what sits behind a sophisticated system
    like that and why its level of accuracy has increased drastically in recent years.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将通过实现第一个现实中的实际应用——语言翻译器，进一步巩固你的技能。你可能已经使用过在线的 Google 翻译软件，并且对其出色的表现感到惊讶。在下一章中，你将了解像这样一个复杂系统背后的原理，以及为什么它的准确度在近年来大幅提高。
- en: I hope the current chapter advanced your deep learning knowledge and that you
    are excited to be exploring more from the world of recurrent neural networks.
    I cannot wait for you to start the next section.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望当前章节能提升你对深度学习的知识，并且让你对探索递归神经网络的世界感到兴奋。我迫不及待想看到你开始下一节。
- en: External links
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部链接
- en: The Unreasonable Effectiveness of Recurrent Neural Networks—[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络的非理性有效性—[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- en: Sigmoid activation function—[https://www.youtube.com/watch?v=WcDtwxi7Ick&t=3s](https://www.youtube.com/watch?v=WcDtwxi7Ick&t=3s)
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid 激活函数—[https://www.youtube.com/watch?v=WcDtwxi7Ick&t=3s](https://www.youtube.com/watch?v=WcDtwxi7Ick&t=3s)
- en: The difference between sigmoid and tanh activation function—[https://stats.stackexchange.com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function](https://stats.stackexchange.com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid 激活函数与 tanh 激活函数的区别—[https://stats.stackexchange.com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function](https://stats.stackexchange.com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function)
- en: Element-wise multiplication—[https://www.youtube.com/watch?v=2GPZlRVhQWY](https://www.youtube.com/watch?v=2GPZlRVhQWY)
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元素级乘法—[https://www.youtube.com/watch?v=2GPZlRVhQWY](https://www.youtube.com/watch?v=2GPZlRVhQWY)
- en: Learning rate in the neural network—[https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10](https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10)
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络中的学习率—[https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10](https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10)
- en: The number of hidden units in TensorFlow—[https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell](https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 中隐藏单元的数量—[https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell](https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell)
- en: The formula for calculating the number of hidden units—[https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-network](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-network)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算隐藏单元数量的公式—[https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-network](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-network)
