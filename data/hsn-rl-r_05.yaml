- en: Markov Decision Processes in Action
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔科夫决策过程实践
- en: Stochastic processes involve systems that evolve over time (but also more generally
    in space) according to probabilistic laws. Such systems or models describe the
    complex phenomena of the real world that have the possibility of being random.
    These phenomena are more frequent than we believe them to be. We encounter these
    phenomena when the quantities we are interested in aren't predictable with absolute
    certainty. However, when such phenomena show a variety of possible outcomes that
    can be somehow explained or described, then we can introduce a probabilistic model
    of the phenomenon.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随机过程涉及根据概率法则随时间（或更广泛地，随空间）演变的系统。此类系统或模型描述了具有随机性的现实世界复杂现象。这些现象比我们想象的更常见。当我们感兴趣的量无法被绝对确定地预测时，我们便遇到这些现象。然而，当这些现象表现出多种可能的结果，并且能够以某种方式被解释或描述时，我们便可以引入该现象的概率模型。
- en: A Markov chain is a stochastic process whereby the evolution of a system depends
    only on its present state and not on its past state. A Markov chain is characterized
    by a set of states and by the probability of a transition occurring between states.
    Think of a point that can move randomly forward or backward along a line at discrete
    intervals of time, covering a certain distance at each interval.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔科夫链是一种随机过程，其中系统的演变仅依赖于其当前状态，而不依赖于其过去状态。马尔科夫链由一组状态和状态之间转移发生的概率所表征。可以想象一个点在一条线上沿着离散时间间隔随机向前或向后移动，每个时间间隔覆盖一定的距离。
- en: In this chapter, we will get to grips with the concepts of the Markov process. Stochastic
    Markov processes will be analyzed in detail. We will be introduced to the Markov
    chain and then we will learn how to use these algorithms to make weather forecasts.
    Finally, we will learn how to evaluate the optimal policy for the solution of
    a Markov reward problem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入理解马尔科夫过程的概念。我们将详细分析随机马尔科夫过程。我们将介绍马尔科夫链，并学习如何利用这些算法进行天气预测。最后，我们将学习如何评估解决马尔科夫奖励问题的最优策略。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: An overview of the Markov process
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马尔科夫过程概述
- en: Introduction to Markov chains
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马尔科夫链简介
- en: Markov chains applications – weather forecasting
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马尔科夫链应用 —— 天气预测
- en: Markov reward model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马尔科夫奖励模型
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，查看代码的实际应用：
- en: '[http://bit.ly/36xyVLj](http://bit.ly/36xyVLj)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/36xyVLj](http://bit.ly/36xyVLj)'
- en: An overview of the Markov process
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔科夫过程概述
- en: As we mentioned in [Chapter 2](aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml),
    *Building Blocks of Reinforcement Learning,* a stochastic process is called **Markovian**
    when a certain instant *t* of observation is chosen. The evolution of the process
    starting with *t* depends only on *t*, while it does not depend on the previous
    instants in any way. Thus, a process is Markovian when, given the moment of observation,
    only that instant determines the future evolution of the process, while this evolution
    does not depend on the past. In the next section, we will explore the concept
    of stochastic processes and we will see how it is related to probability theory.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第二章](aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml)《强化学习的构建模块》中提到的，当选择某一时刻
    *t* 进行观察时，随机过程被称为**马尔科夫过程**。从 *t* 开始的过程演变仅依赖于 *t*，而不依赖于之前的任何时刻。因此，当给定观察时刻后，只有该时刻决定了过程的未来演变，而该演变不依赖于过去时刻时，过程就是马尔科夫过程。在接下来的部分，我们将探索随机过程的概念，并看到它如何与概率论相关联。
- en: Understanding the stochastic process
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解随机过程
- en: In order to provide a formal definition of a Markov process, it is necessary
    to specify what is meant by a set of random variables having a temporal ordering.
    Such a set of random variables can best be represented by a stochastic process.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供马尔科夫过程的正式定义，有必要明确什么是具有时间顺序的随机变量集合。这样一个随机变量集合最好通过一个随机过程来表示。
- en: 'The theory of stochastic processes concerns the study of systems that evolve
    over time (but also more generally in space) according to probabilistic laws.
    Such systems or models describe complex phenomena of the real world that have
    the possibility of being random. These phenomena are more frequent than we believe
    them to be, and we face these situations when the quantities we are interested
    in can''t be predicted with certainty. We define a stochastic process in discrete
    time and discrete states in a sequence that contains the following random variables:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随机过程理论研究的是那些根据概率法则随时间（但也可以更一般地随空间）演变的系统。这类系统或模型描述了现实世界中那些可能是随机的复杂现象。这样的现象比我们想象的更为频繁，我们在面对那些我们无法确定预测的量时，常常会遇到这种情况。我们定义了一个在离散时间和离散状态下的随机过程，它由以下随机变量构成：
- en: '![](img/1a976054-188b-468d-b04a-5c477c331844.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a976054-188b-468d-b04a-5c477c331844.png)'
- en: In the preceding sequence, each *X[n]* is a discrete random variable with values
    in a set *S = s[1], s[2],…, s[n],* called the **space of the states**. Without
    losing generality, suppose that *S* is a subset of the relative integers *Z*.
    Each value of *X[n]* as the index *n* changes will represent the state of the
    system over time. This process we will analyze starts in any of the states represented
    by *X[n]* and will move to the next state *X[n + 1]*. Each transition is called
    a **step**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的序列中，每个 *X[n]* 都是一个离散随机变量，取值来自一个集合 *S = s[1], s[2],…, s[n],* 这个集合被称为**状态空间**。为了不失一般性，假设
    *S* 是整数集 *Z* 的子集。随着索引 *n* 的变化，每个 *X[n]* 的值将代表系统随时间变化的状态。我们将要分析的过程从任何由 *X[n]* 表示的状态开始，并将转移到下一个状态
    *X[n + 1]*。每一次转移称为**一步**。
- en: As time passes, the process can jump from one state to another. If a step *n*
    is in a state *i*, and at the next step, *n + 1* is in a state *j ≠ i*, we can
    say that there has been a transition.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，过程可以从一个状态跳跃到另一个状态。如果第 *n* 步处于状态 *i*，而在下一步 *n + 1* 处于状态 *j ≠ i*，我们可以说发生了转移。
- en: Calculating the probability
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算概率
- en: Given a stochastic process (*X[n]*), we are interested in calculating the probabilities
    associated with it. Now, let's explore the basic concepts of probability. If you
    already know about these concepts, you can skip this section. Either way, this
    section will allow you to explore the basics of probability theory.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个随机过程（*X[n]*），我们感兴趣的是计算与之相关的概率。现在，让我们探讨概率的基本概念。如果你已经了解这些概念，可以跳过本节内容。不管怎样，本节内容将帮助你探索概率论的基础知识。
- en: 'The **probability** (**a priori**) that a given event (*E*) occurs is the ratio
    between the number (*s*) of favorable cases of the event itself and the total
    number (*n*) of the possible cases, provided all the considered cases are equally
    probable:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 给定事件发生的**概率**（**先验概率**）是事件本身的有利情况数（*s*）与所有可能情况总数（*n*）的比率，前提是所有考虑的情况具有相等的概率：
- en: '![](img/5ec2e412-6aaa-429d-9d86-edb35b873d4d.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5ec2e412-6aaa-429d-9d86-edb35b873d4d.png)'
- en: 'Let''s look at a simple example:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的例子：
- en: While throwing a dice, what is the probability that 3 shows up? The number of
    possible cases is 6, {1, 2, 3, 4, 5, 6}, while the number of favorable cases is
    1, that is, {3}. So, P(3) =1/6 =0.166 =16.6 %.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在掷骰子的过程中，掷出 3 的概率是多少？可能的情况数是 6，{1, 2, 3, 4, 5, 6}，而符合条件的情况数是 1，即 {3}。所以，P(3)
    = 1/6 = 0.166 = 16.6%。
- en: 'The probability of an event *P(E)* always being number between 0 and 1 can
    be formulated as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 事件的概率 *P(E)* 总是介于 0 和 1 之间，可以如下表示：
- en: '![](img/c2978ca2-9cda-4ab2-a605-3b5bf4ac782b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c2978ca2-9cda-4ab2-a605-3b5bf4ac782b.png)'
- en: 'The extreme values are defined as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 极值的定义如下：
- en: An event that has a probability of 0 is called an impossible event. Suppose
    we have six red balls in a bag; what is the probability of picking a black ball?
    The number of possible cases is 6; the number of favorable cases is 0 because
    there are no black balls in the bag. Hence, *P(E) = 0/6 = 0*.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率为 0 的事件称为不可能事件。假设我们有六个红球在一个袋子里，那么抽到黑球的概率是多少？可能的情况数是 6，符合条件的情况数是 0，因为袋子里没有黑球。因此，*P(E)
    = 0/6 = 0*。
- en: An event that has a probability of 1 is called a certain event. Suppose we have
    six red balls in a bag; what is the probability of picking a red ball? The number
    of possible cases is 6; the number of favorable cases is 6 because there are only
    red balls in the bag. Therefore, *P(E) = 6/6 = 1*.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率为 1 的事件称为必然事件。假设我们有六个红球在一个袋子里，那么抽到红球的概率是多少？可能的情况数是 6，符合条件的情况数是 6，因为袋子里只有红球。因此，*P(E)
    = 6/6 = 1*。
- en: So far, we've talked about the likelihood of an event, but what happens when
    there's more than one possible event? Two random events, A and B, are independent
    if the probability of the occurrence of event A is not dependent on whether event
    B has occurred, and vice versa.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了事件发生的可能性，那么如果有多个可能的事件会发生呢？如果事件A的发生概率不依赖于事件B是否发生，且反之亦然，那么两个随机事件A和B是独立的。
- en: 'For example, let''s say we have two 52 decks of French playing cards. When
    extracting a card from each deck, the following two events are independent:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设我们有两副52张的法国扑克牌。当从每副牌中抽出一张卡片时，以下两个事件是独立的：
- en: '**E1**: The card that''s extracted from the first deck is an ace.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**E1**: 从第一副牌中抽出的卡片是一个王牌。'
- en: '**E2**: The card that''s extracted from the second deck is a clubs card.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**E2**: 从第二副牌中抽出的卡片是一张梅花牌。'
- en: Each can happen with the same probability, independent of the other's occurrence.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 每个事件都可以以相同的概率发生，且不受另一个事件发生的影响。
- en: 'Conversely, a random event, A, is dependent on another event, B, if the probability
    of event A depends on whether event B has occurred or not. Suppose we have a deck
    of 52 cards; by extracting two cards in succession without putting the first card
    back in the deck, the following two events are dependent:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果事件A的发生概率依赖于事件B是否发生，那么事件A就依赖于事件B。假设我们有一副52张的牌；如果连续抽出两张卡片且不将第一张卡片放回牌堆，以下两个事件是相关的：
- en: '**E1**: The first extracted card is an ace.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**E1**: 第一张抽出的卡片是一个王牌。'
- en: '**E2**: The second extracted card is an ace.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**E2**: 第二张抽出的卡片是一个王牌。'
- en: 'To be precise, the probability of E2 depends on whether or not E1 occurs, as
    follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 精确地说，E2的概率取决于E1是否发生，具体如下：
- en: The probability of E1 is 4/52.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: E1的概率是4/52。
- en: The probability of E2 if the first card was an ace is 3/51.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果第一张卡片是王牌，那么E2的概率是3/51。
- en: The probability of E2 if the first card was not an ace is 4/51.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果第一张卡片不是王牌，那么E2的概率是4/51。
- en: Understanding joint probability
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解联合概率
- en: 'Now, let''s deal with the case of **joint probability**, both independent and
    dependent. Given two events, A and B, if the two events are independent (I mean
    the occurrence of one doesn''t affect the probability of the other), the joint
    probability of the event is equal to the product of the probabilities of A and
    B:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来处理**联合概率**的情况，既包括独立事件，也包括相关事件。给定两个事件A和B，如果这两个事件是独立的（即一个事件的发生不会影响另一个事件的概率），那么事件的联合概率等于A和B的概率的乘积：
- en: '![](img/35371c3c-59bf-46c3-88b9-39e0a1ffff97.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/35371c3c-59bf-46c3-88b9-39e0a1ffff97.png)'
- en: 'Let''s look at an example. We have two decks of 52 cards. By extracting a card
    from each deck, let''s consider the two independent events:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。我们有两副52张的扑克牌。当从每副牌中抽出一张卡片时，让我们考虑这两个独立的事件：
- en: 'A: The card that''s extracted from the first deck is an ace.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'A: 从第一副牌中抽出的卡片是一个王牌。'
- en: 'B: The card that''s extracted from the second deck is a clubs card.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'B: 从第二副牌中抽出的卡片是一张梅花牌。'
- en: What is the probability that both of them occur?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 两个事件都发生的概率是多少？
- en: P(A) = 4/52
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(A) = 4/52
- en: P(B) = 13/52
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(B) = 13/52
- en: P(A ∩ B) = 4/52 * 13/52 = 52 /(52 * 52) = 1/52
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(A ∩ B) = 4/52 * 13/52 = 52 / (52 * 52) = 1/52
- en: 'If the two events are dependent (that is, the occurrence of one affects the
    probability of the other), then the same rule may apply, provided that P(B|A)
    is the probability of event B given that event A has occurred. This condition
    introduces conditional probability, which we are going to dive into:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个事件是相关的（即一个事件的发生会影响另一个事件的概率），那么可以应用相同的规则，只要P(B|A)是事件B在事件A发生的条件下的概率。这个条件引入了条件概率，我们将深入探讨：
- en: '![](img/6885c500-059a-47c2-8a2b-f6124b7053a5.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6885c500-059a-47c2-8a2b-f6124b7053a5.png)'
- en: A bag contains two white balls and three red balls. Two balls are pulled out
    from the bag in two successive extractions without reintroducing the first ball
    that was pulled out of the bag.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个袋子里有两个白球和三个红球。从袋子里连续抽出两颗球，且不将第一颗抽出的球放回袋中。
- en: 'Let''s calculate the probability that the two balls that were extracted were
    both white:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算抽出的两颗球都是白球的概率：
- en: The probability that the first ball is white is 2/5.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一颗球是白色的概率是2/5。
- en: The probability that the second ball is white, provided that the first ball
    is white, is 1/4.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果第一颗球是白色的，那么第二颗球是白色的概率是1/4。
- en: 'The probability of having two white balls is as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 两颗球都是白球的概率如下：
- en: P(two whites) = 2/5 * 1/4 = 2/20 = 1/10
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(两颗白球) = 2/5 * 1/4 = 2/20 = 1/10
- en: Understanding conditional probability
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解条件概率
- en: 'Now, it''s time to introduce you to the concept of conditional probability.
    The probability that event B occurs, calculated by the condition that event A
    occurred, is called conditional probability and is indicated by the symbol P(B
    | A). It is calculated using the following formula:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候向你介绍条件概率的概念了。事件B发生的概率，在事件A发生的条件下计算，被称为条件概率，用符号P(B | A)表示。它是通过以下公式计算的：
- en: '![](img/3e116f0e-c9dc-431e-98bf-a68615d412aa.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e116f0e-c9dc-431e-98bf-a68615d412aa.png)'
- en: 'Now that we are able to understand the different kinds of probabilities, let''s
    apply them to the stochastic processes. Let''s start with the simplest type of
    probability, which is written in the following form:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们能够理解不同种类的概率了，让我们将它们应用到随机过程上。我们从最简单的概率类型开始，它写作以下形式：
- en: '![](img/799e0042-af2a-4233-8709-e63c34c3410c.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/799e0042-af2a-4233-8709-e63c34c3410c.png)'
- en: This represents the probability of observing the system in the state i at step
    n. In addition to these simple probabilities, we should be interested in the calculation
    of more complex probabilities involving multiple steps at the same time.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示在第n步时观察系统处于状态i的概率。除了这些简单的概率之外，我们应该对计算涉及多个步骤的更复杂概率感兴趣。
- en: 'For example, it may be interesting to calculate the probability of being in
    state j at step n + 1, knowing that it is in state i at step n (as we can see,
    this is the conditional probability we defined previously):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，计算在第n + 1步处于状态j的概率，知道在第n步时它处于状态i，可能是很有意思的（正如我们所看到的，这就是我们之前定义的条件概率）：
- en: '![](img/d1d2a758-1e1a-48a3-8413-3dcd5c52aa4d.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1d2a758-1e1a-48a3-8413-3dcd5c52aa4d.png)'
- en: 'This is called the transition probability from i to j at step n. Using the
    conditional probability definition, this rewrites itself, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为从i到j在第n步的转移概率。通过条件概率定义，它可以重写如下：
- en: '![](img/40ae8605-8803-478d-b437-aad895b0a122.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40ae8605-8803-478d-b437-aad895b0a122.png)'
- en: 'Therefore, for this calculation, it is sufficient to know the a priori probability
    and the joint probability. To calculate more complex expressions, it is necessary
    to know the generic joint probabilities given by the following formula:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于这个计算，知道先验概率和联合概率就足够了。要计算更复杂的表达式，需要知道通过以下公式给出的通用联合概率：
- en: '![](img/82c1f1ed-7411-4545-823c-5ab13248bb9b.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82c1f1ed-7411-4545-823c-5ab13248bb9b.png)'
- en: 'All of this occurs due to variations of all the i[0],...,i[n] in the set of
    integers Z. In a certain sense, these probabilities exhaust all possible information:
    the stochastic process is statistically determined when all the combined (discrete)
    densities are known, that is, the densities of all the multiple discrete variables
    (X[1], ..., X[n]) to the variation of all the i[0],...,i[n] Z. The calculation
    of these joint probabilities is a very difficult problem in general.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都源于整数集合Z中所有i[0],...,i[n]的变化。从某种意义上讲，这些概率消耗了所有可能的信息：当所有联合（离散）密度已知时，随机过程在统计上是确定的，也就是说，已知所有多个离散变量（X[1],
    ..., X[n]）的密度对所有i[0],...,i[n] Z的变化。计算这些联合概率通常是一个非常困难的问题。
- en: In the next section, we will delve deeper into the concepts behind the Markov
    process.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将深入探讨马尔可夫过程背后的概念。
- en: Introduction to Markov chains
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫链简介
- en: A Markov chain is a mathematical model of a random phenomenon that evolves over
    time in such a way that the past influences the future only through the present.
    The time can be discrete (whole variable), continuous (real variable), or a totally
    ordered whole. In this section, we will only consider discrete chains. Markov
    chains were introduced in 1906 by Andrei Andreyevich Markov (1856–1922), which
    is where the name is derived.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫链是一个随机现象的数学模型，它随时间演变，以这样的方式：过去通过现在仅影响未来。时间可以是离散的（整数变量）、连续的（实数变量），或完全有序的整数。在本节中，我们将只考虑离散链。马尔可夫链由安德烈·安德烈耶维奇·马尔可夫（1856–1922）在1906年引入，这也是该名称的由来。
- en: A Markov chain is a stochastic model that represents a sequence of possible
    cases in which the probability that each case occurs depends only on the state
    relative to the previous case. So, Markov chains have the **memorylessness** property.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫链是一种随机模型，表示可能的案例序列，其中每个案例发生的概率仅依赖于相对于前一个案例的状态。因此，马尔可夫链具有**无记忆性**特性。
- en: 'Let''s consider a random process described by the sequence of random variables
    X = X[0], ..., X[n] which can assume values in a set, that is, j[0], j[1],…, jn.
    Let''s say that the process we are analyzing has the property of Markov if the
    evolution of the process in the future depends only on the value of the present
    state and not on the past history. In formulas, using the conditional probability,
    we will have the following:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个由随机变量序列X = X[0], ..., X[n]描述的随机过程，它可以在一个集合中取值，即 j[0], j[1], …, jn。假设我们分析的过程具有马尔科夫性质，即过程未来的演化只依赖于当前状态的值，而不依赖于过去的历史。在公式中，使用条件概率，我们将得到以下表达式：
- en: '![](img/90728e8f-b785-4c4e-8df7-d5d489b3b59e.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/90728e8f-b785-4c4e-8df7-d5d489b3b59e.png)'
- en: 'This relationship must apply to all the parameters if they are well-defined
    conditional probabilities. A discrete-time stochastic process X that has the Markov
    property is said to be a Markov chain. A Markov chain is said to be homogeneous
    if the transition probabilities are as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有的参数都是良好定义的条件概率，那么这个关系必须适用于所有参数。一个具有马尔科夫性质的离散时间随机过程 X 被称为马尔科夫链。如果转移概率满足以下条件，则该马尔科夫链是齐次的：
- en: '![](img/fc6a5e1b-a2bf-4cc4-b07a-fb4439356daa.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc6a5e1b-a2bf-4cc4-b07a-fb4439356daa.png)'
- en: 'This does not depend on n, but only on i and j. When this happens, we get the
    following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这与n无关，仅与i和j有关。当这种情况发生时，我们得到以下结果：
- en: '![](img/8808b23b-c001-44e5-a120-6d720fcbf9ee.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8808b23b-c001-44e5-a120-6d720fcbf9ee.png)'
- en: 'We can calculate all the joint probabilities by knowing the numbers p[ij] along
    with the following initial distribution:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过知道概率 p[ij] 和以下初始分布来计算所有的联合概率：
- en: '![](img/71bf319c-a4bb-4ed5-b61a-42d21032f74c.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/71bf319c-a4bb-4ed5-b61a-42d21032f74c.png)'
- en: This probability is called the **distribution of the process over time zero**.
    The p[ij] probabilities are called transition probabilities, while p[ij] is the
    probability of a transition occurring from i to j in a time step.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概率被称为**过程在零时刻的分布**。p[ij] 概率被称为转移概率，而 p[ij] 是从状态 i 到状态 j 在一个时间步长内发生转移的概率。
- en: Understanding the transition matrix
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解转移矩阵
- en: 'The study of homogeneous Markov chains becomes particularly simple and effective
    using matrix representation. In particular, the formula expressed by the previous
    proposition becomes much more readable. Due to this, the structure of a Markov
    chain can be completely represented by the following transition matrix:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 研究齐次马尔科夫链在使用矩阵表示时变得特别简单且有效。特别是，前述命题所表达的公式变得更加易于阅读。由于这一点，马尔科夫链的结构可以通过以下转移矩阵完全表示：
- en: '![](img/954dc61a-be80-46e1-a782-232c21fb7278.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/954dc61a-be80-46e1-a782-232c21fb7278.png)'
- en: 'The properties of the transition probability matrices derive directly from
    the nature of the elements that compose them. In fact, by observing that the elements
    of the matrix are probabilities, they must have a value between 0 and 1\. So,
    this is a positive matrix in which the sum of the elements of each row is unitary.
    In fact, the elements of the i-th row are the probabilities that the chain, being
    in the state Si at the instant t, transits in S1 or in S2,... or in Sn at the
    next step. Such transitions are mutually exclusive and exhaustive of all possibilities.
    Such a matrix (positive with unit sum rows) is called **stochastic**. Therefore,
    we will need to define each positive row vector as stochastic, as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 转移概率矩阵的性质直接来源于构成它们的元素的特性。事实上，通过观察矩阵的元素是概率，它们必须在0和1之间。因此，这是一种正矩阵，其中每一行元素的和为1。事实上，第i行的元素是链在时刻t处于状态Si时，下一步转移到S1或S2，...或Sn的概率。这些转移是互斥的，并且涵盖了所有可能性。这样的矩阵（每行和为1的正矩阵）被称为**随机矩阵**。因此，我们需要将每个正行向量定义为随机矩阵，如下所示：
- en: '![](img/41f49b00-7d14-4652-96e4-f1d28d06375d.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41f49b00-7d14-4652-96e4-f1d28d06375d.png)'
- en: 'In this vector, the sum of the elements takes a unit value, as shown in the
    following formula:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个向量中，元素的和取值为1，如下式所示：
- en: '![](img/e2a878d0-f75e-4bac-a8e2-0637ae862b12.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2a878d0-f75e-4bac-a8e2-0637ae862b12.png)'
- en: 'Now, we will see that a particular form assumes this matrix in the case of
    a one-dimensional random walk. As shown in the following diagram, in a one-dimensional
    random walk, we study the motion of a point-like particle that''s constrained
    to move along a straight line in the two allowed directions (right and left).
    At each movement, it moves (randomly) one step to the right with a fixed probability
    p or to the left with a probability q, in such a way that p+q=1\. Each step is
    of equal length and independent of the others:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看到，在一维随机游走的情况下，这个矩阵采用了特定的形式。如以下图所示，在一维随机游走中，我们研究的是一个类点粒子的运动，它被限制只能沿着直线在两个允许的方向（右和左）上移动。在每次运动中，它以固定的概率p随机向右移动一步，或以概率q向左移动一步，使得p+q=1。每一步的长度相等，并且与其他步独立：
- en: '![](img/05a5cbc3-baa1-42bf-ba6a-e233ae3cec81.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05a5cbc3-baa1-42bf-ba6a-e233ae3cec81.png)'
- en: 'Suppose that the random variables Z[n] with n = 1,2, .. are independent and
    all have the same distribution. Due to this, the position of the particle instant
    n is given by the following formula:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 假设随机变量Z[n]，其中n = 1,2, ..，是独立的并且具有相同的分布。因此，粒子在时刻n的位置由以下公式给出：
- en: '![](img/abc6350b-0da2-4de5-b433-6dae2265a17d.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/abc6350b-0da2-4de5-b433-6dae2265a17d.png)'
- en: 'Here, X[0] = 0\. The state space is S = (0, ±1, ±2,…). The X[n] process is
    a Markov chain because, to determine the probability that the particle is in a
    certain position the next moment, we just need to know where it is at the current
    moment, even if we are aware of where it was in all the moments before the current
    one. This concept can be expressed through the following equation:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，X[0] = 0。状态空间为S = (0, ±1, ±2,…)。X[n]过程是一个马尔可夫链，因为，为了确定粒子下一个时刻处于某个位置的概率，我们只需要知道它在当前时刻的位置，即使我们知道它在当前时刻之前的所有时刻的位置。这一概念可以通过以下方程表达：
- en: '![](img/3e9936e0-7708-4d00-b3d9-c394ce87ce4a.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e9936e0-7708-4d00-b3d9-c394ce87ce4a.png)'
- en: 'Here, the Zn variables are independent. The transition matrix is a matrix with
    finite rows and as many columns, where 0 is on the main diagonal, p is on the
    diagonal above the main, q is on the diagonal below the main, and 0 is everywhere
    else, as shown in the following formula:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，Zn变量是独立的。转移矩阵是一个行数有限且列数相等的矩阵，其中主对角线上是0，主对角线上的上方是p，下方是q，其他地方都是0，公式如下所示：
- en: '![](img/e0dd7625-cb5b-4d05-8567-10c17c0a0a5d.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0dd7625-cb5b-4d05-8567-10c17c0a0a5d.png)'
- en: Here, we can see that this generalization greatly simplifies the problem at
    hand.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，这一概化极大地简化了当前的问题。
- en: Understanding the transition diagram
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解转移图
- en: 'A very intuitive alternative to the description of a Markov chain through the
    transition matrix is that of associating a Markov chain with an oriented graph
    (transition diagram). Here, the following occurs:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 将马尔可夫链与转移矩阵的描述相对直观的替代方法是将马尔可夫链与一个有向图（转移图）关联。在这里，发生以下情况：
- en: Vertices are labeled by the S1,S2,…, Sn states (or, briefly, from the indices
    1, 2, …, n of the states).
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顶点通过S1、S2、…、Sn状态标记（或者简而言之，通过状态的索引1、2、…、n标记）。
- en: There is a directed edge that connects the vertex Si to the vertex Sj if, and
    only if, the probability of transition from Si to Sj is positive (this is the
    probability, which is, in turn, used as a label of the edge itself).
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果且仅如果从Si到Sj的转移概率为正，则存在一个有向边将顶点Si连接到顶点Sj（这个概率也作为边本身的标签）。
- en: 'It is clear that the transition matrix and transition diagram provide the same
    information about the same Markov chain. To understand this duality, we need to
    look at a simple example – consider a Markov chain with three possible states,
    that is, 1, 2, and 3, and the following transition matrix:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，转移矩阵和转移图提供了关于相同马尔可夫链的相同信息。为了理解这种二重性，我们需要看一个简单的例子——考虑一个具有三个可能状态的马尔可夫链，即1、2和3，及其以下的转移矩阵：
- en: '![](img/b5e49b40-837c-41ca-8d4d-2f85fdcdc1d6.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5e49b40-837c-41ca-8d4d-2f85fdcdc1d6.png)'
- en: 'The transition diagram for the newly introduced Markov chain can be seen in
    the following diagram. We can identify three possible states: 1, 2, and 3\. The
    two-state border contains the transition probabilities p[ij]. When there is no
    border between the two states, this means that the probability of transition is
    zero:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 新引入的马尔可夫链的转移图可以在以下图中看到。我们可以识别出三个可能的状态：1、2和3。两状态边界包含转移概率p[ij]。当两个状态之间没有边界时，表示转移的概率为零：
- en: '![](img/ae10ccc2-02e9-40b9-8c06-f71fd00d925e.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae10ccc2-02e9-40b9-8c06-f71fd00d925e.png)'
- en: In the preceding diagram, we can see that the arrows that come out of a state
    always sum up exactly at 1, just like what happens for every row in the transition
    matrix whose values must be added exactly to 1 – which represents the probability
    distribution. By comparing the transition matrix and the transition diagram, we
    can understand the duality between the two resources. As always, a diagram is
    much more explanatory.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图示中，我们可以看到从一个状态出来的箭头总和恰好为 1，就像转移矩阵中每一行的值必须加起来恰好为 1 —— 这代表了概率分布。通过比较转移矩阵和转移图，我们可以理解两者之间的对偶性。像往常一样，图示要比文字更具解释性。
- en: In the next section, we will put what we've learned into practice by addressing
    a prediction problem with Markov chains.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将通过解决一个与马尔可夫链相关的预测问题来实践我们学到的内容。
- en: Markov chain application – weather forecasting
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫链应用 —— 天气预测
- en: 'To apply what we''ve learned so far, we will look at a weather forecasting
    model based on Markov chains. To simplify this model, we will assume that there
    are only three states – rainy, cloudy, and sunny. Let''s also assume that we have
    made some calculations and discovered that tomorrow''s time is somehow based on
    today''s time, according to the following transition matrix:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用我们迄今为止学到的内容，我们将查看一个基于马尔可夫链的天气预测模型。为了简化这个模型，我们假设只有三种状态——雨天、阴天和晴天。我们还假设我们已经进行了一些计算，并发现明天的天气某种程度上依赖于今天的天气，依据以下转移矩阵：
- en: '![](img/0ecf6447-7b56-4345-81c2-92b7d03ba387.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ecf6447-7b56-4345-81c2-92b7d03ba387.png)'
- en: 'Each row must contain non-negative numbers and the sum of them must be equal
    to 1\. Recall that this matrix contains the conditional probabilities of the type
    expressed as *P (A | B)**,*, that is, the probability of *A* given *B*. So, this
    matrix contains the following conditional probabilities:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行必须包含非负数，且它们的和必须等于 1。回想一下，这个矩阵包含了条件概率，形式为 *P (A | B)*，即在给定 *B* 的情况下 *A* 发生的概率。因此，这个矩阵包含了以下条件概率：
- en: '![](img/ad894ba3-27ff-43c8-8c3c-f0016f338828.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad894ba3-27ff-43c8-8c3c-f0016f338828.png)'
- en: 'Here, we have the following properties:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有以下几个属性：
- en: '*Ra*: Rainy'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ra*：雨天'
- en: '*Cl*: Cloudy'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Cl*：阴天'
- en: '*Su*: Sunny'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Su*：晴天'
- en: The weather conditions between two days are not necessarily correlated, so the
    process is Markovian.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 两天之间的天气状况不一定相关，因此该过程是马尔可夫过程。
- en: 'At this point, the following questions come to mind:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，以下问题浮现在脑海中：
- en: If today is sunny, how can we calculate the probability that it is rainy in
    the next few days?
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果今天是晴天，我们如何计算未来几天变成雨天的概率？
- en: After a certain number of days, what will be the proportion of sunny and rainy
    days?
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在若干天之后，晴天和雨天的比例将会是多少？
- en: Both questions, as well as many others that may come to mind, can be answered
    through the tools that make Markov chains available to us.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个问题，以及可能浮现的其他问题，都可以通过使我们能够使用马尔可夫链的工具来解答。
- en: 'The following is the R code that allows us to alternate between sunny, cloudy,
    and rainy days, starting from a specific initial condition:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是允许我们在特定初始条件下交替出现晴天、阴天和雨天的 R 代码：
- en: '[PRE0]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s analyze this code line by line:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行分析这段代码：
- en: 'The first line loads the library:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一行加载了库：
- en: '[PRE1]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Keep in mind that if you need to install a library that isn't present in the
    initial distribution of R, you must use the `install.packages()` function. This
    function should be used just once and not every time the code is run.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，如果需要安装 R 初始分发版中没有的库，必须使用`install.packages()`函数。这个函数只需使用一次，而不是每次运行代码时都使用。
- en: 'For example, to install the `markovchain` package, we should write the following:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，要安装 `markovchain` 包，我们应该写如下代码：
- en: '[PRE2]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This function downloads and installs packages from CRAN-like repositories or
    from local files. Instead, the load command must be used whenever the script is
    executed in a new session of R.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数从 CRAN 类的仓库或本地文件下载并安装软件包。相反，必须在每次在新的 R 会话中执行脚本时使用加载命令。
- en: Importing the markovchain package
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入 markovchain 包
- en: 'The `markovchain` package contains functions and S4 methods that we can use
    to create and manage discrete-time Markov chains. In addition to this, functions
    that we can use to perform statistical (fitting and drawing random variates) and
    probabilistic (analysis of their structural proprieties) analysis are provided.
    A brief description of the `markovchain` package, which can be extracted from
    the official documentation, is shown in the following table:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`markovchain`包包含我们可以用来创建和管理离散时间马尔可夫链的函数和S4方法。除此之外，还提供了可用于执行统计（拟合和绘制随机变量）和概率（分析其结构属性）分析的函数。从官方文档中提取的`markovchain`包的简要描述如下表所示：'
- en: '| Version | 0.6.9.14 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 版本 | 0.6.9.14 |'
- en: '|       Date | 2019-01-20 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 日期 | 2019-01-20 |'
- en: '| Maintainer | Giorgio Alfredo Spedicato |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 维护者 | Giorgio Alfredo Spedicato |'
- en: '|    License | GPL-2 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 许可证 | GPL-2 |'
- en: '|    Authors | Giorgio Alfredo Spedicato, Tae Seung Kang, Sai Bhargav Yalamanchi,
    Mildenberger Thoralf, Deepak Yadav, Ignacio Cordón, Vandit Jain, Toni Giorgino
    |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 作者 | Giorgio Alfredo Spedicato, Tae Seung Kang, Sai Bhargav Yalamanchi, Mildenberger
    Thoralf, Deepak Yadav, Ignacio Cordón, Vandit Jain, Toni Giorgino |'
- en: 'We will use this package in a variety of chapters in this book to demonstrate
    the usefulness of the features it provides. Let''s get started:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的多个章节中使用此包，以演示它所提供的功能的实用性。让我们开始吧：
- en: 'Let''s continue analyzing the code:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续分析代码：
- en: '[PRE3]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `set.seed()` command sets the seed of the R random number generator. This
    is necessary whenever we want to make the example reproducible. When `set.seed()`
    is used, the random numbers that are used in the algorithm will always be the
    same, so that a subsequent reproduction of the algorithm will provide the same
    results. Each seed value will correspond to a sequence of values that are generated
    for a given random number generator.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`set.seed()`命令设置R随机数生成器的种子。每当我们希望使示例可重现时，这都是必要的。当使用`set.seed()`时，算法中使用的随机数将始终相同，因此随后的算法重现将提供相同的结果。每个种子值将对应于为给定随机数生成器生成的值序列。'
- en: 'In the following line, we define the states of the weather condition:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接下来的行中，我们定义天气条件的状态：
- en: '[PRE4]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As shown here, only three states are provided: `Rainy`, `Cloudy`, and `Sunny`.
    At this point, we have to define the possible transitions of weather conditions.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，仅提供了三种状态：`Rainy`、`Cloudy`和`Sunny`。此时，我们需要定义天气条件的可能转移。
- en: 'Let''s move on and define the transition matrix according to what was established
    at the beginning of this section:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们根据本节开始时的内容来定义转移矩阵：
- en: '[PRE5]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Remember that this matrix contains the conditional probabilities of the type
    expressed as *P(A | B)*, that is, the probability of *A* given *B*. As we mentioned
    previously, the rows of this matrix add up to 1.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这个矩阵包含的是条件概率类型，表达式为*P(A | B)*，即给定*B*时*A*发生的概率。正如我们之前提到的，这个矩阵的行加起来等于1。
- en: 'Now, we can create the `markovchain` object:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以创建`markovchain`对象：
- en: '[PRE6]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `markovchain` class has been designed to handle homogeneous Markov chain
    processes. The following slots are passed:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`markovchain`类旨在处理同质马尔可夫链过程。以下槽将被传递：'
- en: '`transitionMatrix`: Square transition matrix containing the probabilities of
    the transition matrix.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transitionMatrix`：包含转移矩阵概率的方阵。'
- en: '`states`: Name of the states. It must be the same as the colnames and rownames
    of the transition matrix. This is a character vector listing the states for which
    transition probabilities are defined.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`states`：状态的名称。它必须与转移矩阵的列名和行名相同。这是一个字符向量，列出了定义了转移概率的状态。'
- en: '`byrow`: Binary flag. A logical element indicating whether transition probabilities
    are shown by row or by column.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`byrow`：二进制标志。一个逻辑元素，指示转移概率是按行显示还是按列显示。'
- en: '`name`: Optional character element to name the discrete-time Markov chains.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`：可选的字符元素，用于为离散时间马尔可夫链命名。'
- en: 'To provide a summary of the model we''ve just created, use the following command:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供我们刚刚创建的模型的总结，请使用以下命令：
- en: '[PRE7]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following results are returned:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE8]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As you can see, the dimensions of the object, the states, and the transition
    matrix are printed. To obtain this information individually, we can use some methods
    associated with the `markovchain` object:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，物体的维度、状态和转移矩阵已被打印出来。为了单独获取这些信息，我们可以使用一些与`markovchain`对象相关的方法：
- en: 'For example, to get the states of the `markovchain` object, we can use the
    `states` method, as follows:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，要获取`markovchain`对象的状态，我们可以使用`states`方法，如下所示：
- en: '[PRE9]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following result is returned:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE10]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To get the dimension of the `markovchain` object, we can use the `dim` method,
    as follows:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获取`markovchain`对象的维度，我们可以使用`dim`方法，如下所示：
- en: '[PRE11]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following result is returned:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE12]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To see which elements are contained in the object we have created, we can use
    the `str()` function, which shows a compact view of the internal structure of
    an R object:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看我们创建的对象中包含哪些元素，我们可以使用`str()`函数，它显示了R对象内部结构的紧凑视图：
- en: '[PRE13]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following results are printed:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 打印以下结果：
- en: '[PRE14]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As we can see, four slots are listed: `states`, `byrow`, `transitionMatrix`,
    and `name`. To retrieve the elements contained in each one, we can use the name
    of the object (`MarkovChainModel`), followed by the name of the slot, separated
    by the `@` symbol.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，列出了四个槽：`states`、`byrow`、`transitionMatrix`和`name`。要检索每个槽中包含的元素，我们可以使用对象的名称（`MarkovChainModel`），后跟槽的名称，中间用`@`符号分隔。
- en: 'For example, to print the transition matrix, we will write the following:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，要打印转移矩阵，我们将写如下代码：
- en: '[PRE15]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following results are returned:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE16]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As we mentioned in the *Transition diagram* section, a very intuitive alternative
    to describing a Markov chain through the transition matrix is that of associating
    a Markov chain with an oriented graph (transition diagram).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*转移图*部分所提到的，通过转移矩阵描述马尔可夫链的一个非常直观的替代方法是将马尔可夫链与有向图（转移图）关联。
- en: Importing the diagram package
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入`diagram`包
- en: To plot a transition diagram, we can use the `diagram` package. This package
    contains several functions for visualizing simple graphs (networks) and plotting
    flow diagrams.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 要绘制转移图，我们可以使用`diagram`包。这个包包含了几个用于可视化简单图（网络）和绘制流程图的函数。
- en: 'We can see a short description of the `diagram` package, which has been extracted
    from the official documentation, in the following table:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下面的表格中看到从官方文档提取的`diagram`包的简短描述：
- en: '| Version | 1.6.4 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 版本 | 1.6.4 |'
- en: '| Date | 2017-08-16 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 日期 | 2017-08-16 |'
- en: '| Maintainer | Karline Soetaert |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 维护者 | Karline Soetaert |'
- en: '| License | GPL-2 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 许可证 | GPL-2 |'
- en: '| Authors | Karline Soetaert |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 作者 | Karline Soetaert |'
- en: 'Now, let''s learn how to use the functions that are available in the package
    to create a diagram:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何使用包中提供的函数来创建图：
- en: 'Let''s start by importing the library:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从导入库开始：
- en: '[PRE17]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we can create the `markovchain` object called `diagram`:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以创建名为`diagram`的`markovchain`对象：
- en: '[PRE18]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following diagram is printed:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 打印以下图：
- en: '![](img/e5690e1e-7f36-4277-8385-7fa14b1e1fd3.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5690e1e-7f36-4277-8385-7fa14b1e1fd3.png)'
- en: In the preceding diagram, we can see that the arrows that come out of the states
    always sum up exactly to 1, just like what happens for every row in the transition
    matrix, whose values must add up to exactly 1\. This represents the probability
    distribution.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到，所有从状态中流出的箭头的总和始终等于1，就像转移矩阵中每一行的值必须加起来等于1一样。这代表了概率分布。
- en: Obtaining transition probability
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取转移概率
- en: 'Something else that we can extract from the newly developed model is the transition
    probability, which represents the probability of passing from one state to another.
    Recall that a Markov chain is said to be homogeneous in time if the probabilities
    of the transition from one state to another are independent of the time index.
    To obtain this information, we will use the `transitionProbability()` function,
    which allows us to get the transition probabilities from initial to subsequent
    states. Let''s get started:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以从新开发的模型中提取转移概率，它表示从一个状态到另一个状态的概率。请记住，如果从一个状态到另一个状态的转移概率不依赖于时间索引，则称一个马尔可夫链是时间齐次的。要获取这些信息，我们将使用`transitionProbability()`函数，它可以帮助我们获取从初始状态到后续状态的转移概率。让我们开始吧：
- en: 'Use the following command to get this information:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令获取此信息：
- en: '[PRE19]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following result is returned:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE20]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We can confirm this result by analyzing the transition matrix and the transition
    diagram. In the transition matrix, the transition from the `Sunny` state to the
    `Rainy` state is given by the element `p31`, which is equal to 0.1\. In the same
    way, in the transition diagram, the branch that leaves the `Sunny` state to arrive
    at the `Rainy` state has a value of 0.1.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过分析转移矩阵和转移图来验证这个结果。在转移矩阵中，从`Sunny`状态到`Rainy`状态的转移由元素`p31`给出，其值为0.1。同样，在转移图中，从`Sunny`状态到`Rainy`状态的分支的值也为0.1。
- en: After correctly setting up our Markov chain-based model, it's time to use it
    to make predictions. But first, we need to set the initial state. Let's say we're
    starting from the sunny (`Sunny`) condition.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在正确设置好我们的基于马尔科夫链的模型后，接下来就是用它来做预测了。但首先，我们需要设定初始状态。假设我们从晴天（`Sunny`）状态开始。
- en: 'Based on the vector containing the three states of our model, this condition
    is represented by the vector (0,0,1). We can set this value like so:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于包含我们模型三种状态的向量，这个条件用向量 (0,0,1) 来表示。我们可以这样设置这个值：
- en: '[PRE21]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'For example, to calculate the state of time in 3 days, we can use a property
    of Markov chains. If X[n] is a homogeneous Markov chain with transition probability
    p[ij] and initial distribution p^([0]), then the following formula holds:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了计算3天后的天气状态，我们可以利用马尔科夫链的一个特性。如果 X[n] 是一个齐次马尔科夫链，具有转移概率 p[ij] 和初始分布 p^([0])，那么以下公式成立：
- en: '![](img/85823075-8c9a-49d5-bb41-a5fd1f0d3f9f.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85823075-8c9a-49d5-bb41-a5fd1f0d3f9f.png)'
- en: 'This formula in vector terms becomes as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式在向量形式下变成了如下：
- en: '![](img/db0b4f6a-900f-4cc0-a011-2cd0f71b561b.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db0b4f6a-900f-4cc0-a011-2cd0f71b561b.png)'
- en: In the preceding formula, p^([n]) and p^([0]) are row vectors and p^([0]) x
    P^([n]) represents a product between a row vector and a matrix (row by column
    product).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，p^([n]) 和 p^([0]) 是行向量，而 p^([0]) x P^([n]) 表示行向量与矩阵之间的乘积（行与列的乘积）。
- en: 'Let''s write this product using our model:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们用我们的模型来写出这个乘积：
- en: '[PRE22]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following result is returned:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE23]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In this way, we get a three-day forecast. To get a forecast for one week, we
    will write the following:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这样，我们得到了一个三天的预测。为了得到一周的预测，我们将写下如下代码：
- en: '[PRE24]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following result is returned:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE25]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Something else that we can get from the model we developed is the stationary
    distribution. The stationary distribution of a Markov chain with transition matrix
    P is a vector, π, so that π⋅P = π (in other words, π is invariant by the matrix
    P.). π is a row vector whose entries are probabilities summing to 1\. This is
    a probability distribution that remains constant as the Markov chain evolves over
    time.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从已开发的模型中还可以得到一个静态分布。一个马尔科夫链的静态分布是一个向量π，满足π⋅P = π（换句话说，π是由矩阵P保持不变的）。π是一个行向量，其元素是概率，总和为1。这个概率分布在马尔科夫链随着时间演变时保持不变。
- en: 'The `markovchain` package has a specific function, called the `steadyStates()`
    function, to obtain the stationary distribution of the Markov chain. Let''s call
    it:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`markovchain` 包含一个特定函数，叫做 `steadyStates()` 函数，用来获取马尔科夫链的静态分布。我们来调用它：'
- en: '[PRE26]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following result is returned:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE27]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, let''s learn how to generate a forecast of the state of time for a
    whole year, day after day, starting from a specific state. To do this, we can
    use the `rmarkovchain()` function, which returns a sequence of states from homogeneous
    or nonhomogeneous Markov chains. Let''s do this:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们学习如何从一个特定状态开始，逐日生成一整年的天气状态预测。为此，我们可以使用 `rmarkovchain()` 函数，它返回来自齐次或非齐次马尔科夫链的状态序列。让我们这样做：
- en: '[PRE28]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The following arguments are passed:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 传入以下参数：
- en: '`n`: Sample size'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n`: 样本大小'
- en: '`object`: Either a `markovchain` or a `markovchainList` object'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`object`: 一个 `markovchain` 或 `markovchainList` 对象'
- en: '`t0`: The initial state'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t0`: 初始状态'
- en: 'At this point, we can extract forecasts for each day of next year. Let''s print
    the forecasts for the next 40 days:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以提取明年每一天的预测结果。让我们打印接下来的40天的预测结果：
- en: '[PRE29]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following results are returned:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE30]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The prediction sequence is well-defined, starting from the initial state.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 预测序列是明确定义的，从初始状态开始。
- en: In the next section, we will develop models that use a reward to extend the
    characteristics of a Markov chain.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将开发使用奖励来扩展马尔科夫链特性的模型。
- en: Markov reward model
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔科夫奖励模型
- en: So far, we have dealt with Markov processes, random processes without memory,
    a sequence of random states that satisfy the Markov property, and much more. This
    process is defined through the space of the state S, and the transition function
    P, which determines its dynamics. In these models, there is no value associated
    with a specific state that allows us to reach a goal.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们处理了马尔科夫过程、没有记忆的随机过程、一系列满足马尔科夫性质的随机状态等等。这个过程是通过状态空间S和转移函数P来定义的，后者决定了其动态。在这些模型中，没有与特定状态相关联的值能够帮助我们达成目标。
- en: If we add a reward rate to each state, we get a Markov reward model, which represents
    a stochastic process that extends the characteristics of a Markov chain or a continuous-time
    Markov chain. The reward accumulated (R) over time is recorded in an additional
    variable. These concepts were introduced in [Chapter 2](aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml),
    *Building Blocks of Reinforcement Learning.* Now, let's try to apply these concepts
    to a practical case of forest management.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们为每个状态添加一个奖励率，我们就得到了一个马尔可夫奖励模型，它表示一个扩展了马尔可夫链或连续时间马尔可夫链特性的随机过程。随时间累积的奖励（R）被记录在一个额外的变量中。这些概念在[第2章](aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml)，*强化学习的基本构建块*中介绍。现在，让我们尝试将这些概念应用于森林管理的实际案例。
- en: Tiny forest management problem
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微型森林管理问题
- en: 'To understand these newly revised concepts, we will use an example from the
    `MDPToolbox` package (`mdp_example_forest`). This example deals with the management
    problem of a forest stand and has two main objectives:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这些新修订的概念，我们将使用`MDPToolbox`包中的一个示例（`mdp_example_forest`）。该示例处理森林经营管理问题，并有两个主要目标：
- en: The first objective is to maintain an old forest for wildlife.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个目标是保持一片古老的森林供野生动物栖息。
- en: The second objective is to earn money by selling the cut wood.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个目标是通过出售砍伐的木材赚取收入。
- en: 'To achieve these objectives, two actions are available: Wait or Cut. An action
    is decided for each 20-year period and applied at the beginning of the period.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这些目标，可以选择两种行动：等待或砍伐。每20年周期决定一次行动，并在该周期开始时应用。
- en: 'Three states are defined according to three tree age classes:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 根据三个树木年龄组定义了三种状态：
- en: '**state 1**: Age group 0-20 years'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态1**：年龄组0-20岁'
- en: '**state 2**: Age group 21-40 years'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态2**：年龄组21-40岁'
- en: '**state 3**: Age group over 40 years'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态3**：年龄组40年以上'
- en: State 3 corresponds to the oldest age class. At the end of a period t, if the
    state is s and the Wait action is chosen, the state at the next period will be
    given by the minimum of the two following values ​​(s + 1, 3)  if a fire does
    not occur. This is because, if there are no fires, then the trees age, but can
    never take on a state higher than 3\. There is a chance that a fire will burn
    the forest down after the application of the action, bringing the whole population
    back into position in the range of a younger age (state 1).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 状态3对应于最老的年龄组。在一个周期t结束时，如果状态是s并选择了等待动作，则下一个周期的状态将由以下两个值中的较小者给出（s + 1, 3），如果没有发生火灾。这是因为，如果没有发生火灾，树木会衰老，但永远不能超过状态3。如果发生火灾，应用动作后可能会烧毁森林，使得整个群体回到较年轻的年龄组（状态1）。
- en: Let's say p = 0.1 is the probability that a wildfire occurs during a period
    of time. The problem is how to manage this in the long term to maximize the reward. This
    problem can be treated as a Markov decision process.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 假设p = 0.1是某一时期内发生野火的概率。问题在于如何从长远来看管理这个过程，以最大化奖励。这个问题可以视为一个马尔可夫决策过程（MDP）。
- en: 'First, we define the transition matrix P (s, s'', a). Remember that it tells
    us what the probabilities are of passing from one state to another. Since the
    available actions are (Wait, Cut), we will define two matrices of transitions.
    If we denote the probability of a fire with p, then we will have the following
    transition matrix relating to the choice of action 1 (Wait):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义过渡矩阵P（s，s'，a）。记住，它告诉我们从一个状态到另一个状态的概率。由于可用的行动是（等待，砍伐），我们将定义两个过渡矩阵。如果我们用p表示火灾的概率，那么我们将得到与选择行动1（等待）相关的以下过渡矩阵：
- en: '![](img/047548c1-221a-47a9-ae90-20af3187227c.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/047548c1-221a-47a9-ae90-20af3187227c.png)'
- en: This is because, if we are in state 1, then we will have a probability of p
    remaining in that state (if a fire occurs) and the remaining 1-p probability of
    moving to the next state (if no fire occurs). While the probability of passing
    to state 3 is equal to 0, it isn't possible to pass from state 1 to 3 directly.
    On the other hand, if we are in state 2, we will have a probability of p passing
    into state 1 (if a fire occurs) and the remaining 1-p probability of passing to
    the next state, that is, 3 (if no fire occurs).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为，如果我们处于状态1，那么我们有概率p保持在该状态（如果发生火灾），而剩余的1-p概率转移到下一个状态（如果没有发生火灾）。虽然转移到状态3的概率为0，不能直接从状态1转移到状态3。另一方面，如果我们处于状态2，我们有概率p转移到状态1（如果发生火灾），而剩余的1-p概率转移到下一个状态，即状态3（如果没有发生火灾）。
- en: Here, the probability of remaining in state 2 is equal to 0. Finally, if we
    are in state 3, we will have a probability equal to p to go into state 1 (if a
    fire occurs) and the remaining 1-p probability to remain in state 3 (if no fire
    occurs) since this is the last to be possible over time. The probability of passing
    to state 2 is equal to 0.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，留在状态2的概率为0。最后，如果我们处于状态3，我们将有概率p进入状态1（如果发生火灾），其余的1-p概率将保持在状态3（如果没有发生火灾），因为这是时间上最后可能发生的情况。进入状态2的概率为0。
- en: 'Now, let''s define the transition matrix in terms of the choice of action 2
    (Cut):'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义在选择行动2（砍伐）情况下的转换矩阵：
- en: '![](img/22f260f3-5622-4d46-9fc2-b1f377a4ff75.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22f260f3-5622-4d46-9fc2-b1f377a4ff75.png)'
- en: In this case, its meaning is more intuitive when choosing to cut the wood. Here,
    the transition leads to state 1 in all three cases with a unit probability.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，当选择砍伐木材时，它的含义更加直观。这里，转换在三种情况下都以单位概率导致进入状态1。
- en: 'Now, we can define the two vectors of the rewards R (s, a):'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义奖励R(s, a)的两个向量：
- en: '![](img/7ab5efeb-3f8d-48d7-823d-fda0b0c67128.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ab5efeb-3f8d-48d7-823d-fda0b0c67128.png)'
- en: 'If the chosen action is to wait for the growth of the forest, then we will
    have 0 for the reward for the first two states and the maximum reward for state
    3\. In this case, we have chosen 4 as a reward, which represents the value that''s
    provided by the system by default. If the chosen action is to cut the wood instead,
    we will have the following formula:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如果选择的行动是等待森林生长，那么前两个状态的奖励将是0，而状态3的奖励将是最大值。在这种情况下，我们选择了4作为奖励，表示系统默认提供的值。如果选择的行动是砍伐木材，则我们将使用以下公式：
- en: '![](img/fd13cb68-1400-4a3e-b40c-47c0f438bbae.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd13cb68-1400-4a3e-b40c-47c0f438bbae.png)'
- en: 'Here, if the chosen action is to cut the wood, then we will have the following
    rewards: 0 for state 1, 1 for state 2, and 2 for state 3.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，如果选择的行动是砍伐木材，那么状态1的奖励为0，状态2的奖励为1，状态3的奖励为2。
- en: 'Our goal is to calculate a policy that allows us to get the maximum reward
    based on the settings we just developed:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是计算一个策略，使我们能够根据刚才开发的设置获得最大回报：
- en: 'Let''s look at the code that allows us to do this:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看实现这个的代码：
- en: '[PRE31]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let''s analyze the code line by line to understand the meaning of each command.
    Let''s start by importing the library:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们逐行分析代码，以理解每个命令的含义。首先，我们导入库：
- en: '[PRE32]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The `MDPtoolbox` package provides functions related to the resolution of discrete-time
    Markov decision processes, that is, finite horizon, value iteration, policy iteration,
    linear programming algorithms with some variants, and some functions related to
    reinforcement learning.  We can see a short description of the `MDPtoolbox` package,
    which can be extracted from the official documentation, in the following table:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '`MDPtoolbox`包提供了与离散时间马尔可夫决策过程求解相关的函数，也就是有限时间、值迭代、策略迭代、线性规划算法的一些变种，以及一些与强化学习相关的函数。我们可以从官方文档中提取出以下表格，简要描述`MDPtoolbox`包：'
- en: '| Version | 4.0.3 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 版本 | 4.0.3 |'
- en: '| Date | 2017-03-02 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 日期 | 2017-03-02 |'
- en: '| Maintainer | Guillaume Chapron |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 维护者 | Guillaume Chapron |'
- en: '| License | BSD_3_clause + file LICENSE |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 许可证 | BSD_3_clause + 文件 LICENSE |'
- en: '| Authors | Iadine Chades, Guillaume Chapron, Marie-Josee Cros, Frederick Garcia,
    Regis Sabbadin |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 作者 | Iadine Chades, Guillaume Chapron, Marie-Josee Cros, Frederick Garcia,
    Regis Sabbadin |'
- en: 'As we anticipated, the first thing we need to do is define the matrices P for
    the transition function and R for the reward function. First, we will use the
    data contained in the example supplied with the package:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们预期的那样，首先需要做的是定义转换函数的矩阵P和奖励函数的矩阵R。首先，我们将使用包中提供的示例数据：
- en: 'To do this, simply invoke the example, as follows:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为此，只需调用示例，如下所示：
- en: '[PRE33]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The object that we''ve created (a list, in this case) contains both the transition
    matrix P and the reward vectors. Let''s look at its content:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建的对象（在这种情况下是一个列表）包含了转换矩阵P和奖励向量。让我们看看它的内容：
- en: '[PRE34]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following results are returned:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是返回的结果：
- en: '[PRE35]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'To extract the transition matrices, we can write the following:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了提取转换矩阵，我们可以写出以下内容：
- en: '[PRE36]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'By doing this, we can see the two transition matrices:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做后，我们可以看到两个转换矩阵：
- en: '[PRE37]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'By default, `p = 0.1`. Here,  p represents the probability of a fire developing.
    Using this value, we can confirm the shape of the transition matrix. Let''s move
    on and view the reward vectors:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认情况下，`p = 0.1`。这里，p表示发生火灾的概率。使用这个值，我们可以确认转换矩阵的形状。接下来，我们来看奖励向量：
- en: '[PRE38]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The following results are returned:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是返回的结果：
- en: '[PRE39]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Before developing the model, it is necessary to verify that P and R satisfy
    the criteria that are necessary for the problem to be of the MDP type. To do this,
    we''ll use the `mdp_check()` function:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开发模型之前，需要验证 P 和 R 是否满足问题属于 MDP 类型所必需的标准。为此，我们将使用 `mdp_check()` 函数：
- en: '[PRE40]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This function checks whether the MDP that''s defined by the transition probability
    array (P) and the reward array (R) is valid. If P and R are correct, the function
    returns an empty error message. If they aren''t correct, the function returns
    an error message describing the problem. The following result is returned:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数检查由转移概率数组 (P) 和奖励数组 (R) 定义的 MDP 是否有效。如果 P 和 R 正确，函数将返回空的错误信息。如果它们不正确，函数将返回描述问题的错误信息。返回的结果如下：
- en: '[PRE41]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Here, we can see that the problem has been set. Now, we can search for the best
    policy for forest management.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到问题已经设定好了。现在，我们可以开始搜索最佳的森林管理策略。
- en: Policy iteration algorithm
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略迭代算法
- en: As we mentioned in [Chapter 2](aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml),
    *Building Blocks of Reinforcement Learning*, policy iteration is a dynamic programming
    algorithm that uses a value function to model the expected return for each pair
    of action-state. We will apply this method to the case in question.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第2章](aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml)中提到的，*强化学习的基础构建模块*，策略迭代是一个动态规划算法，它使用价值函数来模拟每对动作-状态的预期回报。我们将把这个方法应用到当前的问题中。
- en: 'At this point, we will try to solve the problem at hand using the `mdp_policy_iteration()`
    function:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们将尝试使用 `mdp_policy_iteration()` 函数来解决当前的问题：
- en: '[PRE42]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This function solves discounted MDP with the policy iteration algorithm. As
    we mentioned in [Chapter 2](aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml), *Building
    Blocks of Reinforcement Learning*, `policy iteration` is a dynamic programming
    algorithm that uses a value function to model the expected return for each pair
    of action-state. These techniques update the value functions using the immediate
    reward and the (discounted) value of the next state in a process called bootstrapping.
    Therefore, they imply the storage of Q (s, a) in tables or with approximate function
    techniques.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数通过策略迭代算法解决了折扣 MDP 问题。正如我们在[第2章](aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml)中提到的，*强化学习的基础构建模块*，`策略迭代`是一个动态规划算法，它使用价值函数来模拟每对动作-状态的预期回报。这些技术通过使用即时奖励和下一个状态的（折扣）价值来更新价值函数，这一过程称为自举（bootstrapping）。因此，它们意味着将
    Q (s, a) 存储在表格中或使用近似函数技术。
- en: 'Starting from an initial P0 policy, the iteration of the policy alternates
    between the following two phases:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 从初始的 P0 策略开始，策略的迭代在以下两个阶段之间交替进行：
- en: '`Policy evaluation`: Given the current policy P, estimate the action-value
    function QP.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`策略评估`：给定当前的策略 P，估计动作-价值函数 QP。'
- en: '`Policy Improvement`: If we calculate a better policy P '' based on QP, then
    set P'' as the new policy and return to the previous step.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`策略改进`：如果我们基于 QP 计算出一个更好的策略 P''，则将 P'' 设置为新的策略，并返回上一步。'
- en: When the QP value function can be calculated exactly for each action-state pair,
    the policy iteration with the greedy policy improvement leads to convergence by
    returning the optimal policy. Essentially, repeatedly executing these two processes
    converges the general process toward the optimal solution.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 当 QP 值函数可以准确计算每个动作-状态对时，采用贪婪策略改进的策略迭代会导致收敛，并返回最优策略。实际上，反复执行这两个过程使得整个过程向最优解收敛。
- en: 'The following arguments are passed:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 传递以下参数：
- en: '**P**: Transition probability array. P can be a three-dimensional array [S,S,A]
    or a list [[A]], with each element containing a sparse matrix [S,S].'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**P**：转移概率数组。P 可以是一个三维数组 [S,S,A] 或一个列表 [[A]]，每个元素包含一个稀疏矩阵 [S,S]。'
- en: '**R**: Reward array. R can be a three-dimensional array [S,S,A] or a list [[A]],
    with each element containing a sparse matrix [S,S] or a two-dimensional matrix
    [S,A] that''s possibly sparse.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R**：奖励数组。R 可以是一个三维数组 [S,S,A] 或一个列表 [[A]]，每个元素包含一个稀疏矩阵 [S,S] 或一个可能是稀疏的二维矩阵
    [S,A]。'
- en: '**discount**: Discount factor. The discount is a real that belongs to ]0; 1[.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**折扣**：折扣因子。折扣因子是一个实数，属于 ]0; 1[ 区间。'
- en: The policy iteration algorithm improves the policy iteratively using the evaluation
    of the current policy. Iterating is stopped when two successive policies are identical
    or when a specified number (`max_iter`) of iterations have been performed.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 策略迭代算法通过评估当前策略来逐步改进策略。当两个连续的策略相同时，或已执行指定次数（`max_iter`）的迭代时，迭代停止。
- en: 'The following results are returned:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '**V:** Optimal value function. V is an S length vector.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**V：** 最优值函数。V是一个长度为S的向量。'
- en: '**policy**: Optimal policy. The policy is an S length vector. Each element
    is an integer corresponding to an action that maximizes the value function.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**policy**：最优策略。策略是一个长度为S的向量。每个元素是一个整数，表示最大化值函数的行动。'
- en: '**iter**: Number of iterations.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**iter**：迭代次数。'
- en: '**cpu_time**: CPU time used to run the program.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cpu_time**：运行程序所用的CPU时间。'
- en: 'Now that the model is ready, we just have to evaluate the results by checking
    the obtained policy:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已准备就绪，我们只需通过检查获得的策略来评估结果：
- en: 'Let''s learn how to extract these results from our model:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们学习如何从模型中提取这些结果：
- en: '[PRE43]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The first element we have visualized is the optimal value function.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先可视化的元素是最优值函数。
- en: Recall that a value function represents how good a state is for an agent. It
    is equal to the total reward expected for an agent from the status s. The value
    function depends on the policy that the agent selects for the actions to be performed
    on.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，值函数表示一个状态对代理来说有多好。它等于代理从状态s预期的总奖励。值函数取决于代理为执行的动作选择的策略。
- en: 'The following results are returned:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE44]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let''s look at the policy that''s returned by the model:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看模型返回的策略：
- en: '[PRE45]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Recall that a policy defines the behavior of the learning agent at a given time.
    It maps the detected states of the environment and the actions to take when they
    are in those states. This corresponds to what in psychology would be called a
    set of rules or associations of stimulus-response. The policy is the fundamental
    part of a reinforcing learning agent in the sense that it alone is enough to determine
    behavior.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下，策略定义了在给定时间内学习代理的行为。它将环境中检测到的状态映射到在这些状态下要采取的行动。这相当于心理学中所说的刺激-反应规则或联结。策略是强化学习代理的核心部分，因为它单独足以决定行为。
- en: 'The following results are returned:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE46]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Here, the optimal policy is to not cut the forest in all three states. This
    is due to the low probability of developing a fire that causes the wait to be
    the best action to perform. In this way, the forest has time to grow and we can
    achieve both goals: maintain an old forest for wildlife and earn money by selling
    the cut wood.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，最优策略是在三个状态下都不砍伐森林。这是因为火灾发生的概率较低，导致等待成为最佳行动。这样，森林有时间生长，我们可以实现两个目标：保护老森林以供野生动物栖息，并通过销售砍伐的木材赚钱。
- en: 'Now, we can look at how many iterations the model has taken to converge:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以看看模型收敛所需的迭代次数：
- en: '[PRE47]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The following result is printed:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出以下结果：
- en: '[PRE48]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'As we can see, it only takes two iterations to get the result. Finally, we
    want to see how much CPU time it took to process the program:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如我们所见，只需两次迭代就能得到结果。最后，我们想看看处理程序所需的CPU时间：
- en: '[PRE49]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The following result is printed:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出以下结果：
- en: '[PRE50]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: This first example allowed us to understand how easy it is to derive an optimal
    policy from a well-posed problem. Now, let's look at what happens when we modify
    the starting conditions of the system.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这个第一个例子让我们理解了从一个已恰当表述的问题中得出最优策略是多么简单。现在，让我们看看当我们修改系统的初始条件时会发生什么。
- en: New state transition matrix
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新的状态转移矩阵
- en: 'So far, we have seen that, when the probability of a fire developing is low,
    the optimal policy advises us to wait and not cut the forest. But what happens
    if the probability of developing a fire is higher? Here, we just need to change
    the problem settings by changing the probability value p. Let''s get started:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到，当火灾发生的概率较低时，最优策略建议我们等待而不是砍伐森林。那么如果火灾发生的概率更高会怎样呢？在这里，我们只需通过更改概率值p来修改问题设置。让我们开始吧：
- en: 'The following code allows us to do this:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码允许我们做到这一点：
- en: '[PRE51]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let''s analyze the code line by line, focusing on the changes that are made
    to the initial code:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们逐行分析代码，重点关注对初始代码所做的更改：
- en: '[PRE52]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This is the modified code. We haven''t used the values that are provided by
    the problem; instead, we have set new values. Remember that the syntax of the
    `mdp_example_forest()` function is as follows:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这是修改后的代码。我们没有使用问题提供的值；相反，我们设置了新的值。请记住，`mdp_example_forest()`函数的语法如下：
- en: '[PRE53]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The following arguments are passed:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 传递以下参数：
- en: '**S** (optional): Number of states. S is an integer greater than 0\. By default,
    S is set to 3.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**S**（可选）：状态的数量。S是大于0的整数。默认情况下，S设置为3。'
- en: '**r1** (optional): The reward when the forest is in the oldest state and the
    Wait action is performed. r1 is a real greater than 0\. By default, r1 is set
    to 4.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**r1**（可选）：当森林处于最老状态且执行等待操作时的奖励。r1 是一个大于 0 的实数。默认情况下，r1 设置为 4。'
- en: '**r2** (optional): The reward when the forest is in the oldest state and the
    Cut action is performed. r2 is a real greater than 0\. By default, r2 is set to
    2.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**r2**（可选）：当森林处于最老状态且执行砍伐操作时的奖励。r2 是一个大于 0 的实数。默认情况下，r2 设置为 2。'
- en: '**p** (optional): The probability of a wildfire occurring. p is a real in ]0,
    1[. By default, p is set to 0.1.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**p**（可选）：火灾发生的概率。p 是一个实数，范围在 ]0, 1[ 之间。默认情况下，p 设置为 0.1。'
- en: Here, we have confirmed the three states, legally modified the rewards, and
    increased the probability that a fire will develop, bringing it from the initial
    value of 0.1 to the new value of 0.8.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们确认了三个状态，合法地修改了奖励，并增加了火灾发生的概率，将其从初始值 0.1 提高到新的值 0.8。
- en: 'Let''s see what results we get by making this change:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看通过进行此更改我们能得到什么结果：
- en: '[PRE54]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Here, we can see the two transition matrices:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到两个转移矩阵：
- en: '[PRE55]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Let''s move on and view the reward arrays:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续查看奖励数组：
- en: '[PRE56]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The following results are returned:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果被返回：
- en: '[PRE57]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Before developing the model, it is necessary to verify that P and R satisfy
    the criteria for the problem so that they''re of the MDP type. To do this, we''ll
    use the `mdp_check()` function:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开发模型之前，有必要验证 P 和 R 是否满足问题的标准，以确保它们属于 MDP 类型。为此，我们将使用 `mdp_check()` 函数：
- en: '[PRE58]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The following result is returned:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果被返回：
- en: '[PRE59]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Now that the problem has been set, we can try to solve the problem using the `mdp_policy_iteration` function:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在问题已经设置完毕，我们可以尝试使用 `mdp_policy_iteration` 函数来解决问题：
- en: '[PRE60]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Let''s extract the results from the model:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从模型中提取结果：
- en: '[PRE61]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The first element we have visualized is the optimal value function.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经可视化的第一个元素是最优价值函数。
- en: Recall that a value function represents how good a state is for an agent. It
    is equal to the total reward that's expected for an agent from the status s. The
    value function depends on the policy that the agent selects for the actions to
    be performed on.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下，价值函数表示一个状态对代理的价值。它等于代理从状态 s 中预期的总奖励。价值函数依赖于代理为执行动作选择的策略。
- en: 'The following results are returned:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果被返回：
- en: '[PRE62]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'By comparing this with the results we obtained in the initial model, we can
    see that the total rewards have decreased considerably. Let''s look at the policy
    that''s returned by the model:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将其与我们在初始模型中获得的结果进行比较，我们可以看到总奖励明显减少了。让我们看看模型返回的策略：
- en: '[PRE63]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The following results are returned:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果被返回：
- en: '[PRE64]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Here, we can see that the optimal policy has changed. In this case, in states
    1 and 3, the advice to choose the wait action remains. However, in state 2, it
    is recommended to cut, to avoid losing the wood that's been obtained so far.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到最优策略发生了变化。在这种情况下，在状态 1 和状态 3 中，建议选择等待操作。然而，在状态 2 中，建议进行砍伐，以避免失去至今获得的木材。
- en: 'Now, let''s look at how many iterations the model has to go through in order
    to converge:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看模型需要经历多少次迭代才能收敛：
- en: '[PRE65]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The following result is printed:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果被打印出来：
- en: '[PRE66]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Finally, we can see how much CPU time it took to process the program:'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以看到处理程序所需的 CPU 时间：
- en: '[PRE67]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The following result is printed:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果被打印出来：
- en: '[PRE68]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: This example has shown us how to modify the parameters of the problem. Here,
    we can see that by increasing the probability of developing a fire, the optimal
    policy that's developed by the model changes.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了如何修改问题的参数。在这里，我们可以看到，通过增加火灾发生的概率，模型开发出的最优策略发生了变化。
- en: Summary
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at stochastic processes and their applications. The
    theory of stochastic processes concerns the study of systems that evolve over
    time according to probabilistic laws. Due to this, we are interested in calculating
    the probabilities associated with it. For this reason, we learned about the basic
    concepts of probability. The a priori probability, joint probability, and conditional
    probability were all defined, followed by examples of how to calculate them.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们讨论了随机过程及其应用。随机过程理论研究的是根据概率法则随时间演变的系统。由于这一点，我们关注的是与之相关的概率计算。因此，我们学习了概率的基本概念。我们定义了先验概率、联合概率和条件概率，并通过示例展示了如何计算它们。
- en: Then, we were introduced to Markov chains. A Markov chain is a mathematical
    model of a random phenomenon that evolves over time in such a way that the past
    influences the future through the present. In other words, it represents the stochastic
    description of a sequence of possible events. The probability of each event depends
    on the state that was reached in the previous event. Here, we learned how to define
    and read a transition matrix and a transition diagram. We used Markov chains for
    forecasting the weather conditions for 365 consecutive days. Finally, we saw how
    to use the `MDPtoolbox` package to calculate the optimal policy for managing a
    tiny forest.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们被介绍了马尔科夫链。马尔科夫链是一种随机现象的数学模型，随着时间的推移以某种方式演变，使得过去通过现在影响未来。换句话说，它表示了一系列可能事件的随机描述。每个事件的概率取决于前一个事件所达到的状态。在这里，我们学习了如何定义和读取转移矩阵以及转移图。我们使用马尔科夫链预测了连续365天的天气状况。最后，我们看到了如何使用`MDPtoolbox`包来计算管理一个小森林的最优策略。
- en: In the next chapter, we will explore the basic concepts of the multi-armed bandit
    model. We will discover the different techniques that we can use and the meaning
    of the action-value implementation. We will learn how to address a problem using
    a contextual approach and learn how to implement asynchronous actor-critic agents.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨多臂赌博机模型的基本概念。我们将发现可以使用的不同技术以及行动值实现的含义。我们将学习如何通过上下文方法来解决问题，并学习如何实现异步演员-评论家代理。
