- en: '<html:html><html:head><html:title>Querying Our Data, Part 2 – Postprocessing
    and Response Synthesis</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-157">Querying Our Data, Part 2 – Postprocessing and Response Synthesis</html:h1>
    <html:div id="_idContainer077"><html:p>Building on the knowledge acquired in the
    previous chapter, we will now explore various postprocessing techniques to refine
    the retrieved context before covering the final query response synthesis. Afterward,
    we will learn how to bring all these components together into powerful query engines
    so that we can perform end-to-end natural language querying over documents. We’ll
    also get to practice our new skills by working on our <html:span class="No-Break">tutoring
    project.</html:span></html:p> <html:p>In this chapter, we’re going to cover the
    following <html:span class="No-Break">main topics:</html:span></html:p> <html:ul><html:li>Re-ranking,
    transforming, and filtering nodes <html:span class="No-Break">using postprocessors</html:span></html:li>
    <html:li>Understanding the <html:span class="No-Break">response synthesizers</html:span></html:li>
    <html:li>Implementing output <html:span class="No-Break">parsing techniques</html:span></html:li>
    <html:li>Building and using <html:span class="No-Break">query engines</html:span></html:li>
    <html:li>Hands-on – building quizzes <html:span class="No-Break">in PITS</html:span></html:li></html:ul>
    <html:a id="_idTextAnchor157"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Technical
    requirements</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-158">Technical requirements</html:h1> <html:div id="_idContainer077"><html:p>For
    this chapter, you will need to install the following packages in <html:span class="No-Break">your
    environment:</html:span></html:p> <html:ul><html:li><html:em class="italic">spaCy</html:em>
    : <html:a><html:span class="No-Break">https://spacy.io/</html:span></html:a></html:li>
    <html:li><html:span class="No-Break"><html:em class="italic">Guardrails-AI</html:em></html:span>
    <html:span class="No-Break">:</html:span> <html:a><html:span class="No-Break">https://www.guardrailsai.com/</html:span></html:a></html:li>
    <html:li><html:span class="No-Break"><html:em class="italic">pandas</html:em></html:span>
    <html:span class="No-Break">:</html:span> <html:a><html:span class="No-Break">https://pandas.pydata.org/</html:span></html:a></html:li></html:ul>
    <html:p>All the code samples in this chapter can be found in the <html:code class="literal">ch7</html:code>
    subfolder of this book’s GitHub <html:span class="No-Break">repository:</html:span>
    <html:a><html:span class="No-Break">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:a id="_idTextAnchor158"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Re-ranking,
    transforming, and filtering nodes using postprocessors</html:title></html:head>
    <html:body><html:div class="epub-source"><html:h1 id="_idParaDest-159">Re-ranking,
    transforming, and filtering nodes using postprocessors</html:h1> <html:div id="_idContainer077">from
    llama_index.core.postprocessor import SimilarityPostprocessor from llama_index.core
    import VectorStoreIndex, SimpleDirectoryReader reader = SimpleDirectoryReader(''files/other'')
    documents = reader.load_data() index = VectorStoreIndex.from_documents(documents)
    retriever = index.as_retriever(retriever_mode=''default'') nodes = retriever.retrieve(
    "What did Fluffy found in the gentle stream?" ) Print(''Initial nodes:'') for
    node in nodes: print(f"Node: {node.node_id} – Score: {node.score}") Pp = SimilarityPostprocessor(
        nodes=nodes,     similarity_cutoff=0.86 ) remaining_nodes = pp.postprocess_nodes(nodes)
    print(''Remaining nodes:'') for node in remaining_nodes: print(f"Node: {node.node_id}
    – Score: {node.score}") Initial nodes: Node: da51464d-e83f-4aec-a9db-8bd839ab3a4c
    - Score: 0.8516122822966049 Node: f839ec27-e487-4132-b139-79e3695d5500 - Score:
    0.8368901228748273 Remaining nodes: Node: da51464d-e83f-4aec-a9db-8bd839ab3a4c
    - Score: 0.8516122822966049 pip install spacy from llama_index.core.postprocessor
    import KeywordNodePostprocessor from llama_index.core.schema import TextNode,
    NodeWithScore nodes = [     TextNode(         text="Entry no: 1, <SECRET>, Attack
    at Dawn"     ),     TextNode(         text="Entry no: 2, <RESTRICTED>, Go to point
    Bravo"     ),     TextNode(         text="Entry no: 3, <PUBLIC>, text: Roses are
    Red"     ), ] node_with_score_list = [     NodeWithScore(node=node) for node in
    nodes ] pp = KeywordNodePostprocessor(     exclude_keywords=["SECRET", "RESTRICTED"]
    ) remaining_nodes = pp.postprocess_nodes(     node_with_score_list ) print(''Remaining
    nodes:'') for node_with_score in remaining_nodes:     node = node_with_score.node
        print(f"Text: {node.text}") from llama_index.core.postprocessor import     MetadataReplacementPostProcessor
    from llama_index.core.schema import TextNode, NodeWithScore nodes = [     TextNode(
            text="Article 1",         metadata={"summary": "Summary of article 1"}
        ),     TextNode(         text="Article 2",         metadata={"summary": "Summary
    of article 2"}     ), ] node_with_score_list = [     NodeWithScore(node=node)
    for node in nodes ] pp = MetadataReplacementPostProcessor(     target_metadata_key="summary"
    ) processed_nodes = pp.postprocess_nodes(     node_with_score_list ) for node_with_score
    in processed_nodes:     print(f"Replaced Text: {node_with_score.node.text}") Replaced
    Text: Summary of article 1 Replaced Text: Summary of article 2 from llama_index.core.postprocessor.optimizer
    import     SentenceEmbeddingOptimizer optimizer = SentenceEmbeddingOptimizer(
        percentile_cutoff=0.8,     threshold_cutoff=0.7 ) query_engine = index.as_query_engine(
        optimizer=optimizer ) response = query_engine.query("<your_query_here>") <html:p>In
    the previous chapter, we discussed the various retrieval methods that LlamaIndex
    offers. We extracted the necessary context to be able to enrich and improve the
    query we are now sending to the LLM. But is <html:span class="No-Break">this enough?</html:span></html:p>
    <html:p>As we have already discussed, <html:em class="italic">naive</html:em>
    retrieval methods are unlikely to produce ideal results in any scenario. There
    will probably be many situations where the returned nodes will perhaps contain
    irrelevant information or will not be sorted in chronological order. These kinds
    of situations could put the LLM in difficulty, adversely affecting the quality
    of the prompt that our RAG <html:span class="No-Break">application builds.</html:span></html:p>
    <html:p class="callout-heading">A quick side notes</html:p> <html:p class="callout">In
    case it wasn’t already obvious, the main purpose of a RAG flow is to programmatically
    build prompts. Instead of manually building these prompts and then inputting them
    into a ChatGPT-like interface, LlamaIndex dynamically assembles the prompts from
    our documents, which are split into nodes and then indexed and selected using
    retrievers. Many things could go wrong in this process. Maybe we didn’t ingest
    the original documents completely or correctly, or maybe we didn’t choose the
    right <html:code class="literal">chunk_size</html:code> value and ended up with
    nodes that were too granular or too loaded with irrelevant information. Maybe
    we didn’t index them correctly, or maybe the retriever we used simply didn’t select
    the nodes in the correct order or brought in more information than <html:span
    class="No-Break">we wanted.</html:span></html:p> <html:p>There are many points
    where errors could creep into the whole process. That doesn’t sound very encouraging,
    <html:span class="No-Break">does it?</html:span></html:p> <html:p>The good news
    is that we still have an opportunity to improve this context before the final
    step of sending the information to the LLM. This opportunity comes in the form
    of <html:strong class="bold">node postprocessors</html:strong> and <html:span
    class="No-Break"><html:strong class="bold">response synthesizers</html:strong></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>But first, let’s understand
    how <html:span class="No-Break">postprocessors work.</html:span></html:p> <html:p>Node
    postprocessors are critical in refining the results that are obtained from the
    retrieval process. That is because no matter how good the retrieval step is, there
    is always a chance of additional, unnecessary retrieved data <html:em class="italic">polluting</html:em>
    our context and confusing the LLM. In other cases, the retrieved nodes might be
    relevant but not necessarily in the correct order, and that can also affect the
    quality of the <html:span class="No-Break">LLM’s response.</html:span></html:p>
    <html:p><html:span class="No-Break"><html:em class="italic">Figure 7</html:em></html:span>
    <html:em class="italic">.1</html:em> depicts the role of the postprocessors in
    a <html:span class="No-Break">RAG workflow:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 7.1 – The role of node postprocessors in RAG</html:p> <html:p>These
    processors operate on a set of nodes, applying transformations or filters to enhance
    the relevance and quality of the information. They can be used on their own, to
    process a given set of nodes, but they are more commonly used within query engines,
    after the node retrieval step and before response synthesis. LlamaIndex provides
    various built-in processors but also the option of building custom <html:span
    class="No-Break">postprocessing logic.</html:span></html:p> <html:p>Let’s begin
    by understanding the different purposes and operating modes of <html:span class="No-Break">node
    postprocessors.</html:span></html:p> <html:a id="_idTextAnchor159"></html:a><html:h2
    id="_idParaDest-160">Exploring how postprocessors filter, transform, and re-rank
    nodes</html:h2> <html:p>At their core, all node postprocessors work by adjusting
    the retrieved context before that context gets injected into a prompt and sent
    to the LLM for response synthesis. They operate by either filtering, transforming,
    or re-ranking nodes. Let’s have a look at these operating modes to get a <html:span
    class="No-Break">better understanding.</html:span></html:p> <html:h3>Node filtering
    postprocessors</html:h3> <html:p>Node filtering postprocessors are designed to
    remove irrelevant or unnecessary nodes from the <html:a id="_idIndexMarker641"></html:a>set
    of retrieved results. They work by applying specific criteria to each node and
    discarding those that don’t meet the requirements. For example, <html:code class="literal">SimilarityPostprocessor</html:code>
    filters out nodes whose similarity score falls below a specified threshold, ensuring
    that only highly relevant nodes are passed to the language model for response
    generation. Similarly, <html:code class="literal">KeywordNodePostprocessor</html:code>
    keeps only the nodes that contain certain required keywords or excludes nodes
    with specific unwanted keywords. Node filtering helps to reduce information overload
    and improve the quality of the final response by focusing on the most <html:span
    class="No-Break">pertinent information.</html:span></html:p> <html:h3>Node transforming
    postprocessors</html:h3> <html:p>Node transforming postprocessors modify the content
    of the retrieved nodes without necessarily <html:a id="_idIndexMarker642"></html:a>removing
    any of them. These postprocessors aim to enhance the relevance and usefulness
    of the information within each node. One example is <html:code class="literal">MetadataReplacementPostprocessor</html:code>
    , which replaces the content of a node with a specific field from that node’s
    metadata. This allows the text being used to be dynamically adjusted to represent
    a node based on its metadata rather than the original ingested content. Another
    example is <html:code class="literal">SentenceEmbeddingOptimizer</html:code> ,
    which optimizes longer text passages by selecting the most relevant sentences
    within a node based on their semantic similarity to the query. By transforming
    the nodes’ content, these postprocessors help align the information more closely
    with the user’s query and improve the overall quality of the <html:span class="No-Break">generated
    response.</html:span></html:p> <html:h3>Node re-ranking postprocessors</html:h3>
    <html:p>These postprocessors don’t specifically remove or change the retrieved
    nodes. The purpose <html:a id="_idIndexMarker643"></html:a>of a re-ranker is to
    take the initial set of nodes returned by the retriever and reorder them based
    on their relevance to the given query. This is particularly important when dealing
    with long-form queries or complex information needs as many LLMs struggle to effectively
    process and generate accurate responses when provided with lengthy or multi-faceted
    contexts. By employing a re-ranker, the RAG system can prioritize the most pertinent
    information and present it to the LLM in a more coherent format, thus leading
    to <html:span class="No-Break">better responses.</html:span></html:p> <html:p>Re-rankers
    often leverage advanced techniques such as deep learning, transformers, or LLMs
    themselves to assess the relevance of each retrieved document or passage. They
    may consider factors such as semantic similarity, context overlap, or query-document
    alignment to <html:a id="_idIndexMarker644"></html:a>assign relevance scores to
    the retrieved nodes. The top-ranked nodes are then fed into the LLM, which generates
    the final response based on this refined context, enhancing the overall performance
    and utility of the RAG system. By incorporating a re-ranking step into the RAG
    pipeline, the system can overcome the limitations of LLMs in handling long or
    complex queries, ultimately providing more accurate, relevant, and useful responses
    <html:span class="No-Break">to users.</html:span></html:p> <html:p>Next, we’ll
    explore the built-in LlamaIndex postprocessors in all <html:span class="No-Break">three
    categories.</html:span></html:p> <html:a id="_idTextAnchor160"></html:a><html:h2
    id="_idParaDest-161">SimilarityPostprocessor</html:h2> <html:p><html:code class="literal">SimilarityPostprocessor</html:code>
    filters nodes by comparing them to a similarity score threshold. Nodes <html:a
    id="_idIndexMarker645"></html:a>that score below this threshold are removed, ensuring
    only relevant and similar content to the query remains. This is particularly useful
    because it ensures that the nodes that are passed to the language model for response
    generation are relevant by having a high degree of semantic correlation with <html:span
    class="No-Break">the query.</html:span></html:p> <html:p class="callout-heading">A
    potential use cases</html:p> <html:p class="callout">An e-commerce company has
    a customer support chatbot powered by an LLM. Let’s assume that the chatbot retrieves
    nodes from <html:code class="literal">KeywordTableIndex</html:code> and tries
    to identify all contexts based on the keywords contained in the user query. For
    a query such as, <html:em class="italic">How do I return a damaged item I received
    yesterday?</html:em> , the retrieved nodes might include general return policies,
    product descriptions for items ordered by the customer, shipping information,
    and even irrelevant product advertisements or promotions. <html:code class="literal">SimilarityPostprocessor</html:code>
    could filter out nodes that are not closely related to the specific context of
    the query. In this case, it would prioritize nodes specifically discussing return
    policies for damaged items and recent orders by the customer, while discarding
    general product advertisements and unrelated shipping details. That would greatly
    increase the chance of the LLM producing a more <html:span class="No-Break">meaningful
    response.</html:span></html:p> <html:p>This postprocessor takes a list of nodes,
    typically fetched by a retriever, as input, each with an associated similarity
    score. The postprocessor can be configured with a <html:code class="literal">similarity_cutoff</html:code>
    parameter. This threshold determines the minimum score a node must have to be
    considered relevant. If a node’s similarity score is <html:code class="literal">None</html:code>
    or if it’s lower than <html:code class="literal">similarity_cutoff</html:code>
    , the node is considered not to meet the threshold and is therefore excluded from
    the final list. Essentially, this postprocessor filters out any nodes that have
    a similarity score below the set threshold. This ensures that only nodes closely
    <html:a id="_idIndexMarker646"></html:a>related to the query are retained. The
    nodes meeting or exceeding the similarity score threshold is then passed on for
    further processing or response synthesis. Here’s a simple example of how we can
    use it <html:span class="No-Break">in practice:</html:span></html:p> <html:p>In
    the first part of the code, we took care of the imports and then ingested a sample
    file into a document. Then, we created a <html:code class="literal">VectorStoreIndex</html:code>
    index and used the default retriever to fetch relevant nodes based on <html:span
    class="No-Break">a query:</html:span></html:p> <html:p>Here, we printed the original
    list of nodes since they were fetched by the retriever. Now, let’s apply <html:span
    class="No-Break">the postprocessor.</html:span></html:p> <html:p>After building
    <html:a id="_idIndexMarker647"></html:a>and applying the postprocessor on the
    nodes, we print the remaining nodes. The output will be similar to <html:span
    class="No-Break">the following:</html:span></html:p> <html:p>As we can see, the
    second node from the initial list was removed because it had a score below the
    threshold we defined – <html:span class="No-Break">0.85.</html:span></html:p>
    <html:a id="_idTextAnchor161"></html:a><html:h2 id="_idParaDest-162">KeywordNodePostprocessor</html:h2>
    <html:p><html:code class="literal">KeywordNodePostprocessor</html:code> is designed
    to refine the selection of nodes based on specific keywords. This <html:a id="_idIndexMarker648"></html:a>postprocessor
    works by ensuring that the retrieved nodes either contain certain required keywords
    or exclude specific unwanted keywords. It’s a great method for aligning the content
    of the nodes more closely with the user’s query by focusing on <html:span class="No-Break">keyword
    relevance.</html:span></html:p> <html:p class="callout-heading">Practical use
    case in a RAG scenario</html:p> <html:p class="callout">Imagine a scenario in
    a corporate environment where the RAG system is used to retrieve information from
    a vast internal database for employee queries. However, there are certain confidential
    files or sections of files that should not be accessible to all employees. By
    configuring <html:code class="literal">KeywordNodePostprocessor</html:code> with
    keywords that indicate sensitive content (such as <html:em class="italic">confidential</html:em>
    , <html:em class="italic">restricted</html:em> , or specific project code names),
    the system can automatically exclude nodes containing these keywords from the
    retrieval results. This setup ensures that sensitive information is not inadvertently
    disclosed, maintaining the integrity and confidentiality of the <html:span class="No-Break">corporate
    data.</html:span></html:p> <html:p>It takes a list of nodes as input, typically
    fetched by a retriever, and is configured with parameters <html:a id="_idIndexMarker649"></html:a>for
    required and excluded keywords. <html:code class="literal">KeywordNodePostprocessor</html:code>
    then processes these nodes, keeping only those that meet the keyword criteria.
    This ensures that the final set of nodes is highly relevant to the specific query,
    leading to more accurate and useful responses in a <html:span class="No-Break">RAG
    system.</html:span></html:p> <html:p class="callout-heading">Quick note</html:p>
    <html:p class="callout">The postprocessor <html:a id="_idIndexMarker650"></html:a>relies
    on the <html:code class="literal">spaCy</html:code> library ( <html:a>https://pypi.org/project/spacy/</html:a>
    ), which you must install on your system before running the next example. This
    is a powerful Python library for advanced NLP. Its features include neural network
    models for various NLP tasks such as tagging, parsing, and NER. It’s a piece of
    commercial open source software available under an <html:span class="No-Break">MIT
    license.</html:span></html:p> <html:p>To use <html:code class="literal">KeywordNodePostprocessor</html:code>
    , make sure you install spaCy in your environment by running the <html:span class="No-Break">following
    command:</html:span></html:p> <html:p>Here’s a basic example of how to use this
    postprocessor to filter out some log entries based on their <html:span class="No-Break">classification
    label:</html:span></html:p> <html:p>In this example, we’re manually defining the
    nodes instead of ingesting data from external files. After we define the nodes,
    we have to wrap them into <html:code class="literal">NodeWithScore</html:code>
    because that’s <html:a id="_idIndexMarker651"></html:a>the expected input of <html:span
    class="No-Break">the postprocessor:</html:span></html:p> <html:p>In this example,
    <html:code class="literal">KeywordNodePostprocessor</html:code> filters the nodes
    fetched by the retriever, excluding those that include <html:code class="literal">SECRET</html:code>
    <html:span class="No-Break">and</html:span> <html:span class="No-Break"><html:code
    class="literal">RESTRICTED</html:code></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p>Several parameters <html:a id="_idIndexMarker652"></html:a>can be customized
    with this postprocessor. The most important ones are <html:span class="No-Break">as
    follows:</html:span></html:p> <html:ul><html:li><html:code class="literal">required_keywords</html:code>
    : This is a list of strings, where each string represents a keyword <html:a id="_idIndexMarker653"></html:a>that
    must be present in the node for it to be included in the final output. If this
    list is not empty, the postprocessor will filter out any nodes that do not contain
    <html:span class="No-Break">these keywords.</html:span></html:li> <html:li><html:code
    class="literal">exclude_keywords</html:code> : Similar to <html:code class="literal">required_keywords</html:code>
    , this is also a list of strings. However, in this case, any node containing a
    keyword from this list will be excluded <html:a id="_idIndexMarker654"></html:a>from
    the final output. It’s used for filtering out nodes based on <html:span class="No-Break">unwanted
    content.</html:span></html:li> <html:li><html:code class="literal">lang</html:code>
    : This argument <html:a id="_idIndexMarker655"></html:a>specifies the language
    model to be used by the internal spaCy NLP library. The default value is <html:em
    class="italic">en</html:em> for English, but it can be set to other language codes
    supported by Spacy. The effectiveness and accuracy of keyword matching might depend
    on the language-specific processing of the text. For example, the way words are
    tokenized by Spacy can affect how keywords <html:span class="No-Break">are identified.</html:span></html:li></html:ul>
    <html:p>Keep in mind that keywords – both required and excluded – are processed
    in a case-sensitive way. To ensure consistent behavior regardless of case, you
    might consider converting both the keywords and the text in the nodes into the
    same case (for example, all lowercase) <html:span class="No-Break">before processing.</html:span></html:p>
    <html:a id="_idTextAnchor162"></html:a><html:h2 id="_idParaDest-163">PrevNextNodePostprocessor</html:h2>
    <html:p><html:code class="literal">PrevNextNodePostprocessor</html:code> is designed
    to enhance node retrieval by fetching additional <html:a id="_idIndexMarker656"></html:a>nodes
    based on their relational context in the document. This postprocessor can operate
    in three modes – <html:code class="literal">previous</html:code> , <html:code
    class="literal">next</html:code> , or <html:code class="literal">both</html:code>
    – allowing users to retrieve nodes that are either preceding, succeeding, or both
    concerning the current set <html:span class="No-Break">of nodes.</html:span></html:p>
    <html:p class="callout-heading">A potential use cases</html:p> <html:p class="callout">Consider
    a legal research scenario where a user queries a RAG system about a specific legal
    case. <html:code class="literal">PrevNextNodePostprocessor</html:code> can be
    set in <html:em class="italic">both</html:em> modes to retrieve not only the nodes
    directly related to the case but also the preceding and succeeding nodes that
    might contain vital contextual information, such as related legal precedents or
    subsequent rulings. This ensures a comprehensive understanding of the case by
    providing a broader context, which is especially crucial in legal research where
    every <html:span class="No-Break">detail matters.</html:span></html:p> <html:p>The
    process begins by taking a list of nodes, typically fetched by a retriever. It
    then extends this <html:a id="_idIndexMarker657"></html:a>list by adding nodes
    that are directly preceding, succeeding, or both, based on the configured mode.
    This results in a more contextually enriched set of nodes, leading to responses
    that are more nuanced and comprehensive <html:a id="_idIndexMarker658"></html:a>in
    a RAG system. Here’s a list of the parameters for <html:span class="No-Break">this
    postprocessor:</html:span></html:p> <html:ul><html:li><html:code class="literal">docstore</html:code>
    : The actual document store storing <html:span class="No-Break">the nodes.</html:span></html:li>
    <html:li><html:code class="literal">num_nodes</html:code> : This sets the number
    of nodes to return. By default, it returns 1 node in the <html:span class="No-Break">chosen
    direction.</html:span></html:li> <html:li><html:code class="literal">mode</html:code>
    : Can be set to previous, next, <html:span class="No-Break">or both.</html:span></html:li></html:ul>
    <html:p>Additionally, we have <html:code class="literal">AutoPrevNextNodePostprocessor</html:code>
    , which is an advanced variation of <html:code class="literal">PrevNextNodePostprocessor</html:code>
    . This one is intelligently inferring whether to fetch additional nodes based
    on the <html:em class="italic">previous</html:em> , <html:em class="italic">next</html:em>
    , or neither relationship in response to the <html:span class="No-Break">query
    context.</html:span></html:p> <html:p>In comparison to <html:code class="literal">PrevNextNodePostprocessor</html:code>
    , which requires manual setting for mode selection, <html:code class="literal">AutoPrevNextNodePostprocessor</html:code>
    automates this process. It utilizes specific prompts to infer the direction (previous,
    next, or none) based on the current context and <html:span class="No-Break">the
    query.</html:span></html:p> <html:p>This inference is particularly useful in scenarios
    where the direction of node retrieval isn’t explicitly clear or when it needs
    to be dynamically determined based on the nature of the query and existing answers.
    For example, in a scenario where a RAG system is used for historical research,
    <html:code class="literal">AutoPrevNextNodePostprocessor</html:code> can automatically
    determine whether to fetch preceding or succeeding historical events or data points
    based on the query’s context, enhancing the relevance and comprehensiveness of
    <html:span class="No-Break">the response.</html:span></html:p> <html:p>This capability
    makes it useful in applications where the sequence of information and its contextual
    <html:a id="_idIndexMarker659"></html:a>relevance are essential for generating
    accurate and <html:span class="No-Break">useful responses.</html:span></html:p>
    <html:p>The prompts can be customized using the <html:code class="literal">infer_prev_next_tmpl</html:code>
    and <html:code class="literal">refine_prev_next_tmpl</html:code> arguments. There’s
    also a <html:code class="literal">Verbose</html:code> argument, which provides
    more visibility on the <html:span class="No-Break">selection process.</html:span></html:p>
    <html:a id="_idTextAnchor163"></html:a><html:h2 id="_idParaDest-164">LongContextReorder</html:h2>
    <html:p><html:code class="literal">LongContextReorder</html:code> is specifically
    designed to improve the performance of LLMs in handling <html:a id="_idIndexMarker660"></html:a>long
    context scenarios. Research has shown that significant details in extended contexts
    are better utilized when positioned at the start or end of the input context <html:em
    class="italic">(Liu et al., Lost in the Middle: How Language Models Use Long Contexts
    (2023)</html:em> – <html:a>https://arxiv.org/abs/2307.03172</html:a> ). The <html:code
    class="literal">LongContextReorder</html:code> postprocessor addresses this by
    reordering the nodes, placing crucial information where it’s more accessible to
    <html:span class="No-Break">the model.</html:span></html:p> <html:p class="callout-heading">A
    practical scenario</html:p> <html:p class="callout">In a RAG system, particularly
    in academic or research-oriented queries where long, detailed documents are common,
    <html:code class="literal">LongContextReorder</html:code> can be very useful.
    For instance, if a user queries about detailed historical events, the system might
    retrieve lengthy nodes encompassing extensive details. <html:code class="literal">LongContextReorder</html:code>
    would rearrange these nodes, ensuring that the most relevant details are positioned
    at the beginning or end, thereby enhancing the model’s ability to extract and
    utilize this crucial information effectively. This results in responses that are
    more coherent and contextually rich, significantly improving the overall quality
    of the output in cases involving <html:span class="No-Break">lengthy contexts.</html:span></html:p>
    <html:p><html:code class="literal">LongContextReorder</html:code> takes a list
    of nodes, typically fetched by a retriever, and reorders them based on their relevance
    scores. The goal is to optimize the arrangement of information in a way that maximizes
    the language model’s ability to access and process significant details, especially
    in cases where the context length might otherwise <html:span class="No-Break">hinder
    performance.</html:span></html:p> <html:p>This postprocessor is particularly effective
    in scenarios where detailed and comprehensive responses are required, ensuring
    that the most relevant information is presented in a way that is most accessible
    to <html:span class="No-Break">the model.</html:span></html:p> <html:a id="_idTextAnchor164"></html:a><html:h2
    id="_idParaDest-165">PIINodePostprocessor and NERPIINodePostprocessor</html:h2>
    <html:p>These <html:a id="_idIndexMarker661"></html:a>postprocessors mask <html:strong
    class="bold">personally identifiable information</html:strong> ( <html:strong
    class="bold">PII</html:strong> ) in nodes, improving privacy and security. <html:code
    class="literal">PIINodePostprocessor</html:code> is designed to use a local model,
    while <html:code class="literal">NERPIINodePostprocessor</html:code> relies on
    a NER model from Hugging Face. We saw an example of <html:a id="_idIndexMarker662"></html:a>how
    this postprocessor works in <html:a><html:span class="No-Break"><html:em class="italic">Chapter
    4</html:em></html:span></html:a> , <html:em class="italic">Ingesting Data into
    Our RAG Workflow</html:em> , in the <html:em class="italic">Scrubbing personal
    data and other sensitive</html:em> <html:span class="No-Break"><html:em class="italic">information</html:em></html:span>
    <html:span class="No-Break">section.</html:span></html:p> <html:p><html:code class="literal">PIINodePostprocessor</html:code>
    takes <html:a id="_idIndexMarker663"></html:a>the <html:span class="No-Break">following
    arguments:</html:span></html:p> <html:ul><html:li><html:code class="literal">llm</html:code>
    : This object should contain a local model <html:span class="No-Break">for processing.</html:span></html:li>
    <html:li><html:code class="literal">pii_str_tmpl</html:code> : This can be used
    to customize the default prompt template used for masking <html:span class="No-Break">personal
    data.</html:span></html:li> <html:li><html:code class="literal">pii_node_info_key</html:code>
    : This string serves as a key in the node’s metadata to store information related
    to PII processing. It’s used to track and reference the PII data processed within
    each node. It can be used to later recompose the original information <html:span
    class="No-Break">if required.</html:span></html:li></html:ul> <html:p><html:code
    class="literal">NERPIINodePostprocessor</html:code> can be configured with the
    <html:code class="literal">pii_node_info_key</html:code> parameter. Similar to
    <html:a id="_idIndexMarker664"></html:a>the previous postprocessor, this string
    key is used to store information related to PII processing in the node’s metadata.
    It’s a unique identifier within the node metadata for tracking the PII data that
    has <html:span class="No-Break">been processed.</html:span></html:p> <html:p class="callout-heading">Best
    practice</html:p> <html:p class="callout">As we discussed in <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 4</html:em></html:span></html:a>
    , <html:em class="italic">Ingesting Data into Our RAG Workflow</html:em> , for
    maximum privacy, the best approach is to apply PII masking before the actual retrieval.
    This way, you ensure that no sensitive data is sent to any <html:span class="No-Break">external
    LLM.</html:span></html:p> <html:p>Let’s see what other postprocessors <html:span
    class="No-Break">we have.</html:span></html:p> <html:a id="_idTextAnchor165"></html:a><html:h2
    id="_idParaDest-166">MetadataReplacementPostprocessor</html:h2> <html:p><html:code
    class="literal">MetadataReplacementPostProcessor</html:code> is designed to replace
    the content of a node <html:a id="_idIndexMarker665"></html:a>with a specific
    field from that node’s metadata. This allows us to dynamically switch the text
    that’s used to represent a node based on metadata instead of the original <html:span
    class="No-Break">ingested content.</html:span></html:p> <html:p class="callout-heading">A
    useful application for this postprocessor</html:p> <html:p class="callout">Imagine
    a workflow where files are ingested via <html:code class="literal">SentenceWindowNodeParser</html:code>
    , which splits text into sentence-level nodes and captures the surrounding text
    in metadata. By configuring the processor to swap the node’s content with the
    metadata field containing the <html:em class="italic">sentence window</html:em>
    , queries would retrieve full sentence context instead of sentence fragments.
    This allows the retriever to operate on sentences for higher accuracy while still
    exposing broader document context to the LLM. This technique can be very useful
    for processing large documents. You can find a complete example <html:span class="No-Break">here:</html:span>
    <html:a><html:span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/MetadataReplacementDemo.html</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>This postprocessor
    takes a list of nodes as input and is configured with the <html:code class="literal">target_metadata_key</html:code>
    parameter, specifying which metadata field to use for the replacement. <html:code
    class="literal">MetadataReplacementPostProcessor</html:code> processes the nodes
    by replacing the <html:code class="literal">text</html:code> attribute of each
    node with the contents of the given metadata key. If the key is missing, the original
    text is kept. This provides flexibility to transform node content on <html:span
    class="No-Break">the fly.</html:span></html:p> <html:p>Here’s another, simple
    example that will help you understand <html:span class="No-Break">its functionality:</html:span></html:p>
    <html:p>First, we defined two sample nodes on which we’ll now apply the postprocessor.
    We’ll instruct it <html:a id="_idIndexMarker666"></html:a>to replace the content
    of each node with the values stored in the <html:code class="literal">summary</html:code>
    <html:span class="No-Break">metadata field:</html:span></html:p> <html:p>After
    processing takes place, the output should look <html:span class="No-Break">like
    this:</html:span></html:p> <html:p>Let’s explore the other postprocessing options
    that <html:span class="No-Break">LlamaIndex provides.</html:span></html:p> <html:a
    id="_idTextAnchor166"></html:a><html:h2 id="_idParaDest-167">SentenceEmbeddingOptimizer</html:h2>
    <html:p><html:code class="literal">SentenceEmbeddingOptimizer</html:code> is built
    to optimize longer text passages by selecting <html:a id="_idIndexMarker667"></html:a>the
    most relevant sentences given a query based on semantic similarity. It uses advanced
    NLP techniques to score sentence relevance and discard less <html:span class="No-Break">useful
    sentences.</html:span></html:p> <html:p class="callout-heading">Why and where
    should we use it?</html:p> <html:p class="callout">In a workflow that’s ingesting
    lengthy documents, retrieving full passages may exceed model context size limits.
    <html:code class="literal">SentenceEmbeddingOptimizer</html:code> allows us to
    send only the most <html:a id="_idIndexMarker668"></html:a>important sentences
    to the LLM while preserving enough context. This prevents wasted tokens on irrelevant
    text by reducing noisy content. Removing irrelevant parts of the content also
    improves the response time and can greatly reduce the cost associated with the
    final <html:span class="No-Break">LLM call.</html:span></html:p> <html:p>The postprocessor
    takes a list of nodes as input and uses embeddings to analyze the semantic similarity
    of each sentence to the search query. Sentences closest to the query vector are
    retained while distant, unrelated sentences are <html:span class="No-Break">stripped
    away.</html:span></html:p> <html:p>This is how we use it <html:span class="No-Break">in
    practice:</html:span></html:p> <html:p>In this example, <html:code class="literal">SentenceEmbeddingOptimizer</html:code>
    uses a <html:code class="literal">percentile_cutoff</html:code> value of 0.8 and
    a <html:code class="literal">threshold_cutoff</html:code> value of <html:code
    class="literal">0.7</html:code> to select sentences. This means it aims to retain
    the top 80% of sentences by similarity score and further filters to include only
    those with similarity scores above <html:code class="literal">0.7</html:code>
    . The main parameters that can be customized <html:a id="_idIndexMarker669"></html:a>are
    <html:span class="No-Break">as follows:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">percentile_cutoff</html:code> : The percentage of top sentences
    above the similarity threshold to preserve. This allows us to compact nodes to
    the most relevant 75% of sentences, <html:span class="No-Break">for example.</html:span></html:li>
    <html:li><html:code class="literal">threshold_cutoff</html:code> : An absolute
    similarity score threshold where only sentences with similarity above this value
    are kept. This is useful for more <html:span class="No-Break">stringent filtering.</html:span></html:li>
    <html:li><html:code class="literal">context_before</html:code> and <html:code
    class="literal">context_after</html:code> : These allow us to keep several sentences
    before and after the matches for <html:span class="No-Break">more context.</html:span></html:li></html:ul>
    <html:p>In a similar fashion to <html:code class="literal">KeywordNodePostprocessor</html:code>
    , the <html:code class="literal">SentenceEmbeddingOptimizer</html:code> postprocessor
    removes less relevant sentences from nodes. However, in this case, it does so
    use vector search rather <html:span class="No-Break">than keywords.</html:span></html:p>
    <html:p>This postprocessor <html:a id="_idIndexMarker670"></html:a>is more about
    refining and shortening the content within each node for better alignment with
    the query. This allows for optimal information density tailored to the query while
    accounting for the <html:span class="No-Break">LLM’s limitations.</html:span></html:p>
    <html:p>In contrast, processors such as <html:code class="literal">KeywordNodePostprocessor</html:code>
    and <html:code class="literal">SimilarityPostprocessor</html:code> operate at
    the node level, keeping or removing entire nodes based on keywords or similarity
    <html:span class="No-Break">scores, respectively.</html:span></html:p> <html:a
    id="_idTextAnchor167"></html:a><html:h2 id="_idParaDest-168">Time-based postprocessors</html:h2>
    <html:p><html:strong class="bold">Time-based postprocessors</html:strong> are
    designed to prioritize recency and provide users with the <html:a id="_idIndexMarker671"></html:a>latest,
    most up-to-date information. They achieve this goal through various techniques,
    such as sorting nodes by <html:code class="literal">date</html:code> metadata,
    filtering based on embedding similarity, or applying time-decay <html:span class="No-Break">scoring
    models.</html:span></html:p> <html:p>Let’s get an overview of <html:span class="No-Break">these
    processors.</html:span></html:p> <html:h3>FixedRecencyPostprocessor</html:h3>
    <html:p>This simple postprocessor focuses results on the most recent data by sorting
    nodes based on their <html:code class="literal">date</html:code> metadata and
    then returning the <html:code class="literal">top_k</html:code> nodes sorted by
    date. This ensures <html:a id="_idIndexMarker672"></html:a>we get the latest data,
    which is critical for applications such as environmental monitoring, where having
    current <html:a id="_idIndexMarker673"></html:a>information is vital. For example,
    when querying about recent air quality metrics, the postprocessor guarantees that
    only the most up-to-date readings are provided. They focus the results on the
    <html:span class="No-Break">latest information.</html:span></html:p> <html:p>The
    two configurable parameters for this processor are <html:span class="No-Break">as
    follows:</html:span></html:p> <html:ul><html:li><html:code class="literal">top_k</html:code>
    : The number of top recent nodes <html:span class="No-Break">to return</html:span></html:li>
    <html:li><html:code class="literal">date_key</html:code> : The metadata key that’s
    used to identify the date in <html:span class="No-Break">each node</html:span></html:li></html:ul>
    <html:h3>EmbeddingRecencyPostprocessor</html:h3> <html:p>This postprocessor further
    refines recency-sorted results by comparing node contents using embedding similarity
    and removing those too similar to earlier nodes. Nodes that are too <html:a id="_idIndexMarker674"></html:a>similar
    to earlier ones are filtered out, ensuring that the content is both recent and
    diverse. The output it <html:a id="_idIndexMarker675"></html:a>produces is not
    just recent but also diverse in terms of the information <html:span class="No-Break">it
    contains.</html:span></html:p> <html:p><html:code class="literal">EmbeddingRecencyPostprocessor</html:code>
    sorts the nodes by date using the specified <html:code class="literal">date_key</html:code>
    metadata field. Then, it generates a query embedding for each node by inserting
    the node’s content into the <html:code class="literal">query_embedding_tmpl</html:code>
    template. This query embedding is used to find <html:span class="No-Break">similar
    documents.</html:span></html:p> <html:p class="callout-heading">Where could that
    be useful?</html:p> <html:p class="callout">Let’s think, for example, about a
    news aggregation service. When users query about a recent event, the system retrieves
    a set of nodes (news articles, in this case) sorted by date. However, many articles
    might cover the same event, leading to redundant information. <html:code class="literal">EmbeddingRecencyPostprocessor</html:code>
    examines these articles and filters out those that are too similar in content
    to more recent articles. This prevents us from presenting multiple redundant articles
    about the same event by eliminating those whose content significantly overlaps
    with more <html:span class="No-Break">recent coverage.</html:span></html:p> <html:p>Its
    <html:a id="_idIndexMarker676"></html:a>configurable parameters are <html:span
    class="No-Break">as follows:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">similarity_cutoff</html:code> : The threshold for embedding similarity,
    above which nodes are considered too similar and <html:span class="No-Break">filtered
    out</html:span></html:li> <html:li><html:code class="literal">date_key</html:code>
    : This specifies the metadata key that’s used for sorting nodes <html:span class="No-Break">by
    date</html:span></html:li> <html:li><html:code class="literal">query_embedding_tmpl</html:code>
    : This is the template that’s used to generate query embeddings for <html:span
    class="No-Break">each node</html:span></html:li></html:ul> <html:h3>TimeWeightedPostprocessor</html:h3>
    <html:p><html:code class="literal">TimeWeightedPostprocessor</html:code> prioritizes
    newer results by reranking nodes based on a <html:strong class="bold">time-decay
    function</html:strong> accounting <html:a id="_idIndexMarker677"></html:a>for
    how recently they were accessed. This favors fresh, less repeated content, which
    is critical for use cases such as trending news aggregation, where users want
    the latest updates rather than the <html:span class="No-Break">same information.</html:span></html:p>
    <html:p>The scoring dynamically adapts to changing access patterns over time.
    <html:code class="literal">TimeWeightedPostprocessor</html:code> is engineered
    to re-rank nodes based on their recency and <html:a id="_idIndexMarker678"></html:a>prior
    access history, applying a time-weighted scoring system. This postprocessor is
    particularly effective in scenarios where it’s crucial to <html:a id="_idIndexMarker679"></html:a>avoid
    repeatedly presenting the same information and where the freshness of <html:span
    class="No-Break">content matters.</html:span></html:p> <html:p>It works by adjusting
    the score of each node based on the last time it was accessed, applying a decay
    factor to prioritize less recently accessed content. This dynamic reranking ensures
    that the output is not just relevant but also timely and varied. This works great
    for applications where keeping the users updated with the most recent information
    <html:span class="No-Break">is essential.</html:span></html:p> <html:p>It also
    <html:a id="_idIndexMarker680"></html:a>has several parameters that we <html:span
    class="No-Break">can tweak:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">time_decay</html:code> : The decay factor for the <html:span class="No-Break">time-weighted
    scoring</html:span></html:li> <html:li><html:code class="literal">last_accessed_key</html:code>
    : Metadata key for tracking when a node was <html:span class="No-Break">last accessed</html:span></html:li>
    <html:li><html:code class="literal">time_access_refresh</html:code> : A Boolean
    to determine if the last accessed time should <html:span class="No-Break">be updated</html:span></html:li>
    <html:li><html:code class="literal">now</html:code> : An optional <html:a id="_idIndexMarker681"></html:a>parameter
    to set the current time. This is useful <html:span class="No-Break">for testing</html:span></html:li>
    <html:li><html:code class="literal">top_k</html:code> : The number of top nodes
    to return after reranking. The default value <html:span class="No-Break">is 1</html:span></html:li></html:ul>
    <html:p>With these advanced time-aware postprocessors, our RAG system transforms
    into a dynamic information curator, adept at navigating the temporal aspects of
    data. They ensure <html:a id="_idIndexMarker682"></html:a>that our system doesn’t
    just retrieve information <html:a id="_idIndexMarker683"></html:a>but smartly
    selects content that’s not only recent but also varied <html:span class="No-Break">and
    relevant.</html:span></html:p> <html:p>This makes them indispensable for scenarios
    where timely and diverse information is crucial, offering us a consistently fresh
    and <html:span class="No-Break">rich experience.</html:span></html:p> <html:a
    id="_idTextAnchor168"></html:a><html:h2 id="_idParaDest-169">Re-ranking postprocessors</html:h2>
    <html:p>Along with the basic processors we’ve discussed so far, LlamaIndex provides
    several more sophisticated options that make use of LLMs or embedding models for
    re-ranking nodes. As a <html:a id="_idIndexMarker684"></html:a>general principle,
    they work by re-ordering the nodes based on their relevance to the query, rather
    than removing them or altering their content. Some of these postprocessors, such
    as <html:code class="literal">SentenceTransformerRerank</html:code> , also update
    the relevance scores of the nodes to reflect their similarity to <html:span class="No-Break">the
    query.</html:span></html:p> <html:p>They all accept a <html:code class="literal">top_n</html:code>
    parameter, which specifies how many re-ordered nodes they should return. You can
    explore them in full detail by consulting the official <html:span class="No-Break">docs:</html:span>
    <html:a><html:span class="No-Break">https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>This section provides
    a quick overview of the available <html:span class="No-Break">LLM-based processors.</html:span></html:p>
    <html:h3>LLMRerank</html:h3> <html:p>This processor re-orders nodes by asking
    an LLM to assign relevance scores. It selects the <html:code class="literal">top_n</html:code>
    most relevant nodes from a given set based on the user’s query. The prompt that’s
    used by this postprocessor can be customized via the <html:span class="No-Break"><html:code
    class="literal">choice_select_prompt</html:code></html:span> <html:span class="No-Break">parameter.</html:span></html:p>
    <html:p>To increase <html:a id="_idIndexMarker685"></html:a>efficiency, it works
    in batches. The batch <html:a id="_idIndexMarker686"></html:a>size can also be
    customized by using the <html:code class="literal">choice_batch_size</html:code>
    argument. It requires a <html:code class="literal">query_bundle</html:code> argument
    for processing and uses the model configured in <html:code class="literal">llm</html:code>
    . Its reranking process involves formatting node contents into prompts, using
    the LLM to assess relevance, and then reordering nodes based on their calculated
    <html:span class="No-Break">relevance scores.</html:span></html:p> <html:h3>CohereRerank</html:h3>
    <html:p>This <html:a id="_idIndexMarker687"></html:a>processor re-ranks nodes
    using Cohere’s neural models ( <html:a>https://cohere.com/rerank</html:a> ) to
    sort <html:a id="_idIndexMarker688"></html:a>nodes by relevance. The default model
    that’s used is <html:em class="italic">rerank-english-v2.0</html:em> . The <html:code
    class="literal">top_n</html:code> nodes deemed <html:a id="_idIndexMarker689"></html:a>most
    relevant by the Cohere model are selected <html:span class="No-Break">and returned.</html:span></html:p>
    <html:p>This processor allows us to leverage powerful relevance algorithms provided
    by Cohere but requires a Cohere API key and their libraries to be installed in
    the <html:span class="No-Break">local environment.</html:span></html:p> <html:h3>SentenceTransformerRerank</html:h3>
    <html:p><html:code class="literal">SentenceTransformerRerank</html:code> uses
    sentence transformer models to re-rank nodes based <html:a id="_idIndexMarker690"></html:a>on
    their relevance to a <html:span class="No-Break">given query.</html:span></html:p>
    <html:p>This process <html:a id="_idIndexMarker691"></html:a>involves scoring
    nodes using a sentence transformer model, with the default being <html:em class="italic">cross-encoder/stsb-distilroberta-base</html:em>
    , and then reordering them based on these scores. It selects the top-ranked nodes
    to return, up to the specified <html:code class="literal">top_n</html:code> limit.
    You can find more information <html:span class="No-Break">here:</html:span> <html:span
    class="No-Break"><html:span class="P---URL"></html:span></html:span><html:a><html:span
    class="No-Break">https://www.sbert.net/examples/applications/retrieve_rerank/README.html</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:h3>RankGPTRerank</html:h3>
    <html:p>This re-ranking <html:a id="_idIndexMarker692"></html:a>postprocessor
    is designed to <html:a id="_idIndexMarker693"></html:a>improve retrieval results
    relevance using an LLM such as GPT-3.5\. It involves a process where the user’s
    query and content from nodes are formatted into prompts, guiding the language
    model to rank these nodes based <html:span class="No-Break">on relevance.</html:span></html:p>
    <html:p>The model’s output is then used to re-order the nodes, ensuring that the
    most relevant ones appear <html:a id="_idIndexMarker694"></html:a>at the top.
    When the context that’s retrieved is too large for the LLM’s context window, <html:code
    class="literal">RankGPTRerank</html:code> uses a sliding window approach to gradually
    re-rank a segment <html:span class="No-Break">of chunks.</html:span></html:p>
    <html:p>This method <html:a id="_idIndexMarker695"></html:a>is based on a paper
    by Sun et al. (2023), <html:em class="italic">Is ChatGPT Good at Search? Investigating
    Large Language Models as Re-Ranking</html:em> <html:span class="No-Break"><html:em
    class="italic">Agents</html:em></html:span> <html:span class="No-Break">(</html:span>
    <html:a><html:span class="No-Break">https://arxiv.org/abs/2304.09542v2</html:span></html:a>
    <html:span class="No-Break">).</html:span></html:p> <html:h3>LongLLMLinguaPostprocessor</html:h3>
    <html:p>This very useful postprocessor is designed to optimize node texts concerning
    queries by compressing them. It’s based on a method described in a paper by Jiang
    et al. (2023), <html:em class="italic">LLMLingua: Compressing Prompts for Accelerated
    Inference of Large Language</html:em> <html:span class="No-Break"><html:em class="italic">Models</html:em></html:span>
    <html:span class="No-Break">(</html:span> <html:a><html:span class="No-Break">https://arxiv.org/abs/2310.05736v2</html:span></html:a>
    <html:span class="No-Break">).</html:span></html:p> <html:p><html:code class="literal">LongLLMLinguaPostprocessor</html:code>
    addresses several issues associated with LLMs, such as increased API latency,
    context window limit overruns, and expensive <html:span class="No-Break">API costs.</html:span></html:p>
    <html:p>The key idea <html:a id="_idIndexMarker696"></html:a>is to intelligently
    <html:a id="_idIndexMarker697"></html:a>compress prompts in a way that they focus
    on the most relevant information, enabling more efficient and accurate processing
    by the LLM. It offers a balance between performance and efficiency, demonstrating
    that prompt compression – with up to 20x achievements – can lead to substantial
    improvements in model inference and cost-effectiveness without considerable loss
    <html:span class="No-Break">in performance.</html:span></html:p> <html:p>The processor
    is designed to work with a local, well-trained language model. This setup allows
    for the efficient compression of prompts for use with LLMs, supporting the optimization
    process locally without relying on external <html:span class="No-Break">API calls.</html:span></html:p>
    <html:p>You can find a complete demo <html:span class="No-Break">here:</html:span>
    <html:a><html:span class="No-Break">https://github.com/microsoft/LLMLingua/blob/main/examples/RAGLlamaIndex.ipynb</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:h3>Measuring the effectiveness
    of LLM-based re-ranking</html:h3> <html:p>A common source of concern – especially
    when using LLM-based re-rankers – is the quality of their output. Because LLMs
    are trained on vast amounts of data, they can sometimes generate results that
    are biased, inconsistent, or even factually incorrect. This is <html:a id="_idIndexMarker698"></html:a>particularly
    problematic when dealing with specialized domains or sensitive information. To
    verify that the LLM-based postprocessors are re-ranking the nodes well enough,
    it is important to properly evaluate their performance. Here are a few approaches
    you can use to gauge the quality of the <html:span class="No-Break">re-ranking
    step:</html:span></html:p> <html:ul><html:li><html:strong class="bold">Manual
    relevance assessment</html:strong> : Manually examine the re-ranked results to
    check if the most relevant nodes are indeed appearing at the top. This qualitative
    evaluation depends on human judgment to determine if the re-ranking matches the
    query’s intent. While not exactly very scientific, this simple approach may suffice
    for simple use cases, experiments, or non-production <html:span class="No-Break">RAG
    applications.</html:span></html:li> <html:li><html:strong class="bold">Benchmark
    datasets</html:strong> : Evaluate the re-ranking performance on standard <html:strong
    class="bold">information retrieval</html:strong> ( <html:strong class="bold">IR</html:strong>
    ) benchmarks <html:a id="_idIndexMarker699"></html:a>that have pre-defined queries
    and relevance judgments. This process can be time-consuming and it may require
    a well-prepared evaluation dataset but it will save you from troubles later in
    the RAG workflow. By comparing the re-ranked results against the ground truth,
    you can calculate metrics such as precision, recall, and others to quantify the
    re-ranking quality. We’ll cover the evaluation process in more detail in <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 9</html:em></html:span></html:a>
    , <html:em class="italic">Customizing and Deploying Our</html:em> <html:span class="No-Break"><html:em
    class="italic">LlamaIndex Project.</html:em></html:span></html:li> <html:li><html:strong
    class="bold">User feedback</html:strong> : In real-world applications, collect
    user feedback on the re-ranked search results. User satisfaction scores, click-through
    rates, or other engagement metrics can indicate if the re-ranking enhances the
    user experience and provides more relevant results. There’s an inherent advantage
    to this method. Because it relies on human feedback directly collected in the
    live environment, it becomes <html:a id="_idIndexMarker700"></html:a>a form of
    <html:strong class="bold">continuous evaluation</html:strong> . This makes it
    useful in <html:a id="_idIndexMarker701"></html:a>detecting any potential <html:strong
    class="bold">model drift</html:strong> , thus enabling timely adjustments to our
    pipeline to help us avoid quality degradation <html:span class="No-Break">over
    time.</html:span></html:li> <html:li><html:strong class="bold">A/B testing</html:strong>
    : Another form of gathering user feedback would be by running controlled experiments
    where some users are shown the original ranking, while others see the LLM-based
    re-ranked results. Compare the performance metrics between the two groups to assess
    if the re-ranking leads to <html:span class="No-Break">improved outcomes.</html:span></html:li>
    <html:li><html:strong class="bold">Domain expert evaluation</html:strong> : For
    specialized domains, ask subject matter experts to review the re-ranked results
    and provide feedback on their relevance and quality. While <html:a id="_idIndexMarker702"></html:a>more
    expensive and difficult than the other options, this method could be the best
    solution when dealing with highly technical or niche topics that require a deep
    understanding of the <html:span class="No-Break">subject matter.</html:span></html:li></html:ul>
    <html:p>The evaluation method you choose will depend on your specific use case,
    available resources, and the level of rigor you need. Using a mix of qualitative
    and quantitative approaches can give you a thorough assessment of the LLM’s <html:span
    class="No-Break">re-ranking performance.</html:span></html:p> <html:h3>Understanding
    the model drift phenomenon</html:h3> <html:p>While not necessarily specific to
    re-ranking, model drift can significantly impact the quality of <html:a id="_idIndexMarker703"></html:a>our
    RAG pipelines and it’s an important factor to consider. Our models are static
    representations of the snapshot datasets that are used for their training. But
    in time, that data changes. For example, new concepts may emerge that were not
    included in the training data, or the data itself may shift in distribution. This
    phenomenon is known as <html:em class="italic">model drift</html:em> , and it
    can manifest in <html:span class="No-Break">multiple forms:</html:span></html:p>
    <html:ul><html:li><html:strong class="bold">Data drift</html:strong> : This occurs
    when the statistical properties or distribution of the input data <html:a id="_idIndexMarker704"></html:a>change
    over time. For instance, if a model <html:a id="_idIndexMarker705"></html:a>was
    trained on a dataset of customer reviews from a specific period, it may not perform
    as well on newer reviews that contain different language patterns, sentiments,
    <html:span class="No-Break">or topics.</html:span></html:li> <html:li><html:strong
    class="bold">Concept drift</html:strong> : This happens <html:a id="_idIndexMarker706"></html:a>when
    the relationships <html:a id="_idIndexMarker707"></html:a>between the input features
    and the target variable evolve. In a RAG system designed to assist with medical
    queries, the introduction of new diseases, treatments, or medical terminology
    can lead to concept drift. The model’s understanding of the domain becomes outdated,
    and its performance <html:span class="No-Break">may degrade.</html:span></html:li>
    <html:li><html:strong class="bold">Upstream data changes</html:strong> : This
    type of drift happens when the data used to train the <html:a id="_idIndexMarker708"></html:a>model
    is different from the data used in production. For example, if a RAG system is
    trained on a curated dataset but then applied to raw, unprocessed data in production,
    the model’s performance may suffer due to differences in data quality, format,
    <html:span class="No-Break">or distribution.</html:span></html:li> <html:li><html:strong
    class="bold">Feedback loops</html:strong> : In some cases, the outputs of a model
    can influence its future inputs, creating a feedback loop. For instance, if a
    RAG system is used to recommend <html:a id="_idIndexMarker709"></html:a>articles
    to users, and those recommendations are then used to update the retrieval component,
    the model may become biased toward its previous outputs, leading to a narrowing
    of the information it provides <html:span class="No-Break">over time.</html:span></html:li>
    <html:li><html:strong class="bold">Domain shift</html:strong> : This occurs when
    a model is applied to a different domain or context than <html:a id="_idIndexMarker710"></html:a>it
    was originally trained for. In a RAG workflow, if the retrieval component is trained
    on data from one domain (for example, legal documents) but then used to answer
    queries in another domain (for example, medical questions), the model’s performance
    may suffer due to differences in language, terminology, or <html:span class="No-Break">underlying
    concepts.</html:span></html:li> <html:li><html:strong class="bold">Temporal drift</html:strong>
    : This type of drift is related to the passage of time and can encompass <html:a
    id="_idIndexMarker711"></html:a>both data drift and concept drift. As time passes,
    the data and concepts relevant to a particular task may evolve, leading to a gradual
    decline in model performance if <html:span class="No-Break">not addressed.</html:span></html:li></html:ul>
    <html:p>To mitigate these various types of model drift, it’s important to continuously
    monitor the performance of a RAG system, regularly update its retrieval component
    with new data, and adapt it to changes in the underlying data distribution, concepts,
    or domain. Additionally, implementing feedback loops carefully and ensuring that
    the training data is representative of the production environment can help minimize
    the impact of upstream data changes and feedback-related drift. This helps ensure
    that our RAG system remains accurate, up-to-date, and aligned with the evolving
    needs of <html:span class="No-Break">the users.</html:span></html:p> <html:a id="_idTextAnchor169"></html:a><html:h2
    id="_idParaDest-170">Final thoughts about node postprocessors</html:h2> <html:p>If
    the existing <html:a id="_idIndexMarker712"></html:a>ones are not exactly fit
    for our particular use case, we have the option to build our own. <html:strong
    class="bold">Custom postprocessors</html:strong> can be built by extending <html:code
    class="literal">BaseNodePostprocessor</html:code> . You can find a complete example
    <html:span class="No-Break">here:</html:span> <html:a><html:span class="No-Break">https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/root.html#custom-node-postprocessor</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p class="callout-heading">Important
    note</html:p> <html:p class="callout">In more complex scenarios, postprocessors
    can also be chained to apply multiple transformations to the nodes before they’re
    passed to the <html:span class="No-Break">response synthesizer.</html:span></html:p>
    <html:p>The key is <html:a id="_idIndexMarker713"></html:a>applying the right
    processors to remove noise, improve relevance signal, inject diversity, and handle
    sensitive content – leading to higher quality and more reliable <html:span class="No-Break">generated
    responses.</html:span></html:p> <html:p>For now, let’s shift our focus to the
    final piece of our puzzle: <html:span class="No-Break"><html:strong class="bold">response
    synthesizers</html:strong></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:a id="_idTextAnchor170"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Understanding
    response synthesizers</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-171">Understanding response synthesizers</html:h1> <html:div id="_idContainer077">from
    llama_index.core.schema import TextNode, NodeWithScore from llama_index.core import
    get_response_synthesizer nodes = [     TextNode(text=         "The town square
    clock was built in 1895"     ),     TextNode(text=         "A turquoise parrot
    lives in the Amazon"     ),     TextNode(text=         "A rare orchid blooms only
    at midnight"     ), ] node_with_score_list = [NodeWithScore(node=node) for node
    in nodes] synth = get_response_synthesizer(     response_mode="refine",     use_async=False,
        streaming=False, ) response = synth.synthesize(     "When was the clock built?",
        nodes=node_with_score_list ) print(response) The clock was built in 1895.
    <html:p>The final step before sending our hard-worked contextual data to the LLM
    is the response synthesizer. It’s the component that’s responsible for generating
    responses from a language <html:a id="_idIndexMarker714"></html:a>model using
    a user query and the <html:span class="No-Break">retrieved context.</html:span></html:p>
    <html:p>It simplifies the process of querying an LLM and synthesizing an answer
    across our proprietary data. Just like the other components of the framework,
    response synthesizers can be used on their own or configured in query engines
    to handle the final step of response generation after nodes have been retrieved
    <html:span class="No-Break">and postprocessed.</html:span></html:p> <html:p>Here’s
    a simple example demonstrating how to use one directly on a given set <html:span
    class="No-Break">of nodes:</html:span></html:p> <html:p>The first part <html:a
    id="_idIndexMarker715"></html:a>of the code, we’ve defined some arbitrary nodes.
    That’s going to be our <html:em class="italic">proprietary</html:em> context.
    Next, we’ll use a response synthesizer to run an LLM query based on <html:span
    class="No-Break">our context:</html:span></html:p> <html:p>The output is <html:span
    class="No-Break">as follows:</html:span></html:p> <html:p>Curious to take a peek
    under the hood? What happened in the background here? OK, bear with me for the
    next few lines – once you understand this example, you’ll know exactly how a response
    synthesizer works. Let me show you a <html:span class="No-Break">diagram first:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 7.2 – The refine response synthesizer</html:p>
    <html:p>Here’s a <html:a id="_idIndexMarker716"></html:a>description of <html:span
    class="No-Break">the process:</html:span></html:p> <html:ol><html:li>The synthesizer
    begins by building a special-purpose prompt, starting with the first node in the
    list as context. This prompt includes the query, specific instructions, and the
    context – which in this case is our first node. It uses a default value but can
    be customized via the System: "You are an expert Q&A system that is trusted around
    the world. Always answer the query using the provided context information, and
    not prior knowledge. Some rules to follow: 1\. Never directly reference the given
    context in your answer. 2\. Avoid statements like ''Based on the context, '' or
    ''The context information '' or anything along those lines." User: "Context information
    is below. The town square clock was built in 1895\. Given the context information
    and not prior knowledge, answer the query. Query: When was the clock built? Answer:
    " <html:span class="No-Break"><html:code class="literal">text_qa_template</html:code></html:span>
    <html:span class="No-Break">parameter:</html:span></html:li> <html:li>The next
    step is to send this prompt to the LLM and wait for <html:span class="No-Break">an
    answer.</html:span></html:li> <html:li>After the initial answer comes back, it
    builds the prompt for the next node while also integrating the first answer in
    the prompt and refining the final answer using a prompt that can be customized
    <html:span class="No-Break">with</html:span> <html:span class="No-Break"><html:code
    class="literal">refine_template</html:code></html:span> <html:span class="No-Break">.</html:span></html:li>
    <html:li>It then repeats this iterative process for all nodes while constantly
    refining the <html:span class="No-Break">final answer.</html:span></html:li> <html:li>Once
    the nodes are exhausted, it returns the <html:em class="italic">refined</html:em>
    <html:span class="No-Break">final answer.</html:span></html:li></html:ol> <html:p>In
    this case, the <html:a id="_idIndexMarker717"></html:a>behavior of the synthesizer
    is dictated <html:span class="No-Break">by</html:span> <html:span class="No-Break"><html:code
    class="literal">response_mode="refine"</html:code></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p>However, <html:code class="literal">refine</html:code> mode is just one
    of the several predefined synthesizers in LlamaIndex. Synthesizer mode can be
    specified using the <html:code class="literal">response_mode</html:code> parameter.
    Here’s a list of the available <html:a id="_idIndexMarker718"></html:a><html:span
    class="No-Break">response modes:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">refine</html:code> : As we saw in the previous example, <html:code
    class="literal">refine</html:code> queries each node individually using <html:code
    class="literal">text_qa_template</html:code> and <html:code class="literal">refine_template
    prompts</html:code> to iteratively construct a detailed response. This mode is
    ideal for constructing detailed responses, ensuring that each piece of information
    is carefully considered. We can also set <html:code class="literal">Verbose</html:code>
    to <html:code class="literal">True</html:code> for more visibility on the inner
    workings of this synthesizer and use <html:code class="literal">output_cls</html:code>
    to specify a <html:code class="literal">pydantic</html:code> object to use as
    a <html:span class="No-Break">response template.</html:span></html:li> <html:li><html:code
    class="literal">compact</html:code> : This one is similar to <html:code class="literal">refine</html:code>
    but it concatenates nodes to reduce the number of required LLM queries, balancing
    detail, <html:span class="No-Break">and efficiency.</html:span></html:li> <html:li><html:code
    class="literal">tree_summarize</html:code> : This mode uses recursive summarization,
    processing each node with <html:code class="literal">summary_template</html:code>
    . It recursively summarizes and queries nodes, concatenating them in each iteration
    until a single final response remains. It’s very useful for summarization and
    best suited for creating comprehensive summaries from multiple pieces <html:span
    class="No-Break">of information.</html:span></html:li> <html:li><html:code class="literal">simple_summarize</html:code>
    : This mode truncates nodes to fit in one LLM query for basic summarization. It’s
    great for brief overviews as it’s quick and cheap, but it may omit <html:span
    class="No-Break">finer details.</html:span></html:li> <html:li><html:code class="literal">accumulate</html:code>
    : This mode applies the query to each node individually and accumulates <html:a
    id="_idIndexMarker719"></html:a>the responses. It’s best suited for analyzing
    or comparing responses from <html:span class="No-Break">multiple sources.</html:span></html:li>
    <html:li><html:code class="literal">no_text</html:code> : In this operating mode,
    the response synthesizer fetches nodes without querying the LLM. This is mainly
    useful for debugging, analyzing raw data, or inspecting the retrieval or <html:span
    class="No-Break">postprocessing outputs.</html:span></html:li> <html:li><html:code
    class="literal">compact_accumulate</html:code> : A blend of compact and accumulate,
    this mode compacts prompts, similar to <html:code class="literal">compact</html:code>
    mode, and applies the query across nodes. This is especially suitable for efficiently
    processing <html:span class="No-Break">multiple sources.</html:span></html:li></html:ul>
    <html:p>In addition to <html:a id="_idIndexMarker720"></html:a>these predefined
    modes, custom response synthesizers can be created by subclassing <html:code class="literal">BaseSynthesizer</html:code>
    and implementing the <html:code class="literal">get_response</html:code> method.
    You can find a complete example in the official documentation: <html:a>https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html#custom-response-synthesizers</html:a>
    . This provides you with the flexibility to design specialized response <html:span
    class="No-Break">generation approaches.</html:span></html:p> <html:p>Features
    such as <html:code class="literal">structured_answer_filtering</html:code> can
    also be enabled on the <html:em class="italic">refine</html:em> and <html:em class="italic">compact</html:em>
    synthesizers. It uses the LLM to filter out retrieved nodes that are irrelevant
    to the question, improving <html:span class="No-Break">response quality.</html:span></html:p>
    <html:p>Prompt templates such as <html:code class="literal">text_qa_template</html:code>
    and <html:code class="literal">refine_template</html:code> allow us to customize
    the prompts that are used at different stages of response synthesis. Additional
    variables can also be passed to influence <html:span class="No-Break">response
    generation.</html:span></html:p> <html:p>Overall, response synthesizers handle
    the critical task of querying nodes and producing a final response, providing
    options to balance performance, customizability, <html:span class="No-Break">and
    accuracy.</html:span></html:p> <html:p>But guess what? We’re not out of the <html:span
    class="No-Break">woods yet.</html:span></html:p> <html:p>Let’s talk about another
    challenge in <html:span class="No-Break">our path.</html:span></html:p> <html:a
    id="_idTextAnchor171"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Implementing
    output parsing techniques</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-172">Implementing output parsing techniques</html:h1> <html:div
    id="_idContainer077">pip install guardrails-ai from langchain.output_parsers import
    (     StructuredOutputParser, ResponseSchema) from llama_index.core.output_parsers
    import LangchainOutputParser from llama_index.llms.openai import OpenAI from llama_index.core.schema
    import TextNode from llama_index.core import VectorStoreIndex from pydantic import
    BaseModel from typing import List nodes = [     TextNode(         text="Roses
    have vibrant colors and smell nice."),     TextNode(         text="Oak trees are
    tall and have green leaves."), ] schemas = [     ResponseSchema(         name="answer",
            description=(             "answer to the user''s question"         )     ),
        ResponseSchema(         name="source",         description=(             "the
    source text used to answer the user''s question, "             "should be a quote
    from the original prompt."         )     ) ] lc_parser = StructuredOutputParser.from_response_schemas(schemas)
    output_parser = LangchainOutputParser(lc_parser) llm = OpenAI(output_parser=output_parser)
    index = VectorStoreIndex(nodes=nodes) query_engine = index.as_query_engine(llm=llm)
    response = query_engine.query(     "Are oak trees small? yes or no", ) print(response)
    {''answer'': ''no'', ''source'': ''Oak trees are tall and have green leaves.''}
    <html:p>Our next topic addresses a common problem that’s encountered in RAG applications
    that rely on <html:a id="_idIndexMarker721"></html:a>structured outputs produced
    by an LLM. When those outputs are to become inputs in the next processing steps
    of the application, their structure becomes <html:span class="No-Break">very important.</html:span></html:p>
    <html:p class="callout-heading">A bit of background</html:p> <html:p class="callout">Due
    to their non-deterministic nature, LLMs have the bad habit of sometimes producing
    responses in a format other than the requested one, adding unsolicited comments
    or descriptions – just like humans if you think about it. Simply relying on clever
    prompting techniques may not be enough to completely avoid <html:span class="No-Break">this
    behavior.</html:span></html:p> <html:p>Even models specifically trained to follow
    precise instructions occasionally deviate from the structure we’ve requested.
    In cases where that output is simply returned to the user, this doesn’t matter
    much – it might even create a more <html:span class="No-Break">natural experience.</html:span></html:p>
    <html:p>The problems arise when the structure of the response matters – for example,
    when we are going to store that output in a set of variables and then send it
    to further processing. Have a look at <html:span class="No-Break"><html:em class="italic">Figure
    7</html:em></html:span> <html:em class="italic">.3</html:em> for a <html:span
    class="No-Break">better understanding:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 7.3 – LLMs may produce unpredictable outputs</html:p> <html:p>So,
    how can we make sure that we receive a structured and predictable output from
    an LLM? As usual, LlamaIndex comes to our rescue – this time in the form of the
    <html:strong class="bold">output parsers</html:strong> and <html:strong class="bold">Pydantic
    programs</html:strong> . Here’s an <html:a id="_idIndexMarker722"></html:a>overview
    of the methods that are used to ensure a <html:span class="No-Break">structured
    output.</html:span></html:p> <html:a id="_idTextAnchor172"></html:a><html:h2 id="_idParaDest-173">Extracting
    structured outputs using output parsers</html:h2> <html:p>Output parsers <html:a
    id="_idIndexMarker723"></html:a>are essential for managing the unpredictability
    of LLM <html:a id="_idIndexMarker724"></html:a>responses. They ensure that <html:a
    id="_idIndexMarker725"></html:a>outputs from LLMs are structured and formatted
    correctly for subsequent steps in an application. These parsers come in various
    forms, each with a unique approach to handling and refining <html:span class="No-Break">the
    output.</html:span></html:p> <html:h3>GuardrailsOutputParser</html:h3> <html:p>This
    particular <html:a id="_idIndexMarker726"></html:a>one is based on the <html:strong
    class="bold">Guardrails</html:strong> library provided by Guardrails AI: <html:a>https://www.guardrailsai.com/</html:a>
    . Guardrails ensures the outputs from LLMs adhere to specified structures and
    types. This is particularly useful in RAG applications, where outputs need to
    be consistent and structured for <html:span class="No-Break">further processing.</html:span></html:p>
    <html:p>Guardrails does <html:a id="_idIndexMarker727"></html:a>this by validating
    the LLM outputs against a <html:a id="_idIndexMarker728"></html:a>defined format
    and can take corrective actions such as re-asking the LLM if the outputs don’t
    meet the specified standards. This feature is essential for maintaining the integrity
    and usability of LLM outputs in <html:span class="No-Break">automated processes.</html:span></html:p>
    <html:p class="callout-heading">Under the hood</html:p> <html:p class="callout">At
    the core of how Guardrails works, we find the notion of <html:strong class="bold">rails</html:strong>
    . In the Guardrails library, a rail serves as a specification tool for LLM outputs.
    It is used to enforce specific structures, types, and validation criteria on these
    outputs. Rails can be defined using either <html:a id="_idIndexMarker729"></html:a>the
    <html:strong class="bold">Reliable AI Markup Language</html:strong> ( <html:strong
    class="bold">RAIL</html:strong> ) for structured outputs or directly in Python
    <html:span class="No-Break">Pydantic structures.</html:span></html:p> <html:p>The
    purpose of a rail is to ensure that the LLM outputs adhere to predefined quality
    and format standards, which includes setting validators and corrective actions
    if the output deviates from <html:span class="No-Break">these standards.</html:span></html:p>
    <html:p>This parser operates based on the <html:span class="No-Break">following
    logic:</html:span></html:p> <html:ol><html:li>First, it takes the initial prompt
    and an output format specification <html:span class="No-Break">as input.</html:span></html:li>
    <html:li>Based on the output format specification, it re-formats the prompt, adapting
    it for the <html:span class="No-Break">target LLM.</html:span></html:li> <html:li>It
    can also verify the output received from the LLM. If the specification is not
    validated, it can regenerate the output until the structure <html:span class="No-Break">is
    valid.</html:span></html:li></html:ol> <html:p>This parser <html:a id="_idIndexMarker730"></html:a>can
    be configured with the <html:span class="No-Break">following parameters:</html:span></html:p>
    <html:ul><html:li><html:code class="literal">guard</html:code> : An instance of
    the <html:code class="literal">Guard</html:code> class from the Guardrails library.
    This class encapsulates the core functionality of the Guardrails system. It is
    responsible for enforcing the specifications defined in a <html:span class="No-Break">RAIL
    structure</html:span></html:li> <html:li><html:code class="literal">llm</html:code>
    : This parameter is optional and is used to select the language model that’s used
    in conjunction with the <html:span class="No-Break">Guardrails parser</html:span></html:li>
    <html:li><html:code class="literal">format_key</html:code> : This optional parameter
    is useful when you want to inject specific formatting instructions into the query
    based on the output <html:span class="No-Break">format required</html:span></html:li></html:ul>
    <html:p>You can find a complete example of using this method <html:span class="No-Break">here:</html:span>
    <html:a><html:span class="No-Break">https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/output_parser.html#guardrails</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>Once you’ve <html:a
    id="_idIndexMarker731"></html:a>familiarized yourself with the RAIL language,
    the Guardrails library becomes an easy-to-use parsing solution for <html:span
    class="No-Break">your apps.</html:span></html:p> <html:p>Just make sure you install
    the Guardrails library in your environment first by running the <html:span class="No-Break">following
    command:</html:span></html:p> <html:p>In case you’re wondering how you could build
    an output parser and implement any custom guard rail logic in it, you can find
    a complete example <html:span class="No-Break">here:</html:span> <html:a href="https://docs.llamaindex.ai/en/latest/examples/output_parsing/llm_program/#define-a-custom-output-parser"
    target="_blank"><html:span class="No-Break">https://docs.llamaindex.ai/en/latest/examples/output_parsing/llm_program/#define-a-custom-output-parser</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:h3>LangchainOutputParser</html:h3>
    <html:p>Apart from <html:code class="literal">GuardrailsOutputParser</html:code>
    , LlamaIndex also supports the output parsers provided <html:span class="No-Break">by
    Langchain.</html:span></html:p> <html:p>Instead of <html:a id="_idIndexMarker732"></html:a>using
    the more complex RAIL language to define validation criteria and corrective actions,
    <html:code class="literal">LangchainOutputParser</html:code> relies on a simpler
    <html:a id="_idIndexMarker733"></html:a>concept called a <html:span class="No-Break"><html:strong
    class="bold">response schema</html:strong></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p>Response <html:a id="_idIndexMarker734"></html:a>schemas in Langchain
    are primarily used for structuring the output and focus on defining specific fields
    that the output should contain. These schemas guide the Langchain system to ensure
    that the output matches the <html:span class="No-Break">expected format.</html:span></html:p>
    <html:p>This approach is less about enforcing stringent validation rules or corrective
    actions and more about organizing the output data in a coherent and <html:span
    class="No-Break">predictable structure.</html:span></html:p> <html:p>Here’s an
    example that implements a very simple quotation system based on <html:span class="No-Break">this
    method:</html:span></html:p> <html:p>In the first part of our code, we took care
    of the necessary imports and then defined some random <html:em class="italic">proprietary
    data</html:em> contained in two nodes. Next, we must define the response <html:a
    id="_idIndexMarker735"></html:a>schemas that will be used to structure <html:a
    id="_idIndexMarker736"></html:a>the <html:span class="No-Break">LLM’s output:</html:span></html:p>
    <html:p>As you can see, the schema defines the expected output structure. Now,
    we can define the Langchain parser and an OpenAI <html:code class="literal">llm</html:code>
    object that’s been configured to <html:span class="No-Break">use it:</html:span></html:p>
    <html:p>Now, it’s time to build an index and <html:code class="literal">QueryEngine</html:code>
    from our Nodes. <html:code class="literal">QueryEngine</html:code> will be configured
    to use the Langchain parser so that it can structure <html:span class="No-Break">the
    output:</html:span></html:p> <html:p>The output is <html:span class="No-Break">as
    follows:</html:span></html:p> <html:p>Neat, <html:span class="No-Break">isn’t
    it?</html:span></html:p> <html:p>Note that <html:a id="_idIndexMarker737"></html:a>citations
    are useful in a RAG system as they increase <html:a id="_idIndexMarker738"></html:a>transparency
    and allow the answers to be validated against our <html:span class="No-Break">proprietary
    data.</html:span></html:p> <html:p>The Langchain <html:a id="_idIndexMarker739"></html:a>parser
    has two <html:span class="No-Break">configurable parameters:</html:span></html:p>
    <html:ul><html:li><html:code class="literal">output_parser</html:code> : This
    parameter accepts an instance of a Langchain output parser ( <html:code class="literal">LCOutputParser</html:code>
    ). This is where the primary logic for parsing and structuring the output is defined.
    As seen in the previous example, the parser provided here determines how the output
    from the LLM is processed <html:span class="No-Break">and formatted</html:span></html:li>
    <html:li><html:code class="literal">format_key</html:code> : This is an optional
    parameter that, if provided, is used to insert additional format instructions
    into the query. This can be particularly useful when the query needs to be formatted
    with specific instructions that guide the output generation of the <html:span
    class="No-Break">language model</html:span></html:li></html:ul> <html:p>While
    both <html:code class="literal">GuardrailsOutputParser</html:code> and <html:code
    class="literal">LangchainOutputParser</html:code> aim to structure and validate
    LLM outputs, their specific mechanisms and extent of control over the output format
    vary. The Langchain parser is more focused on processing the LLM output, while
    the Guardrails parser has a more proactive role in shaping the query and output
    format. We’ll talk about the other <html:span class="No-Break">method next.</html:span></html:p>
    <html:a id="_idTextAnchor173"></html:a><html:h2 id="_idParaDest-174">Extracting
    structured outputs using Pydantic programs</html:h2> <html:p>Pydantic programs
    represent another way to generate structured outputs. Pydantic programs <html:a
    id="_idIndexMarker740"></html:a>are a form of abstraction in LLM workflows that
    convert input strings into structured pydantic object types. They <html:a id="_idIndexMarker741"></html:a>can
    either call functions or rely on text completions, along with <html:span class="No-Break">output
    parsers.</html:span></html:p> <html:p>They are highly versatile and can be used
    for various applications, being both composable and adaptable for general or specific
    use cases. There are multiple programs available for various <html:span class="No-Break">use
    cases.</html:span></html:p> <html:p>You can find an overview and working examples
    <html:span class="No-Break">here:</html:span> <html:a><html:span class="No-Break">https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/pydantic_program.html</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>You’ll learn how to
    use a Pydantic program – in this case, <html:code class="literal">OpenAIPydanticProgram</html:code>
    , later in this chapter, when we continue working on our PITS <html:span class="No-Break">tutoring
    app.</html:span></html:p> <html:a id="_idTextAnchor174"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Building
    and using query engines</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-175">Building and using query engines</html:h1> <html:div id="_idContainer077">query_engine
    = index.as_query_engine() from llama_index.core.retrievers import SummaryIndexEmbeddingRetriever
    from llama_index.core.postprocessor import SimilarityPostprocessor from llama_index.core.query_engine
    import RetrieverQueryEngine from llama_index.core import (     SummaryIndex, SimpleDirectoryReader,
    get_response_synthesizer) documents = SimpleDirectoryReader("files").load_data()
    index = SummaryIndex.from_documents(documents) retriever = SummaryIndexEmbeddingRetriever(
        index=index,     similarity_top_k=3, ) response_synthesizer = get_response_synthesizer(
        response_mode="tree_summarize",     verbose=True ) pp = SimilarityPostprocessor(similarity_cutoff=0.7)
    query_engine = RetrieverQueryEngine(     retriever=retriever,     response_synthesizer=response_synthesizer,
        node_postprocessors=[pp] ) response = query_engine.query(     "Enumerate iconic
    buildings in ancient Rome" ) print(response) The iconic buildings in ancient Rome
    included the Colosseum and the Pantheon. from llama_index.core.tools import QueryEngineTool
    from llama_index.core.query_engine import RouterQueryEngine from llama_index.core.selectors
    import PydanticMultiSelector from llama_index.core import SummaryIndex, SimpleDirectoryReader
    from llama_index.core.extractors import TitleExtractor documents = SimpleDirectoryReader("files").load_data()
    title_extractor = TitleExtractor() for doc in documents:     title_metadata =
    title_extractor.extract([doc])     doc.metadata.update(title_metadata[0]) indexes
    = [] query_engines = [] tools = [] for doc in documents:     document_title =
    doc.metadata[''document_title'']     index = SummaryIndex.from_documents([doc])
        query_engine = index.as_query_engine(         response_mode="tree_summarize",
            use_async=True,     )     tool = QueryEngineTool.from_defaults(         query_engine=query_engine,
            description=f"Contains data about {document_title}",     )     indexes.append(index)
        query_engines.append(query_engine)     tools.append(tool) qe = RouterQueryEngine(
        selector=PydanticMultiSelector.from_defaults(),     query_engine_tools=tools
    ) response = qe.query(     "Tell me about Rome and dogs" ) print(response) from
    llama_index.core.tools import QueryEngineTool from llama_index.core.query_engine
    import RouterQueryEngine from llama_index.core.query_engine import SubQuestionQueryEngine
    from llama_index.core.selectors import PydanticMultiSelector from llama_index.core.extractors
    import TitleExtractor from llama_index.core import SummaryIndex, SimpleDirectoryReader
    documents = SimpleDirectoryReader("files/sample").load_data() title_extractor
    = TitleExtractor() for doc in documents:     title_metadata = title_extractor.extract([doc])
        doc.metadata.update(title_metadata[0]) indexes = [] query_engines = [] tools
    = [] for doc in documents:     document_title = doc.metadata[''document_title'']
        file_name = doc.metadata[''file_name'']     index = SummaryIndex.from_documents([doc])
        query_engine = index.as_query_engine(         response_mode="tree_summarize",
            use_async=True,     )     tool = QueryEngineTool.from_defaults(         query_engine=query_engine,
            name=file_name,         description=f"Contains data about {document_title}",
        )     indexes.append(index)     query_engines.append(query_engine)     tools.append(tool)
    qe = SubQuestionQueryEngine.from_defaults(     query_engine_tools=tools,     use_async=True
    ) response = qe.query(     "Compare buildings from ancient Athens and ancient
    Rome" ) print(response) <html:p>Our puzzle is now complete. Throughout the previous
    chapters, we’ve gradually learned about the key <html:a id="_idIndexMarker742"></html:a>ingredients
    in a RAG setup. Now, it’s time to bring everything together: the nodes, indexes,
    retrievers, postprocessors, response synthesizers, and <html:span class="No-Break">output
    parsers.</html:span></html:p> <html:p>In this chapter, we’ll focus on blending
    these elements into a complex construct: the query engine. We’ll learn about how
    query engines work and the neat tricks they have up <html:span class="No-Break">their
    sleeves.</html:span></html:p> <html:a id="_idTextAnchor175"></html:a><html:h2
    id="_idParaDest-176">Exploring different methods of building query engines</html:h2>
    <html:p>At its core, <html:code class="literal">QueryEngine</html:code> is an
    <html:a id="_idIndexMarker743"></html:a>interface that processes natural language
    queries to generate rich responses. It often relies on one or more indexes through
    retrievers and can also be combined with other query engines for <html:span class="No-Break">enhanced
    capabilities.</html:span></html:p> <html:p>The easiest <html:a id="_idIndexMarker744"></html:a>way
    to define <html:code class="literal">QueryEngine</html:code> is using the <html:strong
    class="bold">high-level API</html:strong> provided by LlamaIndex, <html:span class="No-Break">like
    this:</html:span></html:p> <html:p>With just a single line of code, we’ve built
    a simple query engine from an existing index. Although fast, this method uses
    <html:code class="literal">RetrieverQueryEngine</html:code> under the hood with
    the default settings and does not provide many opportunities <html:span class="No-Break">for
    customization.</html:span></html:p> <html:p>If we want <html:a id="_idIndexMarker745"></html:a>to
    have complete control over its parameters and full customization options, we can
    use the <html:strong class="bold">low-level API</html:strong> to explicitly build
    the <html:span class="No-Break">query engine.</html:span></html:p> <html:p>Let’s
    have <html:a id="_idIndexMarker746"></html:a>a look at <html:span class="No-Break">an
    example:</html:span></html:p> <html:p>As usual, we start by handling the imports.
    Next, we ingest our demo files and build a <html:span class="No-Break">simple</html:span>
    <html:span class="No-Break"><html:code class="literal">SummaryIndex</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p>Then, we throw in a
    retriever, a response synthesizer, and a node postprocessor. Building a query
    engine with this low-level API approach allows us to fully customize <html:span
    class="No-Break">each component:</html:span></html:p> <html:p>Now, it’s time <html:a
    id="_idIndexMarker747"></html:a>to bring them all together and assemble <html:span
    class="No-Break">our</html:span> <html:span class="No-Break"><html:code class="literal">QueryEngine</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p>The output should look
    similar to <html:span class="No-Break">the following:</html:span></html:p> <html:p>Now
    that we’ve built a simple query engine, let’s take a look at some more <html:span
    class="No-Break">advanced scenarios.</html:span></html:p> <html:a id="_idTextAnchor176"></html:a><html:h2
    id="_idParaDest-177">Advanced uses of the QueryEngine interface</html:h2> <html:p>The
    LlamaIndex <html:a id="_idIndexMarker748"></html:a>community has gradually developed
    – and continues to develop – various advanced query methods while using <html:code
    class="literal">QueryEngine</html:code> as a <html:span class="No-Break">main
    component.</html:span></html:p> <html:p>Apart from the query engines that I’m
    already covering in this book, <html:em class="italic">Table 7.1</html:em> provides
    an overview <html:a id="_idIndexMarker749"></html:a>of other available engines
    at the time <html:span class="No-Break">of writing:</html:span></html:p> <html:table
    class="No-Table-Style _idGenTablePara-1" id="table001-2"><html:thead><html:tr
    class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span class="No-Break"><html:strong
    class="bold">QueryEngine Class</html:strong></html:span></html:p></html:td> <html:td
    class="No-Table-Style"><html:p><html:strong class="bold">Short Description and</html:strong>
    <html:span class="No-Break"><html:strong class="bold">Use Cases</html:strong></html:span></html:p></html:td></html:tr></html:thead>
    <html:tbody><html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break"><html:code class="literal">CitationQueryEngine</html:code></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>Designed for situations requiring citations
    from multiple sources to support answers. It is especially useful in academic
    research, legal analysis, or any context where validated, source-based information
    is important. When generating responses, this query engine incorporates and cites
    relevant sources, ensuring answers are not only accurate but also verifiably supported
    by <html:span class="No-Break">documented evidence.</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break"><html:code class="literal">CogniswitchQueryEngine</html:code></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>Integrates with the Cogniswitch service
    ( <html:a>https://www.cogniswitch.ai/</html:a> ) to answer queries using a combination
    of Cogniswitch’s knowledge processing capabilities and <html:span class="No-Break">OpenAI’s
    models.</html:span></html:p></html:td></html:tr> <html:tr class="No-Table-Style"><html:td
    class="No-Table-Style"><html:p><html:span class="No-Break"><html:code class="literal">ComposableGraphQueryEngine</html:code></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>Designed to operate within a composable
    graph structure, enabling flexible, modular querying across different data sources
    and indices. It is ideal for complex data ecosystems where different types of
    information <html:span class="No-Break">are interconnected.</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break"><html:code class="literal">QASummaryQueryEngineBuilder</html:code></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>Combines <html:code class="literal">SummaryIndex</html:code>
    and <html:code class="literal">VectorStoreIndex</html:code> . This is useful both
    to retrieve specific information from documents and to get concise summaries <html:span
    class="No-Break">of content.</html:span></html:p></html:td></html:tr> <html:tr
    class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span class="No-Break"><html:code
    class="literal">TransformQueryEngine</html:code></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>Designed to preprocess queries using a
    specific transformation before they are submitted to an underlying query engine.
    When queries vary greatly in format or clarity, applying a transformation to normalize
    or enhance them can greatly <html:span class="No-Break">improve retrieval.</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break"><html:code class="literal">MultiStepQueryEngine</html:code></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>Works by decomposing complex queries into
    simpler, sequential steps. It can be useful for handling complex or multi-faceted
    questions that require a series of <html:span class="No-Break">logical steps.</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break"><html:code class="literal">ToolRetrieverRouterQueryEngine</html:code></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>Can dynamically choose from multiple candidate
    query engines based on the query’s context. It uses the most appropriate query
    engine tool for each <html:span class="No-Break">specific query.</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break"><html:code class="literal">SQLJoinQueryEngine</html:code></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>Designed for cases that require a combination
    of SQL database queries and additional information retrieval or processing. This
    is especially useful when the SQL query results need to be augmented or refined
    using <html:span class="No-Break">further queries.</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break"><html:code class="literal">SQLAutoVectorQueryEngine</html:code></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>Integrates SQL database queries with vector-based
    retrieval, enabling a two-step process where a query can be executed against a
    SQL database. Based on those results, further information can be fetched from
    a <html:span class="No-Break">vector store.</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break"><html:code class="literal">RetryQueryEngine</html:code></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>When the initial response to a query does
    not meet certain evaluation criteria, it automatically retries the query if it
    <html:span class="No-Break">fails evaluation.</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break"><html:code class="literal">RetrySourceQueryEngine</html:code></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>Designed to perform retries on a query
    with different source nodes based on evaluation criteria. If the initial response
    from the query engine does not pass the evaluator’s criteria, it attempts to find
    alternative source nodes that may yield a <html:span class="No-Break">better response.</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break"><html:code class="literal">RetryGuidelineQueryEngine</html:code></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>Similar to <html:code class="literal">RetryQueryEngine</html:code>
    , this one also transforms the query on each retry, based on feedback from the
    <html:span class="No-Break">evaluation process.</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break"><html:code class="literal">PandasQueryEngine</html:code></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>Converts natural language queries into
    executable pandas Python code, allowing for data manipulation and analysis over
    <html:span class="No-Break">pandas DataFrames.</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break"><html:code class="literal">JSONalyzeQueryEngine</html:code></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>Designed to analyze JSON list-shaped data
    by converting natural language queries into SQL queries that are executed within
    an in-memory <html:span class="No-Break">SQLite database.</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break"><html:code class="literal">KnowledgeGraphQueryEngine</html:code></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>Generates and processes queries for knowledge
    graphs, translating natural language queries into graph-specific queries and synthesizing
    responses based on graph query results. This is useful for applications requiring
    interaction with <html:span class="No-Break">knowledge graphs.</html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break"><html:code class="literal">FLAREInstructQueryEngine</html:code></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>Implementing the <html:strong class="bold">Forward-Looking
    Active REtrieval</html:strong> ( <html:strong class="bold">FLARE</html:strong>
    ) method, this query engine allows the model to continually access and incorporate
    external knowledge as it generates content. This is particularly useful for generating
    long, knowledge-intensive texts. By actively predicting future content needs and
    retrieving information accordingly, FLARE aims to reduce hallucinations and improve
    the factual accuracy of generated responses. It’s based on a paper by <html:span
    lang="en-US">Jiang et al. (2023),</html:span> <html:em class="italic">Active Retrieval
    Augmented</html:em> <html:span class="No-Break"><html:em class="italic">Generation</html:em></html:span>
    <html:span class="No-Break" lang="en-US">(</html:span> <html:span class="No-Break">https://arxiv.org/abs/2305.06983v2</html:span>
    <html:span class="No-Break">).</html:span></html:p></html:td></html:tr> <html:tr
    class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span class="No-Break"><html:code
    class="literal">SimpleMultiModalQueryEngine</html:code></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>A multi-modal query engine that can process
    queries involving both text and images, assuming that the retrieved text and images
    can fit within the LLM’s context window. It retrieves relevant text and images
    based on the query and then synthesizes a response using a <html:span class="No-Break">multi-modal
    LLM.</html:span></html:p></html:td></html:tr> <html:tr class="No-Table-Style"><html:td
    class="No-Table-Style"><html:p><html:span class="No-Break"><html:code class="literal">SQLTableRetrieverQueryEngine</html:code></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>Converts natural language queries into
    SQL queries but also synthesizes responses from the query results, making the
    responses more understandable and relevant to the user’s natural <html:span class="No-Break">language
    query.</html:span></html:p></html:td></html:tr> <html:tr class="No-Table-Style"><html:td
    class="No-Table-Style"><html:p><html:span class="No-Break"><html:code class="literal">PGVectorSQLQueryEngine</html:code></html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>Designed to work with PGvector (https://github.com/pgvector/pgvector),
    an extension for PostgreSQL that allows vectors to be stored and embedded directly
    within <html:span class="No-Break">the database.</html:span></html:p></html:td></html:tr></html:tbody></html:table>
    <html:p class="IMG---Caption" lang="en-US">Table 7.1 – Different query engine
    modules available in LlamaIndex</html:p> <html:p>The list of advanced implementations
    has already become so long that it could probably be the subject <html:a id="_idIndexMarker750"></html:a>of
    a separate book. Consequently, I did not set out to give a detailed presentation
    of each method. Instead, I encourage you to consult the official project documentation
    on the subject and discover how these building blocks can be used in various <html:span
    class="No-Break">scenarios:</html:span> <html:span class="No-Break">https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/modules.html</html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>There, you will find
    detailed explanations, use cases for each module, and, most importantly, code
    examples with which you can understand the operation and implementation of <html:span
    class="No-Break">each method.</html:span></html:p> <html:p>However, we cannot
    end this chapter without introducing you to at least a few essential modules in
    a RAG scenario. So, that’s what we are going to <html:span class="No-Break">cover
    next.</html:span></html:p> <html:h3>Implementing advanced routing with RouterQueryEngine</html:h3>
    <html:p>Remember <html:a id="_idIndexMarker751"></html:a>when we talked <html:a
    id="_idIndexMarker752"></html:a>about routing retrievers in <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 6</html:em></html:span></html:a>
    , <html:em class="italic">Querying Our Data, Part 1 – Context Retrieval</html:em>
    ? It’s time to see a more advanced routing mechanism, this time implemented at
    the query <html:span class="No-Break">engine level.</html:span></html:p> <html:p><html:span
    class="No-Break"><html:em class="italic">Figure 7</html:em></html:span> <html:em
    class="italic">.4</html:em> summarizes the operation <html:span class="No-Break">of</html:span>
    <html:span class="No-Break"><html:code class="literal">RouterQueryEngine</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 7.4 – How RouterQueryEngine works</html:p> <html:p><html:code
    class="literal">RouterQueryEngine</html:code> is capable of choosing between different
    tools it has available. Depending <html:a id="_idIndexMarker753"></html:a>on the
    user <html:a id="_idIndexMarker754"></html:a>query, the router will decide which
    <html:code class="literal">QueryEngineTool</html:code> should be used to generate
    <html:span class="No-Break">an answer.</html:span></html:p> <html:p>Just like
    in the case of retrievers, we can use <html:code class="literal">PydanticMultiSelector</html:code>
    or <html:code class="literal">PydanticSingleSelector</html:code> to configure
    its behavior. The multi-selector combines multiple options and can handle a broader
    spectrum of <html:span class="No-Break">user queries.</html:span></html:p> <html:p
    class="callout-heading">Potential use case</html:p> <html:p class="callout">Imagine
    a real-life scenario where an organization has its knowledge split into multiple
    individual documents. Such a router would allow for general queries over the entire
    knowledge base, while still enabling and precisely pinpointing the source data
    used to generate <html:span class="No-Break">the answer.</html:span></html:p>
    <html:p>In the following example, we’re building a <html:code class="literal">RouterQueryEngine</html:code>
    engine that operates different query engine tools – each one built over a different
    document. Here’s <html:span class="No-Break">the code:</html:span></html:p> <html:p>The
    first part of the code handles the imports and ingests our sample data. As before,
    we are <html:a id="_idIndexMarker755"></html:a>using two simple <html:a id="_idIndexMarker756"></html:a>text
    files: one containing information about ancient Rome and another containing a
    generic text about dogs. In the next part, we’ll go through each document and
    use <html:code class="literal">TitleExtractor</html:code> to extract a title and
    store it as a <html:span class="No-Break"><html:code class="literal">metadata</html:code></html:span>
    <html:span class="No-Break">field:</html:span></html:p> <html:p>Once the files
    have been ingested and we have generated document titles, we can define <html:code
    class="literal">SummaryIndex</html:code> , <html:code class="literal">QueryEngine</html:code>
    , and <html:code class="literal">QueryEngineTool</html:code> for each of the documents.
    We use the document title to provide the selector with a description of <html:span
    class="No-Break">each tool:</html:span></html:p> <html:p>Now <html:a id="_idIndexMarker757"></html:a>that
    we have a list <html:a id="_idIndexMarker758"></html:a>of available tools, we
    can build our <html:code class="literal">RouterQueryEngine</html:code> based <html:span
    class="No-Break">on</html:span> <html:span class="No-Break"><html:code class="literal">PydanticMultiSelector</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>To do this, we must
    pass the query engine tools as an argument. These will be the options that are
    available for <html:span class="No-Break">the selector:</html:span></html:p> <html:p>Depending
    on the query, the selector will decide which tools to use to gather responses.
    After each tool has responded, the query engine will synthesize and return a <html:span
    class="No-Break">final response:</html:span></html:p> <html:p>For relatively small
    documents, this method will probably work just fine. So long as the text is short
    enough to be properly summarized into a title, this query engine will handle most
    user queries pretty well. In a real-life scenario, though, it’s highly unlikely
    that we could <html:a id="_idIndexMarker759"></html:a>fully summarize <html:a
    id="_idIndexMarker760"></html:a>the whole content in a title. In that case, using
    a document summary instead of the title would <html:span class="No-Break">be preferable.</html:span></html:p>
    <html:h3>Querying multiple documents with SubQuestionQueryEngine</html:h3> <html:p>In
    a real-life scenario involving multiple data sources, as in the previous example,
    users may <html:a id="_idIndexMarker761"></html:a>come up with more complex queries
    – for example, they may ask for comparisons between different subjects <html:a
    id="_idIndexMarker762"></html:a>documented in different files. For this kind of
    situation, we can use <html:code class="literal">SubQuestionQueryEngine</html:code>
    . It is designed to handle complex queries by breaking them down into <html:span
    class="No-Break">smaller sub-questions.</html:span></html:p> <html:p>Each sub-question
    is processed by its designated query engine and the individual responses are then
    combined. A response synthesizer is used to compile these into a coherent final
    response, effectively managing queries that require a multi-faceted approach.
    <html:span class="No-Break"><html:em class="italic">Figure 7</html:em></html:span>
    <html:em class="italic">.5</html:em> describes <html:span class="No-Break">its
    operation:</html:span></html:p> <html:p class="IMG---Caption" lang="en-US">Figure
    7.5 – How SubQuestionQueryEngine works</html:p> <html:p>Let’s have a look at the
    code. The first part is very similar to our previous example <html:span class="No-Break">regarding</html:span>
    <html:span class="No-Break"><html:code class="literal">RouterQueryEngine</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p>After <html:a id="_idIndexMarker763"></html:a>importing
    the necessary <html:a id="_idIndexMarker764"></html:a>modules, we load the files
    and extract <html:span class="No-Break">their titles:</html:span></html:p> <html:p>So
    far, we have completed the same steps that we did for <html:code class="literal">RouterQueryEngine</html:code>
    . One notable change in the next part is that we also extract <html:code class="literal">file_name</html:code>
    from the metadata and use it as a name for the corresponding tool. This way, we’ll
    be able to tell exactly where each answer is <html:span class="No-Break">coming
    from:</html:span></html:p> <html:p>Next, let’s <html:a id="_idIndexMarker765"></html:a>build
    <html:span class="No-Break">our</html:span> <html:span class="No-Break"><html:code
    class="literal">SubQuestionQueryEngine</html:code></html:span> <html:span class="No-Break">:</html:span></html:p>
    <html:p>At this <html:a id="_idIndexMarker766"></html:a>point, we’re ready to
    generate <html:span class="No-Break">the output:</html:span></html:p> <html:p>Along
    with the final response, we’ll be able to see each sub-question generated and
    its corresponding query engine tool name. In our case, the tool name will correspond
    to the filename of each <html:span class="No-Break">source text.</html:span></html:p>
    <html:p><html:code class="literal">SubQuestionQueryEngine</html:code> is particularly
    useful for complex queries that cannot be <html:a id="_idIndexMarker767"></html:a>addressed
    directly <html:a id="_idIndexMarker768"></html:a>in a single step. It produces
    great results in cases such as <html:span class="No-Break">the following:</html:span></html:p>
    <html:ul><html:li><html:strong class="bold">Comparative analysis</html:strong>
    : For queries that require comparing and contrasting different subjects, the engine
    can divide the query into smaller, focused sub-questions to gather detailed information
    about each subject before synthesizing a comparative response. Here’s a sample
    question: <html:em class="italic">Compare and contrast the economic policies of
    Country A and Country B in the</html:em> <html:span class="No-Break"><html:em
    class="italic">last decade.</html:em></html:span></html:li> <html:li><html:strong
    class="bold">Multi-faceted questions</html:strong> : In cases where a query involves
    multiple aspects or criteria, this engine can break down the query into individual
    components, handle each separately, and then combine the results for a comprehensive
    answer. That means questions such as <html:em class="italic">What are the environmental,
    economic, and social impacts of deforestation in the</html:em> <html:span class="No-Break"><html:em
    class="italic">Amazon rainforest?</html:em></html:span></html:li> <html:li><html:strong
    class="bold">Complex research tasks</html:strong> : For research-oriented queries
    that require information to be gathered from various sources or perspectives,
    this engine can efficiently handle the task by segmenting it into more manageable
    sub-questions. Here’s the type of query it could answer: <html:em class="italic">Investigate
    the historical development of renewable energy technologies and their adoption
    across</html:em> <html:span class="No-Break"><html:em class="italic">different
    continents.</html:em></html:span></html:li></html:ul> <html:p>Now that you’ve
    got a general understanding of how query engines work, I’ll let you explore the
    different possibilities and experiment with all the existing query <html:span
    class="No-Break">engine modules.</html:span></html:p> <html:p>In case you’re wondering
    whether you can create custom ones, that option is <html:span class="No-Break">also
    available.</html:span></html:p> <html:p>You can find an example <html:span class="No-Break">here:</html:span>
    <html:span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/query_engine/custom_query_engine.html#option-1-ragqueryengine</html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>Now that we’ve got
    some fresh knowledge, it’s about time we built some new components into our <html:span
    class="No-Break">tutoring project.</html:span></html:p> <html:a id="_idTextAnchor177"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Hands-on
    – building quizzes in PITS</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-178">Hands-on – building quizzes in PITS</html:h1> <html:div id="_idContainer077">from
    llama_index.core import load_index_from_storage, StorageContext from llama_index.program.evaporate.df
    import DFRowsProgram from llama_index.program.openai import OpenAIPydanticProgram
    from global_settings import INDEX_STORAGE, QUIZ_SIZE, QUIZ_FILE import pandas
    as pd pip install pandas def build_quiz(topic):     df = pd.DataFrame({         "Question_no":
    pd.Series(dtype="int"),         "Question_text": pd.Series(dtype="str"),         "Option1":
    pd.Series(dtype="str"),         "Option2": pd.Series(dtype="str"),         "Option3":
    pd.Series(dtype="str"),         "Option4": pd.Series(dtype="str"),         "Correct_answer":
    pd.Series(dtype="str"),         "Rationale": pd.Series(dtype="str"),     }) <html:p>One
    of the features we are building in our PITS project is the ability to generate
    quizzes based on the learning material uploaded by <html:span class="No-Break">the
    user.</html:span></html:p> <html:p>These quizzes <html:a id="_idIndexMarker769"></html:a>will
    initially be used to gauge the overall knowledge of the user on the topic. Based
    on that assessment, the training slides and narration will be adjusted to the
    level of <html:span class="No-Break">the learner.</html:span></html:p> <html:p>The
    same mechanism can also be used to generate intermediate quizzes at the end of
    each section to test the user’s current knowledge. Let’s see how we can easily
    implement the quiz <html:span class="No-Break">builder feature.</html:span></html:p>
    <html:p>We’ll be using one of the LlamaIndex pre-packaged pydantic programs: the
    DataFrame Pydantic extractor. This is designed to extract tabular DataFrames from
    <html:span class="No-Break">raw text.</html:span></html:p> <html:p>Let’s have
    a look at the code <html:span class="No-Break">in</html:span> <html:span class="No-Break"><html:code
    class="literal">quiz_builder.py</html:code></html:span> <html:span class="No-Break">:</html:span></html:p>
    <html:p>First, we imported all the necessary modules, including our global variables
    defined <html:span class="No-Break">in</html:span> <html:span class="No-Break"><html:code
    class="literal">global_settings.py</html:code></html:span> <html:span class="No-Break">:</html:span></html:p>
    <html:ul><html:li><html:code class="literal">INDEX_STORAGE</html:code> : The index’s
    <html:span class="No-Break">storage location</html:span></html:li> <html:li><html:code
    class="literal">QUIZ_SIZE</html:code> : The number of questions to be included
    in <html:span class="No-Break">a quiz</html:span></html:li> <html:li><html:code
    class="literal">QUIZ_FILE</html:code> : The path where the quiz will be saved
    as <html:span class="No-Break">a CSV</html:span></html:li></html:ul> <html:p>We’re
    also importing the <html:code class="literal">load_index_from_storage</html:code>
    function, which we will use to fetch our indexes from storage to avoid the cost
    and time of <html:span class="No-Break">rebuilding them.</html:span></html:p>
    <html:p>Because we’re using DataFrames, we’ll also need to import the pandas library.
    If you don’t have it already installed in your environment, make sure you run
    <html:span class="No-Break">this first:</html:span></html:p> <html:p>OK – let’s
    build <html:a id="_idIndexMarker770"></html:a>our main function. The <html:code
    class="literal">build_quiz</html:code> function will be responsible for generating
    the quiz and saving the questions in a <html:code class="literal">CSV</html:code>
    file for <html:span class="No-Break">further use:</html:span></html:p> <html:ol><html:li>First,
    we set up a DataFrame to structure the quiz questions and their associated options
    and answers. This DataFrame will serve as the foundation for our quiz. It includes
    columns for the question number, question text, four answer options, the correct
    answer, and a rationale for the answer. The use of a pandas DataFrame will make
    handling and manipulating the quiz data <html:span class="No-Break">much easier.</html:span></html:li>
    <html:li>Next, we need to load our vector index from storage. To do this, we must
    define a <html:code class="literal">StorageContext</html:code> object while using
    the <html:code class="literal">INDEX_STORAGE</html:code> folder as <html:span
    class="No-Break">a parameter:</html:span> <html:pre class="source-code">    storage_context
    = StorageContext.from_defaults(         persist_dir=INDEX_STORAGE     )     vector_index
    = load_index_from_storage(         storage_context, index_id="vector"     )</html:pre></html:li>
    <html:li>Here, we used <html:code class="literal">index_id</html:code> to identify
    the <html:em class="italic">vector</html:em> index because there’s also a <html:code
    class="literal">TreeIndex</html:code> index in that storage that we won’t be using
    for now. It’s time to initialize our <html:span class="No-Break"><html:code class="literal">DataFrame</html:code></html:span>
    <html:span class="No-Break">extractor:</html:span> <html:pre class="source-code">    df_rows_program
    = DFRowsProgram.from_defaults(         pydantic_program_cls=OpenAIPydanticProgram,
            df=df     )</html:pre></html:li> <html:li>Now, we can <html:a id="_idIndexMarker771"></html:a>define
    our query engine and craft a prompt that will generate the <html:span class="No-Break">quiz
    questions:</html:span> <html:pre class="source-code">    query_engine = vector_index.as_query_engine()
        quiz_query = (         f"Create {QUIZ_SIZE} different quiz "         "questions
    relevant for testing "         "a candidate''s knowledge about "         f"{topic}.
    Each question will have 4 "         "answer options. Questions must be "         "general
    topic-related, not specific "         "to the provided text. For each "         "question,
    provide also the correct "         "answer and the answer rationale. "         "The
    rationale must not make any "         "reference to the provided context, "         "any
    exams or the topic name. Only "         "one answer option should be correct."
        )     response = query_engine.query(quiz_query)</html:pre></html:li> <html:li>Next,
    the prompt <html:a id="_idIndexMarker772"></html:a>is passed to the query engine,
    and the response is then processed by <html:code class="literal">DFRowsProgram</html:code>
    to convert it into a structured <html:span class="No-Break">DataFrame format:</html:span>
    <html:pre class="source-code">    result_obj = df_rows_program(input_str=response)
        new_df = result_obj.to_df(existing_df=df)     new_df.to_csv(QUIZ_FILE, index=False)
        return new_df</html:pre></html:li> <html:li>Finally, the new DataFrame containing
    the quiz questions is saved as a CSV file in the path defined by <html:code class="literal">QUIZ_FILE</html:code>
    . The function returns the new DataFrame for <html:span class="No-Break">further
    use.</html:span></html:li></html:ol> <html:p>This serves as a simple demonstration
    of how to leverage a combination of LlamaIndex features, Pydantic programs, and
    DataFrame manipulation to create a dynamic quiz generator. We’ll continue working
    on the rest of the features in <html:span class="No-Break">future chapters.</html:span></html:p>
    <html:a id="_idTextAnchor178"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Summary</html:title></html:head>
    <html:body><html:div class="epub-source"><html:h1 id="_idParaDest-179">Summary</html:h1>
    <html:div id="_idContainer077"><html:p>This chapter explored how to refine search
    results with various postprocessors, generate responses using different synthesizers,
    and ensure structured outputs with <html:span class="No-Break">specific parsers.</html:span></html:p>
    <html:p>We also explored how to construct query engines while integrating the
    various components that we discussed in the <html:span class="No-Break">previous
    chapters.</html:span></html:p> <html:p>This chapter also covered handling diverse
    data sources with <html:code class="literal">RouterQueryEngine</html:code> and
    decomposing complex queries with <html:code class="literal">SubQuestionQueryEngine</html:code>
    , and also demonstrated quiz creation in our <html:span class="No-Break">tutoring
    app.</html:span></html:p> <html:p>See you in the next chapter, where we’ll talk
    about chatbots, agents, and conversation tracking <html:span class="No-Break">with
    LlamaIndex.</html:span></html:p></html:div></html:div></html:body></html:html>'
  prefs: []
  type: TYPE_NORMAL
