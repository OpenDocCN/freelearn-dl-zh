- en: '*Chapter 8*: Topic Classification Using AutoKeras'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第8章*：使用 AutoKeras 进行主题分类'
- en: 'Sometimes, we need to categorize some specific text, such as a product or movie
    review, into one or more categories by assigning tags or topics. Topic classification
    is a supervised machine learning technique that does exactly this job: predicting
    which categories a given text belongs to. Being a supervised model, it needs to
    be trained with a set of already categorized train data, along with the texts
    and the categories that each one belongs to.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们需要将某些特定的文本（如产品或电影评论）分类到一个或多个类别中，方法是为其分配标签或主题。主题分类是一种监督式机器学习技术，正是用来完成这项工作的：预测给定文本属于哪个类别。作为一种监督式模型，它需要通过一组已经分类的训练数据进行训练，包括文本及其对应的类别。
- en: This chapter will be mainly practical since we laid the foundations for text-based
    tasks in previous chapters. By the end of this chapter, you will have learned
    how to create a topic classifier with AutoKeras, as well as how to apply it to
    any topic or category-based dataset.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将主要是实践性的，因为我们在前几章为基于文本的任务打下了基础。在本章结束时，您将学会如何使用 AutoKeras 创建主题分类器，并将其应用于任何基于主题或类别的数据集。
- en: 'The main topics that will be covered in this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要主题：
- en: Understanding topic classification
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解主题分类
- en: Creating a topic classifier with AutoKeras
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AutoKeras 创建一个主题分类器
- en: Customizing the model search space
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义模型搜索空间
- en: First, let's look at the technical requirements for this chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们来看一下本章的技术要求。
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: All the code examples in this book are available as Jupyter notebooks that can
    be downloaded from [https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的所有代码示例都可以作为 Jupyter 笔记本下载，地址为 [https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras)。
- en: Since the code cells can be executed, each notebook can be self-installed; simply
    add a code snippet with the requirements you need. For this reason, at the beginning
    of each notebook, there is a code cell for environment setup that installs AutoKeras
    and its dependencies.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 由于代码单元可以执行，因此每个笔记本都可以自我安装；只需添加所需的代码片段即可。因此，在每个笔记本的开头，都会有一个用于环境设置的代码单元，它会安装 AutoKeras
    及其依赖项。
- en: 'So, to run the code examples for this chapter, all you need is a computer with
    Ubuntu Linux as its OS and the Jupyter Notebook installed, which you can do with
    the following line of code:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要运行本章的代码示例，您只需要一台操作系统为 Ubuntu Linux 的计算机，并安装了 Jupyter Notebook，您可以通过以下代码行来安装：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Alternatively, you can run these notebooks using Google Colaboratory, in which
    case you will only need a web browser. See [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029),
    *AutoKeras with Google Colaboratory*, for more details. Furthermore, in the *Installing
    AutoKeras* section of that chapter, you will find other installation options.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，您可以使用 Google Colaboratory 运行这些笔记本，在这种情况下，您只需要一个网页浏览器。详情请见 [*第2章*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029)，*使用
    Google Colaboratory 的 AutoKeras*。此外，在该章节的 *安装 AutoKeras* 部分，您将找到其他安装选项。
- en: Now, let's put what we've learned into practice by looking at some practical
    examples.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过一些实际的示例，将所学知识付诸实践。
- en: Understanding topic classification
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解主题分类
- en: We saw a small example of topic classification in [*Chapter 5*](B16953_05_Final_PG_ePub.xhtml#_idTextAnchor077),
    *Text Classification and Regression Using AutoKeras*, with the example of the
    spam classifier. In that case, we predicted a category (spam/no spam) from the
    content of an email. In this section, we will use a similar text classifier to
    categorize each article in its corresponding topic. By doing this, we will obtain
    a model that determines which topics (categories) correspond to each news item.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [*第5章*](B16953_05_Final_PG_ePub.xhtml#_idTextAnchor077)，*使用 AutoKeras 进行文本分类和回归*
    中，看到过一个简单的主题分类示例，其中有一个垃圾邮件分类器的例子。在那个例子中，我们从电子邮件内容中预测了一个类别（垃圾邮件/非垃圾邮件）。在这一节中，我们将使用类似的文本分类器，将每篇文章分类到其对应的主题中。通过这样做，我们将获得一个模型，能够确定每个新闻项对应的主题（类别）。
- en: 'For example, let''s say our model has input the following title:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们的模型输入了以下标题：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will output the `weather` and `sports` topics, as shown in the following
    diagram:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出 `weather` 和 `sports` 主题，如下图所示：
- en: '![Figure 8.1 – Workflow of a news topic classifier](img/B16953_08_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – 新闻主题分类器工作流程](img/B16953_08_01.jpg)'
- en: Figure 8.1 – Workflow of a news topic classifier
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 新闻主题分类器的工作流
- en: The previous diagram shows a simplified version of a topic classifier pipeline.
    The raw text is processed by the classifier and the output will be one or more
    categories.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图表展示了主题分类器管道的简化版本。原始文本被分类器处理，输出将是一个或多个类别。
- en: Later in this chapter, we will apply a text classifier to a Reuters newswire
    dataset to put every article into one or more of the 46 categories. Most of the
    concepts of text classification were already explained in [*Chapter 5*](B16953_05_Final_PG_ePub.xhtml#_idTextAnchor077),
    *Text Classification and Regression Using AutoKeras*, so in this chapter, we will
    simply review some of them in a practical way by implementing the topic classifier.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后面，我们将应用文本分类器对路透社新闻线数据集进行处理，将每篇文章分配到46个类别中的一个或多个。大多数文本分类的概念已经在[*第 5 章*](B16953_05_Final_PG_ePub.xhtml#_idTextAnchor077)《使用
    AutoKeras 进行文本分类与回归》中讲解过，因此本章我们将通过实现主题分类器以实际方式回顾其中的一些概念。
- en: Creating a news topic classifier
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建新闻主题分类器
- en: The model we are going to create will classify news from the Reuters newswire
    classification dataset. It will read the raw text of each news item and classify
    it into sections, assigning a label corresponding to the section that they belong
    to (Sports, Weather, Travel, and so on).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要创建的模型将对路透社新闻线分类数据集的新闻进行分类。它将读取每篇新闻的原始文本，并将其分类到各个部分，为每个部分分配一个标签（例如：体育、天气、旅游等）。
- en: Reuters newswire is a dataset that contains 11,228 newswires from Reuters, labeled
    over 46 topics.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 路透新闻线是一个包含 11,228 条来自路透社的新闻数据集，按 46 个主题进行标注。
- en: The text of each news item is encoded as a list of word indexes. These are integers
    that are indexed by frequency in the dataset. So, here, integer *1* encodes the
    first most frequent word in the data, *2* encodes the second most frequent, and
    so on.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 每篇新闻的文本被编码为一个单词索引列表。这些是按数据集中的频率索引的整数。因此，在这里，整数 *1* 编码数据中最常见的第一个单词，*2* 编码第二常见的单词，依此类推。
- en: The notebook that contains the complete source code can be found at [https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter08/Chapter8_Reuters.ipynb](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter08/Chapter8_Reuters.ipynb).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 包含完整源代码的笔记本可以在 [https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter08/Chapter8_Reuters.ipynb](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter08/Chapter8_Reuters.ipynb)
    找到。
- en: 'Now, let''s have a look at the relevant cells of the notebook in detail:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们详细查看笔记本中的相关单元格：
- en: '**Installing AutoKeras**: As we''ve mentioned in previous chapters, this snippet
    at the top of the notebook is responsible for installing AutoKeras and its dependencies
    using the pip package manager:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安装 AutoKeras**：正如我们在前面的章节中提到的，笔记本顶部的这段代码负责使用 pip 包管理器安装 AutoKeras 及其依赖项：'
- en: '[PRE2]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`tensorflow`, the built-in Keras Reuters dataset, into memory, as well as `numpy`
    and `AutoKeras`, as the necessary dependencies for this project:'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 `tensorflow`、内置的 Keras 路透社数据集加载到内存中，以及将 `numpy` 和 `AutoKeras` 作为本项目所需的依赖项：
- en: '[PRE3]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`reuters_raw` function. Have a look at the code in the notebook for more details:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reuters_raw` 函数。查看笔记本中的代码以了解更多细节：'
- en: '[PRE4]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here is the output of the preceding code:'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是前面代码的输出：
- en: '[PRE5]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Visualizing the dataset samples**: Next, we can print some words from the
    first sample to have an idea of what it contains:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可视化数据集样本**：接下来，我们可以打印出第一个样本中的一些单词，以了解其包含的内容：'
- en: '[PRE6]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here is the output of the preceding code:'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是前面代码的输出：
- en: '[PRE7]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s look at the distribution of the most frequent words in a word cloud.
    A word cloud (also known as a tag cloud) is a text-based data visualization technique
    where words are displayed in different sizes based on how often they appear in
    the text:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下词云中最常见词语的分布。词云（也称为标签云）是一种基于文本的数据可视化技术，单词的显示大小取决于它们在文本中出现的频率：
- en: '![Figure 8.2 – A word cloud of the newswire dataset](img/B16953_08_02.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2 – 新闻线数据集的词云](img/B16953_08_02.jpg)'
- en: Figure 8.2 – A word cloud of the newswire dataset
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 新闻线数据集的词云
- en: Now, let's create the newswire classifier model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建新闻线分类器模型。
- en: Creating the classifier
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建分类器
- en: 'Now, we will use the AutoKeras `TextClassifier` to find the best classification
    model. Just for this example, we will set `max_trials` (the maximum number of
    different Keras models to try) to 2\. We will not set the epochs parameter; instead,
    we will define an `EarlyStopping` callback of `2` epochs. We''re doing this so
    that the training process stops if the validation loss does not improve in two
    consecutive epochs:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用 AutoKeras 的 `TextClassifier` 来寻找最佳的分类模型。仅此示例，我们将 `max_trials`（尝试的不同
    Keras 模型数量的最大值）设置为 2\. 我们不会设置 epochs 参数，而是定义一个 `EarlyStopping` 回调，设定为 2 个 epoch。这样，训练过程将在验证损失连续两个
    epoch 没有改善时停止：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s run the training process to search for the optimal classifier for the
    training dataset:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行训练过程，寻找训练数据集的最佳分类器：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here is the output:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 8.3 – Notebook output of text classifier training'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.3 – 文本分类器训练的笔记本输出'
- en: '](img/B16953_08_03.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16953_08_03.jpg)'
- en: Figure 8.3 – Notebook output of text classifier training
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 文本分类器训练的笔记本输出
- en: The previous output shows that the accuracy of the training dataset is increasing.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的输出显示了训练数据集的准确性在不断提高。
- en: As we can see, we achieved a 0.965 loss value in the validation set. This is
    a really good number just for 1 minute of training. We have limited the search
    to two architectures (`max_trials = 2`). Increasing this number would give us
    a more accurate model, although it would also take longer to finish.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们在验证集上达到了 0.965 的损失值。仅仅 1 分钟的训练时间就得到了一个非常不错的结果。我们将搜索限制为两种架构（`max_trials
    = 2`）。增加这个数字会给我们一个更精确的模型，尽管也会花费更多的时间。
- en: Evaluating the model
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Now, it''s time to evaluate the best model with the testing dataset:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候用测试数据集来评估最佳模型了：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here is the output:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As we can see, 0.77 (77%) is a good final prediction score for the training
    time we've invested (less than a couple of minutes).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，0.77（77%）是我们投入训练时间（不到几分钟）后得到的一个不错的最终预测得分。
- en: Visualizing the model
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化模型
- en: 'Now, let''s look at a little summary of the architecture for the best generated
    model:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下最佳生成模型的架构简要总结：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here is the output:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 8.4 – Best model architecture summary'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.4 – 最佳模型架构总结'
- en: '](img/B16953_08_04.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16953_08_04.jpg)'
- en: Figure 8.4 – Best model architecture summary
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – 最佳模型架构总结
- en: As we can see, AutoKeras has chosen a convolution model (Conv1D) to perform
    this task. As we explained at the beginning of this chapter, this kind of architecture
    works great when the order of the elements in the sequence is not important for
    the prediction.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，AutoKeras 已选择了一个卷积模型（Conv1D）来执行这个任务。正如我们在本章开头所解释的，当序列中元素的顺序对预测没有重要影响时，这种架构效果很好。
- en: 'Here is a visual representation of this:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的可视化表示：
- en: '![Figure 8.5 – Best model architecture visualization'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.5 – 最佳模型架构可视化'
- en: '](img/B16953_08_05.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16953_08_05.jpg)'
- en: Figure 8.5 – Best model architecture visualization
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – 最佳模型架构可视化
- en: Evaluating the model
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型
- en: As you already know, generating models and choosing the best one is done by
    AutoKeras automatically, but let's explain these blocks in more detail.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，生成模型并选择最佳模型是由 AutoKeras 自动完成的，但我们来更详细地解释这些模块。
- en: Each block represents a layer and the output of each is connected to the input
    of the next except for the first block, whose input is the text, and the last
    block, whose output is the predicted number. The blocks before Conv1D are all
    data preprocessing blocks and they are in charge of vectorizing the text generating
    embeddings to feed this Conv1D block, as well as reducing the dimension of the
    filters through the max pooling layer. Notice that AutoKeras has also added several
    dropout blocks to reduce overfitting.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模块代表一个层级，且每个模块的输出都与下一个模块的输入相连，除了第一个模块，其输入是文本，最后一个模块，其输出是预测的数字。在 Conv1D 之前的模块都是数据预处理模块，负责将文本向量化，生成嵌入以供
    Conv1D 模块使用，并通过最大池化层减少滤波器的维度。请注意，AutoKeras 还添加了几个 dropout 模块以减少过拟合。
- en: Customizing the model search space
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义模型搜索空间
- en: We can customize the model's search to restrict the search space by using `AutoModel`
    instead of `TextClassifier`, for example, by setting `TextBlock` for some specific
    configurations.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用 `AutoModel` 替代 `TextClassifier` 来自定义模型的搜索空间，例如，通过为某些特定配置设置 `TextBlock`。
- en: 'In the following code snippet, we''re telling AutoKeras to only generate models
    that use `''ngram''` to vectorize the sentences. Remember that if we do not specify
    any of these arguments, AutoKeras will automatically try all the possible combinations
    until the number reaches the `max_trial` parameter:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码片段中，我们告诉AutoKeras只生成使用`'ngram'`来向量化句子的模型。记住，如果我们没有指定这些参数，AutoKeras将自动尝试所有可能的组合，直到数量达到`max_trial`参数所设定的值：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now, let's summarize what we've learned in this chapter.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们总结一下本章所学内容。
- en: Summary
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to solve a topic classification task by implementing
    a high-performance text classifier that categorizes news articles in just a few
    lines of code.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何通过实现一个高性能的文本分类器来解决一个主题分类任务，该分类器可以在仅仅几行代码的情况下对新闻文章进行分类。
- en: Now that we've laid the groundwork for working with text, we're ready to move
    on to the next chapter, where you'll learn how to handle multimodal and multitasking
    data using AutoKeras.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为处理文本打下了基础，接下来我们可以进入下一章，在那里你将学习如何使用AutoKeras处理多模态和多任务数据。
