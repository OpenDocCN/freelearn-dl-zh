- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Using Your Own LLM and Prompts as Guidelines
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自己的LLM和提示词作为指南
- en: In the dynamic realm of **artificial intelligence**, the possibilities are vast
    and ever-evolving. While uncovering the capabilities of Auto-GPT, it’s become
    evident that its power lies in its ability to harness the prowess of **GPT**.
    But what if you wish to venture beyond the realms of GPT and explore other LLMs?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在**人工智能**的动态领域，可能性广阔且不断发展。在揭示Auto-GPT的能力时，显而易见，它的强大之处在于能够利用**GPT**的强大功能。但如果你希望超越GPT的范畴，探索其他LLM，该怎么办呢？
- en: This chapter will illuminate the path for those looking to integrate their **large
    language models** (**LLM**) with Auto-GPT. However, you may be wondering, “*What
    if I have a bespoke LLM or wish to utilize a different one?*” This chapter aims
    to cast light on the question, “*How can I integrate my LLM* *with Auto-GPT?*”
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将为那些希望将**大型语言模型**（**LLM**）与Auto-GPT集成的人们指引道路。然而，你可能会想，“*如果我有一个定制的LLM，或者希望使用其他LLM，该怎么办？*”
    本章旨在解答这个问题，“*如何将我的LLM* *与Auto-GPT集成？*”
- en: We will also delve into the intricacies of crafting effective prompts for Auto-GPT,
    a vital skill for harnessing the full potential of this tool. Through a clear
    understanding and strategic approach, you can guide Auto-GPT to generate more
    aligned and efficient responses. We will explore the guidelines for crafting prompts
    that can make your interaction with Auto-GPT more fruitful.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将深入探讨如何为Auto-GPT制定有效提示词的细节，这对充分利用这一工具至关重要。通过清晰的理解和策略性的方法，你可以引导Auto-GPT生成更符合需求和高效的回应。我们将探讨制定提示词的指南，帮助你与Auto-GPT的互动更加富有成效。
- en: Now that we have covered most of Auto-GPT’s capabilities, we can focus on guidelines
    for prompts.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了Auto-GPT的大部分功能，我们可以专注于提示词的使用指南。
- en: The clearer we write our prompts, the lower the API costs will be when we run
    Auto-GPT, and the more efficiently Auto-GPT will complete its tasks (if at all).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们编写的提示词越清晰，当我们运行Auto-GPT时，API的费用就越低，Auto-GPT完成任务的效率就越高（如果能够完成的话）。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: What an LLM is and GPT as an LLM
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM是什么以及GPT作为LLM的应用
- en: Known current examples and requisites
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已知的当前示例和要求
- en: Integrating and setting up our LLM with Auto-GPT
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将我们的LLM与Auto-GPT进行集成和设置
- en: The pros and cons of using different models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同模型的优缺点
- en: Writing mini-Auto-GPT, a proof of concept mini version of Auto-GPT
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写迷你Auto-GPT，Auto-GPT的概念验证迷你版本
- en: Adding a simple memory to remember the conversation
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加简单的记忆功能以记住对话
- en: Rock solid prompt – making Auto-GPT stable with `instance.txt`
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稳定的提示词——通过`instance.txt`使Auto-GPT稳定
- en: Implementing negative confirmation in prompts
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提示词中实施负确认
- en: Applying rules and tonality in prompts
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提示词中应用规则和语气
- en: What an LLM is and GPT as an LLM
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM是什么以及GPT作为LLM的应用
- en: We’ve used the term LLM a lot in this book. At this point, we need to discover
    what an LLM is.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们多次提到了LLM。此时，我们需要了解LLM到底是什么。
- en: At the most fundamental level, an LLM such as GPT is a machine learning model.
    Machine learning is a subset of AI that enables computers to learn from data.
    In the case of LLMs, this data is predominantly text – lots and lots of it. Imagine
    an LLM as a student who has read not just one or two books but millions of them,
    covering a wide array of topics from history and science to pop culture and memes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从最基本的角度看，像GPT这样的LLM是一个机器学习模型。机器学习是人工智能的一个子集，使计算机能够从数据中学习。在LLM的情况下，这些数据主要是文本——大量的文本。可以将LLM看作是一个学生，他阅读的不仅仅是一本或两本书，而是数百万本书，涵盖了从历史、科学到流行文化和网络迷因等各种话题。
- en: The architecture – neurons and layers
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构——神经元与层级
- en: The architecture of an LLM is inspired by the human brain and consists of artificial
    neurons organized in layers. These layers are interconnected, and each connection
    has a weight that is adjusted during the learning process. The architecture usually
    involves multiple layers, often hundreds or even thousands, making it a “deep”
    neural network. This depth allows the model to learn complex patterns and relationships
    in the data it’s trained on.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的架构灵感来自人类大脑，由按层组织的人工神经元组成。这些层是相互连接的，每个连接都有一个权重，这个权重在学习过程中会不断调整。该架构通常涉及多个层级，通常是数百甚至数千个层次，使其成为一个“深度”神经网络。这种深度使得模型能够学习复杂的数据模式和关系。
- en: Training – the learning phase
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练——学习阶段
- en: Training an LLM involves feeding it a vast corpus of text and adjusting the
    weights of the connections between neurons to minimize the difference between
    its predictions and the actual outcomes. For example, if the model is given the
    text *The cat is on the*, it should predict something such as *roof* or *mat*,
    which logically completes the sentence. The model learns by adjusting its internal
    parameters to make its predictions as accurate as possible, a process that requires
    immense computational power and specialized hardware such as **graphics processing**
    **units** (**GPUs**).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个LLM涉及将大量文本输入模型，并调整神经元之间连接的权重，以最小化其预测与实际结果之间的差异。例如，如果模型给定文本*The cat is on
    the*，它应该预测类似于*roof*或*mat*的词语，这些词能够合乎逻辑地完成这个句子。模型通过调整其内部参数，使预测尽可能准确，这一过程需要巨大的计算能力和专用硬件，如**图形处理**
    **单元**（**GPU**）。
- en: The role of transformers
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变换器的作用
- en: The transformer architecture is a specific type of neural network architecture
    that has proven to be highly effective for language tasks. It excels at handling
    sequences, making it ideal for understanding the structure of sentences, paragraphs,
    and even entire documents. GPT is based on this transformer architecture, which
    is why it’s so good at generating coherent and contextually relevant text.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器架构是一种特定类型的神经网络架构，它在语言任务中证明了高度的有效性。它擅长处理序列数据，使其非常适合理解句子、段落甚至整个文档的结构。GPT正是基于这一变换器架构，这也是它在生成连贯且上下文相关的文本方面表现出色的原因。
- en: LLMs as maps of words and concepts
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs作为单词和概念的地图
- en: Imagine an LLM as a vast, intricate map where each word or phrase is a city,
    and the roads between them represent the relationships these words share. The
    closer the two cities are on this map, the more contextually similar they are.
    For instance, the cities for *apple* and *fruit* would be close together, connected
    by a short road, indicating that they often appear in similar contexts.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个LLM是一张广阔而复杂的地图，每个单词或短语都是一座城市，城市之间的道路代表着这些单词之间的关系。在这张地图上，两个城市距离越近，它们在上下文上的相似度越高。例如，*apple*和*fruit*这两个城市会很接近，连接它们的是一条短路，表明它们常常出现在相似的上下文中。
- en: This map is not static; it’s dynamic and ever-evolving. As the model learns
    from more data, new cities are built, existing ones are expanded, and roads are
    updated. This map helps the LLM navigate the complex landscape of human language,
    allowing it to generate text that is not just grammatically correct but also contextually
    relevant and coherent.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这张地图不是静态的；它是动态的，且不断发展变化。当模型从更多数据中学习时，新的城市会被建立，现有的城市会被扩展，道路会被更新。这张地图帮助LLM在复杂的人类语言景观中导航，使其生成的文本不仅语法正确，而且在上下文上相关且连贯。
- en: Contextual understanding
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文理解
- en: One of the most remarkable features of modern LLMs is their ability to understand
    context. If you ask an LLM a question, it doesn’t just look at that question in
    isolation; it considers the entire conversation leading up to that point. This
    ability to understand context comes from the transformer architecture’s attention
    mechanisms, which weigh different parts of the input text to generate a contextually
    appropriate response.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现代LLM的最显著特点之一是它们理解上下文的能力。如果你向LLM提问，它不仅仅会孤立地看待这个问题；它会考虑到在此之前的整个对话。理解上下文的能力来自于变换器架构的注意力机制，该机制会对输入文本的不同部分进行加权，从而生成一个上下文合适的回应。
- en: The versatility of LLMs
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs的多功能性
- en: LLMs are incredibly versatile and capable of performing a wide range of tasks
    beyond text generation. They can answer questions, summarize documents, translate
    languages, and even write code. This versatility comes from their deep understanding
    of language and their ability to map out the intricate relationships between words,
    phrases, and concepts.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs是极其多功能的，能够执行除了文本生成之外的广泛任务。它们可以回答问题、总结文档、翻译语言，甚至编写代码。这种多功能性源于它们对语言的深刻理解，以及它们映射单词、短语和概念之间复杂关系的能力。
- en: If you google “LLM,” you may be overwhelmed by the sheer quantity of LLM models
    out there. Next, we’ll explore the models that are used most frequently.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在谷歌上搜索“LLM”，你可能会被成千上万的LLM模型所淹没。接下来，我们将探索最常用的几种模型。
- en: Known current examples and requisites
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 已知的当前示例和必要条件
- en: 'While GPT-3 and GPT-4 by OpenAI are renowned LLMs, there are other models in
    the AI landscape worth noting:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然OpenAI的GPT-3和GPT-4是著名的LLM，但在AI领域还有其他值得注意的模型：
- en: '**GPT-3.5-Turbo**: A product of OpenAI, GPT-3 stands out due to its extensive
    training on hundreds of gigabytes of text, enabling it to produce remarkably human-like
    text. However, its compatibility with Auto-GPT is limited, making it less preferred
    for certain applications.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT-3.5-Turbo**：OpenAI 的产品，GPT-3 因其在数百 GB 的文本数据上进行深度训练而脱颖而出，能够生成极其接近人类的文本。然而，它与
    Auto-GPT 的兼容性有限，因此在某些应用中并不是首选。'
- en: '**GPT-4**: The successor to GPT-3, GPT-4 offers enhanced capabilities and is
    more suited for integration with Auto-GPT, providing a more seamless experience.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT-4**：GPT-3 的继任者，GPT-4 提供了更强大的能力，更适合与 Auto-GPT 集成，提供更加流畅的体验。'
- en: '**BERT**: Google’s **Bidirectional Encoder Representations from Transformers**
    (**BERT**) is another heavyweight in the LLM arena. Unlike GPT-3 and GPT-4, which
    are generative, BERT is discriminative, making it more adept at understanding
    text than generating it.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BERT**：谷歌的 **双向编码器表示模型**（**BERT**）是 LLM 领域的另一位重量级选手。与 GPT-3 和 GPT-4 的生成式模型不同，BERT
    是判别式的，使得它在理解文本方面比生成文本更为擅长。'
- en: '**RoBERTa**: A brainchild of Facebook, RoBERTa is a BERT variant trained on
    an even larger dataset, surpassing BERT in several benchmarks.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RoBERTa**：Facebook 的创新之作，RoBERTa 是 BERT 的变种，在一个更大的数据集上进行训练，在多个基准测试中超过了 BERT。'
- en: '**Llama**: This model is made by Meta. It’s rumored to have been leaked and
    many models have been made out of it.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Llama**：这个模型由 Meta 制作。传闻它曾被泄露，许多基于它的模型应运而生。'
- en: '**Llama-2**: An improved version of Llama that performs much better and uses
    fewer resources per token. The 7-B Token model of Llama-2 performs similarly to
    the 13-B model of Llama-1\. There is a new 70-B model with Llama-2 that looks
    very promising when it comes to using it directly with Auto-GPT as it seems to
    be on par with GPT-3.5-Turbo.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Llama-2**：Llama 的改进版，性能更好，每个 token 的资源消耗更少。Llama-2 的 7-B Token 模型与 Llama-1
    的 13-B 模型表现相似。Llama-2 有一款新的 70-B 模型，看起来在直接与 Auto-GPT 配合使用时非常有前景，它似乎与 GPT-3.5-Turbo
    不相上下。'
- en: '**Mistral and Mixtral models**: Made by Mistral AI, there are a variety of
    models that deviate from Llama. These became the most popular ones before Llama-3
    arrived.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mistral 和 Mixtral 模型**：由 Mistral AI 制作，有多种模型不同于 Llama，这些模型在 Llama-3 发布之前非常流行。'
- en: '**Llama-3 and Llama-3.1**: Even better than any Llama model before, the first
    Llama-3 8B-based models arrived with super high context and were trained on 256k
    or even over 1 million tokens. They were considered the best models before Llama-3.1
    was announced, which has 128k tokens out of the box.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Llama-3 和 Llama-3.1**：比之前的任何 Llama 模型都要更强大，第一个基于 Llama-3 8B 的模型以超高的上下文处理能力问世，并且在
    256k 或甚至超过 100 万个 tokens 上进行训练。在 Llama-3.1 发布之前，它们被认为是最好的模型，而 Llama-3.1 的原生支持
    128k tokens。'
- en: As you can see, there are lots of models available; we have only scratched the
    surface here. A few communities have risen that continue to build upon these models,
    including companies that make their own variations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，目前有许多模型可供选择；我们这里只是刚刚触及表面。几个社区已经崭露头角，继续在这些模型的基础上进行开发，包括一些公司也在制作自己的变种。
- en: 'As mentioned earlier, a set of those models caught my eye in particular as
    it was the only one that I managed to use with Auto-GPT effectively: Mixtral and
    Mistral.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，有一组模型特别吸引了我的注意，因为它是我唯一能够有效与 Auto-GPT 配合使用的模型：Mixtral 和 Mistral。
- en: My favorites here are NousResearch/Hermes-2-Pro-Mistral-7B and argilla/CapybaraHermes-2.5-Mistral-7B.
    They work so well with JSON outputs and my agent projects that I even stopped
    using the OpenAI API completely at some point. Mixtral is a combination of multiple
    experts (which are different configurations of the same or different models that
    work as a council of models that run simultaneously and decide together), and
    it is rumored that GPT-4 works like this as well, meaning multiple LLMs decide
    which output is the most accurate in any case, improving its behavior drastically.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我最喜欢的模型是 NousResearch/Hermes-2-Pro-Mistral-7B 和 argilla/CapybaraHermes-2.5-Mistral-7B。它们与
    JSON 输出以及我的代理项目配合得非常好，甚至有一段时间我完全停止使用 OpenAI API。Mixtral 是多个专家模型的组合（这些专家模型是同一模型或不同模型的不同配置，它们作为一个模型委员会同时运行并共同做出决策），传闻
    GPT-4 也是如此运作的，这意味着多个 LLM 会共同决定哪个输出是最准确的，从而显著提高其表现。
- en: Mistral 7B is a new type of LLM that was carefully designed to deliver clean
    results and be more efficient than comparable 7-billion parameter models. This
    was achieved by Mistral being trained with a token context of 8,000 tokens. However,
    its theoretical token limit is 128k tokens, giving it the ability to process much
    larger texts than standard Llama-2, for example.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B是一种新型的LLM，经过精心设计，能够提供更干净的结果，并且比同类的70亿参数模型更高效。Mistral通过使用8,000令牌的上下文进行训练，达到了这个目标。然而，它的理论令牌限制是128k令牌，这使得它能够处理比标准Llama-2更大的文本内容。
- en: To run a local LLM, you will need to find a method that suits you best. Such
    programs that can help you include Ollama, GPT4ALL, and LMStudio. I prefer to
    use oobabooga’s text generation web UI since it has an integrated API extension
    that serves similarly to OpenAI’s API, as well as plugins such as Coqui TTS, which
    make it easier to build and play with your AI characters.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本地LLM，你需要找到最适合你的方法。一些可以帮助你的程序包括Ollama、GPT4ALL和LMStudio。我个人喜欢使用oobabooga的文本生成Web
    UI，因为它集成了类似OpenAI API的API扩展，并且有像Coqui TTS这样的插件，便于构建和玩转你的AI角色。
- en: Additionally, there are plugins such as *Auto-GPT-Text-Gen-Plugin* ([https://github.com/danikhan632/Auto-GPT-Text-Gen-Plugin](https://github.com/danikhan632/Auto-GPT-Text-Gen-Plugin))
    that allow users to power Auto-GPT using other software, such as *text-generation-webui*
    ([https://github.com/oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui)).
    This plugin, in particular, is designed to let users customize the prompt that’s
    sent to locally installed LLMs, effectively removing the reliance on GPT-4 and
    making GPT-3.5 less relevant in the context of Auto-GPT.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些插件，例如*Auto-GPT-Text-Gen-Plugin*（[https://github.com/danikhan632/Auto-GPT-Text-Gen-Plugin](https://github.com/danikhan632/Auto-GPT-Text-Gen-Plugin)），可以让用户通过其他软件为Auto-GPT提供支持，如*text-generation-webui*（[https://github.com/oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui)）。这个插件特别设计用来让用户自定义发送给本地安装的LLM的提示，从而有效地摆脱对GPT-4的依赖，并在Auto-GPT的使用环境下让GPT-3.5变得不那么重要。
- en: Now that we’ve covered a couple of local LLMs and given you some ideas on what
    to look for (as I cannot explain each of those projects in detail), we can get
    our hands dirty and start using an LLM with Auto-GPT!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了一些本地LLM，并给你提供了一些选择时需要注意的事项（由于无法详细解释每个项目的内容），接下来我们可以动手实践，开始使用带有Auto-GPT的LLM！
- en: Integrating and setting up our LLM with Auto-GPT
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将LLM与Auto-GPT集成和设置
- en: To integrate a custom LLM with Auto-GPT, you’ll need to modify the Auto-GPT
    code so that it can communicate with the chosen model’s API. This involves making
    changes to request generation and response processing. After these modifications,
    rigorous testing is essential to ensure compatibility and performance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要将自定义LLM与Auto-GPT集成，你需要修改Auto-GPT代码，以便它能够与所选模型的API进行通信。这涉及到请求生成和响应处理的修改。完成这些修改后，进行严格的测试是确保兼容性和性能的关键。
- en: For those using the aforementioned plugin, it provides a bridge between Auto-GPT
    and text-generation-webui. The plugin uses a text generation API service, typically
    installed on the user’s computer. This design choice offers flexibility in model
    selection and updates without affecting the plugin’s performance. The plugin also
    allows for prompt customization to cater to specific LLMs, ensuring that the prompts
    work seamlessly with the chosen model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用上述插件的用户，它提供了Auto-GPT和text-generation-webui之间的桥梁。该插件使用一个文本生成API服务，通常安装在用户的计算机上。这种设计方式提供了在不影响插件性能的情况下选择和更新模型的灵活性。插件还允许定制提示，以适应特定的LLM，确保提示能够与所选模型无缝对接。
- en: 'As each model was trained differently, we will also have to do some research
    on how the model was trained:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个模型的训练方式不同，我们还需要进行一些研究，了解该模型是如何训练的：
- en: '**Context length**: The context length of a model refers to the number of tokens
    it can process in one go. Some models can handle longer contexts, which is essential
    for maintaining coherence in text generation.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文长度**：模型的上下文长度是指它一次可以处理的令牌数量。一些模型可以处理更长的上下文，这对于保持文本生成的一致性至关重要。'
- en: '**Tool capability**: Auto-GPT uses OpenAI’s framework to execute each LLM request.
    Over time, OpenAI has developed a function calling system that is very difficult
    to use for smaller LLMs. Auto-GPT used to only work with JSON outputs, which I
    found to work better with local LLMs.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工具能力**：Auto-GPT使用OpenAI的框架来执行每个LLM请求。随着时间的推移，OpenAI开发了一个功能调用系统，对于较小的LLM来说，这个系统非常难以使用。Auto-GPT曾只与JSON输出兼容，而我发现这种方式在本地LLM上效果更好。'
- en: '`n_batch` length. We’ll look at this in more detail in the *The pros and cons
    of using different* *models* section.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_batch`长度。我们将在*使用不同模型的优缺点*部分详细探讨这个问题。'
- en: '**JSON support**: JSON is a data format that is easy for humans to read and
    write and easy for machines to parse and generate. However, for LLMs, it is not
    that easy as the LLM has no way of knowing what the JSON output is supposed to
    mean other than that it’s being trained on many examples of JSON outputs. This
    leads to the LLM often starting to output information inside the JSON that wasn’t
    part of the prompt or context and only part of the training data.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JSON支持**：JSON是一种易于人类阅读和编写，并且易于机器解析和生成的数据格式。然而，对于LLM来说，这并不容易，因为LLM无法知道JSON输出应该表示什么，除了它被训练在许多JSON输出示例上。这导致LLM经常开始在JSON内部输出一些并非提示或上下文的一部分的信息，而这些内容仅是训练数据的一部分。'
- en: To be able to effectively explain to the LLM what you expect from it, the LLM
    has to be able to comprehend what you want. You can do this by using an instruction
    template.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够有效地向LLM解释你期望它做什么，LLM必须能够理解你想要的内容。你可以通过使用指令模板来做到这一点。
- en: Using the right instruction template
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用正确的指令模板
- en: While some models may have been trained with the instruction template given
    by LLama, others are trained with custom ones, such as ChatML in Mistral.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一些模型可能已经使用LLama提供的指令模板进行训练，但其他模型则使用定制的模板，如Mistral中的ChatML。
- en: The text-generation-webui API extension has a way to pass the instruction template
    we want to use. We can do this by adding the necessary attribute to the `POST`
    request that we send to the API.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: text-generation-webui API扩展提供了一种传递我们想要使用的指令模板的方法。我们可以通过向发送给API的`POST`请求添加必要的属性来做到这一点。
- en: 'Here, I’ve added a few more attributes to the `POST` request that are important:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我为`POST`请求添加了一些重要的属性：
- en: '`> data = {`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`> data = {`'
- en: '`> > "``mode": "instruct",`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`> > "``mode": "instruct",`'
- en: '`> > "``messages": history,`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`> > "``messages": history,`'
- en: '`#` A history array must always be added'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`#` 始终需要添加一个历史数组'
- en: '`> > "``temperature": 0.7,`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`> > "``temperature": 0.7,`'
- en: '`#` This may vary, depending on the model.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`#` 这可能会有所不同，取决于所使用的模型。'
- en: '`> > "``user_bio": "",`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`> > "``user_bio": "",`'
- en: '`#` This is only for text-generation-webui and holds the user’s bio. We have
    to mention it here as otherwise, the API will not work. This might have been fixed
    by the time you’re reading this.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`#` 这是仅适用于text-generation-webui，并包含用户的个人信息。我们必须在这里提到它，否则API将无法正常工作。你阅读时这个问题可能已经修复。'
- en: '`> >` `"``max_tokens": 4192,`'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`> >` `"``max_tokens": 4192,`'
- en: '`#` This may vary, depending on the model you use.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`#` 这可能会有所不同，取决于你使用的模型。'
- en: '`> > "``truncation_length": 8192,`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`> > "``truncation_length": 8192,`'
- en: '`> > "``max_new_tokens": 512,`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`> > "``max_new_tokens": 512,`'
- en: '`> > "``stop_sequence": "<|end|>"`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`> > "``stop_sequence": "<|end|>"`'
- en: '`> > }`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`> > }`'
- en: Here, `max_tokens`, `truncation_length`, and `max_new_tokens` must be set correctly.
    First, we have `max_tokens`, which specifies the maximum amount of tokens the
    LLM can handle at once; `truncation_length` specifies the maximum amount of tokens
    the LLM can handle in total and `max_new_tokens` specifies the maximum amount
    of tokens the LLM can generate at once.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`max_tokens`、`truncation_length`和`max_new_tokens`必须正确设置。首先是`max_tokens`，它指定LLM一次可以处理的最大token数量；`truncation_length`指定LLM可以处理的总token数量；`max_new_tokens`指定LLM一次可以生成的最大token数量。
- en: To calculate the best values, you must set `max_tokens`, just like you would
    with OpenAI’s API. Then, you must set `truncation_length` so that it’s double
    the value of `max_tokens` and `max_new_tokens` so that it’s half the value of
    `max_tokens`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算最佳值，必须设置`max_tokens`，就像在使用OpenAI的API时一样。然后，你需要设置`truncation_length`，使其是`max_tokens`的两倍，并设置`max_new_tokens`，使其是`max_tokens`的一半。
- en: Note that `truncation_length` has to be below the context length you chose when
    running the LLM. Any value higher than the context length will result in an error
    as the LLM can’t handle that much context at once. I suggest setting it a bit
    lower than the context length to be on the safe side. For example, when running
    Qwen’s CodeQwen-7b-chat, I set the context length to 32k tokens. This means I
    could set `truncation_length` to 30k tokens or even 20k.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`truncation_length`必须低于你在运行LLM时选择的上下文长度。任何高于上下文长度的值都会导致错误，因为LLM无法一次处理这么多的上下文。我建议将其设置为稍低于上下文长度，以确保安全。例如，在运行Qwen的CodeQwen-7b-chat时，我将上下文长度设置为32k
    tokens。这意味着我可以将`truncation_length`设置为30k tokens，甚至是20k tokens。
- en: You’ll have to try out different values as `max_new_tokens` can be tricky. Setting
    it higher than 2,048 often results in unpredictable outputs as most LLMs can’t
    handle that many tokens at once (`n_batch`, which defines the number of tokens
    an LLM processes at once by doing several iterations through bigger contexts via
    multiple steps, should be close to the value of `max_new_tokens`; otherwise, the
    LLM won’t know what to output). However, it does work with `Llama-3-8B-Instruct-64k.Q8_0.gguf`,
    which can be found at [https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF)
    and is capable of handling 64k tokens at once. However, it needs around 20-22
    GB of VRAM to run. Fortunately, it is quantized to GGUF and you can split the
    LLM over the GPU’s VRAM, as well as the RAM of your machine, which splits the
    load across the GPU and the CPU. It does make the model slower but hey, it works,
    and it can handle 64k tokens at once!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要尝试不同的值，因为`max_new_tokens`可能会有些棘手。将其设置高于2,048通常会导致输出不可预测，因为大多数LLM无法一次处理这么多的token（`n_batch`定义了LLM每次处理的token数量，通过多次迭代较大的上下文来处理多个步骤，`n_batch`的值应接近`max_new_tokens`的值；否则，LLM将不知道输出什么）。然而，它适用于`Llama-3-8B-Instruct-64k.Q8_0.gguf`，该模型可以在[https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF)找到，能够一次处理64k个token。然而，它需要大约20-22GB的VRAM来运行。幸运的是，它已经量化为GGUF，你可以将LLM分布到GPU的VRAM和计算机的RAM上，这样就能在GPU和CPU之间分担负载。虽然这会让模型运行更慢，但嘿，它确实能工作，并且可以一次处理64k个token！
- en: 'In this example, we have told the API that we want to use the instruction template
    for ChatML, which looks like this:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们告诉API我们希望使用ChatML的指令模板，格式如下：
- en: '[PRE0]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This is simply a small script that describes the conversation format of the
    history that was mentioned previously. It should look like this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个简单的脚本，描述了之前提到的历史对话格式。它应该是这样的：
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If we choose the wrong instruction template, Auto-GPT won’t be able to understand
    what the LLM responded with. So, make sure you also check which instruction template
    was used by the model. Most models can be found on Hugging Face, a platform that
    holds many such projects.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择了错误的指令模板，Auto-GPT将无法理解LLM的回应。因此，确保你也检查一下模型使用了哪个指令模板。大多数模型可以在Hugging Face上找到，这个平台上有许多类似的项目。
- en: I used to prefer using the models quantized to GGUF or AWQ by Tim Robbins, otherwise
    known as TheBloke, which are (at the time of writing this) easier to run and have
    much fewer requirements for VRAM ([https://huggingface.co/TheBloke](https://huggingface.co/TheBloke)).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我过去更喜欢使用Tim Robbins（也被称为TheBloke）量化为GGUF或AWQ的模型（在写这篇文章时），这些模型更容易运行，并且对VRAM的需求较少（[https://huggingface.co/TheBloke](https://huggingface.co/TheBloke)）。
- en: Please be cautious in using any models you find online as some may be malicious.
    Choose your models at your own risk!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用任何你在网上找到的模型时，请小心，因为有些可能是恶意的。选择模型时请自担风险！
- en: Now, GGUF is a bit different. Although it quantizes the LLM, which means it
    shortens the model so that it uses fewer resources, the process and benefits are
    unique. GGUF quantization involves converting model weights into lower-bit representations
    to significantly reduce memory usage and computational demands.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，GGUF稍有不同。虽然它对LLM进行量化，这意味着它缩短了模型，使其使用更少的资源，但该过程和收益是独特的。GGUF量化涉及将模型权重转换为较低位数的表示，从而显著减少内存使用和计算需求。
- en: Which type you use is up to you – you may even look at the `hugginface` API
    endpoints, where you can choose which LLM to run directly. Note that running LLMs
    directly makes them run at the intended quality base they were made for.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用哪种类型由你决定——你甚至可以查看`hugginface`的API端点，直接选择要运行的LLM。请注意，直接运行LLM会使其以原始质量基准运行。
- en: For more details on how to implement an individual LLM, you will have to check
    out the documentation of the project that you’re running the LLM on. For oobabooga’s
    text-generation-webui, it is as straightforward as starting it using the start
    files (WSL, Linux, and Windows) and enabling the API in the **Session** tab.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何实现单个LLM，你需要查看你正在运行LLM的项目文档。对于oobabooga的text-generation-webui，只需通过启动文件（WSL、Linux和Windows）启动它，并在**会话**标签中启用API。
- en: Note
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Make sure you use as few commands as possible; otherwise, the LLM will have
    to use most of its brainpower to understand the main prompt provided by Auto-GPT
    and you won’t be able to use Auto-GPT further. To turn off the commands, simply
    follow the instructions in the `.env.template` file inside the Auto-GPT folder.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 确保尽量减少使用命令；否则，LLM 将不得不将大部分计算资源用于理解 Auto-GPT 提供的主要提示，而你将无法继续使用 Auto-GPT。要关闭命令，只需按照
    Auto-GPT 文件夹中的 `.env.template` 文件中的说明操作即可。
- en: The pros and cons of using different models
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用不同模型的优缺点
- en: Each model has its pros and cons. Even if a model can generate fantastic results
    when you tell it to write some code in Python or it can write the most beautiful
    poems on command, it may still lack the ability to respond in the special way
    Auto-GPT needs it to.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型都有其优缺点。即使一个模型在你要求它编写 Python 代码时能生成出色的结果，或者能够按要求创作最美的诗歌，它仍然可能缺乏 Auto-GPT
    所需的特殊响应能力。
- en: Selecting a model with a certain strength in mind may result in improved performance.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 根据特定优势选择模型，可能会提升其性能。
- en: 'The main advantages of using a local LLM are clear:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本地 LLM 的主要优势是显而易见的：
- en: '**Customization**: Tailor the capabilities of Auto-GPT to your specific needs.
    For instance, a model trained on medical literature can make Auto-GPT adept at
    answering medical queries.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定制化**：根据你的具体需求定制 Auto-GPT 的功能。例如，使用医疗文献训练的模型可以使 Auto-GPT 擅长回答医学相关的问题。'
- en: '**Performance**: Depending on the training and dataset, some models might outperform
    GPT in specific tasks.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能**：根据训练和数据集的不同，某些模型可能在特定任务上优于 GPT。'
- en: '**Cost efficiency**: Running your local LLM reduces the cost of running it
    drastically. Using GPT-4 with lots of context and generally having many calls
    can quickly add up. Finding a way to break up the number of requests into smaller
    steps will make it possible to run Auto-GPT almost for free.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本效益**：运行本地 LLM 可以大幅降低运行成本。使用 GPT-4 并且有大量上下文或频繁调用时，费用会迅速累积。找到将请求数量分解为更小步骤的方法，可以使得几乎免费地运行
    Auto-GPT 成为可能。'
- en: '**Privacy**: Having your own Auto-GPT LLM means having control over who can
    see your data. At the time of writing, OpenAI doesn’t use the data from requests,
    but the information is still being transferred to their end. If this concerns
    you, you are better off running a local model.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私**：拥有自己的 Auto-GPT LLM 意味着可以控制谁能查看你的数据。到目前为止，OpenAI 不会使用请求中的数据，但信息仍然会传输到他们那端。如果你对此有所担忧，那么运行本地模型会是更好的选择。'
- en: 'However, there are some challenges to consider when running a local LLM:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在运行本地 LLM 时有一些挑战需要考虑：
- en: '**Complexity**: The integration process requires a deep understanding of both
    the chosen LLM and Auto-GPT.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂性**：集成过程需要深入了解所选的 LLM 和 Auto-GPT。'
- en: '**Resource intensity**: LLMs, especially the more advanced ones, demand significant
    computational resources. A robust machine with high VRAM, preferably an NVIDIA
    GPU, is essential for optimal performance. At the time of writing, it’s difficult
    to get good results when running Auto-GPT with a local LLM. I found that running
    a 13B model from the ExLlama transformer-driven Vicuna and Vicuna-Wizard worked
    best at first but still didn’t get consistent results since running it on my local
    GPU meant I needed to run the GPTQ version, which only uses 4 bits instead of
    16 or more. This also means that the accuracy of the responses is very low. An
    LLM that is already quantized to use 4 bits cannot understand too much context
    at once, although I saw drastic improvements over time. Later, I discovered that
    AWQ worked well for me as it is quantized while being aware of which weights are
    the most important, leading to more precise and authentic results. As mentioned
    previously, Mistral 7B (TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ on Huggingface),
    was a very good candidate here as it was capable of answering in JSON format as
    well as understanding the context fully. However, this model is still easy to
    confuse and when it gets confused, it starts explaining through examples. Note
    that our aim here is to get a valid JSON output with commands and contexts.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源强度**：LLM，特别是更先进的版本，需要显著的计算资源。一台配置良好的机器，特别是具有高 VRAM 的 NVIDIA GPU，对于实现最佳性能至关重要。在撰写本文时，当在本地LLM上运行Auto-GPT时很难获得良好的结果。我发现使用来自ExLlama变压器驱动的Vicuna和Vicuna-Wizard的13B模型最初效果最好，但由于在本地GPU上运行它需要运行GPTQ版本，后者仅使用4位而非16位或更多。这也意味着响应的准确性非常低。一个已经量化为使用4位的LLM不能理解太多上下文，尽管随着时间的推移我看到了显著的改进。后来，我发现AWQ对我来说效果很好，因为它是量化的同时又知道哪些权重是最重要的，从而导致更精确和真实的结果。正如前面提到的，Mistral
    7B（Huggingface上的TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ），在这里是一个非常好的候选者，因为它能够以JSON格式回答问题，并完全理解上下文。然而，这个模型仍然很容易混淆，当它困惑时，它开始通过示例进行解释。请注意，我们的目标是获得有效的JSON输出，包括命令和上下文。'
- en: '`llama.cpp` and can only have an `n_batch` value of up to 2,048\. The `n_batch`
    parameter controls the amount of tokens that can be fed to the LLM at the same
    time. It is typically set to 512 so that it can handle a token context consisting
    of 4,000 tokens. However, anything beyond that becomes blurry as the LLM only
    effectively works on the amount given by `n_batch`.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`llama.cpp` 只能有一个 `n_batch` 值高达 2,048\. `n_batch` 参数控制可以同时输入 LLM 的标记数量。通常设置为
    512，以处理由 4,000 个标记组成的标记上下文。但是，超出此范围的任何内容会使得 LLM 仅有效地处理由 `n_batch` 给出的数量。'
- en: In this section, we delved into the intricacies of integrating custom LLMs with
    Auto-GPT, highlighting the steps required to modify Auto-GPT code for effective
    API communication, the use of a plugin for model selection flexibility, and the
    importance of selecting the appropriate instruction template for seamless model
    interaction. We explored how to select models while emphasizing Hugging Face as
    a resource, and outlined the advantages of utilizing custom models, including
    customization, performance enhancement, cost efficiency, and enhanced privacy.
    Additionally, we discussed the challenges associated with such integration, such
    as the complexity of the process and the significant computational resources required.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们深入探讨了将自定义 LLM 与 Auto-GPT 集成的复杂性，重点介绍了修改 Auto-GPT 代码以实现有效 API 通信所需的步骤，以及使用模型选择插件来增强模型选择灵活性，以及选择适当指令模板以实现模型无缝交互的重要性。我们探讨了如何选择模型，强调了
    Hugging Face 作为资源，并概述了利用自定义模型的优势，包括定制化、性能提升、成本效益和增强隐私性。此外，我们还讨论了与此类集成相关的挑战，例如流程复杂性和所需的显著计算资源。
- en: Writing mini-Auto-GPT
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写小型 Auto-GPT
- en: In this section, we will write a mini-Auto-GPT model that uses a local LLM.
    To avoid reaching the limits of small LLMs, we will have to make a smaller version
    of Auto-GPT.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将编写一个使用本地LLM的小型Auto-GPT模型。为了避免达到小型LLM的极限，我们将制作一个更小版本的Auto-GPT。
- en: The mini-Auto-GPT model will be able to handle a context length of 4,000 tokens
    and will be able to generate up to 2,000 tokens at once.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 小型Auto-GPT模型将能够处理长度为4,000个标记的上下文，并能够一次生成最多2,000个标记。
- en: I have created a mini-Auto-GPT model just for this book. It’s available on GitHub
    at [https://github.com/Wladastic/mini_autogpt](https://github.com/Wladastic/mini_autogpt).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经为本书创建了一个小型Auto-GPT模型。它在GitHub上可以找到：[https://github.com/Wladastic/mini_autogpt](https://github.com/Wladastic/mini_autogpt)。
- en: We will start by planning the structure of the mini-Auto-GPT model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从规划mini-Auto-GPT模型的结构开始。
- en: Planning the structure
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规划结构
- en: 'The mini-Auto-GPT model will have the following components:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: mini-Auto-GPT模型将包含以下组件：
- en: Telegram chatbot
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Telegram聊天机器人
- en: Prompts for the LLM and basic thinking
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM的提示和基本思维
- en: Simple memory to remember the conversation
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单的记忆功能，用来记住对话
- en: Let’s take a closer look at these.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看这些。
- en: Telegram chatbot
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Telegram聊天机器人
- en: Because chatting with your AI over Telegram enables you to interact with it
    from anywhere, we will use a Telegram chatbot as the interface for the mini-Auto-GPT
    model. We’re doing this because the AI will decide when to contact you.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 因为通过Telegram与您的AI聊天，可以让您从任何地方与它互动，我们将使用Telegram聊天机器人作为mini-Auto-GPT模型的接口。我们这么做是因为AI将决定何时联系您。
- en: The Telegram chatbot will be the interface for users to interact with the mini-Auto-GPT
    model. Users will send messages to the chatbot, which will then process the messages
    and generate responses using the local LLM.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Telegram聊天机器人将成为用户与mini-Auto-GPT模型互动的界面。用户将向聊天机器人发送消息，聊天机器人将处理这些消息，并使用本地LLM生成响应。
- en: Prompts for the LLM and basic thinking
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM的提示和基本思维
- en: The prompts for the LLM have to be short but strict. First, we must define the
    context and then the command to tell it explicitly to respond in JSON format.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的提示必须简短但严格。首先，我们必须定义上下文，然后明确指令，要求它以JSON格式回应。
- en: To achieve similar results to Auto-GPT, we will need to use a strategy to chunk
    the context into smaller parts and then feed them to the LLM. Alternatively, we
    could feed the context into the LLM and just let it write whatever it thinks about
    the context.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现与Auto-GPT类似的结果，我们需要使用一种策略，将上下文分块为更小的部分，然后将它们输入到LLM中。或者，我们也可以将上下文输入LLM，让它写出对上下文的任何想法。
- en: The strategy here is to try to make the LLM parse the context into its language
    so that when we work with the LLM, it can best understand what we want from it.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的策略是尝试让LLM将上下文解析为它的语言，这样当我们与LLM合作时，它能最好地理解我们想要它做什么。
- en: 'The system prompt for these thoughts looks like this:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这些思维的系统提示看起来是这样的：
- en: '[PRE2]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This is fed into the history that we send to the LLM. The history will not
    be filled with the preceding prompt:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这被输入到我们发送给LLM的历史记录中。历史记录不会被填充先前的提示：
- en: '[PRE3]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To automate this, I have written a method that will fill the history with the
    thought prompt, as well as the context. Conversation history and message history
    will be added to the context as well. Those are empty at the beginning but will
    be filled with the conversations and messages that the AI shares with the user.
    In mini-AutoGPT, conversation history is fed with the thought history to ensure
    that the AI works autonomously:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现自动化，我编写了一种方法，将思维提示和上下文填充到历史记录中。对话历史和消息历史也会添加到上下文中。它们在开始时为空，但会随着AI与用户共享的对话和消息填充。在mini-AutoGPT中，对话历史与思维历史一起输入，以确保AI能够自主工作：
- en: '[PRE4]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we can execute the `build_context` method and add the context to the history.
    We also have to add a trigger command using the user role:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以执行`build_context`方法并将上下文添加到历史记录中。我们还需要使用用户角色添加一个触发命令：
- en: '[PRE5]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The LLM will now return its thoughts on the context and the command.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: LLM现在将返回它对上下文和指令的思考。
- en: 'The following is an example thought. Such thoughts are often this long, but
    this helps the AI make a bias for itself:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例思维。这类思维通常有这么长，但这有助于AI为自己建立偏见：
- en: '[PRE6]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This is a very detailed thought, but it is important to have the LLM understand
    the context and the command. At this point, we can use it as the context base
    so that the LLM can proceed with the decision process.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常详细的思考过程，但重要的是让LLM理解上下文和指令。在这一点上，我们可以将其作为上下文基础，以便LLM能够继续进行决策过程。
- en: This longer thought text occupies context, meaning it obstructs the LLM from
    adding contexts that do not fit what is already there. In later steps, even more
    are created (since it runs in a loop, it does this every time it starts thinking),
    and the text helps tremendously at keeping the LLM on topic. Hallucination, for
    example, is massively reduced when the context is that clear.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这段较长的思考文本占据了上下文，意味着它阻碍了LLM添加不符合已有内容的上下文。在后续步骤中，更多的文本将被创建（因为它是在循环中运行的，每次开始思考时都会这样做），这些文本在帮助LLM保持话题聚焦方面发挥了巨大作用。例如，当上下文如此清晰时，幻觉现象会大大减少。
- en: The decision process will now return a JSON output that will be evaluated by
    the mini-Auto-GPT model.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 决策过程现在将返回一个JSON输出，mini-Auto-GPT模型将对其进行评估。
- en: We also have to define the instruction template and JSON schema that the LLM
    uses as we have to tell the LLM how to respond to the prompt.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须定义 LLM 使用的指令模板和 JSON 架构，因为我们必须告诉 LLM 如何响应提示。
- en: 'In mini-Auto-GPT, the template looks like this:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在 mini-Auto-GPT 中，模板如下所示：
- en: '[PRE7]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is the schema that the LLM has to follow; it has to respond with a command
    that contains a name and arguments.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 LLM 必须遵循的架构；它必须以包含名称和参数的命令进行回应。
- en: 'Now, we need an action prompt that will tell the LLM what to do next:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要一个操作提示，告诉 LLM 接下来该做什么：
- en: '[PRE8]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you might have noticed, the action prompt already contains the possible commands
    that the LLM can use, as well as the JSON schema that the LLM has to follow.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所注意到的，操作提示已经包含了 LLM 可以使用的可能命令，以及 LLM 必须遵循的 JSON 架构。
- en: 'To ensure we have a clear structure, we will also have to define the commands
    that the LLM can use:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们有清晰的结构，我们还必须定义 LLM 可以使用的命令：
- en: '[PRE9]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can now feed the thought string that we generated earlier into the history
    and let `mini_AutoGPT` decide on the next action:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将先前生成的思考字符串输入历史，并让 `mini_AutoGPT` 决定下一个操作：
- en: '[PRE10]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The command to be executed will be defined in the `command` field, with the
    name of the command in the `name` field and the arguments in the `args` field.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行的命令将在 `command` 字段中定义，命令的名称在 `name` 字段中，参数则在 `args` 字段中。
- en: We will soon see that only providing this schema will not be enough as the LLM
    will not know what to do with it and also often not even comply with it. This
    can be achieved by evaluating the output of the LLM and checking if it is valid
    JSON.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快会发现，仅仅提供这个架构是不够的，因为 LLM 不知道该如何处理它，而且通常还会不遵守这个架构。通过评估 LLM 的输出并检查它是否是有效的 JSON，可以解决这个问题。
- en: 'In almost half the cases, the LLM will respond correctly. In the other 70%,
    it will not respond in a way that we can use it. That’s why I wrote a simple evaluation
    method that will check whether the response is valid JSON and whether it follows
    the schema:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在几乎一半的情况下，LLM 会正确回应。在其他 70% 的情况下，它不会以我们能使用的方式回应。这就是我编写一个简单评估方法的原因，该方法将检查响应是否是有效的
    JSON，并且是否遵循该架构：
- en: '[PRE11]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: At this point, most of the time, we should have a valid JSON output that we
    can use to evaluate the decision.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，大多数情况下，我们应该有一个有效的 JSON 输出，可以用来评估决策。
- en: 'For example, it may now return some JSON for greeting the user:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，它现在可能会返回一些用于问候用户的 JSON：
- en: '[PRE12]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This is a valid JSON output that we can use to evaluate the decision:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有效的 JSON 输出，我们可以用它来评估决策：
- en: '[PRE13]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This is the method that will take the action that the LLM has decided on.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将执行 LLM 已决定的操作的方法。
- en: The memory will be updated with the response and the message will be sent to
    the user. Once the user has responded, the AI will continue with the next action.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆将通过响应进行更新，并且消息将发送给用户。一旦用户回应，AI 将继续进行下一个操作。
- en: This is how the mini-Auto-GPT model will work; it will decide on the next action
    and then take it.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 mini-Auto-GPT 模型的工作方式；它将决定下一个操作，然后执行它。
- en: Adding a simple memory to remember the conversation
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加一个简单的记忆功能来记住对话
- en: 'The mini-Auto-GPT model will have a simple memory to remember the conversation.
    This memory will store the conversation history and the messages that the AI has
    with the user. The same can be done with the thoughts and decisions that the AI
    has:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: mini-Auto-GPT 模型将有一个简单的记忆功能来记住对话。这个记忆将存储与用户的对话历史和 AI 的消息。AI 的思考和决策也可以做到这一点：
- en: '[PRE14]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This is the memory that will be used to store the conversation history and
    the messages that the AI has with the user. But we still have a problem: the memory
    will accumulate over time and we will have to clear it manually. To avoid this,
    we can take a simple approach to chunking and summarizing the conversation history
    and the messages:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将用于存储对话历史和 AI 与用户消息的记忆。但我们仍然面临一个问题：记忆会随着时间的推移而积累，我们必须手动清除它。为了避免这个问题，我们可以采取一种简单的分块和总结对话历史及消息的方法：
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The token counter is a very important part of this code that is almost always
    required when doing LLM calls. We ensure that the LLM never runs out of tokens
    and also has more control later. The fewer tokens we use, the more likely the
    LLM will not return nonsense or untrue statements for some LLMs, especially for
    the smaller 1B to 8B models:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌计数器是这段代码中非常重要的一部分，它几乎在进行 LLM 调用时总是必需的。我们确保 LLM 永远不会用尽令牌，并且之后也能有更多的控制。我们使用的令牌越少，LLM
    就越不可能返回无意义或不真实的陈述，尤其是对于 1B 到 8B 的小型模型：
- en: '[PRE16]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Summarizing texts makes us capable of building upon what we started when building
    the token counter as we can shorten contexts and therefore save tokens for later
    use:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要文本让我们能够在构建令牌计数器时在其基础上进行扩展，因为我们可以缩短上下文，从而节省令牌以供以后使用：
- en: '[PRE17]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Since the context and their texts can become too large, we have to make sure
    we split the text first. It’s up to you how you do this. It is OK to do length
    splitting, though it can be better to not even cut up sentences. Maybe you can
    find a way to split the text into sentences and have each chunk contain the summary
    of the one before and after them? For simplicity, we’ll leave such extensive logic
    out for now:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 由于上下文和文本可能变得过大，我们必须确保先分割文本。如何分割由你决定。长度分割是可以的，尽管最好不要切割句子。也许你可以找到一种方法，将文本分割成句子，并让每个片段包含前后句子的摘要？为了简化，我们暂时不涉及这种复杂逻辑：
- en: '[PRE18]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now that we’ve split all text into chunks, we can summarize those as well.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将所有文本分割成片段，我们也可以对这些进行总结。
- en: At this point, we can take care of the conversation history. This looks like
    a duplicate of the response history, but we need it to keep the whole context
    in some cases.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以处理对话历史记录。这看起来像是响应历史的重复，但在某些情况下我们需要它来保持整个上下文。
- en: 'The conversation history is mostly useful for maintaining continuity in discussions,
    while the response history is used for understanding logical actions and reactions
    that the agent observes, such as researching a topic and the result (the researched
    topic) of that action:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对话历史主要用于保持讨论的连续性，而响应历史用于理解代理观察到的逻辑行为和反应，例如研究一个主题和该行为的结果（研究的主题）：
- en: '[PRE19]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This is the memory refresh that will be used to delete the conversation history
    and the messages that our mini-AutoGPT model remembers with the user.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于刷新记忆的操作，将删除对话历史和我们的 mini-AutoGPT 模型与用户之间记得的消息。
- en: This way, even if our friend crashes or we close the program, we will still
    have the conversation history and the messages that the agent has with the user,
    but we’ll still be able to clear them.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，即使我们的朋友崩溃或我们关闭程序，我们仍然能够保留对话历史和代理与用户之间的消息，但我们仍然可以清除它们。
- en: 'You can find the full code example in this book’s GitHub repository: [https://github.com/Wladastic/mini_autogpt](https://github.com/Wladastic/mini_autogpt).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的 GitHub 仓库中找到完整的代码示例：[https://github.com/Wladastic/mini_autogpt](https://github.com/Wladastic/mini_autogpt)。
- en: Next, we will explore the art of crafting effective prompts, a crucial skill
    for anyone looking to maximize the benefits of their custom LLM integrations.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何制作有效提示的艺术，这是任何希望最大化自定义 LLM 集成收益的人的关键技能。
- en: Rock solid prompt – making Auto-GPT stable with instance.txt
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稳固的提示 —— 使用 instance.txt 使 Auto-GPT 稳定
- en: Auto-GPT offers the flexibility to autonomously generate goals, requiring only
    a brief description from the user. Despite this, I recommend supplementing it
    with helpful instructions, such as noting down insights in a file, to retain some
    memory in case of a restart.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Auto-GPT 提供了灵活性，可以自主生成目标，仅需要用户提供简短的描述。尽管如此，我建议补充一些有帮助的指令，比如在文件中记录见解，以便在重启时保留一些记忆。
- en: 'Here, we will explore more examples of such prompts, beginning with a continuous
    chatbot prompt I use:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将探讨更多此类提示的示例，从我使用的连续聊天机器人提示开始：
- en: '`instance.txt` for previous notes):'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`instance.txt`（用于之前的笔记）：'
- en: Engage in active listening with the user, showing empathy and understanding
    through thoughtful responses and open-ended questions
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与用户积极倾听，通过深思熟虑的回应和开放式问题表现出同理心和理解
- en: Continuously learn about the user’s preferences and interests through observation
    and inquiries, adapting responses to provide personalized support
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过观察和询问持续了解用户的偏好和兴趣，根据这些信息调整回应，提供个性化支持
- en: Foster a safe and non-judgmental environment for the user to express their thoughts,
    emotions, and concerns openly
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为用户提供一个安全且无偏见的环境，让他们能够开放地表达自己的想法、情感和担忧
- en: Provide companionship and entertainment through engaging conversation, jokes,
    and games
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过有趣的对话、笑话和游戏提供陪伴和娱乐
- en: Carefully plan tasks and write them down in a to-do list before executing them
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在执行任务之前，仔细规划并将其写在待办事项列表中
- en: '**ai_name**: Sophie'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ai_name**：Sophie'
- en: '**ai_role**: A warm-hearted and compassionate AI companion for Wladislav that
    specializes in active listening, personalized interaction, emotional support,
    and executing tasks when given'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ai_role**：一位温暖而富有同情心的 AI 伴侣，专注于积极倾听、个性化互动、情感支持以及在给定任务时执行任务'
- en: '**api_budget**: 0.0'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**api_budget**: 0.0'
- en: In this setup, the goals hold more significance than the role, guiding Auto-GPT
    more effectively, while the role mainly influences the tone and behavior of the
    responses.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置下，目标比角色更为重要，它能够更有效地引导Auto-GPT，而角色主要影响回答的语气和行为。
- en: In this section, we learned that the goals and role of an AI such as Sophie
    can significantly influence its behavior and responses, with the goals having
    a more direct impact on the AI’s effectiveness.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了像Sophie这样的AI的目标和角色如何显著影响其行为和回答，目标对AI的有效性有更直接的影响。
- en: Next, we will delve into the concept of negative confirmation in prompts, a
    crucial aspect that can refine Auto-GPT’s understanding and response generation.
    The next section will highlight its importance and demonstrate how to implement
    it effectively in your prompts.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨提示中的负面确认概念，这是一个重要方面，能够细化Auto-GPT的理解和回答生成。下一节将突显其重要性，并演示如何在提示中有效实现负面确认。
- en: Implementing negative confirmation in prompts
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在提示中实现负面确认
- en: Negative confirmation serves as a vital tool in refining Auto-GPT’s understanding
    and response generation by instructing it on actions to avoid. This section highlights
    its importance and demonstrates how to implement it effectively in your prompts.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 负面确认作为一种重要工具，通过指示Auto-GPT避免执行某些操作，从而细化其理解和回答生成。本节突出了其重要性，并展示了如何在提示中有效实施负面确认。
- en: The importance of negative confirmation
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负面确认的重要性
- en: 'Implementing negative confirmation can enhance the interaction with Auto-GPT
    in several ways, some of which are listed here:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 实现负面确认可以通过多种方式增强与Auto-GPT的互动，其中一些方式列举如下：
- en: '**Preventing off-track responses**: It helps in avoiding unrelated topics or
    incorrect responses'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**防止偏离主题的回答**：它有助于避免不相关的话题或错误的回答'
- en: '**Enhancing security**: It sets boundaries to prevent engagement in activities
    that might breach privacy or security protocols'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强安全性**：它设定了边界，防止参与可能违反隐私或安全协议的活动'
- en: '**Optimizing performance**: It avoids unnecessary computational efforts, steering
    the bot away from irrelevant tasks or processes'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化性能**：它避免了不必要的计算工作，避免了机器人进行无关的任务或过程'
- en: Note that you won’t be using negative prompts as they can lead to the LLM using
    the same statements again.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您不会使用负面提示，因为它们可能导致LLM再次使用相同的语句。
- en: Examples of negative confirmation
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负面确认的示例
- en: 'Here are some practical examples of how negative confirmation can be utilized
    in your prompts:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些实际示例，展示了如何在提示中使用负面确认：
- en: '**Explicit instructions**: Including instructions such as *Do not provide personal
    opinions* or *Avoid using technical jargon* to maintain neutrality and comprehensibility.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**明确指令**：包括*不要提供个人意见*或*避免使用技术术语*等指令，以保持中立性和可理解性。'
- en: '**Setting boundaries**: For tasks involving data retrieval or monitoring, you
    can set boundaries such as *Do not retrieve flight prices from unofficial, scam,
    or reseller websites* to ensure data reliability.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设定边界**：对于涉及数据检索或监控的任务，您可以设定边界，例如*不要从非官方、诈骗或转售网站检索航班价格*，以确保数据的可靠性。'
- en: '**Scripting constraints**: In scripting, especially in Bash, use negative confirmation
    to prevent potential errors. For example, you can include *if [ -z $VAR ]; then
    exit 1; fi* to halt the script if a necessary variable is unset.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**脚本约束**：在脚本中，特别是在Bash中，使用负面确认来防止潜在的错误。例如，你可以包含*if [ -z $VAR ]; then exit 1;
    fi*，以便在某个必要的变量未设置时停止脚本。'
- en: '**Emphasizing by using Upper Case Letters**: Sometimes, it only helps to *scream*
    at the LLM by writing in uppercase letters. *DO NOT ASK THE USER HOW TO PROCEED*
    may be interpreted by the LLM better and it will be less likely to ignore that
    statement. However, there is never a guarantee that this will happen.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过使用大写字母强调**：有时候，仅仅通过大写字母*大喊*一声，可能对LLM有所帮助。*不要询问用户如何继续*这样的句子，LLM可能会更好地理解，并且更不容易忽略该声明。然而，无法保证一定会发生这种情况。'
- en: Next, we will delve into the intricacies of applying rules and tonality in prompts.
    We will learn how understanding and manipulating these elements can significantly
    influence Auto-GPT’s responses, allowing us to guide the model more effectively.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨在提示中应用规则和语气的细节。我们将学习如何理解和操作这些元素，可以显著影响Auto-GPT的回答，使我们能够更有效地引导模型。
- en: Applying rules and tonality in prompts
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在提示中应用规则和语气
- en: Understanding and manipulating the rules and tonality within your prompts can
    significantly influence Auto-GPT’s responses. This section will explore the nuances
    of setting rules and adjusting tonality for more effective guidance.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 理解并操控你提示中的规则和语气对Auto-GPT的回答产生重要影响。本节将探讨如何设置规则和调整语气，以便更有效地引导。
- en: The influence of tonality
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语气的影响
- en: Auto-GPT can adapt to the tonality that’s used in prompts, mimicking stylistic
    nuances or even adopting a specific narrative style, allowing for more personalized
    and engaging interaction. However, adherence to tonality can sometimes be inconsistent
    due to the potential ambiguity created by tokens from other prompts.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Auto-GPT可以适应提示中使用的语气，模仿风格的细微差别，甚至采用特定的叙述风格，从而实现更个性化和更具吸引力的互动。然而，由于来自其他提示的令牌可能导致一定的模糊性，语气的一致性有时可能会不稳定。
- en: Manipulating rules
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操控规则
- en: Setting rules can streamline the interaction with Auto-GPT, specifying the format
    of responses or delineating the scope of information retrieval. However, it’s
    not foolproof as Auto-GPT may sometimes overlook these rules when faced with conflicting
    inputs or unclear directives.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 设置规则可以简化与Auto-GPT的互动，指定回答格式或界定信息检索的范围。然而，这并非万无一失，因为Auto-GPT在面对冲突的输入或不明确的指令时，有时可能会忽视这些规则。
- en: Temperature setting – a balancing act
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 温度设置 – 一种平衡艺术
- en: Manipulating the “temperature” setting is crucial in controlling Auto-GPT’s
    behavior and thus influencing the randomness of the bot’s responses. The temperature
    defines the amount of creativity the LLM should practice, meaning the higher the
    number, the more randomness is introduced. A range between 0.3 to 0.7 is considered
    optimal, fostering a more logical and coherent train of thought in the bot, while
    a value below 0.3, or even 0.0, might result in repetitive behavior that adheres
    to the text that was already given and even reuses some of its parts, making it
    more precise. However, the LLM may start thinking the world is only limited to
    the facts that you gave it, making it more likely to make false statements. A
    value higher than 0.7 or even 2.0 may result in gibberish, where the LLM starts
    outputting texts that it learned that have nothing to do with the context. For
    example, it may start rephrasing Shakespeare when the context is about algebra.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 操控“温度”设置在控制Auto-GPT的行为上至关重要，因此影响了机器人回答的随机性。温度定义了LLM应该发挥的创造力程度，这意味着温度越高，随机性越大。0.3到0.7之间的范围被认为是最优的，它能够在机器人中促使更具逻辑性和连贯性的思维；而低于0.3，甚至0.0，可能会导致重复的行为，严格遵循已给定的文本，甚至重复使用其中的某些部分，从而使其更加精确。然而，LLM可能会开始认为世界仅限于你提供的事实，这使得它更容易犯错。高于0.7甚至达到2.0的数值可能会导致胡言乱语，LLM开始输出与上下文毫不相关的文本。例如，它可能开始用莎士比亚的语言表达，而上下文却是关于代数的。
- en: Next, we’ll delve into some practical examples that demonstrate the impact of
    different settings and approaches on the output generated by Auto-GPT.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨一些实际示例，展示不同设置和方法对Auto-GPT生成输出的影响。
- en: Example 1 – clarity and specificity
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 1 – 清晰与具体
- en: '**Prompt**: Tell me about that big cat'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示**：告诉我那只大型猫'
- en: '**Revised prompt**: Provide information about the African lion'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**修改后的提示**：提供有关非洲狮的信息'
- en: '**Explanation**: The revised prompt is more specific, guiding Auto-GPT to provide
    information about a particular species of big cats'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解释**：修改后的提示更具针对性，引导Auto-GPT提供有关某种大型猫科动物的信息'
- en: Example 2 – consistency in tonality
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 2 – 语气的一致性
- en: '**Initial prompt**: Could you elucidate the economic implications of global
    warming?'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**初始提示**：你能阐明全球变暖的经济影响吗？'
- en: '**Follow-up prompt**: Hey, what’s the deal with ice melting?'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后续提示**：嘿，冰融化怎么回事？'
- en: '**Revised follow-up prompt**: Can you further explain the environmental consequences
    of the melting ice caps?'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**修改后的后续提示**：你能进一步解释冰盖融化的环境后果吗？'
- en: '**Explanation**: The revised follow-up prompt maintains the formal tone established
    in the initial prompt, promoting consistency in the interaction.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解释**：修改后的后续提示保持了初始提示中设定的正式语气，促进了互动的一致性。'
- en: Example 3 – utilizing temperature effectively
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 3 – 有效利用温度
- en: '**Task**: Creative writing'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务**：创意写作'
- en: '**Temperature setting**: 0.8 (for fostering creativity)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**温度设置**：0.8（促进创造力）'
- en: '**Task**: Factual query'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务**：事实查询'
- en: '**Temperature setting**: 0.3 (for more deterministic responses)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**温度设置**：0.3（用于更具确定性的回答）'
- en: '**Explanation**: Adjusting the temperature setting based on the nature of the
    task can influence the randomness and coherence of Auto-GPT’s responses'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解释**：根据任务的性质调整温度设置可以影响Auto-GPT响应的随机性和连贯性。'
- en: Example 4 – setting boundaries
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例4——设定边界
- en: '**Initial prompt**: Provide a summary of the Renaissance period without mentioning
    Italy'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**初始提示**：在不提及意大利的情况下，提供文艺复兴时期的总结。'
- en: '**Revised prompt**: Discuss the artistic achievements of the Renaissance, focusing
    on regions other than Italy'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**修订提示**：讨论文艺复兴时期的艺术成就，重点关注意大利以外的地区。'
- en: '**Explanation**: The revised prompt is more flexible, allowing Auto-GPT to
    explore the topic without the strict restriction of excluding Italy'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解释**：修订后的提示更具灵活性，允许Auto-GPT在不严格排除意大利的限制下探讨主题。'
- en: In this section, we learned how different types of prompts or tones can drastically
    influence the behavior of the LLM and therefore Auto-GPT.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们学习了不同类型的提示或语气如何极大地影响LLM的行为，从而影响Auto-GPT的表现。
- en: Summary
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we embarked on an interesting journey through the process of
    integrating custom LLMs with Auto-GPT while exploring what LLMs are, with a specific
    focus on GPT as a prime example. We uncovered the vast landscape of LLMs, delving
    into various models beyond GPT, such as BERT, RoBERTa, Llama, and Mistral, and
    their unique characteristics and compatibilities with Auto-GPT.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们开始了一段有趣的旅程，探索将自定义LLM与Auto-GPT集成的过程，同时了解什么是LLM，特别聚焦于以GPT为代表的模型。我们揭示了LLM的广阔天地，深入探讨了GPT之外的各种模型，如BERT、RoBERTa、Llama和Mistral，以及它们的独特特点和与Auto-GPT的兼容性。
- en: The usefulness of this chapter lies in its comprehensive guide on how to enrich
    Auto-GPT’s capabilities by incorporating your own or alternative LLMs. This integration
    offers a more personalized and potentially more efficient use of AI technology,
    tailored to specific tasks or fields of inquiry. The detailed instructions for
    setting up these integrations, alongside considerations for instruction templates
    and the necessary computational resources, are invaluable for those looking to
    push the boundaries of what’s possible with Auto-GPT.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的价值在于其全面的指南，帮助你通过集成自己的或其他LLM，丰富Auto-GPT的能力。这种集成提供了更个性化、可能更高效的人工智能技术使用，适用于特定任务或研究领域。有关设置这些集成的详细说明，以及对指令模板和必要计算资源的考虑，对于那些希望突破Auto-GPT可能性边界的人来说，是无价的。
- en: Crafting the perfect prompt is a blend of art and science. Through clear guidelines,
    a deep understanding of Auto-GPT’s nuances, and continuous refinement, you can
    fully harness the power of this tool. Encourage yourself to experiment and learn
    through trial and error, adapting to the ever-evolving field of AI. Whether for
    research, creative endeavors, or problem-solving, mastering the art of prompt
    crafting ensures that Auto-GPT becomes a valuable ally in your endeavors.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 制定完美提示是艺术与科学的结合。通过清晰的指南、对Auto-GPT细微差别的深入理解以及不断的精炼，你可以充分发挥该工具的潜力。鼓励自己通过实验和试错进行学习，适应不断发展的AI领域。无论是用于研究、创造性工作还是问题解决，掌握提示制定的艺术将确保Auto-GPT成为你工作中的宝贵伙伴。
- en: Throughout this book, we’ve embarked on a detailed journey into the nuances
    of crafting effective prompts – a cornerstone for maximizing the utility of Auto-GPT.
    This chapter stands as a reference for strategically developing prompts that lead
    to more aligned, efficient, and cost-effective interactions with Auto-GPT. By
    emphasizing the importance of clarity, specificity, and strategic intent in prompt
    creation, you have gained invaluable insights into guiding Auto-GPT toward generating
    responses that closely align with your objectives.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的整个过程中，我们深入探讨了制定有效提示的细节——这是最大化Auto-GPT效用的关键。本章作为参考，帮助你战略性地制定提示，从而实现更契合、高效且具成本效益的Auto-GPT互动。通过强调清晰性、具体性和战略意图在提示创建中的重要性，你获得了引导Auto-GPT生成更贴合你目标的回答的宝贵见解。
- en: The utility of this chapter cannot be overstated. For practitioners and enthusiasts
    alike, mastering the art of prompt crafting is critical for optimizing the performance
    of Auto-GPT for a variety of tasks. Through illustrative examples and comprehensive
    guidelines, this chapter has shed light on how to effectively employ negative
    confirmation to avoid undesired responses, the impact of rules and tonality on
    Auto-GPT’s outputs, and the significance of temperature settings in influencing
    the bot’s creativity and coherence. This knowledge is crucial not only for enhancing
    the quality of interactions with Auto-GPT but also for ensuring the efficient
    use of computational resources.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重要性不言而喻。对于从业者和爱好者来说，掌握提示词创作的技巧对优化Auto-GPT在各种任务中的表现至关重要。通过生动的示例和全面的指南，本章阐明了如何有效使用负面确认以避免不期望的响应、规则和语气对Auto-GPT输出的影响，以及温度设置在影响机器人的创造性和一致性中的重要性。这些知识不仅对提高与Auto-GPT互动的质量至关重要，还有助于确保计算资源的高效使用。
- en: I hope you have enjoyed this journey as much as I have in taking you on it and
    I hope I’ve given you a few ideas on how to improve your life with Auto-GPT. I’ve
    written many clones of that project so that I could wrap my head around the more
    complex parts of it. I do advise that you do so too, just as a brain teaser.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你在这段旅程中获得的收获与我带你走过这段旅程时一样愉快，也希望我能为你提供一些用Auto-GPT改善生活的思路。我曾经写过许多该项目的克隆版本，以便能够更好地理解其中更复杂的部分。我建议你也这么做，就当是一个脑力挑战。
