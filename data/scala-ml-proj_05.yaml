- en: Topic Modeling - A Better Insight into Large-Scale Texts
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模 - 对大规模文本的深入理解
- en: '**Topic modeling** (**TM**) is a technique widely used in mining text from
    a large collection of documents. These topics can then be used to summarize and
    organize documents that include the topic terms and their relative weights. The
    dataset that will be used for this project is just in plain unstructured text
    format.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**主题建模**（**TM**）是一种广泛用于从大量文档中挖掘文本的技术。通过这些主题，可以总结和组织包含主题词及其相对权重的文档。这个项目将使用的数据集是纯文本格式，未经过结构化处理。'
- en: We will see how effectively we can use the **Latent Dirichlet Allocation** (**LDA**)
    algorithm for finding useful patterns in the data. We will compare other TM algorithms
    and the scalability power of LDA. In addition, we will utilize **Natural Language
    Processing** (**NLP**) libraries, such as Stanford NLP.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到如何有效地使用**潜在狄利克雷分配**（**LDA**）算法来发现数据中的有用模式。我们将比较其他TM算法及LDA的可扩展性。此外，我们还将利用**自然语言处理**（**NLP**）库，如斯坦福NLP。
- en: 'In a nutshell, we will learn the following topics throughout this end-to-end
    project:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在这个端到端的项目中，我们将学习以下主题：
- en: Topic modelling and text clustering
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题建模与文本聚类
- en: How does LDA algorithm work?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDA算法是如何工作的？
- en: Topic modeling with LDA, Spark MLlib, and Standard NLP
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LDA、Spark MLlib和标准NLP进行主题建模
- en: Other topic models and the scalability testing of LDA
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他主题模型与LDA的可扩展性测试
- en: Model deployment
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型部署
- en: Topic modeling and text clustering
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模与文本聚类
- en: In TM, a topic is defined by a cluster of words, with each word in the cluster
    having a probability of occurrence for the given topic, and different topics having
    their respective clusters of words along with corresponding probabilities. Different
    topics may share some words, and a document can have more than one topic associated
    with it. So in short, we have a collection of text datasets—that is, a set of
    text files. Now the challenging part is finding useful patterns about the data
    using LDA.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在TM中，主题是通过一组词汇来定义的，每个词汇在该主题下都有一个出现的概率，不同的主题有各自的词汇集合及其相应的概率。不同的主题可能共享一些词汇，而一个文档可能与多个主题相关联。简而言之，我们有一组文本数据集，即一组文本文件。现在，挑战在于使用LDA从数据中发现有用的模式。
- en: 'There is a popular TM approach, based on LDA, where each document is considered
    a mixture of topics and each word in a document is considered randomly drawn from
    a document''s topics. The topics are considered hidden and must be uncovered via
    analyzing joint distributions to compute the conditional distribution of hidden
    variables (topics), given the observed variables and words in documents. The TM
    technique is widely used in the task of mining text from a large collection of
    documents. These topics can then be used to summarize and organize documents that
    include the topic terms and their relative weights (see *Figure 1*):'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种流行的基于LDA的TM方法，其中每个文档被视为多个主题的混合，每个文档中的词汇被认为是从文档的主题中随机抽取的。这些主题被视为隐藏的，必须通过分析联合分布来揭示，从而计算给定观察变量和文档中的词语的条件分布（主题）。TM技术广泛应用于从大量文档中挖掘文本的任务。这些主题随后可以用来总结和组织包含主题词及其相对权重的文档（见*图1*）：
- en: '![](img/998c2c26-e90d-46c5-8029-8874910126df.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/998c2c26-e90d-46c5-8029-8874910126df.png)'
- en: 'Figure 1: TM in a nutshell (source: Blei, D.M. et al., Probabilistic topic
    models, ACM communication, 55(4(, 77-84, 2012)))'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：TM概述（来源：Blei, D.M.等，概率主题模型，ACM通信，55(4)，77-84，2012）
- en: 'As the number of topics that can be seen in the preceding figure is a lot smaller
    than the vocabulary associated with the document collection, the topic-space representation
    can be viewed as a dimensionality-reduction process as well:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，主题的数量远小于与文档集合相关联的词汇量，因此主题空间的表示可以被看作是一种降维过程：
- en: '![](img/9557f8d8-4641-4391-9691-bae70f289da5.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9557f8d8-4641-4391-9691-bae70f289da5.png)'
- en: 'Figure 2: TM versus text clustering'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：TM与文本聚类的对比
- en: In contrast to TM, in document clustering, the basic idea is to group documents
    into different groups based on a well-known similarity measure. To perform grouping,
    each document is represented by a vector representing the weights assigned to
    words in the document.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与TM相比，在文档聚类中，基本思想是根据一种广为人知的相似性度量，将文档分组。为了进行分组，每个文档都由一个向量表示，该向量表示文档中词汇的权重。
- en: 'It is common to perform weighting using the term frequency-inverse document
    frequency (also known also the **TF-IDF** scheme). The end result of clustering
    is a list of clusters with every document showing up in one of the clusters. The
    basic difference between TM and text clustering can be illustrated by the following
    figure:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通常使用词频-逆文档频率（也称为 **TF-IDF** 方案）来进行加权。聚类的最终结果是一个簇的列表，每个文档出现在某一个簇中。TM 和文本聚类的基本区别可以通过下图来说明：
- en: How does LDA algorithm work?
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LDA 算法是如何工作的？
- en: 'LDA is a topic model that infers topics from a collection of text documents.
    LDA can be thought of as a clustering algorithm where topics correspond to cluster
    centers, and documents correspond to examples (rows) in a dataset. Topics and
    documents both exist in a feature space, where feature vectors are vectors of
    word counts (bags of words). Instead of estimating a clustering using a traditional
    distance, LDA uses a function based on a statistical model of how text documents
    are generated (see in *Figure 3*):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 是一个主题模型，用于从一组文本文档中推断主题。LDA 可以被看作是一个聚类算法，其中主题对应于聚类中心，文档对应于数据集中的实例（行）。主题和文档都存在于特征空间中，其中特征向量是词频向量（词袋）。LDA
    不是通过传统的距离估计聚类，而是使用基于文本文档生成的统计模型的函数（见 *图 3*）：
- en: '![](img/87d01601-71f3-4c34-9fbc-cf468d483985.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87d01601-71f3-4c34-9fbc-cf468d483985.png)'
- en: 'Figure 3: Working principle of LDA algorithms on a collection of documents'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：LDA 算法在一组文档上的工作原理
- en: Particularly, we would like to discuss which topics people talk about most from
    the large collection of text. Since the release of Spark 1.3, MLlib supports the
    LDA, which is one of the most successfully used TM techniques in the area of text
    mining and NLP.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们希望讨论人们在大量文本中最常谈论的话题。自 Spark 1.3 发布以来，MLlib 支持 LDA，这是文本挖掘和自然语言处理领域中最成功使用的主题模型（TM）技术之一。
- en: 'Moreover, LDA is also the first MLlib algorithm to adopt Spark GraphX. The
    following terminologies are worth knowing before we formally start our TM application:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LDA 还是第一个采用 Spark GraphX 的 MLlib 算法。以下术语是我们正式开始 TM 应用之前值得了解的：
- en: '`"word" = "term"`: an element of the vocabulary'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"word" = "term"`：词汇表中的一个元素'
- en: '`"token"`: instance of a term appearing in a document'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"token"`：出现在文档中的术语实例'
- en: '`"topic"`: multinomial distribution over words representing some concept'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"topic"`：表示某一概念的词汇的多项式分布'
- en: 'The RDD-based LDA algorithm developed in Spark is a topic model designed for
    text documents. It is based on the original LDA paper (journal version): Blei,
    Ng, and Jordan, *Latent Dirichlet Allocation*, JMLR, 2003.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中开发的基于 RDD 的 LDA 算法是一个为文本文档设计的主题模型。它基于原始的 LDA 论文（期刊版）：Blei, Ng, 和 Jordan，*Latent
    Dirichlet Allocation*，JMLR，2003。
- en: This implementation supports different inference algorithms via the `setOptimizer`
    function. The `EMLDAOptimizer` learns clustering using **expectation-maximization**
    (**EM**) on the likelihood function and yields comprehensive results, while `OnlineLDAOptimizer`
    uses iterative mini-batch sampling for online variational inference and is generally
    memory-friendly.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现通过 `setOptimizer` 函数支持不同的推理算法。`EMLDAOptimizer` 使用 **期望最大化**（**EM**）在似然函数上进行聚类学习，并提供全面的结果，而
    `OnlineLDAOptimizer` 使用迭代的小批量采样进行在线变分推理，并且通常对内存友好。
- en: EM is an iterative way to approximate the maximum likelihood function. In practice,
    when the input data is incomplete, has missing data points, or has hidden latent
    variables, ML estimation can find the `best fit` model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: EM 是一种迭代方式，用于逼近最大似然函数。在实际应用中，当输入数据不完整、缺失数据点或存在隐藏潜在变量时，最大似然估计可以找到 `最佳拟合` 模型。
- en: 'LDA takes in a collection of documents as vectors of word counts and the following
    parameters (set using the builder pattern):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 输入一组文档，作为词频向量，并使用以下参数（通过构建器模式设置）：
- en: '`K`: Number of topics (that is, cluster centers) (default is 10).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`K`：主题的数量（即聚类中心的数量）（默认值是 10）。'
- en: '`ldaOptimizer`: Optimizer to use for learning the LDA model, either `EMLDAOptimizer`
    or `OnlineLDAOptimizer` (default is `EMLDAOptimizer`).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ldaOptimizer`：用于学习 LDA 模型的优化器，可以是 `EMLDAOptimizer` 或 `OnlineLDAOptimizer`（默认是
    `EMLDAOptimizer`）。'
- en: '`Seed`: Random seed for the reproducibility (optional though).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Seed`：用于可重复性的随机种子（虽然是可选的）。'
- en: '`docConcentration`: Drichilet parameter for prior over documents distributions
    over topics. Larger values encourage smoother inferred distributions (default
    is - `Vectors.dense(-1)`).'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docConcentration`：文档主题分布的 Dirichlet 参数。较大的值会鼓励推断出的分布更平滑（默认值是 `Vectors.dense(-1)`）。'
- en: '`topicConcentration`: Drichilet parameter for prior over topics'' distributions
    over terms (words). Larger values ensure smoother inferred distributions (default
    is -1).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`topicConcentration`: Drichilet 参数，用于先验主题在术语（单词）分布上的分布。较大的值确保推断分布更加平滑（默认为 -1）。'
- en: '`maxIterations`: Limit on the number of iterations (default is 20).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxIterations`: 迭代次数的限制（默认为 20）。'
- en: '`checkpointInterval`: If using checkpointing (set in the Spark configuration),
    this parameter specifies the frequency with which checkpoints will be created.
    If `maxIterations` is large, using check pointing can help reduce shuffle file
    sizes on disk and help with failure recovery (default is 10).'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`checkpointInterval`: 如果使用检查点（在 Spark 配置中设置），此参数指定创建检查点的频率。如果 `maxIterations`
    很大，使用检查点可以帮助减少磁盘上的洗牌文件大小，并有助于故障恢复（默认为 10）。'
- en: '![](img/b1acc9cd-72c8-45e3-a84b-a2184a5e5e0e.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b1acc9cd-72c8-45e3-a84b-a2184a5e5e0e.png)'
- en: 'Figure 4: The topic distribution and how it looks'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：主题分布及其外观
- en: Let's see an example. Assume there are *n* balls in a basket having *w* different
    colors. Now also assume each term in a vocabulary has one of *w* colors. Now also
    assume that the vocabulary terms are distributed in *m* topics. Now the frequency
    of occurrence of each color in the basket is proportional to the corresponding
    term's weight in topic, *φ*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子。假设篮子里有 *n* 个球，有 *w* 种不同的颜色。现在假设词汇表中的每个术语都有 *w* 种颜色之一。现在还假设词汇表中的术语分布在
    *m* 个主题中。现在篮子中每种颜色出现的频率与对应术语在主题 *φ* 中的权重成比例。
- en: Then the LDA algorithm incorporates a term weighting scheme by making the size
    of each ball proportional to the weight of its corresponding term. In *Figure
    4*, *n* terms have the total weights in a topic, for example, topic 0 to 3. *Figure
    4* shows topic distribution from randomly generated Tweet text.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，LDA 算法通过使每个球的大小与其对应术语的权重成比例来包含一个术语加权方案。在 *图 4* 中，*n* 个术语在一个主题中具有总权重，例如主题
    0 到 3。*图 4* 展示了从随机生成的 Twitter 文本中的主题分布。
- en: 'Now that we have seen that by using TM, we find the structure within an unstructured
    collection of documents. Once the structure is **discovered**, as shown in *Figure
    4*, we can answer several questions as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到通过使用 TM，我们可以在非结构化的文档集合中找到结构。一旦 **发现** 结构，如 *图 4* 所示，我们可以回答几个问题，如下所示：
- en: What is document X about?
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档 X 是关于什么的？
- en: How similar are documents X and Y?
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档 X 和 Y 有多相似？
- en: If I am interested in topic Z, which documents should I read first?
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我对主题 Z 感兴趣，我应该先阅读哪些文档？
- en: In the next section, we will see an example of TM using a Spark MLlib-based
    LDA algorithm to answer the preceding questions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将看到一个使用基于 Spark MLlib 的 LDA 算法的 TM 示例，以回答前面的问题。
- en: Topic modeling with Spark MLlib and Stanford NLP
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spark MLlib 和 Stanford NLP 进行主题建模
- en: In this subsection, we represent a semi-automated technique of TM using Spark.
    Using other options as defaults, we train LDA on the dataset downloaded from GitHub
    at [https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test](https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test).
    However, we will use more well-known text datasets in the model reuse and deployment
    phase later in this chapter.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们使用 Spark 表示了一种半自动化的 TM 技术。在模型重用和部署阶段，我们将使用从 GitHub 下载的数据集训练 LDA，位于 [https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test](https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test)。然而，稍后在本章节中，我们将使用更知名的文本数据集。
- en: Implementation
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施
- en: 'The following steps show TM from data reading to printing the topics, along
    with their term weights. Here''s the short workflow of the TM pipeline:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤展示了从数据读取到打印主题的 TM 过程，以及它们的术语权重。以下是 TM 管道的简短工作流程：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We also need to import some related packages and libraries:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要导入一些相关的包和库：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The actual computation on TM is done in the `LDAforTM` class. The `Params`
    is a case class, which is used for loading the parameters to train the LDA model.
    Finally, we train the LDA model using the parameters setting via the `Params`
    class. Now we will explain each step broadly with step-by-step source code:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，TM 的计算是在 `LDAforTM` 类中完成的。`Params` 是一个案例类，用于加载用于训练 LDA 模型的参数。最后，我们通过 `Params`
    类设置的参数来训练 LDA 模型。现在我们将详细解释每个步骤，逐步源代码：
- en: Step 1 - Creating a Spark session
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一步 - 创建一个 Spark 会话
- en: 'Let''s create a Spark session by defining the number of computing cores, the
    SQL warehouse, and the application name as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下方式创建一个 Spark 会话：定义计算核心数量、SQL 仓库和应用程序名称。
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Step 2 - Creating vocabulary and tokens count to train the LDA after text pre-processing
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 2 - 创建词汇表和标记计数以训练 LDA 模型，在文本预处理后进行
- en: 'The `run()` method takes `params` such as input text, predefined vocabulary
    size, and stop word file:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`run()` 方法接受 `params`，如输入文本、预定义的词汇表大小和停用词文件：'
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, it starts text pre-processing for the LDA model as follows (that is,
    inside the `run` method):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它将开始为 LDA 模型进行文本预处理，如下所示（即在 `run` 方法内部）：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `Params` case class is used to define the parameters to train the LDA model.
    This goes as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`Params` case 类用于定义训练 LDA 模型的参数。代码如下所示：'
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For better result, you set these parameters in try and error basis. Alternatively,
    you should go with the cross-validation for even better performance. Now that
    if you want to checkpoint the current parameters, uses the following line of code:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更好的结果，你需要通过反复试验来设置这些参数。或者，你可以选择交叉验证以获得更好的性能。如果你想检查当前的参数，可以使用以下代码：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `preprocess` method is used to process the raw text. First, let''s read
    the whole text using the `wholeTextFiles()` method as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`preprocess` 方法用于处理原始文本。首先，让我们使用 `wholeTextFiles()` 方法读取整个文本，如下所示：'
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the preceding code, `paths` are the path of the text files. Then, we need
    to prepare the morphological RDD from the raw text after, based on the `lemma`
    texts, as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`paths` 是文本文件的路径。然后，我们需要根据 `lemma` 文本准备从原始文本中提取的形态学 RDD，如下所示：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here, the `getLemmaText()` method from the `helperForLDA` class supplies the
    `lemma` texts after filtering the special characters, such as (``"""[! @ # $ %
    ^ & * ( ) _ + - − , " '' ; : . ` ? --]``), as regular expressions, using the `filterSpaecialChatacters()`
    method. The method goes as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，`helperForLDA` 类中的 `getLemmaText()` 方法提供了经过过滤特殊字符后的 `lemma` 文本，如 (``"""[!
    @ # $ % ^ & * ( ) _ + - − , " '' ; : . ` ? --]``)，作为正则表达式，使用 `filterSpecialCharacters()`
    方法。方法如下所示：'
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It is to be noted that the `Morphology()` class computes the base form of English
    words by removing only inflections (not derivational morphology). That is, it
    only does noun plurals, pronoun case, and verb endings, and not things such as
    comparative adjectives or derived nominal. The `getLemmaText()` method takes the
    document and the corresponding morphology and finally returns the lemmatized texts.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，`Morphology()` 类通过移除仅有屈折变化的部分（而非派生形态学）来计算英语单词的基本形式。也就是说，它只处理名词复数、代词的格、动词词尾，而不涉及比较级形容词或派生名词等内容。`getLemmaText()`
    方法接收文档及其对应的形态学信息，并最终返回已词干化的文本。
- en: 'This comes from the Stanford NLP group. To use this, you should have the following
    import in the main class file: `edu.stanford.nlp.process.Morphology`. In the `pom.xml`
    file, you will have to include the following entries as dependencies:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这个来自斯坦福 NLP 小组。要使用它，你需要在主类文件中添加以下导入：`edu.stanford.nlp.process.Morphology`。在
    `pom.xml` 文件中，你需要包含以下条目作为依赖项：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `filterSpecialCharacters()` goes as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`filterSpecialCharacters()` 方法如下所示：'
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once we have the RDD with special characters removed, we can create a DataFrame
    for building the text analytics pipeline:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到了移除特殊字符的 RDD，就可以创建 DataFrame 来构建文本分析管道：
- en: '[PRE12]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The DataFrame contains only document tags. A snapshot of the DataFrame is as
    follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 只包含文档标签。DataFrame 的快照如下：
- en: '![](img/aa7d27a0-30ec-409b-8753-ee85f6410800.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa7d27a0-30ec-409b-8753-ee85f6410800.png)'
- en: 'Figure 5: Raw texts from the input dataset'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：来自输入数据集的原始文本
- en: 'Now if you look at the preceding DataFrame carefully, you will see that we
    still need to tokenize them. Moreover, there are stop words in the DataFrame,
    such as this, with, and so on, so we need to remove them as well. First, let''s
    tokenize them using the `RegexTokenizer` API as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你仔细观察前面的 DataFrame，你会发现我们仍然需要对其进行分词。此外，DataFrame 中包含停用词，如 this、with 等，因此我们还需要将它们移除。首先，让我们使用
    `RegexTokenizer` API 对其进行分词，如下所示：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now let''s remove all the stop words as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们按照以下方式移除所有停用词：
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Furthermore, we also need to apply count vectors to find only the important
    features from the tokens. This will help make the pipeline chained as the pipeline
    stage. Let''s do it as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要应用计数向量，以便从标记中找到仅重要的特征。这将有助于将管道阶段链接在一起。我们按照以下方式进行：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: When an a-priori dictionary is not available, `CountVectorizer` can be used
    as an Estimator to extract the vocabulary and generate a `CountVectorizerModel`.
    In other words, `CountVectorizer` is used to convert a collection of text documents
    to vectors of token (that is, term) counts. The `CountVectorizerModel` produces
    sparse representations for the documents over the vocabulary, which can then be
    fed to LDA. More technically, when the `fit()` method is invoked for the fitting
    process, `CountVectorizer` will select the top `vocabSize` words ordered by term
    frequency across the corpus.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当先验词典不可用时，可以使用`CountVectorizer`作为估算器来提取词汇并生成`CountVectorizerModel`。换句话说，`CountVectorizer`用于将一组文本文档转换为标记（即术语）计数的向量。`CountVectorizerModel`为文档提供稀疏表示，这些文档会基于词汇表然后输入到LDA模型中。从技术上讲，当调用`fit()`方法进行拟合时，`CountVectorizer`会选择按术语频率排序的前`vocabSize`个词。
- en: 'Now, create the pipeline by chaining the transformers (tokenizer, `stopWordsRemover`,
    and `countVectorizer`) as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过链接转换器（tokenizer、`stopWordsRemover`和`countVectorizer`）来创建管道，代码如下：
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, let''s fit and transform the pipeline toward the vocabulary and number
    of tokens:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将管道拟合并转换为词汇表和标记数：
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, return the vocabulary and token count pairs as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，返回词汇和标记计数对，代码如下：
- en: '[PRE18]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Step 3 - Instantiate the LDA model before training
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3步 - 在训练之前实例化LDA模型
- en: 'Let us instantiate the LDA model before we begin training it with the following
    code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练LDA模型之前，让我们实例化LDA模型，代码如下：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Step 4 - Set the NLP optimizer
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4步 - 设置NLP优化器
- en: For better and optimized results from the LDA model, we need to set the optimizer
    that contains an algorithm for LDA, and performs the actual computation that stores
    the internal data structure (for example, graph or matrix) and other parameters
    for the algorithm.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从LDA模型获得更好的优化结果，我们需要设置一个包含LDA算法的优化器，它执行实际的计算，并存储算法的内部数据结构（例如图形或矩阵）及其他参数。
- en: Here we use the `EMLDAOPtimizer` optimizer. You can also use the `OnlineLDAOptimizer()`
    optimizer. The `EMLDAOPtimizer` stores a *data + parameter* graph, plus algorithm
    parameters. The underlying implementation uses EM.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`EMLDAOPtimizer`优化器。你也可以使用`OnlineLDAOptimizer()`优化器。`EMLDAOPtimizer`存储一个*数据+参数*图形，以及算法参数。其底层实现使用EM（期望最大化）。
- en: 'First, let''s instantiate the `EMLDAOptimizer` by adding `(1.0 / actualCorpusSize)`
    along with a very low learning rate (that is, 0.05) to `MiniBatchFraction` to
    converge the training on a tiny dataset like ours as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们通过添加`(1.0 / actualCorpusSize)`以及一个非常低的学习率（即0.05）到`MiniBatchFraction`来实例化`EMLDAOptimizer`，以便在像我们这样的小数据集上收敛训练，代码如下：
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, set the optimizer using the `setOptimizer()` method from the LDA API as
    follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用LDA API中的`setOptimizer()`方法设置优化器，代码如下：
- en: '[PRE21]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Step 5 - Training the LDA model
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5步 - 训练LDA模型
- en: 'Let''s start training the LDA model using the training corpus and keep track
    of the training time as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始使用训练语料库训练LDA模型，并跟踪训练时间，代码如下：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now additionally, we can save the trained model for future reuse that can goes
    as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，此外，我们可以保存训练好的模型以供将来重用，保存的代码如下：
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note that once you have finished the training and got the most optimal training,
    uncomment the preceding line before you deploy the model. Otherwise, it will get
    stopped by throwing an exception in the model reuse phase.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一旦完成训练并获得最优训练效果，部署模型之前需要取消注释前面的行。否则，它将在模型重用阶段因抛出异常而被停止。
- en: 'For the text we have, the LDA model took 6.309715286 seconds to train. Note
    these timing codes are optional. Here we provide them for reference purposes only
    to get an idea of the training time:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们拥有的文本，LDA模型训练耗时6.309715286秒。请注意，这些时间代码是可选的。我们仅提供它们供参考，以便了解训练时间：
- en: Step 6 - Prepare the topics of interest
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6步 - 准备感兴趣的主题
- en: 'Prepare the top 5 topics with each topic having 10 terms. Include the terms
    and their corresponding weights:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 准备前5个主题，每个主题包含10个术语。包括术语及其对应的权重：
- en: '[PRE24]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Step 7 - Topic modelling
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7步 - 主题建模
- en: 'Print the top 10 topics, showing the top-weighted terms for each topic. Also,
    include the total weight in each topic as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出前10个主题，展示每个主题的权重最高的词汇。同时，还包括每个主题的总权重，代码如下：
- en: '[PRE25]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now let''s see the output of our LDA model towards topics modeling:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看LDA模型在主题建模方面的输出：
- en: '[PRE26]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: From the preceding output, we can see that topic five of the input documents
    has the most weight, at `0.28263550964558276`. This topic discusses terms such
    as `captain`, `fogg`, `nemo`, `vessel`, and `land`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述输出中，我们可以看到输入文档的主题五占据了最大权重，权重为 `0.28263550964558276`。该主题涉及的词汇包括 `captain`、`fogg`、`nemo`、`vessel`
    和 `land` 等。
- en: Step 8 - Measuring the likelihood of two documents
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 8 步 - 测量两个文档的似然度
- en: 'Now to get some more statistics, such as maximum likelihood or log likelihood
    on the document, we can use the following code:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在为了获取一些其他统计数据，例如文档的最大似然或对数似然，我们可以使用以下代码：
- en: '[PRE27]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The preceding code calculates the average log likelihood of the LDA model as
    an instance of the distributed version of the LDA model:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码计算了 LDA 模型的平均对数似然度，作为 LDA 分布式版本的一个实例：
- en: '[PRE28]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: For more information on the likelihood measurement, interested readers should
    refer to [https://en.wikipedia.org/wiki/Likelihood_function](https://en.wikipedia.org/wiki/Likelihood_function).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 有关似然度测量的更多信息，感兴趣的读者可以参考[https://en.wikipedia.org/wiki/Likelihood_function](https://en.wikipedia.org/wiki/Likelihood_function)。
- en: 'Now imagine that we''ve computed the preceding metric for document X and Y.
    Then we can answer the following question:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们已经计算了文档 X 和 Y 的前述度量。然后我们可以回答以下问题：
- en: How similar are documents X and Y?
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档 X 和 Y 有多相似？
- en: 'The thing is, we should try to get the lowest likelihood from all the training
    documents and use it as a threshold for the previous comparison. Finally, to answer
    the third and final question:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于，我们应该尝试从所有训练文档中获取最低的似然值，并将其作为前述比较的阈值。最后，回答第三个也是最后一个问题：
- en: If I am interested in topic Z, which documents should I read first?
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我对主题 Z 感兴趣，应该先阅读哪些文档？
- en: 'A minimal answer: taking a close look at the topic distributions and the relative
    term weights, we can decide which document we should read first.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化的答案：通过仔细查看主题分布和相关词汇的权重，我们可以决定首先阅读哪个文档。
- en: Other topic models versus the scalability of LDA
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LDA 的可扩展性与其他主题模型的比较
- en: Throughout this end-to-end project, we have used LDA, which is one of the most
    popular TM algorithms used for text mining. We could use more robust TM algorithms,
    such as **Probabilistic Latent Sentiment Analysis** (**pLSA**), **Pachinko Allocation
    Model** (**PAM**), and **Hierarchical Drichilet Process** (**HDP**) algorithms.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个端到端的项目中，我们使用了 LDA，它是最流行的文本挖掘主题建模算法之一。我们还可以使用更强大的主题建模算法，如 **概率潜在语义分析**（**pLSA**）、**帕金科分配模型**（**PAM**）和
    **层次狄利克雷过程**（**HDP**）算法。
- en: However, pLSA has the overfitting problem. On the other hand, both HDP and PAM
    are more complex TM algorithms used for complex text mining, such as mining topics
    from high-dimensional text data or documents of unstructured text. Finally, non-negative
    matrix factorization is another way to find topics in a collection of documents.
    Irrespective of the approach, the output of all the TM algorithms is a list of
    topics with associated clusters of words.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，pLSA 存在过拟合问题。另一方面，HDP 和 PAM 是更复杂的主题建模算法，用于处理复杂的文本挖掘任务，例如从高维文本数据或非结构化文本文档中挖掘主题。最后，非负矩阵分解是另一种在文档集合中寻找主题的方法。无论采用哪种方法，所有主题建模算法的输出都是一个包含相关词汇簇的主题列表。
- en: The previous example shows how to perform TM using the LDA algorithm as a standalone
    application. The parallelization of LDA is not straightforward, and there have
    been many research papers proposing different strategies. The key obstacle in
    this regard is that all methods involve a large amount of communication.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例展示了如何使用 LDA 算法作为独立应用程序来执行主题建模。LDA 的并行化并不简单，许多研究论文提出了不同的策略。关键障碍在于所有方法都涉及大量的通信。
- en: 'According to the blog on the Databricks website ([https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html](https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html)),
    here are the statistics of the dataset and related training and test sets that
    were used during the experimentation:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Databricks 网站上的博客（[https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html](https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html)），以下是实验过程中使用的数据集以及相关的训练和测试集的统计数据：
- en: '**Training set size**: 4.6 million documents'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集大小**：460万份文档'
- en: '**Vocabulary size**: 1.1 million terms'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词汇表大小**：110万个词条'
- en: '**Training set size**: 1.1 billion tokens (~239 words/document)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集大小**：11亿个词条（约239个词/文档）'
- en: 100 topics
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 100 个主题
- en: 16-worker EC2 cluster, for example, M4.large or M3.medium depending upon budget
    and requirements
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，16个工作节点的EC2集群，可以选择M4.large或M3.medium，具体取决于预算和需求
- en: For the preceding setting, the timing result was 176 seconds/iteration on average
    over 10 iterations. From these statistics, it is clear that LDA is quite scalable
    for a very large number of the corpus as well.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上述设置，经过10次迭代，平均时间结果为176秒/迭代。从这些统计数据可以看出，LDA对于非常大的语料库也是非常具有可扩展性的。
- en: Deploying the trained LDA model
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署训练好的LDA模型
- en: 'For this mini deployment, let''s use a real-life dataset: PubMed. A sample
    dataset containing PubMed terms can be downloaded from: [https://nlp.stanford.edu/software/tmt/tmt-0.4/examples/pubmed-oa-subset.csv](https://nlp.stanford.edu/software/tmt/tmt-0.4/examples/pubmed-oa-subset.csv).
    This link actually contains a dataset in CSV format but has a strange name, `4UK1UkTX.csv`.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个小型部署，我们使用一个现实生活中的数据集：PubMed。包含PubMed术语的示例数据集可以从以下链接下载：[https://nlp.stanford.edu/software/tmt/tmt-0.4/examples/pubmed-oa-subset.csv](https://nlp.stanford.edu/software/tmt/tmt-0.4/examples/pubmed-oa-subset.csv)。这个链接实际上包含一个CSV格式的数据集，但其文件名为奇怪的`4UK1UkTX.csv`。
- en: 'To be more specific, the dataset contains some abstracts of some biological
    articles, their publication year, and the serial number. A glimpse is given in
    the following figure:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，该数据集包含了一些生物学文章的摘要、它们的出版年份以及序列号。以下图像给出了一些示例：
- en: '![](img/a19af830-4cd6-496c-84df-93014ea2a1fc.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a19af830-4cd6-496c-84df-93014ea2a1fc.png)'
- en: 'Figure 6: A snapshot of the sample dataset'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：示例数据集的快照
- en: 'In the following  code, we have already saved the trained LDA model for future
    use as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们已经保存了训练好的LDA模型以备未来使用，如下所示：
- en: '[PRE29]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The trained model will be saved to the previously mentioned location. The directory
    will include data and metadata about the model and the training itself as shown
    in the following figure:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好的模型将被保存到之前提到的位置。该目录将包含模型和训练本身的数据以及元数据，如下图所示：
- en: '![](img/a4dd1974-8b96-405d-9fcb-37e24e57d6e9.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4dd1974-8b96-405d-9fcb-37e24e57d6e9.png)'
- en: 'Figure 7: The directory structure of the trained and saved LDA model'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：训练和保存的LDA模型的目录结构
- en: 'As expected, the data folder has some parquet files containing global topics,
    their counts, tokens and their counts, and the topics with their respective counts.
    Now the next task will be restoring the same model as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，数据文件夹中包含了一些parquet文件，这些文件包含全球主题、它们的计数、标记及其计数，以及主题与相应的计数。现在，接下来的任务是恢复相同的模型，如下所示：
- en: '[PRE30]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Well done! We have managed to reuse the model and do the same prediction. But,
    probably due to the randomness of data, we observed a slightly different prediction.
    Let''s see the complete code to get a clearer view:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好！我们成功地重新使用了模型并进行了相同的预测。但由于数据的随机性，我们观察到略有不同的预测结果。让我们看看完整的代码以便更清晰地了解：
- en: '[PRE32]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Summary
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have seen how effectively we can use and combine the LDA
    algorithm and NLP libraries, such as Stanford NLP, for finding useful patterns
    from large-scale text. We have seen a comparative analysis between TM algorithms
    and the scalability power of LDA.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经看到如何高效地使用和结合LDA算法与自然语言处理库，如斯坦福NLP，从大规模文本中发现有用的模式。我们还进行了TM算法与LDA可扩展性之间的对比分析。
- en: Finally, for a real-life example and use case, interested readers can refer
    to the blog article at [https://blog.codecentric.de/en/2017/01/topic-modeling-codecentric-blog-articles/](https://blog.codecentric.de/en/2017/01/topic-modeling-codecentric-blog-articles/).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于一个现实的示例和用例，有兴趣的读者可以参考以下博客文章：[https://blog.codecentric.de/en/2017/01/topic-modeling-codecentric-blog-articles/](https://blog.codecentric.de/en/2017/01/topic-modeling-codecentric-blog-articles/)。
- en: '**Netflix** is an American entertainment company founded by Reed Hastings and
    Marc Randolph on August 29, 1997, in Scotts Valley, California. It specializes
    in, providing, streaming media and video-on-demand, online and DVD by mail. In
    2013, Netflix expanded into film and television production, as well as online
    distribution. Netflix uses a model-based collaborative filtering approach for
    real-time movie recommendations for its subscribers.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**Netflix** 是一家美国娱乐公司，由Reed Hastings和Marc Randolph于1997年8月29日在加利福尼亚州斯科茨谷成立。它专门提供流媒体媒体和按需视频服务，通过在线和DVD邮寄的方式提供。在2013年，Netflix扩展到电影和电视制作，以及在线分发。Netflix为其订阅者使用基于模型的协同过滤方法进行实时电影推荐。'
- en: 'In the next chapter, we will see two end-to-end projects: an item-based **collaborative
    filtering** for movie-similarity measurements, and a model-based movie-recommendation
    engine with Spark to recommend movies to new users. We will see how to interoperate
    between **ALS** and **Matrix Factorization** for these two scalable movie recommendation
    engines.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将看到两个端到端的项目：一个基于项目的**协同过滤**电影相似度测量，以及一个基于模型的电影推荐引擎，利用Spark为新用户推荐电影。我们将看到如何在这两个可扩展的电影推荐引擎中实现**ALS**与**矩阵分解**的互操作。
