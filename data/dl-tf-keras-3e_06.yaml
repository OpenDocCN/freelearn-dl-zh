- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Transformers
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器
- en: 'The transformer-based architectures have become almost universal in **Natural
    Language Processing** (**NLP**) (and beyond) when it comes to solving a wide variety
    of tasks, such as:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变换器的架构在**自然语言处理**（**NLP**）（及其他领域）中几乎已成为解决各种任务的通用方法，例如：
- en: Neural machine translation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经机器翻译
- en: Text summarization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本摘要
- en: Text generation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本生成
- en: Named entity recognition
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: Question answering
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答系统
- en: Text classification
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类
- en: Text similarity
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本相似度
- en: Offensive message/profanity detection
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冒犯性信息/脏话检测
- en: Query understanding
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询理解
- en: Language modeling
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言建模
- en: Next-sentence prediction
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一句预测
- en: Reading comprehension
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读理解
- en: Sentiment analysis
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析
- en: Paraphrasing
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 意译
- en: and a lot more.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以及更多内容。
- en: In less than four years, when the *Attention Is All You Need* paper was published
    by Google Research in 2017, transformers managed to take the NLP community by
    storm, breaking any record achieved over the previous thirty years.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在不到四年的时间里，谷歌研究团队于2017年发布的*Attention Is All You Need*论文，成功让变换器在自然语言处理（NLP）社区引起轰动，打破了过去三十年间任何记录。
- en: Transformer-based models use the so-called attention mechanisms that identify
    complex relationships between words in each input sequence, such as a sentence.
    Attention helped resolve the challenge of encoding “pairwise correlations”—something
    that its “predecessors,” such as LSTM RNNs and even CNNS, couldn’t achieve when
    modeling sequential data, such as text.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变换器的模型使用所谓的注意力机制，识别输入序列（例如句子）中单词之间的复杂关系。注意力机制帮助解决了编码“成对相关性”的挑战——这一点是其“前辈”，如LSTM
    RNN甚至CNN，在建模顺序数据（如文本）时无法实现的。
- en: 'Models—such as BERT, T5, and GPT (covered in more detail later in this chapter)—now
    constitute the state-of-the-art fundamental building blocks for new applications
    in almost every field, from computer vision to speech recognition, translation,
    or protein and coding sequences. Attention has also been applied in reinforcement
    learning for games: in DeepMind’s AlphaStar ([https://rdcu.be/bVI7G](https://rdcu.be/bVI7G)
    and [https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning](https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning)),
    observations of player and opponent StarCraft game units were processed with self-attention,
    for example. For this reason, Stanford has recently introduced the term “foundation
    models” to define a set of **Large Language Models** (**LLMs**) based on giant
    pretrained transformers.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 模型——例如BERT、T5和GPT（本章稍后会更详细介绍）——如今已经构成了几乎每个领域的新应用的最前沿基础构件，从计算机视觉到语音识别、翻译或蛋白质和编码序列。注意力也已被应用于强化学习中的游戏：例如，DeepMind的AlphaStar（[https://rdcu.be/bVI7G](https://rdcu.be/bVI7G)
    和 [https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning](https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning)）中，玩家和对手的《星际争霸》游戏单位的观察被自注意力处理。例如。因此，斯坦福大学最近提出了“基础模型”这一术语，用来定义一组基于巨型预训练变换器的**大语言模型**（**LLMs**）。
- en: This progress has been made thanks to a few simple ideas, which we are going
    to review in the next few sections.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些进展得益于一些简单的想法，我们将在接下来的几节中进行回顾。
- en: 'You will learn:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你将会学习：
- en: What transformers are
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是变换器
- en: How they evolved over time
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是如何随时间演变的
- en: Some optimization techniques
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些优化技术
- en: Dos and don’ts
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意事项
- en: What the future will look like
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未来会是什么样子
- en: Let’s start turning our attention to transformers. You will be surprised to
    discover that attention indeed is all you need!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始关注变换器。你会惊讶地发现，注意力其实就是你所需要的全部！
- en: Architecture
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: Even though a typical transformer architecture is usually different from that
    of recurrent networks, it is based on several key ideas that originated in RNNs.
    At the time of writing this book, the transformer represents the next evolutionary
    step of deep learning architectures related to texts and any data that can be
    represented as sequences, and as such, it should be an essential part of your
    toolbox.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管典型的变换器架构通常不同于循环网络的架构，但它基于几个源自RNN的关键思想。在撰写本书时，变换器代表了与文本以及任何可以表示为序列的数据相关的深度学习架构的下一个进化步骤，因此，它应该成为你工具箱中的必备部分。
- en: The original transformer architecture is a variant of the encoder-decoder architecture,
    where the recurrent layers are replaced with (self-)attention layers. The transformer
    was initially proposed by Google in the seminal paper titled *Attention Is All
    You Need* by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
    Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin, 2017, [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762),
    to which a reference implementation was provided, which we will refer to throughout
    this discussion.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 原始Transformer架构是编码器-解码器架构的一种变体，其中循环层被（自）注意力层替代。Transformer最初由Google在2017年发布的开创性论文《*Attention
    Is All You Need*》中提出，论文作者包括Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion
    Jones、Aidan N. Gomez、Lukasz Kaiser 和 Illia Polosukhin，[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)，并提供了一个参考实现，我们将在本讨论中使用该实现。
- en: The architecture is an instance of the encoder-decoder models that have been
    popular since 2014-2015 (such as *Sequence to Sequence Learning with Neural Networks*
    by Sutskever et al. (2014), [https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215)).
    Prior to that, attention had been used together with **Long-Short-Term Memory**
    (**LSTM**) and other **RNN** (**Recurrent Neural Network**) models discussed in
    a previous chapter. Attention was introduced in 2014 in *Neural Machine Translation
    by Jointly Learning to Align and Translate* by Bahdanau et al., [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473),
    and applied to neural machine translation in 2015 in *Effective Approaches to
    Attention-based Neural Machine Translation* by Luong et al., [https://arxiv.org/abs/1508.04025](https://arxiv.org/abs/1508.04025),
    and there have been other combinations of attention with other types of models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构是自2014-2015年以来广泛使用的编码器-解码器模型的一个实例（例如 Sutskever 等人（2014）的《*序列到序列学习与神经网络*》，[https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215)）。在此之前，注意力机制已经与**长短期记忆网络**（**LSTM**）以及其他**RNN**（**循环神经网络**）模型结合使用，相关内容在前一章中有讨论。注意力机制首次出现在2014年，由Bahdanau等人提出的论文《*神经机器翻译：联合学习对齐与翻译*》，[https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)，并在2015年应用于神经机器翻译，见Luong等人所写的《*基于注意力的神经机器翻译的有效方法*》，[https://arxiv.org/abs/1508.04025](https://arxiv.org/abs/1508.04025)，同时也有其他注意力机制与不同类型模型的结合。
- en: In 2017, the first transformer demonstrated that you could remove LSTMs from
    **Neural Machine Translation** (**NMT**) models and use the so-called (self-)attention
    blocks (hence the paper title *Attention Is All You Need*).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，首个Transformer模型证明了可以将LSTM从**神经机器翻译**（**NMT**）模型中移除，转而使用所谓的（自）注意力模块（因此论文的标题是《*Attention
    Is All You Need*》）。
- en: Key intuitions
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键直觉
- en: 'Let’s start by defining some concepts that will be useful later on in this
    chapter. The innovation introduced with the transformer in 2017 is based on four
    main key ideas:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义一些将在本章后面有用的概念开始。2017年Transformer带来的创新主要基于四个关键思想：
- en: Positional encoding
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置编码
- en: Attention
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力
- en: Self-attention
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力
- en: Multi-head (self-)attention
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多头（自）注意力
- en: In the next sections, we will discuss them in greater detail.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将更详细地讨论这些内容。
- en: Positional encoding
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 位置编码
- en: RNNs keep the word order by processing words sequentially. The advantage of
    this approach is simplicity, but one of the disadvantages is that this makes parallelization
    hard (training on multiple hardware accelerators). If we want to effectively leverage
    highly parallel architectures, such as GPUs and TPUs, we’d need an alternative
    way to represent ordering.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 通过顺序处理单词来保持词序。这种方法的优点是简单，但其中一个缺点是这使得并行化变得困难（在多个硬件加速器上进行训练）。如果我们想有效地利用高度并行的架构，如
    GPUs 和 TPUs，就需要一种替代的方式来表示顺序。
- en: 'The transformer uses a simple alternative order representation called positional
    encoding, which associates each word with a number representing its position in
    the text. For instance:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer使用一种简单的替代顺序表示方法，称为位置编码，将每个单词与表示其在文本中位置的数字关联。例如：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The key intuition is that enriching transformers with a position allows the
    model to learn the importance of the position of each token (a word in the text/sentence).
    Note that positional encoding existed before transformers (as discussed in the
    chapter on RNNs), but this intuition is particularly important in the context
    of creating transformer-based models. After (absolute) positional encoding was
    introduced in the original transformer paper, there have been other variants,
    such as relative positional encoding (*Self-Attention with Relative Position Representations*
    by Shaw et al., 2018, [https://arxiv.org/abs/1803.02155](https://arxiv.org/abs/1803.02155),
    and rotary positional encoding (*RoFormer: Enhanced Transformer with Rotary Position
    Embedding* by Su et al., 2021, [https://arxiv.org/abs/2104.09864](https://arxiv.org/abs/2104.09864)).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '关键的直觉是，通过为变压器添加位置信息，模型能够学习每个标记（文本/句子中的一个单词）的位置重要性。注意，位置编码在变压器之前就存在（正如在RNN章节中所讨论的那样），但这个直觉在创建基于变压器的模型时尤其重要。在原始变压器论文中引入（绝对）位置编码之后，还出现了其他变体，如相对位置编码（*Self-Attention
    with Relative Position Representations*，Shaw等人，2018年，[https://arxiv.org/abs/1803.02155](https://arxiv.org/abs/1803.02155)），以及旋转位置编码（*RoFormer:
    Enhanced Transformer with Rotary Position Embedding*，Su等人，2021年，[https://arxiv.org/abs/2104.09864](https://arxiv.org/abs/2104.09864)）。'
- en: Now that we have defined positional encoding, let’s turn our attention to the
    *attention* mechanism.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了位置编码，让我们把注意力转向*注意力*机制。
- en: Attention
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意力
- en: Another crucial ingredient of the transformer recipe is attention. This mechanism
    was first introduced in the context of machine translation in 2014 by Bahdanou
    et al. in *Neural Machine Translation by Jointly Learning to Align and Translate*
    by Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio, [https://arxiv.org/pdf/1409.0473.pdf](https://arxiv.org/pdf/1409.0473.pdf).
    Some research papers also attribute the idea behind attention to Alex Graves’
    *Generating Sequences with Recurrent Neural Networks*, which dates back to 2013,
    [https://arxiv.org/pdf/1308.0850.pdf](https://arxiv.org/pdf/1308.0850.pdf).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器配方的另一个关键成分是注意力。这个机制最早是在2014年，由Bahdanou等人首次在机器翻译中引入，见于*Neural Machine Translation
    by Jointly Learning to Align and Translate*（Dzmitry Bahdanau, KyungHyun Cho, Yoshua
    Bengio），[https://arxiv.org/pdf/1409.0473.pdf](https://arxiv.org/pdf/1409.0473.pdf)。一些研究论文还将注意力机制的思想归功于Alex
    Graves的*Generating Sequences with Recurrent Neural Networks*，这篇论文可追溯到2013年，[https://arxiv.org/pdf/1308.0850.pdf](https://arxiv.org/pdf/1308.0850.pdf)。
- en: 'This ingredient—this key idea—has since become a part of the title of the first
    transformer paper, *Attention is All You Need*. To get a high-level overview,
    let’s consider this example from the paper that introduced attention:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个成分——这个关键思想——后来成为了第一篇变压器论文标题的一部分，*Attention is All You Need*。为了获得一个高层次的概述，我们来看看这篇介绍注意力机制的论文中的一个例子：
- en: '*The agreement on the European Economic Area was signed in August 1992.*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*欧洲经济区协议于1992年8月签署。*'
- en: 'In French, this can be translated as:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在法语中，这可以翻译为：
- en: '*L’accord sur la zone économique européenne a été signé en août 1992.*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*L’accord sur la zone économique européenne a été signé en août 1992.*'
- en: 'The initial attempts to perform automatic machine translation back in the early
    80s were based on the sequential translation of each word. This approach was very
    limiting because the text structure can change from a source language to a target
    language in many ways. For instance, some words in the French translation can
    have a different order: in English, adjectives usually precede nouns, like in
    “European Economic Area,” whereas in French, adjectives can go after nouns—”la
    zone économique européenne.” Moreover, unlike in English, the French language
    has gendered words. So, for example, the adjectives “économique” and “européenne”
    must be in their feminine form as they belong to the feminine noun “la zone.”'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 早在80年代初期，最初的自动机器翻译尝试是基于逐字翻译的方法。这种方法非常有限，因为文本结构从源语言到目标语言可能有很多不同的变化。例如，在法语翻译中，一些词的顺序可能会发生变化：在英语中，形容词通常位于名词之前，比如“European
    Economic Area”，而在法语中，形容词可以位于名词之后——“la zone économique européenne”。此外，与英语不同，法语中有性别区分的词汇。例如，形容词“économique”和“européenne”必须使用阴性形式，因为它们属于阴性名词“la
    zone”。
- en: The key intuition behind the attention approach is to build a text model that
    “looks at” every single word in the source sentence when translating words into
    the output language. In the original 2017 transformer paper, the authors point
    out that the cost of doing this is quadratic, but the gain achieved in terms of
    more accurate translation is considerable. More recent works reduced this initial
    quadratic complexity, such as the **Fast Attention Via positive Orthogonal Random**
    (**FAVOR**+) features from the *Rethinking Attention with Performers* paper by
    Choromanski et al. (2020) from Google, DeepMind, the University of Cambridge,
    and the Alan Turing Institute.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力方法背后的关键直觉是构建一个文本模型，在将源句子中的单词翻译成目标语言时，“查看”源句子中的每个单词。在2017年原始的变换器论文中，作者指出，这种做法的计算成本是二次方的，但在翻译准确性上所获得的收益是相当可观的。最近的研究降低了这种初始的二次复杂度，例如Choromanski等人（2020）在《*Rethinking
    Attention with Performers*》论文中提出的**Fast Attention Via Positive Orthogonal Random**（**FAVOR**+）特性，该论文由谷歌、DeepMind、剑桥大学和艾伦·图灵研究所联合发布。
- en: 'Let’s go over a nice example from that original attention paper by Bahdanou
    et al. (2014):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下Bahdanou等人（2014）原始注意力论文中的一个很好的例子：
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B18331_06_01.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing graphical user interface  Description automatically
    generated](img/B18331_06_01.png)'
- en: 'Figure 6.1: An example of attention for the English sentence “The agreement
    on the European Economic Area was signed in August 1992.” The plot visualizes
    “annotation weights”—the weights associated with the annotations. Source: “Neural
    Machine Translation by Jointly Learning to Align and Translate” by Bahdanau et
    al. (2014) (https://arxiv.org/abs/1409.0473)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：一个关于英语句子“The agreement on the European Economic Area was signed in August
    1992.”的注意力示例。该图可视化了“注释权重”——与注释相关的权重。来源：《Neural Machine Translation by Jointly
    Learning to Align and Translate》Bahdanau等人（2014）（https://arxiv.org/abs/1409.0473）
- en: Using the attention mechanism, the neural network can learn a heatmap of each
    source English word in relation to each target French word. Note that relationships
    are not only on the diagonal but might spread across the whole matrix. For instance,
    when the model outputs the French word “européenne,” it will pay a lot of attention
    to the input words “European” and “Economic.” (In *Figure 6.1*, this corresponds
    to the diagonal and the adjacent cell.) The 2014 attention paper by Bahdanou et
    al. demonstrated that the model (which used an RNN encoder-decoder framework with
    attention) can *learn to align and attend* to the input elements without supervision,
    and, as *Figure 6.1* shows, translate the input English sentences into French.
    And, of course, the larger the training set is, the greater the number of correlations
    that the attention-based model can learn.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用注意力机制，神经网络可以学习每个源英语单词与每个目标法语单词之间的热力图。请注意，关系不仅仅存在于对角线位置，还可能遍布整个矩阵。例如，当模型输出法语单词“européenne”时，它会特别关注输入单词“European”和“Economic”。（在*图6.1*中，这对应于对角线及其相邻单元格。）Bahdanou等人于2014年发表的注意力论文表明，模型（使用带有注意力的RNN编码器-解码器框架）能够*学习对齐并关注*输入元素而无需监督，并且正如*图6.1*所示，将输入的英语句子翻译成法语。当然，训练集越大，基于注意力的模型可以学习到的相关性就越多。
- en: In short, the attention mechanism can access all previous words and weigh them
    according to a *learned* measure of relevancy. This way, attention can provide
    relevant information about tokens located far away in the target sentence.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，注意力机制可以访问所有之前的单词，并根据*学习到的*相关性度量对其进行加权。这样，注意力机制就能提供关于目标句子中远离的词元的相关信息。
- en: Now, we can focus on another key ingredient of the transformer—”self-attention.”
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以关注变换器的另一个关键部分——“自注意力”。
- en: Self-attention
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自注意力
- en: The third key idea popularized by the original transformer paper is the use
    of attention within the same sentence in the source language—self-attention. With
    this mechanism, neural networks can be trained to learn the relationships among
    all words (or other elements) in each input sequence (such as a sentence) irrespective
    of their positions before focusing on (machine) translation. Self-attention can
    be attributed to the idea from the 2016 paper called *Long Short-Term Memory-Networks
    for Machine Reading* by Cheng et al., [https://arxiv.org/pdf/1601.06733.pdf](https://arxiv.org/pdf/1601.06733.pdf).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 Transformer 论文中推广的第三个关键思想是使用自注意力机制来处理源语言同一句子中的词汇关系——自注意力。通过这种机制，神经网络可以被训练来学习每个输入序列（例如一句话）中所有单词（或其他元素）之间的关系，而不管它们的位置，随后再集中处理（机器）翻译。自注意力的概念可以追溯到2016年
    Cheng 等人发表的论文 *Long Short-Term Memory-Networks for Machine Reading*，[https://arxiv.org/pdf/1601.06733.pdf](https://arxiv.org/pdf/1601.06733.pdf)。
- en: 'Let’s go through an example with the following two sentences:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下两个句子来举个例子：
- en: “Server, can I have the check?”
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: “Server, can I have the check?”
- en: “Looks like I just crashed the server.”
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: “看起来我刚刚把服务器搞崩了。”
- en: Clearly, the word “server” has a very different meaning in either sentence and
    self-attention can understand each word considering the context of the surrounding
    words. Just to reiterate, the attention mechanism can access all previous words
    and weigh them according to a learned measure of relevancy. Self-attention provides
    relevant information about tokens located far away in the source sentence.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，单词“server”在两种句子中的含义完全不同，而自注意力机制能够理解每个单词，并根据周围单词的上下文来判断它们的含义。再次强调，注意力机制可以访问所有之前的单词，并根据学习到的相关性度量为它们赋予权重。自注意力为位于源句子中较远位置的词汇提供了相关信息。
- en: Multi-head (self-)attention
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多头（自）注意力
- en: The original transformer performs a (self-)attention function multiple times.
    A single set of the so-called weight matrices (which are covered in detail in
    the *How to compute Attention* section) is named an attention head. When you have
    several sets of these matrices, you have multiple attention heads. The multi-head
    (self-)attention layer usually has several parallel (self-)attention layers. Note
    that the introduction of multiple heads allows us to have many definitions of
    which word is “relevant” to each other. Plus, all these definitions of relevance
    can be computed in parallel by modern hardware accelerators, thus speeding up
    the computation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 Transformer 会多次执行（自）注意力功能。一组被称为权重矩阵的集合（在 *如何计算注意力* 部分详细介绍）称为一个注意力头。当你拥有多组这些矩阵时，就有了多个注意力头。多头（自）注意力层通常有多个并行的（自）注意力层。注意，引入多个头的做法使我们能够定义哪些单词彼此之间是“相关”的。而且，所有这些相关性定义可以通过现代硬件加速器并行计算，从而加速计算过程。
- en: Now that we have gone through the high-level definitions of the key ingredients
    of the transformers, let’s deep dive into how to compute the attention mechanism.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经浏览了 Transformer 关键元素的高层定义，接下来我们将深入探讨如何计算注意力机制。
- en: How to compute attention
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何计算注意力
- en: In the original transformer, the self-attention function is computed by using
    the so-called scaled dot-product units. The authors of the 2017 paper even called
    their attention method *Scaled Dot-Product Attention*. You might remember from
    high school studies that the dot-product between two vectors provides a good sense
    of how “close” the vectors are.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的 Transformer 中，自注意力功能是通过使用所谓的缩放点积单元来计算的。2017年论文的作者甚至称他们的注意力方法为 *缩放点积注意力*。你可能还记得高中时学过的，两个向量之间的点积可以很好地判断这两个向量有多“接近”。
- en: Each input token sequence (for example, of a sentence) embedding that passes
    into the transformer (encoder and/or decoder) produces *attention weights* (covered
    in detail below) that are simultaneously calculated between every sequence element
    (such as a word). The output results in embeddings produced for every token containing
    the token itself together with every other relevant token weighted by its relative
    attention weight.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入的令牌序列（例如一句话）的嵌入经过 Transformer（编码器和/或解码器）后，会产生 *注意力权重*（下文会详细介绍），这些权重是同时计算的，计算过程涵盖了每个序列元素（如单词）之间的关系。最终的输出会为每个令牌生成嵌入，这些嵌入包含令牌本身以及与其相关的其他令牌，且每个令牌的权重由其相对的注意力权重决定。
- en: 'The attention layer transforms the input vectors into query, key, and value
    matrices, which are then split into attention heads (hence, multi-head attention):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力层将输入向量转换为查询、键和值矩阵，然后将其拆分为注意力头（因此是多头注意力）：
- en: The query word can be interpreted as the word *for which* we are calculating
    the attention function.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询词可以解释为我们正在计算注意力函数的“目标”词。
- en: The key and value words are the words *to which* we are paying attention.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键字和价值词是我们正在关注的词。
- en: The dot-product (explained further below) tells us the similarity between words.
    If the vectors for two words are more aligned, the *attention score* will be higher.
    The transformer will learn the weights in such a way that if two words in a sentence
    are relevant to each other, then their word vectors will be aligned.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 点积（下面将进一步解释）告诉我们词语之间的相似度。如果两个词的向量更对齐，*注意力得分*会更高。Transformer 将以这样的方式学习权重，即如果一个句子中的两个词相互相关，那么它们的词向量将会对齐。
- en: 'Each attention layer learns three weight matrices:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 每个注意力层学习三个权重矩阵：
- en: The query weights *W*[Q]
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询权重 *W*[Q]
- en: The key weights *W*[K]
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键权重 *W*[K]
- en: The value weights *W*[V]
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值权重 *W*[V]
- en: 'For each word *i*, an input word embedding *x*[i] is computed producing:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个单词 *i*，计算一个输入词嵌入 *x*[i]，得到：
- en: A query vector *q*[i] = *x*[i]*W*[Q]
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个查询向量 *q*[i] = *x*[i]*W*[Q]
- en: A key vector *k*[i] = *x*[i]*W*[K]
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个关键向量 *k*[i] = *x*[i]*W*[K]
- en: A value vector *v*[i] = *x*[i]*W*[V]
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个值向量 *v*[i] = *x*[i]*W*[V]
- en: 'Given the query and the corresponding key vectors, the following dot-product
    formula produces the *attention weight* in the original transformer paper:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 给定查询和相应的关键向量，以下的点积公式生成了原始 transformer 论文中的 *注意力权重*：
- en: '![](img/B18331_06_001.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_06_001.png)'
- en: 'where:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*a*[i,j] is the attention from word *i* to a word *j*.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a*[i,j] 是单词 *i* 到单词 *j* 的注意力。'
- en: . is the dot-product of the query with keys, which will give a sense of how
    “close” the vectors are.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: . 是查询与关键字的点积，这将给我们一个感觉，表示向量之间有多“接近”。
- en: Note that the attention unit for word *i* is the weighted sum of the value vectors
    of all words, weighted by *a*[i,j], the attention from word *i* to a word *j*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，单词 *i* 的注意力单元是所有单词值向量的加权和，权重由 *a*[i,j] 表示，即单词 *i* 到单词 *j* 的注意力。
- en: Now, to stabilize gradients during the training, the attention weights are divided
    by the square root of the dimension of the key vectors ![](img/B18331_06_002.png).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了在训练过程中稳定梯度，注意力权重被除以关键向量维度的平方根 ![](img/B18331_06_002.png)。
- en: Then, the results are passed through a softmax function to normalize the weight.
    Note that the attention function from a word *i* to a word *j* is not the same
    as the attention from word *j* to a word *i*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，结果通过 softmax 函数进行归一化处理。注意，单词 *i* 到单词 *j* 的注意力函数与单词 *j* 到单词 *i* 的注意力函数不同。
- en: Note that since modern deep learning accelerators work well with matrices, we
    can compute attention for all words using large matrices.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于现代深度学习加速器与矩阵的兼容性较好，我们可以使用大矩阵为所有词计算注意力。
- en: 'Define *q*[i], *k*[i], *v*[i] (where *i* is the *i*th row) as matrices *Q*,
    *K*, *V*, respectively. Then, we can summarize the attention function as an attention
    matrix:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 *q*[i]、*k*[i]、*v*[i]（其中 *i* 是第 *i* 行）为矩阵 *Q*、*K*、*V*，分别对应。然后，我们可以将注意力函数总结为一个注意力矩阵：
- en: '![](img/B18331_06_003.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_06_003.png)'
- en: In this section, we discussed how to compute the attention function introduced
    in the original transformer paper. Next, let’s discuss the encoder-decoder architecture.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了如何计算原始 transformer 论文中介绍的注意力函数。接下来，我们将讨论编码器-解码器架构。
- en: Encoder-decoder architecture
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器-解码器架构
- en: 'Similar to the seq2seq models, (*Sequence to Sequence Learning with Neural
    Networks* by Ilya Sutskever, Oriol Vinyals, Quoc V. Le (2014)) described in *Chapter
    5*, *Recurrent Neural Networks*, the original transformer model also used an encoder-decoder
    architecture:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 seq2seq 模型，(*Sequence to Sequence Learning with Neural Networks* 由 Ilya
    Sutskever、Oriol Vinyals、Quoc V. Le（2014）所描述) 中的 *第5章*，*循环神经网络*，原始的 transformer
    模型也使用了编码器-解码器架构：
- en: The encoder takes the input (source) sequence of embeddings and transforms it
    into a new fixed-length vector of input embeddings.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器接受输入（源）嵌入序列，并将其转换为一个新的固定长度的输入嵌入向量。
- en: The decoder takes the output embeddings vector from the encoder and transforms
    it into a sequence of output embeddings.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器获取来自编码器的输出嵌入向量，并将其转化为一系列输出嵌入。
- en: Both the encoder and the decoder consist of several stacked layers. Each encoder
    and decoder layer is using the attention mechanism described earlier.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器和解码器都由多个堆叠的层组成。每个编码器和解码器层都使用前面描述的注意力机制。
- en: We’ll learn about the transformer architecture in much more detail later in
    this section.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节的后面更加详细地了解 transformer 架构。
- en: Since the introduction of the transformer architecture, other newer networks
    have used only the encoder or the decoder components (or both), which are discussed
    in the *Categories of transformers* section of this chapter.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 自从引入transformer架构以来，其他一些更新的网络只使用了编码器或解码器组件（或两者），这些内容将在本章的*transformer分类*部分中讨论。
- en: Next, let’s briefly go over the other components of the original transformer—the
    residual and normalization layers.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们简要回顾一下原始transformer的其他组件——残差和归一化层。
- en: Residual and normalization layers
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 残差连接和归一化层
- en: Typically, transformer-based networks reuse other existing state-of-the-art
    machine learning methodologies, such as attention mechanisms. You shall therefore
    not be surprised if both encoder and decoder layers combine neural networks with
    residual connections (*Deep Residual Learning for Image Recognition* by He et
    al., 2016, [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385))
    and normalization steps (*Layer Normalization* by Ba et al., 2016, [https:/arxiv.org/abs/1607.06450](https://arxiv.org/abs/1607.06450)).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，基于transformer的网络会重用其他现有的最先进的机器学习方法，例如注意力机制。因此，你不必惊讶于编码器和解码器层将神经网络与残差连接（*He等人提出的深度残差学习用于图像识别*，2016年，[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)）和归一化步骤（*Ba等人提出的层归一化*，2016年，[https://arxiv.org/abs/1607.06450](https://arxiv.org/abs/1607.06450)）相结合。
- en: OK, we now have all the key ingredients to deep dive into transformers.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们已经具备了深入研究transformer的所有关键要素。
- en: An overview of the transformer architecture
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer架构概述
- en: Now that we have covered some of the key concepts behind the original transformer,
    let’s deep dive into the architecture introduced in the seminal 2017 paper. Note
    that transformer-based models are usually built by leveraging various attention
    mechanisms without using RNNs. This is also a consequence of the fact that attention
    mechanisms themselves can match and outperform RNN (encoder-decoder) models with
    attention. That’s why the seminal paper was titled *Attention is all You Need*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了原始transformer的一些关键概念，让我们深入探讨2017年开创性论文中介绍的架构。请注意，基于transformer的模型通常通过利用各种注意力机制而不使用RNN来构建。这也是由于注意力机制本身能够与RNN（编码器-解码器）模型的注意力机制相匹配并超越它们。因此，这篇开创性论文的标题是*Attention
    is all You Need*。
- en: '*Figure 6.2* shows a seq2seq network with RNNs and attention, and compares
    it to the original transformer network.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.2*展示了一个带有RNN和注意力的seq2seq网络，并与原始的transformer网络进行了比较。'
- en: 'The transformer is similar to seq2seq with an attention model in the following
    ways:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer与seq2seq加注意力模型在以下方面相似：
- en: Both approaches work with source (inputs) and target (output) *sequences.*
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两种方法都处理源（输入）和目标（输出）*序列*。
- en: Both use an encoder-decoder architecture, as mentioned before.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前所述，两者都使用了编码器-解码器架构。
- en: The output of the last block of the encoder is used as a context—or thought
    vector—for computing the attention function in the decoder.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器最后一个块的输出作为上下文或思维向量，用于计算解码器中的注意力函数。
- en: 'The target (output) sequence embeddings are fed into dense (fully connected)
    blocks, which convert the output embeddings to the final sequence of an integer
    form:'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标（输出）序列的嵌入被输入到密集（全连接）块中，这些块将输出嵌入转换为最终的整数形式序列：
- en: '![Diagram  Description automatically generated](img/B18331_06_02.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图示 说明自动生成](img/B18331_06_02.png)'
- en: 'Figure 6.2: Flow of data in (a) seq2seq + Attention, and (b) Transformer architecture.
    Image Source: Zhang, et al.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：数据流在（a）seq2seq + Attention和（b）Transformer架构中的流动。图像来源：Zhang等人。
- en: 'And the two architectures differ in the following ways:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种架构在以下方面有所不同：
- en: The seq2seq network uses the recurrent and attention layers in the encoder,
    and the recurrent layer in the decoder.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: seq2seq网络在编码器中使用循环层和注意力层，在解码器中使用循环层。
- en: 'The transformer replaced those layers with so-called transformer blocks (a
    stack of N identical layers), as *Figure 6.2* demonstrates:'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Transformer用所谓的transformer块（N个相同层的堆叠）替代了这些层，正如*图6.2*所示：
- en: 'In the encoder, the transformer block consists of a sequence of sub-layers:
    a multi-head (self-)attention layer and a position-wise feedforward layer. Each
    of those two layers has a residual connection, followed by a normalization layer.'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在编码器中，transformer块由一系列子层组成：一个多头（自）注意力层和一个位置逐一的前馈层。每一层都有一个残差连接，之后是一个归一化层。
- en: In the decoder, the transformer block contains a variant of a multi-head (self-)attention
    layer with *masking—*a masked multi-head self-attention—and a feedforward layer
    like in the encoder (with identical residual connections and normalization layers).
    Masking helps prevent positions from attending into the future. Additionally,
    the decoder contains a second multi-head (self-)attention layer which computes
    attention over the outputs of the encoder’s transformer blockmasking is covered
    in more detail later in this section.)
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在解码器中，Transformer 块包含一个变种的多头（自）注意力层，带有 *掩码—* 掩码多头自注意力层，以及一个与编码器中类似的前馈层（具有相同的残差连接和归一化层）。掩码有助于防止位置关注未来。此外，解码器包含第二个多头（自）注意力层，用于计算对编码器
    Transformer 块输出的关注（掩码将在本节稍后详细介绍）。
- en: In the seq2seq with attention network, the encoder state is passed to the first
    recurrent time step as with the seq2seq with attention network.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在带有注意力机制的 seq2seq 网络中，编码器状态传递到第一个递归时间步，就像在带有注意力机制的 seq2seq 网络中一样。
- en: In the transformer, the encoder state is passed to every transformer block in
    the decoder. This allows the transformer network to work in parallel across time
    steps since there is no longer a temporal dependency as with the seq2seq networks.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 Transformer 中，编码器状态会传递到解码器中的每个 Transformer 块。这允许 Transformer 网络在时间步之间并行工作，因为不像
    seq2seq 网络那样存在时间依赖。
- en: The last decoder is followed by a final linear transformation (a dense layer)
    with a softmax function to produce the output (next-token) probabilities.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后的解码器后跟一个最终的线性变换（一个全连接层），并使用 softmax 函数生成输出（下一个标记）的概率。
- en: Because of the parallelism referred to in the previous point, an encoding layer
    is added to provide positional information to distinguish the position of each
    element in the transformer network sequence (positional encoding layer). This
    way, the first encoder takes the positional information and embeddings of the
    input sequence as inputs, rather than only encodings, thus allowing taking positional
    information into account.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于前述提到的并行性，添加了一个编码层来提供位置信息，以区分 Transformer 网络序列中每个元素的位置（位置编码层）。这样，第一个编码器将输入序列的位置信息和嵌入作为输入，而不仅仅是编码，从而考虑到位置信息。
- en: 'Let’s walk through the process of data flowing through the transformer network.
    Later in this chapter, we will use TensorFlow with the Keras API to create and
    train a transformer model from scratch:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步地了解数据如何通过 Transformer 网络流动。在本章后续部分，我们将使用 TensorFlow 和 Keras API 从头开始创建并训练一个
    Transformer 模型：
- en: As part of data preprocessing, the inputs and the outputs are tokenized and
    converted to embeddings.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为数据预处理的一部分，输入和输出会被分词并转换为嵌入。
- en: 'Next, positional encoding is applied to the input and output embeddings to
    have information about the relative position of tokens in the sequences. In the
    encoder section:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，位置编码被应用于输入和输出嵌入，以获得序列中标记的相对位置信息。在编码器部分：
- en: As per *Figure 6.2*, the encoder side consists of an embedding and a positional
    encoding layer, followed by six identical transformer blocks (there were six “layers”
    in the original transformer). As we learned earlier, each transformer block in
    the encoder consists of a multi-head (self-)attention layer and a position-wise
    feedforward layer.
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据 *图 6.2*，编码器部分由一个嵌入层和一个位置编码层组成，后面跟着六个相同的 Transformer 块（原始 Transformer 中有六个“层”）。正如我们之前所学，每个编码器中的
    Transformer 块由一个多头（自）注意力层和一个位置逐元素前馈层组成。
- en: We have already briefly seen that self-attention is the process of attending
    to parts of the same sequence. When we process a sentence, we might want to know
    what other words are most aligned with the current one.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们已经简要看到自注意力是关注同一序列部分的过程。当我们处理一个句子时，我们可能希望知道与当前单词最相关的其他单词是什么。
- en: 'The multi-head attention layer consists of multiple (eight in the reference
    implementation contained in the seminal paper) parallel self-attention layers.
    Self-attention is carried out by constructing three vectors *Q* (query), *K* (key),
    and *V* (value), out of the input embedding. These vectors are created by multiplying
    the input embedding with three trainable weight matrices *W*[Q], *W*[K], and *W*[V].
    The output vector *Z* is created by combining *K*, *Q*, and *V* at each self-attention
    layer using the following formula. Here, *d*[K] refers to the dimension of the
    *K*, *Q*, and *V* vectors (64 in the reference implementation contained in the
    seminal paper):'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多头自注意力层由多个（在开创性论文中参考实现为 8 个）并行的自注意力层组成。自注意力通过构建三个向量 *Q*（查询），*K*（键），和 *V*（值）来实现，这些向量是从输入嵌入中构造的。通过将输入嵌入与三个可训练的权重矩阵
    *W*[Q]，*W*[K]，和 *W*[V] 相乘来创建这些向量。输出向量 *Z* 是通过结合每个自注意力层中的 *K*、*Q* 和 *V*，并使用以下公式生成的。这里，*d*[K]
    是 *K*、*Q* 和 *V* 向量的维度（在参考实现中为 64）：
- en: '![](img/B18331_06_004.png)'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B18331_06_004.png)'
- en: The multi-head attention layer will create multiple values for *Z* (based on
    multiple trainable weight matrices *W*[Q], *W*[K], and *W*[V] at each self-attention
    layer), and then concatenate them for inputs into the position-wise feedforward
    layer.
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多头自注意力层会为 *Z* 创建多个值（基于每个自注意力层中多个可训练的权重矩阵 *W*[Q]，*W*[K] 和 *W*[V]），然后将它们连接起来，作为位置-wise
    前馈层的输入。
- en: The inputs to the position-wise feedforward layer consist of embeddings for
    the different elements in the sequence (or words in the sentence), attended via
    self-attention in the multi-head attention layer. Each token is represented internally
    by a fixed-length embedding vector (512 in the reference implementation introduced
    in the seminal paper). Each vector is run through the feedforward layer in parallel.
    The outputs of the FFN are the inputs to (or fed into) the multi-head attention
    layer in the following transformer block. In the last transformer block of the
    encoder, the outputs are the context vector that is passed to the decoder.
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置-wise 前馈层的输入由序列中不同元素（或句子中的单词）的嵌入表示组成，这些嵌入通过多头自注意力层的自注意力机制进行处理。每个标记在内部由一个固定长度的嵌入向量表示（在开创性论文中，参考实现中为
    512）。每个向量都会并行地通过前馈层。FFN 的输出是下一个 transformer 块中多头自注意力层的输入（或被送入该层）。在编码器的最后一个 transformer
    块中，输出是传递给解码器的上下文向量。
- en: Both the multi-head attention and position-wise FFN layers send out not only
    the signal from the previous layer but also a residual signal from their inputs
    to their outputs. The outputs and residual inputs are passed through a layer-normalization
    step, and this is shown in *Figure 6.2* as the “Add & Norm” layer.
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多头自注意力层和位置-wise FFN 层不仅传递来自前一层的信号，还会将来自输入的残差信号传递到它们的输出中。输出和残差输入会经过一个层归一化步骤，这在*图
    6.2*中作为“Add & Norm”层展示。
- en: Since the entire sequence is consumed in parallel on the encoder, the information
    about the positions of individual elements gets lost. To compensate for this,
    the input embeddings are augmented with a positional embedding, which is implemented
    as a sinusoidal function without learned parameters. The positional embedding
    is added to the input embedding.
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于整个序列在编码器中是并行处理的，因此单个元素的位置信息会丢失。为了弥补这一点，输入的嵌入向量被增强了一个位置嵌入，位置嵌入通过一个不带学习参数的正弦函数实现。位置嵌入被加到输入嵌入中。
- en: 'Next, let’s walk through how the data flows through the decoder:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们来看看数据是如何在解码器中流动的：
- en: The output of the encoder results in a pair of attention vectors *K* and *V*,
    which are sent in parallel to all the transformer blocks in the decoder. The transformer
    block in the decoder is similar to that in the encoder, except that it has an
    additional multi-head attention layer to handle the attention vectors from the
    encoder. This additional multi-head attention layer works similarly to the one
    in the encoder and the one below it, except that it combines the *Q* vector from
    the layer below it and the *K* and *Q* vectors from the encoder state.
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器的输出结果是一个由 *K* 和 *V* 组成的注意力向量对，它们会并行地传送到解码器中的所有 transformer 块。解码器中的 transformer
    块与编码器中的类似，唯一的不同是它增加了一个额外的多头自注意力层，用于处理来自编码器的注意力向量。这个额外的多头自注意力层的工作方式类似于编码器中的那个以及下面的那个，只不过它会将来自下层的
    *Q* 向量与来自编码器状态的 *K* 和 *Q* 向量结合。
- en: Similar to the seq2seq network, the output sequence generates one token at a
    time, using the input from the previous time step. As for the input to the encoder,
    the input to the decoder is also augmented with a positional embedding. Unlike
    the encoder, the self-attention process in the decoder is only allowed to attend
    to tokens at previous time points. This is done by masking out tokens at future
    time points.
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似于seq2seq网络，输出序列一次生成一个token，使用来自前一个时间步的输入。与编码器的输入类似，解码器的输入也会通过位置嵌入进行增强。与编码器不同，解码器中的自注意力过程仅允许关注之前时间点的token。这是通过屏蔽未来时间点的token来实现的。
- en: The output of the last transformer block in the decoder is a sequence of low-dimensional
    embeddings (512 for reference implementation in the seminal paper as noted earlier).
    This is passed to the dense layer, which converts it into a sequence of probability
    distributions across the target vocabulary, from which we generate the most probable
    word either greedily or by a more sophisticated technique such as beam search.
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器中最后一个变换器块的输出是一个低维嵌入序列（参考文献中提到的原始论文中为512）。该序列传递给全连接层，将其转换为一个针对目标词汇表的概率分布序列，从中我们可以通过贪婪算法或更复杂的技术（如束搜索）生成最可能的词汇。
- en: '*Figure 6.3* shows the transformer architecture covering everything that’s
    just been described:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.3* 展示了涵盖了上述所有内容的变换器架构：'
- en: '![Attention_diagram_transformer](img/B18331_06_03.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![Attention_diagram_transformer](img/B18331_06_03.png)'
- en: 'Figure 6.3: The transformer architecture based on original images from “Attention
    Is All You Need” by Vaswani et al. (2017)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：基于Vaswani等人（2017年）《Attention Is All You Need》中的原始图像的变换器架构
- en: Training
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: 'Transformers are typically trained via semi-supervised learning in two steps:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器通常通过半监督学习分两步训练：
- en: First, an unsupervised pretraining, typically on a very large corpus.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，进行无监督预训练，通常是在非常大的语料库上进行。
- en: Then, a supervised fine-tuning on a smaller labeled dataset.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，基于较小的标注数据集进行有监督的微调。
- en: Both pretraining and fine-tuning might require significant resources in terms
    of GPU/TPU, memory, and time. This is especially true, considering that large
    language models (in short, LLMs) have an increasing number of parameters as we
    will see in the next section.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练和微调可能需要大量的GPU/TPU资源、内存和时间。考虑到大型语言模型（简称LLM）的参数数量在不断增加，尤其如此，正如我们将在下一节中看到的。
- en: Sometimes, the second phase has a very limited set of labeled data. This is
    the so-called few-shot learning, which considers making predictions based on a
    limited number of samples.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，第二阶段只有一小部分标注数据。这就是所谓的少量样本学习，考虑基于有限的样本数量进行预测。
- en: Transformers’ architectures
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器架构
- en: In this section, we have provided a high-level overview of both the most important
    architectures used by transformers and of the different ways used to compute attention.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了变换器所使用的最重要架构的高级概述，以及计算注意力的不同方法。
- en: Categories of transformers
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变换器的分类
- en: In this section, we are going to classify transformers into different categories.
    The next paragraph will introduce the most common transformers.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将把变换器分类为不同的类别。下一段将介绍最常见的变换器。
- en: Decoder or autoregressive
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解码器或自回归
- en: A typical example is a **GPT** (**Generative Pre-Trained**) model, which you
    can learn more about in the GPT-2 and GPT-3 sections later in this chapter, or
    refer to [https://openai.com/blog/language-unsupervised](https://openai.com/blog/language-unsupervised)).
    Autoregressive models use only the decoder of the original transformer model,
    with the attention heads that can only see what is before in the text and not
    after with a masking mechanism used on the full sentence. Autoregressive models
    use pretraining to guess the next token after observing all the previous ones.
    Typically, autoregressive models are used for **Natural Language Generation**
    (**NLG**) text generation tasks. Other examples of autoregressive models include
    the original GPT, GPT-2, Transformer-XL, Reformer, and XLNet, which are covered
    later in this chapter.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的例子是**GPT**（**生成预训练模型**），你可以在本章稍后的GPT-2和GPT-3部分了解更多，或者参考[https://openai.com/blog/language-unsupervised](https://openai.com/blog/language-unsupervised)。自回归模型仅使用原始变压器模型的解码器，其注意力头只能看到文本中的前部分内容，而看不见后续内容，并在完整句子上使用遮罩机制。自回归模型通过预训练来猜测在观察到所有前面的标记后，下一个标记是什么。通常，自回归模型用于**自然语言生成**（**NLG**）文本生成任务。其他自回归模型的例子包括原始GPT、GPT-2、Transformer-XL、Reformer和XLNet，稍后将在本章中详细介绍。
- en: Encoder or autoencoding
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器或自编码
- en: A typical example is **BERT** (**Bidirectional Encoder Representations from
    Transformers**), which is covered later in this chapter. Autoencoders correspond
    to the encoder in the original transformer model having access to the full input
    tokens with no masks. Autoencoding models use pretraining by masking/altering
    the input tokens and then trying to reconstruct the original sentence. Frequently,
    the models build a bidirectional representation of the full sentences. Note that
    the only difference between autoencoders and autoregressive is the pretraining
    phase, so the same architecture can be used in both ways. Autoencoders can be
    used for NLG, as well as for classification and many other NLP tasks. Other examples
    of autoencoding models, apart from BERT, include ALBERT, RoBERTa, and ELECTRA,
    which you can learn about later in this chapter.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的例子是**BERT**（**来自变压器的双向编码器表示**），将在本章稍后介绍。自编码器对应于原始变压器模型中的编码器，能够访问完整的输入标记，没有遮罩。自编码模型通过遮罩/更改输入标记然后尝试重构原始句子进行预训练。通常，这些模型构建了完整句子的双向表示。需要注意的是，自编码器和自回归模型的唯一区别在于预训练阶段，因此相同的架构可以用于这两种方式。自编码器可用于NLG，也可用于分类和许多其他NLP任务。除了BERT外，其他自编码模型的例子包括ALBERT、RoBERTa和ELECTRA，你可以在本章后续部分了解更多。
- en: Seq2seq
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Seq2seq
- en: A typical example is **T5** (**Text-to-Text Transfer Transformer**) and the
    original transformer. Sequence-to-sequence models use both the encoder and the
    decoder of the original transformer architecture. Seq2seq can be fine-tuned to
    many tasks such as translation, summarization, ranking, and question answering.
    Another example of a seq2seq model, apart from the original transformer and T5,
    is **Multitask Unified Model** (**MUM**).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的例子是**T5**（**文本到文本转移变压器**）和原始变压器。序列到序列模型使用原始变压器架构中的编码器和解码器。Seq2seq可以针对许多任务进行微调，如翻译、摘要、排序和问答。除了原始变压器和T5外，另一个Seq2seq模型的例子是**多任务统一模型**（**MUM**）。
- en: Multimodal
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多模态
- en: A typical example is MUM. Multimodal models mix text inputs with other kinds
    of content (for example, images, videos, and audio).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的例子是MUM。多模态模型将文本输入与其他类型的内容（例如图像、视频和音频）混合。
- en: Retrieval
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检索
- en: A typical example is RETRO. Some models use document retrieval during (pre)training
    and inference. This is frequently a good strategy to reduce the size of the model
    and rapidly access memorized information saving on the number of used parameters.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的例子是RETRO。一些模型在（预）训练和推理过程中使用文档检索。这通常是减少模型规模、快速访问记忆信息的一种有效策略，从而节省使用的参数数量。
- en: Attention
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力
- en: Now that we have understood how to classify transformers, let’s focus on attention!
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了如何对变压器进行分类，接下来让我们关注注意力机制！
- en: There is a wide variety of attention mechanisms, such as self-attention, local/hard
    attention, and global/soft attention, to name a few. Below, we’ll focus on some
    of the examples.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制有多种类型，比如自注意力、局部/硬注意力和全局/软注意力，下面我们将关注一些例子。
- en: Full versus sparse
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完整与稀疏
- en: As discussed, the (scaled) dot-product attention from the original 2017 transformer
    paper is typically computed on a full squared matrix *O*(*L*²) where *L* is the
    length of the maximal considered sequence (in some configurations *L* = 512).
    The BigBird type of transformer, proposed by Google Research in 2020 and discussed
    in more detail later in this chapter, introduced the idea of using sparse attention
    by leveraging sparse matrices (based on the 2019 work by OpenAI’s *Generating
    long sequences with sparse transformers* by Child et al., [https://arxiv.org/abs/1904.10509](https://arxiv.org/abs/1904.10509)).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，原始2017年Transformer论文中的（缩放）点积注意力通常是在一个完整的平方矩阵 *O*(*L*²) 上计算的，其中 *L* 是最大考虑序列的长度（在某些配置中，*L*
    = 512）。谷歌研究院在2020年提出的BigBird类型Transformer，并在本章后续更详细讨论，提出了通过利用稀疏矩阵来使用稀疏注意力的思想（基于OpenAI
    2019年发布的论文《利用稀疏Transformer生成长序列》，作者为Child等人， [https://arxiv.org/abs/1904.10509](https://arxiv.org/abs/1904.10509)）。
- en: LSH attention
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LSH注意力
- en: The Reformer introduced the idea of reducing the attention mechanism complexity
    with hashing—the model’s authors called it locality-sensitive hashing attention.
    The approach is based on the notion of using only the largest elements when *softmax*(*QK*^T)
    is computed. In other words, for each query ![](img/B18331_06_005.png) only the
    keys ![](img/B18331_06_006.png) that are close to *q* are computed. For computing
    closeness, several hash functions are computed according to local sensitive hashing
    techniques.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Reformer提出了通过哈希化来降低注意力机制复杂度的思想——模型的作者称之为局部敏感哈希注意力。该方法基于仅在计算 *softmax*(*QK*^T)
    时使用最大元素的概念。换句话说，对于每个查询 ![](img/B18331_06_005.png)，只有接近 *q* 的键 ![](img/B18331_06_006.png)
    会被计算。为了计算接近度，使用根据局部敏感哈希技术计算的多个哈希函数。
- en: Local attention
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 局部注意力
- en: Some transformers adopted the idea of having only a local window of context
    (e.g. a few tokens on the right and a few tokens on the left). The idea is that
    using fewer parameters allows us to consider longer sequences but with a limited
    degree of attention. For this reason, local attention is less popular.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一些Transformer模型采用了仅具有局部上下文窗口的思想（例如，右边和左边的几个标记）。其想法是，使用更少的参数允许我们考虑更长的序列，但注意力的程度是有限的。由于这个原因，局部注意力不太流行。
- en: Pretraining
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练
- en: As you have learned earlier, the original transformer had an encoder-decoder
    architecture. However, the research community understood that there are situations
    where it is beneficial to have only the encoder, or only the decoder, or both.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如你之前所学，原始的Transformer具有编码器-解码器架构。然而，研究社区意识到，在某些情况下，仅使用编码器或仅使用解码器，或者同时使用两者是有益的。
- en: Encoder pretraining
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器预训练
- en: As discussed, these models are also called auto-encoding and they use only the
    encoder during the pretraining. Pretraining is carried out by masking words in
    the input sequence and training the model to reconstruct the sequence. Typically,
    the encoder can access all the input words. Encoder-only models are generally
    used for classification.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这些模型也被称为自编码模型，它们仅在预训练过程中使用编码器。预训练通过遮蔽输入序列中的词并训练模型重建序列来进行。通常，编码器可以访问所有输入词。仅编码器模型通常用于分类。
- en: Decoder pretraining
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器预训练
- en: Decoder models are referred to as autoregressive. During pretraining, the decoder
    is optimized to predict the next word. In particular, the decoder can only access
    all the words positioned before a given word in the sequence. Decoder-only models
    are generally used for text generation.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器模型被称为自回归模型。在预训练过程中，解码器被优化为预测下一个词。特别是，解码器只能访问序列中给定词之前的所有词。解码器仅模型通常用于文本生成。
- en: Encoder-decoder pretraining
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器-解码器预训练
- en: In this case, the model can use both the encoder and the decoder. Attention
    in the encoder can use all the words in the sequence, while attention in the decoder
    can only use the words preceding a given word in the sequence. Encoder-decoder
    has a large range of applications including text generation, translation, summarization,
    and generative question answer.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，模型可以同时使用编码器和解码器。编码器中的注意力可以使用序列中的所有词语，而解码器中的注意力只能使用序列中给定词之前的词语。编码器-解码器有广泛的应用，包括文本生成、翻译、摘要和生成式问答。
- en: A taxonomy for pretraining tasks
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练任务的分类
- en: 'It can be useful to organize pretraining into a taxonomy suggested by *Pre-trained
    Models for Natural Language Processing: A Survey*, Xipeng Qiu, 2020, [https://arxiv.org/abs/2003.08271](https://arxiv.org/abs/2003.08271):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于将预训练组织成由*《自然语言处理中的预训练模型：一项调查》*（Xipeng Qiu, 2020）建议的分类法非常有用，详见[https://arxiv.org/abs/2003.08271](https://arxiv.org/abs/2003.08271)：
- en: '**Language Modeling** (**LM**): For unidirectional LM, the task is to predict
    the next token. For bidirectional LM, the task is to predict the previous and
    next tokens.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言建模** (**LM**)：对于单向语言建模，任务是预测下一个标记；对于双向语言建模，任务是预测前一个和下一个标记。'
- en: '**Masked Language Modeling** (**MLM**): The key idea is to mask out some tokens
    from the input sentences. Then, the model is trained to predict the masked tokens
    given the non-masked tokens.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**掩蔽语言建模** (**MLM**)：其核心思想是将输入句子中的一些标记进行掩蔽。然后，训练模型根据未被掩蔽的标记预测被掩蔽的标记。'
- en: '**Permuted Language Modeling** (**PLM**): This is similar to LM, but a random
    permutation of input sequences is performed. Then, a subset of tokens is chosen
    as the target, and the model is trained to predict these targets.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**置换语言建模** (**PLM**)：这与语言建模（LM）类似，但对输入序列进行了随机置换。然后，从中选择一个标记子集作为目标，并训练模型去预测这些目标。'
- en: '**Denoising Autoencoder** (**DAE**): Deliberately provide partially corrupted
    input. For instance, randomly sample input tokens and replace them with special
    [MASK] elements. Alternatively, randomly delete input tokens. Alternatively, shuffle
    sentences in random order. The task is to recover the original undistorted input.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去噪自编码器** (**DAE**)：故意提供部分损坏的输入。例如，随机抽取输入标记并将其替换为特殊的[MASK]元素；或者，随机删除输入标记；又或者，随机打乱句子的顺序。任务是恢复原始未损坏的输入。'
- en: '**Contrastive Learning** (**CTL**): The task is to learn a score function for
    text pairs by assuming that some observed pairs of text are more semantically
    similar than randomly sampled text. This class of techniques includes a number
    of specific techniques such as:'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对比学习** (**CTL**)：任务是通过假设一些观测到的文本对比随机抽取的文本具有更高的语义相似性，来学习文本对的评分函数。这类技术包括一些具体技术，如：'
- en: '**Deep InfoMax** (**DIM**): Maximize mutual information between an input image
    representation and various local regions of the same image.'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度信息最大化** (**DIM**)：最大化输入图像表示与同一图像的各个局部区域之间的互信息。'
- en: '**Replaced Token Detection** (**RTD**): Predict whether an input token is replaced
    given its surroundings.'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**替换标记检测** (**RTD**)：预测给定周围环境下，输入标记是否被替换。'
- en: '**Next Sentence Prediction** (**NSP**): The model is trained to distinguish
    whether two input sentences are contiguous in the training corpus.'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下一个句子预测** (**NSP**)：训练模型去区分两个输入句子是否在训练语料库中是连续的。'
- en: '**Sentence Order Prediction** (**SOP**): The same ideas as NSP with some additional
    signals: two consecutive segments are positive examples, and two swapped segments
    are negative examples.'
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**句子顺序预测** (**SOP**)：与NSP相似，但增加了一些附加信号：两个连续的片段是正例，两个交换的片段是负例。'
- en: In this section, we have briefly reviewed different pretraining techniques.
    The next section will review a selection of the most used transformers.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们简要回顾了不同的预训练技术。下一部分将回顾一些最常用的变换器（transformer）模型。
- en: An overview of popular and well-known models
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些流行且知名模型的概述
- en: After the seminal paper *Attention is All You Need*, a very large number of
    alternative transformer-based models have been proposed. Let’s review some of
    the most popular and well-known ones.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 自从开创性论文《Attention is All You Need》发布后，已经提出了大量基于变换器的替代模型。我们来回顾一些最受欢迎和知名的模型。
- en: BERT
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT
- en: 'BERT, or Bidirectional Encoder Representations from Transformers, is a language
    representation model developed by the Google AI research team in 2018\. Let’s
    go over the main intuition behind that model:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: BERT（双向编码器表示变换器）是谷歌AI研究团队于2018年开发的语言表示模型。我们来了解一下该模型的主要直觉：
- en: BERT considers the context of each word from both the left and the right side
    using the so-called “bidirectional self-attention.”
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERT通过所谓的“双向自注意力”机制，从左右两侧考虑每个单词的上下文。
- en: Training happens by randomly masking the input word tokens, and avoiding cycles
    so that words cannot see themselves indirectly. In NLP jargon, this is called
    “fill in the blank.” In other words, the pretraining task involves masking a small
    subset of unlabeled inputs and then training the network to recover these original
    inputs. (This is an example of MLM.)
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练通过随机掩盖输入的单词标记进行，并避免循环以确保单词不能间接地“看到”自己。在自然语言处理术语中，这被称为“填空”。换句话说，预训练任务涉及掩盖一小部分未标记的输入，然后训练网络恢复这些原始输入。（这是MLM的一个例子。）
- en: The model uses classification for pretraining to predict whether a sentence
    sequence S is before a sentence T. This way, BERT can understand relationships
    among sentences (“Next Sentence Prediction”), such as “does sentence T come after
    sentence S?” The idea of pretraining became a new standard for LLM.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该模型使用分类进行预训练，以预测句子序列S是否在句子T之前。通过这种方式，BERT能够理解句子之间的关系（“下一句预测”），例如“句子T是否在句子S之后？”预训练的思想成为了大语言模型的新标准。
- en: BERT—namely BERT Large—became one of the first large language models with 24
    transformer blocks, 1024-hidden layers, 16 self-attention heads, and 340M parameters.
    The model is trained on a large 3.3 billion words corpus.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERT——即BERT Large——成为第一个大规模语言模型之一，拥有24个变换器模块、1024个隐藏层、16个自注意力头和340M参数。该模型在一个包含33亿个单词的大型语料库上进行训练。
- en: 'BERT produced state-of-the-art results for 11 NLP tasks, including:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: BERT在11项NLP任务中取得了最先进的结果，包括：
- en: GLUE score of 80.4%, a 7.6% absolute improvement from the previous best result.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GLUE得分为80.4%，相比之前的最佳结果提高了7.6%。
- en: 93.2% accuracy on SQuAD 1.1 and outperforming human performance by 2%.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在SQuAD 1.1上取得93.2%的准确率，超越人类表现2%。
- en: 'We will see GLUE and SQuAD metrics later in this chapter. If you want to know
    more, you can explore the following material:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面看到GLUE和SQuAD的指标。如果你想了解更多，可以参考以下资料：
- en: 'The original research paper: *BERT: Pre-training of Deep Bidirectional Transformers
    for Language Understanding* by Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina
    Toutanova, 2018, [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '原始研究论文：*BERT: 深度双向变换器语言理解的预训练*，作者：Jacob Devlin, Ming-Wei Chang, Kenton Lee,
    Kristina Toutanova，2018年，[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)。'
- en: 'The Google AI blog post: *Open Sourcing BERT: State-of-the-Art Pre-training
    for Natural Language Processing*, 2018, which discusses the advancement of the
    (then) state-of-the-art model for 11 NLP tasks ([https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.xhtml](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.xhtml).)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google AI博客文章：*开源BERT：自然语言处理的最先进预训练*，2018年，讨论了当时最先进模型在11项NLP任务中的进展([https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.xhtml](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.xhtml))。
- en: An open source TensorFlow implementation and the pretrained BERT models are
    available at [http://goo.gl/language/bert](http://goo.gl/language/bert) and from
    TensorFlow Model Garden at [https://github.com/tensorflow/models/tree/master/official/nlp/modeling/models](https://github.com/tensorflow/models/tree/master/official/nlp/modeling/models).
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源的TensorFlow实现和预训练的BERT模型可以在[http://goo.gl/language/bert](http://goo.gl/language/bert)以及TensorFlow模型库的[https://github.com/tensorflow/models/tree/master/official/nlp/modeling/models](https://github.com/tensorflow/models/tree/master/official/nlp/modeling/models)中找到。
- en: 'A Colab notebook for BERT is available here: [https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT的Colab笔记本可以在这里找到：[https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)。
- en: 'BERT FineTuning with Cloud TPU: A tutorial that shows how to train the BERT
    model on Cloud TPU for sentence and sentence-pair classification tasks: [https://cloud.google.com/tpu/docs/tutorials/bert](https://cloud.google.com/tpu/docs/tutorials/bert).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT在Cloud TPU上的微调：一个教程，展示了如何在Cloud TPU上训练BERT模型，用于句子和句子对分类任务：[https://cloud.google.com/tpu/docs/tutorials/bert](https://cloud.google.com/tpu/docs/tutorials/bert)。
- en: 'A Google blog post about applying BERT to Google Search to improve language
    understanding. According to Google, BERT “*will help Search better understand
    one in 10 searches in the U.S. in English.*” Moreover, the post mentions that
    “*A powerful characteristic of these systems is that they can take learnings from
    one language and apply them to others. So we can take models that learn from improvements
    in English (a language where the vast majority of web content exists) and apply
    them to other languages.*” (from *Understanding search better than ever before*):
    [https://blog.google/products/search/search-language-understanding-bert/](https://blog.google/products/search/search-language-understanding-bert/).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一篇关于将 BERT 应用于 Google 搜索以提升语言理解的 Google 博客文章。根据 Google 的说法，BERT“*将帮助搜索更好地理解美国
    10 个英语搜索中的一个*。”此外，文章提到，“*这些系统的一个强大特点是它们能够将一种语言的学习成果应用到其他语言中。因此，我们可以将从英语（互联网上大多数内容存在的语言）中的改进中学习到的模型，应用到其他语言中*。”（摘自
    *比以往更好地理解搜索*）：[https://blog.google/products/search/search-language-understanding-bert/](https://blog.google/products/search/search-language-understanding-bert/)。
- en: GPT-2
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-2
- en: GPT-2 is a model introduced by OpenAI in *Language Models Are Unsupervised Multitask
    Learners* by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
    and Ilya Sutskever, [https://openai.com/blog/better-language-models/](https://openai.com/blog/better-language-models/),
    [https://openai.com/blog/gpt-2-6-month-follow-up/](https://openai.com/blog/gpt-2-6-month-follow-up/),
    [https://www.openai.com/blog/gpt-2-1-5b-release/](https://www.openai.com/blog/gpt-2-1-5b-release/),
    and [https://github.com/openai/gpt-2](https://github.com/openai/gpt-2).)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 是 OpenAI 在 *语言模型是无监督多任务学习者*（由 Alec Radford、Jeffrey Wu、Rewon Child、David
    Luan、Dario Amodei 和 Ilya Sutskever 编写）中介绍的模型，[https://openai.com/blog/better-language-models/](https://openai.com/blog/better-language-models/)，[https://openai.com/blog/gpt-2-6-month-follow-up/](https://openai.com/blog/gpt-2-6-month-follow-up/)，[https://www.openai.com/blog/gpt-2-1-5b-release/](https://www.openai.com/blog/gpt-2-1-5b-release/)，以及
    [https://github.com/openai/gpt-2](https://github.com/openai/gpt-2)。
- en: 'Let’s review the key intuitions:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下关键直觉：
- en: The largest of four model sizes was a 1.5 billion-parameter transformer with
    48 layers trained on a new dataset called Webtext containing text from 45 million
    webpages.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四种模型大小中最大的一个是具有 15 亿参数的 48 层变换器，训练数据集为名为 Webtext 的新数据集，包含来自 4500 万个网页的文本。
- en: GPT-2 used the original 2017 transformer-based architecture and a modified version
    of the original GPT model (also developed by OpenAI) by Radford et al., 2018,
    *Improving Language Understanding by Generative Pre-Training*, [https://openai.com/blog/language-unsupervised/](https://openai.com/blog/language-unsupervised/),
    and [https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)).
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2 使用了原始的 2017 年基于变换器的架构，并对 Radford 等人（2018）开发的原始 GPT 模型（同样由 OpenAI 开发）进行了修改，*通过生成预训练提高语言理解*，[https://openai.com/blog/language-unsupervised/](https://openai.com/blog/language-unsupervised/)，以及
    [https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)。
- en: The research demonstrated that an LLM trained on a large and diverse dataset
    can perform well on a wide variety of NLP tasks, such as question answering, machine
    translation, reading comprehension, and summarization. Previously, the tasks had
    been typically approached with supervised learning on task-specific datasets.
    GPT-2 was trained in an unsupervised manner and performed well at zero-shot task
    transfer.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究表明，在一个大型且多样化的数据集上训练的 LLM 可以在各种 NLP 任务中表现良好，如问答、机器翻译、阅读理解和摘要生成。以前，这些任务通常通过在任务特定数据集上的监督学习来处理。GPT-2
    采用无监督方式进行训练，并在零样本任务迁移中表现出色。
- en: 'Initially, OpenAI released only a smaller version of GPT-2 with 117 M parameters,
    “due to concerns about large language models being used to generate deceptive,
    biased, or abusive language at scale.” Then, the model was released: [https://openai.com/blog/gpt-2-1-5b-release/](https://openai.com/blog/gpt-2-1-5b-release/).'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最初，OpenAI 只发布了一个较小版本的 GPT-2，具有 1.17 亿个参数，“因为担心大型语言模型会被用来大规模生成具有误导性、偏见或攻击性的语言。”随后，模型被发布：
    [https://openai.com/blog/gpt-2-1-5b-release/](https://openai.com/blog/gpt-2-1-5b-release/)。
- en: 'Interestingly enough, OpenAI developed an ML-based detection method to test
    whether an actor is generating synthetic texts for propaganda. The detection rates
    were ~95% for detecting 1.5B GPT-2-generated text: [https://github.com/openai/gpt-2-output-dataset](https://github.com/openai/gpt-2-output-dataset).'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有趣的是，OpenAI开发了一种基于机器学习的检测方法，用于测试演员是否生成用于宣传的合成文本。检测率为~95%，用于检测1.5B个GPT-2生成的文本：[https://github.com/openai/gpt-2-output-dataset](https://github.com/openai/gpt-2-output-dataset)。
- en: Similar to the original GPT from 2018, GPT-2 does not require the encoder part
    of the original transformer model – it uses a multi-layer decoder for language
    modeling. The decoder can only get information from the prior words in the sentence.
    It takes word vectors as input and produces estimates for the probability of the
    next word as output, but it is *autoregressive*, meaning that each token in the
    sentence relies on the context of the previous words. On the other hand, BERT
    is not autoregressive, as it uses the entire surrounding context all at once.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 与2018年原版GPT类似，GPT-2不需要原始transformer模型的编码器部分——它使用多层解码器进行语言建模。解码器只能从句子中的前置单词获取信息。它以单词向量为输入，并输出下一个单词的概率估计，但它是*自回归*的，这意味着句子中的每个标记依赖于前一个单词的上下文。另一方面，BERT不是自回归的，因为它一次性使用整个周围上下文。
- en: GPT-2 was the first LLM showing commonsense reasoning, capable of performing
    a number of NLP tasks including translation, question answering, and reading comprehension.
    The model achieved state-of-the-art results on 7 out of 8 tested language modeling
    datasets.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2是第一个展示常识推理的LLM，能够执行多个NLP任务，包括翻译、问答和阅读理解。该模型在8个测试的语言建模数据集中的7个上达到了最先进的结果。
- en: GPT-3
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-3
- en: 'GPT-3 is an autoregressive language model developed by OpenAI and introduced
    in 2019 in *Language Models are Few-Shot Learners* by Tom B. Brown, et al., [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165).
    Let’s look at the key intuitions:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3是OpenAI开发的自回归语言模型，2019年在Tom B. Brown等人发布的*Language Models are Few-Shot
    Learners*论文中介绍，[https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)。我们来看一下关键直觉：
- en: GPT-3 uses an architecture and model similar to GPT-2 with a major difference
    consisting of the adoption of a sparse attention mechanism.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-3使用与GPT-2相似的架构和模型，主要区别在于采用了稀疏注意力机制。
- en: 'For each task, the model evaluation has three different approaches:'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个任务，模型评估有三种不同的方法：
- en: '**Few-shot learning**: The model receives a few demonstrations of the task
    (typically, less than one hundred) at inference time. However, no weight updates
    are allowed.'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**少量学习**：模型在推理时接收少量任务示范（通常少于一百个）。然而，不允许进行权重更新。'
- en: '**One-shot learning**: The model receives only one demonstration and a natural
    language description of the task.'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一次性学习**：模型仅接收一次示范和任务的自然语言描述。'
- en: '**Zero-shot learning**: The model receives no demonstration, but it has access
    only to a natural language description of the task.'
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**零样本学习**：模型不接收任何示范，只能访问任务的自然语言描述。'
- en: For all tasks, GPT-3 is applied without any gradient updates, complete with
    tasks and few-shot demonstrations specified purely via text interaction with the
    model.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于所有任务，GPT-3在没有任何梯度更新的情况下应用，任务和少量示范完全通过与模型的文本交互来指定。
- en: The number of parameters the researchers trained GPT-3 with ranges from 125
    million (GPT-3 Small) to 175 billion (GPT-3 175B). With no fine-tuning, the model
    achieves significant results on many NLP tasks including translation and question
    answering, sometimes surpassing state-of-the-art models. In particular, GPT-3
    showed impressive results in NLG, creating news articles that were hard to distinguish
    from real ones. The model demonstrated it was able to solve tasks requiring on-the-fly
    reasoning or domain adaptation, such as unscrambling words, using a novel word
    in a sentence, or performing 3-digit arithmetic.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员训练GPT-3的参数数量范围从1.25亿（GPT-3 Small）到1750亿（GPT-3 175B）。在没有微调的情况下，该模型在许多NLP任务中取得了显著成果，包括翻译和问答，有时甚至超过了最先进的模型。特别是，GPT-3在自然语言生成（NLG）方面表现出色，创作的新闻文章几乎与真实文章难以区分。该模型证明它能够解决需要即时推理或领域适应的任务，例如解码单词、在句子中使用新词，或进行三位数的算术运算。
- en: GPT-3’s underlying model is not publicly available and we can’t pretrain the
    model, but some datasets statistics are available at [https://github.com/openai/gpt-3](https://github.com/openai/gpt-3)
    and we can run data on and fine-tune GPT-3 engines.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3的底层模型未公开，我们无法对模型进行预训练，但一些数据集统计信息可以在[https://github.com/openai/gpt-3](https://github.com/openai/gpt-3)查看，并且我们可以运行数据并对GPT-3引擎进行微调。
- en: Reformer
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Reformer
- en: 'The Reformer model was introduced in the 2020 paper *Reformer: The Efficient
    Transformer* by UC Berkeley and Google AI researchers Nikita Kitaev, Łukasz Kaiser,
    and Anselm Levskaya, [https://arxiv.org/abs/2001.04451](https://arxiv.org/abs/2001.04451).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 'Reformer模型由UC Berkeley和Google AI研究人员Nikita Kitaev、Łukasz Kaiser和Anselm Levskaya在2020年的论文*Reformer:
    The Efficient Transformer*中介绍，[https://arxiv.org/abs/2001.04451](https://arxiv.org/abs/2001.04451)。'
- en: 'Let’s look at the key intuitions:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下关键的直觉：
- en: The authors demonstrated you can train the Reformer model, which performs on
    par with transformer models in a more memory-efficient and faster way on long
    sequences.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者展示了你可以训练Reformer模型，该模型在处理长序列时，在内存效率和速度上与变换器模型表现相当。
- en: One limitation of transformers is the capacity of dealing with long sequences,
    due to the quadratic time needed for computing attention.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器的一个限制是处理长序列的能力，因为计算注意力需要二次方时间。
- en: Reformer addresses the computations and memory challenges during the training
    of transformers by using three techniques.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reformer通过使用三种技术解决了变换器训练过程中计算和内存的挑战。
- en: 'First, Reformer replaced the (scaled) dot-product attention with an approximation
    using locality-sensitive hashing attention (described briefly earlier in this
    chapter). The paper’s authors changed the former’s *O*(*L*²) factor in attention
    layers with *O*(*LlogL*), where *L* is the length of the sequence (see *Figure
    6.4* where LSH is applied to chunks in sequence). Refer to local sensitive hashing
    introduced in computer science to learn more: [https://en.wikipedia.org/wiki/Locality-sensitive_hashing](https://en.wikipedia.org/wiki/Locality-sensitive_hashing).'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，Reformer将（缩放的）点积注意力替换为使用局部敏感哈希注意力的近似方法（本章前面简要描述了这一点）。论文的作者将前者在注意力层中的*O*(*L*²)因素替换为
    *O*(*LlogL*)，其中*L*是序列的长度（参见图6.4，其中LSH应用于序列中的块）。有关局部敏感哈希的更多信息，请参考计算机科学中的相关介绍：[https://en.wikipedia.org/wiki/Locality-sensitive_hashing](https://en.wikipedia.org/wiki/Locality-sensitive_hashing)。
- en: 'Second, the model combined the attention and feedforward layers with reversible
    residual layers instead of normal residual layers (based on the idea from *The
    reversible residual network: Backpropagation without storing activations* by Gomez
    et al., 2017, [https://proceedings.neurips.cc/paper/2017/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.xhtml](https://proceedings.neurips.cc/paper/2017/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.xhtml)).
    Reversible residual layers allow for storage activations once instead of *N* times,
    thus reducing the cost in terms of memory and time complexity.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '其次，该模型将注意力层和前馈层与可逆残差层结合，而不是使用普通的残差层（基于Gomez等人2017年提出的*The reversible residual
    network: Backpropagation without storing activations*的思想，[https://proceedings.neurips.cc/paper/2017/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.xhtml](https://proceedings.neurips.cc/paper/2017/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.xhtml)）。可逆残差层允许存储激活一次，而不是*N*次，从而减少了内存和时间复杂度的开销。'
- en: Third, Reformer used a chunking technique for certain computations, including
    one for the feedforward layer and for a backward pass.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三，Reformer在某些计算中使用了分块技术，包括前馈层和反向传播的计算。
- en: 'You can read the Google AI blog post to learn more about how the Reformer reached
    efficiency at [https://ai.googleblog.com/2020/01/reformer-efficient-transformer.xhtml](https://ai.googleblog.com/2020/01/reformer-efficient-transformer.xhtml):'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以阅读Google AI博客文章，了解Reformer如何通过提高效率：[https://ai.googleblog.com/2020/01/reformer-efficient-transformer.xhtml](https://ai.googleblog.com/2020/01/reformer-efficient-transformer.xhtml)
- en: '![Diagram  Description automatically generated](img/B18331_06_04.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图示  描述自动生成](img/B18331_06_04.png)'
- en: 'Figure 6.4: Local Sensitive Hashing to improve the transformers’ efficiency
    – source: https://ai.googleblog.com/2020/01/reformer-efficient-transformer.xhtml'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4：局部敏感哈希提高变换器的效率 – 来源：[https://ai.googleblog.com/2020/01/reformer-efficient-transformer.xhtml](https://ai.googleblog.com/2020/01/reformer-efficient-transformer.xhtml)
- en: BigBird
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BigBird
- en: 'BigBird is another type of transformer introduced in 2020 by Google Research
    that uses a sparse attention mechanism for tackling the quadratic complexity needed
    to compute full attention for long sequences. For a deeper overview, see the paper
    *Big Bird: Transformers for Longer Sequences* by Manzil Zaheer, Guru Guruganesh,
    Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh
    Ravula, Qifan Wang, Li Yang, and Amr Ahmed, [https://arxiv.org/pdf/2007.14062.pdf](https://arxiv.org/pdf/2007.14062.pdf).'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 'BigBird 是另一种 transformer，于 2020 年由谷歌研究团队提出，采用稀疏注意力机制，以应对计算长序列时所需的二次复杂度。欲了解更多详情，请参阅论文
    *Big Bird: Transformers for Longer Sequences*，作者包括 Manzil Zaheer、Guru Guruganesh、Avinava
    Dubey、Joshua Ainslie、Chris Alberti、Santiago Ontanon、Philip Pham、Anirudh Ravula、Qifan
    Wang、Li Yang 和 Amr Ahmed， [https://arxiv.org/pdf/2007.14062.pdf](https://arxiv.org/pdf/2007.14062.pdf)。'
- en: 'Let’s look at the key intuitions:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下关键的直觉：
- en: The authors demonstrated that BigBird was capable of handling longer context—much
    longer sequences of up to 8x with BERT on similar hardware. Its performance was
    “drastically” better on certain NLP tasks, such as question answering and document
    summarization.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者们证明了 BigBird 能够处理更长的上下文——在类似硬件上，它的序列长度比 BERT 增加了 8 倍。在某些 NLP 任务中，它的表现“显著”优于
    BERT，例如问答和文档摘要。
- en: BigBird runs on a sparse attention mechanism for overcoming the quadratic dependency
    of BERT. Researchers proved that the complexity reduced from *O*(*L*²) to *O*(*L*).
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BigBird 采用稀疏注意力机制，以克服 BERT 的二次依赖性。研究人员证明了复杂度从 *O*(*L*²) 降低到 *O*(*L*)。
- en: This way, BigBird can process sequences of length up to 8x more than what was
    possible with BERT. In other words, BERT’s limit was 512 tokens and BigBird increased
    to 4,096 tokens.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过这种方式，BigBird 可以处理的序列长度是 BERT 的 8 倍。换句话说，BERT 的限制是 512 个 tokens，而 BigBird 增加到
    4,096 个 tokens。
- en: Transformer-XL
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer-XL
- en: 'Transformer-XL is a self-attention-based model introduced in 2019 by Carnegie
    Mellon University and Google Brain researchers in the paper *Transformer-XL: Attentive
    Language Models Beyond a Fixed-Length Context* by Zihang Dai, Zhilin Yang, Yiming
    Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov, [https://aclanthology.org/P19-1285.pdf](https://aclanthology.org/P19-1285.pdf).'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 'Transformer-XL 是一种基于自注意力机制的模型，由卡内基梅隆大学和谷歌大脑的研究人员于 2019 年在论文 *Transformer-XL:
    Attentive Language Models Beyond a Fixed-Length Context* 中提出，该论文的作者包括 Zihang Dai、Zhilin
    Yang、Yiming Yang、Jaime Carbonell、Quoc V. Le 和 Ruslan Salakhutdinov，[https://aclanthology.org/P19-1285.pdf](https://aclanthology.org/P19-1285.pdf)。'
- en: 'Let’s look at the key intuitions:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下关键的直觉：
- en: Unlike the original transformer and RNNs, Transformer-XL demonstrated it can
    model longer-term dependency beyond a fixed-length context while generating relatively
    coherent text.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与原始 transformer 和 RNNs 不同，Transformer-XL 展示了它能够在生成相对连贯的文本时，建模超出固定长度上下文的长期依赖关系。
- en: Transformer-XL introduced a new segment-level recurrence mechanism and a new
    type of relative positional encodings (as opposed to absolute ones), allowing
    the model to learn dependencies that are 80% longer than RNNs and 450% longer
    than vanilla transformers. Traditionally, transformers split the entire corpus
    into shorter segments due to computational limits and only train the model within
    each segment.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer-XL 引入了一种新的段级递归机制和一种新的相对位置编码类型（与绝对位置编码相对），使得模型能够学习比 RNN 长 80% 和比传统
    transformer 长 450% 的依赖关系。传统上，由于计算限制，transformer 将整个语料库划分为较短的段落，并且仅在每个段落内训练模型。
- en: During training, the hidden state sequence computed for the previous segment
    is fixed and cached to be reused as an extended context when the model processes
    the following new segment, as shown in *Figure 6.5*. Although the gradient still
    remains within a segment, this additional input allows the network to exploit
    information in history, leading to an ability of modeling longer-term dependency
    and avoiding context fragmentation.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，前一个段落计算出的隐藏状态序列被固定并缓存，以便在模型处理下一个新段落时作为扩展上下文重用，如 *图 6.5* 所示。尽管梯度仍然局限于某个段落内，但这种额外的输入使得网络能够利用历史信息，从而具备建模长期依赖的能力，避免了上下文的碎片化。
- en: 'During evaluation, the representations from the previous segments can be reused
    instead of being computed from scratch as in the vanilla model case. This way,
    Transformer-XL proved to be up to 1,800+ times faster than the vanilla model during
    evaluation:'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在评估过程中，可以复用前几个段落的表示，而不需要像传统模型那样从头开始计算。通过这种方式，Transformer-XL 在评估过程中比传统模型快了多达1,800倍以上：
- en: '![Chart, scatter chart  Description automatically generated](img/B18331_06_05.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, scatter chart  Description automatically generated](img/B18331_06_05.png)'
- en: 'Figure 6.5: Transformer-XL and the input with recurrent caching of previous
    segments'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：Transformer-XL与输入的递归缓存前段
- en: XLNet
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XLNet
- en: 'XLNet is an unsupervised language representation learning method developed
    by Carnegie Mellon University and Google Brain researchers in 2019\. It is based
    on generalized permutation language modeling objectives. XLNet employs Transformer-XL
    as a backbone model. The reference paper here is *XLNet: Generalized Autoregressive
    Pre-training for Language Understanding* by Zhilin Yang, Zihang Dai, Yiming Yang,
    Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le, [https://arxiv.org/abs/1906.08237](https://arxiv.org/abs/1906.08237).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 'XLNet是一种无监督语言表示学习方法，由卡内基梅隆大学和谷歌大脑的研究人员在2019年开发。它基于广义排列语言建模目标。XLNet使用Transformer-XL作为主干模型。这里的参考论文是*XLNet:
    Generalized Autoregressive Pre-training for Language Understanding*，作者包括Zhilin
    Yang、Zihang Dai、Yiming Yang、Jaime Carbonell、Ruslan Salakhutdinov和Quoc V. Le，[https://arxiv.org/abs/1906.08237](https://arxiv.org/abs/1906.08237)。'
- en: 'Let’s see the key intuitions:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再来看一下关键的直觉：
- en: Like BERT, XLNet uses a bidirectional context, looking at the words before and
    after a given token to predict what it should be.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像BERT一样，XLNet使用双向上下文，查看给定标记前后的词语，以预测它应该是什么。
- en: XLNet maximizes the expected log-likelihood of a sequence with respect to all
    possible permutations of the factorization order. Thanks to the permutation operation,
    the context for each position can consist of tokens from both left and right.
    In other words, XLNet captures bidirectional context.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XLNet最大化了相对于所有可能排列顺序的因式分解顺序的序列期望对数似然。由于排列操作，每个位置的上下文可以由来自左右两边的标记组成。换句话说，XLNet捕捉到了双向上下文。
- en: XLNet outperforms BERT on 20 tasks and achieves state-of-the-art results on
    18 tasks.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XLNet在20个任务上超越了BERT，并在18个任务上达到了最先进的结果。
- en: 'Code and pretrained models are available here: [https://github.com/zihangdai/xlnet](https://github.com/zihangdai/xlnet).'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码和预训练模型可以在这里找到：[https://github.com/zihangdai/xlnet](https://github.com/zihangdai/xlnet)。
- en: XLNet is considered better than BERT in almost all NLP tasks, outperforming
    BERT on 20 tasks, often by a large margin. When it was introduced, the model achieved
    state-of-the-art performance on 18 NLP tasks, including sentiment analysis, natural
    language inference, question answering, and document ranking.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: XLNet被认为在几乎所有NLP任务中都优于BERT，在20个任务上超越了BERT，通常差距较大。当它被引入时，该模型在18个NLP任务上达到了最先进的性能，包括情感分析、自然语言推理、问答和文档排序。
- en: RoBERTa
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RoBERTa
- en: 'RoBERTa (a Robustly Optimized BERT) is a model introduced in 2019 by researchers
    at the University of Washington and Facebook AI (Meta) in *RoBERTa: A Robustly
    Optimized BERT Pretraining Approach* by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei
    Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin
    Stoyanov, [https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692).'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 'RoBERTa（一个强健优化的BERT模型）是2019年由华盛顿大学和Facebook AI（Meta）的研究人员提出的，在*RoBERTa: A Robustly
    Optimized BERT Pretraining Approach*论文中，由Yinhan Liu、Myle Ott、Naman Goyal、Jingfei
    Du、Mandar Joshi、Danqi Chen、Omer Levy、Mike Lewis、Luke Zettlemoyer和Veselin Stoyanov编写，[https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)。'
- en: 'Let’s look at the key intuitions:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下关键的直觉：
- en: When replicating BERT, the researchers discovered that BERT was “significantly
    undertrained.”
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在复制BERT时，研究人员发现BERT“显著欠训练”。
- en: RoBERTa’s authors proposed a BERT variant that modifies key hyperparameters
    (longer training, larger batches, more data), removing the next-sentence pretraining
    objective, and training on longer sequences. The authors also proposed dynamically
    changing the masking pattern applied to the training data.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RoBERTa的作者提出了一种BERT变种，通过修改关键超参数（更长的训练时间、更大的批量、更多的数据），移除下一个句子预训练目标，并在更长的序列上进行训练。作者还提出了动态变化应用于训练数据的掩蔽模式。
- en: The researchers collected a new dataset called CC-News of similar size to other
    privately used datasets.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究人员收集了一个新的数据集，称为CC-News，大小与其他私有数据集相似。
- en: 'The code is available here: [https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq).'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码可以在这里找到：[https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq)。
- en: RoBERTa outperformed BERT on GLUE and SQuAD tasks and matched XLNet on some
    of them.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa在GLUE和SQuAD任务上超过了BERT，并在其中一些任务上与XLNet持平。
- en: ALBERT
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ALBERT
- en: '**ALBERT** (**A Lite BERT**) is a model introduced in 2019 by researchers at
    Google Research and Toyota Technological Institute at Chicago in the paper titled
    *ALBERT: A Lite BERT for Self-supervised Learning of Language Representations*
    by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,
    and Radu Soricut, [https://arxiv.org/abs/1909.11942v1](https://arxiv.org/abs/1909.11942v1).'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '**ALBERT**（**A Lite BERT**）是由Google Research和芝加哥丰田技术研究院的研究人员于2019年提出的模型，论文标题为*ALBERT:
    A Lite BERT for Self-supervised Learning of Language Representations*，作者包括Zhenzhong
    Lan、Mingda Chen、Sebastian Goodman、Kevin Gimpel、Piyush Sharma和Radu Soricut，[https://arxiv.org/abs/1909.11942v1](https://arxiv.org/abs/1909.11942v1)。'
- en: 'Let’s see the key intuitions:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下关键的直觉：
- en: Large models typically aim at increasing the model size when pretraining natural
    language representations in order to get improved performance. However, increasing
    the model size might become difficult due to GPU/TPU memory limitations, longer
    training times, and unexpected model degradation.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型模型通常通过增加模型大小来提高预训练自然语言表示的性能。然而，由于GPU/TPU内存限制、更长的训练时间和意外的模型退化，增加模型大小可能变得困难。
- en: 'ALBERT attempts to address the memory limitation, the communication overhead,
    and model degradation problems with an architecture that incorporates two parameter-reduction
    techniques: factorized embedding parameterization and cross-layer parameter sharing.
    With factorized embedding parameterization, the size of the hidden layers is separated
    from the size of vocabulary embeddings by decomposing the large vocabulary-embedding
    matrix into two small matrices. With cross-layer parameter sharing, the model
    prevents the number of parameters from growing along with the network depth. Both
    of these techniques improved parameter efficiency without “seriously” affecting
    the performance.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ALBERT试图解决内存限制、通信开销和模型退化问题，采用了一种结合了两种参数减少技术的架构：因式分解嵌入参数化和跨层参数共享。通过因式分解嵌入参数化，隐藏层的大小与词汇嵌入的大小分离，通过将大词汇嵌入矩阵分解为两个小矩阵来实现。通过跨层参数共享，模型防止了随着网络深度增加而参数数量的增长。这两种技术在不“严重”影响性能的情况下提高了参数效率。
- en: ALBERT has 18x fewer parameters and 1.7x faster training compared to the original
    BERT-Large model and achieves only slightly worse performance.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与原始的BERT-Large模型相比，ALBERT的参数量减少了18倍，训练速度提高了1.7倍，性能仅略微下降。
- en: 'The code is available here: [https://github.com/brightmart/albert_zh](https://github.com/brightmart/albert_zh).'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码可在此处获取：[https://github.com/brightmart/albert_zh](https://github.com/brightmart/albert_zh)。
- en: ALBERT claimed it established new state-of-the-art results on all of the current
    state-of-the-art language benchmarks like GLUE, SQuAD, and RACE.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ALBERT声称在所有当前最先进的语言基准（如GLUE、SQuAD和RACE）上都建立了新的最先进成果。
- en: StructBERT
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StructBERT
- en: 'StructBERT is a model introduced in 2019’s paper called *StructBERT: Incorporating
    Language Structures into Pre-training for Deep Language Understanding* by Wei
    Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Jiangnan Xia, Liwei Peng, and Luo Si,
    [https://arxiv.org/abs/1908.04577](https://arxiv.org/abs/1908.04577).'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 'StructBERT是2019年提出的一个模型，论文标题为*StructBERT: Incorporating Language Structures
    into Pre-training for Deep Language Understanding*，作者包括Wei Wang、Bin Bi、Ming Yan、Chen
    Wu、Zuyi Bao、Jiangnan Xia、Liwei Peng和Luo Si，[https://arxiv.org/abs/1908.04577](https://arxiv.org/abs/1908.04577)。'
- en: 'Let’s see the key intuitions:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下关键的直觉：
- en: The Alibaba team suggested extending BERT by leveraging word-level and sentence-level
    ordering during the pretraining procedure. BERT masking during pretraining is
    extended by mixing up a number of tokens and then the model has to predict the
    right order.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阿里巴巴团队建议在预训练过程中通过利用词级和句子级的顺序扩展BERT。通过混合多个token来扩展BERT的预训练掩码，模型需要预测正确的顺序。
- en: In addition, the model randomly shuffles the sentence order and predicts the
    next and the previous sentence with a specific prediction task.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，该模型随机打乱句子顺序，并通过特定的预测任务预测下一句和上一句。
- en: This additional wording and sentence shuffling together with the task of predicting
    the original order allow StructBERT to learn linguistic structures during the
    pretraining procedure.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种附加的词汇和句子打乱以及预测原始顺序的任务使得StructBERT能够在预训练过程中学习语言结构。
- en: StructBERT from Alibaba claimed to have achieved state-of-the-art results on
    different NLP tasks, such as sentiment classification, natural language inference,
    semantic textual similarity, and question answering, outperforming BERT.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 阿里巴巴的StructBERT声称在不同的NLP任务（如情感分类、自然语言推理、语义文本相似性和问答）中达到了最先进的结果，超越了BERT。
- en: T5 and MUM
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: T5 和 MUM
- en: In 2019, Google researchers introduced a framework dubbed Text-to-Text Transfer
    Transformer (in short, T5) in *Exploring the Limits of Transfer Learning with
    a Unified Text-to-Text Transformer* by Colin Raffel, Noam Shazeer, Adam Roberts,
    Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J.
    Liu, [https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683). This
    paper is a fundamental one for transformers.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年，Google的研究人员在Colin Raffel、Noam Shazeer、Adam Roberts、Katherine Lee、Sharan
    Narang、Michael Matena、Yanqi Zhou、Wei Li和Peter J. Liu的论文《*Exploring the Limits
    of Transfer Learning with a Unified Text-to-Text Transformer*》中介绍了一种名为Text-to-Text
    Transfer Transformer（简称T5）的框架，[https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683)。这篇论文是变压器模型领域的基础性论文。
- en: 'Here are some of the key ideas:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些关键观点：
- en: T5 addresses many NLP tasks as a “text-to-text” problem. T5 is a single model
    (with different numbers of parameters) that can be trained on a wide number of
    tasks. The framework is so powerful that it can be applied to summarization, sentiment
    analysis, question answering, and machine translation.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T5将许多自然语言处理任务作为“文本到文本”的问题进行处理。T5是一个单一模型（具有不同数量的参数），可以在众多任务上进行训练。该框架如此强大，能够应用于摘要生成、情感分析、问答和机器翻译等任务。
- en: Transfer learning, where a model is first pretrained on a data-rich task before
    being fine-tuned on a downstream task, is extensively analyzed by comparing pretraining
    objectives, architectures, unlabeled datasets, transfer approaches, and other
    factors on dozens of language understanding tasks.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转移学习，即先在数据丰富的任务上预训练一个模型，再在下游任务上进行微调，通过比较预训练目标、架构、未标注数据集、转移方法及其他因素，在数十个语言理解任务上进行了深入分析。
- en: 'Similar to the original transformer, T5: 1) uses an encoder-decoder structure;
    2) maps the input sequences to learned embeddings and positional embeddings, which
    are passed to the encoder; 3) uses self-attention blocks with self-attention and
    feedforward layers (each with normalization and skip connections) in both the
    encoder and the decoder.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与原始的变压器类似，T5：1）使用编码器-解码器结构；2）将输入序列映射到学习的嵌入和位置嵌入，这些嵌入传递给编码器；3）在编码器和解码器中使用自注意力块，结合自注意力和前馈层（每层都具有归一化和跳跃连接）。
- en: Training happens on a “Colossal Clean Crawled Corpus’’ (C4) dataset and the
    number of parameters per each T5 model varies from 60 million (T5 Small) up to
    11 billion.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练是在一个名为“Colossal Clean Crawled Corpus”（C4）的数据集上进行的，每个T5模型的参数数量从6000万（T5小型）到110亿不等。
- en: The computation costs were similar to BERT, but with twice as many parameters.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算成本与BERT相似，但参数数量是其两倍。
- en: 'The code is available here: [https://github.com/google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer).'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码可在这里获取：[https://github.com/google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer)。
- en: Google also offers T5 with a free TPU in a Colab tutorial at [https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb).
    We will discuss this in more detail later this chapter.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google还提供了在Colab教程中使用免费的TPU运行T5，网址为[https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb)。我们将在本章稍后详细讨论这一内容。
- en: 'When presented, the T5 model with 11 billion parameters achieved state-of-the-art
    performances on 17 out of 24 tasks considered and became de-facto one of the best
    LMs available:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型呈现时，具有110亿参数的T5模型在24个任务中有17个任务上达到了最先进的性能，成为事实上的最佳语言模型之一：
- en: '![Diagram  Description automatically generated](img/B18331_06_06.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![图表 描述自动生成](img/B18331_06_06.png)'
- en: 'Figure 6.6: T5 uses the same model, loss function, hyperparameters, etc. across
    our diverse set of tasks —including translation, question answering, and classification'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6：T5在我们多种任务集上使用相同的模型、损失函数、超参数等——包括翻译、问答和分类任务
- en: 'mT5, developed by Xue et al. at Google Research in 2020, extended T5 by using
    a single transformer to model multiple languages. It was pretrained on a Common
    Crawl-based dataset covering 101 languages. You can read more about it in *mT5:
    A Massively Multilingual Pre-trained Text-to-Text Transformer*, [https://arxiv.org/pdf/2010.11934.pdf](https://arxiv.org/pdf/2010.11934.pdf).'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 'mT5 由 Google Research 的 Xue 等人在 2020 年开发，通过使用单一的 Transformer 模型来处理多语言。它在基于
    Common Crawl 的数据集上进行了预训练，涵盖了 101 种语言。你可以在 *mT5: A Massively Multilingual Pre-trained
    Text-to-Text Transformer* 中阅读更多信息，[https://arxiv.org/pdf/2010.11934.pdf](https://arxiv.org/pdf/2010.11934.pdf)。'
- en: '**MUM** (short for **Multitask Unified Model**) is a model using the T5 text-to-text
    framework and according to Google is 1,000 times more powerful than BERT. Not
    only does MUM understand language, but it also generates it. It is also multimodal,
    covering modalities like text and images (expanding to more modalities in the
    future). The model was trained across 75 different languages and many different
    tasks at once. MUM is currently used to support Google Search ranking: [https://blog.google/products/search/introducing-mum/](https://blog.google/products/search/introducing-mum/).'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '**MUM**（即 **Multitask Unified Model** 的缩写）是一个使用 T5 文本到文本框架的模型，根据 Google 的说法，其性能是
    BERT 的 1,000 倍。MUM 不仅能理解语言，还能生成语言。它还是多模态的，涵盖文本和图像等模态（未来将扩展到更多模态）。该模型在 75 种不同语言和多种任务上进行了训练。目前，MUM
    被用于支持 Google 搜索排名：[https://blog.google/products/search/introducing-mum/](https://blog.google/products/search/introducing-mum/)。'
- en: ELECTRA
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ELECTRA
- en: 'ELECTRA is a model introduced in 2020 by Stanford University and Google Brain
    researchers in *ELECTRA: Pre-training Text Encoders as Discriminators Rather Than
    Generators* by Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning,
    [https://arxiv.org/abs/2003.10555](https://arxiv.org/abs/2003.10555).'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 'ELECTRA 是斯坦福大学和 Google Brain 研究人员于 2020 年推出的模型，发表在 *ELECTRA: Pre-training Text
    Encoders as Discriminators Rather Than Generators* 论文中，作者为 Kevin Clark、Minh-Thang
    Luong、Quoc V. Le 和 Christopher D. Manning，[https://arxiv.org/abs/2003.10555](https://arxiv.org/abs/2003.10555)。'
- en: 'Let’s look at the key intuitions:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下关键的直觉：
- en: BERT pretraining consists of masking a small subset of unlabeled inputs and
    then training the network to recover them. Typically only a small fraction of
    words are used (~15%).
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 的预训练任务包括掩盖一小部分未标记的输入，然后训练网络去恢复这些输入。通常只使用少量的词汇（约 15%）。
- en: 'The ELECTRA authors proposed a new pretraining task named “replaced token detection.”
    The idea is to replace some tokens with alternatives generated by a small language
    model. Then, the pretrained discriminator is used to predict whether each token
    is an original or a replacement. This way, the model can learn from all the tokens
    instead of a subset:'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ELECTRA 的作者提出了一种新的预训练任务——“替换词检测”。其思想是将一些词用由小型语言模型生成的替代词替换。然后，预训练的判别器用来预测每个词是原始词还是替代词。通过这种方式，模型可以从所有词中学习，而不仅仅是一个子集：
- en: '![Diagram  Description automatically generated](img/B18331_06_07.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![图示 描述自动生成](img/B18331_06_07.png)'
- en: 'Figure 6.7: ELECTRA replacement strategy. The discriminator’s task is to detect
    whether the word is an original one or a replacement – source: https://arxiv.org/pdf/2003.10555.pdf'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7：ELECTRA 替换策略。判别器的任务是检测该词是原始词还是替代词——来源：[https://arxiv.org/pdf/2003.10555.pdf](https://arxiv.org/pdf/2003.10555.pdf)
- en: ELECTRA outperformed previous state-of-the-art models, requiring at the same
    time less pretraining efforts. The code is available at [https://github.com/google-research/electra](https://github.com/google-research/electra).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ELECTRA 超越了之前的最先进模型，同时需要更少的预训练工作。代码可以在 [https://github.com/google-research/electra](https://github.com/google-research/electra)
    获取。
- en: DeBERTa
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DeBERTa
- en: 'DeBERTa is a model introduced by Microsoft’s researchers in 2020 in *DeBERTa:
    Decoding-enhanced BERT with Disentangled Attention* by Pengcheng He, Xiaodong
    Liu, Jianfeng Gao, and Weizhu Chen, [https://arxiv.org/abs/2006.03654](https://arxiv.org/abs/2006.03654).'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 'DeBERTa 是微软研究人员在 2020 年推出的模型，发表在 *DeBERTa: Decoding-enhanced BERT with Disentangled
    Attention* 论文中，作者为 Pengcheng He、Xiaodong Liu、Jianfeng Gao 和 Weizhu Chen，[https://arxiv.org/abs/2006.03654](https://arxiv.org/abs/2006.03654)。'
- en: 'Let’s look at the most important ideas:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看最重要的观点：
- en: BERT’s self-attention is focused on content-to-content and content-to-position,
    where the content and position embedding are added before self-attention. DeBERTa
    keeps two separate vectors representing content and position so that self-attention
    is calculated between content-to-content, content-to-position, position-to-content,
    and position-to-position.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 的自注意力集中在内容到内容和内容到位置的关系上，内容和位置的嵌入会在自注意力之前加上。DeBERTa 保持了两个独立的向量来表示内容和位置，从而使自注意力可以在内容到内容、内容到位置、位置到内容以及位置到位置之间进行计算。
- en: DeBERTa keeps absolute position information along with the related position
    information.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeBERTa 保留了绝对位置的信息，并结合了相关的位置信息。
- en: Due to additional structural information used by the model, DeBERTa claimed
    to have achieved state-of-the-art results with half the training data when compared
    with other models such as RoBERTa. The code is available at [https://github.com/microsoft/DeBERTa](https://github.com/microsoft/DeBERTa).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型使用了额外的结构信息，DeBERTa 声称在使用比其他模型（如 RoBERTa）更少的训练数据时，达到了最先进的结果。代码可在 [https://github.com/microsoft/DeBERTa](https://github.com/microsoft/DeBERTa)
    上找到。
- en: The Evolved Transformer and MEENA
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进化变换器与 MEENA
- en: The Evolved Transformer was introduced in 2019 by Google Brain researchers in
    the paper *The Evolved Transformer* by David R. So, Chen Liang, and Quoc V. Le,
    [https://arxiv.org/abs/1901.11117](https://arxiv.org/abs/1901.11117).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 进化变换器是由谷歌大脑（Google Brain）研究人员在 2019 年提出的，发表于 *The Evolved Transformer* 这篇由 David
    R. So、Chen Liang 和 Quoc V. Le 撰写的论文中，[https://arxiv.org/abs/1901.11117](https://arxiv.org/abs/1901.11117)。
- en: 'Let’s go over the main ideas:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下主要的思想：
- en: Transformers are a class of architectures that are manually drafted. The Evolved
    Transformers researchers applied **Neural Architecture Search** (**NAS**), a set
    of automatic optimization techniques that learn how to combine basic architectural
    building blocks to find models better than the ones manually designed by humans.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器是一类手工设计的架构。进化变换器的研究人员应用了 **神经架构搜索** (**NAS**)，这是一套自动优化技术，用于学习如何结合基本架构构件，从而找到比人类手工设计的模型更优秀的模型。
- en: NAS was applied to both the transformer encoder and decoder blocks resulting
    in a new architecture shown in *Figures 6.8* and *6.9*.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NAS 被应用于变换器编码器和解码器块， resulting in a new architecture shown in *Figures 6.8*
    and *6.9*.
- en: 'Evolved Transformers demonstrated consistent improvement compared to the original
    transformer architecture. The model is at the core of MEENA, a multi-turn open-domain
    chatbot trained end-to-end on data mined and filtered from social media conversations
    on public domains. MEENA uses Evolved Transformers with 2.6 billion parameters
    with a single Evolved Transformer encoder block and 13 Evolved Transformer decoder
    blocks. The objective function used for training focuses on minimizing perplexity,
    the uncertainty of predicting the next token. MEENA can conduct conversations
    that are more sensitive and specific than existing state-of-the-art chatbots.
    Refer to the Google blog post *Towards a Conversational Agent that Can Chat About…Anything*,
    [https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.xhtml](https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.xhtml):'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始变换器架构相比，进化变换器（Evolved Transformers）展示了稳定的改进。该模型是 MEENA 的核心，MEENA 是一个多轮开放域聊天机器人，经过端到端训练，使用从公共领域社交媒体对话中挖掘并过滤的数据。MEENA
    使用了 26 亿个参数的进化变换器，拥有一个进化变换器编码器块和 13 个进化变换器解码器块。训练所使用的目标函数专注于最小化困惑度（perplexity），即预测下一个标记时的“不确定性”。与现有的最先进聊天机器人相比，MEENA
    能进行更加敏感和具体的对话。参见谷歌博客文章 *Towards a Conversational Agent that Can Chat About…Anything*，[https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.xhtml](https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.xhtml)：
- en: '![Diagram  Description automatically generated](img/B18331_06_08.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B18331_06_08.png)'
- en: 'Figure 6.8: The Evolved Transformer encoder block, source: https://arxiv.org/pdf/1901.11117.pdf'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8：进化变换器编码器块，来源：[https://arxiv.org/pdf/1901.11117.pdf](https://arxiv.org/pdf/1901.11117.pdf)
- en: '![Diagram  Description automatically generated](img/B18331_06_09.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B18331_06_09.png)'
- en: 'Figure 6.9: The Evolved Transformer decoder block, source: https://arxiv.org/pdf/1901.11117.pdf'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9：进化变换器解码器块，来源：[https://arxiv.org/pdf/1901.11117.pdf](https://arxiv.org/pdf/1901.11117.pdf)
- en: LaMDA
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LaMDA
- en: 'LaMDA is a model introduced in 2022 by Google’s researchers in *LaMDA: Language
    Models for Dialog Applications* by Romal Thoppilan, et al., [https://arxiv.org/abs/2201.08239](https://arxiv.org/abs/2201.08239).
    It is a family of transformer-based neural language models specialized for dialog.
    Let’s see the key intuitions:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 'LaMDA 是谷歌研究人员在 2022 年提出的模型，来源于 *LaMDA: Language Models for Dialog Applications*
    这篇由 Romal Thoppilan 等人撰写的论文，[https://arxiv.org/abs/2201.08239](https://arxiv.org/abs/2201.08239)。它是一种基于变换器（transformer）的神经语言模型，专门用于对话。我们来看一下关键的直觉：'
- en: In the pretraining stage, LaMDA uses a dataset of 1.56 trillion words — nearly
    40x more than what was previously used for LLMs — from public dialog data and
    other public web documents. After tokenizing the dataset into 2.81 trillion SentencePiece
    tokens, the pretraining predicts every next token in a sentence, given the previous
    tokens.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预训练阶段，LaMDA 使用了 1.56 万亿个单词的数据集——比以前用于大规模语言模型的数据多出近 40 倍——来自公共对话数据和其他公共网页文档。在将数据集分词为
    2.81 万亿 SentencePiece 令牌后，预训练根据前面的令牌预测句子中的每个下一个令牌。
- en: In the fine-tuning stage, LaMDA performs a mix of generative tasks to generate
    natural-language responses to given contexts, and classification tasks on whether
    a response is safe and of high quality. The combination of generation and classification
    provides the final answer (see *Figure 6.10*).
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在微调阶段，LaMDA 执行生成任务和分类任务的混合，生成针对给定上下文的自然语言响应，并分类判断响应是否安全且高质量。生成与分类的结合提供了最终答案（见
    *图 6.10*）。
- en: 'LaMDA defines a robust set of metrics for quality, safety, and groundedness:'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LaMDA 定义了一套强健的质量、安全性和根植性评估指标：
- en: 'Quality: This measure is decomposed into three dimensions, **Sensibleness,
    Specificity, and Interestingness** (**SSI**). Sensibleness considers whether the
    model produces responses that make sense in the dialog context. Specificity judges
    whether the response is specific to the preceding dialog context, and not a generic
    response that could apply to most contexts. Interestingness measures whether the
    model produces responses that are also insightful, unexpected, or witty.'
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 质量：该度量被分解为三个维度，**合理性、特异性和趣味性**（**SSI**）。合理性考虑模型生成的响应是否符合对话上下文的逻辑。特异性判断响应是否针对前一个对话上下文，而不是一个可以应用于大多数上下文的通用回答。趣味性则衡量模型生成的响应是否富有见解、出乎意料或机智。
- en: 'Safety: Takes into account how to avoid any unintended result that creates
    risks of harm for the user, and to avoid reinforcing unfair bias.'
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全性：考虑如何避免产生任何可能对用户造成伤害的意外结果，以及避免加剧不公平的偏见。
- en: 'Groundedness: Takes into account plausible information that is, however, contradicting
    information that can be supported by authoritative external sources.![Diagram  Description
    automatically generated](img/B18331_06_10.png)'
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根植性：考虑到信息的可信性，但该信息可能与外部权威来源支持的资料相矛盾。![图表  描述自动生成](img/B18331_06_10.png)
- en: 'Figure 6.10: LaMDA generates and then scores a response candidate. Source:
    https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.xhtml'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.10：LaMDA 生成并评分一个响应候选。来源：[https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.xhtml](https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.xhtml)
- en: 'LaMDA demonstrated results that were impressively close to the human brain
    ones. According to Google ([https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.xhtml](https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.xhtml)),
    LaMDA significantly outperformed the pretrained model in every dimension and across
    all model sizes. Quality metrics (Sensibleness, Specificity, and Interestingness)
    generally improved with the number of model parameters, with or without fine-tuning.
    Safety did not seem to benefit from model scaling alone, but it did improve with
    fine-tuning. Groundedness improved as model size increased, perhaps because larger
    models have a greater capacity to memorize uncommon knowledge, but fine-tuning
    allows the model to access external knowledge sources and to effectively shift
    some of the load of remembering knowledge to an external knowledge source. With
    fine-tuning, the quality gap to human levels can be shrunk, though the model performance
    remains below human levels in safety and groundedness:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: LaMDA 展示了接近人脑水平的惊人表现。根据 Google 的说法（[https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.xhtml](https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.xhtml)），LaMDA
    在各个维度上，以及所有模型大小上，都明显优于预训练模型。质量度量（合理性、特异性和趣味性）随着模型参数的增加而普遍提高，无论是否经过微调。安全性似乎并未仅凭模型扩展得到改善，但经过微调后有了提升。根植性随着模型大小的增加而提高，可能是因为较大的模型能更好地记住不常见的知识，但微调使模型能够访问外部知识源，并有效地将一些记忆负担转移到外部知识源上。经过微调后，模型在质量上的差距可以缩小到接近人类水平，但在安全性和根植性方面，模型的表现仍低于人类水平：
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B18331_06_11.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面  描述自动生成，信心中等](img/B18331_06_11.png)'
- en: 'Figure 6.11: LaMDA performance – source: https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.xhtml'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11：LaMDA 性能 – 来源：[https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.xhtml](https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.xhtml)
- en: Switch Transformer
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Switch Transformer
- en: 'The Switch Transformer is a model introduced in 2021 by Google’s researchers
    in *Switch Transformers: Scaling to Trillion Parameter Models with Simple and
    Efficient Sparsity* by William Fedus, Barret Zoph, and Noam Shazeer, introduced
    in [https://arxiv.org/abs/2101.03961](https://arxiv.org/abs/2101.03961).'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 'Switch Transformer是由谷歌的研究人员在2021年提出的，发表于《Switch Transformers: Scaling to Trillion
    Parameter Models with Simple and Efficient Sparsity》一文中，由William Fedus、Barret
    Zoph和Noam Shazeer撰写，文章链接：[https://arxiv.org/abs/2101.03961](https://arxiv.org/abs/2101.03961)。'
- en: 'Let’s look at the key intuitions:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看关键的直觉：
- en: The Switch Transformer was trained from 7 billion to 1.6 trillion parameters.
    As discussed, a typical transformer is a deep stack of multi-headed self-attention
    layers, and at the end of each layer, there’s an FFN aggregating the outputs coming
    from its multiple heads. The Switch Transformer replaces this single FFN with
    multiple FFNs and calls them “experts.” On each forward pass, at each layer, for
    each token at the input, the model activates exactly one expert:![Diagram  Description
    automatically generated](img/B18331_06_12.png)
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Switch Transformer从70亿到1.6万亿参数进行了训练。如前所述，典型的变换器是一个由多头自注意力层组成的深度堆栈，每层的末端都有一个FFN（前馈神经网络），用于聚合来自多个头的输出。Switch
    Transformer将这个单一的FFN替换为多个FFN，并将其称为“专家”。在每次前向传播中，在每层，对于输入的每个标记，模型会激活一个专家：![图示 说明自动生成](img/B18331_06_12.png)
- en: 'Fig 6.12: The Switch Transformer with multiple routing FFN – The dense FFN
    layer present in the transformer is replaced with a sparse Switch FFN layer (light
    blue). Source: https://arxiv.org/pdf/2101.03961.pdf'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.12：具有多个路由FFN的Switch Transformer——变换器中存在的密集FFN层被稀疏的Switch FFN层（浅蓝色）替代。来源：[https://arxiv.org/pdf/2101.03961.pdf](https://arxiv.org/pdf/2101.03961.pdf)
- en: Switch-Base (7 billion parameters) and Switch-Large (26 billion parameters)
    outperformed T5-Base (0.2 billion parameters) and T5-Large (0.7 billion parameters)
    on tasks such as language modeling, classification, coreference resolution, question
    answering, and summarization.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Switch-Base（70亿参数）和Switch-Large（260亿参数）在语言建模、分类、共指解析、问答和摘要等任务上优于T5-Base（2亿参数）和T5-Large（7亿参数）。
- en: An example implementation of Switch Transformer is available at [https://keras.io/examples/nlp/text_classification_with_switch_transformer/](https://keras.io/examples/nlp/text_classification_with_switch_transformer/).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: Switch Transformer的一个示例实现可以在[https://keras.io/examples/nlp/text_classification_with_switch_transformer/](https://keras.io/examples/nlp/text_classification_with_switch_transformer/)找到。
- en: RETRO
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RETRO
- en: '**RETRO** (**Retrieval-Enhanced Transformer**) is a retrieval-enhanced autoregressive
    language model introduced by DeepMind in 2022 in *Improving language models by
    retrieving from trillions of tokens* by Sebastian Borgeaud et al., [https://arxiv.org/pdf/2112.04426/](https://arxiv.org/pdf/2112.04426/).
    Let’s look at the key intuitions:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '**RETRO**（**检索增强型变换器**）是DeepMind在2022年提出的一种检索增强自回归语言模型，发表于Sebastian Borgeaud等人的《通过从万亿标记中检索来改进语言模型》中，[https://arxiv.org/pdf/2112.04426/](https://arxiv.org/pdf/2112.04426/)。我们来看一下关键的直觉：'
- en: Scaling the number of parameters in an LLM has proven to be a way to improve
    the quality of the results. However, this approach is not sustainable because
    it is computationally expensive.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加LLM中的参数数量已被证明是一种提高结果质量的方法。然而，这种方法并不可持续，因为它在计算上非常昂贵。
- en: RETRO couples a retrieval **Database** (**DB**) with a transformer in a hybrid
    architecture. The idea is first to search with the Nearest Neighbors algorithm
    on pre-computed BERT embeddings stored in a retrieval DB. Then, these embeddings
    are used as input to a transformer’s encoder.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RETRO将一个检索**数据库**（**DB**）与变换器结合，形成混合架构。其思想是首先使用最近邻算法在预计算的BERT嵌入中进行搜索，这些嵌入存储在检索数据库中。然后，将这些嵌入作为输入传递给变换器的编码器。
- en: The combination of retrieval and transformers allows RETRO (scaled from 150
    million to 7 billion non-embedding parameters) to save on the number of parameters
    used by the LLM.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索和变换器的结合使得RETRO（从1.5亿扩展到70亿非嵌入参数）在使用LLM时节省了参数数量。
- en: 'For instance, consider the sample query “The 2021 Women’s US Open was won”
    and *Figure 6.13*, where the cached BERT embeddings are passed to a transformer
    encoder to get the final result:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑查询“2021年美国网球公开赛冠军是”及*图6.13*，其中缓存的BERT嵌入被传递给变换器编码器，以获得最终结果：
- en: '![Diagram  Description automatically generated](img/B18331_06_13.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![图示 说明自动生成](img/B18331_06_13.png)'
- en: 'Figure 6.13: A high-level overview of Retrieval Enhanced Transformers (RETRO).
    Source: https://deepmind.com/research/publications/2021/improving-language-models-by-retrieving-from-trillions-of-tokens'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13：检索增强变压器（RETRO）的高级概览。来源：https://deepmind.com/research/publications/2021/improving-language-models-by-retrieving-from-trillions-of-tokens
- en: Pathways and PaLM
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pathways和PaLM
- en: Google Research announced Pathways ([https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/)),
    a single model that could generalize across domains and tasks while being highly
    efficient. Then, Google introduced **Pathways Language Model** (**PaLM**), a 540-billion
    parameter, dense decoder-only transformer model, which enabled us to efficiently
    train a single model across multiple TPU v4 Pods. Google evaluated PaLM on hundreds
    of language understanding and generation tasks and found that it achieves state-of-the-art
    performance across most tasks, by significant margins in many cases (see [https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.xhtml?m=1](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.xhtml?m=1)).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: Google Research宣布了Pathways（[https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/)），这是一个能够跨领域和任务泛化的单一模型，同时具有高度的效率。随后，Google推出了**Pathways语言模型**（**PaLM**），这是一个包含5400亿参数、密集型解码器-only的变压器模型，使我们能够在多个TPU
    v4 Pods上高效地训练单一模型。Google对PaLM进行了数百项语言理解和生成任务的评估，发现它在大多数任务中都能实现最先进的性能，并且在许多情况下具有显著的优势（见[https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.xhtml?m=1](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.xhtml?m=1)）。
- en: Implementation
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: In this section, we will go through a few tasks using transformers.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过一些使用变压器的任务。
- en: 'Transformer reference implementation: An example of translation'
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变压器参考实现：一个翻译示例
- en: In this section, we will briefly review a transformer reference implementation
    available at [https://www.tensorflow.org/text/tutorials/transformer](https://www.tensorflow.org/text/tutorials/transformer)
    and specifically, we will use the opportunity to run the code in a Google Colab.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要回顾一个可以在[https://www.tensorflow.org/text/tutorials/transformer](https://www.tensorflow.org/text/tutorials/transformer)上找到的变压器参考实现，具体来说，我们将利用这个机会在Google
    Colab中运行代码。
- en: Not everyone realizes the number of GPUs it takes to train a transformer. Luckily,
    you can play with resources available for free at [https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 不是每个人都意识到训练变压器需要的GPU数量。幸运的是，你可以在[https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb)上免费使用可用资源。
- en: 'Note that implementing transformers from scratch is probably not the best choice
    unless you need to realize some very specific customization or you are interested
    in core research. If you are not interested in learning the internals, then you
    can skip to the next section. Our tutorial is licensed under the Creative Commons
    Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License.
    The specific task we are going to perform is translating from Portuguese to English.
    Let’s have a look at the code, step by step:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，除非你需要实现一些非常具体的定制，或者你对核心研究感兴趣，否则从零开始实现变压器可能不是最佳选择。如果你对了解内部实现不感兴趣，可以跳到下一节。我们的教程使用创意共享署名4.0许可证授权，代码示例使用Apache
    2.0许可证授权。我们将要执行的具体任务是将葡萄牙语翻译成英语。让我们一步一步地看一下代码：
- en: 'First, let’s install datasets and import the right libraries. Note that the
    Colab available online is apparently missing the line `import tensorflow_text`,
    which is, however, added here:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们安装数据集并导入正确的库。请注意，在线的Colab似乎缺少`import tensorflow_text`这一行，但在这里已添加：
- en: '[PRE1]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, load the Portuguese to English dataset:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，加载葡萄牙语到英语的数据集：
- en: '[PRE2]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let’s convert text to sequences of token IDs, which are used as indices
    into an embedding:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将文本转换为标记ID的序列，这些标记ID用作嵌入的索引：
- en: '[PRE3]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s see the tokenized IDs and tokenized words:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看一下标记化的ID和标记化的单词：
- en: '[PRE4]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now let’s create an input pipeline. First, we define a function to drop the
    examples longer than `MAX_TOKENS`. Second, we define a function that tokenizes
    the batches of raw text. Third, we create the batches:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们创建一个输入管道。首先，我们定义一个函数来丢弃超过 `MAX_TOKENS` 长度的示例。其次，我们定义一个函数来对原始文本的批次进行标记化。第三，我们创建批次：
- en: '[PRE10]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we add positional encoding, forcing tokens to be closer to each other based
    on the similarity of their meaning and their position in the sentence, in the
    d-dimensional embedding space:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们添加位置编码，强制根据词汇的含义相似度和它们在句子中的位置，使得令牌彼此更接近，位于 d 维嵌入空间中：
- en: '[PRE11]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s now focus on the masking process. The look-ahead mask is used to mask
    the future tokens in a sequence, with the mask indicating which entries should
    not be used. For instance, to predict the third token, only the first and second
    tokens will be used, and to predict the fourth token, only the first, second,
    and third tokens will be used, and so on:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们集中关注掩码过程。前瞻掩码用于掩蔽序列中的未来令牌，掩码指示哪些条目不应被使用。例如，为了预测第三个令牌，只会使用第一个和第二个令牌，而为了预测第四个令牌，只会使用第一个、第二个和第三个令牌，依此类推：
- en: '[PRE12]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We are getting closer and closer to the essence of transformers. Let’s define
    the attention function as a scaled dot-product:'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们离变换器的本质越来越近。让我们将注意力函数定义为一个缩放的点积：
- en: '[PRE13]'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that the attention is defined, we need to implement the multi-head mechanism.
    There are three parts: linear layers, scaled dot-product attention, and the final
    linear layer (see *Figure 6.14*):![Diagram  Description automatically generated](img/B18331_06_14.png)'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在注意力已定义，我们需要实现多头机制。它有三个部分：线性层、缩放点积注意力和最终的线性层（见 *图 6.14*）：![图示  自动生成描述](img/B18331_06_14.png)
- en: 'Figure 6.14: Multi-head attention'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.14：多头注意力
- en: '[PRE14]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we can define a point-wise feedforward network that consists of two fully
    connected layers with a ReLU activation in between:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以定义一个逐点前馈网络，它由两个完全连接的层组成，中间有一个 ReLU 激活函数：
- en: '[PRE15]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can now concentrate on defining the encoder and decoder parts as described
    in *Figure 6.15*. Remember that the traditional transformer takes the input sentence
    through *N* encoder layers, while the decoder uses the encoder output and its
    own input (self-attention) to predict the next word. Each encoder layer has sublayers
    made by multi-head attention (with a padding mask) and then point-wise feedforward
    networks. Each sublayer uses a residual connection to contain the problem of vanishing
    gradient, and a normalization layer:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以集中精力定义如 *图 6.15* 所示的编码器和解码器部分。记住，传统的变换器通过 *N* 个编码器层处理输入句子，而解码器使用编码器输出和它自己的输入（自注意力）来预测下一个词。每个编码器层都有由多头注意力（带填充掩码）和逐点前馈网络组成的子层。每个子层使用残差连接来解决梯度消失问题，并且有一个归一化层：
- en: '[PRE16]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Each decoder layer is made of sublayers. First, a masked multi-head attention
    (with a look-ahead mask and padding mask). Then, a multi-head attention (with
    a padding mask), V (value), and K (key) receive the encoder output as inputs.
    Q (query) receives the output from the masked multi-head attention sublayer and,
    finally, the point-wise feedforward networks:'
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个解码器层由多个子层组成。首先是一个带掩蔽的多头注意力（带前瞻掩码和填充掩码）。然后是一个多头注意力（带填充掩码），V（值）和 K（键）接收编码器输出作为输入。Q（查询）接收来自掩蔽多头注意力子层的输出，最后是逐点前馈网络：
- en: '[PRE17]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now that we have defined the encoder layer, we can use it to define the proper
    encoder. This consists of three stages: input embedding, positional encoding,
    and *N* encoder layers:'
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经定义了编码器层，可以用它来定义合适的编码器。编码器由三个阶段组成：输入嵌入、位置编码和 *N* 个编码器层：
- en: '[PRE18]'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can now focus our attention on the decoder itself. It consists of output
    embedding, positional encoding, and *N* decoder layers:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以专注于解码器本身。解码器由输出嵌入、位置编码和 *N* 个解码器层组成：
- en: '[PRE19]'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now that we have defined the encoder and decoder, we can now turn our attention
    to the transformer itself, which is composed of an encoder, a decoder, and a final
    linear layer (see *Figure 6.15*):'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经定义了编码器和解码器，我们可以把注意力转向变换器本身，它由编码器、解码器和最终的线性层组成（见 *图 6.15*）：
- en: '[PRE20]'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![Diagram  Description automatically generated](img/B18331_06_15.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![图示  自动生成描述](img/B18331_06_15.png)'
- en: 'Figure 6.15: The traditional transformer'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15：传统的变换器
- en: 'We are almost done. We just need to define hyperparameters and the optimizer,
    using exactly the same settings as the seminal paper, and the loss function:'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们快完成了。我们只需要定义超参数和优化器，使用与开创性论文中完全相同的设置，并定义损失函数：
- en: '[PRE21]'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Time to define the transformer. Let’s see the code:'
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是定义变换器的时候了。让我们看看代码：
- en: '[PRE22]'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let’s also define the checkpoints with the following code:'
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也用以下代码定义检查点：
- en: '[PRE23]'
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Remember that the transformer is autoregressive. The current output is used
    to predict what will happen next. We use a look-ahead mask, to prevent the model
    from peeking at the expected output. We are now ready to define `train_step`:'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记住，变换器是自回归的。当前的输出被用来预测接下来会发生什么。我们使用前瞻掩码，以防止模型看到预期的输出。我们现在准备定义 `train_step`：
- en: '[PRE24]'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'After running the training step in Colab, we get the following situation:'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 Colab 中运行训练步骤后，我们得到了以下情况：
- en: '[PRE25]'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We are now ready for translation. The following steps are used to translate:'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在准备进行翻译。以下步骤用于翻译：
- en: Encode the input sentence using the Portuguese tokenizer (`tokenizers.pt`).
  id: totrans-413
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用葡萄牙语分词器（`tokenizers.pt`）对输入句子进行编码。
- en: The decoder input is initialized to the [START] token.
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器输入初始化为 [START] token。
- en: Calculate the padding masks and the look-ahead masks.
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算填充掩码（padding masks）和前瞻掩码（look-ahead masks）。
- en: The decoder then outputs the predictions by looking at the encoder output and
    its own output (self-attention).
  id: totrans-416
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，解码器通过查看编码器输出和自身输出（自注意力）来输出预测结果。
- en: 'Concatenate the predicted token to the decoder input and pass it to the decoder:'
  id: totrans-417
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将预测的 token 拼接到解码器输入中，并传递给解码器：
- en: '[PRE26]'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let’s call the translator on a sample sentence with this code snippet:'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们用以下代码片段对示例句子调用翻译器：
- en: '[PRE27]'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Getting as the result:'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得到的结果是：
- en: '[PRE28]'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In this detailed analysis, we have discussed how a traditional transformer is
    implemented taking positional encoding, multi-head attention, and masking into
    account. The analyzed code is at [https://www.tensorflow.org/text/tutorials/transformer](https://www.tensorflow.org/text/tutorials/transformer).
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在这份详细分析中，我们讨论了如何实现传统的变换器，考虑到位置编码、多头注意力和掩码。分析的代码见 [https://www.tensorflow.org/text/tutorials/transformer](https://www.tensorflow.org/text/tutorials/transformer)。
- en: Next, we will discuss how to use transformers making use of higher-level libraries.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何利用更高层次的库来使用变换器。
- en: Hugging Face
  id: totrans-425
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face
- en: 'As discussed, implementing transformers from scratch is probably not the best
    choice unless you need to realize some very specific customization, or you are
    interested in core research. This is useful if you want to understand the internal
    details of a transformer architecture, or perhaps modify the transformer architecture
    to produce a new variant. Nowadays, there are very good libraries providing high-quality
    solutions. One of them is Hugging Face, which provides some efficient tools. Hugging
    Face is built around the idea of commercializing its open source transformers
    library. Let’s see why the library became so popular:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前讨论的那样，除非你需要实现一些非常特定的定制化，或者对核心研究感兴趣，否则从头实现变换器（transformer）可能不是最佳选择。如果你想理解变换器架构的内部细节，或者希望修改变换器架构以生成新的变种，这样做是有用的。如今，有很多优秀的库提供高质量的解决方案，其中之一就是
    Hugging Face，它提供了一些高效的工具。Hugging Face 的构建围绕着将其开源的变换器库商业化的想法展开。让我们来看一下为什么这个库变得如此流行：
- en: Hugging Face provides a common API to handle many transformer architectures.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face 提供了一个通用的 API 来处理多种变换器架构。
- en: It not only provides the base model, but models with different types of “head”
    to handle specific tasks (for example, for the BERT architecture it provides `TFBertModel`,
    and the `TFBertForSequenceClassification` for tasks like sentiment analysis, `TFBertForTokenClassification`
    for tasks like named entity recognition, and `TFBertForQuestionAnswering` for
    Q and A, among others).
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不仅提供基础模型，还提供具有不同类型“头”的模型，用于处理特定任务（例如，对于 BERT 架构，它提供 `TFBertModel`，以及用于情感分析等任务的
    `TFBertForSequenceClassification`，用于命名实体识别等任务的 `TFBertForTokenClassification`，以及用于问答的
    `TFBertForQuestionAnswering` 等）。
- en: You can also create your own network for a specific task quite easily by using
    the pretrained weights provided here, for example, by using `TFBertForPreTraining`.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你还可以通过使用这里提供的预训练权重，例如使用 `TFBertForPreTraining`，来轻松创建一个用于特定任务的网络。
- en: In addition to the `pipeline()` method in the next subsection, we can also define
    a model in the regular way and use `fit()` to train it and `predict()` to make
    inferences against it, just like a normal TF model (PyTorch also has the Trainer
    interface). We will see an example later on in this chapter.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了下一小节中的 `pipeline()` 方法外，我们还可以按常规方式定义模型，并使用 `fit()` 来训练它，使用 `predict()` 来进行推理，就像普通的
    TF 模型一样（PyTorch 也有 Trainer 接口）。我们将在本章后面看到一个示例。
- en: Now, let’s check some examples of how to use Hugging Face.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一些使用 Hugging Face 的示例。
- en: Generating text
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成文本
- en: 'In this section, we are going to use GPT-2 for natural language generation,
    a software process for producing natural language outputs. Let’s start from the
    beginning by installing the Hugging Face library:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将使用GPT-2进行自然语言生成，这是一个生成自然语言输出的软件过程。让我们从安装Hugging Face库开始：
- en: 'The first step is to create a dedicated virtual environment, where we can install
    the transformer library. In my case, I use the library for TensorFlow 2.0:'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是创建一个专门的虚拟环境，在其中安装transformer库。在我的例子中，我使用的是TensorFlow 2.0的库：
- en: '[PRE29]'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then let’s verify that everything is working correctly by downloading a pretrained
    model used for sentiment analysis:'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后让我们通过下载一个用于情感分析的预训练模型来验证一切是否正常工作：
- en: '[PRE30]'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Since the expected sentiment should be very positive, we shall see something
    like the following:'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于期望的情感应该是非常积极的，我们将看到如下的内容：
- en: '[PRE31]'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now let’s focus on generating text with GPT-2:'
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们专注于使用GPT-2生成文本：
- en: '[PRE32]'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'You should see something like the following:'
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该看到如下的内容：
- en: '[PRE33]'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let’s pass some text to the generator and see what the result is. The first
    sentence is extracted from Tolkien’s work, the second from Einstein’s theories,
    and the third one comes from “Harry Potter”:'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们给生成器传递一些文本，看看结果如何。第一句话来自托尔金的作品，第二句来自爱因斯坦的理论，第三句来自《哈利·波特》：
- en: '[PRE34]'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Pretty easy, isn’t it?
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 很简单，不是吗？
- en: Autoselecting a model and autotokenization
  id: totrans-452
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动选择模型和自动标记化
- en: 'Hugging Face does a great job of helping the developer to automate as many
    steps as possible. Let’s see some examples:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face在帮助开发者自动化尽可能多的步骤方面做得非常出色。让我们看一些例子：
- en: 'You can easily import a pretrained model among the several dozen available.
    A complete list of available models is here: [https://huggingface.co/docs/transformers/model_doc/auto](https://huggingface.co/docs/transformers/model_doc/auto):'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以轻松地从几十种可用的预训练模型中导入一个。所有可用模型的完整列表在这里：[https://huggingface.co/docs/transformers/model_doc/auto](https://huggingface.co/docs/transformers/model_doc/auto)：
- en: '[PRE40]'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: You should probably train this model on a downstream task to use it for predictions
    and inference.
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可能应该在下游任务上训练此模型，以便将其用于预测和推理。
- en: 'You can use `AutoTokenizer` to transform words into tokens used by the models:'
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用`AutoTokenizer`将单词转换为模型使用的标记：
- en: '[PRE42]'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Named entity recognition
  id: totrans-461
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: '**Named Entity Recognition** (**NER**) is a classical NLP task. According to
    Wikipedia, named entity recognition – also known as (named) entity identification,
    entity chunking, and entity extraction – is a subtask of information extraction
    that seeks to locate and classify named entities mentioned in unstructured text
    into predefined categories such as person names, organizations, locations, medical
    codes, time expressions, quantities, monetary values, and percentages, among others.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '**命名实体识别**（**NER**）是一个经典的NLP任务。根据维基百科，命名实体识别——也称为（命名的）实体识别、实体分块和实体提取——是信息提取的一个子任务，旨在定位和分类未结构化文本中提到的命名实体，并将其分为预定义的类别，如人名、组织、地点、医学编码、时间表达、数量、货币值和百分比等。'
- en: 'Let’s see how easily this task can be performed with Hugging Face:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何利用Hugging Face轻松地执行此任务：
- en: 'First of all, let’s create a NER pipeline:'
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个NER流水线：
- en: '[PRE44]'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'You will be able to see something like the following, where the entities are
    recognized:'
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将能够看到如下的内容，其中实体已被识别：
- en: '[PRE45]'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Named entity recognition can understand nine different classes:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别可以理解九种不同的类别：
- en: '`O`: Outside of a named entity.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`O`：命名实体之外。'
- en: '`B-MIS`: Beginning of a miscellaneous entity right after another miscellaneous
    entity.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`B-MIS`：紧接着另一个杂项实体的杂项实体开始。'
- en: '`I-MIS`: Miscellaneous entity.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`I-MIS`：杂项实体。'
- en: '`B-PER`: Beginning of a person’s name right after another person’s name.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`B-PER`：紧接着另一个人名的人的名字开始。'
- en: '`I-PER`: A person’s name.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`I-PER`：一个人的名字。'
- en: '`B-ORG`: Beginning of an organization right after another organization.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`B-ORG`：紧接着另一个组织的组织开始。'
- en: '`I-ORG`: Organization.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`I-ORG`：组织。'
- en: '`B-LOC`: Beginning of a location right after another location.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`B-LOC`：紧接着另一个地点的地点开始。'
- en: '`I-LOC`: Location.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`I-LOC`：地点。'
- en: These entities are defined in the CoNLL-2003 dataset typically used for this
    task and automatically selected by Hugging Face.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实体在通常用于此任务的CoNLL-2003数据集中定义，并由Hugging Face自动选择。
- en: Summarization
  id: totrans-479
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: 'Let’s now turn our attention to summarization, meaning the task of expressing
    the most important facts or ideas about something or someone in a short and clear
    form. Hugging Face makes it incredibly easy to use the T5 model as default for
    this task. Let’s see the code:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们转向摘要任务，即以简洁明了的形式表达某些事物或某人最重要的事实或想法。Hugging Face 让使用 T5 模型作为默认模型变得非常简单。让我们来看一下代码：
- en: 'First of all, let’s create a summarization pipeline using the default T5 small
    model:'
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们使用默认的 T5 small 模型创建一个摘要管道：
- en: '[PRE46]'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'As a result, we will see something similar to the following:'
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果，我们将看到类似以下内容：
- en: '[PRE47]'
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Suppose that you want to change to a different model. That’s extremely simple
    as you only need to change one parameter:'
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你想换一个不同的模型。其实这非常简单，你只需要更改一个参数：
- en: '[PRE48]'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'As a result, we can see something like the following:'
  id: totrans-487
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果，我们可以看到类似以下内容：
- en: '[PRE49]'
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Fine-tuning
  id: totrans-489
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调
- en: One common usage pattern for transformers is to use a pretrained LLM and then
    fine-tune the model for specific downstream tasks. Of course, the fine-tuning
    steps will take place on your own dataset, while pretraining is performed on very
    large datasets. The advantages of this two-step strategy are in terms of both
    saving computation costs and in reducing the carbon footprint. Plus, fine-tuning
    allows you to use state-of-the-art models without having to train one from scratch.
    Let’s see how to fine-tune a model with TF. This example is available at [https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training),
    where the pretrained model used is `bert-base-cased`, which is fine-tuned on the
    “Yelp Reviews” dataset (available at [https://huggingface.co/datasets/yelp_review_full](https://huggingface.co/datasets/yelp_review_full)).
    Let’s see the code from [https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training)
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 transformers，一个常见的使用模式是先使用预训练的大型语言模型（LLM），然后对该模型进行微调以适应特定的下游任务。当然，微调步骤是在你自己的数据集上进行的，而预训练则是在非常大的数据集上完成的。这种两步策略的优势在于节省计算成本并减少碳足迹。此外，微调使得你可以使用最先进的模型，而无需从头开始训练一个。让我们看看如何使用
    TF 进行模型微调。这个例子可以在 [https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training)
    找到，其中使用的预训练模型是 `bert-base-cased`，该模型在“Yelp 评论”数据集上进行了微调（数据集可以在 [https://huggingface.co/datasets/yelp_review_full](https://huggingface.co/datasets/yelp_review_full)
    找到）。让我们从 [https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training)
    查看代码。
- en: 'First, let’s load and tokenize the Yelp dataset:'
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们加载并标记化 Yelp 数据集：
- en: '[PRE50]'
  id: totrans-492
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Then let’s convert them to TF format datasets:'
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将其转换为 TF 格式的数据集：
- en: '[PRE51]'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now, we can use `TFAutoModelForSequenceClassification`, specifically selecting
    `bert-base-cased`:'
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用 `TFAutoModelForSequenceClassification`，并特别选择 `bert-base-cased`：
- en: '[PRE52]'
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Finally, the fine-tuning is simply using the standard way to train a model
    used in Keras/TF 2.0 by compiling the model and then using `fit` on it:'
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，微调就是使用 Keras/TF 2.0 中标准的训练模型方法，通过编译模型，然后使用 `fit` 进行训练：
- en: '[PRE53]'
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'If you want, you can test the code on a public Colab notebook (available at
    [https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training)).
    If you run the code yourself, you should be able to see something similar to *Figure
    6.16*:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意，可以在公开的 Colab 笔记本上测试代码（可访问 [https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training)）。如果你自己运行代码，你应该能够看到类似
    *图 6.16* 的内容：
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B18331_06_16.png)'
  id: totrans-500
  prefs: []
  type: TYPE_IMG
  zh: '![一张计算机的屏幕截图，描述自动生成，信心较高](img/B18331_06_16.png)'
- en: 'Figure 6.16: Fine-tuning BERT on a Colab notebook'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.16：在 Colab 笔记本上微调 BERT
- en: Next, we are going to introduce TFHub.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍 TFHub。
- en: TFHub
  id: totrans-503
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFHub
- en: In the previous section, we discussed how to use the Hugging Face Transformer
    library. Now, we will have a look at another library known as TFHub available
    at [https://tfhub.dev/](https://tfhub.dev/). TensorFlow Hub is a repository of
    trained machine learning models ready for fine-tuning and deployable anywhere.
    The key idea is to reuse trained models like BERT and Faster R-CNN with just a
    few lines of code.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了如何使用 Hugging Face Transformer 库。现在，我们将看看另一个名为 TFHub 的库，网址为[https://tfhub.dev/](https://tfhub.dev/)。TensorFlow
    Hub 是一个包含经过训练的机器学习模型的库，这些模型可以进行微调并部署到任何地方。其核心思想是仅需几行代码，就能重用像 BERT 和 Faster R-CNN
    这样的预训练模型。
- en: 'Using TFHub is as easy as writing a few lines of code. Let’s see a simple example
    where we load a pretrained model for computing embeddings. In this case, we use
    `nnlm-en-dim128`, a token-based text embedding trained on the English Google News
    200B corpus:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TFHub就像写几行代码一样简单。让我们来看一个简单的例子，其中我们加载一个预训练模型来计算嵌入。在这个例子中，我们使用`nnlm-en-dim128`，这是一个基于标记的文本嵌入模型，经过英语Google
    News 200B语料库的训练：
- en: '[PRE54]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now let’s see how to use BERT. This code is adapted from [https://www.tensorflow.org/hub/tutorials/bert_experts](https://www.tensorflow.org/hub/tutorials/bert_experts),
    and it is also available on Hugging Face ([https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training)):'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何使用BERT。此代码改编自[https://www.tensorflow.org/hub/tutorials/bert_experts](https://www.tensorflow.org/hub/tutorials/bert_experts)，也可以在Hugging
    Face上找到([https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training))：
- en: 'Let’s set up the environment and import useful modules:'
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们设置环境并导入一些有用的模块：
- en: '[PRE55]'
  id: totrans-509
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Let’s define a few sentences used for comparing their similarities:'
  id: totrans-510
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义几个句子，用于比较它们之间的相似度：
- en: '[PRE56]'
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Then, let’s use a pretrained BERT model available on TFHub to compute embeddings
    on the input sentences just defined. BERT’s output is the set of embeddings itself:'
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，让我们使用TFHub上的预训练BERT模型来计算输入句子的嵌入。BERT的输出就是这些嵌入本身：
- en: '[PRE57]'
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now let’s define some auxiliary functions to show the similarity among embeddings
    based on `pairwise.cosine_similarity`:'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们定义一些辅助函数，通过`pairwise.cosine_similarity`来展示嵌入之间的相似度：
- en: '[PRE58]'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The interested reader can access the Colab notebook online on the Hugging Face
    website (available at [https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training))
    and visualize a heatmap showing similarities among sentences. Overall, using LLMs
    with TFHub is pretty easy, isn’t it?
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣的读者可以在Hugging Face网站上访问Colab笔记本（可在[https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training)找到），并可视化一个显示句子之间相似度的热图。总的来说，使用TFHub与LLM一起使用真的很简单，不是吗？
- en: Evaluation
  id: totrans-517
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: Evaluating transformers involves considering multiple classes of metrics and
    understanding the cost tradeoffs among these classes. Let’s see the main ones.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 评估transformers模型需要考虑多种类别的指标，并理解这些类别之间的成本权衡。让我们来看一下主要的指标。
- en: Quality
  id: totrans-519
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 质量
- en: The quality of transformers can be measured against a number of generally available
    datasets. Let’s see the most commonly used ones.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: transformers模型的质量可以通过多个常用数据集进行衡量。让我们来看看最常用的几个数据集。
- en: GLUE
  id: totrans-521
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GLUE
- en: The **General Language Understanding Evaluation** (**GLUE**) benchmark is a
    collection of resources for training, evaluating, and analyzing natural language
    understanding systems. GLUE is available at [https://gluebenchmark.com/](https://gluebenchmark.com/).
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '**通用语言理解评估**（**GLUE**）基准是一个用于训练、评估和分析自然语言理解系统的资源集合。GLUE可以在[https://gluebenchmark.com/](https://gluebenchmark.com/)访问。'
- en: 'GLUE consists of:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: GLUE包括：
- en: A benchmark of nine sentence or sentence-pair language understanding tasks built
    on established existing datasets and selected to cover a diverse range of dataset
    sizes, text genres, and degrees of difficulty
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个建立在现有数据集上的九个句子或句子对语言理解任务的基准，选择这些数据集是为了覆盖不同的大小、文本类型和难度级别
- en: A diagnostic dataset designed to evaluate and analyze model performance with
    respect to a wide range of linguistic phenomena found in natural language
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个设计用于评估和分析模型在广泛语言现象方面的表现的诊断数据集，涵盖自然语言中常见的各种语言现象
- en: A public leaderboard for tracking performance on the benchmark and a dashboard
    for visualizing the performance of models on the diagnostic set
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于跟踪基准测试上表现的公开排行榜，以及一个可视化模型在诊断集上的表现的仪表板
- en: '*Figure 6.17* shows the GLUE dashboard from March 2022:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.17*展示了2022年3月的GLUE仪表板：'
- en: '![Graphical user interface, application, email  Description automatically generated](img/B18331_06_17.png)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，电子邮件描述自动生成](img/B18331_06_17.png)'
- en: 'Figure 6.17: GLUE dashboard'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17：GLUE仪表板
- en: SuperGLUE
  id: totrans-530
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SuperGLUE
- en: In recent years, new models and methods for pretraining and transfer learning
    have driven striking performance improvements across a range of language understanding
    tasks. The GLUE benchmark offers a single-number metric that summarizes progress
    on a diverse set of such tasks, but performance on the benchmark has recently
    come close to the level of non-expert humans, suggesting limited headroom for
    further research.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，新的预训练和迁移学习模型与方法推动了语言理解任务在表现上的显著提升。GLUE基准提供了一个单一的评分指标，用于总结在多种语言理解任务上的进展，但该基准的表现最近接近非专家人类的水平，表明进一步研究的空间有限。
- en: 'SuperGLUE is a new benchmark styled after GLUE with a new set of more difficult
    language understanding tasks, improved resources, and a new public leaderboard.
    *Figure 6.18* is the SuperGLUE leaderboard from March 2022:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: SuperGLUE是一个新基准，类似于GLUE，但提供了一组更困难的语言理解任务、改进的资源以及一个新的公开排行榜。*图6.18*是2022年3月的SuperGLUE排行榜：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B18331_06_18.png)'
  id: totrans-533
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件 描述自动生成](img/B18331_06_18.png)'
- en: 'Figure 6.18: SuperGLUE leaderboard'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18：SuperGLUE排行榜
- en: SQuAD
  id: totrans-535
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SQuAD
- en: SQuAD is a dataset used to evaluate questions and answers, [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/).
    Specifically, the **Stanford Question Answering Dataset** (**SQuAD**) is a reading
    comprehension dataset, consisting of questions posed by crowdworkers on a set
    of Wikipedia articles, where the answer to every question is a segment of text,
    or span, from the corresponding reading passage, otherwise the question might
    be unanswerable.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: SQuAD是一个用于评估问题与答案的数据集，[https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)。具体而言，**斯坦福问答数据集**（**SQuAD**）是一个阅读理解数据集，包含由群众工作者在一组维基百科文章上提出的问题，每个问题的答案是相应阅读段落中的一段文本或片段，否则该问题可能无法回答。
- en: SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable
    questions written adversarially by crowdworkers to look similar to answerable
    ones. To do well on SQuAD2.0, systems must not only answer questions when possible
    but also determine when no answer is supported by the paragraph and abstain from
    answering.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: SQuAD2.0结合了SQuAD1.1中的100,000个问题和超过50,000个由群众工作者故意编写的无法回答的问题，这些问题看起来与可以回答的问题相似。为了在SQuAD2.0中取得好成绩，系统不仅需要在可能的情况下回答问题，还必须判断何时没有答案可以支持该段落，并避免回答。
- en: RACE
  id: totrans-538
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RACE
- en: 'The **ReAding Comprehension dataset from Examinations** (**RACE**) dataset
    is a machine reading comprehension dataset consisting of 27,933 passages and 97,867
    questions from English exams, targeting Chinese students aged 12-18\. RACE consists
    of two subsets, RACE-M and RACE-H, from middle school and high school exams, respectively.
    RACE-M has 28,293 questions and RACE-H has 69,574\. Each question is associated
    with four candidate answers, one of which is correct. The data generation process
    of RACE differs from most machine reading comprehension datasets. Instead of generating
    questions and answers by heuristics or crowdsourcing, questions in RACE are specifically
    designed for testing human reading skills and are created by domain experts. RACE
    is available at [https://www.cs.cmu.edu/~glai1/data/race/](https://www.cs.cmu.edu/~glai1/data/race/).
    *Figure 6.19* shows the RACE leaderboard:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '**阅读理解数据集（Examinations）**（**RACE**）是一个机器阅读理解数据集，包含来自英语考试的27,933篇文章和97,867个问题，目标人群是12至18岁的中国学生。RACE包括两个子集，分别是来自初中和高中考试的RACE-M和RACE-H。RACE-M包含28,293个问题，RACE-H包含69,574个问题。每个问题都有四个候选答案，其中一个是正确的。RACE的数据生成过程与大多数机器阅读理解数据集不同，RACE中的问题专门设计用于测试人类阅读技能，由领域专家创建，而不是通过启发式方法或众包生成问题和答案。RACE可在[https://www.cs.cmu.edu/~glai1/data/race/](https://www.cs.cmu.edu/~glai1/data/race/)获取。*图6.19*展示了RACE排行榜：'
- en: '![Table  Description automatically generated](img/B18331_06_19.png)'
  id: totrans-540
  prefs: []
  type: TYPE_IMG
  zh: '![表格 描述自动生成](img/B18331_06_19.png)'
- en: 'Figure 6.19: RACE leaderboard'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19：RACE排行榜
- en: NLP-progress
  id: totrans-542
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NLP-progress
- en: NLP-progress is a repository, made to track progress in NLP, including the datasets
    and the current state-of-the-art models for the most common NLP tasks. The site
    aims to track the progress in NLP and gives an overview of the state-of-the-art
    models across the most common NLP tasks and their corresponding datasets. NLP-progress
    aims to cover both traditional and core NLP tasks such as dependency parsing and
    part-of-speech tagging as well as more recent ones such as reading comprehension
    and natural language inference. If you need a good starting point to find quality
    metrics for your task, then [http://nlpprogress.com/](http://nlpprogress.com/)
    is the place to start with.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: NLP-progress是一个仓库，用于跟踪自然语言处理（NLP）领域的进展，包括最常见的NLP任务的数据库和当前的最先进模型。该网站旨在跟踪NLP领域的进展，并概述最常见的NLP任务及其相应数据集中的最先进模型。NLP-progress旨在涵盖传统和核心NLP任务，如依存句法分析和词性标注，以及更近期的任务，如阅读理解和自然语言推理。如果你需要一个良好的起点来寻找适合你任务的质量指标，那么[http://nlpprogress.com/](http://nlpprogress.com/)是你开始的地方。
- en: Size
  id: totrans-544
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 尺寸
- en: 'The previous section provided an overview of quality metrics. This section
    focuses on the number of parameters used in various transformer architectures.
    As shown in *Figure 6.20*, there has been a race to increase transformers’ size
    during the last few years. Back in 2018, BERT’s size was about 340 million parameters,
    then in 2021, T5 reached 11 billion, and Megatron passed 500 billion. The very
    recent Switch Transformer has more than one trillion parameters and there is an
    expectation that soon we will see the first model with 100 trillion parameters.
    Indeed, there is evidence that the larger the model is the merrier, which can
    memorize information and generalize. However, training such large models requires
    massive computational resources:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节概述了质量指标。本节重点介绍了各种变换器架构中使用的参数数量。如*图6.20*所示，近年来，变换器的规模竞争愈加激烈。回顾2018年，BERT的规模大约为3.4亿参数，到2021年，T5的参数数达到了110亿，而Megatron突破了5000亿参数。最近的Switch
    Transformer拥有超过一万亿个参数，预计很快我们将看到第一个拥有100万亿个参数的模型。事实上，有证据表明，模型越大越有优势，它能够记忆信息并进行泛化。然而，训练如此大的模型需要巨大的计算资源：
- en: '![Table  Description automatically generated](img/B18331_06_20.png)'
  id: totrans-546
  prefs: []
  type: TYPE_IMG
  zh: '![Table  Description automatically generated](img/B18331_06_20.png)'
- en: 'Figure 6.20: Transformers’ size in billions of parameters'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20：变换器的规模（以十亿个参数为单位）
- en: Trillion parameter transformers are on their way!
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 万亿参数的变换器正在到来！
- en: 'In fact, the paper [https://arxiv.org/pdf/1906.02243.pdf](https://arxiv.org/pdf/1906.02243.pdf)
    warns about the sustainability impact of training large models (see *Figure 6.21*)
    in terms of both cloud computing cost and CO2 emissions:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，论文[https://arxiv.org/pdf/1906.02243.pdf](https://arxiv.org/pdf/1906.02243.pdf)警告了训练大规模模型的可持续性影响（见*图6.21*），无论是在云计算成本还是二氧化碳排放方面：
- en: '![Table  Description automatically generated](img/B18331_06_21.png)'
  id: totrans-550
  prefs: []
  type: TYPE_IMG
  zh: '![Table  Description automatically generated](img/B18331_06_21.png)'
- en: 'Figure 6.21: Estimated cost of training a model in terms of CO2 emissions (lbs)
    and cloud computing cost (USD) - source: https://arxiv.org/pdf/1906.02243.pdf'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.21：训练模型的成本估算，包括二氧化碳排放（磅）和云计算成本（美元）- 来源：[https://arxiv.org/pdf/1906.02243.pdf](https://arxiv.org/pdf/1906.02243.pdf)
- en: So, size is not the only factor that enables the quality of transformers to
    improve, as larger sizes can in reality give only marginal gains and require significant
    computational resources for training.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，大小并不是唯一能提升变换器质量的因素，因为更大的模型实际上可能只会带来边际上的改进，而且训练它们需要巨大的计算资源。
- en: Larger doesn’t always mean better
  id: totrans-553
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更大并不总意味着更好
- en: At the beginning of 2022, a new trend is emerging consisting of a hybrid approach
    where large models are used together with a more traditional retrieval mechanism.
    We discussed this approach earlier in the chapter when we discussed RETRO. The
    RETRO language model implements a learning scheme based on the use of external
    memory. DeepMind claimed that RETRO (or “Retrieval Enhanced Transformer”) performs
    like a neural network 25 times its size. GPT-3 has 175 billion parameters and
    RETRO uses just seven billion of them. Of course, this requires less time, energy,
    and computing power to train.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年初，出现了一种新趋势，即采用一种混合方法，将大模型与更传统的检索机制结合使用。我们在本章前面讨论过这种方法，当时我们讨论了RETRO。RETRO语言模型实现了一种基于外部记忆使用的学习方案。DeepMind声称，RETRO（或称“检索增强型变换器”）的表现相当于一个神经网络，大小是其原始模型的25倍。GPT-3拥有1750亿个参数，而RETRO只使用了其中的70亿个。当然，这样做需要更少的时间、能量和计算能力来训练。
- en: Cost of serving
  id: totrans-555
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务成本
- en: The cost of serving a model depends on many factors and it’s difficult to estimate
    it without making reasonable assumptions. Of course, serving is a function of
    the number of parameters in the model. In addition, the number of queries submitted
    to the model for inference is another factor. Then, it’s important to consider
    whether or not a cloud provider manages the model or is served in your on-prem
    infrastructure. In this context, it might be useful to remember that MLOps (see
    [https://en.wikipedia.org/wiki/MLOps](https://en.wikipedia.org/wiki/MLOps)) is
    the process of developing a machine learning model and deploying it as a production
    system. Of course, MLOps’ best practices might be adopted to optimize the costs
    of serving.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 提供模型服务的成本取决于多个因素，没有合理的假设很难估算成本。当然，服务成本与模型中的参数数量有关。此外，提交给模型进行推理的查询次数也是一个因素。然后，重要的是考虑是否由云服务提供商管理模型，或者是否在本地基础设施中提供服务。在这种情况下，可能需要记住，MLOps（参见[https://en.wikipedia.org/wiki/MLOps](https://en.wikipedia.org/wiki/MLOps)）是开发机器学习模型并将其部署为生产系统的过程。当然，MLOps的最佳实践可能被采用来优化服务成本。
- en: In this section, we have seen some key factors used to evaluate transformers,
    namely quality, size, and cost of serving. The list is clearly not inclusive,
    and a proper evaluation will take into account what the optimal tradeoff is among
    these factors. In the next section, we will discuss optimization.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经看到评估变压器模型的几个关键因素，即质量、大小和服务成本。这个列表显然不是完整的，适当的评估会考虑这些因素之间的最佳折衷。在下一节中，我们将讨论优化。
- en: Optimization
  id: totrans-558
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化
- en: Optimizing a transformer involves building lightweight, responsive, and energy-efficient
    models. Let’s see the most common ideas adopted to optimize a model.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 优化变压器模型涉及构建轻量化、响应迅速且能效高的模型。让我们看看优化模型时最常采用的思想。
- en: Quantization
  id: totrans-560
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化
- en: The key idea behind quantization is to approximate the weights of a network
    with a smaller precision. The idea is very simple, but it works quite well in
    practice. If you are interested in knowing more, we recommend the paper *A Survey
    of Quantization Methods for Efficient Neural Network Inference*, by Amir Gholami
    et al., [https://arxiv.org/pdf/2103.13630.pdf](https://arxiv.org/pdf/2103.13630.pdf).
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 量化的核心思想是通过使用较小的精度来逼近网络的权重。这个想法非常简单，但在实际中效果很好。如果你有兴趣了解更多，我们推荐由Amir Gholami等人撰写的论文《*高效神经网络推理的量化方法综述*》，[https://arxiv.org/pdf/2103.13630.pdf](https://arxiv.org/pdf/2103.13630.pdf)。
- en: Weight pruning
  id: totrans-562
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权重剪枝
- en: The key idea behind weight pruning is to remove some connections in the network.
    Magnitude-based weight pruning tends to zero out of model weights during training
    to increase model sparsity. This simple technique has benefits both in terms of
    model size and in cost of serving, as magnitude-based weight pruning gradually
    zeroes out of model weights during the training process to achieve model sparsity.
    Sparse models are easier to compress, and we can skip the zeroes during inference
    for latency improvements.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 权重剪枝的核心思想是移除网络中的一些连接。基于幅度的权重剪枝通常在训练过程中将模型权重逐渐归零，从而增加模型的稀疏性。这种简单的技术在模型大小和服务成本方面都有好处，因为基于幅度的权重剪枝会在训练过程中逐渐归零模型权重，从而实现模型稀疏性。稀疏模型更容易压缩，而且在推理时可以跳过零值，从而提高延迟性能。
- en: 'One more time, weight pruning is about tradeoffs as it might generate some
    quality losses although normally, they are rather small. If you are interested
    to know more, please have a look at the TensorFlow guide about pruning: [https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide](https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide).'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 再说一次，权重剪枝涉及权衡，因为它可能会导致一些质量损失，尽管通常这些损失非常小。如果你有兴趣了解更多，请查看TensorFlow关于剪枝的指南：[https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide](https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide)。
- en: Distillation
  id: totrans-565
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蒸馏
- en: The key idea behind knowledge distillation is to have a small model trained
    to reproduce the behavior of a larger model. This compression technique is sometimes
    referred to as teacher-student learning. The seminal paper you should check is
    *Distilling the Knowledge in a Neural Network* by Geoffrey Hinton, Oriol Vinyals,
    and Jeff Dean, [https://arxiv.org/abs/1503.02531](https://arxiv.org/abs/1503.02531).
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏的核心思想是训练一个小模型来复制大模型的行为。这种压缩技术有时被称为师生学习。你应该查看的开创性论文是由Geoffrey Hinton、Oriol
    Vinyals和Jeff Dean撰写的《*蒸馏神经网络中的知识*》，[https://arxiv.org/abs/1503.02531](https://arxiv.org/abs/1503.02531)。
- en: 'During the last few years, we have seen a number of distilled transformers.
    For instance, DistilBERT is a small, fast, cheap, and light transformer model
    based on the BERT architecture. Knowledge distillation is performed during the
    pretraining phase to reduce the size of a BERT model by 40%. Hugging Face has
    some ready-to-use Python scripts for distilling seq2seq T5 models available online
    at [https://github.com/huggingface/transformers/tree/master/examples/research_projects/seq2seq-distillation](https://github.com/huggingface/transformers/tree/master/examples/research_projects/seq2seq-distillation).
    Using the script is quite intuitive:'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年里，我们看到了许多蒸馏后的变换器。例如，DistilBERT 是一个基于 BERT 架构的小型、快速、便宜且轻量的变换器模型。知识蒸馏在预训练阶段进行，以减少
    BERT 模型的大小 40%。Hugging Face 提供了一些现成的 Python 脚本，用于蒸馏 seq2seq T5 模型，脚本可在[https://github.com/huggingface/transformers/tree/master/examples/research_projects/seq2seq-distillation](https://github.com/huggingface/transformers/tree/master/examples/research_projects/seq2seq-distillation)找到。使用该脚本非常直观：
- en: '[PRE59]'
  id: totrans-568
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: In this section, we have discussed a few techniques used to optimize transformers,
    namely quantization, weight pruning, and distillation. In the next section, we
    will discuss common pitfalls for transformers.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了优化变换器的一些技术，具体包括量化、权重修剪和蒸馏。接下来，我们将讨论变换器常见的陷阱。
- en: 'Common pitfalls: dos and don’ts'
  id: totrans-570
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见陷阱：做与不做
- en: In this section, we will give five dos and a few don’ts that are typically recommended
    when dealing with transformers.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将提供五个必做事项和一些典型的禁止事项，这些建议通常在处理变换器时会被推荐。
- en: Dos
  id: totrans-572
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 必做事项
- en: 'Let’s start with recommended best practices:'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从推荐的最佳实践开始：
- en: '**Do use pretrained large models.** Today, it is almost always convenient to
    start from an already available pretrained model such as T5, instead of training
    your transformer from scratch. If you use a pretrained model, you for sure stand
    on the giant’s shoulders; think about it!'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用预训练的大型模型。** 如今，几乎总是从一个已预训练的模型（如 T5）开始比从头训练变换器更为方便。如果你使用一个预训练模型，肯定是在“巨人的肩膀”上站立，想一想吧！'
- en: '**Do start with few-shot learning.** When you start working with transformers,
    it’s always a good idea to start with a pretrained model and then perform a lightweight
    few-shot learning step. Generally, this would improve the quality of results without
    high computational costs.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确实要从少量样本学习开始。** 当你开始使用变换器时，通常从一个预训练模型开始，然后进行轻量的少量样本学习是一个不错的选择。通常，这样可以在不产生高计算成本的情况下提高结果的质量。'
- en: '**Do use fine-tuning on your domain data and on your customer data.** After
    playing with pretraining models and few-shot learning, you might consider doing
    a proper fine-tuning on your own proprietary data or on publicly available data
    for your domain of interest'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用领域数据和客户数据进行微调。** 在玩转预训练模型和少量样本学习之后，你可以考虑在自己的专有数据或公开的领域数据上进行适当的微调。'
- en: '**Get yourself familiar with transformers’ libraries.** Hugging Face or TFHub
    provide already available state-of-the-art implementations of almost all the known
    transformers. It might be useful to start from there unless you either have some
    very peculiar needs or are doing some innovative research work.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**熟悉变换器的库。** Hugging Face 或 TFHub 提供了几乎所有已知变换器的最先进实现。除非你有一些非常特殊的需求或者在进行创新性研究工作，否则从这些库入手会非常有用。'
- en: '**Get yourself familiar with the most commonly used evaluation metrics.** When
    you use transformers, it is ideal to take into account the tradeoff faced in terms
    of quality, size, cost of serving, and many other factors.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**熟悉最常用的评估指标。** 当你使用变换器时，理想的做法是考虑在质量、大小、服务成本以及许多其他因素之间的权衡。'
- en: Don’ts
  id: totrans-579
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不要做的事
- en: Now let’s have a look at some of the pitfalls that you should avoid!
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看一些你应该避免的陷阱！
- en: '**Do not use very large models as a starting point.** Large models come with
    a cost in terms of training and serving. You will need significant resources to
    fine-tune, and you might pay high costs for serving each query. It might be better
    to start with smaller models and understand whether they are useful for your quality
    needs.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不要使用非常大的模型作为起点。** 大型模型在训练和服务上都有一定的成本。你需要大量的资源进行微调，而且每次查询的服务成本可能也很高。最好从较小的模型开始，并了解它们是否能够满足你的质量需求。'
- en: '**Do not use unoptimized models.** Nowadays, quantization, pruning, and distillation
    are standard techniques that need to be used by any transformer system that is
    put into production.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不要使用未优化的模型。** 如今，量化、修剪和蒸馏是标准技术，任何投入生产的变换器系统都需要使用这些技术。'
- en: In this section, we have seen some of the best practices for transformers. In
    the next section, we will talk about future solutions for these architectures.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经了解了一些变换器的最佳实践。在下一节中，我们将讨论这些架构的未来解决方案。
- en: The future of transformers
  id: totrans-584
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器的未来
- en: 'Transformers found their initial applications in NLP tasks, while CNNs are
    typically used for image processing systems. Recently, transformers have started
    to be successfully used for vision processing tasks. Vision transformers compute
    relationships among pixels in various small sections of an image (for example,
    16 x 16 pixels). This approach has been proposed in the seminar paper *An Image
    is Worth 16x16 Words: Transformers for Image Recognition at Scale* by Alexey Dosovitskiy
    et al., [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929),
    to make the attention computation feasible.'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: '变换器最初应用于自然语言处理任务，而卷积神经网络（CNN）通常用于图像处理系统。最近，变换器开始成功地应用于视觉处理任务。视觉变换器计算图像中各个小区域（例如，16
    x 16 像素）之间的像素关系。这一方法在Alexey Dosovitskiy等人所写的研讨会论文*An Image is Worth 16x16 Words:
    Transformers for Image Recognition at Scale*中被提出，论文链接为[https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)，旨在使注意力计算变得可行。'
- en: '**Vision transformers** (**ViTs**) are today used for complex applications
    such as autonomous driving. Tesla’s engineers showed that their Tesla Autopilot
    uses a transformer on the multi-camera system in cars. Of course, ViTs are also
    used for more traditional computer vision tasks, including but not limited to
    image classification, object detection, video deepfake detection, image segmentation,
    anomaly detection, image synthesis, and cluster analysis. The results are frequently
    better than CNNs.'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: '**视觉变换器**（**ViTs**）如今已被用于复杂的应用，如自动驾驶。特斯拉的工程师展示了他们的特斯拉自动驾驶系统在汽车的多摄像头系统中使用了变换器。当然，ViTs也用于更传统的计算机视觉任务，包括但不限于图像分类、目标检测、视频深度伪造检测、图像分割、异常检测、图像合成和聚类分析。其结果通常优于CNN。'
- en: Another direction to consider is few-shot learning. Few-shot learning refers
    to the practice of feeding a machine learning model with a very small amount of
    training data to guide its predictions, like a few examples at inference time,
    as opposed to standard fine-tuning techniques that require a relatively large
    amount of training data for the pretrained model to adapt to the desired task
    with accuracy.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的方向是少样本学习（FSL）。少样本学习是指通过提供非常少量的训练数据来指导机器学习模型的预测，就像在推理时提供几个示例，而与标准的微调技术不同，后者需要相对大量的训练数据，才能使预训练模型适应所需任务并达到较高的准确性。
- en: So, a model trained for a specific task can be reused for completely new tasks
    with very marginal costs. For instance, suppose we train a text model to generate
    text. Then, we want to perform new tasks such as translation or summarization.
    What we do is give a few examples of translations (say with pairs of text manually
    translated), or a few examples of summarization (again a few pairs). That’s it,
    no need for retraining or fine-tuning training.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为特定任务训练的模型可以以非常低的成本被重新用于全新的任务。例如，假设我们训练了一个文本生成模型。然后，我们希望执行新的任务，如翻译或总结。我们所做的就是提供一些翻译示例（比如一对对手动翻译的文本），或者一些总结示例（同样是几对示例）。仅此而已，不需要重新训练或微调训练。
- en: Since FSL is proven to work well in a number of increasing domains, don’t be
    surprised that the training phase will be less and less relevant for future AI.
    More information can be found in this paper, *Code Generation Tools (Almost) for
    Free? A Study of Few-Shot, Pre-Trained Language Models on Code* by Patrick Bareiß,
    Beatriz Souza, Marcelo d’Amorim, and Michael Pradel. The authors propose to use
    FSL to generate programming code with CodeGen, an open source mode for program
    synthesis (see [https://github.com/salesforce/CodeGen](https://github.com/salesforce/CodeGen)).
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 由于FSL已被证明在多个不断扩展的领域中表现良好，因此不要惊讶于未来的人工智能训练阶段将变得越来越不重要。更多信息可以参考这篇论文，*Code Generation
    Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code*，作者为Patrick
    Bareiß、Beatriz Souza、Marcelo d’Amorim和Michael Pradel。作者提出使用FSL通过CodeGen生成编程代码，CodeGen是一个开源程序合成模型（见[https://github.com/salesforce/CodeGen](https://github.com/salesforce/CodeGen)）。
- en: Summary
  id: totrans-590
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed transformers, a deep learning architecture that
    has revolutionized the traditional natural language processing field. We started
    reviewing the key intuitions behind the architecture, and various categories of
    transformers together with a deep dive into the most popular models. Then, we
    focused on implementations both based on vanilla architecture and on popular libraries
    such as Hugging Face and TFHub. After that, we briefly discussed evaluation, optimization,
    and some of the best practices commonly adopted when using transformers. The last
    section was devoted to reviewing how transformers can be used to perform computer
    vision tasks, a totally different domain from NLP. That requires a careful definition
    of the attention mechanism. In the end, attention is all you need! And at the
    core of attention is nothing more than the cosine similarity between vectors.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们讨论了变压器模型，这是一种深度学习架构，已经彻底改变了传统的自然语言处理领域。我们首先回顾了架构背后的关键直觉，并介绍了各种类别的变压器以及对最流行模型的深入分析。然后，我们重点讲解了基于原始架构和流行库（如Hugging
    Face和TFHub）的实现。接着，我们简要讨论了评估、优化和在使用变压器时常见的最佳实践。最后一部分着重回顾了变压器如何应用于计算机视觉任务，这是一个与自然语言处理完全不同的领域。这需要对注意力机制进行细致的定义。最终，注意力是你所需要的一切！在注意力的核心，仅仅是向量之间的余弦相似度。
- en: The next chapter is devoted to unsupervised learning.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将专注于无监督学习。
- en: Join our book’s Discord space
  id: totrans-593
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区，结识志同道合的人，与2000多名成员一起学习，链接：[https://packt.link/keras](https://packt.link/keras)
- en: '![](img/QR_Code1831217224278819687.png)'
  id: totrans-595
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1831217224278819687.png)'
