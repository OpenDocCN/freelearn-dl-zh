- en: '*Chapter 8*: Deploying a DL Inference Pipeline at Scale'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 8 章*：在大规模环境下部署深度学习推理管道'
- en: Deploying a **deep learning** (**DL**) inference pipeline for production usage
    is both exciting and challenging. The exciting part is that, finally, the DL model
    pipeline can be used for prediction with real-world production data, which will
    provide real value to the business scenarios. However, the challenging part is
    that there are different DL model serving platforms and host environments. It
    is not easy to choose the right framework for the right model serving scenarios,
    which can minimize deployment complexity but provide the best model serving experiences
    in a scalable and cost-effective way. This chapter will cover the topics as an
    overview of different deployment scenarios and host environments, and then provide
    hands-on learning on how to deploy to different environments, including local
    and remote cloud environments using MLflow deployment tools. By the end of this
    chapter, you should be able to confidently deploy an MLflow DL inference pipeline
    to various host environments for either batching or real-time inference services.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 部署 **深度学习**（**DL**）推理管道以供生产使用既令人兴奋又具有挑战性。令人兴奋的部分是，最终深度学习模型管道可以用来对真实生产数据进行预测，为商业场景提供真正的价值。然而，具有挑战性的一点是，有不同的深度学习模型服务平台和托管环境。选择适合的框架来应对合适的模型服务场景并不容易，这需要在最小化部署复杂性的同时提供可扩展、成本效益高的最佳模型服务体验。本章将介绍不同的部署场景和托管环境的概述，然后提供实际操作，学习如何使用
    MLflow 部署工具部署到不同的环境，包括本地和远程云环境。到本章结束时，您应该能够自信地将 MLflow 深度学习推理管道部署到各种托管环境中，用于批处理或实时推理服务。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主要内容：
- en: Understanding the landscape of deployment and hosting environments
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解部署和托管环境的全貌
- en: Deploying locally for batch and web service inference
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地部署用于批处理和 Web 服务推理
- en: Deploying using Ray Serve and MLflow deployment plugins
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ray Serve 和 MLflow 部署插件进行部署
- en: Deploying to AWS SageMaker – a complete end-to-end guide
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署到 AWS SageMaker – 完整的端到端指南
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following items are required for this chapter''s learning:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章学习所需的项目：
- en: 'GitHub repository code for this chapter: [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter08](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter08).'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章的 GitHub 仓库代码：[https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter08](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter08)。
- en: 'Ray serve and `mlflow-ray-serve` plugin: [https://github.com/ray-project/mlflow-ray-serve](https://github.com/ray-project/mlflow-ray-serve).'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ray Serve 和 `mlflow-ray-serve` 插件: [https://github.com/ray-project/mlflow-ray-serve](https://github.com/ray-project/mlflow-ray-serve)。'
- en: 'AWS SageMaker: You will need to have an AWS account. You can create a free
    AWS account easily through the free signup website at [https://aws.amazon.com/free/](https://aws.amazon.com/free/).'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS SageMaker：您需要拥有一个 AWS 账户。您可以通过 [https://aws.amazon.com/free/](https://aws.amazon.com/free/)
    轻松创建一个免费的 AWS 账户。
- en: 'AWS **command-line interface** (**CLI**): [https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html).'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS **命令行界面**（**CLI**）：[https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)。
- en: 'Docker Desktop: [https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Docker Desktop: [https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/)。'
- en: Complete the example in [*Chapter 7*](B18120_07_ePub.xhtml#_idTextAnchor083),
    *Multi-Step Deep Learning Inference Pipeline,* of this book. This will give you
    a ready-to-deploy inference pipeline to use in this chapter.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完成本书中[*第 7 章*](B18120_07_ePub.xhtml#_idTextAnchor083)《多步骤深度学习推理管道》的示例。这将为您提供一个可以在本章中使用的可部署推理管道。
- en: Understanding different deployment tools and host environments
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解不同的部署工具和托管环境
- en: 'There are different deployment tools in the MLOps technology stack that have
    different target use cases and host environments for deploying different model
    inference pipelines. In [*Chapter 7*](B18120_07_ePub.xhtml#_idTextAnchor083),
    *Multi-Step Deep Learning Inference Pipeline*, we learned the different inference
    scenarios and requirements and implemented a multi-step DL inference pipeline
    that can be deployed into a model hosting/serving environment. Now, we will learn
    how to deploy such a model to a few specific model hosting and serving environments.
    This is visualized in *Figure 8.1* as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在MLOps技术栈中有不同的部署工具，针对不同的目标用例和主机环境来部署不同的模型推断管道。在[*第7章*](B18120_07_ePub.xhtml#_idTextAnchor083)，*多步骤深度学习推断管道*，我们学习了不同的推断场景和要求，并实现了一个可以部署到模型托管/服务环境中的多步骤DL推断管道。现在，我们将学习如何将这样的模型部署到几个特定的模型托管和服务环境中。这在*图8.1*中如下所示：
- en: '![Figure 8.1 – Using model deployment tools to deploy a model inference pipeline
    to'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.1 – 使用模型部署工具将模型推断管道部署到'
- en: a model hosting and serving environment
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型托管和服务环境
- en: '](img/B18120_08_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_08_01.jpg)'
- en: Figure 8.1 – Using model deployment tools to deploy a model inference pipeline
    to a model hosting and serving environment
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 使用模型部署工具将模型推断管道部署到模型托管和服务环境
- en: 'As can be seen from *Figure 8.1*, there can be different deployment tools for
    different model hosting and serving environments. Here, we list the three typical
    scenarios as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如图8.1所示，针对不同的模型托管和服务环境可能存在不同的部署工具。在这里，我们列出了三种典型的场景如下：
- en: '**Batch inference at scale**: If we want to do batch inference at a regular
    schedule, we can use the PySpark **user defined function** (**UDF**) to load an
    MLflow model flavor to do this, since we can leverage Spark''s scalable computational
    approach on a distributed cluster ([https://mlflow.org/docs/latest/models.html#export-a-python-function-model-as-an-apache-spark-udf](https://mlflow.org/docs/latest/models.html#export-a-python-function-model-as-an-apache-spark-udf)).
    We will show an example of how to do this in the next section.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规模化批量推断**：如果我们想要定期进行批量推断，我们可以使用PySpark的**用户定义函数**（**UDF**）加载一个MLflow模型格式来执行此操作，因为我们可以利用Spark在分布式集群上的可扩展计算方法（[https://mlflow.org/docs/latest/models.html#export-a-python-function-model-as-an-apache-spark-udf](https://mlflow.org/docs/latest/models.html#export-a-python-function-model-as-an-apache-spark-udf)）。我们将在下一节中展示如何做到这一点的示例。'
- en: '**Streaming inference at scale**: This usually requires an endpoint that hosts
    the **Model as a Service** (**MaaS**). There exist quite a few tools and frameworks
    for production-grade deployment and model serving. We will compare a few tools
    in this section to understand how they work and how well they integrate with MLflow
    before we start learning how to do this type of deployment in this chapter.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规模化流式推断**：这通常需要一个托管**模型即服务**（**MaaS**）的端点。存在许多用于生产级部署和模型服务的工具和框架。在我们开始学习如何在本章中进行这种类型的部署之前，我们将在本节比较几种工具，以了解它们的工作方式及其与MLflow集成的情况。'
- en: '**On-device model inference**: This is an emerging area called **TinyML**,
    which deploys ML/DL models in a resource-limited environment such as mobile, sensor,
    or edge device ([https://www.kdnuggets.com/2021/11/on-device-deep-learning-pytorch-mobile-tensorflow-lite.html](https://www.kdnuggets.com/2021/11/on-device-deep-learning-pytorch-mobile-tensorflow-lite.html)).
    Two popular frameworks are PyTorch Mobile ([https://pytorch.org/mobile/home/](https://pytorch.org/mobile/home/))
    and TensorFlow Lite ([https://www.tensorflow.org/lite](https://www.tensorflow.org/lite)).
    This is not the focus of this book. You are encouraged to check out some further
    reading for this area at the end of this chapter.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设备上的模型推断**：这是一个称为**TinyML**的新兴领域，它在资源有限的环境中部署ML/DL模型，例如移动设备、传感器或边缘设备（[https://www.kdnuggets.com/2021/11/on-device-deep-learning-pytorch-mobile-tensorflow-lite.html](https://www.kdnuggets.com/2021/11/on-device-deep-learning-pytorch-mobile-tensorflow-lite.html)）。两个流行的框架是PyTorch
    Mobile（[https://pytorch.org/mobile/home/](https://pytorch.org/mobile/home/)）和TensorFlow
    Lite（[https://www.tensorflow.org/lite](https://www.tensorflow.org/lite)）。这不是本书的重点。建议您在本章结束时查看一些进一步阅读材料。'
- en: 'Now, let''s look at what kind of tools are available for deploying model inference
    as a service, especially those tools that have support for MLflow model deployment.
    There are three types of model deployment and serving tools, as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看有哪些工具可用于将模型推断部署为服务，特别是那些支持MLflow模型部署的工具。有三种类型的模型部署和服务工具，如下所示：
- en: '**MLflow built-in model deployment**: This comes out of the box from MLflow
    releases, which includes deployments to a local web server, AWS SageMaker, and
    Azure ML. There is also a managed MLflow on Databricks that supports model serving
    in public review as of this writing, which we will not cover in this book since
    this is well presented in the official Databricks documentation (interested readers
    should look up the official documentation on this Databricks feature at this website:
    [https://docs.databricks.com/applications/mlflow/model-serving.html](https://docs.databricks.com/applications/mlflow/model-serving.html)).
    However, we will show you how to use the MLflow built-in model deployment to deploy
    to local and remote AWS SageMaker in this chapter.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLflow内置模型部署**：这是MLflow发布时自带的功能，包括部署到本地Web服务器、AWS SageMaker和Azure ML。Databricks上也有一个托管的MLflow，支持模型服务，在本书写作时处于公开审阅阶段，我们不会在本书中涵盖该内容，因为它在官方Databricks文档中已经有很好的展示（感兴趣的读者可以在此网站查阅有关该Databricks功能的官方文档：[https://docs.databricks.com/applications/mlflow/model-serving.html](https://docs.databricks.com/applications/mlflow/model-serving.html)）。不过，在本章中，我们将展示如何使用MLflow内置的模型部署功能，将模型部署到本地和远程的AWS
    SageMaker。'
- en: '`mlflow-torchserv` ([https://github.com/mlflow/mlflow-torchserve](https://github.com/mlflow/mlflow-torchserve)),
    `mlflow-ray-serve` ([https://github.com/ray-project/mlflow-ray-serve](https://github.com/ray-project/mlflow-ray-serve)),
    and `mlflow-triton-plugin` ([https://github.com/triton-inference-server/server/tree/v2.17.0/deploy/mlflow-triton-plugin](https://github.com/triton-inference-server/server/tree/v2.17.0/deploy/mlflow-triton-plugin)).
    We will show how to use the `mlflow-ray-serve` plugin for deployment in this chapter.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlflow-torchserv`（[https://github.com/mlflow/mlflow-torchserve](https://github.com/mlflow/mlflow-torchserve)），`mlflow-ray-serve`（[https://github.com/ray-project/mlflow-ray-serve](https://github.com/ray-project/mlflow-ray-serve)），以及`mlflow-triton-plugin`（[https://github.com/triton-inference-server/server/tree/v2.17.0/deploy/mlflow-triton-plugin](https://github.com/triton-inference-server/server/tree/v2.17.0/deploy/mlflow-triton-plugin)）。在本章中，我们将展示如何使用`mlflow-ray-serve`插件进行部署。'
- en: '`mlflow-ray-serve` plugin to deploy the MLflow Python model. Note that, although
    in this book we show how to use an MLflow customized plugin to deploy with a generic
    ML serve tool such as Ray Serve, it is important to note that a generic ML serve
    tool can do much more with or without an MLflow customized plugin.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`mlflow-ray-serve`插件部署MLflow Python模型。需要注意的是，尽管在本书中我们展示了如何使用MLflow自定义插件通过Ray
    Serve等通用的机器学习服务工具进行部署，但重要的是要注意，无论是否使用MLflow自定义插件，通用机器学习服务工具都能做更多的事情。
- en: Optimize DL Inference through Specialized Inference Engines
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过专门的推理引擎优化深度学习推理
- en: There are some special MLflow model flavors such as **ONNX** ([https://onnx.ai/](https://onnx.ai/))
    and **TorchScript** ([https://huggingface.co/docs/transformers/v4.17.0/en/serialization#torchscript](https://huggingface.co/docs/transformers/v4.17.0/en/serialization#torchscript))
    that are specially designed for DL model inference runtime. We can convert a DL
    model into an ONNX model flavor ([https://github.com/microsoft/onnxruntime](https://github.com/microsoft/onnxruntime))
    or a TorchScript server ([https://pytorch.org/serve/](https://pytorch.org/serve/)).
    As both ONNX and TorchScript are still evolving and are specifically designed
    for the original DL model part, but not the entire inference pipeline, we are
    not covering them in this chapter.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有一些特殊的MLflow模型格式，比如**ONNX**（[https://onnx.ai/](https://onnx.ai/)）和**TorchScript**（[https://huggingface.co/docs/transformers/v4.17.0/en/serialization#torchscript](https://huggingface.co/docs/transformers/v4.17.0/en/serialization#torchscript)），它们专门设计用于深度学习模型推理运行时。我们可以将深度学习模型转换为ONNX模型格式（[https://github.com/microsoft/onnxruntime](https://github.com/microsoft/onnxruntime)）或TorchScript服务器（[https://pytorch.org/serve/](https://pytorch.org/serve/)）。由于ONNX和TorchScript仍在发展中，并且它们是专门为原始深度学习模型部分设计的，而不是整个推理管道，因此我们在本章中不会涵盖它们。
- en: Now that we have a good understanding of the varieties of the deployment tools
    and model serving frameworks, let's learn how to do the deployment in the following
    sections with concrete examples.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经很好地理解了各种部署工具和模型服务框架，接下来让我们通过具体示例学习如何进行部署。
- en: Deploying locally for batch and web service inference
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地部署用于批量推理和Web服务推理
- en: 'For development and testing purposes, we usually need to deploy our model locally
    to verify it works as expected. Let''s see how to do it for two scenarios: batch
    inference and web service inference.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发和测试过程中，我们通常需要将模型本地部署以验证其是否按预期工作。我们来看看如何在两种场景下进行部署：批量推理和Web服务推理。
- en: Batch inference
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量推理
- en: 'For batch inference, follow these instructions:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于批量推理，请按照以下说明操作：
- en: 'Make sure you have completed [*Chapter 7*](B18120_07_ePub.xhtml#_idTextAnchor083),
    *Multi-Step Deep Learning Inference Pipeline*. This will produce an MLflow `pyfunc`
    DL inference model pipeline URI that can be loaded using standard MLflow Python
    functions. The logged model can be uniquely located by the `run_id` and model
    name as follows:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请确保您已完成[*第7章*](B18120_07_ePub.xhtml#_idTextAnchor083)，*多步骤深度学习推理管道*。这将生成一个MLflow
    `pyfunc`深度学习推理模型管道URI，可以通过标准的MLflow Python函数加载。已记录的模型可以通过`run_id`和模型名称唯一定位，如下所示：
- en: '[PRE0]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The model can also be identified by the model name and version number using
    the model registry as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 模型还可以通过模型注册表中的模型名称和版本号进行识别，如下所示：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Follow the instructions under the *Batch inference at-scale using PySpark UDF
    function* section of this `README.md` file ([https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/README.md](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/README.md))
    to set up the local virtual environment, a full-fledged MLflow tracking server,
    and a few environment variables so that we can execute the code on your local
    environment.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照`README.md`文件中*使用PySpark UDF函数进行批量推理*部分的说明（[https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/README.md](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/README.md)），设置本地虚拟环境、完整的MLflow跟踪服务器和一些环境变量，以便我们能够在本地环境中执行代码。
- en: 'Load the model with the MLflow `mlflow.pyfunc.spark_udf` API to create a PySpark
    UDF function as follows. You may want to check out the `batch_inference.py` file
    from GitHub to follow through ([https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/batch/batch_inference.py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/batch/batch_inference.py)):'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MLflow的`mlflow.pyfunc.spark_udf` API加载模型，以创建一个PySpark UDF函数，如下所示。您可能需要查看GitHub上的`batch_inference.py`文件来跟进：[https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/batch/batch_inference.py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/batch/batch_inference.py)：
- en: '[PRE2]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This will wrap the inference pipeline as a PySpark UDF function with a return
    result type of `String`. This is because our model inference pipeline has a model
    signature requiring the output as a `string` type column.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把推理管道封装为一个返回结果类型为`String`的PySpark UDF函数。这是因为我们的模型推理管道具有一个模型签名，要求输出为`string`类型的列。
- en: 'Now, we can apply the PySpark UDF function to the input DataFrame. Note that
    the input DataFrame must have a `text` column with a `string` data type since
    that''s what the model signature requires:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以将PySpark UDF函数应用于输入的DataFrame。请注意，输入的DataFrame必须包含一个`text`列，并且该列的数据类型必须为`string`，因为这是模型签名的要求：
- en: '[PRE3]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Because our model inference pipeline has defined a model signature, we don't
    need to specify any column parameters if it finds the `text` column in the input
    DataFrame, which is `df` in this example. Note that we can read a large volume
    of data using Spark's `read` API, which supports different data format reading,
    such as CSV, JSON, Parquet, and many more. In our example, we read the `test.csv`
    file from the IMDB dataset. This will leverage Spark's powerful distributed computation
    on a cluster if we have a large volume of data. This enables us to do batch inference
    at scale effortlessly.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的模型推理管道已经定义了模型签名，所以如果输入的DataFrame中包含`text`列（在本示例中是`df`），我们就不需要指定任何列参数。请注意，我们可以使用Spark的`read`
    API读取大量数据，支持多种数据格式，如CSV、JSON、Parquet等。在我们的示例中，我们从IMDB数据集读取了`test.csv`文件。如果数据量较大，这将利用Spark强大的分布式计算在集群上执行。这使得我们可以轻松地进行大规模的批量推理。
- en: 'To run the batch inference code from end to end, you should check out the complete
    code provided in the repository at this location: [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/batch/batch_inference.py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/batch/batch_inference.py).
    Make sure you replace the `logged_model` variable with your own `run_id` and model
    name or the registered model name and version before you run the following command
    in the `batch` folder:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要从头到尾运行批量推理代码，您应该查看存储库中提供的完整代码，地址为：[https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/batch/batch_inference.py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/batch/batch_inference.py)。确保在`batch`文件夹中运行以下命令之前，将`logged_model`变量替换为您自己的`run_id`和模型名称，或注册的模型名称和版本：
- en: '[PRE4]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You should see the output in *Figure 8.2* on the screen:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该能在屏幕上看到*图 8.2*中的输出：
- en: '![Figure 8.2 – Batch inference using PySpark UDF function'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.2 – 使用 PySpark UDF 函数进行批量推理'
- en: '](img/B18120_08_02.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_08_02.jpg)'
- en: Figure 8.2 – Batch inference using PySpark UDF function
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 使用 PySpark UDF 函数进行批量推理
- en: As can be seen from *Figure 8.2*, the multi-step inference pipeline we loaded
    worked correctly and even detected non-English texts and duplicates, although
    the language detector probably produced some false positives. The output is a
    two-column DataFrame where the JSON response of the model prediction is saved
    in the `predictions` column. Note that you can use the same code provided in `batch_inference.py`
    in a Databricks notebook and process a very large volume of input data with a
    Spark cluster by changing the input data and the logged model location.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 8.2*中可以看到，我们加载的多步推理管道工作正常，甚至能够检测到非英文文本和重复内容，尽管语言检测器可能会产生一些误报。输出是一个两列的 DataFrame，其中模型预测的
    JSON 响应保存在 `predictions` 列中。请注意，你可以在 Databricks notebook 中使用 `batch_inference.py`
    中提供的相同代码，通过更改输入数据和已记录模型的位置，利用 Spark 集群处理大量的输入数据。
- en: Now that we know how to do batch inference at scale, let's see how to deploy
    to a local web service for the same model inference pipeline.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道如何进行大规模的批量推理，让我们来看看如何将相同的模型推理管道部署到本地 web 服务中。
- en: Model as a web service
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型作为 web 服务
- en: We can deploy the same logged model inference pipeline to a web service locally
    and have an endpoint that accepts HTTP requests with an HTTP response.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将相同的已记录模型推理管道部署到本地的 web 服务中，并拥有一个接受 HTTP 请求并返回 HTTP 响应的端点。
- en: 'The local deployment is quite straightforward with just one command line. We
    can deploy a logged model or a registered model using the model URI as in the
    previous batch inference, as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本地部署非常简单，只需要一条命令。我们可以使用模型 URI 来部署已记录的模型或注册的模型，就像之前的批量推理一样，具体如下：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You should be able to see the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能够看到如下内容：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will create the conda environment using the logged model so that it will
    have all the dependencies to run. After the conda environment is created, you
    should see the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用已记录的模型创建 conda 环境，确保其拥有运行所需的所有依赖项。创建完 conda 环境后，你应该会看到如下内容：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, the model is deployed as a web service and ready to accept HTTP requests
    for model prediction. Open a different Terminal window and type the following
    command to invoke the model web service to get a prediction response:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，模型已经作为 web 服务部署，并准备好接受 HTTP 请求进行模型预测。打开一个新的终端窗口，输入以下命令调用模型 web 服务来获取预测响应：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can see the following prediction response immediately:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以立即看到如下预测响应：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If you have followed the steps so far and saw the prediction results, you should
    feel very proud that you just deployed a DL model inference pipeline into a local
    web service! This is great for testing and debugging, and the behavior of the
    model will not change on a production web server, so we should make sure it works
    on a local web server.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经按照步骤操作并看到预测结果，你应该为自己感到非常骄傲，因为你刚刚将一个深度学习模型推理管道部署到了本地 web 服务中！这对于测试和调试非常有用，而且在生产环境的
    web 服务器上模型的行为不会发生变化，所以我们应该确保它在本地 web 服务器上正常工作。
- en: So far, we have learned how to use the built-in MLflow deployment tool. Next,
    we will see how to use a generic deployment tool, Ray Serve, to deploy an MLflow
    inference pipeline.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何使用内置的 MLflow 部署工具。接下来，我们将学习如何使用通用的部署工具 Ray Serve 来部署一个 MLflow
    推理管道。
- en: Deploying using Ray Serve and MLflow deployment plugins
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Ray Serve 和 MLflow 部署插件进行部署
- en: 'A more generic way to do deployment is to use a framework such as Ray Serve
    ([https://docs.ray.io/en/latest/serve/index.html](https://docs.ray.io/en/latest/serve/index.html)).
    Ray Serve has several advantages, such as DL model frameworks agnostics, native
    Python support, and supporting complex model composition inference patterns. Ray
    Serve supports all major DL frameworks and any arbitrary business logic. So, can
    we leverage both Ray Serve and MLflow to do model deployment and serve? The good
    news is that we can use the MLflow deployment plugins provided by Ray Serve to
    do this. Let''s walk through how to use the `mlflow-ray-serve` plugin to do MLflow
    model deployment using Ray Serve ([https://github.com/ray-project/mlflow-ray-serve](https://github.com/ray-project/mlflow-ray-serve)).
    Before we begin, we need to install the `mlflow-ray-serve` package:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 更通用的部署方式是使用像Ray Serve这样的框架（[https://docs.ray.io/en/latest/serve/index.html](https://docs.ray.io/en/latest/serve/index.html)）。Ray
    Serve有几个优点，例如支持不同的深度学习模型框架、原生Python支持以及支持复杂的模型组合推理模式。Ray Serve支持所有主要的深度学习框架和任何任意的业务逻辑。那么，我们能否同时利用Ray
    Serve和MLflow进行模型部署和服务呢？好消息是，我们可以使用Ray Serve提供的MLflow部署插件来实现这一点。接下来我们将介绍如何使用`mlflow-ray-serve`插件通过Ray
    Serve进行MLflow模型部署（[https://github.com/ray-project/mlflow-ray-serve](https://github.com/ray-project/mlflow-ray-serve)）。在开始之前，我们需要安装`mlflow-ray-serve`包：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we need to start a single node Ray cluster locally first using the following
    two commands:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要首先使用以下两个命令在本地启动一个单节点Ray集群：
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will start a Ray cluster locally, and you can access its dashboard from
    your web browser at `http://127.0.0.1:8265/#/` as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在本地启动一个Ray集群，并且你可以通过浏览器访问它的仪表板，网址是`http://127.0.0.1:8265/#/`，如下所示：
- en: '![Figure 8.3 – A locally running Ray cluster'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.3 – 本地运行的Ray集群'
- en: '](img/B18120_08_03.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_08_03.jpg)'
- en: Figure 8.3 – A locally running Ray cluster
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 本地运行的Ray集群
- en: '*Figure 8.3* shows a locally running Ray cluster. You can then issue the following
    command to deploy `inference_pipeline_model` into Ray Serve as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.3*展示了一个本地运行的Ray集群。然后，你可以执行以下命令将`inference_pipeline_model`部署到Ray Serve：'
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will show the following screen output:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示以下屏幕输出：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This means that an endpoint at `http://127.0.0.1:8000/dl-inference-model-on-ray`
    is ready to serve an online inference request! You can test this deployment using
    the Python code provided at `chapter08/ray_serve/query_ray_serve_endpoint.py`
    as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着位于`http://127.0.0.1:8000/dl-inference-model-on-ray`的端点已准备好为在线推理请求提供服务！你可以使用以下提供的Python代码在`chapter08/ray_serve/query_ray_serve_endpoint.py`中测试这个部署：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will show results on the screen as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在屏幕上显示如下结果：
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You should see the inference model response as expected. If you followed through
    up to this point, congratulations on your successful deployment using the `mlflow-ray-serve`
    MLflow deployment plugin! If you no longer need this Ray Serve instance, you can
    stop it by executing the following command line:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到预期的推理模型响应。如果你按照这些步骤操作至此，恭喜你成功通过`mlflow-ray-serve` MLflow部署插件完成了部署！如果你不再需要这个Ray
    Serve实例，可以通过执行以下命令行来停止它：
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This will stop all running Ray instances on your local machine.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这将停止你本地机器上所有正在运行的Ray实例。
- en: Deployment Using MLflow Deployment Plugins
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MLflow部署插件进行部署
- en: 'There are several MLflow deployment plugins. We just showed how to use `mlflow-ray-serve`
    to deploy a generic MLflow Python model, `inference_pipeline_model`. This opens
    doors to deploying to many target destinations where you can launch a Ray cluster
    in any cloud provider. We will not cover more details in this chapter as it''s
    beyond the scope of this book. If you are interested, refer to the Ray documentation
    on how to launch cloud clusters (AWS, Azure, and **Google Cloud Platform** (**GCP**)):
    [https://docs.ray.io/en/latest/cluster/cloud.html#:~:text=The%20Ray%20Cluster%20Launcher%20can,ready%20to%20launch%20your%20cluster](https://docs.ray.io/en/latest/cluster/cloud.html#:~:text=The%20Ray%20Cluster%20Launcher%20can,ready%20to%20launch%20your%20cluster).
    Once there is a Ray cluster, you can follow the same procedure to deploy an MLflow
    model.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个MLflow部署插件。我们刚刚展示了如何使用`mlflow-ray-serve`部署一个通用的MLflow Python模型`inference_pipeline_model`。这为将Ray集群部署到许多目标目的地打开了大门，你可以在任何云服务提供商上启动Ray集群。由于本书的范围限制，我们不会在这一章进一步探讨更多细节。如果你感兴趣，可以参考Ray的文档了解如何启动云集群（AWS、Azure和**Google
    Cloud Platform**（**GCP**））：[https://docs.ray.io/en/latest/cluster/cloud.html#:~:text=The%20Ray%20Cluster%20Launcher%20can,ready%20to%20launch%20your%20cluster](https://docs.ray.io/en/latest/cluster/cloud.html#:~:text=The%20Ray%20Cluster%20Launcher%20can,ready%20to%20launch%20your%20cluster)。一旦Ray集群启动，你可以按照相同的流程部署MLflow模型。
- en: Now that we know several ways to deploy locally and could further deploy to
    the cloud using Ray Serve if desirable, let's see how we can deploy to a cloud-managed
    inference service, AWS SageMaker, in the next section, since it is widely used
    and can provide a good lesson on how to deploy in a realistic scenario.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了几种在本地部署的方法，并且如果需要的话可以进一步部署到云端，使用 Ray Serve，让我们看看如何在下一节中部署到云管理的推理服务 AWS
    SageMaker，因为它被广泛使用，并且可以提供在现实场景中如何部署的良好教训。
- en: Deploying to AWS SageMaker – a complete end-to-end guide
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署到 AWS SageMaker —— 完整的端到端指南
- en: 'AWS SageMaker has a cloud-hosted model service managed by AWS. We will use
    AWS SageMaker as an example to show you how to deploy to a remote cloud provider
    for hosted web services that can serve real production traffic. AWS SageMaker
    has a suite of ML/DL-related services including supporting annotation and model
    training and many more. Here, we show how to **bring your own model** (**BYOM**)
    for deployment. This means that you have a model inference pipeline trained outside
    of AWS SageMaker, and now just need to deploy to SageMaker for hosting. Follow
    the next steps to prepare and deploy a DL sentiment model. A few prerequisites
    are required:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: AWS SageMaker 是由 AWS 管理的云托管模型服务。我们将以 AWS SageMaker 为例，展示如何将模型部署到托管 Web 服务的远程云提供商，以服务真实的生产流量。AWS
    SageMaker 提供了一套包括支持注释和模型训练等在内的 ML/DL 相关服务。在这里，我们展示如何为部署 BYOM（Bring Your Own Model，自带模型）做准备和部署。这意味着您已在
    AWS SageMaker 之外训练了模型推理管道，现在只需部署到 SageMaker 进行托管。按照以下步骤准备和部署 DL 情感模型。需要几个先决条件：
- en: You must have Docker Desktop running in your local environment.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您必须在本地环境中运行 Docker Desktop。
- en: You must have an AWS account. You can create a free AWS account easily through
    the free signup website at [https://aws.amazon.com/free/](https://aws.amazon.com/free/).
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您必须拥有一个 AWS 账户。您可以通过免费注册网站 [https://aws.amazon.com/free/](https://aws.amazon.com/free/)
    轻松创建一个免费的 AWS 账户。
- en: 'Once you have these requirements , activate the `dl-model-chapter08` conda
    virtual environment to follow through a few steps for deploying to SageMaker.
    We breakdown these steps into six subsections as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦满足这些要求，激活 `dl-model-chapter08` 的 conda 虚拟环境，按照以下几个步骤部署到 SageMaker。我们将这些步骤细分为六个子章节如下：
- en: Build a local SageMaker Docker image
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建本地 SageMaker Docker 镜像
- en: Add additional model artifacts layers onto the SageMaker Docker image
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将额外的模型构件层添加到 SageMaker Docker 镜像上
- en: Test local deployment with the newly built SageMaker Docker image
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新构建的 SageMaker Docker 镜像进行本地部署测试
- en: Push the SageMaker Docker image to AWS Elastic Container Registry
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 SageMaker Docker 镜像推送到 AWS Elastic Container Registry
- en: Deploy the inference pipeline model to create a SageMaker endpoint
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署推理管道模型以创建 SageMaker 终端节点
- en: Query the SageMaker endpoint for online inference
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查询 SageMaker 终端节点进行在线推理
- en: Let's start with the first step to build a local SageMaker Docker image.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一步开始构建本地 SageMaker Docker 镜像。
- en: 'Step 1: Build a local SageMaker Docker image'
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 1 步：构建本地 SageMaker Docker 镜像
- en: 'We intentionally start with a local build without pushing to the AWS so that
    we can learn how to add additional layers on top of this basic image and verify
    everything locally before incurring any cloud cost:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们故意从本地构建开始，而不是推送到 AWS，这样我们可以学习如何在此基础镜像上添加额外的层，并在本地验证一切，避免产生任何云端费用：
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You will see a lot of screen outputs and at the end, it will show something
    like the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到大量的屏幕输出，最后会显示类似以下内容：
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If you see the image name `mlflow-dl-inference`, that means you have successfully
    created a SageMaker-compatible MLflow-model-serving Docker image. You can verify
    this by running the following command:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您看到镜像名称 `mlflow-dl-inference`，那么说明您已成功创建了一个符合 SageMaker 标准的 MLflow 模型服务 Docker
    镜像。您可以通过运行以下命令来验证：
- en: '[PRE19]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You should see output like the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到类似以下的输出：
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Step 2: Add additional model artifacts layers onto the SageMaker Docker image'
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 2 步：将额外的模型构件层添加到 SageMaker Docker 镜像上
- en: Recall that our inference pipeline model builds on top of a fine-tuned DL model
    and we load the model through the MLflow PythonModel API's `load_context` function
    ([https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel](https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel))
    without serializing the fine-tuned model itself. This is partly because MLflow
    cannot serialize the PyTorch DataLoader ([https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading](https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading))
    properly using pickle since the DataLoader does not implement pickle serialization
    as of this writing. This does give us an opportunity to learn how we can deploy
    when some of the dependencies cannot be serialized properly, especially when dealing
    with a real-world DL model.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们的推理管道模型是基于一个微调的深度学习模型构建的，我们通过 MLflow PythonModel API 的 `load_context`
    函数 ([https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel](https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel))
    加载该模型，而不需要序列化微调后的模型本身。部分原因是因为 MLflow 无法通过 pickle 正确序列化 PyTorch DataLoader ([https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading](https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading))，因为
    DataLoader 从目前的文档来看并不实现 pickle 序列化。这为我们提供了一个机会，去学习当一些依赖项无法正确序列化时，我们该如何进行部署，尤其是在处理现实世界的深度学习模型时。
- en: Two Ways to Allow a Docker Container to Access an MLflow Tracking Server
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让 Docker 容器访问 MLflow 跟踪服务器的两种方法
- en: There are two ways to allow a Docker container such as `mlflow-dl-inference`
    to access and load a fine-tuned model at runtime. The first method is to allow
    the container to include the MLflow tracking server URL and access token. This
    may cause some security concerns in an enterprise environment since the Docker
    image now contains some security credentials. The second method is to directly
    copy all the referenced artifacts to create a new Docker image that's self-sufficient.
    At runtime, it does not have to know where the original MLflow tracking server
    is located since it has all model artifacts locally. This self-contained approach
    eliminates any concerns of security leaking. We use this second approach in this
    chapter for deployment.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以让 Docker 容器（如 `mlflow-dl-inference`）在运行时访问并加载微调模型。第一种方法是让容器包含 MLflow
    跟踪服务器的 URL 和访问令牌。这在企业环境中可能会引发一些安全隐患，因为此时 Docker 镜像中包含了一些安全凭证。第二种方法是直接将所有引用的工件复制到新建的
    Docker 镜像中，从而创建一个自给自足的镜像。在运行时，它不需要知道原始 MLflow 跟踪服务器的位置，因为它已将所有模型工件保存在本地。这种自包含的方法消除了任何安全泄露的担忧。在本章中，我们采用了第二种方法进行部署。
- en: In this chapter, we will copy the referenced fine-tuned model into a new Docker
    image that's built on top of the basic `mlflow-dl-inference` Docker image. This
    will make a new self-contained Docker image without relying on any external MLflow
    tracking server. To do this, you need to either download the fine-tuned DL model
    from a model tracking server to your current local folder, or you can just run
    our MLproject's pipeline locally using the local filesystem as the MLflow tracking
    server backend. Follow the *Deploy to AWS SageMaker* section in the `README.md`
    file to reproduce the local MLflow runs to prepare a fine-tuned model and `inference-pipeline-model`
    in the local folder. For learning purposes, we have provided two example `mlruns`
    artifacts and the `huggingface` cache folder in the GitHub repository in the `chapter08`
    folder ([https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter08](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter08)),
    so that we can start building a new Docker image right away by using these existing
    artifacts.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将把引用的微调模型复制到一个新的 Docker 镜像中，该镜像基于基础的 `mlflow-dl-inference` Docker 镜像构建。这将创建一个新的自包含
    Docker 镜像，无需依赖任何外部的 MLflow 跟踪服务器。为此，你需要将微调的深度学习模型从模型跟踪服务器下载到当前本地文件夹，或者你可以通过使用本地文件系统作为
    MLflow 跟踪服务器后端，直接在本地运行我们的 MLproject 流水线。按照 `README.md` 文件中的 *部署到 AWS SageMaker*
    部分，重现本地的 MLflow 运行，准备微调模型和本地文件夹中的 `inference-pipeline-model`。为了学习目的，我们在 GitHub
    仓库的 `chapter08` 文件夹中提供了两个示例 `mlruns` 工件和 `huggingface` 缓存文件夹 ([https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter08](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter08))，以便我们可以立即使用这些现有的工件开始构建新的
    Docker 镜像。
- en: 'To build a new Docker image, we need to create a Dockerfile as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个新的 Docker 镜像，我们需要创建一个如下所示的 Dockerfile：
- en: '[PRE21]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The first line means that it starts with the existing `mlflow-dl-inference`
    Docker image, and the following three lines of `ADD` will copy one `meta.yaml`
    file and two folders to the corresponding locations in the Docker image. Note
    that if you already have produced your own runs by following the `README` file,
    then you do not need to add the third line. Note that, by default, when the Docker
    container starts, it automatically goes to this`/opt/mlflow/` working directory
    so everything needs to be copied to this folder for easy access. Also, note that
    the `/opt/mlflow` directory requires superuser permission, so you need to be prepared
    to enter your local machine's admin password (usually, on your own laptop, that's
    your own password).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行表示它从现有的`mlflow-dl-inference` Docker 镜像开始，接下来的三行`ADD`将会复制一个`meta.yaml`文件和两个文件夹到
    Docker 镜像中的相应位置。请注意，如果您已经按照`README`文件中的步骤生成了自己的运行实例，则无需添加第三行。需要注意的是，默认情况下，当 Docker
    容器启动时，它会自动进入`/opt/mlflow/`工作目录，因此所有内容都需要复制到这个文件夹中以便于访问。此外，请注意`/opt/mlflow`目录需要超级用户权限，因此您需要准备好输入本地机器的管理员密码（通常，在您的个人笔记本电脑上，密码就是您自己的密码）。
- en: Copy Privately Built Python Packages into Docker Images
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 将私有构建的 Python 包复制到 Docker 镜像中
- en: It is also possible to copy privately built Python packages into Docker images
    so that we can directly reference them in the `conda.yaml` file without going
    outside of the container itself. For example, we can copy a private Python wheel
    package, `cool-dl-package-1.0.py3-none-any.whl`, to the `/usr/private-wheels/cool-dl-package/cool-dl-package-1.0-py3-none-any.whl`
    Docker folder, and then we can point to this path in the `conda.yaml` file. This
    allows MLflow model artifacts to load these locally accessible Python packages
    successfully. In our current example, we don't use this approach since we haven't
    used any privately built Python packages. This is useful for future reference
    if you are interested in exploring this.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以将私有构建的 Python 包复制到 Docker 镜像中，这样我们就可以在`conda.yaml`文件中直接引用它们，而无需走出容器。举例来说，我们可以将一个私有的
    Python wheel 包`cool-dl-package-1.0.py3-none-any.whl`复制到`/usr/private-wheels/cool-dl-package/cool-dl-package-1.0-py3-none-any.whl`
    Docker 文件夹中，然后我们可以在`conda.yaml`文件中指向这个路径。这使得 MLflow 模型工件能够成功加载这些本地可访问的 Python
    包。在当前的示例中，我们没有使用这种方法，因为我们没有使用任何私有构建的 Python 包。如果您有兴趣探索这个方法，未来参考时会有用。
- en: 'Now, you can run the following command to build a new Docker image in the `chapter08`
    folder as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以运行以下命令，在`chapter08`文件夹中构建一个新的 Docker 镜像：
- en: '[PRE25]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This will build a new Docker image, `mlflow-dl-inference-w-finetuned-model`,
    on top of `mlflow-dl-inference`. You should see the following output (only the
    first and last couple of lines are presented for brevity):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这将基于`mlflow-dl-inference`构建一个新的 Docker 镜像`mlflow-dl-inference-w-finetuned-model`。您应该能看到以下输出（为了简洁起见，仅展示第一行和最后几行）：
- en: '[PRE26]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Now, you have a new Docker image named `mlflow-dl-inference-w-finetuned-model`,
    which contains the fine-tuned model. Now, we are ready to deploy our inference
    pipeline model using this new Docker image, which is SageMaker compatible.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经有了一个名为`mlflow-dl-inference-w-finetuned-model`的新 Docker 镜像，其中包含微调后的模型。现在，我们准备使用这个新的、兼容
    SageMaker 的 Docker 镜像来部署我们的推理管道模型。
- en: 'Step 3: Test local deployment with the newly built SageMaker Docker image'
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 3：使用新构建的 SageMaker Docker 镜像测试本地部署
- en: 'Before we deploy to the cloud, let''s test the deployment locally with this
    new SageMaker Docker image. MLflow provides a convenient way to test this locally
    using the following command:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将部署到云端之前，让我们先使用这个新的 SageMaker Docker 镜像在本地进行部署测试。MLflow 提供了一种方便的方式来使用以下命令在本地进行测试：
- en: '[PRE27]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This command will start running the `mlflow-dl-inference-w-finetuned-model`
    Docker container locally and deploy the inference pipeline model with a `dc5f670efa1a4eac95683633ffcfdd79`
    run ID into this container.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令将会在本地启动`mlflow-dl-inference-w-finetuned-model` Docker 容器，并将运行 ID 为`dc5f670efa1a4eac95683633ffcfdd79`的推理管道模型部署到该容器中。
- en: Fix a Potential Docker Error
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 修复潜在的 Docker 错误
- en: Note that you may encounter a Docker error saying **The path /opt/mlflow/mlruns/1/
    dc5f670efa1a4eac95683633ffcfdd79/artifacts/inference_pipeline_model is not shared
    from the host and is not known to Docker**. You can configure shared paths from
    **Docker** | **Preferences...** | **Resources** | **File Sharing** to fix this
    Docker error.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您可能会遇到一个 Docker 错误，提示**路径/opt/mlflow/mlruns/1/dc5f670efa1a4eac95683633ffcfdd79/artifacts/inference_pipeline_model未从主机共享，Docker
    无法识别该路径**。您可以通过配置共享路径，进入**Docker** | **Preferences...** | **Resources** | **File
    Sharing** 来解决此 Docker 错误。
- en: 'We already provided this inference pipeline model in the GitHub repository,
    so this should work out-of-the-box when you check out the repository in your local
    environment. The port for web service is `5555`. Once the command is running,
    you will see a lot of outputs on the screen, and finally, you should see the following:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在GitHub仓库中提供了这个推理管道模型，所以当你在本地环境中检出仓库时，应该可以开箱即用。Web服务的端口是`5555`。命令运行后，你会看到很多屏幕输出，最终你应该能看到以下内容：
- en: '[PRE28]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This means that the service is up and running. You might see a few warnings
    about the PyTorch version not being compatible, but they can be safely ignored.
    Once this service is up and running, you can then test against it in a different
    Terminal window by issuing a `curl` web request as follows, like we have tried
    before:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着服务已经启动并正在运行。你可能会看到一些关于PyTorch版本不兼容的警告，但这些可以安全地忽略。一旦服务启动并运行，你就可以在另一个终端窗口中通过发出`curl`网页请求来进行测试，像我们之前尝试过的那样：
- en: '[PRE29]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Note that the port number is `5555` for the localhost. You should then see
    the response as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本地主机的端口号是`5555`。然后，你应该能看到如下响应：
- en: '[PRE30]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: You may wonder how this is different from the previous section's local web service
    for the inference model. The difference is that this time, we are using a SageMaker
    container locally, while previously, it was just a local web service without a
    Docker container. Having the SageMaker container tested locally is very important
    so that you don't waste time and money deploying a failed model service to the
    cloud.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道这和之前章节的本地推理模型Web服务有什么不同。区别在于，这一次我们使用的是SageMaker容器，而之前只是没有Docker容器的本地Web服务。在本地测试SageMaker容器非常重要，以避免浪费时间和金钱将失败的模型服务部署到云端。
- en: Next, we are ready to deploy this container to AWS SageMaker.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们准备将这个容器部署到AWS SageMaker。
- en: 'Step 4: Push the SageMaker Docker image to AWS Elastic Container Registry'
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步：将SageMaker Docker镜像推送到AWS Elastic Container Registry
- en: 'Now, you can push your newly built `mlflow-dl-inference-w-finetuned-model`
    Docker image to AWS **Elastic Container Registry** (**ECR**) with the following
    command. Make sure you have your AWS access token and access ID set up correctly
    (the real one, not the local development one). Once you have your access key ID
    and token, run the following command to set up your access to the real AWS:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以将新构建的`mlflow-dl-inference-w-finetuned-model` Docker镜像推送到AWS **Elastic
    Container Registry** (**ECR**)，使用以下命令。确保你的AWS访问令牌和访问ID已正确设置（使用真实的，而不是本地开发的）。一旦你拥有访问密钥ID和令牌，运行以下命令以设置对真实AWS的访问：
- en: '[PRE31]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Answer all the questions after executing the command and you will be ready
    to go. Now, you can run the following command to push the `mlflow-dl-inference-w-finetuned-model`
    Docker image to the AWS ECR:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 执行命令后回答所有问题，你就准备好继续了。现在，你可以运行以下命令将`mlflow-dl-inference-w-finetuned-model` Docker镜像推送到AWS
    ECR：
- en: '[PRE32]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Make sure you don''t build a new image with the `--no-build` option included
    in the command since we just want to push the image, not build a new one. You
    will see the following output, which shows the image is being pushed to the ECR.
    Note that in the following output, the AWS account is masked with `xxxxx`. You
    will see your account number showing in the output. Make sure you have the permission
    to write to the AWS ECR store:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在命令中不包含`--no-build`选项来构建新的镜像，因为我们只需要推送镜像，而不是构建新的镜像。你将看到以下输出，显示镜像正在被推送到ECR。请注意，在以下输出中，AWS账户被`xxxxx`掩盖。你将看到你的账户编号出现在输出中。确保你拥有写入AWS
    ECR存储的权限：
- en: '[PRE33]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Once this is done, if you go to the AWS website (for example, if you use the
    `us-west-2` region data center, the URL is [https://us-west-2.console.aws.amazon.com/ecr/repositories?region=us-west-2](https://us-west-2.console.aws.amazon.com/ecr/repositories?region=us-west-2)),
    you should find your newly pushed image in the ECR with a folder named `mlflow-dl-inference-w-finetuned-model`.
    You will then find the image in this folder as follows (*Figure 8.4*):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，如果你访问AWS网站（例如，如果你使用的是`us-west-2`区域的数据中心，网址是[https://us-west-2.console.aws.amazon.com/ecr/repositories?region=us-west-2](https://us-west-2.console.aws.amazon.com/ecr/repositories?region=us-west-2)），你应该能在ECR中找到你新推送的镜像，并且会看到一个名为`mlflow-dl-inference-w-finetuned-model`的文件夹。然后，你会在该文件夹中找到以下镜像（*图8.4*）：
- en: '![Figure 8.4 – AWS ECR repositories with mlflow-dl-inference-w-finetuned-model
    image tag 1.23.1'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.4 – 带有mlflow-dl-inference-w-finetuned-model镜像标签1.23.1的AWS ECR存储库'
- en: '](img/B18120_08_04.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_08_04.jpg)'
- en: Figure 8.4 – AWS ECR repositories with mlflow-dl-inference-w-finetuned-model
    image tag 1.23.1
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 带有mlflow-dl-inference-w-finetuned-model镜像标签1.23.1的AWS ECR存储库
- en: 'Note that the image tag number `Copy URI` option. It will look as follows (with
    the AWS account masked with `xxxxx`):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意镜像标签号`Copy URI`选项。它将如下所示（AWS账户号被屏蔽为`xxxxx`）：
- en: '[PRE34]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: You will need this image URI to deploy to SageMaker in the next step. Let's
    now deploy to SageMaker to create an inference endpoint.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在下一步中需要此镜像URI来部署到SageMaker。现在让我们将模型部署到SageMaker，创建一个推理端点。
- en: 'Step 5: Deploy the inference pipeline model to create a SageMaker endpoint'
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5步：将推理管道模型部署到SageMaker以创建SageMaker端点
- en: 'Now, it is time to deploy the inference pipeline model to SageMaker using this
    image URI we just pushed to the AWS ECR registry. We have included the `sagemaker/deploy_to_sagemaker.py`
    code in the `chapter08` folder in the GitHub repository. You will need to use
    the correct AWS role for the deployment. You can create a new `AWSSageMakerExecutionRole`
    role in your account and assign two permissions policies to this role, `AmazonS3FullAccess`
    and `AmazonSageMakerFullAccess`. In a real-world scenario, you might want to tighten
    the permission to a more restricted policy, but for learning purposes, this will
    work fine. The following figure shows the screen after the role is created:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候使用我们刚刚推送到AWS ECR注册表的镜像URI将推理管道模型部署到SageMaker了。我们在GitHub仓库的`chapter08`文件夹中包含了`sagemaker/deploy_to_sagemaker.py`代码。您需要使用正确的AWS角色进行部署。您可以在您的账户中创建一个新的`AWSSageMakerExecutionRole`角色，并将两个权限策略`AmazonS3FullAccess`和`AmazonSageMakerFullAccess`分配给该角色。在实际场景中，您可能希望将权限收紧到更受限的策略，但为了学习目的，这种设置是可以的。下图显示了创建角色后的屏幕：
- en: '![Figure 8.5 – Create a role that can be used for deployment in SageMaker'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.5 – 创建一个可以在SageMaker中用于部署的角色'
- en: '](img/B18120_08_05.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_08_05.jpg)'
- en: Figure 8.5 – Create a role that can be used for deployment in SageMaker
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – 创建一个可以在SageMaker中用于部署的角色
- en: 'You also need to create an S3 bucket for SageMaker to upload the model artifacts
    and deploy them to SageMaker. In our example, we created a bucket called `dl-inference-deployment`.
    When we execute the deployment script, as shown here, the model to be deployed
    will be first uploaded to the `dl-inference-deployment` bucket and then deployed
    to SageMaker. We have provided the complete deployment script in the `chapter08/sagemaker/deploy_to_sagemaker.py`
    GitHub repository so you can download and execute it as follows (as a reminder,
    before you run this script, make sure you reset the environment variable of `MLFLOW_TRACKING_URI`
    to empty, as in `export MLFLOW_TRACKING_URI=`):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要为SageMaker创建一个S3存储桶，以便SageMaker可以上传模型工件并将其部署到SageMaker。在我们的示例中，我们创建了一个名为`dl-inference-deployment`的存储桶。当我们执行部署脚本时，如下所示，待部署的模型首先会上传到`dl-inference-deployment`存储桶中，然后再部署到SageMaker。我们已经在`chapter08/sagemaker/deploy_to_sagemaker.py`
    GitHub仓库中提供了完整的部署脚本，您可以下载并按如下方式执行（提醒一下，在运行此脚本之前，请确保将环境变量`MLFLOW_TRACKING_URI`重置为空，如`export
    MLFLOW_TRACKING_URI=`）：
- en: '[PRE35]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This script executes the following two tasks:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本执行以下两个任务：
- en: Makes a copy of the local `mlruns` under the `chapter08` folder to a local `/opt/mlflow`
    folder so that SageMaker deployment code can pick up the `inference-pipeline-model`
    to upload. Because the `/opt` path is usually restricted, here we use `sudo` (superuser)
    to do this copy. This will prompt you to type in your user password on your laptop.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将本地`mlruns`文件夹下的内容复制到本地的`/opt/mlflow`文件夹中，以便SageMaker部署代码能够识别`inference-pipeline-model`并进行上传。由于`/opt`路径通常是受限的，因此我们使用`sudo`（超级用户权限）来执行此复制操作。此操作将提示您在笔记本电脑上输入用户密码。
- en: Uses the `mlflow.sagemaker.deploy` API to create a new SageMaker endpoint, `dl-sentiment-model`.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`mlflow.sagemaker.deploy`API来创建一个新的SageMaker端点，`dl-sentiment-model`。
- en: 'The code snippet is as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段如下：
- en: '[PRE36]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The parameters need some explanations so that we fully understand all the preparation
    work that is needed:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数需要一些解释，以便我们能完全理解所需的所有准备工作：
- en: '`model_uri`: This is the inference pipeline model''s URI. In our example, it
    is `runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model`.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_uri`：这是推理管道模型的URI。在我们的示例中，它是`runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model`。'
- en: '`image_url`: This is the Docker image we uploaded to the AWS ECR. In our example,
    it is `xxxxx.dkr.ecr.us-west-2.amazonaws.com/mlflow-dl-inference-w-finetuned-model:1.23.1`.
    Note that you need to replace the masked AWS account number, `xxxxx`, with your
    actual account number.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_url`：这是我们上传到AWS ECR的Docker镜像。在我们的示例中，它是`xxxxx.dkr.ecr.us-west-2.amazonaws.com/mlflow-dl-inference-w-finetuned-model:1.23.1`。请注意，您需要将被屏蔽的AWS账户号`xxxxx`替换为您的实际账户号。'
- en: '`execution_role_arn`: This is the role we created to allow SageMaker to do
    the deployment. In our example, it is `arn:aws:iam::565251169546:role/AWSSageMakerExecutionRole`.
    Again, you need to replace `xxxxx` with your actual AWS account number.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`execution_role_arn`：这是我们创建的角色，允许SageMaker进行部署。在我们的示例中，它是`arn:aws:iam::565251169546:role/AWSSageMakerExecutionRole`。再次提醒，你需要将`xxxxx`替换为你的实际AWS账户号码。'
- en: '`bucket`: This is the S3 bucket we created to allow SageMaker to upload the
    model and then do the actual deployment. In our example, it is `dl-inference-deployment`.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bucket`：这是我们创建的S3桶，允许SageMaker上传模型并进行实际部署。在我们的示例中，它是`dl-inference-deployment`。'
- en: The rest of the parameters are self-explanatory.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的参数不言自明。
- en: 'After you execute the deployment script, you will see the following output
    (where `xxxxx` is the masked AWS account number):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 执行部署脚本后，你将看到如下输出（其中`xxxxx`是隐藏的AWS账户号码）：
- en: '[PRE37]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This may take several minutes (sometimes more than 10 minutes). You may see
    some warning messages regarding PyTorch version compatibility as you saw when
    doing local SageMaker deployment testing. You can also go directly to the SageMaker
    website and you will see the status of the endpoints starting with **Creating**,
    and then eventually turning to a green-colored **InService** status as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要几分钟时间（有时超过10分钟）。你可能会看到一些关于PyTorch版本兼容性的警告信息，就像在进行本地SageMaker部署测试时看到的那样。你也可以直接访问SageMaker网站，在那里你将看到端点的状态从**Creating**开始，最终变成绿色的**InService**状态，如下所示：
- en: '![Figure 8.6 – AWS SageMaker dl-sentiment-model endpoint InService'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.6 – AWS SageMaker dl-sentiment-model端点InService状态'
- en: '](img/B18120_08_06.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_08_06.jpg)'
- en: Figure 8.6 – AWS SageMaker dl-sentiment-model endpoint InService
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – AWS SageMaker dl-sentiment-model端点InService状态
- en: If you see the **InService** status, then congratulations! You have successfully
    deployed a DL inference pipeline model into SageMaker and you can now use it for
    production traffic!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到**InService**状态，那么恭喜你！你已成功将一个DL推理管道模型部署到SageMaker，现在你可以将它用于生产流量！
- en: Now that the status of the service is inService, you can query it using the
    command line in the next step.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在服务状态为InService，你可以在下一步使用命令行进行查询。
- en: 'Step 6: Query the SageMaker endpoint for online inference'
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤6：查询SageMaker端点进行在线推理
- en: 'To query the SageMaker endpoint, you can use the following command line:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 要查询SageMaker端点，你可以使用以下命令行：
- en: '[PRE38]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You will then see the output as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你将看到如下输出：
- en: '[PRE39]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The actual prediction results are stored in a local `response.json` file, which
    can be viewed by running the following command to show the content of the response:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的预测结果存储在本地的`response.json`文件中，你可以通过运行以下命令查看响应内容：
- en: '[PRE40]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This will display the content as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示如下内容：
- en: '[PRE41]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This is the expected response pattern from our inference pipeline model! It
    is also possible to run the query against the SageMaker inference endpoint using
    Python code, which we have provided in the `chapter08/sagemaker/ query_sagemaker_endpoint.py`
    file in the GitHub repository. The core code snippet uses `SageMakerRuntime` client''s
    `invoke_endpoint` to query, as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们推理管道模型的预期响应模式！你也可以通过Python代码查询SageMaker推理端点，我们已在GitHub仓库中的`chapter08/sagemaker/query_sagemaker_endpoint.py`文件中提供了该代码。核心代码片段使用了`SageMakerRuntime`客户端的`invoke_endpoint`进行查询，如下所示：
- en: '[PRE42]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The parameters for `invoke_endpoint` need some explanation:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`invoke_endpoint`的参数需要一些解释：'
- en: '`EndpointName`: This is the inference endpoint name. In our example, it is
    `dl-inference-model`.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EndpointName`：这是推理端点的名称。在我们的示例中，它是`dl-inference-model`。'
- en: '`ContentType`: This is the MIME type of the input data in the request body.
    In our example, we use `application/json; format=pandas-split`.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ContentType`：这是请求体中输入数据的MIME类型。在我们的示例中，我们使用`application/json; format=pandas-split`。'
- en: '`Accept`: This is the desired MIME type of the inference in the response body.
    In our example, we expect the `text/plain` string type.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Accept`：这是推理响应体中期望的MIME类型。在我们的示例中，我们期望返回`text/plain`字符串类型。'
- en: '`Body`: This is the actual text that we want to predict the sentiment using
    the DL model inference service. In our example, it is `{"columns": ["text"],"data":
    [["This is the best movie we saw."], ["What a movie!"]]}`.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Body`：这是我们希望使用DL模型推理服务预测情感的实际文本。在我们的示例中，它是`{"columns": ["text"],"data": [["This
    is the best movie we saw."], ["What a movie!"]]}`。'
- en: 'The full code is provided in the GitHub repository, and you can run it in the
    command line as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码已提供在GitHub仓库中，你可以在命令行中按照以下方式运行它：
- en: '[PRE49]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'You will see the following output on your Terminal screen:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在终端屏幕上看到如下输出：
- en: '[PRE50]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: This is what we expect from our inference pipeline model's response! If you
    have followed this chapter up to here, congratulate yourself on successfully deploying
    our inference pipeline model into production in a remote cloud host, AWS SageMaker!
    When you are done following the lessons in this chapter, make sure to delete the
    endpoint so that it doesn't incur unnecessary costs.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们期望从推理管道模型的响应中得到的结果！如果你已经按照本章内容完成学习，恭喜你成功将推理管道模型部署到AWS SageMaker上的远程云主机中！完成本章内容后，请务必删除端点，以避免不必要的费用。
- en: Let's summarize what we've learned in this chapter.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下我们在本章中学到的内容。
- en: Summary
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have learned different ways to deploy an MLflow inference
    pipeline model for both batch inference and online real-time inference. We started
    with a brief survey on different model serving scenarios (batch, streaming, and
    on-device) and looked at three different categories of tools for MLflow model
    deployment (the MLflow built-in deployment tool, MLflow deployment plugins, and
    generic model inference serving frameworks that could work with the MLflow inference
    model). Then, we covered several local deployment scenarios, using the PySpark
    UDF function to do batch inference and MLflow local deployment for web service.
    Afterward, we learned how to use Ray Serve in conjunction with the `mlflow-ray-serve`
    plugin to deploy an MLflow Python inference pipeline model into a local Ray cluster.
    This opens doors to deploy to any cloud platform such as AWS, Azure ML, or GCP,
    as long as we can set up a Ray cluster in the cloud. Finally, we provided a complete
    end-to-end guide on how to deploy to AWS SageMaker, focusing on a common scenario
    of BYOM, where we have a trained inference pipeline model that's built outside
    of AWS SageMaker and now needs to be deployed to AWS SageMaker for a hosting model
    service. Our step-by-step guide should provide you with the confidence to deploy
    an MLflow inference pipeline model for real production usage.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了不同的方法来部署一个MLflow推理管道模型，适用于批量推理和在线实时推理。我们从对不同模型服务场景（批处理、流式、设备端）的简要调查开始，并研究了三种不同类别的MLflow模型部署工具（MLflow内置部署工具、MLflow部署插件，以及可以与MLflow推理模型配合使用的通用模型推理服务框架）。然后，我们介绍了几种本地部署场景，使用PySpark
    UDF函数进行批量推理，并使用MLflow本地部署进行Web服务部署。接着，我们学习了如何结合使用Ray Serve和`mlflow-ray-serve`插件，将MLflow
    Python推理管道模型部署到本地Ray集群。这为我们打开了部署到任何云平台的大门，比如AWS、Azure ML或GCP，只要我们能在云中设置Ray集群。最后，我们提供了一个完整的端到端指南，讲解如何部署到AWS
    SageMaker，重点介绍了BYOM（Bring Your Own Model）常见场景，在这个场景中，我们有一个在AWS SageMaker外部训练的推理管道模型，现在需要将其部署到AWS
    SageMaker以提供托管服务。我们的逐步指南应该能帮助你信心满满地将MLflow推理管道模型部署到真实生产环境中。
- en: Note that the landscape of deploying DL inference pipeline models is still evolving,
    and we just learned some foundational skills. You are encouraged to explore more
    from the *Further reading* section for more advanced topics.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，部署深度学习推理管道模型的领域仍在发展，我们刚刚学到了一些基础技能。我们鼓励你在*进一步阅读*部分探索更多高级主题。
- en: Now that we know how to deploy and host a DL inference pipeline, we will learn
    how to do model explainability in the next chapter, which is of great importance
    for trustworthy and interpretable model prediction results in many real-world
    scenarios.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道如何部署和托管深度学习推理管道，接下来我们将在下一章学习如何进行模型可解释性，这对于许多现实场景中可信赖和可解释的模型预测结果至关重要。
- en: Further reading
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*An Introduction to TinyML*: [https://towardsdatascience.com/an-introduction-to-tinyml-4617f314aa79](https://towardsdatascience.com/an-introduction-to-tinyml-4617f314aa79)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TinyML简介*：[https://towardsdatascience.com/an-introduction-to-tinyml-4617f314aa79](https://towardsdatascience.com/an-introduction-to-tinyml-4617f314aa79)'
- en: '*Performance Optimizations and MLFlow Integrations – Seldon Core 1.10.0 Released*:
    [https://www.seldon.io/performance-optimizations-and-mlflow-integrations-seldon-core-1-10-0-released/](https://www.seldon.io/performance-optimizations-and-mlflow-integrations-seldon-core-1-10-0-released/)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*性能优化与MLFlow集成 - Seldon Core 1.10.0发布*：[https://www.seldon.io/performance-optimizations-and-mlflow-integrations-seldon-core-1-10-0-released/](https://www.seldon.io/performance-optimizations-and-mlflow-integrations-seldon-core-1-10-0-released/)'
- en: '*Ray & MLflow: Taking Distributed Machine Learning Applications to Production*:
    [https://medium.com/distributed-computing-with-ray/ray-mlflow-taking-distributed-machine-learning-applications-to-production-103f5505cb88](https://medium.com/distributed-computing-with-ray/ray-mlflow-taking-distributed-machine-learning-applications-to-production-103f5505cb88)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ray 与 MLflow：将分布式机器学习应用推向生产环境*: [https://medium.com/distributed-computing-with-ray/ray-mlflow-taking-distributed-machine-learning-applications-to-production-103f5505cb88](https://medium.com/distributed-computing-with-ray/ray-mlflow-taking-distributed-machine-learning-applications-to-production-103f5505cb88)'
- en: '*Managing your machine learning lifecycle with MLflow and Amazon SageMaker*:
    [https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/](https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 MLflow 和 Amazon SageMaker 管理你的机器学习生命周期*: [https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/](https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/)'
- en: '*Deploy A Locally Trained ML Model In Cloud Using AWS SageMaker*: [https://medium.com/geekculture/84af8989d065](https://medium.com/geekculture/84af8989d065)'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在云端使用 AWS SageMaker 部署本地训练的 ML 模型*: [https://medium.com/geekculture/84af8989d065](https://medium.com/geekculture/84af8989d065)'
- en: '*PyTorch vs TensorFlow in 2022*: [https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/](https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/)'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*2022年 PyTorch 与 TensorFlow 对比*: [https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/](https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/)'
- en: '*Try Databricks: Free Trial or Community Edition*: [https://docs.databricks.com/getting-started/try-databricks.html#free-trial-or-community-edition](https://docs.databricks.com/getting-started/try-databricks.html#free-trial-or-community-edition)'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*尝试 Databricks：免费试用或社区版*: [https://docs.databricks.com/getting-started/try-databricks.html#free-trial-or-community-edition](https://docs.databricks.com/getting-started/try-databricks.html#free-trial-or-community-edition)'
- en: '*MLOps with MLflow and Amazon SageMaker Pipelines*: [https://towardsdatascience.com/mlops-with-mlflow-and-amazon-sagemaker-pipelines-33e13d43f238](https://towardsdatascience.com/mlops-with-mlflow-and-amazon-sagemaker-pipelines-33e13d43f238)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 MLflow 和 Amazon SageMaker Pipelines 进行 MLOps*: [https://towardsdatascience.com/mlops-with-mlflow-and-amazon-sagemaker-pipelines-33e13d43f238](https://towardsdatascience.com/mlops-with-mlflow-and-amazon-sagemaker-pipelines-33e13d43f238)'
- en: '*PyTorch JIT and TorchScript*: [https://towardsdatascience.com/pytorch-jit-and-torchscript-c2a77bac0fff](https://towardsdatascience.com/pytorch-jit-and-torchscript-c2a77bac0fff)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*PyTorch JIT 与 TorchScript*: [https://towardsdatascience.com/pytorch-jit-and-torchscript-c2a77bac0fff](https://towardsdatascience.com/pytorch-jit-and-torchscript-c2a77bac0fff)'
- en: '*ML Model Serving Best Tools*: [https://neptune.ai/blog/ml-model-serving-best-tools](https://neptune.ai/blog/ml-model-serving-best-tools)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ML 模型服务最佳工具*: [https://neptune.ai/blog/ml-model-serving-best-tools](https://neptune.ai/blog/ml-model-serving-best-tools)'
- en: '*Deploying Machine Learning models to production — Inference service architecture
    patterns*: [https://medium.com/data-for-ai/deploying-machine-learning-models-to-production-inference-service-architecture-patterns-bc8051f70080](https://medium.com/data-for-ai/deploying-machine-learning-models-to-production-inference-service-architecture-patterns-bc8051f70080)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将机器学习模型部署到生产环境 — 推理服务架构模式*: [https://medium.com/data-for-ai/deploying-machine-learning-models-to-production-inference-service-architecture-patterns-bc8051f70080](https://medium.com/data-for-ai/deploying-machine-learning-models-to-production-inference-service-architecture-patterns-bc8051f70080)'
- en: '*How to Deploy Large-Size Deep Learning Models into Production*: [https://towardsdatascience.com/how-to-deploy-large-size-deep-learning-models-into-production-66b851d17f33](https://towardsdatascience.com/how-to-deploy-large-size-deep-learning-models-into-production-66b851d17f33)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何将大规模深度学习模型部署到生产环境*: [https://towardsdatascience.com/how-to-deploy-large-size-deep-learning-models-into-production-66b851d17f33](https://towardsdatascience.com/how-to-deploy-large-size-deep-learning-models-into-production-66b851d17f33)'
- en: '*Serving ML models at scale using Mlflow on Kubernetes*: [https://medium.com/artefact-engineering-and-data-science/serving-ml-models-at-scale-using-mlflow-on-kubernetes-7a85c28d38e](https://medium.com/artefact-engineering-and-data-science/serving-ml-models-at-scale-using-mlflow-on-kubernetes-7a85c28d38e)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 Mlflow 在 Kubernetes 上按需服务 ML 模型*: [https://medium.com/artefact-engineering-and-data-science/serving-ml-models-at-scale-using-mlflow-on-kubernetes-7a85c28d38e](https://medium.com/artefact-engineering-and-data-science/serving-ml-models-at-scale-using-mlflow-on-kubernetes-7a85c28d38e)'
- en: '*When PyTorch meets MLflow*: [https://mlops.community/when-pytorch-meets-mlflow/](https://mlops.community/when-pytorch-meets-mlflow/)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*当 PyTorch 遇上 MLflow*: [https://mlops.community/when-pytorch-meets-mlflow/](https://mlops.community/when-pytorch-meets-mlflow/)'
- en: '*Deploy a model to an Azure Kubernetes Service Cluster*: [https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service?tabs=python](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service?tabs=python)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将模型部署到 Azure Kubernetes 服务集群*： [https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service?tabs=python](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service?tabs=python)'
- en: '*ONNX and Azure Machine Learning: Create and accelerate ML models*: [https://docs.microsoft.com/en-us/azure/machine-learning/concept-onnx](https://docs.microsoft.com/en-us/azure/machine-learning/concept-onnx)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ONNX 和 Azure 机器学习：创建并加速 ML 模型*： [https://docs.microsoft.com/en-us/azure/machine-learning/concept-onnx](https://docs.microsoft.com/en-us/azure/machine-learning/concept-onnx)'
