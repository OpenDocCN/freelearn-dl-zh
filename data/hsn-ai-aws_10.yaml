- en: Working with Amazon SageMaker
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Amazon SageMaker
- en: In the last few chapters, you have learned about readily-available **Machine
    Learning** (**ML**) APIs that solve business challenges. In this chapter, we will
    deep dive into AWS SageMaker—the service that is used to build, train, and deploy
    models seamlessly when the ML APIs do not completely meet your requirements. SageMaker
    increases the productivity of data scientists and machine learning engineers by
    abstracting away the complexity involved in provisioning compute and storage.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几章中，你已经了解了可用的**机器学习**（**ML**）API，它们解决了商业挑战。本章中，我们将深入探讨AWS SageMaker——当机器学习API无法完全满足你的需求时，SageMaker用于无缝地构建、训练和部署模型。SageMaker通过抽象化计算和存储资源的复杂性，提升了数据科学家和机器学习工程师的生产力。
- en: 'This is what will we cover in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖的内容：
- en: Processing big data through Spark EMR
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Spark EMR处理大数据
- en: Conducting training in Amazon SageMaker
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Amazon SageMaker中进行训练
- en: Deploying trained models and running inference
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署训练好的模型并运行推断
- en: Runninghyperparameter optimization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行超参数优化
- en: Understanding SageMaker experimentation service
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解SageMaker实验服务
- en: Bring your own model – SageMaker, MXNet, and Gluon
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自带模型 – SageMaker、MXNet和Gluon
- en: Bring your own container – R Model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自带容器 – R模型
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For the following sections, we will employ the book rating dataset known as
    `goodbooks-10k` to illustrate all of the topics outlined previously. The dataset
    consists of 6 million ratings on 10,000 books from 53,424 users. More details
    on the goodbooks-10k dataset can be found [https://www.kaggle.com/zygmunt/goodbooks-10k#books.csv](https://www.kaggle.com/zygmunt/goodbooks-10k#books.csv).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将使用名为`goodbooks-10k`的书籍评分数据集来说明之前提到的所有主题。该数据集包含来自53,424个用户对10,000本书籍的600万条评分。有关goodbooks-10k数据集的更多详情，请访问[https://www.kaggle.com/zygmunt/goodbooks-10k#books.csv](https://www.kaggle.com/zygmunt/goodbooks-10k#books.csv)。
- en: 'In the [folder](https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services)
    associated with this chapter, you will find two CSV files:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在与本章相关的[文件夹](https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services)中，你将找到两个CSV文件：
- en: '`ratings.csv`: Contains book ratings, user IDs, book IDs, and rating'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ratings.csv`：包含书籍评分、用户ID、书籍ID和评分信息'
- en: '`books.csv`: Contains book attributes, including title'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`books.csv`：包含书籍属性，包括标题'
- en: It is now time to wrangle big data to create a dataset for modeling.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候整理大数据以创建建模数据集了。
- en: Preprocessing big data through Spark EMR
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过Spark EMR进行大数据预处理
- en: The design pattern to execute models in SageMaker is to read the data placed
    in S3\. The data may not be readily consumable most of the time. If the datasets
    required are large, then wrangling the data in the Jupyter notebook may not be
    practical. In such cases, Spark EMR clusters can be employed to conduct operations
    on big data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在SageMaker中执行模型的设计模式是读取存储在S3中的数据。大多数情况下，这些数据可能并不容易直接使用。如果所需数据集较大，在Jupyter笔记本中整理数据可能不切实际。在这种情况下，可以使用Spark
    EMR集群对大数据进行操作。
- en: Wrangling a big dataset in Jupyter notebooks results in out-of-memory errors.
    Our solution is to employ AWS EMR (Elastic MapReduce) clusters to conduct distributed
    data processing. Hadoop will be used as the underlying distributed filesystem
    while Spark will be used as the distributed computing framework.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jupyter笔记本中清理大数据集时会出现内存溢出错误。我们的解决方案是使用AWS EMR（弹性MapReduce）集群进行分布式数据处理。Hadoop将作为底层分布式文件系统，而Spark将作为分布式计算框架使用。
- en: Now, to run commands against the EMR cluster to process big data, AWS offers
    EMR notebooks. EMR notebooks provide a managed notebook environment, based on
    Jupyter Notebook. These notebooks can be used to interactively wrangle large data,
    visualize the same, and prepare analytics-ready datasets. Data engineers and data
    scientists can employ a variety of languages, Python, SQL, R, and Scala, to process
    large volumes of data. These EMR notebooks can also be saved periodically to a
    persistent data store, S3, so the saved work can be retrieved later. One of the
    critical components of Amazon EMR architecture is the Livy service. It is an open
    source REST interface for interacting with Spark clusters without the need for
    Spark client. The Livy service enables communication between the EMR notebook
    and EMR cluster, where the service is installed.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了对 EMR 集群执行命令来处理大数据，AWS 提供了 EMR 笔记本。EMR 笔记本提供了一个基于 Jupyter Notebook 的托管笔记本环境。用户可以通过这些笔记本交互地处理大数据，进行可视化并准备适合分析的数据集。数据工程师和数据科学家可以使用多种语言（如
    Python、SQL、R 和 Scala）来处理大量数据。这些 EMR 笔记本也可以定期保存到持久数据存储（如 S3），以便稍后检索已保存的工作。Amazon
    EMR 架构的关键组件之一是 Livy 服务。Livy 是一个开源的 REST 接口，用于与 Spark 集群交互，无需 Spark 客户端。Livy 服务使
    EMR 笔记本与安装了该服务的 EMR 集群之间能够进行通信。
- en: 'The following architecture diagram details how EMR notebooks communicate with
    Spark EMR clusters to process large data:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下架构图详细说明了 EMR 笔记本如何与 Spark EMR 集群通信以处理大数据：
- en: '![](img/a8460360-5965-4e0f-9beb-afc46b6cffb1.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a8460360-5965-4e0f-9beb-afc46b6cffb1.png)'
- en: 'Now that we''ve looked at how EMR clusters interact with EMR notebooks to process
    big data interactively, let''s begin by creating an EMR notebook and cluster,
    as shown in the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 EMR 集群如何与 EMR 笔记本交互处理大数据，接下来我们将开始创建 EMR 笔记本和集群，如下所示：
- en: Navigate to Amazon EMR under Services and click on Notebooks.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在“服务”中导航到 Amazon EMR 并点击笔记本。
- en: 'In the Create notebook page, enter Notebook name and Description, as shown
    in the following screenshot:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建笔记本页面，输入笔记本名称和描述，如下所示截图：
- en: '![](img/aa257df7-8459-46e0-85cb-cdf5d2e72172.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa257df7-8459-46e0-85cb-cdf5d2e72172.png)'
- en: Next, select the option Create a cluster, enter Cluster name, and select Instance
    type and number. As you can see in the preceding screenshot, the EMR cluster comes
    with Hadoop, Spark, Livy, and Hive applications.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，选择“创建集群”选项，输入集群名称，并选择实例类型和数量。如前面的截图所示，EMR 集群自带 Hadoop、Spark、Livy 和 Hive
    应用程序。
- en: 'Now, let''s review the policies of EMR role and EC2 instance profile and enter
    the S3 location where EMR notebooks will be saved, as in the following:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下 EMR 角色和 EC2 实例配置文件的策略，并输入 EMR 笔记本将保存的 S3 位置，如下所示：
- en: '![](img/b9185ba5-2ac3-40e9-84cc-b48651fdf7f4.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9185ba5-2ac3-40e9-84cc-b48651fdf7f4.png)'
- en: 'From the preceding visual, we can see the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的可视化图中，我们可以看到以下内容：
- en: The EMR role is used to give the EMR service access to other AWS services (for
    example, EC2).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EMR 角色用于授予 EMR 服务访问其他 AWS 服务（例如，EC2）的权限。
- en: The EMR EC2 instance profile further enables EC2 instances launched by EMR to
    have access to other AWS services (for example, S3).
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EMR EC2 实例配置文件进一步使得由 EMR 启动的 EC2 实例可以访问其他 AWS 服务（例如，S3）。
- en: We configured appropriate security groups around the EMR cluster to allow communication
    between the EMR notebook and master node of the EMR cluster.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们为 EMR 集群配置了适当的安全组，以允许 EMR 笔记本与 EMR 集群的主节点进行通信。
- en: We also assigned a service role to the EMR cluster, so it can interact with
    other AWS services.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还为 EMR 集群分配了一个服务角色，以便它能够与其他 AWS 服务进行交互。
- en: Also, EMR notebooks are saved to the designated S3 location when you click on
    Save in EMR notebooks.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，EMR 笔记本在点击“保存”时会被保存到指定的 S3 位置。
- en: 'Now, click on Create notebook to launch a new EMR notebook. The notebook and
    cluster will start provisioning, as shown in the following:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，点击“创建笔记本”以启动一个新的 EMR 笔记本。笔记本和集群将开始创建，如下所示：
- en: '![](img/48c2f140-2144-48d3-b56c-8790b6c073e1.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/48c2f140-2144-48d3-b56c-8790b6c073e1.png)'
- en: Once the EMR notebook and cluster are provisioned, click on Open to open the
    notebook. We will use the EMR notebook to create a dataset that will be used to
    recommend books to users via the object2vec algorithm, which is a built-in SageMaker
    algorithm used to predict the affinity of a user toward a book.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦 EMR 笔记本和集群创建完成，点击“打开”以打开笔记本。我们将使用 EMR 笔记本创建一个数据集，该数据集将通过 object2vec 算法向用户推荐书籍。object2vec
    是一个内置的 SageMaker 算法，用于预测用户对书籍的亲和力。
- en: 'In the EMR notebook, we do five things:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在 EMR 笔记本中，我们做了以下五个步骤：
- en: Read the ratings and books CSV files.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取评分和书籍的 CSV 文件。
- en: Analyze the ratings dataset to understand the number of ratings by user and
    book.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析评分数据集，以了解每个用户和每本书的评分数量。
- en: Filter the original ratings dataset to only include ratings, where it contains
    users who have rated more than 1% of books and books that have been rated by at
    least 2% of the users.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过滤原始评分数据集，只包含评分，其中包括评分超过1%的用户和至少有2%用户评分的书籍。
- en: Create indexes (starting with zero) for both users and books in the ratings
    dataset—this is required to train the `object2vec` algorithm.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在评分数据集中为用户和书籍创建索引（从零开始）——这是训练`object2vec`算法所必需的。
- en: Write (in parquet format) the resulting ratings dataset, which also includes
    the book title, to relevant S3 bucket. The ratings dataset will then have a rich
    history of user preferences, along with the popularity of books.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将处理后的评分数据集（包括书名）写入相关的 S3 存储桶，并以`parquet`格式存储。评分数据集将包含丰富的用户偏好历史，以及书籍的受欢迎程度。
- en: 'In the following code block, we will whittle down 6 million ratings to ~1 million:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们将把600万条评分数据减少到约100万条：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding code, we filtered ratings to include users who have rated at
    least 130 books and books that have been rated by at least 1,200 users.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们过滤了评分数据，保留了至少评分了130本书的用户和至少有1200个用户评分的书籍。
- en: 'Once the ratings dataset is prepared, we''ll persist it to S3 bucket, as shown
    in the following:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦准备好评分数据集，我们将把它持久化到 S3 存储桶中，如下所示：
- en: '![](img/bcbad147-df09-42ca-a85c-ca24d5dd54a0.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bcbad147-df09-42ca-a85c-ca24d5dd54a0.png)'
- en: 'From the preceding screenshot, the following is understood:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的截图中可以理解以下内容：
- en: Since the data is parallel processed on the EMR cluster, the output contains
    several `parquet` files.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于数据在 EMR 集群上并行处理，输出包含多个`parquet`文件。
- en: Apache Parquet is an open source compressed columnar storage format in the Apache
    Hadoop ecosystem.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Parquet 是 Apache Hadoop 生态系统中的一种开源压缩列式存储格式。
- en: Compared to the traditional approach where data is stored in a row-oriented
    approach, Parquet allows us to be more efficient in terms of storage and performance.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与传统的行式数据存储方式相比，Parquet 允许我们在存储和性能方面更加高效。
- en: Stop the notebook and terminate the cluster after you are done storing the processed
    dataset in S3 to avoid unnecessary costs.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将处理后的数据集存储到 S3 后，停止笔记本并终止集群，以避免不必要的费用。
- en: Now, we are ready to understand the built-in `object2vec` algorithm and start
    training the model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好理解内置的`object2vec`算法并开始训练模型。
- en: Conducting training in Amazon SageMaker
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Amazon SageMaker 中进行训练
- en: Let's begin by spending a few minutes understanding how the `object2vec` algorithm
    works. It is a multi-purpose algorithm that can create lower dimensional embeddings
    of higher dimensional objects. This process is known as dimensionality reduction,
    most commonly implemented through a statistical procedure called **Principal Component
    Analysis** (**PCA**). However, Object2Vec uses neural networks to learn these
    embeddings.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花几分钟时间理解一下`object2vec`算法是如何工作的。它是一种多用途算法，可以创建高维对象的低维嵌入。这一过程被称为维度约简，通常通过一种叫做**主成分分析**（**PCA**）的统计方法来实现。然而，Object2Vec
    使用神经网络来学习这些嵌入。
- en: Some of the common applications of these embeddings include customer segmentation
    and product search. In the case of customer segmentation, similar customers appear
    closer in the lower dimensional space. A customer can be defined through multiple
    attributes such as name, age, home address, and email address. With regards to
    product search, because product embeddings capture the semantics of the underlying
    data, any combination of search terms can be used to retrieve the target product.
    The embedding of these search terms (semantics) should just match that of the
    product.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入的常见应用包括客户细分和产品搜索。在客户细分的情况下，相似的客户在低维空间中会彼此接近。客户可以通过多个属性进行定义，如姓名、年龄、家庭地址和电子邮件地址。关于产品搜索，由于产品嵌入捕捉到基础数据的语义，任何搜索词的组合都可以用来检索目标产品。这些搜索词的嵌入（语义）应与产品的嵌入相匹配。
- en: Let's look at how Object2Vec works.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 Object2Vec 是如何工作的。
- en: Learning how Object2Vec Works
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习 Object2Vec 是如何工作的
- en: Object2vec can learn embeddings of pairs of objects. In our case, the higher
    the rating of the book, the stronger the relationship between the user and the
    book. The idea is that users with similar tastes are likely to rate similar books
    higher. Object2vec approximates the book rating by using embeddings of users and
    books. The closer a user is to some books, the higher the rating given by that
    user to the books. We provide the algorithm with `(user_ind` and `book_ind)` pairs;
    for each such pair, we also provide a **label** that tells the algorithm whether
    the user and book are similar or not. The **label** in our case is the book rating.
    Therefore, the trained model can be used to predict the rating of a book for a
    given user such as the book; in this case, the one which has never been rated
    by the user.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Object2vec 可以学习对象对的嵌入。在我们的案例中，书籍的评分越高，用户与书籍之间的关系就越强。这个想法是，具有相似品味的用户往往会给相似的书籍更高的评分。Object2vec
    通过使用用户和书籍的嵌入来近似书籍评分。用户与某些书籍的接近程度越高，用户对这些书籍的评分也会越高。我们为算法提供 `(user_ind` 和 `book_ind)`
    对；对于每一对这样的组合，我们还提供一个 **标签**，告诉算法用户与书籍是否相似。在我们的案例中，**标签**就是书籍评分。因此，训练好的模型可以用来预测某个给定用户的书籍评分，例如用户从未评分过的书籍。
- en: 'Following is the conceptual diagram of how `object2vec` works:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 `object2vec` 工作原理的概念图：
- en: '![](img/c119c8a2-2164-40f7-8851-9110ce8d48f1.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c119c8a2-2164-40f7-8851-9110ce8d48f1.png)'
- en: 'From the preceding visual, we can see the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图示中，我们可以看到以下内容：
- en: We can see that the user and item or book embeddings are concatenated, which
    are then passed to the **Multiple Layer Perceptron** (**MLP**).
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以看到，用户和项目或书籍的嵌入被拼接在一起，然后传递给 **多层感知机**（**MLP**）。
- en: User and book embeddings are created from a one-hot encoded representation of
    user and book indexes respectively.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户和书籍的嵌入是通过用户和书籍索引的独热编码表示生成的。
- en: Through supervised learning, MLP can learn the weights of the network and these
    weights can be used to predict score or rating of user-book pair.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过监督学习，MLP 可以学习网络的权重，这些权重可以用来预测用户-书籍对的评分或分数。
- en: 'To further understand the inner workings of `object2vec`, see the following
    screenshot:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解 `object2vec` 的内部工作原理，请参见以下截图：
- en: '![](img/ed402b5e-223e-4a8e-b70d-094957f3f948.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed402b5e-223e-4a8e-b70d-094957f3f948.png)'
- en: 'From the preceding visual, we can see the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图示中，我们可以看到以下内容：
- en: Object2vec starts with representing user and book with one-hot encoding. To
    explain, in our case, a user can be represented with an array of the size 12,347,
    which means that there are a total of 12,347 unique users in the dataset.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Object2vec 从使用独热编码（one-hot encoding）表示用户和书籍开始。具体来说，在我们的案例中，用户可以用大小为 12,347 的数组表示，这意味着数据集中总共有
    12,347 个独特的用户。
- en: 'User #1 can be represented by denoting 1 at position 1, while all of the other
    positions in the array have zeros.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '用户 #1 可以通过在位置 1 上表示 1，而数组中的所有其他位置为 0 来表示。'
- en: Books can also be represented in a comparable manner.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 书籍也可以以类似的方式表示。
- en: It is time to now reduce the dimensionality of these representations. Therefore,
    the algorithm uses an embedding layer with 1,024 neurons, each for a user and
    a book.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在是时候减少这些表示的维度了。因此，算法使用了一个嵌入层，包含 1,024 个神经元，每个神经元对应一个用户和一本书。
- en: Object2vec further extracts additional features by conducting element-wise multiplication
    and subtraction between 1,024 user embedding neurons and 1,024 item embedding
    neurons.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Object2vec 通过对 1,024 个用户嵌入神经元和 1,024 个项目嵌入神经元进行逐元素的乘法和减法操作，进一步提取额外的特征。
- en: In other words, the user and book embeddings are compared in different ways.
    Overall, we will then have 4,096 neurons when all of the neurons from the previous
    layers are merged. The algorithm then uses a single perceptron layer with 256
    neurons. This perceptron layer is then fully connected to the output layer with
    one neuron. This one neuron will then predict the rating of a book given by a
    user.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，用户和书籍的嵌入是以不同的方式进行比较的。总的来说，当所有前一层的神经元被合并时，我们将有 4,096 个神经元。然后，算法使用一个包含 256
    个神经元的感知机层。这个感知机层与输出层的一个神经元完全连接。这个神经元将预测用户给书籍的评分。
- en: It is now time to train the Object2Vec algorithm
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候训练 Object2Vec 算法了
- en: Training the Object2Vec algorithm
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 Object2Vec 算法
- en: 'Now that we have an understanding of how the algorithm works, let''s dive into
    the training process:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了算法的工作方式，让我们深入了解训练过程：
- en: '**Data processing**: Feed data in the form of JSON lines; random shuffle the
    data for optimal performance. As you will see later, we send data in the format
    of `user index`, `book index`, `label=rating`.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据处理**：以 JSON 行的形式输入数据；对数据进行随机打乱，以实现最佳性能。如您稍后将看到的，我们以`用户索引`、`书籍索引`、`标签=评分`的格式发送数据。'
- en: '**Model training**: We pass both training and validation data to the algorithm.
    There are multiple hyperparameters that we can configure to fine-tune the model''s
    performance. We will review them in the upcomings sections. The objective function,
    in our case, is to minimize the **Mean Squared Error** (**MSE**). The error is
    the difference between the label (actual value) and the predicted rating.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型训练**：我们将训练数据和验证数据传递给算法。我们可以配置多个超参数来微调模型的性能。我们将在接下来的部分中回顾它们。我们的目标函数是最小化**均方误差**（**MSE**）。误差是标签（实际值）和预测评分之间的差异。'
- en: Once the model has been trained, we will deploy it as an endpoint for inference.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们将其部署为推理端点。
- en: 'In data processing, we will do the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据处理过程中，我们将执行以下操作：
- en: 'First, we will read the ratings dataset stored in `parquet` format on the S3
    bucket, as shown in the following code:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将读取存储在 S3 存储桶中的 `parquet` 格式评分数据集，如下代码所示：
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the preceding code, we can see the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们可以看到以下内容：
- en: '`s3fs` is a Python library that is based on boto3, an AWS SDK for Python. `s3fs`
    provides a filesystem interface for S3.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s3fs` 是一个基于 boto3 的 Python 库，boto3 是 AWS 的 Python SDK。`s3fs` 提供了一个 S3 文件系统接口。'
- en: We use the `pyarrow` Python library to read partitioned `parquet` files from
    a designated s3 bucket.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 `pyarrow` Python 库从指定的 S3 存储桶读取分区的 `parquet` 文件。
- en: Specifically, we call the `ParquetDataset()` function by passing in the dataset
    name and filesystem.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具体来说，我们通过传入数据集名称和文件系统，调用了 `ParquetDataset()` 函数。
- en: 'After reading the dataset, we display it to ensure that the data is read correctly,
    as shown in the following screenshot:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在读取数据集之后，我们会显示它，以确保数据被正确读取，如下图所示：
- en: '![](img/d9f71163-02e1-4eb9-a117-76d0368b087a.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9f71163-02e1-4eb9-a117-76d0368b087a.png)'
- en: Then, we load the dataframe in a format required by the `Object2Vec` algorithm.
    For each user-book pair and rating label, we create an entry in a data list by
    calling the `load_df_data()` function. Please refer to the source code attached
    to this chapter for details.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们以 `Object2Vec` 算法所需的格式加载数据框。对于每个用户-书籍对和评分标签，我们通过调用 `load_df_data()` 函数在数据列表中创建一个条目。有关详细信息，请参考本章附带的源代码。
- en: 'In model training, we start by partitioning the dataset into training, validation,
    and test sets. For each of the sets, we call the `write_data_list_to_jsonl()`
    function to create `.jsonl` (JSON lines) files, the format required by `object2vec`.
    A sample `jsonl` file is shown in the following screenshot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练中，我们首先将数据集划分为训练集、验证集和测试集。对于每个数据集，我们调用 `write_data_list_to_jsonl()` 函数来创建
    `.jsonl`（JSON 行）文件，这是 `object2vec` 所需的格式。以下截图展示了一个 `jsonl` 文件的示例：
- en: '![](img/d714d5cd-cbc6-46e5-9e6f-54f76a075332.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d714d5cd-cbc6-46e5-9e6f-54f76a075332.png)'
- en: Then, we upload the prepared datasets to the designated S3 bucket.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将准备好的数据集上传到指定的 S3 存储桶。
- en: 'We obtain a Docker image of the Object2Vec algorithm, as follows:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们获取了 Object2Vec 算法的 Docker 镜像，如下所示：
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the preceding code, we can see the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们可以看到以下内容：
- en: To get the **Uniform Resource Identifier** (**URI**) of the `object2vec` Docker
    image, we called the `get_image_uri()` function by passing the region name of
    the local SageMaker session and the name of the algorithm as input.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了获取 `object2vec` Docker 镜像的**统一资源标识符**（**URI**），我们通过传入本地 SageMaker 会话的区域名称和算法名称作为输入，调用了
    `get_image_uri()` 函数。
- en: The `get_image_uri()` function is part of the SageMaker Python SDK.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_image_uri()` 函数是 SageMaker Python SDK 的一部分。'
- en: 'After obtaining the `uri` of the `object2vec` algorithm, we define the hyperparameters,
    as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 获取到 `object2vec` 算法的 `uri` 后，我们定义超参数，如下所示：
- en: '**Encoder network**: This includes the following:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器网络**：包括以下内容：'
- en: '`enc0_layers`: This is the number of layers in the encoder network.'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`enc0_layers`：这是编码器网络中的层数。'
- en: '`enc0_max_seq_len`: This is the maximum number of sequences sent to the encoder
    network (in this case, only one user sequence is sent to the network).'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`enc0_max_seq_len`：这是发送到编码器网络的最大序列数（在本例中，仅发送一个用户序列到网络）。'
- en: '`enc0_network`: This defines how embeddings are handled. In this case, since
    we address one user embedding at a time, no aggregation is necessary.'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`enc0_network`: 这定义了如何处理嵌入。在这种情况下，由于我们一次处理一个用户嵌入，因此不需要聚合。'
- en: '`enc0_vocab_size`: This defines the first encoder vocabulary size. It represents
    the number of users in the dataset.'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`enc0_vocab_size`: 这定义了第一个编码器的词汇表大小。它表示数据集中用户的数量。'
- en: 'Since there are two encoders in the network, the same hyperparameters apply
    for encoder 1\. For encoder 1, the vocabulary size needs to be defined appropriately,
    which is the number of books in the dataset—`enc1_vocab_size: 985`.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '由于网络中有两个编码器，因此相同的超参数适用于编码器 1。对于编码器 1，词汇表大小需要适当定义，即数据集中书籍的数量——`enc1_vocab_size:
    985`。'
- en: '**MLP**: This includes the following:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLP**: 这包括以下内容：'
- en: '`mlp_dim`: This is the number of neurons in the MLP layers. In our experiment,
    we set it to 256.'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp_dim`: 这是 MLP 层中的神经元数量。在我们的实验中，我们将其设置为 256。'
- en: '`mlp_layers`: This is the number of layers in the MLP network. We use a single
    layer in our experiment.'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp_layers`: 这是 MLP 网络中的层数。在我们的实验中，我们使用的是单层。'
- en: '`mlp_activation`: This is the activation function for MLP layers. In our experiment,
    we use the **Rectified Linear Unit** (**ReLU**) activation function for faster
    convergence and to avoid vanishing gradient issues. Note that the ReLU activation
    function is given by ![](img/d77d47c4-5a7c-4fa7-8ee1-ac2b6e4dd1d8.png).'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp_activation`: 这是 MLP 层的激活函数。在我们的实验中，我们使用**修正线性单元**（**ReLU**）激活函数，以加速收敛并避免梯度消失问题。请注意，ReLU
    激活函数由以下公式给出：![](img/d77d47c4-5a7c-4fa7-8ee1-ac2b6e4dd1d8.png)。'
- en: '**The following instances control how** `object2vec` **is trained:**'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**以下实例控制** `object2vec` **的训练过程：**'
- en: '`epochs`: This is the number of backward and forward passes. We use 10 in our
    case.'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs`: 这是向后和向前传播的次数。在我们的实验中使用的是 10。'
- en: '`mini_batch_size`: This is the number of training examples to process before
    updating weights. We use 64.'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mini_batch_size`: 这是在更新权重之前处理的训练样本数量。我们使用 64。'
- en: '`early_stopping_patience`: This is the maximum number of bad epochs (epochs
    where loss does not improve) that are executed before stopping. We use 2.'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`early_stopping_patience`: 这是在停止之前执行的最大坏的 epoch 数（即损失没有改善的 epoch）。我们使用的是 2。'
- en: '`early_stopping_tolerance`: This is the improvement in loss function required
    between two consecutive epochs for training to continue. This is after the number
    of patience epochs conclude. We use 0.01 for this parameter.'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`early_stopping_tolerance`: 这是在连续两个 epoch 之间，损失函数所需的改善量，只有在耐心 epoch 数结束后，训练才会继续。我们为该参数使用
    0.01。'
- en: '**Others** includes the following:'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**其他** 包括以下内容：'
- en: '`optimizer`: This is the optimization algorithm to arrive at optimal network
    parameters. In this experiment, we use adaptive moment estimation, also known
    as Adam. It computes the individual learning rate for each parameter. Parameters
    pertaining to features or inputs with sparse data go through large updates relative
    to the ones with dense data. Also, Adam computes individual momentum changes for
    each of the parameters. Remember that, during backpropagation, it is important
    to navigate in the right direction for faster convergence. Momentum changes help
    to navigate in the correct direction.'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer`: 这是优化算法，用于找到最优的网络参数。在本实验中，我们使用自适应矩估计（也称为 Adam）。它为每个参数计算个别的学习率。与密集数据的特征或输入相比，稀疏数据的参数会进行较大的更新。此外，Adam
    还会为每个参数计算个别的动量变化。请记住，在反向传播时，导航至正确的方向对加速收敛至关重要。动量变化有助于朝正确的方向导航。'
- en: '`output_layer`: This defines whether the network is a classifier or a regressor.
    In this case, since the network is trying to learn to rate, we define the output
    layer as a mean squared error (linear).'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_layer`: 这定义了网络是分类器还是回归器。在这种情况下，由于网络试图学习评分，因此我们将输出层定义为均方误差（线性）。'
- en: 'After the hyperparameters have been defined, we fit the `object2vec` estimator
    to the prepared datasets (train and validation), as shown in the following code:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了超参数后，我们将 `object2vec` 估算器拟合到准备好的数据集（训练集和验证集），如下所示的代码：
- en: '[PRE3]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the preceding code, we are doing the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们正在做以下操作：
- en: We begin by creating an `object2vec` estimator by passing the Docker image,
    current execution role, number, and type of training instances, and current `sagemaker`
    session.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先通过传递 Docker 镜像、当前执行角色、训练实例的数量和类型，以及当前的 `sagemaker` 会话来创建一个 `object2vec`
    估算器。
- en: We then set hyperparameters for the newly created `object2vec` estimator using
    the `set_hyperparameters()` function.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用 `set_hyperparameters()` 函数为新创建的 `object2vec` 评估器设置超参数。
- en: Then, we fit the model to the training and validation datasets using the `fit()`
    function of the `Estimator` object.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用 `Estimator` 对象的 `fit()` 函数将模型拟合到训练和验证数据集。
- en: The duration of training depends on the training instance type and the number
    of instances. For one `m5.4xlarge` machine learning instance, it took 2 hours
    to complete 10 epochs.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练持续时间取决于训练实例类型和实例数量。对于一个 `m5.4xlarge` 机器学习实例，完成 10 个周期花费了 2 小时。
- en: 'To monitor the training job in progress, navigate to the Training section on
    the left-hand side of the SageMaker service. Click on Training Jobs and then on
    the job name of your current job. After, navigate to the monitor section to see
    the training job''s progress, as shown in the following screenshot:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 要监视正在进行的训练作业，请导航到 SageMaker 服务左侧的 Training 部分。单击 Training Jobs，然后单击当前作业的作业名称。然后，导航到监控部分以查看训练作业的进度，如下截图所示：
- en: '![](img/6dd6d633-0114-44bf-8d98-be745ae67bfb.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6dd6d633-0114-44bf-8d98-be745ae67bfb.png)'
- en: As you can see in the preceding screenshot, as the training MSE decreases, the
    validation MSE also decreases—although, in the validation dataset, the decrease
    in error is not as steep as the decrease in the training dataset. The training
    throughput can also be monitored through this dashboard.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在前面的截图中所示，随着训练 MSE 的减少，验证 MSE 也在减少，尽管在验证数据集中，误差的减少不及训练数据集的减少。通过此仪表板还可以监控训练吞吐量。
- en: Now that the training is done, let's deploy the trained model as an endpoint
    for inference.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在训练完成了，让我们将训练好的模型作为端点进行部署。
- en: Deploying the trained Object2Vec and running inference
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署训练好的 Object2Vec 并进行推断
- en: 'Now, let''s deploy the trained `object2vec` model. The SageMaker SDK offers
    methods so that we can seamlessly deploy trained models:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们部署训练好的 `object2vec` 模型。SageMaker SDK 提供了方法，使我们可以无缝部署训练好的模型：
- en: 'First, we will create a model from the training job using the `create_model()`
    method of the SageMaker `Estimator` object, as shown in the following code:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将使用 SageMaker `Estimator` 对象的 `create_model()` 方法从训练作业创建模型，如下所示的代码：
- en: '[PRE4]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To the `create_model()` method, we passed the type of serializers and deserializers
    to be used for the payload at the time of inference.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 将序列化器和反序列化器类型传递给 `create_model()` 方法，在推断时用于负载。
- en: 'Once the model has been created, it can be deployed as an endpoint via the
    `deploy()` method of the SageMaker `Model` object, as shown in the following code:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型创建完成，可以通过 SageMaker `Model` 对象的 `deploy()` 方法将其部署为端点，如下所示的代码：
- en: '[PRE5]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To the `deploy()` method, we have specified the number and type of instances
    that you have to launch to host the endpoint.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `deploy()` 方法中，我们已指定启动托管端点所需的实例数量和类型。
- en: 'Once the `object2vec` model has been deployed as an endpoint, we can navigate
    to the Endpoints section under the Inference grouping (present on the left navigation
    menu under the SageMaker service). The status of the deployed endpoint can be
    viewed here, as shown in the following screenshot:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦 `object2vec` 模型部署为端点，我们可以导航到左侧导航菜单下的 Inference 分组下的 Endpoints 部分。可以在此查看部署端点的状态，如下截图所示：
- en: '![](img/15cbd4d2-7734-4865-8f38-d9f8d352ef52.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15cbd4d2-7734-4865-8f38-d9f8d352ef52.png)'
- en: Now that we have the `object2vec` endpoint available, let's run inference.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 `object2vec` 端点已准备就绪，让我们进行推断。
- en: 'We will create the `RealTimePredictor` object (the SageMaker Python SDK) by
    passing the endpoint name, along with the type of serialization and deserialization
    for the input and output, respectively. See the following code on how to initialize
    the `RealTimePredictor` object:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过传递端点名称以及输入和输出的序列化和反序列化类型，我们将创建 `RealTimePredictor` 对象（SageMaker Python SDK）。请参阅以下代码，了解如何初始化
    `RealTimePredictor` 对象：
- en: '[PRE6]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can change the endpoint name to reflect your current endpoint (the first
    argument of the `RealTimePredictor` object).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以更改端点名称以反映您当前的端点（`RealTimePredictor` 对象的第一个参数）。
- en: 'We then invoke the `predict()` method of `RealTimePredictor`, as shown in the
    following code:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们调用 `RealTimePredictor` 的 `predict()` 方法，如下所示的代码：
- en: '[PRE7]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the preceding code, remember that `test_data` should be in a format that''s
    consumable by `object2vec.` We use the `data_list_to_inference_format()` function
    to transform the test data into two components: instances and label. For details
    on this function, please see the source code associated with this chapter. Check
    out the following screenshot to see how the test data should be structured:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，请记住，`test_data`应该是一个`object2vec`可以处理的格式。我们使用`data_list_to_inference_format()`函数将测试数据转换为两个组件：实例和标签。有关此函数的详细信息，请参阅本章相关的源代码。请查看以下截图，了解测试数据的结构：
- en: '![](img/87ccadde-beb8-4ac8-8ad7-e07f9b28168d.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87ccadde-beb8-4ac8-8ad7-e07f9b28168d.png)'
- en: 'As shown in the preceding screenshot, the inputs for 0 (`in0`) and 1 (`in1`)
    should have the indexes of the user and book, respectively. As for the test label,
    we produce a data list of ratings for each of the associated user-book pairs,
    as shown in the following screenshot:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，`in0`（输入0）和`in1`（输入1）的索引分别应为用户和书籍的索引。至于测试标签，我们为每个关联的用户-书籍对生成一个评分数据列表，如下图所示：
- en: '![](img/21b95547-7e80-4592-9686-68aec3f766cf.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21b95547-7e80-4592-9686-68aec3f766cf.png)'
- en: As shown in the preceding screenshot, we pass the first 100 user-book pairs
    from the test dataset to the `predict()` method of `RealTimePredictor`. The result
    is an MSE of 0.110.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，我们将测试数据集中的前100个用户-书籍对传递给`RealTimePredictor`的`predict()`方法。结果是MSE为0.110。
- en: 'Now, let''s compare this MSE with the MSE from the naive options of computing
    book ratings:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将这个MSE与通过简单方法计算书籍评分的MSE进行比较：
- en: '**Baseline 1**: For each user-book pair in the test dataset, compute the rating,
    which is the average book ratings across all of the users, as shown in the following
    code:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基准 1**：对于测试数据集中的每个用户-书籍对，计算评分，即所有用户的平均书籍评分，如下代码所示：'
- en: '[PRE8]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To compute the average rating across all users, we do the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算所有用户的平均评分，我们执行以下操作：
- en: We iterate through all of the ratings in the training dataset to create a labels
    list, `train_label`.
  id: totrans-155
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们遍历训练数据集中的所有评分，以创建标签列表`train_label`。
- en: '`train_label` is then used to compute the mean. To calculate the MSE, in the
    `get_mse_loss()` function, the average rating across all of the users is subtracted
    from each of the ratings in `test_label`.'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_label`接着被用来计算均值。为了计算MSE，在`get_mse_loss()`函数中，从`test_label`中的每个评分中减去所有用户的平均评分。'
- en: The error is then squared and averaged across all of the test users. Please
    see the attached source code for details. The MSE from this option is 1.13.
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后将误差平方，并在所有测试用户中求平均。有关详细信息，请参阅附带的源代码。这个选项的MSE为1.13。
- en: '**Baseline 2**: For each user-book pair in the test dataset, we compute the
    rating, which is the average book rating for that user (that is, the average rating
    across all books rated by the user), as shown in the following code:'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基准 2**：对于测试数据集中的每个用户-书籍对，我们计算评分，即该用户的平均书籍评分（即该用户所有评分书籍的平均评分），如下代码所示：'
- en: '[PRE9]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the `bs2_predictor()` function, we passed the test data and user dictionary
    from the training dataset as inputs. For each user in the test data, if they exist
    in the training dataset, we computed the average book rating across all of the
    books rated by them. If they do not exist in the training dataset, we just get
    the average rating across all of the users, as shown in the following code:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在`bs2_predictor()`函数中，我们将测试数据和来自训练数据集的用户字典作为输入。对于测试数据中的每个用户，如果该用户存在于训练数据集中，我们计算该用户评分的所有书籍的平均评分。如果该用户不存在于训练数据集中，我们就获取所有用户的平均评分，如下代码所示：
- en: '[PRE10]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the preceding `bs2_predictor()` function, the `zip(*)` function is used to
    return lists of books and ratings for each user. `bs1_prediction` is the average
    rating across all of the users in the training dataset. The MSE from this option
    is 0.82.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的`bs2_predictor()`函数中，`zip(*)`函数用于返回每个用户的书籍和评分列表。`bs1_prediction`是训练数据集中所有用户的平均评分。这个选项的MSE为0.82。
- en: 'As we can see, an MSE of 0.110 from `object2vec` is better than the baselines:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，`object2vec`的MSE为0.110，优于基准模型：
- en: '**Baseline 1 MSE**: 1.13, where the predicted book rating is the global average
    book rating across all users'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基准 1 MSE**：1.13，其中预测的书籍评分是所有用户的全局平均书籍评分'
- en: '**Baseline 2 MSE**: 0.82, where the predicted book rating is the average book
    rating by user'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基准 2 MSE**：0.82，其中预测的书籍评分是用户的平均书籍评分'
- en: Now that we have trained and evaluated the built-in SageMaker algorithm, `object2vec`,
    it is time to understand the features that SageMaker offers so that we can automate
    hyperparameter tuning.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练并评估了内置的 SageMaker 算法 `object2vec`，是时候了解 SageMaker 提供的功能，以便我们能够自动化超参数调优。
- en: Running hyperparameter optimization (HPO)
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行超参数优化（HPO）
- en: It takes data scientists numerous hours and experiments to arrive at an optimal
    set of hyperparameters that are required for best model performance. This process
    is mostly based on trial and error.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家通常需要花费大量时间和实验才能找到适合最佳模型性能的超参数集。这个过程大多依赖于试错法。
- en: Although `GridSearch` is one of the techniques that is traditionally used by
    data scientists, it suffers from the curse of dimensionality. For example, if
    we have two hyperparameters, with each taking five possible values, we're looking
    at calculating objective function 25 times (5 x 5). As the number of hyperparameters
    grows, the number of times that the objective function is computed blows out of
    proportion.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 `GridSearch` 是数据科学家传统上使用的技术之一，但它面临维度灾难问题。例如，如果我们有两个超参数，每个超参数有五个可能的值，那么我们需要计算目标函数
    25 次（5 x 5）。随着超参数数量的增加，计算目标函数的次数呈指数级增长。
- en: Random Search addresses this issue by randomly selecting values of hyperparameters,
    without doing an exhaustive search of every single combination of hyperparameters.
    This [paper](http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf)
    by Bergstra et al. claims that a random search of the parameter space is guaranteed
    to be more effective than a grid search.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索通过随机选择超参数的值来解决这个问题，而不是对每一个超参数组合进行穷举搜索。Bergstra 等人发表的 [论文](http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf)
    认为，超参数空间的随机搜索比网格搜索更有效。
- en: 'The idea is that some parameters have much less effect than others on the objective
    function. This is reflected by the number of values that are picked for each parameter
    in the grid search. Random Search enables the exploration of more values for each
    parameter, given several trials. The following is a diagram that illustrates the
    difference between grid search and random search:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这个理念是，一些参数对目标函数的影响远小于其他参数。这一点通过网格搜索中每个参数选择的值的数量得到体现。随机搜索使得在多次试验下可以探索每个参数的更多值。以下是一个示意图，展示了网格搜索和随机搜索之间的区别：
- en: '![](img/5af628ab-0cee-4750-8500-fdc7c9768e62.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5af628ab-0cee-4750-8500-fdc7c9768e62.png)'
- en: As you can see from the preceding screenshot, in a random search, we can test
    more values for important parameters, resulting in increased performance from
    training a model.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的截图所示，在随机搜索中，我们可以测试更多重要参数的值，从而通过训练模型提升性能。
- en: 'Neither of these techniques automate the process of hyperparameter optimization.
    **Hyperparameter Optimization** (**HPO**), from SageMaker, automates the process
    of selecting the optimal combination of hyperparameters. Here is how the tool
    works:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种技术都没有自动化超参数优化过程。**超参数优化**（**HPO**），来自 SageMaker，自动化了选择最佳超参数组合的过程。以下是该工具的工作原理：
- en: '![](img/ab83634c-2e45-4199-ae95-39f64bd9c640.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab83634c-2e45-4199-ae95-39f64bd9c640.png)'
- en: 'Take a look at the following points:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 请看以下几点：
- en: HPO uses a Bayesian technique to iteratively select a combination of hyperparameters
    to train the algorithm.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HPO 使用贝叶斯技术，逐步选择超参数组合来训练算法。
- en: HPO picks the next set of hyperparameters, given the performance of the model
    and the configuration of hyperparameters in all of the historical steps.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HPO 根据模型的性能和所有历史步骤中超参数的配置，选择下一组超参数。
- en: Also, it employs an *acquisition function* to determine where the next best
    opportunity is to lower the cost function.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，它还使用一个 *采集函数* 来确定下一个最佳机会，从而降低成本函数。
- en: After a specified number of iterations, you will arrive at an optimal configuration
    of hyperparameters producing the best model.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在指定的迭代次数后，您将获得一个最佳的超参数配置，从而生成最佳模型。
- en: 'For the `object2vec` algorithm, let''s select the hyperparameters that we want
    to tune:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `object2vec` 算法，让我们选择我们想要调整的超参数：
- en: '`learning_rate`: Controls the speed with which weights in the neural network
    are optimized'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：控制神经网络中权重优化的速度'
- en: '`dropout`: The percent of the neurons in a layer that are ignored in forward
    and backward passes'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout`：在前向和反向传播中，忽略的神经元百分比'
- en: '`enc_dim`: The number of neurons to generate user/item embedding'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`enc_dim`：用于生成用户/项目嵌入的神经元数量'
- en: '`mlp_dim`: The number of neurons in the MLP layer'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp_dim`：MLP层中神经元的数量'
- en: '`weight_decay`: A factor to prevent overfitting (L2 regularization—causes the
    weight to decay in proportion to the factor specified)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay`：防止过拟合的因子（L2正则化——使权重按指定的因子衰减）'
- en: We will use the `HyperparameterTuner` class from the `sagemaker` Python SDK
    to create tuning jobs. The goal of the tuning jobs is to reduce the MSE for the
    validation dataset. Depending on your budget and time, you can choose the number
    of training jobs you want to run. In this case, I chose to run 10 jobs, with only
    one job running at a given moment. You can choose to run multiple jobs in parallel.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`sagemaker` Python SDK中的`HyperparameterTuner`类来创建调优作业。调优作业的目标是减少验证数据集的MSE。根据您的预算和时间，您可以选择运行的训练作业数量。在这种情况下，我选择运行10个作业，并且每次只运行一个作业。您也可以选择并行运行多个作业。
- en: 'To instantiate hyperparameter tuning jobs, we will need to do the following:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要实例化超参数调优作业，我们需要执行以下操作：
- en: 'Define the hyperparameters to tune and specify the objective function, as shown
    in the following code:'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义要调优的超参数并指定目标函数，如下代码所示：
- en: '[PRE11]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As shown in the preceding code, we defined the ranges for each of the hyperparameters.
    For the objective function, we specified it as the mean squared error in the validation
    dataset:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，我们定义了每个超参数的范围。对于目标函数，我们指定其为验证数据集中的均方误差：
- en: Define an estimator to train the `object2vec` model.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个估算器来训练`object2vec`模型。
- en: 'Define the `HyperparameterTuner` job by passing the estimator, the objective
    function and type, and the maximum number of jobs to run, as shown here:'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过传递估算器、目标函数及其类型，以及要运行的最大作业数，来定义`HyperparameterTuner`作业，如下所示：
- en: '[PRE12]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `HyperparameterTuner` object takes the estimator (named `regressor`) as
    one of the inputs. The estimator should be initialized with hyperparameters, along
    with the number and type of instances to be launched. Please see the associated
    source code for this chapter.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`HyperparameterTuner`对象以估算器（名为`regressor`）作为输入之一。该估算器应初始化超参数，以及要启动的实例数量和类型。请参见本章的相关源代码。'
- en: 'Fit the tuner to the training and validation datasets, as shown in the following
    code:'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将调优器拟合到训练集和验证集数据集，如以下代码所示：
- en: '[PRE13]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: To the `fit` method of `hyperparameterTuner`, we pass the location of training
    and validation datasets. We wait for the tuner to finish running all of the jobs.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`hyperparameterTuner`的`fit`方法，我们传递了训练和验证数据集的位置。我们等待调优器完成所有作业的运行。
- en: 'The following screenshot shows a few training jobs with a different set of
    hyperparameters that have been executed by `HyperparameterTuner`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了由`HyperparameterTuner`执行的一些训练作业，它们使用了不同的超参数集：
- en: '![](img/69bfb043-79b5-4a23-a772-90d01bb38b2d.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/69bfb043-79b5-4a23-a772-90d01bb38b2d.png)'
- en: With each job, you can look at the hyperparameters that were used and the value
    of the objective function.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个作业，您可以查看所使用的超参数以及目标函数的值。
- en: 'To look at the best job with the lowest MSE, navigate to the Best job tab,
    as shown in the following screenshot:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看具有最低MSE的最佳作业，请导航到最佳作业标签，如下图所示：
- en: '![](img/267713ac-752b-4961-a157-0a8a1989602d.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/267713ac-752b-4961-a157-0a8a1989602d.png)'
- en: After the jobs are executed, you can run analytics on the results from hyperparameter
    optimization to answer questions, such as how does the MSE vary as the tuning
    jobs are being executed? You can also look at whether there is a correlation between
    the MSE and hyperparameters being tuned, such as the learning rate, dropout, weight
    decay, and the number of dimensions for both the encoder and `mlp`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 作业执行后，您可以对超参数优化的结果进行分析，以回答一些问题，例如，随着调优作业的执行，MSE如何变化？您还可以查看MSE与正在调优的超参数（例如学习率、丢弃率、权重衰减、编码器和`mlp`的维度数量）之间是否存在相关性。
- en: 'In the following code, we plot how the MSE changes as the training jobs are
    being executed:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们绘制了随着训练作业的执行，MSE如何变化：
- en: '[PRE14]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the preceding code, we create an analytics object from `HyperparameterTuner`,
    which we created earlier. We then obtain a DataFrame from the analytics object—the
    DataFrame contains the metadata of all of the training jobs that were executed
    by the tuner. We then plot the MSE against time.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们从`HyperparameterTuner`创建了一个分析对象，该对象是我们之前创建的。然后，我们从分析对象中获取一个DataFrame——该DataFrame包含了所有由调优器执行的训练作业的元数据。接着，我们将均方误差（MSE）与时间进行绘图。
- en: 'In the following diagram, we track how the MSE varies with the training time:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们跟踪了MSE如何随训练时间变化：
- en: '![](img/ab528e01-b0cc-4fb3-a4db-39795733b17d.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab528e01-b0cc-4fb3-a4db-39795733b17d.png)'
- en: As you can see, the plot is very bumpy. If you increase the number of training
    jobs, perhaps the hyperparameter tuning job will converge.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，图表波动较大。如果你增加训练任务的数量，也许超参数调优任务会收敛。
- en: It is time to look at another important feature of SageMaker, that is, the experiment
    service or search.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候了解SageMaker的另一个重要特性了，那就是实验服务或搜索功能。
- en: Understanding the SageMaker experimentation service
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解SageMaker实验服务
- en: The goal of experiment management with SageMaker Search is to accelerate the
    model's development and experimentation phase, improving the productivity of data
    scientists and developers, while also reducing the overall time to market machine
    learning solutions.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SageMaker Search进行实验管理的目标是加速模型的开发和实验阶段，提高数据科学家和开发人员的生产力，同时缩短机器学习解决方案的整体上市时间。
- en: The machine learning life cycle (continuous experimentation and tuning) states
    that when you initiate the training of a new learning algorithm, to improve model
    performance, you conduct hyperparameter tuning. With each iteration of the tuning,
    you will need to check how the model's performance is improving.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习生命周期（持续实验与调优）表明，当你启动新学习算法的训练以提升模型性能时，你会进行超参数调优。在每次调优迭代中，你都需要检查模型性能如何提升。
- en: This leads to hundreds and thousands of experiments and model versions. The
    whole process slows down the selection of a final optimized model. Additionally,
    it is critical to monitor the performance of a production model. If the predictive
    performance of the model is degrading, it is important to know how the real-life
    data is different from the data that's used during training and validation.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了成百上千个实验和模型版本。整个过程减缓了最终优化模型的选择。此外，监控生产模型的性能至关重要。如果模型的预测性能下降，了解真实数据与训练和验证过程中使用的数据有何不同非常重要。
- en: 'SageMaker''s Search tackles all of the challenges we highlighted previously
    by providing the following features:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker的Search通过提供以下功能，解决了我们之前提到的所有挑战：
- en: '**Organizing, tracking, and evaluating model training experiments**: Creating
    leaderboards for winning models, cataloging model training runs, and comparing
    models by performance metrics such as training loss and validation accuracy'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组织、跟踪和评估模型训练实验**：创建获胜模型的排行榜，记录模型训练运行，并通过训练损失、验证准确率等性能指标比较模型。'
- en: '**Seamlessly searching and retrieving the most relevant training runs**: Runs
    that can be searched by key attributes, which can be the training job name, status,
    start time, last modified time, and failure reason, among other things'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无缝搜索并检索最相关的训练运行**：可以按关键属性搜索的运行，这些属性可以是训练任务名称、状态、开始时间、最后修改时间、失败原因等。'
- en: '**Tracking the lineage of a deployed model in a live environment**: Tracking
    the training data used, values of the hyperparameters specified, resulting model
    performance, and version of the model deployed'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跟踪已部署模型在实时环境中的来源**：跟踪使用的训练数据、指定的超参数值、模型的表现以及已部署模型的版本。'
- en: 'Let''s illustrate the features of SageMaker Search:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来说明SageMaker Search的特点：
- en: Navigate to Searchon the left navigation pane of the Amazon SageMaker service.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Amazon SageMaker服务的左侧导航栏中，导航到Search。
- en: 'Search for experiments that have been conducted using the `object2vec` algorithm:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索使用`object2vec`算法进行的实验：
- en: In the Searchpane, under Property, select AlgorithmSpecification.TrainingImage.
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Search面板中，选择Property下的AlgorithmSpecification.TrainingImage。
- en: Under Operator, select Contains.
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Operator下，选择Contains。
- en: 'Under Value, select object2vec, as shown in the following code:'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Value下，选择object2vec，如下代码所示：
- en: '![](img/7a43c9f2-3732-4716-82cd-db0d870b2552.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a43c9f2-3732-4716-82cd-db0d870b2552.png)'
- en: 'You can also search for experiments programmatically using `boto3`, the AWS
    SDK for Python, as shown here:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过编程方式使用`boto3`（AWS的Python SDK）搜索实验，如下所示：
- en: '[PRE15]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding code, we instantiated the `sagemaker` client by passing the
    service name.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们通过传递服务名称实例化了`sagemaker`客户端。
- en: 'We will then call the search function of the SageMaker client by passing search
    parameters, as shown in the following code:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将通过传递搜索参数来调用SageMaker客户端的搜索功能，如下代码所示：
- en: '[PRE16]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In the preceding code block, we defined search parameters such as the type
    of resource to search for the maximum number of results to show, search expression,
    and sort by and order. We pass the search parameters that were defined to the
    search function of the SageMaker client to retrieve results:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们定义了搜索参数，如要搜索的资源类型、显示的最大结果数、搜索表达式、排序方式和顺序。我们将已定义的搜索参数传递给 SageMaker
    客户端的搜索函数，以检索结果：
- en: '**To find the winning training job**, do the following:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**要找到获胜的训练作业**，请执行以下操作：'
- en: Search for the experiments, as we discussed earlier. We can search based on
    several attributes, such as fields related to `TrainingJob`, `TuningJob`, `AlgorithmSpecification`,
    `InputDataConfiguration`, and `ResourceConfiguration`.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索实验，如前所述。我们可以根据多个属性进行搜索，例如与`TrainingJob`、`TuningJob`、`AlgorithmSpecification`、`InputDataConfiguration`和`ResourceConfiguration`相关的字段。
- en: Once the relevant experiments have been retrieved, we can sort them by objective
    metrics to find the winning training job.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索相关实验后，我们可以根据目标指标对其进行排序，以找到获胜的训练作业。
- en: '**To** **deploy the best mode**, follow these steps:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**要** **部署最佳模型**，请按照以下步骤操作：'
- en: Click on the winning training job and click on the Create Model button at the
    top.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击获胜的训练作业，然后点击顶部的“创建模型”按钮。
- en: Specify the location of model artifacts and registry path of the inference image,
    among other details, to create a model. Once the model have been created, navigate
    to Models under the Inference section (the left navigation menu) of the SageMaker
    service.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定模型工件的位置和推理镜像的注册路径等详细信息，以创建一个模型。模型创建后，导航到 SageMaker 服务的推理部分（左侧导航菜单）下的“模型”。
- en: 'You will find two options: Create batch transform job and create endpoint.
    For real-time inference, click on create endpoint and provide configuration details.'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将看到两个选项：创建批处理转换作业和创建端点。对于实时推理，点击“创建端点”并提供配置详细信息。
- en: 'To track the lineage of a deployed model, do the following:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟踪已部署模型的 lineage（继承关系），请执行以下操作：
- en: Choose Endpoints in the left navigation pane and select the endpoint of the
    winning model.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧导航窗格中选择端点，并选择获胜模型的端点。
- en: Scroll to the Endpoint Configuration Settings to locate the hyperlink to the
    Training Job that was used to create the endpoint.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向下滚动到端点配置设置，以找到用于创建端点的训练作业的超链接。
- en: 'Once the hyperlink has been clicked, you should see details on the model and
    training job, as shown in the following screenshot:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击超链接后，您应该能看到有关模型和训练作业的详细信息，如下图所示：
- en: '![](img/d43f5251-49a1-459a-84f3-19cf3053a07d.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d43f5251-49a1-459a-84f3-19cf3053a07d.png)'
- en: 'You can also programmatically track the lineage of a deployed model:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过编程方式跟踪已部署模型的 lineage：
- en: Use `boto3` to get the endpoint configuration by calling the `describe_endpoint_config()`
    function of the SageMaker client.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `boto3` 通过调用 SageMaker 客户端的 `describe_endpoint_config()` 函数来获取端点配置。
- en: From the configuration, select the model name to retrieve the Model Data URL.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从配置中选择模型名称以检索模型数据 URL。
- en: Retrieve a training job from the Model Data URL. By doing this, from a deployed
    endpoint, we can trace back to the training job.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从模型数据 URL 检索训练作业。通过这样做，我们可以从已部署的端点追溯到训练作业。
- en: Let's now turn our attention to how SageMaker allows data scientists to bring
    their own machine learning and deep learning libraries to AWS.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们关注 SageMaker 如何让数据科学家将自己的机器学习和深度学习库引入 AWS。
- en: Bring your own model – SageMaker, MXNet, and Gluon
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带上您自己的模型 – SageMaker、MXNet 和 Gluon
- en: This section focuses on how SageMaker allows you to bring your own deep learning
    libraries to the Amazon Cloud and still utilize the productivity features of SageMaker
    to automate training and deployment at scale.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重点介绍 SageMaker 如何允许您将自己的深度学习库引入 Amazon Cloud，并仍然利用 SageMaker 的生产力特性，自动化大规模的训练和部署。
- en: 'The deep learning library we will bring in here is Gluon:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将引入的深度学习库是 Gluon：
- en: Gluon is an open source deep learning library jointly created by AWS and Microsoft.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gluon 是由 AWS 和 Microsoft 共同创建的开源深度学习库。
- en: The primary goal of the library is to allow developers to build, train, and
    deploy machine learning models in the cloud.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该库的主要目标是允许开发者在云端构建、训练和部署机器学习模型。
- en: In the past, a tremendous amount of research has been conducted on recommender
    systems. In particular, Deep Structured Semantic models attempt to capture information
    from attributes, such as product image, title, and description. Extracting semantic
    information from these additional characteristics will solve the cold start problem
    in the space of recommender systems. In other words, when there is not much consumption
    history for a given user, a recommender system can propose products similar to
    the minimal products that are purchased by the user.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，关于推荐系统进行了大量研究。特别是，深度结构化语义模型试图捕捉来自属性的信息，例如产品图片、标题和描述。从这些附加特征中提取语义信息将解决推荐系统中的冷启动问题。换句话说，当给定用户的消费历史较少时，推荐系统可以建议与用户购买的最少产品相似的产品。
- en: Let's see how pretrained word embeddings, available via the `gluonnlp` library,
    can be used in SageMaker to find books similar to the books that a user likes,
    that is, recommended books whose titles are semantically similar to titles of
    books that a user likes.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何通过 `gluonnlp` 库提供的预训练词嵌入，在 SageMaker 中找到与用户喜欢的书籍相似的书籍，即推荐与用户喜欢的书籍标题在语义上相似的书籍。
- en: 'To do this, we will look at the same book ratings dataset we used in the previous
    sections of this chapter:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将查看本章前面部分中使用的相同书籍评分数据集：
- en: 'Let''s begin by installing the prerequisites:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先安装必要的依赖：
- en: '`mxnet`: This is a deep learning framework.'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mxnet`：这是一个深度学习框架。'
- en: '`gluonnlp`: This builds on top of MXNet. It is an open source deep learning
    library for **natural language processing** (**NLP**).'
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gluonnlp`：这是一个建立在 MXNet 之上的开源深度学习库，用于**自然语言处理**（**NLP**）。'
- en: '`nltk`: This is a Python natural language toolkit.'
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nltk`：这是一个 Python 自然语言工具包。'
- en: Next, we will read the filtered book ratings dataset that we created in the
    *Conduct Training in Amazon SageMaker* section. Then, we will obtain unique book
    titles from the dataset.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将读取在 *Amazon SageMaker 中进行训练* 部分创建的过滤后的书籍评分数据集。然后，我们将从数据集中获取唯一的书名。
- en: 'From each of the book titles, remove words with punctuation marks, numbers,
    and other special characters and only retain words that contain alphabets, as
    shown in the following code:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从每个书名中删除带有标点符号、数字和其他特殊字符的单词，只保留包含字母的单词，如下代码所示：
- en: '[PRE17]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In the preceding code block, we can see the following:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们可以看到以下内容：
- en: We iterate through each of the book titles and create tokens by calling the
    `word_tokenize()` function from `nltk.tokenize`.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们遍历每个书名，并通过调用 `nltk.tokenize` 中的 `word_tokenize()` 函数来创建词元。
- en: For each title, we only retain words containing alphabets by calling the `isapha()`
    method on word strings. In the end, we have a list of lists called `words`.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个书名，我们通过调用 `isapha()` 方法检查单词字符串，确保只保留包含字母的单词。最终，我们得到了一个名为 `words` 的词元列表的列表。
- en: 'Next, we will count the frequency of tokens across all of the book titles,
    as shown in the following:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将计算所有书名中词元的频率，如下所示：
- en: '[PRE18]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the preceding code, we can see the following:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们可以看到以下内容：
- en: To compute the frequency of tokens, we called the `count_tokens()` function
    from `gluonnlp.data` by passing the words list to it.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了计算词元的频率，我们通过传递单词列表调用了 `gluonnlp.data` 中的 `count_tokens()` 函数。
- en: '`counter`is a dictionary containing tokens (keys) and associated frequencies
    (values).'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`counter` 是一个字典，包含词元（键）和相关的频率（值）。'
- en: 'Load the pre-trained word embedding vectors that were trained using fastText—a
    library from the Facebook AI Research lab that''s used to learn word embeddings.
    Then, tie the word embeddings to each of the words in a book title, as shown here:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载通过 fastText 训练的预训练词嵌入向量——fastText 是 Facebook AI 研究实验室开发的一个库，用于学习词嵌入。然后，将词嵌入与书名中的每个单词关联，示例如下：
- en: '[PRE19]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In the preceding code block, we can see the following:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们可以看到以下内容：
- en: We created the indexes of tokens that can be attached to token embeddings by
    instantiating the `Vocab` class.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过实例化 `Vocab` 类，创建了可以与词元嵌入相关联的词元索引。
- en: We then instantiated word/token embeddings by passing embedding type as `fasttext`.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们通过将嵌入类型设置为 `fasttext` 来实例化词/词元嵌入。
- en: We called the `set_embedding()` method of the `Vocab` object to attach pre-trained
    word embedding to each of the tokens.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们调用了 `Vocab` 对象的 `set_embedding()` 方法，将预训练的词嵌入附加到每个词元上。
- en: 'Now, we create the embedding of a book title by averaging across individual
    word embeddings, as shown here:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们通过对单个词嵌入进行平均处理来创建书名的嵌入，示例如下：
- en: '[PRE20]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In the preceding code, we can see the following:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们可以看到以下内容：
- en: We iterated through each of the book titles and computed its embedding by averaging
    across all of the embeddings of the words in the title. This is done by calling
    the `mean()` method of the `ndarray` object, an *n-*dimensional array.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们遍历了每个书名，并通过对书名中所有单词的嵌入进行平均来计算它的嵌入。这是通过调用 `ndarray` 对象的 `mean()` 方法实现的，`ndarray`
    是一个 *n* 维数组。
- en: We then created an array, `title_arr_list`, of title embeddings by using the
    `append()` method of the `numpy` module.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们通过使用 `numpy` 模块的 `append()` 方法创建了一个标题嵌入数组 `title_arr_list`。
- en: 'It is now time to plot book titles—first, we will reduce the dimensions of
    the embeddings from 300 dimensions to 2\. Note that the shape of `title_arr_list`
    is 978 x 300\. This means that the array has 978 unique book titles and each title
    is represented by a vector that''s 300 in size. We will use the **T-distributed
    Stochastic Neighbor Embedding** (**TSNE**) algorithm to reduce the dimensionality
    but still retain its original meaning—that is, the distance between titles in
    a higher dimensional space is going to be the same as the distance between titles
    in a lower dimensional space. To go to a lower dimensional space for the title,
    we instantiate the `TSNE` class from the `sklearn` library, as shown in the following
    code:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是时候绘制书名了——首先，我们将把嵌入的维度从 300 维减少到 2。请注意，`title_arr_list` 的形状是 978 x 300。这意味着数组包含
    978 个独特的书名，每个书名由一个 300 维的向量表示。我们将使用 **T-分布随机邻域嵌入**（**TSNE**）算法来减少维度，同时保留原始含义——即在高维空间中的书名之间的距离将与在低维空间中的距离相同。为了将书名投射到低维空间，我们实例化
    `sklearn` 库中的 `TSNE` 类，如下代码所示：
- en: '[PRE21]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the preceding code block, we called the `fit_transform()` method of the `TSNE`
    object to return the transformed version of embedding.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们调用了 `TSNE` 对象的 `fit_transform()` 方法，以返回转换后的嵌入版本。
- en: 'After we get the transformed embedding, we will do a scatter plot with one
    dimension on the *x*-axis and another dimension on the *y*-axis, as shown in the
    following diagram:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们获取转换后的嵌入后，我们将做一个散点图，其中一个维度位于 *x* 轴，另一个维度位于 *y* 轴，如下图所示：
- en: '![](img/9684b30c-d85a-40d1-a9e1-9f315f2caa23.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9684b30c-d85a-40d1-a9e1-9f315f2caa23.png)'
- en: The proximity of book titles implies that they are semantically similar. For
    example, titles such as *Room* and *A Room with a View* seem to talk about the
    same subject room. These titles are located together in the lower dimensional
    space.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 书名的接近性意味着它们在语义上是相似的。例如，像 *Room* 和 *A Room with a View* 这样的标题似乎都在讨论同一个主题——房间。这些标题在低维空间中彼此相近。
- en: In this section, you learned how to bring pretrained word embeddings from fastText
    via the MXNet deep learning library to SageMaker. It is also possible to also
    train neural networks that have been built using the MXNet deep learning library
    from scratch. The same capabilities of SageMaker, such as training and deployment,
    are equally available for both built-in and custom algorithms.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您学习了如何通过 MXNet 深度学习库将预训练的 fastText 词嵌入引入 SageMaker。您还可以从头开始训练使用 MXNet 深度学习库构建的神经网络。SageMaker
    的相同功能，如训练和部署，适用于内置算法和自定义算法。
- en: Now that we have walked through how to bring your machine and/or deep learning
    library to SageMaker, it is time to look at how to bring your own container.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了如何将您的机器学习和/或深度学习库引入 SageMaker，接下来是如何带入您自己的容器。
- en: Bring your own container – R model
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带入您自己的容器——R 模型
- en: In this section, we will illustrate the process of bringing your own Docker
    container to Amazon SageMaker. Particularly, we will focus on training and hosting
    R models seamlessly in Amazon SageMaker. Rather than reinventing the wheel in
    terms of building ML models using SageMaker's built-in algorithms, data scientists
    and machine learning engineers can reuse the work that they've done in R in SageMaker.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示如何将您自己的 Docker 容器引入 Amazon SageMaker。特别地，我们将重点讨论如何在 Amazon SageMaker
    中无缝地训练和托管 R 模型。数据科学家和机器学习工程师可以在 SageMaker 中重用他们在 R 中已经完成的工作，而不是重新发明轮子来使用 SageMaker
    的内置算法构建 ML 模型。
- en: 'The following is the architecture regarding how different AWS components interact
    to train and host R models:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是有关 AWS 不同组件如何相互作用以训练和托管 R 模型的架构：
- en: '![](img/894122ec-03e5-4497-9581-3b1126170431.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/894122ec-03e5-4497-9581-3b1126170431.png)'
- en: 'To follow the preceding architectural diagram, we start with Amazon **Elastic
    Container Registry** (**ECR**):'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 为了遵循前面的架构图，我们从 Amazon **弹性容器注册表**（**ECR**）开始：
- en: We create a Docker image containing an underlying operating system, prerequisites
    to train a recommender algorithm in R, and R code for training and scoring the
    **User-Based Collaborative Filtering** (**UBCF**) recommender algorithm.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个包含底层操作系统、训练推荐算法所需的先决条件和R代码（用于训练和评分**基于用户的协同过滤**（**UBCF**）推荐算法）的Docker镜像。
- en: The created Docker image is then published to Amazon ECR. Remember that the
    training data for both SageMaker built-in and custom algorithms sits in the S3
    bucket.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建的Docker镜像随后发布到Amazon ECR。请记住，无论是SageMaker内建算法还是自定义算法的训练数据，都位于S3存储桶中。
- en: To start a training job in SageMaker, you designate the location of the training
    data and Docker registry path (in ECR) of the training image.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要启动SageMaker中的训练任务，您需要指定训练数据的位置和训练镜像的Docker注册路径（在ECR中）。
- en: During training, the appropriate R functions are triggered to train the UBCF
    algorithm. The training happens on SageMaker's machine learning compute instances.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练过程中，会触发适当的R函数来训练UBCF算法。训练发生在SageMaker的机器学习计算实例上。
- en: The resulting trained models known as model artifacts, are saved to the designated
    location on the S3 bucket.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果训练好的模型称为模型工件，保存在S3存储桶的指定位置。
- en: 'As for hosting the trained model, SageMaker requires two things:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 至于托管训练后的模型，SageMaker需要两项内容：
- en: Model artifacts
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型工件
- en: The Docker registry path of the inference image
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推断镜像的Docker注册路径
- en: 'To create an inference endpoint, the following takes place:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建推断端点，必须执行以下操作：
- en: SageMaker will create a model by passing the Docker registry path of the inference
    image of the R model and model artifacts.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SageMaker将通过传递R模型的推断镜像Docker注册路径和模型工件来创建模型。
- en: Once the SageMaker model has been created, SageMaker launches machine learning
    to compute instances by instantiating the Docker inference image.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦创建了SageMaker模型，SageMaker会通过实例化Docker推断镜像来启动机器学习计算实例。
- en: The compute instances will have R code for inference available as a RESTful
    API.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算实例将提供用于推断的R代码，作为一个RESTful API。
- en: In this section, we will look at the same book ratings dataset we used in the
    previous sections of this chapter, goodbooks-10k. Our goal is to suggest the top
    five books to users who are not part of the training dataset. We will use the
    `recommenderlab` R package to measure the cosine distance between users (UBCF).
    For our target user, we will pick 10 users/neighbors from the training set based
    on cosine similarity.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将查看在本章前面部分使用的相同书籍评分数据集——goodbooks-10k。我们的目标是向不在训练数据集中的用户推荐排名前五的书籍。我们将使用`recommenderlab`
    R包来衡量用户之间的余弦距离（UBCF）。对于目标用户，我们将从训练集中根据余弦相似度选择10个用户/邻居。
- en: 'To estimate the top five book recommendations for the target user, the UBCF
    algorithm uses two things:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估算目标用户的前五个书籍推荐，UBCF算法使用两项内容：
- en: Target user preferences for some books in the collection
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标用户对某些书籍的偏好
- en: The trained model
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练后的模型
- en: 'With the help of a trained model, we will compute ratings for books that the
    target user has never rated before. The top five books (among all of the books
    in the dataset) with the highest ratings are proposed to a given user. The trained
    model fills in ratings for all the books and for all of the users in the training
    dataset, as shown in the following:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练好的模型的帮助下，我们将为目标用户以前未评分的书籍计算评分。然后，向给定用户推荐评分最高的五本书（在数据集中的所有书籍中）。训练好的模型会为所有书籍和训练数据集中的所有用户填充评分，如下所示：
- en: '![](img/d1670572-6bc3-4aa8-b9d0-45d59653ed10.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1670572-6bc3-4aa8-b9d0-45d59653ed10.png)'
- en: 'During the training process, UBCF computes the missing ratings. Let''s assume
    that we want to fill in missing ratings for user A. User A has only rated **book
    #1** (**BK1**) and **book #3** (**BK3**). To compute ratings for books 2, 4, and
    5, the UBCF algorithm does the following:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，UBCF会计算缺失的评分。假设我们要填充用户A的缺失评分。用户A只对**书籍#1**（**BK1**）和**书籍#3**（**BK3**）进行了评分。为了计算书籍2、4和5的评分，UBCF算法会执行以下操作：
- en: 'It computes the cosine similarity between user A and the rest of the users
    in the training dataset. To compute the similarity between user A and B, we do
    the following:'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它计算用户A与训练数据集中其他用户之间的余弦相似度。为了计算用户A与B之间的相似度，我们执行以下操作：
- en: If users A and B have common books that they've rated, multiply the ratings
    by the book.
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果用户A和B有共同评分的书籍，则通过书籍的评分进行乘法计算。
- en: Add these ratings across all of the common books.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些评分添加到所有共享书籍中。
- en: Then, divide the result by the norm of vectors represented by users A and B.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将结果除以由用户 A 和 B 表示的向量的范数。
- en: 'Given a similarity score for users B through E relative to A, compute the rating
    for a given new book by taking the weighted average of ratings given by users
    B through E for that book:'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定用户 B 到 E 相对于 A 的相似度评分，通过计算用户 B 到 E 给定的该书评分的加权平均值，来计算用户 A 对某本新书的评分：
- en: 'For example, to compute the rating for book #2 for user A, we multiply a rating
    of 3 given by user B for book #2 by a similarity score of 0.29 and multiply a
    rating of 4 given by user C for book #2 by a similarity score of 0.73\. We add
    these two factors together.'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '例如，要计算用户 A 对书籍 #2 的评分，我们将用户 B 对书籍 #2 给出的评分 3 乘以相似度评分 0.29，然后将用户 C 对书籍 #2 给出的评分
    4 乘以相似度评分 0.73。最后，将这两个因子相加。'
- en: We then add the two similarity score of 0.29 and 0.73.
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将相似度评分 0.29 和 0.73 相加。
- en: Finally, we divide the results from 1 with 2.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们用 1 除以 2 得到结果。
- en: 'Now that we have looked at the training and hosting architecture for custom
    containers in SageMaker and discussed the use case, let''s begin the implementation:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经查看了 SageMaker 中自定义容器的训练和托管架构，并讨论了使用案例，让我们开始实现：
- en: The first step is to define the Dockerfile by highlighting the requirements
    to run the R code. The requirements are an underlying operating system, the R
    version, the R packages, and the location of the R logic for training and inference.
    Create and publish a Docker image to the **EC2 Container Registry** (**ECR**).
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是通过突出显示运行 R 代码所需的要求来定义 Dockerfile。要求包括操作系统、R 版本、R 包以及用于训练和推理的 R 逻辑的位置。创建并将
    Docker 镜像发布到 **EC2 容器注册表**（**ECR**）。
- en: 'The following Dockerfile defines the specifications for training and hosting
    R model:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 Dockerfile 定义了训练和托管 R 模型的规格：
- en: '[PRE22]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the preceding code block, we can see the following:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们可以看到以下内容：
- en: We defined the version of the Ubuntu operating system to install.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们定义了要安装的 Ubuntu 操作系统的版本。
- en: We also specified that we need R installed.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还指定了需要安装 R。
- en: 'Additionally, we have specified the R packages that need to be in place for
    the `Recommender` algorithm to work, as shown in the following code:'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，我们还指定了 `Recommender` 算法所需的 R 包，如以下代码所示：
- en: '[PRE23]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In the preceding code, we can see the following:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们可以看到以下内容：
- en: We copied the training (`Recommender.R`) and inference (`plumber.R`) code to
    the appropriate location.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将训练（`Recommender.R`）和推理（`plumber.R`）代码复制到适当的位置。
- en: Later, we specified an entry point (code to run) after the Docker image is instantiated.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后续我们指定了一个入口点（运行的代码），在 Docker 镜像实例化后执行。
- en: 'Now that the Dockerfile has been compiled, it is time to create a Docker image
    and push it to ECR, as shown here:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 既然 Dockerfile 已经编译完成，接下来是创建 Docker 镜像并将其推送到 ECR，如下所示：
- en: '[PRE24]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the preceding code, we can see the following:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们可以看到以下内容：
- en: To build the Docker image locally, we run the `Docker build` command by passing
    the image name to the local SageMaker instance.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了在本地构建 Docker 镜像，我们通过将镜像名称传递给本地的 SageMaker 实例来运行 `Docker build` 命令。
- en: The Dockerfile from the local directory (`"."`) is leveraged.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地目录（`"."`）中的 Dockerfile 被利用。
- en: After tagging the Docker image, we then push it to ECR with the `Docker push`
    command.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在标记 Docker 镜像后，我们通过 `Docker push` 命令将其推送到 ECR。
- en: The next step is to create a SageMaker training job, listing training dataset,
    the latest Docker image for training, and infrastructure specifications. The model
    artifacts from the training job are stored in the relevant S3 bucket. This is
    very similar to running any training job on SageMaker.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是创建一个 SageMaker 训练作业，列出训练数据集、最新的用于训练的 Docker 镜像和基础设施规格。训练作业的模型工件会存储在相关的 S3
    存储桶中。这与在 SageMaker 上运行任何训练作业非常相似。
- en: 'Let''s understand the R functions that are triggered during training:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解一下训练过程中触发的 R 函数：
- en: Remember that the `Recommender.R` code gets executed when ML compute instances
    are launched as part of the training.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请记住，`Recommender.R` 代码会在启动机器学习计算实例进行训练时执行。
- en: 'Depending on the command-line arguments that are passed, either the `train()`
    function or `serve()` function is executed, as shown in the following code:'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据传递的命令行参数，`train()` 函数或 `serve()` 函数会被执行，如下所示：
- en: '[PRE25]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: If the command-line argument contains the `train` keyword, the `train()` function
    gets executed. The same logic holds true for the `serve` keyword.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果命令行参数包含 `train` 关键字，则执行 `train()` 函数。对于 `serve` 关键字，逻辑也是一样的。
- en: 'During training, SageMaker copies the training dataset from the S3 bucket to
    ML compute instances. After we prepare training data for model fitting, we call
    the `Recommender` method (the `recommenderlab` R package) by specifying the number
    of users in the training set, the type of recommender algorithm, and the type
    of output (top N book recommendations), as shown in the following:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，SageMaker 将训练数据集从 S3 存储桶复制到 ML 计算实例。我们准备好模型拟合的训练数据后，通过指定训练集中的用户数量、推荐算法类型以及输出类型（如前
    N 本书推荐）来调用`Recommender`方法（`recommenderlab` R 包），如下所示：
- en: '[PRE26]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In the preceding code block, we can see the following:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码块中，我们可以看到以下内容：
- en: We train the model on 270 users and 973 books.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在 270 名用户和 973 本书上训练模型。
- en: The entire dataset contains 275 users.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个数据集包含 275 名用户。
- en: Please refer to the source code attached to this chapter. Once the UBCF algorithm
    has been trained, the resulting model is saved in the designated location on the
    ML compute instance, which is then pushed to the specified location on the S3
    bucket (model output path).
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考本章附带的源代码。一旦 UBCF 算法训练完成，生成的模型将保存在 ML 计算实例的指定位置，然后推送到 S3 存储桶中的指定位置（模型输出路径）。
- en: The third step is to host the trained model as an endpoint (RESTful API). SageMaker
    will need to create a model before provisioning an endpoint. Model artifacts and
    Docker images from training are required to define a SageMaker model. Note that
    the Docker image that was used for training is also used for inference. The SageMaker
    endpoint takes infrastructure specifications for ML compute instances as input,
    along with the SageMaker model. Again, this process of creating an endpoint in
    SageMaker for custom containers is the same as that for built-in algorithms.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三步是将训练好的模型作为端点（RESTful API）进行托管。SageMaker 在配置端点之前需要先创建模型。训练过程中的模型工件和 Docker
    镜像是定义 SageMaker 模型所必需的。请注意，用于训练的 Docker 镜像也用于推理。SageMaker 端点需要输入 ML 计算实例的基础设施规格以及
    SageMaker 模型。同样，为自定义容器创建 SageMaker 端点的过程与内置算法的过程相同。
- en: Let's understand the R functions that are triggered during inference.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解在推理过程中触发的 R 函数。
- en: 'The following R function is run when SageMaker sends the `serve` command at
    the time of inference, as shown in the following code:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 当 SageMaker 在推理时发送 `serve` 命令时，以下 R 函数将被执行，如下所示：
- en: '[PRE27]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the preceding code, we can see the following:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们可以看到以下内容：
- en: We have used the plumber R package to turn R functions into REST endpoints.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用了 plumber R 包，将 R 函数转化为 REST 端点。
- en: R functions that will need to be converted in to REST APIs are decorated with
    appropriate comments.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要转化为 REST API 的 R 函数会加上适当的注释。
- en: We used the `plumb()` method to host the `plumber.R` code as an endpoint.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用了`plumb()`方法将`plumber.R`代码作为一个端点托管。
- en: 'For each of the HTTP requests that''s sent to the endpoint, the appropriate
    function is called, as shown here:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个发送到端点的 HTTP 请求，将调用相应的函数，如下所示：
- en: '[PRE28]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In the preceding code, we can see the following:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们可以看到以下内容：
- en: At the time of inference, we load the trained model by calling the `load()`
    method and passing the path to the model artifacts.
  id: totrans-365
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理时，我们通过调用 `load()` 方法并传递模型工件的路径来加载训练好的模型。
- en: We then call the `predict()` method by specifying the name of the trained model,
    the new user vector or book preferences, and the number of books to recommend.
  id: totrans-366
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们通过指定训练好的模型名称、新的用户向量或书籍偏好以及推荐书籍数量来调用`predict()`方法。
- en: 'Note that the ratings matrix, `ratings_mat`, contains all 275 users and their
    ratings, where present, for books. In this case, we are interested in user #272\.
    Remember that, in the dataset for this section, we have a total of 275 users and
    973 books.'
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '请注意，评分矩阵 `ratings_mat` 包含所有 275 名用户及其对书籍的评分（若存在）。在本例中，我们关注的是用户 #272。请记住，在本节的数据集中，我们共有
    275 名用户和 973 本书。'
- en: 'The fourth step is to run model inference, as shown here:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第四步是运行模型推理，如下所示：
- en: '[PRE29]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In the preceding code, we can see the following:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们可以看到以下内容：
- en: We captured the entire dataset of 275 users in a CSV file called **payload**.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将包含 275 名用户的完整数据集捕获在名为 **payload** 的 CSV 文件中。
- en: We then pass the payload file as input to the `invoke_endpoint()` method of
    the SageMaker runtime, along with the endpoint name and content type.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将 payload 文件作为输入传递给 SageMaker 运行时的 `invoke_endpoint()` 方法，同时传递端点名称和内容类型。
- en: 'The endpoint responds with results, as shown in the following:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 端点将返回结果，如下所示：
- en: '![](img/5275b7b3-6cc1-408f-aebd-d80134ae0e43.png)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5275b7b3-6cc1-408f-aebd-d80134ae0e43.png)'
- en: By doing this, we have seen how seamless it is to bring your own container to
    SageMaker to train and host models, reusing training and scoring (inference) logic
    that's been written in other languages.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们看到了将自己的容器带到SageMaker来训练和托管模型的无缝体验，同时可以重用用其他语言编写的训练和评分（推理）逻辑。
- en: Summary
  id: totrans-376
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you've learned how to process big data to create an analytics-ready
    dataset. You've also seen how SageMaker automates most of the steps of the machine
    learning life cycle, enabling you to build, train, and deploy models seamlessly.
    Additionally, we've illustrated some of the productivity features, such as hyperparameter
    optimization and experimentation service, which enable data scientists to run
    multiple experiments and deploy the winning model. Finally, we have looked at
    bringing our own models and containers to the SageMaker ecosystem. Through bringing
    our own models based on open source machine learning libraries, we can readily
    build solutions based on open source frameworks, while still leveraging all of
    the capabilities of the platform. Similarly, by bringing our own container, we
    can readily port solutions, written in other programming languages besides Python,
    to SageMaker.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经学会了如何处理大数据，创建适合分析的数据集。你还看到SageMaker如何自动化机器学习生命周期的大部分步骤，使你能够无缝地构建、训练和部署模型。此外，我们还展示了一些生产力功能，如超参数优化和实验服务，这些功能使数据科学家能够进行多次实验并部署最终的优胜模型。最后，我们还讨论了如何将自己的模型和容器带入SageMaker生态系统。通过将基于开源机器学习库的模型带入，我们可以轻松地基于开源框架构建解决方案，同时仍能利用平台的所有功能。类似地，通过引入自己的容器，我们可以轻松地将用其他编程语言（除了Python）编写的解决方案移植到SageMaker。
- en: Learning all of the aforementioned aspects of Amazon SageMaker enables data
    scientists and machine learning engineers to decrease speed-to-market machine
    learning solutions.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 学习上述所有关于Amazon SageMaker的内容使数据科学家和机器学习工程师能够减少将机器学习解决方案推向市场的时间。
- en: In the next chapter, we will cover how to create training and inference pipelines
    so that models can be trained and deployed for efficiently running inferences
    (by creating reusable components).
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将介绍如何创建训练和推理流水线，以便能够训练和部署模型，以高效地运行推理（通过创建可重用的组件）。
- en: Further reading
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For extended examples and details on working with SageMaker, please refer to
    the following AWS blogs:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 有关在SageMaker中工作时的扩展示例和详细信息，请参阅以下AWS博客：
- en: '[https://github.com/awslabs/amazon-sagemaker-examples](https://github.com/awslabs/amazon-sagemaker-examples)'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/awslabs/amazon-sagemaker-examples](https://github.com/awslabs/amazon-sagemaker-examples)'
- en: '[https://aws.amazon.com/blogs/machine-learning/introduction-to-amazon-sagemaker-object2vec/](https://aws.amazon.com/blogs/machine-learning/introduction-to-amazon-sagemaker-object2vec/)'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://aws.amazon.com/blogs/machine-learning/introduction-to-amazon-sagemaker-object2vec/](https://aws.amazon.com/blogs/machine-learning/introduction-to-amazon-sagemaker-object2vec/)'
- en: '[https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-now-comes-with-new-capabilities-for-accelerating-machine-learning-experimentation/](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-now-comes-with-new-capabilities-for-accelerating-machine-learning-experimentation/)'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-now-comes-with-new-capabilities-for-accelerating-machine-learning-experimentation/](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-now-comes-with-new-capabilities-for-accelerating-machine-learning-experimentation/)'
