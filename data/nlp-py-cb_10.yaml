- en: Advanced Applications of Deep Learning in NLP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在自然语言处理中的高级应用
- en: 'In this chapter, we will cover the following advanced recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下高级实例：
- en: Automated text generation from Shakespeare's writings using LSTM
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LSTM从莎士比亚的作品中生成自动文本
- en: Questions and answers on episodic data using memory networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用记忆网络处理情节数据的问答
- en: Language modeling to predict the next best word using recurrent neural networks
    – LSTM
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用循环神经网络（LSTM）进行语言建模，以预测下一个最佳单词
- en: Generative chatbot development using deep learning recurrent networks – LSTM
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习循环神经网络（LSTM）开发生成型聊天机器人
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Deep learning techniques are being utilized well to solve some open-ended problems.
    This chapter discusses these types of problems, in which a simple *yes* or *no* would
    be difficult to say. We are hopeful that you will enjoy going through these recipes
    to obtain the viewpoint of what cutting-edge works are going on in this industry
    at the moment, and try to learn some basic building blocks of the same with relevant
    coding snippets.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习技术正在被广泛应用于解决一些开放性问题。本章讨论了这些类型的问题，在这些问题中，简单的*是*或*不是*回答是很困难的。我们希望你在阅读这些实例时能够享受，并了解到目前这一行业中正在进行的前沿工作，并尝试通过相关的代码片段学习一些基本构建块。
- en: Automated text generation from Shakespeare's writings using LSTM
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LSTM从莎士比亚的作品中生成自动文本
- en: In this recipe, we will use deep **recurrent neural networks** (**RNN**) to
    predict the next character based on the given length of a sentence. This way of
    training a model can generate automated text continuously, which imitates the
    writing style of the original writer with enough training on the number of epochs
    and so on.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实例中，我们将使用深度**循环神经网络**（**RNN**）来预测基于给定句子长度的下一个字符。这种训练模型的方式可以不断生成自动文本，模仿原作者的写作风格，经过足够的训练（包括多个训练周期等）。
- en: Getting ready...
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪...
- en: 'The *Project Gutenberg* eBook of the complete works of William Shakespeare''s
    dataset is used to train the network for automated text generation. Data can be
    downloaded from [http://www.gutenberg.org/](http://www.gutenberg.org/) for the
    raw file used for training:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*Project Gutenberg*提供的莎士比亚全集电子书数据集来训练网络进行自动文本生成。数据可以从[http://www.gutenberg.org/](http://www.gutenberg.org/)下载，用于训练的原始文件：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following code is used to create a dictionary of characters to indices
    and vice-versa mapping, which we will be using to convert text into indices at
    later stages. This is because deep learning models cannot understand English and
    everything needs to be mapped into indices to train these models:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于创建字符到索引的字典及其反向映射，我们将在后续阶段使用它将文本转换为索引。这是因为深度学习模型无法理解英文，所有内容都需要映射为索引来训练这些模型：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](img/daf470df-74a1-4e55-b293-16f7fcfba4f5.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/daf470df-74a1-4e55-b293-16f7fcfba4f5.png)'
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How to do it...
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Before training the model, various preprocessing steps are involved to make
    it work. The following are the major steps involved:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型之前，需要进行多种预处理步骤才能使其正常工作。以下是主要步骤：
- en: '**Preprocessing**: Prepare *X* and *Y* data from the given entire story text
    file and converting them into indices vectorized format.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预处理**：从给定的完整故事文本文件中准备*X*和*Y*数据，并将它们转换为索引向量格式。'
- en: '**Deep learning model training and validation**: Train and validate the deep
    learning model.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**深度学习模型的训练与验证**：训练和验证深度学习模型。'
- en: '**Text generation**: Generate the text with the trained model.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文本生成**：使用训练好的模型生成文本。'
- en: How it works...
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The following lines of code describe the entire modeling process of generating
    text from Shakespeare''s writings. Here we have chosen character length. This
    needs to be considered as `40` to determine the next best single character, which
    seems to be very fair to consider. Also, this extraction process jumps by three
    steps to avoid any overlapping between two consecutive extractions, to create
    a dataset more fairly:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码行描述了从莎士比亚的作品中生成文本的整个建模过程。这里我们选择了字符长度。需要考虑的是`40`，用于确定下一个最佳字符，这似乎是一个相当合理的选择。此外，这一提取过程以三步为单位跳跃，以避免连续两次提取之间的重叠，从而更公平地创建数据集：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following screenshot depicts the total number of sentences considered,
    `193798`, which is enough data for text generation:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了考虑的总句子数`193798`，这些数据足以进行文本生成：
- en: '![](img/dcf0bdeb-2033-4410-ab4e-9decd697a6db.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dcf0bdeb-2033-4410-ab4e-9decd697a6db.png)'
- en: 'The next code block is used to convert the data into a vectorized format for
    feeding into deep learning models, as the models cannot understand anything about
    text, words, sentences and so on. Initially, total dimensions are created with
    all zeros in the NumPy array and filled with relevant places with dictionary mappings:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个代码块用于将数据转换为向量化格式，以便输入深度学习模型，因为模型无法理解文本、单词、句子等。最初，NumPy数组中的所有维度都用零初始化，并用字典映射填充相关位置：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The deep learning model is created with RNN, more specifically Long Short-Term
    Memory networks with `128` hidden neurons, and the output is in the dimensions
    of the characters. The number of columns in the array is the number of characters.
    Finally, the `softmax` function is used with the `RMSprop` optimizer. We encourage
    readers to try with other various parameters to check out how results vary:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型使用RNN创建，更具体地说，是使用`128`个隐藏神经元的长短期记忆网络（LSTM），输出的维度是字符维度。数组中的列数即为字符数。最后，`softmax`函数与`RMSprop`优化器一起使用。我们鼓励读者尝试其他不同的参数，以查看结果的变化：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](img/34c322b4-4201-4882-94da-07bdfff39e8b.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/34c322b4-4201-4882-94da-07bdfff39e8b.png)'
- en: 'As mentioned earlier, deep learning models train on number indices to map input
    to output (given a length of 40 characters, the model will predict the next best
    character). The following code is used to convert the predicted indices back to
    the relevant character by determining the maximum index of the character:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，深度学习模型通过训练数字索引将输入映射到输出（给定40个字符的长度，模型将预测下一个最佳字符）。以下代码用于通过确定字符的最大索引，将预测的索引转换回相关字符：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The model will be trained over `30` iterations with a batch size of `128`.
    And also, the diversity has been changed to see the impact on the predictions:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将通过`30`次迭代进行训练，批处理大小为`128`。此外，还更改了多样性，以查看其对预测的影响：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The results are shown in the next screenshot to compare the first iteration
    (`Iteration 1`) and final iteration (`Iteration 29`). It is apparent that with
    enough training, the text generation seems to be much better than with `Iteration
    1`:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在下一张截图中，用于比较第一次迭代（`Iteration 1`）和最后一次迭代（`Iteration 29`）。显然，通过足够的训练，文本生成似乎比`Iteration
    1`时要好得多：
- en: '![](img/ad139862-1a1c-4394-af42-5ee640ba9f18.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad139862-1a1c-4394-af42-5ee640ba9f18.png)'
- en: 'Text generation after `Iteration 29` is shown in this image:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`Iteration 29`后的文本生成如图所示：'
- en: '![](img/c0bb264f-1530-4235-8eb9-bcf4281467c4.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0bb264f-1530-4235-8eb9-bcf4281467c4.png)'
- en: Though the text generation altogether seems to be a bit magical, we have generated
    text using Shakespeare's writings, proving that with the right training and handling,
    we can imitate any writer's style of writing.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管整体上文本生成似乎有些神奇，但我们使用莎士比亚的著作生成了文本，证明了只要有适当的训练和处理，我们可以模仿任何作家的写作风格。
- en: Questions and answers on episodic data using memory networks
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用记忆网络对情节数据进行问答
- en: In this recipe, we will use deep RNN to create a model to work on a question-and-answer
    system based on episodic memory. It will extract the relevant answers for a given
    question by reading a story in a sequential manner. For further reading, refer
    to the paper *Dynamic Memory Networks for Natural Language Processing* by Ankit
    Kumar et. al. ([https://arxiv.org/pdf/1506.07285.pdf](https://arxiv.org/pdf/1506.07285.pdf)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用深度RNN创建一个模型，基于情节记忆系统工作，进行问答。它将通过顺序读取故事，提取给定问题的相关答案。如需进一步阅读，请参考Ankit
    Kumar等人的论文*Dynamic Memory Networks for Natural Language Processing*（[https://arxiv.org/pdf/1506.07285.pdf](https://arxiv.org/pdf/1506.07285.pdf)）。
- en: Getting ready...
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备中...
- en: Facebook's bAbI data has been used for this example, and the same can be downloaded
    from [http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz](http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz).
    It consists of about 20 types of tasks, among which we have taken the first one,
    a single supporting-fact-based question-and-answer system.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例使用了Facebook的bAbI数据集，可以从[http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz](http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz)下载。它包含大约20种任务类型，其中我们选择了第一个任务，一个基于单一支持事实的问答系统。
- en: 'After unzipping the file, go to the `en-10k` folder and use the files starting
    with `qa1_single supporting-fact` for both the train and test files. The following
    code is used for extraction of stories, questions, and answers in this particular
    order to create the data required for training:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 解压文件后，进入`en-10k`文件夹，使用以`qa1_single supporting-fact`开头的文件作为训练和测试文件。以下代码用于提取故事、问题和答案，以此顺序创建训练所需的数据：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After extraction, it seems that about 10k observations were created in the
    data for both train and test datasets:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 提取后，似乎在训练和测试数据集的数据显示中，共创建了约10k个观测值：
- en: '![](img/a777c1c7-bf50-4b1a-97c5-6dee55c5a679.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a777c1c7-bf50-4b1a-97c5-6dee55c5a679.png)'
- en: How to do it...
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'After extraction of basic datasets, we need to follow these steps:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 基本数据集提取后，我们需要遵循以下步骤：
- en: '**Preprocessing**: Create a dictionary and map the story, question and answers
    to vocab to map into vector format.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预处理**：创建字典，并将故事、问题和答案映射到词汇中，再转换为向量格式。'
- en: '**Model development and validation**: Train the deep learning models and test
    on the validation data sample.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型开发与验证**：训练深度学习模型，并在验证数据样本上进行测试。'
- en: '**Predicting outcomes based on the trained model**: Trained models are utilized
    for predicting outcomes on test data.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基于训练模型预测结果**：训练过的模型用于预测测试数据上的结果。'
- en: How it works...
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: After train and test data creation, the remaining methodology is described as
    follows.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和测试数据创建后，剩余的方法论如下所示：
- en: 'First, we will create a dictionary for vocabulary, in which for every word
    from the story, question and answer data mapping is created. Mappings are used
    to convert words into integer numbers and subsequently into vector space:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将为词汇创建一个字典，其中每个单词都会与故事中的问题和答案数据进行映射。映射用于将单词转换为整数，然后再转换为向量空间：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following screenshot depicts all the words in the vocabulary. It has only
    `22` words, including the `PAD` word, which has been created to fill blank spaces
    or zeros:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了词汇表中的所有单词。它只包含`22`个单词，包括`PAD`词，这个词是用来填充空白或零的：
- en: '![](img/6567905e-5112-4afc-9a3e-136b5e303f04.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6567905e-5112-4afc-9a3e-136b5e303f04.png)'
- en: 'The following code is used to determine the maximum length of words. By knowing
    this, we can create a vector of maximum size, which can incorporate all lengths
    of words:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于确定单词的最大长度。知道这一点后，我们可以创建一个最大尺寸的向量，能够容纳所有长度的单词：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The maximum length of words for story is `14`, and for questions it is `4`.
    For some of the stories and questions, the length could be less than the maximum
    length; those words will be replaced with `0` (or `PAD` word). The reason? This
    padding of extra blanks will make all the observations of equal length. This is
    for computation efficiency, or else it will be difficult to map different lengths,
    or creating parallelization in GPU for computation will be impossible:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 故事中单词的最大长度为`14`，而问题的最大长度为`4`。对于某些故事和问题，长度可能会小于最大长度；这些单词将被替换为`0`（或`PAD`词）。原因是什么？这种填充额外空白的方式将使所有观测值具有相同的长度。这是为了计算效率，否则将很难映射不同长度的数据，或者在GPU上进行并行计算将变得不可能：
- en: '![](img/84c1ebfd-8183-4b30-b28c-f9bba117674b.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84c1ebfd-8183-4b30-b28c-f9bba117674b.png)'
- en: 'Following snippets of code does import various functions from respective classes
    which we will be using in the following section:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段导入了来自各个类的函数，我们将在接下来的部分中使用这些函数：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Word-to-vectorized mapping is being performed in the following code after considering
    the maximum lengths for story, question, and so on, while also considering vocab
    size, all of which we have computed in the preceding segment of code:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中执行了词到向量化的映射，考虑了故事、问题等的最大长度，同时还考虑了我们在前一段代码中计算出的词汇大小：
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The application of `data_vectorization` is shown in this code:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`data_vectorization`的应用在此代码中展示：'
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following image describes the dimensions of train and test data for story,
    question, and answer segments accordingly:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像描述了训练和测试数据在故事、问题和答案部分的维度：
- en: '![](img/8133b229-445b-4f66-a8e7-9a239f8070d2.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8133b229-445b-4f66-a8e7-9a239f8070d2.png)'
- en: 'Parameters are initialized in the following code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 参数在以下代码中进行初始化：
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The core building blocks of the model are explained here:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的核心构建块在这里进行了解释：
- en: '[PRE15]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'By reading the model summary in following image, you can see how blocks are
    connected and the see total number of parameters required to be trained to tune
    the model:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过阅读以下图像中的模型总结，您可以看到各个模块是如何连接的，并查看训练模型所需调优的总参数数量：
- en: '![](img/aa0bd335-47c0-4da5-8e42-dbb492fd5af2.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa0bd335-47c0-4da5-8e42-dbb492fd5af2.png)'
- en: 'Following code does perform model fitting on train data:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码对训练数据进行了模型拟合：
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The model accuracy has significantly improved from the first iteration (*train
    accuracy = 19.35%* and *validation accuracy = 28.98%*) to the 40^(th) (*train
    accuracy = 82.22%* and *validation accuracy = 84.51%*), which can be shown in
    the following image:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的准确性从第一次迭代（*训练准确率 = 19.35%* 和 *验证准确率 = 28.98%*）到第 40 次迭代（*训练准确率 = 82.22%*
    和 *验证准确率 = 84.51%*）显著提高，如下图所示：
- en: '![](img/d13ea6d1-b563-4a0d-af7b-0acd4db56e73.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d13ea6d1-b563-4a0d-af7b-0acd4db56e73.png)'
- en: 'Following code does plot both training & validation accuracy change with respective
    to change in epoch:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了随着训练周期变化，训练和验证准确率的变化情况：
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The change in accuracy with the number of iterations is shown in the following
    image. It seems that the accuracy has improved marginally rather than drastically
    after `10` iterations:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 随着迭代次数的增加，准确率的变化如图所示。从图中看出，在`10`次迭代后，准确率有所提高，但提升幅度并不显著：
- en: '![](img/b00044e5-cedb-4f5a-8c4a-7028cec8a801.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b00044e5-cedb-4f5a-8c4a-7028cec8a801.png)'
- en: 'In the following code, results are predicted which is finding probability for
    each respective class and also applying `argmax` function for finding the class
    where the probability is maximum:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，结果是预测每个类别的概率，并使用`argmax`函数找出概率最大的位置，进而确定分类：
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: After training the model enough and achieving better accuracies on validation
    data such as 84.51%, it is time to verify with actual test data to see how much
    the predicted answers are in line with the actual answers.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在对模型进行足够训练并在验证数据上获得更好的准确率（例如 84.51%）后，接下来是使用实际测试数据进行验证，查看预测结果与实际结果的吻合程度。
- en: 'Out of ten randomly drawn questions, the model was unable to predict the correct
    question only once (for the sixth question; the actual answer is `bedroom` and
    the predicted answer is `office`). This means we have got 90% accuracy  on the
    sample. Though we may not be able to generalize the accuracy value, this gives
    some idea to reader about the prediction ability of the model:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在十个随机抽取的问题中，模型仅有一次未能预测正确（第六个问题；实际答案是 `bedroom`，预测答案是 `office`）。这意味着我们在这个样本上的准确率达到了
    90%。虽然我们可能无法对准确率值进行广泛的概括，但这至少能给读者提供一些关于模型预测能力的直观感受：
- en: '![](img/29f8f2b6-ff9e-4e9e-abb5-33338550761e.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29f8f2b6-ff9e-4e9e-abb5-33338550761e.png)'
- en: Language modeling to predict the next best word using recurrent neural networks
    LSTM
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用循环神经网络 LSTM 进行语言建模，以预测下一个最佳单词
- en: Predicting the next word based on some typed words has many real-word applications.
    An example would be to suggest the word while typing it into the Google search
    bar. This type of feature does improve user satisfaction in using search engines.
    Technically, this can be called **N-grams** (if two consecutive words are extracted,
    it will be called **bi-grams**). Though there are so many ways to model this,
    here we have chosen deep RNNs to predict the next best word based on *N-1* pre-words.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 基于一些已输入的单词预测下一个单词有许多现实世界的应用。例如，在输入单词到 Google 搜索栏时，系统会建议下一个单词。这类功能确实能够提升用户在使用搜索引擎时的满意度。从技术上讲，这可以被称为**N-gram**（如果提取的是两个连续的单词，那么就叫做**bi-gram**）。虽然有很多种方法可以建模这个问题，但在这里我们选择了深度循环神经网络（RNN）来基于*N-1*个先前单词预测下一个最佳单词。
- en: Getting ready...
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正在准备中...
- en: 'Alice in Wonderland data has been used for this purpose and the same data can
    be downloaded from [http://www.umich.edu/~umfandsf/other/ebooks/alice30.txt](http://www.umich.edu/~umfandsf/other/ebooks/alice30.txt).
    In the initial data preparation stage, we have extracted N-grams from continuous
    text file data, which looks like a story file:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为此目的使用了《爱丽丝梦游仙境》的数据，用户可以从[http://www.umich.edu/~umfandsf/other/ebooks/alice30.txt](http://www.umich.edu/~umfandsf/other/ebooks/alice30.txt)下载相同的数据。在初始数据准备阶段，我们从连续的文本文件数据中提取了N-gram，这些数据看起来像是一个故事文件：
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'N-grams are selected with the following `N` value. In the following code, we
    have chosen `N` as `3`, which means each piece of data has three words consecutively.
    Among them, two pre-words (bi-grams) used to predict the next word in each observation.
    Readers are encouraged to change the value of `N` and see how the model predicts
    the words:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: N-gram 是通过以下的 `N` 值进行选择的。在以下代码中，我们选择了 `N = 3`，这意味着每一段数据包含三个连续的单词。在这些单词中，使用了两个先前的单词（bi-gram）来预测下一个单词。鼓励读者修改
    `N` 的值，看看模型是如何预测单词的：
- en: 'Note: With the increase in N-grams to 4, 5, and 6 or so, we need to provide
    enough amount of incremental data to compensate for the curse of dimensionality.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：随着N-gram的增大到4、5、6等，我们需要提供足够的增量数据以弥补维度灾难。
- en: '[PRE20]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: How to do it...
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'After extracting basic data observations, we need to perform the following
    operations:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 提取基本数据观察后，我们需要执行以下操作：
- en: '**Preprocessing**: In the preprocessing step, words are converted to vectorized
    form, which is needed for working with the model.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预处理**：在预处理步骤中，单词被转换为向量化形式，这是模型运作所需的。'
- en: '**Model development and validation**: Create a convergent-divergent model to
    map the input to the output, followed by training and validation data.'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型开发和验证**：创建一个收敛-发散模型，将输入映射到输出，接着是训练和验证数据。'
- en: '**Prediction of next best word**: Utilize the trained model to predict the
    next best word on test data.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预测下一个最佳单词**：利用训练好的模型预测测试数据中的下一个最佳单词。'
- en: How it works...
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Vectorization of the given words (*X* and *Y* words) to vector space using `CountVectorizer`
    from scikit-learn:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`CountVectorizer`从scikit-learn库将给定的单词（*X*和*Y*单词）向量化到向量空间：
- en: '[PRE21]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After converting the data into vectorized form, we can see that the column
    value remains the same, which is the vocabulary length (2559 of all possible words):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据转换为向量化形式后，我们可以看到列值保持不变，即词汇长度（2559个所有可能的单词）：
- en: '![](img/35884aee-14dc-47da-ab5f-70fc2de54929.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/35884aee-14dc-47da-ab5f-70fc2de54929.png)'
- en: 'The following code is the heart of the model, consisting of convergent-divergent
    architecture that reduces and expands the shape of the neural network:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是模型的核心，包含收敛-发散架构，能够缩小和扩展神经网络的形状：
- en: '[PRE22]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This screenshot depicts the complete architecture of the model, consisting
    of a convergent-divergent structure:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这张截图展示了模型的完整架构，包含收敛-发散结构：
- en: '![](img/52e02c04-33fd-4dea-9ab5-ddb15528aba3.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52e02c04-33fd-4dea-9ab5-ddb15528aba3.png)'
- en: '[PRE23]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The model is trained on data with 100 epochs. Even after a significant improvement
    in the train accuracy (from 5.46% to 63.18%), there is little improvement in the
    validation accuracy (6.63% to 10.53%). However, readers are encouraged to try
    various settings to improve the validation accuracy further:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在100个epoch的数据上进行了训练。尽管训练准确率有显著提高（从5.46%提高到63.18%），但验证准确率的提升较小（从6.63%提高到10.53%）。然而，鼓励读者尝试不同的设置，以进一步提高验证准确率：
- en: '![](img/b5db198b-1382-4fe9-914c-0a66d3ef6c1b.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5db198b-1382-4fe9-914c-0a66d3ef6c1b.png)'
- en: '[PRE24]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Less validation accuracy provides a hint that the model might not predict the
    word very well. The reason could be the very-high-dimensional aspect of taking
    the word rather than the character level (character dimensions are 26, which is
    much less than the 2559 value of words). In the following screenshot, we have
    predicted about two times out of `10`. However, it is very subjective to say whether
    it is a yes or no. Sometimes, the word predicted could be close but not the same:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 较低的验证准确率提示模型可能无法很好地预测单词。原因可能是选择单词而非字符级别的高维度问题（字符维度为26，远小于单词的2559维度）。在以下截图中，我们预测大约有两次是正确的，约为`10`次中的两次。然而，是否可以认为是“对”或“错”是非常主观的。有时，预测的单词可能接近但不完全相同：
- en: '![](img/78c49c83-741f-4a96-ab85-88af03bcfd8f.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78c49c83-741f-4a96-ab85-88af03bcfd8f.png)'
- en: Generative chatbot using recurrent neural networks (LSTM)
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用递归神经网络（LSTM）的生成式聊天机器人
- en: Generative chatbots are very difficult to build and operate. Even today, most
    workable chatbots are retrieving in nature; they retrieve the best response for
    the given question based on semantic similarity, intent, and so on. For further
    reading, refer to the paper *Learning Phrase Representations using RNN Encoder-Decoder
    for Statistical Machine Translation* by Kyunghyun Cho et. al. ([https://arxiv.org/pdf/1406.1078.pdf](https://arxiv.org/pdf/1406.1078.pdf)).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式聊天机器人非常难以构建和操作。即便是今天，大多数可用的聊天机器人也具有检索性质；它们基于语义相似度、意图等从给定问题中检索出最佳响应。欲了解更多内容，请参考Kyunghyun
    Cho等人所著的论文《使用RNN编码器-解码器进行统计机器翻译的短语表示学习》（[https://arxiv.org/pdf/1406.1078.pdf](https://arxiv.org/pdf/1406.1078.pdf)）。
- en: Getting ready...
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪...
- en: 'The A.L.I.C.E Artificial Intelligence Foundation dataset `bot.aiml`  **Artificial
    Intelligence Markup Language** (**AIML**), which is customized syntax such as
    XML file has been used to train the model. In this file, questions and answers
    are mapped. For each question, there is a particular answer. Complete `.aiml`
    files are available at *aiml-en-us-foundation-alice.v1-9* from [https://code.google.com/archive/p/aiml-en-us-foundation-alice/downloads](https://code.google.com/archive/p/aiml-en-us-foundation-alice/downloads).
    Unzip the folder to see the `bot.aiml` file and open it using Notepad. Save as
    `bot.txt` to read in Python:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: A.L.I.C.E 人工智能基金会数据集 `bot.aiml` **人工智能标记语言**（**AIML**），这是一种类似 XML 文件的定制语法，已被用来训练模型。在此文件中，问题和答案已被映射。对于每个问题，都有一个特定的答案。完整的
    `.aiml` 文件可以在 *aiml-en-us-foundation-alice.v1-9* 中找到，下载地址为 [https://code.google.com/archive/p/aiml-en-us-foundation-alice/downloads](https://code.google.com/archive/p/aiml-en-us-foundation-alice/downloads)。解压文件夹以查看
    `bot.aiml` 文件，并使用记事本打开。保存为 `bot.txt` 以供 Python 阅读：
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'AIML files have unique syntax, similar to XML. The `pattern` word is used to
    represent the question and the `template` word for the answer. Hence, we are extracting
    respectively:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: AIML 文件具有独特的语法，类似于 XML。`pattern` 词用于表示问题，`template` 词用于表示答案。因此，我们分别提取以下内容：
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The question and answers are joined to extract the total vocabulary used in
    the modeling, as we need to convert all words/characters into numeric representation.
    The reason is the same as mentioned before—deep learning models can't read English
    and everything is in numbers for the model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 问题和答案被连接起来，以提取建模中使用的所有词汇，因为我们需要将所有的单词/字符转换为数字表示。原因与之前提到的相同——深度学习模型无法理解英语，一切对于模型而言都是数字。
- en: '![](img/113ecf8f-6dc5-4f66-86b2-577e574ddafd.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/113ecf8f-6dc5-4f66-86b2-577e574ddafd.png)'
- en: How to do it...
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'After extracting the question-and-answer pairs, the following steps are needed
    to process the data and produce the results:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 提取问题和答案对后，需要进行以下步骤来处理数据并生成结果：
- en: '**Preprocessing**: Convert the question-and-answer pairs into vectorized format,
    which will be utilized in model training.'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预处理**：将问题和答案对转换为向量化格式，用于模型训练。'
- en: '**Model building and validation**: Develop deep learning models and validate
    the data.'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型构建和验证**：开发深度学习模型并验证数据。'
- en: '**Prediction of answers from trained model**: The trained model will be used
    to predict answers for given questions.'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**从训练模型预测答案**：将使用训练好的模型来预测给定问题的答案。'
- en: How it works...
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The question and answers are utilized to create the vocabulary of words to
    index mapping, which will be utilized for converting words into vector mappings:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用问题和答案来创建词汇表的词到索引的映射，这将用于将词转换为向量映射：
- en: '[PRE27]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](img/cdec47c6-3e7d-49ca-88a0-b276eb4a0e61.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cdec47c6-3e7d-49ca-88a0-b276eb4a0e61.png)'
- en: 'Encoding and decoding functions are used to convert text to indices and indices
    to text respectively. As we know, Deep learning models work on numeric values
    rather than text or character data:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 编码和解码函数用于分别将文本转换为索引，将索引转换为文本。正如我们所知，深度学习模型依赖数字值而非文本或字符数据：
- en: '[PRE28]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The following code is used to vectorize the question and answers with the given
    maximum length for both questions and answers. Both might be different lengths.
    In some pieces of data, the question length is greater than answer length, and
    in a few cases, it''s length is less than answer length. Ideally, the question
    length is good to catch the right answers. Unfortunately in this case, question
    length is much less than the answer length, which is a very bad example to develop
    generative models:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于将问题和答案向量化，并为问题和答案设置最大长度。两者的长度可能不同。在某些数据中，问题的长度大于答案的长度，而在某些情况下，问题的长度小于答案的长度。理想情况下，问题的长度应该足够长，以便捕捉到正确的答案。不幸的是，在这种情况下，问题的长度远小于答案的长度，这是一个非常糟糕的例子，不适合开发生成模型：
- en: '[PRE29]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following code is an important part of the chatbot. Here we have used recurrent
    networks, repeat vector, and time-distributed networks. The repeat vector used
    to match dimensions of input to output values. Whereas time-distributed networks
    are used to change the column vector to the output dimension''s vocabulary size:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是聊天机器人的重要部分。在这里，我们使用了递归网络、重复向量和时间分布式网络。重复向量用于匹配输入和输出值的维度。而时间分布式网络用于将列向量转换为输出维度的词汇表大小：
- en: '[PRE30]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following model summary describes the change in flow of model size across
    the model. The input layer matches the question''s dimension and the output matches
    the answer''s dimension:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下模型摘要描述了模型规模变化的流程。输入层匹配问题的维度，输出层匹配答案的维度：
- en: '![](img/a06e5d6b-d4e4-4ae4-ad8e-c214afa531bc.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a06e5d6b-d4e4-4ae4-ad8e-c214afa531bc.png)'
- en: '[PRE31]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The results are a bit tricky in the following screenshot even though the accuracy
    is significantly higher. The chatbot model might produce complete nonsense, as
    most of the words are padding here. The reason? The number of words in this data
    is less:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管准确度显著提高，下面的截图中的结果仍然有些棘手。聊天机器人模型可能会产生完全无意义的内容，因为这里的大多数词语都是填充词。原因是什么？数据中的词语数量较少：
- en: '>![](img/fb81b2d5-3b8b-41ce-910c-25ee240b59be.png)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '>![](img/fb81b2d5-3b8b-41ce-910c-25ee240b59be.png)'
- en: '[PRE32]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The following screenshot depicts the sample output on test data. The output
    does not seem to make sense, which is an issue with generative models:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了在测试数据上的样本输出。输出似乎没有意义，这是生成模型的一个问题：
- en: '![](img/cc37d6c6-52a3-4209-ab28-5c9323203e98.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc37d6c6-52a3-4209-ab28-5c9323203e98.png)'
- en: 'Our model did not work well in this case, but still some areas of improvement
    are possible going forward with generative chatbot models. Readers can give it
    a try:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型在这种情况下表现不佳，但随着生成式聊天机器人模型的发展，仍然有一些改进的空间。读者可以尝试一下：
- en: Have a dataset with lengthy questions and answers to catch signals well
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有包含长问题和答案的数据集，以便更好地捕捉信号
- en: Create a larger architecture of deep learning models and train over longer iterations
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建更大架构的深度学习模型，并进行更长时间的训练
- en: Make question-and-answer pairs more generic rather than factoid-based, such
    as retrieving knowledge and so on, where generative models fail miserably
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使问答对更具通用性，而非基于事实，例如检索知识等领域，在这些领域生成模型常常失败
