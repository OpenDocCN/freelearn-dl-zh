- en: Sequence to Sequence Models-Parlez-vous Français?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列到序列模型——你会说法语吗？
- en: Thus far, much of our work was with images. Working with images is helpful as
    the results are almost uncanny in how quickly and succinctly progress can be made.
    However, the world of machine learning is broader and the next several chapters
    will cover these other aspects. We will start with sequence-to-sequence models.
    The results are just as uncanny, though the setup is a bit more involved and training
    datasets are much larger.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的工作主要集中在图像上。处理图像是有帮助的，因为结果几乎令人难以置信，进展可以如此快速和简洁。然而，机器学习的世界更为广泛，接下来的几章将涉及这些其他方面。我们将从序列到序列模型开始。结果同样令人惊叹，尽管设置稍微复杂一些，且训练数据集要大得多。
- en: 'In this chapter, we will focus on several areas, which are as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点介绍以下几个领域：
- en: Understanding how sequence-to-sequence models work
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解序列到序列模型的工作原理
- en: Understanding the setup required to feed a sequence-to-sequence model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解如何为序列到序列模型提供数据
- en: Writing an English to French translator using sequence-to-sequence models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用序列到序列模型编写英语到法语的翻译器
- en: A quick preview
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速预览
- en: Yes, you read that correctly...we will be writing an English to French translator.
    The pre-machine learning world might have approached this with a series of parsers
    and rules on how to translate words and phrases to others, but our approach will
    be far more elegant, generalizable, and quick. We will just use examples, many
    examples, to train our translator.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，你没看错……我们将编写一个英语到法语的翻译器。机器学习前的世界可能会用一系列解析器和规则来翻译单词和短语，但我们的方法将更加优雅、通用且快速。我们将只使用例子，很多例子，来训练我们的翻译器。
- en: The game here will be finding a dataset with enough English sentences translated
    into French (actually, it will work in any language). Translated essays and news
    articles will not be as helpful as we won't necessarily be able to place specific
    sentences from one language to another side by side. So, we will need to be more
    creative. Fortunately, organizations such as the United Nations often need to
    do exactly this—they need to do line by line translations to meet the needs of
    their diverse constituencies. How convenient for us!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的任务是找到一个包含足够多英语句子并已翻译成法语的数据集（实际上，它适用于任何语言）。翻译的文章和新闻报道不太有用，因为我们未必能够将一个语言中的具体句子与另一语言的句子逐一对应。因此，我们需要更具创意。幸运的是，像联合国这样的组织常常需要做这种工作——它们需要逐行翻译，以满足其多样化群体的需求。对我们来说，这真是太方便了！
- en: The *Workshop on Statistical Machine Translation* had a conference in 2010 that
    released a nice packaged training set, which can be used. The full details are
    available at [http://www.statmt.org/wmt10/](http://www.statmt.org/wmt10/).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*统计机器翻译研讨会*在2010年召开了一次会议，发布了一个很好的训练集包，可以使用。详细信息可见[http://www.statmt.org/wmt10/](http://www.statmt.org/wmt10/)。'
- en: 'We will be using specific files just for French, as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将仅使用特定的法语文件，如下所示：
- en: '[http://www.statmt.org/wmt10/training-giga-fren.tar](http://www.statmt.org/wmt10/training-giga-fren.tar)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.statmt.org/wmt10/training-giga-fren.tar](http://www.statmt.org/wmt10/training-giga-fren.tar)'
- en: '[http://www.statmt.org/wmt15/dev-v2.tgz](http://www.statmt.org/wmt15/dev-v2.tgz)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.statmt.org/wmt15/dev-v2.tgz](http://www.statmt.org/wmt15/dev-v2.tgz)'
- en: 'The following is an excerpt of what the source data looks like on the English
    side:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是源数据在英语端的一个摘录：
- en: Food, where European inflation slipped up
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 食品——欧洲通胀的隐性问题
- en: The skyward zoom in food prices is the dominant force behind the speed up in
    eurozone inflation
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 食品价格飙升是推动欧元区通胀加速的主要因素
- en: November price hikes were higher than expected in the 13 eurozone countries,
    with October's 2.6 percent yr/yr inflation rate followed by 3.1 percent in November,
    the EU's Luxembourg-based statistical office reported
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧元区13个国家的11月价格上涨高于预期，10月的年通胀率为2.6%，11月为3.1%，欧盟卢森堡的统计局报告称
- en: Official forecasts predicted just three percent, Bloomberg said
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 官方预测仅为3%，彭博社报道
- en: As opposed to the US, UK, and Canadian central banks, the **European Central
    Bank** (**ECB**) did not cut interest rates, arguing that a rate drop combined
    with rising raw material prices and declining unemployment would trigger an inflationary
    spiral
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与美国、英国和加拿大的中央银行不同，**欧洲中央银行**（**ECB**）并未降息，理由是降息结合原材料价格上涨和失业率下降，将引发通货膨胀的螺旋。
- en: The ECB wants to hold inflation to under two percent, or somewhere in that vicinity
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧洲中央银行希望将通胀控制在2%以下，或者在该范围内
- en: According to one analyst, ECB has been caught in a Catch-22, and it needs to
    **talk down** inflation, to keep from having to take action to push it down later
    in the game
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据一位分析师的说法，欧洲央行陷入了一个**两难境地**，它需要**降低**通胀，以避免在稍后的阶段采取行动来将其压低。
- en: 'And, here is the French equivalent:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是法语的对应版本：
- en: L'inflation, en Europe, a dérapé sur l'alimentation
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在欧洲，食品价格推动了通胀的失控。
- en: L'inflation accélérée, mesurée dans la zone euro, est due principalement à l'augmentation
    rapide des prix de l'alimentation
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧元区的加速通胀，主要是由于食品价格的快速上涨。
- en: En novembre, l'augmentation des prix, dans les 13 pays de la zone euro, a été
    plus importante par rapport aux prévisions, après un taux d'inflation de 2,6 pour
    cent en octobre, une inflation annuelle de 3,1 pour cent a été enregistrée, a
    indiqué le bureau des statistiques de la Communauté Européenne situé à Luxembourg
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 11月，欧元区13个国家的价格上涨幅度超过了预测，在10月通胀率为2.6%的情况下，年通胀率达到了3.1%，欧盟统计局位于卢森堡的办公室表示。
- en: Les prévisions officielles n'ont indiqué que 3 pour cent, a communiqué Bloomberg
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 官方预测仅为3个百分点，彭博社报道。
- en: Contrairement aux banques centrales américaine, britannique et canadienne, la
    Banque centrale européenne (BCE) n'a pas baissé le taux d'intérêt directeur en
    disant que la diminution des intérêts, avec la croissance des prix des matières
    premières et la baisse du taux de chômage, conduirait à la génération d'une spirale
    inflationniste
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与美国、英国和加拿大的中央银行不同，欧洲中央银行（ECB）没有降低基准利率，称降息将会导致原材料价格上涨和失业率下降，从而引发通胀螺旋。
- en: La BCE souhaiterait maintenir le taux d'inflation au-dessous mais proche de
    deux pour cent
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧洲央行希望将通胀率保持在接近但低于2%。
- en: 'Selon un analyste, c''est le Catch 22 pour la BCE-: "il faut dissuader" l''inflation
    afin de ne plus avoir à intervenir sur ce sujet ultérieurement'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据一位分析师的说法，欧洲央行陷入了“Catch 22”：必须“遏制”通胀，以避免稍后对此采取干预措施。
- en: It is usually good to do a quick sanity check, when possible, to ensure that
    the files do actually line up. We can see the `Catch 22` phrase on line 7 of both
    files, which gives us comfort.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在可能的情况下进行快速的合理性检查是很好的做法，以确保文件确实对齐。我们可以看到两个文件第7行都出现了`Catch 22`这一短语，这让我们感到放心。
- en: Of course, 7 lines are far from sufficient for a statistical approach. We will
    achieve an elegant, generalizable solution only with mounds of data. And the mounds
    of data we will get for our training set will consist of 20 gigabytes of text,
    translated line by line very much like the preceding excerpts.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，7行远远不足以进行统计分析。我们只有通过大量数据才能实现一个优雅、可泛化的解决方案。我们将为训练集获取的海量数据将由20GB的文本组成，每行翻译就像前面的摘录一样。
- en: Just as we did with images, we'll use subsets for training, validation, and
    testing. We will also define a loss function and attempt to minimize that loss.
    Let's start with the data though.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们处理图像数据一样，我们将使用子集进行训练、验证和测试。我们还将定义损失函数并尝试最小化该损失。让我们从数据开始吧。
- en: Drinking from the firehose
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从消防栓里喝水
- en: As you did earlier, you should grab the code from [https://github.com/mlwithtf/MLwithTF/](https://github.com/mlwithtf/MLwithTF/).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 就像你之前做的那样，你应该从[https://github.com/mlwithtf/MLwithTF/](https://github.com/mlwithtf/MLwithTF/)抓取代码。
- en: 'We will be focusing on the `chapter_05` subfolder that has the following three
    files:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于`chapter_05`子文件夹，它包含以下三个文件：
- en: '`data_utils.py`'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_utils.py`'
- en: '`translate.py`'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`translate.py`'
- en: '`seq2seq_model.py`'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seq2seq_model.py`'
- en: 'The first file handles our data, so let''s start with that. The `prepare_wmt_dataset`
    function handles that. It is fairly similar to how we grabbed image datasets in
    the past, except now we''re grabbing two data subsets:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个文件处理我们的数据，让我们从这个开始。`prepare_wmt_dataset`函数处理这个问题。它与我们过去抓取图像数据集的方式非常相似，只不过现在我们抓取的是两个数据子集：
- en: '`giga-fren.release2.fr.gz`'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`giga-fren.release2.fr.gz`'
- en: '`giga-fren.release2.en.gz`'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`giga-fren.release2.en.gz`'
- en: Of course, these are the two languages we want to focus on. The beauty of our
    soon-to-be-built translator will be that the approach is entirely generalizable,
    so we can just as easily create a translator for, say, German or Spanish.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这就是我们想要关注的两种语言。我们即将构建的翻译器的美妙之处在于，它的方式是完全通用的，因此我们可以轻松地为德语或西班牙语等语言创建翻译器。
- en: 'The following screenshot is the specific subset of code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了特定的代码子集：
- en: '![](img/6a0c104a-e823-49f9-94af-80994af1b221.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a0c104a-e823-49f9-94af-80994af1b221.png)'
- en: 'Next, we will run through the two files of interest from earlier line by line
    and do two things—create vocabularies and tokenize the individual words. These
    are done with the `create_vocabulary` and `data_to_token_ids` functions, which
    we will get to in a moment. For now, let''s observe how to create the vocabulary
    and tokenize on our massive training set as well as a small development set, `newstest2013.fr`
    and `dev/newstest2013.en`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将逐行处理之前提到的两个文件，做两件事——创建词汇表并对单个单词进行分词。这些工作将通过`create_vocabulary`和`data_to_token_ids`函数完成，我们稍后会介绍。现在，让我们观察如何在庞大的训练集以及一个小的开发集（`newstest2013.fr`和`dev/newstest2013.en`）上创建词汇表和进行分词：
- en: '![](img/0449ecaa-9e2d-41fa-bdcb-a9a1178d8e61.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0449ecaa-9e2d-41fa-bdcb-a9a1178d8e61.png)'
- en: 'We created a vocabulary earlier using the following `create_vocabulary` function.
    We will start with an empty vocabulary map, `vocab = {}`, and run through each
    line of the data file and for each line, create a bucket of words using a basic
    tokenizer. (Warning: this is not to be confused with the more important token
    in the following ID function.)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前使用以下`create_vocabulary`函数创建了一个词汇表。我们将从一个空的词汇映射开始，`vocab = {}`，然后逐行读取数据文件，对每一行，使用基本的分词器创建一个单词桶。（警告：这与后面ID函数中的更重要的令牌不应混淆。）
- en: 'If we encounter a word we already have in our vocabulary, we will increment
    it as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们遇到一个我们词汇表中已有的单词，我们将按如下方式递增它：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Otherwise, we will initialize the count for that word, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，我们将初始化该单词的计数，如下所示：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We will keep doing this until we run out of lines on our training dataset. Next,
    we will sort our vocabulary by order of frequency using `sorted(vocab`, `key=vocab.get`,
    and `reverse=True)`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续这样做，直到我们的训练数据集中的所有行都处理完毕。接下来，我们将使用`sorted(vocab, key=vocab.get, reverse=True)`按频率对我们的词汇表进行排序。
- en: 'This is important because we won''t keep every single word, we''ll only keep
    the *k* most frequent words, where *k* is the vocabulary size we defined (we had
    defined this to 40,000 but you can choose different values and see how the results
    are affected):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这很重要，因为我们不会保留每一个单词，我们只会保留*最频繁的k*个单词，其中*k*是我们定义的词汇表大小（我们将其定义为40,000，但你可以选择不同的值，看看结果如何变化）：
- en: '![](img/b8b6bf07-d504-41c8-a06a-f4b76beab673.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b8b6bf07-d504-41c8-a06a-f4b76beab673.png)'
- en: 'While working with sentences and vocabularies is intuitive, this will need
    to get more abstract at this point—we''ll temporarily translate each vocabulary
    word we''ve learned into a simple integer. We will do this line by line using
    the `sequence_to_token_ids` function:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管处理句子和词汇表是直观的，但此时需要更抽象一些——我们将暂时把我们学到的每个词汇单词转换成一个简单的整数。我们将逐行使用`sequence_to_token_ids`函数来完成这项工作：
- en: '![](img/c1a33035-42f6-4cab-8e14-f78af870c26f.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c1a33035-42f6-4cab-8e14-f78af870c26f.png)'
- en: 'We will apply this approach to the entire data file using the `data_to_token_ids`
    function, which reads our training file, iterates line by line, and runs the `sequence_to_token_ids`
    function, which then uses our vocabulary listing to translate individual words
    in each sentence to integers:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`data_to_token_ids`函数将这种方法应用于整个数据文件，该函数读取我们的训练文件，逐行迭代，并运行`sequence_to_token_ids`函数，然后使用我们的词汇表将每个句子中的单个单词转换为整数：
- en: '![](img/eb56f8d6-2c8a-4747-9554-dad5f6898326.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb56f8d6-2c8a-4747-9554-dad5f6898326.png)'
- en: Where does this leave us? With two datasets of just numbers. We have just temporarily
    translated our English to French problem to a numbers to numbers problem with
    two sequences of sentences consisting of numbers mapping to vocabulary words.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我们处于什么状态呢？我们有了两个仅包含数字的数据集。我们只是暂时将我们的英语到法语的问题转化为一个数字到数字的问题，包含两组将单词映射到词汇表单词的数字序列。
- en: 'If we start with `["Brooklyn", "has", "lovely", "homes"]` and generate a `{"Brooklyn":
    1, "has": 3, "lovely": 8, "homes": 17"}` vocabulary, we will end up with `[1,
    3, 8, 17]`.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '如果我们从`["Brooklyn", "has", "lovely", "homes"]`开始，并生成一个`{"Brooklyn": 1, "has":
    3, "lovely": 8, "homes": 17}`的词汇表，那么我们将得到`[1, 3, 8, 17]`。'
- en: 'What does the output look like? The following typical file downloads:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输出看起来怎样？以下是典型的文件下载：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: I won't repeat the English section of the dataset processing as it is exactly
    the same. We will read the gigantic file line by line, create a vocabulary, and
    tokenize the words line by line for each of the two language files.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会重复数据集处理中的英语部分，因为它完全相同。我们将逐行读取庞大的文件，创建词汇表，并逐行对每个语言文件中的单词进行分词。
- en: Training day
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练日
- en: The crux of our effort will be the training, which is shown in the second file
    we encountered earlier—`translate.py`. The `prepare_wmt_dataset` function we reviewed
    earlier is, of course, the starting point as it creates our two datasets and tokenizes
    them into nice clean numbers.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们努力的关键是训练，这在我们之前遇到的第二个文件`translate.py`中展示。我们之前回顾过的`prepare_wmt_dataset`函数，当然是起点，它创建了我们的两个数据集并将它们标记为干净的数字。
- en: 'The training starts as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 训练如下开始：
- en: '![](img/66bd8fe5-2f96-47ac-aba9-0056eb700c57.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/66bd8fe5-2f96-47ac-aba9-0056eb700c57.png)'
- en: After preparing the data, we will create a TensorFlow session, as usual, and
    construct our model. We'll get to the model later; for now, let's look at our
    preparation and training loop.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好数据后，我们将像往常一样创建一个TensorFlow会话，并构建我们的模型。我们稍后会详细介绍模型；现在，先来看一下我们的准备工作和训练循环。
- en: We will define a dev set and a training set later, but for now, we will define
    a scale that is a floating point score ranging from 0 to 1\. Nothing complex here;
    the real work comes in the following training loop. This is very different from
    what we've done in previous chapters, so close attention is required.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会定义开发集和训练集，但现在，我们将定义一个范围从0到1的浮动分数。这部分没有什么复杂的；真正的工作将在接下来的训练循环中进行。这与我们在之前章节中所做的非常不同，所以需要格外注意。
- en: 'Our main training loop is seeking to minimize our error. There are two key
    statements. Here''s the first one:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要训练循环旨在最小化我们的误差。这里有两个关键的语句。首先是第一个：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'And, the second key is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个关键点如下：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `get_batch` function is essentially used to convert the two sequences into
    batch-major vectors and associated weights. These are then used on the model step,
    which returns our loss.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_batch`函数本质上是用来将这两个序列转换为批量主向量和相关的权重。然后，这些会被用于模型步骤，从而返回我们的损失。'
- en: 'We don''t deal with the loss though, we will use `perplexity`, which is `e`
    raised to the power of the loss:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不会直接处理损失，而是使用`perplexity`，它是损失的`e`次方：
- en: '![](img/41bc47dd-7cb9-4fb6-aa62-ce0db0ea3d45.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41bc47dd-7cb9-4fb6-aa62-ce0db0ea3d45.png)'
- en: 'At every *X* steps, we will save our progress using `previous_losses.append(loss)`,
    which is important because we will compare our current batch''s loss to previous
    losses. When losses start going up, we will reduce our learning rate using:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 每经过*X*步，我们将使用`previous_losses.append(loss)`保存进度，这非常重要，因为我们将比较当前批次的损失与之前的损失。当损失开始上升时，我们将使用以下方式降低学习率：
- en: '`sess.run(model.learning_rate_decay_op)`, and evaluate the loss on our `dev_set`,
    much like we used our validation set in earlier chapters:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`sess.run(model.learning_rate_decay_op)`，并在`dev_set`上评估损失，就像我们在之前的章节中使用验证集一样：'
- en: '![](img/03519901-49f4-4a9b-92c7-6f1d57294cff.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03519901-49f4-4a9b-92c7-6f1d57294cff.png)'
- en: 'We will get the following output when we run it:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行时，我们会得到以下输出：
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will see outputs at every 200 steps. This is one of about a dozen settings
    we''re using, which we defined at the top of the file:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会在每200步看到输出。这是我们使用的约十几个设置之一，它们在文件顶部定义：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We will use most of these settings when constructing the model object. That
    is, the final piece of the puzzle is the model itself, so let's look at that.
    We'll return to the third and final of the three files in our project—`seq2seq_model.py`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在构建模型对象时使用这些设置。也就是说，最终的拼图就是模型本身，所以让我们来看一下这个部分。我们将回到我们项目中的第三个也是最后一个文件——`seq2seq_model.py`。
- en: 'Recall how we created the model at the start of the training process after
    creating the TensorFlow session? Most of the parameters we''ve defined are used
    to initialize the following model:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得我们在训练开始时创建模型的过程吗？我们定义的大多数参数用于初始化以下模型：
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: However, what the initialize is accomplishing is inside `seq2seq_model.py`,
    so let's jump to that.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，初始化所做的工作在`seq2seq_model.py`中，所以让我们跳到那部分。
- en: You will find that the model is enormous, which is why we won't explain line
    by line but instead chunk by chunk.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现模型非常庞大，这就是为什么我们不逐行解释，而是逐块解释。
- en: 'The first section is the initialization of the model, demonstrated by the following
    two figures:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分是模型的初始化，通过以下两张图示范：
- en: '![](img/7e66ffcf-cafd-4922-b7b0-814aa7415731.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e66ffcf-cafd-4922-b7b0-814aa7415731.png)'
- en: The model starts with an initialization, which sets off the required parameters.
    We'll skip the setting of these parameters as we're already familiar with them—we
    initialized these parameters ourselves before the training, by just passing the
    values into the model construction statement, and they are finally passed into
    internal variables via `self.xyz` assignments.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 模型从初始化开始，这会设置所需的参数。我们将跳过这些参数的设置部分，因为我们已经熟悉它们——我们在训练之前就已经自己初始化了这些参数，只需将值传入模型构建语句中，最终通过`self.xyz`赋值将它们传递到内部变量中。
- en: Recall how we passed in the size of each model layer (size=1024) and the number
    of layers (3). These are pretty important as we construct the weights and biases
    (`proj_w` and `proj_b`). The weights are *A x B* where *A* is the layer size and
    *B* is the vocabulary size of the target language. The biases are just passed
    based on the size of the target vocabulary.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们是如何传入每个模型层的大小（size=1024）和层数（3）的。这些非常重要，因为我们在构建权重和偏置（`proj_w` 和 `proj_b`）时用到了它们。权重是*
    A x B *，其中*A*是层的大小，*B*是目标语言的词汇大小。偏置则仅根据目标词汇的大小传入。
- en: 'Finally, the weights and biases from our `output_project` tuple - `output_projection
    = (w, b)`- and use the transposed weights and biases to form our `softmax_loss_function`,
    which we''ll use over and over to gauge performance:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们从`output_project`元组中获取权重和偏置 - `output_projection = (w, b)` - 并使用转置的权重和偏置来构建我们的`softmax_loss_function`，我们将反复使用它来衡量性能：
- en: '![](img/cabd2352-cfe1-44fb-b859-e18a47c3196b.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cabd2352-cfe1-44fb-b859-e18a47c3196b.png)'
- en: 'The next section is the step function, which is shown in the following figure.
    The first half is just error checking, so we''ll skip through it. Most interesting
    is the construction of the output feed using stochastic gradient descent:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分是步进函数，如下图所示。前半部分只是错误检查，我们会跳过。最有趣的是使用随机梯度下降法构建输出馈送：
- en: '![](img/a210e471-a641-461c-b26f-c0c9030328ec.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a210e471-a641-461c-b26f-c0c9030328ec.png)'
- en: 'The final section of the model is the `get_batch` function, which is shown
    in the following figure. We will explain the individual parts with inline comments:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的最后部分是`get_batch`函数，如下图所示。我们将通过内联注释解释各个部分：
- en: '![](img/e45db696-3d48-4455-b613-e36fb35ec921.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e45db696-3d48-4455-b613-e36fb35ec921.png)'
- en: 'When we run this, we can get a perfect training run, as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行此程序时，我们可以获得一个完美的训练过程，如下所示：
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Alternatively, we may find steps where we have reduced our learning rate after
    consistent increases in losses. Either way, we will keep testing on our *development*
    set until our accuracy increases.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可能会发现，在损失持续增加后，我们的学习速率有所降低。无论哪种情况，我们都会在*开发*集上进行测试，直到我们的准确率提高。
- en: Summary
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered sequence-to-sequence networks and wrote a language
    translator using a series of known sentence-by-sentence translations as a training
    set. We were introduced to RNNs as a base for our work and likely crossed the
    threshold of big data as we trained using a 20 GB set of training data.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了序列到序列网络，并利用一系列已知的逐句翻译作为训练集编写了一个语言翻译器。我们介绍了作为工作基础的循环神经网络（RNN），并可能在训练过程中跨越了大数据的门槛，因为我们使用了一个20GB的训练数据集。
- en: Next, we'll jump into tabular data and make predictions on economic and financial
    data. We'll use parts of our prior work so we can hit the ground running, namely
    the initial pipeline work we've written so far to download and prepare training
    data. However, we'll focus on a time series problem, so it will be quite different
    from the image and text work we've done to date.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进入表格数据，并对经济和金融数据进行预测。我们将使用之前工作的一部分，以便我们能够迅速开始，具体来说，就是我们迄今为止编写的初始数据管道工作，用于下载和准备训练数据。然而，我们将重点关注时间序列问题，因此这与我们迄今为止所做的图像和文本工作将有很大的不同。
