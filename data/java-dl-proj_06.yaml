- en: Real-Time Object Detection using YOLO, JavaCV, and DL4J
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用YOLO、JavaCV和DL4J进行实时物体检测
- en: '[PRE0]'
  id: totrans-1
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Considering this drawback, in this chapter, we will develop an end-to-end project
    that will detect objects from video frames when a video clip plays continuously.
    We will be utilizing a trained YOLO model for transfer learning and JavaCV techniques
    on top of **Deeplearning4j** (**DL4J**) to do this. In short, the following topics
    will be covered throughout this end-to-end project:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一缺点，在本章中，我们将开发一个端到端的项目，该项目将在视频片段连续播放时从视频帧中检测物体。我们将利用训练好的YOLO模型进行迁移学习，并在**Deeplearning4j**（**DL4J**）的基础上使用JavaCV技术来实现。简而言之，以下主题将贯穿整个端到端的项目：
- en: Object detection
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体检测
- en: Challenges in object detection from videos
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频中物体检测的挑战
- en: Using YOLO with DL4J
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用YOLO与DL4J
- en: Frequently asked questions (FAQs)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见问题解答（FAQ）
- en: Object detection from images and videos
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像与视频中的物体检测
- en: Deep learning has been widely applied to various computer vision tasks such
    as image classification, object detection, semantic segmentation, and human pose
    estimation. When we intend to solve the object detection problem from images,
    the whole process starts from object classification. Then we perform object localization
    and finally, we perform the object detection.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已广泛应用于各种计算机视觉任务，如图像分类、物体检测、语义分割和人体姿态估计。当我们打算解决图像中的物体检测问题时，整个过程从物体分类开始。接着我们进行物体定位，最后进行物体检测。
- en: This project is highly inspired by the Java Autonomous driving – Car detection
    article by Klevis Ramo ([http://ramok.tech/](http://ramok.tech/)). Also, some
    theoretical concepts are used (but significantly extended for this need) with
    due permission from the author.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目深受Klevis Ramo的《Java自动驾驶——汽车检测》一文的启发（[http://ramok.tech/](http://ramok.tech/)）。同时，部分理论概念（但在此需求下已显著扩展）已获得作者的授权。
- en: Object classification, localization, and detection
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体分类、定位与检测
- en: In an object classification problem, from a given image (or video clip), we're
    interested to know if it contains a region of interest (ROI) or object. More formally,
    "the image contains a car" versus "the image does not contain any car." To solve
    this problem, over the last few years, ImageNet and PASCAL VOC (see more at [http://host.robots.ox.ac.uk/pascal/VOC/](http://host.robots.ox.ac.uk/pascal/VOC/))
    have been in use and are based on deep CNN architectures .
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在物体分类问题中，给定一张图像（或视频片段），我们关心的是它是否包含感兴趣的区域（ROI）或物体。更正式地说，就是“图像包含一辆车”与“图像不包含任何车”。为了解决这个问题，在过去的几年中，ImageNet和PASCAL
    VOC（详见[http://host.robots.ox.ac.uk/pascal/VOC/](http://host.robots.ox.ac.uk/pascal/VOC/)）被广泛使用，并且基于深度卷积神经网络（CNN）架构。
- en: Moreover, of course, the latest technological advances (that is, both software
    and hardware) have contributed to boosting the performance to the next level.
    Despite such success of state-of-the-art approaches for still images, detecting
    objects in videos is not easy. However, object detection from videos brings up
    new questions, possibilities, and challenges on how to solve the object detection
    problem for videos effectively and robustly.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当然，最新的技术进展（即软硬件的进步）也推动了性能提升，达到了新的水平。尽管如此，尽管最先进的方法在静态图像中的成功，视频中的物体检测依然不容易。然而，从视频中进行物体检测引出了许多新的问题、可能性和挑战，如何有效且稳健地解决视频中的物体检测问题。
- en: Answering this question is not an easy job. First, let's try to solve the problem
    step by step. First let's try to answer for an still image. Well, when we want
    to use a CNN to predict if an image contains a particular object or not, we need
    to localize the object position in the image. To do that, we need to specify the
    position of the object in the image along with the classification task. This is
    usually done by marking the object with a rectangular box commonly known as a
    bounding box.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 解答这个问题并不容易。首先，让我们一步步地尝试解决这个问题。首先，让我们先尝试解决静态图像的情况。好吧，当我们想要使用CNN来预测图像是否包含特定物体时，我们需要在图像中定位物体的位置。为此，我们需要指定物体在图像中的位置，并与分类任务一起完成。这通常通过用矩形框标记物体来完成，矩形框通常被称为边界框（bounding
    box）。
- en: 'Now, the idea of a bounding box is that at each frame of a video, the algorithm
    is required to annotate bounding boxes and confidence scores on objects of each
    class. To grab the idea clearly, take a look at what a bounding box is. A bounding
    box is usually represented by the center (*b^x*, *b^y*), rectangle height (*b^h*),
    and rectangle width (*b^w*), as shown in the following diagram:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，边界框的概念是，在视频的每一帧中，算法需要对每个类别的物体标注边界框和置信度分数。为了清楚地理解这一点，我们来看一下什么是边界框。边界框通常由中心
    (*b^x*, *b^y*)、矩形高度 (*b^h*) 和矩形宽度 (*b^w*) 来表示，如下图所示：
- en: '![](img/518cdb57-d806-492e-9d1d-57fe445afe3d.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/518cdb57-d806-492e-9d1d-57fe445afe3d.png)'
- en: A bounding box representation
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 边界框表示
- en: 'Now that we know how to represent such a bounding box, we can perceive what
    we need to define these things in our training data for each of the objects in
    the images too. Only then will the network be able to output the following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何表示这样的边界框，我们可以理解在我们的训练数据中定义这些信息所需的内容，针对每张图像中的每个物体。只有这样，网络才能输出以下内容：
- en: The probability of an image's class number (for example, 20% probability of
    being a car, 60% probability of being a bus, 10% probability of being a truck,
    or 10% probability of being a train)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像类别的概率（例如，20% 的概率是汽车，60% 的概率是公交车，10% 的概率是卡车，或 10% 的概率是火车）
- en: Also, the four variables defining the bounding box of the object
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，定义物体边界框的四个变量
- en: Knowing only this information is not enough. Interestingly, with this minimum
    contextual information about bounding box points (that is, center, width, and
    height), our model is still able to predict and give us a more detailed view of
    the content. In other words, using this approach, we can solve the object localization
    problem, but still it is applicable only for a single object.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 仅知道这些信息还不够。有趣的是，凭借关于边界框点的最小上下文信息（即中心、宽度和高度），我们的模型仍然能够进行预测，并为我们提供更详细的内容视图。换句话说，采用这种方法，我们可以解决物体定位问题，但它仍然仅适用于单一物体。
- en: Therefore, we can even go a step further by localizing not just a single object
    but multiple or all objects in the image, which will help us move toward the object
    detection problem. Although the structure of the original image remains the same,
    we need to deal with multiple bounding boxes in a single image.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们甚至可以进一步推进，不仅仅定位单一物体，而是定位图像中的多个或所有物体，这将帮助我们向物体检测问题迈进。尽管原始图像的结构保持不变，但我们需要在单张图像中处理多个边界框。
- en: Now to crack this problem, a state-of-the-art technique is dividing the image
    into smaller rectangles. And we have the same additional five variables we have
    already seen (*P^c, b^x, b^y*, *b^h*, *b^w*) and, of course, the normal prediction
    probabilities at each bounding box.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了解决这个问题，一种最先进的技术是将图像划分为较小的矩形。我们已经看到的五个额外变量（*P^c, b^x, b^y*, *b^h*, *b^w*）以及每个边界框的正常预测概率，仍然适用。
- en: The idea sounds easy, but how would it work in practice? If we just need to
    deal with a static image classification problem, things become easier. Using a
    Naïve approach would be cropping each car image from a collection of thousands
    of car images and then we training a convolution neural network (for example,
    VGG-19) to train the model with all these images (although, each image might have
    a different size).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法听起来很简单，但它在实际中如何运作呢？如果我们只需要处理一个静态图像分类问题，事情会变得更简单。使用一种朴素方法，即从成千上万张汽车图像中裁剪出每一张汽车图像，然后训练一个卷积神经网络（例如，VGG-19）来使用所有这些图像训练模型（尽管每张图像的大小可能不同）。
- en: '![](img/182e01a5-2ec9-4892-8184-ef2d89b6918c.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/182e01a5-2ec9-4892-8184-ef2d89b6918c.png)'
- en: Typical highway traffic
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的高速公路交通
- en: Now, to handle this scenario, we can scan the image with a sliding rectangle
    window and each time let our model predict if there is a car in it or not. As
    we can see by using different sizes of rectangles, we can figure out quite a different
    shape for cars and their positions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了处理这种情况，我们可以使用滑动矩形窗口扫描图像，每次让我们的模型预测其中是否有汽车。正如我们所看到的，通过使用不同大小的矩形，我们可以为汽车及其位置推测出非常不同的形状。
- en: Albeit, this works quite well in detecting only cars, but imagine a more practical
    problem such as developing an autonomous driving application. On a typical highway,
    in a city even in a suburb, there will be many cars, buses, trucks, motorbikes,
    bicycles, and other vehicles. Moreover, there will be other objects, such as pedestrians,
    traffic signs, bridges, dividers, and lamp-posts. These will make the scenario
    more complicated. Nevertheless, the size of real images will very different (that
    is, much bigger) compared to cropped ones. Also, in the front, many cars might
    be approaching, so manual resizing, feature extraction and then handcraft training
    would be necessary.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法在检测汽车方面效果很好，但假设一个更实际的问题，比如开发自动驾驶应用。在典型的高速公路上，城市甚至郊区，会有许多汽车、公交车、卡车、摩托车、自行车和其他交通工具。此外，还会有行人、交通标志、桥梁、隔离带和路灯等其他物体。这些因素会使场景变得更加复杂。然而，实际图像的尺寸会与裁剪图像的尺寸差异很大（即，实际图像要大得多）。此外，在前方，许多汽车可能正在接近，因此需要手动调整大小、特征提取，然后进行手工训练。
- en: Another issue would be the slowness of the training algorithm, so it would not
    be used for real-time video object detection. This application will be built so
    that you folks can learn something useful so that the same knowledge can be extended
    and applied to emerging applications, such as autonomous driving.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是训练算法的慢速，因此它不能用于实时视频目标检测。这个应用将被构建出来，以便大家可以学到一些有用的知识，从而将相同的知识扩展并应用到新兴的应用中，比如自动驾驶。
- en: Anyway, let's come to the original discussion. When moving the rectangle (right,
    left, up and down), many shared pixels may not be reused but they are just recalculated
    repeatedly. Even with very accurate and different bounding box sizes, this approach
    will fail to mark the object with a bounding box very precisely.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 不管怎样，让我们回到最初的讨论。当矩形（向右、向左、向上和向下）移动时，许多共享的像素可能无法被重用，而是被反复重新计算。即使使用非常精确和不同大小的边界框，这种方法也无法非常准确地标出物体的边界框。
- en: Consequently, the model may not output the class of the vehicle very accurately
    as if the box may include only a part of the object. This might lead an autonomous
    driving car to be accident-prone—that is, may collide with other vehicles or objects.
    Now to get rid of this limitation, one of the state-of-the-art approaches is using
    the **Convolutional Sliding Window** (**CSW**) solution, which is pretty much
    utilized in YOLO (we will see this later on).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，模型可能无法非常准确地输出车辆的类别，因为框中可能只包含物体的一部分。这可能导致自动驾驶汽车容易发生事故——即，可能会与其他车辆或物体发生碰撞。为了摆脱这一限制，当前最先进的一个方法是使用**卷积滑动窗口**（**CSW**）解决方案，这在YOLO中被广泛使用（我们稍后会看到）。
- en: Convolutional Sliding Window (CSW)
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积滑动窗口（CSW）
- en: In the previous subsection, we have seen that the Naïve sliding window-based
    approach has severe performance drawbacks since this type of approach is not able
    to reuse many of the values already computed.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的子章节中，我们看到基于朴素滑动窗口的方法存在严重的性能缺陷，因为这种方法无法重用已经计算出的许多值。
- en: 'Nevertheless, when each individual window moves, we need to execute millions
    of hyperparameters for all pixels in order to get a prediction. In reality, most
    of the computation could be reused by introducing convolution (refer to [Chapter
    5](2d4fc6f2-3753-456c-8774-3f41dbe13cfb.xhtml), *Image Classification using Transfer
    Learning*, to get to know more on transfer learning using pre-trained DCNN architecture
    for image classification). This can be achieved in two incremental ways:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当每个独立的窗口移动时，我们需要为所有像素执行数百万个超参数计算才能得到预测。实际上，通过引入卷积，大部分计算可以被重用（参见[第5章](2d4fc6f2-3753-456c-8774-3f41dbe13cfb.xhtml)，*使用迁移学习的图像分类*，了解更多关于使用预训练的深度卷积神经网络（DCNN）架构进行图像分类的迁移学习）。这一点可以通过两种增量方式实现：
- en: By turning full-connected CNN layers into convolution
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将全连接CNN层转化为卷积
- en: Using CSW
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CSW
- en: We have seen that whatever DCNN architectures people are using (for example,
    DarkNet, VGG-16, AlexNet, ImageNet, ResNet, and Inception), regardless of their
    size and configuration, in the end, they were used to feed fully connected neural
    nets with a different number of layers and output several predictions depending
    on classes.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，无论人们使用的是哪种DCNN架构（例如，DarkNet、VGG-16、AlexNet、ImageNet、ResNet和Inception），无论其大小和配置如何，最终它们都被用来喂入全连接神经网络，具有不同数量的层，并根据类别输出多个预测结果。
- en: In addition, these deep architectures typically have so many layers that it
    is difficult to interpret them well. Therefore, taking a smaller network sounds
    a reasonable choice. In the following diagram, the network takes a colored image
    (that is, RGB) of size 32 x 32 x 3 as input. It then uses the same convolution,
    which leaves the first two dimensions (that is, width x height) unchanged at 3
    x 3 x 64 in order to get an output 32 x 32 x 64\. This way, the 3rd dimension
    (that is, 64) remains the same as conv matrix.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这些深度架构通常有非常多的层，以至于很难很好地解释它们。因此，选择一个更小的网络听起来是一个合理的选择。在以下图示中，网络以一个32 x 32 x
    3的彩色图像（即RGB）作为输入。然后，它使用相同的卷积，这使得前两个维度（即宽度x高度）保持不变，仍为3 x 3 x 64，以获得输出32 x 32 x
    64。通过这种方式，第三维度（即64）与卷积矩阵保持一致。
- en: '![](img/14c94add-449d-4c09-ab47-49d25838b4d0.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/14c94add-449d-4c09-ab47-49d25838b4d0.png)'
- en: Then, a maximum pooling layer is placed to reduce the width and height but leaving
    the 3rd dimension unchanged at 16 x 16 x 64\. After that, the reduced layer is
    fed into a dense layer having 2 hidden layers with 256 and 128 neurons each. In
    the end, the network outputs the probabilities of five classes using a Softmax
    layer.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，放置一个最大池化层来减少宽度和高度，但保持第三维度不变，仍为16 x 16 x 64。之后，将减少后的层输入到一个密集层，该层有两个隐藏层，每个隐藏层包含256和128个神经元。最后，网络使用Softmax层输出五个类别的概率。
- en: 'Now, let''s see how we can replace **fully connected** (**FC**) layers with
    conv layers while leaving the linear function of the input at 16 x 16 x 64, as
    illustrated by the following diagram:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何将**全连接**（**FC**）层替换为卷积层，同时保持输入的线性函数为16 x 16 x 64，如下图所示：
- en: '![](img/a8745d97-47a6-4e71-9f1b-3e913238ab0f.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a8745d97-47a6-4e71-9f1b-3e913238ab0f.png)'
- en: 'In the preceding diagram, we just replaced the FC layers with conv filters.
    In reality, a 16 x 16 x 256 conv filter is equivalent to a 16 x 16 x 64 x 256
    matrix. In that case, the third dimension, 64, is always the same as the third
    dimension input 16 x 16 x 64\. Therefore, it can be written as a 16 x 16 x 256
    conv filter by omitting the 64, which is actually equivalent to the corresponding
    FC layer. The following math provides the answer as to why:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，我们只是将FC层替换为卷积滤波器。实际上，一个16 x 16 x 256的卷积滤波器相当于一个16 x 16 x 64 x 256的矩阵。在这种情况下，第三维度64总是与输入的第三维度16
    x 16 x 64相同。因此，它可以通过省略64来表示为16 x 16 x 256的卷积滤波器，这实际上相当于对应的FC层。以下的数学公式可以解答这个问题：
- en: '*Out: 1 x 1 x 256 = in: [16 x 16 x 64] * conv: [16 x 16 x 64 x 256]*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出: 1 x 1 x 256 = 输入: [16 x 16 x 64] * 卷积: [16 x 16 x 64 x 256]*'
- en: 'The preceding math signifies that every element of the output 1 x 1 x 256 is
    a linear function of a corresponding element of the input 16 x 16 x 64\. The reason
    why we bothered to convert FC layers to convolution layers is that it will give
    us more flexibility in the way output is generated by the network: with an FC
    layer, we will always have the same output size, which is a number of classes.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 上述数学公式意味着输出1 x 1 x 256的每个元素都是输入16 x 16 x 64中相应元素的线性函数。我们将FC层转换为卷积层的原因是，它将为我们提供更多的灵活性来生成网络的输出：对于FC层，我们将始终得到相同的输出大小，也就是类别的数量。
- en: Now, in order to see the effectiveness of replacing the FC layers with a convolution
    filter, we have to take an input image having a bigger size, say 36 x 36 x 3\.
    Now, if we use the Naïve sliding window technique, where stride is 2 and we have
    FC, we need to move the original image size nine times to cover all, therefore,
    executing the model nine times as well. Hence, it definitely does not make sense
    to adopt that approach. Instead, let us try now to apply this new bigger matrix
    as input for our new model with convolution layers only.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了看到将FC层替换为卷积滤波器的效果，我们需要使用一个更大的输入图像，比如36 x 36 x 3。如果我们使用简单的滑动窗口技术，步长为2并且有FC，我们需要将原始图像的大小移动九次才能覆盖所有区域，因此也需要执行九次模型。因此，采用这种方法显然没有意义。相反，让我们尝试将这个新的更大的矩阵作为输入，应用到我们只包含卷积层的新模型中。
- en: '![](img/7025aace-6796-4bb6-bca7-0181884071df.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7025aace-6796-4bb6-bca7-0181884071df.png)'
- en: Now, we can see that the output has changed from 1 x 1 x 5 to 3 x 3 x 5, which
    is in comparison to FC. Again, if we recall the CSW-based approach, where we had
    to move the sliding window nine times to cover all images, which is interestingly
    equal to the 3 x 3 output of conv, nonetheless, each 3 x 3 cell represents the
    probability prediction results of a sliding window of 1 x 1 x 5 class! Therefore,
    instead of having only one 1 x 1 x 5 with 9 lots of sliding window moves, the
    output now with one shot is 3 x 3 x 5.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到输出已经从1 x 1 x 5变成了3 x 3 x 5，这与全连接（FC）进行对比时有所不同。再回想一下基于CSW的方法，我们必须将滑动窗口移动九次以覆盖所有图像，这巧妙地与卷积的3
    x 3输出相等，不过，每个3 x 3的单元代表了一个1 x 1 x 5类别的滑动窗口的概率预测结果！因此，最终输出不再是仅有的一个1 x 1 x 5并经过9次滑动窗口移动，而是通过一次操作得到的3
    x 3 x 5。
- en: Now, using the CSW-based approach, we have been able to solve the object detection
    problem from images. Yet, this approach is not so accurate but still, will produce
    an acceptable result with marginal accuracy. Nevertheless, when it comes to real-time
    video, things get much more complicated. We will see how YOLO has solved the remaining
    limitation later in this chapter. For the time being, let's try to understand
    the underlying complexity in detecting objects from video clips.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，采用基于CSW的方法，我们已经能够解决图像中的物体检测问题。然而，这种方法并不十分准确，但仍然能在精度稍逊的情况下产生一个可接受的结果。不过，当涉及到实时视频时，情况变得更加复杂。我们将在本章稍后看到YOLO是如何解决剩余的局限性的。现在，先让我们试着理解从视频片段中检测物体的底层复杂性。
- en: Object detection from videos
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从视频中进行物体检测
- en: Let's think of a simple scenario before digging down deeper. Suppose we have
    a video clip containing the movement of a cat or wolf in a forest. Now we want
    to detect this animal but while on the move at each timestep.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究之前，让我们先思考一个简单的情境。假设我们有一个视频片段，包含一只猫或狼在森林中的移动。现在，我们希望在每个时间点上检测到这个移动中的动物。
- en: The following graph shows the challenges in such a scenario. The red boxes are
    ground truth annotations. The upper part of the figure (that is **a**) shows still-image
    object detection methods have large temporal fluctuations across frames, even
    on ground truth bounding boxes. The fluctuations may result from motion blur,
    video defocus, part occlusion and bad pose. Information from boxes of the same
    object on adjacent frames needs to be utilized for object detection in video.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了这种情境下的挑战。红色框是实际标注。图像上半部分（即**a**）显示静态图像物体检测方法在帧间存在较大的时间波动，甚至在实际标注的边界框上。波动可能是由运动模糊、视频失焦、部分遮挡或姿态问题引起的。相邻帧中同一物体的框的信息需要被利用，以便在视频中进行物体检测。
- en: On the other hand, (**b**) shows tracking is able to relate boxes of the same
    object. However, due to occlusions, appearance changes and pose variations, the
    tracked boxes may drift to non-target objects. Object detectors should be incorporated
    into tracking algorithms to constantly start new tracks when drifting occurs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，(**b**)显示了跟踪能够关联同一物体的框。然而，由于遮挡、外观变化和姿态变化，跟踪框可能会漂移到非目标物体上。物体检测器应与跟踪算法结合，以便在漂移发生时持续开始新的跟踪。
- en: '![](img/6e9d0c22-358d-4b9c-ad96-984e81c33202.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e9d0c22-358d-4b9c-ad96-984e81c33202.png)'
- en: 'Challenges in object detection from the video (source: Kai Kang et al, Object
    Detection from Video Tubelets with Convolutional Neural Networks)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 从视频中进行物体检测的挑战（来源：Kai Kang 等，《基于卷积神经网络的视频管道物体检测》）
- en: There are methods to tackle this problem. However, most of them focus on detecting
    one specific class of objects, such as pedestrians, cars, or humans with actions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些方法可以解决这个问题。然而，大多数方法侧重于检测一种特定类别的物体，比如行人、汽车或有动作的人类。
- en: Fortunately, similar to object detection in still images being able to assist
    tasks including image classification, localization, and object detection, accurately
    detecting objects in videos could possibly boost the performance of video classification
    as well. By locating objects in the videos, the semantic meaning of a video could
    also be described more clearly, which results in more robust performance for video-based
    tasks.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，类似于静态图像中的物体检测能够协助图像分类、定位和物体检测等任务，准确检测视频中的物体也可能提升视频分类的性能。通过定位视频中的物体，也可以更清晰地描述视频的语义含义，从而增强视频任务的鲁棒性。
- en: In other words, existing methods for general object detection cannot be applied
    to solve this problem effectively. Their performance may suffer from large appearance
    changes of objects in videos. For instance, in the preceding graph (**a**), if
    the cat faces the camera at first and then turns back, it's back image cannot
    be effectively recognized as a cat because it contains little texture information
    and is not likely to be included in the training. Nevertheless, this is a simpler
    scenario where we have to detect a single object (that is, an animal).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，现有的通用目标检测方法无法有效解决这个问题。它们的性能可能会受到视频中物体外观变化较大的影响。例如，在上面的图（**a**）中，如果猫最初面对相机然后转身，背部的图像可能无法有效地被识别为猫，因为它包含的纹理信息很少，而且不太可能包含在训练数据中。然而，这是一个更简单的场景，我们只需要检测一个物体（即动物）。
- en: When we want to develop an application for an autonomous driving car, we will
    have to deal with so many objects and considerations. Anyway, since we cannot
    cover all the aspects through this chapter, let's move to solving the problem
    with only a modicum of knowledge.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想为自动驾驶汽车开发应用程序时，我们将不得不处理许多物体和考虑因素。无论如何，由于我们无法在本章中涵盖所有方面，我们就以最基本的知识来解决这个问题。
- en: Furthermore, implementing and training these types of applications from scratch
    is time consuming and challenging too. Therefore, nowadays, transfer learning
    techniques are becoming popular and viable options. By utilizing a trained model,
    we can develop much more easily. One such trained object detection framework is
    YOLO, which is one of the state-of-the-art real-time object detection systems.
    These challenges and YOLO like frameworks have motivated me to develop this project
    with a minimum of effort.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，从零开始实现和训练这些类型的应用程序既耗时又具有挑战性。因此，如今，迁移学习技术正在成为流行且可行的选择。通过利用已经训练好的模型，我们可以更轻松地进行开发。一个这样的训练过的目标检测框架是YOLO，它是最先进的实时目标检测系统之一。这些挑战和像YOLO这样的框架激励我以最小的努力开发这个项目。
- en: You Only Look Once (YOLO)
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 只看一次（YOLO）
- en: 'Although we already addressed issues in object detection from static images
    by introducing convolution-sliding windows, our model still may not output very
    accurate bounding boxes, even with several bounding box sizes. Let''s see how
    YOLO solves that problem well:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经通过引入卷积滑动窗口解决了静态图像中的目标检测问题，但即便使用了多个边界框尺寸，我们的模型可能仍然无法输出非常准确的边界框。让我们看看YOLO是如何很好地解决这个问题的：
- en: '![](img/a8d5df7e-72c6-47c5-a951-4a387edd4f3b.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a8d5df7e-72c6-47c5-a951-4a387edd4f3b.jpg)'
- en: Using the bounding box specification, we go to each image and mark the objects
    we want to detect
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用边界框规格，我们查看每张图像并标记我们想要检测的目标
- en: We need to label our training data in some specific way so that the YOLO algorithm
    will work correctly. YOLO V2 format requires bounding box dimensions of *b^x*,
    *b^y* and *b^h*, *b*^(*w*) in order to be relative to the original image width
    and height.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要以特定的方式标记训练数据，以便YOLO算法能够正确工作。YOLO V2格式要求边界框的尺寸为*b^x*、*b^y*、*b^h*、*b^w*，这些尺寸必须相对于原始图像的宽度和高度。
- en: First, we normally go to each image and mark the objects we want to detect.
    After that, each image is split into a smaller number of rectangles (boxes), usually,
    13 x 13 rectangles, but here, for simplicity, we have 8 x 9\. Both the bounding
    box (blue) and the object can be part of several boxes (green), so we can assign
    the object and the bounding box only to the box owning the centre of the object
    (yellow boxes).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通常会查看每一张图像，并标记我们想要检测的目标。之后，每张图像会被分割成更小的矩形（框），通常是13 x 13个矩形，但在这里为了简化，我们使用8
    x 9个矩形。边界框（蓝色）和目标可以属于多个框（绿色），因此我们只将目标和边界框分配给包含目标中心的框（黄色框）。
- en: This way, we train our model with four additional (besides identifying the object
    as a car) variables (*b^x*, *b^y*, *b^h*, and *b^w*) and assign those to the box
    owning the center *b^x*, *b^y*. Since the neural network is trained with this
    labeled data, it also predicts this four variables (besides what object is) values
    or bounding boxes.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们用四个附加变量（除了识别目标是汽车外）训练我们的模型（*b^x*、*b^y*、*b^h* 和 *b^w*），并将这些变量分配给拥有目标中心的框*b^x*、*b^y*。由于神经网络是用这些标记过的数据训练的，它也会预测这四个变量（除了目标是什么）的值或边界框。
- en: Instead of scanning with predefined bounding box sizes and trying to fit the
    object, we let the model learn how to mark objects with bounding boxes. Therefore,
    bounding boxes are now flexible. This is definitely a better approach and the
    accuracy of the bounding boxes is much higher and more flexible.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不再使用预定义的边界框大小进行扫描并尝试拟合物体，而是让模型学习如何用边界框标记物体。因此，边界框现在是灵活的。这无疑是一种更好的方法，边界框的准确性更高、更灵活。
- en: Let us see how we can represent the output now that we have added four variables
    (*b^x*, *b^y*, *b^h*, and *b^w*) beside classes like 1-car, 2-pedestrian. In reality,
    another variable, *P^c*, is also added, which simply tells if the image has any
    of the objects we want to detect at all.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看现在如何表示输出，考虑到我们在类别（例如 1-汽车，2-行人）旁边添加了四个变量（*b^x*，*b^y*，*b^h* 和 *b^w*）。实际上，还添加了另一个变量
    *P^c*，它简单地告诉我们图像是否包含我们想要检测的任何物体。
- en: '**P^c =1(red)**: This means there is at least one of the objects, so it is
    worth looking at probabilities and bounding boxes'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**P^c =1(red)**: 这意味着至少有一个物体存在，所以值得关注概率和边界框。'
- en: '**P^c =0(red)**: The image has none of the objects we want so we do not care
    about probabilities or bounding box specifications'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**P^c =0(red)**: 该图像没有我们想要的任何物体，因此我们不关心概率或边界框规格。'
- en: The resultant predictions, *b[w,]* and *b[h]*, are normalized by the height
    and width of the image. (Training labels are chosen this way.) So, if the predictions
    *b[x]* and *b[y]* for the box containing the car are (0.3, 0.8), then the actual
    width and height on 13 x 13 feature maps are (13 x 0.3, 13 x 0.8).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 结果预测，*b[w,]* 和 *b[h]*，是通过图像的高度和宽度进行归一化的。（训练标签是这样选择的。）因此，如果包含汽车的边界框预测 *b[x]*
    和 *b[y]* 为 (0.3, 0.8)，那么在 13 x 13 特征图上的实际宽度和高度为 (13 x 0.3, 13 x 0.8)。
- en: 'A bounding box predicted by YOLO is defined relative to the box that owns the
    center of the object (yellow). The upper left corner of the box starts from (0,
    0) and the bottom right (1, 1). Since the point is inside the box, in such a scenario,
    the sigmoid activation function makes sure that the center (*b[x]* , *b[y]*) is
    in the range 0-1, as outlined in the following diagram:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO 预测的边界框是相对于拥有物体中心的框（黄色）来定义的。框的左上角从 (0, 0) 开始，右下角是 (1, 1)。由于该点位于框内，因此在这种情况下，sigmoid
    激活函数确保中心 (*b[x]* , *b[y]*) 的值在 0 到 1 之间，如下图所示：
- en: '![](img/4ff68531-32ea-4470-8ffb-db6422a055b9.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ff68531-32ea-4470-8ffb-db6422a055b9.png)'
- en: Sigmoid activation function makes sure that the center (b[x] , b[y])is in the
    range 0-1
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 激活函数确保中心点 (*b[x]* , *b[y]*) 的值在 0 到 1 之间。
- en: 'While *b[h]*, *b[^w]* are calculated in proportion to *w* and *h* values (yellow)
    of the box, values can be greater than 1 (exponential used for positive values).
    In the picture, we can see that the width *b[w]* of the bounding box is almost
    1.8 times the size of the box width *w*. Similarly, *b[h]* is approximately 1.6
    times the size of box height *h*, as outlined in the following diagram:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 *b[h]*，*b[^w]* 是按照框的 *w* 和 *h* 值（黄色）按比例计算的，值可以大于 1（对于正值使用指数）。从图中我们可以看到，边界框的宽度
    *b[w]* 几乎是框宽度 *w* 的 1.8 倍。类似地，*b[h]* 约为框高度 *h* 的 1.6 倍，如下图所示：
- en: '![](img/2925cbfc-4935-4abb-a3ab-5a696df75677.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2925cbfc-4935-4abb-a3ab-5a696df75677.png)'
- en: 'Now, the question would be what is the probability that an object is contained
    in the bounding box? Well, to answer this question, we need to know the object
    score, which represents the probability that an object is contained inside a bounding
    box. It should be nearly 1 for the red and the neighboring grids, while almost
    0 for, say, the grid at the corners. The following formulas describe how the network
    output is transformed so as to obtain bounding box predictions:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，问题是物体包含在边界框中的概率是多少？要回答这个问题，我们需要知道物体得分，它表示物体被包含在边界框中的概率。对于红色及邻近的网格，它应该接近 1，而对于角落的网格来说，几乎是
    0。以下公式描述了网络输出如何被转换以获取边界框预测：
- en: '![](img/320f4dc8-05b4-4843-9e59-cf0253df142b.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/320f4dc8-05b4-4843-9e59-cf0253df142b.png)'
- en: In the preceding formulas*, b[x], b[y], b[w],* and *b[h]* are the *x*, *y* center
    coordinates, width and height, respectively, of our prediction. On the other hand,
    *t[x], t[y], t[w,]* and *t[h]* are what the network outputs. Furthermore, *c[x]*
    and *c[y]* are the top-left coordinates of the grid. Finally, *p[w]* and *p[h]*
    are anchor dimensions for the box.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，*b[x]*，*b[y]*，*b[w]* 和 *b[h]* 分别是我们预测的 *x*、*y* 中心坐标、宽度和高度。另一方面，*t[x]*，*t[y]*，*t[w]*
    和 *t[h]* 是网络输出的值。此外，*c[x]* 和 *c[y]* 是网格的左上坐标。最后，*p[w]* 和 *p[h]* 是框的锚点尺寸。
- en: The abjectness score is also passed through a sigmoid, as it is to be interpreted
    as a probability. Then, the class confidences are used that represent the probabilities
    of the detected object belonging to a particular class. After prediction, we see
    how much the predicted box intersects with the real bounding box labeled at the
    beginning. We try to maximize the intersection between them so ideally, the predicted
    bounding box is fully intersecting to the labelled bounding box.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 物体性得分也通过sigmoid函数进行处理，因为它需要被解释为概率。然后，使用类别置信度表示检测到的物体属于特定类别的概率。预测后，我们查看预测框与最初标记的真实边界框的交集程度。我们试图最大化它们之间的交集，因此理想情况下，预测的边界框与标记的边界框完全重叠。
- en: In short, we provide enough labeled data with bounding boxes (*b^x* , *b^y*,
    *b^h*, *b^w*), then we split the image and assign it to the box containing the
    center, train using CSW network and predict the object and its position. So first,
    we classify, and then localize the object and detect it.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们提供足够的带有边界框的标注数据（*b^x*，*b^y*，*b^h*，*b^w*），然后将图像分割并分配给包含中心的框，使用CSW网络进行训练并预测物体及其位置。所以，首先我们进行分类，然后本地化物体并检测它。
- en: Up to this point, we can see that we have been able to overcome most of the
    obstacles using YOLO. However, in reality, there are further two small problems
    to solve. Firstly, even though if the object is assigned to one box (one containing
    the center of the object) in the training time, during inferencing, the trained
    model assumes several boxes (that is, yellow) have the center of the object (with
    red). Therefore, this confusion introduces its own bounding boxes for the same
    object.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经能够克服大部分障碍，使用YOLO解决了问题。然而，实际上还有两个小问题需要解决。首先，尽管在训练时物体被分配到一个框中（即包含物体中心的框），但在推理时，训练好的模型会假设有多个框（即黄色框）包含物体的中心（即红色框）。因此，这种混淆为同一物体引入了额外的边界框。
- en: 'Fortunately, the non-max suppression algorithm can solve this: first, the algorithm
    chooses a prediction box with a maximum *P^c* probability so that it has a value
    between 0 and 1 instead of binary 0 or 1 values. Then, every intersecting box
    above a certain threshold with respect to that box is removed. The same logic
    is repeated until there are no more bounding boxes left. Secondly, since we are
    predicting multiple objects (car, train, bus, and so on), the center of two or
    more objects may lie in a single box. This issue can be solved by introducing
    anchor boxes:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，非最大抑制算法可以解决这个问题：首先，算法选择一个具有最大*P^c*概率的预测框，这样它的值介于0和1之间，而不是二进制的0或1值。然后，移除与该框交集超过某个阈值的每个框。重复相同的逻辑，直到没有更多的边界框剩余。其次，由于我们预测多个物体（如汽车、火车、公共汽车等），两个或更多物体的中心可能位于一个框内。这个问题可以通过引入锚框来解决：
- en: '![](img/15810d89-ef14-47f1-bdfa-ee44007e1f80.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15810d89-ef14-47f1-bdfa-ee44007e1f80.png)'
- en: An anchor box specification
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一个锚框规范
- en: With anchor boxes, we choose several shapes of bounding boxes we find frequently
    used for the object we want to detect. Then, the dimensions of the bounding box
    are predicted by applying a log-space transform to the output and then multiplying
    them by an anchor.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通过锚框，我们选择几种经常用于检测目标的边界框形状。然后，通过对输出应用对数空间变换，预测边界框的维度，再将其乘以锚框。
- en: Developing a real-time object detection project
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发一个实时物体检测项目
- en: In this section, we will be developing a video object classification application
    using pre-trained YOLO models (that is, transfer learning), DL4J, and OpenCV that
    can detect labels such as cars, and trees inside the video frame. To be frank,
    this application is also about extending an image detection problem to video detection.
    So let's get started.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将使用预训练的YOLO模型（即迁移学习）、DL4J和OpenCV开发一个视频物体分类应用程序，可以检测视频帧中的标签，如汽车和树木。坦率地说，这个应用程序实际上是将图像检测问题扩展到视频检测。所以让我们开始吧。
- en: Step 1 – Loading a pre-trained YOLO model
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤1 - 加载一个预训练的YOLO模型
- en: 'Since Alpha release 1.0.0, DL4J provides a Tiny YOLO model via ZOO. For this,
    we need to add a dependency to your Maven friendly `pom.xml` file:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 自Alpha版本1.0.0以来，DL4J通过ZOO提供了一个Tiny YOLO模型。为此，我们需要在Maven友好的`pom.xml`文件中添加一个依赖项：
- en: '[PRE1]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Apart from this, if possible, make sure you utilize the CUDA and cuDNN by adding
    the following dependencies (see [Chapter 2](e27fb252-7892-4659-81e2-2289de8ce570.xhtml),
    *Cancer Types Prediction Using Recurrent Type Networks*, for more details):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，如果可能的话，确保你通过添加以下依赖项来使用CUDA和cuDNN（更多细节请参见[第2章](e27fb252-7892-4659-81e2-2289de8ce570.xhtml)，*使用递归类型网络进行癌症类型预测*）：
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we are ready to load the pre-trained Tiny YOLO model as a `ComputationGraph`
    with the following lines of code:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们准备加载预训练的Tiny YOLO模型作为`ComputationGraph`，代码如下：
- en: '[PRE3]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the preceding code segment, the `createObjectLabels()` method is referring
    to the labels from the PASCAL Visual Object Classes (PASCAL VOC) dataset that
    was used to train the YOLO 2 model. The signature of the method can be seen as
    follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，`createObjectLabels()`方法指的是用于训练YOLO 2模型的PASCAL视觉物体类（PASCAL VOC）数据集中的标签。该方法的签名如下所示：
- en: '[PRE4]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, let''s create a Tiny YOLO model instance:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个Tiny YOLO模型实例：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, out of curiosity, let''s take a look at the model architecture and the
    number of hyperparameters in each layer:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，出于好奇，让我们看看模型架构以及每一层的超参数数量：
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](img/f8ad0f0e-daa2-4a32-8575-441734544417.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8ad0f0e-daa2-4a32-8575-441734544417.png)'
- en: Network summary and layer structure of a pre-trained Tiny YOLO model
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练Tiny YOLO模型的网络总结和层结构
- en: Therefore, our Tiny YOLO model has around 1.6 million parameters across its
    29-layer network. However, the original YOLO 2 model has more layers. Interested
    readers can look at the original YOLO 2 at [https://github.com/yhcc/yolo2/blob/master/model_data/model.png](https://github.com/yhcc/yolo2/blob/master/model_data/model.png).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的Tiny YOLO模型在其29层网络中大约有160万个参数。然而，原始YOLO 2模型的层数更多。有兴趣的读者可以查看原始YOLO 2，链接地址为[https://github.com/yhcc/yolo2/blob/master/model_data/model.png](https://github.com/yhcc/yolo2/blob/master/model_data/model.png)。
- en: Step 2 – Generating frames from video clips
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤2 - 从视频片段生成帧
- en: 'Now, to deal with real-time video, we can use video processing tools or frameworks
    such as JavaCV frameworks that can split a video into individual frames, and we
    take the image height and width. For this, we have to include the following dependency
    in the `pom.xml` file:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了处理实时视频，我们可以使用视频处理工具或框架，如JavaCV框架，它可以将视频拆分为单独的帧，并获取图像的高度和宽度。为此，我们需要在`pom.xml`文件中包含以下依赖项：
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: JavaCV uses wrappers from the JavaCPP presets of libraries commonly used by
    researchers in the field of computer vision (for example, OpenCV and FFmpeg),
    and provides utility classes to make their functionality easier to use on the
    Java platform, including Android. More details can be found at [https://github.com/bytedeco/javacv](https://github.com/bytedeco/javacv).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: JavaCV使用JavaCPP预设的库包装器，这些库通常被计算机视觉领域的研究人员使用（例如，OpenCV和FFmpeg），并提供了有用的工具类，使它们的功能在Java平台上（包括Android）更容易使用。更多细节请参见[https://github.com/bytedeco/javacv](https://github.com/bytedeco/javacv)。
- en: 'For this project, I have collected two video clips (each 1 minute long) that
    should give you a glimpse into an autonomous driving car. I have downloaded the
    dataset from YouTube from the following links:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我收集了两段视频片段（每段1分钟），它们应该能让你对自动驾驶汽车有一个初步了解。我从YouTube上下载了以下链接的数据集：
- en: '*Building Self Driving Car - Local Dataset - Day*: [https://www.youtube.com/watch?v=7BjNbkONCFw](https://www.youtube.com/watch?v=7BjNbkONCFw)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*构建自动驾驶汽车 - 本地数据集 - 白天*：[https://www.youtube.com/watch?v=7BjNbkONCFw](https://www.youtube.com/watch?v=7BjNbkONCFw)'
- en: '*Building Self Driving Car - Local Dataset - Night*: [https://www.youtube.com/watch?v=ev5nddpQQ9I](https://www.youtube.com/watch?v=ev5nddpQQ9I)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*构建自动驾驶汽车 - 本地数据集 - 夜间*：[https://www.youtube.com/watch?v=ev5nddpQQ9I](https://www.youtube.com/watch?v=ev5nddpQQ9I)'
- en: 'After downloading them from YouTube downloader (or so), I renamed them as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 从YouTube下载后（或其他方式），我将它们重命名如下：
- en: '`SelfDrivingCar_Night.mp4`'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SelfDrivingCar_Night.mp4`'
- en: '`SelfDrivingCar_Day.mp4`'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SelfDrivingCar_Day.mp4`'
- en: 'Now, if you play these clips, you will see how German people drive cars at
    160 km/h or even faster. Now, let us parse the video (first we use day 1) and
    see some properties to get an idea of video quality hardware requirements:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你播放这些视频片段，你会看到德国人开车时时速达到160 km/h甚至更快。现在，让我们解析视频（首先使用白天1）并查看一些属性，了解视频质量的硬件要求：
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We then grab each frame and use `Java2DFrameConverter`; it helps us to convert
    frames to JPEG images:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们抓取每一帧，并使用`Java2DFrameConverter`；它帮助我们将帧转换为JPEG图片：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this way, the preceding code will generate 1,802 JPEG images against an
    equal number of frames. Let''s take a look at the generated images:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，前面的代码将生成1,802张JPEG图片，与相同数量的帧一一对应。我们来看看生成的图片：
- en: '![](img/66694124-6340-48c9-9cc2-2ace25135ed5.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/66694124-6340-48c9-9cc2-2ace25135ed5.png)'
- en: From video clip to video frame to image
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 从视频片段到视频帧再到图像
- en: Thus, the 1-minute long video clip has a fair number (that is, 1,800) of frames
    and is 30 frames per second. In short, this video clip has 720p video quality.
    So you can understand that processing this video should require good hardware;
    in particular, having a GPU configured should help.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这段1分钟长的视频片段包含了相当数量的帧（即1,800帧），并且每秒30帧。简而言之，这段视频的分辨率为720p。所以，你可以理解，处理这个视频需要较好的硬件配置，特别是配置GPU会有帮助。
- en: Step 3 – Feeding generated frames into Tiny YOLO model
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三步 – 将生成的帧输入Tiny YOLO模型
- en: 'Now that we know some properties of the clip, we can start generating the frames
    to be passed to the Tiny YOLO pre-trained model. First, let''s look at a less
    important but transparent approach:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了片段的一些属性，可以开始生成帧并传递给Tiny YOLO预训练模型。首先，让我们看看一种不太重要但透明的方法：
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the preceding code block, we send each frame to the model. Then we use the
    `Mat` class to represent each frame in an n-dimensional, dense, numerical multi-channel
    (that is, RGB) array.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们将每一帧传送到模型。然后，我们使用`Mat`类以n维、密集、数值化的多通道（即RGB）数组表示每一帧。
- en: To know more, visit [https://docs.opencv.org/trunk/d3/d63/classcv_1_1Mat.html#details](https://docs.opencv.org/trunk/d3/d63/classcv_1_1Mat.html#details).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多信息，请访问[https://docs.opencv.org/trunk/d3/d63/classcv_1_1Mat.html#details](https://docs.opencv.org/trunk/d3/d63/classcv_1_1Mat.html#details)。
- en: In other words, we split the video clip into multiple frames and pass into the
    Tiny YOLO model to process them one by one. This way, we apply a single neural
    network to the full image.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们将视频片段分割成多个帧，并逐一传入Tiny YOLO模型进行处理。通过这种方式，我们对整张图像应用了一个神经网络。
- en: Step 4 – Object detection from image frames
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四步 – 从图像帧中进行物体检测
- en: 'Tiny YOLO extracts the features from each frame as an n-dimensional, dense,
    numerical multi-channel array. Then each image is split into a smaller number
    of rectangles (boxes):'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Tiny YOLO从每帧中提取特征，生成一个n维的、密集的、数值化的多通道数组。然后将每张图像分割成较少数量的矩形（边界框）：
- en: '[PRE11]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the preceding code, the `prepareImage()` method takes video frames as images,
    parses them using the `NativeImageLoader class`, does the necessary preprocessing,
    and extracts image features that are further converted into `INDArray` format,
    consumable by the model:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`prepareImage()`方法将视频帧作为图像传入，使用`NativeImageLoader`类进行解析，进行必要的预处理，并提取图像特征，进一步转换成`INDArray`格式，供模型使用：
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Then, the `markWithBoundingBox()` method is used for non-max suppression in
    the case of more than one bounding box.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`markWithBoundingBox()`方法将在有多个边界框的情况下用于非最大抑制。
- en: Step 5 – Non-max suppression in case of more than one bounding box
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五步 – 在有多个边界框的情况下进行非最大抑制
- en: 'As YOLO predicts more than one bounding box per object, non-max suppression
    is implemented; it merges all detections that belong to the same object. Therefore,
    instead of using *b^x*, *b^y*, *b^h*, and *b^w***,** we can use the top-left and
    bottom-right points. `gridWidth` and `gridHeight` are the number of small boxes
    we split our image into. In our case, this is 13 x 13, where `w` and `h` are the
    original image frame dimensions:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 由于YOLO每个物体可能预测多个边界框，因此需要实施非最大抑制；它将所有属于同一物体的检测结果合并。因此，我们不再使用*b^x*、*b^y*、*b^h*和*b^w*，**而是可以使用左上角和右下角的坐标。`gridWidth`和`gridHeight`是我们将图像分割成的小框数量。在我们的情况下，这个值是13
    x 13，其中`w`和`h`分别是原始图像帧的宽度和高度：
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we remove those objects that intersect with the max suppression, as
    follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们去除那些与最大抑制框重叠的物体，具体操作如下：
- en: '[PRE14]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the second block, we scaled each image into 416 x 416 x 3 (that is, W x
    H x 3 RGB channels). This scaled image is then passed to Tiny YOLO for predicting
    and marking the bounding boxes as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个代码块中，我们将每张图像缩放为416 x 416 x 3（即，W x H x 3 RGB通道）。然后，这张缩放后的图像被传递给Tiny YOLO进行预测，并标记边界框，操作如下：
- en: '![](img/7b10e461-0e7c-42ed-b47f-bde5329922f3.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b10e461-0e7c-42ed-b47f-bde5329922f3.png)'
- en: Our Tiny YOLO model predicts the class of an object detected in a bounding box
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Tiny YOLO模型预测图像中检测到的物体的类别
- en: 'Once the `markObjectWithBoundingBox()` method is executed, the following logs
    containing the predicted class, *b^x*, *b^y*, *b^h*, *b^w*, and confidence (that
    is, the detection threshold) will be generated and shown on the console:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦`markObjectWithBoundingBox()`方法执行完成，下面的日志将被生成并显示在控制台上，包括预测的类别、*b^x*、*b^y*、*b^h*、*b^w*以及置信度（即检测阈值）：
- en: '[PRE15]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Step 6 – wrapping up everything and running the application
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六步 – 整合所有步骤并运行应用程序
- en: 'Since, up to this point, we know the overall workflow of our approach, we can
    now wrap up everything and see whether it really works. However, before that,
    let''s take a look at the functionalities of different Java classes:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 既然到目前为止我们已经知道了我们方法的整体工作流程，现在可以将所有内容汇总，看看它是否真的有效。然而，在此之前，让我们先看一下不同Java类的功能：
- en: '`FramerGrabber_ExplorartoryAnalysis.java`: This shows how to grab frames from
    the video clip and save each frame as a JPEG image. Besides, it also shows some
    exploratory properties of the video clip.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FramerGrabber_ExplorartoryAnalysis.java`：此类展示了如何从视频片段中抓取帧并将每一帧保存为JPEG图像。此外，它还展示了一些视频片段的探索性属性。'
- en: '`TinyYoloModel.java`: This instantiates the Tiny YOLO model and generates the
    label. It also creates and marks the object with the bounding box. Nonetheless,
    it shows how to handle non-max suppression for more than one bounding box per
    object.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TinyYoloModel.java`：此类实例化Tiny YOLO模型并生成标签。它还创建并标记带有边界框的对象。然而，它展示了如何处理每个对象的多个边界框的非最大抑制。'
- en: '`ObjectDetectorFromVideo.java`: The main class. It continuously grabs the frames
    and feeds them to the Tiny YOLO model (that is, until the user presses the *Esc*
    key). Then it predicts the corresponding class of each object successfully detected
    inside the normal or overlapped bounding boxes with non-max suppression (if required).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ObjectDetectorFromVideo.java`：主类。它持续抓取帧并将其传递给Tiny YOLO模型（即，直到用户按下*Esc*键）。然后，它成功预测每个对象的相应类别，这些对象被检测到，并位于正常或重叠的边界框内，使用非最大抑制（如果需要）。'
- en: 'In short, first, we create and instantiate the Tiny YOLO model. Then we grab
    the frames and treat each frame as a separate JPEG image. Next, we pass all the
    images to the model and the model does its trick as outlined previously. The whole
    workflow can now be depicted with some Java code as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，首先，我们创建并实例化Tiny YOLO模型。然后我们抓取每一帧，并将每一帧视为单独的JPEG图像。接着，我们将所有图像传递给模型，模型根据之前概述的方式进行处理。整个工作流程现在可以通过以下Java代码进行描述：
- en: '[PRE16]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once the preceding class is executed, the application should load the pretrained
    model and the UI should be loaded, showing each object being classified:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦执行前述类，应用程序应加载预训练模型，并且用户界面应加载，显示每个被分类的对象：
- en: '![](img/2cdfb2ce-0c92-46aa-93cb-9c2fd1af9a3b.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cdfb2ce-0c92-46aa-93cb-9c2fd1af9a3b.png)'
- en: Our Tiny YOLO model can predict multiple cars simultaneously from a video clip
    (day)
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Tiny YOLO模型可以同时从视频片段中预测多辆汽车（白天）
- en: 'Now, to see the effectiveness of our model even in night mode, we can perform
    a second experiment on the night dataset. To do this, just change one line in
    the `main()` method, as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了查看我们的模型在夜间模式下的有效性，我们可以对夜间数据集进行第二次实验。为此，只需在`main()`方法中更改一行，如下所示：
- en: '[PRE17]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Once the preceding class is executed using this clip, the application should
    load the pretrained model and the UI should be loaded, showing each object being
    classified:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前述类时，应用程序应该加载预训练模型，并且用户界面应该加载，显示每个被分类的对象：
- en: '![](img/6f66961b-2b0a-4755-b510-9be340da10c5.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f66961b-2b0a-4755-b510-9be340da10c5.png)'
- en: Our Tiny YOLO model can predict multiple cars simultaneously from a video clip
    (night)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Tiny YOLO模型可以同时从视频片段中预测多辆汽车（夜间）
- en: Furthermore, to see the real-time output, execute the given screen recording
    clips showing the output of the application.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了查看实时输出，可以执行给定的屏幕录制片段，展示应用程序的输出。
- en: Frequently asked questions (FAQs)
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见问题解答（FAQs）
- en: In this section, we will see some frequently asked questions that might already
    be on your mind. Answers to these questions can be found in Appendix A.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到一些可能已经出现在你脑海中的常见问题。有关这些问题的答案，请参阅附录A。
- en: Can't we train YOLO from scratch?
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不能从头开始训练YOLO吗？
- en: I was wondering whether we could use the YOLO v3 model.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我在想是否可以使用YOLO v3模型。
- en: What changes to the code do I need to make it work for my own video clip?
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我需要对代码做哪些更改才能使其适用于我自己的视频片段？
- en: The application provided can detect cars and another vehicle in the video clip.
    However, the processing is not smooth. It seems to halt. How can I solve this
    issue?
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供的应用程序可以检测视频片段中的汽车和其他车辆。然而，处理并不流畅，似乎停滞不前。我该如何解决这个问题？
- en: Can I extend this app and make it work for a real-time video from a webcam?
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我可以扩展这个应用程序，使其能够处理来自摄像头的实时视频吗？
- en: Summary
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we saw how to develop an end-to-end project that will detect
    objects from video frames when video clips play continuously. We saw how to utilize
    the pre-trained Tiny YOLO model, which is a smaller variant of the original YOLO
    v2 model.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了如何开发一个端到端的项目，该项目可以在视频剪辑连续播放时从视频帧中检测对象。我们学习了如何利用预训练的Tiny YOLO模型，它是原始YOLO
    v2模型的一个更小的变种。
- en: Furthermore, we covered some typical challenges in object detection from both
    still images and videos, and how to solve them using bounding box and non-max
    suppression techniques. We learned how to process a video clip using the JavaCV
    library on top of DL4J. Finally, we saw some frequently asked questions that should
    be useful in implementing and extending this project.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还讨论了从静态图像和视频中进行目标检测时常见的一些挑战，并介绍了如何使用边界框和非最大抑制技术解决这些问题。我们学习了如何利用JavaCV库在DL4J之上处理视频剪辑。最后，我们还回顾了一些常见问题，这些问题对于实现和扩展这个项目非常有帮助。
- en: In the next chapter, we will see how to develop anomaly detection, which is
    useful in fraud analytics in finance companies such as banks, and insurance and
    credit unions. It is an important task to grow the business. We will use unsupervised
    learning algorithms such as variational autoencoders and reconstructing probability.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何开发异常检测系统，这在金融公司（如银行、保险公司和信用合作社）的欺诈分析中非常有用。这是一个推动业务增长的重要任务。我们将使用无监督学习算法，如变分自编码器和重构概率。
- en: Answers to questions
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题的答案
- en: '**Answer** **to question 1**: We can train a YOLO network from scratch, but
    that would take a lot of work (and costly GPU hours). As engineers and data scientists,
    we want to leverage as many prebuilt libraries and machine learning models as
    we can, so we are going to use a pre-trained YOLO model to get our application
    into production faster and more cheaply.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题1的回答**：我们可以从头开始训练一个YOLO网络，但那会需要大量的工作（以及昂贵的GPU计算时间）。作为工程师和数据科学家，我们希望尽可能利用现成的库和机器学习模型，因此我们将使用一个预训练的YOLO模型，以便更快、更便宜地将我们的应用程序投入生产。'
- en: '**Answer** **to question 2**: Perhaps yes, but the latest DL4J release provides
    only YOLO v2\. However, when I talked to their Gitter (see [https://deeplearning4j.org/](https://deeplearning4j.org/)),
    they informed me that with some additional effort, you can make it work. I mean
    you can import YOLO v3 with Keras import. Unfortunately, I tried but could not
    make it workfullly.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题2的回答**：也许可以，但最新的DL4J发布版本仅提供YOLO v2。然而，当我与他们的Gitter（见[https://deeplearning4j.org/](https://deeplearning4j.org/)）进行交流时，他们告诉我，通过一些额外的努力，你可以让它工作。我的意思是，你可以通过Keras导入导入YOLO
    v3。不幸的是，我尝试过，但没能完全实现。'
- en: '**Answer** **to question 3**: You should be able to directly feed your own
    video. However, if it does not work, or throws any unwanted exception, then video
    properties such as frame rate, width, and the height of each frame should be the
    same as the bounding box specifications.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题3的回答**：你应该能够直接输入你自己的视频。不过，如果无法正常工作，或者抛出任何不必要的异常，那么视频的属性，如帧率、宽度和每一帧的高度，应该与边界框的规格一致。'
- en: '**Answer** **to question 4**: Well, I''ve already stated that your machine
    should have good hardware specifications and processing should not cause any delays.
    For example, my machine has 32 GB of RAM, a core i7 processor, and GeForce GTX
    1050 GPU with 4 GB of main memory, and the apps run very smoothly.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题4的回答**：嗯，我已经说明过，你的机器应该有良好的硬件配置，处理过程不应该造成任何延迟。例如，我的机器有32GB的内存，Core i7处理器，GeForce
    GTX 1050 GPU，4GB的主内存，应用程序运行得非常流畅。'
- en: '**Answer** **to question 5:** Perhaps, yes. In that case, the main source of
    the video should be from the webcam directly. According to the documentation provided
    at [https://github.com/bytedeco/javacv](https://github.com/bytedeco/javacv), JavaCV
    also comes with a hardware-accelerated full-screen image display, easy-to-use
    methods to execute code in parallel on multiple cores, user-friendly geometric
    and color calibration of cameras, projectors, and so on.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题5的回答**：也许可以。在这种情况下，视频的主要来源应该是直接来自网络摄像头。根据[https://github.com/bytedeco/javacv](https://github.com/bytedeco/javacv)提供的文档，JavaCV还带有硬件加速的全屏图像显示、易于使用的方法来在多个核心上并行执行代码、以及对摄像头、投影仪等设备的用户友好的几何和颜色校准。'
