- en: A General Production Framework for Deep Learning-Enabled Websites
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习驱动网站的一般生产框架
- en: We have covered decent ground on using industry-grade cloud **Deep Learning**
    (**DL**) APIs in our applications in previous chapters and we have learned about
    their use through practical examples. In this chapter, we will cover a general
    outline for developing DL-enabled websites. This will require us to bring together
    all the things that we have learned so far so that we can put them to use in real-life
    use cases. In this chapter, we will learn how to structure a DL web application
    for production by first preparing the dataset. We will then train a DL model in
    Python and then wrap the DL models in APIs using Flask.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在前几章中覆盖了如何在我们的应用程序中使用行业级的云**深度学习**（**DL**）API，并通过实际示例学习了它们的使用。在这一章中，我们将涵盖开发深度学习驱动网站的总体框架。这将需要我们将迄今为止学到的所有内容结合起来，以便在实际案例中加以应用。在这一章中，我们将学习如何通过首先准备数据集来构建生产环境中的深度学习Web应用程序。接着我们将在
    Python 中训练一个深度学习模型，然后使用 Flask 将深度学习模型包装成 API。
- en: 'The following is a high-level summary of this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章的高层次总结：
- en: Defining our problem statement
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义我们的项目问题陈述
- en: Breaking the problem into several components
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将问题分解为多个组件
- en: Building a mental model to bind the project components
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个心理模型来绑定项目组件
- en: How we should be collecting the data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该如何收集数据
- en: Following a directory structure for our project
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遵循项目的目录结构
- en: Building the project from scratch
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零开始构建项目
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can access the code used in this chapter at [https://github.com/PacktPublishing/Hands-On-Python-Deep-Learning-for-Web/tree/master/Chapter9](https://github.com/PacktPublishing/Hands-On-Python-Deep-Learning-for-Web/tree/master/Chapter9).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [https://github.com/PacktPublishing/Hands-On-Python-Deep-Learning-for-Web/tree/master/Chapter9](https://github.com/PacktPublishing/Hands-On-Python-Deep-Learning-for-Web/tree/master/Chapter9)
    访问本章使用的代码。
- en: 'To run the code used in this chapter, you''ll need the following software:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本章使用的代码，你需要以下软件：
- en: Python 3.6+
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3.6+
- en: The Python PIL library
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python PIL 库
- en: NumPy
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: Pandas
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas
- en: The **Natural Language Toolkit** (**NLTK**)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言工具包**（**NLTK**）'
- en: 'Flask 1.1.0+ and compatible versions of the following:'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flask 1.1.0+ 及以下兼容版本：
- en: FlaskForm
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: FlaskForm
- en: wtforms
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: wtforms
- en: flask_restful
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: flask_restful
- en: flask_jsonpify
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: flask_jsonpify
- en: All other installations will be described during the course of this chapter.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所有其他安装将在本章中介绍。
- en: Defining the problem statement
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义问题陈述
- en: Any project should start with a well-defined problem statement or the project
    development is bound to suffer. The problem statement governs all the major steps
    involved in an overall project development pipeline, starting from project planning
    to project cost.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 任何项目都应该从明确的问题陈述开始，否则项目开发注定会受到影响。问题陈述主导着项目开发管道中的所有主要步骤，从项目规划到项目成本。
- en: 'In a DL-based web project, for example, the problem statement will direct us
    to the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于深度学习的Web项目中，问题陈述会引导我们如下：
- en: Determine what kind of data we would need.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定我们需要什么样的数据。
- en: How much complexity there would be in terms of code, planning, and other resources.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在代码、规划和其他资源方面会有多少复杂性。
- en: What kind of user interface we would develop.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将开发什么样的用户界面。
- en: How much human involvement there would be so that an estimate can be prepared
    on the project’s manpower and so on.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类的参与程度有多少，以便我们可以准备关于项目的人工资源等的估算。
- en: Hence, a well-defined problem statement is really required in order for us to
    get started with further project development.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了开始进一步的项目开发，确实需要一个明确的问题陈述。
- en: Imagine being a DL engineer at a company that is planning to build a recommendation
    system to suggest products from a product listing based on some user-provided
    criteria. Your boss has asked you to develop a **Proof of Concept** (**PoC**)
    based on this. So, how should we go about it? As mentioned previously, let’s start
    by defining the problem statement first.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你是一个深度学习工程师，所在的公司计划建立一个推荐系统，根据用户提供的某些标准从产品列表中推荐产品。你的老板要求你基于此开发一个**概念验证**（**PoC**）。那么，我们该如何开始呢？如前所述，我们先从定义问题陈述开始。
- en: 'The main entity that provides inputs to the final recommendation system is
    a user. Based on the user’s preferences (let’s call the input features preferences
    for now), the system would provide a list of products that best match their preference.
    So, long story short, the problem statement can be written as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 向最终推荐系统提供输入的主要实体是用户。基于用户的偏好（我们暂时称这些输入特征为偏好），系统将提供一份最匹配他们偏好的产品清单。所以，简而言之，问题陈述可以写成如下：
- en: '*Given a set of input features (user preferences), our task is to suggest a
    list of products.*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*给定一组输入特征（用户偏好），我们的任务是推荐一份产品清单。*'
- en: Now that we have a well-defined problem statement to proceed, let’s go ahead
    and build up the next steps in the following section.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经有了明确的问题陈述，可以继续前进，接下来我们将建立下一步的计划。
- en: Building a mental model of the project
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建项目的思维模型
- en: Looking at the problem statement, you might feel tempted to open a browser and
    start searching for some datasets. But when it comes to properly develop a project,
    definite planning is required to structure it piece by piece. A project without
    a structure is nothing more than a rudderless ship. So, we will be cautious about
    this from the start. We will discuss the modules that are going to play a very
    essential role in our project. This includes several mental considerations as
    well. I like to call this phase building a mental model of the project.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 看着问题陈述，你可能会忍不住打开浏览器，开始寻找一些数据集。但是，正确地开发一个项目需要明确的规划，将其一步步地结构化。一个没有结构的项目不过是无舵的船。因此，我们从一开始就要对此保持谨慎。我们将讨论在我们的项目中扮演至关重要角色的模块。这还包括一些思维上的考虑。我喜欢将这一阶段称为构建项目的思维模型。
- en: Let’s take some time to discuss the problem statement further, so as to figure
    out the essential modules we would need to develop.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花些时间进一步讨论问题陈述，以便弄清楚我们需要开发的核心模块。
- en: Our project concerns recommending products to users based on their preferences.
    So, in order to perform this recommendation, we would need a system that knows
    how to understand the set of preferences a user is providing to it. To be able
    to make sense of these preferences, the system would need some kind of training
    that we would be implementing DL. But what about preferences? How would they look
    like? You will often encounter these questions in real-world project situations
    that need humans in the loop.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的项目涉及根据用户的偏好向其推荐产品。因此，为了进行推荐，我们需要一个系统，能够理解用户提供的偏好集。为了理解这些偏好，系统需要进行某种类型的训练，我们将在其中实施深度学习（DL）。但是，偏好是什么？它们会是什么样的？你将在现实世界的项目中经常遇到这些问题，通常这些问题需要人类参与其中。
- en: 'Now, think for a second and try to think of the aspects you typically look
    for while choosing a product to buy. Let’s list them here:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，花点时间思考一下，在选择购买产品时你通常会关注哪些方面。让我们列出这些内容：
- en: What are the specifications of the product? If I want a large size T-shirt,
    I should not be recommended a small size T-shirt.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个产品的规格是什么？如果我想要一件大号T恤，就不应该推荐我一件小号T恤。
- en: What is the cost of the product? Users have a limited amount of money is this
    recommendation good for their wallet?
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个产品的价格是多少？用户的钱有限，这个推荐对他们的钱包是否合适？
- en: What brand is this product? Users often have brand preferences for similar products
    manufactured by several companies.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个产品是什么品牌？用户通常会有对几家制造商生产的类似产品的品牌偏好。
- en: Note that the preceding pointers are not in any particular order.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面的这些要点并没有特定的顺序。
- en: So, from the preceding section, we are starting to get a sense of what we would
    need, which is an interface (which will essentially be a web page, in our case)
    for a user to provide their preferences. Taking these preferences into account,
    our system would predict a set of products that it found to be the most appropriate
    ones. This is where the DL part comes into play. As we will recollect from earlier
    chapters, for a DL model to work on a given problem, it needs to be trained on
    some data that represents the problem as closely as possible. Let’s now discuss
    the data part of our system.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，从前一部分开始，我们已经开始有了一个大致的了解，我们需要一个界面（在我们的案例中，本质上是一个网页），让用户提供他们的偏好。根据这些偏好，我们的系统将预测一组它认为最合适的产品。这就是深度学习（DL）部分的作用所在。正如我们从前面的章节回顾到的那样，深度学习模型要在给定问题上发挥作用，需要通过一些能够尽可能贴近问题的数据进行训练。现在，让我们讨论一下系统的数据部分。
- en: 'We have a readily available dataset for our project—the Amazon Fine Food Reviews
    dataset provided by Amazon and created by the Stanford Network Analysis Project
    team. While the dataset is large in size, we will not be using the full dataset
    when creating the demonstration in this chapter. An immediate question that might
    get triggered here is how would the dataset look? We need to formulate a rough
    plan to decide the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个现成的项目数据集——由亚马逊提供、斯坦福网络分析项目团队创建的亚马逊美食评论数据集。虽然该数据集的大小很大，但在本章创建示例时，我们不会使用完整的数据集。此时可能会产生一个直接的问题，那就是数据集的样子如何？我们需要制定一个大致的计划，以决定以下内容：
- en: What features we would be choosing to construct the dataset
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将选择哪些特征来构建数据集
- en: Where we would be looking to collect the data
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将从哪里收集数据
- en: 'Let’s add a bit of enhancement to the original problem statement before proceeding
    further from here. Here’s the original problem statement:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续讨论之前，我们对原问题陈述做一点增强。以下是原问题陈述：
- en: '*Given a set of input features (user preferences), our task is to suggest a
    list of products.*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*给定一组输入特征（用户偏好），我们的任务是建议一份产品清单。*'
- en: 'Users will not like our system if it recommends them substandard products.
    So, we would modify the problem statement a bit, as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的系统推荐低质量产品，用户会不喜欢它。因此，我们将稍微修改问题陈述，如下所示：
- en: '*Given a set of input features (user preferences), our task is to suggest a
    list of the best possible products to buy.*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*给定一组输入特征（用户偏好），我们的任务是建议一份最佳购买产品的清单。*'
- en: 'For our system to recommend a list of the best possible products with respect
    to a given criterion, it first needs to know the average ratings of the products.
    Along with the average ratings, it would be useful to have the following information
    about a particular product, apart from its name:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的系统根据给定标准推荐最佳产品列表，首先需要知道产品的平均评分。除了产品名称之外，关于某个特定产品，以下信息也是有用的：
- en: Specifications
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规格
- en: Category of product
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品类别
- en: Seller name
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卖家名称
- en: Average price
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均价格
- en: Expected delivery time
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预期交货时间
- en: While preparing the data, we would look for the previous pointers about a particular
    product. Now comes the question of where we would be collecting the data from.
    The answer is Amazon! Amazon is known for its services in the e-commerce industry
    in providing us with various products and information about them, such as their
    ratings, product specifications, the price of the items, and so on. But say Amazon
    does not allow you to download this data directly as a zipped file. In order to
    get the data from Amazon in the desired form, we would have to resort to web scraping.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备数据时，我们会查找有关特定产品的前述指示。接下来是一个问题，我们将从哪里收集这些数据？答案是亚马逊！亚马逊以其在电子商务行业的服务而闻名，提供各种产品及其相关信息，如评分、产品规格、商品价格等。但是假设亚马逊不允许你直接下载这些数据作为压缩文件。为了以所需的形式从亚马逊获取数据，我们将不得不使用网页抓取技术。
- en: 'Up to this point in the discussion, we are certain on two broad areas of the
    project:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在讨论中，我们已经确定了项目的两个大致方向：
- en: An interface to receive preferences from the user
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个接收用户偏好的接口
- en: Data that would represent the problem statement we are dealing with
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代表我们所处理问题的相关数据
- en: For DL modeling, we will be starting with simple, fully-connected, neural network-based
    architecture. It’s often useful to start with a simple model and gradually increase
    the complexity because it makes the code base easier to debug.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度学习建模，我们将从简单的全连接神经网络架构开始。通常从一个简单的模型开始并逐步增加复杂度是很有用的，因为这样可以使代码库更容易调试。
- en: 'So, from this, it is safe enough to say that the following three modules are
    going to play an essential role in this project:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，从这一点来看，我们可以比较确定地说，以下三个模块将在本项目中发挥重要作用：
- en: An interface
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个接口
- en: Data
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据
- en: A DL model
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个深度学习模型
- en: Hopefully, you now have a fair idea about approaching the development of a project
    in the first place. What questions you should be asking at this stage and what
    considerations you may have to make can be worked out from the involved framework
    you now have.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 希望到目前为止，你已经对如何着手开发项目有了一个大致的了解。你此时应该提出哪些问题，以及你可能需要考虑的事项，可以从你现在掌握的框架中得出。
- en: We would not want our recommendation system to be biased toward anything. There
    can be many types of biases hidden in the data and naturally enough, it can cause
    the DL system that uses it to inherit that bias.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不希望我们的推荐系统对任何事物产生偏见。数据中可能隐藏着许多类型的偏见，而这些偏见很自然地会导致使用这些数据的深度学习系统继承这些偏见。
- en: To find out more about different types of biases in machine learning systems,
    you are encouraged to refer to this article at [https://developers.google.com/machine-learning/crash-course/fairness/types-of-bias](https://developers.google.com/machine-learning/crash-course/fairness/types-of-bias).
    In our case, a staggering example of bias would be a situation where a male visitor
    gets product recommendations that are averaged out. The recommendations might
    only come on the basis of his gender but not based on any other visitor-browsing
    pattern. This can be erroneous and may have been done mistakenly. But instances
    like this can make our model very inappropriate. In the next section, we will
    be discussing a few points to learn how can we avoid bias on the data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 若想了解更多关于机器学习系统中不同类型的偏见，建议参考这篇文章：[https://developers.google.com/machine-learning/crash-course/fairness/types-of-bias](https://developers.google.com/machine-learning/crash-course/fairness/types-of-bias)。在我们的例子中，一个惊人的偏见实例是，男性访客得到的产品推荐仅仅基于其性别，而不是根据任何其他访客浏览模式。这可能是错误的，也可能是无意中产生的，但类似情况会使我们的模型变得非常不合适。在接下来的部分，我们将讨论如何避免数据偏见的一些方法。
- en: Avoiding the chances of getting erroneous data in the first place
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 避免在一开始就获取错误数据的机会
- en: 'What is erroneous data? Are we only talking about data with wrong values? The
    answer is no. Besides data having wrong or missing values, erroneous data can
    have subtle but grave errors that may lead to poor training of the model or even
    bias. So, it is important to identify such erroneous data and remove it before
    training our model. There are five main ways of identifying these errors:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是错误数据？我们是不是只在谈论那些有错误值的数据？答案是否定的。除了数据有错误值或缺失值外，错误数据还可能包含一些微妙但严重的错误，这些错误可能导致模型训练效果差，甚至产生偏见。因此，识别这些错误数据并在训练模型之前将其去除是非常重要的。识别这些错误的主要方法有五种：
- en: Look for missing values.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找缺失值。
- en: Look for values that seem out of scale or possibility—in other words, outliers.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找那些看起来不合常理或不可能的值——换句话说，异常值。
- en: Do not include any features in the dataset that might cause data leakage.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要在数据集中包括可能导致数据泄露的任何特征。
- en: Ensure that all categories of evaluation have a similar number of samples in
    the dataset.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保数据集中所有类别的评估样本数量大致相同。
- en: Ensure that your design of the solution to the problem itself does not introduce
    a bias.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你解决问题的设计本身不会引入偏见。
- en: Once we are clear on these points, we are ready to move on to the more specific
    areas that we need to be careful about during the collection of data. It is important
    that during data collection a proper plan is prepared to keep in mind all the
    properties of the data source and the requirements of the problem statement.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们明确了这些要点，就可以进入数据收集过程中需要特别注意的更具体领域。在数据收集过程中，制定一个适当的计划，牢记数据源的所有特性和问题要求是非常重要的。
- en: Suppose you are scraping data for products from US-based outlets on Amazon and
    end up searching for products on the Indian version of Amazon instead. The scraper
    might give you data from India-based outlets, which may not be suitable for recommendation
    to US-based residents.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在爬取亚马逊美国站的产品数据时，错误地访问了亚马逊印度站。此时，爬虫可能会提供来自印度站的数据，这些数据可能不适合向美国居民推荐。
- en: Further, since Amazon—and similar services, such as Flipkart—takes the help
    of recommender systems to target the *most suitable* products for their customers,
    during data collection, the scraper should not become prey to such recommendations.
    It is important that the scraper clears its context every now and then and avoids
    getting biased results due to the AI put in place by Amazon.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于亚马逊及类似的服务（如Flipkart）借助推荐系统来为客户推荐*最合适*的产品，因此在数据收集过程中，爬虫不应受这些推荐的影响。爬虫应定期清除其上下文，以避免由于亚马逊实施的AI系统而产生偏见的结果。
- en: 'Let''s take an example from the Amazon Fine Food Reviews dataset. Though on
    the first look the dataset looks pretty balanced, we can uncover a lot of bias
    in the dataset. Consider the length of the text that the customers write for their
    reviews of products. Let''s plot them in a graph against the score they were rated.
    The following graphs show the plot for products rated 1 and 2 stars:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以亚马逊美食评论数据集为例。尽管从表面上看，这个数据集似乎非常平衡，但我们可以揭示出其中的很多偏差。考虑一下顾客为产品撰写评论时的文字长度。让我们将评论的长度与评分绘制成图表。以下图表展示了1星和2星评分的产品分布：
- en: '![](img/29b500df-2cd7-4404-b9da-a9cbc49b7fe0.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29b500df-2cd7-4404-b9da-a9cbc49b7fe0.png)'
- en: 'The following graphs show the plot for products rated 3 and 4 stars:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了3星和4星评分的产品分布：
- en: '![](img/8325444b-269a-4dca-9f6d-b1cb9cb77b8a.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8325444b-269a-4dca-9f6d-b1cb9cb77b8a.png)'
- en: 'The following graph shows the plot for products rated 5 stars:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了5星评分的产品分布：
- en: '![](img/4b4555fa-698d-4e04-bf56-b8a2ba2c4cb1.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b4555fa-698d-4e04-bf56-b8a2ba2c4cb1.png)'
- en: Notice how more positive reviews have more written text in them. This would
    directly convert into most of the words in the dataset, leading to a higher rating
    from the user. Now, consider a scenario where a user writes a lengthy review with
    a low rating and a generally negative opinion about the product. Since our model
    is trained to associate larger lengths of reviews to positive ratings, it would
    mark the negative review as positive.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到更多的正面评价中包含更多的文字。这会直接导致数据集中的大多数词汇，都与较高的评分相关联。现在，设想一个场景，其中用户写了一个冗长的负面评论，给出了较低的评分。由于我们的模型训练时将较长的评论与正面评分相关联，它可能会将这个负面评论误判为正面评价。
- en: The bottom line here is that real-world data can contain many edge cases, as
    shown, and if they are not handled in a proper manner, you will most likely get
    an erroneous model.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 关键问题在于，现实世界的数据可能包含许多边缘案例，如图所示，如果这些边缘情况没有得到妥善处理，模型很可能会出错。
- en: How not to build an AI backend
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何构建一个不理想的AI后端
- en: Considering the vastness that web applications can grow to and the strong dependence
    of nearly every other platform on a backend that runs as a web-based service,
    it is important for the backend to be well thought of and properly executed. AI-based
    applications, even in a PoC stage, are often not blazingly fast in responding
    or take a lot of time to train on the new samples.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到Web应用程序可能发展到的庞大规模，以及几乎所有其他平台都强烈依赖于作为Web服务运行的后端，因此，后端的设计和执行至关重要。即使是处于概念验证阶段的基于AI的应用程序，通常在响应时也不会非常快速，或者需要大量时间来训练新样本。
- en: While we will be discussing tips and tricks to make a backend that does not
    choke under pressure due to bottlenecks, we need to lay down a few pointers that
    need to be avoided in the best possible way when developing an AI-integrated backend
    for a website.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们将讨论一些技巧和窍门，以帮助构建一个不会因瓶颈而在压力下崩溃的后端，但在开发一个集成了AI的后端时，我们需要先确定一些在开发过程中应该尽量避免的问题。
- en: Expecting the AI part of the website to be real time
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 期望网站的AI部分实现实时响应
- en: AI is computationally expensive and needless to say, this is undesirable for
    a website that aims to serve its clients in the quickest time possible. While
    smaller models or using browser AI (such as TensorFlow.js or other libraries)
    can provide the experience of real-time AI responses, even they suffer issues
    where the client is in a slow network area or using a low-end device. So, both
    the methods of in-browser AI models or lightweight AI models replying near instantaneously
    are subject to device configuration and network bandwidth. Hence, the backend
    of the website, which is supposed to make quick responses to the client, should
    ideally be separated from the part that handles the AI model responses. Both,
    working in parallel, should maintain a common data storage and a proper method
    of interaction between the two, such that the backend code responsible for responding
    to the clients has less dependence on the AI model part.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: AI计算代价高昂，显而易见，这对于一个旨在尽快为客户提供服务的网站来说是不可取的。虽然使用更小的模型或浏览器内的AI（例如TensorFlow.js或其他库）可以提供实时AI响应的体验，但它们在客户端处于慢速网络环境或使用低端设备时，依然会遇到问题。因此，无论是浏览器内的AI模型还是快速响应的轻量级AI模型，都受设备配置和网络带宽的限制。因此，网站的后端应该理想上与处理AI模型响应的部分分开，这样它就能快速响应客户端。两者需要并行工作，共享一个数据存储，并保持合适的交互方式，使得负责响应客户端的后端代码不依赖于AI模型部分。
- en: Assuming the incoming data from a website is ideal
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 假设来自网站的数据是理想的
- en: Even though the website or app corresponding to the project might resemble an
    ideal method of data collection, the data coming from it must not be assumed to
    be free of errors. Bad network requests, malicious connections, or simply garbage
    input provided by users can lead to data that is unfit for training. A non-malicious
    user may have network issues and refresh the same page 10 to 20 times in a short
    time frame, which should not add to the viewing-based importance of that page.
    All data collected from the website must be subject to cleanup and filtering based
    on the requirements of the model. It must be kept in mind that the challenges
    faced by websites will almost certainly affect the quality of data collected.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管与项目对应的网站或应用可能看起来是理想的数据收集方法，但来自它的数据不能假设没有错误。网络请求失败、恶意连接，或者用户提供的垃圾输入都可能导致数据不适合训练。一个非恶意用户可能会因为网络问题而在短时间内刷新同一页面10到20次，这不应该增加该页面的查看重要性。所有从网站收集的数据必须根据模型的要求进行清理和过滤。必须牢记，网站面临的挑战几乎肯定会影响收集数据的质量。
- en: A sample end-to-end AI-integrated web application
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个示例的端到端 AI 集成网络应用
- en: Now that we have discussed an overview and the pitfalls to avoid when creating
    an AI-powered website backend, let's move on to creating one—albeit a rather simple
    one—that demonstrates the general overview of the solution.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了创建 AI 驱动网站后台时的概览和避免的陷阱，让我们开始构建一个——尽管是一个相对简单的——展示解决方案整体概览的系统。
- en: 'We will cover the following steps, as stated previously:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下步骤，如前所述：
- en: The collection of data as per the problem statement
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据问题陈述收集数据
- en: Cleaning and preprocessing the data
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据清理和预处理
- en: Building the AI model
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建 AI 模型
- en: Creating an interface
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建界面
- en: Using the AI model on the interface
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在界面上使用 AI 模型
- en: While we have previously discussed the pitfalls of collecting the data, we will
    briefly discuss here the tools and methods that can be employed to complete the
    task.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们之前已经讨论了收集数据时的陷阱，但我们在这里简要讨论可以用来完成任务的工具和方法。
- en: Data collection and cleanup
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据收集与清理
- en: 'For the purpose of collecting data, from a general perspective, there could
    be several data sources. You could scrape data off websites or simply download
    some prepared dataset. Other methods could also be employed, such as the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 从一般角度来看，收集数据的目的可以有多个数据源。您可以从网站抓取数据，或简单地下载一些准备好的数据集。还可以采用其他方法，例如以下几种：
- en: Generating data on the fly within the runtime of applications/websites
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在应用程序/网站的运行时生成数据
- en: Logging from applications or smart devices
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从应用程序或智能设备进行日志记录
- en: Collecting data directly from users via systematic forms (such as quizzes or
    surveys)
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过系统化的表单（例如测验或调查）直接收集用户数据
- en: Collecting data from survey agencies
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从调查机构收集数据
- en: Observational data measured by specific methods (scientific data) and other
    ways
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过特定方法（科学数据）和其他方式测量的观测数据
- en: '`beautifulsoup` is a library commonly used to perform web scraping. `Scrapy`
    is yet another popular tool and can be used very rapidly.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`beautifulsoup` 是一个常用的网页抓取库。`Scrapy` 是另一个流行的工具，并且可以非常迅速地使用。'
- en: The data cleaning would entirely depend on the form of data collected by you
    and has been discussed in previous chapters of the book. We will assume that you
    are able to convert your data into a format that is suitable for how you wish
    to proceed with the model-building part. For the further topics in this section,
    we will use a prepared dataset titled Amazon Fine Food Reviews, which can be downloaded
    from [https://www.kaggle.com/snap/amazon-fine-food-reviews.](https://www.kaggle.com/snap/amazon-fine-food-reviews)
    Once you extract the downloaded ZIP file, you'll get the dataset as a file called
    `Reviews.csv`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理完全取决于您收集的数据形式，这在本书的前几章中已有讨论。我们将假设您能够将数据转换为适合您想要继续进行模型构建部分的格式。在本节的后续主题中，我们将使用一个名为
    Amazon Fine Food Reviews 的准备好数据集，您可以从 [https://www.kaggle.com/snap/amazon-fine-food-reviews.](https://www.kaggle.com/snap/amazon-fine-food-reviews)
    下载该数据集。解压下载的 ZIP 文件后，您会得到一个名为 `Reviews.csv` 的数据文件。
- en: A good starting point to observe how to perform web scraping and prepare a clean
    dataset is [https://github.com/Nilabhra/kolkata_nlp_workshop_2019](https://github.com/Nilabhra/kolkata_nlp_workshop_2019).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 观察如何进行网页抓取并准备清理数据集的一个良好起点是 [https://github.com/Nilabhra/kolkata_nlp_workshop_2019](https://github.com/Nilabhra/kolkata_nlp_workshop_2019)。
- en: Building the AI model
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 AI 模型
- en: Now, we will prepare the AI model, which will recommend products based on the
    user's query. To do so, let's create a new Jupyter notebook.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将准备AI模型，该模型将根据用户的查询推荐产品。为此，让我们创建一个新的Jupyter Notebook。
- en: Making the necessary imports
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行必要的导入
- en: 'We begin by importing the required Python modules to the project:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入所需的Python模块到项目中：
- en: '[PRE0]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We import `TfidfVectorizer` to help us create **Term Frequency-Inverse Document
    Frequency** (**TF-IDF**) vectors for performing natural language processing. TF-IDF
    is a numerical measure of how important a word in a single document is, given
    a number of documents that may or may not contain the words. Numerically, it increases
    the importance value when a single word occurs frequently in a single document
    but not in other documents. TF-IDF is so popular that over 80% of the world's
    natural language-based recommender systems currently use it.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入`TfidfVectorizer`，帮助我们创建**词频-逆文档频率**（**TF-IDF**）向量，以进行自然语言处理。TF-IDF是衡量一个词在单一文档中重要性的数值指标，考虑到一组可能包含或不包含该词的文档。数值上，它增加了当一个词在单一文档中频繁出现而在其他文档中不常出现时的权重。TF-IDF非常流行，目前全球超过80%的基于自然语言的推荐系统都在使用它。
- en: We are also importing `WordPunctTokenizer`. A tokenizer performs the function
    of breaking down a text into elemental tokens. For example, a large paragraph
    may be broken down into sentences and further into words.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还导入了`WordPunctTokenizer`。分词器的作用是将文本拆分为基本的词元。例如，一个大段落可以被拆分为句子，进一步拆分为单词。
- en: Reading the dataset and preparing cleaning functions
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取数据集并准备清理函数
- en: 'We will read the Amazon Fine Food Reviews dataset with the `ISO-8859-1` encoding.
    This is only to ensure that we do not lose out on any special symbols used in
    the text of the review:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`ISO-8859-1`编码读取Amazon Fine Food Reviews数据集。这样做是为了确保我们不会丢失评论文本中使用的任何特殊符号：
- en: '[PRE1]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Since the dataset is very large, we've restricted our work in this chapter to
    the first 10,000 rows in the dataset.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集非常庞大，我们在本章中将工作范围限制为数据集中的前10,000行。
- en: 'We would need to remove stop words from the text and filter out symbols such
    as brackets and other symbols not natural to written text. We will create a function
    named `cleanText()`, which will perform the filtering and removal of stop words:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要从文本中去除停用词，并过滤掉如括号等不属于自然书面文本的符号。我们将创建一个名为`cleanText()`的函数，执行过滤和去除停用词的操作：
- en: '[PRE2]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Using the preceding function, we have removed the stop words and any words shorter
    than three characters from the text. We have filtered out punctuation and are
    only keeping the relevant characters from the text.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的函数，我们已经从文本中去除了停用词和任何少于三个字符的词。我们已经过滤掉了标点符号，并且只保留了文本中相关的字符。
- en: Slicing out the required data
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 切片出所需的数据
- en: 'The dataset contains more data than is useful to us for the demo at hand. We
    will extract the `ProductId`, `UserId`, `Score`, and `Text` columns to prepare
    our demo. The names of the products are encrypted for privacy reasons, just as
    the names of the users are encrypted:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含的数据比我们当前演示所需的更多。我们将提取`ProductId`、`UserId`、`Score`和`Text`列，以准备我们的演示。产品名称出于隐私原因进行了加密，就像用户名称也被加密一样：
- en: '[PRE3]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Keeping data encrypted and free of personal information is a challenge in data
    science. It is important to remove parts from the dataset that would make it possible
    to identify the private entities that are a part of the dataset. For example,
    you would need to remove people and organization names from the text of the review
    to stop the products and users from being identified, despite them having encrypted
    product and user IDs.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学中，保持数据加密并且不包含个人信息是一个挑战。必须从数据集中删除可能使其识别出私密实体的部分。例如，您需要从评论文本中删除人名和组织名，以防止即使已经加密了产品和用户ID，产品和用户仍然能被识别出来。
- en: Applying text cleaning
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用文本清理
- en: 'We will now apply the text filtering and stop word removal function to clean
    the text in the dataset:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将应用文本过滤和停用词移除函数来清理数据集中的文本：
- en: '[PRE4]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The time taken for the task is displayed.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 显示任务所需的时间。
- en: Note that the preceding code block will only work in Jupyter Notebook and not
    in normal Python scripts. To run it on normal Python scripts, remove the `%%time`
    command.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面的代码块仅适用于Jupyter Notebook，而不能在普通的Python脚本中运行。要在普通Python脚本中运行，请移除`%%time`命令。
- en: Splitting the dataset into train and test parts
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据集拆分为训练集和测试集
- en: 'Since we have a single dataset, we will break it into two parts, with the feature
    and label parts separated:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有一个单一的数据集，我们将其分成两部分，特征和标签部分分开：
- en: '[PRE5]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We will use the `train_test_split()` method from the `sklearn` module to split
    the dataset into 80% for training and 20% for testing.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `sklearn` 模块中的 `train_test_split()` 方法，将数据集分为 80% 用于训练，20% 用于测试。
- en: Aggregating text about products and users
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合关于产品和用户的文本
- en: 'We will now aggregate the dataset''s reviews by users and product IDs. We''ll
    need the reviews for each product to determine what that product would be a good
    choice for:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将按用户和产品 ID 聚合数据集中的评论。我们需要每个产品的评论，以确定该产品适合什么样的用户：
- en: '[PRE6]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Similarly, reviews aggregated by users will help us determine what a user likes.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，通过用户聚合的评论将帮助我们确定用户喜欢什么。
- en: Creating TF-IDF vectorizers of users and products
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建用户和产品的 TF-IDF 向量化器
- en: 'We will now create two different vectorizers one is for users and the other
    for products. We will need these vectorizers in place to determine the similarity
    between the requirements of the users and what the reviews tell us about any given
    product. First, we will create the vectorizer for users and display its shape:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将创建两个不同的向量化器，一个是用于用户，另一个是用于产品。我们需要这些向量化器来确定用户需求与产品评论之间的相似性。首先，我们将为用户创建向量化器并显示其形状：
- en: '[PRE7]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we will create the vectorizer for products:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将为产品创建向量化器：
- en: '[PRE8]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We use `WordPunctTokenizer` to break down the text and use the `fit_transform`
    method of the `TfidfVectorizer` object to prepare the vectors, which map the word
    dictionary to their importance in documents.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `WordPunctTokenizer` 来分解文本，并使用 `TfidfVectorizer` 对象的 `fit_transform` 方法来准备向量，这些向量将单词字典映射到它们在文档中的重要性。
- en: Creating an index of users and products by the ratings provided
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按评分创建用户和产品的索引
- en: 'We use the `pivot_table` method of the `pandas` module to create a matrix of
    user ratings against products. We will use this matrix to perform matrix factorization
    to determine the products that a user likes:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `pandas` 模块的 `pivot_table` 方法，创建用户对产品的评分矩阵。我们将使用这个矩阵进行矩阵分解，以确定用户喜欢的产品：
- en: '[PRE9]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We will also convert the `TfidfVectorizer` vectors for users and products into
    matrices suitable for matrix factorization:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将把用户和产品的 `TfidfVectorizer` 向量转换成适用于矩阵分解的矩阵：
- en: '[PRE10]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can now create the matrix factorization function.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以创建矩阵分解函数。
- en: Creating the matrix factorization function
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建矩阵分解函数
- en: 'We will now create a function to perform matrix factorization. Matrix factorization
    became a popular family of algorithms used for recommender systems during the
    Netflix Prize challenge in 2006\. It is a family of algorithms that decomposes
    a user-item matrix into a set of two lower-dimension rectangular matrices that
    can be multiplied to restore the original higher-order matrix:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将创建一个函数来执行矩阵分解。矩阵分解在 2006 年的 Netflix 奖挑战赛中成为推荐系统算法的热门方法。它是一类算法，将用户-项目矩阵分解成两个较低维度的矩阵，这两个矩阵可以相乘以恢复原始的高阶矩阵：
- en: '[PRE11]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We then perform the matrix factorization and log the time taken:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们执行矩阵分解并记录所花费的时间：
- en: '[PRE12]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: After this, we need to save the model.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们需要保存模型。
- en: Saving the model as pickle
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将模型保存为 pickle 文件
- en: 'Now, create a folder called `api` in the `root` directory of your project.
    Then, save the trained model, which is the lower-order matrices obtained after
    factorization of the user-products rating matrix:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在项目的 `root` 目录下创建一个名为 `api` 的文件夹。然后，保存经过训练的模型，即在用户-产品评分矩阵分解后得到的低阶矩阵：
- en: '[PRE13]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Saving the models as binary pickle files allows us to quickly load them back
    into the memory during deployment of the model on the backend of the website.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型保存为二进制的 pickle 文件，使我们可以在模型部署到网站后端时快速将其加载到内存中。
- en: Now that we are done developing the predictive model, we will move on to building
    an interface for the application to work on.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了预测模型的开发，我们将继续构建一个应用程序界面来使其能够运行。
- en: Building an interface
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建接口
- en: To build an interface for the web application, we need to think about how we
    would want our users to interact with the system. In our case, we are expecting
    the user to be presented with suggestions based on what they search for in a search
    bar the moment they submit the search query. This means we need the system to
    respond in real time and generate suggestions on the fly. To build this system,
    we will create an API that will respond to the search query.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建Web应用的接口，我们需要考虑如何让用户与系统交互。在我们的案例中，我们希望用户提交搜索查询时，系统会根据他们搜索的内容在搜索框中即时展示建议。这意味着我们需要系统能够实时响应并即时生成建议。为了构建该系统，我们将创建一个API来响应搜索查询。
- en: Creating an API to answer search queries
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个API来回答搜索查询
- en: 'We will create an API that accepts queries in the form of HTTP requests and
    replies with suggestions of products based on the search query entered by the
    user. To do so, follow these steps:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个接受HTTP请求形式查询的API，并根据用户输入的搜索查询回复产品建议。为此，请遵循以下步骤：
- en: 'We will begin by importing the required modules for the API. We discussed these
    imported modules in the previous section:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从导入API所需的模块开始。我们在前一节中讨论了这些导入的模块：
- en: '[PRE14]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We will also import the `Flask` module to create a quick HTTP server that can
    serve on a defined route in the form of an API. We will instantiate the `Flask`
    app object as shown:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将导入`Flask`模块，创建一个快速的HTTP服务器，可以在定义的路由上以API的形式提供服务。我们将按如下所示实例化`Flask`应用对象：
- en: '[PRE15]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The value of `SECRET_KEY` in the app configuration is up to you.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 应用配置中的`SECRET_KEY`值由你自行决定。
- en: 'We will then create a `class` function to handle the text input that we receive
    in the form of a search query from the user:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将创建一个`class`函数来处理我们收到的以搜索查询形式的文本输入：
- en: '[PRE16]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To encapsulate the API methods, we wrap them in a `Flask_Work` class:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了封装API方法，我们将它们包装在`Flask_Work`类中：
- en: '[PRE17]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `cleanText()` method we used during model creation is again required. It
    will be used to clean and filter out the search query entered by the user:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在模型创建过程中使用的`cleanText()`方法再次被需要。它将用于清理并过滤用户输入的搜索查询：
- en: '[PRE18]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We define a home page for the application, which will be loaded from the `index.html`
    file that we create in the templates later:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为应用程序定义了一个主页，该主页将从稍后在模板中创建的`index.html`文件加载：
- en: '[PRE19]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We create the `post` method-based prediction route, which will respond with
    the product suggestions upon receiving the user''s search query:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了基于`post`方法的预测路由，在接收到用户的搜索查询后，将返回产品建议：
- en: '[PRE20]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We attach the `Flask_Work` class to the `Flask` server. This completes the
    script on running. We have put an API in place that suggests products based on
    the user''s search query:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`Flask_Work`类附加到`Flask`服务器。这完成了运行时的脚本。我们已经设置了一个API，根据用户的搜索查询建议产品：
- en: '[PRE21]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Save this file as `main.py`. With the API script created, we need to host the
    server.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 将此文件保存为`main.py`。创建好API脚本后，我们需要托管服务器。
- en: 'To do so on a local machine, run the following command in the terminal:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地计算机上执行此操作，请在终端中运行以下命令：
- en: '[PRE22]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This will start the server on the computer on port `4000`, as shown:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在计算机上启动服务器，并绑定到端口`4000`，如所示：
- en: '![](img/5f7e2b07-8b1d-4ba9-a2ac-c4002fc9a22a.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f7e2b07-8b1d-4ba9-a2ac-c4002fc9a22a.png)'
- en: However, we still need to prepare a user interface to use this API. We will
    do so in the following section.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们仍然需要准备一个用户界面来使用这个API。我们将在接下来的部分进行。
- en: Creating an interface to use the API
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建用于使用API的接口
- en: We will now create a simple, minimal UI to use the API we created. In essence,
    we will create a single search bar where the user enters the product or product
    specification that they want and the API returns recommendations based on the
    user's query. We will not be discussing the code for building the UI, but we have
    included it in the GitHub repository, which can be found at [http://tiny.cc/DL4WebCh9](http://tiny.cc/DL4WebCh9).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个简单、简约的UI来使用我们创建的API。实际上，我们将创建一个单一的搜索框，用户在其中输入他们想要的产品或产品规格，API则根据用户的查询返回推荐。我们不会讨论构建UI的代码，但我们已经将其包含在GitHub存储库中，可以通过[http://tiny.cc/DL4WebCh9](http://tiny.cc/DL4WebCh9)找到。
- en: This UI will be visible at `http://127.0.0.1:4000` once you start the server,
    as shown in step 9 of the *Creating an API to answer search queries* section.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启动服务器，该UI将在`http://127.0.0.1:4000`上可见，如*创建API以回答搜索查询*部分的第9步所示。
- en: 'The interface we created looks like this:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的界面如下所示：
- en: '![](img/65ce8f08-b037-4d00-a8b5-a94172b2d2bd.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65ce8f08-b037-4d00-a8b5-a94172b2d2bd.png)'
- en: 'The user enters a search query and gets recommendations, as shown:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 用户输入搜索查询并获得推荐，如下所示：
- en: '![](img/e161cee5-2998-42d1-a4ef-4ed108ad54ad.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e161cee5-2998-42d1-a4ef-4ed108ad54ad.png)'
- en: Our application does not have the benefit of saving user sessions. Also, it
    does not have parameters for the expected budget of the user, which is often a
    deciding factor in whether the product is a good fit for the user. It is easy
    to add these features to web applications and leverage their benefits.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用程序没有保存用户会话的好处。同样，它没有用户预期预算的参数，这通常是决定产品是否适合用户的重要因素。将这些功能添加到Web应用程序并利用它们的好处是很容易的。
- en: Summary
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: As a general overview, web applications that hone the power of DL have a few
    set methods to do so via APIs, in-browser JavaScript, or by silently embedding
    DL models in the backend of the application. In this chapter, we saw how to use
    the most common of these methods—an API-based DL web application—while at the
    same time, we saw a rough overview of how to design similar solutions. We covered
    the thought process that goes into the identification of the problem statement
    and a subsequent solution, along with the pitfalls and pain points to avoid during
    the design of a web application that integrates DL models.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一般概述，利用DL强大功能的Web应用程序有几种通过API、浏览器内JavaScript或通过在应用程序后端悄悄嵌入DL模型的设置方法。在本章中，我们看到如何使用其中最常见的方法——基于API的DL
    Web应用程序——同时，我们还大致概述了如何设计类似解决方案。我们涵盖了问题陈述的思考过程以及随后的解决方案，以及在设计集成DL模型的Web应用程序时要避免的缺陷和痛点。
- en: In the next chapter, we will cover an end to end project that integrates DL
    on web applications for security purposes. We will see how DL can help us recognize
    suspicious activity and block spam users.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将涵盖一个端到端项目，该项目将DL集成到Web应用程序中，以用于安全目的。我们将看到DL如何帮助我们识别可疑活动并阻止垃圾用户。
