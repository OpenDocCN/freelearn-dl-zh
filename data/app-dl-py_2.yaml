- en: Data Cleaning and Advanced Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据清洗与高级机器学习
- en: The goal of data analytics in general is to uncover actionable insights that
    result in positive business outcomes. In the case of predictive analytics, the
    aim is to do this by determining the most likely future outcome of a target, based
    on previous trends and patterns.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析的总体目标是发现可操作的洞察，从而带来积极的商业成果。在预测分析中，目标是通过基于过去的趋势和模式，确定目标最可能的未来结果。
- en: The benefits of predictive analytics are not restricted to big technology companies.
    Any business can find ways to benefit from machine learning, given the right data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 预测分析的好处不仅仅局限于大科技公司。任何企业，只要拥有合适的数据，都可以找到从机器学习中获益的方式。
- en: Companies all around the world are collecting massive amounts of data and using
    predictive analytics to cut costs and increase profits. Some of the most prevalent
    examples of this are from the technology giants Google, Facebook, and Amazon,
    who utilize big data on a huge scale. For example, Google and Facebook serve you
    personalized ads based on predictive algorithms that guess what you are most likely
    to click on. Similarly, Amazon recommends personalized products that you are most
    likely to buy, given your previous purchases.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 世界各地的公司正在收集大量数据，并利用预测分析来降低成本并增加利润。最常见的例子之一来自科技巨头谷歌、脸书和亚马逊，他们在大规模运用大数据。例如，谷歌和脸书根据预测算法为你提供个性化广告，这些算法预测你最有可能点击什么内容。类似地，亚马逊根据你的历史购买记录，推荐你最有可能购买的个性化产品。
- en: Modern predictive analytics is done with machine learning, where computer models
    are trained to learn patterns from data. As we saw briefly in the previous chapter,
    software such as scikit-learn can be used with Jupyter Notebooks to efficiently
    build and test machine learning models. As we will continue to see, Jupyter Notebooks
    are an ideal environment for doing this type of work, as we can perform adhoc
    testing and analysis, and easily save the results for reference later.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 现代预测分析依赖于机器学习，在机器学习中，计算机模型被训练来从数据中学习模式。正如我们在上一章中简要看到的，软件如 scikit-learn 可以与 Jupyter
    Notebooks 配合使用，来高效地构建和测试机器学习模型。正如我们将继续看到的，Jupyter Notebooks 是进行这类工作的理想环境，因为我们可以进行临时测试和分析，并且轻松保存结果以便后续参考。
- en: In this chapter, we will again take a hands-on approach by running through various examples
    and activities in a Jupyter Notebook. Where we saw a couple of examples of machine
    learning in the previous chapter, here we'll take a much slower and more thoughtful
    approach. Using an employee retention problem as our overarching example for the
    chapter, we will discuss how to approach predictive analytics, what things to
    consider when preparing the data for modeling, and how to implement and compare
    a variety of models using Jupyter Notebooks.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过在 Jupyter Notebook 中运行各种示例和活动来再次采用实践方法。上一章我们看到了一些机器学习的例子，而在本章中，我们将采取更加缓慢且深思熟虑的方法。以员工留存问题作为本章的核心示例，我们将讨论如何处理预测分析，在为建模准备数据时需要考虑的因素，以及如何使用
    Jupyter Notebooks 实施并比较各种模型。
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Plan a machine learning classification strategy
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制定机器学习分类策略
- en: Preprocess data to prepare it for machine learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理数据以准备进行机器学习
- en: Train classification models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练分类模型
- en: Use validation curves to tune model parameters
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用验证曲线来调整模型参数
- en: Use dimensionality reduction to enhance model performance
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用降维方法来提升模型性能
- en: Preparing to Train a Predictive Model
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备训练预测模型
- en: Here, we will cover the preparation required to train a predictive model. Although
    not as technically glamorous as training the models themselves, this step should
    not be taken lightly. It's very important to ensure you have a good plan before
    proceeding with the details of building and training a reliable model. Furthermore,
    once you've decided on the right plan, there are technical steps in preparing
    the data for modeling that should not be overlooked.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将介绍训练预测模型所需的准备工作。尽管这一步骤在技术上可能没有训练模型本身那么引人注目，但不应掉以轻心。确保你在开始构建和训练可靠模型的细节之前有一个良好的计划非常重要。此外，一旦你决定了合适的计划，在准备数据进行建模时，一些技术步骤也不容忽视。
- en: We must be careful not to go so deep into the weeds of technical tasks that
    we lose sight of the goal.Technical tasks include things that require programming
    skills, for example, constructing visualizations, querying databases, and validating predictive
    models. It's easy to spend hours trying to implement a specific feature or get
    the plots looking just right. Doing this sort of thing is certainly beneficial
    to our programming skills, but we should not forget to ask ourselves if it's really
    worth our time with respect to the current project.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须小心不要在技术任务的细节中迷失方向，从而失去目标。技术任务包括那些需要编程技能的工作，例如构建可视化、查询数据库和验证预测模型。很容易花费数小时试图实现某个特定功能，或将图表调整得完美无缺。虽然这样做无疑有益于提高我们的编程技能，但我们也不应忘记问问自己，这些工作是否真的值得投入时间，尤其是在当前项目的背景下。
- en: Also, keep in mind that Jupyter Notebooks are particularly well-suited for this
    step, as we can use them to document our plan, for example, by writing rough notes
    about the data or a list of models we are interested in training. Before starting
    to train models, it's good practice to even take this a step further and write
    out a well-structured plan to follow. Not only will this help you stay on track
    as you build and test the models, but it will allow others to understand what
    you're doing when they see your work.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请记住，Jupyter Notebooks 特别适合这个步骤，因为我们可以用它们来记录我们的计划，例如，写下关于数据的粗略笔记或我们有兴趣训练的模型列表。在开始训练模型之前，最好更进一步，写出一个结构清晰的计划。这样不仅能帮助你在构建和测试模型时保持进度，还能让其他人在看到你的工作时理解你在做什么。
- en: After discussing the preparation, we will also cover another step in preparing
    to train the predictive model, which is cleaning the dataset. This is another
    thing that Jupyter Notebooks are well-suited for, as they offer an ideal testing
    ground for performing dataset transformations and keeping track of the exact changes.
    The data transformations required for cleaning raw data can quickly become intricate
    and convoluted; therefore, it's important to keep track of your work. As discussed
    in the fist chapter, tools other than Jupyter Notebooks just don't offer very
    good options for doing this efficiently.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论准备工作后，我们还将介绍准备训练预测模型的另一个步骤，即清理数据集。这是Jupyter Notebooks特别适合的任务，因为它们为执行数据集转换并跟踪准确的更改提供了理想的测试平台。清理原始数据所需的数据转换可能变得复杂和繁琐，因此跟踪你的工作非常重要。如第一章所述，其他工具无法提供像Jupyter
    Notebooks这样高效的选项。
- en: Determining a Plan for Predictive Analytics
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定预测分析计划
- en: When formulating a plan for doing predictive modeling, one should start by considering
    stakeholder needs. A perfect model will be useless if it doesn't solve a relevant
    problem. Planning a strategy around business needs ensures that a successful model
    will lead to actionable insights.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在制定预测建模计划时，应首先考虑利益相关者的需求。如果模型不能解决相关问题，那么即便是完美的模型也毫无意义。围绕业务需求制定战略，确保成功的模型能带来可操作的洞察。
- en: Although it may be possible in principle to solve many business problems, the
    ability to deliver the solution will always depend on the availability of the
    necessary data. Therefore, it's important to consider the business needs in the
    context of the available data sources. When data is plentiful, this will have
    little effect, but as the amount of available data becomes smaller, so too does
    the scope of problems that can be solved.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然原则上可以解决许多业务问题，但能否交付解决方案始终取决于必要数据的可用性。因此，考虑业务需求时需要结合可用的数据源。在数据充足时，这不会产生太大影响，但随着可用数据量的减少，可解决的问题范围也会变小。
- en: 'These ideas can be formed into a standard process for determining a predictive
    analytics plan, which goes as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些想法可以形成一个标准的过程，用于确定预测分析计划，流程如下：
- en: Look at the available data to understand the range of realistically solvable
    business problems. At this stage, it might be too early to think about the exact
    problems that can be solved. Make sure you understand the data fields available
    and the
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看可用数据，以了解现实中可解决的业务问题范围。在这个阶段，可能还为时过早去思考具体能解决哪些问题。确保你了解可用的数据字段以及它们适用的
- en: time frames they apply to.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 时间框架。
- en: Determine the business needs by speaking with key stakeholders. Seek out a problem
    where the solution will lead to actionable business decisions.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过与关键利益相关者交谈，确定业务需求。寻找一个通过解决该问题可以得出可操作的商业决策的场景。
- en: Assess the data for suitability by considering the availability of a sufficiently diverse
    and large feature space. Also, take into account the condition of the data: are
    there large chunks of missing values for certain variables or time ranges?
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估数据的适用性时，考虑特征空间是否足够多样和大。此外，还要考虑数据的状况：是否存在某些变量或时间范围的大量缺失值？
- en: Steps 2 and 3 should be repeated until a realistic plan has taken shape. At
    this point, you will already have a good idea of what the model input will be
    and what you might expect as output.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 2 和步骤 3 应该重复执行，直到一个实际可行的计划逐渐成型。此时，你已经能大致了解模型的输入是什么，以及你可能期望的输出是什么。
- en: 'Once we''ve identified a problem that can be solved with machine learning,
    along with the appropriate data sources, we should answer the following questions
    to lay a framework for the project. Doing this will help us determine which types
    of machine learning models we can use to solve the problem:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定了一个可以用机器学习解决的问题，并且有了适当的数据源，我们应该回答以下问题，为项目奠定框架。这样做将帮助我们确定可以使用哪些类型的机器学习模型来解决问题：
- en: Is the training data labeled with the target variable we want to predict?
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据是否已标注了我们想要预测的目标变量？
- en: If the answer is yes, then we will be doing supervised machine learning. Supervised
    learning has many real-world use cases, whereas it's much rarer to find business
    cases for doing predictive analytics on unlabeled data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果答案是肯定的，那么我们将进行监督学习。监督学习有许多实际应用，而针对未标注数据进行预测分析的商业案例则较为罕见。
- en: If the answer is no, then you are using unlabeled data and hence doing unsupervised
    machine learning. An example of an unsupervised learning method is cluster analysis,
    where labels are assigned to the nearest cluster for each sample.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果答案是否定的，那么你正在使用未标注的数据，因此是在进行无监督学习。一个无监督学习方法的例子是聚类分析，其中标签会被分配给每个样本所属的最近聚类。
- en: If the data is labeled, then are we solving a regression or classification problem?
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据是已标注的，那么我们是在解决回归问题还是分类问题？
- en: In a regression problem, the target variable is continuous, for example, predicting
    the amount of rain tomorrow in centimeters. In a classification problem, the target
    variable is discrete and we are predicting class labels. The simplest type of
    classification problem is binary, where each sample is grouped into one of two
    classes. For example, will it rain tomorrow or not?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归问题中，目标变量是连续的，例如预测明天的降水量（以厘米为单位）。在分类问题中，目标变量是离散的，我们要预测的是类别标签。最简单的分类问题是二分类问题，其中每个样本被分为两个类别之一。例如，明天会下雨吗？
- en: What does the data look like? How many distinct sources?
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据长什么样子？有多少个不同的数据源？
- en: Consider the size of the data in terms of width and height, where width refers
    to the number of columns (features) and height refers to the number of rows. Certain
    algorithms are more effective at handling large numbers of features than others.
    Generally, the bigger the dataset, the better in terms of accuracy. However, training
    can be very slow and memory intensive for large datasets. This can always be reduced
    by performing aggregations on the data or using dimensionality reduction techniques.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑数据的大小，包括宽度和高度，其中宽度指的是列数（特征），高度指的是行数。某些算法在处理大量特征时比其他算法更有效。一般来说，数据集越大，准确性就越高。然而，对于大数据集，训练可能会非常慢且内存消耗较大。这可以通过对数据进行聚合或使用降维技术来减少。
- en: 'If there are different data sources, can they be merged into a single table?
    If not, then we may want to train models for each and take an ensemble average
    for the final prediction model. An example where we may want to do this is with
    various sets of times series data on different scales. Consider we have the following
    data sources: a table with the AAPL stock closing prices on a daily time scale
    and iPhone sales data on a monthly time scale.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有不同的数据源，它们能否合并成一个单一的表格？如果不能，那么我们可能需要为每个数据源训练模型，并为最终预测模型进行集成平均。一个可能需要这样做的例子是，拥有不同规模的多组时间序列数据。假设我们有以下数据源：一个每日时间尺度上的AAPL股票收盘价格表和一个按月时间尺度的iPhone销量数据。
- en: We could merge the data by adding the monthly sales data to each sample in the
    daily time scale table, or grouping the daily data by month, but it might be better
    to build two models, one for each dataset, and use a combination of the results
    from each in the final prediction model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将每个样本的月销售数据添加到日时间尺度表中来合并数据，或者将每日数据按月分组，但可能更好的方式是为每个数据集建立两个模型，然后在最终的预测模型中结合每个模型的结果。
- en: Preprocessing Data for Machine Learning
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习数据预处理
- en: Data preprocessing has a huge impact on machine learning. Like the saying "you
    are what you eat," the model's performance is a direct reflection of the data
    it's trained on. Many models depend on the data being transformed so that the
    continuous feature values have comparable limits. Similarly, categorical features
    should be encoded into numerical values. Although important, these steps are relatively
    simple and do not take very long.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理对机器学习有着巨大的影响。就像“你就是你吃的东西”这句谚语一样，模型的表现直接反映了它所训练的数据。许多模型依赖于数据的转换，使得连续特征值具有可比的限制。同样，分类特征应当被编码为数值型数据。虽然这些步骤很重要，但相对简单，不会花费太多时间。
- en: The aspect of preprocessing that usually takes the longest is cleaning up messy
    data. Just take a look at this pie plot showing what data scientists from a particular
    survey spent most of their time doing.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理中通常最耗时的部分是清理杂乱的数据。只需看看这张饼图，显示了一项特定调查中数据科学家们花费最多时间的工作。
- en: '![](img/32567c8e-b535-47f7-926b-58915f4cb7cf.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32567c8e-b535-47f7-926b-58915f4cb7cf.png)'
- en: Another thing to consider is the size of the datasets being used by many data
    scientists. As the dataset size increases, the prevalence of messy data increases
    as well, along with the difficulty in cleaning it.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的方面是许多数据科学家使用的数据集大小。随着数据集大小的增加，杂乱数据的出现频率也随之增加，清理这些数据的难度也随之增加。
- en: Simply dropping the missing data is usually not the best option, because it's
    hard to justify throwing away samples where most of the fields have values. In
    doing so, we could lose valuable information that may hurt final model performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地丢弃缺失数据通常不是最佳选择，因为很难证明丢弃大多数字段都有值的样本是合理的。这样做可能会丧失宝贵的信息，进而影响最终模型的表现。
- en: 'The steps involved in data preprocessing can be grouped as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理的步骤可以归纳如下：
- en: Merging data sets on common fields to bring all data into a single table
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据共同字段合并数据集，将所有数据汇总成一个表格
- en: Feature engineering to improve the quality of data, for example, the use of dimensionality
    reduction techniques to build new features
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程以提高数据质量，例如使用降维技术构建新特征
- en: Cleaning the data by dealing with duplicate rows, incorrect or missing values,
    and other issues that arise
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过处理重复行、错误或缺失值以及其他出现的问题来清理数据
- en: Building the training data sets by standardizing or normalizing the required
    data and splitting it into training and testing sets
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过标准化或归一化所需数据并将其拆分为训练集和测试集来构建训练数据集
- en: Let's explore some of the tools and methods for doing the preprocessing.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索一些进行数据预处理的工具和方法。
- en: Exploring data preprocessing tools and methods
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据预处理工具和方法
- en: Start the `NotebookApp` from the project directory by executing `jupyter notebook`.Navigate
    to the `chapter-2` directory and open up the `chapter-2-workbook.ipynb` file.
    Find the cell near the top where the packages are loaded, and run it.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在项目目录中通过执行`jupyter notebook`启动`NotebookApp`。然后进入`chapter-2`目录并打开`chapter-2-workbook.ipynb`文件。找到位于顶部附近加载包的单元并运行它。
- en: We are going to start by showing off some basic tools from Pandas and scikit-learn.
    Then, we'll take a deeper dive into methods for rebuilding missing data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先展示一些来自Pandas和scikit-learn的基本工具。接着，我们将深入研究重建缺失数据的方法。
- en: 'Scroll down to Subtopic `Preprocessing data for machine learning` and run the
    cell containing `pd.merge`? to display the docstring for the merge function in
    the notebook:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向下滚动到子主题`预处理机器学习数据`，运行包含`pd.merge`的单元，以在笔记本中显示合并函数的文档字符串：
- en: '![](img/6f724102-2815-42ef-869b-f59d448858ec.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f724102-2815-42ef-869b-f59d448858ec.png)'
- en: As we can see, the function accepts a left and right DataFrame to merge. You
    can specify one or more columns to group on as well as how they are grouped, that
    is,to use the left, right, outer, or inner sets of values. Let's see an example
    of this in use.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，该函数接受左右两个DataFrame进行合并。你可以指定一个或多个列来进行分组，并且可以选择如何进行分组，也就是说，使用左、右、外部或内部的值集。让我们来看一个使用的例子。
- en: 'Exit the help popup and run the cell containing the following sample DataFrames:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 退出帮助弹窗并运行包含以下示例 DataFrame 的单元格：
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, we will build two simple DataFrames from scratch. As can be seen, they
    contain a `product` column with some shared entries.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将从零开始构建两个简单的 DataFrame。如你所见，它们包含一个名为 `product` 的列，其中有一些共享的条目。
- en: Now, we are going to perform an inner merge on the `product` shared column and
    print the result.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将对 `product` 共享列执行内部合并并打印结果。
- en: 'Run the next cell to perform the inner merge:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行下一个单元格以执行内部合并：
- en: '![](img/9c21a6e7-9029-4b81-a4a2-b0ded6813e94.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c21a6e7-9029-4b81-a4a2-b0ded6813e94.png)'
- en: Note how only the shared items, **red shirt** and white dress, are included.
    To include all entries from both tables, we can do an outer merge instead. Let's
    do this now.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，只有共享的项目 **红色衬衫** 和白色连衣裙被包含在内。为了包含两个表中的所有条目，我们可以改用外部合并。现在就来做吧。
- en: 'Run the next cell to perform an outer merge:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行下一个单元格以执行外部合并：
- en: '![](img/cb3d8fcc-b912-44f1-a9ff-aa8df63a2f7e.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb3d8fcc-b912-44f1-a9ff-aa8df63a2f7e.png)'
- en: This returns all of the data from each table where missing values have been
    labeled with `NaN`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回每个表中的所有数据，其中缺失值已标记为 `NaN`。
- en: 'Run the next cell to perform an outer merge:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行下一个单元格以执行外部合并：
- en: '![](img/cb3d8fcc-b912-44f1-a9ff-aa8df63a2f7e.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb3d8fcc-b912-44f1-a9ff-aa8df63a2f7e.png)'
- en: This returns all of the data from each table where missing values have been
    labeled with `NaN`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回每个表中的所有数据，其中缺失值已标记为 `NaN`。
- en: Since this is our fist time encountering an `NaN` value in this book, now is
    a good time to discuss how these work in Python.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是我们第一次在本书中遇到 `NaN` 值，现在是讨论它们在 Python 中如何工作的好时机。
- en: First of all, you can define an `NaN` variable by doing, for example, `a = float('nan')`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你可以通过以下方式定义一个 `NaN` 变量，例如 `a = float('nan')`。
- en: However, if you want to test for equality, you cannot simply use standard comparison methods.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你想测试相等性，你不能简单地使用标准比较方法。
- en: 'It''s best to do this instead with a high-level function from a library such
    as `NumPy`. This is illustrated with the following code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最好通过像`NumPy`这样的库中的高层函数来实现。以下代码演示了这一点：
- en: '![](img/4e351060-c60e-41e8-98fb-de8f72b9e064.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e351060-c60e-41e8-98fb-de8f72b9e064.png)'
- en: 'Some of these results may seem counter intuitive. There is logic behind this
    behavior, however, and for a deeper understanding of the fundamental reasons for
    standard comparisons returning False, check out this excellent Stack Overflow
    thread: [https://stackoverflow.com/questions/1565164/what-is-the-rationale-for-all-comparisons-returning-false-for-ieee754-nan-values.](https://stackoverflow.com/questions/1565164/what-is-the-rationale-for-all-comparisons-returning-false-for-ieee754-nan-values)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果可能看起来有些反直觉。然而，这背后是有逻辑的。如果你想更深入地理解标准比较返回 False 的根本原因，可以查看这个非常好的 Stack Overflow
    讨论：[https://stackoverflow.com/questions/1565164/what-is-the-rationale-for-all-comparisons-returning-false-for-ieee754-nan-values.](https://stackoverflow.com/questions/1565164/what-is-the-rationale-for-all-comparisons-returning-false-for-ieee754-nan-values)
- en: You may have noticed that our most recently merged table has duplicated data
    in the fist few rows. Let's see how to handle this.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可能已经注意到，我们最近合并的表格在前几行有重复的数据。让我们看看如何处理这个问题。
- en: 'Run the cell containing `df.drop_duplicates()` to return a version of the DataFrame
    with no duplicate rows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 运行包含 `df.drop_duplicates()` 的单元格，以返回没有重复行的 DataFrame 版本：
- en: '![](img/a6eb2f78-014e-415c-9f05-60aa2a799270.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6eb2f78-014e-415c-9f05-60aa2a799270.png)'
- en: This is the easiest and "standard" way to drop duplicate rows. To apply these
    changes to df, we can either `set inplace=True` or do something like `df = df.drop_duplicated()`
    . Let's see another method, which uses masking to select or drop duplicate rows.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最简单的“标准”去除重复行的方法。要将这些更改应用到 df，我们可以选择 `set inplace=True` 或做类似 `df = df.drop_duplicated()`
    的操作。让我们来看另一种方法，它使用掩码来选择或去除重复行。
- en: 'Run the cell containing `df.duplicated()` to print the True/False series, marking
    duplicate rows:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行包含 `df.duplicated()` 的单元格，打印出 True/False 序列，标记重复行：
- en: '![](img/76823424-ab27-4c20-b56a-053833baa96f.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76823424-ab27-4c20-b56a-053833baa96f.png)'
- en: We can take the sum of this result to determine how many rows have duplicates,
    or it can be used as a mask to select the duplicated rows.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对这个结果求和，以确定有多少行是重复的，或者它可以用作掩码来选择重复的行。
- en: 'Do this by running the next two cells:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行接下来的两个单元格来实现：
- en: '![](img/3c292a6c-ab36-4293-9e9b-4288b5cba555.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c292a6c-ab36-4293-9e9b-4288b5cba555.png)'
- en: 'We can compute the opposite of the mask with a simple tilde (`~`) to extract
    the deduplicated DataFrame. Run the following code and convince yourself the output is
    the same as that from `df.drop_duplicates()` :'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用简单的波浪线（`~`）计算掩码的相反值，以提取去重后的DataFrame。运行以下代码，并确信输出与`df.drop_duplicates()`的结果相同：
- en: '`df[~df.duplicated()]`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`df[~df.duplicated()]`'
- en: '![](img/9a3e75fd-9f5c-42ce-8b71-cb95595583d6.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a3e75fd-9f5c-42ce-8b71-cb95595583d6.png)'
- en: 'This can also be used to drop duplicates from a subset of the full DataFrame.
    For example, run the cell containing the following code:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这也可以用来从完整的DataFrame子集删除重复项。例如，运行包含以下代码的单元格：
- en: '`df[~df[''product''].duplicated()]`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`df[~df[''product''].duplicated()]`'
- en: '![](img/cac8d491-5f80-4fb4-b09f-440e34f9453c.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cac8d491-5f80-4fb4-b09f-440e34f9453c.png)'
- en: 'Here, we are doing the following things:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在做以下事情：
- en: Creating a mask (a `True`/`False` series) for the product row, where duplicates
    are marked with `e`
  id: totrans-89
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为产品行创建一个掩码（一个`True`/`False`系列），其中重复项用`e`标记
- en: Using the tilde (`~`) to take the opposite of that mask, so that duplicates
    are instead marked with False and everything else is `True`
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用波浪线（`~`）获取该掩码的相反值，这样重复项就被标记为False，其他所有项为`True`
- en: Using that mask to filter out the `False` rows of `df`, which correspond to
    the duplicated products
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用该掩码过滤掉`df`中`False`的行，这些行对应于重复的产品
- en: As expected, we now see that only the first red shirt row remains, as the duplicate
    product rows have been removed.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们现在看到只有第一行红色衬衫留下来，因为重复的产品行已被删除。
- en: In order to proceed with the steps, let's replace `df` with a deduplicated version
    of itself. This can be done by running `drop_duplicates` and passing the parameter
    `inplace=True`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续步骤，让我们用`df`的去重版本替换它。这可以通过运行`drop_duplicates`并传递`inplace=True`参数来实现。
- en: 'Deduplicate the DataFrame and save the result by running the cell containing
    the following code:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元格，去重DataFrame并保存结果：
- en: '`df.drop_duplicates(inplace=True)`'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`df.drop_duplicates(inplace=True)`'
- en: Continuing on to other preprocessing methods, let's ignore the duplicated rows
    and first deal with the missing data. This is necessary because models cannot
    be trained on incomplete samples. Using the missing price data for blue pants
    and white tuxedo as an example, let's show some different options for handling
    `NaN` values.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 继续其他预处理方法，让我们忽略重复行，首先处理缺失数据。这是必要的，因为模型不能在不完整的样本上进行训练。以蓝色裤子和白色燕尾服的缺失价格数据为例，我们展示一些处理`NaN`值的不同选项。
- en: 'One option is to drop the rows, which might be a good idea if your NaN samples are
    missing the majority of their values. Do this by running the cell containing `df.dropna()`:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个选择是删除行，如果你的NaN样本大部分值缺失，这可能是个好主意。通过运行包含`df.dropna()`的单元格来执行此操作：
- en: '![](img/aeae3f40-b4db-4577-a076-173d33862f59.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aeae3f40-b4db-4577-a076-173d33862f59.png)'
- en: 'If most of the values are missing for a feature, it may be best to drop that
    column entirely. Do this by running the cell containing the same method as before,
    but this time with the axes parameter passed to indicate columns instead of rows:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果某个特征的大部分值都缺失，最好完全删除该列。通过运行包含之前相同方法的单元格来执行此操作，但这次传递`axes`参数以指示列而不是行：
- en: '![](img/98ef084c-27d4-496d-8fea-db589f8eb018.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98ef084c-27d4-496d-8fea-db589f8eb018.png)'
- en: Simply dropping the `NaN` values is usually not the best option, because losing
    data is never good, especially if only a small fraction of the sample values is
    missing.Pandas offers a method for filling in `NaN` entries in a variety of different
    ways, some of which we'll illustrate now.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 仅删除`NaN`值通常不是最佳选择，因为丢失数据永远不好，特别是当样本值只有少部分缺失时。Pandas提供了一种以多种不同方式填充`NaN`条目的方法，其中一些我们将在本章中展示。
- en: 'Run the cell containing `df.fillna?` to print the docstring for the Pandas
    `NaN-fill` method:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行包含`df.fillna?`的单元格，以打印Pandas `NaN-fill` 方法的文档字符串：
- en: '![](img/4fb8a796-95f2-4d38-a0db-f9624e3b14a5.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4fb8a796-95f2-4d38-a0db-f9624e3b14a5.png)'
- en: Note the options for the value parameter; this could be, for example, a single
    value or a dictionary/series type map based on index. Alternatively, we can leave
    the value as None and pass a fill method instead. We'll see examples of each in
    this chapter.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意`value`参数的选项；例如，它可以是一个单一值，或者是基于索引的字典/系列类型映射。或者，我们可以将`value`保持为None，并传递一个填充方法。我们将在本章中展示每种情况的示例。
- en: 'Fill in the missing data with the average product price by running the cell containing
    the following code:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元格，用平均产品价格填充缺失的数据：
- en: '`df.fillna(value=df.price.mean())`'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`df.fillna(value=df.price.mean())`'
- en: '![](img/d2508e74-e8e6-49b6-ab28-f45226336411.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d2508e74-e8e6-49b6-ab28-f45226336411.png)'
- en: 'Now, fill in the missing data using the pad method by running the cell containing the
    following code instead:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，通过运行包含以下代码的单元格，使用pad方法填充缺失的数据：
- en: '`df.fillna(method=''pad'')`'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`df.fillna(method=''pad'')`'
- en: '![](img/06574946-eeaf-44c8-8d7c-ff33db9d53e6.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06574946-eeaf-44c8-8d7c-ff33db9d53e6.png)'
- en: Notice how the **white dress** price was used to pad the missing values below
    it.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，**白色裙子**的价格被用来填充下面缺失的值。
- en: To conclude this section, we will prepare our simple table to be used for training
    a machine learning algorithm. Don't worry, we won't actually try to train any
    models on such a small dataset! We start this process by encoding the class labels
    for the categorical data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结本节内容，我们将准备一个简单的表格，用于训练机器学习算法。别担心，我们不会在这么小的数据集上训练任何模型！我们从编码类别数据的类标签开始这个过程。
- en: 'Before encoding the labels, run the fist cell in the `Building training data
    sets` section to add another column of data representing the average product ratings:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在编码标签之前，先运行`Building training data sets`部分中的第一个单元格，以添加另一列数据，表示产品的平均评分：
- en: '![](img/780a265b-e2e9-4999-94c9-bebd4b545d11.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/780a265b-e2e9-4999-94c9-bebd4b545d11.png)'
- en: Imagining we want to use this table to train a predictive model, we should first
    think about changing all the variables to numeric types.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们希望使用这个表格来训练一个预测模型，我们应该首先考虑将所有变量更改为数值类型。
- en: 'The simplest column to handle is the Boolean list:`in_stock`. This should be
    changed to numeric values, for example, 0 and 1, before using it to train a predictive
    model. This can be done in many ways, for example, by running the cell containing
    the following code: `df.in_stock = df.in_stock.map({False: 0, True: 1})`'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '最简单的列是布尔列表：`in_stock`。在使用该列来训练预测模型之前，应将其转换为数值型数据，例如0和1。这可以通过多种方式实现，例如运行包含以下代码的单元格：`df.in_stock
    = df.in_stock.map({False: 0, True: 1})`'
- en: '![](img/870248af-69ea-4dec-b052-fca89b2e7dbe.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/870248af-69ea-4dec-b052-fca89b2e7dbe.png)'
- en: 'Another option for encoding features is scikit-learn''s LabelEncoder, which
    can be used to map the class labels to integers at a higher level. Let''s test
    this by running the cell containing the following code:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一种编码特征的选择是scikit-learn的LabelEncoder，它可以用于将类别标签映射为整数，方法上更为简便。我们通过运行包含以下代码的单元格来测试这一点：
- en: '[PRE1]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](img/c6e2ce61-258a-4a0a-bfe5-743d9a90ce04.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6e2ce61-258a-4a0a-bfe5-743d9a90ce04.png)'
- en: This might bring to mind the preprocessing we did in the previous chapter, when
    building the polynomial model. Here, we instantiate a label encoder and then "train"
    it and "transform" our data using the `fit_transform` method. We apply the result
    to a copy of our DataFrame, `_df`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会让你想起我们在上一章构建多项式模型时进行的预处理。在这里，我们实例化一个标签编码器，然后“训练”它并使用`fit_transform`方法“转换”我们的数据。我们将结果应用于我们的数据框副本`_df`。
- en: 'The features can then be converted back using the class we reference with the variable
    `rating_encoder, by running rating_encoder.inverse_transform(df.rating)`:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，可以使用我们在变量`rating_encoder`中引用的类，通过运行`rating_encoder.inverse_transform(df.rating)`将特征转换回来：
- en: '![](img/d0c3289c-bfb4-4b4f-9566-163d2acb87d5.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0c3289c-bfb4-4b4f-9566-163d2acb87d5.png)'
- en: You may notice a problem here. We are working with a so-called "ordinal" feature,
    where there's an inherent order to the labels. In this case, we should expect
    that a rating of "low" would be encoded with a 0 and a rating of "high" would
    be encoded with a 2\. However, this is not the result we see. In order to achieve
    proper ordinal label encoding, we should again use map, and build the dictionary
    ourselves.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到一个问题。我们正在处理一个所谓的“序数”特征，其中标签具有固有的顺序。在这种情况下，我们应该期望“低”评级会被编码为0，而“高”评级会被编码为2。然而，结果并不是我们所看到的那样。为了实现正确的序数标签编码，我们应该再次使用map，并自行构建字典。
- en: 'Encode the ordinal labels properly by running the cell containing the following code:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元格，正确地编码序数标签：
- en: '[PRE2]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](img/001e575c-a915-4803-9378-acd1388b0269.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/001e575c-a915-4803-9378-acd1388b0269.png)'
- en: We first create the mapping dictionary. This is done using a dictionary comprehension
    and enumeration, but looking at the result, we see that it could just as easily
    be defined *manually* instead. Then, as done earlier for the `in_stock` column,
    we apply the dictionary mapping to the feature. Looking at the result, we see
    that rating now makes more sense than before, where `low` is labeled with 0, `medium`
    with 1, and `high` with 2.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建映射字典。通过字典推导式和枚举来实现，但从结果来看，这个字典也可以很容易地手动定义。然后，就像之前处理`in_stock`列时那样，我们将字典映射应用到特征上。从结果来看，我们可以看到评分（rating）现在比之前更有意义，其中`low`被标记为0，`medium`为1，`high`为2。
- en: Now that we've discussed ordinal features, let's touch on another type called
    nominal features. These are fields with no inherent order, and in our case, we
    see that `product` is a perfect example.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了有序特征，让我们简单谈一下另一种类型的特征，即名义特征（nominal features）。这些字段没有固有的顺序，在我们的例子中，`product`就是一个完美的例子。
- en: Most scikit-learn models can be trained on data like this, where we have strings
    instead of integer-encoded labels. In this situation, the necessary conversions
    are done under the hood. However, this may not be the case for all models in scikit
    learn, or other machine learning and deep learning libraries. Therefore, it's
    good practice to encode these ourselves during preprocessing
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数scikit-learn模型可以在这样的数据上进行训练，其中我们使用字符串而非整数编码标签。在这种情况下，必要的转换是在幕后完成的。然而，这并不适用于所有scikit-learn模型，或者其他机器学习和深度学习库。因此，最好在预处理阶段自行进行编码。
- en: 'A commonly used technique to convert class labels from strings to numerical
    values is called one-hot encoding. This splits the distinct classes out into separate
    features. It can be accomplished elegantly with `pd.get_dummies()` . Do this by
    running the cell containing the following code: `df = pd.get_dummies(df)`'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个常用的技术是将类别标签从字符串转换为数值，这称为独热编码（one-hot encoding）。这种方法将不同的类别拆分成独立的特征。可以通过`pd.get_dummies()`优雅地完成。执行以下代码单元来实现：`df
    = pd.get_dummies(df)`
- en: 'The final DataFrame then looks as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的DataFrame如下所示：
- en: '![](img/96dcf83e-e7c7-47ed-b2f4-3c92f4714a67.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96dcf83e-e7c7-47ed-b2f4-3c92f4714a67.png)'
- en: 'Here, we see the result of one-hot encoding: the product column has been split into
    4, one for each unique value. Within each column, we find either a 1 or 0 representing
    whether that row contains the particular value or product.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到了独热编码的结果：`product`列被拆分成了4列，每列代表一个唯一的值。在每一列中，我们看到1或0，表示该行是否包含特定的值或产品。
- en: Moving on and ignoring any data scaling (which should usually be done), the
    final step is to split the data into training and test sets to use for machine
    learning. This can be done using scikit-learn's train_test_split. Let's assume
    we are going to try to predict whether an item is in stock, given the other feature
    values.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，忽略数据缩放（通常应该进行数据缩放），最后一步是将数据划分为训练集和测试集，供机器学习使用。这可以通过使用scikit-learn的train_test_split来完成。假设我们要预测某个项目是否有库存，基于其他特征值。
- en: 'Split the data into training and test sets by running the cell containing the
    following code:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码单元来将数据划分为训练集和测试集：
- en: '[PRE3]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](img/086e1b46-616e-436b-8560-0681c6cf3ce2.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/086e1b46-616e-436b-8560-0681c6cf3ce2.png)'
- en: Here, we are selecting subsets of the data and feeding them into the `train_test_
    split` function. This function has four outputs, which are unpacked into the training
    and testing splits for features (X) and the target (y).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们选择数据的子集并将其输入`train_test_split`函数。此函数有四个输出，分别解包为特征（X）和目标（y）的训练集和测试集。
- en: Observe the shape of the output data, where the test set has roughly 30% of
    the samples and the training set has roughly 70%.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 观察输出数据的形状，其中测试集大约包含30%的样本，而训练集大约包含70%的样本。
- en: We'll see similar code blocks later, when preparing real data to use for training
    predictive models.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后，我们会看到类似的代码块，用于准备真实数据以用于训练预测模型。
- en: This concludes the section on cleaning data for use in machine learning applications.
    Let's take a minute to note how effective our Jupyter Notebook was for testing
    various methods of transforming the data, and ultimately documenting the pipeline
    we decided upon. This could easily be applied to an updated version of the data
    by altering only specific cells of code, prior to processing. Also, should we
    desire any changes to the processing, these can easily be tested in the notebook,
    and specific cells may be changed to accommodate the alterations. The best way
    to achieve this would probably be to copy the notebook over to a new file, so
    that we can always keep a copy of the original analysis for reference.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分内容结束了，涉及到为机器学习应用清理数据的过程。我们花点时间注意一下，我们的Jupyter Notebook在测试各种数据转换方法时的效果，并最终记录了我们选择的处理流程。这可以轻松应用于数据的更新版本，只需要在处理之前调整特定代码单元。并且，如果我们想对处理流程进行任何更改，这些更改可以轻松地在笔记本中进行测试，特定的单元格可以更改以适应调整。最好的方法可能是将笔记本复制到一个新文件中，这样我们始终可以保留原始分析的副本作为参考。
- en: Moving on to an activity, we'll now apply the concepts from this section to
    a large dataset as we prepare it for use in training predictive models.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来进入一个活动，我们将应用本节的概念，对大型数据集进行处理，并为训练预测模型做准备。
- en: Activity:Preparing to Train a Predictive Model for the Employee-Retention Problem
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动：为员工留任问题准备训练预测模型
- en: Suppose you are hired to do freelance work for a company who wants to find insights
    into why their employees are leaving. They have compiled a set of data they think
    will be helpful in this respect. It includes details on employee satisfaction
    levels, evaluations, time spent at work, department, and salary.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你被聘请为一家公司做自由职业工作，帮助他们找出员工流失的原因。他们已经收集了一些认为有助于此目的的数据，包括员工满意度、评估、工作时长、部门和薪水等详细信息。
- en: 'The company shares their data with you by sending you a file called `hr_data.csv`
    and asking what you think can be done to help stop employees from leaving. To
    apply the concepts we''ve learned thus far to a real-life problem. In particular,
    we seek to:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 公司通过发送名为`hr_data.csv`的文件与您共享他们的数据，并询问您认为可以做些什么来帮助减少员工流失。将我们目前所学的概念应用到实际问题中，特别是我们旨在：
- en: Determine a plan for using predictive analytics to provide impactful business insights,
    given the available data.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据可用数据，确定使用预测分析提供有影响力的商业洞察的计划。
- en: Prepare the data for use in machine learning models.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为机器学习模型准备数据。
- en: Starting with this activity and continuing through the remainder of this chapter,
    we'll be using *Human Resources Analytics*, which is a Kaggle dataset. There is
    a small difference between the dataset we use in this book and the online version.
    Our human resource analytics data contains some `NaN` values. These were manually
    removed from the online version of the
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一活动开始，直到本章的剩余部分，我们将使用*人力资源分析*，这是一个Kaggle数据集。我们在本书中使用的数据集与在线版本之间有一个小的区别。我们的员工资源分析数据包含一些`NaN`值，而这些数据在在线版本中已被手动移除。
- en: dataset, for the purposes of illustrating data cleaning techniques. We have also
    added a column of data called `is_smoker`, for the same purposes.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集用于展示数据清洗技巧。我们还添加了一个名为`is_smoker`的数据列，目的是为了相同的目的。
- en: With the `chapter-2-workbook.ipynb` notebook file open, scroll to the Activity
    section.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`chapter-2-workbook.ipynb`笔记本文件，滚动到活动部分。
- en: 'Check the head of the table by running the following code:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下代码检查表头：
- en: '[PRE4]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Judging by the output, convince yourself that it looks to be in standard CSV
    format. For CSV files, we should be able to simply load the data with `pd.read_csv`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 根据输出结果，确保它看起来是标准的CSV格式。对于CSV文件，我们应该能够简单地通过`pd.read_csv`加载数据。
- en: Load the data with Pandas by running `df = pd.read_csv('../data/hranalytics/hr_data.csv')`
    . Use tab completion to help type the file path.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行`df = pd.read_csv('../data/hranalytics/hr_data.csv')`来使用Pandas加载数据。使用自动完成功能来帮助输入文件路径。
- en: 'Inspect the columns by printing df.columns and make sure the data has loaded as
    expected by printing the DataFrame head and tail with `df.head()` and `df.tail()`
    :'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过打印`df.columns`来检查列，确保数据已按预期加载，然后通过`df.head()`和`df.tail()`打印DataFrame的前后部分：
- en: '![](img/1a181777-dd12-4549-b10e-63fb4fcfe683.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a181777-dd12-4549-b10e-63fb4fcfe683.png)'
- en: We can see that it appears to have loaded correctly. Based on the tail index
    values,there are nearly 15,000 rows; let's make sure we didn't miss any.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到它似乎已正确加载。根据尾部索引值，数据有将近 15,000 行；让我们确保没有遗漏任何数据。
- en: 'Check the number of rows (including the header) in the `CSV file` with the
    following code:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码检查`CSV文件`中的行数（包括标题）：
- en: '[PRE5]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](img/18be9678-2c67-40ca-a02d-0ae482179cfc.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18be9678-2c67-40ca-a02d-0ae482179cfc.png)'
- en: 'Compare this result to `len(df)` to make sure we''ve loaded all the data:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此结果与`len(df)`进行比较，以确保我们已加载所有数据：
- en: '![](img/b9116ca7-4323-4383-baf0-3067742ecd08.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9116ca7-4323-4383-baf0-3067742ecd08.png)'
- en: Now that our client's data has been properly loaded, let's think about how we
    can use predictive analytics to find insights into why their employees are leaving.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，客户的数据已经正确加载，让我们思考如何使用预测分析找出员工离职的原因。
- en: 'Let''s run through the fist steps for creating a predictive analytics plan:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从创建预测分析计划的第一步开始：
- en: '**Look at the available data**: We''ve already done this by looking at the
    columns, datatypes, and the number of samples'
  id: totrans-166
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查看可用数据**：我们已经通过查看列、数据类型和样本数量完成了这一步'
- en: '**Determine the business needs**: The client has clearly expressed their needs:
    reduce the number of employees who leave'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确定业务需求**：客户已经明确表达了他们的需求：减少离职员工的数量'
- en: '**Assess the data for suitability**: Let''s try to determine a plan that can
    help satisfy the client''s needs, given the provided data'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估数据的适用性**：让我们尝试制定一个计划，以便根据提供的数据帮助满足客户的需求'
- en: Recall, as mentioned earlier, that effective analytics techniques lead to impactful
    business decisions. With that in mind, if we were able to predict how likely an
    employee is to quit, the business could selectively target those employees for
    special treatment. For example, their salary could be raised or their number of
    projects reduced. Furthermore, the impact of these changes could be estimated
    using the model!
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，正如前面提到的，有效的分析技术能带来具有影响力的商业决策。考虑到这一点，如果我们能预测员工离职的可能性，企业就可以有针对性地对这些员工进行特别处理。例如，可以提高他们的薪水或减少他们的项目数量。此外，利用模型还可以估算这些变化的影响！
- en: To assess the validity of this plan, let's think about our data. Each row represents
    an employee who either works for the company or has **left**, as labeled by the
    column named left. We can therefore train a model to predict this target, given
    a set of features.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估这个计划的有效性，让我们思考一下我们的数据。每一行代表一个员工，员工要么在公司工作，要么已**离职**，这由名为“left”的列标记。因此，我们可以训练一个模型，根据一组特征来预测这个目标。
- en: 'Assess the target variable. Check the distribution and number of missing entries
    by running the following code:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 评估目标变量。通过运行以下代码检查分布和缺失值数量：
- en: '[PRE6]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](img/9efa19b0-e218-4946-961a-2806e606a906.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9efa19b0-e218-4946-961a-2806e606a906.png)'
- en: 'Here''s the output of the second code line:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第二行代码的输出：
- en: '![](img/3d6017b4-3f4f-49e8-8e9d-c2baa58efef3.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d6017b4-3f4f-49e8-8e9d-c2baa58efef3.png)'
- en: About three-quarters of the samples are employees who have not left. The group
    who has left makes up the other quarter of the samples. This tells us we are dealing
    with an imbalanced classification problem, which means we'll have to take special
    measures to account for each class when calculating accuracies. We also see that
    none of the target variables are missing (no `NaN` values).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 大约四分之三的样本是没有离职的员工，离职的员工占其余四分之一。这告诉我们我们正在处理一个不平衡的分类问题，这意味着在计算准确度时，我们需要采取特别措施来考虑每个类别。我们还发现，目标变量没有缺失值（没有`NaN`值）。
- en: 'Now, we''ll assess the features:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来评估特征：
- en: 'Print the datatype of each by executing `df.dtypes`. Observe how we have a
    mix of continuous and discrete features:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行`df.dtypes`打印每列的数据类型。请注意，我们有连续型和离散型特征的混合：
- en: '![](img/1bc604a8-aa56-4d85-8823-477ee6a0c060.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1bc604a8-aa56-4d85-8823-477ee6a0c060.png)'
- en: 'Display the feature distributions by running the following code:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下代码显示特征分布：
- en: '[PRE7]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This code snippet is a little complicated, but it's very useful for showing
    an overview of both the continuous and discrete features in our dataset. Essentially,
    it assumes each feature is continuous and attempts to plot its distribution, and
    reverts to simply plotting the value counts if the feature turns out to be discrete.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码稍显复杂，但它非常有用，可以展示数据集中连续型和离散型特征的概览。实际上，它假设每个特征都是连续的，并尝试绘制其分布；如果特征是离散的，它会转而绘制值计数。
- en: 'The result is as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![](img/7aa02a0b-5938-487c-8d83-534b3a3881bc.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7aa02a0b-5938-487c-8d83-534b3a3881bc.png)'
- en: '![](img/fda3c924-026b-4987-8e31-55e64f06fc49.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fda3c924-026b-4987-8e31-55e64f06fc49.png)'
- en: '![](img/114b846f-53c7-47d7-a659-2b59c0c918a2.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/114b846f-53c7-47d7-a659-2b59c0c918a2.png)'
- en: For many features, we see a wide distribution over the possible values, indicating
    a good variety in the feature spaces. This is encouraging; features that are strongly
    grouped around a small range of values may not be very informative for the model.
    This is the case for `promotion_last_5years`, where we see that the vast majority
    of samples are 0.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多特征，我们看到其值分布广泛，表明特征空间具有很好的多样性。这是令人鼓舞的；如果特征值紧密分布在一个小范围内，可能对模型来说不太有用。`promotion_last_5years`就是这种情况，我们可以看到绝大多数样本的值为0。
- en: The next thing we need to do is remove any `NaN` values from the dataset.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要做的是从数据集中移除所有的`NaN`值。
- en: 'Check how many `NaN` values are in each column by running the following code:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码来检查每列中有多少`NaN`值：
- en: '`df.isnull().sum() / len(df) * 100`'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`df.isnull().sum() / len(df) * 100`'
- en: '![](img/362bc015-f9d4-41c1-94d7-2914c79cbc09.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/362bc015-f9d4-41c1-94d7-2914c79cbc09.png)'
- en: We can see there are about 2.5% missing for a`verage_montly_hours`, 1% missing for
    t`ime_spend_company`, and 98% missing for `is_smoker`! Let's use a couple of different
    strategies that we've learned about to handle these.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，大约有2.5%的`average_monthly_hours`数据缺失，`time_spend_company`有1%的数据缺失，而`is_smoker`有98%的数据缺失！让我们使用一些已经学到的策略来处理这些缺失值。
- en: 'Since there is barely any information in the `is_smoker` metric, let''s drop
    this column. Do this by running: `del df[''is_smoker'']` .'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于`is_smoker`指标几乎没有任何信息，让我们删除这一列。通过运行以下代码来实现：`del df['is_smoker']`。
- en: 'Since time_spend_company is an integer fild, we''ll use the median value to
    fil the NaN values in this column. This can be done with the following code:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于`time_spend_company`是一个整数字段，我们将使用中位数值来填充该列中的`NaN`值。这可以通过以下代码实现：
- en: '[PRE8]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The final column to deal with is `average_montly_hours`. We could do something similar
    and use the median or rounded mean as the integer fill value. Instead though,
    let's try to take advantage of its relationship with another variable. This may
    allow us to fill the missing data more accurately.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 处理的最后一列是`average_montly_hours`。我们可以做类似的操作，使用中位数或四舍五入后的均值作为整数填充值。不过，我们不妨尝试利用它与另一个变量之间的关系。这可能使我们能够更准确地填充缺失的数据。
- en: 'Make a boxplot of `average_montly_hours` segmented by `number_project`. This can
    be done by running the following code:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制`average_monthly_hours`按`number_project`分组的箱形图。可以通过运行以下代码来实现：
- en: '[PRE9]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/3ad737c9-9977-47cd-aacb-389f83329698.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ad737c9-9977-47cd-aacb-389f83329698.png)'
- en: We can see how the number of projects is correlated with a`verage_monthly_hours`,
    a result that is hardly surprising. We'll exploit this relationship by filling
    in the `NaN` values of `average_montly_hours` differently, depending on the number
    of projects for that sample. Specifically, we'll use the mean of each group.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到项目数量与`average_monthly_hours`之间的相关性，这一结果并不令人惊讶。我们将利用这种关系，通过根据每个样本的项目数量，使用该组的均值来填充`average_monthly_hours`中的`NaN`值。
- en: 'Calculate the mean of each group by running the following code:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下代码来计算每个组的均值：
- en: '[PRE10]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](img/3a759d97-a02d-493a-995e-bbdd922d3a5f.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a759d97-a02d-493a-995e-bbdd922d3a5f.png)'
- en: We can then map this onto the `number_project` column and pass the resulting series
    object as the argument to `fillna`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将其映射到`number_project`列，并将结果序列对象作为参数传递给`fillna`。
- en: 'Fill the `NaN` values in `average_montly_hours` by executing the following
    code:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下代码来填充`average_monthly_hours`中的`NaN`值：
- en: '[PRE11]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Confirm that `df` has no more `NaN` values by running the following assertion
    test. If it does not raise an error, then you have successfully removed the `NaNs`
    from the table:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下断言测试，确认`df`中没有更多的`NaN`值。如果没有抛出错误，那么就说明你已经成功地从表格中移除了`NaN`：
- en: '`assert df.isnull().sum().sum() == 0`'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`assert df.isnull().sum().sum() == 0`'
- en: 'Finally, we will transform the string and Boolean filds into integer representations.In
    particular, we''ll manually convert the target variable `left` from `yes` and
    `no` to` 1` and `0` and build the one-hot encoded features. Do this by running
    the following code:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将把字符串和布尔字段转换为整数表示。特别地，我们将手动将目标变量`left`从`yes`和`no`转换为`1`和`0`，并构建独热编码特征。通过运行以下代码来实现：
- en: '[PRE12]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Print `df.columns` to show the fields:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印`df.columns`以显示字段：
- en: '![](img/1c2b5d84-ee4a-4f17-bda4-495ebf1c5f51.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c2b5d84-ee4a-4f17-bda4-495ebf1c5f51.png)'
- en: We can see that `department` and `salary` have been split into various binary
    features.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，`department`和`salary`已经被拆分成了多个二元特征。
- en: The final step to prepare our data for machine learning is scaling the features,
    but for various reasons (for example, some models do not require scaling), we'll
    do it as part of the model-training workflow in the next activity.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 准备机器学习数据的最后一步是特征缩放，但出于各种原因（例如，某些模型不需要缩放），我们将在下一个活动的模型训练工作流中进行这一步骤。
- en: 'We have completed the data preprocessing and are ready to move on to training models!
    Let''s save our preprocessed data by running the following code:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经完成了数据预处理，准备进入模型训练阶段！让我们通过运行以下代码来保存我们预处理过的数据：
- en: '[PRE13]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Again, we pause here to note how well the Jupyter Notebook suited our needs
    when performing this initial data analysis and clean-up. Imagine, for example,
    we left this project in its current state for a few months. Upon returning to
    it, we would probably not remember what exactly was going on when we left it.
    Referring back to this notebook though, we would be able to retrace our steps
    and quickly recall what we previously learned about the data. Furthermore, we
    could update the data source with any new data and re-run the notebook to prepare
    the new set of data for use in our machine learning algorithms. Recall that in
    this situation, it would be best to make a copy of the notebook fist, so as not
    to lose the initial analysis.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提醒，在执行初步数据分析和清理时，Jupyter Notebook 是多么适合我们的需求。举个例子，假设我们把这个项目放置在当前状态下几个月后再回来。那时，我们可能已经不记得我们离开时到底发生了什么。然而，参考回这个笔记本，我们就能重新追溯我们的步骤，并迅速回忆起之前对数据的学习内容。此外，我们还可以用新的数据更新数据源，并重新运行笔记本，准备新数据集以便用于我们的机器学习算法。请记住，在这种情况下，最好首先复制笔记本，以免丢失最初的分析。
- en: 'To summarize, we''ve learned and applied methods for preparing to train a machine
    learning model. We started by discussing steps for identifying a problem that
    can be solved with predictive analytics. This consisted of:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们已经学习并应用了训练机器学习模型的准备方法。我们从讨论如何识别可以通过预测分析解决的问题的步骤开始。这包括：
- en: Looking at the available data
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看可用数据
- en: Determining the business needs
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定业务需求
- en: Assessing the data for suitability
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估数据的适用性
- en: We also discussed how to identify supervised versus unsupervised and regression
    versus classification problems.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了如何识别有监督学习与无监督学习，以及回归问题与分类问题。
- en: After identifying our problem, we learned techniques for using Jupyter Notebooks
    to build and test a data transformation pipeline. These techniques included methods
    and best practices for filing missing data, transforming categorical features,
    and building train/test data sets.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在识别问题后，我们学习了使用 Jupyter Notebooks 构建和测试数据转换管道的技巧。这些技巧包括处理缺失数据、转换分类特征和构建训练/测试数据集的方法和最佳实践。
- en: In the remainder of this chapter, we will use this preprocessed data to train
    a variety of classification models. To avoid blindly applying algorithms we don't
    understand, we start by introducing them and overviewing how they work. Then,
    we use Jupyter to train and compare their predictive capabilities. Here, we have
    the opportunity to discuss more advanced topics in machine learning like overfitting,
    k-fold cross-validation, and validation curves.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将使用这些预处理过的数据来训练各种分类模型。为了避免盲目应用我们不了解的算法，我们首先介绍这些算法并概述它们的工作原理。然后，我们使用
    Jupyter 训练并比较它们的预测能力。在这里，我们有机会讨论机器学习中的一些更高级的主题，比如过拟合、k折交叉验证和验证曲线。
- en: Training Classification Models
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练分类模型
- en: As we've already seen in the previous chapter, using libraries such as scikit-learn
    and platforms such as Jupyter, predictive models can be trained in just a few
    lines of code. This is possible by abstracting away the difficult computations
    involved with optimizing model parameters. In other words, we deal with a black
    box where the internal operations are hidden instead. With this simplicity also
    comes the danger of misusing algorithms, for example, by over fitting during training
    or failing to properly test on unseen data. We'll show how to avoid these pitfalls
    while training classification models and produce trustworthy results with the
    use of k-fold cross validation and validation curves.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中已经看到的，使用诸如scikit-learn这样的库和Jupyter等平台，预测模型可以通过几行代码进行训练。这是通过抽象化优化模型参数所涉及的复杂计算来实现的。换句话说，我们处理的是一个“黑盒”，其中内部操作被隐藏起来。正是这种简化带来了误用算法的风险，例如，在训练过程中过度拟合，或者未能在未见过的数据上进行正确测试。我们将展示如何避免在训练分类模型时遇到这些陷阱，并通过使用k折交叉验证和验证曲线来产生可信的结果。
- en: Introduction to Classification Algorithms
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类算法简介
- en: 'Recall the two types of supervised machine learning: regression and classification.
    In regression, we predict a continuous target variable. For example, recall the
    linear and polynomial models from the fist chapter. In this chapter, we focus
    on the other type of supervised machine learning: classification. Here, the goal
    is to predict the class of a sample using the available metrics.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾两种监督式机器学习方法：回归和分类。在回归中，我们预测一个连续的目标变量。例如，回忆一下第一章中的线性和多项式模型。在本章中，我们关注的是另一种监督式机器学习方法：分类。这里的目标是使用可用的度量标准来预测样本的类别。
- en: In the simplest case, there are only two possible classes, which means we are
    doing binary classification. This is the case for the example problem in this
    chapter, where we try to predict whether an employee has left or not. If we have
    more than two class labels instead, we are doing multi-class classification.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的情况下，只有两种可能的类别，这意味着我们正在进行二分类。这是本章示例问题的情况，我们试图预测一个员工是否离职。如果我们有两个以上的类别标签，那么我们就是在进行多分类。
- en: Although there is little difference between binary and multi-class classification
    when training models with scikit-learn, what's done inside the "black box" is
    notably different. In particular, multi-class classification models often use
    the one-versus-rest method. This works as follows for a case with three class
    labels. When the model is "fit" with the data, three models are trained, and each
    model predicts whether the sample is part of an individual class or part of some
    other class. This might bring to mind the one-hot encoding for features that we
    did earlier. When a prediction is made for a sample, the class label with the
    highest confidence level is returned.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用scikit-learn训练模型时，二分类和多分类之间几乎没有差别，但在“黑盒”内部进行的操作却明显不同。特别是，多分类模型通常使用一对多方法。对于具有三个类别标签的情况，方法如下：当模型“拟合”数据时，训练三个模型，每个模型预测样本是否属于某个特定类别或其他类别。这可能会让人联想到我们之前做的特征的独热编码。当对一个样本进行预测时，返回的是具有最高置信度的类别标签。
- en: 'In this chapter, we''ll train three types of classification models: Support
    Vector Machines, Random Forests, and k-Nearest Neighbors classifiers. Each of
    these algorithms are quite different. As we will see, however, they are quite
    similar to train and use for predictions thanks to scikit-learn. Before swapping
    over to the Jupyter Notebook and implementing these, we''ll briefly see how they
    work. SVM''s attempt to find the best hyperplane to divide classes by. This is
    done by maximizing the distance between the hyperplane and the closest samples
    of each class, which are called support vectors.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将训练三种类型的分类模型：支持向量机、随机森林和k-近邻分类器。这些算法各不相同。然而，正如我们将看到的，由于使用了scikit-learn，它们在训练和用于预测时是非常相似的。在切换到Jupyter
    Notebook并实现这些模型之前，我们将简要了解它们的工作原理。支持向量机尝试找到最佳超平面以区分不同类别。这是通过最大化超平面与每个类别最近样本之间的距离来实现的，这些样本被称为支持向量。
- en: This linear method can also be used to model nonlinear classes using the kernel
    trick. This method maps the features into a higher-dimensional space in which
    the hyper plane is determined. This hyperplane we've been talking about is also
    referred to as the decision surface, and we'll visualize it when training our
    models.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这种线性方法也可以通过核技巧来建模非线性类别。该方法将特征映射到一个更高维度的空间，在其中确定超平面。我们所讨论的这个超平面也被称为决策面，我们将在训练模型时可视化它。
- en: k-Nearest Neighbors classification algorithms memorize the training data and
    make predictions depending on the K nearest samples in the feature space. With
    three features, this can be visualized as a sphere surrounding the prediction
    sample. Often, however, we are dealing with more than three features and therefore
    hyperspheres are drawn to find the closest K samples.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: k-最近邻分类算法记住训练数据，并根据特征空间中最近的 K 个样本进行预测。对于三个特征，这可以通过一个包围预测样本的球体来可视化。然而，通常我们处理的不止三个特征，因此绘制超球体来找到最近的
    K 个样本。
- en: Random Forests are an ensemble of decision trees, where each has been trained
    on different subsets of the training data.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一组决策树，每棵树都在不同的训练数据子集上训练。
- en: A decision tree algorithm classifies a sample based on a series of decisions.
    For example, the fist decision might be "if feature x_1 is less than or greater
    than 0." The data would then be split on this condition and fed into descending
    branches of the tree. Each step in the decision tree is decided based on the feature
    split that maximizes the information gain.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法通过一系列决策对样本进行分类。例如，第一个决策可能是“如果特征 x_1 小于或大于 0”。然后，数据会根据这个条件被分割，并输入到树的下行分支中。决策树中的每一步都根据最大化信息增益的特征分割来做出决策。
- en: Essentially, this term describes the mathematics that attempts to pick the best
    possible split of the target variable.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，这个术语描述了试图选择目标变量最佳分割的数学方法。
- en: Training a Random Forest consists of creating bootstrapped (that is, randomly
    sampled data with replacement) datasets for a set of decision trees. Predictions
    are then made based on the majority vote. These have the benefit of less overfitting
    and better generalizability.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 训练随机森林的过程包括为一组决策树创建自助采样（即带替换的随机抽样数据）数据集。然后根据多数投票做出预测。这些模型的好处是减少过拟合，并且具有更好的泛化能力。
- en: 'Decision trees can be used to model a mix of continuous and categorical data,
    which make them very useful. Furthermore, as we will see later in this chapter,
    the tree depth can be limited to reduce overfitting. For a detailed (but brief)
    look into the decision tree algorithm, check out this popular Stack Overflw answer:
    [https://stackoverflow. com/a/1859910/3511819](https://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain/1859910#1859910).
    There, the author shows a simple example and discusses concepts such as node purity,
    information gain, and entropy.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以用来建模连续数据和分类数据的混合，这使得它们非常有用。此外，正如我们在本章后面会看到的那样，可以通过限制树的深度来减少过拟合。要详细了解（但简洁的）决策树算法，可以查看这个流行的
    Stack Overflow 回答：[https://stackoverflow. com/a/1859910/3511819](https://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain/1859910#1859910)。在那里，作者展示了一个简单的示例，并讨论了节点纯度、信息增益和熵等概念。
- en: Training two-feature classification models with scikitlearn
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 训练双特征分类模型
- en: 'We''ll continue working on the employee retention problem that we introduced
    in the fist topic. We previously prepared a dataset for training a classification
    model, in which we predicted whether an employee has left or not. Now, we''ll
    take that data and use it to train classification models:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续处理在第一个话题中介绍的员工留存问题。我们之前准备了一个数据集来训练分类模型，预测员工是否离职。现在，我们将使用这个数据来训练分类模型：
- en: If you have not already done so, start the `NotebookApp` and open the `chapter-2-workbook.ipynb
    file`. Scroll down to `Topic Training classification models`. Run the fist couple
    of cells to set the default fiure size and load the processed data that we previously
    saved to a `CSV file`.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你还没有这样做，请启动 `NotebookApp` 并打开 `chapter-2-workbook.ipynb` 文件。向下滚动到 `Topic Training
    classification models`。运行前几行代码来设置默认图像大小并加载我们之前保存到 `CSV 文件` 的处理数据。
- en: 'For this example, we''ll be training classification models on two continuous
    features:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将训练基于两个连续特征的分类模型：
- en: '`satisfaction_level` and `last_evaluation.`'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`satisfaction_level` 和 `last_evaluation`。'
- en: 'Draw the bivariate and univariate graphs of the continuous target variables
    by running the cell with the following code:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元格，绘制连续目标变量的双变量和单变量图：
- en: '[PRE14]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](img/1e1e552c-adc3-4adf-8df2-b8416adbf185.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1e1e552c-adc3-4adf-8df2-b8416adbf185.png)'
- en: As you can see in the preceding image, there are some very distinct patterns
    in the data.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的图片所示，数据中存在一些非常明显的模式。
- en: 'Re-plot the bivariate distribution, segmenting on the target variable, by running
    the cell containing the following code:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元格，重新绘制目标变量分段的双变量分布：
- en: '[PRE15]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](img/64b9bb12-90af-4e78-bf54-a418f9543255.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64b9bb12-90af-4e78-bf54-a418f9543255.png)'
- en: Now, we can see how the patterns are related to the target variable. For the remainder
    of this section, we'll try to exploit these patterns to train effective classification
    models.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到这些模式与目标变量之间的关系。接下来，我们将尝试利用这些模式来训练有效的分类模型。
- en: 'Split the data into training and test sets by running the cell containing the
    following code:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元格，将数据划分为训练集和测试集：
- en: '[PRE16]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Our first two models, the Support Vector Machine and k-Nearest Neighbors algorithm,
    are most effective when the input data is scaled so that all of the features are
    on the same order. We'll accomplish this with scikit-learn's `StandardScaler`.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的前两个模型——支持向量机和 k 最近邻算法——在输入数据进行缩放，使所有特征处于相同的数量级时最为有效。我们将通过 scikit-learn 的
    `StandardScaler` 来完成这一点。
- en: 'Load `StandardScaler` and create a new instance, as referenced by the scaler variable.
    Fit the `scaler` on the training set and transform it. Then, transform the test set.
    Run the cell containing the following code:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 `StandardScaler` 并创建一个新的实例，如变量 `scaler` 所引用。将 `scaler` 拟合到训练集并进行转换，然后转换测试集。运行包含以下代码的单元格：
- en: '[PRE17]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: An easy mistake to make when doing machine learning is to "fit" the scaler on
    the whole dataset, when in fact it should only be "fit" to the training data.
    For example, scaling the data before splitting into training and testing sets
    is a mistake. We don't want this because the model training should not be influenced
    in any way by the test data.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行机器学习时，一个常见的错误是将 `scaler` 拟合到整个数据集，而实际上它应该只拟合训练数据。例如，在将数据划分为训练集和测试集之前进行缩放是错误的。我们不希望这样做，因为模型训练不应受到测试数据的任何影响。
- en: 'Import the scikit-learn support vector machine class and fit the model on the
    training data by running the cell containing the following code:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 scikit-learn 的支持向量机类，并通过运行包含以下代码的单元格，将模型拟合到训练数据上：
- en: '[PRE18]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Then, we train a linear SVM classification model. The C parameter controls the
    penalty for misclassification, allowing the variance and bias of the model to
    be controlled.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们训练一个线性支持向量机分类模型。C 参数控制误分类的惩罚，从而可以控制模型的方差和偏差。
- en: 'Compute the accuracy of this model on unseen data by running the cell containing the
    following code:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元格，计算此模型在未见数据上的准确度：
- en: '[PRE19]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We predict the targets for our test samples and then use scikit-learn's `accuracy_
    score` function to determine the accuracy. The result looks promising at ~75%!
    Not bad for our first model. Recall, though, the target is imbalanced. Let's see
    how accurate the predictions are for each class.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预测测试样本的目标值，然后使用 scikit-learn 的 `accuracy_score` 函数来确定准确度。结果看起来很有前景，大约为 75%！对于我们的第一个模型来说，这还不错。不过，请记住，目标变量是不平衡的。让我们看看每个类别的预测准确度。
- en: 'Calculate the confusion matrix and then determine the accuracy within each
    class by running the cell containing the following code:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算混淆矩阵，然后通过运行包含以下代码的单元格来确定每个类别内的准确度：
- en: '[PRE20]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: It looks like the model is simply classifying every sample as 0, which is clearly
    not helpful at all. Let's use a contour plot to show the predicted class at each
    point in the feature space. This is commonly known as the decision-regions plot.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来模型只是将每个样本都分类为 0，这显然是完全没有帮助的。让我们使用等高线图显示特征空间中每个点的预测类别。这通常被称为决策区域图。
- en: 'Plot the decision regions using a helpful function from the `mlxtend` library.
    Run the cell containing the following code:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `mlxtend` 库中的一个有用函数绘制决策区域。运行包含以下代码的单元格：
- en: '[PRE21]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](img/c998fa17-26e7-4a5a-8e40-4d599eebc52f.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c998fa17-26e7-4a5a-8e40-4d599eebc52f.png)'
- en: The function plots decision regions along with a set of samples passed as arguments.
    In order to see the decision regions properly without too many samples obstructing
    our view, we pass only a 200-sample subset of the test data to the`plot_ decision_regions`
    function. In this case, of course, it does not matter. We see the result is entirely
    red, indicating every point in the feature space would be classified as 0.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数绘制决策区域以及作为参数传递的一组样本。为了正确看到决策区域而不受太多样本阻碍视线，我们仅传递了测试数据的一个包含200个样本的子集给`plot_decision_regions`函数。在这种情况下，当然无关紧要。我们看到结果完全是红色的，表示特征空间中的每个点都将被分类为0。
- en: It shouldn't be surprising that a linear model can't do a good job of describing
    these nonlinear patterns. Recall earlier we mentioned the kernel trick for using
    SVM's to classify nonlinear problems. Let's see if doing this can improve the
    result.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 不出意料，线性模型无法很好地描述这些非线性模式。回想一下，我们先前提到了使用SVM分类非线性问题的内核技巧。让我们看看这样做是否可以改善结果。
- en: 'Print the docstring for scikit-learn''s SVM by running the cell containing
    SVC. Scroll down and check out the parameter descriptions. Notice the `kernel`
    option, which is actually enabled by default as `rbf`. Use this kernel option
    to train a new SVM by running the cell containing the following code:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行包含SVC单元格以打印scikit-learn SVM的文档字符串。向下滚动并查看参数描述。请注意`kernel`选项，默认情况下实际上是启用的`rbf`。使用此内核选项训练新的SVM，运行包含以下代码的单元格：
- en: '[PRE22]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In order to assess this and future model performance more easily, let's define
    a function called `check_model_fit`, which computes various metrics that we can
    use to compare the models. Run the cell where this function is defied.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了更轻松地评估此及未来模型的性能，让我们定义一个名为`check_model_fit`的函数，计算各种度量标准，以便比较模型。运行定义此函数的单元格。
- en: Each computation done in this function has already been seen in this example;
    it simply calculates accuracies and plots the decision regions.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数中进行的每个计算在此示例中均已看到；它仅计算准确性并绘制决策区域。
- en: 'Show the newly trained kernel-SVM results on the training data by running the
    cell containing the following code:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行包含以下代码的单元格，显示对训练数据进行新训练的核支持向量机（kernel-SVM）的结果：
- en: '[PRE23]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](img/82fd597b-ca0e-452a-981b-688c63eadbc0.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82fd597b-ca0e-452a-981b-688c63eadbc0.png)'
- en: '![](img/e509aa55-12cd-47ce-953c-da0043bc4b5a.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e509aa55-12cd-47ce-953c-da0043bc4b5a.png)'
- en: The result is much better. Now, we are able to capture some of the non-linear patterns
    in the data and correctly classify the majority of the employees who have left.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 结果好多了。现在，我们能够捕捉数据中一些非线性模式，并正确分类大多数离职员工。
- en: The plot_decision_regions Function
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制决策区域函数
- en: The `plot_decision_regions` function is provided by `mlxtend`, a Python library
    developed by Sebastian *Raschka*. It's worth taking a peek at the source code
    (which is of course written in Python) to understand how these plots are drawn.
    It's really not too complicated.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '`plot_decision_regions`函数由Sebastian *Raschka*开发的Python库`mlxtend`提供。值得一看源代码（当然是用Python编写的）以理解这些绘图是如何绘制的。这并不是太复杂。'
- en: 'In a Jupyter Notebook, import the function with from `mlxtend.plotting` import
    `plot_ decision_regions`, and then pull up the help with `plot_decision_regions?`
    and scroll to the bottom to see the local file path:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jupyter Notebook中，从`mlxtend.plotting`导入`plot_decision_regions`函数，然后使用`plot_decision_regions?`查看帮助，并滚动到底部查看本地文件路径：
- en: '![](img/e86d69a7-2040-42ae-9d66-8be5df420c35.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e86d69a7-2040-42ae-9d66-8be5df420c35.png)'
- en: 'Then, open up the file and check it out! For example, you could run cat in
    the notebook:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，打开文件并查看它！例如，你可以在笔记本中运行`cat`：
- en: '![](img/00b708c7-fe1c-4dd7-9efa-89912901b660.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00b708c7-fe1c-4dd7-9efa-89912901b660.png)'
- en: This is okay, but not ideal as there's no color markup for the code. It's better
    to copy it (so you don't accidentally alter the original) and open it with your
    favorite text editor.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这样也可以，但是对于代码没有颜色标记不理想。最好复制它（以免意外更改原始内容），然后使用您喜爱的文本编辑器打开它。
- en: When drawing attention to the code responsible for mapping the decision regions,
    we see a contour plot of predictions `Z` over an array `X_predict` that spans
    the feature space.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 当注意绘制决策区域映射代码时，我们看到预测`Z`在跨越特征空间的数组`X_predict`上的等高线图。
- en: '![](img/1fbc10b9-e569-4e2c-ab8f-6c81dcc7f82c.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1fbc10b9-e569-4e2c-ab8f-6c81dcc7f82c.png)'
- en: 'Let''s move on to the next model: k-Nearest Neighbors.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续下一个模型：k最近邻。
- en: Training k-nearest neighbors for our model
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为我们的模型训练k最近邻
- en: 'Load the scikit-learn KNN classification model and print the docstring by running the
    cell containing the following code:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 scikit-learn 的 KNN 分类模型，并通过运行包含以下代码的单元格来打印文档字符串：
- en: '[PRE24]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `n_neighbors` parameter decides how many samples to use when making a classification.
    If the weights parameter is set to uniform, then class labels are decided by majority
    vote. Another useful choice for the weights is distance, where closer samples
    have a higher weight in the voting. Like most model parameters, the best choice
    for this depends on the particular dataset.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_neighbors` 参数决定了在进行分类时使用多少个样本。如果权重参数设置为均匀（uniform），则类别标签由多数投票决定。另一个有用的权重选择是距离，其中距离较近的样本在投票中具有较高的权重。像大多数模型参数一样，这个选择的最佳值取决于特定的数据集。'
- en: 'Train the KNN classifier with `n_neighbors=3`, and then compute the accuracy
    and decision regions. Run the cell containing the following code:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个 KNN 分类器，`n_neighbors=3`，然后计算准确率和决策区域。运行包含以下代码的单元格：
- en: '[PRE25]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](img/3a2dc2b3-be30-45cf-938f-143f97d0bdae.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a2dc2b3-be30-45cf-938f-143f97d0bdae.png)'
- en: '![](img/581d3d92-766f-410c-91b7-9c4d89c603c3.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/581d3d92-766f-410c-91b7-9c4d89c603c3.png)'
- en: We see an increase in overall accuracy and a significant improvement for class
    1 in particular. However, the decision region plot would indicate we are overfitting
    the data. This is evident by the hard, "choppy" decision boundary, and small pockets
    of blue everywhere. We can soften the decision boundary and decrease overfitting
    by increasing the number of nearest neighbors.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到整体准确度有所提高，尤其是类别 1 的准确度有了显著改善。然而，决策区域图表表明我们正在对数据进行过拟合。通过硬性的、"锯齿状"的决策边界以及随处可见的小块蓝色区域可以明显看出这一点。我们可以通过增加最近邻的数量来软化决策边界，并减少过拟合。
- en: 'Train a KNN model with `n_neighbors=25` by running the cell containing the following
    code:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元格，训练一个 `n_neighbors=25` 的 KNN 模型：
- en: '[PRE26]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](img/60bcd33b-7a68-47dd-b3f6-89b6e32e3043.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60bcd33b-7a68-47dd-b3f6-89b6e32e3043.png)'
- en: '![](img/c5e825a1-6d29-4ddb-977b-78cd61dcce20.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5e825a1-6d29-4ddb-977b-78cd61dcce20.png)'
- en: As we can see, the decision boundaries are significantly less choppy, and there
    are far less pockets of blue. The accuracy for class 1 is slightly less, but we
    would need to use a more comprehensive method such as k-fold cross validation
    to decide if
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，决策边界明显不再那么锯齿状，蓝色区域也大大减少。类别 1 的准确率略有下降，但我们需要使用更全面的方法，比如 k 折交叉验证，来决定是否
- en: there's a significant difference between the two models.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型之间有显著的差异。
- en: Note that increasing `n_neighbors` has no effect on training time, as the model
    is simply memorizing the data. The prediction time, however, will be greatly affected.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，增加`n_neighbors`对训练时间没有影响，因为模型仅仅是在记忆数据。然而，预测时间会受到很大影响。
- en: When doing machine learning with real-world data, it's important for the algorithms
    to run quick enough to serve their purposes. For example, a script to predict
    tomorrow's weather that takes longer than a day to run is completely useless!
    Memory is also a consideration that should be taken into account when dealing
    with substantial amounts of data.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用真实世界数据进行机器学习时，算法的运行速度足够快以达到其目的非常重要。例如，预测明天天气的脚本如果运行时间超过一天，那就完全没有用！内存也是一个需要考虑的因素，尤其是在处理大量数据时。
- en: Training a Random Forest
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练随机森林
- en: Observe how similar it is to train and make predictions on each model, despite
    them each being so different internally.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 观察训练和预测每个模型的相似性，尽管它们内部差异很大。
- en: 'Train a Random Forest classification model composed of 50 decision trees, each with
    a max depth of 5\. Run the cell containing the following code:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个由 50 棵决策树组成的随机森林分类模型，每棵树的最大深度为 5。运行包含以下代码的单元格：
- en: '[PRE27]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](img/14e57a88-7257-4dca-ade2-8f71dbf835dc.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/14e57a88-7257-4dca-ade2-8f71dbf835dc.png)'
- en: '![](img/3383eccc-dfd2-4adc-adb3-9a3d11a74b87.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3383eccc-dfd2-4adc-adb3-9a3d11a74b87.png)'
- en: Note the distinctive axes-parallel decision boundaries produced by decision
    tree machine learning algorithms.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 注意决策树机器学习算法所产生的独特的轴对齐决策边界。
- en: We can access any of the individual decision trees used to build the Random
    Forest. These trees are stored in the `estimators_attribute` of the model. Let's
    draw one of these decision trees to get a feel for what's going on. Doing this
    requires the **graph** viz dependency, which can sometimes be difficult to install.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以访问构建随机森林时使用的任何单个决策树。这些树被存储在模型的 `estimators_attribute` 中。让我们绘制其中一棵决策树，以便了解其中的内容。执行此操作需要**图形**可视化依赖项，安装起来有时可能会有些困难。
- en: 'Draw one of the decision trees in the Jupyter Notebook by running the cell containing
    the following code:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元格，在Jupyter Notebook中绘制其中一棵决策树：
- en: '[PRE28]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![](img/6993f559-1315-4153-bf82-10085abc0ac1.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6993f559-1315-4153-bf82-10085abc0ac1.png)'
- en: We can see that each path is limited to five nodes as a result of setting `max_depth=5`.
    The orange boxes represent predictions of `no` (has not left the company), and
    the blue boxes represent `yes` (has left the company). The shade of each box (light,
    dark, and so on) indicates the confidence level, which is related to the `gini`
    value.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，每条路径被限制为五个节点，这是由于设置了`max_depth=5`。橙色框表示`no`（没有离开公司）的预测，蓝色框表示`yes`（已经离开公司）的预测。每个框的阴影（浅色、深色等）表示置信度，和`gini`值相关。
- en: 'To summarize, we have accomplished two of the learning objectives in this section:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们已经完成了本节中的两个学习目标：
- en: We gained a qualitative understanding of support vector machines (SVMs), k-Nearest
    Neighbor classifiers (kNNs), and Random Forest
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们获得了对支持向量机（SVM）、k-近邻分类器（kNN）和随机森林的定性理解。
- en: We are now able to train a variety of models using scikit-learn and Jupyter
    Notebooks so that we can confidently build and compare predictive models
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们现在能够使用scikit-learn和Jupyter Notebook训练多种模型，从而有信心构建和比较预测模型。
- en: 'In particular, we used the preprocessed data from our employee retention problem
    to train classification models to predict whether an employee has left the company
    or not. For the purposes of keeping things simple and focusing on the algorithms,
    we built models to predict this given only two features: the satisfaction level
    and last evaluation value. This two-dimensional feature space also allowed us
    to visualize the decision boundaries and identify what overfitting looks like.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们使用了员工离职问题的预处理数据来训练分类模型，预测员工是否已经离开公司。为了简化问题并专注于算法，我们构建了一个模型，仅根据两个特征来预测：满意度和最后评估值。这种二维特征空间还使我们能够可视化决策边界，并识别过拟合的表现。
- en: 'In the following section, we will introduce two important topics in machine
    learning: k-fold cross-validation and validation curves'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将介绍机器学习中的两个重要主题：k折交叉验证和验证曲线。
- en: Assessing Models with k-Fold Cross-Validation and Validation Curves
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用k折交叉验证和验证曲线评估模型
- en: Thus far, we have trained models on a subset of the data and then assessed performance
    on the unseen portion, called the test set. This is good practice because the
    model performance on training data is not a good indicator of its effectiveness
    as a predictor. It's very easy to increase accuracy on a training dataset by overfitting
    a model, which can result in poorer performance on unseen data.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经在数据的一个子集上训练了模型，然后在未见过的部分——即测试集——上评估了性能。这是一个良好的做法，因为模型在训练数据上的表现并不能很好地反映它作为预测器的效果。通过过拟合一个模型，很容易在训练数据集上提高准确率，但这可能会导致在未见过数据上的表现较差。
- en: That said, simply training models on data split in this way is not good enough.
    There is a natural variance in data that causes accuracies to be different (if
    even slightly) depending on the training and test splits. Furthermore, using only
    one training/test split to compare models can introduce bias towards certain models
    and lead to overfitting.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，仅仅在这样分割的数据上训练模型是不够的。数据中存在自然的变异，导致不同的训练和测试分割下准确率有所不同（即使是轻微的）。此外，仅使用一个训练/测试分割来比较模型可能会引入偏向某些模型的偏差，并导致过拟合。
- en: '**k-fold** **cross validation** offers a solution to this problem and allows
    the variance to be accounted for by way of an error estimate on each accuracy
    calculation. This, in turn, naturally leads to the use of validation curves for
    tuning model parameters. These plot the accuracy as a function of a hyper parameter
    such as the number of decision trees used in a Random Forest or the max depth.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '**k折** **交叉验证** 提供了解决这个问题的方案，并通过每次准确性计算的误差估计来考虑数据的变异性。这反过来又自然地引出了使用验证曲线来调整模型参数。这些曲线绘制了准确率与超参数之间的关系，比如在随机森林中使用的决策树数量或最大深度。'
- en: This is our fist time using the term hyperparameter. It references a parameter
    that is defined when initializing a model, for example, the C parameter of the
    SVM. This is in contradistinction to a parameter of the trained model, such as
    the equation of the decision boundary hyperplane for a trained SVM.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们第一次使用超参数这个术语。它指的是在初始化模型时定义的参数，例如SVM的C参数。这与训练好的模型参数不同，后者是训练好的SVM的决策边界超平面方程。
- en: 'The method is illustrated in the following diagram, where we see how the k-folds
    can be selected from the dataset:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法在下图中进行了说明，我们可以看到如何从数据集中选择k个折叠：
- en: '![](img/f874d66c-9bf3-4666-8a1d-862230c92a86.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f874d66c-9bf3-4666-8a1d-862230c92a86.png)'
- en: 'The k-fold cross validation algorithm goes as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: k折交叉验证算法如下：
- en: Split data into k "folds" of near-equal size.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据划分为大小接近的k个“折叠”。
- en: Test and train k models on different fold combinations. Each model will include
    *k - 1* folds of training data and the left-out fold is used for testing. In this
    method, each fold ends up being used as the validation data exactly once.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在不同的折叠组合上测试和训练k个模型。每个模型将包括*k - 1*个训练数据折叠，留下的折叠用于测试。在这种方法中，每个折叠最终都会作为验证数据使用一次。
- en: Calculate the model accuracy by taking the mean of the k values. The standard deviation
    is also calculated to provide error bars on the value.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过取k个值的均值来计算模型的精度。同时计算标准偏差，以便为该值提供误差条。
- en: It's standard to set *k = 10*, but smaller values for k should be considered
    if using a big data set.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 通常设置*k = 10*，但如果使用大数据集，应考虑选择较小的k值。
- en: This validation method can be used to reliably compare model performance with
    different hyperparameters (for example, the C parameter for an SVM or the number
    of nearest neighbors in a KNN classifier). It's also suitable for comparing entirely
    different models.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这种验证方法可以可靠地比较不同超参数下的模型性能（例如，SVM的C参数或KNN分类器中的最近邻数量）。它也适用于比较完全不同的模型。
- en: Once the best model has been identified, it should be re-trained on the entirety
    of the dataset before being used to predict actual classifications.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦识别出最佳模型，应在整个数据集上重新训练该模型，然后再用于预测实际分类。
- en: When implementing this with scikit-learn, it's common to use a slightly improved
    variation of the normal k-fold algorithm instead. This is called stratified k-fold.
    The improvement is that stratified k-fold cross validation maintains roughly even
    class label populations in the folds. As you can imagine, this reduces the overall
    variance in the models and decreases the likelihood of highly unbalanced models
    causing bias.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用scikit-learn实现时，通常会使用一种稍微改进的普通k折算法。这被称为分层k折。其改进之处在于，分层k折交叉验证在折叠中保持大致均衡的类标签分布。正如你想象的那样，这减少了模型的整体方差，并降低了高度不平衡的模型造成偏差的可能性。
- en: Validation curves are plots of a training and validation metric as a function
    of some model parameter. They allow to us to make good model parameter selections.
    In this book, we will use the accuracy score as our metric for these plots.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 验证曲线是训练和验证度量值与某些模型参数的函数的图表。它们帮助我们做出合理的模型参数选择。在本书中，我们将使用精度得分作为这些图表的度量标准。
- en: The documentation for plot validation curves is available here:[ http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html](http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 有关绘制验证曲线的文档，请访问：[ http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html](http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html)。
- en: 'Consider this validation curve, where the accuracy score is plotted as a function
    of the gamma SVM parameter:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个验证曲线，其中精度得分作为gamma SVM参数的函数进行绘制：
- en: '![](img/c5981de6-b9f2-4114-8744-ea06e9a7a8ed.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5981de6-b9f2-4114-8744-ea06e9a7a8ed.png)'
- en: Starting on the left side of the plot, we can see that both sets of data are
    agreeing on the score, which is good. However, the score is also quite low compared
    to other gamma values, so therefore we say the model is underfitting the data.
    Increasing the gamma, we can see a point where the error bars of these two lines
    no longer overlap. From this point on, we see the classifier overfitting the data
    as the models behave increasingly well on the training set compared to the validation
    set. The optimal value for the gamma parameter can be found by looking for a high
    validation score with overlapping error bars on the two lines.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表的左侧开始，我们可以看到两组数据在得分上是一致的，这是好现象。然而，得分与其他gamma值相比也相对较低，因此我们认为模型存在欠拟合数据的情况。随着gamma值的增加，我们可以看到一个点，此时这两条线的误差条不再重叠。从这个点开始，我们看到分类器开始对数据进行过拟合，因为模型在训练集上的表现越来越好，而在验证集上的表现相对较差。通过寻找误差条重叠的两条线上的高验证得分，我们可以找到gamma参数的最优值。
- en: Keep in mind that a learning curve for some parameter is only valid while the
    other parameters remain constant. For example, if training the SVM in this plot,
    we could decide to pick gamma on the order of 10-4\. However, we may want to optimize
    the C parameter as well. With a different value for C, the preceding plot would
    be different and our selection for gamma may no longer be optimal.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，对于某个参数的学习曲线只有在其他参数保持不变的情况下才有效。例如，如果在此图中训练SVM，我们可以选择γ值为10⁻⁴。但我们也可能希望优化C参数。C的不同值会导致前面的图像发生变化，我们选择的γ值可能不再是最优的。
- en: Using k-fold cross validation and validation curves in Python with scikit-learn
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Python中使用K折交叉验证和验证曲线，结合scikit-learn工具。
- en: If you've not already done so, start the `NotebookApp` and open the `chapter-2-
    workbook.ipynb file`. Scroll down to Subtopic `K-fold cross-validation` and `validation
    curves`.
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你还没有这样做，启动`NotebookApp`并打开`chapter-2- workbook.ipynb 文件`。向下滚动到子主题`K-fold交叉验证`和`验证曲线`部分。
- en: The training data should already be in the notebook's memory, but let's reload
    it as a reminder of what exactly we're working with.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据应该已经加载到笔记本的内存中，但为了提醒我们正在使用的数据内容，我们可以重新加载它。
- en: 'Load the data and select the `satisfaction_level` and `last_evaluation` features for
    the training/validation set. We will not use the train-test split this time because we
    are going to use k-fold validation instead. Run the cell containing the following code:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据并选择`satisfaction_level`和`last_evaluation`特征作为训练/验证集。我们这次不使用训练集和测试集的划分，因为我们将使用K折交叉验证。运行包含以下代码的单元：
- en: '[PRE29]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Instantiate a Random Forest model by running the cell containing the following code:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元来实例化一个随机森林模型：
- en: '[PRE30]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: To train the model with stratified k-fold cross validation, we'll use the `model_
    selection.cross_val_score function`.
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使用分层K折交叉验证训练模型，我们将使用`model_selection.cross_val_score`函数。
- en: 'Train 10 variations of our model `clf` using stratified k-fold validation.
    Note that scikit-learn''s c`ross_val_score` does this type of validation by default.
    Run the cell containing the following code:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分层K折验证训练我们模型`clf`的10个变种。注意，scikit-learn的`cross_val_score`默认执行这种类型的验证。运行包含以下代码的单元：
- en: '[PRE31]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Note how we use `np.random.seed` to set the seed for the random number generator,
    therefore ensuring reproducibility with respect to the randomly selected samples
    for each fold and decision tree in the Random Forest.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们如何使用`np.random.seed`来设置随机数生成器的种子，从而确保每一折和随机森林中的每棵决策树在随机选择样本时具有可重复性。
- en: 'With this method, we calculate the accuracy as the average of each fold. We
    can also see the individual accuracies for each fold by printing scores. To see
    these, run `print(scores)`:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过这种方法，我们将准确度计算为每折的平均值。我们还可以通过打印分数来查看每一折的单独准确度。要查看这些，请运行`print(scores)`：
- en: '[PRE32]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Using `cross_val_score` is very convenient, but it doesn''t tell us about the
    accuracies within each class. We can do this manually with the `model_selection`.
    `StratifiedKFold` class. This class takes the number of folds as an initialization
    parameter, then the split method is used to build randomly sampled "masks" for
    the data. A mask is simply an array containing indexes of items in another array,
    where the items can then be returned by doing this: `data[mask]` .'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`cross_val_score`非常方便，但它没有告诉我们每个类的准确度。我们可以通过`model_selection`中的`StratifiedKFold`类手动执行此操作。该类以折数作为初始化参数，然后使用split方法为数据构建随机抽样的“掩码”。掩码只是一个数组，包含另一个数组中项目的索引，之后可以通过`data[mask]`来返回这些项目。
- en: 'Define a custom class for calculating k-fold cross validation class accuracies.
    Run the cell containing the following code:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个自定义类来计算K折交叉验证的分类准确度。运行包含以下代码的单元：
- en: '[PRE33]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We can then calculate the class accuracies with code that''s very similar to
    step 4\. Do this by running the cell containing the following code:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过类似第4步的代码来计算分类准确度。执行以下代码来实现：
- en: '[PRE34]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now we can see the class accuracies for each fold! Pretty neat, right?
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到每折的分类准确度了！是不是很酷？
- en: Let's move on to show how a validation curve can be calculated using `model_
    selection.validation_curve`. This function uses stratified k-fold cross validation
    to train models for various values of a given parameter.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来我们展示如何使用`model_selection.validation_curve`计算验证曲线。该函数使用分层K折交叉验证来训练模型，以求得给定参数的不同值。
- en: 'Do the calculations required to plot a validation curve by training Random
    Forests over a range of max_depth values. Run the cell containing the following
    code:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 进行必要的计算，通过训练随机森林模型在一系列 max_depth 值上绘制验证曲线。运行包含以下代码的单元格：
- en: '[PRE35]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This will return arrays with the cross validation scores for each model, where
    the models have different max depths. In order to visualize the results, we'll
    leverage a function provided in the scikit-learn documentation.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回每个模型的交叉验证得分数组，其中模型具有不同的最大深度。为了可视化结果，我们将利用在 scikit-learn 文档中提供的一个函数。
- en: 'Run the cell in which `plot_validation_curve` is defined. Then, run the cell
    containing the following code to draw the plot:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行定义了 `plot_validation_curve` 的单元格。然后，运行包含以下代码的单元格以绘制图表：
- en: '[PRE36]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![](img/47959f19-1125-4c31-bae3-2c3c929d2941.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47959f19-1125-4c31-bae3-2c3c929d2941.png)'
- en: Recall how setting the max depth for decision trees limits the amount of overfitting?
    This is reflected in the validation curve, where we see overfitting taking place
    for large max depth values to the right. A good value for `max_depth` appears
    to be `6`, where we see the training and validation accuracies in agreement. When
    `max_depth` is equal to `3`, we see the model underfitting the data as training
    and validation accuracies are lower.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，设置决策树的最大深度如何限制过拟合？这在验证曲线中得到了体现，我们看到在较大的 max_depth 值右侧发生了过拟合。`max_depth`
    的一个良好值似乎是 `6`，此时训练和验证的准确率一致。当 `max_depth` 等于 `3` 时，我们看到模型在训练和验证准确率较低的情况下出现了欠拟合。
- en: To summarize, we have learned and implemented two important techniques for building
    reliable predictive models. The fist such technique was k-fold cross-validation,
    which is used to split the data into various train/test batches and generate a
    set accuracy. From this set, we then calculated the average accuracy and the standard
    deviation as a measure of the error. This is important so that we have a gauge
    of the variability of our model and we can produce trustworthy accuracy.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们已经学习并实现了两种重要的构建可靠预测模型的技术。第一种技术是 k 折交叉验证，它用于将数据拆分为多个训练/测试批次并生成准确率。然后，我们从这些准确率中计算出平均值和标准差，作为误差的衡量标准。这很重要，因为它可以帮助我们衡量模型的可变性，并提供可信的准确率。
- en: 'We also learned about another such technique to ensure we have trustworthy
    results: validation curves. These allow us to visualize when our model is overfitting
    based on comparing training and validation accuracies. By plotting the curve over
    a range of our selected hyperparameter, we are able to identify its optimal value.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学到了另一种确保结果可信的技术：验证曲线。这些曲线可以帮助我们通过比较训练和验证准确率，直观地看到模型是否发生了过拟合。通过在选定的超参数范围内绘制曲线，我们能够识别其最优值。
- en: 'In the final section of this chapter, we take everything we have learned so
    far and put it together in order to build our final predictive model for the employee
    retention problem. We seek to improve the accuracy, compared to the models trained
    thus far, by including all of the features from the dataset in our model. We''ll
    see now-familiar topics such as k-fold cross-validation and validation curves,
    but we''ll also introduce something new: dimensionality reduction techniques.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后部分，我们将迄今为止学到的所有内容结合起来，构建我们最终的员工流失问题预测模型。我们希望通过将数据集中的所有特征包含在模型中，相较于之前训练的模型，提高模型的准确性。我们将再次看到熟悉的主题，如
    k 折交叉验证和验证曲线，但也会介绍一些新的内容：降维技术。
- en: Dimensionality Reduction Techniques
  id: totrans-376
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维技术
- en: Dimensionality reduction can simply involve removing unimportant features from
    the training data, but more exotic methods exist, such as **Principal Component
    Analysis (PCA)** and **Linear Discriminant Analysis (LDA)**. These techniques
    allow for data compression, where the most important information from a large
    group of features can be encoded in just a few features.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 降维可以简单地通过从训练数据中删除不重要的特征来实现，但也存在一些更复杂的方法，如**主成分分析（PCA）**和**线性判别分析（LDA）**。这些技术允许数据压缩，将从大量特征中提取的最重要信息压缩成少数几个特征。
- en: In this subtopic, we'll focus on PCA. This technique transforms the data by
    projecting it into a new subspace of orthogonal "principal components," where
    the components with the highest eigenvalues encode the most information for training
    the model. Then, we can simply select a few of these principal components in place
    of the original high-dimensional dataset. For example, PCA could be used to encode
    the information from every pixel in an image. In this case, the original feature
    space would have dimensions equal to the number of pixels in the image. This high-dimensional
    space could then be reduced with PCA, where the majority of useful information
    for training predictive models might be reduced to just a few dimensions. Not
    only does this save time when training and using models, it allows them to perform
    better by removing noise in the dataset.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个子主题中，我们将重点介绍PCA。这项技术通过将数据投影到一个新的正交“主成分”子空间来转换数据，其中具有最高特征值的成分编码了用于训练模型的大部分信息。然后，我们可以选择这些主成分中的一部分来代替原始的高维数据集。例如，PCA可以用来编码图像中每个像素的信息。在这种情况下，原始特征空间的维度等于图像中像素的数量。然后，可以使用PCA来减少这个高维空间，在训练预测模型时，大部分有用的信息可能会减少到只有几个维度。这不仅节省了训练和使用模型的时间，还通过去除数据集中的噪声提高了模型的性能。
- en: Like the models we've seen, it's not necessary to have a detailed understanding
    of PCA in order to leverage the benefits. However, we'll dig into the technical
    details of PCA just a bit further so that we can conceptualize it better. The
    key insight of PCA is to identify patterns between features based on correlations,
    so the PCA algorithm calculates the co variance matrix and then decomposes this
    into eigen vectors and eigenvalues. The vectors are then used to transform the
    data into a new subspace, from which a filed number of principal components can
    be selected.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 和我们之前看到的模型一样，利用PCA的好处并不需要对PCA有详细的理解。然而，我们将进一步深入PCA的技术细节，以便更好地理解它。PCA的关键思想是通过基于相关性的特征间模式来识别特征，因此PCA算法计算协方差矩阵，并将其分解为特征向量和特征值。然后，使用这些向量将数据转换到一个新的子空间，从中可以选择一定数量的主成分。
- en: In the following section, we'll see an example of how PCA can be used to improve
    our Random Forest model for the employee retention problem we have been working
    on. This will be done after training a classification model on the full feature
    space, to see how our accuracy is affected by dimensionality reduction.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将看到一个例子，展示如何使用PCA来改善我们在员工留存问题上使用的随机森林模型。这将是在对完整特征空间训练分类模型之后进行的，以查看降维如何影响我们的准确性。
- en: Training a predictive model for the employee retention problem
  id: totrans-381
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为员工留存问题训练预测模型
- en: We have already spent considerable effort planning a machine learning strategy,
    preprocessing the data, and building predictive models for the employee retention
    problem. Recall that our business objective was to help the client prevent employees
    from leaving. The strategy we decided upon was to build a classification model
    that would predict the probability of employees leaving. This way, the company
    can assess the likelihood of current employees leaving and take action to prevent
    it.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经花费了大量的精力来规划机器学习策略、预处理数据并构建员工留存问题的预测模型。回顾一下我们的商业目标是帮助客户防止员工离职。我们决定的策略是建立一个分类模型，预测员工离职的概率。通过这种方式，公司可以评估当前员工离职的可能性，并采取措施加以防范。
- en: 'Given our strategy, we can summarize the type of predictive modeling we are
    doing as follows:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的策略，我们可以总结出我们正在进行的预测建模类型如下：
- en: Supervised learning on labeled training data
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于标记训练数据的监督学习
- en: Classification problems with two class labels (binary)
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有两个类标签（即二分类问题）的分类问题
- en: In particular, we are training models to determine whether an employee has left
    the company, given a set of continuous and categorical features. After preparing
    the data for machine learning in Activity, *Preparing to Train a Predictive Model
    for the Employee-Retention Problem*, we went on to implement SVM, k-Nearest Neighbors,
    and Random Forest algorithms using just two features. These models were able to
    make predictions with over 90% overall accuracy. When looking at the specific
    class accuracies, however, we found that employees who had left (`class-label
    1`) could only be predicted with 70-80% accuracy. Let's see how much this can
    be improved by utilizing the full feature space.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们正在训练模型来确定员工是否已经离开公司，前提是有一组连续和类别特征。在活动*为员工留任问题训练预测模型的准备*中，我们已经准备好了机器学习数据，接着使用了SVM、k近邻和随机森林算法，且只用了两个特征。这些模型能够以90%以上的总体准确率做出预测。然而，当查看具体类别的准确性时，我们发现，离职员工（`class-label
    1`）的预测准确率只能达到70-80%。让我们看看通过利用完整的特征空间能提高多少准确率。
- en: In the `chapter-2-workbook.ipynb notebook`, scroll down to the code for this
    section. We should already have the preprocessed data loaded from the previous
    sections, but this can be done again, if desired, by executing `df = pd.read_
    csv('../data/hr-analytics/hr_data_processed.csv')` . Then, print the DataFrame
    columns with `print(df.columns)` .
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`chapter-2-workbook.ipynb`笔记本中，向下滚动到本节的代码。我们应该已经加载了前几节处理过的数据，但如果需要，可以通过执行`df
    = pd.read_csv('../data/hr-analytics/hr_data_processed.csv')`重新加载数据。然后，使用`print(df.columns)`打印DataFrame的列。
- en: 'Define list of all the features by copy and pasting the output from `df.columns` into
    a new list (making sure to remove the target variable left). Then, define `X`
    and `Y` as we have done before. This goes as follows:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过复制并粘贴`df.columns`的输出到一个新列表中来定义所有特征的列表（确保去掉目标变量）。然后，像之前一样定义`X`和`Y`。步骤如下：
- en: '[PRE37]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Looking at the feature names, recall what the values look like for each one.
    Scroll up to the set of histograms we made in the first activity to help jog your
    memory. The first two features are continuous; these are what we used for training
    models in the previous two exercises. After that, we have a few discrete features,
    such as `number_ project` and `time_spend_company`, followed by some binary fields
    such as `work_ accident` and `promotion_last_5years`. We also have a bunch of
    binary features, such as `department_IT` and `department_accounting`, which were
    created by one hot encoding.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 查看特征名称，回忆每个特征的值是什么样的。向上滚动回到我们在第一项活动中制作的直方图，帮助提醒你记忆。前两个特征是连续型的，这些特征我们在之前的两次练习中用于训练模型。接下来，我们有一些离散特征，比如`number_project`和`time_spend_company`，然后是一些二元特征，如`work_accident`和`promotion_last_5years`。我们还有一些二进制特征，比如`department_IT`和`department_accounting`，这些特征是通过独热编码创建的。
- en: Given a mix of features like this, Random Forests are a very attractive type
    of model. For one thing, they're compatible with feature sets composed of both
    continuous and categorical data, but this is not particularly special; for instance,
    an SVM can be trained on mixed feature types as well (given proper preprocessing).
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种混合特征类型的数据，随机森林是一种非常有吸引力的模型。首先，它们能够处理由连续型和类别数据组成的特征集，尽管这并不特别；例如，SVM也可以在混合特征类型上进行训练（只要进行适当的预处理）。
- en: 'If you''re interested in training an SVM or k-Nearest Neighbors classifier
    on mixed-type input features, you can use the data-scaling prescription from this
    StackExchange answer: [https://stats.stackexchange. com/questions/82923/mixing-continuous-and-binary-datawith-linear-svm/83086#83086](https://stats.stackexchange.com/questions/82923/mixing-continuous-and-binary-data-with-linear-svm).'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣使用SVM或k近邻分类器训练混合型输入特征，可以使用这个StackExchange回答中的数据缩放方法：[https://stats.stackexchange.com/questions/82923/mixing-continuous-and-binary-data-with-linear-svm/83086#83086](https://stats.stackexchange.com/questions/82923/mixing-continuous-and-binary-data-with-linear-svm)。
- en: 'A simple approach would be to preprocess data as follows:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法是按如下方式预处理数据：
- en: Standardize continuous variables
  id: totrans-394
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化连续变量
- en: One-hot-encode categorical features
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对类别特征进行独热编码
- en: Shift binary values to `-1` and 1 instead of `0` and `1`
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将二元值从`0`和`1`转换为`-1`和`1`
- en: Then, the mixed-feature data could be used to train a variety of classification
    models
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，可以使用这些混合特征数据来训练各种分类模型。
- en: 'We need to figure out the best parameters for our Random Forest model. Let''s
    start by tuning the`max_depth` hyperparameter using a validation curve. Calculate
    the training and validation accuracies by running the following code:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要为我们的随机森林模型找出最佳的参数。首先，通过使用验证曲线调整`max_depth`超参数。通过运行以下代码计算训练和验证的准确度：
- en: '[PRE38]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We are testing 10 models with k-fold cross validation. By setting `k = 5`, we
    produce five estimates of the accuracy for each model, from which we extract the
    mean and standard deviation to plot in the validation curve. In total, we train
    50 models, and since `n_estimators` is set to 20, we are training a total of 1,000
    decision trees! All in roughly 10 seconds!
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用k折交叉验证测试10个模型。通过设置`k = 5`，我们为每个模型生成五个准确度估计，从中提取平均值和标准差来绘制验证曲线。总共训练50个模型，由于`n_estimators`设为20，我们一共训练了1,000棵决策树！所有这些大约只用了10秒钟！
- en: 'Plot the validation curve using our custom plot_validation_curve function from
    the last exercise. Run the following code:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们在上一练习中创建的自定义`plot_validation_curve`函数绘制验证曲线。运行以下代码：
- en: '[PRE39]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](img/d53046ce-fa55-44c4-a852-eab90b5f9500.png)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d53046ce-fa55-44c4-a852-eab90b5f9500.png)'
- en: For small max depths, we see the model underfitting the data. Total accuracies
    dramatically increase by allowing the decision trees to be deeper and encode more
    complicated patterns in the data. As the max depth is increased further and the
    accuracy approaches 100%, we find the model overfits the data, causing the training
    and validation accuracies to grow apart. Based on this figure, let's select a
    `max_ depth` of 6 for our model.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较小的最大深度，我们看到模型对数据出现欠拟合。通过允许决策树更深并编码数据中更复杂的模式，总体准确度显著提高。随着最大深度的进一步增加，准确度接近100%，我们发现模型开始对数据过拟合，导致训练和验证准确度之间的差距增大。根据此图，我们选择将`max_depth`设为6。
- en: We should really do the same for `n_estimators`, but in the spirit of saving
    time, we'll skip it. You are welcome to plot it on your own; you should find agreement
    between training and validation sets for a large range of values. Usually, it's
    better to use more decision tree estimators in the Random Forest, but this comes
    at the cost of increased training times. We'll use 200 estimators to train our
    model.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本该对`n_estimators`做同样的操作，但为了节省时间，我们将跳过这一部分。你可以自己绘制它；你会发现对于一个较大范围的值，训练集和验证集之间是一致的。通常，使用更多的决策树估计器来训练随机森林会更好，但这会增加训练时间。我们将使用200个估计器来训练我们的模型。
- en: 'Use `cross_val_class_score`, the k-fold cross validation by class function
    we created earlier, to test the selected model, a Random Forest with `max_depth
    = 6` and `n_estimators = 200`:'
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们之前创建的`cross_val_class_score`函数（按类别进行k折交叉验证）来测试所选模型，随机森林，`max_depth = 6`和`n_estimators
    = 200`：
- en: '[PRE40]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The accuracies are way higher now that we're using the full feature set, compared
    to before when we only had the two continuous features!
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用完整的特征集，准确度明显提高，远高于之前仅使用两个连续特征时的情况！
- en: 'Visualize the accuracies with a boxplot by running the following code:'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码使用箱线图可视化准确度：
- en: '[PRE41]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![](img/0bda50cd-77fc-4490-a3d6-d27fa4e65b6b.png)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0bda50cd-77fc-4490-a3d6-d27fa4e65b6b.png)'
- en: Random Forests can provide an estimate of the feature performances.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林可以提供特征性能的估计。
- en: 'The feature importance in scikit-learn is calculated based on how the node
    impurity changes with respect to each feature. For a more detailed explanation,
    take a look at the following Stack Overflow thread about how feature importance
    is determined in Random Forest Classifier: [https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined](https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined) .'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，特征重要性是通过衡量每个特征对节点不纯度变化的贡献来计算的。关于如何确定随机森林分类器中的特征重要性，您可以查看以下Stack
    Overflow帖子：[https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined](https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined)。
- en: 'Plot the feature importance, as stored in the attribute `feature_importances_`,
    by running the following code:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码绘制存储在`feature_importances_`属性中的特征重要性：
- en: '[PRE42]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![](img/a84fa4bd-e710-4a80-bc11-a6081cee8370.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a84fa4bd-e710-4a80-bc11-a6081cee8370.png)'
- en: 'It doesn''t look like we''re getting much in the way of useful contribution
    from the one-hot encoded variables: department and salary. Also, the p`romotion_
    last_5years` and `work_accident` features don''t appear to be very useful.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来来自独热编码变量（部门和薪资）并没有提供太多有用的贡献。此外，`promotion_last_5years`和`work_accident`特征似乎也不是很有用。
- en: Let's use Principal Component Analysis (PCA) to condense all of these weak features
    into just a few principal components.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用主成分分析（PCA）将所有这些弱特征压缩成少数几个主成分。
- en: 'Import the PCA class from scikit-learn and transform the features. Run the
    following code:'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从scikit-learn中导入PCA类，并转换特征。运行以下代码：
- en: '[PRE43]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Look at the string representation of `X_pca` by typing it alone and executing
    the cell:'
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入并执行单独的`X_pca`来查看其字符串表示：
- en: '[PRE44]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Since we asked for the top three components, we get three vectors returned.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们要求返回前三个成分，因此会返回三个向量。
- en: 'Add the new features to our DataFrame with the following code:'
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将新特征添加到我们的DataFrame中：
- en: '[PRE45]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Select our reduced-dimension feature set to train a new Random Forest with. Run
    the following code:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 选择我们降维后的特征集来训练新的随机森林。运行以下代码：
- en: '[PRE46]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Assess the new model''s accuracy with k-fold cross validation. This can be
    done by running the same code as before, where X now points to different features.
    The code is as follows:'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用k折交叉验证评估新模型的准确性。可以通过运行与之前相同的代码来完成，其中X现在指向不同的特征。代码如下：
- en: '[PRE47]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Visualize the result in the same way as before, using a box plot. The code
    is as follows:'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以与之前相同的方式，使用箱线图可视化结果。代码如下：
- en: '[PRE48]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](img/0bda50cd-77fc-4490-a3d6-d27fa4e65b6b.png)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0bda50cd-77fc-4490-a3d6-d27fa4e65b6b.png)'
- en: Comparing this to the previous result, we find an improvement in the class 1
    accuracy! Now, the majority of the validation sets return an accuracy greater
    than 90%. The average accuracy of 90.6% can be compared to the accuracy of 85.6%
    prior to dimensionality reduction!
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的结果相比，我们发现类别1的准确性有所提升！现在，大多数验证集返回的准确率都超过了90%。90.6%的平均准确率与降维前的85.6%准确率相比，提升明显！
- en: Let's select this as our final model. We'll need to re-train it on the full
    sample space before using it in production.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选择这个作为最终模型。在投入生产之前，我们需要在整个样本空间上重新训练它。
- en: 'Train the final predictive model by running the following code:'
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码训练最终的预测模型：
- en: '[PRE49]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Save the trained model to a binary file using `externals.joblib.dump`. Run
    the following code:'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`externals.joblib.dump`将训练好的模型保存到二进制文件中。运行以下代码：
- en: '[PRE50]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Check that it''s saved into the working directory, for example, by running:
    `!ls *.pkl`. Then, test that we can load the model from the file by running the
    following code:'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查它是否已保存到工作目录中，例如，运行：`!ls *.pkl`。然后，通过运行以下代码，测试我们能否从文件中加载模型：
- en: '[PRE51]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Congratulations! We've trained the final predictive model! Now, let's see an example
    of how it can be used to provide business insights for the client.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！我们已经训练好了最终的预测模型！现在，让我们看一个示例，了解它如何为客户提供业务洞察。
- en: Say we have a particular employee, who we'll call Sandra. Management has noticed
    she is working very hard and reported low job satisfaction in a recent survey.
    They would therefore like to know how likely it is that she will quit.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个特定的员工，我们称她为Sandra。管理层注意到她工作非常努力，并在最近的调查中报告了低工作满意度。因此，他们希望了解她离职的可能性。
- en: For the sake of simplicity, let's take her feature values as a sample from the
    training set (but pretend that this is unseen data instead).
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们将她的特征值作为训练集中的一个样本（但假装这是假数据）。
- en: 'List the feature values for Sandra by running the following code:'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码列出Sandra的特征值：
- en: '[PRE52]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The next step is to ask the model which group it thinks she should be in.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是询问模型它认为Sandra应该属于哪个组。
- en: 'Predict the class label for Sandra by running the following code:'
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码，预测Sandra的类别标签：
- en: '[PRE53]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The model classifies her as having already left the company; not a good sign!
    We can take this a step further and calculate the probabilities of each class
    label.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将她分类为已经离开公司；这不是一个好兆头！我们可以进一步采取措施，计算每个类别标签的概率。
- en: 'Use `clf.predict_proba` to predict the probability of our model predicting
    that Sandra has quit. Run the following code:'
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`clf.predict_proba`预测我们的模型预测Sandra离职的概率。运行以下代码：
- en: '[PRE54]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We see the model predicting that she has quit with 93% accuracy.Since this is
    clearly a red flag for management, they decide on a plan to reduce her number
    of monthly hours to 100 and the time spent at the company to 1.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到模型预测她已离职的准确率为93%。由于这显然是管理层的一个警示信号，他们决定减少她每月的工时至100小时，并将她在公司的时间减少到1年。
- en: 'Calculate the new probabilities with Sandra''s newly planned metrics. Run the following
    code:'
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Sandra 新计划的指标计算新的概率。运行以下代码：
- en: '[PRE55]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Excellent! We can now see that the model returns a mere 38% likelihood that
    she has quit! Instead, it now predicts she will not have left the company.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒！我们现在可以看到模型仅返回了38%的可能性，表明她已经离职！相反，现在它预测她不会离开公司。
- en: Our model has allowed management to make a data-driven decision. By reducing
    her amount of time with the company by this particular amount, the model tells
    us that she will most likely remain an employee at the company!
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型帮助管理层做出了数据驱动的决策。通过减少她在公司的时间，模型告诉我们她最有可能继续留在公司！
- en: Summary
  id: totrans-457
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have seen how predictive models can be trained in Jupyter
    Notebooks.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经了解了如何在 Jupyter Notebooks 中训练预测模型。
- en: To begin with, we talked about how to plan a machine learning strategy. We thought
    about how to design a plan that can lead to actionable business insights and stressed
    the importance of using the data to help set realistic business goals. We also
    explained machine learning terminology such as supervised learning, unsupervised
    learning, classification, and regression.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们讨论了如何规划机器学习策略。我们思考了如何设计一个能够提供可操作的商业洞察的计划，并强调了利用数据帮助设定现实商业目标的重要性。我们还解释了机器学习术语，如监督学习、无监督学习、分类和回归。
- en: 'Next, we discussed methods for preprocessing data using scikit-learn and pandas.
    This included lengthy discussions and examples of a surprisingly time-consuming
    part of machine learning: dealing with missing data.'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论了使用 scikit-learn 和 pandas 进行数据预处理的方法。这包括了关于机器学习中一个令人意外且非常耗时的部分——处理缺失数据的详细讨论和示例。
- en: In the latter half of the chapter, we trained predictive classification models
    for our binary problem, comparing how decision boundaries are drawn for various
    models such as the SVM, k-Nearest Neighbors, and Random Forest. We then showed
    how validation curves can be used to make good parameter choices and how dimensionality
    reduction can improve model performance. Finally, at the end of our activity,
    we explored how the final model can be used in practice to make data-driven decisions.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后半部分，我们训练了用于二元问题的预测分类模型，比较了各种模型（如支持向量机（SVM）、k-最近邻（k-NN）和随机森林）的决策边界。然后，我们展示了如何使用验证曲线做出好的参数选择，以及如何通过降维来提高模型性能。最后，在活动的结尾，我们探索了如何将最终模型应用于实际中，以做出数据驱动的决策。
