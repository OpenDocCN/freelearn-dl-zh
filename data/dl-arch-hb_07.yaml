- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Deep Neural Architecture Search
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经架构搜索
- en: The previous chapters introduced and recapped different **neural networks**
    (**NNs**) that are designed to handle different types of data. Designing these
    networks requires knowledge and intuition that can only be gained by consuming
    years of research in the field. The bulk of these networks are hand-designed by
    experts and researchers. This includes inventing completely novel NN layers and
    constructing an actually usable architecture by combining and stacking NN layers
    that already exist. Both tasks require a ton of iterative experimentation time
    to burn to actually achieve success in creating a network that is useful.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 前几章介绍并回顾了不同的**神经网络**（**NNs**），这些网络被设计用来处理不同类型的数据。设计这些网络需要的知识和直觉是通过多年在该领域的研究才能获得的。大部分网络是由专家和研究人员手工设计的。这包括发明全新的
    NN 层，并通过组合和堆叠已存在的 NN 层来构建一个实际可用的架构。这两个任务都需要大量的迭代实验时间，才能成功创建出一个有用的网络。
- en: 'Now, imagine a world where we can focus on inventing useful novel layers while
    the software takes care of automating the final architecture-building process.
    Automated architecture search methods help to accomplish exactly that by streamlining
    the task of designing the best final NN architecture, as long as appropriate search
    spaces are selected based on deep domain knowledge. In this chapter, we will focus
    on the task of constructing an actual usable architecture from already existing
    NN layers using an automated architecture creation process called **neural architecture
    search** (**NAS**). By understanding the different types of NAS, you will be able
    to choose the most straightforward automated search optimization approach based
    on your current model-building setup, which ranges from simple to efficiently
    complicated. Specifically, the following topics will be introduced:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一个我们可以专注于发明有用的新型层，而软件负责自动化最终架构构建过程的世界。自动化架构搜索方法正是通过简化设计最佳最终 NN 架构的任务来实现这一目标，只要根据深厚的领域知识选择合适的搜索空间。在本章中，我们将重点介绍如何通过一种叫做
    **神经架构搜索**（**NAS**）的自动化架构创建过程，从已有的 NN 层构建一个实际可用的架构。通过理解不同类型的 NAS，你将能够根据当前的模型构建设置，选择最简单的自动化搜索优化方法，从简单到高效复杂都有涉及。具体来说，以下主题将被介绍：
- en: Understanding the big picture of NAS
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 NAS 的大局
- en: Understanding general hyperparameter search-based NAS
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解基于一般超参数搜索的 NAS
- en: Understanding **reinforcement learning** (**RL**)-based NAS
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解基于 **强化学习**（**RL**）的 NAS
- en: Understanding non-RL-based NAS
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解非 RL 基于的 NAS
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter includes practical implementation in the Python programming language.
    These simple methods will need to have the following libraries installed:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括使用 Python 编程语言的实际实现。这些简单的方法需要安装以下库：
- en: '`numpy`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`'
- en: '`pytorch`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytorch`'
- en: '`catalyst ==` `21.12`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`catalyst ==` `21.12`'
- en: '`scikit-learn`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn`'
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_7](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_7).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 GitHub 上找到本章的代码文件，地址为 [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_7](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_7)。
- en: Understanding the big picture of NAS
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 NAS 的大局
- en: Before we dive into the details of the big picture of NAS methods, it’s important
    to note that although NAS minimizes the manual effort necessary for shaping the
    final architecture, it doesn’t completely negate the need for expertise in the
    field. As we discussed earlier, foundational knowledge in **deep learning** (**DL**)
    is crucial for selecting appropriate search spaces and interpreting the results
    of NAS accurately. Search spaces are the set of possible options or configurations
    that can be explored during a search. Furthermore, the performance of NAS heavily
    relies on the quality of the training data and the relevance of the search space
    to the task at hand. Therefore, domain expertise is still necessary to ensure
    that the final architecture is not only efficient but also accurate and relevant
    to the problem being solved. By the end of this section, you will have a better
    understanding of how to leverage your domain expertise to optimize the effectiveness
    of NAS.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨NAS方法的全貌之前，重要的是要注意，尽管NAS减少了构建最终架构所需的手动工作，但它并没有完全消除该领域专业知识的需求。正如我们之前讨论的，**深度学习**（**DL**）的基础知识对于选择合适的搜索空间并准确解读NAS的结果至关重要。搜索空间是搜索过程中可以探索的可能选项或配置的集合。此外，NAS的性能在很大程度上依赖于训练数据的质量以及搜索空间与任务的相关性。因此，领域专业知识仍然是必要的，以确保最终架构不仅高效，而且准确且与解决的问题相关。通过本节内容的学习，你将更好地理解如何利用你的领域专业知识来优化NAS的效果。
- en: The previous chapters on NNs have only introduced a few prominent NN layer types
    and only scratched the surface of the entire library of neural layers out there.
    Today, there are too many variations of NN layers, which makes it hard to design
    precisely which layers get used at which point in the architecture. The main problem
    is that the space of possible NN architectures is infinitely big. Additionally,
    evaluating any possible architectural design is slow and expensive in terms of
    resources. These are the reasons that make it impossible to evaluate all the possible
    NN architectures. Let’s take the training of a **convolutional NN** (**CNN**)
    ResNet50 architecture on ImageNet, for example, to get a sense of how impossible
    this is. This would take around 3-4 days with a single RTX 3080 Ti Nvidia GPU,
    which is a GPU meant for normal consumers and available to be procured off-the-shelf.
    Business consumers, on the other hand, usually obtain industrial-grade GPU variants
    that have much greater processing power, which can bring down the runtime to under
    a day.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 前几章关于神经网络（NN）的内容仅介绍了少数几种显著的神经网络层类型，并且只是略微触及了现有神经网络层库的表面。如今，神经网络层的变种非常多，这使得精确设计在架构中的各个环节使用哪些层变得困难。主要问题在于，可能的神经网络架构空间是无限大的。此外，评估任何可能的架构设计需要耗费大量的时间和资源。这些因素使得评估所有可能的神经网络架构变得不可能。以在ImageNet上训练**卷积神经网络**（**CNN**）ResNet50架构为例，来感受这种不可能性的程度。仅使用一块RTX
    3080 Ti Nvidia GPU（这是一款面向普通消费者的GPU，且可以在市场上购买到）进行训练，大约需要3-4天的时间。另一方面，商业消费者通常使用工业级的GPU变体，这些GPU具有更强的处理能力，可以将训练时间缩短至一天以内。
- en: 'Typically, researchers will hand-design architectures with already available
    NN layers and operations by intuition. This manual method is a one-off effort,
    and doing so repeatedly when newer and better core NN layers are invented is not
    scalable. This is where NAS comes into play. NAS leverages already invented NN
    layers and operations to build a more performant NN architecture. The core of
    NAS lies in using a smarter way to conceptually search through different architectures.
    The searching mechanism of NAS can be implemented in three ways, namely: **general
    hyperparameter search optimization**, RL, and NAS methods that do not use RL.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，研究人员会凭直觉手动设计已经存在的神经网络（NN）层和操作。这种手动方法是一次性的努力，当出现更新、更好的核心神经网络层时，反复进行这种操作并不可扩展。这就是神经架构搜索（NAS）发挥作用的地方。NAS利用已经发明的神经网络层和操作来构建更高效的神经网络架构。NAS的核心在于通过更智能的方式在不同架构之间进行概念上的搜索。NAS的搜索机制可以通过三种方式实现，即：**一般超参数搜索优化**、强化学习（RL）以及不使用强化学习的NAS方法。
- en: 'General hyperparameter search optimization pertains to methods that can be
    applied to any **machine learning** (**ML**) algorithm hyperparameter optimizations.
    RL is another high-level ML method, alongside **supervised learning** (**SL**)
    and **unsupervised learning** (**UL**), that deals with some form of optimizing
    actions taken in an environment that produces states with a quantifiable reward
    or punishment. Non-RL-based NAS can be further broken down into three distinctive
    types: progressive architecture growing from a small architecture baseline, progressive
    architecture downsizing from a complex fully defined architecture graph, and evolutionary
    algorithms. The progressive architecture-growing method includes all algorithms
    that slowly grow a simple network to be a larger network with increasing depth
    or width. Vice versa, there are methods that first define an architecture with
    all the possible connections and operations and slowly drop these connections.
    Finally, **evolutionary algorithms** are a branch of algorithms that are based
    on biological phenomena such as mutation and breeding. In this chapter, we will
    only cover some general hyperparameter search optimization methods, RL methods,
    a simple form of progressive growing-based NAS, and a competitive version of progressive
    downsizing-based NAS. Technical implementations will be available for the progressive
    growing-based NAS methods but not for the other more complicated methods. Open
    sourced implementations from the authors of the more complicated methods will
    be referred to instead.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通用超参数搜索优化方法适用于任何**机器学习**（**ML**）算法的超参数优化。强化学习（RL）是另一种高级机器学习方法，与**监督学习**（**SL**）和**无监督学习**（**UL**）一起，它涉及某种形式的优化，在一个环境中采取的行动会产生具有可量化奖励或惩罚的状态。非
    RL 基础的 NAS 方法可以进一步分为三种不同类型：从小的架构基线逐步增长的架构、从复杂的完全定义架构图逐步缩小的架构，以及进化算法。逐步增长架构的方法包括所有将简单网络逐渐发展成更大网络的算法，增加其深度或宽度。反之，也有一些方法首先定义一个包含所有可能连接和操作的架构，然后逐步去掉这些连接。最后，**进化算法**是一类基于生物现象（如变异和繁殖）的算法。在本章中，我们将只介绍一些通用超参数搜索优化方法、RL
    方法、基于逐步增长的简单 NAS 形式，以及逐步缩小的 NAS 的竞争版本。技术实现将提供给基于逐步增长的 NAS 方法，但其他更复杂的方法将不涉及，更多复杂方法的开源实现将另行参考。
- en: Before diving into any of the mentioned NAS methods, you need to first understand
    the notion of **microarchitecture** and **macroarchitecture**. Microarchitecture
    refers to the details of the exact combination of layers being used in a logical
    block. As introduced in [*Chapter 3*](B18187_03.xhtml#_idTextAnchor051), *Understanding
    Convolutional Neural Networks*, some of these logical blocks can be repeatedly
    stacked onto each other to generate the architecture that will be actually used.
    There can also be different logical blocks with different layer configurations
    in the final created architecture. Macroarchitecture, in comparison, refers to
    a higher-level overview of how the different blocks are combined to form the final
    NN architecture. The core idea behind NAS methods always revolves around reducing
    the search space based on already curated knowledge about which layer or which
    layer configurations work the best. The methods that will be introduced in this
    chapter will either keep the macroarchitecture setup fixed while only searching
    in the microarchitecture space or have the flexibility to explore both the micro-
    and macroarchitecture space with creative tricks to make searching feasible.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究上述提到的 NAS 方法之前，你需要先了解**微架构**和**宏架构**的概念。微架构指的是在逻辑模块中所使用的层组合的具体细节。正如在[*第
    3 章*](B18187_03.xhtml#_idTextAnchor051)《理解卷积神经网络》中介绍的那样，一些逻辑模块可以重复堆叠在一起，以生成实际使用的架构。最终生成的架构中也可能包含不同的逻辑模块和不同的层配置。相比之下，宏架构指的是从更高层次上概述不同模块如何组合形成最终的神经网络架构。NAS
    方法的核心思想总是围绕着基于已经整理好的知识来减少搜索空间，这些知识帮助我们知道哪些层或哪些层配置效果最佳。本章将介绍的这些方法，要么在保持宏架构不变的情况下，只在微架构空间中进行搜索，要么具有灵活性，可以通过创新技巧探索微架构和宏架构空间，从而使搜索变得可行。
- en: First, let’s start with the simplest NAS method, which is general hyperparameter
    search optimization algorithms.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从最简单的 NAS 方法开始，它是通用超参数搜索优化算法。
- en: Understanding general hyperparameter search-based NAS
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解基于通用超参数搜索的 NAS
- en: In ML, parameters typically refer to the weights and biases that a model learns
    during training, while **hyperparameters** are values that are set before training
    begins and influence how the model learns. Examples of hyperparameters include
    learning rate and batch size. General hyperparameter search optimization algorithms
    are a type of NAS method to automatically search for the best hyperparameters
    to use for constructing a given NN architecture. Let’s go through a few of the
    possible hyperparameters. In a **multi-layer perceptron** (**MLP**), hyperparameters
    could be the number of layers that control the depth of the MLP, the width of
    each of the layers, and the type of intermediate layer activation used. In a CNN,
    hyperparameters could be the filter size of the convolutional layer, the stride
    size of each of the layers, and the type of intermediate layer activation used
    after each convolutional layer.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，参数通常指的是模型在训练过程中学习到的权重和偏差，而**超参数**是训练开始前设定的值，影响模型的学习方式。超参数的例子包括学习率和批量大小。通用的超参数搜索优化算法是一种NAS方法，用于自动搜索用于构建给定神经网络架构的最佳超参数。让我们来了解一些可能的超参数。在**多层感知机**（**MLP**）中，超参数可能是控制MLP深度的层数、每层的宽度以及使用的中间层激活函数类型。在卷积神经网络（CNN）中，超参数可能是卷积层的滤波器大小、每层的步幅大小以及每个卷积层后使用的中间层激活函数类型。
- en: For NN architectures, the available types of hyperparameters that you can configure
    depend heavily on the capabilities of the helper tools and methods used to create
    and initialize the NN. For instance, consider the task of configuring the hidden
    layer size of three layers individually. Having a method that produces an MLP
    with a fixed number of layers of three makes it possible to perform a hyperparameter
    search only on the hidden layer sizes. This is achievable by simply adding three
    hyperparameters to the function, which sets the three hidden layer sizes respectively.
    However, to enable the flexibility to perform a hyperparameter search for both
    the number of layers and the hidden layer size, you have to build a helper method
    that can dynamically apply these hyperparameters to create an MLP.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于神经网络架构，可配置的超参数类型在很大程度上取决于用于创建和初始化神经网络的辅助工具和方法的能力。例如，考虑单独配置三层隐藏层的任务。如果有一种方法可以生成一个具有固定三层的多层感知机（MLP），那么就可以只对隐藏层大小进行超参数搜索。这可以通过简单地向函数中添加三个超参数来实现，分别设置三层隐藏层的大小。然而，要想实现既能进行层数的超参数搜索，又能进行隐藏层大小的搜索，就必须构建一个辅助方法，动态地应用这些超参数来创建一个MLP。
- en: The simplest form of NAS leverages these tools to perform a slightly smarter
    search of the defined hyperparameters. Three well-known variations of hyperparameter
    search will be covered here; these include `pytorch`, as the implementation is
    short enough to fit in a chapter. Let’s start with successive halving.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的神经架构搜索（NAS）形式利用这些工具来对已定义的超参数进行稍微智能的搜索。这里将介绍三种广为人知的超参数搜索变体；其中包括`pytorch`，因为其实现简短，足够在一章内介绍。让我们从连续二分法开始。
- en: Searching neural architectures by using successive halving
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用连续二分法搜索神经网络架构
- en: The most basic method to search with a reduced search space to optimize runtime
    is to randomly sample a few hyperparameter configurations and execute the full
    training and evaluation only of the sampled configurations. This method is simply
    called **random search**. What if we know that certain configurations are almost
    certain to perform badly after a certain quantity of resources are consumed?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的通过缩小搜索空间来优化运行时间的方法是随机采样一些超参数配置，仅对采样的配置执行完整的训练和评估。这种方法简单地称为**随机搜索**。如果我们知道某些配置在消耗一定资源后几乎肯定会表现差呢？
- en: Successive halving is an extension of random search that helps to save resources
    while searching for the best neural architecture. The idea behind successive halving
    is to eliminate half of the poorly performing configurations at each step, allowing
    us to focus on the more promising ones. This way, we don’t waste time on configurations
    that are less likely to yield good results.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 连续二分法是随机搜索的一种扩展，它有助于在寻找最佳神经网络架构时节省资源。连续二分法的理念是在每一步中淘汰表现不佳的一半配置，从而使我们能够集中精力在更有前景的配置上。这样，我们就不会浪费时间在那些不太可能产生良好结果的配置上。
- en: Let’s break down the concept using a simple example. Imagine you are trying
    to find the best configuration for an MLP with varying hyperparameters such as
    the number of layers and layer sizes. You start by randomly sampling 100 different
    configurations. Now, instead of training all 100 configurations to completion,
    you apply successive halving. You train each of the 100 configurations for a short
    period (for example, 5 epochs) and then evaluate their performance on a validation
    dataset. At this point, you eliminate the 50 worst-performing configurations and
    continue training the remaining 50 configurations for another 5 epochs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来分解这个概念。假设你正在尝试为一个 MLP 找到最佳配置，该配置包含不同的超参数，如层数和每层的大小。你从随机抽取 100 个不同的配置开始。现在，不是将所有
    100 个配置训练到完成，你应用了逐步缩减（successive halving）。你将每个配置训练一段短时间（例如 5 个 epoch），然后评估它们在验证集上的表现。此时，你会淘汰表现最差的
    50 个配置，继续训练剩下的 50 个配置，再进行 5 个 epoch。
- en: After this second round of training, you again evaluate the performance of the
    remaining configurations and eliminate the 25 worst-performing ones. The top 25
    configurations can then continue to train until convergence. By applying successive
    halving, you save resources and time by focusing on the most promising configurations
    while discarding the poorly performing ones early in the process. This allows
    you to more efficiently search for the best neural architecture for your problem.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二轮训练后，你再次评估剩余配置的性能，并淘汰表现最差的 25 个配置。然后，前 25 个配置可以继续训练，直到收敛。通过应用逐步缩减，你通过专注于最有前景的配置，同时早期淘汰表现不佳的配置，节省了资源和时间。这使得你能够更高效地寻找问题的最佳神经网络架构。
- en: 'Let’s dive into the technical implementation of successive halving that will
    also set the stage for all the other methods under general hyperparameter tuning-based
    NAS:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入研究逐步缩减的技术实现，这将为基于超参数调整的 NAS 下的所有其他方法奠定基础：
- en: 'Let’s start by importing the relevant libraries and setting the `pytorch` library
    seed to ensure reproducibility:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先导入相关的库，并设置`pytorch`库的随机种子，以确保结果的可复现性：
- en: '[PRE0]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, let’s define a `pytorch` MLP class that has the functionality to build
    an MLP dynamically based on the number of hidden layers and hidden sizes of each
    layer:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们定义一个`pytorch` MLP 类，该类具有根据隐藏层的数量和每层的隐藏层大小动态构建 MLP 的功能：
- en: '[PRE1]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we need the logic that trains this MLP when provided with a specific
    layer configuration list. Here, we will use the `pytorch` abstraction library
    called `catalyst` to train the model and save the best and last epoch model with
    a few convenient methods:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要为该 MLP 编写训练逻辑，并提供一个特定的层配置列表。在这里，我们将使用名为`catalyst`的`pytorch`抽象库来训练模型，并通过一些便捷的方法保存最佳和最后的训练模型：
- en: '[PRE2]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we need a method that generates a random number of hyperparameters for
    the MLP. The hyperparameter is structured to be a list of hidden layer size specifications
    where the number of items in the list determines the number of layers. We fix
    the range number of hidden layers to be between 1 and 6 layers and hidden layer
    sizes to be between 2 and 100:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个方法来生成 MLP 的随机超参数。超参数结构为隐藏层大小的规格列表，其中列表中的项数决定了层数。我们将隐藏层的数量固定在 1 到 6
    层之间，隐藏层的大小则在 2 到 100 之间：
- en: '[PRE3]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, with the helpers defined, let’s set up our tabular dataset to apply MLP
    with. The `iris` dataset from `scikit-learn` will be used here. We will load it,
    scale the values, split the dataset into train and validation partitions, and
    prepare it to be consumed by the `catalyst` library. Note that the code up until
    this step will be reused for the next two methods:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，随着助手函数的定义完成，让我们设置我们的表格数据集来应用 MLP。这里将使用来自`scikit-learn`的`iris`数据集。我们将加载数据集，缩放数据值，将数据集划分为训练集和验证集，并为`catalyst`库做准备。请注意，直到此步骤的代码将在接下来的两个方法中重复使用：
- en: '[PRE4]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The approach we are going to take here with successive halving is to use epochs
    as the resource component where we will execute three iterations of successive
    halving once a predefined number of epochs for that iteration has been executed.
    Half of the top-performing configurations will continue to be trained in the next
    iteration. Here, we use 20 initial configurations and 3 iterations of successive
    halving with 5 epochs each. Let’s start by defining these values along with the
    seeded random number generator that controls the randomness of the generated configurations:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这里采用的连续二分方法是，将 epoch 作为资源组件，在预定义的 epoch 数量执行完后，执行三次连续二分迭代。表现最好的配置的前一半将在下一次迭代中继续训练。这里，我们使用
    20 个初始配置，每个配置进行 3 次连续二分，每次 5 个 epoch。让我们首先定义这些值，并定义控制生成配置随机性的随机数生成器：
- en: '[PRE5]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we will define the execution logic for successive halving. Note that
    the last trained epoch weights are used here, at the next iteration, instead of
    the epoch with the best validation score:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将定义连续二分的执行逻辑。请注意，这里使用的是最后训练的 epoch 权重，而不是具有最佳验证得分的 epoch，下一次迭代时会使用这些权重：
- en: '[PRE6]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The best configuration can then be found via the following logic:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最佳配置可以通过以下逻辑来找到：
- en: '[PRE7]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In successive halving, some configurations might only be performant at the later
    stages of the training process, while some configurations can be performant from
    the early stages of the training process. Choosing either a longer wait time or
    a faster wait time will put some models at a disadvantage and requires finding
    a balance that we might not know the truth about. The Hyperband method that will
    be introduced next is an attempt to solve this issue.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在连续二分中，一些配置可能只有在训练过程的后期阶段才能表现出色，而一些配置则可以在训练过程的早期阶段就表现出色。选择更长或更短的等待时间将使一些模型处于不利地位，并且需要找到一种我们可能尚不知道真相的平衡。接下来将介绍的
    Hyperband 方法正是尝试解决这一问题。
- en: Searching neural architectures by using Hyperband
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Hyperband 搜索神经网络架构
- en: 'Hyperband improves upon the caveats in successive halving by executing multiple
    separate end-to-end iterations of successive halving called brackets. Each consecutive
    bracket would have smaller original sample configurations but has a higher number
    of resources allocated. This algorithm essentially allows some randomly sampled
    configurations to be trained longer, increasing the probability that the inherent
    potential for good performance is shown so that abandoning these configurations
    won’t be a waste at the later brackets. The full algorithm is shown in *Figure
    7**.1*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Hyperband 通过执行多个独立的端到端连续二分迭代（称为括号），改进了连续二分中的局限性。每个连续的括号将拥有更小的初始样本配置，但分配的资源更多。该算法本质上允许一些随机抽样的配置训练更长时间，从而增加了展示良好性能的固有潜力的概率，因此在后续的括号中放弃这些配置不会是浪费。完整的算法在*图
    7.1* 中展示：
- en: '![Figure 7.1 – Hyperband algorithm pseudocode](img/B18187_07_1.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – Hyperband 算法伪代码](img/B18187_07_1.jpg)'
- en: Figure 7.1 – Hyperband algorithm pseudocode
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – Hyperband 算法伪代码
- en: 'Two user input configurations are needed for this algorithm: specifically,
    R, the maximum amount of resources to train and evaluate a single configuration,
    and η, the divider number that decides the number of configurations to keep at
    the end of every successive halving iteration. The total number of brackets, s max,
    the total resource allocated for each bracket, B, the total number of configurations
    by bracket and iteration n and n i, and the resource allocated by brackets r i,
    are all computed by formula. To make this easier to digest, *Figure 7**.2* shows
    the example Hyperband resulting configuration results in each bracket when R =
    81, and η = 3:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法需要两个用户输入配置：具体而言，R，表示训练和评估单个配置的最大资源量；以及 η，表示决定每次连续二分迭代结束时保留配置数量的除数。每个括号的总数
    s_max、每个括号分配的总资源 B、每个括号和迭代的总配置数量 n 和 n_i 以及每个括号分配的资源 r_i 都通过公式计算。为了更容易理解，*图 7.2*
    显示了当 R = 81 且 η = 3 时，每个括号中 Hyperband 结果配置示例：
- en: '![Figure 7.2 – Example Hyperband resulting configuration results in each bracket](img/B18187_07_2.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 每个括号中 Hyperband 结果配置示例](img/B18187_07_2.jpg)'
- en: Figure 7.2 – Example Hyperband resulting configuration results in each bracket
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 每个括号中 Hyperband 结果配置示例
- en: These settings produce a total of 5 brackets and produce a total of 10 final
    models. The best model out of these 10 models would then be used as the final
    produced model from the search operation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设置生成总共 5 个括号并生成总共 10 个最终模型。这 10 个模型中的最佳模型将作为搜索操作生成的最终模型。
- en: 'Note that, in this method, expert knowledge can be explicitly injected into
    the process of the two search methods by, for example, fixing the macroarchitecture
    of the model and only searching the hyperparameters for a microarchitecture logical
    block. Let’s go through an implementation of Hyperband using the methods and dataset
    defined in the successive halving topic:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在此方法中，可以通过将模型的宏架构固定并仅搜索微架构逻辑块的超参数，将专家知识显式地注入到两种搜索方法的过程中。让我们通过在成功的减半主题中定义的方法和数据集实现
    Hyperband 来进行实现：
- en: 'First, let’s define the additional library needed here to compute logarithms:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们定义在这里计算对数所需的额外库：
- en: '[PRE8]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we will define the two input parameters needed for the Hyperband implementation,
    the maximum resource we want to run per configuration in terms of epochs, and
    the divisor of configurations, `N`, after every successive halving operation in
    the Hyperband algorithm:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义 Hyperband 实现所需的两个输入参数，即我们希望在每个配置的最大资源（以 epochs 表示），以及在 Hyperband 算法中的每个成功的减半操作后的配置除数`N`：
- en: '[PRE9]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we will define the main logic of Hyperband according to *Figure 7**.1*:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将根据*图 7.1*定义 Hyperband 的主要逻辑：
- en: '[PRE10]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The two methods utilize random search heavily without being very smart about
    choosing the hyperparameters’ configuration. Next, we will explore a search method
    that optimizes the next choice of hyperparameters after some initial searching
    has been done.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法在选择超参数配置时都广泛使用随机搜索，而不是很聪明。接下来，我们将探讨一种搜索方法，在进行了一些初始搜索后，优化下一个超参数选择的方法。
- en: Searching neural architectures by using Bayesian hyperparameter optimization
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用贝叶斯超参数优化搜索神经架构
- en: 'Bayesian hyperparameter optimization is a method that utilizes a surrogate
    performance estimation model to choose an estimated best set of configurations
    to sample and evaluate from. The act of sampling configurations to train and evaluate
    is formally called the **acquisition function**. Instead of random sampling and
    hoping that it’ll perform well, Bayesian optimization attempts to leverage the
    prior information gained from an initial random configuration sampling and actual
    training and evaluation to find new configurations that are estimated to perform
    well. Bayesian optimization takes the following steps:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯超参数优化是一种利用代理性能估计模型选择估计最佳配置集以从中采样和评估的方法。正式地说，采样配置来训练和评估称为**采集函数**。贝叶斯优化尝试利用从初始随机配置采样和实际训练评估中获得的先验信息，找到预计表现良好的新配置。贝叶斯优化的步骤如下：
- en: Sample a number of hyperparameter configurations.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对一些超参数配置进行采样。
- en: Perform full training and evaluation with these configurations to obtain performance
    scores.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些配置进行完整的训练和评估，以获取性能分数。
- en: Train a surrogate regression model (typically, a **Gaussian processes** (**GP**)
    model is used) with all the available data to estimate the performance scores
    based on the hyperparameters.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所有可用数据训练一个代理回归模型（通常使用**高斯过程**（**GP**）模型）来估计基于超参数的性能分数。
- en: Either use all possible hyperparameter configurations or randomly sample a good
    amount of hyperparameter configurations and predict the performance score using
    the surrogate model.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要么使用所有可能的超参数配置，要么随机采样大量超参数配置，并使用代理模型预测性能分数。
- en: Get the *k* hyperparameter configurations that have the minimum estimated performance
    scores from the surrogate model.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从代理模型中获取具有最小估计性能分数的*k*个超参数配置。
- en: Repeat *step 1* to *step 5* either a predetermined number of times, until a
    good enough result is obtained, or until your resource budget is all used up.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤 1*到*步骤 5*，要么预先确定的次数，直到获得足够好的结果，要么直到资源预算用完为止。
- en: 'The process essentially attempts to speed up the actual training and evaluation
    process by estimating the scores it will produce and only actually training the
    estimated top few configurations. The optimization only works if the surrogate
    model performance score estimation function is considerably faster than the actual
    training and evaluation of the main model. Note that the standard Bayesian optimization
    works only in the continuous space and can’t deal with discrete hyperparameters
    properly. Let’s go through the technical implementation of Bayesian optimization-based
    NAS using the same methods and dataset defined earlier:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程本质上试图通过估计将要产生的分数来加速实际的训练和评估过程，只有在估算的前几个最佳配置进行实际训练时才会进行训练。优化只有在代理模型性能评分估计函数显著快于实际训练和评估主模型时才有效。请注意，标准的贝叶斯优化只适用于连续空间，无法正确处理离散超参数。让我们通过之前定义的相同方法和数据集来回顾基于贝叶斯优化的NAS的技术实现：
- en: 'Let’s start by importing the main powerhouse behind the Bayesian optimization
    approach here, which is the GP regressor from `scikit-learn`:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从导入贝叶斯优化方法背后的主要工具开始，这就是`scikit-learn`中的GP回归器：
- en: '[PRE11]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, let’s define the method that creates a structured fixed-sized column
    of 6, which is the maximum number of possible layers defined earlier. When there
    are fewer than 6 layers, the later columns will just have 0 layers as a feature:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们定义一个方法，创建一个结构化的固定大小为6的列，这是之前定义的最大可能层数。当层数少于6时，后面的列将只具有0层作为特征：
- en: '[PRE12]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, let’s define three important parameters for Bayesian optimization-based
    NAS using MLP. The first parameter is the number of configurations. The approach
    we are taking here is to initially train 100 configurations in the first iteration
    according to the specified epochs per configuration. After that, we build a GP
    regressor to predict the validation loss. Then, we will sample configurations
    in the next few iterations and use the model to predict and pick the top five
    configurations to perform full training. In every iteration, a new regressor model
    is built with all the available validation loss data:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们定义用于基于贝叶斯优化的NAS与MLP的三个重要参数。第一个参数是配置数。我们在这里采取的方法是根据每个配置的指定训练轮次，在第一次迭代时初步训练100个配置。之后，我们建立一个GP回归器来预测验证损失。然后，我们将在接下来的几次迭代中采样配置，并使用模型进行预测，选择前五个配置进行完整训练。在每次迭代中，都会使用所有可用的验证损失数据来构建一个新的回归器模型：
- en: '[PRE13]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, let’s define the main logic that accomplishes a version of Bayesian
    optimization-based NAS with MLP:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们定义实现基于贝叶斯优化的NAS版本的主要逻辑，使用MLP：
- en: '[PRE14]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: With that, we’ve achieved MLP hyperparameter search with Bayesian optimization!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，我们已经通过贝叶斯优化实现了MLP超参数搜索！
- en: 'In addition to the hyperparameter search-based NAS methods discussed in this
    chapter, it is worth mentioning three other approaches, which are hierarchical
    search, proxy models, and evolutionary algorithms:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 除了本章讨论的基于超参数搜索的NAS方法外，值得一提的还有三种其他方法：分层搜索、代理模型和进化算法：
- en: Hierarchical search focuses on optimizing architectures at different levels
    of granularity, allowing for a more efficient exploration of the search space
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分层搜索专注于在不同粒度级别上优化架构，允许更高效地探索搜索空间。
- en: Proxy models serve as lightweight approximations of the target models, reducing
    the computational cost of evaluating candidate architectures during the search
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理模型作为目标模型的轻量级近似，减少了在搜索过程中评估候选架构的计算成本。
- en: Lastly, evolutionary algorithms are inspired by natural selection processes
    and can be applied to the NAS problem, enabling the exploration and optimization
    of architectures through mutation, crossover, and selection operations
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，进化算法受自然选择过程的启发，可以应用于NAS问题，通过变异、交叉和选择操作来探索和优化架构。
- en: These methods can also be considered when choosing among hyperparameter search-based
    NAS techniques.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择基于超参数搜索的NAS技术时，这些方法也可以作为参考。
- en: NAS with general hyperparameter search methods provides a simple way to search
    different configurations in a smarter way than just plain random or brute-force
    search. It provides the most help when you already have the infrastructure ready
    to choose different hyperparameters easily, along with the expert knowledge from
    the field already built in under the hood.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用通用超参数搜索方法的NAS提供了一种比纯粹的随机搜索或暴力搜索更智能的配置搜索方式。它最有帮助的时机是当你已经具备了基础设施，能够轻松选择不同的超参数，并且已经将该领域的专家知识内嵌在系统中。
- en: However, NAS with general hyperparameter search generally requires a lot of
    out-of-algorithm tooling for building the model and formalizing the helper methods
    that can reliably be controlled by hyperparameters. On top of that, it is still
    required to have quite a bit of knowledge of which types of layers to use along
    with out-of-algorithm crafting of the macro- and microarchitecture of the NN model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用通用超参数搜索的NAS通常需要大量超出算法范围的工具来构建模型并正式化那些可以通过超参数可靠控制的辅助方法。除此之外，仍然需要掌握一定的知识，知道使用哪种类型的层，以及如何超出算法外构建神经网络（NN）模型的宏观和微观架构。
- en: In the next section, we will go through a line of NAS methods that covers more
    extensively all the steps needed to achieve NAS for any NN, called RL-based NAS.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍一系列NAS方法，这些方法更全面地涵盖了实现任何NN的NAS所需的所有步骤，称为基于RL的NAS。
- en: Understanding RL-based NAS
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解基于RL的NAS
- en: 'RL is a family of learning algorithms that deal with the learning of a policy
    that allows an agent to make consecutive decisions on its actions while interacting
    with states in an environment. *Figure 7**.3* shows a general overview of RL algorithms:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: RL是一类学习算法，旨在学习一种策略，使得智能体在与环境中的状态交互时能够做出连续的行动决策。*图 7.3*展示了RL算法的总体概述：
- en: '![Figure 7.3 – General overview of RL algorithms](img/B18187_07_3.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – RL算法的总体概述](img/B18187_07_3.jpg)'
- en: Figure 7.3 – General overview of RL algorithms
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – RL算法的总体概述
- en: This line of algorithms is most popularly utilized to create intelligent bots
    for games that can act as offline players against real humans. In the context
    of a digital game, the environment represents the entire setting in which the
    agent operates, including aspects such as the position and status of the in-game
    character, as well as conditions of the in-game world. The state, on the other
    hand, is a snapshot of the environment at a given time, reflecting the current
    conditions of the game. One key component in RL is the environment feedback component
    that can provide either a reward or punishment. In digital games, examples of
    rewards and punishments are some forms of a competitive scoring system, in-game
    cash, the leveling system, or sometimes negatively through death. When applied
    to the realm of NAS, the state will then be the generated NN architecture, and
    the environment will be the evaluation of the generated NN configurations. The
    rewards and punishments will then be the latency performance and metric performance
    of the resulting architecture after training and evaluating it on a chosen dataset.
    Another key component is the term **policy**, which is the component responsible
    for producing an action based on the state.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类算法最常用于创建智能机器人，这些机器人可以作为离线玩家与真实人类对战。在数字游戏的背景下，环境代表了智能体操作的整个设置，包括诸如游戏角色的位置和状态，以及游戏世界的条件等方面。另一方面，状态是给定时刻环境的快照，反映了游戏的当前状况。RL的一个关键组成部分是环境反馈组件，它可以提供奖励或惩罚。在数字游戏中，奖励和惩罚的例子包括某种形式的竞争性评分系统、游戏内现金、升级系统，或有时通过死亡来进行负面惩罚。当应用到NAS领域时，状态将是生成的NN架构，而环境将是对生成的NN配置的评估。奖励和惩罚则是训练并在选定数据集上评估后的架构的延迟性能和度量性能。另一个关键组成部分是**策略**，即负责根据状态产生行动的组件。
- en: Recall that in the general hyperparameter search-based NAS, the NN configuration
    sample acquisition is based on random sampling. In RL-based NAS approaches, the
    goal is not only to optimize the search process but also the acquisition process
    that produces NN configurations based on prior experiences. The exact methods
    of how the configurations can be produced, however, differ in different RL-based
    NAS methods.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在基于通用超参数搜索的NAS中，NN配置样本的获取是基于随机采样的。在基于RL的NAS方法中，目标不仅是优化搜索过程，还包括基于先前经验产生NN配置的获取过程。然而，如何产生这些配置的方法在不同的基于RL的NAS方法中有所不同。
- en: 'In this section, we will dive into a few RL methods specific to NAS:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将深入研究与NAS相关的几种RL方法：
- en: Founding NAS based on the RL method
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于RL方法的创始NAS
- en: '**Efficient NAS** (**ENAS**) via parameter sharing'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高效NAS**（**ENAS**）通过参数共享'
- en: '**Mobile** **NAS** (**MNAS**)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mobile NAS**（**MNAS**）'
- en: Let’s start with the first founding NAS based on the RL method.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从基于RL方法的第一个创立的NAS开始。
- en: Understanding founding NAS based on RL
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解基于RL的创始NAS
- en: 'RL can be implemented with NNs, and in the use case of NAS, a **recurrent NN**
    (**RNN**) is used to act as the missing piece needed to probabilistically generate
    the main NN configurations at test time. *Figure 7**.4* roughly shows an architectural
    overview of the foundational NAS with the RL method:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: RL可以通过神经网络实现，在NAS的使用案例中，**递归神经网络**（**RNN**）用作在测试时间概率生成主NN配置所需的关键部分。*Figure 7**.4*
    简略展示了采用RL方法的基础NAS的架构概述：
- en: '![Figure 7.4 – Architectural overview of NAS with RL workflow](img/B18187_07_4.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 带有RL工作流的NAS的架构概述](img/B18187_07_4.jpg)'
- en: Figure 7.4 – Architectural overview of NAS with RL workflow
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 带有RL工作流的NAS的架构概述
- en: 'The RNN is the policy that determines the state after a learning step from
    the previous environment interaction. In the case of NAS, the action is equivalent
    to the state. Recall that an RNN is composed of multiple sequential recurrent-based
    cells where each cell is capable of producing an intermediate sequential output.
    In NAS, these intermediate sequential outputs are designed to predict specific
    configurations for the main NN. The predictions are then fed into the next RNN
    cell as a cell input. Consider the NAS task to search for the best CNN architecture
    in the image domain. *Figure 7**.5* shows the structure of this task using the
    RNN-based NN configuration predictions:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: RNN是决定从先前环境交互中学习步骤后状态的策略。在NAS的情况下，动作等同于状态。回想一下，RNN由多个顺序的递归单元组成，其中每个单元能够产生一个中间的顺序输出。在NAS中，这些中间的顺序输出被设计用来预测主NN的特定配置。然后，这些预测被作为单元输入馈送到下一个RNN单元中。考虑到在图像领域搜索最佳CNN架构的NAS任务。*Figure
    7**.5* 展示了使用基于RNN的NN配置预测结构的任务结构：
- en: "![\uFEFFFigure 7.5 – LSTM-based CNN layer configurations prediction for NAS](img/B18187_07_5.jpg)"
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – NAS的基于LSTM的CNN层配置预测](img/B18187_07_5.jpg)'
- en: Figure 7.5 – LSTM-based CNN layer configurations prediction for NAS
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – NAS的基于LSTM的CNN层配置预测
- en: 'A convolutional layer has a few specifications that need to be decided: namely,
    the number of convolutional filters, the size of the convolutional filters, and
    the size of the stride. *Figure 7**.5* shows the predictions for a single CNN
    layer. For subsequent CNN layers, the same **long short-term memory** (**LSTM**)
    cells are repeatedly sequentially predicted with the state and cell outputs from
    the last LSTM cell as input. For a four-layered CNN, the LSTM would then be autoregressively
    executed four times to obtain all required configuration predictions.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层有几个需要决定的规格：卷积滤波器的数量、卷积滤波器的大小和步幅的大小。*Figure 7**.5* 展示了单个CNN层的预测。对于后续的CNN层，相同的**长短期记忆**（**LSTM**）单元被重复顺序预测，上一个LSTM单元的状态和单元输出作为输入。对于四层CNN，LSTM将自回执行四次以获取所有所需的配置预测。
- en: 'As for how the parameters of the LSTM are updated, a process called **policy
    gradient** will be used. Policy gradient is a group of methods that uses gradients
    to update the policy. Specifically, the **reinforce** rule is used here to compute
    the gradients for updating the parameters. In more understandable terminology,
    the following formula shows how the gradients are computed:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM的参数更新方式采用一种称为**策略梯度**的过程。策略梯度是一组使用梯度来更新策略的方法。具体而言，这里使用**reinforce**规则来计算更新参数的梯度。更易理解的术语是，以下公式展示了如何计算梯度：
- en: gradients = average of (cross entropy loss x (reward − moving average of previous
    rewards)) for all sampled architectures
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度 = （所有抽样结构的交叉熵损失 x （奖励 − 先前奖励的移动平均）的平均）
- en: The cross-entropy loss here is specifically used to emphasize that the configuration
    prediction tasks are framed as a multiclass classification problem so that the
    number of search parameters can be constrained to a small number while making
    sure boundaries are set. For example, you wouldn’t want a million filters for
    a single CNN layer or a million-neurons-sized fully connected layers in an MLP.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的交叉熵损失专门用于强调配置预测任务被框架为一个多类分类问题，这样可以将搜索参数的数量限制在一个较小的范围内，同时确保边界被设置。例如，你不希望一个CNN层使用一百万个滤波器，或者一个MLP中的全连接层有一百万个神经元。
- en: The RL process here is guided by the concept of exploration versus exploitation.
    If we continue to use only the predicted states of the RNN and use that as the
    labels for computing the cross-entropy loss, the policy will just become more
    and more biased toward its own parameters. Using the RNN predictions as labels
    is known as the *exploitation process*, where the idea is to just allow the RNN
    to be more confident about its own predictions. This process grows the model deeper
    toward its current intelligence instead of toward intelligence that can be gained
    from external data exploration. *Exploration* here is when network configurations
    are randomly sampled to act as the label for the cross-entropy loss at each RNN
    cell. The idea here is to start with lots of exploration and slowly reduce exploration
    going into later stages of policy learning and depending more on exploitation.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的强化学习（RL）过程是由探索与利用的概念引导的。如果我们仅仅继续使用RNN的预测状态，并将其作为计算交叉熵损失的标签，策略就会变得越来越偏向其自身的参数。将RNN预测作为标签被称为*利用过程*，其理念是让RNN对其自身的预测更有信心。这个过程使得模型在当前的智能水平上变得更加深刻，而不是朝着可以通过外部数据探索获得的智能方向发展。这里的*探索*是指在每个RNN单元中，网络配置是随机采样的，用来作为交叉熵损失的标签。这里的理念是从大量的探索开始，并逐渐减少探索，进入策略学习的后期，更多依赖于利用。
- en: Until now, the steps only allow for a relatively simple form of CNN, but modifications
    can be added to the RNN agent to account for more complex CNN builds, such as
    parallel connections or skip connections from ResNet or DenseNet. In the original
    method, the addition of skip connections for complexity is attempted where an
    additional cell is added at the end of the five sequential RNN cells shown in
    *Figure 7**.5* to act as something called the `tanh` activation function, multiplied
    by a learnable weight, and finally applied with a sigmoid activation function
    that bounds the output values in between `0` to `1`. The key information here
    is that the sigmoid function provides a probabilistic value that allows a binary
    classification task of “to add a skip connection or not” to be executed. A `0.5`
    value can be used to determine whether the output is `1` or `0`. One problem,
    however, is that the size of the outputs between different layers might not be
    compatible. A trick is automatically applied to solve the incompatibility by padding
    smaller output feature maps with zeros so that both feature maps have the same
    size.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这些步骤仅允许相对简单的CNN形式，但可以向RNN代理中添加修改，以支持更复杂的CNN构建，例如来自ResNet或DenseNet的并行连接或跳跃连接。在原始方法中，尝试通过在*图7.5*中展示的五个顺序RNN单元的末尾添加一个额外的单元来增加跳跃连接的复杂性，作为一种叫做`tanh`的激活函数，再乘以一个可学习的权重，最后应用一个sigmoid激活函数，将输出值限定在`0`到`1`之间。这里的关键信息是，sigmoid函数提供一个概率值，允许执行“是否添加跳跃连接”的二分类任务。可以使用`0.5`的值来决定输出是`1`还是`0`。然而，一个问题是，不同层之间的输出大小可能不兼容。为了解决这个不兼容问题，会自动应用一种技巧，通过用零填充较小的输出特征图，以确保两个特征图具有相同的大小。
- en: 'This method allows you to dynamically add skip connections to a CNN in NAS.
    *Figure 7**.6* shows the final architecture obtained from this NAS method using
    the `CIFAR-10` image dataset:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法允许你在NAS中动态地向CNN添加跳跃连接。*图7.6*展示了使用`CIFAR-10`图像数据集，通过这种NAS方法获得的最终架构：
- en: '![Figure 7.6 – CNN obtained through NAS with RL from the https://arxiv.org/abs/1611.01578v2
    paper](img/B18187_07_6.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – 从https://arxiv.org/abs/1611.01578v2论文中通过NAS与RL获得的CNN](img/B18187_07_6.jpg)'
- en: Figure 7.6 – CNN obtained through NAS with RL from the https://arxiv.org/abs/1611.01578v2
    paper
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 从https://arxiv.org/abs/1611.01578v2论文中通过NAS与RL获得的CNN
- en: The architecture, although simple, is capable of deciding the best skip connections
    needed to achieve a good result. This resulting architecture shows how complex
    an architecture can be and shows how hard it would be for a human to design this
    outcome manually without proper searching algorithms. Note again that any complexity
    and modifications can be added to the RNN policy to account for additional components
    such as the learning rate, pooling method, normalization method, or activation
    methods, which emphasizes the flexibility of the idea. Additionally, the NAS method
    can also be applied to search MLP or RNN main NNs. These additional adaptations
    and complexities, however, won’t be covered here.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管架构简单，但它能够决定实现良好结果所需的最佳跳跃连接。由此产生的架构展示了架构的复杂性，也表明如果没有适当的搜索算法，人类手动设计出这种结果将是多么困难。再次指出，任何复杂性和修改都可以添加到
    RNN 策略中，以考虑额外的组件，如学习率、池化方法、归一化方法或激活方法，这突显了这一思路的灵活性。此外，NAS 方法还可以应用于搜索 MLP 或 RNN
    主神经网络。然而，这些额外的适配和复杂性在此不作讨论。
- en: Note that this technique fixed the microarchitecture structure in the sense
    that a standard convolutional layer is used. The technique, however, enabled some
    form of macroarchitecture designing by allowing skip connections. One of the main
    problems of this foundational technique is the time needed to evaluate the randomly
    generated or predicted architecture configurations. Next, we will explore a method
    that attempts to minimize this problem.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这项技术固定了微架构结构，因为它使用了标准的卷积层。然而，该技术通过允许跳跃连接，支持某种形式的宏架构设计。该基础技术的一个主要问题是评估随机生成或预测的架构配置所需的时间。接下来，我们将探索一种旨在最小化此问题的方法。
- en: Understanding ENAS
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 ENAS
- en: ENAS is a method that extends foundational NAS with the RL method by making
    the evaluation of generated architectures more efficient. Additionally, ENAS provides
    two different methods that allow either the macroarchitecture or microarchitecture
    to be searched. Parameter sharing is a concept that relates to **transfer learning**
    (**TL**), where what is learned from one task can be transferred to another task
    and fine-tuned for that subsequent task to get better results. Training and evaluating
    the main child architectures in this way provides an obvious way to speed up the
    process. However, it does rely heavily on weights pre-trained from the previous
    architectures and doesn’t provide an unbiased evaluation of the final searched
    architecture even if it performs well. Regardless, the method still proves to
    be valuable when combined with the novel search space.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ENAS 是一种扩展基础 NAS 和 RL 方法的技术，通过提高生成架构的评估效率。此外，ENAS 提供了两种不同的方法，允许搜索宏架构或微架构。参数共享是一个与
    **迁移学习**（**TL**）相关的概念，即从一个任务中学到的知识可以转移到另一个任务，并对该任务进行微调，以获得更好的结果。以这种方式训练和评估主要子架构提供了一种明显的加速过程的方法。然而，它在很大程度上依赖于从先前架构预训练的权重，并且即使最终搜索到的架构表现良好，也无法提供对最终架构的无偏评估。尽管如此，这种方法在与新颖的搜索空间结合时仍然证明是有价值的。
- en: 'ENAS applies RL using an RNN as well but does so in an entirely different searching
    direction and predicting different components with its RNN. The search space used
    by ENAS is through a single **directed acyclic graph** (**DAG**) where the number
    of nodes determines the number of layers of the child architecture. *Figure 7**.7*
    shows an example of a four-node DAG:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ENAS 也通过 RNN 应用 RL，但其搜索方向完全不同，并且通过 RNN 预测不同的组件。ENAS 使用的搜索空间是通过一个 **有向无环图**（**DAG**）实现的，其中节点的数量决定了子架构的层数。*图
    7.7* 显示了一个四节点 DAG 的示例：
- en: '![Figure 7.7 – A four-node DAG representing the search space of ENAS](img/B18187_07_7.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – 一个四节点的有向无环图（DAG）表示 ENAS 的搜索空间](img/B18187_07_7.jpg)'
- en: Figure 7.7 – A four-node DAG representing the search space of ENAS
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 一个四节点的有向无环图（DAG）表示 ENAS 的搜索空间
- en: 'The RNN will then act as the controller that predicts two components for any
    architecture type: namely, which previous nodes to connect to and which computation
    operation to use. The RNN in this case will autoregressively predict the two components
    four times to account for four nodes. The red lines in *Figure 7**.7* show the
    predicted previous nodes to connect to. There will be a fixed number of computation
    operations that can be chosen for every node. Since there will be a random sampling
    of the computation operation procedure to ensure an unbiased trajectory, the procedure
    will be based on the same search space. The parameter-sharing method is applied
    in the computation operation component for these nodes. After each training iteration,
    the weights for each computation operation at each layer will be saved for future
    parameter-sharing use. Parameter sharing works in a way that each computation
    operation at each node number will be used as an identifier to save and reload
    weights whenever it is used again at the same layer with the same computation
    operation.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，RNN将作为控制器，预测任何架构类型的两个组成部分：即，连接哪个先前节点和使用哪个计算操作。在这种情况下，RNN将自回归地预测这两个组成部分四次，以考虑四个节点。*图
    7.7*中的红线显示了预测的先前节点连接。每个节点将有一个固定的计算操作可以选择。由于会进行计算操作的随机抽样以确保轨迹不偏，程序将基于相同的搜索空间。参数共享方法应用于这些节点的计算操作部分。在每次训练迭代后，每个层的每个计算操作的权重将被保存，以供将来进行参数共享使用。参数共享的工作方式是，每个节点号上的每个计算操作将作为标识符，在同一层和相同计算操作下再次使用时，保存并重新加载权重。
- en: 'ENAS can be applied to search for RNN architectures, CNN architectures, and
    MLP architectures and is generally extensible to any other architecture types.
    Let’s take the case of searching for CNN architectures for ENAS. For CNN, ENAS
    introduced two methods for searching; the first is to perform a macroarchitecture
    search, and the second is to perform a microarchitecture search. For the macroarchitecture
    search, six operations were proposed, which consisted of convolutional filters
    with filter sizes of 3 x 3 and 5 x 5, depthwise-separable convolutions with filter
    sizes 3 x 3 and 5 x 5, and max pooling and average pooling of kernel size 3 x
    3\. This set of operations allows for more diversity instead of just the plain
    convolutional layer, but instead of allowing more dynamic values of convolutional
    layer configuration, the same configurations are set to fixed values. Another
    implementation detail here is that when more than one previous node is selected
    for connection, the outputs from the layers of the previous nodes are concatenated
    along their depth dimension before being sent to the layer of the current node.
    *Figure 7**.8* shows the result of a macroarchitecture search using the `CIFAR-10`
    dataset:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ENAS可以应用于搜索RNN架构、CNN架构和MLP架构，并且通常可以扩展到任何其他架构类型。以ENAS搜索CNN架构为例，对于CNN，ENAS引入了两种搜索方法；第一种是执行宏架构搜索，第二种是执行微架构搜索。在宏架构搜索中，提出了六种操作，这些操作包括大小为3
    x 3和5 x 5的卷积滤波器、大小为3 x 3和5 x 5的深度可分离卷积、以及大小为3 x 3的最大池化和平均池化。这一操作集提供了更多的多样性，而不仅仅是普通的卷积层，但它没有允许卷积层配置值的更多动态变化，而是将相同的配置设定为固定值。这里的另一个实现细节是，当选择多个先前节点进行连接时，先前节点各层的输出会在深度维度上进行连接，然后发送到当前节点的层。*图
    7.8*显示了使用`CIFAR-10`数据集进行宏架构搜索的结果：
- en: '![Figure 7.8 – Result of ENAS using the macroarchitecture search strategy](img/B18187_07_8.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 – 使用宏架构搜索策略的ENAS结果](img/B18187_07_8.jpg)'
- en: Figure 7.8 – Result of ENAS using the macroarchitecture search strategy
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 使用宏架构搜索策略的ENAS结果
- en: The result was achieved using only 0.32 days using an outdated NVIDIA GTX 1080
    Ti GPU, albeit on the `CIFAR-10` dataset instead of ImageNet, and achieved only
    a 3.87 error rate on the `CIFAR-10` validation dataset.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 该结果仅使用0.32天，通过一块过时的NVIDIA GTX 1080 Ti GPU在`CIFAR-10`数据集上实现，虽然没有使用ImageNet数据集，但在`CIFAR-10`验证集上仅取得了3.87的错误率。
- en: 'As for the microarchitecture search, the idea is to build low-level logical
    blocks and repeat the same logical blocks so that the architecture can be scaled
    easily. Two different logical blocks are searched in ENAS: a logical block consisting
    of the main convolutional operations, and a reduction logical block intended to
    reduce dimensionality. *Figure 7**.9* shows the macroarchitecture of the final
    architecture used to scale microarchitecture decisions in ENAS:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 至于微架构搜索，思路是构建低级逻辑块并重复相同的逻辑块，以便架构能够轻松扩展。ENAS中搜索了两种不同的逻辑块：一个由主要卷积操作组成的逻辑块，以及一个旨在减少维度的缩减逻辑块。*图7.9*展示了用于在ENAS中扩展微架构决策的最终架构的宏架构：
- en: '![Figure 7.9 – Final macroarchitecture structure to scale a microarchitecture
    logical block](img/B18187_07_009.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图7.9 – 扩展微架构逻辑块的最终宏架构结构](img/B18187_07_009.jpg)'
- en: Figure 7.9 – Final macroarchitecture structure to scale a microarchitecture
    logical block
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – 扩展微架构逻辑块的最终宏架构结构
- en: '*N* is a fixed number that stays constant throughout the search duration. As
    this is microarchitecture, the architecture construction process was made to allow
    more complex interactions between layers, specifically the addition of operations
    to the output of previous nodes in skip connections. Due to this, for the convolutional
    logical block, the RNN was adapted by fixing the first two RNN cells for a node
    to specify which two previous node indexes to connect to and the subsequent RNN
    cells to predict the computation operation to apply individually for the two chosen
    previous node indexes. As for the reduction logical block, the main idea is to
    choose any operation and use a stride of two, which effectively reduces the spatial
    dimension of its input by two. The reduction logical block can be predicted along
    with the convolutional logical block using the same number of nodes and the same
    RNN. The parameter-sharing method in the microarchitecture case is adapted from
    the general case equivalently by similarly using the layer number and computation
    type as an identifier to save and load trained child architecture weights. *Figure
    7**.10* shows the result of using the microarchitecture searching strategy in
    ENAS:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*N*是一个固定的数字，在整个搜索过程中保持不变。由于这是微架构，因此架构构建过程被设计成允许层之间进行更复杂的交互，特别是通过跳跃连接向前一节点的输出添加操作。由于这一点，对于卷积逻辑块，RNN被调整，通过固定前两个RNN单元来指定连接到哪两个前置节点索引，接下来的RNN单元则预测要对这两个选择的前置节点索引分别应用的计算操作。至于缩减逻辑块，主要思路是选择任何操作并使用步幅为2的卷积，这有效地将输入的空间维度缩小了一半。缩减逻辑块可以与卷积逻辑块一起预测，使用相同数量的节点和相同的RNN。微架构中的参数共享方法等效地从一般情况调整，通过类似的方式使用层数和计算类型作为标识符来保存和加载训练好的子架构权重。*图7.10*展示了在ENAS中使用微架构搜索策略的结果：'
- en: '![Figure 7.10 – Result of ENAS microarchitecture search strategy](img/B18187_07_10.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图7.10 – ENAS微架构搜索策略的结果](img/B18187_07_10.jpg)'
- en: Figure 7.10 – Result of ENAS microarchitecture search strategy
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – ENAS微架构搜索策略的结果
- en: The result was achieved in only 0.45 days using an outdated NVIDIA GTX 1080Ti
    GPU, albeit on the `CIFAR-10` dataset instead of ImageNet, and achieved only a
    2.89 error rate on the `CIFAR-10` validation dataset.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 该结果仅使用过时的NVIDIA GTX 1080Ti GPU，在`CIFAR-10`数据集上完成，虽然没有使用ImageNet数据集，且在`CIFAR-10`验证数据集上的误差率仅为2.89。
- en: Next, let’s go through the final RL-based NAS method, called MNAS.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们介绍基于强化学习的最终NAS方法，称为MNAS。
- en: Understanding MNAS
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解MNAS
- en: 'MNAS is the searching method that was used to create the CNN architecture called
    **MnasNet**, which is a CNN-based architecture. MNAS was later utilized to build
    the EfficientNet architecture family introduced in [*Chapter 3*](B18187_03.xhtml#_idTextAnchor051),
    *Understanding Convolutional Neural Networks*. However, the method can still be
    used to generate other architecture types such as RNN or MLP. MNAS’s main goal
    is to account for the latency component, which is the main concern for architectures
    meant to be run at the edge, or in mobile devices, as the name suggests. MNAS
    extends the RL NAS-based concept and introduces a search space that is more flexible
    than the microarchitecture search in ENAS, allowing the creation of more varied
    layers at different blocks, albeit with a fixed macroarchitecture. *Figure 7**.11*
    shows an MNAS fixed macroarchitecture layout with seven blocks while allowing
    different types of configurations with different layers in each block. The seven-block
    structure is adapted from the MobileNetV2 architecture:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: MNAS 是一种用于创建 CNN 架构的方法，称为 **MnasNet**，这是一种基于 CNN 的架构。MNAS 后来被用于构建在 [*第 3 章*](B18187_03.xhtml#_idTextAnchor051)《理解卷积神经网络》中介绍的
    EfficientNet 架构系列。然而，该方法仍然可以用于生成其他架构类型，如 RNN 或 MLP。MNAS 的主要目标是考虑延迟组件，这是用于边缘计算或移动设备架构时的主要关注点，正如其名称所示。MNAS
    扩展了基于 RL NAS 的概念，并引入了一个比 ENAS 中的微架构搜索更灵活的搜索空间，从而允许在不同模块中创建更多样化的层，尽管宏架构是固定的。*图
    7.11* 显示了一个 MNAS 固定宏架构布局，包含七个模块，同时允许每个模块中的不同层具有不同的配置。七模块结构来自于 MobileNetV2 架构：
- en: '![Figure 7.11 – Example of a fixed macroarchitecture of seven blocks along
    with the configurations for each block that will be randomly sampled or predicted](img/B18187_07_11.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.11 – 固定的七个模块宏架构示例，以及每个模块的配置，这些配置将被随机采样或预测](img/B18187_07_11.jpg)'
- en: Figure 7.11 – Example of a fixed macroarchitecture of seven blocks along with
    the configurations for each block that will be randomly sampled or predicted
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 – 固定的七个模块宏架构示例，以及每个模块的配置，这些配置将被随机采样或预测
- en: 'An RNN is used here as the controller network to predict the following configurations
    for each CNN block:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用 RNN 作为控制网络，以预测每个 CNN 模块的以下配置：
- en: '**Convolutional operation**: Standard Convolutional layer, depthwise convolutional
    layer, and the MBConv layer from MobileNet.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积操作**：标准卷积层、深度卷积层，以及来自 MobileNet 的 MBConv 层。'
- en: '**Convolutional filter size**: 3x3 and 5x5.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积滤波器大小**：3x3 和 5x5。'
- en: '**Squeeze-and-excitation ratio**: 0, 0.25.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**压缩与激励比率**：0，0.25。'
- en: '**Skip connection operation**: Pooling, identity residual, or no skip connection.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跳跃连接操作**：池化、恒等残差或没有跳跃连接。'
- en: '**The number of layers per block**: 0, +1, -1\. This is structured to be in
    reference to the number of layers in the same block numbers in MobileNetV2.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每个模块的层数**：0，+1，-1。该结构还参考了 MobileNetV2 中相同模块的层数。'
- en: '**Output filter size per layer**: 0.75, 1.0, 1.25\. This is also structured
    to be in reference to the filter size of the convolution layer at the same positions
    in MobileNetV2.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每层输出滤波器大小**：0.75，1.0，1.25。该结构还参考了 MobileNetV2 中相同位置卷积层的滤波器大小。'
- en: The search space introduced is crucial to allow for a more efficient network
    and higher capacity to achieve better metric performance. In CNN, for example,
    a lot of the computation is dominated at the earlier layers as the feature sizes
    are much larger and require more efficient layers compared to later layers.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 引入的搜索空间对提高网络效率和增加容量至关重要，以实现更好的度量性能。例如，在 CNN 中，大部分计算集中在较早的层，因为这些层的特征尺寸较大，相比于后期的层，需要更高效的层。
- en: 'A big issue about latency is that it is a component that is dependent on both
    the software and hardware environment. For example, let’s say architecture *A*
    is faster than architecture *B* in hardware and software *C*. When tested on another
    hardware and software *D*, it is possible for architecture *B* to be faster than
    architecture *A*. Additionally, the number of parameters and **floating-point
    operations per second** (**FLOPs**) specification of the architecture is also
    a proxy to the actual latency that depends also on the degree of parallelism of
    the architecture and the computational cores of the hardware. Based on these reasons,
    MobileNet adds the latency component to the reward computation by evaluating it
    objectively in the software and hardware environment of a mobile phone, combining
    both the metric computation and latency. *Figure 7**.12* shows an overview of
    the entire MNAS process:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 关于延迟的一个大问题是，它是一个依赖于软件和硬件环境的组件。例如，假设架构 *A* 在硬件和软件 *C* 上比架构 *B* 更快。当在另一个硬件和软件环境
    *D* 上测试时，架构 *B* 可能比架构 *A* 更快。此外，架构的参数数量和**每秒浮动点运算**（**FLOPs**）规格，也是延迟的一个代理，延迟还依赖于架构的并行度和硬件的计算核心。基于这些原因，MobileNet
    将延迟组件添加到奖励计算中，通过在手机的软件和硬件环境中客观评估它，结合度量计算和延迟。*图 7.12* 展示了整个 MNAS 过程的概览：
- en: '![Figure 7.12 – Overview of MNAS, a platform-aware NAS with a latency guarantee](img/B18187_07_12.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.12 – MNAS 概述，一种具有延迟保证的平台感知 NAS](img/B18187_07_12.jpg)'
- en: Figure 7.12 – Overview of MNAS, a platform-aware NAS with a latency guarantee
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 – MNAS 概述，一种具有延迟保证的平台感知 NAS
- en: 'Latency can be computed on the actual target software and hardware environment
    and is not restricted to just using mobile phones. The reward is computed using
    the formula with the capability to input the desired target latency in seconds:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟可以在实际的目标软件和硬件环境中计算，而不仅仅局限于使用手机。奖励是通过公式计算的，可以输入所需的目标延迟（以秒为单位）：
- en: reward = accuracy x [ latency _ target latency ] w
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: reward = accuracy x [ 延迟 - 目标延迟 ] ^ w
- en: 'Another key detail about MnasNet is that another policy gradient method called
    **proximal policy optimization** (**PPO**) by OpenAI was used to train the RNN
    policy network instead of the reinforce method. PPO is a method that accomplishes
    two things over the standard reinforcement policy gradient, namely:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: MnasNet 的另一个关键细节是，OpenAI 使用了另一种叫做**近端策略优化**（**PPO**）的策略梯度方法来训练 RNN 策略网络，而不是强化方法。PPO
    是一种方法，它相比标准的强化学习策略梯度，完成了以下两件事：
- en: Make smaller gradient updates to the policy so that the policy learns in a stabler
    way and is thus capable of achieving more efficient convergence
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对策略进行较小的梯度更新，使策略以更稳定的方式学习，从而能够实现更高效的收敛
- en: Use the generated probabilities themselves as sampling probabilities for the
    random sample generation that automatically balances exploration and exploitation
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用生成的概率本身作为随机样本生成的采样概率，从而自动平衡探索与利用
- en: 'The first point is achieved by two means:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 第一点通过两种方式实现：
- en: Weighing the loss using the probabilities of the current actor network with
    the old probabilities generated from the actor-network before a parameter update
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用当前演员网络的概率与参数更新前由演员网络生成的旧概率来加权损失
- en: Clipping the probabilities in an interval [1 - ϵ , 1 + ϵ ], where ϵ can be varied
    but the value of 0.2 was used
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将概率裁剪到区间 [1 - ϵ , 1 + ϵ ]，其中 ϵ 可以变化，但使用了 0.2 的值
- en: 'The method is performed through two networks instead of one, called actor-network
    and critic network. The critic network is structured to predict an unconstrained
    single value that serves as part of the evaluation logic of the generated architecture,
    along with the reward from measuring the metric performance. The actor-network,
    on the other hand, is the network we know is the main network responsible for
    generating ideal network architecture configurations. Both networks can be implemented
    with RNNs. This is depicted well in *Figure 7**.12*. The two network parameters
    are jointly updated per batch. The loss of the actor-network can be computed with
    the following formula:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法通过两个网络而非一个网络执行，分别称为演员网络和评论员网络。评论员网络的结构是预测一个不受约束的单一值，该值作为生成架构的评估逻辑的一部分，并与度量性能的奖励一起使用。另一方面，演员网络是我们所知的主要网络，负责生成理想的网络架构配置。这两个网络都可以通过
    RNN 实现。此过程在*图 7.12*中得到了很好的展示。两个网络参数在每个批次中共同更新。演员网络的损失可以通过以下公式计算：
- en: Loss =  current predicted configuration probability   _____________________________   old
    predicted configuration probability  x Advantage
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 损失 = 当前预测配置概率  _____________________________  旧预测配置概率 x 优势
- en: 'A minimum of this loss and another version of the loss with clipped probabilities
    will then be used as the final loss. The clipped loss is defined as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用该损失的最小值和另一种带有剪切概率的损失版本作为最终损失。剪切损失定义如下：
- en: clipped loss = ( current predicted configuration probability   _____________________________   old
    predicted configuration probability , 1 − ϵ, 1+ ϵ) x Advantage
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 剪切损失 = （当前预测配置概率  _____________________________  旧预测配置概率，1 − ϵ，1 + ϵ） x 优势
- en: 'The advantage here is a custom loss logic that provides a quantified evaluation
    number of the sampled child architecture that uses both the reward (using metric
    performance) and the single value predicted from the critic network. In the reinforce
    method, the **exponential moving average** (**EMA**) of previous rewards was used.
    Similarly, here, a form of the EMA is used to reduce the advantage at different
    timesteps. The logic is slightly more scientific, but for those who would like
    to know more, it can be computed using the following formula:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的优势是一个自定义的损失逻辑，提供了对采样的子架构的量化评估数值，既考虑了奖励（使用度量性能），又考虑了评论员网络预测的单一值。在强化学习法中，**指数加权移动平均**（**EMA**）被用来处理之前的奖励。类似地，这里使用了一种EMA形式来减少不同时间步的优势。这个逻辑稍微更具科学性，但对于那些想了解更多的人，可以通过以下公式进行计算：
- en: Advantage = discoun t DISTANCE FROM FIRST TIMESTEP x critic and reward evaluation
    discount = λγ
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 优势 = 折扣 第一时间步的距离 x 评论员和奖励评估折扣 = λγ
- en: critic and reward evaluation = ( reward + γ(critic prediction value at t + 1)
    − critic prediction value at t)
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 评论员和奖励评估 = （奖励 + γ（t+1时刻的评论员预测值） − t时刻的评论员预测值）
- en: 'The lambda, λ , and gamma, γ, are constants with values between 0 and 1\. They
    each control the level of weight decay of the discount of advantages at each timestep
    moving forward. Additionally, the gamma also controls the contribution of predicted
    critic values at future timesteps. As for the loss of the critic network, it can
    be defined using the following formula:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: λ（lambda）和γ（gamma）是介于0和1之间的常量。它们分别控制每个时间步前进时优势折扣的权重衰减程度。此外，γ还控制着未来时间步中预测的评论员（critic）值的贡献。至于评论员网络的损失，它可以通过以下公式来定义：
- en: critic loss = (advantage + future critic values− current critic value) 2
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 评论员损失 = （优势 + 未来评论员值 − 当前评论员值）²
- en: The final total loss will be the summation of the critic loss and the actor
    loss. PPO generally performs better than the vanilla reinforcement policy gradient
    in efficiency and convergence. This sums up the PPO logic at a more intuitive
    level.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的总损失将是评论员损失和演员损失的总和。与传统的强化学习策略梯度相比，PPO在效率和收敛性方面通常表现更好。这总结了PPO的逻辑，便于理解。
- en: The RL search space here is not efficient and takes approximately 4.5 days to
    train on ImageNet directly with a whopping 64 TPUv2 devices. However, this resulted
    in a child architecture called MnasNet that is more efficient than MobileNetV2
    at the same accuracy, or more accurate than MobileNetV2 at the same latency when
    benchmarked on ImageNet. The same MNAS methods eventually got adopted in EfficientNet,
    which has become one of the most efficient CNN model families today.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的强化学习搜索空间效率较低，直接在64个TPUv2设备上训练ImageNet大约需要4.5天。然而，这导致了一个名为MnasNet的子架构，其效率超过了相同准确度下的MobileNetV2，或者在相同延迟下比MobileNetV2更准确，且在ImageNet上进行了基准测试。同样的MNAS方法最终被应用于EfficientNet，成为今天最有效的CNN模型家族之一。
- en: Summarizing NAS with RL methods
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结NAS与RL方法
- en: 'RL allows a way for us to smartly learn the most performant NN architectures
    through sampling, training, and evaluation of neural architectures and apply the
    experience learned by predictively generating the most efficient neural architecture
    configurations. Simply said, NAS with RL trains an NN to generate the best NN
    architecture! The biggest problem with NAS with RL is still the expensive compute
    time needed. A few tricks carried out by different methods to try to circumvent
    this issue are listed here:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）为我们提供了一种通过采样、训练和评估神经网络架构来智能地学习最具表现力的神经网络（NN）架构的方法，并通过预测性地生成最有效的神经网络架构配置来应用所学的经验。简而言之，使用RL的神经架构搜索（NAS）训练一个神经网络来生成最佳的神经网络架构！使用RL的NAS最大的挑战仍然是所需的高昂计算时间。为了规避这个问题，下面列出了一些不同方法所采取的技巧：
- en: Training NAS with RL on a smaller but still representative dataset as a proxy
    task and training and evaluating the final obtained neural architecture on the
    main larger dataset
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个较小但仍具有代表性的数据集上训练NAS作为代理任务，并在主要的大数据集上训练和评估最终获得的神经架构。
- en: Parameter sharing by the unique layer number and computation type can, fortunately,
    be generically adapted for other methods from ENAS
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过唯一的层数和计算类型进行参数共享，幸运的是，这可以通用地适应ENAS的其他方法。
- en: Balancing the macroarchitecture and microarchitecture search flexibility to
    reduce the search space while making sure it is flexible enough to take advantage
    of key differences needed at different stages of a network to achieve efficiency
    and good metric performance
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平衡宏观架构和微观架构的搜索灵活性，以减少搜索空间，同时确保它足够灵活，可以利用网络不同阶段所需的关键差异，从而实现高效性和良好的度量性能。
- en: Directly embedding target and achieved latency as part of the reward structure
    and as a result searching only mostly architectures around the specified latency
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将目标延迟和实际延迟直接嵌入作为奖励结构的一部分，结果是只搜索大多数位于指定延迟范围内的架构。
- en: Do note that the methods that were introduced here do not provide an exhaustive
    overview of RL and its potential.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里介绍的方法并没有提供关于强化学习（RL）及其潜力的详尽概述。
- en: Although RL provides a concrete way to accomplish NAS, it is not strictly necessary.
    In the next section, we will go through examples of a category of NAS-specific
    methods that do not use RL.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管RL提供了实现NAS的具体方式，但它并不是绝对必要的。在接下来的部分，我们将通过一些不使用RL的NAS特定方法的示例。
- en: Understanding non-RL-based NAS
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解非RL基础的NAS
- en: The core of NAS is about intelligently searching through different child architecture
    configurations by making decisions based on prior search experience to find the
    best child architecture in a non-random and non-brute-force way. The core of RL,
    on the other hand, involves utilizing a controller-based system to achieve that
    intelligence. Intelligent NAS can be achieved without using RL, and in this section,
    we will go through a simplified version of the progressive growing-from-scratch
    style of NAS without a controller and another competitive version of elimination
    from a complex fully defined NN macroarchitecture and microarchitecture.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: NAS的核心在于通过基于先前搜索经验做出决策，智能地在不同的子架构配置中进行搜索，从而以非随机和非暴力的方式找到最佳的子架构。而RL的核心则是利用基于控制器的系统来实现这种智能。无需使用RL也能实现智能化的NAS，在这一部分，我们将介绍一种简化版的逐步从零开始的NAS方法，它不使用控制器，以及另一种来自复杂完全定义的神经网络宏观架构和微观架构的竞争性消除方法。
- en: Understanding path elimination-based NAS
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解基于路径消除的神经架构搜索（NAS）
- en: First and foremost, `softmax` activation before the weighted addition process.
    During testing, the top *k* paths or operations between nodes are chosen to act
    as the actual network, whereas the other paths are pruned away. When the weight
    vector gets updated, however, the child architecture essentially changes. Instead
    of training and evaluating this new child architecture to obtain the new metric
    performance on the holdout or validation partition of the dataset, only a single
    training epoch is used for the entire architecture to obtain an estimate of the
    best validation performance using the training loss. This estimate will be used
    to update the parameters of the overparameterized architecture through gradient
    descent.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，`softmax` 激活在加权求和过程之前。在测试时，选择节点间的前 *k* 条路径或操作作为实际网络，而其他路径则被剪枝。当权重向量更新时，子架构本质上发生了变化。然而，并不会训练和评估这个新的子架构以获取其在保留或验证数据集上的新度量性能，而是仅使用整个架构的一个训练周期，通过训练损失来估算最佳的验证性能。这个估算值将通过梯度下降更新超参数架构的参数。
- en: '`BinaryConnect`. These binarized weights act as gates that allow data to travel
    through only when it is enabled. This addition helps to alleviate the biggest
    issue with any overparameterized architectures: the GPU memory size needed to
    hold the parameters of the defined architecture. The second addition is the latency
    component to the overall loss component, which is crucial to make sure the search
    takes latency into consideration and doesn’t attempt to utilize more paths just
    to get better metric performance. Let’s uncover the details step by step by first
    describing the overall training method used in proxyless NAS:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`BinaryConnect`。这些二值化的权重充当门控，只有在启用时才允许数据通过。这一添加有助于缓解任何过度参数化架构的最大问题：需要的GPU内存来存储定义架构的参数。第二个补充是将延迟组件添加到整体损失中，这对于确保搜索考虑延迟并避免仅为获得更好的度量性能而尝试使用更多路径至关重要。让我们逐步揭示细节，首先描述代理NAS中使用的整体训练方法：'
- en: Train only the BinaryConnect weight vector based on a single randomly sampled
    path for each node based on the probabilities specified by the `softmax` conditioned
    weight vector using the training dataset loss. This is achieved by freezing the
    parameters of the rest of the architecture and using standard cross-entropy loss.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅根据每个节点的单个随机采样路径，基于`softmax`条件化的权重向量，使用训练数据集损失训练BinaryConnect权重向量。通过冻结架构的其余参数，并使用标准的交叉熵损失来实现这一点。
- en: 'Train only the architecture parameters based on two randomly sampled paths
    for each based on the probabilities specified by the `softmax` conditioned weight
    vector using the validation dataset loss. This is achieved by freezing the parameters
    of the weight vector and using an approximate gradient formula for the architecture
    parameters:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅根据每条路径的概率，通过`softmax`条件化的权重向量，基于验证数据集损失训练架构参数。这是通过冻结权重向量的参数，并使用架构参数的近似梯度公式来实现的：
- en: gradient of parameters through path 1 = gradient of binary weight 2 x path 2
    probability x (− path 1 probability) + gradient of binary weight 1 x path 1 probability
    x (1 − path 2 probability)
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过路径1的参数梯度 = 二进制权重2的梯度 x 路径2的概率 x (− 路径1的概率) + 二进制权重1的梯度 x 路径1的概率 x (1 − 路径2的概率)
- en: gradient of parameters through path 2 = gradient of binary weight 1 x path 1
    probability x (− path 2 probability) + gradient of binary weight 2 x path 2 probability
    x (1 − path 1 probability)
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过路径2的参数梯度 = 二进制权重1的梯度 x 路径1的概率 x (− 路径2的概率) + 二进制权重2的梯度 x 路径2的概率 x (1 − 路径1的概率)
- en: The formula computes gradients for path 1 and path 2\. The loss used is cross-entropy
    loss summed with a predicted latency after pruning the paths similar to DARTS.
    The latency is predicted with an external ML model trained to predict latency
    based on the parameters of the architecture due to the reason that latency evaluations
    take up too much time and usually require an average of multiple runs to get a
    reliable estimate. Any ML model can be used to build the latency predictor model
    and is a one-off process before starting the NAS process.
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该公式计算路径1和路径2的梯度。所使用的损失是交叉熵损失，与修剪后的路径预测延迟相加，类似于DARTS。延迟是通过一个外部机器学习模型预测的，该模型经过训练以根据架构的参数预测延迟，因为延迟评估占用了过多时间，通常需要多次运行的平均值才能得到可靠的估计。任何机器学习模型都可以用来构建延迟预测模型，并且这只是NAS过程开始之前的一次性操作。
- en: Repeat *steps 1-2* until convergence, a predefined number of epochs, or any
    early stopping without improvements on the validation loss for a predefined number
    of epochs.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤1-2*，直到收敛，达到预定的训练周期数，或在验证损失上没有改进时提前停止，且停止的周期数是预设的。
- en: 'Recall that BinaryConnect is used to achieve binary weights that act as gates.
    One detail is that the standard unconstrained non-binary weight vector itself
    is still present but a binarization operation is applied. The binarization process
    is executed by the following steps:'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请记住，BinaryConnect用于实现充当门控的二进制权重。一个细节是，标准的无约束非二进制权重向量仍然存在，但会应用二值化操作。二值化过程通过以下步骤执行：
- en: Set all binary weights to `0`.
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有二进制权重设置为`0`。
- en: Sample the desired number of chosen paths by using the `softmax` conditioned
    weight vector as probabilities.
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`softmax`条件化的权重向量作为概率，采样所需数量的选择路径。
- en: Set the chosen path’s binary weights to `1`.
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所选路径的二进制权重设置为`1`。
- en: BinaryConnect saves memory by only loading nonzero paths to memory.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BinaryConnect通过仅将非零路径加载到内存中来节省内存。
- en: PNAS manages to achieve an 85.1 top-1 accuracy on ImageNet directly without
    using a proxy dataset such as `CIFAR-10`, using only 8.3 days of search using
    the NVIDIA GTX 1080 Ti GPU.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: PNAS直接在ImageNet上实现了85.1%的top-1准确率，而没有使用诸如`CIFAR-10`之类的代理数据集，仅使用NVIDIA GTX 1080
    Ti GPU进行了8.3天的搜索。
- en: Next, we will go through a simple progressive growth-based NAS method as an
    introduction.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍一种简单的基于渐进增长的NAS方法。
- en: Understanding progressive growth-based NAS
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解基于渐进增长的NAS（Progressive NAS）。
- en: The progressive growth-based NAS method’s key differentiator is that the method
    can be structured to be unbounded in both macroarchitecture and microarchitecture.
    Most of the techniques introduced in this chapter have placed a lot of domain
    knowledge in terms of general structures found to be useful. Growth-based NAS
    is naturally non-finite in terms of search space and can potentially help to discover
    novel macroarchitectural structures that work well. This line of NAS will continue
    to evolve to a stage where it can be competitive with other NAS methods, but,
    in this book, we will only go into a method to search the microarchitectural structure
    of the child architecture called **progressive NAS** (**PNAS**) to act as an introduction.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 基于渐进增长的NAS方法的关键区别在于，该方法可以在宏观架构和微观架构中都不受限制地构建。本章介绍的大多数技术在一般结构方面都有很多领域知识的投入，被发现是有用的。基于增长的NAS在搜索空间方面自然是非有限的，可能有助于发现效果良好的新型宏观架构结构。NAS的这一路线将继续发展到可以与其他NAS方法竞争的阶段，但在本书中，我们将只介绍一种搜索名为**渐进NAS**（PNAS）的子架构微观结构的方法。
- en: 'PNAS adopts a progressive growth-based approach in NAS by simply using concepts
    defined in Bayesian optimization introduced earlier in this chapter and searching
    at the microarchitecture level while fixing the macroarchitecture structure similar
    to the ENAS microarchitecture search method. The macroarchitecture structure is
    adapted to the size of the dataset, `CIFAR-10`, with a smaller structure, and
    ImageNet with a deeper structure. *Figure 7**.13* shows these structures:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: PNAS通过在NAS中采用基于渐进增长的方法简单地使用在本章前面介绍的贝叶斯优化概念，并在固定宏观架构结构的同时在微观架构水平上进行搜索，类似于ENAS微架构搜索方法。宏观架构结构适应数据集的大小，如`CIFAR-10`采用较小的结构，而ImageNet采用较深的结构。*图
    7**.13*显示了这些结构：
- en: '![Figure 7.13 – PNAS macroarchitecture structure from the https://arxiv.org/abs/1712.00559v3
    paper](img/B18187_07_013.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.13 – 来自https://arxiv.org/abs/1712.00559v3 论文的PNAS宏观架构结构](img/B18187_07_013.jpg)'
- en: Figure 7.13 – PNAS macroarchitecture structure from the https://arxiv.org/abs/1712.00559v3
    paper
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 – 来自https://arxiv.org/abs/1712.00559v3 论文的PNAS宏观架构结构
- en: 'The method can be accomplished with the steps defined next, along with an initial
    predefined max number of blocks in the cell and starting with zero blocks in the
    cell:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法可以通过以下定义的步骤来完成，同时在单元格中有一个初始预定义的最大块数，并以零块开始：
- en: Start with the first block. Construct the full CNN with all cell options iteratively
    and evaluate all of the CNN.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从第一个块开始。逐步构建所有单元格选项的完整CNN，并评估所有CNN。
- en: Train an RNN surrogate model to predict the metric performance using the cell
    configurations.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个RNN代理模型，以预测使用单元格配置的指标性能。
- en: Expand to the next block and predict the metric performance of the possible
    cell option combinations for the next block using all available chosen and evaluated
    previous block variations.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩展到下一个块，并预测可能的单元格选项组合的下一个块的指标性能，使用所有已选择和评估的先前块变体。
- en: Take the top two best metric performance cell options for the next block and
    train and evaluate the fully constructed CNN using the two cell options.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择前两个最佳指标性能单元格选项用于下一个块，并训练和评估使用这两个单元格选项的完全构建的CNN。
- en: Fine-tune the RNN surrogate model using the extra two data points obtained in
    *step 4*.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用在*步骤 4*获得的额外两个数据点对RNN代理模型进行微调。
- en: Repeat *step 3* to *step 5* until the total number of blocks reaches the max
    number of blocks or until the metric performance does not improve anymore.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤 3*到*步骤 5*，直到总块数达到最大块数或者指标性能不再改善为止。
- en: Each block in a cell will have a configuration defined with five variables similar
    to ENAS; namely, the first input, the second input, the operation to the first
    input, the operation to the second input, and the method to combine the outputs
    of the operation to the first input and operation to the second input. The set
    of possible inputs is all the previous blocks, the output of the previous cell,
    and the output of the cell before the previous cell. This means that the cell
    can interact with other cells. All the possible input combinations, operation
    types, and combination methods are laid out in each progressive block step and
    fed to the RNN model to predict the metric performance.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单元格中的块都将有一个配置，这个配置定义了五个变量，类似于ENAS；即，第一个输入、第二个输入、第一个输入的操作、第二个输入的操作，以及组合第一个输入操作和第二个输入操作输出的方法。可能的输入集包括所有先前的块、前一个单元格的输出，以及前一个单元格的前一个单元格的输出。这意味着该单元格可以与其他单元格进行交互。所有可能的输入组合、操作类型和组合方法都被列出在每个逐步的块步骤中，并传递给RNN模型以预测指标性能。
- en: PNAS managed to achieve an 84.2 top-1 accuracy on ImageNet but utilized a whopping
    225 GPU days using an NVIDIA GTX 1080 GPU.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: PNAS成功地在ImageNet上实现了84.2的top-1准确率，但使用了225个GPU天，使用的GPU是NVIDIA GTX 1080。
- en: The progressive growth-based line of NAS methods has since progressed to the
    stage where it is possible to achieve an accuracy of 84 top-1 accuracy with only
    5 days of searching with a method called **efficient forward architecture search**,
    but this is beyond the scope of this book.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 基于逐步增长的NAS方法已经发展到可以通过一种叫做**高效前向架构搜索**的方法，仅用5天的搜索就能实现84的top-1准确率，但这超出了本书的讨论范围。
- en: 'To end this chapter, *Figure 7**.14* shows a summary performance comparison
    among the methods introduced for the image domain, excluding general hyperparameter
    search methods:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，*图7.14*展示了图像领域中所有已介绍方法的性能总结比较，不包括通用超参数搜索方法：
- en: '| **Method** | **Number of** **params (million)** | **Search** **time (days)**
    | **CIFAR-10** **test error** | **ImageNet** **test error** |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| **方法** | **参数数量（百万）** | **搜索时间（天）** | **CIFAR-10** **测试误差** | **ImageNet**
    **测试误差** |'
- en: '| Vanilla RL NAS | 4.2 | 1680 | 4.47 | N/A |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| Vanilla RL NAS | 4.2 | 1680 | 4.47 | N/A |'
- en: '| ENAS macro | 21.3 | 0.32 | 4.23 | N/A |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| ENAS宏 | 21.3 | 0.32 | 4.23 | N/A |'
- en: '| ENAS micro | 4.6 | 0.45 | 2.89 | N/A |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| ENAS微 | 4.6 | 0.45 | 2.89 | N/A |'
- en: '| DARTS | 3.4 | 4 | 2.83 | 26.9 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| DARTS | 3.4 | 4 | 2.83 | 26.9 |'
- en: '| Proxyless NAS | 7.1 | 8.3 | N/A | 24.9 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| Proxyless NAS | 7.1 | 8.3 | N/A | 24.9 |'
- en: '| PNAS | 5.1 | 225 | 3.41 | 25.8 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| PNAS | 5.1 | 225 | 3.41 | 25.8 |'
- en: Figure 7.14 – Performance comparison of all the introduced NAS methods, excluding
    general hyperparameter search methods
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 – 所有已介绍的NAS方法的性能比较，不包括通用超参数搜索方法
- en: This table includes the number of parameters, search time, and test error rates
    for `CIFAR-10` and ImageNet datasets for each NAS method introduced. Each NAS
    method has its own strengths and weaknesses in terms of latency, complexity, and
    accuracy. The ENAS micro method, in particular, stands out with a relatively low
    number of parameters, a short search time, and a low test error rate for `CIFAR-10`.
    It could be a recommended choice for neural architecture search in the image domain.
    However, the specific choice depends on the requirements and constraints of the
    project, such as available compute resources and desired accuracy.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 本表格包括每个已介绍的NAS方法在`CIFAR-10`和ImageNet数据集上的参数数量、搜索时间和测试误差率。每个NAS方法在延迟、复杂度和准确性方面各有优缺点。特别是ENAS微方法，具有相对较少的参数、较短的搜索时间和较低的`CIFAR-10`测试误差率。它可能是图像领域神经架构搜索的推荐选择。然而，具体的选择依赖于项目的需求和约束，例如可用的计算资源和期望的准确性。
- en: Summary
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: NAS is a method that is generalized to any NN type, allowing for the automation
    of creating new and advanced NNs without the need for manual neural architecture
    design. As you may have guessed, NAS dominates the image-based field of NNs. The
    EfficientNet model family exemplifies the impact NAS provides to the image-based
    NN field. This is due to the inherent availability of a wide variety of CNN components
    that make it more complicated to design when compared to a simple MLP. For sequential
    or time-series data handling, there are not many variations of RNN cells, and
    thus the bulk of work in NAS for RNNs is focused on designing a custom recurrent
    cell. More work could have been done to accommodate transformers as it is the
    current state of the art, capable of being adapted to a variety of data modalities.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: NAS是一种可以推广到任何类型神经网络（NN）的方法，允许自动创建新的和先进的神经网络，而无需手动设计神经网络架构。正如你可能猜到的，NAS在基于图像的神经网络领域占据主导地位。EfficientNet模型系列展示了NAS对基于图像的神经网络领域所带来的影响。这是因为与简单的MLP相比，CNN组件种类繁多，设计起来更为复杂。对于序列或时间序列数据处理，RNN单元的变化不多，因此NAS在RNN上的主要工作集中在设计自定义的循环单元上。为了适应当前最前沿的Transformer技术，还可以做更多工作，因为Transformer能够适应多种数据模态。
- en: NAS is mainly adopted by researchers or practitioners in larger institutions.
    One of the key traits practitioners want when trying to train better models for
    their use cases is the speed to the final result. NAS by itself is still a process
    that takes days to accomplish, and if applied to a large dataset, it can take
    up to months. This deters most practitioners’ usage of NAS directly. Instead,
    they mostly use the existing architectures from published open source implementations.
    Using the existing architectures makes no difference in speed when compared to
    using manually defined architecture and thus gives practitioners the motivation
    they need to use it instead. It is also widely known that pre-training helps to
    improve the performance of the model, thus using NAS directly means you’d have
    to also pre-train the resulting architecture yourself on a large generalized dataset,
    which further extends the time needed to complete the NAS process. Use cases in
    ML often require a lot of time to explore the problem setup and figure out the
    potential performance that is achievable from the available dataset. Thus, quick
    iteration between model experimentations is crucial to the success of the project.
    Slow experimentations dampen the time to identify success. These reasons are why
    NAS is mainly adopted by practitioners in bigger institutions or researchers who
    are willing to spend time designing generalized custom neural architectures that
    can be amortized across different domains instead of building custom architectures
    for a specific use case.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: NAS主要被大型机构中的研究人员或从业者采用。从业者在尝试为他们的用例训练更好的模型时，最关心的一个关键特性是达到最终结果的速度。NAS本身仍然是一个需要数天完成的过程，如果应用于大规模数据集，可能需要几个月。这使得大多数从业者直接使用NAS变得困难。相反，他们通常使用已发布的开源实现中的现有架构。与手动定义的架构相比，使用现有架构在速度上没有差异，因此它给从业者提供了使用现有架构的动力。众所周知，预训练有助于提高模型的性能，因此直接使用NAS意味着你还需要在一个大型的通用数据集上预训练生成的架构，这进一步延长了完成NAS过程所需的时间。机器学习中的用例通常需要大量时间来探索问题设置，并找出从可用数据集中能够实现的潜在性能。因此，模型实验之间的快速迭代对于项目的成功至关重要。实验进展缓慢会延迟成功的识别。这些原因解释了为什么NAS主要被大型机构的从业者或那些愿意花时间设计可以跨领域使用的通用自定义神经架构的研究人员所采用，而不是为特定用例构建定制的架构。
- en: However, NAS still undoubtedly provides a unique way to find unique custom architectures
    for your use cases as long as time is not of concern and the goal is either to
    maximize the performance you can get with a target latency or to generally get
    the best-performing model without latency considerations.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，NAS无疑仍为你的用例提供了一种独特的方式，去寻找独特的自定义架构，只要时间不成问题，并且目标是要么在特定延迟下最大化性能，要么通常获取最佳性能的模型，而不考虑延迟。
- en: In the next chapter, we will go into the details of different problem types
    in SL, along with general tips and tricks for **supervised** **DL** (**SDL**).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将详细讨论监督学习（**supervised** **DL**，**SDL**）中的不同问题类型，并提供一些通用的技巧和建议。
