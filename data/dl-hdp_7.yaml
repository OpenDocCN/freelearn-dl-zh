- en: Chapter 7. Miscellaneous Deep Learning Operations using Hadoop
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七章. 使用 Hadoop 的杂项深度学习操作
- en: '|   | *"In pioneer days they used oxen for heavy pulling, and when one ox couldn''t
    budge a log, they didn''t try to grow a larger ox. We shouldn''t be trying for
    bigger computers, but for more systems of computers."* |   |'
  id: totrans-1
  prefs: []
  type: TYPE_TB
  zh: '|   | *“在开拓者时代，他们使用牛来进行重型牵引，当一头牛无法移动一根原木时，他们不会试图养一头更大的牛。我们不应该追求更大的计算机，而是应该追求更多的计算机系统。”*
    |   |'
- en: '|   | --*Grace Hopper* |'
  id: totrans-2
  prefs: []
  type: TYPE_TB
  zh: '|   | --*格蕾丝·霍普* |'
- en: So far in this book, we discussed various deep neural network models and their
    concepts, applications, and implementation of the models in distributed environments.
    We have also explained why it is difficult for a centralized computer to store
    and process vast amounts of data and extract information using these models. Hadoop
    has been used to overcome the limitations caused by large-scale data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书讨论了各种深度神经网络模型及其概念、应用和在分布式环境中的实现。我们还解释了为什么集中式计算机难以存储和处理大量数据，并使用这些模型提取信息。Hadoop
    被用于克服大规模数据带来的限制。
- en: As we have now reached the final chapter of this book, we will mainly discuss
    the design of the three most commonly used machine learning applications. We will
    explain the general concept of large-scale video processing, large-scale image
    processing, and natural language processing using the Hadoop framework.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们已进入本书的最后一章，本章将主要讨论三种最常用的机器学习应用的设计。我们将解释使用 Hadoop 框架进行大规模视频处理、大规模图像处理和自然语言处理的一般概念。
- en: 'The organization of this chapter is as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的组织结构如下：
- en: Large-scale distributed video processing using Hadoop
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Hadoop 进行大规模分布式视频处理
- en: Large-scale image processing using Hadoop
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Hadoop 进行大规模图像处理
- en: Natural language processing using Hadoop
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Hadoop 进行自然语言处理
- en: The large amount of videos available in the digital world are contributing to
    the lion's share of the big data generated in recent days. In [Chapter 2](ch02.html
    "Chapter 2.  Distributed Deep Learning for Large-Scale Data") , *Distributed Deep
    Learning for Large-Scale Data* we discussed how millions of videos are uploaded
    to various social media websites such as YouTube and Facebook. Apart from this,
    surveillance cameras installed for security purposes in various shopping malls,
    airports, or government organizations generate loads of videos on a daily basis.
    Most of these videos are typically stored as compressed video files due to their
    huge storage consumption. In most of these enterprises, the security cameras operate
    for the whole day and later store the important videos, to be investigated in
    future.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数字世界中大量的视频正在为近年来产生的大数据贡献着巨大的份额。在[第二章](ch02.html "第二章. 分布式深度学习与大规模数据")《分布式深度学习与大规模数据》中，我们讨论了如何将数百万个视频上传到各种社交媒体网站，如
    YouTube 和 Facebook。除此之外，安装在各大商场、机场或政府机构中的监控摄像头也会每天生成大量的视频。由于这些视频占用巨大的存储空间，大多数视频通常以压缩格式存储。在许多企业中，监控摄像头全天运行，随后存储重要的视频，以备将来调查。
- en: These videos contain hidden "hot data" or information, which needs to be processed
    and extracted quickly. As a consequence, the need to process and analyze these
    large-scale videos has become one of the priorities for data enthusiasts. Also,
    in many different fields of studies, such as bio-medical engineering, geology,
    and educational research, there is a need to process these large-scale videos
    and make them available at different locations for detailed analysis.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这些视频包含了隐藏的“热数据”或信息，需要迅速处理和提取。因此，处理和分析这些大规模视频已经成为数据爱好者的优先事项之一。此外，在许多不同的研究领域，如生物医学工程、地质学和教育研究等，都需要处理这些大规模视频，并将它们提供给不同地点进行详细分析。
- en: In this section, we will look into the processing of large-scale video datasets
    using the Hadoop framework. The primary challenge of large-scale video processing
    is to transcode the videos from compressed to uncompressed format. For this reason,
    we need a distributed video transcoder that will write the video in the **Hadoop
    Distributed File System** (**HDFS**), decode the bit stream chunks in parallel,
    and generate a sequence file.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨使用 Hadoop 框架处理大规模视频数据集的问题。大规模视频处理的主要挑战是将视频从压缩格式转码为非压缩格式。因此，我们需要一个分布式视频转码器，它将视频写入
    **Hadoop 分布式文件系统**（**HDFS**），并行解码比特流块，生成序列文件。
- en: When a block of the input data is processed in the HDFS, each mapper process
    accesses the lines in each split separately. However, in case of a large-scale
    video dataset, when it is split into multiple blocks of predefined sizes, each
    mapper process is supposed to interpret the blocks of bit-stream separately. The
    mapper process will then provide access to the decoded video frames for subsequent
    analysis. In the following subsections, we will discuss how each block of the
    HDFS containing the video bit-stream can be transcoded into sets of images to
    be processed for the further analyses.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当HDFS中的输入数据块被处理时，每个mapper进程分别访问每个分片中的行。然而，在大规模视频数据集的情况下，当它被分割成多个预定义大小的块时，每个mapper进程应该单独解释这些位流块。然后，mapper进程将提供对解码后的视频帧的访问，以供后续分析。在以下小节中，我们将讨论如何将包含视频位流的HDFS中的每个块转码成图像集，以便进行进一步的分析。
- en: Distributed video decoding in Hadoop
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop中的分布式视频解码
- en: 'Most of the popular video compression formats, such as MPEG-2 and MPEG-4, follow
    a hierarchical structure in the bit-stream. In this subsection, we will assume
    that the compression format used has a hierarchical structure for its bit-stream.
    For simplicity, we have divided the decoding task into two different Map-reduce
    jobs:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数流行的视频压缩格式，如MPEG-2和MPEG-4，遵循位流中的分层结构。在这一小节中，我们假设使用的压缩格式具有分层结构。为简单起见，我们将解码任务分为两个不同的Map-reduce作业：
- en: '**Extraction of video sequence level information**: From the outset, it can
    be easily predicted that the header information of all the video dataset can be
    found in the first block of the dataset. In this phase, the aim of the map-reduce
    job is to collect the sequence level information from the first block of the video
    dataset and output the result as a text file in the HDFS. The sequence header
    information is needed to set the format for the decoder object.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提取视频序列级别信息**：一开始就可以很容易预测，所有视频数据集的头信息可以在数据集的第一个块中找到。在这个阶段，map-reduce作业的目的是从视频数据集的第一个块中收集序列级别信息，并将结果输出为HDFS中的文本文件。序列头信息对于设置解码器对象的格式是必需的。'
- en: 'For the video files, a new `FileInputFormat` should be implemented with its
    own record reader. Each record reader will then provide a `<key, value>` pair
    in this format to each map process: `<LongWritable, BytesWritable>`. The input
    key denotes the byte offset within the file; the value that corresponds to `BytesWritable`
    is a byte array containing the video bit-stream for the whole block of data.'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于视频文件，应实现一个新的`FileInputFormat`，并具有自己的记录读取器。每个记录读取器将以这种格式向每个map过程提供`<key, value>`对：`<LongWritable,
    BytesWritable>`。输入键表示文件中的字节偏移量；与`BytesWritable`对应的值是一个字节数组，包含整个数据块的视频位流。
- en: For each map process, the key value is compared with `0` to identify if it is
    the first block of the video file. Once the first block is identified, the bit-stream
    is parsed to determine the sequence level information. This information is then
    dumped to a `.txt` file to be written to  HDFS. Let's denote the name of the `.txt`
    file as `input_filename_sequence_level_header_information.txt`. As only the map
    process can provide us the desired output, the reducer count for this method is
    set to `0`.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个map过程，键值与`0`进行比较，以确定它是否是视频文件的第一个块。一旦确定了第一个块，位流将被解析以确定序列级别信息。然后，这些信息将被转储到`.txt`文件中，并写入HDFS。我们将该`.txt`文件的名称表示为`input_filename_sequence_level_header_information.txt`。由于只有map过程可以为我们提供所需的输出，因此此方法的reducer数量设置为`0`。
- en: Note
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Assume a text file with the following data: **Deep Learning** **with Hadoop**
    Now the offset for the first line is `0` and the input to the Hadoop job will
    be `<0,Deep Learning>` and for the second line the offset will be `<14,with Hadoop>`.
    Whenever we pass the text file to the Hadoop job, it internally calculates the
    byte offset.'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假设有一个包含以下数据的文本文件：**深度学习** **与Hadoop** 现在，第一行的偏移量为`0`，Hadoop作业的输入将是`<0,深度学习>`，第二行的偏移量将是`<14,与Hadoop>`。每当我们将文本文件传递给Hadoop作业时，它会内部计算字节偏移量。
- en: '**Decode and convert the blocks of videos into sequence files**: The aim of
    this Map-reduce job is to decode  each block  of the video datasets and generate
    a corresponding sequence file. The sequence file will contain the decoded video
    frames of each block of data in JPEG format. The `InputFileFormat` file and record
    reader should be kept same as the first Map-reduce job. Therefore, the `<key,
    value>` pairs of the mapper input is `<LongWritable, BytesWritable>`.![Distributed
    video decoding in Hadoop](img/B05883_07_01-1.jpg)'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**解码并转换视频块为序列文件**：这个 Map-reduce 任务的目的是解码每个视频数据集的块，并生成相应的序列文件。序列文件将包含每个视频块的解码视频帧，格式为
    JPEG。`InputFileFormat` 文件和记录读取器应与第一个 Map-reduce 任务保持一致。因此，mapper 输入的 `<key, value>`
    对是 `<LongWritable, BytesWritable>`。[Hadoop 分布式视频解码](img/B05883_07_01-1.jpg)'
- en: 'Figure 7.1: The overall representation of video decoding with Hadoop'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.1：Hadoop 视频解码的整体表示
- en: In this second phase, the output of the first job is considered as the input
    to this second Map-reduce job. Therefore, each mapper of this job will read the
    sequence information file in the HDFS and pass this information along with the
    bit-stream buffer, which comes as the `BytesWritable` input.
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二阶段，第一次任务的输出作为第二次 Map-reduce 任务的输入。因此，这个任务的每个 mapper 将读取 HDFS 中的序列信息文件，并将此信息与以
    `BytesWritable` 输入形式传入的位流缓冲区一起传递。
- en: The map process basically converts the decoded video frames to JPEG images and
    generates a `<key, value>` pair as the output of the map process. The key of this
    output of the map process encodes the input video filename and the block number
    as `video_filename_block_number`. The output value that corresponds to this key
    is `BytesWritable`, and it stores the JPEG bit-stream of the decoded video block.
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: map 过程基本上将解码的视频帧转换为 JPEG 图像，并生成 `<key, value>` 对，作为 map 过程的输出。该输出的 key 编码了输入视频文件名和块编号，格式为
    `video_filename_block_number`。对应于此 key 的输出值是 `BytesWritable`，它存储了解码视频块的 JPEG 位流。
- en: The reducers will then take the blocks of data as input and simply write the
    decoded frames into a sequence file containing JPEG images as output format for
    further processing. A simple format and overview of the whole process is shown
    in *Figure 7.1*. We have taken an input video `sample.m2v` for illustration purposes.
    Further, in this chapter, we will discuss how to process the large-scale image
    files (from the sequence files) with the HDFS.
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，reducers 会将数据块作为输入，并将解码的帧简单地写入包含 JPEG 图像的序列文件中，作为输出格式进行进一步处理。一个简单的格式和整个过程的概览如
    *图 7.1* 所示。我们使用一个输入视频 `sample.m2v` 进行说明。此外，在本章中，我们将讨论如何使用 HDFS 处理大规模的图像文件（来自序列文件）。
- en: Note
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Input `<key,value>` for Mapper: `<LongWritable, BytesWritable>`'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Mapper 的输入 `<key,value>`：`<LongWritable, BytesWritable>`
- en: 'For example: `<17308965, BytesWritable>` Output `<key,value>` from Mapper:
    `<Text, BytesWritable>` For example: `<sample.m2v_3, BytesWritable>`'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如：`<17308965, BytesWritable>` Mapper 输出的 `<key,value>`：`<Text, BytesWritable>`
    例如：`<sample.m2v_3, BytesWritable>`
- en: Large-scale image processing using Hadoop
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Hadoop 进行大规模图像处理
- en: 'We have already mentioned in the earlier chapters how the size and volume of
    images are increasing day by day; the need to store and process these vast amount
    of images is difficult for centralized computers. Let''s consider an example to
    get a practical idea of such situations. Let''s take a large-scale image of size
    81025 pixels by 86273 pixels. Each pixel is composed of three values:red, green,
    and blue. Consider that, to store each of these values, a 32-bit precision floating
    point number is required. Therefore, the total memory consumption of that image
    can be calculated as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的章节中已经提到，图像的大小和数量正日益增加；存储和处理这些庞大的图像对于集中式计算机来说是一项挑战。我们来考虑一个实际的例子，以便更好地理解这种情况。假设我们有一张尺寸为
    81025 像素 x 86273 像素的大规模图像。每个像素由三个值组成：红色、绿色和蓝色。假设每个值需要使用 32 位精度的浮动点数来存储。那么，该图像的总内存消耗可以通过以下公式计算：
- en: '*86273 * 81025 * 3 * 32 bits = 78.12 GB*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*86273 * 81025 * 3 * 32 位 = 78.12 GB*'
- en: Leave aside doing any post processing on this image, as it can be clearly concluded
    that it is impossible for a traditional computer to even store this amount of
    data in its main memory. Even though some advanced computers come with higher
    configurations, given the return on investment, most companies do not opt for
    these computers as they are much too expensive to be acquired and maintained.
    Therefore, the proper solution should be to run the images in commodity hardware
    so that the images can be stored in their memory. In this section, we will explain
    the use of Hadoop to process these vast amounts of images in a distributed manner.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 抛开对图像进行任何后处理，因为可以清楚地得出结论，传统计算机甚至无法将如此大量的数据存储在其主内存中。即使一些高级计算机具有更高的配置，但考虑到投资回报率，大多数公司并不选择这些计算机，因为它们的购买和维护成本过于昂贵。因此，适当的解决方案应该是使用通用硬件来运行这些图像，以便图像能够存储在其内存中。在本节中，我们将解释如何使用
    Hadoop 以分布式方式处理这些大量图像。
- en: Application of Map-Reduce jobs
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Map-Reduce 作业的应用
- en: In this section, we will discuss how to process large image files using Map-reduce
    jobs with Hadoop. Before the job starts, all the input images to be processed
    are loaded to the HDFS. During the operation, the client sends a job request,
    which goes through NameNode. NameNode collects that request from the client, searches
    its metadata mapping, and then sends the data block information of the filesystem
    as well as location of the data block back to the client. Once the client gets
    the block's metadata, it automatically accesses the DataNodes, where the requested
    data block resides, then processes this data via the applicable commands.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何使用 Map-reduce 作业和 Hadoop 来处理大型图像文件。在作业开始之前，所有待处理的输入图像将被加载到 HDFS
    中。在操作过程中，客户端发送一个作业请求，该请求经过 NameNode。NameNode 从客户端接收该请求，搜索其元数据映射，然后将文件系统的数据块信息以及数据块的位置发送回客户端。一旦客户端获得了数据块的元数据，它会自动访问存储该数据块的
    DataNodes，然后通过适用的命令处理该数据。
- en: The Map-reduce jobs used for large-scale image processing are primarily responsible
    for controlling the whole task. Basically, here we explain the concept of an executable
    shell script file, which is responsible for collecting the executable file's input
    data from the HDFS.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 用于大规模图像处理的 Map-reduce 作业主要负责控制整个任务。基本上，在这里我们解释了可执行 shell 脚本文件的概念，该文件负责从 HDFS
    中收集可执行文件的输入数据。
- en: The best way to use the Map-reduce programming model is to design our own Hadoop
    data types for processing large numbers of image files directly. The system will
    use Hadoop Streaming technology, which helps the users to create and run special
    kinds of Map-reduce jobs. These special kinds of jobs can be performed through
    an executable file mentioned earlier, which will act as a mapper or reducer. The
    mapper implementation of the program will use a shell script to perform the necessary
    operation. The shell script is responsible for calling the executable files of
    the image processing. The lists of image files are taken as the input to these
    executable files for further processing. The results of this processing or output
    are later written back to the HDFS.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Map-reduce 编程模型的最佳方式是设计我们自己的 Hadoop 数据类型，用于直接处理大量图像文件。系统将使用 Hadoop Streaming
    技术，帮助用户创建和运行特殊类型的 Map-reduce 作业。这些特殊类型的作业可以通过前面提到的可执行文件来执行，该文件将充当映射器或化简器。程序的映射器实现将使用一个
    shell 脚本来执行必要的操作。这个 shell 脚本负责调用图像处理的可执行文件。这些图像文件列表作为输入传递给这些可执行文件，进行进一步处理。该处理的结果或输出稍后会写回到
    HDFS 中。
- en: So, the input image files should be written to the HDFS first, and then a file
    list is generated in a particular directory of Hadoop Streaming's input. The directory
    will store a collection of file lists. Each line of the file list will contain
    the HDFS address of the images files to be processed. The input of the mapper
    will be `Inputsplit` class, which is a text file. The shell script manager reads
    the files line by line and retrieves the images from the metadata. It then calls
    the image processing executable file for further processing of the images, and
    then write the result back to the HDFS. Hence, the output of the mapper is the
    final desired result. The mapper thus does all the jobs, retrieving the image
    file from the HDFS, image processing, and then writing it back to the HDFS. The
    number of reducers in this process can be set to zero.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，输入的图像文件应首先写入HDFS，然后在Hadoop Streaming的输入目录中生成一个文件列表。该目录将存储文件列表的集合。文件列表的每一行将包含要处理的图像文件的HDFS地址。Mapper的输入是`Inputsplit`类，这是一个文本文件。Shell脚本管理器逐行读取文件并从元数据中检索图像。接着，它调用图像处理可执行文件进一步处理图像，然后将结果写回HDFS。因此，Mapper的输出就是最终所需的结果。Mapper完成了所有工作，包括从HDFS检索图像文件、图像处理，然后将其写回HDFS。该过程中的Reducer数量可以设置为零。
- en: This is a simple design of how to process large numbers of images using Hadoop
    by the binary image processing method. Other complex image processing methods
    can also be deployed to process large-scale image datasets.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个使用Hadoop按二进制图像处理方法处理大量图像的简单设计。其他复杂的图像处理方法也可以部署来处理大规模的图像数据集。
- en: Natural language processing using Hadoop
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Hadoop进行自然语言处理
- en: The exponential growth of information in the Web has increased the intensity
    of diffusion of large-scale unstructured natural language textual resources. Hence,
    in the last few years, the interest to extract, process, and share this information
    has increased substantially. Processing these sources of knowledge within a stipulated
    time frame has turned out to be a major challenge for various research and commercial
    industries. In this section, we will describe the process used to crawl the web
    documents, discover the information and run natural language processing in a distributed
    manner using Hadoop.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 网络信息的指数增长增加了大规模非结构化自然语言文本资源的传播强度。因此，在过去几年中，提取、处理和共享这些信息的兴趣显著增加。在规定的时间内处理这些知识源已成为各个研究和商业行业面临的主要挑战。在这一部分，我们将描述使用Hadoop以分布式方式爬取网页文档、发现信息并运行自然语言处理的过程。
- en: To design architecture for **natural language processing** (**NLP**), the first
    task to be performed is the extraction of annotated keywords and key phrases from
    the large-scale unstructured data. To perform the NLP on a distributed architecture,
    the Apache Hadoop framework can be chosen for its efficient and scalable solution,
    and also to improve the failure handling and data integrity. The large-scale web
    crawler can be set to extract all the unstructured data from the Web and write
    it in the Hadoop Distributed File System for further processing. To perform the
    particular NLP tasks, we can use the open source GATE application as shown in
    the paper [136]. An overview of the tentative design of a distributed natural
    language processing architecture is shown in *Figure 7.2*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设计**自然语言处理**（**NLP**）的架构，首先要进行的任务是从大规模非结构化数据中提取标注的关键词和关键短语。为了在分布式架构上执行NLP，可以选择Apache
    Hadoop框架，因为它提供了高效且可扩展的解决方案，并且能改善故障处理和数据完整性。大规模网页爬虫可以被设置为从Web中提取所有非结构化数据并将其写入Hadoop分布式文件系统以供进一步处理。为了执行特定的NLP任务，我们可以使用开源的GATE应用程序，如论文[136]所示。分布式自然语言处理架构的初步设计概览如*图7.2*所示。
- en: To distribute the working of the web crawler, map-reduce can be used and run
    across multiple nodes. The execution of the NLP tasks and also the writing of
    the final output is performed with Map-reduce. The whole architecture will depend
    on two input files i) the `seedurls` given for crawling a particular web page
    stored in `seed_urls.txt` and ii) the path location of the NLP application (such
    as where GATE is installed). The web crawler will take `seedurls` from the `.txt`
    file and run the crawler for those in parallel. Asynchronously, an extraction
    plugin searches the keywords and key phrases on the crawled web pages and executes
    independently along with the web pages crawled. At the last step, a dedicated
    program stores the extracted keywords and key phrases in an external SQL database
    or a NoSQL database such as `Elasticsearch`, as per the requirements. All these
    modules mentioned in the architecture are described in the following subsections.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分配网络爬虫的工作，可以使用 Map-reduce 并在多个节点上运行。NLP 任务的执行以及最终输出的写入都是通过 Map-reduce 来完成的。整个架构将依赖于两个输入文件：i）存储在
    `seed_urls.txt` 中的用于抓取特定网页的 `seedurls`，ii）NLP 应用程序的路径位置（例如 GATE 的安装位置）。网络爬虫将从
    `.txt` 文件中获取 `seedurls`，并为这些 URL 并行运行爬虫。异步地，一个提取插件将在抓取的网页上搜索关键词和关键短语，并与网页一起独立执行。在最后一步，一个专用程序会根据需求将提取的关键词和关键短语存储在外部
    SQL 数据库或 NoSQL 数据库中，如 `Elasticsearch`。架构中提到的所有这些模块将在以下子章节中进行描述。
- en: Web crawler
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络爬虫
- en: To explain this phase, we won't go into a deep explanation, as it's almost out
    of the scope of this book. Web crawling has a few different phases. The first
    phase is the URL discovery stage, where the process takes each seed URL as the
    input of the `seed_urls.txt` file and navigates through the pagination URLs to
    discover relevant URLs. This phase defines the set of URLs that are going to be
    fetched in the next phase.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释这一阶段，我们不打算深入讨论，因为这几乎超出了本书的范围。网络爬虫有几个不同的阶段。第一阶段是 URL 发现阶段，该过程将每个种子 URL 作为
    `seed_urls.txt` 文件的输入，并通过分页 URL 来发现相关的 URL。这个阶段定义了下一阶段要抓取的 URL 集合。
- en: The next phase is fetching the page content of the URLs and saving in the disk.
    The operation is done segment-wise, where each segment will contain some predefined
    numbers of URLs. The operation will run in parallel on different `DataNodes`.
    The final outcome of the phases is stored in the Hadoop Distributed File System.
    The Keyword extractor will work on these saved page contents for the next phase.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 下一阶段是抓取 URL 页面内容并保存在磁盘中。操作是按段进行的，每个段包含一定数量的预定义 URL。操作将在不同的 `DataNode` 上并行执行。各个阶段的最终结果将保存在
    Hadoop 分布式文件系统中。关键词提取器将在这些保存的页面内容上工作，为下一阶段做准备。
- en: '![Web crawler](img/B05883_07_02-1.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![网络爬虫](img/B05883_07_02-1.jpg)'
- en: 'Figure 7.2: The representation of  how natural language processing is performed
    in Hadoop that is going to be fetched in the next phase. The next phase is fetching
    the page content of the URLs and saving in the disk. The operation is done segment
    wise, where each segment will contain some pre-defined numbers of URLs. The operation
    will run in parallel on different DataNodes. The final outcome of the phases is stored
    in Hadoop Distributed File System. The keyword extractor will work on these saved
    page contents for the next phase.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2：展示了自然语言处理在 Hadoop 中的执行过程，下一阶段将会获取这些数据。下一阶段是抓取 URL 页面内容并保存在磁盘中。该操作是按段进行的，每个段包含一定数量的预定义
    URL。操作将在不同的 DataNode 上并行执行。各个阶段的最终结果将保存在 Hadoop 分布式文件系统中。关键词提取器将在这些保存的页面内容上工作，为下一阶段做准备。
- en: Extraction of keyword and module for natural language processing
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键词提取和自然语言处理模块
- en: For the page content of each URL, a **Document Object Model** (**DOM**) is created
    and stored back in the HDFS. In the *DOM*, documents have a logical structure
    like a tree. Using DOM, one can write the `xpath` to collect the required keywords
    and phrases in the natural language processing phase. In this module, we will
    define the Map-reduce job for executing the natural language processing application
    for the next phase. The map function defined as a `<key, value>` pair key is the
    URL, and values are a corresponding DOM of the URL. The *reduce* function will
    perform the configuration and execution of the natural language processing part.
    The subsequent estimation of the extracted keywords and phrases at the web domain
    level will be performed in the `reduce` method. For this purpose, we can write
    a custom plugin to generate the rule files to perform various string manipulations
    to filter out the noisy, undesired words from the extracted texts. The rule files
    can be a JSON file or any other easy to load and interpret file based on the use
    case. Preferably, the common nouns and adjectives are identified as common keywords
    from the texts.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个 URL 的页面内容，创建并存储一个 **文档对象模型** (**DOM**) 回到 HDFS 中。在 *DOM* 中，文档具有类似树的逻辑结构。通过使用
    DOM，可以编写 `xpath` 来收集自然语言处理阶段所需的关键词和短语。在本模块中，我们将定义 Map-reduce 作业来执行下一阶段的自然语言处理应用程序。定义的
    map 函数是一个 `<key, value>` 对，key 是 URL，value 是该 URL 对应的 DOM。*reduce* 函数将执行自然语言处理部分的配置和执行。接下来的关键词和短语在网页域级别的估算将在
    `reduce` 方法中执行。为此，我们可以编写一个自定义插件来生成规则文件，通过执行各种字符串操作，过滤掉从提取文本中获得的噪声和不需要的词汇。规则文件可以是
    JSON 文件或任何其他易于加载和解析的文件，具体取决于用例。通常，我们将常见的名词和形容词识别为文本中的常见关键词。
- en: Estimation of relevant keywords from a page
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从页面估算相关关键词
- en: The paper [136] has presented a very important formulation to find the relevant
    keywords and key phrases from a web document. They have provided the **Term Frequency
    - Inverse Document Frequency** (**TF-IDF**) metric to estimate the relevant information
    from the whole corpus, composed of all the documents and pages that belong to
    a single web domain. Computing the value of *TD-IDF* and assigning it a threshold
    value for discarding other keywords allows us to generate the most relevant words
    from the corpus. In other words, it discards the common articles and conjunctions
    that might have a high frequency of occurrence in the text, but generally do not
    possess any meaningful information. The *TF-IDF* metric is basically the product
    of two functions, *TF* and *IDF*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 论文 [136] 提出了一个非常重要的公式，用于从网页文档中找到相关的关键词和关键短语。他们提供了 **词频 - 逆文档频率** (**TF-IDF**)
    度量，用于从整个语料库中估算相关信息，语料库由属于同一网页域的所有文档和页面组成。计算 *TF-IDF* 的值并为丢弃其他关键词设定阈值，使我们能够从语料库中生成最相关的词语。换句话说，它丢弃了可能在文本中出现频率较高的常见冠词和连词，这些词通常不包含任何有意义的信息。*TF-IDF*
    度量本质上是两个函数 *TF* 和 *IDF* 的乘积。
- en: '*TF* provides the frequency of each word in the corpus, that is, how many times
    a word is present in the corpus. Whereas *IDF* behaves as a balance term, showing
    higher values for terms having the lower frequency in the whole corpus.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*TF* 提供了每个词在语料库中的频率，即一个词在语料库中出现的次数。而 *IDF* 起到了平衡作用，对于在整个语料库中出现频率较低的词，给出较高的值。'
- en: 'Mathematically, the metric *TF-IDF* for a keyword or key phrase *i* in a document
    *d* contained in the document *D* is given by the following equation:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，关键词或关键短语 *i* 在文档 *d* 中的 *TF-IDF* 度量公式如下：
- en: '*(TF-IDF)[i] = TF[i] . IDF[i]*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*(TF-IDF)[i] = TF[i] . IDF[i]*'
- en: Here *TF[i] = f[i]/n[d]* and *IDF[i] = log N[d]/N[i]*
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 *TF[i] = f[i]/n[d]* 和 *IDF[i] = log N[d]/N[i]*
- en: Here *f[i]* is the frequency of the candidate keyword or key phrase *i* in the
    document *d* and *n[d]* is the total number of terms in the document *d*. In *IDF*,
    *N[D] *denotes the total number of documents present in the corpus *D*, whereas
    *N[i]* denotes the number of documents in which the keyword or key phrase *i*
    is present.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 *f[i]* 是候选关键词或关键短语 *i* 在文档 *d* 中的频率，*n[d]* 是文档 *d* 中的总词数。在 *IDF* 中，*N[D]*
    表示语料库 *D* 中的文档总数，而 *N[i]* 表示包含关键词或关键短语 *i* 的文档数量。
- en: Based on the use cases, one should define a generic threshold frequency for
    *TF-IDF*. For a keyword or key phrase *i* if the value of *TF-IDF* becomes higher
    than the threshold value, that keyword or key phrase is accepted as final as written
    directly to the HDFS. On the other hand, if the corresponding value is less than
    the threshold value, that keyword is dropped from the final collection. In that
    way, finally, all the desired keywords will be written to the HDFS.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 根据使用案例，应该为*TF-IDF*定义一个通用的阈值频率。对于一个关键字或关键短语*i*，如果*TF-IDF*的值超过阈值，该关键字或关键短语将被接受并直接写入HDFS。另一方面，如果相应的值低于阈值，则该关键字将从最终集合中删除。通过这种方式，最终所有所需的关键字将被写入HDFS。
- en: Summary
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter discussed the most widely used applications of Machine learning
    and how they can be designed in the Hadoop framework. First, we started with a
    large video set and showed how the video can be decoded in the HDFS and later
    converted into a sequence file containing images for later processing. Large-scale
    image processing was discussed next in the chapter. The mapper used for this purpose
    has a shell script which performs all the tasks necessary. So, no reducer is necessary
    to perform this operation. Finally, we discussed how the natural language processing
    model can be deployed in Hadoop.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了机器学习中最广泛使用的应用程序以及如何在Hadoop框架中设计它们。首先，我们从一个大型视频集开始，展示了如何在HDFS中解码视频，并将其转换为包含图像的序列文件，以便后续处理。接下来，本章讨论了大规模图像处理。用于此目的的mapper有一个shell脚本，执行所有必要的任务。因此，不需要reducer来执行此操作。最后，我们讨论了如何将自然语言处理模型部署到Hadoop中。
