- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Harnessing the Power of RAG
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用 RAG 的力量
- en: By now, we know the FMs are trained using large datasets. However, the data
    used to train FMs might not be recent, and this can cause the models to hallucinate.
    In this chapter, we will harness the power of RAG by augmenting the model with
    external data sources to overcome the challenge of hallucination.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们知道 FM 是使用大量数据集进行训练的。然而，用于训练 FM 的数据可能不是最新的，这可能导致模型产生幻觉。在本章中，我们将通过增强模型以外部数据源来利用
    RAG 的力量，以克服幻觉的挑战。
- en: We will explore the importance of RAG in generative AI scenarios, how RAG works,
    and its components. We will then delve into the integration of RAG with Amazon
    Bedrock, including a fully managed RAG experience by Amazon Bedrock called Knowledge
    Bases. The chapter will then take a hands-on approach to the implementation of
    Knowledge Bases and using APIs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨 RAG 在生成式 AI 场景中的重要性，RAG 的工作原理及其组件。然后我们将深入研究 RAG 与 Amazon Bedrock 的集成，包括
    Amazon Bedrock 提供的完全管理的 RAG 体验，称为知识库。本章将随后以实践方法介绍知识库的实现和使用 API。
- en: We will explore some real-world scenarios of RAG and discuss a few solution
    architectures for implementing RAG. You will also be introduced to implementing
    a RAG framework using Amazon Bedrock, LangChain orchestration, and other generative
    AI systems. We will end by examining current limitations and future research directions
    with Amazon Bedrock in the context of RAG.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨一些 RAG 的实际场景，并讨论一些实现 RAG 的解决方案架构。您还将了解到如何使用 Amazon Bedrock、LangChain 调度和其他生成式
    AI 系统实现 RAG 框架。最后，我们将探讨在 RAG 的背景下，Amazon Bedrock 的当前局限性和未来研究方向。
- en: By the end of this chapter, you will be able to understand the importance of
    RAG and will be able to implement it with Amazon Bedrock. Learning these methods
    will empower you to apply the concept of RAG in your own enterprise use cases
    and build production-level applications, such as conversational interfaces, question
    answering systems, or module summarization workflows.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将能够理解 RAG 的重要性，并能够使用 Amazon Bedrock 来实现它。学习这些方法将使您能够将 RAG 的概念应用于您自己的企业用例，并构建生产级别的应用程序，例如对话界面、问答系统或模块摘要工作流程。
- en: 'Here are the key topics that will be covered in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下关键主题：
- en: Decoding RAG
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码 RAG
- en: Implementing RAG with Amazon Bedrock
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Amazon Bedrock 实现 RAG
- en: Implementing RAG with other methods
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用其他方法实现 RAG
- en: Advanced RAG techniques
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级 RAG 技术
- en: Limitations and future directions
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局限性和未来方向
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter requires you to have access to an AWS account. If you don’t have
    one already, you can go to [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    and create an AWS account.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求您有权访问 AWS 账户。如果您还没有账户，可以访问 [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    创建 AWS 账户。
- en: 'Secondly, you will need to install and configure AWS CLI ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))
    after you create an account, which will be needed to access Amazon Bedrock FMs
    from your local machine. Since the majority chunk of code cells we will be executing
    is based in Python, setting up an AWS Python SDK (Boto3) ([https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html))
    would be beneficial at this point. You can carry out the Python setup in these
    ways: install it on your local machine, or use AWS Cloud9, or AWS Lambda, or leverage
    Amazon SageMaker.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，您需要在创建账户后安装和配置 AWS CLI ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))，这将用于从您的本地机器访问
    Amazon Bedrock FMs。由于我们将要执行的代码单元格的大部分基于 Python，此时设置 AWS Python SDK (Boto3) ([https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html))
    将非常有用。您可以通过以下方式执行 Python 设置：在您的本地机器上安装它，或使用 AWS Cloud9、AWS Lambda，或利用 Amazon SageMaker。
- en: Note
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: There will be a charge associated with the invocation and customization of FMs
    of Amazon Bedrock. Please refer to [https://aws.amazon.com/bedrock/pricing/](https://aws.amazon.com/bedrock/pricing/)
    to learn more.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Amazon Bedrock 的 FMs 的调用和定制将产生费用。请参阅 [https://aws.amazon.com/bedrock/pricing/](https://aws.amazon.com/bedrock/pricing/)
    了解更多信息。
- en: Decoding RAG
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码 RAG
- en: RAG is an approach in NLP that combines large-scale retrieval with neural generative
    models. The key idea is to retrieve relevant knowledge from large corpora and
    incorporate that knowledge into the text-generation process. This allows generative
    models such as Amazon Titan Text, Anthropic Claude, and **Generative Pre-trained
    Transformer 3** (**GPT-3**) to produce more factual, specific, and coherent text
    by grounding generations in external knowledge.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是一种NLP方法，它结合了大规模检索和神经生成模型。关键思想是从大型语料库中检索相关知识并将其纳入文本生成过程。这使得生成模型如Amazon Titan
    Text、Anthropic Claude和**生成预训练Transformer 3**（**GPT-3**）能够通过在外部知识的基础上进行生成，产生更真实、具体和连贯的文本。
- en: RAG has emerged as a promising technique to make neural generative models more
    knowledgeable and controllable. In this section, we will provide an overview of
    RAG, explain how it works, and discuss key applications.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: RAG已成为一种有前途的技术，可以使神经生成模型更加知识渊博和可控。在本节中，我们将概述RAG，解释它是如何工作的，并讨论关键应用。
- en: What is RAG?
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是RAG？
- en: Traditional generative models, such as BART, T5 or GPT-4 are trained on vast
    amounts of text data in a self-supervised fashion. While this allows them to generate
    fluent and human-like text, a major limitation is that they lack world knowledge
    beyond what is contained in their training data. This can lead to factual inconsistencies,
    repetitions, and hallucinations in the generated text.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的生成模型，如BART、T5或GPT-4，以自监督的方式在大量的文本数据上进行训练。虽然这使它们能够生成流畅且类似人类的文本，但一个主要的局限性是它们缺乏训练数据之外的世界知识。这可能导致生成文本中的事实不一致、重复和幻觉。
- en: RAG aims to ground generations in knowledge by retrieving relevant context from
    large external corpora. For example, if the model is generating text about Paris,
    it could retrieve *Wikipedia* passages about Paris to inform the generation. This
    retrieved context is encoded and integrated into the model to guide the text generation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: RAG旨在通过从大型外部语料库中检索相关上下文来将生成内容与知识联系起来。例如，如果模型正在生成关于巴黎的文本，它可能会检索关于巴黎的*维基百科*段落来指导生成。这个检索到的上下文被编码并整合到模型中以指导文本生成。
- en: Augmenting generative models with retrieved knowledge has been shown to produce
    more factual, specific, and coherent text across a variety of domains.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将检索到的知识增强到生成模型中，已经在各种领域产生了更真实、具体和连贯的文本。
- en: 'The key components of RAG systems are the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: RAG系统的关键组件如下：
- en: A GenAI model – specifically, an FM or LLM – that can generate fluent text (or
    multi-modal) outputs.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个能够生成流畅文本（或多模态）输出的GenAI模型——具体来说，是一个FM或LLM。
- en: A corpus of data to retrieve relevant information from (for example, *Wikipedia*,
    web pages, documents).
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于从数据集中检索相关信息的数据集（例如，*维基百科*，网页，文档）。
- en: Retriever module, which encodes the input query and retrieves relevant passages
    from the knowledge corpus based on relevance to the query.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索模块，它编码输入查询并根据与查询的相关性从知识语料库中检索相关段落。
- en: Re-ranker to select the optimal contextual information by re-scoring and ranking
    the retrieved passages based on relevance to the query. (This step is optional
    in building basic RAG systems but becomes crucial when building enterprise-scale
    systems with advanced RAG techniques).
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新排序器，通过重新评分和根据与查询的相关性对检索到的段落进行排序来选择最佳上下文信息。（在构建基本的RAG系统时，这一步是可选的，但在构建具有高级RAG技术的企业级系统时变得至关重要）。
- en: Fusion module to integrate retrieval into the language model. This can involve
    techniques such as concatenation or allowing the language model to condition on
    relevant external knowledge.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 融合模块，用于将检索集成到语言模型中。这可能涉及诸如连接或允许语言模型根据相关外部知识进行条件化等技术。
- en: Other components may also include query reformulation, hybrid search techniques,
    and multi-stage retrieval, which will be covered later in this chapter.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他组件可能还包括查询重写、混合搜索技术和多阶段检索，这些将在本章后面进行介绍。
- en: 'In order to gain a better understanding of RAG approaches, let us walk through
    a simple example:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解RAG方法，让我们通过一个简单的例子来了解一下：
- en: '`What are` `the key events in the life of` `Marie Curie?`'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`What are` `the key events in the life of` `Marie Curie?`'
- en: '`What are the key events in the life of Marie Curie?` into a dense vector representation.
    It then searches through the knowledge corpus (for example, *Wikipedia*, web pages)
    to find relevant passages. For example, it may retrieve the following:'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`What are the key events in the life of Marie Curie?`转换成一个密集的向量表示。然后它通过知识语料库（例如，*维基百科*，网页）来查找相关段落。例如，它可能会检索以下内容：
- en: '`Marie Curie was a Polish physicist and chemist who conducted pioneering research`
    `on radioactivity...`'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`玛丽·居里是一位波兰物理学家和化学家，她进行了关于放射性` `的先驱研究...`'
- en: '`In 1903, Curie became the first woman to win a Nobel Prize for her study of`
    `spontaneous radiation...`'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`1903年，居里因研究` `自发辐射` `而成为第一位获得诺贝尔奖的女性。`'
- en: '`Curie won a second Nobel Prize in 1911, this time in chemistry, for her discovery
    of the elements radium` `and polonium...`'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`居里在1911年获得了第二个诺贝尔奖，这次是化学奖，因为她发现了元素镭` `和钋...`'
- en: '**Re-ranker**: The re-ranker scores and re-ranks the retrieved passages based
    on their relevance to the original query using cross-attention. It may determine
    that passages *II* and *III* are more relevant than *I*.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重新排序器**：重新排序器根据与原始查询的相关性使用交叉注意力对检索到的段落进行评分和重新排序。它可能确定段落 *II* 和 *III* 比段落
    *I* 更相关。'
- en: '**Fusion module**: The top re-ranked passages (for example, *II* and *III*)
    are then integrated into the generative language model, either by concatenating
    them, summarizing them, or allowing the model to attend over them; that is, focus
    on different parts of the retrieved passages as needed while generating the output.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**融合模块**：然后，将最高重新排序的段落（例如，*II* 和 *III*）整合到生成语言模型中，通过连接、总结或允许模型关注检索到的段落的不同部分来实现；也就是说，在生成输出时根据需要关注检索到的段落的不同部分。'
- en: Note that the goal of the fusion step is to provide the generative language
    model with the most pertinent external knowledge in a manner that allows effective
    conditioning of the generated output on that knowledge, leading to more accurate,
    informative, and grounded responses.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，融合步骤的目标是以一种允许对生成的输出进行有效条件化知识的方式，向生成语言模型提供最相关的外部知识，从而产生更准确、信息丰富和有根据的响应。
- en: '`The key events in the life of Marie` `Curie include:`'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`玛丽·居里的关键事件包括：`'
- en: '`In 1903, she became the first woman to win a Nobel Prize for her study of
    spontaneous` `radiation (radioactivity).`'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1903年，她因研究` `自发辐射` `（放射性）而成为第一位获得诺贝尔奖的女性。`'
- en: '`In 1911, she won a second Nobel Prize in chemistry for her discovery of the
    elements radium` `and polonium.`'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1911年，她因发现元素镭` `和钋而获得了第二个诺贝尔化学奖。`'
- en: By retrieving relevant knowledge from an external corpus and integrating it
    into the language model, the RAG system can generate a more informative and accurate
    response, overcoming the limitations of relying solely on the model’s training
    data.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过从外部语料库检索相关知识和将其整合到语言模型中，RAG系统可以生成更信息丰富和准确的响应，克服了仅依赖模型训练数据的局限性。
- en: Note
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Dense vector representations, also known as dense embeddings or dense vectors,
    are a way of encoding meaning and semantic relationships in a numerical format
    that can be effectively processed by machines. This allows techniques such as
    cosine similarity to identify semantically related words/texts even without exact
    keyword matches. Dense vectors power many modern NLP applications, such as semantic
    search, text generation, translation, and so on, by providing effective semantic
    representations as inputs to **deep** **learning** models.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 稠密向量表示，也称为密集嵌入或密集向量，是一种将意义和语义关系编码为机器可以有效地处理的数值格式的方法。这允许使用余弦相似度等技术识别语义相关的单词/文本，即使没有精确的关键词匹配。密集向量通过为深度学习模型提供有效的语义表示，为许多现代NLP应用（如语义搜索、文本生成、翻译等）提供动力。
- en: We will further dive deep into these components in the *Components of RAG* section.
    Since you now have a brief understanding of RAG, it’s time to realize the importance
    of RAG in the context of the GenAI universe.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*RAG组件*部分进一步深入探讨这些组件。既然你现在对RAG有了简要的了解，是时候认识到RAG在GenAI宇宙中的重要性了。
- en: Importance of RAG
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG的重要性
- en: Before we dive into how RAG works and its components, it’s important to understand
    why RAG is needed. As LLMs become more capable of generating fluent and coherent
    text, it also becomes more important to ground them in factual knowledge and guard
    against potential hallucinations. If you ask an LLM questions pertaining to recent
    events, you might notice the model to be hallucinating. With RAG, you can augment
    the latest knowledge as context to the model to improve content quality by reducing
    the chance of factual errors.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨RAG的工作原理及其组件之前，了解为什么需要RAG是很重要的。随着大型语言模型越来越能够生成流畅和连贯的文本，将它们基于事实知识进行定位并防止潜在的幻觉也变得更加重要。如果你向LLM提出与最近事件相关的问题，你可能会注意到模型正在进行幻觉。通过RAG，你可以将最新知识作为模型的上下文来增强，通过减少事实错误的可能性来提高内容质量。
- en: 'Another major advantage of RAG is overcoming the limited context length (input
    token limit) of the model. When providing pieces of text as a context that fits
    within the token limit of the model, you may not need to use RAG and leverage
    in-context prompting. However, if you want to provide a large corpus of documents
    as a context to the model, using RAG would be a better approach. However, RAG
    is beneficial even when the corpus can fit in the context due to needle-in-a-haystack
    problems, which can affect retrieval accuracy. To summarize, RAG becomes specifically
    useful in two primary use cases:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: RAG的另一个主要优势是克服了模型有限的上下文长度（输入令牌限制）。当提供的文本片段作为上下文，且在模型的令牌限制内时，你可能不需要使用RAG并利用上下文提示。然而，如果你想向模型提供一个大型文档集合作为上下文，使用RAG将是一个更好的方法。即使文档集合可以放入上下文中，RAG也有其益处，因为“大海捞针”的问题可能会影响检索准确性。总结来说，RAG在两个主要用例中特别有用：
- en: When the corpus size exceeds that of the context length
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当文档集合的大小超过上下文长度时
- en: When we want to dynamically provide context to the model instead of feeding
    it the entire corpus in context
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们想要动态地向模型提供上下文，而不是一次性提供整个文档集合时
- en: RAG has many potential applications for improving GenAI. It can help build contextual
    chatbots that rely on real enterprise data. It can enable personalized search
    and recommendations based on user history and preferences. A RAG approach can
    also aid real-time summarization of large documents by retrieving and condensing
    key facts. For example, applying RAG to summarize extensive legal texts or academic
    papers allows for the extraction and condensation of important information, providing
    succinct summaries that capture the core points. Overall, RAG is an important
    technique for overcoming some limitations of current generative models and grounding
    them in factual knowledge. This helps make the generated content more useful,
    reliable, and personalized.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: RAG在改善生成式AI方面有许多潜在的应用。它可以帮助构建依赖于真实企业数据的上下文聊天机器人。它可以根据用户的历史和偏好实现个性化搜索和推荐。RAG方法还可以通过检索和压缩关键事实来帮助实时总结大量文档。例如，将RAG应用于总结大量的法律文本或学术论文，可以提取和压缩重要信息，提供简洁的总结，捕捉核心要点。总的来说，RAG是克服当前生成模型某些限制并使其基于事实知识的重要技术。这有助于使生成内容更加有用、可靠和个性化。
- en: Key applications
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键应用
- en: 'Compared to other LLM customization techniques, such as prompt engineering
    or fine-tuning, RAG offers several advantages:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他LLM定制技术（如提示工程或微调）相比，RAG提供了几个优势：
- en: '**Flexibility of knowledge source**: The knowledge base can be customized for
    each use case without changing the underlying LLM. Knowledge can be easily added,
    removed, or updated without costly model retraining. This is especially useful
    for organizations whose knowledge is rapidly evolving.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识源的灵活性**：知识库可以根据每个用例进行定制，而无需更改底层LLM。知识可以轻松添加、删除或更新，无需昂贵的模型重新训练。这对于知识快速发展的组织特别有用。'
- en: '**Cost-effective**: RAG allows a single-hosted LLM to be shared across many
    use cases through swappable knowledge sources. There is no need to train bespoke
    models for each use case, which means greater cost efficiency.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经济高效**：RAG允许单个主机LLM通过可交换的知识源在许多用例之间共享。无需为每个用例训练定制模型，这意味着更高的成本效率。'
- en: '**Natural language queries**: RAG relies on natural language for context retrieval
    from the knowledge source, unlike prompt engineering, which uses rigid prompt
    templates. This enables users to be more flexible when working with the models.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言查询**：RAG依赖于自然语言从知识源检索上下文，与使用僵化提示模板的提示工程不同。这使得用户在与模型工作时更加灵活。'
- en: For most organizations with a custom knowledge pool of information, RAG strikes
    a balance between cost, flexibility, and usability. Prompt engineering is sufficient
    for small amounts of context, while full model fine-tuning entails high training
    costs and rigid knowledge. RAG allows easy knowledge base updates and sharing
    of LLMs across use cases.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数拥有定制信息知识库的组织来说，RAG在成本、灵活性和可用性之间取得了平衡。对于少量上下文，提示工程就足够了，而完整的模型微调则涉及高昂的训练成本和僵化的知识。RAG允许轻松更新知识库，并在不同用例之间共享LLM。
- en: For example, RAG is well suited for **business-to-business Software-as-a-Service**
    (**B2B SaaS**) companies that manage evolving document bases across many customers.
    A single-hosted LLM can handle queries across clients by swapping their context
    documents, eliminating the need for per-client models.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，RAG非常适合管理跨多个客户不断演变的文档库的**企业对企业软件即服务（B2B SaaS**）公司。单个托管的大型语言模型（LLM）可以通过交换其上下文文档来处理跨客户的查询，从而消除为每个客户创建模型的需求。
- en: Now that we understand the importance and potential applications of RAG in different
    scenarios, let us jump into exploring the working of RAG.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了RAG在不同场景中的重要性和潜在应用，让我们深入探讨RAG的工作原理。
- en: How does RAG work?
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG是如何工作的？
- en: '*Figure 5**.1* provides a high-level overview of how RAG works:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.1*提供了RAG工作原理的高级概述：'
- en: '![Figure 5.1 – Simplified RAG](img/B22045_05_01.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图5.1 – 简化的RAG](img/B22045_05_01.jpg)'
- en: Figure 5.1 – Simplified RAG
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 简化的RAG
- en: 'Let us now understand these steps in detail:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在详细理解这些步骤：
- en: Given a prompt from the user, the retriever module is invoked to encode the
    input query in a dense vector representation.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定用户的提示，检索器模块被调用以将输入查询编码为密集向量表示。
- en: The retriever module then finds relevant context (passages or documents) from
    the knowledge corpus, based on maximum inner product similarity (semantic similarity)
    between the query vector and pre-computed dense vector representations of the
    corpus contents.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索器模块随后根据查询向量与知识库内容的预计算密集向量表示之间的最大内积相似度（语义相似度），从知识库中找到相关的上下文（段落或文档）。
- en: An optional re-ranker module can then re-score and re-rank the initially retrieved
    results and select the best context passages to augment the generation. The re-ranker
    helps surface the most relevant passages.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可选的重新排序模块可以重新评分和重新排序最初检索到的结果，并选择最佳的上下文段落来增强生成。重新排序器有助于突出最相关的段落。
- en: The top-ranked retrieved contexts are fused with the input query to form an
    augmented prompt (query and context).
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 排名最高的检索上下文与输入查询融合，形成增强提示（查询和上下文）。
- en: The generative model, i.e. the FM or LLM then produces the output text conditioned
    on both the original query prompt and the retrieved relevant knowledge contexts.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成模型，即FM或LLM，随后根据原始查询提示和检索到的相关知识上下文产生输出文本。
- en: In some RAG systems, the retrieval and re-ranking process can be repeated during
    the generation step to dynamically retrieve more relevant knowledge as the output
    is being generated.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在某些RAG系统中，检索和重新排序过程可以在生成步骤中重复进行，以便在生成输出时动态检索更多相关知识。
- en: The key benefits of RAG are ensuring that the generated outputs are grounded
    in accurate and up-to-date information from trusted external sources, providing
    source citations for transparency, and reducing hallucinations or inaccuracies
    from the language model’s training data alone.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: RAG的关键好处是确保生成的输出基于来自可信外部来源的准确和最新信息，提供源引用以实现透明度，并减少仅从语言模型的训练数据中产生的幻觉或不准确性。
- en: RAG systems meet enterprise requirements for GenAI, such as being comprehensive,
    trustworthy, transparent, and credible by properly sourcing, vetting, and customizing
    the underlying data sources and models for specific use cases.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: RAG系统通过适当来源、审查和定制特定用例的基础数据源和模型，满足企业对通用人工智能（GenAI）的需求，如全面性、可信性、透明性和可靠性。
- en: Components of RAG
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG的组成部分
- en: As explained earlier, once a query is received, relevant context is retrieved
    from the knowledge source and condensed into a context document. This context
    is then concatenated with the original query and fed into the LLM to generate
    a final response. The knowledge source acts as a dynamic long-term memory, in
    a way that can be frequently updated, while the LLM contributes its strong language
    generation capabilities.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，一旦收到查询，就会从知识源检索相关上下文并将其压缩为上下文文档。然后，将此上下文与原始查询连接起来，输入到LLM中以生成最终响应。知识源充当可频繁更新的动态长期记忆，而LLM则贡献其强大的语言生成能力。
- en: The knowledge base
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 知识库
- en: A key component of RAG models is the knowledge base, which contains the external
    knowledge used for retrieval. The knowledge base stores information in a format
    optimized for fast retrieval, such as dense vectors or indexes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: RAG模型的一个关键组件是知识库，它包含用于检索的外部知识。知识库以优化快速检索的格式存储信息，例如密集向量或索引。
- en: Popular knowledge sources used in RAG include *Wikipedia*, news archives, books,
    scientific papers, and proprietary knowledge bases created specifically for RAG
    models. The knowledge can consist of both structured (for example, tables and
    lists) and unstructured (for example, free text) data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG中使用的流行知识源包括*维基百科*、新闻档案、书籍、科学论文以及专门为RAG模型创建的专有知识库。知识可以包括结构化（例如，表格和列表）和非结构化（例如，自由文本）数据。
- en: In a typical RAG scenario, the textual contents of the documents (or web pages)
    that make up the knowledge corpus and need to be converted into dense vector representations
    or embeddings are encoded data into smaller chunks. To preserve this structure
    of tables or lists while encoding, more advanced encoding techniques are used
    that can embed entire tables/lists as a single vector while retaining their row/column
    relationships.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的RAG场景中，构成知识语料库的文档（或网页）的文本内容需要被转换成密集向量表示或嵌入，这些编码数据被分成更小的块。为了在编码时保留表格或列表的结构，使用了更高级的编码技术，这些技术可以将整个表格/列表嵌入为一个单一的向量，同时保留它们的行/列关系。
- en: Long unstructured text passages are typically chunked or split into smaller
    text segments or passages of a maximum length (for example, 200 tokens). Each
    of these chunks or passages is then encoded independently into a dense vector
    representation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 长的非结构化文本段落通常会被分成更小的文本段或最大长度（例如，200个标记）的段落。然后，这些块或段落被独立地编码成密集向量表示。
- en: This embedding process typically happens asynchronously or as a batch process,
    separate from and ahead of time before any user queries are received by the RAG
    system.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此嵌入过程通常在异步或批量处理中发生，在接收任何用户查询之前，与RAG系统分开进行。
- en: 'The embeddings for the entire document corpus are pre-computed and stored before
    the system is deployed or used for any query answering. This pre-computation step
    is necessary for the following reasons:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在系统部署或用于任何查询回答之前，整个文档语料库的嵌入被预先计算并存储。这一预计算步骤的必要性有以下原因：
- en: The document corpus can typically be very large (for example, *Wikipedia* has
    millions of articles)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档语料库通常可能非常大（例如，*维基百科*有数百万篇文章）
- en: Embedding the full corpus at query time would be extremely slow and inefficient
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在查询时嵌入整个语料库将非常慢且效率低下
- en: Pre-computed embeddings allow fast maximum inner product search at query time
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预计算的嵌入允许在查询时快速进行最大内积搜索
- en: By embedding the sources asynchronously ahead of time, the RAG system can quickly
    retrieve relevant documents by comparing the query embedding against the pre-computed
    document embeddings using efficient vector similarity search methods such as cosine
    similarity, Euclidean distance, **Microprocessor without Interlocked Pipelined
    Stages** (**MIPS**), or **Facebook AI Similarity Search** (**FAISS**). Readers
    are encouraged to review the paper *A Survey on Efficient Processing of Similarity
    Queries over Neural Embeddings* ([https://arxiv.org/abs/2204.07922](https://arxiv.org/abs/2204.07922)),
    which debriefs methods on efficient processing of similarity queries.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提前异步嵌入来源，RAG系统可以通过比较查询嵌入与预先计算的文档嵌入，使用高效的向量相似性搜索方法（如余弦相似度、欧几里得距离、**无互锁流水级阶段的微处理器**（**MIPS**）或**Facebook
    AI相似性搜索**（**FAISS**））快速检索相关文档。鼓励读者回顾论文《关于在神经嵌入上高效处理相似性查询的综述》([https://arxiv.org/abs/2204.07922](https://arxiv.org/abs/2204.07922))，该论文概述了高效处理相似性查询的方法。
- en: Note that the sizes and scope of the knowledge base has a major influence on
    the capabilities of the RAG system. Larger knowledge bases with more diverse,
    high-quality knowledge provide more contextual information for the model to draw
    from, based on the user’s questions.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，知识库的大小和范围对RAG系统的能力有重大影响。具有更多样化、高质量知识的更大知识库为模型提供了更多上下文信息，以便根据用户的问题进行抽取。
- en: Retriever module
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检索模块
- en: The retriever module is responsible for finding and retrieving the most relevant
    knowledge from the knowledge base for each specific context. The input to the
    retrieval model is typically the prompt or context from the user.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 检索模块负责为每个特定上下文从知识库中找到和检索最相关的知识。检索模型的输入通常是用户的提示或上下文。
- en: The embedding model encodes the prompt into a vector representation and matches
    it against encoded representations of the knowledge base to find the closest matching
    entries.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入模型将提示编码成向量表示，并将其与知识库的编码表示进行匹配，以找到最接近的匹配条目。
- en: Common retrieval methods include sparse methods such as **Term Frequency-Inverse
    Document Frequency** (**TF-IDF**) or **Best Match 25** (**BM25**), as well as
    dense methods such as semantic search over embedded representations from a dual-encoder
    model. The retrieval model ranks the knowledge and returns the top *k* most relevant
    pieces back to the generative model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的检索方法包括稀疏方法，如**词频-逆文档频率**（**TF-IDF**）或**最佳匹配25**（**BM25**），以及密集方法，如从双编码器模型中提取的嵌入表示的语义搜索。检索模型对知识进行排序，并将最相关的*
    k *个片段返回给生成模型。
- en: The tighter the integration between the retrieval model and the generative model,
    the better the retrieval results.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 检索模型与生成模型之间的集成越紧密，检索结果越好。
- en: Conditioning the generative model
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 条件化生成模型
- en: The key aspect that makes the RAG process generative is the conditional generative
    model. This model takes the retrieved knowledge along with the original prompt
    and generates the output text.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使RAG过程成为生成性的关键方面是条件生成模型。该模型将检索到的知识与原始提示一起生成输出文本。
- en: 'The knowledge can be provided in different ways to condition the generation:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 知识可以通过不同的方式提供以条件化生成：
- en: Concatenating the retrieved text to the prompt
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将检索到的文本连接到提示中
- en: Encoding the retrieved text into dense vectors
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将检索到的文本编码为密集向量
- en: Inserting the retrieved text into the input at particular positions
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在特定位置将检索到的文本插入到输入中
- en: For example, in a typical scenario, the retrieved knowledge is augmented with
    the input prompt and fed to the LLM to provide a succinct response to the end
    user. This allows the LLM to directly condition the text generation on the relevant
    facts and context. Users are encouraged to check out the paper *Leveraging Passage
    Retrieval with Generative Models for Open Domain Question Answering* ([https://arxiv.org/pdf/2007.01282.pdf](https://arxiv.org/pdf/2007.01282.pdf))
    in order to gain a deeper understanding of the complexity of RAG in the realm
    of question answering frameworks.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在典型场景中，检索到的知识被输入提示增强，并输入到LLM中，以向最终用户提供简洁的响应。这允许LLM直接根据相关事实和上下文对文本生成进行条件化。鼓励用户查阅论文《利用生成模型进行开放域问答中的段落检索》（[https://arxiv.org/pdf/2007.01282.pdf](https://arxiv.org/pdf/2007.01282.pdf)），以深入了解问答框架领域中RAG的复杂性。
- en: The generative model is usually a large pre-trained language model such as GPT-4,
    Anthropic Claude 3, Amazon Titan Text G1, and so on. The model can be further
    fine-tuned end to end on downstream RAG tasks, if needed, in order to optimize
    the integration of the retrieved knowledge for domain-specific use cases. Now,
    let us dive into exploring RAG with Amazon Bedrock.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型通常是大型预训练语言模型，如GPT-4、Anthropic Claude 3、Amazon Titan Text G1等。如果需要，该模型可以进一步在下游RAG任务上进行端到端微调，以优化特定用例中检索知识的集成。现在，让我们深入了解使用Amazon
    Bedrock探索RAG。
- en: Implementing RAG with Amazon Bedrock
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Amazon Bedrock实现RAG
- en: 'Prior to responding to user queries, the system must ingest and index the provided
    documents. This process can be considered as *step 0*, and consists of these sub-steps:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在响应用户查询之前，系统必须摄入和索引提供的文档。这个过程可以被认为是*步骤0*，包括以下子步骤：
- en: Ingest the raw text documents into the knowledge base.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将原始文本文档摄入到知识库中。
- en: Preprocess the documents by splitting them into smaller chunks to enable more
    granular retrieval.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将文档分割成更小的块来预处理文档，以实现更细粒度的检索。
- en: Generate dense vector representations for each passage using an embedding model
    such as Amazon Bedrock’s Titan Text Embeddings model. This encodes the semantic
    meaning of each passage into a high-dimensional vector space.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用嵌入模型（如Amazon Bedrock的Titan Text Embeddings模型）为每个段落生成密集向量表示。这把每个段落的语义意义编码到高维向量空间中。
- en: Index the passages and their corresponding vector embeddings into a specialized
    search index optimized for efficient **nearest neighbor** (**NN**) search. These
    are also referred to as **vector databases**, which store numerical representations
    of text in the form of vectors. This index powers fast retrieval of the most relevant
    passages in response to user queries.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将段落及其对应的向量嵌入索引到一个专门为高效**最近邻**（**NN**）搜索优化的搜索索引中。这些也被称为**向量数据库**，它们以向量的形式存储文本的数值表示。此索引能够快速检索对用户查询最相关的段落。
- en: By completing this workflow, the system constructs an indexed corpus ready to
    serve relevant results for natural language queries over the ingested document
    collection. The passage splitting, embedding, and indexing steps enable robust
    ranking and retrieval capabilities.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通过完成此工作流程，系统构建了一个索引语料库，准备好为摄入的文档集合中的自然语言查询提供相关结果。段落分割、嵌入和索引步骤使系统能够实现强大的排名和检索能力。
- en: 'The flow diagram depicted in *Figure 5**.2* exemplifies the overall flow of
    the RAG process as described previously:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.2*中展示的流程图展示了之前描述的RAG过程的总体流程：'
- en: '![Figure 5.2 – RAG with Amazon Bedrock](img/B22045_05_02.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图5.2 – 使用亚马逊Bedrock的RAG](img/B22045_05_02.jpg)'
- en: Figure 5.2 – RAG with Amazon Bedrock
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – 使用亚马逊Bedrock的RAG
- en: 'When documents have been properly indexed, the system can provide contextual
    answers to natural language questions through the following pipeline:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当文档被正确索引后，系统可以通过以下流程通过自然语言问题提供上下文答案：
- en: '**Step 1**: Encode the input question into a dense vector representation (embedding)
    using an embedding model, such as the Amazon Titan Text Embeddings model or Cohere’s
    embedding model, both of which can be accessed via Amazon Bedrock. This captures
    the semantic meaning of the question.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤1**：使用嵌入模型（如亚马逊Titan Text Embeddings模型或Cohere的嵌入模型）将输入问题编码为密集向量表示（嵌入），这些模型都可以通过亚马逊Bedrock访问。这捕捉了问题的语义意义。'
- en: '**Step 2**: Compare the question embedding to indexed document embeddings using
    cosine similarity or other distance metrics. This retrieves the most relevant
    document chunks. Append the top-ranking document chunks to the prompt as contextual
    information. This provides relevant background knowledge for the model.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤2**：使用余弦相似度或其他距离度量比较问题嵌入与索引文档嵌入。这检索到最相关的文档片段。将排名靠前的文档片段附加到提示中作为上下文信息。这为模型提供了相关的背景知识。'
- en: '**Step 3**: Pass the prompt with context to an LLM available on Amazon Bedrock
    such as Anthropic Claude 3, Meta Llama 3, or Amazon Titan Text G1 - Express. This
    leverages the model’s capabilities to generate an answer conditioned on the retrieved
    documentation.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤3**：将带有上下文的提示传递给亚马逊Bedrock上可用的LLM，例如Anthropic Claude 3、Meta Llama 3或亚马逊Titan
    Text G1 - Express。这利用了模型根据检索到的文档生成答案的能力。'
- en: Finally, return the model-generated answer, which should show an understanding
    of the question in relation to the contextual documents.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，返回模型生成的答案，这应该显示出对上下文文档中问题的理解。
- en: The system thus leverages Amazon Bedrock FMs to provide natural language question
    answering grounded in relevant documentation and context. Careful indexing and
    encoding of documents enable seamless integration of retrieval with generative
    models for more informed and accurate answers.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，系统利用亚马逊Bedrock FMs提供基于相关文档和上下文的自然语言问题回答。仔细索引和编码文档使得检索与生成模型的集成无缝，从而提供更明智和准确的答案。
- en: 'Here is an example of RAG implementation with Amazon Bedrock and Amazon OpenSearch
    Serverless as a vector engine: [https://aws.amazon.com/blogs/big-data/build-scalable-and-serverless-rag-workflows-with-a-vector-engine-for-amazon-opensearch-serverless-and-amazon-bedrock-claude-models/](https://aws.amazon.com/blogs/big-data/build-scalable-and-serverless-rag-workflows-with-a-vector-engine-for-amazon-opensearch-serverless-and-amazon-bedrock-claude-models/).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个使用亚马逊Bedrock和亚马逊OpenSearch Serverless作为向量引擎的RAG实现的示例：[https://aws.amazon.com/blogs/big-data/build-scalable-and-serverless-rag-workflows-with-a-vector-engine-for-amazon-opensearch-serverless-and-amazon-bedrock-claude-models/](https://aws.amazon.com/blogs/big-data/build-scalable-and-serverless-rag-workflows-with-a-vector-engine-for-amazon-opensearch-serverless-and-amazon-bedrock-claude-models/).
- en: Now that we have discussed some details around implementing RAG with Amazon
    Bedrock, let us dive deep into tackling use cases using RAG through Knowledge
    Bases on Amazon Bedrock.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了使用亚马逊Bedrock实现RAG的一些细节，让我们深入探讨通过亚马逊Bedrock上的知识库使用RAG解决用例。
- en: Amazon Bedrock Knowledge Bases
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊Bedrock知识库
- en: Amazon Bedrock provides a fully managed RAG experience with Knowledge Bases,
    handling the complexity behind the scenes while giving you control over your data.
    Bedrock’s Knowledge Base capability enables the aggregation of diverse data sources
    into a centralized repository of machine-readable information. Knowledge Bases
    automate the creation of vector embeddings from your data, store them in a managed
    vector index, and handle embedding, querying, source attribution, and short-term
    memory for production RAG.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊Bedrock通过知识库提供完全管理的RAG体验，在幕后处理复杂性，同时让你控制你的数据。Bedrock的知识库功能使能够将多样化的数据源聚合到一个集中的机器可读信息库中。知识库自动从你的数据创建向量嵌入，将它们存储在管理的向量索引中，并处理嵌入、查询、来源归因和生产RAG的短期记忆。
- en: 'The key benefits of Knowledge Bases in Amazon Bedrock include the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊Bedrock中知识库的关键优势包括以下内容：
- en: '**Seamless RAG workflow**: There’s no need to set up and manage the components
    yourself. You can just provide your data and let Amazon Bedrock handle ingestion,
    embedding, storage, and querying.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无缝RAG工作流程**：无需自行设置和管理组件。你只需提供你的数据，让亚马逊Bedrock处理导入、嵌入、存储和查询。'
- en: '**Custom vector embeddings**: Your data is ingested and converted into vector
    representations tailored to your use case with a choice of embedding models.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义向量嵌入**：你的数据被导入并转换为针对你的用例定制的向量表示，可以选择嵌入模型。'
- en: '`RetrieveAndGenerate` API within Amazon Bedrock provides attribution back to
    source documents and manages conversation history for contextual responses.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊Bedrock中的`RetrieveAndGenerate` API提供对源文档的归因，并管理会话历史以进行上下文响应。
- en: '**Flexible integration**: Incorporate RAG into your workflows with API access
    and integration support for other GenAI tools.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活集成**：通过API访问和集成支持其他GenAI工具将RAG集成到你的工作流程中。'
- en: Amazon Bedrock Knowledge Base setup
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊Bedrock知识库设置
- en: 'Objectively speaking, the following steps facilitate Knowledge Base creation
    and integration:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 客观地说，以下步骤有助于知识库的创建和集成：
- en: Identify and prepare data sources for ingestion
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别和准备用于导入的数据源
- en: Upload data to **Amazon Simple Storage Service** (**Amazon S3**) for centralized
    access
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据上传到**亚马逊简单存储服务**（**Amazon S3**）以实现集中访问
- en: Generate embeddings for data via FMs and persist in a vector store
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过FM生成数据嵌入并持久化到向量存储中
- en: Connect applications and agents to query and incorporate Knowledge Base into
    workflows
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将应用程序和代理连接到查询并将知识库集成到工作流程中
- en: 'To create ingestion jobs, follow the next steps:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建数据导入作业，请按照以下步骤操作：
- en: '**Set up your Knowledge Base**: Before you can ingest data, you need to create
    a knowledge base. This involves defining the structure and schema of the knowledge
    base to ensure it can store and manage the data effectively.'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置你的知识库**：在你可以导入数据之前，你需要创建一个知识库。这涉及到定义知识库的结构和模式，以确保它可以有效地存储和管理数据。'
- en: '**Prepare your** **data source**:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**准备你的** **数据源**：'
- en: Ensure your data is stored in Amazon S3\. The data can be in various formats,
    including structured (for example, CSV, JSON) and unstructured (for example, text
    files, PDFs).
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你的数据存储在Amazon S3中。数据可以是各种格式，包括结构化（例如，CSV、JSON）和非结构化（例如，文本文件、PDF）。
- en: Organize your data in a way that makes it easy to manage and retrieve.
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以便于管理和检索的方式组织你的数据。
- en: '**Create an** **ingestion job**:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建一个** **数据导入作业**：'
- en: Navigate to the AWS Bedrock console and go to the **Knowledge** **base** section.
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导航到AWS Bedrock控制台并转到**知识库**部分。
- en: Select the option to create a new ingestion job.
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择创建新的数据导入作业选项。
- en: Provide the necessary details, such as the name of the job, the S3 bucket location,
    and the data format.
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供必要的详细信息，例如作业名称、S3存储桶位置和数据格式。
- en: Configure the job to specify how the data should be processed and ingested into
    the knowledge base.
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置作业以指定数据应该如何处理并导入到知识库中。
- en: '**Configure** **sync settings**:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**配置** **同步设置**：'
- en: Set up the sync settings to ensure the knowledge base is updated with the most
    recent data from your S3 location.
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置同步设置以确保知识库使用最新的数据从你的S3位置更新。
- en: You can configure the sync to run at regular intervals (for example, daily or
    weekly) or trigger it manually as needed.
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以配置同步以定期运行（例如，每天或每周）或根据需要手动触发。
- en: Ensure that the sync settings are optimized to handle large volumes of data
    efficiently.
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保同步设置已优化以高效处理大量数据。
- en: '**Run the** **ingestion job**:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**运行** **数据导入作业**：'
- en: Once the job is configured, you can start the ingestion process.
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦配置了作业，您就可以开始导入过程。
- en: Monitor the job’s progress through the AWS Bedrock console. You can view logs
    and status updates to ensure the job is running smoothly.
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 AWS Bedrock 控制台监控作业的进度。您可以查看日志和状态更新，以确保作业运行顺利。
- en: Now that we have a basic understanding of the ingestion process, let us walk
    through these details thoroughly.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对导入过程有了基本的了解，让我们彻底地了解这些细节。
- en: 'In order to initiate this pipeline within the AWS console, one can navigate
    to the **Orchestration** section within the Amazon Bedrock page, as shown in *Figure
    5**.3*:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 AWS 控制台中启动此管道，用户可以导航到 Amazon Bedrock 页面中的**编排**部分，如图 *5.3* 所示：
- en: '![Figure 5.3 – Knowledge base](img/B22045_05_03.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 知识库](img/B22045_05_03.jpg)'
- en: Figure 5.3 – Knowledge base
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 知识库
- en: 'Now, let’s look at these steps in greater depth:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更深入地看看这些步骤：
- en: Click on **Knowledge base** and enter the details pertaining to the knowledge
    base you intend to create. You can provide a custom knowledge base name, description,
    and the respective **Identity and Access Management** (**IAM**) permissions for
    creating either a new service role or leveraging an existing service role for
    the knowledge base. You can also provide tags to this resource for easy searching
    and filtering of your resource or tracking AWS costs associated with the service
    in the knowledge base details section.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**知识库**并输入您打算创建的知识库的相关详细信息。您可以提供自定义的知识库名称、描述以及创建新的服务角色或利用现有服务角色为知识库设置相应的**身份和访问管理**（**IAM**）权限。您还可以为此资源提供标签，以便于搜索和筛选您的资源，或在知识库详细信息部分跟踪与知识库相关的
    AWS 成本。
- en: 'In the next step, you will set up the data source by specifying the S3 location
    where the data to be indexed resides. You can specify a particular data source
    name (or leverage the default pre-filled name) and provide the S3 URI (Uniform
    Resource Identifier) of the bucket containing the source data, as depicted in
    *Figure 5**.4*:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一步中，您将通过指定数据要索引的 S3 位置来设置数据源。您可以指定特定的数据源名称（或利用默认的预填充名称）并提供包含源数据的存储桶的 S3 URI（统一资源标识符），如图
    *5.4* 所示：
- en: '![Figure 5.4 – Knowledge base: Set up data source](img/B22045_05_04.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 知识库：设置数据源](img/B22045_05_04.jpg)'
- en: 'Figure 5.4 – Knowledge base: Set up data source'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 知识库：设置数据源
- en: 'You can also provide a customer-managed **Key Management Service** (**KMS**)
    key you used for encrypting your S3 data, in order to allow the Bedrock service
    to decrypt it when ingesting the given data into the vector database. Under **Advanced
    settings** (as shown in *Figure 5**.5*), users have the option to choose the default
    KMS key or customize encryption settings by choosing a different key of their
    choice by entering the **Amazon Resource Name** (**ARN**) or searching for their
    stored customized key (or creating a new AWS KMS key on the fly). Providing a
    customer-managed KMS key for encrypting S3 data sources ingested by Amazon Bedrock
    is desired for enhanced data security, compliance, and control. It allows data
    sovereignty, key rotation/revocation, **separation of duties** (**SoD**), auditing/logging
    capabilities, and integration with existing key management infrastructure. By
    managing your own encryption keys, you gain greater control over data protection,
    meeting regulatory requirements and aligning with organizational security policies
    for sensitive or regulated data:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以提供您用于加密 S3 数据的客户管理的**密钥管理服务**（**KMS**）密钥，以便 Bedrock 服务在将给定数据导入向量数据库时解密它。在**高级设置**（如图
    *5.5* 所示），用户可以选择默认的 KMS 密钥或通过输入**亚马逊资源名称**（**ARN**）或搜索他们存储的定制密钥（或即时创建新的 AWS KMS
    密钥）来自定义加密设置。为通过 Amazon Bedrock 导入的 S3 数据源提供客户管理的 KMS 密钥是为了增强数据安全、合规性和控制。它允许数据主权、密钥轮换/吊销、**职责分离**（**SoD**）、审计/日志功能，并与现有的密钥管理基础设施集成。通过管理自己的加密密钥，您可以获得对数据保护更大的控制权，满足监管要求并与组织的安全策略保持一致，以处理敏感或受监管的数据：
- en: '![Figure 5.5 – Knowledge base: Advanced settings](img/B22045_05_05.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – 知识库：高级设置](img/B22045_05_05.jpg)'
- en: 'Figure 5.5 – Knowledge base: Advanced settings'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 知识库：高级设置
- en: 'Under **Chunking strategy** (as shown in *Figure 5**.6*), users have the option
    to select how to break down text in the source location into smaller segments
    before creating the embedding. By default, the knowledge base will automatically
    split your data into tiny chunks each containing, at most, 300 tokens. If a document
    or, in other words, source data contains fewer than 300 tokens, it is not split
    any further in that case:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **分块策略**（如图 *图 5.6* 所示）下，用户可以选择在创建嵌入之前如何将源位置中的文本分解成更小的段。默认情况下，知识库将自动将您的数据分成包含最多
    300 个标记的小块。如果一个文档或换句话说，源数据包含少于 300 个标记，则在该情况下不会进一步分割：
- en: '![Figure 5.6 – Knowledge base: Chunking strategy](img/B22045_05_06.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – 知识库：分块策略](img/B22045_05_06.jpg)'
- en: 'Figure 5.6 – Knowledge base: Chunking strategy'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – 知识库：分块策略
- en: Alternatively, you have the option to customize the chunk size using **Fixed
    size chunking** or simply opt for **No chunking** in case you have already preprocessed
    your source documents into separate files of smaller chunks and don’t intend to
    chunk your documents any further using Bedrock.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以选择使用 **固定大小分块** 来自定义块大小，或者如果您已经将源文档预处理为更小的块文件，并且不打算使用 Bedrock 进一步分块文档，则简单地选择
    **不进行分块**。
- en: 'In the next stage, users will select an embedding model to convert their selected
    data into an embedding. Currently, there are four embeddings models that are supported,
    as shown in *Figure 5**.7*. Further, under **Vector database**, users have the
    option to go with the recommended route – that is, select the quick create option,
    which will create an Amazon OpenSearch Serverless vector store in the background
    automatically in the respective account of their choice:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个阶段，用户将选择一个嵌入模型将所选数据转换为嵌入。目前，有四种嵌入模型受到支持，如图 *图 5.7* 所示。此外，在 **向量数据库** 下，用户可以选择推荐路线——即选择快速创建选项，这将自动在所选账户的背景中创建一个
    Amazon OpenSearch Serverless 向量存储：
- en: Note
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Vector embeddings are numeric representations of text data that encode semantic
    or contextual meaning. In NLP pipelines, text documents are passed through an
    embedding model to convert the chunks, including discrete tokens such as words
    into dense vectors in a continuous vector space. Good vector representations allow
    **machine learning** (**ML**) models to understand similarities, analogies, and
    other patterns between words and concepts. In other words, if the vector representations
    (embeddings) are trained well on a large dataset, they will capture meaningful
    relationships in the data. This allows ML models that use those embeddings to
    recognize things such as the following:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 向量嵌入是文本数据的数值表示，它编码语义或上下文意义。在 NLP 管道中，文本文档通过嵌入模型转换，将包括单词等离散标记的块转换为连续向量空间中的密集向量。良好的向量表示允许
    **机器学习**（**ML**）模型理解单词和概念之间的相似性、类比以及其他模式。换句话说，如果向量表示（嵌入）在大数据集上训练良好，它们将捕获数据中的有意义关系。这使得使用这些嵌入的
    ML 模型能够识别如下事物：
- en: '- Which words are similar in meaning (for example, *king* and *queen*)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '- 哪些单词在意义上相似（例如，*国王* 和 *王后*）'
- en: '- Which concepts follow an analogical pattern (for example, *man* is to *king*
    as *woman* is to *queen*)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '- 哪些概念遵循类比模式（例如，*人* 对 *国王* 就像 *女人* 对 *王后*）'
- en: '- Other patterns in how the concepts are represented in the embedding space'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在嵌入空间中如何表示概念的其他模式'
- en: The well-trained embeddings essentially provide the ML models with a numeric
    map of the relationships and patterns inherent in the data. This makes it easier
    for the models to then learn and make inferences about those patterns during training
    on downstream tasks.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 训练良好的嵌入实际上为 ML 模型提供了数据中固有的关系和模式的数值映射。这使得模型在下游任务训练期间更容易学习和推断这些模式。
- en: Hence, simply put, good embeddings help ML models understand similarities and
    relationships between words and concepts rather than just treating them as isolated
    data points.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，简单来说，好的嵌入有助于 ML 模型理解单词和概念之间的相似性和关系，而不仅仅是将它们视为孤立的数据点。
- en: '![Figure 5.7 – Knowledge base: Configure vector store](img/B22045_05_07.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7 – 知识库：配置向量存储](img/B22045_05_07.jpg)'
- en: 'Figure 5.7 – Knowledge base: Configure vector store'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – 知识库：配置向量存储
- en: 'Alternatively, you have the option to choose your own vector store (as shown
    in *Figure 5**.8*). At the time of writing this book, you have the option to select
    **Vector engine for Amazon OpenSearch Serverless**, **Amazon Aurora**, **MongoDB
    Atlas**, **Pinecone**, or **Redis Enterprise Cloud**. Once selected, you can provide
    the field mapping to proceed with the knowledge base creation final setup. Depending
    on the use case, developers or teams may opt for one vector database over another.
    You can read more about the role of vector datastores in GenAI applications at
    [https://aws.amazon.com/blogs/database/the-role-of-vector-datastores-in-generative-ai-applications/](https://aws.amazon.com/blogs/database/the-role-of-vector-datastores-in-generative-ai-applications/):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以选择自己的向量存储（如图 *图 5.8* 所示）。在撰写本书时，你可以选择 **Amazon OpenSearch Serverless 向量引擎**、**Amazon
    Aurora**、**MongoDB Atlas**、**Pinecone** 或 **Redis Enterprise Cloud**。一旦选择，你可以提供字段映射以继续进行知识库创建的最终设置。根据用例，开发人员或团队可能会选择一个向量数据库而不是另一个。你可以在
    [https://aws.amazon.com/blogs/database/the-role-of-vector-datastores-in-generative-ai-applications/](https://aws.amazon.com/blogs/database/the-role-of-vector-datastores-in-generative-ai-applications/)
    上了解更多关于向量数据存储在 GenAI 应用中的作用：
- en: '![Figure 5.8 – Knowledge base: Vector database](img/B22045_05_08.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8 – 知识库：向量数据库](img/B22045_05_08.jpg)'
- en: 'Figure 5.8 – Knowledge base: Vector database'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – 知识库：向量数据库
- en: You can check out [https://aws.amazon.com/blogs/aws/preview-connect-foundation-models-to-your-company-data-sources-with-agents-for-amazon-bedrock/](https://aws.amazon.com/blogs/aws/preview-connect-foundation-models-to-your-company-data-sources-with-agents-for-amazon-bedrock/)
    to learn more about how you can set up your own vector store with Pinecone, OpenSearch
    Serverless, or Redis.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看 [https://aws.amazon.com/blogs/aws/preview-connect-foundation-models-to-your-company-data-sources-with-agents-for-amazon-bedrock/](https://aws.amazon.com/blogs/aws/preview-connect-foundation-models-to-your-company-data-sources-with-agents-for-amazon-bedrock/)
    了解如何使用 Pinecone、OpenSearch Serverless 或 Redis 设置自己的向量存储。
- en: 'Assuming that you opt for the default route, involving the creation of a new
    Amazon OpenSearch Serverless vector store, you can proceed and click on **Create
    knowledge base** post reviewing all the provided details as depicted in *Figure
    5**.9*:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你选择了默认路由，涉及创建新的 Amazon OpenSearch Serverless 向量存储，你可以在审查所有提供的信息后继续操作，并点击如图
    *图 5.9* 所示的 **创建知识库**：
- en: '![Figure 5.9 – Knowledge base: Review and create](img/B22045_05_09.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – 知识库：审查和创建](img/B22045_05_09.jpg)'
- en: 'Figure 5.9 – Knowledge base: Review and create'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 知识库：审查和创建
- en: 'Once created, you can sync the information to ensure the knowledge base is
    ingesting and operating on the most recent data stored in your Amazon S3 location.
    After syncing is completed for the knowledge base, users can test said knowledge
    base by selecting the appropriate model suitable for their use case by clicking
    on **Select Model**, as shown in *Figure 5**.10*:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 创建完成后，你可以同步信息以确保知识库正在摄取和操作存储在您的 Amazon S3 位置的最新数据。知识库同步完成后，用户可以通过点击 **选择模型**
    并选择适合其用例的适当模型来测试该知识库，如图 *图 5.10* 所示：
- en: '![Figure 5.10 – Test knowledge base: Select Model](img/B22045_05_10.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10 – 测试知识库：选择模型](img/B22045_05_10.jpg)'
- en: 'Figure 5.10 – Test knowledge base: Select Model'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – 测试知识库：选择模型
- en: 'Once the appropriate model has been selected, you can test it by entering a
    particular query in the textbox and receiving a particular response generated
    by the model, depicted in *Figure 5**.11*:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适当的模型后，你可以在文本框中输入特定查询，并接收模型生成的特定响应，如图 *图 5.11* 所示：
- en: '![Figure 5.11 – Test Knowledge base](img/B22045_05_11.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.11 – 测试知识库](img/B22045_05_11.jpg)'
- en: Figure 5.11 – Test Knowledge base
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – 测试知识库
- en: At its core, Amazon Bedrock transforms the user’s query into vector representations
    of meaning; that is, embeddings. It then searches the knowledge base for relevant
    information using these embeddings as the search criteria. Any knowledge retrieved
    is combined with the prompt engineered for the FM, providing essential context.
    The FM integrates this contextual knowledge into its response generation to answer
    the user’s question. For conversations spanning multiple turns, Amazon Bedrock
    leverages its knowledge base to maintain conversation context and history, delivering
    increasingly relevant results.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，Amazon Bedrock 将用户的查询转换为意义的向量表示；即嵌入。然后，它使用这些嵌入作为搜索标准在知识库中搜索相关信息。检索到的任何知识都将与为
    FM 工程的提示结合，提供必要上下文。FM 将此上下文知识集成到其响应生成中，以回答用户的问题。对于跨越多个回合的对话，Amazon Bedrock 利用其知识库来维护对话上下文和历史，提供越来越相关的结果。
- en: Additional information for testing the knowledge base and inspecting source
    chunks can be found at [https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-test.html](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-test.html).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 关于测试知识库和检查源块的相关信息，请参阅 [https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-test.html](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-test.html)。
- en: To ensure your knowledge base is always up to date, it is essential to automate
    the syncing process. This can be achieved by using AWS Lambda functions or AWS
    Step Functions to trigger ingestion jobs based on specific events or schedules.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保您的知识库始终保持最新，自动化同步过程至关重要。这可以通过使用 AWS Lambda 函数或 AWS Step Functions 根据特定事件或计划触发摄取作业来实现。
- en: '**AWS Lambda** is a serverless compute service that allows you to run code
    without provisioning or managing servers. You can create Lambda functions to automate
    tasks such as triggering data ingestion jobs, processing data, or sending notifications.
    Lambda functions can be triggered by various events, including file uploads to
    Amazon S3, changes to DynamoDB tables, or scheduled events using Amazon CloudWatch
    Events.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**AWS Lambda** 是一种无服务器计算服务，允许您在不配置或管理服务器的情况下运行代码。您可以通过创建 Lambda 函数来自动化诸如触发数据摄取作业、处理数据或发送通知等任务。Lambda
    函数可以由各种事件触发，包括上传到 Amazon S3 的文件、DynamoDB 表的更改，或使用 Amazon CloudWatch Events 定时的事件。'
- en: '**AWS Step Functions** is a serverless function orchestrator that allows you
    to coordinate multiple AWS services into business workflows. You can create state
    machines that define a series of steps, including Lambda functions, data processing
    tasks, and error-handling logic. Step Functions can be particularly useful for
    orchestrating complex data ingestion pipelines or ML workflows.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**AWS Step Functions** 是一种无服务器函数编排器，允许您将多个 AWS 服务协调到业务工作流程中。您可以创建状态机，定义一系列步骤，包括
    Lambda 函数、数据处理任务和错误处理逻辑。Step Functions 特别适用于编排复杂的数据摄取管道或机器学习工作流程。'
- en: Regularly monitoring and managing data sources is crucial to maintain their
    relevance and accuracy. **Amazon CloudWatch** is a monitoring and observability
    service that provides data and actionable insights across your AWS resources.
    You can utilize CloudWatch to set up alarms and notifications for any issues or
    anomalies in the data syncing process. CloudWatch can monitor metrics such as
    Lambda function invocations, Step Functions executions, and Amazon S3 bucket activity,
    allowing you to proactively identify and address potential issues.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 定期监控和管理数据源对于保持其相关性和准确性至关重要。**Amazon CloudWatch** 是一种监控和可观察性服务，它为您的 AWS 资源提供数据和可操作见解。您可以使用
    CloudWatch 设置警报和通知，以解决数据同步过程中的任何问题或异常。CloudWatch 可以监控指标，如 Lambda 函数调用、Step Functions
    执行和 Amazon S3 存储桶活动，使您能够主动识别和解决潜在问题。
- en: 'Adhering to best practices for data management, such as organizing data logically,
    maintaining data quality, and ensuring data security, is vital. AWS provides various
    services and tools to support data management best practices:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循数据管理最佳实践，例如逻辑组织数据、维护数据质量和确保数据安全，至关重要。AWS 提供各种服务和工具来支持数据管理最佳实践：
- en: You can organize your data in Amazon S3 buckets and leverage features such as
    versioning, lifecycle policies, and access controls to maintain data quality and
    security.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在 Amazon S3 存储桶中组织您的数据，并利用版本控制、生命周期策略和访问控制等功能来维护数据质量和安全性。
- en: '**AWS Glue** is a fully managed **extract, transform, and load** (**ETL**)
    service that can help you prepare and move data reliably between different data
    stores. Glue can be used to clean, transform, and enrich your data before ingesting
    it into your knowledge base.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Glue** 是一个全托管的 **提取、转换和加载** (**ETL**) 服务，可以帮助您在不同数据存储之间可靠地准备和移动数据。Glue
    可以用于在将数据摄入到您的知识库之前清理、转换和丰富您的数据。'
- en: '**AWS Lake Formation** is a service that helps you build, secure, and manage
    data lakes on Amazon S3\. It provides features such as data cataloging, access
    control, and auditing, which can help ensure data security and governance.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Lake Formation** 是一项帮助您在 Amazon S3 上构建、保护和管理工作数据湖的服务。它提供数据编目、访问控制和审计等功能，可以帮助确保数据安全和治理。'
- en: Regular reviews and updates of the knowledge base should be conducted to remove
    outdated information and incorporate new, relevant data. AWS provides services
    such as **Amazon Kendra** and **Amazon Comprehend** that can help you analyze
    and understand your knowledge base content, identify outdated or irrelevant information,
    and suggest updates or improvements.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 应定期审查和更新知识库，以删除过时信息并纳入新的、相关数据。AWS 提供了如 **Amazon Kendra** 和 **Amazon Comprehend**
    等服务，可以帮助您分析和理解您的知识库内容，识别过时或不相关信息，并提出更新或改进建议。
- en: Tracking actionable metrics, such as search success rate, user engagement, and
    data freshness, is also important. These metrics can help continuously improve
    the knowledge base, ensuring it meets the needs of its users effectively.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪可操作的指标，例如搜索成功率、用户参与度和数据新鲜度，也很重要。这些指标可以帮助持续改进知识库，确保其有效地满足用户的需求。
- en: Amazon CloudWatch can be used to collect and analyze metrics from various AWS
    services, including your knowledge base application. You can create custom metrics,
    dashboards, and alarms to monitor the performance and usage of your knowledge
    base.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon CloudWatch 可以用于收集和分析来自各种 AWS 服务（包括您的知识库应用程序）的指标。您可以创建自定义指标、仪表板和警报来监控您知识库的性能和用法。
- en: By leveraging AWS services such as Lambda, Step Functions, CloudWatch, S3, Glue,
    Lake Formation, Kendra, and Comprehend, you can automate the syncing process,
    monitor and manage data sources, adhere to data management best practices, and
    track actionable metrics to ensure your knowledge base remains up to date, relevant,
    and effective in meeting the needs of your users.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用 AWS 服务，如 Lambda、Step Functions、CloudWatch、S3、Glue、Lake Formation、Kendra
    和 Comprehend，您可以自动化同步过程，监控和管理数据源，遵守数据管理最佳实践，并跟踪可操作的指标，以确保您的知识库保持最新、相关且有效地满足用户的需求。
- en: Readers are encouraged to visit the Amazon Bedrock RAG GitHub repository ([https://github.com/aws-samples/amazon-bedrock-rag](https://github.com/aws-samples/amazon-bedrock-rag))
    to explore and implement a fully managed RAG solution using Knowledge Bases for
    Amazon Bedrock.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励读者访问 Amazon Bedrock RAG GitHub 仓库 ([https://github.com/aws-samples/amazon-bedrock-rag](https://github.com/aws-samples/amazon-bedrock-rag))，以探索和实现一个使用
    Amazon Bedrock 知识库的完全托管 RAG 解决方案。
- en: API calls
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: API 调用
- en: 'For users who wish to invoke Bedrock outside of the console, the `RetrieveAndGenerate`
    API provides programmatic access to execute this same workflow. This allows Bedrock’s
    capabilities to be tightly integrated into custom applications via API calls rather
    than console interaction. The `RetrieveAndGenerate` API gives developers the flexibility
    to build Amazon Bedrock-powered solutions tailored to their specific needs. *Figure
    5**.12* illustrates the RAG workflow using Amazon Bedrock’s `RetrieveAndGenerate`
    API:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于希望在控制台之外调用 Bedrock 的用户，`RetrieveAndGenerate` API 提供了程序化访问以执行此相同的工作流程。这允许通过
    API 调用而不是控制台交互将 Bedrock 的功能紧密集成到自定义应用程序中。`RetrieveAndGenerate` API 为开发者提供了构建针对其特定需求定制的
    Amazon Bedrock 解决方案的灵活性。*图 5**.12* 展示了使用 Amazon Bedrock 的 `RetrieveAndGenerate`
    API 的 RAG 工作流程：
- en: '![Figure 5.12 – RetrieveAndGenerate API](img/B22045_05_12.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.12 – RetrieveAndGenerate API](img/B22045_05_12.jpg)'
- en: Figure 5.12 – RetrieveAndGenerate API
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 – RetrieveAndGenerate API
- en: 'In the `RetrieveAndGenerate` API, the generated response output contains three
    components: the text of the model-generated response itself, source attribution
    indicating where the FM retrieved information from, and the specific text excerpts
    that were retrieved from those sources as part of generating the response. The
    API provides full transparency by returning not just the final output text but
    also the underlying source materials and attributions that informed the FM’s response
    generation process. This allows users to inspect both the final output as well
    as the intermediate retrieved texts that were used by the system during response
    generation.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在`RetrieveAndGenerate` API中，生成的响应输出包含三个组件：模型生成的响应本身的文本、来源归属表明FM从哪里检索信息，以及作为生成响应一部分从那些来源检索的具体文本摘录。API通过返回最终输出文本以及支持FM响应生成过程的底层源材料和归属，提供了完全的透明度。这使用户能够检查最终输出以及系统在响应生成过程中使用的中间检索文本。
- en: The following is a code sample for running the same operation as showcased in
    the console using the API.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码示例用于使用API运行与控制台展示相同的操作。
- en: Note
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Ensure you have the latest version of the `boto3` and `botocore` packages prior
    to running the code shown next. In case the packages are not installed, run the
    following command in your Jupyter notebook. Note that `!` will not be needed if
    you’re running Python code from a Python terminal:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行下面的代码之前，请确保您拥有`boto3`和`botocore`包的最新版本。如果这些包尚未安装，请在您的Jupyter笔记本中运行以下命令。请注意，如果您从Python终端运行Python代码，则不需要`!`：
- en: '`!pip install` `boto3 botocore`'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`!pip install boto3 botocore`'
- en: '[PRE0]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This script assumes that readers have already created a knowledge base and ingested
    the relevant documents, following the procedures outlined in the preceding section.
    With this prerequisite fulfilled, invoking the `RetrieveAndGenerate` API will
    enable the system to fetch the associated documents using the provided code sample.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本假定读者已经根据前述部分中概述的程序创建了知识库并摄取了相关文档。在满足此先决条件后，调用`RetrieveAndGenerate` API将使系统能够使用提供的代码示例检索相关文档。
- en: The code provided will print the extracted text output to display the relevant
    information from the data source in context to the input query, formatted as desired.
    The response is generated by contextualizing pertinent details from the data source
    with respect to the specifics of the input query. The output is then formatted
    and presented in the requested structure. This allows customized extraction and
    formatting of relevant data from the source to provide responses tailored to the
    input query in a suitable structure.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的代码将打印提取的文本输出，以显示从数据源中根据输入查询提取的相关信息，并按所需格式化。响应是通过将数据源中的相关细节与输入查询的具体内容进行上下文关联而生成的。然后，输出将按请求的结构进行格式化和展示。这允许从源中定制提取和格式化相关数据，以提供针对输入查询的、适合的结构化响应。
- en: Note
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Please ensure you have the right permissions to invoke Amazon Bedrock APIs by
    navigating to IAM roles and permissions, searching for the respective role (if
    you are running the notebook in Amazon SageMaker, search for the execution role
    that was assigned when you created the Amazon SageMaker domain), and attaching
    Amazon Bedrock policies for invoking Bedrock models and Bedrock agent runtime
    APIs.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保您有正确的权限通过导航到IAM角色和权限，搜索相应的角色（如果您在Amazon SageMaker中运行笔记本，请搜索在创建Amazon SageMaker域时分配的执行角色），并附加Amazon
    Bedrock策略以调用Bedrock模型和Bedrock代理运行时API。
- en: 'Yet another resourceful Amazon Bedrock API, the `Retrieve` API, enables more
    advanced processing and utilization of the retrieved text segments. This API transforms
    user queries into vector representations, performs similarity searches against
    the knowledge base, and returns the most relevant results along with relevance
    scores. The `Retrieve` API provides users with more fine-grained control to build
    custom pipelines leveraging semantic search capabilities. Through the `Retrieve`
    API, developers can orchestrate subsequent stages of text generation based on
    the search results, implement additional relevance filtering, or derive other
    workflow optimizations. *Figure 5**.13* exemplifies the usage of the `Retrieve`
    API in Amazon Bedrock in a RAG pipeline:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的Amazon Bedrock API，即`Retrieve` API，允许更高级的处理和利用检索到的文本片段。此API将用户查询转换为向量表示，对知识库执行相似度搜索，并返回最相关的结果及其相关性分数。`Retrieve`
    API为用户提供更细粒度的控制，以利用语义搜索能力构建自定义管道。通过`Retrieve` API，开发者可以根据搜索结果编排文本生成的后续阶段，实现额外的相关性过滤，或推导其他工作流程优化。*图5.13*展示了在Amazon
    Bedrock RAG管道中使用`Retrieve` API的示例：
- en: '![Figure 5.13 – Retrieve API](img/B22045_05_13.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图5.13 – Retrieve API](img/B22045_05_13.jpg)'
- en: Figure 5.13 – Retrieve API
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 – Retrieve API
- en: 'Within the Amazon Bedrock console, you can toggle the switch to disable the
    `What is Quantum Computing?` again. *Figure 5**.14* showcases the generated responses
    retrieved from the knowledge base pertaining to the question on quantum computing.
    Note that Amazon Bedrock cites the references along with the generated responses:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在Amazon Bedrock控制台中，您可以切换开关再次禁用`什么是量子计算？`。*图5.14*展示了从知识库检索到的有关量子计算问题的生成响应。请注意，Amazon
    Bedrock在生成响应的同时引用了参考文献：
- en: '![Figure 5.14 – Test knowledge base](img/B22045_05_14.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图5.14 – 测试知识库](img/B22045_05_14.jpg)'
- en: Figure 5.14 – Test knowledge base
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 – 测试知识库
- en: This time, instead of a fluid natural language response, notice that the output
    displays the retrieved text chunks alongside links to the original source documents
    from which they were extracted. This approach provides transparency by explicitly
    showing the relevant information retrieved from the knowledge base and its provenance.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，与流畅的自然语言响应不同，请注意输出显示了检索到的文本片段以及链接到原始源文档的链接。这种方法通过明确显示从知识库检索到的相关信息及其来源，提供了透明度。
- en: Note
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Ensure you have the latest version of the `boto3` and `botocore` packages prior
    to running the code shown next. In case the packages are not installed, run the
    following command in your Jupyter notebook. Note that `!` will not be needed if
    you’re running Python code from a Python terminal:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行下面展示的代码之前，请确保您拥有`boto3`和`botocore`包的最新版本。如果这些包尚未安装，请在您的Jupyter笔记本中运行以下命令。请注意，如果您从Python终端运行Python代码，则不需要使用`!`：
- en: '`!pip install` `boto3 botocore`'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`!pip install boto3 botocore`'
- en: 'Leveraging the `Retrieve` API using `boto3` looks like this:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 利用`boto3`的`Retrieve` API看起来是这样的：
- en: '[PRE1]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `Retrieve` API returns a response containing the retrieved text excerpts,
    as well as metadata about the source of each excerpt. Specifically, the response
    includes the location type and URI of the source data from which each text chunk
    was retrieved. Additionally, each retrieved text chunk is accompanied by a relevancy
    score. This score provides an indication of how closely the semantic content of
    the retrieved chunk matches the user’s input query. Text chunks with higher scores
    are more relevant matches to the query compared to chunks with lower scores. By
    examining the scores of the retrieved chunks, the user can focus on the most relevant
    excerpts returned by the `Retrieve` API. Therefore, the `Retrieve` API provides
    not only the retrieved text but also insightful metadata to enable productive
    utilization of the API response.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`Retrieve` API返回一个包含检索到的文本摘录以及每个摘录来源的元数据的响应。具体来说，响应包括从每个文本片段检索到的源数据的位置类型和URI。此外，每个检索到的文本片段都伴随着一个相关性分数。这个分数提供了检索到的片段的语义内容与用户输入查询的匹配程度的指示。与低分数片段相比，得分较高的文本片段与查询的相关性更高。通过检查检索到的片段的分数，用户可以专注于`Retrieve`
    API返回的最相关摘录。因此，`Retrieve` API不仅提供了检索到的文本，还提供了有洞察力的元数据，以使API响应能够得到有效利用。'
- en: 'By tapping into the custom chunking and vector store capabilities within the
    RAG framework, you gain more fine-grained control over how your NLP workflows
    operate under the hood. Expertly applying these customizations helps ensure RAG
    is tailored to your specific needs and use cases. Note that at the time of writing
    this book, when creating a data source for your knowledge base, you can specify
    the chunking strategy in the `ChunkingConfiguration` object:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用 RAG 框架中的自定义分块和向量存储功能，您可以获得更多细粒度的控制，以了解您的 NLP 工作流程在底层如何运行。熟练地应用这些自定义可以帮助确保
    RAG 符合您的特定需求和用例。请注意，在撰写本书时，在创建知识库的数据源时，您可以在 `ChunkingConfiguration` 对象中指定分块策略：
- en: '[PRE2]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s look at this in a bit more detail:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看：
- en: '`FIXED_SIZE` allows you to set a fixed chunk size in tokens for splitting your
    data sources'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FIXED_SIZE` 允许您为分割数据源设置固定大小的令牌块'
- en: '`NONE` treats each file as a single chunk, giving you full control over pre-chunking
    your data'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NONE` 将每个文件视为单个块，让您完全控制预分块数据'
- en: Further information on using the API with the AWS Python SDK can be found at
    [https://aws.amazon.com/blogs/aws/knowledge-bases-now-delivers-fully-managed-rag-experience-in-amazon-bedrock/](https://aws.amazon.com/blogs/aws/knowledge-bases-now-delivers-fully-managed-rag-experience-in-amazon-bedrock/).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [https://aws.amazon.com/blogs/aws/knowledge-bases-now-delivers-fully-managed-rag-experience-in-amazon-bedrock/](https://aws.amazon.com/blogs/aws/knowledge-bases-now-delivers-fully-managed-rag-experience-in-amazon-bedrock/)
    可以找到有关使用 AWS Python SDK 与 API 的高级信息。
- en: Knowledge Bases for Amazon Bedrock reduces the complexity of RAG, allowing you
    to enhance language generation with your own grounded knowledge. The capabilities
    open new possibilities for building contextual chatbots, question answering applications,
    and other AI systems that need to generate informed specific responses.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Bedrock 的知识库简化了 RAG 的复杂性，允许您通过自己的有根知识来增强语言生成。这些能力为构建上下文聊天机器人、问答应用和其他需要生成具体信息的
    AI 系统开辟了新的可能性。
- en: Let us further explore how the RAG approach can be implemented using the LangChain
    orchestrator and other GenAI systems.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步探讨如何使用 LangChain 协调器和其他 GenAI 系统实现 RAG 方法。
- en: Implementing RAG with other methods
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用其他方法实现 RAG
- en: Amazon Bedrock is not the only way to implement RAG, and in this section, we
    will learn about the other ways. Starting with LangChain, we will also look at
    some other GenAI systems.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Bedrock 不是实现 RAG 的唯一方式，在本节中，我们将了解其他方法。从 LangChain 开始，我们还将探讨一些其他 GenAI
    系统。
- en: Using LangChain
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 LangChain
- en: 'LangChain provides an excellent framework for building RAG models by integrating
    retrieval tools and LLMs. In this section, we will look at how to implement RAG
    with LangChain using the following components:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 通过集成检索工具和 LLMs 提供了一个构建 RAG 模型的优秀框架。在本节中，我们将探讨如何使用以下组件使用 LangChain
    实现 RAG：
- en: '**LLMs**: LangChain integrates with Amazon Bedrock’s powerful LLMs using Bedrock’s
    available FM invocation APIs. Amazon Bedrock can be used to generate fluent NL
    responses after reviewing the retrieved documents.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLMs**：LangChain 通过 Bedrock 的可用 FM 调用 API 与 Amazon Bedrock 的强大 LLM 集成。在审查检索到的文档后，可以使用
    Amazon Bedrock 生成流畅的 NL 响应。'
- en: '**Embedding model**: Text embedding models available via Amazon Bedrock, such
    as Amazon Titan Text Embeddings, generate vector representations of text passages.
    This allows comparing textual similarity in order to retrieve relevant contextual
    information to augment the input prompt for composing a final response.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入模型**：通过 Amazon Bedrock 可用的文本嵌入模型，例如 Amazon Titan Text Embeddings，生成文本段落的向量表示。这允许比较文本相似性，以便检索相关的上下文信息，以增强输入提示，用于组成最终响应。'
- en: '**Document loader**: LangChain provides a PDF loader to ingest documents from
    local storage. This can be replaced by a loader to retrieve enterprise documents.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档加载器**：LangChain 提供了一个 PDF 加载器，用于从本地存储中获取文档。这可以被替换为用于检索企业文档的加载器。'
- en: '`pgvector`, can be leveraged based on the use case.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pgvector` 可以根据用例进行利用。'
- en: '**Index**: The vector index matches input embeddings with stored document embeddings
    to find the most relevant contexts.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**索引**：向量索引将输入嵌入与存储的文档嵌入匹配，以找到最相关的上下文。'
- en: '**Wrapper**: LangChain provides a wrapper class that abstracts away the underlying
    logic, handling retrieval, embeddings, indexing, and generation.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**包装器**：LangChain 提供了一个包装器类，它抽象了底层逻辑，处理检索、嵌入、索引和生成。'
- en: 'The RAG workflow via LangChain orchestration is as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 LangChain 协调器的 RAG 工作流程如下：
- en: Ingest a collection of documents into the document loader
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文档集合导入文档加载器
- en: Generate embeddings for all documents using the embedding model
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用嵌入模型为所有文档生成嵌入
- en: Index all document embeddings in the vector store
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在向量存储中索引所有文档嵌入
- en: For an input question, generate its embedding using the embedding model
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于输入问题，使用嵌入模型生成其嵌入
- en: Use the index to retrieve the most similar document embeddings
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用索引检索最相似的文档嵌入
- en: Pass the relevant documents to the LLM to generate a natural language answer
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将相关文档传递给 LLM 以生成自然语言答案
- en: By orchestrating retrieval and generation in this way, LangChain provides an
    easy yet powerful framework for developing RAG models. The modular architecture
    allows flexibility, extensibility, and scalability. For more details on RAG implementation
    with LangChain, follow the steps in the Amazon Bedrock workshop at [https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/01_Langchain_KnowledgeBases_and_RAG_examples/01_qa_w_rag_claude.ipynb](https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/01_Langchain_KnowledgeBases_and_RAG_examples/01_qa_w_rag_claude.ipynb).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式编排检索和生成，LangChain 为开发 RAG 模型提供了一个简单而强大的框架。模块化架构提供了灵活性、可扩展性和可伸缩性。有关使用 LangChain
    实现 RAG 的更多详细信息，请遵循 Amazon Bedrock 工作坊中的步骤，见 [https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/01_Langchain_KnowledgeBases_and_RAG_examples/01_qa_w_rag_claude.ipynb](https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/01_Langchain_KnowledgeBases_and_RAG_examples/01_qa_w_rag_claude.ipynb)。
- en: Other GenAI systems
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他 GenAI 系统
- en: RAG models can be integrated with other GenAI tools and applications to create
    more powerful and versatile AI systems. For instance, RAG’s knowledge retrieval
    capabilities can be combined with conversational agents built on Amazon Bedrock.
    This allows the agents to perform multi-step tasks and leverage external knowledge
    bases to generate responses that are more contextually relevant.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 模型可以与其他 GenAI 工具和应用集成，以创建更强大和通用的 AI 系统。例如，RAG 的知识检索能力可以与基于 Amazon Bedrock
    的对话代理结合使用。这使得代理能够执行多步任务并利用外部知识库生成更具情境相关性的响应。
- en: Additionally, the RAG knowledge base retrieval enables seamless integration
    of RAG into custom GenAI pipelines. Developers can retrieve knowledge from RAG
    indexes and fuse it with LangChain’s generative capabilities. This unlocks new
    use cases such as building AI assistants that can provide expert domain knowledge
    alongside general conversational abilities.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，RAG 知识库检索使 RAG 能够无缝集成到自定义 GenAI 管道中。开发者可以从 RAG 索引中检索知识并将其与 LangChain 的生成能力融合。这解锁了新的用例，例如构建能够提供专家领域知识的同时具备一般对话能力的
    AI 助手。
- en: Further information on LangChain retrievers can be found at [https://python.langchain.com/docs/integrations/retrievers](https://python.langchain.com/docs/integrations/retrievers).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 LangChain 检索器的更多信息可以在 [https://python.langchain.com/docs/integrations/retrievers](https://python.langchain.com/docs/integrations/retrievers)
    找到。
- en: We will cover more details about agents for Amazon Bedrock in [*Chapter 10*](B22045_10.xhtml#_idTextAnchor192)
    where we will uncover more RAG-based integration with Amazon Bedrock agents.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 [*第 10 章*](B22045_10.xhtml#_idTextAnchor192) 中详细介绍 Amazon Bedrock 的代理，我们将揭示更多基于
    RAG 的与 Amazon Bedrock 代理的集成。
- en: With Amazon Bedrock’s managed approach, incorporating real-world knowledge into
    FMs has become more accessible than ever. Now, let us uncover some advanced RAG
    techniques that are rapidly growing as a mechanism to improve upon current RAG
    approaches.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Amazon Bedrock 的托管方法，将现实世界知识纳入 FM 已变得比以往任何时候都更容易。现在，让我们揭示一些正在迅速增长的 RAG 技术作为改进当前
    RAG 方法的一种机制。
- en: Advanced RAG techniques
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级 RAG 技术
- en: While basic RAG pipelines involve retrieving relevant documents and directly
    providing them as context to the LLM, advanced RAG techniques employ various methods
    to enhance the quality, relevance, and factual accuracy of generated responses.
    These advanced techniques go beyond the naive approach of simple document retrieval
    and context augmentation, aiming to optimize various stages of the RAG pipeline
    for improved performance.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基本的 RAG 管道涉及检索相关文档并直接将其作为上下文提供给 LLM，但高级 RAG 技术采用各种方法来提高生成响应的质量、相关性和事实准确性。这些高级技术超越了简单文档检索和上下文增强的朴素方法，旨在优化
    RAG 管道的各个阶段以改善性能。
- en: Let’s now look at some key areas where advanced RAG techniques focus.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在看看高级 RAG 技术关注的几个关键领域。
- en: Query handler – query reformulation and expansion
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询处理器 – 查询重构和扩展
- en: 'One key area of advancement is query reformulation and expansion. Instead of
    relying solely on the user’s initial query, advanced RAG systems employ NLP techniques
    to generate additional related queries. This increases the chances of retrieving
    a more comprehensive set of relevant information from the knowledge base. Query
    reformulation can involve techniques such as the following:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键的发展领域是查询重构和扩展。除了依赖用户的初始查询外，高级RAG系统采用NLP技术生成额外的相关查询。这增加了从知识库中检索更全面的相关信息的机会。查询重构可能涉及以下技术：
- en: '`"``Hurricane formation"`'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"``飓风形成"`'
- en: '*Expanded query*: `"Hurricane formation" OR "Tropical cyclone genesis" OR "Tropical`
    `storm development"`'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*扩展查询*：`“飓风形成” OR “热带气旋生成” OR “热带风暴发展”`'
- en: '`What` `causes hurricanes?`*   *Rewritten query*: `Explain the meteorological
    conditions and processes that lead to the formation of hurricanes or` `tropical
    cyclones.`*   `When was the first` `iPhone released?`*   *Extracted* *entities*:
    `iPhone`*   *Expanded query*: `iPhone AND ("product launch" OR "release date"`
    `OR "history")`*   `Causes of the American` `Civil War`”*   *Generated queries*:'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`什么原因导致飓风？`*   *重写查询*：`解释导致飓风或热带气旋形成的气象条件和过程。`*   `第一代`iPhone`是什么时候发布的？`*   *提取实体*：`iPhone`*   *扩展查询*：`iPhone
    AND ("产品发布" OR "发布日期" OR "历史")`*   `美国内战的原因`”*   *生成的查询*：'
- en: '`What were the key political and economic factors that led to the American`
    `Civil War?`'
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`导致美国内战的关键政治和经济因素是什么？`'
- en: '`How did the issue of slavery contribute to starting the American` `Civil War?`'
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`奴隶制问题是如何导致美国内战爆发的？`'
- en: '`What were the major events and incidents that precipitated the outbreak of
    the Civil War` `in America?`'
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`导致美国内战爆发的主要事件和事件有哪些？`'
- en: '*Figure 5**.15* illustrates an overview of a query handler with rewriting and
    re-ranking mechanisms:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.15* 展示了一个带有重写和重新排序机制的查询处理器的概述：'
- en: '![Figure 5.15 – Query handler with rewriting and re-ranking mechanisms](img/B22045_05_15.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图5.15 – 带有重写和重新排序机制的查询处理器](img/B22045_05_15.jpg)'
- en: Figure 5.15 – Query handler with rewriting and re-ranking mechanisms
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15 – 带有重写和重新排序机制的查询处理器
- en: By retrieving information for multiple reformulated queries, the system can
    gather a richer context to better understand the user’s intent and provide more
    complete and accurate responses.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检索多个重构查询的信息，系统可以收集更丰富的上下文，更好地理解用户的意图，并提供更完整、更准确的响应。
- en: Hybrid search and retrieval
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合搜索和检索
- en: 'Advanced RAG systems often employ hybrid retrieval strategies that combine
    different retrieval methods to leverage their respective strengths. For example,
    a system might use sparse vector search for initial filtering, followed by dense
    vector search for re-ranking and surfacing the most relevant documents. Other
    hybrid approaches include the following:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 高级RAG系统通常采用混合检索策略，结合不同的检索方法以利用它们各自的优势。例如，一个系统可能会使用稀疏向量搜索进行初始过滤，然后使用密集向量搜索进行重新排序和展示最相关的文档。其他混合方法包括以下：
- en: Combining keyword matching with vector similarity search
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将关键词匹配与向量相似度搜索相结合
- en: Using different retrieval methods for different types of data (for example,
    structured versus unstructured)
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为不同类型的数据使用不同的检索方法（例如，结构化与非结构化）
- en: Hierarchical retrieval, where coarse-grained retrieval is followed by fine-grained
    re-ranking
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次检索，其中粗粒度检索后跟细粒度重新排序
- en: 'Here’s a simple example to illustrate hybrid search and retrieval:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个简单的例子，用于说明混合搜索和检索：
- en: '![Figure 5.16 – Hybrid search and retrieval approach](img/B22045_05_16.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![图5.16 – 混合搜索和检索方法](img/B22045_05_16.jpg)'
- en: Figure 5.16 – Hybrid search and retrieval approach
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.16 – 混合搜索和检索方法
- en: Let’s say you are searching for information about `apple products` on a website
    that sells electronics and grocery items.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在一个销售电子产品和杂货的网站上搜索有关 `apple products` 的信息。
- en: 'The hybrid search approach combines two retrieval methods:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 混合搜索方法结合了两种检索方法：
- en: '`apple products`, it will retrieve documents/pages that contain the words `apple`
    and `products`, such as the following:'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apple products`，它将检索包含单词 `apple` 和 `products` 的文档/页面，例如以下内容：'
- en: '`Buy the latest Apple iPhone` `models here`'
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`在此处购买最新的苹果iPhone型号`'
- en: '`Apple MacBook Pro laptops` `on sale`'
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`正在销售的苹果 MacBook Pro 笔记本电脑`'
- en: '`Apple cider and apple juice in the` `grocery section`'
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`杂货区中的苹果 cider 和苹果汁`'
- en: '`apple products`, it may retrieve documents such as the following:'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apple products`，它可能会检索到以下文档：'
- en: '`Top tech gadgets and accessories for students` (semantically related to electronics/products)'
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`顶级科技小工具和配件为学生`（与电子产品/产品语义相关）'
- en: '`Healthy fruits and snacks for kids'' lunchboxes` (semantically related to
    apple as a fruit)'
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`适合儿童午餐盒的健康水果和零食`（与苹果作为水果的语义相关）'
- en: 'The hybrid search can then combine and re-rank the results from both retrieval
    methods:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 混合搜索可以随后结合并重新排序来自两种检索方法的结果：
- en: '`Buy the latest Apple iPhone` `models here`'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`在此处购买最新的苹果 iPhone 模型`'
- en: '`Apple MacBook Pro laptops` `on sale`'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`促销中的苹果 MacBook Pro 笔记本电脑`'
- en: '`Top tech gadgets and accessories` `for students`'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`顶级科技小工具和配件` `为学生`'
- en: '`Apple cider and apple juice in the` `grocery section`'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`杂货区中的苹果醋和苹果汁`'
- en: '`Healthy fruits and snacks for` `kids'' lunchboxes`'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`适合儿童午餐盒的健康水果和零食`'
- en: By combining keyword matching (for brand/product names) and semantic understanding
    (for broader context), the hybrid approach can provide more comprehensive and
    relevant search results compared to using just one method.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合关键词匹配（用于品牌/产品名称）和语义理解（用于更广泛的上下文），混合方法可以提供比仅使用一种方法更全面和相关的搜索结果。
- en: The key benefit is retrieving documents that are relevant both lexically (containing
    the exact query keywords) and semantically (related conceptually to the query
    intent), improving overall search quality.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 关键好处是检索到既在词汇上（包含确切的查询关键词）又在语义上（概念上与查询意图相关）相关的文档，从而提高整体搜索质量。
- en: Embedding and index optimization
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入和索引优化
- en: 'The quality of the vector embeddings and indexes used for retrieval can significantly
    impact the performance of RAG systems. Advanced techniques in this area include
    the following:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 用于检索的向量嵌入和索引的质量可以显著影响 RAG 系统的性能。该领域的先进技术包括以下内容：
- en: '**Embedding fine-tuning**: Instead of using a general pre-trained embedding
    model, the embedding model can be fine-tuned on domain-specific data to better
    capture the semantics and nuances of that domain.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入微调**：不是使用通用的预训练嵌入模型，嵌入模型可以在特定领域的数据上进行微调，以更好地捕捉该领域的语义和细微差别。'
- en: For example, if building a RAG system for a medical question answering task,
    the embedding model can be further fine-tuned on a large corpus of medical literature,
    such as research papers, clinical notes, and so on. This allows the model to better
    understand domain-specific terminology, abbreviations, and contextual relationships.
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，如果为医学问答任务构建 RAG 系统，则可以在大型医学文献语料库（如研究论文、临床笔记等）上进一步微调嵌入模型。这允许模型更好地理解特定领域的术语、缩写和上下文关系。
- en: '**Index structuring and partitioning**: Instead of storing all document embeddings
    in a single flat index, the index can be structured or partitioned in ways that
    improve retrieval efficiency; for example, clustering, hierarchical indexing,
    and metadata filtering:'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**索引结构和分区**：不是将所有文档嵌入存储在单个平面索引中，索引可以以改进检索效率的方式进行结构化或分区；例如，聚类、分层索引和元数据过滤：'
- en: '**Clustering**: Documents can be clustered based on their embeddings, and separate
    indices created for each cluster. At query time, the query embedding is compared
    against cluster centroids to identify the relevant cluster(s) to search within.'
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：可以根据文档的嵌入进行聚类，并为每个聚类创建单独的索引。在查询时，将查询嵌入与聚类中心进行比较，以确定要搜索的相关聚类（们）。'
- en: '**Hierarchical indexing**: A coarse-level index can first retrieve relevant
    high-level topics/categories, and then finer-grained indices are searched within
    those topics.'
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分层索引**：粗粒度索引首先检索相关的高级主题/类别，然后在那些主题内搜索更细粒度的索引。'
- en: '**Metadata filtering**: If document metadata such as type, source, date, and
    so on is available, the index can be partitioned based on that metadata to allow
    filtering before vector search.'
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元数据过滤**：如果文档元数据（如类型、来源、日期等）可用，则可以根据该元数据对索引进行分区，以便在向量搜索之前进行过滤。'
- en: '*Figure 5**.17* depicts an advanced retrieval mechanism enriched with metadata:'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*图 5.17* 展示了一个增强元数据的先进检索机制：'
- en: '![Figure 5.17 – Retrieval mechanism enriched with metadata](img/B22045_05_17.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.17 – 增强元数据的检索机制](img/B22045_05_17.jpg)'
- en: Figure 5.17 – Retrieval mechanism enriched with metadata
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17 – 增强元数据的检索机制
- en: '**Approximate NN (ANN) indexing**: For very large vector indices, techniques
    such as **Hierarchical Navigable Small World** (**HNSW**), FAISS, or **Approximate
    Nearest Neighbors Oh Yeah** (**Annoy**) can be used to create ANN indices. This
    allows trading off some accuracy for massive computational speedups in retrieval
    time over brute-force search. Interested readers can read more details about indexing
    for NN search in the paper *Learning to Index for Nearest Neighbor* *Search* ([https://arxiv.org/pdf/1807.02962](https://arxiv.org/pdf/1807.02962)).'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**近似最近邻（ANN）索引**：对于非常大的向量索引，可以使用**分层可导航小世界**（**HNSW**）、FAISS或**近似最近邻哦，是的**（**Annoy**）等技术来创建ANN索引。这允许在检索时间上通过暴力搜索进行大量计算速度的提升，同时牺牲一些准确性。感兴趣的读者可以阅读关于NN搜索索引的论文《学习为最近邻搜索索引》(*Learning
    to Index for Nearest Neighbor Search*) ([https://arxiv.org/pdf/1807.02962](https://arxiv.org/pdf/1807.02962))
    的更多细节。'
- en: '**Index compression and quantization**: The size of vector indices can be reduced
    through compression and quantization techniques without significantly impacting
    retrieval accuracy. This includes methods such as product quantization, scalar
    quantization, residual quantization, and so on.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**索引压缩和量化**：可以通过压缩和量化技术减小向量索引的大小，而不会显著影响检索准确性。这包括产品量化、标量量化、残差量化等方法等。'
- en: 'The paper *Vector Quantization for Recommender Systems: A Review and Outlook*
    ([https://arxiv.org/html/2405.03110v1](https://arxiv.org/html/2405.03110v1)) provides
    a detailed overview of vector quantization for recommender systems. By optimizing
    the embeddings and indexes, advanced RAG systems can improve the relevance and
    comprehensiveness of the retrieved information, leading to better overall performance.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '论文《推荐系统中的向量量化：综述与展望》(*Vector Quantization for Recommender Systems: A Review
    and Outlook*) ([https://arxiv.org/html/2405.03110v1](https://arxiv.org/html/2405.03110v1))
    提供了关于推荐系统中向量量化的详细概述。通过优化嵌入和索引，高级RAG系统可以提高检索信息的关联性和全面性，从而提高整体性能。'
- en: Retrieval re-ranking and filtering
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索重新排序和过滤
- en: 'Even after initial retrieval, advanced RAG systems often employ additional
    re-ranking and filtering techniques to surface the most relevant information.
    These techniques include the following:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在初步检索之后，高级RAG系统通常还会采用额外的重新排序和过滤技术来展示最相关的信息。这些技术包括以下内容：
- en: '**Cross-attention re-ranking**: Using more expensive cross-attention models
    can be leveraged in order to re-score and re-rank initially retrieved documents
    based on their relevance to the query. The paper *Multi-Vector Attention Models
    for Deep Re-ranking* ([https://aclanthology.org/2021.emnlp-main.443.pdf](https://aclanthology.org/2021.emnlp-main.443.pdf))
    provides a mechanism for deep re-ranking using multi-vector attention models.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉注意力重新排序**：可以利用更昂贵的交叉注意力模型来重新评分和重新排序最初检索到的文档，基于它们与查询的相关性。论文《用于深度重新排序的多向量注意力模型》(*Multi-Vector
    Attention Models for Deep Re-ranking*) ([https://aclanthology.org/2021.emnlp-main.443.pdf](https://aclanthology.org/2021.emnlp-main.443.pdf))
    提供了一种使用多向量注意力模型进行深度重新排序的机制。'
- en: '**Learned re-rankers**: Training neural networks or other ML models specifically
    for the task of re-ranking retrieved documents can assist in improving the search
    results augmented with the input query.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习型重新排序器**：针对重新排序检索文档的任务训练神经网络或其他机器学习模型可以帮助提高带有输入查询的搜索结果。'
- en: '**Filtering and pruning**: Removing less relevant or redundant documents from
    the initial retrieval set based on various heuristics or models can provide contextual
    optimization.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过滤和修剪**：根据各种启发式方法或模型从初始检索集中移除不太相关或冗余的文档，可以提供上下文优化。'
- en: 'For example, the user may ask a query: `What were the causes of the American`
    `Civil War?`'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，用户可能会提出这样的查询：`美国内战的原因是什么？`
- en: 'Here are some examples of initial retrieval via vector search:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些通过向量搜索进行初始检索的例子：
- en: '`The issue of slavery was a primary cause of the` `Civil War...`'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`奴隶问题是美国内战` `的主要原因...`'
- en: '`Economic differences between North and South led` `to tensions...`'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`南北之间的经济差异导致了` `紧张...`'
- en: '`The election of Abraham Lincoln in 1860` `triggered secession...`'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`亚伯拉罕·林肯1860年的选举` `触发了分离...`'
- en: '`The Missouri Compromise failed to resolve` `slavery expansion...`'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`密苏里妥协未能解决` `奴隶扩张...`'
- en: '`The Underground Railroad helped enslaved` `people escape...`'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`地下铁路帮助奴隶` `逃离...`'
- en: 'Here are some examples of re-ranking:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些重新排序的例子：
- en: '`The election of Abraham Lincoln in 1860` `triggered secession...`'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`亚伯拉罕·林肯1860年的选举` `触发了分离...`'
- en: '`The issue of slavery was a primary cause of the` `Civil War...`'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`奴隶问题是美国内战` `的主要原因...`'
- en: '`The Missouri Compromise failed to resolve` `slavery expansion...`'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`密苏里妥协未能解决` `奴隶扩张...`'
- en: '`Economic differences between North and South led` `to tensions...`'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`南北之间的经济差异导致了紧张...`'
- en: '`The Underground Railroad helped enslaved` `people escape...`'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`地下铁路帮助奴隶` `逃离...`'
- en: 'Here are the top 3 re-ranked and filtered results:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是前3个重新排序和过滤的结果：
- en: '`The election of Abraham Lincoln in 1860` `triggered secession...`'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`亚伯拉罕·林肯在1860年的选举` `引发了分离...`'
- en: '`The issue of slavery was a primary cause of the` `Civil War...`'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`奴隶问题是内战的主要原因...`'
- en: '`The Missouri Compromise failed to resolve` `slavery expansion...`'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`密苏里妥协未能解决` `奴隶扩张...`'
- en: The top 3 re-ranked and filtered results are then provided as context to the
    language model for generating a final response about the causes of the Civil War,
    focused on the most relevant information.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将前3个重新排序和过滤的结果作为上下文提供给语言模型，以生成关于内战原因的最终响应，重点关注最相关的信息。
- en: By re-ranking and filtering the retrieved information, advanced RAG systems
    can provide the LLM with a more focused and relevant context, improving the quality
    and factual accuracy of the generated responses.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重新排序和过滤检索到的信息，高级RAG系统可以为LLM提供更专注和相关的上下文，从而提高生成响应的质量和事实准确性。
- en: '*Figure 5**.18* demonstrates a complete architectural flow using Amazon Bedrock
    and some of the advanced RAG techniques (re-ranking with hybrid search mechanism)
    in order to further enhance the output response:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5**.18*展示了使用Amazon Bedrock和一些高级RAG技术（使用混合搜索机制的重新排序）的完整架构流程，以进一步增强输出响应：'
- en: '![Figure 5.18 – Advanced RAG approach with Amazon Bedrock](img/B22045_05_18.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![图5.18 – 使用Amazon Bedrock的高级RAG方法](img/B22045_05_18.jpg)'
- en: Figure 5.18 – Advanced RAG approach with Amazon Bedrock
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.18 – 使用Amazon Bedrock的高级RAG方法
- en: As depicted in *Figure 5**.18*, employing hybrid search (natively available)
    within Knowledge Bases for Amazon Bedrock can greatly enhance contextual search
    quality. Additionally, instead of parsing the data chunks directly to the LLM,
    feeding the retrieved data chunks to a re-ranker model in order to rank the contextual
    results can further improve the quality of the output. Cohere Rerank, Meta’s Dense
    Passage Retrieval, BERT for re-ranking, or open source models in Hugging Face
    (`cross-encoder`/`ms-marco-MiniLM-L6-v2`) are a few examples of re-ranking models
    that can be utilized for such ranking optimization tasks. Finally, once the augmented
    prompt is created with an enhanced query and optimized context, wherein the prompt
    is parsed to the Amazon Bedrock LLM for outputting a desirable response.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图5**.18*所示，在Amazon Bedrock的知识库中采用混合搜索（原生可用）可以极大地提高上下文搜索质量。此外，而不是直接将数据块解析到LLM，将检索到的数据块馈送到一个重新排序模型以对上下文结果进行排序，可以进一步提高输出质量。Cohere
    Rerank、Meta的密集段落检索、BERT用于重新排序，或Hugging Face（`cross-encoder`/`ms-marco-MiniLM-L6-v2`）的开源模型是一些可以用于此类排名优化任务的重新排序模型示例。最后，一旦创建了带有增强查询和优化上下文的增强提示，其中提示被解析到Amazon
    Bedrock LLM以输出期望的响应。
- en: In such a manner, advanced RAG techniques can aim to enhance the quality, relevance,
    and factual accuracy of language model outputs by improving various stages of
    the RAG pipeline by incorporating these techniques. Readers are encouraged to
    visit [https://aws.amazon.com/blogs/machine-learning/create-a-multimodal-assistant-with-advanced-rag-and-amazon-bedrock/](https://aws.amazon.com/blogs/machine-learning/create-a-multimodal-assistant-with-advanced-rag-and-amazon-bedrock/)
    to learn how to implement **multimodal RAG** (**mmRAG**) with Amazon Bedrock using
    advanced RAG techniques. This solution also uncovers comprehensive solutioning
    by leveraging advanced LangChain capabilities.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，高级RAG技术可以通过将这些技术融入RAG管道的各个阶段，旨在通过提高质量、相关性和事实准确性来增强语言模型输出的效果。读者被鼓励访问[https://aws.amazon.com/blogs/machine-learning/create-a-multimodal-assistant-with-advanced-rag-and-amazon-bedrock/](https://aws.amazon.com/blogs/machine-learning/create-a-multimodal-assistant-with-advanced-rag-and-amazon-bedrock/)，了解如何使用高级RAG技术通过Amazon
    Bedrock实现**多模态RAG**（**mmRAG**）。此解决方案还通过利用高级LangChain功能，揭示了全面的解决方案。
- en: 'The paper *RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation*
    ([https://arxiv.org/html/2404.00610v1](https://arxiv.org/html/2404.00610v1)) walks
    through yet another advanced RAG approach – **Refine Query for RAG** (**RQ-RAG**),
    which can aid in further optimization of queries by equipping it with capabilities
    for explicit rewriting, decomposition, and disambiguation.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 论文《RQ-RAG：学习用于检索增强生成的查询优化》([https://arxiv.org/html/2404.00610v1](https://arxiv.org/html/2404.00610v1))介绍了另一种高级的RAG方法——**针对RAG的查询优化**（**RQ-RAG**），它通过提供显式重写、分解和消歧的能力，有助于进一步优化查询。
- en: Since we have uncovered deeper details into RAG functionality, training, and
    its implementation with Bedrock and other GenAI systems, one should also keep
    in mind some limitations and research problems. This provides an opportunity for
    us to evolve with further enhancements with RAG and lead GenAI pathways with more
    insightful thought processes. Some of the limitations and future directions are
    discussed in the next section.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经深入了解了RAG的功能、训练以及与Bedrock和其他GenAI系统的实现，我们还应记住一些局限性和研究问题。这为我们提供了进一步通过RAG进行改进和以更具洞察力的思维过程引领GenAI路径的机会。一些局限性和未来方向将在下一节中讨论。
- en: Limitations and future directions
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 局限性和未来方向
- en: 'While promising, RAG models also come with challenges and open research problems,
    including the following:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有前景，但RAG模型也带来了挑战和开放的研究问题，包括以下内容：
- en: '**Knowledge selection**:One of the critical challenges in RAG is determining
    the most relevant and salient knowledge to retrieve from the knowledge base. With
    vast amounts of information available, it becomes crucial to identify and prioritize
    the most pertinent knowledge for the given context. Existing retrieval methods
    may struggle to capture the nuances and subtleties of the query, leading to the
    retrieval of irrelevant or tangential information. Developing more sophisticated
    query understanding and knowledge selection mechanisms is a key area of research.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识选择**：在RAG中，确定从知识库中检索的最相关和显著的知识是一个关键挑战。在大量信息可用的情况下，识别和优先考虑给定上下文中最相关的知识变得至关重要。现有的检索方法可能难以捕捉查询的细微差别和微妙之处，导致检索到不相关或旁枝末节的信息。开发更复杂的查询理解和知识选择机制是研究的关键领域。'
- en: '**Knowledge grounding**: Seamlessly integrating retrieved knowledge into the
    generation process is a non-trivial task. RAG models need to understand the retrieved
    knowledge, reason over it, and coherently weave it into the generated text. This
    process requires advanced **NL understanding** (**NLU**) and **NL generation**
    (**NLG**) capabilities, as well as a deep understanding of the context and discourse
    structure. Failure to ground the retrieved knowledge properly can lead to inconsistencies,
    incoherence, or factual errors in the generated output.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识归一化**：无缝地将检索到的知识整合到生成过程中是一个非平凡的任务。RAG模型需要理解检索到的知识，对其进行推理，并将其连贯地编织到生成的文本中。这个过程需要高级的**自然语言理解**（**NLU**）和**自然语言生成**（**NLG**）能力，以及对上下文和话语结构的深入理解。未能正确归一化检索到的知识可能导致生成的输出中出现不一致性、不连贯性或事实错误。'
- en: '**Training objectives**: One of the major limitations of RAG is the lack of
    large-scale supervised datasets for end-to-end training. Creating such datasets
    requires extensive human annotation, which is time-consuming and costly. Additionally,
    defining suitable training objectives that balance the retrieval and generation
    components is challenging. Existing training objectives may not adequately capture
    the complexity of the task, leading to sub-optimal performance.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练目标**：RAG（Retrieval Augmented Generation）的一个主要局限性是缺乏用于端到端训练的大规模监督数据集。创建这样的数据集需要大量的人工标注，这既耗时又昂贵。此外，定义合适的训练目标以平衡检索和生成组件是一项挑战。现有的训练目标可能无法充分捕捉任务的复杂性，从而导致性能不佳。'
- en: '**Knowledge base construction**: The quality and coverage of the knowledge
    base play a crucial role in the effectiveness of RAG models. Creating broad-coverage
    knowledge bases that span diverse domains and topics is a daunting task. Existing
    knowledge bases may be incomplete, biased, or outdated, limiting the model’s ability
    to retrieve relevant information. Furthermore, ensuring the accuracy and factual
    correctness of the knowledge base is essential but challenging, especially for
    rapidly evolving or controversial topics.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识库构建**：知识库的质量和覆盖范围在RAG模型的有效性中起着至关重要的作用。创建涵盖多个领域和主题的广泛覆盖知识库是一项艰巨的任务。现有的知识库可能不完整、有偏见或过时，限制了模型检索相关信息的能力。此外，确保知识库的准确性和事实正确性是至关重要的，但具有挑战性，尤其是在快速演变或具有争议性的主题上。'
- en: '**Multi-step reasoning**: RAG systems often struggle with combining retrieved
    knowledge across multiple steps to perform complex reasoning or inference tasks.
    Technical domains frequently require multi-step reasoning, such as deriving conclusions
    from multiple premises, following intricate logical chains, or synthesizing information
    from diverse sources. Current RAG systems may lack the capability to effectively
    integrate and reason over retrieved knowledge in a coherent and logical manner,
    limiting their applicability in scenarios involving intricate reasoning processes.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多步推理**：RAG系统通常难以在多个步骤中结合检索到的知识以执行复杂的推理或推断任务。技术领域通常需要多步推理，例如从多个前提中得出结论、遵循复杂的逻辑链或从不同的来源综合信息。当前的RAG系统可能缺乏有效整合和推理检索知识的能力，以连贯和逻辑的方式，限制了它们在涉及复杂推理过程的场景中的应用。'
- en: '**Evaluation**: Evaluating the performance of RAG models is challenging due
    to the complexity of the task. Traditional metrics for text generation, such as
    perplexity or **BiLingual Evaluation Understudy** (**BLEU**) scores, may not adequately
    capture the factual correctness, coherence, and consistency of the generated output.
    Developing robust evaluation methodologies that consider these aspects, as well
    as the quality of the retrieved knowledge, is an open research problem. Readers
    are encouraged to check out Ragas ([https://docs.ragas.io/en/v0.1.6/index.html](https://docs.ragas.io/en/v0.1.6/index.html)),
    which is essentially a framework to assist you evaluate your RAG pipelines at
    scale.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估**：由于任务复杂，评估RAG模型的性能具有挑战性。传统的文本生成指标，如困惑度或**双语评估助手**（**BLEU**）分数，可能无法充分捕捉生成输出的事实正确性、连贯性和一致性。开发考虑这些方面以及检索知识质量的稳健评估方法是一个开放的研究问题。鼓励读者查看Ragas（[https://docs.ragas.io/en/v0.1.6/index.html](https://docs.ragas.io/en/v0.1.6/index.html)），它本质上是一个框架，用于帮助您大规模评估您的RAG管道。'
- en: Despite these limitations, RAG holds significant promise for enhancing the capabilities
    of GenAI models by leveraging external knowledge sources. Addressing these challenges
    will be crucial for the widespread adoption and success of RAG in various applications,
    such as question answering, dialogue systems, and content generation.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些限制，RAG通过利用外部知识源来增强GenAI模型的能力，具有重大的发展潜力。解决这些挑战对于RAG在各种应用（如问答、对话系统和内容生成）中的广泛应用和成功至关重要。
- en: Key research priorities going forward include improving retrieval precision,
    developing more sophisticated fusion methods, exploring efficient large-scale
    training techniques, and creating better evaluation benchmarks.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的关键研究优先事项包括提高检索精度、开发更复杂的融合方法、探索高效的大规模训练技术以及创建更好的评估基准。
- en: '**Future directions**'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '**未来方向**'
- en: Researchers are exploring advanced techniques such as dense passage retrieval,
    learned sparse representations, and hybrid approaches that combine symbolic and
    neural methods. Additionally, incorporating external knowledge sources beyond
    traditional corpora, such as structured databases or knowledge graphs, could significantly
    improve retrieval precision and context understanding.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员正在探索诸如密集篇章检索、学习稀疏表示和结合符号和神经方法的混合方法等高级技术。此外，将外部知识源（如结构化数据库或知识图谱）纳入传统语料库之外，可以显著提高检索精度和上下文理解。
- en: Developing more sophisticated fusion methods is another critical area of research.
    While current approaches such as retrieval-augmented language models have shown
    promising results, they often rely on simple concatenation or attention mechanisms
    to fuse retrieved information with the language model’s generation. Researchers
    are investigating more advanced fusion techniques that can better capture the
    complex relationships between retrieved knowledge and the generation context,
    potentially leveraging techniques from areas such as multi-modal learning, **graph
    neural networks** (**GNNs**), and neuro-symbolic reasoning.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 开发更复杂的融合方法也是另一个关键的研究领域。虽然当前方法，如检索增强语言模型，已经显示出有希望的结果，但它们通常依赖于简单的连接或注意力机制来融合检索信息与语言模型的生成。研究人员正在研究更先进的融合技术，这些技术可以更好地捕捉检索知识与生成上下文之间的复杂关系，可能利用来自多模态学习、**图神经网络**（**GNNs**）和神经符号推理等领域的技巧。
- en: Exploring efficient large-scale training techniques is essential for scaling
    RAG to massive knowledge sources and complex domains. Current systems are often
    trained on relatively small datasets due to computational constraints, limiting
    their ability to effectively leverage vast knowledge repositories. Researchers
    are investigating techniques such as distributed training, knowledge distillation,
    and efficient retrieval indexing to enable training on large-scale knowledge sources
    while maintaining computational feasibility.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 探索高效的大规模训练技术对于将RAG扩展到庞大的知识来源和复杂领域至关重要。由于计算限制，当前系统通常在相对较小的数据集上训练，这限制了它们有效利用庞大知识库的能力。研究人员正在研究分布式训练、知识蒸馏和高效的检索索引等技术，以在保持计算可行性的同时，在大型知识源上进行训练。
- en: Finally, creating better evaluation benchmarks is crucial for accurately assessing
    the performance of RAG systems and driving progress in the field. Existing benchmarks
    often focus on specific tasks or domains, making it challenging to evaluate the
    generalization capabilities of these systems. Researchers are working on developing
    more comprehensive and challenging benchmarks that cover a wider range of knowledge
    sources, domains, and generation tasks, as well as incorporating more sophisticated
    evaluation metrics that go beyond traditional measures such as perplexity or BLEU
    scores.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，创建更好的评估基准对于准确评估RAG系统的性能并推动该领域的发展至关重要。现有的基准通常专注于特定的任务或领域，这使得评估这些系统的泛化能力变得具有挑战性。研究人员正在努力开发更全面、更具挑战性的基准，这些基准涵盖更广泛的知识来源、领域和生成任务，并纳入更复杂的评估指标，这些指标超越了传统的度量标准，如困惑度或BLEU分数。
- en: By addressing these key research priorities, the field of RAG can continue to
    advance, enabling the development of more powerful and versatile language generation
    systems that can effectively leverage vast knowledge repositories to produce high-quality,
    informative, and context-relevant text.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 通过解决这些关键研究重点，RAG领域可以继续进步，使开发更强大、更通用的语言生成系统成为可能，这些系统能够有效地利用庞大的知识库来生成高质量、信息丰富且与上下文相关的文本。
- en: Summary
  id: totrans-364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: RAG is a rapidly evolving technique that overcomes knowledge limitations in
    neural generative models by conditioning them on relevant external contexts. We
    uncovered how training LLMs with a RAG approach works and how to implement RAG
    with Amazon Bedrock, the LangChain orchestrator, and other GenAI systems. We further
    explored the importance and limitations of RAG approaches in the GenAI realm.
    As indicated, early results across a variety of domains are promising and demonstrate
    the potential of grounding text generation in real-world knowledge. As research
    addresses current limitations, retrieval augmentation could enable GenAI systems
    that are factual, informative, and safe.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是一种快速发展的技术，通过在相关外部上下文中对神经生成模型进行条件化，克服了这些模型在知识方面的局限性。我们揭示了使用RAG方法训练LLM的工作原理以及如何使用Amazon
    Bedrock、LangChain编排器和其他GenAI系统实现RAG。我们还进一步探讨了RAG方法在GenAI领域中的重要性及其局限性。正如所指示的，跨多个领域的早期结果很有希望，并展示了将文本生成基于现实世界知识的潜力。随着研究解决当前的局限性，检索增强可能使GenAI系统变得事实性、信息性和安全性。
- en: In the next chapter, we will delve into practical applications by employing
    various approaches on Amazon Bedrock. We will commence with a text summarization
    use case, and then explore insights into the methodologies and techniques in depth.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过在Amazon Bedrock上采用各种方法来深入探讨实际应用。我们将从文本摘要用例开始，然后深入探讨方法和技术的见解。
- en: 'Part 2: Amazon Bedrock Architecture Patterns'
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：Amazon Bedrock架构模式
- en: In this part, we will explore various architectural patterns and use cases for
    leveraging the powerful capabilities of Amazon Bedrock. These include text generation,
    building question answering systems, entity extraction, code generation, image
    creation, and developing intelligent agents. In addition, we will dive deep into
    the real-world applications, equipping you with the knowledge and skills to maximize
    Amazon Bedrock’s capabilities in your own projects.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分，我们将探讨各种架构模式和利用Amazon Bedrock强大功能的用例。这包括文本生成、构建问答系统、实体提取、代码生成、图像创建和开发智能代理。此外，我们将深入探讨现实世界应用，为您提供知识和技能，以便在您的项目中最大化Amazon
    Bedrock的功能。
- en: 'This part contains the following chapters:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 6*](B22045_06.xhtml#_idTextAnchor117), *Generating and Summarizing
    Text with Amazon Bedrock*'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第6章*](B22045_06.xhtml#_idTextAnchor117), *使用Amazon Bedrock生成和总结文本*'
- en: '[*Chapter 7*](B22045_07.xhtml#_idTextAnchor132), *Building Question Answering
    Systems and Conversational Interfaces*'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B22045_07.xhtml#_idTextAnchor132), *构建问答系统和对话界面*'
- en: '[*Chapter 8*](B22045_08.xhtml#_idTextAnchor151), *Extracting Entities and Generating
    Code with Amazon Bedrock*'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B22045_08.xhtml#_idTextAnchor151), *使用Amazon Bedrock提取实体和生成代码*'
- en: '[*Chapter 9*](B22045_09.xhtml#_idTextAnchor171), *Generating and Transforming*
    *Images Using Amazon Bedrock*'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B22045_09.xhtml#_idTextAnchor171), *使用Amazon Bedrock生成和转换图像*'
- en: '[*Chapter 10*](B22045_10.xhtml#_idTextAnchor192), *Developing Intelligent Agents
    with Amazon Bedrock*'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B22045_10.xhtml#_idTextAnchor192), *使用Amazon Bedrock开发智能代理*'
