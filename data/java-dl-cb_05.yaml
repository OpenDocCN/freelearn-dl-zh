- en: Implementing Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现自然语言处理
- en: In this chapter, we will discuss word vectors (Word2Vec) and paragraph vectors
    (Doc2Vec) in DL4J. We will develop a complete running example step by step, covering
    all the stages, such as ETL, model configuration, training, and evaluation. Word2Vec and Doc2Vec are
    **natural language processing** (**NLP**) implementations in DL4J. It is worth
    mentioning a little about the bag-of-words algorithm before we talk about Word2Vec.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论DL4J中的词向量（Word2Vec）和段落向量（Doc2Vec）。我们将逐步开发一个完整的运行示例，涵盖所有阶段，如ETL、模型配置、训练和评估。Word2Vec和Doc2Vec是DL4J中的**自然语言处理**（**NLP**）实现。在讨论Word2Vec之前，值得简单提一下词袋模型算法。
- en: '**Bag-of-words** is an algorithm that counts the instances of words in documents.
    This will allow us to perform document classification. Bag of words and Word2Vec are
    just two different types of text classification. **Word2Vec** can use a bag of
    words extracted from a document to create vectors. In addition to these text classification
    methods, **term frequency–inverse document frequency** (**TF-IDF**) can be used
    to judge the topic/context of the document. In the case of TF-IDF, a score will
    be calculated for all the words, and word counts will be replaced with this score.
    TF-IDF is a simple scoring scheme, but word embeddings may be a better choice,
    as the semantic similarity can be captured by word embedding. Also, if your dataset
    is small and the context is domain-specific, then bag of words may be a better
    choice than Word2Vec.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**词袋模型**是一种计数文档中词汇出现次数的算法。这将使我们能够执行文档分类。词袋模型和Word2Vec只是两种不同的文本分类方法。**Word2Vec**可以利用从文档中提取的词袋来创建向量。除了这些文本分类方法之外，**词频-逆文档频率**（**TF-IDF**）可以用来判断文档的主题/上下文。在TF-IDF的情况下，将计算所有单词的分数，并将词频替换为该分数。TF-IDF是一种简单的评分方案，但词嵌入可能是更好的选择，因为词嵌入可以捕捉到语义相似性。此外，如果你的数据集较小且上下文是特定领域的，那么词袋模型可能比Word2Vec更适合。'
- en: Word2Vec is a two-layer neural network that processes text. It converts the
    text corpus to vectors.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec是一个两层神经网络，用于处理文本。它将文本语料库转换为向量。
- en: Note that Word2Vec is not a **deep neural network** (**DNN**). It transforms
    text data into a numerical format that a DNN can understand, making customization
    possible.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Word2Vec并不是一个**深度神经网络**（**DNN**）。它将文本数据转化为DNN可以理解的数字格式，从而实现定制化。
- en: We can even combine Word2Vec with DNNs to serve this purpose. It doesn't train
    the input words through reconstruction; instead, it trains words using the neighboring
    words in the corpus.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以将Word2Vec与DNN结合使用来实现这一目的。它不会通过重建训练输入词；相反，它使用语料库中的邻近词来训练词汇。
- en: Doc2Vec (paragraph vectors) associates documents with labels, and is an extension
    of Word2Vec. Word2Vec tries to correlate words with words, while Doc2Vec (paragraph
    vectors) correlates words with labels. Once we represent documents in vector formats,
    we can then use these formats as an input to a supervised learning algorithm to
    map these vectors to labels.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Doc2Vec（段落向量）将文档与标签关联，它是Word2Vec的扩展。Word2Vec尝试将词与词相关联，而Doc2Vec（段落向量）则将词与标签相关联。一旦我们将文档表示为向量格式，就可以将这些格式作为输入提供给监督学习算法，将这些向量映射到标签。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下几种方法：
- en: Reading and loading text data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取和加载文本数据
- en: Tokenizing data and training the model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据进行分词并训练模型
- en: Evaluating the model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型
- en: Generating plots from the model
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从模型生成图形
- en: Saving and reloading the model
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存和重新加载模型
- en: Importing Google News vectors
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入Google News向量
- en: Troubleshooting and tuning Word2Vec models
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排查问题和调整Word2Vec模型
- en: Using Word2Vec for sentence classification using CNNs
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Word2Vec进行基于CNN的句子分类
- en: Using Doc2Vec for document classification
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Doc2Vec进行文档分类
- en: Technical requirements
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The examples discussed in this chapter can be found at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的示例可以在[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples)找到。
- en: After cloning our GitHub repository, navigate to the directory called `Java-Deep-Learning-Cookbook/05_Implementing_NLP/sourceCode`.Then,
    import the `cookbookapp` project as a Maven projectby importing `pom.xml`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 克隆我们的GitHub仓库后，导航到名为 `Java-Deep-Learning-Cookbook/05_Implementing_NLP/sourceCode` 的目录。然后，通过导入 `pom.xml` 将 `cookbookapp` 项目作为Maven项目导入。
- en: 'To get started with NLP in DL4J, add the following Maven dependency in `pom.xml`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用DL4J中的NLP，请在 `pom.xml` 中添加以下Maven依赖：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Data requirements
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据要求
- en: 'The project directory has a `resource` folder with the required data for the `LineIterator`
    examples:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 项目目录中有一个 `resource` 文件夹，其中包含用于 `LineIterator` 示例所需的数据：
- en: '![](img/afff3d5b-69da-41a6-8309-79bab888a6c6.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/afff3d5b-69da-41a6-8309-79bab888a6c6.png)'
- en: 'For `CnnWord2VecSentenceClassificationExample` or `GoogleNewsVectorExampleYou`,
    you can download datasets from the following URLs:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `CnnWord2VecSentenceClassificationExample` 或 `GoogleNewsVectorExampleYou`，你可以从以下网址下载数据集：
- en: '**Google News vector**: [https://deeplearning4jblob.blob.core.windows.net/resources/wordvectors/GoogleNews-vectors-negative300.bin.gz](https://deeplearning4jblob.blob.core.windows.net/resources/wordvectors/GoogleNews-vectors-negative300.bin.gz)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google News 向量**: [https://deeplearning4jblob.blob.core.windows.net/resources/wordvectors/GoogleNews-vectors-negative300.bin.gz](https://deeplearning4jblob.blob.core.windows.net/resources/wordvectors/GoogleNews-vectors-negative300.bin.gz)'
- en: '**IMDB review data**: [http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IMDB 评论数据**: [http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz)'
- en: Note that IMDB review data needs to be extracted twice in order to get the actual
    dataset folder.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，IMDB评论数据需要提取两次才能获得实际的数据集文件夹。
- en: For the **t-Distributed Stochastic Neighbor Embedding** (**t-SNE**) visualization
    example, the required data (`words.txt`) can be located in the project root directory
    itself.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**t-分布随机邻域嵌入**（**t-SNE**）可视化示例，所需的数据（`words.txt`）可以在项目根目录中找到。
- en: Reading and loading text data
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取和加载文本数据
- en: We need to load raw sentences in text format and iterate them using an underlined
    iterator that serves the purpose. A text corpus can also be subjected to preprocessing,
    such as lowercase conversion. Stop words can be mentioned while configuring the Word2Vec
    model. In this recipe, we will extract and load text data from various data-input
    scenarios.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要加载原始文本格式的句子，并使用一个下划线迭代器来迭代它们。文本语料库也可以进行预处理，例如转换为小写。在配置 Word2Vec 模型时，可以指定停用词。在本教程中，我们将从各种数据输入场景中提取并加载文本数据。
- en: Getting ready
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: Select an iterator approach from step 1 to step 5 depending on what kind of
    data you're looking for and how you want to load it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你要加载的数据类型和加载方式，从第1步到第5步选择一个迭代器方法。
- en: How to do it...
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Create a sentence iterator using `BasicLineIterator`:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `BasicLineIterator` 创建句子迭代器：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For an example, go to [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/BasicLineIteratorExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/BasicLineIteratorExample.java).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，访问 [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/BasicLineIteratorExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/BasicLineIteratorExample.java)。
- en: 'Create a sentence iterator using `LineSentenceIterator`:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `LineSentenceIterator` 创建句子迭代器：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For an example, go to **[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/LineSentenceIteratorExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/LineSentenceIteratorExample.java)**.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，访问**[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/LineSentenceIteratorExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/LineSentenceIteratorExample.java)**。
- en: 'Create a sentence iterator using `CollectionSentenceIterator`:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `CollectionSentenceIterator` 创建句子迭代器：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For an example, go to [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CollectionSentenceIteratorExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CollectionSentenceIteratorExample.java).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 查看示例，请访问[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CollectionSentenceIteratorExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CollectionSentenceIteratorExample.java)。
- en: 'Create a sentence iterator using `FileSentenceIterator`:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`FileSentenceIterator`创建一个句子迭代器：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: For an example, go to**[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/FileSentenceIteratorExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/FileSentenceIteratorExample.java).
    [](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/FileSentenceIteratorExample.java)**
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 查看示例，请访问**[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/FileSentenceIteratorExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/FileSentenceIteratorExample.java)**。
- en: Create a sentence iterator using `UimaSentenceIterator`.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`UimaSentenceIterator`创建一个句子迭代器。
- en: 'Add the following Maven dependency:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 添加以下 Maven 依赖：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then use the iterator, as shown here:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用迭代器，如下所示：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can also use it like this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以像这样使用它：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For an example, go to [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/UimaSentenceIteratorExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/UimaSentenceIteratorExample.java).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 查看示例，请访问[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/UimaSentenceIteratorExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/UimaSentenceIteratorExample.java)。
- en: 'Apply the preprocessor to the text corpus:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将预处理器应用到文本语料库：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For an example, go to [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/SentenceDataPreProcessor.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/SentenceDataPreProcessor.java).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 查看示例，请访问[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/SentenceDataPreProcessor.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/SentenceDataPreProcessor.java)。
- en: How it works...
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In step 1, we used `BasicLineIterator`, which is a basic, single-line sentence
    iterator without any customization involved.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 1 步中，我们使用了`BasicLineIterator`，这是一个基础的单行句子迭代器，没有涉及任何自定义。
- en: In step 2, we used `LineSentenceIterator` to iterate through multi-sentence
    text data. Each line is considered a sentence here. We can use them for multiple
    lines of text.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 2 步中，我们使用`LineSentenceIterator`来遍历多句文本数据。这里每一行都被视为一个句子。我们可以用它来处理多行文本。
- en: In step 3, **`CollectionSentenceIterator`** will accept a list of strings as
    text input where each string represents a sentence (document). This can be a list
    of tweets or articles.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 步中，**`CollectionSentenceIterator`**将接受一个字符串列表作为文本输入，每个字符串表示一个句子（文档）。这可以是一个包含推文或文章的列表。
- en: In step 4, **`FileSentenceIterator`** processes sentences in a file/directory.
    Sentences will be processed line by line from each file.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 4 步中，**`FileSentenceIterator`**处理文件/目录中的句子。每个文件的句子将逐行处理。
- en: For anything complex, we recommend that you use **`UimaSentenceIterator`**,
    which is a proper machine learning level pipeline. It iterates over a set of files
    and segments the sentences. The `UimaSentenceIterator` pipeline can perform tokenization,
    lemmatization, and part-of-speech tagging. The behavior can be customized based
    on the analysis engines that are passed on. This iterator is the best fit for
    complex data, such as data returned from the Twitter API. An analysis engine is
    a text-processing pipeline.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何复杂的情况，我们建议使用**`UimaSentenceIterator`**，它是一个适当的机器学习级别管道。它会遍历一组文件并分割句子。 `UimaSentenceIterator` 管道可以执行分词、词形还原和词性标注。其行为可以根据传递的分析引擎进行自定义。这个迭代器最适合复杂数据，比如来自
    Twitter API 的数据。分析引擎是一个文本处理管道。
- en: You need to use the `reset()` method if you want to begin the iterator traversal
    from the beginning after traversing once.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在遍历一次后重新开始迭代器的遍历，你需要使用 `reset()` 方法。
- en: We can normalize the data and remove anomalies by defining a preprocessor on
    the data iterator. Hence, we defined a normalizer (preprocessor) in step 5.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在数据迭代器上定义预处理器来规范化数据并移除异常。因此，在步骤 5 中，我们定义了一个归一化器（预处理器）。
- en: There's more...
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We can also create a sentence iterator using `UimaSentenceIterator` by passing
    an analysis engine, as shown in the following code:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过传递分析引擎来使用 `UimaSentenceIterator` 创建句子迭代器，代码如下所示：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The concept of an analysis engine is borrowed from UIMA's text-processing pipeline.
    DL4J has standard analysis engines available for common tasks that enable further
    text customization and decide how sentences are defined. Analysis engines are
    thread safe compared to OpenNLP text-processing pipelines. ClearTK-based pipelines
    are also used to handle common text-processing tasks in DL4J.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 分析引擎的概念借鉴自 UIMA 的文本处理管道。DL4J 提供了用于常见任务的标准分析引擎，支持进一步的文本自定义并决定句子的定义方式。分析引擎是线程安全的，相较于
    OpenNLP 的文本处理管道。基于 ClearTK 的管道也被用来处理 DL4J 中常见的文本处理任务。
- en: See also
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: '**UIMA**: [http://uima.apache.org/](http://uima.apache.org/)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**UIMA**: [http://uima.apache.org/](http://uima.apache.org/)'
- en: '**OpenNLP**: [http://opennlp.apache.org/](http://opennlp.apache.org/)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenNLP**: [http://opennlp.apache.org/](http://opennlp.apache.org/)'
- en: Tokenizing data and training the model
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词数据并训练模型
- en: We need to perform tokenization in order to build the Word2Vec models. The context
    of a sentence (document) is determined by the words in it. Word2Vec models require
    words rather than sentences (documents) to feed in, so we need to break the sentence
    into atomic units and create a token each time a white space is hit. DL4J has
    a tokenizer factory that is responsible for creating the tokenizer. The `TokenizerFactory`
    generates a tokenizer for the given string. In this recipe, we will tokenize the
    text data and train the Word2Vec model on top of them.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要执行分词操作以构建 Word2Vec 模型。句子（文档）的上下文是由其中的单词决定的。Word2Vec 模型需要的是单词而非句子（文档）作为输入，因此我们需要将句子拆分为原子单元，并在每次遇到空格时创建一个令牌。DL4J
    拥有一个分词器工厂，负责创建分词器。 `TokenizerFactory` 为给定的字符串生成分词器。在这个教程中，我们将对文本数据进行分词，并在其上训练
    Word2Vec 模型。
- en: How to do it...
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create a tokenizer factory and set the token preprocessor:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建分词器工厂并设置令牌预处理器：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Add the tokenizer factory to the Word2Vec model configuration:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将分词器工厂添加到 Word2Vec 模型配置中：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Train the Word2Vec model:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练 Word2Vec 模型：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: How it works...
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何运作...
- en: In step 1, we used `DefaultTokenizerFactory()` to create the tokenizer factory
    to tokenize the words. This is the default tokenizer for Word2Vec and it is based
    on a string tokenizer, or stream tokenizer. We also used `CommonPreprocessor`
    as the token preprocessor. A preprocessor will remove anomalies from the text
    corpus. The `CommonPreprocessor` is a token preprocessor implementation that removes
    punctuation marks and converts the text to lowercase. It uses the `toLowerCase(String)` method
    and its behavior depends on the default locale.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 1 中，我们使用了 `DefaultTokenizerFactory()` 来创建分词器工厂，用于将单词进行分词。 这是 Word2Vec 的默认分词器，它基于字符串分词器或流分词器。我们还使用了 `CommonPreprocessor` 作为令牌预处理器。预处理器会从文本语料库中移除异常。 `CommonPreprocessor` 是一个令牌预处理器实现，它移除标点符号并将文本转换为小写。它使用 `toLowerCase(String)` 方法，其行为取决于默认区域设置。
- en: 'Here are the configurations that we made in step 2:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们在步骤 2 中所做的配置：
- en: '`minWordFrequency()`: This is the minimum number of times in which a word must
    exist in the text corpora. In our example, if a word appears fewer than five times,
    then it is not learned. Words should occur multiple times in text corpora in order
    for the model to learn useful features about them. In very large text corpora,
    it''s reasonable to raise the minimum value of word occurrences.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minWordFrequency()`：这是词语在文本语料库中必须出现的最小次数。在我们的示例中，如果一个词出现次数少于五次，那么它将不会被学习。词语应在文本语料库中出现多次，以便模型能够学习到关于它们的有用特征。在非常大的文本语料库中，适当提高词语出现次数的最小值是合理的。'
- en: '`layerSize()`: This defines the number of features in a word vector. This is
    equivalent to the number of dimensions in the feature space. Words represented
    by 100 features become points in a 100-dimensional space.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layerSize()`：这定义了词向量中的特征数量。它等同于特征空间的维度数。用100个特征表示的词会成为100维空间中的一个点。'
- en: '`iterate()`: This specifies the batch on which the training is taking place.
    We can pass in an iterator to convert to word vectors. In our case, we passed
    in a sentence iterator.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iterate()`：这指定了训练正在进行的批次。我们可以传入一个迭代器，将其转换为词向量。在我们的例子中，我们传入了一个句子迭代器。'
- en: '`epochs()`: This specifies the number of iterations over the training corpus
    as a whole.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs()`：这指定了整个训练语料库的迭代次数。'
- en: '`windowSize():` This defines the context window size.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`windowSize()`：这定义了上下文窗口的大小。'
- en: There's more...
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: 'The following are the other tokenizer factory implementations available in
    DL4J Word2Vec to generate tokenizers for the given input:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是DL4J Word2Vec中可用的其他词法分析器工厂实现，用于为给定输入生成词法分析器：
- en: '`NGramTokenizerFactory`: This is the tokenizer factory that creates a tokenizer
    based on the *n*-gram model. *N*-grams are a combination of contiguous words or
    letters of length *n* that are present in the text corpus.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NGramTokenizerFactory`：这是一个基于*n*-gram模型创建词法分析器的工厂。*N*-grams是由文本语料库中的连续单词或字母组成，长度为*n*。'
- en: '`PosUimaTokenizerFactory`: This creates a tokenizer that filters part of the
    speech tags.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PosUimaTokenizerFactory`：这是一个创建词法分析器的工厂，能够过滤部分词性标注。'
- en: '`UimaTokenizerFactory`: This creates a tokenizer that uses the UIMA analysis
    engine for tokenization. The analysis engine performs an inspection of unstructured
    information, makes a discovery, and represents semantic content. Unstructured
    information is included, but is not restricted to text documents.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UimaTokenizerFactory`：这是一个使用UIMA分析引擎进行词法分析的工厂。该分析引擎对非结构化信息进行检查、发现并表示语义内容。非结构化信息包括但不限于文本文件。'
- en: 'Here are the inbuilt token preprocessors (not including `CommonPreprocessor`)
    available in DL4J:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是DL4J中内置的词元预处理器（不包括`CommonPreprocessor`）：
- en: '`EndingPreProcessor`: This is a preprocessor that gets rid of word endings
    in the text corpus—for example, it removes *s*, *ed*, *.*, *ly*, and *ing* from
    the text.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EndingPreProcessor`：这是一个去除文本语料库中词尾的预处理器——例如，它去除词尾的*s*、*ed*、*.*、*ly*和*ing*。'
- en: '`LowCasePreProcessor`: This is a preprocessor that converts text to lowercase
    format.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LowCasePreProcessor`：这是一个将文本转换为小写格式的预处理器。'
- en: '`StemmingPreprocessor`: This tokenizer preprocessor implements basic cleaning
    inherited from `CommonPreprocessor` and performs English porter stemming on tokens.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StemmingPreprocessor`：该词法分析器预处理器实现了从`CommonPreprocessor`继承的基本清理，并对词元执行英文Porter词干提取。'
- en: '`CustomStemmingPreprocessor`: This is the stemming preprocessor that is compatible
    with different stemming processors defined as lucene/tartarus `SnowballProgram`,
    such as `RussianStemmer`, `DutchStemmer`, and `FrenchStemmer`. This means that
    it is suitable for multilanguage stemming.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CustomStemmingPreprocessor`：这是一个词干预处理器，兼容不同的词干处理程序，例如lucene/tartarus定义的`SnowballProgram`，如`RussianStemmer`、`DutchStemmer`和`FrenchStemmer`。这意味着它适用于多语言词干化。'
- en: '`EmbeddedStemmingPreprocessor`: This tokenizer preprocessor uses a given preprocessor
    and performs English porter stemming on tokens on top of it.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EmbeddedStemmingPreprocessor`：该词法分析器预处理器使用给定的预处理器并在其基础上对词元执行英文Porter词干提取。'
- en: We can also implement our own token preprocessor—for example, a preprocessor
    to remove all stop words from the tokens.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以实现自己的词元预处理器——例如，一个移除所有停用词的预处理器。
- en: Evaluating the model
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: We need to check the feature vector quality during the evaluation process. This
    will give us an idea of the quality of the Word2Vec model that was generated.
    In this recipe, we will follow two different approaches to evaluate the Word2Vec
    model.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在评估过程中检查特征向量的质量。这将帮助我们了解生成的Word2Vec模型的质量。在本食谱中，我们将采用两种不同的方法来评估Word2Vec模型。
- en: How to do it...
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Find similar words to a given word:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到与给定词语相似的词：
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You will see an *n* output similar to the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到类似以下的*n*输出：
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Find the cosine similarity of the given two words:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到给定两个词的余弦相似度：
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'For the preceding example, the cosine similarity is calculated as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前面的示例，余弦相似度的计算方法如下：
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: How it works...
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In step 1, we found the top *n* similar words (similar in context) to a given
    word by calling `wordsNearest()`, providing both the input and count `n`. The `n` count
    is the number of words that we want to list.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们通过调用`wordsNearest()`，提供输入和数量`n`，找到了与给定词语上下文最相似的前*n*个词。`n`的数量是我们希望列出的词数。
- en: In step 2, we tried to find the similarity of two given words. To do this, we
    actually calculated the **cosine similarity** between the two given words. The
    cosine similarity is one of the useful metrics that we can use to find the similarity
    between words/documents. We converted input words into vectors using our trained
    model.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二步中，我们尝试找出两个给定词语的相似度。为此，我们实际上计算了这两个给定词语之间的**余弦相似度**。余弦相似度是我们用来衡量词语/文档相似度的有用度量之一。我们使用训练好的模型将输入词语转化为向量。
- en: There's more...
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Cosine similarity is the similarity between two nonzero vectors measured by
    the cosine of the angle between them. This metric measures the orientation instead
    of the magnitude because cosine similarity calculates the angle between document
    vectors instead of the word count. If the angle is zero, then the cosine value
    reaches 1, indicating that they are very similar. If the cosine similarity is
    near zero, then this indicates that there's less similarity between documents,
    and the document vectors will be orthogonal (perpendicular) to each other. Also,
    the documents that are dissimilar to each other will yield a negative cosine similarity.
    For such documents, cosine similarity can go up to -1, indicating an angle of
    1,800 between document vectors.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度是通过计算两个非零向量之间的角度余弦值来度量相似度的。这个度量方法衡量的是方向性，而不是大小，因为余弦相似度计算的是文档向量之间的角度，而不是词频。如果角度为零，那么余弦值将达到1，表示它们非常相似。如果余弦相似度接近零，则表示文档之间的相似度较低，文档向量将是正交（垂直）关系。此外，彼此不相似的文档会产生负的余弦相似度。对于这些文档，余弦相似度可能会达到-1，表示文档向量之间的角度为180度。
- en: Generating plots from the model
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从模型生成图表
- en: We have mentioned that we have been using a layer size of `100` while training
    the Word2Vec model. This means that there can be 100 features and, eventually,
    a 100-dimensional feature space. It is impossible to plot a 100-dimensional space,
    and therefore we rely on t-SNE to perform dimensionality reduction. In this recipe,
    we will generate 2D plots from the Word2Vec model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到，在训练Word2Vec模型时，我们使用了`100`的层大小。这意味着可以有100个特征，并最终形成一个100维的特征空间。我们无法绘制一个100维的空间，因此我们依赖于t-SNE进行降维。在本食谱中，我们将从Word2Vec模型中生成2D图表。
- en: Getting ready
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe, refer to the t-SNE visualization example found at: [//github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/TSNEVisualizationExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/TSNEVisualizationExample.java).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个配方，请参考以下t-SNE可视化示例：[//github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/TSNEVisualizationExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/TSNEVisualizationExample.java)。
- en: The example generates t-SNE plots in a CSV file.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 示例将在CSV文件中生成t-SNE图表。
- en: How to do it...
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Add the following snippet (at the beginning of the source code) to set the
    data type for the current JVM runtime:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在源代码的开头添加以下代码片段，以设置当前JVM运行时的数据类型：
- en: '[PRE17]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Write word vectors into a file:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将词向量写入文件：
- en: '[PRE18]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Separate the weights of the unique words into their own list using `WordVectorSerializer`:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`WordVectorSerializer`将唯一单词的权重分离成自己的列表：
- en: '[PRE19]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Create a list to add all unique words:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个列表来添加所有独特的词：
- en: '[PRE20]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Build a dual-tree t-SNE model for dimensionality reduction using `BarnesHutTsne`:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `BarnesHutTsne` 构建一个双树 t-SNE 模型来进行降维：
- en: '[PRE21]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Establish the t-SNE values and save them to a file:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立 t-SNE 值并将其保存到文件：
- en: '[PRE22]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: How it works...
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: In step 2, word vectors from the trained model are saved to your local machine
    for further processing.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 2 步中，来自训练模型的词向量被保存到本地计算机，以便进一步处理。
- en: In step 3, we extracted data from all the unique word vectors by using `WordVectorSerializer`.
    Basically, this will load an in-memory VocabCache from the mentioned input words.
    But it doesn't load whole vocab/lookup tables into the memory, so it is capable
    of processing large vocabularies served over the network.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 步中，我们使用 `WordVectorSerializer` 从所有独特的词向量中提取数据。基本上，这将从提到的输入词汇中加载一个内存中的 VocabCache。但它不会将整个词汇表/查找表加载到内存中，因此能够处理通过网络传输的大型词汇表。
- en: A `VocabCache` manages the storage of information required for the Word2Vec
    lookup table. We need to pass the labels to the t-SNE model, and labels are nothing
    but the words represented by word vectors.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`VocabCache` 管理存储 Word2Vec 查找表所需的信息。我们需要将标签传递给 t-SNE 模型，而标签就是通过词向量表示的词。'
- en: In step 4, we created a list to add all unique words.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 4 步中，我们创建了一个列表来添加所有独特的词。
- en: The `BarnesHutTsne` phrase is the DL4J implementation class for the dual-tree
    t-SNE model. The Barnes–Hut algorithm takes a dual-tree approximation strategy.
    It is recommended that you reduce the dimension by up to 50 using another method,
    such as **principal component analysis** (**PCA**) or similar.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`BarnesHutTsne` 短语是 DL4J 实现类，用于双树 t-SNE 模型。Barnes–Hut 算法采用双树近似策略。建议使用其他方法，如
    **主成分分析** (**PCA**) 或类似方法，将维度降到最多 50。'
- en: 'In step 5, we used `BarnesHutTsne` to design a t-SNE model for the purpose.
    This model contained the following components:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 5 步中，我们使用 `BarnesHutTsne` 设计了一个 t-SNE 模型。这个模型包含以下组件：
- en: '`theta()`: This is the Barnes–Hut trade-off parameter.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`theta()`：这是 Barnes–Hut 平衡参数。'
- en: '`useAdaGrad()`: This is the legacy AdaGrad implementation for use in NLP applications.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`useAdaGrad()`：这是在自然语言处理应用中使用的传统 AdaGrad 实现。'
- en: Once the t-SNE model is designed, we can fit it with weights loaded from words. We
    can then save the feature plots to an Excel file, as demonstrated in step 6.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 t-SNE 模型设计完成，我们可以使用从词汇中加载的权重来拟合它。然后，我们可以将特征图保存到 Excel 文件中，如第 6 步所示。
- en: 'The feature coordinates will look like the following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 特征坐标将如下所示：
- en: '![](img/f566cd82-b2d0-4d28-abf4-e46bf33e8092.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f566cd82-b2d0-4d28-abf4-e46bf33e8092.png)'
- en: We can plot these coordinates using gnuplot or any other third-party libraries.
    DL4J also supports JFrame-based visualizations.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 gnuplot 或任何其他第三方库来绘制这些坐标。DL4J 还支持基于 JFrame 的可视化。
- en: Saving and reloading the model
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存并重新加载模型
- en: Model persistence is a key topic, especially while operating with different
    platforms. We can also reuse the model for further training (transfer learning)
    or performing tasks.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 模型持久化是一个关键话题，尤其是在与不同平台操作时。我们还可以重用该模型进行进一步的训练（迁移学习）或执行任务。
- en: In this recipe, we will persist (save and reload) the Word2Vec models.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将持久化（保存和重新加载）Word2Vec 模型。
- en: How to do it...
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Save the Word2Vec model using `WordVectorSerializer`:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `WordVectorSerializer` 保存 Word2Vec 模型：
- en: '[PRE23]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Reload the Word2Vec model using `WordVectorSerializer`:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `WordVectorSerializer` 重新加载 Word2Vec 模型：
- en: '[PRE24]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: How it works...
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: In step 1, the `writeWord2VecModel()` method saves the Word2Vec model into a
    compressed ZIP file and sends it to the output stream. It saves the full model,
    including `Syn0` and `Syn1`. The `Syn0` is the array that holds raw word vectors
    and is a projection layer that can convert one-hot encoding of a word into a dense
    embedding vector of the right dimension. The `Syn1` array represents the model's
    internal hidden weights to process the input/output.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 1 步中，`writeWord2VecModel()` 方法将 Word2Vec 模型保存为压缩的 ZIP 文件，并将其发送到输出流。它保存了完整的模型，包括
    `Syn0` 和 `Syn1`。`Syn0` 是存储原始词向量的数组，并且是一个投影层，可以将词的独热编码转换为正确维度的密集嵌入向量。`Syn1` 数组代表模型的内部隐含权重，用于处理输入/输出。
- en: 'In step 2, the `readWord2VecModel()`method loads the models that are in the
    following format:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 2 步中，`readWord2VecModel()` 方法加载以下格式的模型：
- en: Binary model, either compressed or not compressed
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二进制模型，可以是压缩的或未压缩的
- en: Popular CSV/Word2Vec text format
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行的 CSV/Word2Vec 文本格式
- en: DL4J compressed format
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL4J 压缩格式
- en: Note that only weights will be loaded by this method.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，只有权重会通过此方法加载。
- en: Importing Google News vectors
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入 Google 新闻向量
- en: Google provides a large, pretrained Word2Vec model with around 3 million 300-dimension
    English word vectors. It is large enough, and pretrained to display promising
    results. We will use Google vectors as our input word vectors for the evaluation.
    You will need at least 8 GB of RAM to run this example. In this recipe, we will
    import the Google News vectors and then perform an evaluation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Google 提供了一个预训练的大型 Word2Vec 模型，包含大约 300 万个 300 维的英文单词向量。它足够大，并且预训练后能够展示出良好的结果。我们将使用
    Google 向量作为输入单词向量进行评估。运行此示例至少需要 8 GB 的 RAM。在本教程中，我们将导入 Google News 向量并进行评估。
- en: How to do it...
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Import the Google News vectors:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 Google News 向量：
- en: '[PRE25]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Run an evaluation on the Google News vectors:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 Google News 向量进行评估：
- en: '[PRE26]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: How it works...
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In step 1, the `readWord2VecModel()` method is used to load the pretrained Google
    News vector that was saved in compressed file format.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 1 中，使用 `readWord2VecModel()` 方法加载保存为压缩文件格式的预训练 Google News 向量。
- en: In step 2, the `wordsNearest()` method is used to find the nearest words to
    the given word based on positive/negative scores.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 2 中，使用 `wordsNearest()` 方法根据正负分数查找与给定单词最相近的单词。
- en: 'After performing step 2, we should see the following results:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 执行完步骤 2 后，我们应该看到以下结果：
- en: '![](img/8e2f1a8c-a69b-4cec-82f7-cc6513924b93.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e2f1a8c-a69b-4cec-82f7-cc6513924b93.png)'
- en: You can try this technique using your own inputs to see different results.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试使用自己的输入来测试此技术，看看不同的结果。
- en: There's more...
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The Google News vector''s compressed model file is sized at 1.6 GB. It can
    take a while to load and evaluate the model. You might observe an `OutOfMemoryError` error
    if you''re running the code for the first time:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Google News 向量的压缩模型文件大小为 1.6 GB。加载和评估该模型可能需要一些时间。如果你第一次运行代码，可能会观察到 `OutOfMemoryError`
    错误：
- en: '![](img/93bb0b0b-11ae-405f-9f12-b31a8ae5f1f8.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93bb0b0b-11ae-405f-9f12-b31a8ae5f1f8.png)'
- en: 'We now need to adjust the VM options to accommodate more memory for the application.
    You can adjust the VM options in IntelliJ IDE, as shown in the following screenshot.
    You just need to make sure that you assign enough memory value and restart the
    application:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要调整虚拟机（VM）选项，以便为应用程序提供更多内存。你可以在 IntelliJ IDE 中调整 VM 选项，如下图所示。你只需要确保分配了足够的内存值，并重新启动应用程序：
- en: '![](img/ff3fbe8d-7414-42ef-a681-9eb114dc1ba4.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff3fbe8d-7414-42ef-a681-9eb114dc1ba4.png)'
- en: Troubleshooting and tuning Word2Vec models
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 故障排除和调优 Word2Vec 模型
- en: Word2Vec models can be tuned further to produce better results. Runtime errors
    can happen in situations where there is high memory demand and less resource availability.
    We need to troubleshoot them to understand why they are happening and take preventative
    measures. In this recipe, we will troubleshoot Word2Vec models and tune them.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 模型可以进一步调整，以产生更好的结果。在内存需求较高而资源不足的情况下，可能会发生运行时错误。我们需要对它们进行故障排除，理解发生的原因并采取预防措施。在本教程中，我们将对
    Word2Vec 模型进行故障排除和调优。
- en: How to do it...
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Monitor `OutOfMemoryError` in the application console/logs to check whether
    the heap space needs to be increased.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在应用程序控制台/日志中监控 `OutOfMemoryError`，检查是否需要增加堆空间。
- en: Check your IDE console for out-of-memory errors. If there are out-of-memory
    errors, then add VM options to your IDE to increase the Java memory heap.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查 IDE 控制台中的内存溢出错误。如果出现内存溢出错误，请向 IDE 中添加 VM 选项，以增加 Java 堆内存。
- en: 'Monitor `StackOverflowError` while running Word2Vec models. Watch out for the
    following error:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行 Word2Vec 模型时，监控 `StackOverflowError`。注意以下错误：
- en: '![](img/5b2382d7-5db2-4dae-98d2-795986783447.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b2382d7-5db2-4dae-98d2-795986783447.png)'
- en: This error can happen because of unwanted temporary files in a project.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这个错误可能是由于项目中不必要的临时文件造成的。
- en: Perform hyperparameter tuning for Word2Vec models. You might need to perform
    multiple training sessions with different values for the hyperparameters, such
    as `layeSize`, `windowSize`, and so on.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 Word2Vec 模型进行超参数调优。你可能需要多次训练，使用不同的超参数值，如 `layerSize`、`windowSize` 等。
- en: Derive the memory consumption at the code level. Calculate the memory consumption
    based on the data types used in the code and how much data is being consumed by
    them.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在代码层面推导内存消耗。根据代码中使用的数据类型及其消耗的数据量来计算内存消耗。
- en: How it works...
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Out-of-memory errorsare an indication that VM options need to be adjusted. How
    you adjust these parameters will depend on the RAM capacity of the hardware. For
    step 1, if you're using an IDE such as IntelliJ, you can provide the VM options
    using VM attributes such as `-Xmx`, `-Xms`, and so on. VM options can also be
    used from the command line.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 内存溢出错误通常表明需要调整 VM 选项。如何调整这些参数取决于硬件的内存容量。对于步骤 1，如果你使用的是像 IntelliJ 这样的 IDE，你可以通过
    VM 属性如`-Xmx`、`-Xms`等提供 VM 选项。VM 选项也可以通过命令行使用。
- en: For example, to increase the maximum memory consumption to 8 GB, you will need
    to add the `-Xmx8G ``VM` argument to your IDE.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要将最大内存消耗增加到 8 GB，你需要在 IDE 中添加`-Xmx8G` `VM`参数。
- en: 'To mitigate `StackOverflowError` mentioned in step 2, we need to delete the
    temporary files created under the project directory where our Java program is
    executed. These temporary files should look like the following:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解步骤 2 中提到的`StackOverflowError`，我们需要删除在项目目录下由 Java 程序执行时创建的临时文件。这些临时文件应类似于以下内容：
- en: '![](img/d0394d7e-63de-493b-bd02-525542a51f6e.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0394d7e-63de-493b-bd02-525542a51f6e.png)'
- en: 'With regard to step 3, if you observe that your Word2Vec model doesn''t hold
    all the words from the raw text data, then you might be interested in increasing
    the layer size of the Word2Vec model. This `layerSize` is nothing but the output
    vector dimension or the feature space dimension. For example, we had `layerSize` of `100` in
    our code. This means that we can increase it to a larger value, say `200`, as
    a workaround:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 关于步骤 3，如果你观察到你的 Word2Vec 模型未能包含原始文本数据中的所有词汇，那么你可能会考虑增加 Word2Vec 模型的 层大小 。这个`layerSize`实际上就是输出向量维度或特征空间维度。例如，在我们的代码中，我们的`layerSize`为`100`。这意味着我们可以将其增大到更大的值，比如`200`，作为一种解决方法：
- en: '[PRE27]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: If you have a GPU-powered machine, you can use this to accelerate the Word2Vec training
    time. Just make sure that the dependencies for the DL4J and ND4J backend are added
    as usual. If the results still don't look right, then make sure there are no normalization
    issues.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一台 GPU 加速的机器，你可以用它来加速 Word2Vec 的训练时间。只要确保 DL4J 和 ND4J 后端的依赖项按常规添加即可。如果结果看起来仍然不对，确保没有归一化问题。
- en: Tasks such as `wordsNearest()` use normalized weights by default, and others
    require weights without normalization applied.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如`wordsNearest()`等任务默认使用归一化权重，而其他任务则需要未归一化的权重。
- en: 'With regard to step 4, we can use the conventional approach. The weights matrix has
    the most memory consumption in Word2Vec. It is calculated as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 关于步骤 4，我们可以使用传统方法。权重矩阵是 Word2Vec 中内存消耗最大的部分。其计算方法如下：
- en: '*NumberOfWords * NumberOfDimensions * 2 * DataType memory footprint*'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '*词汇数 * 维度数 * 2 * 数据类型内存占用*'
- en: For example, if our Word2Vec model with 100,000 words uses `long` as the data
    type, and 100 dimensions, the memory footprint will be 100,000 * 100 * 2 * 8 (long
    data type size) = 160 MB RAM, just for the weights matrix.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们的 Word2Vec 模型包含 100,000 个词汇，且使用 `long` 数据类型，100 个维度，则权重矩阵的内存占用为 100,000
    * 100 * 2 * 8（long 数据类型大小）= 160 MB RAM，仅用于权重矩阵。
- en: Note that DL4J UI will only provide a high-level overview of memory consumption.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，DL4J UI 仅提供内存消耗的高层概览。
- en: See also
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: Refer to the official DL4J documentation at [https://deeplearning4j.org/docs/latest/deeplearning4j-config-memory ](https://deeplearning4j.org/docs/latest/deeplearning4j-config-memory)to
    learn more about memory management
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请参考官方的 DL4J 文档，[https://deeplearning4j.org/docs/latest/deeplearning4j-config-memory](https://deeplearning4j.org/docs/latest/deeplearning4j-config-memory)以了解更多关于内存管理的信息。
- en: Using Word2Vec for sentence classification using CNNs
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Word2Vec 进行 CNN 的句子分类
- en: Neural networks require numerical inputs to perform their operations as expected.
    For text inputs, we cannot directly feed text data into a neural network. Since Word2Vec converts
    text data to vectors, it is possible to exploit Word2Vec so that we can use it with
    neural networks. We will use a pretrained Google News vector model as a reference
    and train a CNN network on top of it. At the end of this process, we will develop
    an IMDB review classifier to classify reviews as positive or negative. As per
    the paper found at [https://arxiv.org/abs/1408.5882](https://arxiv.org/abs/1408.5882),
    combining a pretrained Word2Vec model with a CNN will give us better results.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络需要数值输入才能按预期执行操作。对于文本输入，我们不能直接将文本数据输入神经网络。由于`Word2Vec`将文本数据转换为向量，因此可以利用Word2Vec，使得我们可以将其与神经网络结合使用。我们将使用预训练的Google
    News词向量模型作为参考，并在其基础上训练CNN网络。完成此过程后，我们将开发一个IMDB评论分类器，将评论分类为正面或负面。根据论文[https://arxiv.org/abs/1408.5882](https://arxiv.org/abs/1408.5882)中的内容，结合预训练的Word2Vec模型和CNN会带来更好的结果。
- en: We will employ custom CNN architecture along with the pretrained word vector
    model as suggested by Yoon Kim in his 2014 publication, [https://arxiv.org/abs/1408.5882](https://arxiv.org/abs/1408.5882).
    The architecture is slightly more advanced than standard CNN models. We will also
    be using two huge datasets, and so the application might require a fair amount
    of RAM and performance benchmarks to ensure a reliable training duration and no
    `OutOfMemory` errors.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用定制的CNN架构，并结合2014年Yoon Kim在其论文中建议的预训练词向量模型，[https://arxiv.org/abs/1408.5882](https://arxiv.org/abs/1408.5882)。该架构稍微比标准CNN模型更为复杂。我们还将使用两个巨大的数据集，因此应用程序可能需要相当大的RAM和性能基准，以确保可靠的训练时间并避免`OutOfMemory`错误。
- en: In this recipe, we will perform sentence classification using both Word2Vec
    and a CNN.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用Word2Vec和CNN进行句子分类。
- en: Getting ready
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: Use the example found at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CnnWord2VecSentenceClassificationExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CnnWord2VecSentenceClassificationExample.java) for
    reference.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 参考示例：[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CnnWord2VecSentenceClassificationExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CnnWord2VecSentenceClassificationExample.java)。
- en: You should also make sure that you add more Java heap space through changing
    the VM options—for example, if you have 8 GB of RAM, then you may set `-Xmx2G
    -Xmx6G` as VM arguments.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 你还应该确保通过更改VM选项来增加更多的Java堆空间——例如，如果你的RAM为8GB，可以设置`-Xmx2G -Xmx6G`作为VM参数。
- en: 'We will extract the IMDB data to start with in step 1\. The file structure
    will look like the following:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第一步中提取IMDB数据。文件结构将如下所示：
- en: '![](img/e06d569b-8da9-447e-995c-5018afb8c890.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e06d569b-8da9-447e-995c-5018afb8c890.png)'
- en: 'If we further navigate to the dataset directories, you will see them labeled
    as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们进一步进入数据集目录，你会看到它们被标记为以下内容：
- en: '![](img/6169b9f6-fc75-4f34-9af6-9a8641177fde.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6169b9f6-fc75-4f34-9af6-9a8641177fde.png)'
- en: How to do it...
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何执行...
- en: 'Load the word vector model using `WordVectorSerializer`:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`WordVectorSerializer`加载词向量模型：
- en: '[PRE28]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Create a sentence provider using `FileLabeledSentenceProvider`:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`FileLabeledSentenceProvider`创建句子提供器：
- en: '[PRE29]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Create train iterators or test iterators using `CnnSentenceDataSetIterator`
    to load the IMDB review data:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`CnnSentenceDataSetIterator`创建训练迭代器或测试迭代器，以加载IMDB评论数据：
- en: '[PRE30]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Create a `ComputationGraph` configuration by adding default hyperparameters:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过添加默认的超参数来创建`ComputationGraph`配置：
- en: '[PRE31]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Configure layers for `ComputationGraph` using the `addLayer()` method:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`addLayer()`方法配置`ComputationGraph`的层：
- en: '[PRE32]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Set the convolution mode to stack the results later:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将卷积模式设置为稍后堆叠结果：
- en: '[PRE33]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Create a `ComputationGraph` model and initialize it:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建并初始化`ComputationGraph`模型：
- en: '[PRE34]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Perform the training using the `fit()` method:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`fit()`方法进行训练：
- en: '[PRE35]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Evaluate the results:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估结果：
- en: '[PRE36]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Retrieve predictions for the IMDB reviews data:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取IMDB评论数据的预测：
- en: '[PRE37]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: How it works...
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In step 1, we used `loadStaticModel()` to load the model from the given path;
    however, you can also use `readWord2VecModel()`. Unlike `readWord2VecModel()`,
    `loadStaticModel()` utilizes host memory.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步，我们使用 `loadStaticModel()` 从给定路径加载模型；但是，你也可以使用 `readWord2VecModel()`。与 `readWord2VecModel()`
    不同，`loadStaticModel()` 使用主机内存。
- en: In step 2, `FileLabeledSentenceProvider` is used as a data source to load the
    sentences/documents from the files. We created `CnnSentenceDataSetIterator` using
    the same. `CnnSentenceDataSetIterator` handles the conversion of sentences to
    training data for CNNs, where each word is encoded using the word vector from
    the specified word vector model. Sentences and labels are provided by a `LabeledSentenceProvider`
    interface. Different implementations of `LabeledSentenceProvider` provide different
    ways of loading the sentence/documents with labels.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2步，`FileLabeledSentenceProvider` 被用作数据源从文件中加载句子/文档。我们使用相同的方式创建了 `CnnSentenceDataSetIterator`。`CnnSentenceDataSetIterator`
    处理将句子转换为CNN的训练数据，其中每个单词使用指定的词向量模型进行编码。句子和标签由 `LabeledSentenceProvider` 接口提供。`LabeledSentenceProvider`
    的不同实现提供了不同的加载句子/文档及标签的方式。
- en: 'In step 3, we created `CnnSentenceDataSetIterator` to create train/test dataset
    iterators. The parameters we configured here are as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3步，我们创建了 `CnnSentenceDataSetIterator` 来创建训练/测试数据集迭代器。我们在这里配置的参数如下：
- en: '`sentenceProvider()`: Adds a sentence provider (data source) to `CnnSentenceDataSetIterator`'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sentenceProvider()`：将句子提供者（数据源）添加到 `CnnSentenceDataSetIterator`。'
- en: '`wordVectors()`: Adds a word vector reference to the dataset iterator—for example,
    the Google News vectors'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wordVectors()`：将词向量引用添加到数据集迭代器中——例如，Google News 词向量。'
- en: '`useNormalizedWordVectors()`: Sets whether normalized word vectors can be used'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`useNormalizedWordVectors()`：设置是否可以使用标准化的词向量。'
- en: In step 5, we created layers for a `ComputationGraph` model.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5步，我们为 `ComputationGraph` 模型创建了层。
- en: The `ComputationGraph` configuration is a configuration object for neural networks
    with an arbitrary connection structure. It is analogous to multilayer configuration,
    but allows considerably greater flexibility for the network architecture.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`ComputationGraph` 配置是一个用于神经网络的配置对象，具有任意连接结构。它类似于多层配置，但允许网络架构具有更大的灵活性。'
- en: We also created multiple convolution layers stacked together with multiple filter
    widths and feature maps.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还创建了多个卷积层，并将它们按不同的滤波宽度和特征图堆叠在一起。
- en: In step 6, `MergeVertex` performs in-depth concatenation on activation of these
    three convolution layers.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6步，`MergeVertex` 在激活这三层卷积层时执行深度连接。
- en: 'Once all steps up to step 8 are completed, we should see the following evaluation
    metrics:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦第8步之前的所有步骤完成，我们应该会看到以下评估指标：
- en: '![](img/e363c5d4-89d7-4980-97d2-e1b856408dd5.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e363c5d4-89d7-4980-97d2-e1b856408dd5.png)'
- en: In step 10, `contents` refers to the content from a single-sentence document
    in string format.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在第10步，`contents` 指的是来自单句文档的字符串格式内容。
- en: 'For negative review content, we would see the following result after step 9:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 对于负面评论内容，在第9步后我们会看到以下结果：
- en: '![](img/424f42b6-b2c7-42a8-8dd9-91e56cf9a770.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/424f42b6-b2c7-42a8-8dd9-91e56cf9a770.png)'
- en: This means that the document has a 77.8% probability of having a negative sentiment.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着该文档有77.8%的概率呈现负面情绪。
- en: There's more...
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: Initializing word vectors with those retrieved from pretrained unsupervised
    models is a known method for increasing performance. If you can recall what we
    have done in this recipe, you will remember that we used pretrained Google News
    vectors for the same purpose. For a CNN, when applied to text instead of images,
    we will be dealing with one-dimensional array vectors that represent the text.
    We perform the same steps, such as convolution and max pooling with feature maps,
    as discussed in [Chapter 4](4a688ef9-2dd8-47de-abaf-456fa88bcfc2.xhtml), *Building
    Convolutional Neural Networks*. The only difference is that instead of image pixels,
    we use vectors that represent text. CNN architectures have subsequently shown
    great results against NLP tasks. The paper found at [https://www.aclweb.org/anthology/D14-1181](https://www.aclweb.org/anthology/D14-1181) will
    contain further insights on this.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 使用从预训练无监督模型中提取的词向量进行初始化是提升性能的常见方法。如果你记得我们在这个示例中做过的事情，你会记得我们曾为同样的目的使用了预训练的 Google
    News 向量。对于 CNN，当其应用于文本而非图像时，我们将处理一维数组向量来表示文本。我们执行相同的步骤，如卷积和最大池化与特征图，正如在[第4章](4a688ef9-2dd8-47de-abaf-456fa88bcfc2.xhtml)《构建卷积神经网络》中讨论的那样。唯一的区别是，我们使用的是表示文本的向量，而不是图像像素。随后，CNN
    架构在 NLP 任务中展现了出色的结果。可以在[https://www.aclweb.org/anthology/D14-1181](https://www.aclweb.org/anthology/D14-1181)中找到有关此主题的进一步见解。
- en: The network architecture of a computation graph is a directed acyclic graph,
    where each vertex in the graph is a graph vertex. A graph vertex can be a layer
    or a vertex that defines a random forward/backward pass functionality. Computation
    graphs can have a random number of inputs and outputs. We needed to stack multiple
    convolution layers, which was not possible in the case of a normal CNN architecture.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图的网络架构是一个有向无环图，其中图中的每个顶点都是一个图顶点。图顶点可以是一个层，或者是定义随机前向/反向传递功能的顶点。计算图可以有任意数量的输入和输出。我们需要叠加多个卷积层，但在普通的
    CNN 架构中这是不可能的。
- en: '`ComputaionGraph` has an option to set the configuration known as `convolutionMode`.
    `convolutionMode` determines the network configuration and how the convolution
    operations should be performed for convolutional and subsampling layers (for a
    given input size). Network configurations such as `stride`/`padding`/`kernelSize`
    are applicable for a given convolution mode. We are setting the convolution mode
    using `convolutionMode` because we want to stack the results of all three convolution
    layers as one and generate the prediction.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`ComputaionGraph` 提供了一个配置选项，称为 `convolutionMode`。`convolutionMode` 决定了网络配置以及卷积和下采样层的卷积操作如何执行（对于给定的输入大小）。网络配置如
    `stride`/`padding`/`kernelSize` 适用于特定的卷积模式。我们通过设置 `convolutionMode` 来配置卷积模式，因为我们希望将三个卷积层的结果叠加为一个并生成预测。'
- en: 'The output sizes for convolutional and subsampling layers are calculated in
    each dimension as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层和下采样层的输出尺寸在每个维度上计算方式如下：
- en: '*outputSize = (inputSize - kernelSize + 2*padding) / stride + 1*'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '*outputSize = (inputSize - kernelSize + 2*padding) / stride + 1*'
- en: 'If `outputSize` is not an integer, an exception will be thrown during the network
    initialization or forward pass. We have discussed `MergeVertex`, which was used
    to combine the activations of two or more layers. We used `MergeVertex` to perform
    the same operation with our convolution layers. The merge will depend on the type
    of inputs—for example, if we wanted to merge two convolution layers with a sample
    size (`batchSize`) of `100`, and `depth` of `depth1` and `depth2` respectively,
    then `merge` will stack the results where the following applies:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `outputSize` 不是整数，则在网络初始化或前向传递过程中将抛出异常。我们之前讨论过 `MergeVertex`，它用于组合两个或更多层的激活值。我们使用
    `MergeVertex` 来执行与卷积层相同的操作。合并将取决于输入类型——例如，如果我们想要合并两个卷积层，样本大小（`batchSize`）为 `100`，并且
    `depth` 分别为 `depth1` 和 `depth2`，则 `merge` 将按以下规则叠加结果：
- en: '*depth = depth1 + depth2*'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '*depth = depth1 + depth2*'
- en: Using Doc2Vec for document classification
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Doc2Vec 进行文档分类
- en: Word2Vec correlates words with words, while the purpose of Doc2Vec (also known
    as paragraph vectors) is to correlate labels with words. We will discuss Doc2Vec in
    this recipe. Documents are labeled in such a way that the subdirectories under
    the document's root represent document labels. For example, all finance-related
    data should be placed under the `finance` subdirectory. In this recipe, we will
    perform document classification using Doc2Vec.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec将词汇与词汇相关联，而Doc2Vec（也称为段落向量）的目的是将标签与词汇相关联。在本食谱中，我们将讨论Doc2Vec。文档按特定方式标记，使文档根目录下的子目录表示文档标签。例如，所有与金融相关的数据应放在`finance`子目录下。在这个食谱中，我们将使用Doc2Vec进行文档分类。
- en: How to do it...
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Extract and load the data using `FileLabelAwareIterator`:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`FileLabelAwareIterator`提取并加载数据：
- en: '[PRE38]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Create a tokenizer using `TokenizerFactory`:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`TokenizerFactory`创建一个分词器：
- en: '[PRE39]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Create a `ParagraphVector` model definition:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`ParagraphVector`模型定义：
- en: '[PRE40]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Train `ParagraphVectors` by calling the `fit()` method:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`fit()`方法训练`ParagraphVectors`：
- en: '[PRE41]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Assign labels to unlabeled data and evaluate the results:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为未标记的数据分配标签并评估结果：
- en: '[PRE42]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Store the weight lookup table:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存储权重查找表：
- en: '[PRE43]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Predict labels for every unclassified document, as shown in the following pseudocode:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下伪代码所示，预测每个未分类文档的标签：
- en: '[PRE44]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Create the tokens from the document and use the iterator to retrieve the document
    instance:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从文档中创建标记，并使用迭代器来检索文档实例：
- en: '[PRE45]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Use the lookup table to get the vocabulary information (`VocabCache`):'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用查找表来获取词汇信息（`VocabCache`）：
- en: '[PRE46]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Count all the instances where the words are matched in `VocabCache`:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算`VocabCache`中匹配的所有实例：
- en: '[PRE47]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Store word vectors of the matching words in the vocab:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将匹配词的词向量存储在词汇中：
- en: '[PRE48]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Calculate the domain vector by calculating the mean of the word embeddings:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过计算词汇嵌入的平均值来计算领域向量：
- en: '[PRE49]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Check the cosine similarity of the document vector with labeled word vectors:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查文档向量与标记词向量的余弦相似度：
- en: '[PRE50]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Display the results:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示结果：
- en: '[PRE51]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: How it works...
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In step 1, we created a dataset iterator using `FileLabelAwareIterator`.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中，我们使用`FileLabelAwareIterator`创建了数据集迭代器。
- en: 'The `FileLabelAwareIterator` is a simple filesystem-based `LabelAwareIterator`
    interface. It assumes that you have one or more folders organized in the following
    way:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '`FileLabelAwareIterator`是一个简单的基于文件系统的`LabelAwareIterator`接口。它假设你有一个或多个按以下方式组织的文件夹：'
- en: '**First-level subfolder**: Label name'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一级子文件夹**：标签名称'
- en: '**Second-level subfolder**: The documents for that label'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二级子文件夹**：该标签对应的文档'
- en: 'Look at the following screenshot for an example of this data structure:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下截图，了解此数据结构的示例：
- en: '![](img/d4d9286f-50f4-431b-b705-d6beb0cb8d00.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d4d9286f-50f4-431b-b705-d6beb0cb8d00.png)'
- en: In step 3, we created **`ParagraphVector`** by adding all required hyperparameters. The
    purpose of paragraph vectors is to associate arbitrary documents with labels. Paragraph
    vectors are an extension to Word2Vec that learn to correlate labels and words,
    while Word2Vec correlates words with other words. We need to define labels for
    the paragraph vectors to work.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3步中，我们通过添加所有必需的超参数创建了**`ParagraphVector`**。段落向量的目的是将任意文档与标签关联。段落向量是Word2Vec的扩展，学习将标签和词汇相关联，而Word2Vec将词汇与其他词汇相关联。我们需要为段落向量定义标签，才能使其正常工作。
- en: 'For more information on what we did in step 5, refer to the following directory
    structure (under the `unlabeled` directory in the project):'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 有关第5步中所做内容的更多信息，请参阅以下目录结构（项目中的`unlabeled`目录下）：
- en: '![](img/37bfbc65-3bb3-4cad-acba-462579d829b5.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37bfbc65-3bb3-4cad-acba-462579d829b5.png)'
- en: The directory names can be random and no specific labels are required. Our task
    is to find the proper labels (document classifications) for these documents. Word
    embeddings are stored in the lookup table. For any given word, a word vector of
    numbers will be returned.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 目录名称可以是随机的，不需要特定的标签。我们的任务是为这些文档找到适当的标签（文档分类）。词嵌入存储在查找表中。对于任何给定的词汇，查找表将返回一个词向量。
- en: Word embeddings are stored in the lookup table. For any given word, a word vector
    will be returned from the lookup table.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇嵌入存储在查找表中。对于任何给定的词汇，查找表将返回一个词向量。
- en: In step 6, we created `InMemoryLookupTable` from paragraph vectors. `InMemoryLookupTable`
    is the default word lookup table in DL4J. Basically, the lookup table operates
    as the hidden layer and the word/document vectors refer to the output.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6步中，我们通过段落向量创建了`InMemoryLookupTable`。`InMemoryLookupTable`是DL4J中的默认词汇查找表。基本上，查找表作为隐藏层操作，词汇/文档向量作为输出。
- en: Step 8 to step 12 are solely used for the calculation of the domain vector of
    each document.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 第8步到第12步仅用于计算每个文档的领域向量。
- en: In step 8, we created tokens for the document using the tokenizer that was created
    in step 2\. In step 9, we used the lookup table that was created in step 6 to
    obtain `VocabCache`. `VocabCache` stores the information needed to operate the
    lookup table. We can look up words in the lookup table using `VocabCache`.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在第8步中，我们使用第2步中创建的分词器为文档创建了令牌。在第9步中，我们使用第6步中创建的查找表来获取`VocabCache`。`VocabCache`存储了操作查找表所需的信息。我们可以使用`VocabCache`在查找表中查找单词。
- en: In step 11, we store the word vectors along with the occurrence of a particular
    word in an INDArray.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在第11步中，我们将单词向量与特定单词的出现次数一起存储在一个INDArray中。
- en: In step 12, we calculated the mean of this INDArray to get the document vector.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在第12步中，我们计算了这个INDArray的均值，以获取文档向量。
- en: The mean across the zero dimension means that it is calculated across all dimensions.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在零维度上的均值意味着它是跨所有维度计算的。
- en: In step 13, the cosine similarity is calculated by calling the `cosineSim()` method
    provided by ND4J. We use cosine similarity to calculate the similarity of document
    vectors. ND4J provides a functional interface to calculate the cosine similarity
    of two domain vectors. `vecLabel` represents the document vector for the labels
    from classified documents. Then, we compared `vecLabel` with our unlabeled document
    vector, `documentVector`.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在第13步中，余弦相似度是通过调用ND4J提供的`cosineSim()`方法计算的。我们使用余弦相似度来计算文档向量的相似性。ND4J提供了一个功能接口，用于计算两个领域向量的余弦相似度。`vecLabel`表示从分类文档中获取的标签的文档向量。然后，我们将`vecLabel`与我们的未标记文档向量`documentVector`进行比较。
- en: 'After step 14, you should see an output similar to the following:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在第14步之后，你应该看到类似于以下的输出：
- en: '![](img/adff8732-4749-4183-9cb0-006c7a32869e.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/adff8732-4749-4183-9cb0-006c7a32869e.png)'
- en: We can choose the label that has the higher cosine similarity value. From the
    preceding screenshots, we can infer that the first document is more likely finance-related
    content with a 69.7% probability. The second document is more likely health-related
    content with a 53.2% probability.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择具有更高余弦相似度值的标签。从前面的截图中，我们可以推断出第一篇文档更可能是与金融相关的内容，概率为69.7%。第二篇文档更可能是与健康相关的内容，概率为53.2%。
