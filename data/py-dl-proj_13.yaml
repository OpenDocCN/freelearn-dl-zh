- en: Image Translation Using GANs for Style Transfer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GAN 进行图像翻译以实现风格迁移
- en: Welcome to the chapter on **Generative Adversarial Networks** (**GANs**). In
    this chapter, we will be building a neural network that fills in the missing part
    of a handwritten digit. Previously, we have built a digit classifier for the restaurant
    chain. But they have also noticed that sometimes, when customers write in their
    phone number, small sections/regions of the digits are missing. This may be a
    combination of the customer not having a smooth flow when writing on the iPad application,
    as well as issues with the iPad application not processing the complete user gesture
    on the screen. This makes it hard for the handwritten digit classifier to predict
    the correct digit corresponding to the handwritten number. Now, they want us to
    reconstruct (generate back) the missing parts of the handwritten numbers so that
    the classifier receives clear handwritten numbers for conversion into digits. With
    this, the classifier will be able to do a much more accurate job of classifying
    handwritten digits and the notice gets sent to the right hungry customer!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到关于**生成对抗网络**（**GANs**）的章节。在本章中，我们将构建一个神经网络，填补手写数字缺失的部分。之前，我们为餐饮连锁店构建了一个数字分类器。但他们也注意到，有时当顾客写下电话号码时，数字的某些小部分缺失。这可能是由于顾客在
    iPad 应用程序上写字时没有流畅的书写动作，也可能是因为 iPad 应用程序没有正确处理用户在屏幕上的完整手势。这使得手写数字分类器很难预测出与手写数字对应的正确数字。现在，他们希望我们重建（生成回）手写数字缺失的部分，以便分类器能够接收到清晰的手写数字并转化为数字。这样，分类器将能够更准确地分类手写数字，通知也能发送到正确的饥饿顾客！
- en: 'We will mostly focus on the generation/reconstruction of the missing sections
    of a digit and we will do this with the help of neural inpainting with GANs; see
    the following flowchart:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将主要关注数字缺失部分的生成/重建，并且我们将借助 GAN 的神经修复来完成这一点；请参见以下流程图：
- en: '![](img/ad2ddbe3-8372-4f2f-a581-4fa73724f769.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad2ddbe3-8372-4f2f-a581-4fa73724f769.png)'
- en: 'Figure 13.1: GAN flowchart'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1：GAN 流程图
- en: 'What we''ll learn in this chapter is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将学习以下内容：
- en: What is a GAN
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 GAN
- en: What is a generator and a discriminator
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是生成器和判别器
- en: Coding the model and defining hyperparameters
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写模型并定义超参数
- en: Building and understanding the training loop
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建并理解训练循环
- en: Testing the model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试模型
- en: Extending the model to new datasets
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型扩展到新的数据集
- en: 'In this chapter, you will implement the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将实现以下内容：
- en: Build an MNIST digit classifier
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个 MNIST 数字分类器
- en: Simulate a dataset of handwritten digits with sections of the handwritten numbers
    missing
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模拟一个手写数字数据集，其中部分手写数字缺失
- en: Use the MNIST classifier to predict on noised/masked MNIST digits dataset (simulated
    dataset)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 MNIST 分类器预测带噪声/掩码的 MNIST 数字数据集（模拟数据集）
- en: Implement GAN to generate back the missing regions of the digit
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现 GAN 来恢复数字缺失的部分
- en: Use the MNIST classifier to predict on the generated digits from GAN
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 MNIST 分类器预测来自 GAN 生成的数字
- en: Compare performance between masked data and generated data
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较掩码数据和生成数据的性能
- en: It would be better if you implement the code snippets as you go along in this
    chapter, either in a Jupyter Notebook or any source code editor. This will make
    it easier for you to follow along, as well as understand what each part of the
    code does.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最好在本章中边学边实现代码片段，可以在 Jupyter Notebook 或任何源代码编辑器中进行。这将帮助你更容易地跟随课程，并理解每一部分代码的作用。
- en: All of the Python files and Jupyter Notebook files for this chapter can be found
    here: [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter13](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter13).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有 Python 文件和 Jupyter Notebook 文件可以在这里找到：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter13](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter13)。
- en: Let's code the implementation!
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们开始实现代码吧！
- en: In this exercise, we will be using the Keras deep learning library, which is
    a high-level neural network API, capable of running on top of Tensorflow, Theano,
    or Cognitive Toolkit (CNTK).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用 Keras 深度学习库，它是一个高级神经网络 API，可以在 Tensorflow、Theano 或 Cognitive Toolkit
    (CNTK) 上运行。
- en: Know the code! We will not spend time on understanding how Keras works but,
    if you are interested, refer to this easy-to-understand Keras official documentation
    at [https://keras.io/](https://keras.io/).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 了解代码！我们不会花时间去理解 Keras 的工作原理，但如果你感兴趣，可以参考这个通俗易懂的 Keras 官方文档：[https://keras.io/](https://keras.io/)。
- en: Importing all of the dependencies
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入所有依赖包
- en: 'We will be using `numpy`, `matplotlib`, `keras`, `tensorflow`, and the `tqdm`
    package in this exercise. Here, TensorFlow is used as the backend for Keras. You
    can install these packages with `pip`. For the MNIST data, we will be using the dataset
    available in the `keras` module with a simple import:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用 `numpy`、`matplotlib`、`keras`、`tensorflow` 和 `tqdm` 包。这里，TensorFlow
    被用作 Keras 的后端。你可以使用 `pip` 安装这些包。对于 MNIST 数据，我们将使用 `keras` 模块中的数据集，只需简单导入即可：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It is important that you set `seed` for reproducibility:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 `seed` 以确保可重现性是很重要的：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Exploring the data
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据
- en: 'We will load the MNIST data into our session from the `keras` module with `mnist.load_data()`.
    After doing so, we will print the shape and the size of the dataset, as well as
    the number of classes and unique labels in the dataset:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 `keras` 模块中使用 `mnist.load_data()` 加载 MNIST 数据到会话中。完成后，我们将打印数据集的形状和大小，以及数据集中类别的数量和唯一标签：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We have a dataset with 10 different classes and 60,000 images, with each image
    having a shape of 28*28 and each class having 6,000 images.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个包含 10 个不同类别、60,000 张图像的数据集，每张图像的形状是 28*28，每个类别有 6,000 张图像。
- en: 'Let''s plot and see what the handwritten images look like:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制并看看这些手写图像是什么样子的：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output is as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/c7df75a3-b0ef-4719-a96d-765558f6fb40.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7df75a3-b0ef-4719-a96d-765558f6fb40.png)'
- en: 'Figure 13.2: Plot of nine MNIST digits from the training set'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2：来自训练集的九个 MNIST 数字图
- en: 'Let''s plot a handwritten digit from each class:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制来自每个类别的一个手写数字：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output is as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/a4f2888b-eb6b-4838-992f-2c0a702f5534.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4f2888b-eb6b-4838-992f-2c0a702f5534.png)'
- en: 'Figure 13.3: Plot of an MNIST digit from each class'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3：来自每个类别的一个 MNIST 数字图
- en: 'Look at the maximum and the minimum pixel value in the dataset:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 查看数据集中最大和最小的像素值：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/e7b3ce82-d1f9-42a7-a8b7-7753ab9b3e8e.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7b3ce82-d1f9-42a7-a8b7-7753ab9b3e8e.png)'
- en: 'Figure 13.5: Plot of nine noised/masked MNIST digits'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5：九个带噪声/掩蔽的 MNIST 数字图
- en: We see that the maximum pixel value in the dataset is 255 and the minimum is
    0.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到数据集中的最大像素值为 255，最小值为 0。
- en: Preparing the data
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: Type conversion, centering, scaling, and reshaping are some of the pre-processing
    we will implement in this chapter.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 类型转换、居中、缩放和重塑是我们将在本章中实现的一些预处理操作。
- en: Type conversion, centering, and scaling
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类型转换、居中和缩放
- en: Set the type to `np.float32`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 将类型设置为 `np.float32`。
- en: '**Important**: One of the main reasons for doing this is that the weights will
    all be of the `float` type, and multiplication between floating numbers is much
    faster than between an integer and a float. So it''s better to convert the input
    into the `float` type.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**重要**：这样做的主要原因之一是权重将全部为 `float` 类型，而浮动数值间的乘法运算比整数和浮动数值间的乘法运算要快得多。因此，将输入转换为
    `float` 类型是更好的选择。'
- en: For centering, we subtract the dataset by 127.5\. The values in the dataset
    will now range between -127.5 to 127.5.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于居中，我们通过 127.5 从数据集中减去。数据集中的值将现在介于 -127.5 到 127.5 之间。
- en: 'For scaling, we divide the centered dataset by half of the maximum pixel value
    in the dataset, that is, *255/2*. This will result in a dataset with values ranging
    between -1 and 1:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于缩放，我们通过数据集中最大像素值的一半，即 *255/2*，来除以居中的数据集。这将导致一个值范围介于 -1 和 1 之间的数据集：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s define a function to rescale the pixel values of the scaled image to
    range between 0 and 255:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个函数，将缩放后的图像的像素值重新缩放到 0 到 255 之间：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Matplotlib tip**: Rescaling needs to be done so that you avoid errors with Matplotlib
    if you were to use the scaled image as is without upscaling.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**Matplotlib 提示**：需要进行缩放，以避免在使用未经放大的缩放图像时遇到 Matplotlib 错误。'
- en: 'A plot of `9` centered and scaled images after upscaling:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 放大后的 9 张居中和缩放后的图像：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/aa5cf429-bf29-4d84-87c3-ad1ecc57ebf8.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa5cf429-bf29-4d84-87c3-ad1ecc57ebf8.png)'
- en: 'Figure 13.4: Plot of nine centered and scaled MNIST digits after upscaling'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4：放大后九个居中和缩放的 MNIST 数字图
- en: Masking/inserting noise
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 掩蔽/插入噪声
- en: For the needs of this project, we need to simulate a dataset of incomplete digits.
    So, let's write a function to mask small regions in the original image to form
    the noised dataset.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 根据本项目的需求，我们需要模拟一个不完整数字的数据集。因此，让我们编写一个函数，遮挡原始图像中的小区域，形成噪声数据集。
- en: 'The idea is to mask an 8*8 region of the image with the top-left corner of
    the mask falling between the 9^(th) and 13^(th) pixel (between index 8 and 12)
    along both the *x* and *y* axis of the image. This is to make sure that we are
    always masking around the center part of the image:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是将图像的一个 8*8 区域进行遮挡，遮挡区域的左上角落在图像的第 9 到第 13 个像素之间（即在 x 和 y 轴上索引 8 到 12 之间）。目的是确保我们总是遮住图像的中心部分：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The bigger the size of the mask, the harder it will be for the MNIST classifier
    to predict the right digit.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙版的大小越大，MNIST 分类器预测正确数字的难度就越大。
- en: Feel free to experiment with the size of the masked region, that is, try smaller/bigger,
    as well as the location of the mask on the image.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 随意尝试不同的遮挡区域大小，可以尝试更小或更大，也可以尝试不同的遮挡位置。
- en: 'A plot of `9` scaled noised images after upscaling:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`9` 张经过放大处理的噪声图像的图：'
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/b4707253-1495-46c1-81b4-701aeb5e733d.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4707253-1495-46c1-81b4-701aeb5e733d.png)'
- en: 'Figure 13.5: Plot of nine noised/masked MNIST digits'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5：九个带噪声/遮挡的 MNIST 数字的图示
- en: Reshaping
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重塑
- en: 'Reshape the original dataset and the noised dataset to a shape of 60000*28*28*1\.
    This is important since the 2D convolutions expect to receive images of a shape
    of 28*28*1:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始数据集和噪声数据集重塑为 60000*28*28*1 的形状。这非常重要，因为 2D 卷积期望接收的图像形状是 28*28*1：
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If you are doing multiple training runs on the GPU, it is always a good idea
    to clear space on the GPU after each run so that your next run executes efficiently
    without errors related to **resource exhaustion***,* which is pretty common with
    GPUs. This can be done with the following code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在 GPU 上进行多次训练，最好在每次训练后清理 GPU 空间，以确保下次训练能够高效执行，避免**资源耗尽**相关的错误，这在 GPU 上比较常见。可以使用以下代码来完成：
- en: '`from keras import backend as k`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`from keras import backend as k`'
- en: '`k.clear_session()`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`k.clear_session()`'
- en: MNIST classifier
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST 分类器
- en: To start off with modeling, let's build a simple **convolutional neural network** (**CNN**)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始建模，让我们构建一个简单的**卷积神经网络**（**CNN**）。
- en: digit classifier.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 数字分类器。
- en: 'The first layer is a convolution layer that has `32` filters of a shape of
    3*3, with `relu` activation and `Dropout` as the regularizer. The second layer
    is a convolution layer that has `64` filters of a shape of 3*3, with `relu` activation
    and `Dropout` as the regularizer. The third layer is a convolution layer that
    has `128` filters of a shape of 3*3, with `relu` activation and `Dropout` as the
    regularizer, which is finally flattened. The fourth layer is a `Dense` layer of
    `1024` neurons with `relu` activation. The final layer is a `Dense` layer with
    `10` neurons corresponding to the 10 classes in the MNIST dataset, and the activation
    used here is `softmax`, `batch_size` is set to `128`, the `optimizer` used is
    `adam`, and `validation_split` is set to `0.2`. This means that 20% of the training
    set will be used as the validation set:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层是一个卷积层，包含 32 个 3*3 大小的滤波器，使用 `relu` 激活函数，并且使用 `Dropout` 作为正则化。第二层是一个卷积层，包含
    64 个 3*3 大小的滤波器，使用 `relu` 激活函数，并且使用 `Dropout` 作为正则化。第三层是一个卷积层，包含 128 个 3*3 大小的滤波器，使用
    `relu` 激活函数，并且使用 `Dropout` 作为正则化，最后进行展平处理。第四层是一个 `Dense` 层，包含 1024 个神经元，使用 `relu`
    激活函数。最后一层是一个 `Dense` 层，包含 10 个神经元，对应于 MNIST 数据集中的 10 个类别，使用 `softmax` 激活函数，`batch_size`
    设置为 128，使用的 `optimizer` 是 `adam`，`validation_split` 设置为 0.2。这意味着 20% 的训练集将作为验证集使用：
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/65564be8-ebeb-46d3-8d82-ec7489f86a7c.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65564be8-ebeb-46d3-8d82-ec7489f86a7c.png)'
- en: 'Figure 13.6: MNIST CNN classifier training for three epochs'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.6：MNIST CNN 分类器训练三轮
- en: 'Use the built CNN digit classifier on the masked images to get a measure of
    its performance on digits that are missing small sections:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用已构建的 CNN 数字分类器在被遮挡的图像上进行测试，来评估其在缺少部分数字的图像上的表现：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: On the masked images, the CNN digit classifier is 74.9% accurate. It might be
    slightly different when you run it, but it will still be very close.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在被遮挡的图像上，CNN 数字分类器的准确率为 74.9%。当你运行时，结果可能会略有不同，但应该会非常接近。
- en: We have not used maxpooling in the preceding classifier. Try building the same
    classifier with maxpooling or other pooling options.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的分类器中，我们没有使用最大池化（maxpooling）。尝试使用最大池化或其他池化选项构建相同的分类器。
- en: Defining hyperparameters for GAN
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义 GAN 的超参数
- en: 'The following are some of the hyperparameters defined that we will be using
    throughout the code and are totally configurable:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些在代码中定义并将使用的超参数，完全可以配置：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Experiment with different learning rates, optimizers, batch sizes, and smoothing
    values to see how these factors affect the quality of your model and, if you get
    better results, show it to the deep learning community.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试不同的学习率、优化器、批大小和光滑值，观察这些因素如何影响模型的质量，如果得到更好的结果，展示给深度学习社区。
- en: Building the GAN model components
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建GAN模型组件
- en: With the idea that the final GAN model will be able to fill in the part of the
    image that is missing (masked), let's define the generator.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 基于最终的GAN模型能够填充缺失（被遮蔽）图像部分的想法，让我们来定义生成器。
- en: Defining the generator
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义生成器
- en: The generator that we are using here is a simple convolution autoencoder that
    is a combination of two parts—an encoder and a decoder.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用的生成器是一个简单的卷积自编码器，它由两部分组成——编码器和解码器。
- en: 'In the encoder, we have the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器中，我们有以下内容：
- en: The first layer is a convolution 2D layer with `32` filters of a size of 3*3,
    followed by batch normalization, with activation as `relu`, followed by downsampling
    done with `AveragePooling2D` of size 2*2
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层是一个卷积2D层，使用`32`个3x3大小的滤波器，接着进行批归一化，激活函数为`relu`，然后是使用`AveragePooling2D`进行2x2大小的下采样。
- en: The second layer is a convolution 2D layer with `64` filters of a size of 3*3,
    followed by batch normalization, with activation as `relu`, followed by downsampling
    with `AveragePooling2D` of a size of 2*2
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二层是一个卷积2D层，使用`64`个3x3大小的滤波器，接着进行批归一化，激活函数为`relu`，然后是使用`AveragePooling2D`进行2x2大小的下采样。
- en: The third layer or the final layer in this encoder part is again a convolution
    2D layer with `128` filters of a size of 3*3, batch normalization, with activation
    as `relu`
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三层或该编码器部分的最终层仍然是一个卷积2D层，使用`128`个3x3大小的滤波器，进行批归一化，激活函数为`relu`。
- en: 'In the decoder, we have the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码器中，我们有以下内容：
- en: The first layer is a convolution 2D layer with `128` filters of a size of 3*3
    with activation as `relu`, followed by upsampling done with `UpSampling2D`
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层是一个卷积2D层，使用`128`个3x3大小的滤波器，激活函数为`relu`，接着是使用`UpSampling2D`进行上采样。
- en: The second layer is a convolution 2D layer with `64` filters of a size of 3*3
    with activation as `relu`, followed by upsampling with `UpSampling2D`
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二层是一个卷积2D层，使用`64`个3x3大小的滤波器，激活函数为`relu`，接着是使用`UpSampling2D`进行上采样。
- en: The third layer or the final layer in this decoder part is again a convolution
    2D layer with `1` filters of a size of 3*3 with activation as `tanh`
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三层或该解码器部分的最终层仍然是一个卷积2D层，使用`1`个3x3大小的滤波器，激活函数为`tanh`。
- en: 'Remember, in the encoder, if you have `32`, `64`, `128` filters, it should
    be followed by `128`, `64`, `image_channels` filters in the decoder. `image_channels`
    is the number of channels in the input image, which is one in the MNIST dataset.
    If you have `64`, `128`, `256`, `512` filters in the first, second, third, and
    fourth layers of the encoder, the following filters in the decoder should be `256`,
    `128`, `64`, `image_channels`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在编码器中，如果你有`32`、`64`、`128`个滤波器，那么在解码器中应该跟随`128`、`64`、`image_channels`个滤波器。`image_channels`是输入图像的通道数，在MNIST数据集中为1。如果编码器的第一、第二、第三和第四层有`64`、`128`、`256`、`512`个滤波器，那么解码器中的对应滤波器应该是`256`、`128`、`64`、`image_channels`。
- en: '[PRE15]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Two important things to remember here about the final convolution layer in the
    generator. One is to use `tanh` as the activation function since the dataset range
    is between -1 and 1, and the other is, to use the same number of filter(s) as
    the number of channels in the input image. This is to make sure that the image
    being generated has the same number of channels as the input image.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 关于生成器中最终卷积层，有两点需要记住。一是使用`tanh`作为激活函数，因为数据集的范围是-1到1，另一个是使用与输入图像通道数相同数量的滤波器。这是为了确保生成的图像与输入图像有相同数量的通道。
- en: If you decide to center and scale your data like we have done in this exercise,
    you need to use batch normalization in the generator during downsampling, otherwise,
    the loss will not converge. You can witness the effects of not using the batch
    normalization by training the generator without the batch normalization layer.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你决定像我们在本次练习中做的那样对数据进行居中和缩放，你需要在生成器中使用批归一化进行下采样，否则损失将无法收敛。你可以通过在没有批归一化层的情况下训练生成器，亲自见证不使用批归一化的效果。
- en: 'In the following `summary` of the generator, if you refer to the output shape,
    you see the downscaling or compression of the image in the first half of the network
    and the upscaling of the images in the second half of the network:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下的生成器`summary`中，如果查看输出形状，您会看到网络前半部分是图像的降采样或压缩，后半部分是图像的放大：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/f094db57-93b0-4f6e-8c45-8e36752266ac.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f094db57-93b0-4f6e-8c45-8e36752266ac.png)'
- en: 'Figure 13.7: Summary of the generator (autoencoder)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.7：生成器概述（自动编码器）
- en: Consider the following when you are not obtaining good results with the autoencoder.
    Use `AveragePooling2D` first and then check out `MaxPooling2D` for downsampling.
    Use `LeakyReLU` first and then `relu` next. For all of the convolution layers
    except the final one, use either `LeakyReLU` or `relu` activation. Try using a
    deeper autoencoder. Feel free to use more filters in the convolution layers, play
    with the filter sizes and the pooling sizes.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用自动编码器时，如果没有得到好的结果，请考虑以下几点。首先使用`AveragePooling2D`，然后尝试`MaxPooling2D`进行降采样。先使用`LeakyReLU`，然后再尝试`relu`。除了最后一层的卷积层外，所有卷积层都使用`LeakyReLU`或`relu`激活函数。尝试使用更深的自动编码器。可以在卷积层中使用更多的滤波器，调整滤波器和池化层的大小。
- en: Defining the discriminator
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义判别器
- en: The discriminator is a simple CNN binary classifier that takes in the image
    generated by the generator and tries to classify the image as original or fake.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器是一个简单的CNN二分类器，接收由生成器生成的图像，并尝试将其分类为原始图像或伪造图像。
- en: 'The first layer is a convolution 2D layer with `64` filters of a size of 3*3 
    with the activation as `LeakyReLU` and `Dropout` as the regularizer. The secondand
    third layers are the same as the first layer except the second layer has `128`
    filters and the third layer has `256` filters. The final layer is a `Dense` layer
    with `sigmoid` activation since we are doing a binary classification:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层是一个卷积2D层，具有`64`个3*3大小的滤波器，激活函数为`LeakyReLU`，并使用`Dropout`作为正则化器。第二层和第三层与第一层相同，唯一不同的是第二层有`128`个滤波器，第三层有`256`个滤波器。最后一层是一个`Dense`层，使用`sigmoid`激活函数，因为我们正在进行二分类：
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/fb119d52-9834-457c-a53d-e9c313a387f4.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb119d52-9834-457c-a53d-e9c313a387f4.png)'
- en: 'Figure 13.8: Summary of the discriminator'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.8：判别器概述
- en: Play around with the parameters of the discriminator to suit the needs of the
    problem you are trying to solve. Include a `MaxPooling` layer in the model if
    needed.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你需要解决的问题，调整判别器的参数。如果需要，可以在模型中加入`MaxPooling`层。
- en: Defining the DCGAN
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义DCGAN
- en: 'The following function pipes the input followed by the generator, which is
    then followed by the discriminator to form the DCGAN architecture:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数将输入传递给生成器，然后是判别器，从而形成DCGAN架构：
- en: '[PRE18]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: If you have not seen how to use the `Model` function API before, please visit
    the detailed documentation by Keras on using the `Model` function API and compiling
    it at [https://keras.io/models/model/](https://keras.io/models/model/).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你以前没有使用过`Model`函数API，请访问Keras的详细文档，了解如何使用`Model`函数API并进行编译，链接：[https://keras.io/models/model/](https://keras.io/models/model/)。
- en: Training GAN
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练GAN
- en: We've built the components of the GAN.  Let's train the model in the next steps!
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经构建了GAN的各个组件。接下来，让我们开始训练模型吧！
- en: Plotting the training – part 1
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制训练过程 - 第1部分
- en: 'During each epoch, the following function plots `9` generated images. For comparison,
    it will also plot the corresponding `9` original target images and `9` noised
    input images. We need to use the `upscale` function we''ve defined when plotting
    to make sure the images are scaled to range between 0 and 255, so that you do
    not encounter issues when plotting:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个epoch中，以下函数将绘制`9`张生成的图像。为了对比，它还将绘制对应的`9`张原始目标图像和`9`张带噪声的输入图像。绘制时我们需要使用已经定义的`upscale`函数，确保图像缩放到0到255之间，以避免绘图时出现问题：
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output of this function is as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数的输出如下：
- en: '![](img/1a05ba02-d646-4d6b-87cd-a43022388eef.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a05ba02-d646-4d6b-87cd-a43022388eef.png)'
- en: 'Figure 13.9: Sample/expected output of the generated_images_plot function'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.9：生成图像绘制函数的示例/预期输出
- en: Plotting the training – part 2
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制训练过程 - 第2部分
- en: Let's define another function that plots the images generated during each epoch.
    To reflect the difference, we will also include the original and the masked/noised
    images in the plot.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义另一个函数，用于绘制每个epoch生成的图像。为了反映差异，我们还将在图中包含原始图像和带噪声/掩码的图像。
- en: The top row contains the original images, the middle row contains the masked
    images, and the bottom row contains the generated images.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 顶行是原始图像，中间行是遮罩图像，底行是生成的图像。
- en: The plot has `12` rows with the sequence, row 1 - original, row 2 - masked,
    row3 - generated, row 4 - original, row5 - masked,..., row 12 - generated.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 绘图有`12`行，顺序为：第1行 - 原始图像，第2行 - 遮罩图像，第3行 - 生成图像，第4行 - 原始图像，第5行 - 遮罩图像，...，第12行
    - 生成图像。
- en: 'Let''s take a look at the code for the same:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看相应的代码：
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output is as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/1b9b9b1b-321a-41d2-8a75-0c0494f4baae.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b9b9b1b-321a-41d2-8a75-0c0494f4baae.png)'
- en: 'Figure 13.10: Sample/expected output from the plot_generated_images_combined
    function'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.10：来自`plot_generated_images_combined`函数的样本/预期输出
- en: Training loop
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练循环
- en: 'Now we are at the most important part of the code; the part where all of the
    functions we previously defined will be used. The following are the steps:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来到了代码中最重要的部分；即之前定义的所有函数将会在这一部分使用。以下是步骤：
- en: Load the generator by calling the `img_generator()` function.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`img_generator()`函数加载生成器。
- en: Load the discriminator by calling the `img_discriminator()` function and compile
    it with the binary cross-entropy loss and optimizer as `optimizer_d`, which we
    have defined under the hyperparameters section.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`img_discriminator()`函数加载判别器，并使用二元交叉熵损失和优化器`optimizer_d`进行编译，该优化器在超参数部分已定义。
- en: Feed the generator and the discriminator to the `dcgan()` function and compile
    it with the binary cross-entropy loss and optimizer as `optimizer_g`, which we
    have defined under the hyperparameters section.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将生成器和判别器输入到`dcgan()`函数中，并使用二元交叉熵损失和优化器`optimizer_g`进行编译，该优化器在超参数部分已定义。
- en: Create a new batch of original images and masked images. Generate new fake images
    by feeding the batch of masked images to the generator.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一批新的原始图像和遮罩图像。通过将这批遮罩图像输入生成器，生成新的假图像。
- en: Concatenate the original and generated images so that the first 128 images are
    all original and the next 128 images are all fake. It is important that you do
    not shuffle the data here, otherwise it will be hard to train. Label the generated
    images as `0` and original images as `0.9` instead of 1\. This is one-sided label
    smoothing on the original images. The reason for using label smoothing is to make
    the network resilient to adversarial examples. It's called one-sided because we
    are smoothing labels only for the real images.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始图像和生成的图像拼接，使得前128张图像为原始图像，接下来的128张为假图像。重要的是，这里不要打乱数据，否则训练会变得困难。将生成的图像标记为`0`，将原始图像标记为`0.9`，而不是1。
    这就是对原始图像进行的单边标签平滑。使用标签平滑的原因是使网络能够抵抗对抗样本。这是单边标签平滑，因为我们只对真实图像进行标签平滑。
- en: Set `discriminator.trainable` to `True` to enable training of the discriminator
    and feed this set of 256 images and their corresponding labels to the discriminator
    for classification.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置`discriminator.trainable`为`True`，以启用判别器的训练，并将这256张图像及其相应的标签输入判别器进行分类。
- en: Now, set `discriminator.trainable` to `False` and feed a new batch of 128 masked
    images labeled as 1 to the GAN (DCGAN) for classification. It is important to
    set `discriminator.trainable` to `False` to make sure the discriminator is not
    getting trained while training the generator.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将`discriminator.trainable`设置为`False`，并将新的128张标记为1的遮罩图像输入GAN（DCGAN）进行分类。将`discriminator.trainable`设置为`False`非常重要，以确保在训练生成器时，判别器不会参与训练。
- en: Repeat steps 4 through 7 for the desired number of epochs.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤4至7，直到达到期望的训练轮次。
- en: Batch size used here is 128.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的批量大小为128。
- en: We have placed the `plot_generated_images_combined()` function and the `generated_images_plot()`
    function so that we get a plot generated by both functions after the first iteration
    in the first epoch and after the end of each epoch.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将`plot_generated_images_combined()`函数和`generated_images_plot()`函数放置在一起，以便在第一轮的第一次迭代和每轮结束后，通过这两个函数生成一个绘图。
- en: 'Feel free to place these plot functions according to the frequency of plots
    you need displayed:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你需要显示图像的频率，随意放置这些绘图函数：
- en: '[PRE21]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output is as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/9aeddfc1-d406-4376-98e2-ce070c29ce8b.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9aeddfc1-d406-4376-98e2-ce070c29ce8b.png)'
- en: '![](img/13b3e1eb-d6ea-457b-a00e-cbefbeb24557.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13b3e1eb-d6ea-457b-a00e-cbefbeb24557.png)'
- en: 'Figure 13.11.1: Generated images plotted with training plots at the end of
    the first iteration of epoch 1'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.11.1：在第1轮的第一次迭代结束时，绘制的生成图像与训练图像一起显示
- en: '![](img/06d5ad24-8d9e-4841-b8c9-5d5d721ff8e1.png)![](img/596bcf52-a662-4628-a6b4-c812009afc2e.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06d5ad24-8d9e-4841-b8c9-5d5d721ff8e1.png)![](img/596bcf52-a662-4628-a6b4-c812009afc2e.png)'
- en: Figure 13.11.2: Generated images plotted with training plots at the end of epoch
    2
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.11.2：第 2 轮训练结束时，生成的图像与训练图表的结合
- en: '![](img/514a895e-8ad0-441c-9a93-00c040784658.png)![](img/ea8432b1-3134-44eb-a3a5-a90d4f6fc350.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/514a895e-8ad0-441c-9a93-00c040784658.png)![](img/ea8432b1-3134-44eb-a3a5-a90d4f6fc350.png)'
- en: Figure 13.11.3: Generated images plotted with training plots at the end of epoch
    5
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.11.3：第 5 轮训练结束时，生成的图像与训练图表的结合
- en: '![](img/0a3a9e71-8a94-4698-b26d-a249f113c972.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a3a9e71-8a94-4698-b26d-a249f113c972.png)'
- en: 'Figure 13.12: Plot of discriminator and adversarial loss during training'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.12：训练过程中判别器与对抗损失的变化图
- en: Play around with the learning rate for both the generator and the discriminator
    to find the optimal values for your use case. In general, when training GANs,
    you train it for a large number of epochs and then use the preceding loss versus
    iteration plot to identify the minimum spot you would like for the training to
    stop.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试调整生成器和判别器的学习率，找出最适合你使用场景的最优值。通常，在训练 GAN 时，你需要训练多个轮次，然后使用前述的损失与迭代次数图来找到你希望训练停止时的最小点。
- en: Predictions
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测
- en: 'This is what we''ve been building to: making predictions!'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们一直在构建的目标：进行预测！
- en: CNN classifier predictions on the noised and generated images
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN 分类器对噪声和生成图像的预测
- en: 'Now, we will call the generator on the masked MNIST test data to generate images,
    that is, fill in the missing part of the digits:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将在遮蔽的 MNIST 测试数据上调用生成器生成图像，也就是填补数字缺失的部分：
- en: '[PRE22]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, we will pass the generated MNIST digits to the digit classifier we have
    modeled already:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将把生成的 MNIST 数字传递给已经建好的数字分类器：
- en: '[PRE23]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The MNIST CNN classifier is 87.82% accurate on the generated data.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST CNN 分类器在生成数据上的准确率为 87.82%。
- en: 'The following is a plot showing 10 generated images by the generator, the actual
    label of the generated image, and the label predicted by the digit classifier
    after processing the generated image:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个图表，展示了生成器生成的 10 张图像、生成图像的实际标签以及经过处理后由数字分类器预测的标签：
- en: '[PRE24]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output is as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/ff167a28-9428-405b-a0cd-6ec9973e1064.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff167a28-9428-405b-a0cd-6ec9973e1064.png)'
- en: Figure 13.13:  Plot of MNIST classifier predictions on the generated images
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.13：MNIST 分类器对生成图像的预测图
- en: Scripts in modular form
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块化形式的脚本
- en: The entire script can be split into four modules named `train_mnist.py`, `training_plots.py`,
    `GAN.py`, and `train_gan.py`. Store these in a folder of your choice, for example, `gan`.
    Set `gan` as the project folder in your favorite source code editor and just run
    the `train_gan.py` file.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 整个脚本可以分为四个模块，分别为 `train_mnist.py`、`training_plots.py`、`GAN.py` 和 `train_gan.py`。将这些文件保存在你选择的文件夹中，例如
    `gan`。将 `gan` 设为项目文件夹，然后在你喜欢的源代码编辑器中运行 `train_gan.py` 文件。
- en: The `train_gan.py` Python file will import functions from all of the other modules
    in places where they're needed for execution.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_gan.py` Python 文件将从其他模块导入函数，在需要执行的地方调用它们。'
- en: Now, let's walk through the contents of each file.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们逐一浏览每个文件的内容。
- en: Module 1 – train_mnist.py
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块 1 – train_mnist.py
- en: 'This Python file contains the `train_mnist()` function that we have used previously
    to train a CNN classifier on MNIST digits:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Python 文件包含了我们之前用来训练 MNIST 数字 CNN 分类器的 `train_mnist()` 函数：
- en: '[PRE25]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Module 2 – training_plots.py
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块 2 – training_plots.py
- en: 'This Python file contains the four functions, `upscale()`, `generated_images_plot()`,
    `plot_generated_images_combined()`, and `plot_training_loss()`:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Python 文件包含了四个函数：`upscale()`、`generated_images_plot()`、`plot_generated_images_combined()`
    和 `plot_training_loss()`：
- en: '[PRE26]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'For the remaining part of this code, please visit: [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter13/training_plots.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter13/training_plots.py)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 若要查看代码的其余部分，请访问：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter13/training_plots.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter13/training_plots.py)
- en: Module 3 – GAN.py
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块 3 – GAN.py
- en: 'This module contains the DCGAN components, namely `img_generator()`, `img_discriminator()`,
    and `dcgan()`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 该模块包含了 DCGAN 组件，即 `img_generator()`、`img_discriminator()` 和 `dcgan()`：
- en: '[PRE27]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'For the remaining part of this code, please visit: [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter13/GAN.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter13/GAN.py)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此代码的其余部分，请访问：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter13/GAN.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter13/GAN.py)
- en: Module 4 – train_gan.py
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块4 – train_gan.py
- en: 'In this module, we will include the hyperparameters, pre-process the data,
    generate synthetic data, train the GAN, train the CNN classifier, and import all
    of the necessary functions from other modules:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在本模块中，我们将包括超参数，预处理数据，生成合成数据，训练GAN，训练CNN分类器，并从其他模块导入所有必要的函数：
- en: '[PRE28]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'For the remaining part of this module, please visit: [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter13/train_gan.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter13/train_gan.py)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本模块的其余部分，请访问：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter13/train_gan.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter13/train_gan.py)
- en: You can use the same modules you have created to train on fashion MNIST data.
    All you have to do is replace line 11 in the `train_gan.py` file with (`from keras.datasets
    import fashion_mnist`) and replace line 28 with (`(X_train, y_train), (X_test,
    y_test) =  fashion_mnist.load_data()`). The results will be good but not excellent
    since the parameters set here work best on the MNIST digit data. This will be
    a good exercise for you to get incredible results without much effort.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用你创建的相同模块来训练时尚MNIST数据。你需要做的就是将`train_gan.py`文件中的第11行替换为（`from keras.datasets
    import fashion_mnist`），并将第28行替换为（`(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()`）。结果会很好，但不会非常出色，因为这里设置的参数在MNIST数字数据上表现最佳。这将是一个很好的练习，你可以在不费力的情况下获得令人难以置信的结果。
- en: 'Here is a resource on tips to train GANs that you must check out:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些关于训练GAN的技巧资源，你一定要查看：
- en: '[https://github.com/soumith/ganhacks.](https://github.com/soumith/ganhacks)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/soumith/ganhacks.](https://github.com/soumith/ganhacks)'
- en: The Jupyter Notebook code files for the preceding DCGAN MNIST inpainting can
    be found at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2014/DCGAN_MNIST.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2014/DCGAN_MNIST.ipynb).
    The Jupyter Notebook code files for the DCGAN Fashion MNIST inpainting can be
    found at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2014/DCGAN_Fashion_MNIST.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2014/DCGAN_Fashion_MNIST.ipynb).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 前述DCGAN MNIST修复的Jupyter Notebook代码文件可以在[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2014/DCGAN_MNIST.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2014/DCGAN_MNIST.ipynb)找到。DCGAN时尚MNIST修复的Jupyter
    Notebook代码文件可以在[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2014/DCGAN_Fashion_MNIST.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2014/DCGAN_Fashion_MNIST.ipynb)找到。
- en: The conclusion to the project
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目的结论
- en: The goal of this project was to build a GAN to solve the problem of regenerating
    missing parts/regions of handwritten digits. In the initial chapters, we applied
    deep learning to enable customers of a restaurant chain to write their phone numbers
    in a simple iPad application to get a text notification that their party could
    be seated. The use case of this chapter was to apply deep learning to generate
    missing parts of the digits of the phone number so that a text notification can
    be sent to the right person.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的目标是构建一个GAN，解决手写数字缺失部分/区域重生的问题。在最初的章节中，我们应用深度学习技术，使餐饮连锁店的顾客可以通过一个简单的iPad应用写下他们的电话号码，以便收到通知，提示他们的座位已准备好。本章节的使用案例是应用深度学习技术生成电话号码中缺失的数字部分，从而能将文本通知发送给正确的人。
- en: The CNN digit classifier model accuracy hit 98.84% on the MNIST validation data.
    With the data we generated to simulate missing parts of a digit when fed to the
    CNN digit classifier, the model was only 74.90% accurate.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: CNN数字分类器在MNIST验证数据上的准确率达到了98.84%。使用我们生成的数据来模拟数字缺失部分时，输入到CNN数字分类器中时，模型的准确率仅为74.90%。
- en: The same dataset with missing sections of the digit was passed to the generator
    to recover the missing parts. The resulting digits were then passed to the CNN
    classifier and the model was 87.82% accurate. See if you can tweak both the CNN
    classifier and the GAN to generate clearer digits, as well as much higher accuracy
    on these generated images.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的缺失部分数字数据被传递给生成器，以恢复丢失的部分。然后，生成的数字被传递给CNN分类器，模型的准确率为87.82%。看看你能否调整CNN分类器和GAN，生成更清晰的数字，以及大幅提高这些生成图像的准确性。
- en: Let's follow the same technique we have been following in the previous chapters
    for evaluating the performance of the models from the restaurant chain point of
    view.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们沿用之前章节中评估模型表现的相同方法，从餐饮连锁的角度进行分析。
- en: What are the implications of this accuracy? Let's calculate the incidence of
    an error occurring that would result in a customer service issue (that is, the
    customer not getting the text that their table is ready and getting upset for
    an excessively long wait time at the restaurant).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这种准确性有什么意义呢？我们来计算一下错误发生的几率，错误导致客户服务问题的情况（也就是说，客户没有收到他们的桌子准备好的通知，反而因为餐厅等待时间过长而感到不满）。
- en: Each customer's phone number is ten digits long. Let's assume our hypothetical
    restaurant has an average of 30 tables at each location and those tables turn
    over two times per night during the rush hour when the system is likely to be
    used, and finally, the restaurant chain has 35 locations. This means that each
    day of operation there are approximately 21,000 handwritten numbers captured (30 tables x
    2 turns/day x 35 locations x 10 digit phone number).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 每个客户的电话号码由十位数字组成。假设我们假设的餐厅在每个地点平均有30张桌子，这些桌子在高峰时段，每晚翻台两次，并且餐厅连锁有35个地点。这意味着每天的运营中大约会捕获21,000个手写数字（30张桌子
    × 每天2次翻台 × 35个地点 × 10位数字的电话号码）。
- en: Obviously, all digits must be correctly classified for the text to get to the
    proper waiting restaurant patron. So any single digit misclassification causes
    a failure. With the simulated data, the model accuracy was 74.90%, which means
    a total of 5,271 digits are misclassified. With the recovered data (on the simulated
    data) from the generator of the trained GAN, the model accuracy was 87.82%, which
    would improperly classify 2,558 digits per day in our example. The worst case
    for the hypothetical scenario would be if there occurred only one improperly classified
    digit in each phone number. Since there are only 2,100 patrons and corresponding
    phone numbers, this would mean that every phone number had an error in classification
    (100% failure) and not a single customer would get their text notification that
    their party could be seated! The best case scenario would be if all 10 digits
    were misclassified in each phone number and that would result in 263 wrong phone
    numbers out of 2,100 (12.5% failure rate). Still not a level of performance the
    restaurant chain would be likely to be happy with, so you can see why we'd need
    to continue fine-tuning the models to get the maximum performance possible.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，所有数字必须正确分类，才能确保文本通知发送到正确的等待餐厅顾客。因此，任何一个数字的错误分类都会导致失败。在模拟数据上，模型准确率为74.90%，意味着总共有5,271个数字被误分类。通过从训练好的GAN的生成器中恢复的数据（基于模拟数据），模型的准确率为87.82%，这意味着在我们的例子中每天会错误分类2,558个数字。假设最坏的情况是每个电话号码中只发生一个错误分类数字。那么，考虑到只有2,100个顾客及相应的电话号码，这就意味着每个电话号码都会有一个分类错误（100%的失败率），没有一个顾客能收到通知，知道他们的聚会可以入座！最好的情况是每个电话号码中的10个数字都被误分类，这将导致2,100个电话号码中有263个错误（12.5%的失败率）。这仍然不是餐饮连锁可能满意的表现水平，因此你可以看到为什么我们需要继续微调模型，以获得可能的最大性能。
- en: Summary
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In the project in this chapter, we have successfully built a deep convolution
    GAN in Keras on handwritten MNIST digits. We understood the function of the generator
    and the discriminator component of the GAN. We have defined some key hyperparameters,
    as well as, in some places, reasoned with why we used what we did. Finally, we
    tested the GAN's performance on unseen data and determined that we succeeded in
    achieving our goals.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的项目中，我们成功地在Keras中构建了一个深度卷积GAN，应用于手写的MNIST数字。我们了解了GAN中生成器和判别器组件的功能，定义了一些关键的超参数，并且在某些地方解释了我们为何使用这些参数。最后，我们在未见过的数据上测试了GAN的表现，并确定我们达成了预期目标。
