- en: Deep Deterministic Policy Gradient
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度确定性策略梯度（Deep Deterministic Policy Gradient）
- en: In earlier chapters, you saw the use of **reinforcement learning** (**RL**)
    to solve discrete action problems, such as those that arise in Atari games. We
    will now build on this to tackle continuous, real-valued action problems. Continuous
    control problems are copious—for example, the motor torque of a robotic arm; the
    steering, acceleration, and braking of an autonomous car; the wheeled robotic
    motion on terrain; and the roll, pitch, and yaw controls of a drone. For these
    problems, we train neural networks in an RL setting to output real-valued actions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，你已经看到如何使用**强化学习**（**RL**）来解决离散动作问题，如Atari游戏中的问题。现在我们将基于此，处理连续的、实值动作问题。连续控制问题非常普遍，例如，机器人手臂的电机扭矩；自动驾驶汽车的转向、加速和刹车；地形上的轮式机器人运动；以及无人机的俯仰、滚转和偏航控制。对于这些问题，我们在强化学习环境中训练神经网络以输出实值动作。
- en: Many continuous control algorithms involve two neural networks—one referred
    to as the **actor** (policy-based), and the other as the **critic** (value-based)—and
    therefore, this family of algorithms is referred to as **Actor-Critic algorithms**.
    The role of the actor is to learn a good policy that can predict good actions
    for a given state. The role of the critic is to ascertain whether the actor undertook
    a good action, and to provide feedback that serves as the learning signal for
    the actor. This is akin to a student-teacher or employee-boss relationship, wherein
    the student or employee undertakes a task or work, and the role of the teacher
    or boss is to provide feedback on the quality of the action performed.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 许多连续控制算法涉及两个神经网络——一个被称为**演员**（基于策略），另一个被称为**评论员**（基于价值）——因此，这类算法被称为**演员-评论员算法**。演员的角色是学习一个好的策略，能够根据给定状态预测出好的动作。评论员的角色是评估演员是否采取了一个好的动作，并提供反馈，作为演员学习的信号。这就像学生-老师或员工-老板的关系，其中学生或员工完成一项任务或工作，而老师或老板的角色是提供关于所执行动作质量的反馈。
- en: The foundation of continuous control RL is through what is called the **policy
    gradient**, which is an estimate of how much a neural network's weights should
    be altered so as to maximize the long-term cumulative discounted rewards. Specifically,
    it uses the **chain rule** and it is an estimate of the gradient that needs to
    be back-propagated into the actor network for the policy to improve. It is evaluated
    as an average over a mini-batch of samples. We will cover these topics in this
    chapter. In particular, we will cover an algorithm called the **Deep Deterministic
    Policy Gradient** (**DDPG**), which is a state-of-the-art RL algorithm for continuous
    control.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 连续控制强化学习的基础是通过所谓的**策略梯度**，它是一个估计值，用来表示神经网络的权重应该如何调整，以最大化长期累计折扣奖励。具体来说，它利用了**链式法则**，并且它是一个需要反向传播到演员网络中的梯度估计，以便改进策略。这个估计通过小批量样本的平均值来评估。我们将在本章中介绍这些话题，特别是，我们将介绍一个名为**深度确定性策略梯度**（**DDPG**）的算法，它是用于连续控制的最先进的强化学习算法。
- en: Continuous control has many real-world applications. For instance, continuous
    control can involve the evaluation of the steering, acceleration, and braking
    of an autonomous car. It can also be applied to determine the torques required
    for the actuator motors of a robot. Or, it can be used in biomedical applications,
    where the control could be to determine the muscle movements for a humanoid locomotion.
    Thus, continuous control problem applications abound.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 连续控制在现实世界中有许多应用。例如，连续控制可以用于评估自动驾驶汽车的转向、加速和刹车。它还可以用于确定机器人执行器电机所需的扭矩。或者，它可以应用于生物医学领域，控制可能是确定类人运动的肌肉运动。因此，连续控制问题的应用非常广泛。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Actor-Critic algorithms and policy gradients
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演员-评论员算法和策略梯度
- en: DDPG
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DDPG
- en: Training and testing DDPG on Pendulum-v0
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Pendulum-v0上训练和测试DDPG
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In order to successfully complete this chapter, the following items are required:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了成功完成本章，以下内容是必需的：
- en: Python (2 and above)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python（2及以上版本）
- en: NumPy
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: Matplotlib
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib
- en: TensorFlow (version 1.4 or higher)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow（版本1.4或更高）
- en: A computer with at least 8 GB of RAM (higher than this is even better!)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台至少拥有8GB内存的电脑（更高内存更佳！）
- en: Actor-Critic algorithms and policy gradients
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演员-评论员算法和策略梯度
- en: In this section, we will cover what Actor-Critic algorithms are. You will also
    see what policy gradients are and how they are useful to Actor-Critic algorithms.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍什么是演员-评论家（Actor-Critic）算法。你还将了解什么是策略梯度以及它们如何对演员-评论家算法有所帮助。
- en: How do students learn at school? Students normally make a lot of mistakes as
    they learn. When they do well at learning a task, their teacher provides positive
    feedback. On the other hand, if students do poorly at a task, the teacher provides
    negative feedback. This feedback serves as the learning signal for the student
    to get better at their tasks. This is the crux of Actor-Critic algorithms.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 学生如何在学校学习呢？学生在学习过程中通常会犯很多错误。当他们在某个任务上表现良好时，老师会给予正面反馈。另一方面，如果学生在某个任务上表现不佳，老师会提供负面反馈。这些反馈作为学生改进任务的学习信号。这就是演员-评论家算法的核心。
- en: 'The following is a summary of the steps involved:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是涉及的步骤概述：
- en: We will have two neural networks—one referred to as the actor, and the other
    as the critic
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将有两个神经网络，一个被称为演员，另一个被称为评论家。
- en: The actor is like the student, as we described previously, and takes an action
    at a given state
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演员就像我们之前描述的学生，在给定状态下采取行动。
- en: The critic is like the teacher, as we described previously, and provides feedback
    for the actor to learn
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评论家就像我们之前描述的老师，提供反馈给演员学习。
- en: Unlike a teacher in a school, the critic network should also be trained from
    scratch, which makes the problem challenging
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与学校中的老师不同，评论家网络也应该从头开始训练，这使得问题变得具有挑战性。
- en: The policy gradient is used to train the actor
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略梯度用于训练演员
- en: The L2 norm on the Bellman update is used to train the critic
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellman 更新的 L2 范数用于训练评论家
- en: Policy gradient
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度
- en: 'The **policy gradient** is defined as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**策略梯度**定义如下：'
- en: '![](img/bd6043e6-9bab-4949-b906-f96405cf58c5.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd6043e6-9bab-4949-b906-f96405cf58c5.png)'
- en: '*J* is the long-term reward function that needs to be maximized, *θ* is the
    policy neural network parameters, *N* is the mini-batch size, *Q(s,a)* is the
    state-action value function, and *π* is the policy. In other words, we compute
    the gradient of the state-action value function with respect to actions and the
    gradient of the policy with respect to the network parameters, multiply them,
    and take an average of them over *N* samples of data from a mini-batch. We can
    then use this policy gradient in a gradient ascent setting to update the policy
    parameters. Note that it is essentially a chain rule of calculus that is being
    used to evaluate the policy gradient.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*J* 是需要最大化的长期奖励函数，*θ* 是策略神经网络参数，*N* 是小批量的样本大小，*Q(s,a)* 是状态-动作值函数，*π* 是策略。换句话说，我们计算状态-动作值函数相对于动作的梯度，以及策略相对于网络参数的梯度，将它们相乘，并对来自小批量数据的
    *N* 个样本取平均。然后，我们可以在梯度上升的设置中使用这个策略梯度来更新策略参数。请注意，本质上是利用微积分的链式法则来评估策略梯度。'
- en: Deep Deterministic Policy Gradient
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度确定性策略梯度（DDPG）
- en: We will now delve into the DDPG algorithm, which is a state-of-the-art RL algorithm
    for continuous control. It was originally published by Google DeepMind in 2016
    and has gained a lot of interest in the community, with several new variants proposed
    thereafter. As was the case in DQN, DDPG also uses target networks for stability.
    It also uses a replay buffer to reuse past data, and therefore, it is an off-policy
    RL algorithm.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将深入探讨 DDPG 算法，它是一个用于连续控制的最先进的强化学习（RL）算法。它最早由 Google DeepMind 于 2016 年发布，并在社区中引起了广泛关注，随后提出了若干新变体。与
    DQN 类似，DDPG 也使用目标网络以提高稳定性。它还使用回放缓冲区来重用过去的数据，因此，它是一个脱离策略的强化学习算法。
- en: The `ddpg.py` file is the main file from which we start the training and testing.
    It will call the training or testing functions, which are present in `TrainOrTest.py`.
    The `AandC.py` file has the TensorFlow code for the actor and the critic networks.
    Finally, `replay_buffer.py` stores the samples in a replay buffer by using a deque
    data structure. We will train the DDPG to learn to hold an inverted pendulum vertically,
    using OpenAI Gym's Pendulum-v0, which has three states and one continuous action,
    which is the torque to be applied to hold the pendulum as vertically inverted.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`ddpg.py` 文件是我们开始训练和测试的主要文件。它将调用存在于 `TrainOrTest.py` 中的训练或测试函数。`AandC.py` 文件包含演员和评论家网络的
    TensorFlow 代码。最后，`replay_buffer.py` 通过使用双端队列（deque）数据结构将样本存储在回放缓冲区中。我们将训练 DDPG
    来学习保持倒立摆竖直，使用 OpenAI Gym 的 Pendulum-v0，它有三个状态和一个连续动作，即施加的扭矩，用来保持倒立摆的竖直状态。'
- en: Coding ddpg.py
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码 ddpg.py
- en: We will first code the `ddpg.py` file. The steps that are involved are as follows.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先编写 `ddpg.py` 文件。涉及的步骤如下：
- en: 'We will now summarize the DDPG code:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将总结 DDPG 代码：
- en: '**Importing the required packages**: We will import the required packages and
    other Python files:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**导入所需的包**：我们将导入所需的包和其他 Python 文件：'
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Defining the** **train()** **function**: We will define the `train()` function.
    This takes the argument parser object, `args`. We create a TensorFlow session
    as `sess`. The name of the environment is used to make a Gym environment stored
    in the `env` object. We also set the random number of seeds and the maximum number
    of steps for an episode of the environment. We also set the state and action dimensions
    in `state_dim` and `action_dim`, which take the values of `3` and `1`, respectively,
    for the Pendulum-v0 problem. We then create actor and critic objects, which are
    instances of the `ActorNetwork` class and the `CriticNetwork` class, respectively,
    which will be described later, in the `AandC.py file`. We then call the `trainDDPG()`
    function, which will start the training of the RL agent.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义** **train()** **函数**：我们将定义 `train()` 函数。它接受参数解析器对象 `args`。我们创建一个 TensorFlow
    会话作为 `sess`。环境的名称用于创建一个 Gym 环境并存储在 `env` 对象中。我们还设置了随机数种子和环境中每一剧集的最大步数。我们还在 `state_dim`
    和 `action_dim` 中设置了状态和动作维度，对于 Pendulum-v0 问题，它们的值分别是 `3` 和 `1`。然后，我们创建演员和评论员对象，这些对象分别是
    `ActorNetwork` 类和 `CriticNetwork` 类的实例，稍后将在 `AandC.py 文件` 中描述。接着，我们调用 `trainDDPG()`
    函数，这将开始强化学习代理的训练。'
- en: 'Finally, we save the TensorFlow model by using `tf.train.Saver()` and `saver.save()`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过使用 `tf.train.Saver()` 和 `saver.save()` 保存 TensorFlow 模型：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Defining the** **test()** **function**: The `test()` function is defined
    next. This will be used once we have finished the training and want to test how
    well our agent is performing. The code is as follows for the `test()` function
    and is very similar to `train()`. We will restore the saved model from `train()` by
    using `tf.train.Saver()` and `saver.restore()`. We call the `testDDPG()` function
    to test the model:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义** **test()** **函数**：接下来定义 `test()` 函数。这将在我们完成训练后使用，用来测试我们的代理的表现如何。`test()`
    函数的代码如下，与 `train()` 非常相似。我们将通过使用 `tf.train.Saver()` 和 `saver.restore()` 从 `train()`
    恢复已保存的模型。然后我们调用 `testDDPG()` 函数来测试该模型：'
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Defining the** **main** **function**: Finally, the `main` function is as
    follows. We define an argument parser by using Python''s `argparse`. The learning
    rates for the actor and critic are specified, including the discount factor, `gamma`,
    and the target network exponential average parameter, `tau`. The buffer size,
    mini-batch size, and number of episodes are also specified in the argument parser.
    The environment that we are interested in is Pendulum-v0, and this is also specified
    in the argument parser.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义** **主** **函数**：最后，`main` 函数如下所示。我们通过使用 Python 的 `argparse` 定义了一个参数解析器。为演员和评论员指定了学习率，包括折扣因子
    `gamma` 和目标网络指数平均参数 `tau`。缓冲区大小、迷你批次大小和剧集数量也在参数解析器中指定。我们感兴趣的环境是 Pendulum-v0，且该环境在参数解析器中也被指定。'
- en: '**Calling the** **train()** **or** **test()** **function, as appropriate**:
    The mode for running this code is train or test, and it will call the appropriate
    eponymous function, which we defined previously:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**调用** **train()** **或** **test()** **函数，根据需要**：运行此代码的模式为 train 或 test，并且它会调用适当的同名函数，这是我们之前定义的：'
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: That's it for `ddpg.py`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 `ddpg.py` 的全部内容。
- en: Coding AandC.py
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写 AandC.py
- en: 'We will specify the `ActorNetwork` class and the `CriticNetwork` class in `AandC.py`.
    The steps involved are as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 `AandC.py` 中指定 `ActorNetwork` 类和 `CriticNetwork` 类。涉及的步骤如下：
- en: '**Importing packages**: First, we import the packages:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**导入包**：首先，我们导入所需的包：'
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Define initializers for the weights and biases**: Next, we define the weights
    and biases initializers:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义权重和偏差的初始化器**：接下来，我们定义权重和偏差的初始化器：'
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Defining the** **ActorNetwork** **class**: The `ActorNetwork` class is specified
    as follows. First, it receives parameters as arguments in the `__init__` constructor.
    We then call `create_actor_network()`, which will return `inputs`, `out`, and
    `scaled_out` objects. The actor model parameters are stored in `self.network_params`
    by calling TensorFlow''s `tf.trainable_variables()`. We replicate the same for
    the actor''s target network, as well. Note that the target network is required
    for stability reasons; it is identical to the actor in neural network architecture,
    albeit the parameters gradually change. The target network parameters are collected
    and stored in `self.target_network_params` by calling `tf.trainable_variables()` again:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义** **ActorNetwork** **类**：`ActorNetwork`类定义如下。首先，它在`__init__`构造函数中接收参数。然后我们调用`create_actor_network()`，该函数将返回`inputs`、`out`和`scaled_out`对象。演员模型的参数通过调用TensorFlow的`tf.trainable_variables()`存储在`self.network_params`中。我们对演员的目标网络也做相同的操作。需要注意的是，目标网络是为了稳定性考虑而存在的；它的神经网络架构与演员网络相同，只是参数会逐渐变化。目标网络的参数通过再次调用`tf.trainable_variables()`被收集并存储在`self.target_network_params`中：'
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Defining** **self.update_target_network_params**: Next, we define `self.update_target_network_params`,
    which will weigh the current actor network parameters with `tau` and the target
    network''s parameters with `1-tau`, and add them to store them as a TensorFlow
    operation. We are thus gradually updating the target network''s model parameters.
    Note the use of `tf.multiply()` for multiplying the weights with `tau` (or `1-tau`,
    as the case may be). We then create a TensorFlow placeholder called `action_gradient`
    to store the gradient of *Q*, with respect to the action, which is to be supplied
    by the critic. We also use `tf.gradients()` to compute the gradient of the output
    of the policy network with respect to the network parameters. Note that we then
    divide by the `batch_size`, in order to average the summation over a mini-batch.
    This gives us the averaged policy gradient, which we can then use to update the
    actor network parameters:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义** **self.update_target_network_params**：接下来，我们定义`self.update_target_network_params`，它会将当前的演员网络参数与`tau`相乘，将目标网络的参数与`1-tau`相乘，然后将它们加在一起，存储为一个TensorFlow操作。这样我们就逐步更新目标网络的模型参数。注意使用`tf.multiply()`来将权重与`tau`（或者根据情况是`1-tau`）相乘。然后，我们创建一个TensorFlow占位符，命名为`action_gradient`，用来存储与动作相关的*Q*的梯度，这个梯度由评论员提供。我们还使用`tf.gradients()`计算策略网络输出相对于网络参数的梯度。注意，接着我们会除以`batch_size`，以便对小批量数据的求和结果进行平均。这样，我们就得到了平均策略梯度，接下来可以用来更新演员网络参数：'
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Using Adam optimization**: We use Adam optimization to apply the policy gradients,
    in order to optimize the actor''s policy:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用Adam优化**：我们使用Adam优化算法来应用策略梯度，从而优化演员的策略：'
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Defining the** **create_actor_network()** **function**: We now define the
    `create_actor_network()` function. We will use a neural network with two layers,
    with `400` and `300` neurons, respectively. The weights are initialized by using
    **Xavier initialization**, and the biases are zeros to begin with. We use the `relu`
    activation function, and also batch normalization, for stability. The final output
    layer has weights initialized with a uniform distribution and a `tanh` activation
    function in order to keep it bounded. For the Pendulum-v0 problem, the actions
    are bounded in the range [*-2,2*], and since `tanh` is bounded in the range [*-1,1*],
    we need to multiply the output by two to scale accordingly; this is done by using
    `tf.multiply()`, where `action_bound = 2` for the inverted pendulum problem:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义** **create_actor_network()** **函数**：现在我们定义`create_actor_network()`函数。我们将使用一个包含两层神经元的神经网络，第一层有`400`个神经元，第二层有`300`个神经元。权重使用**Xavier初始化**，偏置初始值为零。我们使用`relu`激活函数，并使用批归一化（batch
    normalization）以确保稳定性。最终的输出层的权重使用均匀分布初始化，并采用`tanh`激活函数，以保持输出值在一定范围内。对于Pendulum-v0问题，动作的范围是[*-2,2*]，而`tanh`的输出范围是[*-1,1*]，因此我们需要将输出乘以2来进行缩放；这可以通过`tf.multiply()`来实现，其中`action_bound
    = 2`表示倒立摆问题中的动作范围：'
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Define the actor functions**: Finally, we have the remaining functions that
    are required to complete the `ActorNetwork` class. We will define `train()`, which
    will run a session on `self.optimize`; the `predict()` function runs a session
    on `self.scaled_out`, that is, the output of the `ActorNetwork`; the `predict_target()`
    function will run a session on `self.target_scaled_out`, that is, the output action
    of the actor''s target network. Then, `update_target_network()` will run a session
    on `self.update_target_network_params`, which will perform the weighted average
    of the network parameters.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义演员函数**：最后，我们定义完成`ActorNetwork`类所需的剩余函数。我们将定义`train()`，它将运行`self.optimize`会话；`predict()`函数运行`self.scaled_out`会话，即`ActorNetwork`的输出；`predict_target()`函数将运行`self.target_scaled_out`会话，即演员目标网络的输出动作。接着，`update_target_network()`将运行`self.update_target_network_params`会话，执行网络参数的加权平均。'
- en: 'Finally, the `get_num_trainable_vars()` function returns a count of the number
    of trainable variables:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`get_num_trainable_vars()`函数返回可训练变量的数量：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Defining** **CriticNetwork class**: We will now define the `CriticNetwork`
    class. Similar to `ActorNetwork`, we receive the model hyperparameters as arguments.
    We then call the `create_critic_network()` function, which will return `inputs`,
    `action`, and `out`. We also create the target network for the critic by calling
    `create_critic_network()` again:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义** **CriticNetwork 类**：现在我们将定义`CriticNetwork`类。与`ActorNetwork`类似，我们将模型超参数作为参数传递。然后调用`create_critic_network()`函数，它将返回`inputs`、`action`和`out`。我们还通过再次调用`create_critic_network()`来创建评论者的目标网络：'
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Critic target network**: Similar to the actor''s target, the critic''s target
    network is also updated by using weighted averaging. We then create a TensorFlow
    placeholder called `predicted_q_value`, which is the target value. We then define
    the L2 norm in `self.loss`, which is the quadratic error on the Bellman residual.
    Note that `self.out` is the *Q(s,a)* that we saw earlier, and `predicted_q_value`
    is the *r + γQ(s'',a'')* in the Bellman equation. Again, we use the Adam optimizer
    to minimize this L2 loss function. We then evaluate the gradient of *Q(s,a)* with
    respect to the actions by calling `tf.gradients()`, and we store this in `self.action_grads`.
    This gradient is used later in the computation of the policy gradients:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评论者目标网络**：与演员的目标网络类似，评论者的目标网络也是通过加权平均进行更新。然后，我们创建一个名为`predicted_q_value`的TensorFlow占位符，它是目标值。接着，我们在`self.loss`中定义L2范数，它是贝尔曼残差的平方误差。请注意，`self.out`是我们之前看到的*Q(s,a)*，`predicted_q_value`是贝尔曼方程中的*r
    + γQ(s'',a'')*。我们再次使用Adam优化器来最小化这个L2损失函数。然后，通过调用`tf.gradients()`来评估*Q(s,a)*相对于动作的梯度，并将其存储在`self.action_grads`中。这个梯度稍后会在计算策略梯度时使用：'
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Defining create_critic_network()**: Next, we will define the `create_critic_network()`
    function. The critic network is also similar to the actor in architecture, except
    that it takes both the states and the actions as input. There are two hidden layers,
    with `400` and `300` neurons, respectively. The last output layer has only one
    neuron, that is, is the *Q(s,a)* state-action value function. Note that the last
    layer has no activation function, as `Q(s,a)` is, in theory, unbounded:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义 create_critic_network()**：接下来，我们将定义`create_critic_network()`函数。评论者网络的架构与演员相似，唯一不同的是它同时接受状态和动作作为输入。网络有两层隐藏层，分别有`400`和`300`个神经元。最后的输出层只有一个神经元，即*Q(s,a)*状态-动作值函数。请注意，最后一层没有激活函数，因为理论上`Q(s,a)`是无界的：'
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, the functions required to complete the `CriticNetwork` are as follows.
    These are similar to the `ActorNetwork`, so we do not elaborate further for brevity.
    One difference, however, is the `action_gradients()` function, which is the gradient
    of *Q(s,a)* with respect to the actions, which is computed by the critic and supplied
    to the actor, to be used in the evaluation of the policy gradients:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终，完成`CriticNetwork`所需的功能如下。这些与`ActorNetwork`类似，因此为了简洁起见，我们不再详细说明。不过有一个不同之处，即`action_gradients()`函数，它是*Q(s,a)*相对于动作的梯度，由评论者计算并提供给演员，以便用于策略梯度的评估：
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: That's it for `AandC.py`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是`AandC.py`的内容。
- en: Coding TrainOrTest.py
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Coding TrainOrTest.py
- en: 'The `trainDDPG()` and `testDDPG()` functions that we used earlier will now
    be defined in `TrainOrTest.py`. The steps that are involved are as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前使用的`trainDDPG()`和`testDDPG()`函数将会在`TrainOrTest.py`中定义。涉及的步骤如下：
- en: '**Import packages and functions**: The `TrainOrTest.py` file starts with the
    importing of the packages and other Python files:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**导入包和函数**：`TrainOrTest.py`文件首先导入了相关的包和其他Python文件：'
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Define the trainDDPG() function**: Next, we define the `trainDDPG()` function.
    First, we initialize all of the networks by calling a `sess.run()` on `tf.global_variables_initializer()`.
    Then, we initialize the target network weights and the replay buffer. Then, we
    start the main loop over the training episodes. Inside of this loop, we reset
    the environment (Pendulum-v0, in our case) and also start the loop over time steps
    for each episode (recall that each episode has a `max_episode_len` number of time
    steps).'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义trainDDPG()函数**：接下来，我们定义`trainDDPG()`函数。首先，我们通过调用`sess.run()`和`tf.global_variables_initializer()`初始化所有网络。然后，我们初始化目标网络的权重和重放缓冲区。接着，我们开始训练回合的主循环。在这个循环内，我们重置环境（在我们的案例中是Pendulum-v0），并开始每个回合内的时间步循环（回顾一下，每个回合有`max_episode_len`个时间步）。'
- en: 'The actor''s policy is sampled to obtain the action for the current state.
    We feed this action into `env.step()`, which takes one time step of this action
    and, in the process, moves to the next state, `s2`. The environment also gives
    this a reward, `r`, and information on whether the episode is terminated is stored
    in the Boolean variable `terminal`. We add the tuple (`state`, `action`, `reward`,
    `terminal`, `new state`) to the replay buffer for sampling later and for training:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 演员的策略被采样以获得当前状态的动作。我们将这个动作输入`env.step()`，它执行该动作的一个时间步，并在此过程中移动到下一个状态，`s2`。环境还会给出这个动作的奖励`r`，并将是否终止的状态信息存储在布尔变量`terminal`中。我们将元组（`state`，`action`，`reward`，`terminal`，`new
    state`）添加到重放缓冲区，以便稍后采样和训练：
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Sample a mini-batch of data from the replay buffer**: Once we have more than
    the mini-batch size of samples in the replay buffer, we sample a mini-batch of
    data from the buffer. For the subsequent state, `s2`, we use the critic''s target
    network to compute the target *Q* value and store it in `target_q`. Note the use
    of the critic''s target and not the critic—this is done for stability reasons.
    We then use the Bellman equation to evaluate the target, `y_i`, which is computed
    as *r + γ Q* for non-Terminal time steps and as *r* for Terminal steps:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**从重放缓冲区采样小批量数据**：一旦重放缓冲区中的样本数量超过小批量大小，我们就从缓冲区中采样一个小批量的数据。对于后续的状态`s2`，我们使用评论员的目标网络来计算目标*Q*值，并将其存储在`target_q`中。注意使用评论员的目标网络而不是评论员网络本身——这是出于稳定性的考虑。然后，我们使用贝尔曼方程来评估目标`y_i`，其计算为*r
    + γ Q*（对于非终止时间步）和*r*（对于终止时间步）：'
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Use the preceding to train the actor and critic**: We then train the critic
    for one step on the mini-batch by calling `critic.train()`. Then, we compute the
    gradient of *Q* with respect to the action by calling `critic.action_gradients()` and
    we store it in `grads`; note that this action gradient is used to compute the
    policy gradient, as we mentioned previously. We then train the actor for one step
    by calling `actor.train()` and passing `grads` as an argument, along with the
    state that we sampled from the replay buffer. Finally, we update the actor and
    critic target networks by calling the appropriate functions for the actor and
    critic objects:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用前述内容训练演员和评论员**：然后，我们通过调用`critic.train()`在小批量数据上训练评论员一步。接着，我们通过调用`critic.action_gradients()`计算*Q*相对于动作的梯度，并将其存储在`grads`中；请注意，这个动作梯度将用于计算策略梯度，正如我们之前提到的。然后，我们通过调用`actor.train()`并将`grads`作为参数，以及从重放缓冲区采样的状态，训练演员一步。最后，我们通过调用演员和评论员对象的相应函数更新演员和评论员的目标网络：'
- en: '[PRE18]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The new state, `s2`, is assigned to the current state, `s`, as we proceed to
    the next time step. If the episode has terminated, we print the episode reward
    and other observations on the screen, and we write them into a text file called
    `pendulum.txt` for later analysis. We also break out of the inner `for` loop,
    as the episode has terminated:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 新状态`s2`被分配给当前状态`s`，我们继续到下一个时间步。如果回合已经结束，我们将回合的奖励和其他观测值打印到屏幕上，并将它们写入名为`pendulum.txt`的文本文件，以便后续分析。由于回合已经结束，我们还会跳出内部`for`循环：
- en: '[PRE19]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**Defining testDDPG()**: This concludes the `trainDDPG()` function. We will
    now present the `testDDPG()` function that is used to test how well our model
    is performing. The `testDDPG()` function is more or less the same as `trainDDPG()`,
    except that we do not have a replay buffer and we do not train the neural networks.
    Like before, we have two `for` loops—the outer one for episodes, and the inner
    loop over time steps for each episode. We sample actions from the trained actor''s
    policy by using `actor.predict()` and use it to evolve the environment by using
    `env.step()`. Finally, we terminate the episode if `terminal == True`:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义 testDDPG()**：这就完成了 `trainDDPG()` 函数。接下来，我们将展示 `testDDPG()` 函数，用于测试我们模型的表现。`testDDPG()`
    函数与 `trainDDPG()` 函数基本相同，不同之处在于我们没有重放缓冲区，也不会训练神经网络。和之前一样，我们有两个 `for` 循环——外层循环控制回合数，内层循环遍历每个回合的时间步。我们通过
    `actor.predict()` 从训练好的演员策略中采样动作，并使用 `env.step()` 让环境按照动作演化。最后，如果 `terminal ==
    True`，我们终止当前回合：'
- en: '[PRE20]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This concludes `TrainOrTest.py`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 `TrainOrTest.py` 的全部内容。
- en: Coding replay_buffer.py
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写 replay_buffer.py
- en: 'We will use the deque data structure for storing our replay buffer. The steps
    that are involved are as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 deque 数据结构来存储我们的重放缓冲区。涉及的步骤如下：
- en: '**Import the packages**: First, we import the required packages.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**导入所需的包**：首先，我们导入所需的包。'
- en: '**Define the ReplayBuffer class**:We then define the `ReplayBuffer` class,
    with the arguments passed to the `__init__()` constructor. The `self.buffer =
    deque()` function is the instance of the data structure to store the data in a
    queue:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义 ReplayBuffer 类**：接着我们定义 `ReplayBuffer` 类，传递给 `__init__()` 构造函数的参数。`self.buffer
    = deque()` 函数是用来存储数据的队列实例：'
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Define** **the add** **and** **size** **functions**: We then define the `add()`
    function to add the experience as a tuple (`state`, `action`, `reward`, `terminal`,
    `new state`). The `self.count` function keeps a count of the number of samples
    we have in the replay buffer. If this count is less than the replay buffer size
    (`self.buffer_size`), we append the current experience to the buffer and increment
    the count. On the other hand, if the count is equal to (or greater than) the buffer
    size, we discard the old samples from the buffer by calling `popleft()`, which
    is a built-in function of deque. Then, we add the experience to the replay buffer;
    the count need not be incremented, as we discarded one old data sample in the
    replay buffer and replaced it with the new data sample or experience, so the total
    number of samples in the buffer remains the same. We also define the `size()`
    function to obtain the current size of the replay buffer:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义** **add** **和** **size** **函数**：接着我们定义 `add()` 函数，将经验作为元组（`state`，`action`，`reward`，`terminal`，`new
    state`）添加到缓冲区。`self.count` 函数用来记录重放缓冲区中样本的数量。如果样本数量小于重放缓冲区的大小（`self.buffer_size`），我们将当前经验添加到缓冲区，并递增计数。另一方面，如果计数等于（或大于）缓冲区大小，我们通过调用
    `popleft()`（deque 的内置函数）丢弃缓冲区中的旧样本。然后，我们将新的经验添加到重放缓冲区；不需要增加计数，因为我们已经丢弃了一个旧的数据样本，并用新数据样本或经验替代了它，因此缓冲区中的样本总数保持不变。我们还定义了
    `size()` 函数，用于获取当前重放缓冲区的大小：'
- en: '[PRE22]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '**Define** **the sample_batch** **and** **clear** **functions**: Next, we define
    the `sample_batch()` function to sample a `batch_size` number of samples from
    the replay buffer. If the count of the number of samples in the buffer is less
    than the `batch_size`, we sample count the number of samples from the buffer.
    Otherwise, we sample the `batch_size` number of samples from the replay buffer.
    Then, we convert these samples to `NumPy` arrays and return them. Lastly, the
    `clear()` function is used to completely clear the reply buffer and make it empty:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义** **sample_batch** **和** **clear** **函数**：接下来，我们定义 `sample_batch()` 函数，从重放缓冲区中采样
    `batch_size` 个样本。如果缓冲区中的样本数量小于 `batch_size`，我们就从缓冲区中采样所有样本的数量。否则，我们从重放缓冲区中采样 `batch_size`
    个样本。然后，我们将这些样本转换为 `NumPy` 数组并返回。最后，`clear()` 函数用于完全清空重放缓冲区，使其变为空：'
- en: '[PRE23]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: That concludes the code for the DDPG. We will now test it.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 DDPG 代码的全部内容。我们现在开始测试它。
- en: Training and testing the DDPG on Pendulum-v0
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Pendulum-v0 上训练和测试 DDPG
- en: 'We will now train the preceding DDPG code on Pendulum-v0\. To train the DDPG
    agent, simply type the following in the command line at the same level as the
    rest of the code:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将在 Pendulum-v0 上训练前面的 DDPG 代码。要训练 DDPG 代理，只需在与代码文件相同的目录下，在命令行中输入以下命令：
- en: '[PRE24]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This will start the training:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这将开始训练：
- en: '[PRE25]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Once the training is complete, you can also test the trained DDPG agent, as
    follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，你也可以测试训练好的DDPG智能体，如下所示：
- en: '[PRE26]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can also plot the episodic rewards during training by using the following
    code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过以下代码绘制训练过程中的每个回合奖励：
- en: '[PRE27]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The plot is presented as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 绘图如下所示：
- en: '![](img/0b31b8d7-d68d-4efe-9a3c-65afec2fa8e5.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b31b8d7-d68d-4efe-9a3c-65afec2fa8e5.png)'
- en: 'Figure 1: Plot showing the episode rewards during training for the Pendulum-v0
    problem, using the DDPG'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：在训练过程中，使用DDPG算法时Pendulum-v0问题的回报曲线
- en: As you can see, the DDPG agent has learned the problem very well. The maximum
    rewards are slightly negative, and this is the best for this problem.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，DDPG智能体已经非常好地学习了这个问题。最大奖励值略为负值，这对于这个问题来说是最好的结果。
- en: Summary
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we were introduced to our first continuous actions RL algorithm,
    DDPG, which also happens to be the first Actor-Critic algorithm in this book.
    DDPG is an off-policy algorithm, as it uses a replay buffer. We also covered the
    use of policy gradients to update the actor, and the use of the L2 norm to update
    the critic. Thus, we have two different neural networks. The actor learns the
    policy and the critic learns to evaluate the actor's policy, thereby providing
    a learning signal to the actor. You saw how to compute the gradient of the state-action
    value, *Q(s,a)*, with respect to the action, and also the gradient of the policy,
    both of which are combined to evaluate the policy gradient, which is then used
    to update the actor. We trained the DDPG on the inverted pendulum problem, and
    the agent learned it very well.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了第一个连续动作强化学习算法DDPG，它恰好也是本书中的第一个演员-评论家算法。DDPG是一个离策略算法，因为它使用了回放缓冲区。我们还讨论了使用策略梯度来更新演员，以及使用L2范数来更新评论家。因此，我们有两个不同的神经网络。演员学习策略，而评论家学习评估演员的策略，从而为演员提供学习信号。你学到了如何计算状态-动作值*Q(s,a)*相对于动作的梯度，以及策略的梯度，这两个梯度合并来评估策略梯度，然后用来更新演员。我们在倒立摆问题上训练了DDPG，智能体学得非常好。
- en: We have come a long way in this chapter. You have learned about Actor-Critic
    algorithms and how to code your first continuous control RL algorithm. In the
    next chapter, you will learn about the **A3C algorithm**, which is an on-policy
    deep RL algorithm.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们走了很长一段路。你已经了解了演员-评论家算法，并且学习了如何编写第一个连续控制强化学习算法。在下一章，你将学习**A3C算法**，它是一种在策略的深度强化学习算法。
- en: Questions
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Is the DDPG an on-policy or off-policy algorithm?
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DDPG是一个在策略算法还是离策略算法？
- en: We used the same neural network architectures for both the actor and the critic.
    Is this required, or can we choose different neural network architectures for
    the actor and the critic?
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为演员和评论家使用了相同的神经网络架构。这样做是必须的吗？还是我们可以为演员和评论家选择不同的神经网络架构？
- en: Can we use the DDPG for Atari Breakout?
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以用DDPG来玩Atari Breakout吗？
- en: Why are the biases of the neural networks initialized to small positive values?
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么神经网络的偏置初始化为小的正值？
- en: 'This is left as an exercise: Can you modify the code in this chapter to train
    an agent to learn InvertedDoublePendulum-v2, which is more challenging than the
    Pendulum-v0 that you saw in this chapter?'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是一个练习：你能修改本章中的代码来训练一个智能体，学习更具挑战性的InvertedDoublePendulum-v2问题吗？这个问题比本章中的Pendulum-v0要更具挑战性。
- en: 'Here is another exercise: Vary the neural network architecture and check whether
    the agent can learn the Pendulum-v0 problem. For instance, keep decreasing the
    number of neurons in the first hidden layer with the values 400, 100, 25, 10,
    5, and 1, and check how the agent performs for the different number of neurons
    in the first hidden layer. If the number of neurons is too small, it can lead
    to information bottlenecks, where the input of the network is not sufficiently
    represented; that is, the information is lost as we go deeper into the neural
    network. Do you observe this effect?'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里有另一个练习：改变神经网络架构，检查智能体是否能学会Pendulum-v0问题。例如，可以逐步减少第一层隐藏层中神经元的数量，使用值400、100、25、10、5和1，然后检查智能体在不同神经元数量下的表现。如果神经元数量太小，可能会导致信息瓶颈，即网络的输入没有得到充分表示；也就是说，随着网络的加深，信息会丢失。你观察到了这个效果吗？
- en: Further reading
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Continuous control with deep reinforcement learning*, by *Timothy P. Lillicrap*,
    *Jonathan J. Hunt*, *Alexander Pritzel*, *Nicolas Heess*, *Tom Erez*, *Yuval Tassa*,
    *David Silver*, and *Daan Wierstra*, original DDPG paper from *DeepMind*, arXiv:1509.02971:
    [https://arxiv.org/abs/1509.02971](https://arxiv.org/abs/1509.02971)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度强化学习中的连续控制*，作者：*Timothy P. Lillicrap*、*Jonathan J. Hunt*、*Alexander Pritzel*、*Nicolas
    Heess*、*Tom Erez*、*Yuval Tassa*、*David Silver* 和 *Daan Wierstra*，原始 DDPG 论文来自
    *DeepMind*，arXiv:1509.02971：[https://arxiv.org/abs/1509.02971](https://arxiv.org/abs/1509.02971)'
