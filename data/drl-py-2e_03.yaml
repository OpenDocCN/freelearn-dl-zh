- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: The Bellman Equation and Dynamic Programming
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝尔曼方程和动态规划
- en: In the previous chapter, we learned that in reinforcement learning our goal
    is to find the optimal policy. The optimal policy is the policy that selects the
    correct action in each state so that the agent can get the maximum return and
    achieve its goal. In this chapter, we'll learn about two interesting classic reinforcement
    learning algorithms called the value and policy iteration methods, which we can
    use to find the optimal policy.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习到，在强化学习中，我们的目标是找到最优策略。最优策略是在每个状态下选择正确的动作，使得智能体可以获得最大回报并实现其目标。在本章中，我们将学习两种有趣的经典强化学习算法——值迭代和策略迭代方法，我们可以使用它们来找到最优策略。
- en: Before diving into the value and policy iteration methods directly, first, we
    will learn about the Bellman equation. The Bellman equation is ubiquitous in reinforcement
    learning and it is used for finding the optimal value and Q functions. We will
    understand what the Bellman equation is and how it finds the optimal value and
    Q functions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在直接探讨值迭代和策略迭代方法之前，我们首先将学习贝尔曼方程。贝尔曼方程在强化学习中无处不在，广泛用于求解最优值函数和Q函数。我们将理解贝尔曼方程是什么，以及它如何找到最优值函数和Q函数。
- en: After understanding the Bellman equation, we will learn about two interesting
    dynamic programming methods called value and policy iterations, which use the
    Bellman equation to find the optimal policy. At the end of the chapter, we will
    learn how to solve the Frozen Lake problem by finding an optimal policy using
    the value and policy iteration methods.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解贝尔曼方程之后，我们将学习两种有趣的动态规划方法——值迭代和策略迭代，它们使用贝尔曼方程来寻找最优策略。在本章结束时，我们将学习如何通过使用值迭代和策略迭代方法来找到最优策略，解决冻结湖问题。
- en: 'In this chapter, we will learn about the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: The Bellman equation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝尔曼方程
- en: The Bellman optimality equation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝尔曼最优性方程
- en: The relationship between the value and Q functions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值函数和Q函数之间的关系
- en: Dynamic programming – value and policy iteration methods
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态规划 – 值迭代和策略迭代方法
- en: Solving the Frozen Lake problem using value and policy iteration
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用值迭代和策略迭代方法解决冻结湖问题
- en: The Bellman equation
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝尔曼方程
- en: The Bellman equation, named after Richard Bellman, helps us solve the **Markov
    decision process** (**MDP**). When we say solve the MDP, we mean finding the optimal
    policy.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程以理查德·贝尔曼（Richard Bellman）命名，帮助我们解决**马尔可夫决策过程**（**MDP**）。当我们说解决MDP时，我们指的是找到最优策略。
- en: As stated in the introduction of the chapter, the Bellman equation is ubiquitous
    in reinforcement learning and is widely used for finding the optimal value and
    Q functions recursively. Computing the optimal value and Q functions is very important
    because once we have the optimal value or optimal Q function, then we can use
    them to derive the optimal policy.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章引言中所述，贝尔曼方程在强化学习中无处不在，广泛用于递归地求解最优值函数和Q函数。计算最优值函数和Q函数非常重要，因为一旦我们得到了最优值函数或最优Q函数，我们就可以利用它们推导出最优策略。
- en: In this section, we'll learn what exactly the Bellman equation is and how we
    can use it to find the optimal value and Q functions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习贝尔曼方程究竟是什么，以及如何使用它来找到最优值函数和Q函数。
- en: The Bellman equation of the value function
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 值函数的贝尔曼方程
- en: 'The Bellman equation states that the value of a state can be obtained as a
    sum of the immediate reward and the discounted value of the next state. Say we
    perform an action *a* in state *s* and move to the next state ![](img/B15558_03_001.png)
    and obtain a reward *r*, then the Bellman equation of the value function can be
    expressed as:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程指出，状态的值可以通过即时奖励与下一个状态的折扣值之和来获得。假设我们在状态 *s* 执行动作 *a*，并移动到下一个状态 ![](img/B15558_03_001.png)
    并获得奖励 *r*，那么值函数的贝尔曼方程可以表示为：
- en: '![](img/B15558_03_002.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_002.png)'
- en: 'In the above equation, the following applies:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，以下内容适用：
- en: '![](img/B15558_03_003.png) implies the immediate reward obtained while performing
    an action *a* in state *s* and moving to the next state ![](img/B15558_03_004.png)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B15558_03_003.png) 表示在状态 *s* 执行动作 *a* 并移动到下一个状态时所获得的即时奖励 ![](img/B15558_03_004.png)'
- en: '![](img/B15558_03_005.png) is the discount factor'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_03_005.png) 是折扣因子'
- en: '![](img/B15558_03_006.png) implies the value of the next state'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_03_006.png) 表示下一个状态的值'
- en: 'Let''s understand the Bellman equation with an example. Say we generate a trajectory
    ![](img/B15558_03_007.png) using some policy ![](img/B15558_03_008.png):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解贝尔曼方程。假设我们使用某个策略 ![](img/B15558_03_008.png) 生成一条轨迹 ![](img/B15558_03_007.png)：
- en: '![](img/B15558_03_01.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_01.png)'
- en: 'Figure 3.1: Trajectory'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：轨迹
- en: 'Let''s suppose we need to compute the value of state *s*[2]. According to the
    Bellman equation, the value of state *s*[2] is given as:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们需要计算状态 *s*[2] 的值。根据贝尔曼方程，状态 *s*[2] 的值表示为：
- en: '![](img/B15558_03_009.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_009.png)'
- en: In the preceding equation, ![](img/B15558_03_010.png) implies the immediate
    reward we obtain while performing an action *a*[2] in state *s*[2] and moving
    to state *s*[3]. From the trajectory, we can tell that the immediate reward ![](img/B15558_03_011.png)
    is *r*[2]. And the term ![](img/B15558_03_012.png) is the discounted value of
    the next state.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，![](img/B15558_03_010.png) 表示我们在状态 *s*[2] 中执行动作 *a*[2] 并移动到状态 *s*[3]
    时获得的即时奖励。从轨迹中可以看出，即时奖励 ![](img/B15558_03_011.png) 为 *r*[2]。而项 ![](img/B15558_03_012.png)
    是下一个状态的折扣值。
- en: 'Thus, according to the Bellman equation, the value of state *s*[2] is given
    as:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据贝尔曼方程，状态 *s*[2] 的值表示为：
- en: '![](img/B15558_03_013.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_013.png)'
- en: 'Thus, the Bellman equation of the value function can be expressed as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，价值函数的贝尔曼方程可以表示为：
- en: '![](img/B15558_03_014.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_014.png)'
- en: Where the superscript ![](img/B15558_01_047.png) implies that we are using policy
    ![](img/B15558_03_016.png). The right-hand side term ![](img/B15558_03_017.png)
    is often called the **Bellman backup**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，上标 ![](img/B15558_01_047.png) 表示我们正在使用策略 ![](img/B15558_03_016.png)。右侧项 ![](img/B15558_03_017.png)
    通常被称为 **贝尔曼备份**。
- en: The preceding Bellman equation works only when we have a deterministic environment.
    Let's suppose our environment is stochastic, then in that case, when we perform
    an action *a* in state *s*, it is not guaranteed that our next state will always
    be ![](img/B15558_03_018.png); it could be some other states too. For instance,
    look at the trajectory in *Figure 3.2*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上述贝尔曼方程仅在我们拥有确定性环境时有效。假设我们的环境是随机的，那么在这种情况下，当我们在状态 *s* 中执行动作 *a* 时，并不能保证我们的下一个状态总是
    ![](img/B15558_03_018.png)，它也可能是其他状态。例如，看看 *图3.2* 中的轨迹。
- en: 'As we can see, when we perform an action *a*[1] in state *s*[1], with a probability
    0.7, we reach state s[2], and with a probability 0.3, we reach state s[3]:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，当我们在状态 *s*[1] 中执行动作 *a*[1] 时，以 0.7 的概率到达状态 s[2]，以 0.3 的概率到达状态 s[3]：
- en: '![](img/B15558_03_02.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_02.png)'
- en: 'Figure 3.2: Transition probability of performing action *a*[1] in state *s*[1]'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：在状态 *s*[1] 中执行动作 *a*[1] 的转移概率
- en: Thus, when we perform action *a*[1] in state *s*[1], there is a 70% chance the
    next state will be *s*[2] and a 30% chance the next state will be *s*[3]. We learned
    that the Bellman equation is a sum of immediate reward and the discounted value
    of the next state. But when our next state is not guaranteed due to the stochasticity
    present in the environment, how can we define our Bellman equation?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们在状态 *s*[1] 中执行动作 *a*[1] 时，下一个状态为 *s*[2] 的概率为 70%，为 *s*[3] 的概率为 30%。我们了解到，贝尔曼方程是即时奖励和下一个状态的折扣值之和。但是，当由于环境中的随机性，无法保证下一个状态时，我们该如何定义贝尔曼方程呢？
- en: 'In this case, we can slightly modify our Bellman equation with the expectations
    (the weighted average), that is, a sum of the Bellman backup multiplied by the
    corresponding transition probability of the next state:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以稍微修改贝尔曼方程，引入期望值（加权平均），也就是说，将贝尔曼备份与下一个状态的转移概率相乘，再求和：
- en: '![](img/B15558_03_019.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_019.png)'
- en: 'In the preceding equation, the following applies:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，以下内容适用：
- en: '![](img/B15558_03_020.png) denotes the transition probability of reaching ![](img/B15558_03_021.png)
    by performing an action *a* in state *s*'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_03_020.png) 表示通过在状态 *s* 中执行动作 *a* 到达 ![](img/B15558_03_021.png)
    的转移概率'
- en: '![](img/B15558_03_022.png) denotes the Bellman backup'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_03_022.png) 表示贝尔曼备份'
- en: 'Let''s understand this equation better by considering the same trajectory we
    just used. As we notice, when we perform an action *a*[1] in state *s*[1], we
    go to *s*[2] with a probability of 0.70 and *s*[3] with a probability of 0.30\.
    Thus, we can write:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过考虑刚才使用的相同轨迹来更好地理解这个方程。正如我们所注意到的，当我们在状态 *s*[1] 中执行动作 *a*[1] 时，我们以 0.70 的概率到达
    *s*[2]，以 0.30 的概率到达 *s*[3]。因此，我们可以写成：
- en: '![](img/B15558_03_023.png)![](img/B15558_03_024.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_023.png)![](img/B15558_03_024.png)'
- en: 'Thus, the Bellman equation of the value function including the stochasticity
    present in the environment using the expectation (weighted average) is expressed
    as:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，包含环境中的随机性，利用期望（加权平均）的价值函数的贝尔曼方程表达为：
- en: '![](img/B15558_03_019.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_019.png)'
- en: 'Okay, but what if our policy is a stochastic policy? We learned that with a
    stochastic policy, we select actions based on a probability distribution; that
    is, instead of performing the same action in a state, we select an action based
    on the probability distribution over the action space. Let''s understand this
    with a different trajectory, shown in *Figure 3.3*. As we see, in state *s*[1],
    with a probability of 0.8, we select action *a*[1] and reach state *s*[2], and
    with a probability of 0.2, we select action *a*[2] and reach state *s*[3]:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，但如果我们的策略是一个随机策略呢？我们了解到，使用随机策略时，我们是基于概率分布来选择动作；也就是说，在某个状态下，不是执行相同的动作，而是根据动作空间中的概率分布选择一个动作。让我们通过一个不同的轨迹来理解这一点，如*图
    3.3*所示。正如我们所见，在状态*s*[1]下，以0.8的概率选择动作*a*[1]，并到达状态*s*[2]，以0.2的概率选择动作*a*[2]，并到达状态*s*[3]：
- en: '![](img/B15558_03_03.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_03.png)'
- en: 'Figure 3.3: Trajectory using a stochastic policy'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3：使用随机策略的轨迹
- en: Thus, when we use a stochastic policy, our next state will not always be the
    same; it will be different states with some probability. Now, how can we define
    the Bellman equation including the stochastic policy?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们使用随机策略时，下一状态将不再是固定的，它会是多个状态中的某一个，且具有一定的概率。那么，如何定义包括随机策略的贝尔曼方程呢？
- en: We learned that to include the stochasticity present in the environment in the
    Bellman equation, we took the expectation (the weighted average), that is, a sum
    of the Bellman backup multiplied by the corresponding transition probability of
    the next state.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们了解到，要将环境中的随机性包含进贝尔曼方程中，我们采取了期望（加权平均），也就是说，贝尔曼备份的总和乘以下一个状态的对应转移概率。
- en: Similarly, to include the stochastic nature of the policy in the Bellman equation,
    we can use the expectation (the weighted average), that is, a sum of the Bellman
    backup multiplied by the corresponding probability of action.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似地，要将策略的随机性包含在贝尔曼方程中，我们可以使用期望（加权平均），也就是说，将贝尔曼备份乘以相应的动作概率的总和。
- en: 'Thus, our final Bellman equation of the value function can be written as:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们最终的价值函数贝尔曼方程可以写为：
- en: '![](img/B15558_03_026.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_026.png)'
- en: 'The preceding equation is also known as the **Bellman expectation equation**
    of the value function. We can also express the above equation in expectation form.
    Let''s recollect the definition of expectation:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程也被称为**贝尔曼期望方程**，是价值函数的贝尔曼期望方程。我们也可以将上面的方程表达为期望形式。让我们回顾一下期望的定义：
- en: '![](img/B15558_03_027.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_027.png)'
- en: In equation (1), ![](img/B15558_03_028.png) and ![](img/B15558_03_029.png) and
    ![](img/B15558_03_030.png) which denote the probability of the stochastic environment
    and stochastic policy, respectively.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程(1)中，![](img/B15558_03_028.png)、![](img/B15558_03_029.png)和![](img/B15558_03_030.png)分别表示随机环境和随机策略的概率。
- en: 'Thus, we can write the Bellman equation of the value function as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将价值函数的贝尔曼方程写为：
- en: '![](img/B15558_03_031.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_031.png)'
- en: The Bellman equation of the Q function
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q函数的贝尔曼方程
- en: 'Now, let''s learn how to compute the Bellman equation of the state-action value
    function, that is, the Q function. The Bellman equation of the Q function is very
    similar to the Bellman equation of the value function except for a small difference.
    Similar to the Bellman equation of the value function, the Bellman equation of
    the Q function states that the Q value of a state-action pair can be obtained
    as a sum of the immediate reward and the discounted Q value of the next state-action
    pair:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何计算状态-动作价值函数的贝尔曼方程，也就是Q函数。Q函数的贝尔曼方程与价值函数的贝尔曼方程非常相似，除了一个小的区别。与价值函数的贝尔曼方程类似，Q函数的贝尔曼方程指出，某个状态-动作对的Q值可以通过即时奖励与下一个状态-动作对的折扣Q值的总和得到：
- en: '![](img/B15558_03_032.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_032.png)'
- en: 'In the preceding equation, the following applies:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，适用以下内容：
- en: '![](img/B15558_03_033.png) implies the immediate reward obtained while performing
    an action *a* in state *s* and moving to the next state ![](img/B15558_03_034.png)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B15558_03_033.png)表示在状态*s*下执行动作*a*并移动到下一个状态时获得的即时奖励！[](img/B15558_03_034.png)'
- en: '![](img/B15558_03_035.png) is the discount factor'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_03_035.png)是折扣因子。'
- en: '![](img/B15558_03_036.png) is the Q value of the next state-action pair'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_03_036.png) 是下一个状态-动作对的Q值'
- en: 'Let''s understand this with an example. Say we generate a trajectory ![](img/B15558_03_037.png)
    using some policy ![](img/B15558_03_038.png) as shown in *Figure 3.4*:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解这个问题。假设我们使用某个策略 ![](img/B15558_03_038.png) 生成一个轨迹 ![](img/B15558_03_037.png)，如图*3.4*所示：
- en: '![](img/B15558_03_04.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_04.png)'
- en: 'Figure 3.4: Trajectory'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：轨迹
- en: 'Let''s suppose we need to compute the Q value of a state-action pair (*s*[2],
    *a*[2]). Then, according to the Bellman equation, we can write:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们需要计算状态-动作对(*s*[2], *a*[2])的Q值。那么，根据Bellman方程，我们可以写出：
- en: '![](img/B15558_03_039.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_039.png)'
- en: 'In the above equation, *R*(*s*[2], *a*[2], *a*[3]) represents the immediate
    reward we obtain while performing an action *a*[2] in state *s*[2] and moving
    to state *s*[3]. From the preceding trajectory, we can tell that the immediate
    reward *R*(*s*[2], *a*[2], *s*[3]) is *r*[2]. And the term ![](img/B15558_03_040.png)
    represents the discounted Q value of the next state-action pair. Thus:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，*R*(*s*[2], *a*[2], *a*[3])表示我们在执行动作*a*[2]时，从状态*s*[2]转移到状态*s*[3]所获得的即时奖励。从前面的轨迹中，我们可以看出，即时奖励*R*(*s*[2],
    *a*[2], *s*[3])是*r*[2]。而项 ![](img/B15558_03_040.png) 表示下一个状态-动作对的折扣Q值。因此：
- en: '![](img/B15558_03_041.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_041.png)'
- en: 'Thus, the Bellman equation for the Q function can be expressed as:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Q函数的Bellman方程可以表示为：
- en: '![](img/B15558_03_042.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_042.png)'
- en: Where the superscript ![](img/B15558_03_038.png) implies that we are using the
    policy ![](img/B15558_03_038.png) and the right-hand side term ![](img/B15558_03_045.png)
    is the **Bellman backup**.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，上标 ![](img/B15558_03_038.png) 表示我们正在使用策略 ![](img/B15558_03_038.png)，右侧项 ![](img/B15558_03_045.png)
    是**Bellman备份**。
- en: Similar to what we learned in the Bellman equation of the value function, the
    preceding Bellman equation works only when we have a deterministic environment
    because in the stochastic environment our next state will not always be the same
    and it will be based on a probability distribution. Suppose we have a stochastic
    environment, then when we perform an action *a* in state *s*. It is not guaranteed
    that our next state will always be ![](img/B15558_03_046.png); it could be some
    other states too with some probability.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们在值函数的Bellman方程中学到的内容，上述Bellman方程仅在我们拥有确定性环境时有效，因为在随机环境中我们的下一个状态不总是相同的，它将基于一个概率分布。假设我们有一个随机环境，那么当我们在状态*s*中执行动作*a*时，并不能保证我们的下一个状态总是
    ![](img/B15558_03_046.png)；它也有可能是其他状态，且具有一定的概率。
- en: 'So, just like we did in the previous section, we can use the expectation (the
    weighted average), that is, a sum of the Bellman backup multiplied by their corresponding
    transition probability of the next state, and rewrite our Bellman equation of
    the Q function as:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，就像我们在上一节中所做的那样，我们可以使用期望（加权平均），即Bellman备份的和乘以下一个状态的相应转移概率，重新写出Q函数的Bellman方程：
- en: '![](img/B15558_03_047.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_047.png)'
- en: 'Similarly, when we use a stochastic policy, our next state will not always
    be the same; it will be different states with some probability. So, to include
    the stochastic nature of the policy, we can rewrite our Bellman equation with
    the expectation (the weighted average), that is, a sum of Bellman backup multiplied
    by the corresponding probability of action, just like we did in the Bellman equation
    of the value function. Thus, the Bellman equation of the Q function is given as:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，当我们使用随机策略时，我们的下一个状态不一定总是相同的；它将是具有一定概率的不同状态。所以，为了包括策略的随机性，我们可以通过期望（加权平均）的方式重新写出Bellman方程，即Bellman备份的和乘以相应的动作概率，就像我们在值函数的Bellman方程中所做的那样。因此，Q函数的Bellman方程可以表示为：
- en: '![](img/B15558_03_048.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_048.png)'
- en: But wait! There is a small change in the above equation. Why do we need to add
    the term ![](img/B15558_03_049.png) in the case of a Q function? Because in the
    value function *V*(*s*), we are given only a state *s* and we choose an action
    *a* based on the policy ![](img/B15558_03_050.png). So, we added the term ![](img/B15558_03_049.png)
    to include the stochastic nature of the policy. But in the case of the Q function
    *Q*(*s*, *a*), we will be given both state *s* and action *a*, so we don't need
    to add the term ![](img/B15558_03_049.png) in our equation since we are not selecting
    any action *a* based on the policy ![](img/B15558_03_050.png).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 等等！上述方程有一个小变化。为什么在Q函数的情况下需要添加项![](img/B15558_03_049.png)？因为在值函数 *V*(*s*) 中，我们只给定一个状态
    *s*，并根据策略![](img/B15558_03_050.png)选择一个动作 *a*。因此，我们添加了项![](img/B15558_03_049.png)来包括策略的随机性。但在Q函数
    *Q*(*s*, *a*) 的情况下，我们既给定了状态 *s*，又给定了动作 *a*，因此我们不需要在方程中添加项![](img/B15558_03_049.png)，因为我们并不是根据策略![](img/B15558_03_050.png)来选择任何动作
    *a*。
- en: 'However, if you look at the above equation, we need to select action ![](img/B15558_03_054.png)
    based on the policy ![](img/B15558_03_055.png) while computing the Q value of
    the next state-action pair ![](img/B15558_03_056.png) since ![](img/B15558_03_057.png)
    will not be given. So, we can just place the term ![](img/B15558_03_058.png) before
    the Q value of the next state-action pair. Thus, our final Bellman equation of
    the Q function can be written as:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你查看上面的方程，我们需要根据策略![](img/B15558_03_055.png)选择动作![](img/B15558_03_054.png)，以计算下一个状态-动作对![](img/B15558_03_056.png)的Q值，因为![](img/B15558_03_057.png)不会给定。因此，我们可以将项![](img/B15558_03_058.png)放置在下一个状态-动作对的Q值之前。这样，我们的最终贝尔曼Q函数方程可以写为：
- en: '![](img/B15558_03_059.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_059.png)'
- en: 'Equation (3) is also known as the Bellman expectation equation of the Q function.
    We can also express the equation (3) in expectation form as:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 方程（3）也被称为Q函数的贝尔曼期望方程。我们还可以将方程（3）表示为期望形式：
- en: '![](img/B15558_03_060.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_060.png)'
- en: Now that we have understood what the Bellman expectation equation is, in the
    next section, we will learn about the Bellman optimality equation and explore
    how it is useful for finding the optimal Bellman value and Q functions.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了贝尔曼期望方程是什么，在下一节中，我们将学习贝尔曼最优性方程，并探讨它如何有助于找到最优的贝尔曼值和Q函数。
- en: The Bellman optimality equation
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝尔曼最优性方程
- en: 'The Bellman optimality equation gives the optimal Bellman value and Q functions.
    First, let''s look at the optimal Bellman value function. We learned that the
    Bellman equation of the value function is expressed as:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼最优性方程给出了最优贝尔曼值和Q函数。首先，让我们看看最优贝尔曼值函数。我们已经学过，值函数的贝尔曼方程可以表示为：
- en: '![](img/B15558_03_061.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_061.png)'
- en: In the first chapter, we learned that the value function depends on the policy,
    that is, the value of the state varies based on the policy we choose. There can
    be many different value functions according to different policies. The optimal
    value function, ![](img/B15558_03_062.png) is the one that yields the maximum
    value compared to all the other value functions. Similarly, there can be many
    different Bellman value functions according to different policies. The optimal
    Bellman value function is the one that has the maximum value.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们学习了值函数依赖于策略，即状态的值根据我们选择的策略而变化。根据不同的策略，可能会有很多不同的值函数。最优值函数，![](img/B15558_03_062.png)，是相较于所有其他值函数，能够产生最大值的函数。同样，根据不同的策略，也可能会有许多不同的贝尔曼值函数。最优贝尔曼值函数是具有最大值的那个。
- en: Okay, how can we compute the optimal Bellman value function that has the maximum
    value?
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，如何计算具有最大值的最优贝尔曼值函数呢？
- en: We can compute the optimal Bellman value function by selecting the action that
    gives the maximum value. But we don't know which action gives the maximum value,
    so, we compute the value of state using all possible actions, and then we select the
    maximum value as the value of the state.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过选择产生最大值的动作来计算最优贝尔曼值函数。但是我们不知道哪个动作能产生最大值，所以我们计算所有可能动作下的状态值，然后选择最大值作为该状态的值。
- en: 'That is, instead of using some policy ![](img/B15558_01_047.png) to select
    the action, we compute the value of the state using all possible actions, and
    then we select the maximum value as the value of the state. Since we are not using
    any policy, we can remove the expectation over the policy ![](img/B15558_03_055.png)
    and add the max over the action and express our optimal Bellman value function
    as:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们不再使用某个策略 ![](img/B15558_01_047.png) 来选择行动，而是通过计算所有可能行动的状态值，然后选择最大值作为状态的值。由于我们没有使用任何策略，因此可以去掉对策略的期望
    ![](img/B15558_03_055.png)，并对行动取最大值，将最优贝尔曼值函数表示为：
- en: '![](img/B15558_03_065.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_065.png)'
- en: 'It''s just the same as the Bellman equation, except here we are taking a maximum
    over all the possible actions instead of the expectation (weighted average) over
    the policy since we are only interested in the maximum value. Let''s understand
    this with an example. Say we are in a state *s* and we have two possible actions
    in the state. Let the actions be 0 and 1\. Then ![](img/B15558_03_066.png) is
    given as:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这与贝尔曼方程是一样的，只不过这里我们对所有可能的行动取最大值，而不是对策略取期望（加权平均），因为我们只关心最大值。让我们通过一个例子来理解这一点。假设我们在状态
    *s* 中，并且该状态下有两个可能的行动。假设这两个行动分别是 0 和 1。那么 ![](img/B15558_03_066.png) 由以下公式给出：
- en: '![](img/B15558_03_067.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_067.png)'
- en: As we can observe from the above equation, we compute the state value using
    all possible actions (0 and 1) and then select the maximum value as the value
    of the state.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述公式中可以观察到，我们使用所有可能的行动（0 和 1）来计算状态值，然后选择最大值作为状态的值。
- en: 'Now, let''s look at the optimal Bellman Q function. We learned that the Bellman
    equation of the Q function is expressed as:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下最优贝尔曼 Q 函数。我们已经学到，Q 函数的贝尔曼方程表示为：
- en: '![](img/B15558_03_068.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_068.png)'
- en: 'Just like we learned with the optimal Bellman value function, instead of using
    the policy to select action ![](img/B15558_03_069.png) in the next state ![](img/B15558_03_070.png),
    we choose all possible actions in that state ![](img/B15558_03_004.png) and compute
    the maximum Q value. It can be expressed as:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在学习最优贝尔曼值函数时一样，**我们不再通过策略来选择在下一个状态** ![](img/B15558_03_069.png) **的行动**
    ![](img/B15558_03_070.png)，**而是选择在该状态** ![](img/B15558_03_004.png) **下所有可能的行动，并计算最大
    Q 值**。可以表示为：
- en: '![](img/B15558_03_072.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_072.png)'
- en: 'Let''s understand this with an example. Say we are in a state *s* with an action
    *a*. We perform action *a* in state *s* and reach the next state ![](img/B15558_03_073.png).
    We need to compute the Q value for the next state ![](img/B15558_03_018.png).
    There can be many actions in state ![](img/B15558_03_018.png). Let''s say we have
    two actions 0 and 1 in state ![](img/B15558_03_034.png). Then we can write the
    optimal Bellman Q function as:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解这一点。假设我们在状态 *s* 中，并采取了一个行动 *a*。我们在状态 *s* 中执行行动 *a*，到达下一个状态 ![](img/B15558_03_073.png)。我们需要计算下一个状态的
    Q 值 ![](img/B15558_03_018.png)。在状态 ![](img/B15558_03_018.png) 中可能有多种行动。假设我们在状态
    ![](img/B15558_03_034.png) 中有两个行动 0 和 1，那么我们可以将最优贝尔曼 Q 函数写为：
- en: '![](img/B15558_03_077.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_077.png)'
- en: 'Thus, to summarize, the Bellman optimality equations of the value function
    and Q function are:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总结一下，值函数和 Q 函数的贝尔曼最优性方程是：
- en: '![](img/B15558_03_078.png)![](img/B15558_03_079.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_078.png)![](img/B15558_03_079.png)'
- en: 'We can also expand the expectation and rewrite the preceding Bellman optimality
    equations as:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以展开期望，并将前面的贝尔曼最优性方程改写为：
- en: '![](img/B15558_03_080.png)![](img/B15558_03_081.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_080.png)![](img/B15558_03_081.png)'
- en: The relationship between the value and Q functions
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 值函数与 Q 函数之间的关系
- en: 'Let''s take a little detour and recap the value and Q functions we covered
    in *Chapter 1*, *Fundamentals of Reinforcement Learning*. We learned that the
    value of a state (value function) denotes the expected return starting from that
    state following a policy ![](img/B15558_03_082.png):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微绕一下，回顾一下我们在*第一章*《强化学习基础》中学到的值函数和 Q 函数。我们学到，状态的值（值函数）表示从该状态开始，按照某个策略 ![](img/B15558_03_082.png)
    进行的期望回报：
- en: '![](img/B15558_03_083.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_083.png)'
- en: 'Similarly, the Q value of a state-action pair (Q function) represents the expected
    return starting from that state-action pair following a policy ![](img/B15558_03_084.png):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，状态-行动对的 Q 值（Q 函数）表示从该状态-行动对开始，按照某个策略 ![](img/B15558_03_084.png) 进行的期望回报：
- en: '![](img/B15558_03_085.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_085.png)'
- en: 'We learned that the optimal value function gives the maximum state-value:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学到，最优值函数给出了最大的状态值：
- en: '![](img/B15558_03_086.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_086.png)'
- en: 'And the optimal Q function gives the maximum state-action value (Q value):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最优Q函数给出了最大的状态-动作值（Q值）：
- en: '![](img/B15558_03_087.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_087.png)'
- en: 'Can we derive some relation between the optimal value function and optimal
    Q function? We know that the optimal value function has the maximum expected return
    when we start from a state *s* and the optimal Q function has the maximum expected
    return when we start from state *s* performing some action *a*. So, we can say
    that the optimal value function is the maximum of optimal Q value over all possible
    actions, and it can be expressed as follows (that is, we can derive V from Q):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否推导出最优值函数与最优Q函数之间的关系？我们知道，当我们从一个状态*s*开始时，最优值函数具有最大的期望回报，而最优Q函数在我们从状态*s*执行某个动作*a*时，具有最大的期望回报。因此，我们可以说，最优值函数是所有可能动作中最优Q值的最大值，可以表示为以下形式（即，我们可以从Q推导出V）：
- en: '![](img/B15558_03_088.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_088.png)'
- en: 'Alright, now let''s get back to our Bellman equations. Before going ahead,
    let''s just recap the Bellman equations:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在让我们回到贝尔曼方程。在继续之前，我们先回顾一下贝尔曼方程：
- en: '**Bellman expectation equation of the value function and Q function**:'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贝尔曼期望方程的值函数和Q函数**：'
- en: '![](img/B15558_03_089.png)'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B15558_03_089.png)'
- en: '![](img/B15558_03_090.png)'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B15558_03_090.png)'
- en: '**Bellman** **optimality** **equation of the value function and Q function**:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贝尔曼** **最优性** **方程的值函数和Q函数**：'
- en: '![](img/B15558_03_091.png)'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B15558_03_091.png)'
- en: '![](img/B15558_03_092.png)'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B15558_03_092.png)'
- en: 'We learned that the optimal Bellman Q function is expressed as:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到的是，最优贝尔曼Q函数可以表示为：
- en: '![](img/B15558_03_092.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_092.png)'
- en: 'If we have an optimal value function ![](img/B15558_03_094.png), then we can
    use it to derive the preceding optimal Bellman Q function, (that is, we can derive
    *Q* from *V*):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个最优值函数 ![](img/B15558_03_094.png)，那么我们可以使用它来推导出之前的最优贝尔曼Q函数（即，我们可以从*V*推导出*Q*）：
- en: '![](img/B15558_03_095.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_095.png)'
- en: The preceding equation is one of the most useful identities in reinforcement
    learning, and we will see how it will help us in finding the optimal policy in
    the upcoming section.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程是强化学习中最有用的恒等式之一，我们将在接下来的章节中看到它如何帮助我们找到最优策略。
- en: 'Thus, to summarize, we learned that we can derive *V* from *Q*:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总结一下，我们学到的是可以从*Q*推导出*V*：
- en: '![](img/B15558_03_096.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_096.png)'
- en: 'And derive *Q* from *V*:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 并从*V*推导出*Q*：
- en: '![](img/B15558_03_097.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_097.png)'
- en: 'Substituting equation (8) in equation (7), we can write:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 将方程（8）代入方程（7），我们可以写成：
- en: '![](img/B15558_03_080.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_080.png)'
- en: As we can observe, we just obtained the optimal Bellman value function. Now
    that we understand the Bellman equation and the relationship between the value
    and the Q function, we can move on to the next section on how to make use of these
    equations to find the optimal policy.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所观察到的，我们刚刚获得了最优贝尔曼值函数。现在我们理解了贝尔曼方程以及值函数与Q函数之间的关系，接下来我们可以继续学习如何利用这些方程来找到最优策略。
- en: Dynamic programming
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态规划
- en: '**Dynamic programming** (**DP**) is a technique for solving complex problems.
    In DP, instead of solving a complex problem as a whole, we break the problem into
    simple sub-problems, then for each sub-problem, we compute and store the solution.
    If the same subproblem occurs, we don''t recompute; instead, we use the already
    computed solution. Thus, DP helps in drastically minimizing the computation time.
    It has its applications in a wide variety of fields including computer science,
    mathematics, bioinformatics, and so on.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态规划**（**DP**）是一种解决复杂问题的技术。在动态规划中，我们不是将复杂问题作为一个整体来解决，而是将问题分解为简单的子问题，然后对于每个子问题，我们计算并存储解决方案。如果出现相同的子问题，我们就不再重新计算，而是使用已计算的结果。因此，动态规划有助于大幅减少计算时间。它在计算机科学、数学、生物信息学等广泛领域都有应用。'
- en: 'Now, we will learn about two important methods that use DP to find the optimal
    policy. The two methods are:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将学习两种使用动态规划来寻找最优策略的重要方法。这两种方法是：
- en: Value iteration
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值迭代
- en: Policy iteration
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略迭代
- en: Note that dynamic programming is a model-based method meaning that it will help
    us to find the optimal policy only when the model dynamics (transition probability)
    of the environment are known. If we don't have the model dynamics, we cannot apply
    DP methods.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，动态规划是一种基于模型的方法，这意味着它只有在已知环境的模型动态（转移概率）的情况下，才能帮助我们找到最优策略。如果我们没有模型动态，就无法应用动态规划方法。
- en: The upcoming sections are explained with manual calculations, for a better understanding,
    follow along with a pen and paper.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分将通过手动计算进行说明，为了更好的理解，建议用纸笔跟着一起做。
- en: Value iteration
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 值迭代
- en: 'In the value iteration method, we try to find the optimal policy. We learned
    that the optimal policy is the one that tells the agent to perform the correct
    action in each state. In order to find the optimal policy, first, we compute the
    optimal value function and once we have the optimal value function, we can use
    it to derive the optimal policy. Okay, how can we compute the optimal value function?
    We can use our optimal Bellman equation of the value function. We learned that,
    according to the Bellman optimality equation, the optimal value function can be
    computed as:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在值迭代方法中，我们试图找到最优策略。我们学习到，最优策略是指示代理在每个状态下执行正确动作的策略。为了找到最优策略，首先需要计算出最优值函数，一旦得到了最优值函数，我们可以利用它推导出最优策略。那么，如何计算最优值函数呢？我们可以使用最优贝尔曼方程来计算值函数。我们学习到，根据贝尔曼最优性方程，最优值函数可以计算为：
- en: '![](img/B15558_03_099.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_099.png)'
- en: 'In the *The relationship between the value and Q functions* section, we learned
    that given the value function, we can derive the Q function:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在*值函数与Q函数之间的关系*部分，我们学习了在给定值函数的情况下，如何推导Q函数：
- en: '![](img/B15558_03_100.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_100.png)'
- en: 'Substituting (10) in (9), we can write:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 将（10）代入（9），我们可以写为：
- en: '![](img/B15558_03_088.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_088.png)'
- en: Thus, we can compute the optimal value function by just taking the maximum over
    the optimal Q function. So, in order to compute the value of a state, we compute
    the Q value for all state-action pairs. Then, we select the maximum Q value as
    the value of the state.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以通过仅对最优Q函数取最大值来计算最优值函数。因此，为了计算某一状态的值，我们需要计算所有状态-动作对的Q值。然后，我们选择最大的Q值作为该状态的值。
- en: 'Let''s understand this with an example. Say we have two states, *s*[0] and
    *s*[1], and we have two possible actions in these states; let the actions be 0
    and 1\. First, we compute the Q value for all possible state-action pairs. *Table
    3.1* shows the Q values for all possible state-action pairs:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解这一点。假设我们有两个状态，*s*[0]和*s*[1]，在这些状态下我们有两个可能的动作；假设这些动作是0和1。首先，我们计算所有可能状态-动作对的Q值。*表3.1*显示了所有可能状态-动作对的Q值：
- en: '![](img/B15558_03_05.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_05.png)'
- en: 'Table 3.1: Q values of all possible state-action pairs'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1：所有可能状态-动作对的Q值
- en: 'Then, in each state, we select the maximum Q value as the optimal value of
    a state. Thus, the value of state *s*[0] is 3 and the value of state *s*[1] is
    4\. The optimal value of the state (value function) is shown *Table 3.2*:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在每个状态中，我们选择最大的Q值作为该状态的最优值。因此，状态*s*[0]的值为3，状态*s*[1]的值为4。状态的最优值（值函数）如*表3.2*所示：
- en: '![](img/B15558_03_06.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_06.png)'
- en: 'Table 3.2: Optimal state values'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2：最优状态值
- en: Once we obtain the optimal value function, we can use it to extract the optimal
    policy.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了最优值函数，就可以使用它来提取最优策略。
- en: Now that we have a basic understanding of how the value iteration method finds
    the optimal value function, in the next section, we will go into detail and learn
    how exactly the value iteration method works and how it finds the optimal policy
    from the optimal value function.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对值迭代方法如何找到最优值函数有了基本的理解。在下一部分，我们将详细了解值迭代方法是如何工作的，以及它如何从最优值函数中找到最优策略。
- en: The value iteration algorithm
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 值迭代算法
- en: 'The algorithm of value iteration is given as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 值迭代的算法如下：
- en: Compute the optimal value function by taking the maximum over the Q function,
    that is, ![](img/B15558_03_088.png)
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对Q函数取最大值来计算最优值函数，即！[](img/B15558_03_088.png)
- en: Extract the optimal policy from the computed optimal value function
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从计算出的最优值函数中提取最优策略
- en: 'Let''s go into detail and learn exactly how the above two steps work. For better
    understanding, let''s perform the value iteration manually. Consider the small
    grid world environment shown in *Figure 3.5*. Let''s say we are in state **A**
    and our goal is to reach state **C** without visiting the shaded state **B**,
    and say we have two actions, 0—left/right, and 1—up/down:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解并学习上述两个步骤是如何工作的。为了更好的理解，我们手动执行值迭代。考虑*图3.5*中显示的小型网格世界环境。假设我们处于状态**A**，我们的目标是到达状态**C**，并且不经过阴影状态**B**，假设我们有两个动作，0——左/右，和1——上/下：
- en: '![](img/B15558_03_07.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_07.png)'
- en: 'Figure 3.5: Grid world environment'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5：网格世界环境
- en: Can you think of what the optimal policy is here? The optimal policy here is
    the one that tells us to perform action 1 in state **A** so that we can reach
    **C** without visiting **B**. Now we will see how to find this optimal policy
    using value iteration.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你能想出这里的最优策略是什么吗？这里的最优策略是告诉我们在状态**A**执行动作 1，这样我们就可以到达**C**而不经过**B**。现在我们将看到如何使用值迭代找到这个最优策略。
- en: '*Table 3.3* shows the model dynamics of state **A**:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 3.3* 显示了状态**A**的模型动态：'
- en: '![](img/B15558_03_08.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_08.png)'
- en: 'Table 3.3: Model dynamics of state A'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.3：状态 A 的模型动态
- en: Step 1 – Compute the optimal value function
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 1 – 计算最优值函数
- en: We can compute the optimal value function by computing the maximum over the
    Q function.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过计算 Q 函数的最大值来计算最优值函数。
- en: '![](img/B15558_03_088.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_088.png)'
- en: That is, we compute the Q value for all state-action pairs and then we select
    the maximum Q value as the value of a state.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们计算所有状态-动作对的 Q 值，然后选择最大 Q 值作为状态的值。
- en: 'The Q value for a state *s* and action *a* can be computed as:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 状态 *s* 和动作 *a* 的 Q 值可以计算为：
- en: '![](img/B15558_03_104.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_104.png)'
- en: 'For notation simplicity, we can denote ![](img/B15558_03_105.png) by ![](img/B15558_03_106.png)
    and ![](img/B15558_03_107.png) by ![](img/B15558_03_108.png)and rewrite the preceding
    equation as:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化符号，我们可以将 ![](img/B15558_03_105.png) 表示为 ![](img/B15558_03_106.png)，将 ![](img/B15558_03_107.png)
    表示为 ![](img/B15558_03_108.png)，并将前面的公式重写为：
- en: '![](img/B15558_03_109.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_109.png)'
- en: Thus, using the preceding equation, we can compute the Q function. If you look
    at the equation, to compute the Q function, we need the transition probability
    ![](img/B15558_03_106.png), the reward function ![](img/B15558_03_108.png)**,**and
    the value of the next state ![](img/B15558_03_112.png). The model dynamics provide
    us with the transition probability ![](img/B15558_03_106.png) and the reward function
    ![](img/B15558_03_108.png). But what about the value of the next state ![](img/B15558_03_115.png)?
    We don't know the value of any states yet. So, we will initialize the value function
    (state values) with random values or zeros as shown in *Table 3.4* and compute
    the Q function.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用前面的公式，我们可以计算 Q 函数。如果你查看这个公式，要计算 Q 函数，我们需要过渡概率 ![](img/B15558_03_106.png)、奖励函数
    ![](img/B15558_03_108.png)**，**以及下一个状态的值 ![](img/B15558_03_112.png)。模型动态为我们提供了过渡概率
    ![](img/B15558_03_106.png) 和奖励函数 ![](img/B15558_03_108.png)。但是下一个状态的值 ![](img/B15558_03_115.png)
    呢？我们还不知道任何状态的值。所以，我们将用随机值或零初始化值函数（状态值），如*表 3.4*所示，并计算 Q 函数。
- en: '![](img/B15558_03_09.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_09.png)'
- en: 'Table 3.4: Initial value table'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.4：初始值表
- en: '**Iteration 1**:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**迭代 1**：'
- en: 'Let''s compute the Q value of state **A**. We have two actions in state **A,**
    which are 0 and 1\. So, first let''s compute the Q value for state **A** and action
    0 (note that we use the discount factor ![](img/B15558_03_116.png) throughout
    this section):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算状态**A**的 Q 值。状态**A**有两个动作，分别是 0 和 1。因此，首先让我们计算状态**A**和动作 0 的 Q 值（注意，我们在整个章节中都使用折扣因子
    ![](img/B15558_03_116.png)）：
- en: '![](img/B15558_03_117.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_117.png)'
- en: 'Now, let''s compute the Q value for state **A** and action 1:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算状态**A**和动作 1 的 Q 值：
- en: '![](img/B15558_03_118.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_118.png)'
- en: 'After computing the Q values for both the actions in state **A**, we can update
    the Q table as shown in *Table 3.5*:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算了状态**A**中两个动作的 Q 值之后，我们可以更新 Q 表，如*表 3.5*所示：
- en: '![](img/B15558_03_10.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_10.png)'
- en: 'Table 3.5: Q table'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.5：Q 表
- en: We learned that the optimal value of a state is just the max of the Q function.
    That is, ![](img/B15558_03_088.png). By looking at *Table 3.5*, we can say that
    the value of state **A**, *V*(*A*), is *Q*(*A*, 1) since *Q*(*A*, 1) has a higher
    value than *Q*(*A*, 0). Thus, *V*(*A*) = 0.9.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，状态的最优值就是 Q 函数的最大值。也就是说，![](img/B15558_03_088.png)。通过查看*表 3.5*，我们可以说，状态**A**的值，*V*(*A*)，是
    *Q*(*A*, 1)，因为 *Q*(*A*, 1) 的值比 *Q*(*A*, 0) 高。因此，*V*(*A*) = 0.9。
- en: 'We can update the value of state **A** in our value table as shown in *Table
    3.6*:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在我们的值表中更新状态**A**的值，如*表 3.6*所示：
- en: '![](img/B15558_03_11.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_11.png)'
- en: 'Table 3.6: Updated value table'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.6：更新后的值表
- en: Similarly, in order to compute the value of state **B**, *V*(*B*), we compute
    the Q value of *Q*(*B*, 0) and *Q*(*B*, 1) and select the highest Q value as the
    value of state **B**. In the same way, to compute the values of other states,
    we compute the Q value for all state-action pairs and select the maximum Q value
    as the value of a state.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，为了计算状态**B**的值*V*(B)，我们计算*Q*(B, 0)和*Q*(B, 1)的 Q 值，并选择最高的 Q 值作为状态**B**的值。
    同样地，为了计算其他状态的值，我们计算所有状态-动作对的 Q 值，并选择最大的 Q 值作为状态的值。
- en: 'After computing the value of all the states, our updated value table may resemble
    *Table 3.7*. This is the result of the first iteration:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算所有状态的值之后，我们的更新后的值表可能类似于*表 3.7*。 这是第一次迭代的结果：
- en: '![](img/B15558_03_12.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_12.png)'
- en: 'Table 3.7: Value table from iteration 1'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3.7: 第 1 次迭代的值表'
- en: However, the value function (value table) shown in *Table 3.7* obtained as a
    result of the first iteration is not an optimal one. But why? We learned that
    the optimal value function is the maximum of the optimal Q function. That is,
    ![](img/B15558_03_088.png). Thus to find the optimal value function, we need the
    optimal Q function. But the Q function may not be an optimal one in the first
    iteration as we computed the Q function based on the randomly initialized state
    values.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，作为第一次迭代的结果得到的值函数（值表）在不是最优的。 但为什么？ 我们了解到最优值函数是最优 Q 函数的最大值。 也就是说，![](img/B15558_03_088.png)。
    因此，为了找到最优值函数，我们需要最优 Q 函数。 但是 Q 函数可能在第一次迭代中并不是最优的，因为我们是基于随机初始化的状态值计算 Q 函数的。
- en: As the following shows, when we started off computing the Q function, we used
    the randomly initialized state values.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 正如下文所示，当我们开始计算 Q 函数时，我们使用了随机初始化的状态值。
- en: '![](img/B15558_03_31.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_31.png)'
- en: So, what we can do is, in the next iteration, while computing the Q function,
    we can use the updated state values obtained as a result of the first iteration.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在下一次迭代中，在计算 Q 函数时，我们可以使用作为第一次迭代结果得到的更新的状态值。
- en: 'That is, in the second iteration, to compute the value function, we compute
    the Q value of all state-action pairs and select the maximum Q value as the value
    of a state. In order to compute the Q value, we need to know the state values,
    in the first iteration, we used the randomly initialized state values. But in
    the second iteration, we use the updated state values (value table) obtained from
    the first iteration as the following shows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，在第二次迭代中，为了计算值函数，我们计算所有状态-动作对的 Q 值，并选择最大的 Q 值作为状态的值。 为了计算 Q 值，我们需要知道状态值，在第一次迭代中，我们使用随机初始化的状态值。
    但在第二次迭代中，我们使用从第一次迭代得到的更新的状态值（值表）如下所示：
- en: '![](img/B15558_03_32.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_32.png)'
- en: '**Iteration 2**:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 2 次迭代**：'
- en: Let's compute the Q value of state **A**. Remember that while computing the
    Q value, we use the updated state values from the previous iteration.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算状态**A**的 Q 值。 请记住，在计算 Q 值时，我们使用来自上一次迭代的更新的状态值。
- en: 'First, let''s compute the Q value of state **A** and action 0:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们计算状态**A**和动作 0 的 Q 值：
- en: '![](img/B15558_03_121.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_121.png)'
- en: 'Now, let''s compute the Q value for state **A** and action 1:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算状态**A**和动作 1 的 Q 值：
- en: '![](img/B15558_03_122.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_122.png)'
- en: 'As we may observe, since the Q value of action 1 in state A is higher than
    action 0, the value of state A becomes 1.44\. Similarly, we compute the value
    for all the states and update the value table. *Table 3.8* shows the updated value
    table:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，由于状态 A 中动作 1 的 Q 值高于动作 0，因此状态 A 的值变为 1.44。 类似地，我们计算所有状态的值并更新值表。 *表 3.8*
    显示了更新后的值表：
- en: '![](img/B15558_03_13.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_13.png)'
- en: 'Table 3.8: Value table from iteration 2'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3.8: 第 2 次迭代的值表'
- en: '**Iteration 3**:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 3 次迭代**：'
- en: We repeat the same steps we saw in the previous iteration and compute the value
    of all the states by selecting the maximum Q value. Remember that while computing
    the Q value, we use the updated state values (value table) obtained from the previous
    iteration. So, we use the updated state values from iteration 2 to compute the
    Q value.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复前一次迭代中看到的相同步骤，并通过选择最大的 Q 值来计算所有状态的值。 请记住，在计算 Q 值时，我们使用来自上一次迭代的更新的状态值（值表）。
    因此，我们使用第 2 次迭代中得到的更新的状态值来计算 Q 值。
- en: '*Table 3.9* shows the updated state values obtained as a result of the third
    iteration:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 3.9* 显示作为第三次迭代结果得到的更新的状态值：'
- en: '![](img/B15558_03_14.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_14.png)'
- en: 'Table 3.9: Value table from iteration 3'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3.9: 第 3 次迭代的值表'
- en: So, we repeat these steps for many iterations until we find the optimal value
    function. But how can we understand whether we have found the optimal value function
    or not? When the value function (value table) does not change over iterations
    or when it changes by a very small fraction, then we can say that we have attained
    convergence, that is, we have found an optimal value function.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们重复这些步骤进行多次迭代，直到找到最优值函数。但是我们怎么知道是否找到了最优值函数呢？当值函数（值表）在多次迭代中没有变化，或者变化非常小的时候，我们可以说我们已经达到了收敛，也就是说，我们找到了最优值函数。
- en: Okay, how can we find out whether the value table is changing or not changing
    from the previous iteration? We can calculate the difference between the value
    table obtained from the previous iteration and the value table obtained from the
    current iteration. If the difference is very small—say, the difference is less
    than a very small threshold number—then we can say that we have attained convergence
    as there is not much change in the value function.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们如何判断值表是否发生变化，或者与上次迭代相比没有变化呢？我们可以计算当前迭代得到的值表与上次迭代得到的值表之间的差异。如果差异非常小——例如，差异小于一个非常小的阈值——那么我们可以说我们已经达到了收敛，因为值函数变化不大。
- en: 'For example, let''s suppose *Table 3.10* shows the value table obtained as
    a result of **iteration 4**:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设*表 3.10*显示的是**迭代 4**得到的值表：
- en: '![](img/B15558_03_14.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_14.png)'
- en: 'Table 3.10: Value table from iteration 4'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.10：迭代 4 得到的值表
- en: As we can notice, the difference between the value table obtained as a result
    of iteration 4 and iteration 3 is very small. So, we can say that we have attained
    convergence and we take the value table obtained as a result of iteration 4 as
    our optimal value function. Please note that the above example is just for better
    understanding; in practice, we cannot attain convergence in just four iterations—it
    usually takes many iterations.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，迭代 4 和迭代 3 得到的值表之间的差异非常小。所以，我们可以说我们已经达到了收敛，并且我们将迭代 4 得到的值表作为我们的最优值函数。请注意，上面的例子只是为了更好地理解；在实际操作中，我们不可能只通过四次迭代就达到收敛——通常需要多次迭代。
- en: Now that we have found the optimal value function, in the next step, we will
    use this optimal value function to extract an optimal policy.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经找到了最优值函数，在接下来的步骤中，我们将使用这个最优值函数来提取最优策略。
- en: Step 2 – Extract the optimal policy from the optimal value function obtained
    from step 1
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 2 – 从步骤 1 中获得的最优值函数中提取最优策略
- en: 'As a result of *Step 1*, we obtained the optimal value function:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 作为*步骤 1*的结果，我们得到了最优值函数：
- en: '![](img/B15558_03_15.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_15.png)'
- en: 'Table 3.11: Optimal value table (value function)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.11：最优值表（值函数）
- en: Now, how can we extract the optimal policy from the obtained optimal value function?
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何从获得的最优值函数中提取最优策略呢？
- en: We generally use the Q function to compute the policy. We know that the Q function
    gives the Q value for every state-action pair. Once we have the Q values for all
    state-action pairs, we extract the policy by selecting the action that has the
    maximum Q value in each state. For example, consider the Q table in *Table 3.12*.
    It shows the Q values for all state-action pairs. Now we can extract the policy
    from the Q function (Q table) by selecting action 1 in the state *s*[0] and action
    0 in the state *s*[1] as they have the maximum Q value.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常使用 Q 函数来计算策略。我们知道 Q 函数为每个状态-动作对提供 Q 值。一旦我们得到了所有状态-动作对的 Q 值，我们就可以通过在每个状态中选择具有最大
    Q 值的动作来提取策略。例如，考虑*表 3.12*中的 Q 表。它显示了所有状态-动作对的 Q 值。现在，我们可以通过选择状态*s*[0]中的动作 1 和状态*s*[1]中的动作
    0（因为它们具有最大的 Q 值）来从 Q 函数（Q 表）中提取策略。
- en: '![](img/B15558_03_16.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_16.png)'
- en: 'Table 3.12: Q table'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.12：Q 表
- en: Okay, now we compute the Q function using the optimal value function obtained
    from *Step 1*. Once we have the Q function, then we extract the policy by selecting
    the action that has the maximum Q value in each state. Since we are computing
    the Q function using the optimal value function, the policy extracted from the
    Q function will be the optimal policy.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们使用*步骤 1*中获得的最优值函数来计算 Q 函数。一旦我们得到 Q 函数，我们就可以通过选择每个状态中具有最大 Q 值的动作来提取策略。由于我们是使用最优值函数来计算
    Q 函数的，因此从 Q 函数中提取的策略将是最优策略。
- en: 'We learned that the Q function can be computed as:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，Q 函数可以通过以下方式计算：
- en: '![](img/B15558_03_123.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_123.png)'
- en: 'Now, while computing Q values, we use the optimal value function we obtained
    from *step 1*. After computing the Q function, we can extract the optimal policy
    by selecting the action that has the maximum Q value:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在计算 Q 值时，我们使用从*步骤 1* 获得的最优价值函数。在计算 Q 函数后，我们可以通过选择具有最大 Q 值的动作来提取最优策略：
- en: '![](img/B15558_03_124.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_124.png)'
- en: 'For instance, let''s compute the Q value for all actions in state **A** using
    the optimal value function. The Q value for action 0 in state **A** is computed
    as:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用最优价值函数计算状态 **A** 中所有动作的 Q 值。在状态 **A** 中，动作 0 的 Q 值计算如下：
- en: '![](img/B15558_03_125.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_125.png)'
- en: 'The Q value for action 1 in state **A** is computed as:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在状态 **A** 中，动作 1 的 Q 值计算如下：
- en: '![](img/B15558_03_126.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_126.png)'
- en: 'Since *Q*(*A*, 1) is higher than *Q*(*A*, 0), our optimal policy will select
    action 1 as the optimal action in state **A**. *Table 3.13* shows the Q table
    after computing the Q values for all state-action pairs using the optimal value
    function:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *Q*(*A*, 1) 高于 *Q*(*A*, 0)，因此我们的最优策略会在状态 **A** 中选择动作 1 作为最优动作。*表3.13* 显示了使用最优价值函数计算所有状态-动作对的
    Q 值后的 Q 表：
- en: '![](img/B15558_03_17.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_17.png)'
- en: 'Table 3.13: Q table'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.13：Q 表
- en: From this Q table, we pick the action in each state that has the maximum value
    as an optimal policy. Thus, our optimal policy would select action 1 in state
    **A**, action 1 in state **B**, and action 1 in state **C**.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个 Q 表中，我们选择每个状态中 Q 值最大的动作作为最优策略。因此，我们的最优策略会在状态 **A** 中选择动作 1，在状态 **B** 中选择动作
    1，在状态 **C** 中选择动作 1。
- en: Thus, according to our optimal policy, if we perform action 1 in state **A**,
    we can reach state **C** without visiting state **B**.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据我们的最优策略，如果我们在状态 **A** 执行动作 1，我们可以到达状态 **C**，而不经过状态 **B**。
- en: In this section, we learned how to compute the optimal policy using the value
    iteration method. In the next section, we will learn how to implement the value
    iteration method to compute the optimal policy in the Frozen Lake environment
    using the Gym toolkit.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用价值迭代法计算最优策略。在下一节中，我们将学习如何使用 Gym 工具包在冰湖环境中实现价值迭代法来计算最优策略。
- en: Solving the Frozen Lake problem with value iteration
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用价值迭代法解决冰湖问题
- en: 'In the previous chapter, we learned about the Frozen Lake environment. The
    Frozen Lake environment is shown in *Figure 3.6*:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解了冰湖环境。冰湖环境如*图3.6*所示：
- en: '![](img/B15558_03_18.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_18.png)'
- en: 'Figure 3.6: Frozen Lake environment'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6：冰湖环境
- en: 'Let''s recap the Frozen Lake environment a bit. In the Frozen Lake environment
    shown in *Figure 3.6*, the following applies:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下冰湖环境。在*图3.6*所示的冰湖环境中，以下内容适用：
- en: '**S** implies the starting state'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**S** 表示起始状态'
- en: '**F** implies the frozen states'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F** 表示冻结状态'
- en: '**H** implies the hole states'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**H** 表示洞穴状态'
- en: '**G** implies the goal state'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**G** 表示目标状态'
- en: 'We learned that in the Frozen Lake environment, our goal is to reach the goal
    state **G** from the starting state **S** without visiting the hole states **H**.
    That is, while trying to reach the goal state **G** from the starting state **S**,
    if the agent visits the hole states **H**, then it will fall into the hole and
    die as *Figure 3.7* shows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在冰湖环境中，我们的目标是从起始状态 **S** 到达目标状态 **G**，同时避免经过洞穴状态 **H**。也就是说，在尝试从起始状态 **S**
    到达目标状态 **G** 时，如果代理访问了洞穴状态 **H**，它就会掉进洞里并死亡，如*图3.7*所示：
- en: '![](img/B15558_03_19.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_19.png)'
- en: 'Figure 3.7: Agent falling into the hole'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7：代理掉进洞里
- en: 'So, we want the agent to avoid the hole states **H** to reach the goal state
    **G** as shown in the following:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望代理避免洞穴状态 **H**，以到达目标状态 **G**，如以下所示：
- en: '![](img/B15558_03_20.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_20.png)'
- en: 'Figure 3.8: Agent reaches the goal state'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8：代理到达目标状态
- en: How can we achieve this goal? That is, how can we reach state **G** from **S**
    without visiting **H**? We learned that the optimal policy tells the agent to
    perform the correct action in each state. So, if we find the optimal policy, then
    we can reach state **G** from **S** without visiting state **H**. Okay, how can
    we find the optimal policy? We can use the value iteration method we just learned
    to find the optimal policy.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何实现这个目标呢？也就是说，如何从 **S** 状态到达 **G** 状态，而不经过 **H** 状态？我们了解到，最优策略会告诉代理在每个状态中执行正确的动作。因此，如果我们找到最优策略，就能从
    **S** 到达 **G**，而不经过 **H**。好吧，我们如何找到最优策略？我们可以使用刚才学习的价值迭代法来找到最优策略。
- en: Remember that all our states (**S** to **G**) will be encoded from 0 to 16 and
    all four actions—*left*, *down*, *up*, *right*—will be encoded from 0 to 3 in
    the Gym toolkit.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，所有的状态（**S** 到 **G**）将在 Gym 工具包中从 0 到 16 编码，所有四个动作——*左*，*下*，*上*，*右*——将在 Gym
    工具包中从 0 到 3 编码。
- en: In this section, we will learn how to find the optimal policy using the value
    iteration method so that the agent can reach state **G** from **S** without visiting
    **H**.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将学习如何使用值迭代方法找到最优策略，使得代理可以从 **S** 状态到达 **G** 状态，而不会经过 **H**。
- en: 'First, let''s import the necessary libraries:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库：
- en: '[PRE0]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, let''s create the Frozen Lake environment using Gym:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 Gym 创建 Frozen Lake 环境：
- en: '[PRE1]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s look at the Frozen Lake environment using the `render` function:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `render` 函数查看 Frozen Lake 环境：
- en: '[PRE2]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding code will display:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将显示：
- en: '![](img/B15558_03_21.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_21.png)'
- en: 'Figure 3.9: Gym Frozen Lake environment'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9：Gym 冻湖环境
- en: As we can notice, our agent is in state **S** and it has to reach state **G**
    without visiting the **H** states. So, let's learn how to compute the optimal
    policy using the value iteration method.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，代理处于状态 **S**，并且必须到达状态 **G**，而不能经过 **H** 状态。所以，让我们学习如何使用值迭代方法计算最优策略。
- en: 'In the value iteration method, we perform two steps:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在值迭代方法中，我们执行两个步骤：
- en: Compute the optimal value function by taking the maximum over the Q function,
    that is, ![](img/B15558_03_088.png)
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对 Q 函数取最大值来计算最优值函数，也就是 ![](img/B15558_03_088.png)
- en: Extract the optimal policy from the computed optimal value function
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从计算得到的最优值函数中提取最优策略
- en: First, let's learn how to compute the optimal value function, and then we will
    see how to extract the optimal policy from the computed optimal value function.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们学习如何计算最优值函数，然后我们将看到如何从计算得到的最优值函数中提取最优策略。
- en: Computing the optimal value function
  id: totrans-282
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算最优值函数
- en: We will define a function called `value_iteration` where we compute the optimal
    value function iteratively by taking the maximum over the Q function, that is,
    ![](img/B15558_03_088.png). For better understanding, let's closely look at every
    line of the function, and then we'll look at the complete function at the end,
    which will provide more clarity.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个名为 `value_iteration` 的函数，在这个函数中，我们通过对 Q 函数取最大值来迭代计算最优值函数，也就是 ![](img/B15558_03_088.png)。为了更好地理解，我们将仔细查看该函数的每一行代码，最后再看完整的函数，这将更有助于理解。
- en: 'Define the `value_iteration` function, which takes the environment as a parameter:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 `value_iteration` 函数，它以环境作为参数：
- en: '[PRE3]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Set the number of iterations:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 设置迭代次数：
- en: '[PRE4]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Set the threshold number for checking the convergence of the value function:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 设置用于检查值函数收敛的阈值：
- en: '[PRE5]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We also set the discount factor ![](img/B15558_03_035.png) to 1:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将折扣因子 ![](img/B15558_03_035.png) 设置为 1：
- en: '[PRE6]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we will initialize the value table by setting the value of all states
    to zero:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过将所有状态的值初始化为零来初始化值表：
- en: '[PRE7]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'For every iteration:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次迭代：
- en: '[PRE8]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Update the value table, that is, we learned that on every iteration, we use
    the updated value table (state values) from the previous iteration:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 更新值表，也就是说，我们在每次迭代时使用上一迭代中的更新值表（状态值）：
- en: '[PRE9]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we compute the value function (state value) by taking the maximum of the
    Q value:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们通过取 Q 值的最大值来计算值函数（状态值）：
- en: '![](img/B15558_03_088.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_088.png)'
- en: Where ![](img/B15558_03_123.png).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_03_123.png)。
- en: 'Thus, for each state, we compute the Q values of all the actions in the state
    and then we update the value of the state as the one that has the maximum Q value:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于每个状态，我们计算该状态下所有动作的 Q 值，然后将状态的值更新为具有最大 Q 值的值：
- en: '[PRE10]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Compute the Q value of all the actions, ![](img/B15558_03_123.png):'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 计算所有动作的 Q 值，![](img/B15558_03_123.png)：
- en: '[PRE11]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Update the value of the state as a maximum Q value, ![](img/B15558_03_088.png):'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态的值更新为最大 Q 值，![](img/B15558_03_088.png)：
- en: '[PRE12]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After computing the value table, that is, the value of all the states, we check
    whether the difference between the value table obtained in the current iteration
    and the previous iteration is less than or equal to a threshold value. If the
    difference is less than the threshold, then we break the loop and return the value
    table as our optimal value function as the following code shows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 计算完值表，也就是所有状态的值后，我们检查当前迭代中获得的值表与前一次迭代中的值表之间的差异是否小于或等于阈值。如果差异小于阈值，我们就跳出循环并返回值表作为我们的最优值函数，如下代码所示：
- en: '[PRE13]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now that we have computed the optimal value function by taking the maximum of
    the Q values, let's see how to extract the optimal policy from the optimal value
    function.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经通过取Q值的最大值计算了最优值函数，让我们看看如何从最优值函数中提取最优策略。
- en: Extracting the optimal policy from the optimal value function
  id: totrans-311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从最优值函数中提取最优策略
- en: In the previous step, we computed the optimal value function. Now, let's see
    how to extract the optimal policy from the computed optimal value function.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一步中，我们计算了最优值函数。现在，让我们看看如何从计算出的最优值函数中提取最优策略。
- en: 'First, we define a function called `extract_policy`, which takes `value_table`
    as a parameter:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义一个名为`extract_policy`的函数，接收`value_table`作为参数：
- en: '[PRE15]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Set the discount factor ![](img/B15558_03_005.png) to 1:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 将折扣因子 ![](img/B15558_03_005.png) 设置为1：
- en: '[PRE16]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'First, we initialize the policy with zeros, that is, we set the actions for
    all the states to be zero:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将策略初始化为零，也就是将所有状态的动作都设置为零：
- en: '[PRE17]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we compute the Q function using the optimal value function obtained from
    the previous step. We learned that the Q function can be computed as:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用从前一步获得的最优值函数来计算Q函数。我们了解到Q函数可以这样计算：
- en: '![](img/B15558_03_123.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_123.png)'
- en: After computing the Q function, we can extract the policy by selecting the action
    that has the maximum Q value. Since we are computing the Q function using the
    optimal value function, the policy extracted from the Q function will be the optimal
    policy.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算了Q函数后，我们可以通过选择具有最大Q值的动作来提取策略。由于我们使用最优值函数来计算Q函数，从Q函数提取出的策略将是最优策略。
- en: '![](img/B15558_03_124.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_124.png)'
- en: As the following code shows, for each state, we compute the Q values for all
    the actions in the state and then we extract the policy by selecting the action
    that has the maximum Q value.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下代码所示，对于每个状态，我们计算该状态下所有动作的Q值，然后通过选择具有最大Q值的动作来提取策略。
- en: 'For each state:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个状态：
- en: '[PRE18]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Compute the Q value of all the actions in the state, ![](img/B15558_03_123.png):'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 计算该状态下所有动作的Q值，![](img/B15558_03_123.png)：
- en: '[PRE19]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Extract the policy by selecting the action that has the maximum Q value, ![](img/B15558_03_124.png):'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择具有最大Q值的动作来提取策略，![](img/B15558_03_124.png)：
- en: '[PRE20]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: That's it! Now, we will see how to extract the optimal policy in our Frozen
    Lake environment.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！现在，我们将看看如何在我们的Frozen Lake环境中提取最优策略。
- en: Putting it all together
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将所有内容结合起来
- en: We learned that in the Frozen Lake environment, our goal is to find the optimal
    policy that selects the correct action in each state so that we can reach state
    **G** from state **A** without visiting the hole states.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在Frozen Lake环境中，我们的目标是找到最优策略，在每个状态下选择正确的动作，以便我们能够从状态**A**到达状态**G**，而不经过陷阱状态。
- en: 'First, we compute the optimal value function using our `value_iteration` function
    by passing our Frozen Lake environment as the parameter:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过传递我们的Frozen Lake环境作为参数，使用我们的`value_iteration`函数计算最优值函数：
- en: '[PRE22]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we extract the optimal policy from the optimal value function using our
    `extract_policy` function:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用我们的`extract_policy`函数从最优值函数中提取最优策略：
- en: '[PRE23]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can print the obtained optimal policy:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以打印出获得的最优策略：
- en: '[PRE24]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The preceding code will print the following. As we can observe, our optimal
    policy tells us to perform the correct action in each state:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将打印出以下内容。正如我们所观察到的，我们的最优策略告诉我们在每个状态下执行正确的动作：
- en: '[PRE25]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now that we have learned what value iteration is and how to perform the value
    iteration method to compute the optimal policy in our Frozen Lake environment,
    in the next section, we will learn about another interesting method, called policy
    iteration.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了什么是值迭代，以及如何执行值迭代方法来计算我们Frozen Lake环境中的最优策略，在下一部分，我们将学习另一个有趣的方法，叫做策略迭代。
- en: Policy iteration
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略迭代
- en: In the value iteration method, first, we computed the optimal value function
    by taking the maximum over the Q function (Q values) iteratively. Once we found
    the optimal value function, we used it to extract the optimal policy. Whereas
    in policy iteration we try to compute the optimal value function using the policy
    iteratively, once we found the optimal value function, we can use it to extract
    the optimal policy.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在值迭代方法中，我们首先通过迭代地在Q函数（Q值）上取最大值来计算最优值函数。一旦我们找到了最优值函数，就可以使用它来提取最优策略。而在策略迭代中，我们尝试通过迭代地使用策略来计算最优值函数，一旦找到了最优值函数，就可以用它来提取最优策略。
- en: 'First, let''s learn how to compute the value function using a policy. Say we
    have a policy ![](img/B15558_03_139.png), how can we compute the value function
    using the policy ![](img/B15558_03_140.png)? Here, we can use our Bellman equation.
    We learned that according to the Bellman equation, we can compute the value function
    using the policy ![](img/B15558_03_082.png) as the following shows:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们学习如何使用策略计算价值函数。假设我们有一个策略 ![](img/B15558_03_139.png)，我们如何使用这个策略 ![](img/B15558_03_140.png)
    来计算价值函数呢？在这里，我们可以使用贝尔曼方程。我们知道，根据贝尔曼方程，我们可以使用策略 ![](img/B15558_03_082.png) 来计算价值函数，如下所示：
- en: '![](img/B15558_03_142.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_142.png)'
- en: 'Let''s suppose our policy is a deterministic policy, so we can remove the term
    ![](img/B15558_03_143.png) from the preceding equation since there is no stochasticity
    in the policy and rewrite our Bellman equation as:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的策略是一个确定性策略，因此我们可以从前面的方程中去掉项 ![](img/B15558_03_143.png)，因为策略中没有随机性，并将我们的贝尔曼方程改写为：
- en: '![](img/B15558_03_144.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_144.png)'
- en: 'For notation simplicity, we can denote ![](img/B15558_03_145.png) by ![](img/B15558_03_106.png)
    and ![](img/B15558_03_147.png) with ![](img/B15558_03_108.png)and rewrite the
    preceding equation as:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化符号，我们可以将 ![](img/B15558_03_145.png) 记作 ![](img/B15558_03_106.png)，将 ![](img/B15558_03_147.png)
    记作 ![](img/B15558_03_108.png)，并将前面的方程改写为：
- en: '![](img/B15558_03_149.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_149.png)'
- en: Thus using the above equation we can compute the value function using a policy.
    Our goal is to find the optimal value function because once we have found the
    optimal value function, we can use it to extract the optimal policy.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过使用上述方程，我们可以使用策略来计算价值函数。我们的目标是找到最优价值函数，因为一旦我们找到了最优价值函数，我们就可以用它来提取最优策略。
- en: We will not be given any policy as an input. So, we will initialize the random
    policy and compute the value function using the random policy. Then we check if
    the computed value function is optimal or not. It will not be optimal since it
    is computed based on the random policy.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会被提供任何策略作为输入。因此，我们将初始化一个随机策略，并使用这个随机策略来计算价值函数。然后，我们检查计算得到的价值函数是否是最优的。由于它是基于随机策略计算的，它将不是最优的。
- en: 'So, we will extract a new policy from the computed value function, then we
    will use the extracted new policy to compute the new value function, and then
    we will check if the new value function is optimal. If it''s optimal we will stop,
    else we repeat these steps for a series of iterations. For a better understanding,
    look at the following steps:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们将从计算得到的价值函数中提取一个新的策略，然后我们将使用提取的新的策略来计算新的价值函数，再检查新的价值函数是否最优。如果它是最优的，我们就停止，否则我们将在一系列迭代中重复这些步骤。为了更好地理解，参见以下步骤：
- en: '**Iteration 1**: Let ![](img/B15558_03_150.png) be the random policy. We use
    this random policy to compute the value function ![](img/B15558_03_151.png). Our
    value function will not be optimal as it is computed based on the random policy.
    So, from ![](img/B15558_03_152.png), we extract a new policy ![](img/B15558_03_153.png).'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '**迭代 1**：让 ![](img/B15558_03_150.png) 为随机策略。我们使用这个随机策略来计算价值函数 ![](img/B15558_03_151.png)。由于是基于随机策略计算的，我们的价值函数将不是最优的。因此，从
    ![](img/B15558_03_152.png) 中，我们提取出一个新的策略 ![](img/B15558_03_153.png)。'
- en: '**Iteration 2**: Now, we use the new policy ![](img/B15558_03_153.png) derived
    from the previous iteration to compute the new value function ![](img/B15558_03_155.png),
    then we check if ![](img/B15558_03_156.png) is optimal. If it is optimal, we stop,
    else from this value function ![](img/B15558_03_157.png), we extract a new policy
    ![](img/B15558_03_158.png).'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '**迭代 2**：现在，我们使用从上一轮迭代中得到的新策略 ![](img/B15558_03_153.png) 来计算新的价值函数 ![](img/B15558_03_155.png)，然后检查
    ![](img/B15558_03_156.png) 是否是最优的。如果它是最优的，我们就停止，否则从这个价值函数 ![](img/B15558_03_157.png)
    中，我们提取出一个新的策略 ![](img/B15558_03_158.png)。'
- en: '**Iteration 3**: Now, we use the new policy ![](img/B15558_03_159.png) derived
    from the previous iteration to compute the new value function ![](img/B15558_03_160.png),
    then we check if ![](img/B15558_03_161.png) is optimal. If it is optimal, we stop,
    else from this value function ![](img/B15558_03_162.png), we extract a new policy
    ![](img/B15558_03_163.png).'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '**迭代 3**：现在，我们使用从上一轮迭代中得到的新策略 ![](img/B15558_03_159.png) 来计算新的价值函数 ![](img/B15558_03_160.png)，然后检查
    ![](img/B15558_03_161.png) 是否是最优的。如果它是最优的，我们就停止，否则从这个价值函数 ![](img/B15558_03_162.png)
    中，我们提取出一个新的策略 ![](img/B15558_03_163.png)。'
- en: 'We repeat this process for many iterations until we find the optimal value
    function ![](img/B15558_03_164.png) as the following shows:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复这个过程多次迭代，直到我们找到最优价值函数 ![](img/B15558_03_164.png)，如下所示：
- en: '![](img/B15558_03_165.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_165.png)'
- en: The preceding step is called policy evaluation and improvement. Policy evaluation
    implies that at each step we evaluate the policy by checking if the value function
    computed using that policy is optimal. Policy improvement means that at each step
    we find the new improved policy to compute the optimal value function.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 上述步骤称为策略评估与改进。策略评估意味着在每一步中，我们通过检查使用该策略计算的价值函数是否最优来评估策略。策略改进意味着在每一步中，我们找到新的改进策略以计算最优价值函数。
- en: Once we have found the optimal value function ![](img/B15558_03_166.png), then
    it implies that we have also found the optimal policy. That is, if ![](img/B15558_03_164.png)
    is optimal, then the policy that is used to compute ![](img/B15558_03_168.png)
    will be an optimal policy.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们找到了最优的价值函数 ![](img/B15558_03_166.png)，那么这意味着我们也找到了最优策略。也就是说，如果 ![](img/B15558_03_164.png)
    是最优的，那么用于计算 ![](img/B15558_03_168.png) 的策略将是最优策略。
- en: 'To get a better understanding of how policy iteration works, let''s look into
    the below steps with pseudocode. In the first iteration, we will initialize a
    random policy and use it to compute the value function:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解策略迭代是如何工作的，让我们通过以下伪代码步骤进行分析。在第一次迭代中，我们将初始化一个随机策略并用它来计算价值函数：
- en: '[PRE26]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Since we computed the value function using the random policy, the computed value
    function will not be optimal. So, we need to find a new policy with which we can
    compute the optimal value function.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们是使用随机策略计算的价值函数，因此计算出来的价值函数并非最优。因此，我们需要找到一个新的策略，用它来计算最优的价值函数。
- en: 'So, we extract a new policy from the value function computed using a random
    policy:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们从使用随机策略计算得到的价值函数中提取出一个新的策略：
- en: '[PRE27]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, we will use this new policy to compute the new value function:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用这个新策略来计算新的价值函数：
- en: '[PRE28]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If the new value function is optimal, we stop, else we repeat the preceding
    steps for a number of iterations until we find the optimal value function. The
    following pseudocode gives us a better understanding:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 如果新的价值函数是最优的，我们就停止，否则我们将重复前面的步骤多次，直到找到最优价值函数。以下伪代码可以帮助我们更好地理解：
- en: '[PRE29]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Wait! How do we say our value function is optimal? If the value function is
    not changing over iterations, then we can say that our value function is optimal.
    Okay, how can we check if the value function is not changing over iterations?
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 等等！我们怎么判断我们的价值函数是最优的呢？如果价值函数在迭代过程中不再变化，那么我们可以说我们的价值函数是最优的。那么，如何检查价值函数在迭代过程中没有发生变化呢？
- en: We learned that we compute the value function using a policy. If the policy
    is not changing over iterations, then our value function also doesn't change over
    the iterations. Thus, when the policy doesn't change over iterations, then we
    can say that we have found the optimal value function.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，价值函数是使用策略计算的。如果策略在迭代过程中没有变化，那么我们的价值函数在迭代过程中也不会变化。因此，当策略在迭代过程中不再变化时，我们可以说我们找到了最优的价值函数。
- en: 'Thus, over a series of iterations when the policy and new policy become the
    same, then we can say that we obtained the optimal value function. The following
    final pseudocode is given for clarity:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在一系列迭代中，当策略和新策略变得相同时，我们可以说我们已经得到了最优的价值函数。以下是为了清晰起见给出的最终伪代码：
- en: '[PRE30]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Thus, when the policy is not changing, that is, when the policy and new policy
    become the same, then we can say that we obtained the optimal value function and
    the policy that is used to compute the optimal value function will be the optimal
    policy.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当策略不再变化时，也就是当策略和新策略变得相同时，我们可以说我们已经得到了最优的价值函数，而用于计算最优价值函数的策略就是最优策略。
- en: Remember that in the value iteration method, we compute the optimal value function
    using the maximum over Q function (Q value) iteratively and once we have found
    the optimal value function, we extract the optimal policy from it. But in the
    policy iteration method, we compute the optimal value function using the policy
    iteratively and once we have found the optimal value function, then the policy that
    is used to compute the optimal value function will be the optimal policy.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在价值迭代方法中，我们通过对 Q 函数（Q 值）进行最大化迭代来计算最优价值函数，一旦我们找到了最优价值函数，就从中提取出最优策略。但在策略迭代方法中，我们是通过策略的迭代来计算最优价值函数，一旦我们找到了最优价值函数，那么用于计算最优价值函数的策略将是最优策略。
- en: Now that we have a basic understanding of how the policy iteration method works,
    in the next section, we will get into the details and learn how to compute policy
    iteration manually.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经基本理解了策略迭代方法的工作原理，接下来我们将进入细节，学习如何手动计算策略迭代。
- en: Algorithm – policy iteration
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法 – 策略迭代
- en: 'The steps of the policy iteration algorithm is given as follows:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 策略迭代算法的步骤如下：
- en: Initialize a random policy
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个随机策略
- en: Compute the value function using the given policy
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用给定策略计算价值函数
- en: Extract a new policy using the value function obtained from *step* 2
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用步骤2获得的价值函数提取新的策略
- en: If the extracted policy is the same as the policy used in *step 2*, then stop,
    else send the extracted new policy to *step 2* and repeat *steps 2* to *4*
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果提取的策略与步骤2中使用的策略相同，则停止；否则，将提取的新的策略发送到步骤2，并重复步骤2至步骤4
- en: 'Now, let''s get into the details and learn how exactly the preceding steps
    work. For a clear understanding, let''s perform policy iteration manually. Let''s
    take the same grid world environment we used in the value iteration method. Let''s
    say we are in state **A** and our goal is to reach state **C** without visiting
    the shaded state **B**, and say we have two actions, 0 – *left*/*right* and 1
    – *up*/*down*:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解具体细节，学习前述步骤是如何工作的。为了清楚理解，让我们手动执行策略迭代。我们使用在值迭代方法中相同的网格世界环境。假设我们处于状态**A**，目标是到达状态**C**，而不经过阴影状态**B**，假设我们有两个动作，0
    – *左*/*右*和1 – *上*/*下*：
- en: '![](img/B15558_03_22.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_22.png)'
- en: 'Figure 3.10: Grid world environment'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10：网格世界环境
- en: We know that in the above environment, the optimal policy is the one that tells
    us to perform action 1 in state **A** so that we can reach **C** without visiting
    **B**. Now, we will see how to find this optimal policy using policy iteration.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，在上述环境中，最优策略是告诉我们在状态**A**执行动作1，以便我们可以到达**C**而不经过**B**。现在，我们将看到如何通过策略迭代找到这个最优策略。
- en: '*Table 3.14* shows the model dynamics of state **A**:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '*表3.14*展示了状态**A**的模型动态：'
- en: '![](img/B15558_03_23.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_23.png)'
- en: 'Table 3.14: Model dynamics of state A'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.14：状态A的模型动态
- en: Step 1 – Initialize a random policy
  id: totrans-390
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤1 – 初始化一个随机策略
- en: 'First, we will initialize a random policy. As the following shows, our random
    policy tells us to perform action 1 in state **A**, 0 in state **B**, and action
    1 in state **C**:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将初始化一个随机策略。如下所示，我们的随机策略指示我们在状态**A**执行动作1，在状态**B**执行动作0，在状态**C**执行动作1：
- en: '![](img/B15558_03_169.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_169.png)'
- en: Step 2 – Compute the value function using the given policy
  id: totrans-393
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤2 – 使用给定的策略计算价值函数
- en: This step is exactly the same as how we computed the value function in value
    iteration but with a small difference. In value iteration, we computed the value
    function by taking the maximum over the Q function. But here in policy iteration,
    we will compute the value function using the policy.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步与我们在值迭代中计算价值函数的方式完全相同，但有一个小小的区别。在值迭代中，我们通过在Q函数上取最大值来计算价值函数。但在策略迭代中，我们将使用策略来计算价值函数。
- en: 'To understand this step better, let''s quickly recollect how we compute the
    value function in value iteration. In value iteration, we compute the optimal
    value function as the maximum over the optimal Q function as the following shows:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这一点，让我们快速回顾一下在值迭代中是如何计算价值函数的。在值迭代中，我们通过在最优Q函数上取最大值来计算最优价值函数，具体如下所示：
- en: '![](img/B15558_03_088.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_088.png)'
- en: where ![](img/B15558_03_171.png)
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_03_171.png)
- en: 'In policy iteration, we compute the value function using a policy ![](img/B15558_03_172.png),
    unlike value iteration, where we computed the value function using the maximum
    over the Q function. The value function using a policy ![](img/B15558_03_140.png)
    can be computed as:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在策略迭代中，我们使用策略 ![](img/B15558_03_172.png) 来计算价值函数，这与值迭代不同，后者是通过在Q函数上取最大值来计算价值函数。使用策略
    ![](img/B15558_03_140.png) 计算的价值函数可以通过以下方式得到：
- en: '![](img/B15558_03_174.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_174.png)'
- en: 'If you look at the preceding equation, to compute the value function, we need
    the transition probability ![](img/B15558_03_175.png), the reward function ![](img/B15558_03_176.png)and
    the value of the next state ![](img/B15558_03_177.png). The values of the transition
    probability ![](img/B15558_03_175.png) and the reward function ![](img/B15558_03_176.png)
    can be obtained from the model dynamics. But what about the value of the next
    state ![](img/B15558_03_180.png)? We don''t know the value of any states yet.
    So, we will initialize the value function (state values) with random values or
    zeros as *Figure 3.15* shows and compute the value function:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看看前面的公式，要计算值函数，我们需要转移概率 ![](img/B15558_03_175.png)、奖励函数 ![](img/B15558_03_176.png)
    和下一个状态的值 ![](img/B15558_03_177.png)。转移概率 ![](img/B15558_03_175.png) 和奖励函数 ![](img/B15558_03_176.png)
    的值可以从模型动态中获得。那么，下一个状态的值 ![](img/B15558_03_180.png) 呢？我们还不知道任何状态的值。所以，我们将值函数（状态值）初始化为随机值或零，如*图
    3.15* 所示，并计算值函数：
- en: '![](img/B15558_03_24.png)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_24.png)'
- en: 'Table 3.15: Initial value table'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.15：初始值表
- en: '**Iteration 1**:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一次迭代**：'
- en: Let's compute the value of state **A** (note that here, we only compute the
    value for the action given by the policy, unlike in value iteration, where we
    computed the Q value for all the actions in the state and selected the maximum
    value).
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算状态**A**的值（请注意，这里我们只计算策略给定的动作的值，不像在值迭代中，我们计算了所有动作的Q值并选择最大值）。
- en: 'So, the action given by the policy for state **A** is 1 and we can compute
    the value of state **A** as the following shows (note that we have used a discount
    factor ![](img/B15558_03_181.png) throughout this section):'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，策略给定的状态**A**的动作是1，我们可以计算状态**A**的值，如下所示（请注意，在本节中，我们使用了折扣因子 ![](img/B15558_03_181.png)）：
- en: '![](img/B15558_03_182.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_182.png)'
- en: 'Similarly, we compute the value for all the states using the action given by
    the policy. *Table 3.16* shows the updated state values obtained as a result of
    the first iteration:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们使用策略给定的动作来计算所有状态的值。*表 3.16* 显示了第一次迭代结果中更新后的状态值：
- en: '![](img/B15558_03_25.png)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_25.png)'
- en: 'Table 3.16: Value table from iteration 1'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.16：第一次迭代的值表
- en: However, the value function (value table) shown in *Table 3.16* obtained as
    a result of the first iteration will not be accurate. That is, the state values
    (value function) will not be accurate according to the given policy.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，*表 3.16* 中第一次迭代得到的值函数（值表）将不准确。也就是说，给定政策的状态值（值函数）将不准确。
- en: Note that unlike the value iteration method, here we are not checking whether
    our value function is optimal or not; we just check whether our value function
    is accurately computed according to the given policy.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与值迭代方法不同，这里我们并没有检查我们的值函数是否最优；我们只是检查我们的值函数是否根据给定的策略准确计算。
- en: 'The value function will not be accurate because when we started off computing
    the value function using the given policy, we used the randomly initialized state
    values:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 值函数将不准确，因为当我们开始使用给定的策略计算值函数时，我们使用了随机初始化的状态值：
- en: '![](img/B15558_03_33.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_33.png)'
- en: 'So, in the next iteration, while computing the value function, we will use
    the updated state values obtained as a result of the first iteration:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在下一次迭代中，在计算值函数时，我们将使用第一次迭代结果中得到的更新后的状态值：
- en: '![](img/B15558_03_34.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_34.png)'
- en: '**Iteration 2**:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二次迭代**：'
- en: Now, in iteration 2, we compute the value function using the policy ![](img/B15558_03_082.png).
    Remember that while computing the value function, we will use the updated state
    values (value table) obtained from iteration 1.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在第二次迭代中，我们使用策略 ![](img/B15558_03_082.png) 来计算值函数。记住，在计算值函数时，我们将使用第一次迭代中得到的更新后的状态值（值表）。
- en: 'For instance, let''s compute the value of state A:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们计算状态 A 的值：
- en: '![](img/B15558_03_184.png)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_184.png)'
- en: 'Similarly, we compute the value for all the states using the action given by
    the policy. *Table 3.17* shows the updated state values obtained as a result of
    the second iteration:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们使用策略给定的动作来计算所有状态的值。*表 3.17* 显示了第二次迭代结果中更新后的状态值：
- en: '![](img/B15558_03_26.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_26.png)'
- en: 'Table 3.17: Value table from iteration 2'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.17：第二次迭代的值表
- en: '**Iteration 3**:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '**第三次迭代**：'
- en: Similarly, in iteration 3, we compute the value function using the policy ![](img/B15558_03_185.png)
    and while computing the value function, we will use the updated state values (value
    table) obtained from iteration 2.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在第3次迭代中，我们使用策略![](img/B15558_03_185.png)计算价值函数，并在计算价值函数时，使用第2次迭代获得的更新后的状态值（价值表）。
- en: '*Table 3.18* shows the updated state values obtained from the third iteration:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '*表3.18*显示了通过第三次迭代更新后的状态值：'
- en: '![](img/B15558_03_27.png)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_27.png)'
- en: 'Table 3.18: Value table from iteration 3'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.18：第3次迭代的价值表
- en: 'We repeat this for many iterations until the value table does not change or
    changes very little over iterations. For example, let''s suppose *Table 3.19*
    shows the value table obtained as a result of **iteration 4**:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对这个过程进行多次迭代，直到价值表不再变化或变化非常小。例如，假设*表3.19*显示了**第4次迭代**得到的价值表：
- en: '![](img/B15558_03_28.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_28.png)'
- en: 'Table 3.19: Value table from iteration 4'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.19：第4次迭代的价值表
- en: As we can see, the difference between the value tables obtained from iteration
    4 and iteration 3 is very small. So, we can say that the value table is not changing
    much over iterations and we stop at this iteration and take this as our final
    value function.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，从第4次迭代和第3次迭代获得的价值表之间的差异非常小。因此，我们可以说，价值表在多次迭代中变化不大，我们在此迭代中停止，并将其作为最终的价值函数。
- en: Step 3 – Extract a new policy using the value function obtained from the previous
    step
  id: totrans-432
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第3步 – 使用上一阶段得到的价值函数提取新策略
- en: 'As a result of *step 2*, we obtained the value function, which is computed
    using the given random policy. However, this value function will not be optimal
    as it is computed using the random policy. So will extract a new policy from the
    value function obtained in the previous step. The value function (value table)
    obtained from the previous step is shown in *Table 3.20*:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 通过*第2步*的结果，我们得到了价值函数，这是使用给定的随机策略计算的。然而，由于该价值函数是基于随机策略计算的，它并不是最优的。因此，我们将从上一阶段获得的价值函数中提取新策略。从上一阶段获得的价值函数（价值表）显示在*表3.20*中：
- en: '![](img/B15558_03_29.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_29.png)'
- en: 'Table 3.20: Value table from the previous step'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.20：上一阶段的价值表
- en: 'Okay, how can we extract a new policy from the value function? (Hint: This
    step is exactly the same as how we extracted a policy given the value function
    in *step 2* of the value iteration method.)'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们如何从价值函数中提取新策略呢？（提示：此步骤与如何在价值迭代方法的*第2步*中根据价值函数提取策略完全相同。）
- en: 'In order to extract a new policy, we compute the Q function using the value
    function (value table) obtained from the previous step. Once we compute the Q
    function, we pick up actions in each state that have the maximum value as a new
    policy. We know that the Q function can be computed as:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取新策略，我们使用从上一阶段获得的价值函数（价值表）计算Q函数。一旦计算出Q函数，我们就选择在每个状态中具有最大值的动作作为新策略。我们知道Q函数可以通过以下方式计算：
- en: '![](img/B15558_03_186.png)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_186.png)'
- en: Now, while computing Q values, we use the value function we obtained from the
    previous step.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在计算Q值时，我们使用从上一阶段获得的价值函数。
- en: 'For instance, let''s compute the Q value for all actions in state **A** using
    the value function obtained from the previous step. The Q value for action 0 in
    state **A** is computed as:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们计算在状态**A**中所有动作的Q值，使用从上一阶段获得的价值函数。状态**A**中动作0的Q值计算为：
- en: '![](img/B15558_03_187.png)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_187.png)'
- en: 'The Q value for action 1 in state **A** is computed as:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 状态**A**中动作1的Q值计算为：
- en: '![](img/B15558_03_188.png)'
  id: totrans-443
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_188.png)'
- en: '*Table 3.21* shows the Q table after computing the Q values for all state-action
    pairs:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '*表3.21*显示了计算所有状态-动作对的Q值后的Q表：'
- en: '![](img/B15558_03_30.png)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_30.png)'
- en: 'Table 3.21: Q table'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.21：Q表
- en: From this *Q* table, we pick up actions in each state that have the maximum
    value as a new policy.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个*Q*表中，我们选择在每个状态中具有最大值的动作作为新策略。
- en: '![](img/B15558_03_189.png)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_189.png)'
- en: Step 4 – Check the new policy
  id: totrans-449
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第4步 – 检查新策略
- en: Now we will check if the extracted new policy from *step 3* is the same as the
    policy we used in *step 2*. If it is the same, then we stop, else we send the
    extracted new policy to *step 2* and repeat *steps 2* to *4*.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将检查从*第3步*提取的新策略是否与我们在*第2步*中使用的策略相同。如果相同，我们就停止，否则我们将提取的新策略送回*第2步*并重复*第2步*至*第4步*。
- en: Thus, in this section, we learned how to compute the optimal policy using the
    policy iteration method. In the next section, we will learn how to implement the
    policy iteration method to compute the optimal policy in the Frozen Lake environment
    using the Gym toolkit.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本节中，我们学习了如何使用策略迭代法计算最优策略。在下一节中，我们将学习如何使用 Gym 工具包在 Frozen Lake 环境中实现策略迭代法以计算最优策略。
- en: Solving the Frozen Lake problem with policy iteration
  id: totrans-452
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用策略迭代法解决 Frozen Lake 问题
- en: We learned that in the Frozen Lake environment, our goal is to reach the goal
    state **G** from the starting state **S** without visiting the hole states **H**.
    Now, let's learn how to compute the optimal policy using the policy iteration
    method in the Frozen Lake environment.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到，在 Frozen Lake 环境中，我们的目标是从起始状态 **S** 到达目标状态 **G**，而不经过洞穴状态 **H**。现在，让我们学习如何在
    Frozen Lake 环境中使用策略迭代法计算最优策略。
- en: 'First, let''s import the necessary libraries:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库：
- en: '[PRE31]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, let''s create the Frozen Lake environment using Gym:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 Gym 创建 Frozen Lake 环境：
- en: '[PRE32]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We learned that in the policy iteration, we compute the value function using
    the policy iteratively. Once we have found the optimal value function, then the
    policy that is used to compute the optimal value function will be the optimal
    policy.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到，在策略迭代中，我们是通过迭代使用策略来计算价值函数。一旦我们找到了最优价值函数，那么用于计算最优价值函数的策略就是最优策略。
- en: So, first, let's learn how to compute the value function using the policy.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，首先，让我们学习如何使用策略计算价值函数。
- en: Computing the value function using the policy
  id: totrans-460
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用策略计算价值函数
- en: This step is exactly the same as how we computed the value function in the value
    iteration method but with a small difference. Here, we compute the value function
    using the policy but in the value iteration method, we compute the value function
    by taking the maximum over Q values. Now, let's learn how to define a function
    that computes the value function using the given policy.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步与我们在价值迭代法中计算价值函数的方法完全相同，只是有一点小区别。这里，我们使用策略来计算价值函数，而在价值迭代法中，我们是通过取 Q 值的最大值来计算价值函数。现在，让我们学习如何定义一个函数，使用给定的策略来计算价值函数。
- en: 'Let''s define a function called `compute_value_function`, which takes the policy
    as a parameter:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个名为 `compute_value_function` 的函数，它将策略作为参数：
- en: '[PRE33]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, let''s define the number of iterations:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义迭代次数：
- en: '[PRE34]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Define the threshold value:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 定义阈值：
- en: '[PRE35]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Set the discount factor ![](img/B15558_03_190.png) value to 1.0:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 将折扣因子 ![](img/B15558_03_190.png) 的值设置为 1.0：
- en: '[PRE36]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, we will initialize the value table by setting all the state values to
    zero:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过将所有状态值初始化为零来初始化价值表：
- en: '[PRE37]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'For every iteration:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次迭代：
- en: '[PRE38]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Update the value table; that is, we learned that on every iteration, we use
    the updated value table (state values) from the previous iteration:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 更新价值表；也就是说，我们学到了在每次迭代中，我们使用来自前一次迭代的更新后的价值表（状态值）：
- en: '[PRE39]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, we compute the value function using the given policy. We learned that
    a value function can be computed according to some policy ![](img/B15558_01_047.png)
    as follows:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用给定的策略计算价值函数。我们学到，价值函数可以根据某些策略 ![](img/B15558_01_047.png) 按如下方式计算：
- en: '![](img/B15558_03_192.png)'
  id: totrans-477
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_192.png)'
- en: Thus, for each state, we select the action according to the policy and then
    we update the value of the state using the selected action as follows.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于每个状态，我们根据策略选择动作，然后使用选择的动作更新状态的值，如下所示。
- en: 'For each state:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个状态：
- en: '[PRE40]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Select the action in the state according to the policy:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 根据策略在状态中选择动作：
- en: '[PRE41]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Compute the value of the state using the selected action,![](img/B15558_03_192.png):'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 使用选择的动作计算状态的值，![](img/B15558_03_192.png)：
- en: '[PRE42]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'After computing the value table, that is, the value of all the states, we check
    whether the difference between the value table obtained in the current iteration
    and the previous iteration is less than or equal to a threshold value. If it is
    less, then we break the loop and return the value table as an accurate value function
    of the given policy:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算完价值表，也就是所有状态的值后，我们检查当前迭代中获得的价值表与前一次迭代之间的差值是否小于或等于阈值。如果小于或等于，我们就结束循环，并将价值表作为给定策略的准确价值函数返回：
- en: '[PRE43]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Now that we have computed the value function of the policy, let's see how to
    extract the policy from the value function.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算了策略的价值函数，让我们看看如何从价值函数中提取策略。
- en: Extracting the policy from the value function
  id: totrans-488
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从价值函数中提取策略
- en: 'This step is exactly the same as how we extracted the policy from the value
    function in the value iteration method. Thus, similar to what we learned in the
    value iteration method, we define a function called `extract_policy` to extract
    a policy given the value function:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步与在值迭代方法中从值函数中提取策略的方式完全相同。因此，类似于我们在值迭代方法中学到的那样，我们定义一个叫做`extract_policy`的函数，给定值函数来提取策略：
- en: '[PRE44]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Putting it all together
  id: totrans-491
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 综合起来
- en: 'First, let''s define a function called `policy_iteration`, which takes the
    environment as a parameter:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义一个叫做`policy_iteration`的函数，该函数将环境作为参数：
- en: '[PRE45]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Set the number of iterations:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 设置迭代次数：
- en: '[PRE46]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We learned that in the policy iteration method, we begin by initializing a
    random policy. So, we will initialize the random policy, which selects the action
    0 in all the states:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在策略迭代方法中，我们从初始化一个随机策略开始。所以，我们将初始化一个随机策略，在所有状态下选择动作0：
- en: '[PRE47]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'For every iteration:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代：
- en: '[PRE48]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Compute the value function using the policy:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 使用策略计算值函数：
- en: '[PRE49]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Extract the new policy from the computed value function:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 从计算得到的值函数中提取新的策略：
- en: '[PRE50]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'If `policy` and `new_policy` are the same, then break the loop:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`policy`和`new_policy`相同，则退出循环：
- en: '[PRE51]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Else update the current `policy` to `new_policy`
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 否则将当前的`policy`更新为`new_policy`
- en: '[PRE52]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now, let''s learn how to perform policy iteration and find the optimal policy
    in the Frozen Lake environment. So, we just feed the Frozen Lake environment to
    our `policy_iteration` function as shown here and get the optimal policy:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何在Frozen Lake环境中执行策略迭代并找到最优策略。所以，我们只需将Frozen Lake环境传递给我们的`policy_iteration`函数，如下所示，并获得最优策略：
- en: '[PRE53]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We can print the optimal policy:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以打印出最优策略：
- en: '[PRE54]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The preceding code will print the following:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将输出以下内容：
- en: '[PRE55]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: As we can observe, our optimal policy tells us to perform the correct action
    in each state. Thus, we learned how to perform the policy iteration method to
    compute the optimal policy.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们的最优策略告诉我们在每个状态下执行正确的动作。因此，我们学会了如何执行策略迭代方法来计算最优策略。
- en: Is DP applicable to all environments?
  id: totrans-515
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态规划（DP）是否适用于所有环境？
- en: In dynamic programming, that is, in the value and policy iteration methods,
    we try to find the optimal policy.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 在动态规划中，也就是在值迭代和策略迭代方法中，我们尝试找到最优策略。
- en: '**Value iteration**: In the value iteration method, we compute the optimal
    value function by taking the maximum over the Q function (Q values) iteratively:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '**值迭代**：在值迭代方法中，我们通过对Q函数（Q值）进行迭代计算最大值来得到最优值函数：'
- en: '![](img/B15558_03_088.png)'
  id: totrans-518
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_088.png)'
- en: Where ![](img/B15558_03_123.png). After finding the optimal value function,
    we extract the optimal policy from it.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 其中！[](img/B15558_03_123.png)。找到最优值函数后，我们从中提取最优策略。
- en: '**Policy iteration**: In the policy iteration method, we compute the optimal
    value function using the policy iteratively:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '**策略迭代**：在策略迭代方法中，我们通过迭代计算策略来得到最优值函数：'
- en: '![](img/B15558_03_192.png)'
  id: totrans-521
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_03_192.png)'
- en: We will start off with the random policy and compute the value function. Once
    we have found the optimal value function, then the policy that is used to create
    the optimal value function will be the optimal policy.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从随机策略开始并计算值函数。一旦找到最优值函数，那么用来生成最优值函数的策略就是最优策略。
- en: If you look at the preceding two equations, in order to find the optimal policy,
    we compute the value function and Q function. But to compute the value and the
    Q function, we need to know the transition probability ![](img/B15558_03_175.png)
    of the environment, and when we don't know the transition probability of the environment,
    we cannot compute the value and the Q function in order to find the optimal policy.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看一下前面的两个方程，为了找到最优策略，我们计算值函数和Q函数。但为了计算值函数和Q函数，我们需要知道环境的转移概率 ![](img/B15558_03_175.png)，而当我们不知道环境的转移概率时，就无法计算值函数和Q函数来找到最优策略。
- en: That is, dynamic programming is a model-based method and to apply this method,
    we need to know the model dynamics (transition probability) of the environment,
    and when we don't know the model dynamics, we cannot apply the dynamic programming
    method.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，动态规划是一种基于模型的方法，要应用此方法，我们需要知道环境的模型动态（转移概率），而当我们不知道模型动态时，就无法应用动态规划方法。
- en: Okay, how can we find the optimal policy when we don't know the model dynamics
    of the environment? In such a case, we can use model-free methods. In the next
    chapter, we will learn about one of the interesting model-free methods, called
    Monte Carlo, and how it is used to find the optimal policy without requiring the
    model dynamics.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，当我们不知道环境的模型动态时，如何找到最优策略呢？在这种情况下，我们可以使用无模型方法。在下一章中，我们将学习一种有趣的无模型方法，叫做蒙特卡洛方法，看看它如何在不需要模型动态的情况下找到最优策略。
- en: Summary
  id: totrans-526
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started off the chapter by understanding the Bellman equation of the value
    and Q functions. We learned that, according to the Bellman equation, the value
    of a state is the sum of the immediate reward, and the discounted value of the
    next state and the value of a state-action pair is the sum of the immediate reward
    and the discounted value of the next state-action pair. Then we learned about
    the optimal Bellman value function and the Q function, which gives the maximum
    value.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过理解价值函数和Q函数的贝尔曼方程开始本章内容。我们了解到，根据贝尔曼方程，一个状态的值是即时奖励与下一个状态的折扣值之和，而一个状态-动作对的值是即时奖励与下一个状态-动作对的折扣值之和。接着我们学习了最优贝尔曼值函数和Q函数，后者给出了最大值。
- en: Moving forward, we learned about the relation between the value and Q functions.
    We learned that the value function can be extracted from the Q function as ![](img/B15558_03_088.png)
    and then we learned that the Q function can be extracted from the value function
    as ![](img/B15558_03_199.png).
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们学习了价值函数与Q函数之间的关系。我们了解到，价值函数可以从Q函数中提取，公式为 ![](img/B15558_03_088.png)，然后我们学习了Q函数如何从价值函数中提取，公式为
    ![](img/B15558_03_199.png)。
- en: Later we learned about two interesting methods called value iteration and policy
    iteration, which use dynamic programming to find the optimal policy.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 后来我们学习了两种有趣的方法，分别是价值迭代和策略迭代，它们使用动态规划来找到最优策略。
- en: In the value iteration method, first, we compute the optimal value function
    by taking the maximum over the Q function iteratively. Once we have found the
    optimal value function, then we use it to extract the optimal policy. In the policy
    iteration method, we try to compute the optimal value function using the policy
    iteratively. Once we have found the optimal value function, then the policy that
    is used to create the optimal value function will be extracted as the optimal
    policy.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 在价值迭代方法中，首先我们通过对Q函数取最大值来迭代计算最优价值函数。找到最优价值函数后，我们使用它来提取最优策略。而在策略迭代方法中，我们尝试通过策略迭代计算最优价值函数。一旦找到最优价值函数，使用该价值函数生成的策略就会被提取为最优策略。
- en: Questions
  id: totrans-531
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s try answering the following questions to assess our knowledge of what
    we learned in this chapter:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试回答以下问题，以评估我们对本章所学知识的掌握程度：
- en: Define the Bellman equation.
  id: totrans-533
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义贝尔曼方程。
- en: What is the difference between the Bellman expectation and Bellman optimality
    equations?
  id: totrans-534
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 贝尔曼期望方程与贝尔曼最优方程有何区别？
- en: How do we derive the value function from the Q function?
  id: totrans-535
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何从Q函数推导出价值函数？
- en: How do we derive the Q function from the value function?
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何从价值函数推导出Q函数？
- en: What are the steps involved in value iteration?
  id: totrans-537
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 价值迭代涉及哪些步骤？
- en: What are the steps involved in policy iteration?
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 策略迭代涉及哪些步骤？
- en: How does policy iteration differ from value iteration?
  id: totrans-539
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 策略迭代与价值迭代有什么不同？
