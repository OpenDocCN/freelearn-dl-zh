- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Regression and Classification
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归与分类
- en: 'Regression and classification are two fundamental tasks ubiquitously present
    in almost all machine learning applications. They find application in varied fields
    ranging from engineering, physical science, biology, and the financial market,
    to the social sciences. They are the fundamental tools in the hands of statisticians
    and data scientists. In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 回归和分类是几乎所有机器学习应用中的两大基本任务。它们在从工程学、物理科学、生物学、金融市场到社会科学等众多领域都有应用。这些任务是统计学家和数据科学家手中的基本工具。在本章中，我们将涵盖以下主题：
- en: Regression
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归
- en: Classification
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类
- en: Difference between classification and regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类与回归的区别
- en: Linear regression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归
- en: Different types of linear regression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型的线性回归
- en: Classification using the TensorFlow Keras API
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Keras API 进行分类
- en: Applying linear regression to estimate the price of a house
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用线性回归估算房价
- en: Applying logistic regression to identify handwritten digits
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用逻辑回归识别手写数字
- en: All the code files for this chapter can be found at [https://packt.link/dltfchp2](https://packt.link/dltfchp2)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码文件可以在[https://packt.link/dltfchp2](https://packt.link/dltfchp2)找到
- en: Let us first start with understanding what regression really is.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先了解回归究竟是什么。
- en: What is regression?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是回归？
- en: Regression is normally the first algorithm that people in machine learning work
    with. It allows us to make predictions from data by learning about the relationship
    between a given set of dependent and independent variables. It has its use in
    almost every field; anywhere that has an interest in drawing relationships between
    two or more things will find a use for regression.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 回归通常是机器学习领域中人们接触的第一个算法。它通过学习给定的一组因变量和自变量之间的关系，让我们能够从数据中进行预测。回归几乎在每个领域都有应用；任何需要分析两者或更多事物之间关系的地方，都能找到回归的用武之地。
- en: 'Consider the case of house price estimation. There are many factors that can
    have an impact on the house price: the number of rooms, the floor area, the locality,
    the availability of amenities, the parking space, and so on. Regression analysis
    can help us in finding the mathematical relationship between these factors and
    the house price.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以房价估算为例。影响房价的因素有很多：房间数量、面积、地理位置、设施的可用性、停车空间等。回归分析可以帮助我们找到这些因素与房价之间的数学关系。
- en: 'Let us imagine a simpler world where only the area of the house determines
    its price. Using regression, we could determine the relationship between the area
    of the house (**independent variable**: these are the variables that do not depend
    upon any other variables) and its price (**dependent variable**: these variables
    depend upon one or more independent variables). Later, we could use this relationship
    to predict the price of any house, given its area. To learn more about dependent
    and independent variables and how to identify them, you can refer to this post:
    [https://medium.com/deeplearning-concepts-and-implementation/independent-and-dependent-variables-in-machine-learning-210b82f891db](https://medium.com/deeplearning-concepts-and-implementation/independent-and-dependent-variables-in-machine-learning-210b82f891db).
    In machine learning, the independent variables are normally input into the model
    and the dependent variables are output from our model.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设想一个更简单的世界，在这个世界中，只有房屋的面积决定其价格。通过回归分析，我们可以确定房屋面积（**自变量**：这些是不会依赖于其他变量的变量）与其价格（**因变量**：这些变量依赖于一个或多个自变量）之间的关系。之后，我们可以利用这个关系来预测任何房屋的价格，只要知道其面积。若想进一步了解自变量和因变量以及如何识别它们，可以参考这篇文章：[https://medium.com/deeplearning-concepts-and-implementation/independent-and-dependent-variables-in-machine-learning-210b82f891db](https://medium.com/deeplearning-concepts-and-implementation/independent-and-dependent-variables-in-machine-learning-210b82f891db)。在机器学习中，自变量通常作为输入提供给模型，而因变量则是从模型中输出的结果。
- en: 'Depending upon the number of independent variables, the number of dependent
    variables, and the relationship type, we have many different types of regression.
    There are two important components of regression: the *relationship* between independent
    and dependent variables, and the *strength of impact* of different independent
    variables on dependent variables. In the following section, we will learn in detail
    about the widely used linear regression technique.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 根据自变量的数量、因变量的数量以及关系类型，我们有许多不同类型的回归。回归的两个重要组成部分是：自变量和因变量之间的 *关系*，以及不同自变量对因变量的
    *影响强度*。在接下来的部分中，我们将详细学习广泛使用的线性回归技术。
- en: Prediction using linear regression
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用线性回归进行预测
- en: '**Linear regression** is one of the most widely known modeling techniques.
    Existing for more than 200 years, it has been explored from almost all possible
    angles. Linear regression assumes a linear relationship between the input variable
    (*X*) and the output variable (*Y*). The basic idea of linear regression is building
    a model, using training data that can predict the output given the input, such
    that the predicted output ![](img/B18331_02_001.png) is as near the observed training
    output *Y* for the input *X*. It involves finding a linear equation for the predicted
    value ![](img/B18331_02_001.png) of the form:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性回归** 是最广为人知的建模技术之一。已有超过 200 年的历史，几乎从所有可能的角度进行了探索。线性回归假设输入变量（*X*）与输出变量（*Y*）之间存在线性关系。线性回归的基本思想是建立一个模型，利用训练数据在给定输入的情况下预测输出，使得预测输出
    ![](img/B18331_02_001.png) 尽可能接近训练数据中的观测输出 *Y*。它涉及寻找预测值 ![](img/B18331_02_001.png)
    的线性方程，形式为：'
- en: '![](img/B18331_02_003.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_003.png)'
- en: 'where ![](img/B18331_02_004.png) are the *n* input variables, and ![](img/B18331_02_005.png)
    are the linear coefficients, with *b* as the bias term. We can also expand the
    preceding equation to:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B18331_02_004.png) 是 *n* 个输入变量，![](img/B18331_02_005.png) 是线性系数，*b*
    是偏差项。我们还可以将前述方程扩展为：
- en: '![](img/B18331_02_006.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_006.png)'
- en: 'The bias term allows our regression model to provide an output even in the
    absence of any input; it provides us with an option to shift our data for a better
    fit. The error between the observed values (*Y*) and predicted values (![](img/B18331_02_001.png))
    for an input sample *i* is:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差项使我们的回归模型即使在没有任何输入的情况下也能提供输出；它为我们提供了一个将数据偏移以更好拟合的选项。对于输入样本 *i*，观测值（*Y*）和预测值
    (![](img/B18331_02_001.png)) 之间的误差为：
- en: '![](img/B18331_02_008.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_008.png)'
- en: The goal is to find the best estimates for the coefficients *W* and bias *b*,
    such that the error between the observed values *Y* and the predicted values ![](img/B18331_02_001.png)
    is minimized. Let’s go through some examples to better understand this.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是找到最佳的系数 *W* 和偏差 *b* 的估计值，使得观测值 *Y* 和预测值 ![](img/B18331_02_001.png) 之间的误差最小化。让我们通过一些例子更好地理解这一点。
- en: Simple linear regression
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单线性回归
- en: 'If we consider only one independent variable and one dependent variable, what
    we get is a simple linear regression. Consider the case of house price prediction,
    defined in the preceding section; the area of the house (*A*) is the independent
    variable, and the price (*Y*) of the house is the dependent variable. We want
    to find a linear relationship between predicted price ![](img/B18331_02_001.png)
    and *A*, of the form:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只考虑一个自变量和一个因变量，那么我们得到的是一个简单的线性回归。考虑前一节定义的房价预测的例子；房屋的面积（*A*）是自变量，而房屋的价格（*Y*）是因变量。我们希望找到预测价格
    ![](img/B18331_02_001.png) 和 *A* 之间的线性关系，形式为：
- en: '![](img/B18331_02_011.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_011.png)'
- en: 'where *b* is the bias term. Thus, we need to determine *W* and *b*, such that
    the error between the price *Y* and the predicted price ![](img/B18331_02_001.png)
    is minimized. The standard method used to estimate *W* and *b* is called the method
    of least squares, that is, we try to minimize the sum of the square of errors
    (*S*). For the preceding case, the expression becomes:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *b* 是偏差项。因此，我们需要确定 *W* 和 *b*，使得价格 *Y* 和预测价格 ![](img/B18331_02_001.png) 之间的误差最小化。估计
    *W* 和 *b* 的标准方法称为最小二乘法，即我们试图最小化误差平方和（*S*）。对于前述情况，表达式变为：
- en: '![](img/B18331_02_013.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_013.png)'
- en: 'We want to estimate the regression coefficients, *W* and *b*, such that *S*
    is minimized. We use the fact that the derivative of a function is 0 at its minima
    to get these two equations:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望估计回归系数 *W* 和 *b*，使得 *S* 最小化。我们利用函数在极值点的导数为 0 的事实，得到这两个方程：
- en: '![](img/B18331_02_014.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_014.png)'
- en: '![](img/B18331_02_015.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_015.png)'
- en: 'These two equations can be solved to find the two unknowns. To do so, we first
    expand the summation in the second equation:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个方程可以解出两个未知数。为此，我们首先展开第二个方程中的求和：
- en: '![](img/B18331_02_016.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_016.png)'
- en: 'Take a look at the last term on the left-hand side; it just sums up a constant
    *N* time. Thus, we can rewrite it as:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下左侧最后一项，它只是将常数*N*加和了*N*次。因此，我们可以将其重新写为：
- en: '![](img/B18331_02_017.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_017.png)'
- en: 'Reordering the terms, we get:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 重排项后，我们得到：
- en: '![](img/B18331_02_018.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_018.png)'
- en: 'The two terms on the right-hand side can be replaced by ![](img/B18331_02_019.png),
    the average price (output), and ![](img/B18331_02_020.png), the average area (input),
    respectively, and thus we get:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 右边的两个项可以被 ![](img/B18331_02_019.png)（平均价格，输出）和 ![](img/B18331_02_020.png)（平均面积，输入）替代，从而得到：
- en: '![](img/B18331_02_021.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_021.png)'
- en: 'In a similar fashion, we expand the partial differential equation of *S* with
    respect to weight *W*:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，我们展开关于权重*W*的* S*的偏微分方程：
- en: '![](img/B18331_02_022.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_022.png)'
- en: 'Substitute the expression for the bias term *b*:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 代入偏置项*b*的表达式：
- en: '![](img/B18331_02_023.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_023.png)'
- en: 'Reordering the equation:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 重排方程：
- en: '![](img/B18331_02_024.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_024.png)'
- en: 'Playing around with the mean definition, we can get from this the value of
    weight *W* as:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过玩弄均值定义，我们可以从中得到权重*W*的值，如下所示：
- en: '![](img/B18331_02_025.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_025.png)'
- en: 'where ![](img/B18331_02_019.png) and ![](img/B18331_02_020.png) are the average
    price and area, respectively. Let us try this on some simple sample data:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B18331_02_019.png) 和 ![](img/B18331_02_020.png) 分别是平均价格和面积。让我们尝试在一些简单的样本数据上进行此操作：
- en: 'We import the necessary modules. It is a simple example, so we’ll be using
    only NumPy, pandas, and Matplotlib:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入必要的模块。这个是一个简单的例子，因此我们只使用 NumPy、pandas 和 Matplotlib：
- en: '[PRE0]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we generate random data with a linear relationship. To make it more realistic,
    we also add a random noise element. You can see the two variables (the cause,
    `area`, and the effect, `price`) follow a positive linear dependence:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们生成具有线性关系的随机数据。为了使其更具现实性，我们还添加了一个随机噪声元素。你可以看到两个变量（原因，`area`，和效果，`price`）呈正线性关系：
- en: '[PRE1]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Chart, scatter chart  Description automatically generated](img/B18331_02_01.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, scatter chart  Description automatically generated](img/B18331_02_01.png)'
- en: 'Figure 2.1: Scatter plot between the area of the house and its price'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：房屋面积与价格之间的散点图
- en: 'Now, we calculate the two regression coefficients using the equations we defined.
    You can see the result is very much near the linear relationship we have simulated:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用我们定义的方程来计算两个回归系数。你可以看到结果非常接近我们模拟的线性关系：
- en: '[PRE2]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let us now try predicting the new prices using the obtained weight and bias
    values:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用获得的权重和偏置值预测新的价格：
- en: '[PRE4]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we plot the predicted prices along with the actual price. You can see
    that predicted prices follow a linear relationship with the area:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将预测的价格与实际价格一起绘制出来。你可以看到，预测的价格与面积呈线性关系：
- en: '[PRE5]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![A close up of a map  Description automatically generated](img/B18331_02_02.png)'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![A close up of a map  Description automatically generated](img/B18331_02_02.png)'
- en: 'Figure 2.2: Predicted values vs the actual price'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.2：预测值与实际价格
- en: From *Figure 2.2*, we can see that the predicted values follow the same trend
    as the actual house prices.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 2.2*中，我们可以看到，预测值与实际房价趋势相同。
- en: Multiple linear regression
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多元线性回归
- en: 'The preceding example was simple, but that is rarely the case. In most problems,
    the dependent variables depend upon multiple independent variables. Multiple linear
    regression finds a linear relationship between the many independent input variables
    (*X*) and the dependent output variable (*Y*), such that they satisfy the predicted
    *Y* value of the form:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的例子很简单，但在大多数问题中并非如此。在大多数问题中，因变量依赖于多个自变量。多元线性回归找到许多自变量(*X*)和因变量(*Y*)之间的线性关系，使得它们满足如下形式的预测值*Y*：
- en: '![](img/B18331_02_003.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_003.png)'
- en: where ![](img/B18331_02_029.png) are the *n* independent input variables, and
    ![](img/B18331_02_005.png)are the linear coefficients, with *b* as the bias term.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B18331_02_029.png) 是*n*个独立的输入变量，而 ![](img/B18331_02_005.png) 是线性系数，*b*是偏置项。
- en: 'As before, the linear coefficients *W*[s] are estimated using the method of
    least squares, that is, minimizing the sum of squared differences between predicted
    values (![](img/B18331_02_001.png)) and observed values (*Y*). Thus, we try to
    minimize the loss function (also called squared error, and if we divide by *n*,
    it is the mean squared error):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，线性系数*W*[s]是通过最小二乘法来估计的，即最小化预测值（![](img/B18331_02_001.png)）和观测值（*Y*）之间的平方差之和。因此，我们试图最小化损失函数（也叫平方误差，如果我们除以*n*，它就是均方误差）：
- en: '![](img/B18331_02_032.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_032.png)'
- en: where the sum is over all the training samples.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中求和是对所有训练样本进行的。
- en: As you might have guessed, now, instead of two, we will have *n+1* equations,
    which we will need to simultaneously solve. An easier alternative will be to use
    the TensorFlow Keras API. We will learn shortly how to use the TensorFlow Keras
    API to perform the task of regression.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所猜测的那样，现在我们将不再是两个方程，而是会有*n+1*个方程，我们需要同时求解它们。一个更简单的替代方法是使用TensorFlow Keras
    API。我们将很快学习如何使用TensorFlow Keras API来执行回归任务。
- en: Multivariate linear regression
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多元线性回归
- en: 'There can be cases where the independent variables affect more than one dependent
    variable. For example, consider the case where we want to predict a rocket’s speed
    and its carbon dioxide emission – these two will now be our dependent variables,
    and both will be affected by the sensors reading the fuel amount, engine type,
    rocket body, and so on. This is a case of multivariate linear regression. Mathematically,
    a multivariate regression model can be represented as:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，自变量可能影响多个因变量。例如，考虑这样一个情况，我们希望预测火箭的速度和二氧化碳排放量——这两个将是我们的因变量，且它们都会受到传感器读取的燃料量、发动机类型、火箭机体等因素的影响。这就是多元线性回归的一个例子。在数学上，一个多元回归模型可以表示为：
- en: '![](img/B18331_02_033.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_033.png)'
- en: where ![](img/B18331_02_034.png) and ![](img/B18331_02_035.png). The term ![](img/B18331_02_036.png)
    represents the *j*^(th) predicted output value corresponding to the *i*^(th) input
    sample, *w* represents the regression coefficients, and *x*[ik] is the *k*^(th)
    feature of the *i*^(th) input sample. The number of equations needed to solve
    in this case will now be *n x m*. While we can solve these equations using matrices,
    the process will be computationally expensive as it will involve calculating the
    inverse and determinants. An easier way would be to use the gradient descent with
    the sum of least square error as the loss function and to use one of the many
    optimizers that the TensorFlow API includes.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中![](img/B18331_02_034.png)和![](img/B18331_02_035.png)。项![](img/B18331_02_036.png)表示第*j*个预测输出值，对应于第*i*个输入样本，*w*表示回归系数，*x*[ik]是第*i*个输入样本的第*k*个特征。需要求解的方程个数将是*n
    x m*。虽然我们可以使用矩阵来解这些方程，但这个过程计算量较大，因为它需要计算矩阵的逆和行列式。一个更简单的方法是使用梯度下降，并将最小二乘误差的和作为损失函数，使用TensorFlow
    API中包含的多种优化器之一。
- en: In the next section, we will delve deeper into the TensorFlow Keras API, a versatile
    higher-level API to develop your model with ease.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入探讨TensorFlow Keras API，这是一种多功能的高级API，可以轻松开发你的模型。
- en: Neural networks for linear regression
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于线性回归的神经网络
- en: In the preceding sections, we used mathematical expressions for calculating
    the coefficients of a linear regression equation. In this section, we will see
    how we can use the neural networks to perform the task of regression and build
    a neural network model using the TensorFlow Keras API.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们使用了数学表达式来计算线性回归方程的系数。在本节中，我们将看到如何使用神经网络来执行回归任务，并利用TensorFlow Keras
    API构建一个神经网络模型。
- en: 'Before performing regression using neural networks, let us first review what
    a neural network is. Simply speaking, a neural network is a network of many artificial
    neurons. From *Chapter 1*, *Neural Network Foundations with TF*, we know that
    the simplest neural network, the (simple) perceptron, can be mathematically represented
    as:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用神经网络进行回归之前，我们先回顾一下什么是神经网络。简而言之，神经网络是由许多人工神经元组成的网络。从*第1章*，《使用TF的神经网络基础》中，我们知道最简单的神经网络——（简单）感知器，可以用数学表达式表示为：
- en: '![](img/B18331_02_037.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_037.png)'
- en: where *f* is the activation function. Consider, if we have *f* as a linear function,
    then the above expression is similar to the expression of linear regression that
    we learned in the previous section. In other words, we can say that a neural network,
    which is also called a function approximator, is a generalized regressor. Let
    us try to build a neural network simple regressor next using the TensorFlow Keras
    API.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*f*是激活函数。假设我们将*f*设为线性函数，那么上述表达式类似于我们在前一节中学习的线性回归的表达式。换句话说，我们可以说，神经网络（也叫做函数逼近器）是一个广义的回归器。接下来，让我们尝试使用TensorFlow
    Keras API构建一个简单的神经网络回归器。
- en: Simple linear regression using TensorFlow Keras
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorFlow Keras进行简单线性回归
- en: 'In the first chapter, we learned about how to build a model in TensorFlow Keras.
    Here, we will use the same `Sequential` API to build a single-layered perceptron
    (fully connected neural network) using the `Dense` class. We will continue with
    the same problem, that is, predicting the price of a house given its area:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们学习了如何在TensorFlow Keras中构建模型。在这里，我们将使用相同的`Sequential` API来使用`Dense`类构建一个单层感知机（全连接神经网络）。我们将继续使用相同的问题，也就是根据房屋的面积预测价格：
- en: 'We start with importing the packages we will need. Notice the addition of the
    `Keras` module and the `Dense` layer in importing packages:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从导入所需的包开始。注意到在导入包时添加了`Keras`模块和`Dense`层：
- en: '[PRE6]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we generate the data, as in the previous case:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们生成数据，像之前的案例一样：
- en: '[PRE7]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The input to neural networks should be normalized; this is because input gets
    multiplied with weights, and if we have very large numbers, the result of multiplication
    will be large, and soon our metrics may cross infinity (the largest number your
    computer can handle):'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络的输入应该进行标准化；这是因为输入会与权重相乘，如果我们有非常大的数值，那么乘积结果会很大，很快我们的度量可能会超出计算机能够处理的最大值（无穷大）：
- en: '[PRE8]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let us now build the model; since it is a simple linear regressor, we use a
    `Dense` layer with only one unit:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们构建模型；由于这是一个简单的线性回归器，我们使用一个仅有一个单元的`Dense`层：
- en: '[PRE9]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To train a model, we will need to define the loss function and optimizer. The
    loss function defines the quantity that our model tries to minimize, and the optimizer
    decides the minimization algorithm we are using. Additionally, we can also define
    metrics, which is the quantity we want to log as the model is trained. We define
    the loss function, `optimizer` (see *Chapter 1*, *Neural Network Foundations with
    TF*), and metrics using the `compile` function:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要训练一个模型，我们需要定义损失函数和优化器。损失函数定义了我们的模型试图最小化的量，而优化器决定了我们使用的最小化算法。此外，我们还可以定义度量指标，即在模型训练过程中我们希望记录的量。我们通过`compile`函数定义损失函数、`optimizer`（参见*第一章*，*神经网络基础与TF*）和度量指标：
- en: '[PRE11]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now that model is defined, we just need to train it using the `fit` function.
    Observe that we are using a `batch_size` of 32 and splitting the data into training
    and validation datasets using the `validation_spilt` argument of the `fit` function:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在模型已经定义好了，我们只需要使用`fit`函数训练它。注意我们使用了`batch_size`为32，并通过`fit`函数的`validation_split`参数将数据划分为训练集和验证集：
- en: '[PRE12]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Well, you have successfully trained a neural network to perform the task of
    linear regression. The mean squared error after training for 100 epochs is 0.0815
    on training data and 0.074 on validation data. We can get the predicted value
    for a given input using the `predict` function:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 好的，你已经成功地训练了一个神经网络来执行线性回归任务。训练100个epoch后的均方误差为训练数据上的0.0815，验证数据上的0.074。我们可以通过`predict`函数获得给定输入的预测值：
- en: '[PRE14]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we plot a graph of the predicted and the actual data:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们绘制预测数据与实际数据的图表：
- en: '[PRE15]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '*Figure 2.3* shows the plot between the predicted data and the actual data.
    You can see that, just like the linear regressor, we have got a nice linear fit:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*图2.3*显示了预测数据与实际数据之间的图表。你可以看到，就像线性回归器一样，我们得到了一个良好的线性拟合：'
- en: '![Chart, scatter chart  Description automatically generated](img/B18331_02_03.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, scatter chart  Description automatically generated](img/B18331_02_03.png)'
- en: 'Figure 2.3: Predicted price vs actual price'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3：预测价格与实际价格
- en: 'In case you are interested in knowing the coefficients `W` and `b`, we can
    do it by printing the weights of the model using `model.weights`:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你有兴趣了解系数`W`和`b`，我们可以通过打印模型的权重`model.weights`来查看：
- en: '[PRE16]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We can see from the result above that our coefficients are `W= 0.69` and bias
    `b= 0.127`. Thus, using linear regression, we can find a linear relationship between
    the house price and its area. In the next section, we explore multiple and multivariate
    linear regression using the TensorFlow Keras API.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的结果中我们可以看到我们的系数是`W= 0.69`，偏差是`b= 0.127`。因此，使用线性回归，我们可以找到房价与面积之间的线性关系。在下一节中，我们将使用
    TensorFlow Keras API 探索多元线性回归。
- en: Multiple and multivariate linear regression using the TensorFlow Keras API
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Keras API 进行多元线性回归
- en: The example in the previous section had only one independent variable, the *area*
    of the house, and one dependent variable, the *price* of the house. However, problems
    in real life are not that simple; we may have more than one independent variable,
    and we may need to predict more than one dependent variable. As you must have
    realized from the discussion on multiple and multivariate regression, they involve
    solving multiple equations. We can make our tasks easier by using the Keras API
    for both tasks.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 前面一节中的示例只有一个自变量，即房子的*面积*，和一个因变量，即房子的*价格*。然而，现实生活中的问题并非如此简单；我们可能有多个自变量，并且可能需要预测多个因变量。正如你从多元线性回归的讨论中所意识到的那样，这些问题涉及到求解多个方程。我们可以通过使用
    Keras API 来简化这两个任务。
- en: 'Additionally, we can have more than one neural network layer, that is, we can
    build a **deep neural network**. A deep neural network is like applying multiple
    function approximators:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以有多个神经网络层，也就是说，我们可以构建一个**深度神经网络**。深度神经网络就像是应用多个函数逼近器：
- en: '![](img/B18331_02_038.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_038.png)'
- en: 'with ![](img/B18331_02_039.png) being the function at layer *L*. From the expression
    above, we can see that if *f* was a linear function, adding multiple layers of
    a neural network was not useful; however, using a non-linear activation function
    (see *Chapter 1*, *Neural Network Foundations with TF*, for more details) allows
    us to apply neural networks to the regression problems where dependent and independent
    variables are related in some non-linear fashion. In this section, we will use
    a deep neural network, built using TensorFlow Keras, to predict the fuel efficiency
    of a car, given its number of cylinders, displacement, acceleration, and so on.
    The data we use is available from the UCI ML repository (Blake, C., & Merz, C.
    (1998), the UCI repository of machine learning databases ([http://www.ics.uci.edu/~mlearn/MLRepository.xhtml](http://www.ics.uci.edu/~mlearn/MLRepository.xhtml)):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B18331_02_039.png) 是*L*层的函数。从上面的表达式中，我们可以看到，如果*f*是线性函数，添加多层神经网络没有意义；然而，使用非线性激活函数（更多细节请参见*第一章*，*使用
    TF 的神经网络基础*）允许我们将神经网络应用于回归问题，在这些问题中，因变量和自变量以某种非线性方式相关。在本节中，我们将使用基于 TensorFlow
    Keras 构建的深度神经网络来预测一辆车的燃油效率，给定其气缸数、排量、加速度等。我们使用的数据来自 UCI ML 数据库（Blake, C., & Merz,
    C. (1998)，UCI 机器学习数据库：[http://www.ics.uci.edu/~mlearn/MLRepository.xhtml](http://www.ics.uci.edu/~mlearn/MLRepository.xhtml)）：
- en: 'We start by importing the modules that we will need. In the previous example,
    we normalized our data using the DataFrame operations. In this example, we will
    make use of the Keras `Normalization` layer. The `Normalization` layer shifts
    the data to a zero mean and one standard deviation. Also, since we have more than
    one independent variable, we will use Seaborn to visualize the relationship between
    different variables:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入我们需要的模块。在前面的示例中，我们使用了 DataFrame 操作来标准化数据。在本示例中，我们将使用 Keras 的`Normalization`层。`Normalization`层将数据移到均值为零，标准差为一的位置。此外，由于我们有多个自变量，我们将使用
    Seaborn 来可视化不同变量之间的关系：
- en: '[PRE17]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Let us first download the data from the UCI ML repo.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先从 UCI ML 数据库下载数据。
- en: '[PRE18]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The data consists of eight features: mpg, cylinders, displacement, horsepower,
    weight, acceleration, model year, and origin. Though the origin of the vehicle
    can also affect the fuel efficiency “mpg” (*miles per gallon*), we use only seven
    features to predict the mpg value. Also, we drop any rows with NaN values:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包括八个特征：mpg、气缸数、排量、马力、重量、加速度、模型年份和原产地。尽管车辆的原产地也可能影响燃油效率“mpg”（*每加仑英里数*），但我们只使用七个特征来预测mpg值。此外，我们删除包含NaN值的行：
- en: '[PRE19]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We divide the dataset into training and test datasets. Here, we are keeping
    80% of the 392 datapoints as training data and 20% as test dataset:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将数据集分为训练集和测试集。在这里，我们将392个数据点的80%作为训练数据，20%作为测试数据：
- en: '[PRE20]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we use Seaborn’s `pairplot` to visualize the relationship between the
    different variables:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用Seaborn的`pairplot`来可视化不同变量之间的关系：
- en: '[PRE21]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can see that mpg (fuel efficiency) has dependencies on all the other variables,
    and the dependency relationship is non-linear, as none of the curves are linear:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以看到，mpg（燃油效率）与其他所有变量都有依赖关系，而且这种依赖关系是非线性的，因为没有一条曲线是线性的：
- en: '![A picture containing text, electronics, display  Description automatically
    generated](img/B18331_02_04.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含文本、电子设备、显示器的图片 自动生成的描述](img/B18331_02_04.png)'
- en: 'Figure 2.4: Relationship among different variables of auto-mpg data'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4：auto-mpg数据中不同变量之间的关系
- en: 'For convenience, we also separate the variables into input variables and the
    label that we want to predict:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了方便，我们还将变量分为输入变量和我们想要预测的标签：
- en: '[PRE22]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, we use the Normalization layer of Keras to normalize our data. Note that
    while we normalized our inputs to a value with mean 0 and standard deviation 1,
    the output prediction `''mpg''` remains as it is:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用Keras的归一化层对数据进行归一化。请注意，尽管我们将输入归一化为均值为0，标准差为1的值，但输出预测的`'mpg'`保持不变：
- en: '[PRE23]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We build our model. The model has two hidden layers, with 64 and 32 neurons,
    respectively. For the hidden layers, we have used **Rectified Linear Unit** (**ReLU**)
    as our activation function; this should help in approximating the non-linear relation
    between fuel efficiency and the rest of the variables:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们构建了模型。模型有两个隐藏层，分别有64个和32个神经元。对于隐藏层，我们使用了**修正线性单元**（**ReLU**）作为激活函数；这应该有助于逼近燃油效率与其他变量之间的非线性关系：
- en: '[PRE24]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Earlier, we used stochastic gradient as the optimizer; this time, we try the
    Adam optimizer (see *Chapter 1*, *Neural Network Foundations with TF*, for more
    details). The loss function for the regression we chose is the mean squared error
    again:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之前我们使用随机梯度作为优化器，这次我们尝试使用Adam优化器（更多细节请参见*第1章*，*神经网络基础与TF*）。我们选择的回归损失函数仍然是均方误差：
- en: '[PRE25]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we train the model for 100 epochs:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们训练模型100个epoch：
- en: '[PRE26]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Cool, now that the model is trained, we can check if our model is overfitted,
    underfitted, or properly fitted by plotting the loss curve. Both validation loss
    and training loss are near each other as we increase the training epochs; this
    suggests that our model is properly trained:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 很酷，现在模型已经训练好了，我们可以通过绘制损失曲线来检查我们的模型是过拟合、欠拟合，还是拟合良好。随着训练epoch的增加，验证损失和训练损失彼此接近；这表明我们的模型已经得到了适当的训练：
- en: '[PRE27]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![Chart, line chart  Description automatically generated](img/B18331_02_05.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图 自动生成的描述](img/B18331_02_05.png)'
- en: 'Figure 2.5: Model error'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：模型误差
- en: 'Let us finally compare the predicted fuel efficiency and the true fuel efficiency
    on the test dataset. Remember that the model has not seen a test dataset ever,
    thus this prediction is from the model’s ability to generalize the relationship
    between inputs and fuel efficiency. If the model has learned the relationship
    well, the two should form a linear relationship:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们比较预测的燃油效率与测试数据集的真实燃油效率。记住，模型之前从未见过测试数据集，因此这个预测来自于模型对输入与燃油效率之间关系的泛化能力。如果模型已经很好地学习了这个关系，二者应该形成线性关系：
- en: '[PRE28]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![Chart, scatter chart  Description automatically generated](img/B18331_02_06.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图 自动生成的描述](img/B18331_02_06.png)'
- en: 'Figure 2.6: Plot between predicted fuel efficiency and actual values'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6：预测燃油效率与实际值之间的图示
- en: 'Additionally, we can also plot the error between the predicted and true fuel
    efficiency:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，我们还可以绘制预测值与真实燃油效率之间的误差：
- en: '[PRE29]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![Chart, histogram  Description automatically generated](img/B18331_02_07.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图表，直方图 自动生成的描述](img/B18331_02_07.png)'
- en: 'Figure 2.7: Prediction error'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7：预测误差
- en: In case we want to make more than one prediction, that is, dealing with a multivariate
    regression problem, the only change would be that instead of one unit in the last
    dense layer, we will have as many units as the number of variables to be predicted.
    Consider, for example, we want to build a model which takes into account a student’s
    SAT score, attendance, and some family parameters, and wants to predict the GPA
    score for all four undergraduate years; then we will have the output layer with
    four units. Now that you are familiar with regression, let us move toward the
    classification tasks.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要进行多个预测，即处理多元回归问题，那么唯一的变化就是：在最后的全连接层中，不是一个单元，而是根据要预测的变量个数来决定单元的数量。例如，假设我们想构建一个模型，该模型考虑学生的
    SAT 分数、出勤率和一些家庭参数，并预测四年本科学习的 GPA 分数；那么输出层就会有四个单元。现在你已经熟悉了回归，我们将开始讨论分类任务。
- en: Classification tasks and decision boundaries
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类任务与决策边界
- en: 'Till now, the focus of the chapter was on regression. In this section, we will
    talk about another important task: the task of classification. Let us first understand
    the difference between regression (also sometimes referred to as prediction) and
    classification:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本章的重点是回归。在本节中，我们将讨论另一个重要任务：分类任务。首先让我们了解回归（有时也称为预测）和分类之间的区别：
- en: In classification, the data is grouped into classes/categories, while in regression,
    the aim is to get a continuous numerical value for given data. For example, identifying
    the number of handwritten digits is a classification task; all handwritten digits
    will belong to one of the ten numbers lying between 0-9\. The task of predicting
    the price of the house depending upon different input variables is a regression
    task.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分类中，数据被分组为不同的类别，而在回归中，目标是为给定数据预测一个连续的数值。例如，识别手写数字是一个分类任务；所有的手写数字都会属于 0 到 9
    之间的某个数字。预测房价的任务则是一个回归任务，依据的是不同的输入变量。
- en: In a classification task, the model finds the decision boundaries separating
    one class from another. In the regression task, the model approximates a function
    that fits the input-output relationship.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分类任务中，模型会找到决策边界，将一个类别与另一个类别分开。而在回归任务中，模型会近似一个函数，拟合输入输出关系。
- en: Classification is a subset of regression; here, we are predicting classes. Regression
    is much more general.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类是回归的一个子集；在分类中，我们是预测类别。回归则更为通用。
- en: '*Figure 2.8* shows how classification and regression tasks differ. In classification,
    we need to find a line (or a plane or hyperplane in multidimensional space) separating
    the classes. In regression, the aim is to find a line (or plane or hyperplane)
    that fits the given input points:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.8* 显示了分类与回归任务的区别。在分类中，我们需要找到一条线（或者在多维空间中是平面或超平面）来分隔不同的类别。而在回归中，目标是找到一条线（或平面或超平面），使其尽可能适合给定的输入点：'
- en: '![](img/B18331_02_08.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_08.png)'
- en: 'Figure 2.8: Classification vs regression'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8：分类与回归
- en: In the following section, we will explain logistic regression, which is a very
    common and useful classification technique.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将解释逻辑回归，这是一种非常常见且有用的分类技术。
- en: Logistic regression
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'Logistic regression is used to determine the probability of an event. Conventionally,
    the event is represented as a categorical dependent variable. The probability
    of the event is expressed using the sigmoid (or “logit”) function:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归用于确定事件发生的概率。通常，事件被表示为一个类别型的因变量。事件的概率通过 sigmoid（或“logit”）函数表示：
- en: '![](img/B18331_02_040.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_040.png)'
- en: 'The goal now is to estimate weights ![](img/B18331_02_005.png) and bias term
    *b*. In logistic regression, the coefficients are estimated using either the maximum
    likelihood estimator or stochastic gradient descent. If *p* is the total number
    of input data points, the loss is conventionally defined as a cross-entropy term
    given by:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的目标是估计权重 ![](img/B18331_02_005.png) 和偏置项 *b*。在逻辑回归中，系数是通过最大似然估计或随机梯度下降法来估计的。如果
    *p* 是输入数据点的总数，则损失通常定义为交叉熵项，公式为：
- en: '![](img/B18331_02_042.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_042.png)'
- en: Logistic regression is used in classification problems. For example, when looking
    at medical data, we can use logistic regression to classify whether a person has
    cancer or not. If the output categorical variable has two or more levels, we can
    use multinomial logistic regression. Another common technique used for two or
    more output variables is one versus all.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归用于分类问题。例如，在查看医疗数据时，我们可以使用逻辑回归来分类一个人是否患有癌症。如果输出的类别变量有两个或更多级别，我们可以使用多项式逻辑回归。另一个常用的技术是“一个对所有”的方法，适用于两个或更多输出变量。
- en: 'For multiclass logistic regression, the cross-entropy loss function is modified
    as:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多类逻辑回归，交叉熵损失函数被修改为：
- en: '![](img/B18331_02_043.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_02_043.png)'
- en: where *K* is the total number of classes. You can read more about logistic regression
    at [https://en.wikipedia.org/wiki/Logistic_regression](https://en.wikipedia.org/wiki/Logistic_regression).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *K* 是类别的总数。你可以在 [https://en.wikipedia.org/wiki/Logistic_regression](https://en.wikipedia.org/wiki/Logistic_regression)
    上阅读更多关于逻辑回归的内容。
- en: Now that you have some idea about logistic regression, let us see how we can
    apply it to any dataset.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你对逻辑回归有了一些了解，让我们看看如何将它应用于任何数据集。
- en: Logistic regression on the MNIST dataset
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 MNIST 数据集上进行逻辑回归
- en: Next, we will use TensorFlow Keras to classify handwritten digits using logistic
    regression. We will be using the **MNIST** (**Modified National Institute of Standards
    and Technology**) dataset. For those working in the field of deep learning, MNIST
    is not new, it is like the ABC of machine learning. It contains images of handwritten
    digits and a label for each image, indicating which digit it is. The label contains
    a value lying between 0-9 depending on the handwritten digit. Thus, it is a multiclass
    classification.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 TensorFlow Keras 通过逻辑回归对手写数字进行分类。我们将使用 **MNIST**（**修改后的国家标准与技术研究院**）数据集。对于从事深度学习领域的人来说，MNIST
    并不陌生，它就像机器学习的 ABC。它包含手写数字的图像，并为每张图像提供标签，指示它是哪个数字。标签的值介于 0 到 9 之间，取决于手写数字。因此，这是一个多类分类问题。
- en: To implement the logistic regression, we will make a model with only one dense
    layer. Each class will be represented by a unit in the output, so since we have
    10 classes, the number of units in the output would be 10\. The probability function
    used in the logistic regression is similar to the sigmoid activation function;
    therefore, we use sigmoid activation.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现逻辑回归，我们将构建一个只有一个密集层的模型。每个类别将在输出中由一个单元表示，因此由于我们有 10 个类别，输出中的单元数将是 10。逻辑回归中使用的概率函数类似于
    Sigmoid 激活函数；因此，我们使用 Sigmoid 激活。
- en: 'Let us build our model:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建我们的模型：
- en: 'The first step is, as always, importing the modules needed. Notice that here
    we are using another useful layer from the Keras API, the `Flatten` layer. The
    `Flatten` layer helps us to resize the 28 x 28 two-dimensional input images of
    the MNIST dataset into a 784 flattened array:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一阶段，和往常一样，导入所需的模块。请注意，在这里我们使用了 Keras API 中另一个非常有用的层——`Flatten` 层。`Flatten`
    层帮助我们将 MNIST 数据集中的 28 x 28 的二维输入图像调整为一个 784 的展平数组：
- en: '[PRE30]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We take the input data of MNIST from the `tensorflow.keras` dataset:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从 `tensorflow.keras` 数据集中获取 MNIST 输入数据：
- en: '[PRE31]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we preprocess the data. We normalize the images; the MNIST dataset images
    are black and white images with the intensity value of each pixel lying between
    0-255\. We divide it by 255, so that now the values lie between 0-1:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们对数据进行预处理。我们对图像进行归一化；MNIST 数据集中的图像是黑白图像，每个像素的强度值介于 0 到 255 之间。我们将其除以 255，使得现在的值介于
    0 到 1 之间：
- en: '[PRE32]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, we define a very simple model; it has only one `Dense` layer with `10`
    units, and it takes an input of size 784\. You can see from the output of the
    model summary that only the `Dense` layer has trainable parameters:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们定义了一个非常简单的模型；它只有一个 `Dense` 层，具有 `10` 个单元，输入的大小是 784。你可以从模型摘要的输出中看到，只有 `Dense`
    层具有可训练的参数：
- en: '[PRE33]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Since the test labels are integral values, we will use `SparseCategoricalCrossentropy`
    loss with `logits` set to `True`. The optimizer selected is Adam. Additionally,
    we also define accuracy as metrics to be logged as the model is trained. We train
    our model for 50 epochs, with a train-validation split of 80:20:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于测试标签是整数值，我们将使用 `SparseCategoricalCrossentropy` 损失，并将 `logits` 设置为 `True`。选择的优化器是
    Adam。此外，我们还定义了准确度作为训练过程中要记录的指标。我们将训练模型 50 个 epoch，训练-验证拆分比例为 80:20：
- en: '[PRE35]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let us see how our simple model has fared by plotting the loss plot. You can
    see that since the validation loss and training loss are diverging, as the training
    loss is decreasing, the validation loss increases, thus the model is overfitting.
    You can improve the model performance by adding hidden layers:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过绘制损失图来看看我们简单模型的表现。你可以看到，由于验证损失和训练损失在分歧，训练损失在减少，而验证损失在增加，因此模型出现了过拟合。你可以通过添加隐藏层来提高模型性能：
- en: '[PRE36]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![Chart, line chart  Description automatically generated](img/B18331_02_09.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, line chart  Description automatically generated](img/B18331_02_09.png)'
- en: 'Figure 2.9: Loss plot'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9：损失图
- en: 'To better understand the result, we build two utility functions; these functions
    help us in visualizing the handwritten digits and the probability of the 10 units
    in the output:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了更好地理解结果，我们构建了两个实用函数；这些函数帮助我们可视化手写数字及输出中10个单元的概率：
- en: '[PRE37]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Using these utility functions, we plot the predictions:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些实用函数，我们绘制了预测结果：
- en: '[PRE38]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The plot on the left is the image of the handwritten digit, with the predicted
    label, the confidence in the prediction, and the true label. The image on the
    right shows the probability (logistic) output of the 10 units; we can see that
    the unit which represents the number 4 has the highest probability:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 左侧的图像是手写数字的图像，包含预测标签、预测的置信度以及真实标签。右侧的图像显示了10个单元的概率（逻辑斯蒂回归）输出；我们可以看到代表数字4的单元具有最高的概率：
- en: '![A picture containing logo  Description automatically generated](img/B18331_02_10.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing logo  Description automatically generated](img/B18331_02_10.png)'
- en: 'Figure 2.10: Predicted digit and confidence value of the prediction'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10：预测的数字及预测的置信度值
- en: 'In this code, to stay true to logistic regression, we used a sigmoid activation
    function and only one `Dense` layer. For better performance, adding dense layers
    and using softmax as the final activation function will be helpful. For example,
    the following model gives 97% accuracy on the validation dataset:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这段代码中，为了保持逻辑回归的特点，我们使用了 Sigmoid 激活函数和仅有的一个 `Dense` 层。为了获得更好的性能，添加更多的密集层并使用
    Softmax 作为最终激活函数将会更有帮助。例如，以下模型在验证数据集上达到了 97% 的准确率：
- en: '[PRE39]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: You can experiment by adding more layers, or by changing the number of neurons
    in each layer, and even changing the optimizer. This will give you a better understanding
    of how these parameters influence the model performance.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过添加更多层，或者改变每层的神经元数量，甚至更改优化器来进行实验。这将帮助你更好地理解这些参数如何影响模型性能。
- en: Summary
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter dealt with different types of regression algorithms. We started
    with linear regression and used it to predict house prices for a simple one-input
    variable case. We built simple and multiple linear regression models using the
    TensorFlow Keras API. The chapter then moved toward logistic regression, which
    is a very important and useful technique for classifying tasks. The chapter explained
    the TensorFlow Keras API and used it to implement both linear and logistic regression
    for some classical datasets. The next chapter will introduce you to convolutional
    neural networks, the most commercially successful neural network models for image
    data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了不同类型的回归算法。我们从线性回归开始，并用它来预测简单单输入变量案例中的房价。我们使用 TensorFlow Keras API 构建了简单和多重线性回归模型。接下来，本章讨论了逻辑回归，这是一种非常重要且实用的分类任务技术。章节解释了
    TensorFlow Keras API，并用它实现了线性回归和逻辑回归在一些经典数据集上的应用。下一章将向你介绍卷积神经网络，这是最成功的商业神经网络模型之一，专门用于图像数据。
- en: References
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Here are some good resources if you are interested in knowing more about the
    concepts we’ve covered in this chapter:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣了解本章所涵盖的概念，以下是一些不错的资源：
- en: 'TensorFlow website: [https://www.tensorflow.org/](https://www.tensorflow.org/)'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorFlow 官网：[https://www.tensorflow.org/](https://www.tensorflow.org/)
- en: '*Exploring bivariate numerical data*: [https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data](https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data)'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*探索双变量数值数据*：[https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data](https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data)'
- en: 'Murphy, K. P. (2022). *Probabilistic Machine Learning: An introduction*, MIT
    Press.'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Murphy, K. P. (2022). *概率机器学习：导论*，MIT出版社。
- en: 'Blake, C., & Merz, C. (1998). UCI repository of machine learning databases:
    [http://www.ics.uci.edu/~mlearn/MLRepository.xhtml](http://www.ics.uci.edu/~mlearn/MLRepository.xhtml%20)'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Blake, C., & Merz, C. (1998). UCI机器学习数据库库： [http://www.ics.uci.edu/~mlearn/MLRepository.xhtml](http://www.ics.uci.edu/~mlearn/MLRepository.xhtml%20)
- en: Join our book’s Discord space
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区，与志同道合的人相遇，并与2000多名成员一起学习： [https://packt.link/keras](https://packt.link/keras)
- en: '![](img/QR_Code18312172242788196871.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code18312172242788196871.png)'
