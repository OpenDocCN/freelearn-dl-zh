- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Welcome to Q-Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欢迎来到Q学习
- en: Ladies and gentlemen, things are about to get even more interesting than before.
    The next model we are about to tackle is at the heart of many AIs built today;
    robots, autonomous vehicles, and even AI players of video games. They all use
    Q-learning at the core of their model. Some of them even combine Q-learning with
    deep learning, making a highly advanced version of Q-learning called deep Q-learning,
    which we will cover in *Chapter 9*, *Going Pro with Artificial Brains – Deep Q-Learning*.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 各位女士们，先生们，事情即将变得比以往更有趣。我们即将处理的下一个模型是今天许多人工智能的核心；机器人、自动驾驶车辆，甚至视频游戏的AI玩家都在使用Q学习作为核心模型。它们中的一些甚至将Q学习与深度学习结合，创造出了一个更高级的Q学习版本，叫做深度Q学习，我们将在*第9章*《成为人工智能专家——深度Q学习》中讲解。
- en: 'All of the AI fundamentals still apply to Q-learning, as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的AI基础仍然适用于Q学习，具体如下：
- en: Q-learning is a Reinforcement Learning model.
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q学习是一个强化学习模型。
- en: Q-learning works on the inputs (states) and outputs (actions) principle.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q学习基于输入（状态）和输出（动作）的原理。
- en: Q-learning works on a predefined environment, including the states (the inputs),
    the actions (the outputs), and the rewards.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q学习在一个预定义的环境中工作，包括状态（输入）、动作（输出）和奖励。
- en: Q-learning is modeled by a Markov decision process.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q学习是通过马尔可夫决策过程建模的。
- en: Q-learning uses a training mode, during which the parameters that are learned
    are called the Q-values, and an inference mode.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q学习使用一种训练模式，在这个模式中，学习到的参数称为Q值，还有一种推理模式。
- en: 'Now we can add two more fundamentals, this time specific to Q-learning:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以再添加两个基本原理，这次是特定于Q学习的：
- en: There are a finite number of states (there is not an infinity of possible inputs).
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 状态是有限的（没有无限多的可能输入）。
- en: There are a finite number of actions (only a certain number of actions can be performed).
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动作是有限的（只能执行有限的动作）。
- en: That's all! There are no more fundamentals to keep in mind; now we can really
    dig into Q-learning, which you'll see is not that hard and really quite intuitive.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！没有更多需要记住的基础了；现在我们可以真正深入探讨Q学习，你会发现它其实并不难，而且非常直观。
- en: 'To explain Q-learning, we''ll use an example so that you won''t get lost inside
    pure theory, and so that you can visualize what''s happening. On that note: welcome
    to the Maze.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释Q学习，我们将使用一个例子，这样你就不会迷失在纯粹的理论中，也能直观地理解发生了什么。顺便说一句：欢迎来到迷宫。
- en: The Maze
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迷宫
- en: 'You are going to learn how Q-learning works inside a maze. Let''s draw our
    maze right away; here it is:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习Q学习在迷宫中的工作原理。我们现在就来画出我们的迷宫；它在这里：
- en: '![](img/B14110_07_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_07_01.png)'
- en: 'Figure 1: The Maze'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：迷宫
- en: I know, it's the simplest maze you have ever seen. That's important for the
    sake of simplicity, so that you can mostly focus on how the AI works its magic.
    Imagine if you got lost in this chapter because of the maze and not because of
    the AI formulas! The important thing is that you have a clear maze, and you can
    visualize how the AI might manage to find its way from the beginning to the end.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道，这个迷宫是你见过的最简单的迷宫。这对于简化问题很重要，这样你可以主要专注于人工智能如何发挥它的魔力。想象一下，如果你因为迷宫而迷失在这一章，而不是因为AI公式的话，那可就糟糕了！关键是你有一个清晰的迷宫，你可以想象人工智能是如何从起点走到终点的。
- en: Speaking of the beginning and the end, imagine a little robot inside this maze,
    starting at point **E** (Entrance). Its goal is to find the quickest way to point
    **G** (Goal). We humans can figure that out in no time, but that's only because
    our maze is so simple. What you are going to build is an AI that can go from a
    starting point to an ending point, regardless of how complex the maze is. Let's
    get started!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 说到开始和结束，想象一下一个小机器人在这个迷宫中，从**E**（入口）点开始。它的目标是找到通向**G**（目标）点的最快路径。我们人类能在瞬间搞定，但那只是因为我们的迷宫太简单了。你将要构建的是一个能够从起点走到终点的人工智能，不管迷宫有多复杂。我们开始吧！
- en: Beginnings
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开始
- en: 'Here is a question for you: what do you think is going to be the very first
    step?'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个问题：你认为第一步应该是什么？
- en: 'I''ll give you three possible answers:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我将给你三个可能的答案：
- en: We start writing some math equations.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们开始写一些数学方程。
- en: We build the environment.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们构建环境。
- en: We try to make it work with Thompson Sampling (the AI model of the previous
    chapter).
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们尝试通过汤普森采样（上一章的AI模型）使其工作。
- en: The correct answer is…
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案是……
- en: 2\. We build the environment.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 我们构建环境。
- en: That was easy, but I wanted to highlight that in a question to make sure you
    keep in mind that this must always be the first step when building an AI. After
    clearly understanding the problem, the first step of building your AI solution
    is always to set up the environment.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单，但我想通过提问来强调这一点，以确保你记住构建 AI 时这一点必须始终是第一步。在明确理解问题后，构建 AI 解决方案的第一步始终是设置环境。
- en: 'That begs a further question:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了一个进一步的问题：
- en: What steps, exactly, are you going to take when building that environment?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 构建该环境时，具体需要采取哪些步骤？
- en: Try to remember the answer—you've already learned this—and then read on for
    a recap.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试记住答案——你已经学过了——然后继续阅读以进行回顾。
- en: Firstly, you'll define the states (the inputs of your AI).
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你将定义状态（你 AI 的输入）。
- en: Secondly, you'll define the actions that can be performed (the outputs of your AI).
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其次，你将定义可以执行的动作（你 AI 的输出）。
- en: Thirdly, you'll define the rewards. Remember, the reward is what the AI gets after
    performing an action in a certain state.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三，你将定义奖励。记住，奖励是 AI 在某个状态下执行动作后获得的结果。
- en: Now we've secured the basics, so you can tackle that first step of defining
    the environment.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了基础，你可以开始处理定义环境的第一步。
- en: Building the environment
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建环境
- en: To build the environment, we need to define the states, the actions, and the
    rewards.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建环境，我们需要定义状态、动作和奖励。
- en: The states
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 状态
- en: Let's begin with the states. What do you think are going to be the states for
    this problem? Remember, the states are the inputs of your AI. And they should
    contain enough information for the AI to be able to take an action that will lead
    it to its final goal (reaching point E).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从状态开始。你认为这个问题的状态是什么？记住，状态是你 AI 的输入。它们应该包含足够的信息，让 AI 能够采取一种行动，从而带领它实现最终目标（到达
    E 点）。
- en: In this model, we don't have too much of a choice. The state, at a specific
    time or specific iteration, is simply going to be the position of the AI at that
    time. In other words, it is going to be the letter of the location, from **A**
    to **L**, where the AI is in at a specific time.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，我们没有太多选择。在特定时间或特定迭代时，状态将仅仅是 AI 当时的位置。换句话说，它将是 **A** 到 **L** 之间的字母，表示
    AI 在特定时刻所在的位置。
- en: 'As you might guess, the next step after building the environment will be writing
    the mathematical equations at the heart of the AI, and to help you with that,
    it makes it much easier to encode the states into unique integers instead of keeping
    them as letters. That''s exactly what we are going to do, with the following mapping:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能猜到的，构建环境之后的下一步是编写 AI 核心的数学方程式，为了帮助你做到这一点，将状态编码为独特的整数要比将其保持为字母更加容易。这正是我们将要做的，使用以下映射：
- en: '![](img/B14110_07_02.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_07_02.png)'
- en: 'Figure 2: Location to state mapping'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：位置与状态映射
- en: 'Notice that we abide by the first specific fundamental of Q-learning, that
    is: **there are a finite number of states**.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们遵守 Q 学习的第一个基本原则，即：**状态的数量是有限的**。
- en: Let's move on to the actions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续讨论动作。
- en: The actions
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动作
- en: The actions are simply going to be the next moves the AI can make to go from
    one location to the next. For example, let's say the AI is in location **J**;
    the possible actions that the AI can perform are to go to **I**, to **F**, or
    to **K**. Again, since you'll be working with math equations, you can encode these
    actions with the same indexes as for the states.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 动作将简单地是 AI 从一个位置移动到下一个位置的下一步。例如，假设 AI 当前在 **J** 位置，AI 可以执行的可能动作是前往 **I**、**F**
    或 **K**。同样，由于你将使用数学方程式，你可以将这些动作用与状态相同的索引进行编码。
- en: 'Following the example where the AI is in location **J** at a specific time,
    the possible actions that the AI can perform are **5**, **8**, and **10**, according
    to our previous mapping above: the index **5** corresponds to **F**, the index
    **8** corresponds to **I**, and the index **10** corresponds to **K**.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前的示例，假设 AI 在特定时间位于 **J** 位置，AI 可以执行的可能动作是 **5**、**8** 和 **10**，根据我们之前的映射：索引
    **5** 对应 **F**，索引 **8** 对应 **I**，索引 **10** 对应 **K**。
- en: 'Hence, the possible actions are simply the indexes of the different locations
    that can be reached:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可能的动作只是可以到达的不同位置的索引：
- en: Possible actions = {0,1,2,3,4,5,6,7,8,9,10,11}
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 可能的动作 = {0,1,2,3,4,5,6,7,8,9,10,11}
- en: 'Notice that again, we abide by the second specific fundamental of Q-learning,
    that is: **there are a finite number of actions**.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们再次遵循Q学习的第二个基本原则，即：**动作的数量是有限的**。
- en: Now obviously, when in a specific location, there are some actions that the
    AI cannot perform. Taking the same previous example, if the AI is in location
    **J**, it can perform the actions **5**, **8**, and **10**, but it cannot perform
    the other actions. You can make sure to specify that by attributing a 0 reward
    to the actions it cannot perform, and a 1 reward to the actions it can perform.
    That brings us to the rewards.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，在一个特定的位置时，有些动作是AI无法执行的。以之前的例子为例，如果AI处于位置**J**，它能执行动作**5**、**8**和**10**，但无法执行其他动作。你可以通过为无法执行的动作指定0奖励，为能够执行的动作指定1奖励来确保这一点。这就涉及到了奖励。
- en: The rewards
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 奖励
- en: 'You''re almost done with your environment—last, but not least, you have to
    define a system of rewards. More specifically, you have to define a reward function
    *R* that takes as input a state *s* and an action *a*, and returns a numerical
    reward *r* that the AI will get by performing the action *a* in the state *s*:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你离构建环境已经不远了——最后，你需要定义一个奖励系统。更具体地说，你需要定义一个奖励函数 *R*，它接受状态 *s* 和动作 *a* 作为输入，并返回一个数值奖励
    *r*，即AI在状态 *s* 下执行动作 *a* 时将获得的奖励：
- en: 'R: (s, a) ![](img/B14110_07_001.png) ![](img/B14110_07_002.png)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 'R: (s, a) ![](img/B14110_07_001.png) ![](img/B14110_07_002.png)'
- en: So, how can you build such a function for our case study? Here, it is simple.
    Since there are a discrete and finite number of states (the indexes from 0 to
    11), as well as a discrete and finite number of actions (same indexes from 0 to
    11), the best way to build your reward function *R* is to simply make a matrix.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何为我们的案例研究构建这样的函数呢？这里很简单。由于有离散且有限的状态数量（从0到11的索引），以及离散且有限的动作数量（同样是从0到11的索引），构建奖励函数
    *R* 的最佳方法就是直接构建一个矩阵。
- en: 'Your reward function will be a matrix of exactly 12 rows and 12 columns, where
    the rows correspond to the states, and the columns correspond to the actions.
    That way, in your function R: (s, a) ![](img/B14110_07_003.png) ![](img/B14110_07_004.png),
    *s* will be the row index of the matrix, *a* will be the column index of the matrix,
    and *r* will be the cell of index (*s*, *a*) in the matrix.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你的奖励函数将是一个恰好有12行12列的矩阵，其中行对应状态，列对应动作。这样，在你的函数R中： (s, a) ![](img/B14110_07_003.png)
    ![](img/B14110_07_004.png)，*s* 将是矩阵的行索引，*a* 将是矩阵的列索引，而 *r* 将是矩阵中索引为(*s*, *a*)的单元格的值。
- en: 'To build this reward matrix, what you first have to do is attribute, for each
    of the 12 locations, a 0 reward to the actions that the robot cannot perform,
    and a 1 reward to the actions the robot can perform. By doing that for each of
    the 12 locations, you will end up with a matrix of rewards. Let''s build it step
    by step, starting with the first location: location **A**.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建这个奖励矩阵，首先你需要为12个位置中的每一个指定一个奖励。对于机器人不能执行的动作，给予0奖励；对于机器人能执行的动作，给予1奖励。通过对每个位置执行这一步，你将得到一个奖励矩阵。我们从第一个位置：位置**A**开始，一步一步地构建它。
- en: 'When in location **A**, the robot can only go to location **B**. Therefore,
    since location **A** has index 0 (first row of the matrix) and location **B**
    has index 1 (second column of the matrix), the first row of the matrix of rewards
    will get a 1 on the second column, and a 0 on all the other columns, like so:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当机器人处于位置**A**时，它只能前往位置**B**。因此，由于位置**A**的索引为0（矩阵的第一行），而位置**B**的索引为1（矩阵的第二列），所以奖励矩阵的第一行将在第二列标记1，其他列则为0，如下所示：
- en: '![](img/B14110_07_03.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_07_03.png)'
- en: 'Figure 3: Rewards matrix – Step 1'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：奖励矩阵 - 第一步
- en: 'Let''s move on to location **B**. When in location **B**, the robot can only
    go to three different locations: **A**, **C**, and **F**. Since **B** has index
    1 (second row), and **A**, **C**, and **F** have respective indexes 0, 2, and
    5 (1st, 3rd, and 6th column), then the second row of the matrix of rewards will
    get a 1 on the 1st, 3rd, and 6th columns, and 0 on all the other columns:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续讨论位置**B**。当机器人处于位置**B**时，它只能前往三个不同的位置：**A**、**C**和**F**。由于**B**的索引为1（第二行），而**A**、**C**和**F**分别有索引0、2和5（第一列、第三列和第六列），因此奖励矩阵的第二行将在第一列、第三列和第六列上标记1，其他列则为0：
- en: '![](img/B14110_07_04.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_07_04.png)'
- en: 'Figure 4: Rewards matrix – Step 2'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：奖励矩阵 - 第二步
- en: '**C** (of index 2) is only connected to **B** and **G** (of indexes 1 and 6)
    so the third row of the matrix of rewards is:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**C**（索引为2）仅与**B**和**G**（索引为1和6）相连，因此奖励矩阵的第三行是：'
- en: '![](img/B14110_07_05.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_07_05.png)'
- en: 'Figure 5: Rewards matrix – Step 3'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：奖励矩阵 – 步骤3
- en: 'By doing the same for all the other locations, you eventually get your final
    matrix of rewards:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有其他地点做相同的操作，最终你将得到最终的奖励矩阵：
- en: '![](img/B14110_07_06.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_07_06.png)'
- en: 'Figure 6: Rewards matrix - Step 4'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：奖励矩阵 - 步骤4
- en: And that's how you initialize the matrix of rewards.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是如何初始化奖励矩阵的方式。
- en: 'But wait—you''re not actually finished. There is one final thing you need to
    do. It''s a step that''s crucial to understand. In fact, let me ask you another
    question, the ultimate one, which will check if your intuition is already shaping
    up:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 但是等一下——你其实还没有完成。还有一件最后的事情需要做。这一步至关重要，需要理解。事实上，让我问你一个问题，最终问题，这将检查你的直觉是否已经开始形成：
- en: '**How can you let the AI know that it has to go to that top priority location
    G?**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**你如何让AI知道它必须去那个优先级最高的地点G？**'
- en: It's easy—you do it simply by playing with the rewards. You must keep in mind
    that with Reinforcement Learning, everything works from the rewards. If you attribute
    a high reward to location **G**, for example 1000, then the AI will automatically
    try to go and catch that high reward, simply because it is larger than the rewards
    of the other locations.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单——你只需通过奖励来玩弄它。你必须记住，在强化学习中，一切都从奖励开始。如果你为地点**G**分配一个高奖励，例如1000，那么AI将自动试图去获取这个高奖励，仅仅因为它比其他地点的奖励要大。
- en: In short, and it's a fundamental point to understand and remember in Reinforcement
    Learning in general, **the AI is always looking for the highest reward**. That's
    why the trick to reach location **G** is simply to attribute it a higher reward
    than the other locations.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这是强化学习中需要理解和记住的一个基本点，**AI始终在寻找最高的奖励**。这就是为什么到达地点**G**的诀窍就是给予它比其他地点更高的奖励。
- en: 'For now, manually put a high reward (1000) inside the cell corresponding to
    location **G**, because it is the goal location where we want our AI to go. Since
    location **G** has an index of 6, we put a 1000 reward on the cell of row 6 and
    column 6\. Accordingly, our matrix of rewards becomes:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，手动在对应地点**G**的单元格中放入一个高奖励（1000），因为它是我们希望AI前往的目标地点。由于地点**G**的索引是6，我们就在第6行第6列的单元格中放置1000的奖励。相应地，我们的奖励矩阵变为：
- en: '![](img/B14110_07_07.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_07_07.png)'
- en: 'Figure 7: Rewards matrix - Step 5'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：奖励矩阵 - 步骤5
- en: You have defined the rewards! You did it by simply building this matrix of rewards.
    It is important to understand that this is usually the way we define the system
    of rewards when doing Q-learning.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经定义了奖励！你通过构建这个奖励矩阵实现了这一点。重要的是要理解，这通常是我们在进行Q学习时定义奖励系统的方式。
- en: In *Chapter 9*, *Going Pro with Artificial Brains – Deep Q-Learning,* which
    is about deep Q-learning, you will see that we will proceed very differently and
    build the environment much more easily. In fact, deep Q-learning is the advanced
    version of Q-learning that is widely used today in AI, far more than the simple
    Q-learning model itself. But you have to tackle Q-learning first, in depth, in
    order to be ready for deep Q-learning.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第9章*，*与人工大脑一同成长——深度Q学习*中，你将看到我们将以非常不同的方式进行，并且构建环境的过程会更加简单。事实上，深度Q学习是Q学习的高级版本，它在今天的AI中被广泛使用，远远超过了简单的Q学习模型。但是，你必须首先深入理解Q学习，才能为深度Q学习做好准备。
- en: Since you've defined the states, the actions, and the rewards, you have finished
    building the environment. This means you are ready to tackle the next step, where
    you will build the AI itself that will do its magic inside this environment that
    you've just defined.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经定义了状态、动作和奖励，你就完成了环境的构建。这意味着你已经准备好处理下一步，构建将在你刚刚定义的这个环境中发挥作用的AI。
- en: Building the AI
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建AI
- en: Now that you have built an environment in which you clearly defined the goal
    with a relevant system of rewards, it's time to build the AI. I hope you're ready
    for a little math.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经构建了一个明确定义了目标并有相关奖励系统的环境，是时候构建AI了。我希望您已经准备好迎接一点数学挑战。
- en: 'I''ll break down this second step into several sub-steps, leading you to the
    final Q-learning model. To that end, we''ll cover three important concepts at
    the heart of Q-learning, in the following order:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我将把第二步分解成几个子步骤，引导你完成最终的Q学习模型。为此，我们将按以下顺序讲解Q学习核心的三个重要概念：
- en: The Q-value
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q值
- en: The temporal difference
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 时间差
- en: The Bellman equation
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 贝尔曼方程
- en: Let's get started by learning about the Q-value.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始了解Q值。
- en: The Q-value
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Q值
- en: 'Before you start getting into the details of Q-learning, I need to explain
    the concept of the Q-value. Here''s how it works:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在你开始深入Q学习的细节之前，我需要解释Q值的概念。它是这样运作的：
- en: 'To each couple of state and action (*s*, *a*), we are going to associate a
    numeric value *Q*(*s*, *a*):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一对状态和动作（*s*，*a*），我们将关联一个数值*Q*（*s*，*a*）：
- en: '![](img/B14110_07_005.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_07_005.png)'
- en: We will say that *Q*(*s*, *a*) is "the Q-value of the action *a* performed in
    the state *s*."
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将说*Q*（*s*，*a*）是“在状态*s*下执行动作*a*的Q值。”
- en: 'Now I know the sort of questions you might be asking in your head: What does
    this Q-value mean? What does it represent? How do I even compute it? These were
    some of the questions I had in my mind when I first learned Q-learning.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我知道你脑海中可能会问这样的问题：这个Q值是什么意思？它代表什么？我到底该怎么计算它？这些都是我第一次学习Q学习时心里想的问题。
- en: In order to answer these questions, I need to introduce the temporal difference.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这些问题，我需要引入时间差。
- en: The temporal difference
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 时间差（Temporal difference）
- en: This is where the math really comes in. Let's say we are in a specific state
    ![](img/B14110_04_001.png), at a specific time *t*. Let's just perform an action
    randomly, any of them. That brings us to the next state ![](img/B14110_07_045.png)
    and we get the reward ![](img/B14110_07_008.png).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是数学真正发挥作用的地方。假设我们处于一个特定的状态！[](img/B14110_04_001.png)，在特定的时间*t*。我们随机执行一个动作，任何一个动作都可以。这样我们就进入了下一个状态！[](img/B14110_07_045.png)，并获得了奖励！[](img/B14110_07_008.png)。
- en: 'The temporal difference at time *t*, denoted by ![](img/B14110_07_038.png),
    is the difference between:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 时间差在时间*t*时刻，记作！[](img/B14110_07_038.png)，是以下两者的差：
- en: '![](img/B14110_07_010.png), that is, the reward ![](img/B14110_07_011.png)
    obtained by performing the action ![](img/B14110_07_012.png) in the state ![](img/B14110_07_013.png),
    plus the Q-value of the best action performed in the future state ![](img/B14110_07_045.png),
    discounted by a factor ![](img/B14110_07_015.png), called the discount factor'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![](img/B14110_07_010.png)，即执行动作！[](img/B14110_07_012.png)在状态！[](img/B14110_07_013.png)下获得的奖励！[](img/B14110_07_011.png)，加上在未来状态！[](img/B14110_07_045.png)下执行的最佳动作的Q值，按一个因子！[](img/B14110_07_015.png)折扣，这个因子被称为折扣因子。'
- en: and ![](img/B14110_07_016.png), that is, the Q-value of the action ![](img/B14110_07_017.png)
    performed in the state ![](img/B14110_07_018.png).
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 和！[](img/B14110_07_016.png)，即在状态！[](img/B14110_07_018.png)下执行动作！[](img/B14110_07_017.png)的Q值。
- en: 'This leads to:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了：
- en: '![](img/B14110_07_019.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_07_019.png)'
- en: You might think that's great, that you understand all the terms, but you're
    probably also thinking "But what does that all mean?" Don't worry—that's exactly
    what I was thinking when I was learning this.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会觉得太棒了，理解了所有术语，但你可能也在想：“那到底是什么意思？”别担心——这正是我在学习这个过程时的想法。
- en: 'I''m going to explain while at the same time improving your AI intuition. The
    first thing to understand is that the temporal difference represents how well
    the AI is learning. Here''s how it works exactly, with respect to the training
    process (during which the Q-values are learned):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我将一边解释，一边提升你的AI直觉。首先要理解的是，时间差（temporal difference）表示AI学习的好坏。它是如何运作的，关于训练过程（在这个过程中Q值被学习）如下：
- en: At the beginning of the training, the Q-values are set to zero. Since the AI
    is looking to get the good rewards (here 1 or 1000), it is looking for the high
    temporal differences (see the formula of TD). Accordingly, if in the first iterations,
    ![](img/B14110_07_020.png) is high, the AI gets a "pleasant surprise" because
    that means the AI was able to find a good reward. On the other hand, if ![](img/B14110_07_021.png)
    is small, the AI gets a "frustration."
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练的开始，Q值被设置为零。由于AI的目标是获取好的奖励（这里是1或1000），它在寻找高的时间差（参见TD公式）。因此，在前几次迭代中，如果！[](img/B14110_07_020.png)很高，AI会得到一个“愉快的惊讶”，因为这意味着AI能够找到一个好的奖励。另一方面，如果！[](img/B14110_07_021.png)很小，AI就会感到“沮丧”。
- en: When the AI gets a great reward, the specific Q-value of the (state, action)
    that led to that great reward increases, so the AI can remember how it got to
    that high reward (you'll see exactly how it increases in the next section). For
    example, let's say that it was the action ![](img/B14110_07_022.png) performed
    in the state ![](img/B14110_07_018.png) that led to that high reward ![](img/B14110_07_024.png).
    That would mean the Q-value ![](img/B14110_07_025.png) increases automatically
    (remember, you'll see how in the next section). Those increased Q-values are important
    information, because they indicate to the AI which transitions lead to the good
    rewards.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 AI 获得了很大的奖励时，导致这个巨大奖励的（状态，动作）的特定 Q 值会增加，这样 AI 就可以记住如何达到那个高奖励（你将在下一节中看到确切的增加方式）。例如，假设是在状态
    ![](img/B14110_07_018.png) 中执行的动作 ![](img/B14110_07_022.png) 导致了那个高奖励 ![](img/B14110_07_024.png)。这意味着
    Q 值 ![](img/B14110_07_025.png) 会自动增加（请记住，你将在下一节中看到具体如何增加）。这些增加的 Q 值是重要信息，因为它们向
    AI 指示了哪些过渡通向了好的奖励。
- en: The next step of the AI is not only to look for the great rewards, but also
    to look at the same time for the high Q-values. Why? Because the high Q-values
    are the ones that lead to the great reward. In fact, the high Q-values are the
    ones that lead to higher Q-values, themselves leading to even higher Q-values,
    themselves leading eventually to the highest reward (1000). That's the role of
    ![](img/B14110_07_026.png) in the temporal difference formula. Everything will
    become crystal clear when you put this into practice. The AI looks for the high
    Q-values, and as soon as it finds them, the Q-values of the (state, action) that
    led to these high Q-values will increase again, since they indicate the right
    path towards the goal.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI 的下一步不仅是寻找很大的奖励，同时也要同时寻找高 Q 值。为什么？因为高 Q 值才是导致大奖励的那些。事实上，高 Q 值会导致更高的 Q 值，它们自己又会导致更高的
    Q 值，最终导致最高奖励（1000）。这就是时差公式中 ![](img/B14110_07_026.png) 的作用。当你将这一切付诸实践时，一切都会变得清晰明了。AI
    寻找高 Q 值，一旦找到，又会增加（状态，动作）的 Q 值，因为它们指示了通向目标的正确路径。
- en: At some point, the AI will know all the transitions that lead to the good rewards
    and high Q-values. Since the Q-values of these transitions have already been increased
    over time, the temporal differences decrease in the end. In fact, the closer we
    get to the final goal, the smaller the temporal differences become.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在某些时候，AI 将知道所有导致好的奖励和高 Q 值的过渡。由于这些过渡的 Q 值随时间已经增加，最终时差会减小。事实上，我们越接近最终目标，时差就变得越小。
- en: In conclusion, the temporal difference is like a temporary intrinsic reward,
    of which the AI will try to find the large values at the beginning of the training.
    Eventually, the AI will minimize this reward as it gets to the end of the training—that
    is, as it gets closer to the final goal.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，时差就像一个临时的内在奖励，AI 会在训练开始时试图找到这些大值。最终，随着训练接近尾声——即接近最终目标时，AI 将最小化这个奖励。
- en: That's exactly the intuition of the temporal difference you must have in mind,
    because it will really help you understand the magic of Q-learning. Speaking of that magic,
    we are about to reveal the last piece of the puzzle.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是你必须记住的时差的直觉，因为它确实会帮助你理解 Q 学习的魔力。说到那种魔力，我们即将揭示这个难题的最后一部分。
- en: Now you understand that the AI will iterate some updates of the Q-values towards
    the high temporal differences, which are ultimately decreased. But how does it
    do that? There is a specific answer to that question—the Bellman equation, the
    most famous equation in Reinforcement Learning.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你明白了，AI 将迭代一些 Q 值向高时差的更新，这些时差最终会减小。但它是如何做到的呢？这个问题有一个具体的答案——贝尔曼方程，强化学习中最著名的方程。
- en: The Bellman equation
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贝尔曼方程
- en: 'In order to perform better and better actions that will lead the AI to reach
    its goal, you have to increase the Q-values of actions when you find high temporal
    differences. Only one question remains: How will the AI update these Q-values?
    Richard Bellman, a pioneer of Reinforcement Learning, created the answer. At each
    iteration, you update the Q-values from time t-1 (previous iteration) to t (current
    iteration) through the following equation, called the Bellman equation:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行更好、更有效的动作，引导 AI 达到目标，当你发现时间差较大时，必须增加动作的 Q 值。剩下的唯一问题是：AI 如何更新这些 Q 值？强化学习的先驱理查德·贝尔曼为此提供了答案。在每次迭代中，你通过以下方程更新从
    t-1（上一次迭代）到 t（当前迭代）的 Q 值，这就是著名的 Bellman 方程：
- en: '![](img/B14110_07_027.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_07_027.png)'
- en: where ![](img/B14110_07_028.png) is the learning rate, which dictates how fast
    the learning of the Q-values goes. Its value is usually between 0 and 1, for example,
    0.75\. The lower the value of ![](img/B14110_07_029.png), the smaller the updates
    of the Q-values, and the longer the Q-learning will take. The higher its value,
    the bigger the updates of the Q-values and the faster the Q-learning will be.
    As you can clearly see in this equation, when the temporal difference ![](img/B14110_07_030.png)
    is high, the Q-value ![](img/B14110_07_031.png) increases.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B14110_07_028.png) 是学习率，决定了 Q 值学习的速度。它的值通常介于 0 和 1 之间，比如 0.75。![](img/B14110_07_029.png)
    的值越低，Q 值的更新就越小，Q-learning 所需的时间就越长。它的值越高，Q 值的更新就越大，Q-learning 的速度就越快。正如你在这个方程中清楚地看到的那样，当时间差
    ![](img/B14110_07_030.png) 较大时，Q 值 ![](img/B14110_07_031.png) 会增加。
- en: Reinforcement intuition
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 强化学习直觉
- en: Now you have all the elements of Q-learning—congratulations, by the way—let's connect
    the dots between all these elements to reinforce your AI intuition.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经掌握了所有 Q-learning 的元素——顺便说一句，祝贺你——让我们将这些元素连接起来，强化你对 AI 的直觉。
- en: The Q-values measure the accumulation of "good surprise" or "frustration" associated
    with the couple of action and state ![](img/B14110_07_032.png).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Q 值衡量的是与一对动作和状态相关的“好惊讶”或“挫败感”的积累 ![](img/B14110_07_032.png)。
- en: In the "good surprise" case of a high temporal difference, the AI is reinforced,
    and in the "frustration" case of a low temporal difference, the AI is weakened.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间差较大的“好惊讶”情况下，AI 会得到强化；而在时间差较小的“挫败感”情况下，AI 会变得较弱。
- en: We want to learn the Q-values that will give the AI the maximum "good surprise,"
    and that's exactly what the Bellman equation does by updating the Q-values at
    each iteration.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望学习出能给 AI 带来最大“好惊讶”的 Q 值，而这正是 Bellman 方程通过在每次迭代中更新 Q 值所做的事情。
- en: You've learned quite a lot of new information, and even though you've finished
    with an intuition section that connects the dots, that's not enough to get a really
    solid grasp of Q-learning. The next step is to take a step back, and the best
    way to do that is to go through the whole Q-learning process from start to finish
    so that it becomes crystal clear in your head.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学到了不少新信息，尽管你已经完成了一个将所有知识点连接起来的直觉部分，但这还不足以真正掌握 Q-learning。下一步是后退一步，最好的方法是从头到尾过一遍完整的
    Q-learning 过程，这样它在你脑中就会变得清晰透彻。
- en: The whole Q-learning process
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完整的 Q-learning 过程
- en: Let's summarize the different steps of the whole Q-learning process. To be clear,
    the only purpose of this process is to update the Q-values over a certain number
    of iterations until they are no longer updated (we refer to that point as convergence).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下整个 Q-learning 过程的不同步骤。为了明确，整个过程的唯一目的是在一定数量的迭代中更新 Q 值，直到 Q 值不再更新（我们称这一点为收敛）。
- en: The number of iterations depends on the complexity of the problem. For our problem,
    1,000 will be enough, but for more complex problems you might want to consider
    higher numbers such as 10,000\. In short, the Q-learning process is the part where
    we train our AI, and it's called Q-learning because it's the process during which
    the Q-values are learned. Then I'll explain what happens for the inference part (pure
    predictions), which comes, as always, after the training. The full Q-learning
    process starts with training mode.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代次数取决于问题的复杂性。对于我们的问题，1,000 次迭代就足够了，但对于更复杂的问题，你可能需要考虑更高的迭代次数，比如 10,000 次。简而言之，Q-learning
    过程就是我们训练 AI 的部分，之所以叫 Q-learning，是因为这是一个学习 Q 值的过程。接下来，我会解释推理部分（纯预测）发生了什么，这部分总是在训练后进行。完整的
    Q-learning 过程从训练模式开始。
- en: Training mode
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模式
- en: '**Initialization (First iteration)**:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**初始化（第一次迭代）**：'
- en: For all couples of states *s* and actions *a*, the Q-values are initialized
    to 0.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有状态 *s* 和动作 *a* 的组合，Q值被初始化为 0。
- en: '**Next iterations**:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**下一次迭代**：'
- en: 'At each iteration *t* ≥ 1, you repeat for a certain number of times (chosen
    by you the developer) the following steps:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代 *t* ≥ 1 时，你会重复以下步骤若干次（由你作为开发者选择）：
- en: You select a random state ![](img/B14110_07_033.png) from the possible states.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你从可用的状态中选择一个随机状态 ![](img/B14110_07_033.png)。
- en: From that state, you perform a random action ![](img/B14110_07_034.png) that
    can lead to a next possible state, that is, such that ![](img/B14110_07_035.png).
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从该状态出发，你执行一个随机动作 ![](img/B14110_07_034.png)，该动作可以导致下一个可能的状态，即使得 ![](img/B14110_07_035.png)。
- en: You reach the next state ![](img/B14110_07_0451.png) and you get the reward
    ![](img/B14110_07_037.png).
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你到达下一个状态 ![](img/B14110_07_0451.png)，并获得奖励 ![](img/B14110_07_037.png)。
- en: You compute the temporal difference ![](img/B14110_07_0381.png):![](img/B14110_07_039.png)
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你计算时间差异 ![](img/B14110_07_0381.png):![](img/B14110_07_039.png)
- en: You update the Q-value by applying the Bellman equation:![](img/B14110_07_027.png)
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你通过应用贝尔曼方程更新 Q值：![](img/B14110_07_027.png)
- en: At the end of this process, you have obtained Q-values that no longer update.
    That means only one thing; you are ready to hack the maze by going into inference
    mode.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程结束时，你将获得不再更新的 Q值。这意味着只有一个结果：你准备好通过进入推理模式来破解迷宫了。
- en: Inference mode
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理模式
- en: 'The training is complete, and now begins the inference. To remind you, the
    inference part is when you have a fully trained model with which you are going
    to make predictions. In our maze, the predictions that you are going to make are
    the actions to perform to take you from start (Location **E**) to finish (Location
    **G**). So, the question is:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成，现在开始推理。提醒一下，推理部分是当你有了一个完全训练好的模型，准备用来进行预测时。在我们的迷宫中，你将要做出的预测是执行哪些动作，以便将你从起点（位置
    **E**）带到终点（位置 **G**）。所以，问题是：
- en: How are you going to use the learned Q-values to perform the actions?
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何使用学习到的 Q值来执行动作？
- en: 'Good news; for Q-learning this is very simple. When in a certain state ![](img/B14110_07_018.png),
    you simply perform the action ![](img/B14110_07_042.png) that has the highest
    Q-value for that state ![](img/B14110_04_001.png):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息；对于 Q-learning 这是非常简单的。当处于某一状态 ![](img/B14110_07_018.png) 时，你只需执行在该状态下具有最高
    Q值的动作 ![](img/B14110_07_042.png)：
- en: '![](img/B14110_07_044.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_07_044.png)'
- en: That's all—by doing this at each location (each state), you get to your final
    destination through the shortest route. We'll implement this and see the result
    in the practical activities or the next chapter.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样——通过在每个位置（每个状态）执行此操作，你将通过最短路径到达最终目的地。我们将在实际活动或下一章中实现这个过程并查看结果。
- en: Summary
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter we studied the Q-learning model, which is only applied to environments
    that have a finite number of input states and a finite number of possible actions
    to perform.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们研究了 Q-learning 模型，该模型仅应用于具有有限数量输入状态和有限数量可执行动作的环境。
- en: When performing Q-learning, the AI learns Q-values through an iterative process,
    so that the higher the Q-value of a (state, action) pair, the closer the AI gets
    to the top reward.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行 Q-learning 时，AI 通过迭代过程学习 Q值，从而使得 (状态，动作) 对的 Q值越高，AI 越接近最高奖励。
- en: At each iteration the Q-values are updated through the Bellman equation, which
    simply consists of adding the temporal difference, discounted by a learning rate
    factor. We will get to work on a full practical Q-learning activity in the next
    chapter, applied to a real-world business problem.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，Q值通过贝尔曼方程进行更新，该方程仅仅是将时间差异与学习率因子折扣相加。我们将在下一章中进行完整的实际 Q-learning 活动，将其应用于一个真实的商业问题。
